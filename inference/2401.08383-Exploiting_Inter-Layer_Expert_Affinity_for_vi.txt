# Khai thác Ái lực Chuyên gia Liên tầng để Tăng tốc Suy luận Mô hình Hỗn hợp Chuyên gia

Jinghan Yao, Quentin Anthony, Aamir Shafi, Hari Subramoni, Dhabaleswar K. (DK) Panda
Khoa Khoa học Máy tính và Kỹ thuật
Đại học Bang Ohio
Columbus, OH, Hoa Kỳ
{yao.877, anthony.301, shafi.16, subramoni.1 }@osu.edu, panda@cse.ohio-state.edu

**Tóm tắt** —Trong lĩnh vực các mô hình ngôn ngữ lớn (LLM) như Transformer Tạo sinh Được đào tạo trước (GPT), mô hình Hỗn hợp Chuyên gia (MoE) đã nổi lên như một kỹ thuật mạnh mẽ để tăng cường khả năng biểu đạt và độ chính xác của mô hình. Tuy nhiên, việc triển khai các mô hình GPT MoE cho suy luận song song trên các hệ thống phân tán đặt ra những thách thức đáng kể, chủ yếu do việc truyền thông Alltoall rộng rãi cần thiết cho việc định tuyến và tổng hợp chuyên gia. Nút thắt cổ chai truyền thông này làm trầm trọng thêm bối cảnh tính toán đã phức tạp, cản trở việc sử dụng hiệu quả các tài nguyên tính toán hiệu năng cao. Trong bài báo này, chúng tôi đề xuất một kỹ thuật tối ưu hóa nhẹ được gọi là ExFlow, để phần lớn tăng tốc suy luận của các mô hình MoE này. Chúng tôi có một góc nhìn mới về việc giảm bớt chi phí truyền thông bằng cách khai thác ái lực chuyên gia liên tầng. Không giống các phương pháp trước đây, giải pháp của chúng tôi có thể được áp dụng trực tiếp cho các mô hình MoE được đào tạo trước mà không cần tinh chỉnh hay suy giảm độ chính xác. Bằng cách đề xuất song song chuyên gia nhất quán ngữ cảnh trên các hệ thống phân tán, thiết kế ExFlow của chúng tôi chỉ sử dụng một truyền thông Alltoall để cung cấp chức năng tương tự trong khi các phương pháp trước đây đều yêu cầu hai Alltoall. Bằng cách kiểm tra cẩn thận xác suất có điều kiện trong việc định tuyến token qua nhiều tầng, chúng tôi đã chứng minh rằng các mô hình GPT MoE được đào tạo trước ngầm thể hiện ái lực chuyên gia liên tầng mạnh mẽ. Sau đó chúng tôi thiết kế một mô hình lập trình số nguyên hiệu quả để nắm bắt chính xác các đặc điểm như vậy và cho thấy bằng cách đặt các chuyên gia một cách phù hợp trên các GPU tương ứng, chúng tôi có thể giảm đến 67% độ trễ định tuyến token qua GPU trên các cấu hình phần cứng và tô-pô khác nhau. Giải pháp của chúng tôi vượt trội hơn Deepspeed-MoE tiên tiến trong các mô hình GPT MoE với số chuyên gia từ 8 đến 64, với cải thiện thông lượng suy luận lên đến 2.2x. Theo hiểu biết của chúng tôi, đây là công trình đầu tiên trong việc khai thác ái lực chuyên gia liên tầng để tăng tốc suy luận của các mô hình GPT MoE. Chúng tôi cũng cung cấp một nghiên cứu chi tiết về cách mô hình ngầm thu được ái lực chuyên gia này ở giai đoạn đào tạo rất sớm và cách ái lực này phát triển và ổn định trong quá trình đào tạo.

**Từ khóa chỉ mục** —Hỗn hợp chuyên gia, Suy luận song song, Truyền thông tập thể, Mô hình tạo sinh, Hệ thống phân tán

## I. GIỚI THIỆU

Trong bối cảnh phát triển của trí tuệ nhân tạo (AI) và học sâu (DL), mô hình Hỗn hợp Chuyên gia (MoE) [1]–[6] đã nổi lên như một kỹ thuật then chốt, tăng cường hiệu quả và khả năng thích ứng của các mô hình. MoE hoạt động dựa trên nguyên tắc phân phối nhiệm vụ giữa các chuyên gia chuyên biệt trong một kiến trúc mô hình rộng hơn, tự động định tuyến đầu vào

*Nghiên cứu này được hỗ trợ một phần bởi các tài trợ NSF #1818253, #1854828, #1931537, #2007991, #2018627, #2112606, #2311830, #2312927, và tài trợ XRAC #NCR-130002.

![Hình 1: Cho một mô hình MoE được đào tạo trước, (a) chiến lược đặt chuyên gia thông thường gây ra truyền thông qua GPU dày đặc. (b) khai thác ái lực chuyên gia liên tầng có thể tránh truyền thông Alltoall không cần thiết.]

đến chuyên gia thành thạo nhất dựa trên ngữ cảnh. Trong khi MoE là một kỹ thuật không phụ thuộc vào lĩnh vực đã đạt được thành công trong các lĩnh vực như thị giác [7], [8], MoE đã đặc biệt có ích cho các mô hình ngôn ngữ lớn (LLM) [9]–[11] trong việc mở rộng khả năng mô hình hóa ngôn ngữ trong khi kiểm soát chi phí tính toán. Mặc dù mạnh mẽ trong việc mở rộng quy mô khả năng của các mô hình ngôn ngữ lớn, nó thường yêu cầu các chiến lược song song đặc biệt để giảm bớt yêu cầu bộ nhớ, vì việc chứa tất cả chuyên gia trên một GPU là không khả thi do mỗi chuyên gia thực tế là một mạng feed-forward (FFN) lớn.

### A. Phát biểu Vấn đề

Các mô hình làm việc MoE [6], [12]–[14] hiện tại nghiêm ngặt yêu cầu hai Alltoall tập thể trong mỗi tầng MoE, vì theo quyết định định tuyến được đưa ra bởi hàm cổng, mỗi GPU sẽ đầu tiên phân tán đầu vào của nó đến các chuyên gia trên các GPU khác, và sau đó thu thập chúng trở lại sau khi tính toán. Tùy thuộc vào số lượng GPU liên quan, các tập thể Alltoall sẽ tạo ra chi phí đáng kể. Các giải pháp hiện có [13], [14] giới thiệu tổn thất nhận biết tô-pô trong quá trình đào tạo, cố gắng để hàm cổng định tuyến nhiều token hơn đến các GPU cục bộ với độ trễ truyền thông thấp hơn, tuy nhiên, trong khi nó có thể tăng tốc đào tạo, ràng buộc heuristic này cản trở hiệu suất của mô hình và trở nên không hợp lệ trong giai đoạn suy luận một khi tô-pô phần cứng thay đổi. Do bản chất tốn nhiều tài nguyên của việc đào tạo LLM và các kịch bản suy luận đa dạng, một thiết kế định tuyến MoE hiệu quả truyền thông, có thể áp dụng toàn cầu vẫn là một yêu cầu cấp bách.

### B. Động lực

Trong một tầng Hỗn hợp Chuyên gia (MoE), mỗi mô hình chuyên gia chuyên biệt trong một lĩnh vực kiến thức riêng biệt [15], [16]. Các mô hình MoE hiện đại thường xếp chồng nhiều tầng MoE như vậy để tại mỗi tầng, đầu vào sẽ được định tuyến đến một hoặc một vài chuyên gia. Kiến thức lĩnh vực mà mỗi chuyên gia chịu trách nhiệm có thể khác nhau trên các mô hình và đào tạo khác nhau. Tuy nhiên, đối với một mô hình MoE được đào tạo trước, chúng tôi tò mò về việc liệu có tồn tại một số mối tương quan giữa việc lựa chọn chuyên gia qua các tầng MoE khác nhau. Nói cách khác, đối với một mô hình MoE được đào tạo trước với nhiều chuyên gia mỗi tầng, cho một token đầu vào, nếu chúng ta biết rằng nó được định tuyến đến một chuyên gia cụ thể tại tầng i, xác suất của điểm đến định tuyến token tại các tầng tiếp theo là gì? Liệu nó sẽ hoàn toàn ngẫu nhiên? Hay một số chuyên gia sẽ thể hiện xác suất cao hơn để được chọn làm điểm đến tiếp theo?

Hình 2 cho thấy bản đồ nhiệt của sở thích định tuyến trên một mô hình GPT [17], [18] MoE được đào tạo trước. Mô hình bao gồm 12 tầng MoE, và mỗi tầng có 32 chuyên gia, tham khảo II để biết thêm chi tiết. Chúng tôi chọn bốn cặp tầng liên tiếp và theo dõi việc lựa chọn chuyên gia của các token trong các tầng này. Trục Y mô tả chuyên gia tại tầng trước, và trục X mô tả chuyên gia tại tầng sau. Khối đỏ tại tọa độ (x, y) đại diện cho xác suất có điều kiện của các token được định tuyến đến chuyên gia y tại tầng i sau đó được định tuyến đến chuyên gia x tại tầng i+1. Màu càng đậm, xác suất có điều kiện càng cao. Nhìn vào cả bốn bản đồ nhiệt, chúng ta có thể quan sát rõ ràng rằng việc lựa chọn chuyên gia không ngẫu nhiên và các quyết định định tuyến ở các tầng trước sẽ ảnh hưởng lớn đến các quyết định định tuyến của tầng sau, và điều này đúng cho bất kỳ tầng nào trong mô hình. Do đó, chúng tôi định nghĩa xác suất có điều kiện như vậy trong việc lựa chọn chuyên gia qua các tầng khác nhau là ái lực chuyên gia. [19], [20] trước đây đã quan sát hiện tượng này, tuy nhiên nó chưa được nghiên cứu rộng rãi, thúc đẩy cuộc khám phá sâu sắc của chúng tôi và đề xuất tối ưu hóa tiếp theo.

### C. Các Giải pháp Đề xuất

Trong bài báo này, chúng tôi giới thiệu một góc nhìn mới về việc tối ưu hóa các mô hình Alltoall MoE bằng cách vượt ra ngoài các tầng MoE riêng lẻ và khai thác ái lực của các chuyên gia liên tầng. Với việc kiểm tra cẩn thận song song dữ liệu ngầm trong song song chuyên gia hiện tại, chúng tôi đề xuất một thiết kế nhất quán ngữ cảnh trong đó mỗi GPU giữ ngữ cảnh của tất cả các token đang xử lý, sau đó cho phép chúng tôi trực tiếp cắt giảm một nửa số hoạt động Alltoall trong mỗi tầng MoE. Hơn nữa, bằng cách khai thác ái lực chuyên gia qua nhiều tầng MoE, chúng tôi có thể giảm đến 40% trao đổi dữ liệu trong truyền thông Alltoall còn lại, mà không tạo ra bất kỳ bản sao nào của các chuyên gia bổ sung không thuộc về GPU rank hiện tại. Chiến lược tối ưu hóa của chúng tôi được điều chỉnh cho các tô-pô khác nhau của các nút tính toán, tận dụng băng thông phân cấp của việc sao chép bộ nhớ GPU, NVLINK [21] nội bộ nút, và mạng liên nút. Giải pháp của chúng tôi có thể được áp dụng nhanh chóng cho các mô hình GPT MoE được đào tạo trước khác nhau mà không cần đào tạo lại hoặc tinh chỉnh trước đó trên mô hình, và được đảm bảo mang lại lợi ích bất kể số lượng chuyên gia có thể được lưu trữ trong khả năng bộ nhớ của một GPU duy nhất.

Theo hiểu biết của chúng tôi, đây là công trình đầu tiên trong việc khám phá ái lực chuyên gia liên tầng để tăng tốc suy luận GPT MoE được đào tạo trước. Do đó, sau khi kiểm tra kỹ lưỡng các mô hình thịnh hành trong song song chuyên gia, chúng tôi liệt kê các đóng góp của mình như sau:

1) Chúng tôi khai thác tính chất ái lực chuyên gia tồn tại ngầm trong các mô hình GPT MoE được đào tạo trước hiện tại bằng cách nắm bắt xác suất có điều kiện kết hợp của các quyết định định tuyến chuyên gia qua nhiều tầng MoE.

2) Chúng tôi phân tích kỹ lưỡng các mô hình tính toán và truyền thông trong các chiến lược song song hiện có, và đề xuất song song chuyên gia nhất quán ngữ cảnh để giảm lớn số lượng tập thể Alltoall trong các mô hình GPT MoE hiện tại.

3) Chúng tôi thiết kế một thuật toán ngoại tuyến hiệu quả nhưng chính xác để nắm bắt ái lực chuyên gia trong bất kỳ mô hình GPT MoE được đào tạo trước nào bằng cách công thức hóa nó như một bài toán Lập trình Tuyến tính Số nguyên, cho phép các giải pháp gần tối ưu. Kết quả thu được thông báo cho chiến lược đặt chuyên gia trong quá trình tải mô hình GPU.

4) Chúng tôi đề xuất một giải pháp tối ưu hóa song song chuyên gia mới dựa trên nhất quán ngữ cảnh và ái lực chuyên gia, có tên ExFlow. Nó là bất khả tri triển khai và dễ dàng áp dụng cho bất kỳ mô hình GPT MoE đã cho. ExFlow có thể tăng tốc đáng kể suy luận của các mô hình này.

5) Chúng tôi so sánh ExFlow với các khung suy luận MoE tiên tiến hiện có trên nhiều mô hình GPT MoE được đào tạo trước. Đối với các mô hình GPT MoE-16/32/64, giải pháp của chúng tôi cung cấp tới 56%/65%/67% giảm truyền thông Alltoall

và cải thiện thông lượng suy luận 120%/60%/80%, tương ứng.

## II. BỐI CẢNH

### A. Mô hình Chuyên gia

Các mô hình chuyên gia, đặc biệt trong các lĩnh vực thị giác và ngôn ngữ, hoạt động dựa trên nguyên tắc chuyên biệt hóa phân tán, trong đó mỗi chuyên gia chịu trách nhiệm cho một tập con cụ thể của lĩnh vực kiến thức tổng thể. Những chuyên gia này thường là các mô-đun mạng nơ-ron được đào tạo để xuất sắc trong các nhiệm vụ như nhận dạng đối tượng, phân tích cú pháp ngữ nghĩa, hoặc phân tích cảm xúc. Hệ thống tổng thể sử dụng cơ chế cổng để định tuyến dữ liệu đầu vào đến các chuyên gia có liên quan nhất, qua đó tận dụng kiến thức cụ thể lĩnh vực để cải thiện hiệu suất. Song song chuyên gia là một chiến lược tính toán cho phép thực thi đồng thời các chuyên gia này, tăng tốc đáng kể suy luận và đào tạo. Điều này quan trọng để xử lý dữ liệu quy mô lớn và các mô hình phức tạp, vì nó cho phép phân phối tải tính toán qua nhiều bộ gia tốc phần cứng.

Chuyển sang các mô hình ngôn ngữ lớn (LLM), kiến trúc Hỗn hợp Chuyên gia (MoE) đã có vai trò quan trọng trong việc mở rộng quy mô khả năng của chúng mà không tăng tuyến tính chi phí tính toán. MoE tích hợp nhiều chuyên gia vào một mô hình duy nhất và sử dụng cơ chế cổng mềm để kết hợp đầu ra của chúng. Tuy nhiên, song song chuyên gia trong MoE cần thiết hai hoạt động Alltoall quan trọng: 1) định tuyến dữ liệu đầu vào đến các chuyên gia phù hợp, và 2) tổng hợp đầu ra từ tất cả chuyên gia cho dự đoán cuối cùng. Trong khi các hoạt động này là cần thiết cho chức năng của mô hình, chúng gây ra chi phí truyền thông đáng kể, đặc biệt trong các kịch bản suy luận phân tán liên quan đến nhiều GPU. Chi phí này có thể trở thành nút thắt cổ chai, cản trở khả năng mở rộng và hiệu quả của hệ thống.

### B. Chiến lược Cổng và Tối ưu hóa

Trong các mô hình Hỗn hợp Chuyên gia (MoE), hàm cổng [1] là một thành phần quan trọng định tuyến dữ liệu đầu vào đến các chuyên gia chuyên biệt, tối ưu hóa hiệu suất thông qua kiến thức cụ thể lĩnh vực. Cổng GShard [22] sử dụng phương pháp dựa trên softmax, tập trung vào hiệu quả tính toán nhưng có khả năng làm trầm trọng thêm chi phí truyền thông do phương pháp bất khả tri đối với tô-pô phần cứng. Mặt khác, cổng nhận biết tô-pô [13], [14] giảm thiểu chi phí này bằng cách giới thiệu các điều khoản tổn thất bổ sung vào mục tiêu đào tạo, có nhạy cảm với tô-pô phần cứng. Tuy nhiên, điều này cần thiết đào tạo lại mô hình từ đầu, một nỗ lực tốn nhiều tài nguyên đặc biệt cho các mô hình quy mô lớn như GPT.

Bảng I so sánh các khung MoE tiên tiến khác nhau với thiết kế đề xuất của chúng tôi. Trong khi cổng nhận biết tô-pô có thể hiệu quả giảm chi phí truyền thông trong quá trình đào tạo, lợi ích của nó không áp dụng được trong quá trình suy luận nếu tô-pô phần cứng thay đổi. Hạn chế này đặt ra vấn đề không nhất quán đáng kể cho các mô hình như GPT, thường được triển khai trên các cấu hình phần cứng khác nhau. Yêu cầu đào tạo lại với cổng nhận biết tô-pô cũng thêm một lớp gánh nặng tính toán, làm cho nó trở thành lựa chọn ít thực tế hơn cho các mô hình đã được đào tạo và triển khai. Quan trọng hơn, các tối ưu hóa hiện có trên các hàm cổng vẫn còn trong tầng MoE riêng lẻ, trong khi thất bại trong việc điều tra có hệ thống luồng dữ liệu tổng thể qua nhiều tầng MoE trong mô hình.

## III. THÁCH THỨC

### A. Địa phương Dữ liệu

Như được hiển thị trong Hình 3, các phương pháp song song chuyên gia phổ biến tích hợp cả song song dữ liệu (DP) và song song mô hình (MP) [23]. Trong thiết lập này, DP đảm bảo rằng các token và ngữ cảnh liên quan của chúng, như được duy trì bởi các GPU riêng lẻ, vẫn riêng biệt. Mặt khác, MP đảm bảo mỗi GPU giữ quyền sở hữu độc quyền của các chuyên gia riêng biệt. Do đó, hai truyền thông Alltoall là không thể thiếu tại mỗi tầng MoE. Đầu tiên, trong tính toán attention [24], các token chú ý đến ngữ cảnh của chúng trên GPU cục bộ, và sau đó hàm cổng sẽ xác định điểm đến chuyên gia của mỗi token. Alltoall đầu tiên sẽ định tuyến các token trên mỗi GPU đến các chuyên gia mục tiêu trên các GPU khác, và điều này được gọi là phân phối token. Một khi Alltoall đầu tiên hoàn thành, mỗi GPU sẽ đưa các token nhận được vào các chuyên gia mà nó giữ. Lưu ý rằng vì các chuyên gia về cơ bản là các FFN chỉ thực hiện biến đổi phi tuyến trên các token, chúng không yêu cầu thông tin ngữ cảnh nào, không giống mô-đun attention trước đó. Tuy nhiên, bởi vì nhiều tầng MoE được xếp chồng,

![Hình 3: Do song song chuyên gia hiện tại bao gồm dữ liệu song song, các GPU khác nhau không chia sẻ ngữ cảnh của các token. Do đó, T2 cần quay lại GPU 2 để thực hiện attention trong tầng tiếp theo.]

các mô-đun attention trong tầng sau sẽ yêu cầu các token được căn chỉnh với ngữ cảnh cục bộ, do đó ở cuối mỗi tầng, một truyền thông Alltoall khác được thực hiện để thu thập các token đã phân phối đó. Do đó, để loại bỏ hoạt động Alltoall thứ hai, chúng ta cần vượt qua ràng buộc địa phương của song song dữ liệu, sao cho các token luôn có thể chú ý đến ngữ cảnh tương ứng của chúng bất kể GPU đang cư trú của chúng.

### B. Ánh xạ Ái lực Chuyên gia vào Tô-pô Phần cứng

Vì mục tiêu là giảm thiểu độ trễ suy luận của các mô hình MoE với song song chuyên gia, chúng tôi quan tâm đến việc giảm chi phí truyền thông Alltoall càng nhiều càng tốt. Do đó, việc xác định ái lực chuyên gia trong một mô hình MoE được đào tạo trước là bước đầu tiên của chúng tôi, việc ánh xạ phù hợp ái lực này vào phần cứng cơ bản là một nhiệm vụ quan trọng. Cho một mô hình MoE được đào tạo trước, suy luận có thể xảy ra trên nhiều cấu hình phần cứng khác nhau, do đó chúng tôi cần một thuật toán ánh xạ toàn cầu để thiết kế của chúng tôi có thể thích ứng liền mạch với các tô-pô không đồng nhất mà không cần thiết bất kỳ sửa đổi hoặc tinh chỉnh nào của mô hình MoE.

## IV. THIẾT KẾ EXFLOW

Trong phần này, chúng tôi đầu tiên giới thiệu song song chuyên gia nhất quán ngữ cảnh. Chúng tôi sẽ cố gắng vượt qua ràng buộc địa phương dữ liệu như đã đề cập ở trên, vì điều này quan trọng khi chúng tôi sau đó khai thác ái lực chuyên gia. Sau đó chúng tôi sẽ giới thiệu cách mô hình hóa ái lực chuyên gia một cách hiệu quả, và sử dụng nó để tăng tốc lớn suy luận của các mô hình GPT MoE.

### A. Nhất quán Ngữ cảnh Token trong Song song Chuyên gia

Trước khi đi sâu vào thiết kế của chúng tôi, chúng tôi muốn xem xét lại pipeline suy luận của các mô hình GPT. Cho một mô hình GPT được đào tạo trước và một yêu cầu suy luận, nó lấy l từ mỗi yêu cầu làm đầu vào, điều này thường được gọi là Prompts. Prompts về cơ bản là các token. Khi mô hình cố gắng tạo phản hồi cho yêu cầu này, nó sẽ tham khảo prompts để biết thông tin ngữ cảnh. Vì GPT thực sự là một mô hình tạo sinh, nó sẽ tạo ra một hoặc một vài từ trong mỗi lần lặp, và nối chúng vào các token prompt hiện tại, sau đó trở thành ngữ cảnh để tạo từ trong lần lặp tiếp theo. Quan trọng, một khi được tạo ra, các token này vẫn không thay đổi trong các lần lặp tiếp theo, phục vụ hoàn toàn như ngữ cảnh và không trải qua các biến đổi hoặc cập nhật thêm bởi mạng. Để đơn giản, chúng tôi sử dụng thuật ngữ ngữ cảnh để đại diện cho prompts cũng như các token được tạo trong các lần lặp trước. Giả sử chúng ta có n GPU trong nhóm song song dữ liệu, để đạt được nhất quán ngữ cảnh token qua tất cả GPU trong nhóm, chúng tôi sẽ tập trung vào cả phần trước suy luận và sau lần lặp liên quan đến quá trình suy luận MoE tổng thể, như được hiển thị trong Hình 4.

![Hình 4: Trước suy luận, chúng tôi sử dụng Allgather để đảm bảo mỗi GPU có tất cả ngữ cảnh. Sau mỗi lần lặp, một Allgather khác được thực hiện trên các token mới được tạo, sau đó chúng tôi nối chúng vào ngữ cảnh hiện tại cho lần lặp tiếp theo.]

Tại bắt đầu suy luận, GPU i có gi yêu cầu, i ∈ {1, 2, . . . , n}, chúng tôi sẽ đầu tiên thực hiện truyền thông AllGather qua các GPU trong đó mỗi GPU sẽ phát sóng gi ngữ cảnh của nó đến tất cả các GPU khác. Sau đó, mỗi GPU trong nhóm sẽ có Σi=1^n gi ngữ cảnh, có nghĩa là tất cả ngữ cảnh hiện tại nhất quán qua tất cả GPU. Lưu ý rằng, mặc dù mỗi GPU hiện có ngữ cảnh từ các GPU khác, nó vẫn sẽ chỉ tạo token cho các yêu cầu của riêng mình, tuân thủ các nguyên tắc song song dữ liệu.

Khi hoàn thành lần lặp, mỗi GPU đã tạo ra một số token cho các yêu cầu của riêng mình, để đảm bảo ngữ cảnh vẫn nhất quán trong lần lặp tiếp theo, chúng ta cần mỗi GPU phát sóng các token mới được tạo này đến các GPU khác. Do đó, chúng tôi cũng thực hiện một hoạt động AllGather bổ sung qua nhóm. Bằng cách làm như vậy, tại đầu một lần lặp mới, mỗi GPU có ngữ cảnh cập nhật của tất cả các yêu cầu.

Lợi ích của việc đảm bảo nhất quán ngữ cảnh token là gì? Trong Hình 3, chúng tôi đã đề cập lý do tại sao song song chuyên gia hiện tại nghiêm ngặt yêu cầu hai hoạt động Alltoall, đó là do loại trừ dữ liệu vì song song chuyên gia hiện tại ngầm thể hiện song song dữ liệu. Bây giờ, vì các ngữ cảnh nhất quán và hiển thị trên tất cả GPU cho tất cả token, một token có thể thực hiện tính toán attention tại chỗ với ngữ cảnh của nó, bất kể GPU nào nó hiện đang ở.

Hình 16 cho thấy một mô hình MoE-8 3 tầng chạy trên 4 GPU, trong đó mỗi GPU giữ hai chuyên gia mỗi tầng. Chúng ta có token 1 trên GPU 1 và token 2 trên GPU 3. Token 1 sẽ được định tuyến đến E0 tại tầng 0, E4 tại tầng 1, E2 tại tầng 2 tương ứng. Token 2 sẽ được định tuyến đến E5 tại tầng 0, E5 tại tầng 1, E4 tại tầng 2 tương ứng. Hình 16(a) mô tả đường đi mà token 1 và token 2 sẽ di chuyển theo mô hình song song chuyên gia thông thường. Sau mỗi tầng, cả hai token cần quay lại GPU gốc của chúng để tính toán attention. Đáng chú ý, đối với token 2, ngay cả khi tất cả các chuyên gia trên đường đi của nó đều ở GPU 2, nó vẫn phải thường xuyên quay lại GPU 3 vì ngữ cảnh của nó không nhất quán hiển thị trên GPU 2. Đối với ví dụ đã cho, token 1 yêu cầu bốn truyền thông qua GPU, và token 2 yêu cầu sáu truyền thông qua GPU. Hình 16(b) minh họa, tuy nhiên, đường đi cả hai token đi khi chúng tôi sử dụng song song chuyên gia nhất quán ngữ cảnh. Đối với token 1, khi nó được định tuyến đến GPU 0 tại tầng 0, nó hoàn thành FFN E0, và thực hiện tính toán attention tại chỗ với ngữ cảnh được lưu trữ trên GPU 0. Sau đó, nó không phải quay lại GPU 1, thay vào đó, nó có thể trực tiếp đi đến E4 tại tầng 1, điều này tiết kiệm 1 truyền thông qua GPU. Đối với token 2, cải thiện là phi thường, vì tất cả các chuyên gia trên đường đi của nó đều ở GPU 2, nó chỉ yêu cầu một truyền thông qua GPU tại tầng 0, sau đó, tất cả FFN và attention có thể được thực hiện tại chỗ trên GPU 2. Lưu ý rằng, hàm cổng được chia sẻ giữa tất cả GPU, để bất kể token trên GPU nào, hàm cổng luôn có thể định tuyến nó đến chuyên gia đúng.

Bảng I cho thấy khối lượng truyền thông tổng thể trong song song chuyên gia nhất quán ngữ cảnh của chúng tôi so với các phương pháp hiện có, như FasterMoE [13], TA-MoE [14], và Deepspeed-MoE [12]. Trong thiết kế của chúng tôi, chúng tôi cắt giảm một nửa Alltoall, trong khi giới thiệu một AllGather ở cuối mỗi lần lặp. Chúng tôi thấy rằng khi mô hình có nhiều tầng hơn, chi phí của AllGather trở nên ít quan trọng hơn vì nó chỉ xảy ra tại tầng cuối cùng.

### B. Mô hình hóa Ái lực Chuyên gia Liên tầng

Trong phần này, chúng tôi sẽ thảo luận về cách mô hình hóa ái lực chuyên gia trong các mô hình GPT MoE được đào tạo trước và cách ái lực hướng dẫn chúng tôi trong việc giảm truyền thông Alltoall. Đầu tiên, vì chúng tôi đang tìm cách xác định mô hình lựa chọn chuyên gia, chúng tôi cần một tập hợp các token mà mô hình sẽ suy luận và chúng ta có thể theo dõi việc lựa chọn chuyên gia của chúng tại mỗi tầng. Ở đây chúng tôi lấy mẫu các token từ bộ dữ liệu Pile [25] để phân tích mô hình định tuyến chuyên gia, một nghiên cứu chi tiết hơn về lấy mẫu token sẽ được thảo luận trong V-G.

Trong Hình 2, chúng tôi cho thấy bản đồ nhiệt của việc lựa chọn chuyên gia các tầng liên tiếp trong một mô hình GPT 350M MoE-32 được đào tạo trước. Chúng tôi đạt được điều này bằng cách tính toán xác suất có điều kiện qua các chuyên gia trong các tầng liên tiếp. Ở đây chúng tôi đưa ra một dạng toán học của ái lực chuyên gia. Cho một mô hình MoE được đào tạo trước với E chuyên gia mỗi tầng, giả sử chúng ta có N token, ký hiệu bởi Tk, k ∈ {1, 2, . . . , N}, và chúng tôi ký hiệu chuyên gia thứ i tại tầng j là Ei,j. Khi đó ái lực chuyên gia giữa Ei,j và Ep,j+1 có thể được đóng gói bởi xác suất có điều kiện sau:

P(Ep,j+1|Ei,j) = 1/N ∑(k=1 to N) P(Ep,j+1|Ei,j, Tk)     (1)

Đối với chuyên gia Ei,j, mục tiêu của chúng tôi là tìm một chuyên gia EA*,j+1, sao cho:

1/n ∑(k=1 to n) P(EA*,j+1|Ei,j, Tk) ≥ 1/N ∑(k=1 to N) P(Ep,j+1|Ei,j, Tk)

cho tất cả p ∈ {1, 2, . . . , E} với i ≠ A*     (2)

Chúng tôi do đó khẳng định chuyên gia EA*,j+1 là chuyên gia có ái lực cao nhất với chuyên gia Ei,j. Ái lực P(EA*,j+1|Ei,j) này làm sáng tỏ khả năng của các token tại Ei,j tiếp theo được định tuyến đến EA*,j+1. Chạy một mô hình với 8 chuyên gia mỗi tầng (E = 8) trên tám GPU, ví dụ, sẽ dẫn đến mỗi GPU giữ 1 chuyên gia mỗi tầng. Về cơ bản, đối với bất kỳ hai tầng liên tiếp nào, chúng ta có tám cặp chuyên gia. Việc đặt chiến lược những chuyên gia có ái lực này trên các GPU giống hệt nhau đảm bảo rằng một token, một khi được định tuyến đến một GPU, thể hiện xu hướng cao để ở lại GPU đó, cho rằng các chuyên gia có ái lực cao nhất của nó trong các tầng tiếp theo cũng cư trú trên cùng GPU đó. Tuy nhiên, một thách thức phát sinh khi công thức 2 suy ra EA*,j+1 cho nhiều chuyên gia từ tầng j, dẫn đến lặp lại. Điều này cần thiết một chiến lược toàn diện để xác định ái lực chuyên gia tối ưu toàn cầu.

Hơn nữa, khi GPU có khả năng lớn hơn, có nghĩa là mỗi GPU có thể giữ nhiều hơn một chuyên gia mỗi tầng, như được hiển thị trong Hình 16, không gian tìm kiếm trở nên lớn hơn nhiều. Cho khả năng C1 của một GPU duy nhất, ký hiệu số lượng chuyên gia nó sẽ giữ mỗi tầng, vấn đề trước đó thay đổi thành như sau:

Cho các chuyên gia
Ex1,j, Ex2,j, Ex3,j, ..., ExC1,j,
trong đó x1,...,C1 ∈ {1, 2, . . . , E}     (3)

Chúng tôi muốn tìm các chuyên gia
Ey1,j+1, Ey2,j+1, Ey3,j+1, ..., EyC1,j+1,
trong đó y1,...,C1 ∈ {1, 2, . . . , E}     (4)

tối đa hóa xác suất có điều kiện kết hợp sau:

1/n ∑(k=1 to n) ∑(p=1 to C1) ∑(q=1 to C1) P(Eyq,j+1|Exp,j, Tk)     (5)

Việc giải quyết hàm mục tiêu tổng hợp này đảm bảo các token được định tuyến đến các chuyên gia x1, x2, x3, ..., xC tại tầng j thể hiện xu hướng cao để được định tuyến tiếp theo đến y1, y2, y3, ..., yC tại tầng j + 1.

Hình 16(c) cho thấy cách giải pháp cho các vấn đề trên có thể hướng dẫn chúng ta trong việc đặt chuyên gia sao cho khối lượng truyền thông Alltoall có thể được giảm thiểu. Ví dụ, nếu chúng ta biết rằng các chuyên gia E0 và E4 tại tầng 0 có ái lực kết hợp cao với các chuyên gia E4 và E7 tại tầng 1, chúng ta có thể đặt chúng lên GPU 0. Tương tự, chúng ta có thể tìm thấy các chuyên gia E2 và E5 tại tầng 2 có ái lực cao với các chuyên gia trước đó. Chúng tôi thấy rằng với việc đặt này, token 1 chỉ cần 1 truyền thông Alltoall để đến GPU 0, và nó sẽ đơn giản thực hiện tính toán attention tại chỗ vì tất cả các chuyên gia liên quan của nó trong các tầng sau đều ở GPU 0.

### C. Ái lực Chuyên gia Theo giai đoạn

Trong Hình 1(b), chúng tôi mô tả một kịch bản phức tạp hơn nhưng thực tế, trong đó mỗi GPU giữ bốn chuyên gia mỗi tầng. Vì các cụm hiện đại tận dụng NVLINK cho truyền thông nội bộ nút, và kết nối tốc độ cao cho liên nút. Nếu một token vẫn cần được định tuyến đến một chuyên gia bên ngoài GPU hiện tại của nó, chúng ta muốn chuyên gia đó được giữ tại một GPU nội bộ nút (ký hiệu bởi đường nét đứt đỏ), thay vì một GPU trên nút khác (ký hiệu bởi đường nét đứt xanh), vì thực hiện truyền thông nội bộ nút có băng thông cao hơn nhiều và độ trễ thấp hơn. Do đó, đối với các GPU trên cùng nút, chúng ta muốn các chuyên gia mà chúng giữ cũng thể hiện một mức độ ái lực nào đó với nhau. Trong trường hợp này, mỗi chuyên gia có hai mức độ ái lực. Mức độ ái lực đầu tiên là với các chuyên gia trên cùng GPU, trong khi mức độ ái lực thứ hai là với các chuyên gia trên cùng nút. Do đó, chúng ta có thể thêm một ràng buộc vào công thức 5 trước đó, do đó các chuyên gia trên một GPU hiện có dạng ái lực sau:

1/N ∑(k=1 to N) ∑(p=1 to C1) ∑(q=1 to C1) P(Eyq,j+1|Exp,j, Tk) +
1/N ∑(k=1 to N) ∑(p=1 to C1) ∑(o=1 to C2-C1) P(Eyo,j+1|Exp,j, Tk)     (6)

Lưu ý rằng C2 ký hiệu khả năng chuyên gia mỗi tầng của toàn bộ nút, và Eyo,j+1 đại diện cho chuyên gia yo tại tầng j + 1 được giữ bởi các GPU nội bộ nút khác.

### D. Giải quyết tính nhị nguyên của Ái lực bằng Lập trình Số nguyên

Trong công thức 2, 5, và 6, chúng tôi cung cấp các hàm mục tiêu đơn giản để tìm một cách đặt chuyên gia như vậy đảm bảo ái lực chuyên gia tốt nhất. Tuy nhiên, như chúng tôi đã đề cập, các hàm mục tiêu này chỉ đứng trong góc độ của một GPU. Việc tìm chiến lược đặt chuyên gia tốt nhất cho tất cả GPU trên tất cả nút, tuy nhiên, là một bài toán tối ưu hóa tổ hợp phức tạp.

Để khắc phục điều này, chúng tôi xoay trục tập trung vào Bài toán Đối ngẫu Lagrange liên quan. Ý tưởng là biến đổi bài toán tối đa hóa của chúng tôi thành một bài toán tối thiểu hóa tương đương, có thể tính toán dễ dàng hơn. Về cơ bản, ái lực cao ngụ ý định tuyến lại token tối thiểu. Do đó, tính nhị nguyên nảy sinh từ các mục tiêu đan xen này: một tìm kiếm củng cố tích cực thông qua ái lực, và cái khác nhắm mục tiêu giảm thiểu các gián đoạn đối với ái lực này. Để chuyển đổi sang bài toán đối ngẫu, chúng ta phải thiết lập mối quan hệ giữa việc tối đa hóa ái lực tổng hợp này và giảm thiểu chi phí định tuyến lại token. Điều này tự nhiên dẫn chúng ta xem xét các gián đoạn đối với ái lực, có thể được định lượng như chi phí định tuyến lại token giữa các GPU.

Cho mục tiêu của chúng tôi để giảm thiểu định tuyến lại token, chúng ta hình thành hàm đối ngẫu g(λ):

g(λ, Ei,j) = inf Ep,j+1 [P(Ep,j+1|Ei,j) − λG(Ep,j+1, Ei,j)]     (7)

Trong đó G(Ep,j+1, Ei,j) đại diện cho chi phí liên quan đến định tuyến lại token do việc lựa chọn chuyên gia trên các GPU khác nhau, và λ phục vụ như một điều khoản chính quy hóa để cân bằng ái lực và định tuyến lại token.

Vì truyền thông liên nút luôn có độ trễ cao nhất và băng thông thấp nhất, ưu tiên đầu tiên của chúng tôi là giảm lượng định tuyến liên nút. Hình 1 (b) cho thấy một ví dụ về ái lực chuyên gia theo giai đoạn, trong đó mũi tên xanh lá cây ký hiệu ái lực chuyên gia cao, mũi tên đỏ ký hiệu mức độ ái lực trung bình, và mũi tên xanh dương ký hiệu ái lực chuyên gia thấp nhất. Mục tiêu của chúng tôi là giữ các chuyên gia ái lực cao bên trong các GPU đơn lẻ, và giữ các chuyên gia ái lực trung bình bên trong các nút đơn lẻ. Trong giai đoạn 1, chúng tôi sẽ giảm định tuyến liên nút càng nhiều càng tốt, và trong giai đoạn 2, chúng tôi sẽ giảm thiểu định tuyến nội bộ nút dựa trên kết quả giai đoạn 1. Do đó, chúng tôi đề xuất một hàm mục tiêu không có hệ số theo chiến lược tối ưu hóa từ trên xuống.

Công thức 8 là hàm mục tiêu, trong đó x^p_i,j là một biến nhị phân, ký hiệu liệu Ei,j có được giữ bởi GPU p, Rk,j là một biến nhị phân, bằng một ký hiệu rằng đối với một token k tại tầng j, nó sẽ được định tuyến đến một chuyên gia bên ngoài nút hiện tại (hoặc GPU). Lưu ý rằng, ở đây chúng tôi áp dụng hàm mục tiêu hoàn toàn giống hệt cho cả giai đoạn 1 và 2 (như đã đề cập ở trên). Trong giai đoạn 1, Rk,j = 1 đại diện cho một định tuyến liên nút, trong khi trong giai đoạn 2, nó đại diện cho một định tuyến nội bộ nút. Đối với các ràng buộc, chúng ta cần tất cả các nút (hoặc GPU) được cân bằng tải, có nghĩa là đối với mỗi tầng, mỗi nút (hoặc GPU) nên giữ cùng số lượng chuyên gia, được định nghĩa bởi công thức 9, trong đó E ký hiệu tổng số chuyên gia mỗi tầng, và P ký hiệu tổng số nút (hoặc GPU). Hơn nữa, để hoàn chỉnh, chúng ta cần công thức 10 để đảm bảo mỗi chuyên gia được giữ độc quyền bởi một nút (hoặc GPU). Công thức 11 và 12 là các bất đẳng thức cần thiết ánh xạ việc đặt chuyên gia với việc liệu một định tuyến cụ thể có phải là qua nút (hoặc GPU) hay không. Sau khi giải quyết bài toán lập trình tuyến tính số nguyên trên, biến x^p_i,j trong giải pháp sẽ được sử dụng trực tiếp như chiến lược đặt chuyên gia khi tải mô hình MoE lên GPU.

Minimize ∑(k=1 to N) ∑(j=1 to L-1) Rk,j     (8)

Biến x^p_i,j ∈ {0,1}, Rk,j ∈ {0,1}

Tùy thuộc vào ∑(i=1 to E) x^p_i,j = E/P,
∀j ∈ {1, 2, . . . , L}, p ∈ {1, 2, . . . , P}     (9)

∑(p=1 to P) x^p_i,j = 1,
∀j ∈ {1, 2, . . . , L}, i ∈ {1, 2, . . . , E}     (10)

Rk,j ≥ x^p_i,j − x^p_i,j+1     (11)

Rk,j ≥ x^p_i,j+1 − x^p_i,j     (12)

### E. Tính không nhạy cảm của Ái lực Chuyên gia

Để nắm bắt chính xác ái lực chuyên gia liên tầng trong một mô hình MoE được đào tạo trước, chúng ta cần sử dụng đủ token và theo dõi quyết định định tuyến của chúng tại mỗi tầng MoE. Vì mục tiêu là tăng tốc suy luận, chúng ta cũng cần điều tra liệu ái lực chuyên gia có không nhạy cảm với phân phối của bộ dữ liệu. Lý do là chúng ta không thể dự đoán các token thực tế và ngữ cảnh của chúng khi mô hình đang phục vụ các yêu cầu từ người dùng, do đó, ái lực chuyên gia mà chúng ta học được từ bộ dữ liệu ngoại tuyến phải vẫn hiệu quả trong suy luận trực tuyến. Chúng tôi sẽ phân tích thêm điều này trong thí nghiệm.

## V. ĐÁNH GIÁ THỰC NGHIỆM

### A. Thiết lập

**Phần cứng:** Chúng tôi tiến hành tất cả các thí nghiệm trên cụm GPU Wilkes3 Ampere, trong đó mỗi nút có 2 bộ xử lý AMD EPYC 7763 64-Core, và 4 GPU NVIDIA A100-SXM4-80GB, được kết nối bởi NVLINK. Đối với liên nút, nó được trang bị kết nối InfiniBand HDR200 Mellanox dual-rail.

**Các mô hình:** Chúng tôi sử dụng thư viện Deepspeed-Megatron [26], [27] để đào tạo trước. Trong quá trình suy luận, tất cả các mô hình đều với cổng Top-1 và khả năng token biến đổi.¹

| Mô hình | Cơ sở | Chuyên gia | Tầng | D |
|---------|-------|------------|------|---|
| MoE GPT-M | 350M | 8 | 24 | 1024 |
| | | 16 | | |
| | | 32 | | |
| | | 64 | | |
| MoE GPT-M | 470M | 32 | 32 | 1024 |
| | 590M | 40 | | |
| MoE GPT-XL | 1.3B | 16 | 24 | 2048 |

**BẢNG II:** Danh sách các mô hình MoE chúng tôi sử dụng cho thí nghiệm.

**Bộ dữ liệu:** Chúng tôi chia bộ dữ liệu Pile [25] thành một tập đào tạo và một tập đánh giá. Tập đào tạo được sử dụng để đào tạo mô hình, trong quá trình đào tạo, chúng tôi ghi lại các quyết định định tuyến chuyên gia của các token tại mỗi tầng. Chúng tôi giải quyết hàm mục tiêu 8 dựa trên nhật ký theo dõi và sau đó xác định chiến lược đặt chuyên gia. Sau đó, chúng tôi tải mô hình lên GPU theo chiến lược đặt và đánh giá hiệu suất trên tập đánh giá.

![Hình 6: Trên các mô hình GPT MoE được đào tạo trước khác nhau, chúng tôi thay đổi kích thước song song chuyên gia và kiểm tra chi phí truyền thông tổng thể. 32L và 40L ký hiệu các mô hình GPT với 32 và 40 tầng.]

¹Mã sẽ có sẵn tại https://github.com/YJHMITWEB/ExFlow.

### B. Giảm Truyền thông Tập thể với Song song Chuyên gia Nhất quán Ngữ cảnh

Trong song song chuyên gia thông thường, mỗi tầng MoE nghiêm ngặt yêu cầu hai tập thể Alltoall vì các token cần được thu thập bởi GPU tương ứng của chúng trong nhóm song song dữ liệu để thực hiện tính toán attention. Với thiết kế nhất quán ngữ cảnh của chúng tôi, tuy nhiên, ngữ cảnh của tất cả các token nhất quán và hiển thị trên mỗi GPU, có nghĩa là chúng ta không còn cần Alltoall thứ hai để truy xuất token, thay vào đó, chúng có thể thực hiện tính toán attention tại chỗ trên bất kỳ GPU nào. Hình 6 cho thấy chi phí truyền thông trong song song chuyên gia gốc và thiết kế nhất quán ngữ cảnh của chúng tôi.

Chúng tôi đã kiểm tra mô hình GPT 350M với 8, 16, 32, và 64 chuyên gia mỗi tầng tương ứng, và phát hiện ra rằng với nhất quán ngữ cảnh, một tỷ lệ lớn truyền thông Alltoall trở nên dư thừa vì các token sẽ thực hiện tính toán attention tại chỗ. Lưu ý rằng việc giảm chi phí truyền thông Alltoall là hơn 50%, lý do là các token có thể tìm thấy chuyên gia của chúng trên các GPU cục bộ mặc dù các chuyên gia này không được tải theo cách nhận biết tô-pô, do đó chúng có thể được định tuyến trực tiếp đến những chuyên gia đó mà không quay lại GPU gốc của chúng. Ngoài ra, chúng tôi thấy chi phí sử dụng AllGather để đảm bảo nhất quán ngữ cảnh ở cuối mỗi lần lặp là không đáng kể trong các trường hợp 8 và 16-GPU. Mặc dù nó trở nên hơi nặng hơn với 32 và 64 GPU, truyền thông tổng thể vẫn ít hơn nhiều so với baseline. Ngoài ra, khi mô hình có 32 và 40 tầng, AllGather trở nên ít quan trọng hơn.

### C. Giảm Định tuyến Token Qua GPU với Ái lực Chuyên gia

![Hình 7: Được đánh giá trên GPT 350M MoE-64 được đào tạo trước, các thanh ký hiệu tỷ lệ phần trăm trung bình của các token được định tuyến đến chuyên gia trên GPU hiện tại của chúng. Biểu đồ cho thấy có bao nhiều truyền thông qua GPU được giảm với thiết kế ái lực chuyên gia của chúng tôi.]

Hình 7 minh họa việc giảm truyền thông qua GPU từ hai góc độ. Đầu tiên, cho một mô hình MoE-64, khi sử dụng ít GPU hơn để thực hiện song song chuyên gia, mỗi GPU sẽ giữ nhiều chuyên gia hơn mỗi tầng. Ví dụ, khi sử dụng 4 GPU, mỗi GPU giữ 16 chuyên gia mỗi tầng, so với khi sử dụng 32 GPU, mỗi GPU chỉ có thể giữ 2 chuyên gia mỗi tầng, do đó một token có thể có khả năng được định tuyến đến các GPU khác cao hơn. Khung Deepspeed baseline không có bất kỳ tối ưu hóa nào về việc đặt các chuyên gia liên tầng, có nghĩa là các token có thể được định tuyến đến bất kỳ GPU nào với cơ hội như nhau. Trong thiết kế của chúng tôi, vì chúng tôi khai thác ái lực chuyên gia giữa các tầng, trên 4 GPU, chúng ta có thể quan sát trung bình hơn một nửa số token không tham gia vào truyền thông Alltoall. Khi mở rộng ra 8 GPU, thiết kế ái lực chuyên gia của chúng tôi có thể giữ 40% token vẫn ở trên cùng GPU, trong khi baseline giảm mạnh. Khi chúng tôi tải mô hình với 32 GPU, chúng tôi vẫn có thể duy trì ở 28%. Hơn nữa, biểu đồ mô tả cải thiện trong việc giảm số lượng token đi ra thực sự sẽ yêu cầu truyền thông Alltoall, trong đó 40% truyền thông được tiết kiệm khi sử dụng 4 GPU, và 25% được tiết kiệm khi sử dụng 32 GPU.

### D. Giảm Định tuyến Token Liên nút với Ái lực Chuyên gia

![Hình 8: Tương tự như hình trước, nhưng ở đây các thanh ký hiệu tỷ lệ phần trăm trung bình của các token được định tuyến đến chuyên gia trên nút hiện tại của chúng. Biểu đồ cho thấy có bao nhiều truyền thông liên nút được giảm với thiết kế ái lực chuyên gia của chúng tôi.]

Các xu hướng tương tự có thể được quan sát trong Hình 8, trong đó chúng tôi đo số lượng token được định tuyến đến các chuyên gia nội bộ nút. Vì ái lực chuyên gia theo giai đoạn của chúng tôi nhằm giảm định tuyến liên nút với ưu tiên cao nhất, nhiều token có khả năng ở lại trong cùng nút thay vì được định tuyến đến các chuyên gia trên các nút khác. Trong các thí nghiệm của chúng tôi, chúng tôi thấy rằng với thiết kế ái lực chuyên gia, các token trung bình có khả năng ở lại trong cùng nút cao gấp 2 lần mà không tham gia vào truyền thông liên nút.

![Hình 9: Tỷ lệ chi phí Alltoall so với thời gian dành cho tính toán. Ở đây chúng tôi chỉ đo bốn hoạt động quan trọng nhất trong mô hình MoE, vì những hoạt động khác là không đáng kể.]

### E. Đánh giá Suy luận GPT MoE

Sau khi kiểm tra tính chất của định tuyến cục bộ token, bây giờ chúng tôi thực hiện kiểm tra suy luận đầu cuối đến cuối trên toàn bộ mô hình được đào tạo trước. Khi sử dụng số lượng nút khác nhau để thực hiện song song chuyên gia, tỷ lệ chi phí Alltoall thay đổi rất nhiều. Hình 9 mô tả tỷ lệ mà mỗi hoạt động chiếm trong mô hình MoE. Khi chỉ sử dụng một nút, tất cả GPU được kết nối nội bộ qua NVLINK, có tốc độ cao và độ trễ thấp, do đó, chi phí truyền thông Alltoall

khoảng 15%, và tính toán chiếm ưu thế thời gian tổng thể. Trong tình huống này, sẽ không có quá nhiều không gian cho chúng ta để tối ưu hóa.

Khi chúng tôi bao gồm nhiều nút tính toán hơn, chi phí của Alltoall trở nên quan trọng hơn trong song song chuyên gia thông thường. Khi sử dụng 2 nút, chúng tôi quan sát sự tăng vọt của chi phí Alltoall lên khoảng 63% tổng thời gian. Khi mở rộng ra 8 nút, suy luận gần như hoàn toàn bị ràng buộc bởi truyền thông, với 76% thời gian dành cho Alltoall. Hình 10 cho thấy bốn mô hình GPT 350M MoE được đào tạo trước khác nhau dưới một loạt cấu hình song song. Đối với mô hình MoE-8, chúng tôi sử dụng 4 và 8 GPU để thực hiện song song chuyên gia, vì chi phí Alltoall trở nên nổi bật hơn trong truyền thông liên nút, chiến lược ái lực chuyên gia của chúng tôi mang lại 10% tăng tốc khi sử dụng 8 GPU. Đối với mô hình MoE-16, chúng tôi quan sát tăng tốc đáng kể nhất 2.2x được đạt được khi mỗi GPU giữ 2 chuyên gia mỗi tầng. Khi mở rộng ra 16 GPU trong đó mỗi GPU chỉ giữ 1 chuyên gia mỗi tầng, cải thiện khoảng 20%. Trong các mô hình MoE-32, khi mô hình chạy trên 8 và 16 GPU, các phương pháp của chúng tôi có thể đạt được tăng tốc 1.6x. Và tương tự, đối với mô hình MoE-64, lợi ích cao nhất về thông lượng là khi mỗi GPU giữ 8, 4, 2 chuyên gia mỗi tầng.

Bằng cách kiểm tra xu hướng trong các kết quả này, chúng tôi tìm thấy một hành vi thú vị rằng khi mỗi GPU giữ nhiều chuyên gia hơn, thiết kế nhất quán ngữ cảnh và ái lực chuyên gia có thể mang lại nhiều lợi ích hiệu suất hơn vì nó có thể khai thác lớn ái lực chuyên gia trong mỗi GPU, có nghĩa là nó có thể tiết kiệm hầu hết các truyền thông Alltoall. Tuy nhiên, khi mỗi GPU chỉ giữ 1 chuyên gia mỗi tầng, ái lực chuyên gia sẽ chủ yếu ở mức nội bộ nút, ví dụ MoE-32 trên 32 GPU, và MoE-64 trên 64 GPU. Trong các trường hợp này, chi phí trong việc giới thiệu nhiều nút hơn trong truyền thông trở nên nổi bật so với những gì chúng ta tiết kiệm với ái lực chuyên gia nội bộ nút. Đối với trường hợp 4-GPU, mặc dù mỗi GPU giữ nhiều chuyên gia mỗi tầng, không có nhiều lợi ích hiệu suất do chi phí Alltoall nội bộ nút không đáng kể trên hệ thống phần cứng mà các thí nghiệm của chúng tôi được tiến hành.

![Hình 10: Thông lượng suy luận mô hình GPT MoE đầu cuối đến cuối. Chúng tôi kiểm tra 7 biến thể của các mô hình được đào tạo trước với nhiều nút tính toán, mỗi nút có 4 GPU. Kết quả được chuẩn hóa để trực quan hóa tốt hơn.]

### F. Các Tính chất Phát triển của Ái lực Chuyên gia trong quá trình Đào tạo Mô hình MoE

Trong phần này, chúng tôi muốn điều tra cách ái lực chuyên gia phát triển với việc đào tạo mô hình. Hình 11 cho thấy tỷ lệ định tuyến chuyên gia tại đầu đào tạo. Ở đây chúng tôi chỉ cho thống kê của tầng MoE cuối cùng để đơn giản, vì chúng tôi xác nhận rằng các tầng MoE khác có phân phối tương tự. Tại khởi đầu đào tạo, mô hình thể hiện phân phối rất lệch, chỉ ra sự mất cân bằng rõ rệt giữa các chuyên gia, phù hợp với kết quả trong các nghiên cứu khác [13]. Tuy nhiên, khi đào tạo tiến bộ, một phân phối đều và cân bằng hơn được quan sát. Điều này cũng được phản ánh trong Hình 12a, trong đó chúng tôi đo ái lực chuyên gia bằng cách giải công thức 8 tại các lần lặp khác nhau của đào tạo. Hàng trăm lần lặp đầu tiên chỉ thấy một vài chuyên gia được kích hoạt thường xuyên mỗi tầng MoE, và mô hình thực sự có thể có một ái lực chuyên gia rất

![Hình 11: Tỷ lệ token được định tuyến đến mỗi chuyên gia tại tầng MoE cuối cùng, mỗi màu đại diện cho một chuyên gia. Hình này cho thấy lần lặp đào tạo 0 đến 2000, khi đào tạo bắt đầu với các tham số mô hình ngẫu nhiên, hàng trăm lần lặp đầu tiên thấy một vài chuyên gia nhận được hầu hết các token. Các mô hình được đào tạo với tổn thất GShard, do đó, tất cả đều thể hiện cân bằng tải trong việc lựa chọn chuyên gia.]

cao vì hầu hết các token được định tuyến đến một tập hợp chuyên gia cố định tại mỗi tầng. Sau khi vượt qua giai đoạn ban đầu, phân phối định tuyến chuyên gia trở nên đa dạng, và do đó ái lực giảm vì có nhiều chuyên gia tham gia vào định tuyến. Sau 2k lần lặp đầu tiên của đào tạo, mô hình bắt đầu thể hiện ái lực chuyên gia ổn định hơn nhiều và nó tiếp tục tăng khi các chuyên gia trở nên cụ thể lĩnh vực hơn, do đó ái lực trở nên nổi bật hơn giữa các chuyên gia tại các tầng khác nhau.

![Hình 12: Điều tra ái lực chuyên gia khi mô hình được đào tạo từ đầu. Ái lực được chia tỷ lệ để trực quan hóa tốt hơn.]

### G. Cần Bao nhiêu Token để Nắm bắt Ái lực Chuyên gia trong Mô hình Được đào tạo trước?

Trong V-A, chúng tôi đã đề cập ngắn gọn về cách giải quyết đúng cách bài toán lập trình tuyến tính số nguyên 8 để có được ái lực chuyên gia trong một mô hình MoE được đào tạo trước. Tuy nhiên, trong thực tế, vì bộ dữ liệu Pile chứa hàng trăm tỷ token, việc theo dõi và ghi lại tất cả các quyết định định tuyến của token là không khả thi. Do đó, chúng tôi chọn lấy mẫu ngẫu nhiên một phần token. Hình 13 cho thấy tăng tốc tương đối trong truyền thông Alltoall khi chúng tôi sử dụng số lượng token khác nhau để nắm bắt ái lực chuyên gia. Vì ái lực chuyên gia về cơ bản là một dạng xác suất có điều kiện giữa các sở thích định tuyến chuyên gia liên tầng, việc sử dụng thông tin của nhiều token hơn chắc chắn sẽ cho một xấp xỉ tốt hơn. Ở đây, chúng tôi thấy rằng cho các mô hình GPT MoE được đào tạo trước, chúng tôi thường chỉ cần hàng nghìn token để nắm bắt chính xác ái lực chuyên gia. Đối với các mô hình MoE-8, 1000 token là đủ, và đối với các mô hình MoE-64, 3000 token là đủ. Do đó, công thức 8 có thể được giải quyết hiệu quả bằng cách chỉ kiểm tra nhiều token này.

![Hình 13: Số lượng token được lấy mẫu ngẫu nhiên sử dụng để khai thác ái lực chuyên gia và tăng tốc tương đối của nó trong suy luận. Các mô hình với nhiều chuyên gia hơn mỗi tầng yêu cầu nhiều token hơn để nắm bắt chính xác ái lực chuyên gia.]

### H. Tính nhất quán trên Bộ dữ liệu Ngoài phân phối

Trong triển khai, các mô hình MoE có thể được sử dụng trong các kịch bản trong đó ngữ cảnh khác với bộ dữ liệu đào tạo của nó. Khi đó câu hỏi là, các mô hình MoE có luôn thể hiện ái lực chuyên gia tương tự bất kể ngữ cảnh của dữ liệu đầu vào? Nói cách khác, ái lực chuyên gia có thực sự là một tính chất nội tại của mô hình MoE được đào tạo trước?

| Pile [25] | C4 [28] | Dolma [29] | Yelp [30] |
|-----------|---------|------------|-----------|
| Intra-GPU | 1.000 | 0.998 | 0.998 | 1.005 |
| Intra-Node | 1.000 | 0.997 | 0.989 | 1.003 |

**BẢNG III:** Sử dụng Pile để phân tích ái lực chuyên gia và kiểm tra trên C4, Dolma, và Yelp. Các số được chuẩn hóa theo hàng.

Bảng III cho thấy ái lực chuyên gia trên các bộ dữ liệu không được bao gồm trong dữ liệu đào tạo. Chúng tôi sử dụng bộ dữ liệu Pile [25] để phân tích ái lực chuyên gia của mô hình GPT 350M MoE-32, sau đó chúng tôi trực tiếp đo liệu ái lực chuyên gia này có đúng trên ba bộ dữ liệu khác, cụ thể là C4 [28], Dolma [29], và Yelp Reviews [30]. Việc đặt chuyên gia mà chúng tôi giải quyết từ bộ dữ liệu Pile cho thấy ái lực gần như giống hệt trên các bộ dữ liệu ngoài phân phối khác, chứng minh ái lực chuyên gia là một đặc tính vốn có trong các mô hình MoE được đào tạo trước.

## VI. CÔNG TRÌNH LIÊN QUAN

Trong khi nhiều công trình trước đây tồn tại để tối ưu hóa bước đào tạo trước của các mô hình MoE. Trong khi các phương pháp của chúng để đạt được điều này khác nhau như kết hợp song song chuyên gia với các chiến lược song song khác như song song tensor [31] và trình tối ưu hóa phân mảnh [6], hoặc phát triển các kernel định tuyến tối ưu hóa [32], [33], hoặc sử dụng CPU và SSD offload [34], hầu hết các chiến lược được tối ưu hóa cho mô hình đào tạo MoE đào tạo trước và phần cứng. Công trình của chúng tôi bổ sung cho những điều này, và được áp dụng độc quyền tại thời điểm suy luận.

Jiamin Li và cộng sự đã đề xuất mức độ phổ biến chuyên gia [19] giữa hai tầng liên tiếp. Tuy nhiên, họ chỉ tính toán các chuyên gia top-k phổ biến tại mỗi tầng MoE và sau đó tạo bản sao của những chuyên gia phổ biến nhất đó trên các GPU cục bộ. Điều này tương tự với công thức 2 trong phần phương pháp của chúng tôi. Như chúng tôi đã thảo luận, điều này chỉ đảm bảo một tối ưu cục bộ cho các chuyên gia cụ thể, do đó, thay vì thực hiện tối ưu hóa đặt chuyên gia toàn cầu, họ sử dụng bộ nhớ bổ sung để chứa những chuyên gia phổ biến này cục bộ. Trong thiết kế của chúng tôi, chúng tôi không cần những bản sao rõ ràng của các chuyên gia phổ biến như vậy vì chúng tôi hình thành nó như một hàm mục tiêu tối ưu hóa toàn cầu. Trong các trường hợp cực đoan nhất, trong đó bộ nhớ GPU chỉ có thể chứa một chuyên gia mỗi tầng, phương pháp của chúng tôi vẫn có thể cung cấp tăng tốc bằng cách tận dụng ái lực chuyên gia nội bộ nút, như được hiển thị trong Hình 10.

Jiaao He và cộng sự đã đề xuất FasterMoE [13], và Chang Chen và cộng sự [14] giới thiệu TA-MoE, cả hai tối ưu hóa đào tạo mô hình MoE quy mô lớn thông qua các chiến lược cổng nhận biết tô-pô. Tuy nhiên, tính hợp lệ của các chiến lược này bị tổn hại đáng kể trong quá trình suy luận do bản chất thay đổi của các tô-pô phần cứng. Sự khác biệt trong các cấu hình phần cứng trong suy luận làm cho phương pháp cổng nhận biết tô-pô trở nên không hiệu quả, nhấn mạnh một hạn chế quan trọng của các phương pháp này trong việc thích ứng với các môi trường phần cứng động. Mingshu Zhai và cộng sự đã đề xuất SmartMoE [35], trong đó họ điều tra một chiến lược ngoại tuyến để đào tạo tối ưu hóa các mô hình MoE. Nó chủ yếu xoay quanh việc phân tách không gian song song lai thành các nhóm tĩnh, thực sự cũng là để giải quyết một bài toán tối ưu hóa tổ hợp.

## VII. ĐÓNG GÓP VÀ KẾT LUẬN

Tóm lại, chúng tôi đã giới thiệu ExFlow, một kỹ thuật tối ưu hóa mới tăng tốc đáng kể suy luận của các mô hình Hỗn hợp Chuyên gia (MoE) dựa trên GPT trong các hệ thống phân tán. Bằng cách khai thác ái lực chuyên gia liên tầng vốn có, ExFlow loại bỏ một truyền thông Alltoall quan trọng, giảm cả độ trễ và chi phí truyền thông. Phương pháp của chúng tôi tận dụng mô hình lập trình số nguyên để đặt chuyên gia tối ưu, tạo điều kiện giảm đến 67% độ trễ định tuyến qua GPU và cải thiện thông lượng lên đến 120% so với các phương pháp hiện có, mà không hy sinh độ chính xác của mô hình. Những tiến bộ này không chỉ cung cấp một giải pháp có thể mở rộng cho suy luận dựa trên MoE mà còn cung cấp những hiểu biết có giá trị về việc thu nhận giai đoạn sớm và ổn định của ái lực chuyên gia trong đào tạo mô hình, do đó mở đường cho nghiên cứu tương lai trong lĩnh vực này.

**TÀI LIỆU THAM KHẢO**

[1] W. Fedus, B. Zoph, and N. Shazeer, "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity," The Journal of Machine Learning Research, vol. 23, no. 1, pp. 5232–5270, 2022. 1, 3

[2] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean, "Outrageously large neural networks: The sparsely-gated mixture-of-experts layer," arXiv preprint arXiv:1701.06538, 2017. 1

[3] Y. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Zhao, A. M. Dai, Q. V. Le, J. Laudon et al., "Mixture-of-experts with expert choice routing," Advances in Neural Information Processing Systems, vol. 35, pp. 7103–7114, 2022. 1

[4] S. Masoudnia and R. Ebrahimpour, "Mixture of experts: a literature survey," Artificial Intelligence Review, vol. 42, pp. 275–293, 2014. 1

[5] J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang, "FastMoE: A fast mixture-of-expert training system," arXiv preprint arXiv:2103.13262, 2021. 1

[6] M. Artetxe, S. Bhosale, N. Goyal, T. Mihaylov, M. Ott, S. Shleifer, X. V. Lin, J. Du, S. Iyer, R. Pasunuru et al., "Efficient large scale language modeling with mixtures of experts," arXiv preprint arXiv:2112.10684, 2021. 1, 2, 10

[7] S. Shen, Z. Yao, C. Li, T. Darrell, K. Keutzer, and Y. He, "Scaling Vision-Language Models with Sparse Mixture of Experts," 2023. 1

[8] C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. S. Pinto, D. Keysers, and N. Houlsby, "Scaling Vision with Sparse Mixture of Experts," 2021. 1

[9] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al., "Language models are unsupervised multitask learners," OpenAI blog, vol. 1, no. 8, p. 9, 2019. 1

[10] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020. 1

[11] OpenAI, "GPT-4 Technical Report," 2023. 1

[12] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y. Aminabadi, A. A. Awan, J. Rasley, and Y. He, "Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale," in International Conference on Machine Learning. PMLR, 2022, pp. 18 332–18 346. 2, 4, 5

[13] J. He, J. Zhai, T. Antunes, H. Wang, F. Luo, S. Shi, and Q. Li, "FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models," in Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, 2022, pp. 120–134. 2, 3, 4, 5, 9, 11

[14] C. Chen, M. Li, Z. Wu, D. Yu, and C. Yang, "TA-MoE: Topology-Aware Large Scale Mixture-of-Expert Training," Advances in Neural Information Processing Systems, vol. 35, pp. 22 173–22 186, 2022. 2, 3, 4, 5, 11

[15] T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste, "Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks," The Journal of Machine Learning Research, vol. 22, no. 1, pp. 10 882–11 005, 2021. 2

[16] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson, "How transferable are features in deep neural networks?" Advances in neural information processing systems, vol. 27, 2014. 2

[17] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., "Improving language understanding by generative pre-training," 2018. 2

[18] R. Child, S. Gray, A. Radford, and I. Sutskever, "Generating long sequences with sparse transformers," arXiv preprint arXiv:1904.10509, 2019. 2

[19] J. Li, Y. Jiang, Y. Zhu, C. Wang, and H. Xu, "Accelerating Distributed MoE Training and Inference with Lina," in 2023 USENIX Annual Technical Conference (USENIX ATC 23), 2023, pp. 945–959. 2, 10

[20] R. Yi, L. Guo, S. Wei, A. Zhou, S. Wang, and M. Xu, "EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models," arXiv preprint arXiv:2308.14352, 2023. 2

[21] NVIDIA, "Nvlink," 2022. [Online]. Available: https://www.nvidia.com/en-us/data-center/nvlink/ 2

[22] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, "Gshard: Scaling giant models with conditional computation and automatic sharding," arXiv preprint arXiv:2006.16668, 2020. 3

[23] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, "Megatron-lm: Training multi-billion parameter language models using model parallelism," arXiv preprint arXiv:1909.08053, 2019. 3

[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems, vol. 30, 2017. 3

[25] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima et al., "The pile: An 800gb dataset of diverse text for language modeling," arXiv preprint arXiv:2101.00027, 2020. 5, 7, 10

[26] Microsoft, "Megatron-deepspeed," 2023. [Online]. Available: https://github.com/microsoft/Megatron-DeepSpeed 7

[27] Nvidia, "Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, the World's Largest and Most Powerful Generative Language Model," 2021. [Online]. Available: https://developer.nvidia.com/blog/ 7

[28] A. I. for AI, "C4: The colossal clean crawled corpus," https://huggingface.co/datasets/allenai/c4, 2020. 10

[29] L. Soldaini, R. Kinney, A. Bhagia, D. Schwenk, D. Atkinson, R. Authur, B. Bogin, K. Chandu, J. Dumas, Y. Elazar, V. Hofmann, A. H. Jha, S. Kumar, L. Lucy, X. Lyu, I. Magnusson, J. Morrison, N. Muennighoff, A. Naik, C. Nam, M. E. Peters, A. Ravichander, K. Richardson, Z. Shen, E. Strubell, N. Subramani, O. Tafjord, E. P. Walsh, H. Hajishirzi, N. A. Smith, L. Zettlemoyer, I. Beltagy, D. Groeneveld, J. Dodge, and K. Lo, "Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research," arXiv preprint, 2023. 10

[30] "Yelp dataset," https://www.yelp.com/dataset, accessed: 2024-01-15. 10

[31] S. Singh, O. Ruwase, A. A. Awan, S. Rajbhandari, Y. He, and A. Bhatele, "A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training," in Proceedings of the 37th International Conference on Supercomputing, ser. ICS '23. New York, NY, USA: Association for Computing Machinery, 2023, p. 203–214. [Online]. Available: https://doi.org/10.1145/3577193.3593704 10

[32] X. Nie, P. Zhao, X. Miao, T. Zhao, and B. Cui, "HetuMoE: An efficient trillion-scale mixture-of-expert distributed training system," arXiv preprint arXiv:2203.14685, 2022. 10

[33] C. Hwang, W. Cui, Y. Xiong, Z. Yang, Z. Liu, H. Hu, Z. Wang, R. Salas, J. Jose, P. Ram et al., "Tutel: Adaptive mixture-of-experts at scale," Proceedings of Machine Learning and Systems, vol. 5, 2023. 10

[34] L. Shen, Z. Wu, W. Gong, H. Hao, Y. Bai, H. Wu, X. Wu, J. Bian, H. Xiong, D. Yu et al., "SE-MoE: A scalable and efficient mixture-of-experts distributed training and inference system," arXiv preprint arXiv:2205.10034, 2022. 10

[35] M. Zhai, J. He, Z. Ma, Z. Zong, R. Zhang, and J. Zhai, "SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization," in 2023 USENIX Annual Technical Conference (USENIX ATC 23), 2023, pp. 961–975. 11

## PHỤ LỤC

![Hình 14: Mô hình GPT 350M MoE-32 được đào tạo trước, ái lực chuyên gia của tầng 0, 1, 2 đến tất cả các tầng sau.]

![Hình 15: Mô hình GPT 350M MoE-32 được đào tạo trước, ái lực chuyên gia của tầng 3, 4, 5, 6 đến tất cả các tầng sau.]

![Hình 16: Mô hình GPT 350M MoE-32 được đào tạo trước, ái lực chuyên gia của tầng 7, 8, 9, 10 đến tất cả các tầng sau.]
