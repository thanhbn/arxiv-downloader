# 2311.04934.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/inference/2311.04934.pdf
# Kích thước tệp: 905557 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
PROMPT CACHE : TÍNH NĂNG TÁI SỬ DỤNG CHÚ Ý THEO MODULE
CHO SUY LUẬN ĐỘ TRỄ THẤP
In Gim1Guojun Chen1Seung-seob Lee1Nikhil Sarda2Anurag Khandelwal1Lin Zhong1
TÓM TẮT
Chúng tôi giới thiệu Prompt Cache, một phương pháp tăng tốc suy luận cho các mô hình ngôn ngữ lớn (LLM) bằng cách tái sử dụng trạng thái chú ý qua các prompt LLM khác nhau. Nhiều prompt đầu vào có các đoạn văn bản chồng chéo, chẳng hạn như thông điệp hệ thống, mẫu prompt và tài liệu được cung cấp làm ngữ cảnh. Hiểu biết chính của chúng tôi là bằng cách tính toán trước và lưu trữ trạng thái chú ý của những đoạn văn bản xuất hiện thường xuyên này trên máy chủ suy luận, chúng ta có thể tái sử dụng chúng một cách hiệu quả khi những đoạn này xuất hiện trong prompt của người dùng. Prompt Cache sử dụng một schema để định nghĩa rõ ràng những đoạn văn bản có thể tái sử dụng như vậy, được gọi là module prompt. Schema đảm bảo độ chính xác vị trí trong quá trình tái sử dụng trạng thái chú ý và cung cấp cho người dùng giao diện để truy cập các trạng thái đã cache trong prompt của họ. Sử dụng một triển khai nguyên mẫu, chúng tôi đánh giá Prompt Cache trên nhiều LLM. Chúng tôi cho thấy Prompt Cache giảm đáng kể độ trễ trong thời gian đến token đầu tiên, đặc biệt đối với các prompt dài hơn như trả lời câu hỏi dựa trên tài liệu và đề xuất. Các cải thiện dao động từ 8× cho suy luận dựa trên GPU đến 60× cho suy luận dựa trên CPU, tất cả trong khi duy trì độ chính xác đầu ra và không cần sửa đổi tham số mô hình.

1 GIỚI THIỆU
Một phần đáng kể các prompt mô hình ngôn ngữ lớn (LLM) được tái sử dụng thường xuyên. Ví dụ, các prompt thường bắt đầu với "thông điệp hệ thống" giống hệt nhau cung cấp hướng dẫn ban đầu cho chức năng của nó. Tài liệu cũng có thể chồng chéo trong nhiều prompt. Trong một loạt các ứng dụng LLM ngữ cảnh dài, chẳng hạn như phân tích pháp lý (Cui et al., 2023; Nay et al., 2023), ứng dụng chăm sóc sức khỏe (Steinberg et al., 2021; Rasmy et al., 2021), và giáo dục (Shen et al., 2021), prompt bao gồm một hoặc nhiều tài liệu từ một nhóm. Ngoài ra, prompt thường được định dạng với các mẫu có thể tái sử dụng (White et al., 2023) như kết quả của kỹ thuật prompt. Những ví dụ như vậy phổ biến trong LLM cho robot và học công cụ (Huang et al., 2022; Driess et al., 2023; Qin et al., 2023). Điều này dẫn đến mức độ chồng chéo cao giữa các prompt sử dụng cùng một mẫu.

Chúng tôi giới thiệu một kỹ thuật mới được gọi là Prompt Cache để giảm chi phí tính toán trong suy luận LLM sinh tạo. Prompt Cache được thúc đẩy bởi quan sát rằng các prompt đầu vào cho LLM thường có cấu trúc có thể tái sử dụng. Ý tưởng chính là tính toán trước trạng thái chú ý của các đoạn prompt được xem lại thường xuyên trong bộ nhớ, và tái sử dụng chúng khi những đoạn này xuất hiện trong prompt để giảm độ trễ.

1Khoa Khoa học Máy tính, Đại học Yale, Hoa Kỳ. {in.gim, guojun.chen, seung-seob.lee, anurag.khandelwal, lin.zhong}@yale.edu 2Google, Mountain View, California, Hoa Kỳ. nikhilsarda@google.com. Liên hệ với: Lin Zhong <lin.zhong@yale.edu>.

Proceedings of the 5thMLSys Conference, Santa Clara, CA, USA, 2024. Copyright 2024 by the author(s).

Tái sử dụng trạng thái chú ý là một chiến lược phổ biến để tăng tốc phục vụ một prompt đơn (Pope et al., 2022). Phương pháp hiện có, thường được gọi là Key-Value (KV) Cache, tái sử dụng các trạng thái chú ý key-value của token đầu vào trong quá trình sinh token tự hồi quy. Điều này loại bỏ nhu cầu tính toán chú ý đầy đủ cho mỗi lần sinh token (§2.2). Bằng cách cache các key-value chú ý được tính toán cho token được sinh trước đó, mỗi lần sinh token chỉ yêu cầu tính toán trạng thái chú ý key-value một lần.

Xây dựng trên KV Cache, Prompt Cache mở rộng việc tái sử dụng trạng thái chú ý từ một prompt đơn lẻ sang nhiều prompt bằng cách làm cho việc tái sử dụng trạng thái chú ý trở nên theo module. Trong phương pháp của chúng tôi, các đoạn văn bản được tái sử dụng thường xuyên được tính toán trước riêng lẻ và lưu trữ trong bộ nhớ. Khi những đoạn "đã cache" như vậy xuất hiện trong prompt đầu vào, hệ thống sử dụng các trạng thái chú ý key-value đã tính toán trước từ bộ nhớ thay vì tính toán lại chúng. Kết quả là, tính toán chú ý chỉ cần thiết cho các đoạn văn bản chưa được cache. Hình 1 minh họa sự khác biệt giữa sinh tự hồi quy đầy đủ, KV Cache và Prompt Cache. Chúng tôi lưu ý rằng lợi thế hiệu suất trở nên rõ rệt hơn khi kích thước của các đoạn đã cache tăng lên vì chi phí tính toán của trạng thái chú ý tỷ lệ bậc hai với kích thước chuỗi đầu vào (Keles et al., 2022; Tay et al., 2023) trong khi độ phức tạp không gian và tính toán của Prompt Cache tỷ lệ tuyến tính với kích thước.

Hai thách thức phát sinh khi tái sử dụng trạng thái chú ý qua các prompt.

arXiv:2311.04934v2 [cs.CL] 25 Apr 2024

--- TRANG 2 ---
Prompt Cache: Tái Sử Dụng Chú Ý Theo Module cho Suy Luận Độ Trễ Thấp

LLMABC
AB1 2 3
(a) Sinh token tự hồi quy

LLM... ABC
ABTrạng thái chú ý
1 2 3 (b) Sinh với KV Cache

LLMTrạng thái chú ý
...
Prompt
CacheABC
AB1 2 3 (c) Sinh với Prompt Cache

Hình 1. So sánh các phương pháp sinh token LLM, mỗi phương pháp cho thấy ba bước (1 đến 3). Mỗi hộp biểu thị một token. Các hộp màu xanh đại diện cho prompt. (a) Một LLM nhận một prompt (token màu xanh) và dự đoán token tiếp theo (A) (1). Sau đó nó nối token được sinh (A) vào prompt để dự đoán token tiếp theo (B) (2). Quá trình này, được gọi là tự hồi quy, tiếp tục cho đến khi điều kiện dừng được đáp ứng. (b) KV Cache tính toán trạng thái chú ý cho prompt chỉ một lần (1) và tái sử dụng chúng trong các bước tiếp theo; (c) Prompt Cache tái sử dụng trạng thái KV qua các dịch vụ để bỏ qua tính toán chú ý prompt. Prompt Cache điền cache của nó khi một schema được tải và tái sử dụng các trạng thái đã cache cho các prompt được tạo từ schema (1). Hình 2 mô tả thêm Bước 1.

prompt. Đầu tiên, trạng thái chú ý phụ thuộc vào vị trí do mã hóa vị trí trong Transformers. Do đó, trạng thái chú ý của một đoạn văn bản chỉ có thể được tái sử dụng nếu đoạn đó xuất hiện ở cùng vị trí. Thứ hai, hệ thống phải có khả năng nhận dạng hiệu quả một đoạn văn bản mà trạng thái chú ý của nó có thể đã được cache để tái sử dụng.

Để giải quyết hai vấn đề này, Prompt Cache kết hợp hai ý tưởng. Đầu tiên là làm cho cấu trúc của prompt trở nên rõ ràng với Prompt Markup Language (PML). PML làm cho các đoạn văn bản có thể tái sử dụng trở nên rõ ràng như các module, tức là module prompt. Nó không chỉ giải quyết vấn đề thứ hai ở trên mà còn mở ra cánh cửa để giải quyết vấn đề đầu tiên, vì mỗi module prompt có thể được gán với ID vị trí duy nhất. Ý tưởng thứ hai của chúng tôi là phát hiện thực nghiệm rằng LLM có thể hoạt động trên trạng thái chú ý với ID vị trí không liên tục. Điều này có nghĩa là chúng ta có thể trích xuất các đoạn khác nhau của trạng thái chú ý và nối chúng lại để tạo thành tập hợp con của ý nghĩa. Chúng tôi tận dụng điều này để cho phép người dùng chọn module prompt dựa trên nhu cầu của họ, hoặc thậm chí cập nhật một số module prompt trong thời gian chạy.

Chúng tôi giải thích cách Prompt Cache hoạt động trong §3. Tóm lại, người dùng LLM viết prompt của họ bằng PML, với ý định rằng họ có thể tái sử dụng trạng thái chú ý dựa trên module prompt. Quan trọng là, họ phải tạo prompt từ một schema, cũng được viết bằng PML. Hình 2 cho thấy một ví dụ prompt dựa trên một schema ví dụ. Khi Prompt Cache nhận một prompt, trước tiên nó xử lý schema của nó và tính toán trạng thái chú ý cho các module prompt của nó. Nó tái sử dụng những trạng thái này cho các module prompt trong prompt và các prompt khác được tạo từ cùng schema. Trong §4, chúng tôi báo cáo một triển khai nguyên mẫu của Prompt Cache trên thư viện transformers HuggingFace (Wolf et al., 2020). Trong khi Prompt Cache có thể hoạt động với bất kỳ kiến trúc Transformer nào tương thích với KV Cache, chúng tôi thử nghiệm với ba kiến trúc Transformer phổ biến cung cấp năng lượng cho các LLM mã nguồn mở sau: Llama2 (Touvron et al., 2023), Falcon (Penedo et al., 2023), và MPT (MosaicML, 2023). Chúng tôi xem xét hai loại bộ nhớ để lưu trữ module prompt: bộ nhớ CPU và GPU. Trong khi bộ nhớ CPU có thể mở rộng đến mức terabyte, nó mang lại chi phí sao chép bộ nhớ từ host đến thiết bị. Ngược lại, bộ nhớ GPU không yêu cầu sao chép nhưng có dung lượng hạn chế.

Sử dụng nguyên mẫu, chúng tôi tiến hành đánh giá benchmark toàn diện để kiểm tra hiệu suất và định lượng độ chính xác của Prompt Cache trên các bộ dữ liệu ngữ cảnh dài khác nhau (§5). Chúng tôi sử dụng bộ LongBench (Bai et al., 2023), bao gồm các tác vụ đề xuất và trả lời câu hỏi (QA) dựa trên nhiều tài liệu. Trong đánh giá của chúng tôi, Prompt Cache giảm độ trễ thời gian đến token đầu tiên (TTFT) từ 1.5× đến 10× cho suy luận GPU với module prompt trên bộ nhớ GPU và từ 20× đến 70× cho suy luận CPU, tất cả mà không có bất kỳ mất mát độ chính xác đáng kể nào. Ngoài ra, chúng tôi phân tích chi phí bộ nhớ của các trạng thái chú ý đã tính toán trước cho mỗi mô hình và thảo luận về các hướng tối ưu hóa dấu chân bộ nhớ của Prompt Cache. Chúng tôi sau đó trình bày một số tác vụ sinh tạo, bao gồm cá nhân hóa, sinh mã và prompt có tham số, để chứng minh tính biểu cảm của schema prompt và cải thiện hiệu suất với suy giảm chất lượng không đáng kể.

Trong nghiên cứu hiện tại, chúng tôi tập trung chính vào các kỹ thuật tái sử dụng chú ý theo module. Tuy nhiên, chúng tôi dự đoán Prompt Cache sẽ được sử dụng như một thành phần nền tảng cho các hệ thống phục vụ LLM trong tương lai. Những hệ thống như vậy có thể kết hợp quản lý module prompt nâng cao và chiến lược thay thế cache GPU, tối ưu hóa lợi thế của cả DRAM host và GPU HBM. Mã nguồn và dữ liệu được sử dụng để đánh giá có sẵn tại github.com/yale-sys/prompt-cache.

2 BỐI CẢNH VÀ CÔNG TRÌNH LIÊN QUAN

Prompt Cache xây dựng trên ý tưởng của KV Cache, tức là tái sử dụng trạng thái chú ý key-value trong quá trình giải mã tự hồi quy trong LLM. Phần này xem xét sinh token tự hồi quy

--- TRANG 3 ---
Prompt Cache: Tái Sử Dụng Chú Ý Theo Module cho Suy Luận Độ Trễ Thấp

trong LLM, giải thích cách việc kết hợp KV Cache có thể tăng tốc quá trình sinh token, xác định các xấp xỉ của nó, và khảo sát công trình gần đây tận dụng KV Cache để tăng tốc. Chúng tôi cũng thảo luận ngắn gọn về các kỹ thuật hiện có khác để tăng tốc suy luận LLM.

2.1 Sinh Token Tự Hồi Quy

Một LLM sinh token đầu ra một cách tự hồi quy (Radford et al., 2018). Nó bắt đầu với một đầu vào ban đầu, thường được gọi là prompt, và sinh token tiếp theo dựa trên prompt. Mô hình sau đó nối token vào prompt và sử dụng nó để sinh token tiếp theo. Quá trình sinh tiếp tục cho đến khi điều kiện dừng được đáp ứng. Điều này có thể là sau một số token được xác định trước, khi sinh một token kết thúc chuỗi đặc biệt, hoặc khi chuỗi được sinh đạt mức độ mạch lạc hoặc hoàn chỉnh thỏa đáng.

Quan trọng là, trong mỗi bước, mô hình lấy toàn bộ prompt và các token được sinh cho đến nay làm đầu vào, và lặp lại.

2.2 Key-Value Cache

Sinh token tự hồi quy được mô tả ở trên phát sinh chi phí tính toán đáng kể do cơ chế tự chú ý được áp dụng trên toàn bộ đầu vào trong mỗi bước. Để giảm thiểu điều này, cơ chế Key-Value (KV) Cache (Pope et al., 2022) thường được sử dụng. Kỹ thuật này tính toán các embedding key và value cho mỗi token chỉ một lần trong suốt quá trình sinh token tự hồi quy.

Để chi tiết hóa, ký hiệu một prompt người dùng như một chuỗi n token: s1, ..., sn, và k token được sinh tiếp theo là sn+1, ..., sn+k. Trong sinh token tự hồi quy ngây thơ, các trạng thái chú ý {(k1, v1), ..., (kn+k, vn+k)} được tính toán lại hoàn toàn ở mỗi bước. Ngược lại, KV Cache ban đầu tính toán trạng thái chú ý cho đầu vào, được biểu diễn bởi S0={(ki, vi)|i≤n}, và cache chúng trong bộ nhớ. Bước này thường được gọi là giai đoạn prefill. Cho mỗi bước tiếp theo j≤k, mô hình tái sử dụng các giá trị đã cache Sj={(ki, vi)|i < n +j} để tính toán trạng thái chú ý (kn+j, vn+j) của token mới sn+j. Phương pháp này giảm đáng kể tính toán cần thiết cho tự chú ý.

Cụ thể, tính toán trong mỗi bước, được đo bằng FLOP cho các phép toán ma trận, được giảm đi một hệ số 1/n. Số phép toán giảm từ khoảng 6nd2+ 4n2d xuống 6d2+ 4nd, trong đó d là kích thước chiều ẩn. Sau mỗi bước, các trạng thái chú ý (kn+j, vn+j) mới được tính toán được nối vào cache để sử dụng tiếp theo.

Trong các mô hình ngôn ngữ nhân quả, chiếm hầu hết các LLM, việc sử dụng KV Cache không ảnh hưởng đến độ chính xác của mô hình, vì chú ý tại vị trí i được tính toán chỉ dựa trên các token ở các vị trí nằm trước token thứ i.

KV Cache đã thúc đẩy khám phá thêm về tăng tốc LLM. Các nghiên cứu tiếp theo đã tập trung vào việc tinh chỉnh quản lý bộ nhớ cho KV Cache, như được chứng minh trong paged attention (Kwon et al., 2023), về việc cắt tỉa dữ liệu KV Cache thừa (Zhang et al., 2023), hoặc nén nó (Liu et al., 2023b). Có một số công trình sơ bộ khám phá việc tái sử dụng KV Cache qua các yêu cầu khác nhau. (Feng et al., 2023) tái sử dụng các trạng thái chú ý được ghi nhớ dựa trên một metric tương tự embedding. Paged attention cũng chứng minh việc chia sẻ tiền tố đơn giản, trong đó các prompt khác nhau với tiền tố giống hệt nhau chia sẻ KV Cache. Tuy nhiên, các phương pháp hiện có dành riêng cho các kịch bản nhất định, trong khi chúng tôi điều tra việc tái sử dụng chú ý cho các prompt LLM tổng quát.

2.3 Các Phương Pháp Khác cho Suy Luận LLM Độ Trễ Thấp

Prompt Cache giới thiệu một chiến lược tối ưu hóa trực giao bổ sung cho các hệ thống hiện có dành riêng cho suy luận LLM hiệu quả. Điều này bao gồm các hệ thống sử dụng nhiều GPU cho suy luận (Aminabadi et al., 2022) và những hệ thống với kernel GPU hiệu suất cao cho tính toán điểm chú ý softmax (Dao et al., 2022). Mặc dù trọng tâm hiện tại của chúng tôi là đạt được suy luận độ trễ thấp trong LLM, Prompt Cache cũng có thể mang lại lợi ích cho các hệ thống nhắm đến thông lượng cao (Sheng et al., 2023) thông qua việc giảm tính toán.

3 THIẾT KẾ PROMPT CACHE

Tính hiệu quả của KV Cache dẫn chúng ta đến câu hỏi tiếp theo: Liệu trạng thái chú ý có thể được tái sử dụng qua nhiều yêu cầu suy luận? Chúng tôi quan sát rằng các prompt khác nhau thường có các đoạn văn bản chồng chéo. Ví dụ, "thông điệp hệ thống" giống hệt nhau, hoặc metaprompt thường được chèn vào đầu prompt để khơi gợi phản hồi mong muốn từ LLM. Một ví dụ khác, trong nhiều ứng dụng LLM pháp lý và y tế (Cui et al., 2023; Steinberg et al., 2021; Rasmy et al., 2021), cùng một bộ tài liệu thường được cung cấp làm ngữ cảnh cho các prompt khác nhau. Cuối cùng, các định dạng prompt có thể tái sử dụng, tức là mẫu prompt, thường được sử dụng bởi các ứng dụng LLM trong robot và học công cụ (Driess et al., 2023; Qin et al., 2023), vì hầu hết các tác vụ là biến thể của một vài tác vụ phổ biến. Trong phần này, chúng tôi mô tả phương pháp của chúng tôi được gọi là Prompt Cache, trả lời câu hỏi trên một cách khẳng định. Prompt Cache cải thiện hiệu quả tính toán thông qua việc tái sử dụng trạng thái chú ý giữa các yêu cầu bằng cách tận dụng các đoạn được chia sẻ theo cách có cấu trúc.

3.1 Tổng Quan

Trạng thái chú ý của một đoạn văn bản chỉ có thể được tái sử dụng nếu đoạn đó xuất hiện ở cùng vị trí trong đầu vào LLM. Điều này là do các kiến trúc transformer tích hợp embedding vị trí duy nhất vào trạng thái chú ý (k, v). Điều này không phải là vấn đề để phục vụ một prompt đơn sử dụng KV Cache, vì cùng văn bản prompt được đặt ở cùng vị trí, tức là đầu của đầu vào, trong tất cả các bước.

Mặt khác, các đoạn văn bản được chia sẻ có thể xuất hiện ở các

--- TRANG 4 ---
Prompt Cache: Tái Sử Dụng Chú Ý Theo Module cho Suy Luận Độ Trễ Thấp

Schema
<schema name="cities">
  <module name="city-info"> ...
  </module> 
  <module name="trip-plan"> ...
    <param name=" duration"  len=2/>
  </module>
  <module name="tokyo"> ... 
  </module>
  <module name="miami"> ... 
  </module> ... 
</schema>PromptLLMTrạng thái chú ý prompt
<prompt schema="cities">
  <trip-plan duration=   "3 days"  /> <miami/>  Highlight the surf spots.
</prompt>Prompt Cache
khoảng trống12 2
3 45

<city-info/> <trip-plan/> <tokyo/> <miami/> <paris/>

Hình 2. Cơ chế tái sử dụng trong Prompt Cache: (i) Đầu tiên, PML (§3.2) làm cho các module prompt có thể tái sử dụng trở nên rõ ràng trong cả Schema và Prompt. Một module prompt có thể có tham số như trip-plan. Một prompt nhập module cung cấp giá trị (3 days) cho tham số (duration). Prompt có thể bao gồm các đoạn văn bản mới thay cho các module và tham số bị loại trừ và ở cuối. (ii) Thứ hai, mã hóa module prompt (§3.3) tính toán trước trạng thái chú ý (1) cho tất cả các module trong schema và cache chúng để tái sử dụng trong tương lai. (iii) Thứ ba, khi prompt được phục vụ, Prompt Cache sử dụng suy luận đã cache (§3.4): nó truy xuất các trạng thái chú ý đã cache cho các module prompt được nhập (2), tính toán chúng cho tham số (3) và các đoạn văn bản mới (4), và cuối cùng nối chúng lại để tạo ra trạng thái chú ý cho toàn bộ prompt (5). Hình này là một mở rộng của Bước 1 trong Hình 1c.

vị trí khác nhau trong các prompt khác nhau. Để tái sử dụng trạng thái chú ý của chúng qua các prompt, một hệ thống cache phải giải quyết hai vấn đề. Đầu tiên, nó phải cho phép tái sử dụng mặc dù một đoạn văn bản xuất hiện ở các vị trí khác nhau trong các prompt khác nhau. Thứ hai, hệ thống phải có khả năng nhận dạng hiệu quả một đoạn văn bản mà trạng thái chú ý của nó có thể đã được cache để tái sử dụng, khi hệ thống nhận được một prompt mới.

Để giải quyết hai vấn đề này, chúng tôi kết hợp hai ý tưởng. Đầu tiên là làm cho cấu trúc của prompt trở nên rõ ràng với Prompt Markup Language (PML). Như được minh họa bởi Hình 2, PML làm cho các đoạn văn bản có thể tái sử dụng trở nên rõ ràng như các module, tức là module prompt. Nó không chỉ giải quyết vấn đề thứ hai ở trên mà còn mở ra cánh cửa để giải quyết vấn đề đầu tiên, vì mỗi module prompt có thể được gán với ID vị trí duy nhất.

Ý tưởng thứ hai của chúng tôi là phát hiện thực nghiệm rằng LLM có thể hoạt động trên trạng thái chú ý với ID vị trí không liên tục. Miễn là vị trí tương đối của các token được bảo toàn, chất lượng đầu ra không bị ảnh hưởng. Điều này có nghĩa là chúng ta có thể trích xuất các đoạn khác nhau của trạng thái chú ý và nối chúng lại để tạo thành ý nghĩa mới. Chúng tôi tận dụng điều này để cho phép người dùng chọn module prompt dựa trên nhu cầu của họ, hoặc thậm chí thay thế một số ý nghĩa trong thời gian chạy.

Prompt Cache kết hợp hai ý tưởng này như sau. Người dùng LLM viết prompt của họ bằng PML, với ý định rằng họ có thể tái sử dụng trạng thái chú ý dựa trên module prompt. Quan trọng là, họ phải tạo prompt từ một schema, cũng được viết bằng PML. Hình 2 cho thấy một ví dụ prompt dựa trên một schema ví dụ. Khi Prompt Cache nhận một prompt, trước tiên nó xử lý schema của nó và tính toán trạng thái chú ý cho các module prompt của nó. Nó tái sử dụng những trạng thái này cho các module prompt trong prompt và các prompt khác được tạo từ cùng schema.

Chúng tôi chi tiết thiết kế của PML trong §3.2 với trọng tâm vào các kỹ thuật tối đa hóa cơ hội tái sử dụng. Chúng tôi giải thích cách Prompt Cache tính toán trạng thái chú ý của các module prompt trong một schema trong §3.3, và cách nó có thể ảnh hưởng đến chất lượng đầu ra. Chúng tôi giải thích cách Prompt Cache tái sử dụng trạng thái chú ý từ một schema cho việc phục vụ một prompt trong §3.4.

Việc xây dựng KV cache theo module trong Prompt Cache có sự tương đồng với các xấp xỉ được quan sát trong chú ý masked cục bộ (Beltagy et al., 2020; Tay et al., 2023), tối ưu hóa tính toán bằng cách đặt một cửa sổ hạn chế cho tính toán điểm chú ý thay vì trải rộng chú ý của nó qua mọi token trong chuỗi đầu vào của nó. Xem xét một kịch bản trong Prompt Cache trong đó mỗi module prompt được mã hóa độc lập. Cho rằng trạng thái chú ý được tính toán nghiêm ngặt trong giới hạn của module prompt, điều này rất giống với thiết lập của một mask chú ý che giấu các chuỗi bên ngoài module prompt. Do đó, xấp xỉ được thực hiện bởi Prompt Cache là giới hạn cửa sổ chú ý cho mỗi module prompt. Chúng tôi lưu ý rằng việc sử dụng những mask chú ý như vậy không nhất thiết làm giảm chất lượng đầu ra, như chúng tôi sẽ thảo luận trong §5. Trong một số ngữ cảnh, những mask này thậm chí có thể giới thiệu bias quy nạp có lợi bằng cách lọc hiệu quả thông tin không liên quan.

3.2 Prompt Markup Language (PML)

Tiếp theo chúng tôi mô tả các tính năng chính của PML được sử dụng để định nghĩa cả schema và prompt được tạo từ schema.

3.2.1 Schema vs. Prompt

Schema là một tài liệu định nghĩa các module prompt và phân định vị trí tương đối và thứ bậc của chúng. Mỗi schema có một định danh duy nhất (qua thuộc tính name) và chỉ định các module prompt với thẻ <module>. Văn bản không được bao quanh bởi thẻ <module> hoặc định danh không được chỉ định

--- TRANG 5 ---
Prompt Cache: Tái Sử Dụng Chú Ý Theo Module cho Suy Luận Độ Trễ Thấp

được coi là module prompt ẩn danh và luôn được bao gồm trong các prompt được xây dựng từ schema.

Đối với người dùng LLM, schema phục vụ như một giao diện để tạo và tái sử dụng trạng thái chú ý cho các module prompt. Người dùng có thể xây dựng một prompt từ một schema, với thẻ <prompt>. Thẻ này chỉ định schema sử dụng thông qua thuộc tính schema, liệt kê các module prompt để nhập, và thêm bất kỳ hướng dẫn bổ sung (chưa được cache) nào. Ví dụ, để nhập module miami từ schema trong Hình 2, người ta sẽ biểu thị nó như <miami/>. Prompt Cache sẽ chỉ tính toán trạng thái chú ý cho văn bản không được chỉ định trong schema, ví dụ, Highlights the surf spots trong Hình 2, và tái sử dụng trạng thái chú ý cho các module được nhập, ví dụ, trip-plan và miami, do đó giảm độ trễ.

3.2.2 Tối Đa Hóa Tái Sử Dụng với Tham Số

PML cho phép một module prompt được tham số hóa để tối đa hóa cơ hội tái sử dụng. Một tham số là một placeholder có tên với độ dài được chỉ định có thể xuất hiện ở bất cứ đâu trong một module prompt trong schema. Nó được định nghĩa bằng thẻ <param>, với các thuộc tính name và len chỉ ra tên của nó và số lượng token tối đa cho đối số, tương ứng. Khi một prompt nhập module prompt, nó có thể cung cấp giá trị cho tham số. Hình 2 cho thấy một ví dụ về module prompt được tham số hóa (trip-plan) và cách một prompt sẽ bao gồm module prompt và cung cấp giá trị (3 days) cho đối số của nó (duration). Các giá trị tăng cường không được cache.

Có hai cách sử dụng quan trọng của các module prompt được tham số hóa. Đầu tiên, thường xảy ra rằng một module prompt khác với module khác chỉ ở một số vị trí được định nghĩa rõ. Tham số cho phép người dùng cung cấp đối số cụ thể để tùy chỉnh module tại thời gian chạy và vẫn được hưởng lợi từ việc tái sử dụng. Hình 2 minh họa trường hợp sử dụng này với trip-plan. Điều này đặc biệt hữu ích cho các prompt có mẫu. Thứ hai, một tham số có thể được sử dụng để tạo "buffer" ở đầu hoặc cuối của một module prompt trong schema. Buffer này cho phép người dùng thêm một đoạn văn bản tùy ý trong prompt miễn là đoạn đó không dài hơn độ dài token tham số mà nó thay thế.

3.2.3 Các Tính Năng Khác

Module liên hợp: Một số module prompt thể hiện mối quan hệ loại trừ lẫn nhau. Nghĩa là, trong một tập hợp các module, chỉ một module nên được chọn. Ví dụ, xem xét một prompt yêu cầu LLM đề xuất một cuốn sách để đọc dựa trên hồ sơ người đọc được mô tả bởi một module prompt. Có thể có nhiều module prompt mỗi module mô tả một hồ sơ người đọc nhưng prompt chỉ có thể bao gồm một trong số chúng.

<union>
<module name="doc-en-US"> ... </module>
<module name="doc-zh-CN"> ... </module>
</union>

Để chứa những mối quan hệ loại trừ này, chúng tôi giới thiệu khái niệm liên hợp cho các module prompt. Một liên hợp của các module được ký hiệu bằng thẻ <union>. Các module prompt lồng trong cùng một liên hợp chia sẻ cùng ID vị trí bắt đầu. Một liên hợp không chỉ hợp lý hóa tổ chức bố cục mà còn bảo tồn các ID vị trí được sử dụng để mã hóa các module prompt. Hơn nữa, hệ thống có thể sử dụng cấu trúc này cho các tối ưu hóa, chẳng hạn như prefetching.

Trong khi các module được tham số hóa và module liên hợp có vẻ tương tự, chúng khác nhau ở hai khía cạnh. Đầu tiên, như chúng tôi sẽ cho thấy trong §3.3, tham số và module liên hợp được mã hóa theo các cách khác nhau. Thứ hai, chúng phục vụ các mục đích khác nhau: tham số được sử dụng cho các sửa đổi nội tuyến để tối đa hóa việc tái sử dụng module, trong khi module liên hợp được dành cho cấu trúc prompt tốt hơn và sử dụng hiệu quả hơn các ID vị trí.

Module lồng nhau: PML cũng hỗ trợ module lồng nhau để biểu thị các module prompt phân cấp. Nghĩa là, một module prompt có thể bao gồm các module prompt hoặc liên hợp như các thành phần. Trong prompt, module lồng nhau được nhập như module trong module như được hiển thị trong Hình 8.

Tương thích với mẫu cụ thể LLM: LLM được tinh chỉnh hướng dẫn thường tuân theo các mẫu cụ thể để định dạng cuộc trò chuyện. Ví dụ, trong Llama2, một tương tác đơn giữa người dùng và trợ lý tuân theo mẫu: <s>[INST] user message [/INST] assistant message </s>. Để giảm nỗ lực cần thiết để định dạng thủ công schema prompt để phù hợp với những mẫu như vậy cho các LLM khác nhau, chúng tôi giới thiệu ba thẻ chuyên dụng: <system> cho prompt cấp hệ thống, <user> cho prompt do người dùng tạo ra, và <assistant> cho phản hồi mẫu được LLM tạo ra. Prompt Cache chuyển đổi và biên dịch động những thẻ chuyên biệt này để phù hợp với mẫu prompt được chỉ định của LLM đang sử dụng.

3.2.4 Tạo PML từ Chương Trình Prompt

Để đơn giản hóa việc viết PML, Prompt Cache có thể tự động chuyển đổi chương trình prompt (Beurer-Kellner et al., 2023; Guidence, 2023) từ các ngôn ngữ như Python thành PML, loại bỏ nhu cầu viết schema thủ công. Điều này được thực hiện chủ yếu bằng cách sử dụng một API Python chuyển đổi các hàm Python thành schema PML tương ứng. Quá trình chuyển đổi rất đơn giản: câu lệnh if trở thành cấu trúc <module> trong PML, bao bọc các prompt có điều kiện bên trong. Khi một điều kiện đánh giá thành true, module tương ứng được kích hoạt. Câu lệnh Choose-one, chẳng hạn như if-else hoặc câu lệnh switch, được ánh xạ thành thẻ <union>. Lời gọi hàm được dịch thành module prompt lồng nhau. Ngoài ra, chúng tôi đã triển khai một decorator để quản lý tham số, cụ thể là để hạn chế độ dài đối số tối đa. Điều này tương ứng với thuộc tính len trong <param>.

Quá trình biên dịch Python-to-PML này ẩn độ phức tạp PML khỏi người dùng và cung cấp khả năng bảo trì tốt hơn cho prompt.

--- TRANG 6 ---
Prompt Cache: Tái Sử Dụng Chú Ý Theo Module cho Suy Luận Độ Trễ Thấp

3.3 Mã Hóa Schema

Lần đầu tiên trạng thái chú ý của một module prompt được cần, chúng phải được tính toán và lưu trữ trong bộ nhớ thiết bị, mà chúng tôi gọi là mã hóa module prompt.

Đầu tiên, Prompt Cache trích xuất chuỗi token của một module prompt từ schema. Sau đó nó gán ID vị trí cho mỗi token. ID vị trí bắt đầu được xác định bởi vị trí tuyệt đối của module prompt trong schema. Ví dụ, nếu hai module prompt trước đó có kích thước chuỗi token lần lượt là 50 và 60, module prompt được gán ID vị trí bắt đầu là 110. Một ngoại lệ tồn tại cho các module liên hợp. Vì các module prompt trong liên hợp bắt đầu từ cùng vị trí, kích thước chuỗi token của chúng được xem xét với kích thước của con lớn nhất.

Từ chuỗi token của module prompt và các ID vị trí tương ứng, chúng sau đó được truyền cho LLM để tính toán trạng thái chú ý (k, v). Chúng tôi lưu ý rằng các ID vị trí được gán không bắt đầu từ số không. Điều này về mặt ngữ nghĩa có thể chấp nhận được vì khoảng trắng không làm thay đổi ý nghĩa của văn bản được tính toán trước. Tuy nhiên, nhiều triển khai mã hóa vị trí transformer hiện có, chẳng hạn như RoPE, thường yêu cầu các điều chỉnh để chứa các ID vị trí không liên tục, mà chúng tôi sẽ thảo luận trong (§4.2).

Để mã hóa các module prompt được tham số hóa, chúng tôi sử dụng ý tưởng rằng có khoảng trắng trong prompt không ảnh hưởng đến ngữ nghĩa của nó. Tham số được thay thế bằng một số token <unk> được xác định trước, tương đương với giá trị thuộc tính len của chúng. Các ID vị trí tương ứng với những token <unk> này được ghi lại để thay thế trong tương lai. Khi module này được tích hợp vào prompt của người dùng và được ghép với các đối số liên quan, chuỗi token của những đối số được cung cấp này áp dụng các ID vị trí trước đó được liên kết với các token <unk>. Các trạng thái chú ý (k, v) kết quả sau đó thay thế các trạng thái chú ý ban đầu được phân bổ cho các token <unk>. Chúng tôi lưu ý rằng độ dài của các token mới được cung cấp có thể nhỏ hơn độ dài tham số được chỉ định, vì khoảng trắng ở cuối không thay đổi ngữ nghĩa.

Hiệu ứng masking chú ý: Prompt Cache giới hạn tính toán điểm chú ý trong phạm vi của mỗi module prompt, masking trạng thái chú ý qua các module. Hiệu ứng masking này có thể nâng cao hoặc làm giảm chất lượng đầu ra tùy thuộc vào tính độc lập ngữ nghĩa của các module. Đối với các module độc lập ngữ nghĩa, masking giảm nhiễu và cải thiện chất lượng. Tuy nhiên, đối với các module phụ thuộc ngữ nghĩa, nó có thể có hiệu ứng ngược lại. Do đó, mỗi module prompt nên tự đủ và độc lập ngữ nghĩa với các module khác. Một cách để loại bỏ hiệu ứng masking là sử dụng phương pháp mà chúng tôi gọi là scaffolding. Với chi phí bộ nhớ bổ sung, chúng tôi cho phép người dùng chỉ định "scaffold", là tập hợp các module prompt được mã hóa cùng nhau để chia sẻ phạm vi chú ý, ngoài các trạng thái chú ý riêng lẻ của chúng. Khi tất cả các module prompt trong một scaffold được nhập trong một prompt, trạng thái chú ý của scaffold ghi đè các trạng thái chú ý riêng lẻ. Scaffolding đánh đổi bộ nhớ bổ sung để có tính nhất quán đầu ra, có thể hữu ích cho các ứng dụng cần kết quả xác định.

3.4 Suy Luận Đã Cache

Khi một prompt được cung cấp cho Prompt Cache, Prompt Cache phân tích nó để đảm bảo phù hợp với schema được tuyên bố. Nó xác minh tính hợp lệ của các module được nhập. Sau đó, như được minh họa trong Hình 2, Prompt Cache truy xuất các trạng thái chú ý (k, v) cho các module prompt được nhập từ cache (2), tính toán những trạng thái cho các đoạn văn bản mới (3 và 4), và nối chúng lại để tạo ra trạng thái chú ý cho toàn bộ prompt (5), thay thế thao tác prefill.

Để chi tiết hóa quá trình, Prompt Cache bắt đầu bằng cách nối các tensor trạng thái KV tương ứng với mỗi module prompt được nhập trong prompt. Ví dụ, khi prompt người dùng sử dụng các module A, B, tensor KV được nối được tạo thành như: (kC, vC) = (concat(kA, kB), (concat(vA, vB)). Đáng chú ý là thứ tự nối không quan trọng do tính bất biến hoán vị của transformer (Dufter et al., 2022). Bước này chỉ yêu cầu sao chép bộ nhớ. Sau đó, Prompt Cache tính toán trạng thái chú ý cho các đoạn của prompt không được cache, cụ thể là chuỗi token không được định nghĩa trong schema và đối số cho các module prompt được tham số hóa. Prompt Cache đầu tiên xác định các ID vị trí của văn bản chưa cache dựa trên vị trí của chúng tương đối với các module prompt được sử dụng khác. Ví dụ, nếu văn bản nằm giữa module A và B, nó được gán ID vị trí bắt đầu từ vị trí kết thúc của A, giả sử có khoảng cách giữa các vị trí của A và B. Đối số cho các module prompt được tham số hóa được gán cho các ID vị trí của token <unk>. Tiếp theo, chuỗi token và ID vị trí được tổng hợp và truyền cho LLM sử dụng (kC, vC) như một KV Cache, để tính toán trạng thái chú ý cho toàn bộ prompt. Quan trọng là độ phức tạp tính toán để sinh các token tiếp theo vẫn nhất quán với KV Cache, vì các module prompt không được sử dụng vượt ra ngoài token ban đầu.

Về cơ bản, Prompt Cache giảm thiểu độ trễ liên quan đến việc tạo ra token đầu tiên, hoặc thời gian đến token đầu tiên (TTFT).

Tối ưu hóa bộ nhớ trong suy luận batch: Prompt thường được phục vụ trong một batch để sử dụng GPU tốt hơn. Các prompt khác nhau được tạo từ cùng schema có thể bao gồm cùng các module prompt, chẳng hạn như prompt hệ thống. Điều này mở ra cơ hội tối ưu hóa bổ sung bằng cách giảm dư thừa KV Cache trong một batch. Paged attention (Kwon et al., 2023) có thể giải quyết vấn đề này bằng cách chia sẻ con trỏ đến cùng module prompt qua các prompt khác nhau, thay vì nhân đôi trạng thái chú ý. Ở đây, việc sử dụng Prompt Cache có thể cải thiện ngầm thông lượng hệ thống bằng cách cho phép xử lý nhiều prompt song song hơn.

--- TRANG 7 ---
Prompt Cache: Tái Sử Dụng Chú Ý Theo Module cho Suy Luận Độ Trễ Thấp

4 TRIỂN KHAI

Chúng tôi xây dựng nguyên mẫu Prompt Cache sử dụng thư viện transformers HuggingFace (Wolf et al., 2020) trong PyTorch và bao gồm 3K dòng mã Python. Chúng tôi nhắm đến tích hợp liền mạch với codebase LLM hiện có và tái sử dụng các weight của nó. Chúng tôi triển khai Prompt Cache để sử dụng cả bộ nhớ CPU và GPU để chứa các module prompt và đánh giá nó trên cả hai nền tảng.

4.1 Lưu Trữ Module Prompt trong Bộ Nhớ

Chúng tôi lưu trữ các module prompt đã mã hóa trong hai loại bộ nhớ: bộ nhớ CPU (host DRAM) và bộ nhớ GPU (HBM). Để quản lý tensor qua cả hai loại bộ nhớ, chúng tôi sử dụng bộ phân bổ bộ nhớ PyTorch (Paszke et al., 2019). Ngoài việc đơn giản ghép CPU với các module prompt trong bộ nhớ CPU và GPU với bộ nhớ GPU, chúng tôi cũng cho phép GPU truy cập các module prompt được lưu trữ trong bộ nhớ CPU. Điều này được thực hiện bằng cách sao chép các module prompt từ host sang thiết bị khi cần. Quá trình này phát sinh chi phí sao chép bộ nhớ từ host đến thiết bị. Tuy nhiên, nó cho phép GPU tận dụng bộ nhớ CPU dồi dào, có thể mở rộng đến mức terabyte. Như chúng tôi sẽ cho thấy trong §5, tiết kiệm tính toán từ Prompt Cache hơn bù đắp cho độ trễ gây ra bởi các thao tác sao chép bộ nhớ. Sử dụng GPU phơi bày sự đánh đổi giữa dung lượng bộ nhớ và độ trễ: bộ nhớ GPU nhanh hơn nhưng có dung lượng hạn chế, trong khi bộ nhớ CPU có thể mở rộng dễ dàng nhưng phát sinh chi phí sao chép bộ nhớ bổ sung. Có vẻ khả thi để xem xét một cơ chế cache tận dụng cả bộ nhớ CPU và GPU. Chúng tôi để lại việc phát triển một hệ thống kết hợp chiến lược thay thế cache và prefetching cho nghiên cứu tương lai.

4.2 Điều Chỉnh Kiến Trúc Transformer

Triển khai Prompt Cache yêu cầu hỗ trợ cho các ID vị trí không liên tục (§3.2). Mặc dù thư viện Transformers hiện tại không cung cấp những tính năng này, chúng có thể được tích hợp với các sửa đổi nhỏ. Ví dụ, khoảng 20 dòng mã bổ sung cần thiết cho mỗi LLM. Chúng tôi nêu ra các điều chỉnh cần thiết:

Bảng embedding: Các mô hình sớm như BERT (Vaswani et al., 2023) và GPT-2 (Radford et al., 2018) sử dụng bảng tra cứu để ánh xạ ID vị trí thành embedding đã học hoặc bias cố định, không yêu cầu thay đổi.

RoPE: Các LLM như Llama2 (Touvron et al., 2023) và Falcon (Penedo et al., 2023) áp dụng RoPE (Su et al., 2021), sử dụng ma trận xoay cho mã hóa vị trí trong tính toán chú ý. Chúng tôi tạo một bảng tra cứu cho mỗi ma trận xoay, cho phép truy xuất dựa trên ID vị trí.

ALiBi: Được sử dụng trong các mô hình như MPT (MosaicML, 2023) và Bloom (Scao et al., 2022), ALiBi (Press et al., 2022)

tích hợp một bias tĩnh trong quá trình tính toán điểm softmax. Tương tự như RoPE, chúng tôi thiết kế một bảng tra cứu để điều chỉnh ma trận bias theo các ID vị trí được cung cấp.

Chúng tôi cũng ghi đè toán tử nối của PyTorch để phân bổ bộ nhớ hiệu quả hơn. PyTorch chỉ hỗ trợ tensor liên tục, và do đó, nối hai tensor luôn dẫn đến phân bổ bộ nhớ mới. Prompt Cache cần nối trạng thái chú ý của các module prompt, và hành vi mặc định sẽ dẫn đến phân bổ bộ nhớ dư thừa. Chúng tôi triển khai một toán tử nối có buffer tái sử dụng bộ nhớ khi nối tensor. Tối ưu hóa này cải thiện dấu chân bộ nhớ của Prompt Cache và giảm chi phí phân bổ bộ nhớ.

5 ĐÁNH GIÁ

Đánh giá Prompt Cache của chúng tôi tập trung vào trả lời ba câu hỏi nghiên cứu sau: (i) Tác động của Prompt Cache đối với độ trễ thời gian đến token đầu tiên (TTFT) và chất lượng đầu ra là gì (§5.2 – §5.4), (ii) Chi phí lưu trữ bộ nhớ là gì (§5.5), và (iii) Những ứng dụng nào phù hợp với Prompt Cache (§5.6). Chúng tôi sử dụng KV Cache thông thường (Pope et al., 2022) làm baseline. Prompt Cache và KV Cache chia sẻ chính xác cùng pipeline suy luận ngoại trừ tính toán trạng thái chú ý. Chúng tôi sử dụng độ trễ TTFT để so sánh, đo thời gian để sinh token đầu tiên, vì Prompt Cache và KV Cache có cùng độ trễ giải mã sau token đầu tiên.

5.1 Môi Trường Đánh Giá

Chúng tôi đánh giá Prompt Cache trên hai cấu hình CPU: Intel i9-13900K đi kèm với 128 GB DDR5 RAM ở 5600 MT/s và AMD Ryzen 9 7950X được ghép với 128 GB DDR4 RAM ở 3600 MT/s. Cho benchmark GPU của chúng tôi, chúng tôi triển khai ba GPU NVIDIA: RTX 4090, được ghép với Intel i9-13900K, và A40 và A100, cả hai đều là node ảo được lưu trữ trên NCSA Delta, mỗi node được cung cấp với 16-core AMD EPIC 7763 và 224 GB RAM. Chúng tôi sử dụng một số LLM mã nguồn mở, bao gồm Llama2, CodeLlama, MPT và Falcon. Chúng tôi sử dụng các LLM phù hợp với dung lượng bộ nhớ của một GPU đơn (40 GB). Chúng tôi sử dụng bộ LongBench (Bai et al., 2023) để đánh giá cải thiện TTFT và thay đổi chất lượng đầu ra. LongBench bao gồm một mẫu con được tuyển chọn của dữ liệu dài, dao động từ 4K đến 10K độ dài ngữ cảnh, các đoạn trích từ 21 bộ dữ liệu qua 6 danh mục, bao gồm các tác vụ như trả lời câu hỏi đa tài liệu (Yang et al., 2018; Ho et al., 2020; Trivedi et al., 2022; Koˇcisk`y et al., 2018; Joshi et al., 2017), tóm tắt (Huang et al., 2021; Zhong et al., 2021; Fabbri et al., 2019), và hoàn thành mã (Guo et al., 2023; Liu et al., 2023a). Chúng tôi định nghĩa các tài liệu trong bộ dữ liệu LongBench, chẳng hạn như trang wiki và bài báo tin tức, làm module prompt. Chúng tôi giữ các chỉ thị cụ thể cho tác vụ làm văn bản người dùng chưa cache.

--- TRANG 8 ---
Prompt Cache: Tái Sử Dụng Chú Ý Theo Module cho Suy Luận Độ Trễ Thấp

Baseline
Prompt Cache (bộ nhớ CPU)Prompt Cache (bộ nhớ GPU)

NVIDIA A40NVIDIA A100NVIDIA RTX 4090

Hình 3. Đo độ trễ GPU: thời gian đến token đầu tiên (TTFT) cho tám bộ dữ liệu LongBench trên ba GPU NVIDIA.

5.2 Cải Thiện Độ Trễ trên Bộ Dữ Liệu Benchmark

Chúng tôi đo độ trễ TTFT trên cả GPU và CPU sử dụng Llama 7B, như được hiển thị trong Hình 3 và Hình 4. Trong đánh giá GPU của chúng tôi, chúng tôi sử dụng hai thiết lập bộ nhớ: lưu trữ module prompt trong bộ nhớ CPU hoặc GPU. Cho thử nghiệm CPU, chúng tôi sử dụng bộ nhớ CPU. Do hạn chế không gian, chúng tôi chỉ trình bày 8 benchmark. Benchmark hoàn chỉnh từ 21 bộ dữ liệu có thể được tìm thấy trong Phụ lục.

5.2.1 Độ Trễ Suy Luận GPU

Chúng tôi tóm tắt phát hiện của chúng tôi trong Hình 3, đánh giá trên ba GPU NVIDIA: RTX 4090, A40 và A100. Thanh màu vàng đại diện cho việc tải module prompt từ bộ nhớ CPU, trong khi thanh màu xanh đại diện cho trường hợp trong bộ nhớ GPU. Có một xu hướng độ trễ nhất quán qua các bộ dữ liệu vì các mẫu LongBench có độ dài tương đương, trung bình 5K token.

Chúng tôi quan sát giảm độ trễ TTFT đáng kể qua tất cả bộ dữ liệu và GPU, dao động từ 1.5× đến 3× khi sử dụng bộ nhớ CPU, và từ 5× đến 10× khi sử dụng bộ nhớ GPU. Những kết quả này phân định ranh giới trên và dưới của việc giảm độ trễ có thể với Prompt Cache. Việc giảm độ trễ thực tế trong thực tiễn sẽ nằm giữa những ranh giới này, dựa trên việc sử dụng bao nhiều mỗi loại bộ nhớ.

5.2.2 Độ Trễ Suy Luận CPU

Hình 4 cho thấy Prompt Cache đạt được giảm độ trễ lên đến 70× và 20× trên CPU Intel và AMD, tương ứng. Chúng tôi đoán rằng sự khác biệt này bị ảnh hưởng

Intel i9-13900K
AMD Ryzen 9 7950XBaseline Prompt Cache (bộ nhớ CPU)

Hình 4. Đo độ trễ CPU: thời gian đến token đầu tiên (TTFT) cho tám bộ dữ liệu LongBench trên hai CPU.

bởi sự khác biệt về băng thông bộ nhớ trong thiết lập hệ thống (5600MT/s DDR5 RAM trên CPU Intel so với 3600MT/s DDR4 RAM trên CPU AMD). Như mong đợi, độ trễ cao hơn cho các bộ dữ liệu với tỷ lệ prompt chưa cache lớn hơn, chẳng hạn như TriviaQA. Thú vị là, suy luận CPU được hưởng lợi đáng kể hơn từ Prompt Cache so với suy luận GPU. Điều này được quy cho độ trễ tính toán chú ý lớn hơn nhiều trong CPU, đặc biệt khi chuỗi trở nên dài hơn (ví dụ, FP16/FP32 FLOP thấp hơn so với GPU). Điều này chỉ ra rằng Prompt Cache đặc biệt có lợi cho việc tối ưu hóa suy luận trong môi trường hạn chế tài nguyên, chẳng hạn như thiết bị edge hoặc máy chủ cloud với tài nguyên GPU hạn chế.

5.3 Độ Chính Xác với Prompt Cache

Để xác minh tác động của Prompt Cache đối với chất lượng phản hồi LLM, không có scaffolding, chúng tôi đo điểm chính xác với bộ LongBench. Để chứng minh tính áp dụng tổng quát, chúng tôi áp dụng Prompt Cache cho ba LLM có kiến trúc transformer khác nhau (§4.2): Llama2, MPT và Falcon. Kết quả benchmark độ chính xác được hiển thị trong Bảng 1 chứng minh Prompt Cache bảo tồn độ chính xác của đầu ra. Chúng tôi sử dụng lấy mẫu xác định trong đó token có xác suất cao nhất được chọn ở mỗi bước để kết quả với và không có Prompt Cache có thể so sánh được. Qua tất cả bộ dữ liệu, độ chính xác của đầu ra với Prompt Cache có thể so sánh với baseline.

5.4 Hiểu Cải Thiện Độ Trễ

Về mặt lý thuyết, Prompt Cache nên cung cấp giảm độ trễ TTFT bậc hai so với KV Cache thông thường. Điều này là bởi vì, trong khi chi phí memcpy của Prompt Cache tăng tuyến tính với độ dài chuỗi, tính toán tự chú ý có độ phức tạp tính toán bậc hai đối với độ dài chuỗi. Để xác nhận điều này, chúng tôi kiểm tra Prompt Cache trên một

--- TRANG 9 ---
Prompt Cache: Tái Sử Dụng Chú Ý Theo Module cho Suy Luận Độ Trễ Thấp

Bộ dữ liệu | Metric | Llama2 7B | Llama2 13B | MPT 7B | Falcon 7B
---|---|---|---|---|---
 | | Baseline | Cached | Baseline | Cached | Baseline | Cached | Baseline | Cached
Narrative QA | F1 | 19.93 | 19.38 | 20.37 | 19.94 | 10.43 | 11.33 | 7.14 | 8.87
2 Wiki Multi-Hop QA | F1 | 16.63 | 13.95 | 14.59 | 17.69 | 10.44 | 13.70 | 14.42 | 15.07
MuSiQue | F1 | 7.31 | 8.57 | 10.03 | 12.14 | 7.38 | 7.32 | 4.81 | 5.86
GovReport | Rouge L | 24.67 | 25.37 | 28.13 | 28.18 | 26.96 | 27.49 | 22.39 | 23.40
QMSum | Rouge L | 19.24 | 19.46 | 18.80 | 18.82 | 15.19 | 15.51 | 12.84 | 12.96
MultiNews | Rouge L | 24.33 | 24.22 | 25.43 | 26.23 | 25.42 | 25.66 | 20.91 | 21.19
TriviaQA | F1 | 13.04 | 12.33 | 23.19 | 22.38 | 10.57 | 9.17 | 13.31 | 11.42
Passage Retrieval | Acc | 7.50 | 4.25 | 9.08 | 6.50 | 3.03 | 3.85 | 3.00 | 3.45

Bảng 1. Benchmark độ chính xác trên bộ dữ liệu LongBench. Chúng tôi đánh dấu các giá trị ngoại lệ bằng chữ đậm, trong đó hiệu suất cao hơn 2.5 so với đối tác.

Lợi thế Cache
trong CPU

Lợi thế Cache
trong A40

Lợi thế Cache
trong RTX 4090

Hình 5. Lợi thế cache: So sánh chi phí tính toán và cache trong GPU và CPU. Trong khi chi phí tính toán chú ý tăng bậc hai, chi phí sao chép bộ nhớ trạng thái chú ý (tức là Prompt Cache) tăng tuyến tính. Ở đây, GPU tải module prompt trực tiếp từ bộ nhớ CPU.

LLM | BERT | Falcon 1B | Llama 7B | Llama 13B
---|---|---|---|---
MB/token | 0.03 | 0.18 | 0.50 | 0.78

LLM | MPT 30B | Falcon 40B | Llama 70B | Falcon 180B
---|---|---|---|---
MB/token | 1.31 | 1.87 | 2.5 | 4.53

Bảng 2. Chi phí bộ nhớ cache một token đơn

bộ dữ liệu tổng hợp với độ dài chuỗi khác nhau, giả sử tất cả prompt đều được cache. Chúng tôi so sánh độ trễ TTFT của Prompt Cache với KV Cache thông thường sử dụng CPU Intel i9-13900K và hai GPU (NVIDIA RTX 4090 và A40) với mô hình Llama2 7B. Cho cả CPU và GPU, bộ nhớ CPU được sử dụng để lưu trữ module prompt.

Cải thiện bậc hai: Phát hiện của chúng tôi, được trình bày trong Hình 5, cho thấy độ trễ của KV Cache tăng bậc hai với độ dài chuỗi, trong khi chi phí sao chép bộ nhớ của Prompt Cache tăng tuyến tính. Điều này có nghĩa là lợi thế độ trễ của Prompt Cache (khoảng cách giữa hai đường cong) mở rộng bậc hai với độ dài chuỗi. Sự khác biệt này rõ rệt hơn trên CPU so với GPU vì CPU trải qua độ trễ tính toán chú ý cao hơn, trong khi sự khác biệt giữa chi phí của Prompt Cache, tức là memcpy từ host đến thiết bị trong GPU và memcpy từ host đến host trong CPU không đáng kể. Với trạng thái chú ý với 5K token, độ trễ cho memcpy từ host đến host, host đến thiết bị, và thiết bị đến thiết bị lần lượt là 3.79 ms, 5.34 ms, và 0.23 ms.

Hiệu ứng của kích thước mô hình: Hơn nữa, khi kích thước tham số của mô hình tăng, chi phí tính toán cho KV Cache cũng tăng theo. Ví dụ, chuyển từ mô hình 7B sang 13B ở độ dài token 3K thêm 220 ms độ trễ, trong khi Prompt Cache chỉ thêm 30 ms. Sự khác biệt này xuất phát từ việc độ phức tạp LLM cũng tỷ lệ bậc hai với kích thước chiều ẩn. Ví dụ, FLOP của chú ý là 6nd2+ 4n2d, cho thao tác prefill. Điều này gợi ý rằng lợi thế của Prompt Cache so với KV Cache cũng tăng bậc hai với kích thước mô hình (tức là chiều ẩn).

Độ trễ end-to-end: Vì Prompt Cache chỉ giảm TTFT, tác động của nó đối với thời gian cần thiết để nhận phản hồi LLM hoàn chỉnh giảm khi số token được sinh tăng. Ví dụ, trên RTX 4090 với Llama 7B cho ngữ cảnh 3K, Prompt Cache nâng cao TTFT từ 900 ms xuống 90 ms, trong khi thời gian sinh token hoặc thời gian đến token tiếp theo (TTST) vẫn nhất quán giữa KV Cache và Prompt Cache ở trung bình 32 ms mỗi token, bất kể độ dài token. Tuy nhiên, thời gian phản hồi nhanh hơn đóng góp tích cực vào trải nghiệm người dùng và độ trễ end-to-end tổng thể (Lew et al., 2018; Liu et al., 2023b). Ví dụ, cho rằng Prompt Cache nâng cao TTFT từ 900 ms xuống 90 ms, điều này tương đương với việc sinh thêm 25 token trong cùng khung thời gian. Một yếu tố khác là Prompt Cache cho phép chia sẻ trạng thái chú ý trong cùng batch, như chúng tôi đã thảo luận trong §3.4. Tùy thuộc vào đặc điểm workload, Prompt Cache có thể cải thiện thông lượng tổng thể bằng cách sử dụng kích thước batch lớn hơn được kích hoạt bởi dấu chân bộ nhớ giảm. Ví dụ, giả sử có 100 yêu cầu, mỗi yêu cầu có prompt 2K token. Nếu tất cả prompt chia sẻ cùng module 1K token, Prompt Cache có thể giảm dấu chân bộ nhớ 50% khi kết hợp với các phương pháp như paged attention, cho phép

--- TRANG 10 ---
Prompt Cache: Tái Sử Dụng Chú Ý Theo Module cho Suy Luận Độ Trễ Thấp

<unit.py/>
<map.py/>
<player.py/>
<game.py/>
<database.py/>
<user>
Create a main entry point for 
the game, using Map, Player, 
and Game classes.
</user>    map = Map(grid_size=100)
    player1 = Player(player_id=1, name='Player 1')
    player2 = Player(player_id=2, name='Player 2')
    game = Game(players=[player1, player2], map=map)
    game.start_game()
    map = Map(grid_size=100)
    player1 = Player(player_id=1, name='Player 1')
    player2 = Player(player_id=2, name='Player 2')
    game = Game(map=map, players=[player1, player2])
    game.start_game()Baseline
Prompt CachingPrompt người dùng (GPU: 924ms, CPU: 75,976ms)
(GPU: 93ms, CPU: 861ms)

Hình 6. Sinh mã sử dụng Prompt Cache: Mỗi tệp nguồn trở thành một module prompt, cho phép người dùng "nhập" tệp trong ngữ cảnh prompt của họ với chi phí tối thiểu.

<middle-school/>
<beginner/>
<studied-a-year-before/>
<auditory/>
<essay/>
<high-intrinsic-motivation/>
<user>
Concisely describe the learn-
er's profile.
</user>The learner is a middle school student transi-
tioning from elementary school, … They are at the 
beginning stage of learning new subjects … learn-
ing style, with a preference for auditory learn-
ing … They are motivated by intrinsic factors, … 
The learner is a middle school student transi-
tioning from elementary school, … They are at the 
beginning stage of their learning journey … The 
learner's preferred learning style is auditory, … 
they have a high intrinsic motivation for … Baseline
Prompt CachingPrompt người dùng (GPU: 216ms, CPU: 22,449ms)
(GPU: 65ms, CPU: 686ms)

Hình 7. Ví dụ cá nhân hóa: Sáu danh mục mỗi danh mục có năm đặc điểm. Các đặc điểm trong cùng danh mục được nhóm trong <union>.

kích thước batch làm việc lớn hơn và do đó thông lượng cao hơn.

5.5 Chi Phí Bộ Nhớ

Chi phí bộ nhớ liên quan đến Prompt Cache tỷ lệ với số lượng token được cache tổng hợp. Chi phí này có thể được xác định bằng cách tham chiếu cả schema prompt và LLM đích. Trong Bảng 2, chúng tôi làm rõ chi phí bộ nhớ trên cơ sở mỗi token, dưới giả định sử dụng độ chính xác 16-bit cho số dấu phẩy động. Cho các mô hình nhỏ gọn, chẳng hạn như Falcon 1B, cache một tài liệu chứa 1K token sẽ yêu cầu khoảng 180 MB bộ nhớ. Nếu có hàng trăm module prompt, tiêu thụ bộ nhớ kết hợp sẽ dao động trong hàng chục gigabyte—một số lượng trong giới hạn bộ nhớ của GPU cấp máy chủ. Ngược lại, cho các mô hình lớn hơn như Llama 70B, cache một module độ dài 1K sẽ chiếm 2.5 GB bộ nhớ đáng kể mỗi tài liệu, điều này để lại bộ nhớ CPU là lựa chọn duy nhất để lưu trữ module prompt.

Cho những cân nhắc này, các phương pháp nén cho trạng thái chú ý (Zhang et al., 2023) vẫn là một hướng cho nghiên cứu tương lai trong các kỹ thuật prompt caching.

5.6 Ứng Dụng của Prompt Cache

Chúng tôi chứng minh tính biểu cảm của PML với các trường hợp sử dụng ví dụ yêu cầu cấu trúc prompt phức tạp hơn và các tính năng nâng cao (§3.2) so với benchmark LongBench:

<travel-plan for="a week">
  <overseas>
    <tokyo/>
  </overseas>
</travel-plan>
<user>
Create a travel plan
</user>Great! Based on your preferences, I have created 
a 7-day travel plan for you to explore the vi-
brant city of Tokyo, Japan … Day 1: Arrival and 
Exploration of Tokyo * Arrive at Narita or Haneda 
Airport and transfer to your hotel … * Visit the 
famous Shibuya Crossing  …
Great, I'd be happy to help you plan your trip to 
Tokyo, Japan! Here's a 7-day itinerary that in-
cludes … Day 1: Arrival and Exploring Tokyo * 
Arrive at Narita or Haneda Airport and take a 
train or bus to your hotel * Visit the famous 
Shibuya Crossing, …Baseline
Prompt CachingPrompt người dùng (GPU: 75ms, CPU: 4,725ms)
(GPU: 54ms, CPU: 479ms)

Hình 8. Prompt có tham số: <travel-plan> được cấu hình lại tại thời gian chạy trong khi duy trì hiệu quả cache, cung cấp cấu trúc prompt linh hoạt.

mark: (i) nhiều module trong một truy vấn, (ii) liên hợp, và (iii) tham số hóa. Hơn nữa, những tác vụ này nhấn mạnh việc giảm độ trễ đáng chú ý khi số token được cache tăng trong những trường hợp sử dụng phức tạp như vậy. Qua các trường hợp sử dụng, chúng tôi cung cấp đánh giá định tính về đầu ra bằng cách đối chiếu sinh cached và non-cached, cho thấy Prompt Cache duy trì chất lượng đầu ra, cùng với việc giảm độ trễ đạt được bởi Prompt Cache. Chúng tôi sử dụng Llama2 7B và lưu trữ module prompt trong bộ nhớ cục bộ (tức là bộ nhớ GPU cho suy luận GPU). Schema đầy đủ cho những tác vụ này có sẵn trong Phụ lục B.

5.6.1 Sinh Mã

LLM thường được sử dụng cho sinh mã (Guo et al., 2023; Liu et al., 2023a), hỗ trợ lập trình viên trong việc hỗ trợ hoặc trực tiếp sinh mã. Các phương pháp hiện có, chẳng hạn như Copilot (GitHub, 2023), thường tập trung vào các tệp nguồn riêng lẻ. Tuy nhiên, Prompt Cache có thể mở rộng điều này ra nhiều tệp tận dụng bản chất modular của mã nguồn. Ví dụ, mỗi class hoặc hàm có thể là một module prompt riêng biệt. Hình 6 minh họa sinh mã đa nguồn sử dụng CodeLlama 7B (Rozìere et al., 2023). Chúng tôi coi các class riêng lẻ như Unit, Map, Game và Player làm module prompt trong schema của chúng tôi cho lập trình game. Người dùng sau đó có thể bao gồm những module prompt này bất cứ khi nào họ cần chúng trong mã. Có cải thiện 4× trong độ trễ TTFT trên GPU trong khi đầu ra vẫn giống hệt nhau.

5.6.2 Cá Nhân Hóa

Hình 7 cho thấy lợi ích độ trễ và chất lượng đầu ra của Prompt Cache trong trường hợp sử dụng cá nhân hóa. Cá nhân hóa là không thể thiếu đối với nhiều hệ thống đề xuất (Wu et al., 2023), tìm thấy ứng dụng nổi bật trong ngữ cảnh LLM như giáo dục, đề xuất nội dung và tiếp thị nhắm mục tiêu. Chúng tôi làm nổi bật hiệu quả của cá nhân hóa dựa trên tính năng thông qua Prompt Cache. Ở đây, cá nhân hóa phụ thuộc vào một tập hợp tính năng được định nghĩa. Mỗi tính năng được đại diện như một module prompt riêng biệt, với mối quan hệ giữa các tính năng được ký hiệu bằng thẻ liên hợp như cấp độ, thành thạo,

--- TRANG 11 ---
Prompt Cache: Tái Sử Dụng Chú Ý Theo Module cho Suy Luận Độ Trễ Thấp

lịch sử học tập, phong cách học tập và loại đánh giá.

5.6.3 Prompt Có Tham Số

Trong Hình 8, chúng tôi cho thấy trường hợp sử dụng lập kế hoạch du lịch tận dụng tham số hóa (§3.2). Schema được sử dụng trong trường hợp sử dụng này bao gồm một tham số có thể điều chỉnh để chỉ định thời lượng chuyến đi cùng với hai module liên hợp để chọn điểm đến. Người dùng có thể tái sử dụng prompt có mẫu với tham số tùy chỉnh, tận hưởng độ trễ TTFT thấp hơn và cùng chất lượng phản hồi LLM được kích hoạt bởi Prompt Cache.

6 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Chúng tôi giới thiệu Prompt Cache, một kỹ thuật tăng tốc dựa trên hiểu biết rằng trạng thái chú ý có thể được tái sử dụng qua các prompt LLM. Prompt Cache sử dụng schema prompt để phân định những đoạn văn bản được tái sử dụng như vậy, tạo thành chúng thành cấu trúc modular và có tính chất vị trí mạch lạc được gọi là "module prompt". Điều này cho phép người dùng LLM kết hợp những module này một cách liền mạch vào prompt của họ, do đó tận dụng chúng cho ngữ cảnh với tác động độ trễ không đáng kể. Đánh giá của chúng tôi trên bộ dữ liệu benchmark chỉ ra giảm độ trễ TTFT lên đến 8× trên GPU và 60× trên CPU.

Cho công việc tương lai, chúng tôi có kế hoạch sử dụng Prompt Cache như một khối xây dựng cho các hệ thống phục vụ LLM tương lai. Một hệ thống như vậy có thể được trang bị các chiến lược thay thế cache GPU được tối ưu hóa để đạt được ranh giới độ trễ thấp hơn có thể bởi Prompt Cache. Các chiến lược khác nhau để giảm chi phí bộ nhớ từ host đến thiết bị cũng có thể có lợi, chẳng hạn như tích hợp các kỹ thuật nén trong KV cache, hoặc sử dụng grouped query attention. Một khám phá hứa hẹn khác là các primitive GPU để chia sẻ trạng thái chú ý qua các yêu cầu đồng thời, như chúng tôi đã thảo luận ngắn gọn trong §3.4. Điều này không chỉ có thể giảm độ trễ TTFT mà còn độ trễ thời gian mỗi token đầu ra (TPOT) bằng cách đóng gói nhiều yêu cầu hơn vào một batch đơn. Cuối cùng, Prompt Cache có thể trực tiếp tăng tốc các phương pháp sinh tăng cường ngữ cảnh (RAG) trong ngữ cảnh, trong đó hệ thống truy xuất thông tin về cơ bản phục vụ như một cơ sở dữ liệu của các module prompt. Prompt Cache có thể đặc biệt hữu ích cho các ứng dụng RAG nhạy cảm với độ trễ trong trả lời câu hỏi thời gian thực và hệ thống đối thoại.

LỜI CẢM ƠN

Công trình này được hỗ trợ một phần bởi NSF Athena AI Institute (Giải thưởng #2112562), NSF Award #2047220, và Đại học Yale. Công trình này đã sử dụng hệ thống Delta tại National Center for Supercomputing Applications (NCSA) thông qua phân bổ CIS230289 từ chương trình Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS), được hỗ trợ bởi các tài trợ National Science Foundation (Giải thưởng #2138259, #2138286, #2138307, #2137603, và #2138296).

TÀI LIỆU THAM KHẢO

Aminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed- inference: Enabling efficient inference of transformer models at unprecedented scale. In Wolf, F., Shende, S., Culhane, C., Alam, S. R., and Jagode, H. (eds.), SC22: International Conference for High Performance Computing, Networking, Storage and Analysis, Dallas, TX, USA, November 13-18, 2022, pp. 46:1–46:15. IEEE, 2022. doi: 10.1109/SC41404.2022.00051. URL https://doi.org/10.1109/SC41404.2022.00051.

Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., Dong, Y., Tang, J., and Li, J. Longbench: A bilingual, multitask benchmark for long context understanding. CoRR, abs/2308.14508, 2023. doi: 10.48550/ARXIV.2308.14508. URL https://doi.org/10.48550/arXiv.2308.14508.

Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004.05150.

Beurer-Kellner, L., Fischer, M., and Vechev, M. Prompting is programming: A query language for large language models. Proceedings of the ACM on Programming Languages, 7(PLDI):1946–1969, 2023.

Cui, J., Li, Z., Yan, Y., Chen, B., and Yuan, L. Chatlaw: Open-source legal large language model with integrated external knowledge bases, 2023.

Dao, T., Fu, D., Ermon, S., Rudra, A., and Ré, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344–16359, 2022.

Driess, D., Xia, F., Sajjadi, M. S. M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., and Florence, P. Palm-e: An embodied multimodal language model, 2023.

Dufter, P., Schmitt, M., and Schütze, H. Position information in transformers: An overview. Computational Linguistics, 48(3):733–763, 2022.

Fabbri, A. R., Li, I., She, T., Li, S., and Radev, D. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1074–1084, 2019.

--- TRANG 12 ---
Prompt Cache: Tái Sử Dụng Chú Ý Theo Module cho Suy Luận Độ Trễ Thấp

Feng, Y., Jeon, H., Blagojevic, F., Guyot, C., Li, Q., and Li, D. Attmemo: Accelerating transformers with memoization on big memory systems, 2023.

GitHub. Github copilot ·your ai pair programmer, 2023. URL https://github.com/features/copilot.

Guidence. A guidance language for controlling large language models. https://github.com/guidance-ai/guidance, 2023.

Guo, D., Xu, C., Duan, N., Yin, J., and McAuley, J. Longcoder: A long-range pre-trained language model for code completion. arXiv preprint arXiv:2306.14893, 2023.

Ho, X., Nguyen, A.-K. D., Sugawara, S., and Aizawa, A. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 6609–6625, 2020.

Huang, L., Cao, S., Parulian, N., Ji, H., and Wang, L. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1419–1436, 2021.

Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022.

Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601–1611, 2017.

Keles, F. D., Wijewardena, P. M., and Hegde, C. On the computational complexity of self-attention, 2022.

Koˇcisk`y, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317–328, 2018.

Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. arXiv preprint arXiv:2309.06180, 2023.

Lew, Z., Walther, J. B., Pang, A., and Shin, W. Interactivity in online chat: Conversational contingency and response latency in computer-mediated communication. Journal of Computer-Mediated Communication, 23(4):201–221, 2018.

Liu, T., Xu, C., and McAuley, J. Repobench: Benchmarking repository-level code auto-completion systems. arXiv preprint arXiv:2306.03091, 2023a.

Liu, Y., Li, H., Du, K., Yao, J., Cheng, Y., Huang, Y., Lu, S., Maire, M., Hoffmann, H., Holtzman, A., Ananthanarayanan, G., and Jiang, J. Cachegen: Fast context loading for language model applications, 2023b.

MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.

Nay, J. J., Karamardian, D., Lawsky, S. B., Tao, W., Bhat, M., Jain, R., Lee, A. T., Choi, J. H., and Kasai, J. Large language models as tax attorneys: A case study in legal capabilities emergence, 2023.

Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.

Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and Launay, J. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.

Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Levskaya, A., Heek, J., Xiao, K., Agrawal, S., and Dean, J. Efficiently scaling transformer inference, 2022.

Press, O., Smith, N. A., and Lewis, M. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0.

Qin, Y., Hu, S., Lin, Y., Chen, W., Ding, N., Cui, G., Zeng, Z., Huang, Y., Xiao, C., Han, C., Fung, Y. R., Su, Y., Wang, H., Qian, C., Tian, R., Zhu, K., Liang, S., Shen, X., Xu, B., Zhang, Z., Ye, Y., Li, B., Tang, Z., Yi, J., Zhu, Y., Dai, Z., Yan, L., Cong, X., Lu, Y., Zhao, W., Huang, Y., Yan, J., Han, X., Sun, X., Li, D., Phang, J., Yang,

--- TRANG 13 ---
Prompt Cache: Tái Sử Dụng Chú Ý Theo Module cho Suy Luận Độ Trễ Thấp

C., Wu, T., Ji, H., Liu, Z., and Sun, M. Tool learning with foundation models. CoRR, abs/2304.08354, 2023. doi: 10.48550/ARXIV.2304.08354. URL https://doi.org/10.48550/arXiv.2304.08354.

Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Improving language understanding by generative pre-training. 2018.

Rasmy, L., Xiang, Y., Xie, Z., Tao, C., and Zhi, D. Medbert: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. npj Digit. Medicine, 4, 2021. doi: 10.1038/S41746-021-00455-Y. URL https://doi.org/10.1038/s41746-021-00455-y.

Rozìere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Canton-Ferrer, C., Grattafiori, A., Xiong, W., Défossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and Synnaeve, G. Code llama: Open foundation models for code. CoRR, abs/2308.12950, 2023. doi: 10.48550/ARXIV.2308.12950. URL https://doi.org/10.48550/arXiv.2308.12950.

Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilic, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., Tow, J., Rush, A. M., Biderman, S., Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., Muennighoff, N., del Moral, A. V., Ruwase, O., Bawden, R., Bekman, S., McMillan-Major, A., Beltagy, I., Nguyen, H., Saulnier, L., Tan, S., Suarez, P. O., Sanh, V., Laurençon, H., Jernite, Y., Launay, J., Mitchell, M., Raffel, C., Gokaslan, A., Simhi, A., Soroa, A., Aji, A. F., Alfassy, A., Rogers, A., Nitzav, A. K., Xu, C., Mou, C., Emezue, C., Klamm, C., Leong, C., van Strien, D., Adelani, D. I., and et al. BLOOM: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100, 2022. doi: 10.48550/ARXIV.2211.05100. URL https://doi.org/10.48550/arXiv.2211.05100.

Shen, J. T., Yamashita, M., Prihar, E., Heffernan, N. T., Wu, X., and Lee, D. Mathbert: A pre-trained language model for general NLP tasks in mathematics education. CoRR, abs/2106.07340, 2021. URL https://arxiv.org/abs/2106.07340.

Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Chen, B., Liang, P., Ré, C., Stoica, I., and Zhang, C. Flexgen: High-throughput generative inference of large language models with a single GPU. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 31094–31116. PMLR, 2023. URL https://proceedings.mlr.press/v202/sheng23a.html.

Steinberg, E., Jung, K., Fries, J. A., Corbin, C. K., Pfohl, S. R., and Shah, N. H. Language models are an effective representation learning technique for electronic health record data. J. Biomed. Informatics, 113:103637, 2021. doi: 10.1016/J.JBI.2020.103637. URL https://doi.org/10.1016/j.jbi.2020.103637.

Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding. CoRR, abs/2104.09864, 2021. URL https://arxiv.org/abs/2104.09864.

Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient transformers: A survey. ACM Comput. Surv., 55(6):109:1–109:28, 2023. doi: 10.1145/3530811. URL https://doi.org/10.1145/3530811.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models, 2023.

Trivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A. Musique: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539–554, 2022.

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2023.

White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A., Spencer-Smith, J., and Schmidt, D. C. A prompt pattern catalog to enhance prompt engineering with chatgpt. CoRR, abs/2302.11382, 2023. doi: 10.48550/ARXIV.2302.11382. URL https://doi.org/10.48550/arXiv.2302.11382.

Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., Drame, M.,

--- TRANG 14 ---
Prompt Cache: Tái Sử Dụng Chú Ý Theo Module cho Suy Luận Độ Trễ Thấp

Lhoest, Q., and Rush, A. M. Huggingface's transformers: State-of-the-art natural language processing, 2020.

Wu, L., Zheng, Z., Qiu, Z., Wang, H., Gu, H., Shen, T., Qin, C., Zhu, C., Zhu, H., Liu, Q., Xiong, H., and Chen, E. A survey on large language models for recommendation. CoRR, abs/2305.19860, 2023. doi: 10.48550/ARXIV.2305.19860. URL https://doi.org/10.48550/arXiv.2305.19860.

Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2369–2380, 2018.

Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Ré, C., Barrett, C., Wang, Z., and Chen, B. H2o: Heavy-hitter oracle for efficient generative inference of large language models, 2023.

Zhong, M., Yin, D., Yu, T., Zaidi, A., Mutuma, M., Jha, R., Hassan, A., Celikyilmaz, A., Liu, Y., Qiu, X., et al. Qmsum: A new benchmark for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 5905–5921, 2021.
