# 2203.14685.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/distributed/2203.14685.pdf
# File size: 1083028 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
HetuMoE: An Efﬁcient Trillion-scale
Mixture-of-Expert Distributed Training System
Xiaonan Nie, Pinxue Zhao, Xupeng Miao, Tong Zhao, Bin Cui
School of Computer Science,
Peking University, Beijing, China
{xiaonan.nie, 1800017766, xupeng.miao, zhaotong, bin.cui}@pku.edu.cn
Abstract
As giant dense models advance quality but require large amounts of GPU budgets
for training, the sparsely gated Mixture-of-Experts (MoE), a kind of conditional
computation architecture, is proposed to scale models while keeping their com-
putation constant. Speciﬁcally, the input tokens are routed by the gate network
and only activates part of the expert network. Existing MoE training systems only
support part of mainstream MoE models (e.g. Top k) training under expensive high-
bandwidth GPU clusters. In this paper, we present HetuMoE, a high-performance
large-scale sparse MoE training system built on Hetu. HetuMoE provides multiple
gating strategies and efﬁcient GPU kernel implementations. To further improve
the training efﬁciency on commodity GPU clusters (e.g, with only 1 NiC), we
introduce the hierarchical AllToAll communication that combines hierarchical
networks and aggregating messages. Compared with existing state-of-the-art MoE
systems, HetuMoE obtains at least 15% speedup. Speciﬁcally, HetuMoE outper-
forms DeepSpeed-MoE up to 8:1under the switch gate with a batch size of 32.
Our code is available at: https://github.com/PKU-DAIR/Hetu .
1 Introduction
Models tend to perform better with increasing data size, parameter size in many ﬁelds, such as natural
language processing (NLP) and computer vision (CV) [2, 3, 9, 14, 15]. However, these large models
always require huge amounts of GPU resources and take weeks or even months to train. For example,
it takes approximately 288 years with a single V100 NVIDIA GPU to train the GPT-3 model with
175 billion parameters [1, 12]. The sparsely gated Mixture of Experts (MoE), as a kind of conditional
computation architecture, has been proved to be an effective way to expand model size without
signiﬁcantly increasing the computation [4, 5, 10, 17, 19]. Speciﬁcally, an MoE model contains a
gating network and a pool of experts. During training, a token will be assigned to a small number of
experts for computation. This kind of sparsely-activated nature enables MoE to signiﬁcantly expand
the model size while keeping the amount of computation almost constant. The sparsely-activated
training paradigm necessitates new systems’ support. However, existing MoE training systems,
including DeepSpeed-MoE [16], Tutel [7], and FastMoE [6], are still facing some limitations in
both usability and efﬁciency. First, they only support parts of mainstream MoE models and gate
networks (e.g. Top k). Second, most of them assume “hyperclusters” that have expensive, high-speed
interconnects such as NVLink or Inﬁniband. They are suffering from a severe communication
bottleneck for common clusters. The above shortcomings have restricted their usage and limited the
exploration of MoE models in real applications.
In this paper, we ﬁrst brieﬂy introduce the common training process of MoE models and then propose
HetuMoE, a high-performance distributed MoE training system built on Hetu, which supports a
variety of mainstream gating strategies and achieves state-of-the-art training speed compared to
Preprint.arXiv:2203.14685v3  [cs.DC]  17 Nov 2022

--- PAGE 2 ---
Algorithm 1: General MoE Training Process
Data: xS: a group of tokens of size S,
Gate : gate network
E: expert network
Result: yS: tokens after precessing
1//Step 1: Get the target experts and corresponding weights. ;
2W(S; E ); idS=Gate (xS);
3//Step 2: Layout Transform on Input Data ;
4x0
S=Layout _Transform (xS; idS);
5//Step 3: AllToAll Communication ;
6x0
S=AllToAll (x0
S);
7//Step 4: Expert Processing ;
8fori 1to S do
9ys= 0;
10 foridx2idido
11 ys=ys+w(i; idx )eidx(xi);
12//Step 5: AllToAll Communication ;
13yS=AllToAll (yS);
14//Step 6: Reverse Layout Transform on Output Data ;
15yS=Reverse _Layout _Transform (yS; idS);
existing baseline systems. HetuMoE adopts several customized gating kernels’ implementation and
utilizes the hierarchical All-To-All communication optimization. Experiments show that HetuMoE
outperforms existing MoE systems at least a 15% speedup under different settings. Meanwhile,
HetuMoE outperforms DeepSpeed-MoE up to 8:1under the switch gate with the batch size of 32.
2 MoE Training Process
An MoE model consists of a gating network and a pool of sub-networks (e.g. Feed-Forward Networks
in Transformer), which are called experts. We formulate the common training process of MoE models
in Algorithm 1.
(1) The input tokens are ﬁrst processed by the gate network to know the target experts.
(2)The layout transform operation is conducted on each device to put tokens with the same
target experts in a continuous memory buffer for the incoming communication. (Line 4)
(3)An AllToAll communication operation is incurred to dispatch tokens to their corresponding
experts. (Line 6)
(4) Then each expert process its tokens respectively. (Line 8-11)
(5) The processed tokens are dispatched back to their GPUs. (Line 13)
(6)The reverse layout transform operation is conducted to put tokens back to their original
position in the training batch for future computation. (Line 15)
Figure 1: Time consumption of MoE layer of DeepSpeed-MoE under 8 A100 GPUs in a single node
2

--- PAGE 3 ---
Figure 2: HetuMoE compared with other MoE systems
We then evaluate DeepSpeed-MoE and proﬁle its time costs of MoE layer under 8 A100 GPUs in
a single node and the result is shown in Figure 1. As we can see, the computation of gate network,
including layout transform and its reverse operation, and AllToAll communication account for more
than 50% of training time totally. When scaling training into distributed scenarios across multiple
nodes, AllToAll communication overheads would deteriorate the entire training process, which takes
around 99% of training time under a 100 Gbps network.
3 HetuMoE
In order to solve above problems, we develop HetuMoE based on Hetu1, a high-performance
distributed deep learning system. HetuMoE supports various gating strategies proposed by recent
MoE approaches, such as Switch [5], GShard [10], M6 [21], BASE Layer [11] , Hash Layer [18],
SAM [8], and Dense-to-Sparse [13]. In addition, we implement hierarchical AllToAll [20], which
greatly improves network bandwidth utilization in the case of multi-node distributed training under
commodity network conditions, compared with vanilla AllToAll. The feature supported in HetuMoE
is compared with other MoE systems in Figure 2.
3.1 Gating Strategy in HetuMoE
Top1/Top2/Topk Shazeer et al., 2017 [19] proposed to utilize Topk activated MoE layer in LSTM
increase the model capacity up to 1000with only minor losses in efﬁciency. The Topk gate
is formulated in Equation 1, where xis the Ninput tokens, Wis the gate’s weight, E experts
ei(i21::E)andyis the output tokens. GShard [10] and Switch [5] propose to simplify the gate to
Top2 and Top1, and utilize a capacity factor Cto force the max received tokens by each expert. As
theKincreases, the MoE layer tends to perform better while leading to more computation. It’s a
trade-off on Kconsidering computation efﬁciency and model performance.
g=softmax (TopK (xW; K ))
y=EX
i=1g(x)iei(x)(1)
kTop1 Inspired by the observation that Kand expert capacity Cin Topk routing can signiﬁcantly
make a difference in model performance, M6-T [21] propose the KTop1 gating strategy, where
experts are divided into kprototypes and each token is assigned to the highest scored expert in each
prototype. Finally, the outputs of different prototypes are summed together as for the same input
token.
Hierarchical Topk(H Topk) As increasing the number of activated experts can boost the model
performance with a higher sparse ratio, SAM [8] (Switch and Mixture) proposed an efﬁcient hierar-
chical routing mechanism that divides the experts into different groups according to their devices
and activates multiple experts in the same group to avoid the remote communication cost between
1https://github.com/PKU-DAIR/Hetu
3

--- PAGE 4 ---
(a) Different Expert Number
 (b) Different Token Number
Figure 3: Topk kernel performance comparison with PyTorch
devices. Speciﬁcally, the Switch Router ﬁrst selects one group and then the Mixture Router selects
multiple experts in the same group for each token.
BASE Layer BASELayer [11] formulates token-to-expert allocation as a linear assignment problem
to improve efﬁciency, where balanced loads are guaranteed among each expert. Because no new
parameters or auxiliary expert-balanced losses are introduced, the training process is simpliﬁed. The
problem is formulated in Equation 2, where Ntokens with representations xiandEexperts with
embeddings we, we assign each token to an expert via the assignment index ai20::E:
maxNX
i=1xiwat
s:t:8eNX
i=11ai=e=N
E(2)
Hash Layer Hash Layer [18] utilizes various hash functions to map tokens to experts, such as
Random Hashes, Balanced assignment, and Clustered Hashes. Speciﬁcally, in Hash Layer, tokens are
placed in the corresponding buckets according to the used hash function and each bucket refers to an
expert.
(a) Transformation
 (b) Reverse transformation
(c) Performance comparison with Tutel
Figure 4: Data layout transformation illustration and performance comparison
Dense-To-Sparse Dense-To-Sparse Gate [13] considers that current approaches of jointly training
experts and the sparse gate introduce a negative impact on model accuracy, diminishing the efﬁciency
of expensive large-scale model training [5]. It proposes to begin as a dense gate that routes tokens to
all experts, then gradually and adaptively become sparser while routing to fewer experts. Speciﬁcally,
it utilizes the Gumbel Softmax and decreases the temperature during training.
4

--- PAGE 5 ---
Figure 5: Illustration of NCCL AllToAll.
Figure 6: Illustration of hierarchical AllToAll
3.2 HetuMoE Optimization
As shown in Algorithm 1, Gate ,Layout Transform andAllToAll are three key components
related to MoE models’ training, where the module of expert networks also exists in common models
and thus is not our target in MoE-speciﬁc optimization. The optimizations in HetuMoE about these
three modules are detailed as follows.
Gate Optimization Various gating strategies are supported in HetuMoE and we mainly optimize
thetopk operator as it is widely adopted in mainstream models [5, 10, 17]. The operator’s input is a
2-D tensor which has shape (num _tokens; num _experts )and executes as ﬁnding the top-k values
and the corresponding index for each row. The Topk operator implemented in PyTorch or Tensorﬂow
for arbitrary K. We conduct algorithmic optimizations for useful kin MoE, such as 1 and 2. We vary
num _experts andnum _tokens to conduct comparison with PyTorch and the experimental results
are shown in Figure 3. We outperform PyTorch Topk by 25% speed improvement averagely.
Layout Transform Optimization Data layout transformation is another important step in the MoE
training process. After the gating network decides the token-to-expert mapping, tokens assigned to
the same expert need to be put in physically continuous memory locations. We undertake kernel level
optimization to accelerate this process, and we achieve more than 26% improvement compared with
state-of-art implementation.
All-To-All Optimization In AllToAll operation, each GPU sends its data to all GPUs (one-for-all)
and receives data sent by all GPUs (all-for-one), where each data will be divided equally into n parts,
illustrated in Figure 3.2. Current AllToAll operations implemented in NCCL and MPI may suffer
from low utilization of network bandwidth because of the small message size. Speciﬁcally, if we
have N nodes, each with G GPUs, and each GPU has data size B, then the size of data transferred
between 2 GPUs isB
GN. A common setting is N= 8,G= 8, andB= 16 MB.
In HetuMoE, we utilize Hierarchical AllToAll , which combines hierarchical networks (intra-
node and inter-node) and aggregates messages, to optimize the communication between multi-
nodes equipped with one NIC. Jesper et al., 2014 [20] also utilize hierarchy to optimize alltoall
communication for MPI. Illustrated as Figure 6, it ﬁrst gathers the data of all GPUs inside one
5

--- PAGE 6 ---
(a) 4 nodes
 (b) 8 nodes
Figure 7: Hierarchical AllToAll performance
(a) Switch Gate
 (b) GShard Gate
Figure 8: Overall performance comparison with DeepSpeed-MoE, FastMoE, and Tutel.
node into one GPU. Then, a data layout transformation is undertaken to place the token assigned to
the same node in physically continuous memory. Afterwards, it launches All2All communication
between nodes. After AllToAll is done, it conducts the corresponding data layout transformation
and scatter operation to put each token to its corresponding expert. In this way, the size of data
transferred between nodes becomesBG
N, which is G2times larger than before. Meanwhile, this
two-level decoupled AllToAll also fully utilizes the intra-node (NvLink or PCIe) and inter-node
bandwidth (Inﬁniband or Ethernet). Experiments show that Hierarchical AllToAll achieves
1:66speedup in 48GPUs setting and 2speedup in 88GPUs setting.
Overall Performance Experiments are conducted on GPU Clusters, where each node is equipped
with 8 TITAN RTX GPUs and these eight GPUs are connected through PCIe. We compared HetuMoE
with existing MoE systems, including Tutel, FastMoE and DeepSpeed-MoE. Our test model is a
16-experts MoE layer, where each expert represents a FeedForward Network and the hidden size is
2048, The sequence length and embedding dimension of input data is 1024 and 2048 respectively.
We vary a batch size to compare the performance of each system.
As shown in Figure 8, HetuMoE achieves the state-of-art training performance in both Switch gate
and GShard gate. Speciﬁcally, compared with FastMoE, we achieve 18% speed-up in Switch Gate
and15% speed-up in GShard gate. Meanwhile, HetuMoE outperforms DeepSpeed-MoE up to 8:1
under the switch gate with a batch size of 32.
4 Conclusion
Existing MoE training systems lack various gating strategy support and perform badly in distributed
training. In this paper, we propose HetuMoE, a high-performance distributed MoE training system
built on Hetu, which supports a variety of mainstream gating strategies and achieves state-of-the-
art training speed compared to existing baseline systems. HetuMoE adopts several customized
gating kernels’ implementation and utilizes the hierarchical All-To-All to optimize distributed
communication by combining hierarchical network (intra-node and inter-node) and aggregating small
messages.
6

--- PAGE 7 ---
References
[1]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural
information processing systems , 33:1877–1901, 2020.
[2]J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional
transformers for language understanding. CoRR , abs/1810.04805, 2018.
[3]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,
M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for
image recognition at scale. arXiv preprint arXiv:2010.11929 , 2020.
[4]N. Du, Y . Huang, A. M. Dai, S. Tong, D. Lepikhin, Y . Xu, M. Krikun, Y . Zhou, A. W. Yu,
O. Firat, et al. Glam: Efﬁcient scaling of language models with mixture-of-experts. arXiv
preprint arXiv:2112.06905 , 2021.
[5]W. Fedus, B. Zoph, and N. Shazeer. Switch transformers: Scaling to trillion parameter models
with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961 , 2021.
[6]J. He, J. Qiu, A. Zeng, Z. Yang, J. Zhai, and J. Tang. Fastmoe: A fast mixture-of-expert training
system. arXiv preprint arXiv:2103.13262 , 2021.
[7]C. Hwang, W. Cui, Y . Xiong, Z. Yang, Z. Liu, H. Hu, Z. Wang, R. Salas, J. Jose, P. Ram, et al.
Tutel: Adaptive mixture-of-experts at scale. arXiv preprint arXiv:2206.03382 , 2022.
[8]H. Jiang, K. Zhan, J. Qu, Y . Wu, Z. Fei, X. Zhang, L. Chen, Z. Dou, X. Qiu, Z. Guo, R. Lai,
J. Wu, E. Hu, Y . Zhang, Y . Jia, F. Yu, and Z. Cao. Towards more effective and economic
sparsely-activated model. CoRR , abs/2110.07431, 2021.
[9]J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Rad-
ford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361 , 2020.
[10] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun, N. Shazeer, and Z. Chen.
Gshard: Scaling giant models with conditional computation and automatic sharding. CoRR ,
abs/2006.16668, 2020.
[11] M. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer. BASE layers: Simplifying
training of large, sparse models. CoRR , abs/2103.16716, 2021.
[12] D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V . Korthikanti, D. Vainbrand,
P. Kashinkunti, J. Bernauer, B. Catanzaro, et al. Efﬁcient large-scale language model training
on gpu clusters using megatron-lm. In Proceedings of the International Conference for High
Performance Computing, Networking, Storage and Analysis , pages 1–15, 2021.
[13] X. Nie, S. Cao, X. Miao, L. Ma, J. Xue, Y . Miao, Z. Yang, Z. Yang, and B. Cui. Dense-to-sparse
gate for mixture-of-experts. CoRR , abs/2112.14397, 2021.
[14] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are
unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
[15] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li, and P. J. Liu.
Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. arXiv preprint
arXiv:1910.10683 , 2019.
[16] S. Rajbhandari, C. Li, Z. Yao, M. Zhang, R. Y . Aminabadi, A. A. Awan, J. Rasley, and Y . He.
Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation
ai scale. arXiv preprint arXiv:2201.05596 , 2022.
[17] C. Riquelme, J. Puigcerver, B. Mustafa, M. Neumann, R. Jenatton, A. Susano Pinto, D. Keysers,
and N. Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information
Processing Systems , 34:8583–8595, 2021.
[18] S. Roller, S. Sukhbaatar, A. Szlam, and J. Weston. Hash layers for large sparse models. CoRR ,
abs/2106.04426, 2021.
[19] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outra-
geously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint
arXiv:1701.06538 , 2017.
7

--- PAGE 8 ---
[20] J. L. Träff and A. Rougier. Mpi collectives and datatypes for hierarchical all-to-all commu-
nication. In Proceedings of the 21st European MPI Users’ Group Meeting , pages 27–32,
2014.
[21] A. Yang, J. Lin, R. Men, C. Zhou, L. Jiang, X. Jia, A. Wang, J. Zhang, J. Wang, Y . Li, D. Zhang,
W. Lin, L. Qu, J. Zhou, and H. Yang. Exploring sparse expert models and beyond. CoRR ,
abs/2105.15082, 2021.
8
