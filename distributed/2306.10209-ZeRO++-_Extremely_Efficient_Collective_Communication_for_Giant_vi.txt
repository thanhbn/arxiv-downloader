# 2306.10209.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/distributed/2306.10209.pdf
# Kích thước file: 1859983 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
ZeRO++: Giao tiếp tập thể cực kỳ hiệu quả cho huấn luyện mô hình khổng lồ
Guanhua Wang*, Heyang Qin*, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari
Olatunji Ruwase, Feng Yan1, Lei Yang2, Yuxiong He
Microsoft
{ guanhuawang, heyangqin, samjacobs, connorholmes, samyamr, olruwase, yuxhe } @microsoft.com
TÓM TẮT
Zero Redundancy Optimizer (ZeRO) đã được sử dụng để huấn luyện nhiều mô hình ngôn ngữ lớn trên các cụm GPU khổng lồ do tính dễ sử dụng, hiệu quả và khả năng mở rộng tốt. Tuy nhiên, khi huấn luyện trên các cụm có băng thông thấp, hoặc ở quy mô buộc kích thước batch trên mỗi GPU phải nhỏ, thông lượng hiệu quả của ZeRO bị giới hạn do khối lượng giao tiếp cao từ việc thu thập trọng số trong quá trình truyền tiến, truyền ngược, và tính trung bình gradient. Bài báo này giới thiệu ba kỹ thuật giảm khối lượng giao tiếp, mà chúng tôi gọi chung là ZeRO++, nhắm vào từng giao tiếp tập thể trong ZeRO. Đầu tiên là all-gather dựa trên lượng tử hóa khối. Thứ hai là ánh xạ lại dữ liệu để đánh đổi giao tiếp lấy thêm bộ nhớ. Thứ ba là một mô hình tính trung bình gradient lượng tử hóa dựa trên all-to-all mới như một sự thay thế cho giao tiếp tập thể reduce-scatter, điều này bảo toàn độ chính xác mặc dù giao tiếp dữ liệu độ chính xác thấp. Tổng cộng, ZeRO++ giảm khối lượng giao tiếp của ZeRO 4 lần, cho phép thông lượng tốt hơn lên đến 2.16x ở quy mô 384 GPU.
TỪ KHÓA
Huấn luyện mô hình lớn, Điện toán hiệu năng cao, Học sâu
Định dạng tham chiếu ACM:
Guanhua Wang*, Heyang Qin*, Sam Ade Jacobs, Connor Holmes, Samyam
Rajbhandari and Olatunji Ruwase, Feng Yan1, Lei Yang2, Yuxiong He. 2023.
ZeRO++: Extremely Efficient Collective Communication for Giant Model
Training. In Proceedings of ABC. ACM, New York, NY, USA, 12 pages. https:
//doi.org/10.1145/nnnnnnn.nnnnnnn
1 GIỚI THIỆU MỞ RỘNG
Các mô hình học sâu (DL) đã được áp dụng thành công trong nhiều lĩnh vực khác nhau như phân tích hình ảnh/video, xử lý ngôn ngữ tự nhiên, nhận dạng giọng nói, v.v. Qua nhiều năm, chất lượng, chức năng và phạm vi của các mô hình này đã tiếp tục cải thiện. Kích thước mô hình đã là một yếu tố quan trọng trong việc cải thiện này. Có một mối tương quan mạnh mẽ giữa kích thước mô hình với độ chính xác và chức năng được cải thiện, và do đó, kích thước mô hình đã tăng trưởng đáng kể trong những năm gần đây. Ví dụ, kích thước tham số tăng từ 100 triệu lên 500+ tỷ từ BERT [9] đến Megatron-Turing NLG [33].

Giấy phép để tạo bản sao kỹ thuật số hoặc bản cứng của toàn bộ hoặc một phần tác phẩm này cho mục đích cá nhân hoặc lớp học được cấp miễn phí với điều kiện các bản sao không được tạo ra hoặc phân phối để kiếm lợi nhuận hoặc lợi thế thương mại và các bản sao phải có thông báo này và toàn bộ trích dẫn trên trang đầu tiên. Bản quyền cho các thành phần của tác phẩm này thuộc sở hữu của bên khác ngoài ACM phải được tôn trọng. Việc trừu tượng hóa có ghi nhận nguồn được cho phép. Để sao chép khác hoặc tái xuất bản, để đăng trên máy chủ hoặc để phân phối lại cho danh sách, cần có sự cho phép cụ thể trước và/hoặc phí. Yêu cầu quyền từ permissions@acm.org.
1University of Houston,2University of Nevada-Reno, * Đóng góp ngang nhau.
ABC, 2023, USA
©2023 Association for Computing Machinery.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
https://doi.org/10.1145/nnnnnnn.nnnnnnn

8IB
(800Gbps)4IB
(400Gbps)2IB
(200Gbps)1IB
(100Gbps)0204060TFLOPs trên mỗi GPU6157
44
2847
4037
26(a) Băng thông giao tiếp
Ảnh hưởng đến hiệu năng
Kích thước Batch
trên mỗi GPU:
2K TokensKích thước Batch
trên mỗi GPU:
1K TokensKích thước Batch
trên mỗi GPU:
512 Tokens0204060TFLOPs trên mỗi GPU61
48
2847
31
16(b) Kích thước Batch trên mỗi GPU
Ảnh hưởng đến hiệu năng 64 GPUs 384 GPUs

Hình 1: Thông lượng huấn luyện quy mô lớn bị hạn chế bởi băng thông mạng và kích thước batch trên mỗi GPU

Với việc tăng kích thước mô hình, yêu cầu bộ nhớ và tính toán để huấn luyện đã tăng đáng kể vượt quá khả năng của một bộ gia tốc đơn lẻ (ví dụ, một GPU). Huấn luyện các mô hình khổng lồ đòi hỏi việc sử dụng hiệu quả sức mạnh tính toán và bộ nhớ tổng hợp trên hàng trăm hoặc thậm chí hàng nghìn thiết bị GPU. Có hai cách tiếp cận phổ biến cho việc này, cụ thể là song song 3D [22,36] và Zero Redundancy Optimizer (ZeRO) [29].

Song song 3D kết hợp song song dữ liệu [2,6], song song pipeline [13,14,21] và song song tensor [32] để phân phối khối lượng công việc huấn luyện mô hình trên hàng trăm GPU. Cách tiếp cận này có thể đạt được hiệu quả tính toán và bộ nhớ tuyệt vời trên mỗi GPU. Tuy nhiên, một nhược điểm lớn ở đây là tính phức tạp của hệ thống và người dùng. Nó đặt gánh nặng tái cấu trúc mã GPU đơn để hoạt động với song song 3D lên các nhà khoa học dữ liệu và người thực hành AI, điều này không tầm thường và thường khó khăn.

Ngược lại, ZeRO cung cấp một giải pháp thay thế không yêu cầu tái cấu trúc mã mô hình. ZeRO là một biến thể tiết kiệm bộ nhớ của song song dữ liệu [2,6] nơi các trạng thái mô hình được phân vùng trên tất cả các GPU, thay vì được sao chép, và được tái tạo bằng cách sử dụng các giao tiếp tập thể dựa trên gather theo thời gian thực trong quá trình huấn luyện. Điều này cho phép ZeRO tận dụng hiệu quả bộ nhớ GPU tổng hợp trên các máy, với chi phí giao tiếp tối thiểu so với huấn luyện song song dữ liệu tiêu chuẩn (2M so với 3M cho kích thước mô hình M) [29], trong khi vẫn đạt được khả năng mở rộng thông lượng tuyệt vời [30].

1.1 Hạn chế của ZeRO
Tính dễ sử dụng của ZeRO kết hợp với khả năng mở rộng hiệu quả trên hàng trăm đến hàng nghìn GPU, đã dẫn đến việc áp dụng rộng rãi. Tuy nhiên, có hai tình huống quan trọng mà hiệu quả của ZeRO có thể bị hạn chế do chi phí giao tiếp: i) các cụm có băng thông thấp, và ii) ở kích thước batch rất nhỏ trên mỗi GPU.

--- TRANG 2 ---
ABC, 2023, USA Guanhua Wang*, Heyang Qin*, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari and Olatunji Ruwase, Feng Yan1, Lei Yang2, Yuxiong He

Một mặt, các cụm có băng thông thấp là phổ biến trong phần lớn các môi trường điện toán đám mây. Mặc dù các nút hiệu năng cao như hộp DGX [10,11] được trang bị NVLink [25] và NVSwitch [26] băng thông cao làm kết nối liên nút, các liên kết xuyên nút thường ít hơn 100Gbps ethernet khiến nó trở thành nút thắt cổ chai giao tiếp. Như được thể hiện trong Hình 1(a), thông lượng trên mỗi GPU trên các cụm băng thông thấp chỉ bằng một nửa so với các cụm băng thông cao.

Mặt khác, ngay cả trên các cụm băng thông cao, khi chạy trên hàng nghìn GPU, kích thước batch trên mỗi GPU bị hạn chế bởi kích thước batch toàn cục tối đa có thể được sử dụng trong quá trình huấn luyện mà không hy sinh hiệu quả hội tụ [2,17,39]. Nói cách khác, khi kích thước batch toàn cục không thể tăng vô hạn mà không làm chậm hội tụ mô hình, huấn luyện trên hàng nghìn GPU buộc kích thước batch trên mỗi GPU phải rất nhỏ, điều này làm giảm tỷ lệ tính toán-giao tiếp và do đó tạo ra nút thắt cổ chai giao tiếp. Như được thể hiện trong Hình 1(b), thông lượng trên mỗi GPU bị ảnh hưởng nặng nề bởi kích thước batch nhỏ trên mỗi GPU, đây là kết quả của nút thắt cổ chai giao tiếp.

Tuy nhiên, ít nỗ lực đã được thực hiện để tối ưu hóa hiệu quả giao tiếp đầu-cuối cho ZeRO. Có nhiều công trình trước đó về việc giảm chi phí giao tiếp trong huấn luyện mô hình phân tán, như 1-bit LAMB [18], 1-bit Adam [35] và các kỹ thuật nén bù lỗi khác cho việc tính trung bình gradient [1,12,31,34]. Tuy nhiên, không có kỹ thuật nào trong số chúng có thể hoạt động với ZeRO vì tất cả đều giả định sự sao chép trạng thái mô hình, trong khi các trạng thái mô hình được phân vùng trong ZeRO. Chúng tôi bắt đầu từ đầu và cung cấp một hệ thống đầu-cuối để giảm tất cả chi phí giao tiếp trong huấn luyện ZeRO.

1.2 ZeRO++
Trong bài báo này, chúng tôi trình bày một hệ thống mới gồm các tối ưu hóa giao tiếp được gọi chung là ZeRO++ cung cấp việc giảm khối lượng giao tiếp đáng kể cho ZeRO. Dưới đây chúng tôi thảo luận về các chi phí giao tiếp chính trong ZeRO, tiếp theo là ba tối ưu hóa giao tiếp khác nhau trong ZeRO++ để giải quyết chúng.

Giả sử kích thước mô hình là M. Trong quá trình truyền tiến, ZeRO [29] thực hiện một thao tác all-gather để thu thập tất cả các tham số (M) cần thiết để huấn luyện cho tất cả các lớp mô hình. Trong quá trình truyền ngược, ZeRO thu thập lại các tham số (M) với all-gather trước, sau đó mỗi GPU có thể tính toán gradient cục bộ. Sau đó, ZeRO thực hiện hàm reduce-scatter để tổng hợp và phân phối lại gradient (M) trên các bộ gia tốc. Tổng cộng, ZeRO có tổng khối lượng giao tiếp 3M, phân bổ đều trên 2 all-gather và 1 reduce-scatter.

Để giảm các chi phí giao tiếp này, ZeRO++ có ba bộ tối ưu hóa giao tiếp, nhắm vào từng ba giao tiếp tập thể được đề cập ở trên:

Giao tiếp trọng số lượng tử hóa cho ZeRO (qwZ) Đầu tiên, để giảm khối lượng giao tiếp tham số trong quá trình all-gather truyền tiến, chúng tôi áp dụng lượng tử hóa trên trọng số để thu nhỏ mỗi tham số mô hình từ kiểu dữ liệu FP16 (2 bytes) xuống INT8 (1 byte) trước khi giao tiếp, do đó giảm khối lượng giao tiếp một nửa. Tuy nhiên, việc thực hiện lượng tử hóa trên trọng số một cách ngây thơ có thể mất độ chính xác huấn luyện mô hình. Để bảo toàn độ chính xác huấn luyện mô hình tốt, chúng tôi áp dụng lượng tử hóa dựa trên khối [8], điều này thực hiện lượng tử hóa độc lập trên từng tập con của các tham số mô hình. Không có triển khai hiện có nào cho lượng tử hóa dựa trên khối hiệu năng cao. Do đó, chúng tôi triển khai các kernel CUDA lượng tử hóa tối ưu cao từ đầu.

Phân vùng trọng số phân cấp cho ZeRO (hpZ) Thứ hai, để giảm chi phí giao tiếp của all-gather trên trọng số trong quá trình truyền ngược, chúng tôi đánh đổi bộ nhớ GPU lấy giao tiếp. Cụ thể hơn, thay vì phân tán toàn bộ trọng số mô hình trên tất cả các máy, chúng tôi duy trì một bản sao mô hình đầy đủ trong mỗi máy. Với chi phí bộ nhớ cao hơn, điều này cho phép chúng tôi thay thế all-gather xuyên máy đắt đỏ trên trọng số bằng all-gather nội máy, nhanh hơn đáng kể do băng thông giao tiếp nội máy cao hơn nhiều.

Giao tiếp gradient lượng tử hóa cho ZeRO (qgZ) Thứ ba, việc giảm chi phí giao tiếp của gradient bằng reduce-scatter thậm chí còn thách thức hơn. Việc áp dụng trực tiếp lượng tử hóa để giảm khối lượng giao tiếp là không khả thi. Vấn đề chính là, ngay cả khi tích hợp lượng tử hóa dựa trên khối vào thao tác reduce-scatter, nó vẫn sẽ làm tổn hại đáng kể đến độ chính xác huấn luyện mô hình. Lý do chính đằng sau là lượng tử hóa sẽ giảm độ chính xác giá trị. Và việc giảm trên các giá trị độ chính xác thấp sẽ tích tụ và khuếch đại lỗi. Do đó, chúng tôi đề xuất một mô hình giao tiếp gradient mới và hiệu quả hơn nhiều như một sự thay thế chung cho giao tiếp tập thể reduce-scatter, nơi các gradient được nén bằng cách sử dụng lượng tử hóa INT4 dựa trên khối trong quá trình giao tiếp để giảm khối lượng giao tiếp, nhưng độ chính xác đầy đủ được khôi phục trước toán tử giảm để bảo toàn độ chính xác huấn luyện. Chúng tôi gọi đây là qgZ, và được thiết kế để i) khắc phục mất mát độ chính xác đáng kể sẽ xảy ra từ việc giảm độ chính xác thấp nếu chúng tôi chỉ đơn giản triển khai reduce-scatter trong INT4/INT8, và ii) tránh suy giảm độ chính xác và chi phí độ trễ đáng kể của một chuỗi dài các bước lượng tử hóa và giải lượng tử hóa cần thiết bởi một reduce-scatter dựa trên vòng [23] hoặc cây [5,37] (ví dụ, bên trái của Hình 5), ngay cả khi chúng tôi thực hiện việc giảm ở độ chính xác đầy đủ. Hơn nữa, qgZ tận dụng bản chất phân cấp của các cụm GPU hiện đại, nơi băng thông nội nút cao hơn đáng kể so với liên nút, để giảm gradient trong một nút trước khi thực hiện giảm xuyên nút để giảm thiểu khối lượng giao tiếp liên nút, dẫn đến giảm khối lượng giao tiếp 2/4x (INT8/4) so với reduce-scatter FP16. Chúng tôi tiếp tục giảm độ trễ đầu-cuối của qgZ bằng cách đường ống giao tiếp nội nút và liên nút và thực hiện kết hợp kernel CUDA.

Giảm khối lượng giao tiếp Bằng cách tích hợp cả ba thành phần trên, chúng tôi giảm khối lượng giao tiếp xuyên nút từ 3M xuống 0.75M. Cụ thể hơn, đối với thao tác all-gather truyền tiến trên trọng số mô hình, bằng cách áp dụng lượng tử hóa INT8, chúng tôi giảm kích thước giao tiếp từ M xuống 0.5M. Trong quá trình all-gather truyền ngược trên trọng số, với bản sao thứ hai của các tham số mô hình, chúng tôi giảm kích thước giao tiếp từ M xuống 0. Bằng cách thay thế reduce-scatter fp16 truyền ngược trên gradient bằng reduce-scatter INT4 dựa trên all-to-all mới của chúng tôi, chúng tôi giảm giao tiếp xuyên nút từ M xuống 0.25M. Do đó, tổng cộng, chúng tôi giảm giao tiếp 3M xuống 0.75M.

Đánh giá Chúng tôi triển khai ZeRO++ và thực hiện đánh giá mở rộng thể hiện ba kết quả chính: i) khả năng mở rộng của các mô hình giống GPT-3 trên lên đến 384 GPU đạt được hơn 45% thông lượng đỉnh duy trì, ii) tăng tốc nhất quán lên đến 2.4x so với baseline ZeRO [29] trên các mô hình từ 10-138B tham số, và iii) so sánh với baseline trong cụm băng thông cao hơn 4x, ZeRO++

--- TRANG 3 ---
ZeRO++: Extremely Efficient Collective Communication for Giant Model Training ABC, 2023, USA

đạt được thông lượng tương tự trong cài đặt băng thông thấp. Ngoài ra, chúng tôi cho thấy tác động của từng ba tối ưu hóa trong ZeRO++ và cách chúng kết hợp với nhau. Hơn nữa, chúng tôi cũng cho thấy tác động của các triển khai kernel tối ưu của chúng tôi đối với thông lượng hệ thống đầu-cuối. Cuối cùng, chúng tôi tiến hành đánh giá hội tụ cho thấy ZeRO++ có tác động không đáng kể đến hội tụ mô hình và duy trì độ chính xác huấn luyện mô hình tương tự như baseline ZeRO.

Các đóng góp chính của bài báo này như sau:
• Trọng số lượng tử hóa khối (qwZ) giảm khối lượng giao tiếp của all-gather trọng số 50%.
• Phân vùng phân cấp trọng số mô hình (hpZ) hoàn toàn loại bỏ giao tiếp all-gather liên nút trong truyền ngược.
• Giao tiếp tập thể giảm gradient lượng tử hóa all-to-all mới (qgZ) giảm giao tiếp gradient 75% so với reduce-scatter.
• Tích hợp tối ưu từng kỹ thuật trên vào triển khai ZeRO hiện có, cho phép chồng chéo giao tiếp và tính toán, và tận dụng các kernel CUDA hiệu năng cao tùy chỉnh cho lượng tử hóa, giải lượng tử hóa, cũng như kết hợp toán tử (phần 4). Triển khai của chúng tôi chuyển đổi việc giảm khối lượng giao tiếp 4x của ZeRO++ thành cải thiện thông lượng thực tế.
• Các thí nghiệm mở rộng cho thấy i) hơn 45% thông lượng đỉnh duy trì ngay cả ở kích thước batch nhỏ, ii) cải thiện hệ thống đầu-cuối lên đến 2.4x so với ZeRO, và iii) đạt được thông lượng tương tự trong cụm băng thông thấp so với baseline trong cụm băng thông cao. Ngoài ra, chúng tôi trình bày phân tích chi tiết hiệu năng và phân tích các thành phần khác nhau của ZeRO++. Huấn luyện đầu-cuối của chúng tôi cho thấy ZeRO++ không ảnh hưởng đến hội tụ mô hình.
• ZeRO++ là mã nguồn mở và được phát hành như một phần của https://github.com/microsoft/DeepSpeed

2 BỐI CẢNH VÀ CÔNG TRÌNH LIÊN QUAN

2.1 Song song dữ liệu, mô hình và 3D

Song song dữ liệu (DP), song song pipeline (PP), và song song tensor (TP) là ba hình thức song song được sử dụng để huấn luyện các mô hình lớn trên các cụm multi-GPU. [6,15,20,22] DP thường được sử dụng khi kích thước mô hình vừa với bộ nhớ GPU đơn lẻ. Trong DP, mỗi GPU giữ một bản sao đầy đủ của trọng số mô hình và huấn luyện trên dữ liệu đầu vào riêng biệt. MP là trực giao với DP, và thường được sử dụng trong các trường hợp kích thước mô hình không thể vừa với bộ nhớ của một GPU. Thay vì chia dữ liệu đầu vào, song song mô hình phân vùng một mô hình đầy đủ thành các mảnh và gán mỗi mảnh mô hình lên một GPU. Có chủ yếu hai cách tiếp cận cho song song mô hình: i) song song pipeline (PP) và ii) song song tensor (TP). PP [14,15,20] chia mô hình theo chiều dọc, tạo ra các giai đoạn tuần tự bao gồm một tập con liên tục của các lớp. Trong khi có sự phụ thuộc tuần tự giữa các giai đoạn cho một micro-batch đầu vào, các giai đoạn có thể được thực hiện song song trên các micro-batch. Ngược lại, TP [22] chia mỗi lớp trên nhiều GPU, nơi mỗi GPU làm việc trên một phần khác nhau của lớp cho cùng một đầu vào.

Song song 3D [33,36] đề cập đến sự kết hợp của DP, PP, và TP, và có khả năng đạt được thông lượng và khả năng mở rộng tuyệt vời, và đã được sử dụng để huấn luyện một loạt các mô hình ngôn ngữ lớn [4,19,22,28]. Mặc dù rất hiệu quả, song song 3D bị hạn chế nghiêm trọng bởi thực tế là nó yêu cầu viết lại hoàn toàn mô hình và pipeline huấn luyện để làm cho chúng tương thích với song song 3D [33].

Algorithm 1: Thuật toán ZeRO
Input: model, worldSize
Output: model
1 while model not converged do
2   all_gather_Parameters(worldSize);
3   model.forward();
4   partition(worldSize);
5   all_gather_Parameters(worldSize);
6   model.backward();
7   partition(worldSize);
8   reduce_scatter_Gradients(worldSize);
9   optimizer.step();
10 end while
11 Return: model

2.2 ZeRO Optimizer

ZeRO là một giải pháp tối ưu bộ nhớ cho huấn luyện song song dữ liệu. ZeRO phân vùng và phân phối tất cả các trạng thái mô hình (tức là, tham số, gradient, trạng thái optimizer) giữa các GPU đang sử dụng và thu thập lại các trạng thái mô hình chỉ khi lớp cần được tính toán. Có ba giai đoạn khác nhau để sử dụng ZeRO để tối ưu hóa việc sử dụng bộ nhớ trên thiết bị. Trong ZeRO stage 1 (ZeRO-1), chỉ các trạng thái optimizer được chia và phân tán trên tất cả các GPU đang sử dụng. ZeRO stage 2 (ZeRO-2) phân vùng cả trạng thái optimizer và gradient, trong khi ZeRO stage 3 (ZeRO-3) chia tất cả ba thành phần của trạng thái mô hình là tham số, gradient, và trạng thái optimizer.

ZeRO-3 là giải pháp tiết kiệm bộ nhớ nhất cho huấn luyện mô hình ở quy mô lớn, nhưng với chi phí của nhiều giao tiếp tập thể hơn. Thuật toán 1 minh họa mã giả cấp cao cho ZeRO-3. Trong quá trình huấn luyện mô hình, ZeRO-3 lập lịch lười biếng việc tìm nạp tham số cho đến khi tính toán cần xảy ra trên một lớp cụ thể. Trước khi truyền tiến, ZeRO khởi chạy một all-gather để thu thập trọng số mô hình đầy đủ và sau đó tính toán quá trình truyền tiến (dòng 2-3) của Thuật toán 1. Sau đó ZeRO làm rỗng bộ đệm trọng số all-gather sau khi tính toán truyền tiến hoàn thành (dòng 4). Trong quá trình truyền ngược, ZeRO thu thập lại tất cả trọng số mô hình qua một all-gather thứ hai (dòng 5) để tính toán gradient (dòng 6). Một khi gradient được tính toán trên mỗi GPU, ZeRO làm rỗng bộ đệm trọng số lại (dòng 7) và thực hiện một thao tác reduce-scatter để thực hiện tính trung bình gradient và phân phối lại (dòng 8). Trạng thái mô hình và tham số được cập nhật trong bước optimizer (dòng 9). Tóm lại, để giảm thiểu dấu chân bộ nhớ trên thiết bị bằng ZeRO-3, ba thao tác giao tiếp tập thể được phát hành ở mỗi lần lặp huấn luyện, bao gồm 2 all-gather trên trọng số và 1 reduce-scatter trên gradient.

2.3 Kỹ thuật giảm giao tiếp

Lượng tử hóa: Lượng tử hóa thường được sử dụng để giảm dấu chân bộ nhớ, và khối lượng di chuyển dữ liệu bằng cách sử dụng độ chính xác thấp để biểu diễn dữ liệu [7,8]. Tuy nhiên, việc mất thông tin từ việc biểu diễn dữ liệu độ chính xác cao với độ chính xác thấp hơn thường đi kèm với suy giảm độ chính xác. Nhiều công trình liên quan tập trung vào việc cải thiện

--- TRANG 4 ---
ABC, 2023, USA Guanhua Wang*, Heyang Qin*, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari and Olatunji Ruwase, Feng Yan1, Lei Yang2, Yuxiong He

độ chính xác lượng tử hóa. Thách thức cơ bản của độ chính xác lượng tử hóa nằm ở sự khác biệt lớn về phạm vi số và độ chi tiết giữa dữ liệu độ chính xác cao và độ chính xác thấp (Ví dụ: FP32/16 so với INT8). Một số công trình liên quan [41] đề xuất lọc các giá trị ngoại lai trong dữ liệu để giảm thiểu khoảng cách trong phạm vi số. Tuy nhiên, độ chính xác của chúng phụ thuộc vào chất lượng lọc ngoại lai và nó mang lại chi phí lọc bổ sung. Dettmers et al. [8] đề xuất sử dụng lượng tử hóa dựa trên khối trên các trạng thái optimizer để cải thiện độ chính xác lượng tử hóa nhưng nó yêu cầu thay đổi cấu trúc mô hình do đó hạn chế khả năng sử dụng.

Nén gradient: Bắt đầu từ 1-bit SGD của nén bù lỗi [31], nén gradient đã được đẩy đến hướng cực đoan chỉ sử dụng một bit duy nhất. Để đối phó với các optimizer dựa trên gradient phi tuyến như Adam hoặc Lamb, các thuật toán lượng tử hóa 1-bit như 1-bit Adam [35] và 1-bit Lamb [18] được đề xuất, đạt được giao tiếp gradient cực kỳ hiệu quả trong huấn luyện phân tán. Tuy nhiên, 1-bit Adam/LAMB không thể áp dụng trực tiếp cho ZeRO-3. Lý do chính là 1-bit Adam/Lamb giả định mỗi GPU có cái nhìn đầy đủ về các trạng thái optimizer (OS) cho mô hình, nhưng ZeRO-3 chia nó trên tất cả các GPU đang sử dụng. Do đó, việc áp dụng trực tiếp các kỹ thuật nén gradient hiện có cho ZeRO-3 là không khả thi và chúng tôi cần thiết kế riêng cho mình.

Giảm giao tiếp ZeRO: Để giảm giao tiếp xuyên nút đắt đỏ, tối ưu hóa gần đây trên ZeRO-3, như MiCS [40], đánh đổi bộ nhớ trên thiết bị lấy giao tiếp. Trong MiCS, cụm GPU được chia thành các nhóm con, và các trạng thái mô hình được phân vùng trong một nhóm con nhưng được sao chép trên các nhóm con. Bằng cách giữ kích thước nhóm con nhỏ, MiCS có thể tận dụng kết nối liên nút nội bộ băng thông cao, hoặc sử dụng giao tiếp phân cấp để giảm khối lượng giao tiếp. hpZ trong ZeRO++ áp dụng cách tiếp cận tương tự đánh đổi bộ nhớ lấy ít giao tiếp hơn. Sự khác biệt chính là hpZ chỉ thực hiện phân vùng thứ hai trên trọng số, trong khi giữ tất cả các trạng thái mô hình khác được phân vùng trên tất cả các GPU. Điều này cho phép hpZ đạt được giảm giao tiếp đáng kể mà không có chi phí bộ nhớ khổng lồ của MiCS.

3 THIẾT KẾ

Trong phần này, chúng tôi trình bày chi tiết về thiết kế của ba tối ưu hóa chính trong ZeRO++ được giới thiệu trong Phần 1 để giảm chi phí giao tiếp của ZeRO: i) Giao tiếp trọng số lượng tử hóa cho ZeRO (qwZ), ii) Phân vùng phân cấp cho ZeRO (hpZ), và iii) Giao tiếp gradient lượng tử hóa cho ZeRO (qgZ). Sau đó, chúng tôi thảo luận về tác động đầu-cuối của các tối ưu hóa này để giảm tổng khối lượng giao tiếp của ZeRO.

3.1 Giao tiếp trọng số lượng tử hóa cho ZeRO (qwZ)

Như đã thảo luận trong Phần 2.2, ZeRO phân vùng trọng số mô hình trên tất cả các thứ hạng (tức là, GPU) và tìm nạp trọng số FP16 theo từng lớp ngay trước khi chúng cần thiết trong tính toán qua all-gather cho truyền tiến và truyền ngược của mỗi lần lặp huấn luyện. Để giảm chi phí giao tiếp của all-gather truyền tiến trên trọng số, qwZ, lượng tử hóa trọng số FP16 thành INT8 ngay trong quá trình all-gather, và giải lượng tử hóa chúng trở lại FP16 ở phía người nhận, và sau đó tiến hành tính toán lớp.

Hình 2: Minh họa và ví dụ về lượng tử hóa dựa trên khối so với baseline

Hình 3: hpZ loại bỏ lưu lượng xuyên nút trong all-gather truyền ngược bằng cách giữ các phân vùng trọng số thứ hai trong bộ nhớ trên thiết bị.

Trong khi điều này giảm khối lượng giao tiếp của all-gather 2x, việc làm như vậy một cách ngây thơ dẫn đến hai vấn đề chính: i) việc giảm độ chính xác dẫn đến suy giảm độ chính xác đáng kể trong quá trình huấn luyện như đã thảo luận trong 2.3, và ii) chi phí lượng tử hóa và giải lượng tử hóa làm mất tác dụng của bất kỳ lợi ích thông lượng nào từ việc giảm khối lượng giao tiếp. Chúng tôi thảo luận về triển khai tối ưu của qwZ để giảm thiểu chi phí lượng tử hóa và giải lượng tử hóa trong Phần 4. Ở đây, chúng tôi chủ yếu tập trung vào các lựa chọn thiết kế để giảm thiểu suy giảm độ chính xác.

qwZ sử dụng lượng tử hóa dựa trên khối để cải thiện độ chính xác lượng tử hóa. Như được minh họa trong Hình 2, mỗi tensor trọng số được chia thành các khối nhỏ hơn, và được chuyển đổi thành INT8 bằng lượng tử hóa đối xứng, sử dụng hệ số tỷ lệ lượng tử hóa độc lập. Bằng cách giữ độ chi tiết lượng tử hóa nhỏ, chúng tôi giảm thiểu đáng kể khoảng cách về phạm vi số và độ chi tiết.

Chúng tôi cho thấy một ví dụ về lỗi lượng tử hóa khi thực hiện lượng tử hóa dựa trên khối so với baseline lượng tử hóa không khối trong Hình 2(a). Hình 2(b) cho thấy một nghiên cứu trường hợp về lượng tử hóa trọng số trên mô hình BERT, nơi lượng tử hóa dựa trên khối giảm lỗi lượng tử hóa 3x. Các đánh giá hội tụ chi tiết hơn được thể hiện trong Mục 5.

--- TRANG 5 ---
ZeRO++: Extremely Efficient Collective Communication for Giant Model Training ABC, 2023, USA

3.2 Phân vùng phân cấp cho ZeRO (hpZ)

ZeRO-3 phân vùng tất cả các trạng thái mô hình trên tất cả các thứ hạng của nó, dẫn đến các giao tiếp tập thể trải rộng trên tất cả các GPU. Với hpZ, chúng tôi nhận thấy rằng có thể có phân vùng khác nhau cho các trạng thái mô hình khác nhau, hạn chế các giao tiếp tập thể cho một tập con của các GPU. Cho rằng trên các cụm GPU hiện đại, băng thông giao tiếp nội nút cao hơn đáng kể so với băng thông giao tiếp liên nút, điều này mang lại cơ hội để giảm giao tiếp liên nút.

Cụ thể hơn, trong hpZ, chúng tôi loại bỏ all-gather liên nút trong quá trình truyền ngược bằng cách giữ phân vùng trọng số FP16 thứ hai trong mỗi nút. Chúng tôi thực hiện điều này bằng cách tạo ra một chiến lược phân vùng phân cấp gồm hai phân vùng: đầu tiên, tất cả các trạng thái mô hình được phân vùng toàn cục trên tất cả các thiết bị như trong ZeRO-3, mà chúng tôi gọi là phân vùng chính. Thứ hai, một bản sao thứ hai của tham số FP16 được phân vùng ở cấp độ con-toàn cục (ví dụ, nút tính toán, xem hình 3), mà chúng tôi gọi là phân vùng thứ hai. Bản sao thứ hai này của tham số FP16 được sao chép trên nhiều phân vùng thứ hai.

Xem xét một cụm 64 nút, mỗi nút có 8 GPU. Trọng số mô hình được phân vùng trong hai giai đoạn: i) trên tất cả 512 GPU mà chúng tôi gọi là phân vùng chính, và ii) cùng các trọng số cũng được phân vùng trong một nút tính toán trên 8 GPU, mà chúng tôi gọi là phân vùng thứ hai. Trong ví dụ này, đối với phân vùng thứ hai, mỗi nút tính toán trong cụm giữ một bản sao đầy đủ của trọng số FP16 được phân vùng giữa 8 GPU trong nút, và có tổng cộng 64 bản sao như vậy.

3.2.1 Một lần lặp huấn luyện với hpZ. Trong quá trình truyền tiến của một lần lặp huấn luyện, chúng tôi all-gather trọng số dựa trên phân vùng chính trên tất cả các GPU. Tuy nhiên, một khi trọng số được tiêu thụ trong quá trình truyền tiến, chúng được phân vùng dựa trên phân vùng thứ hai. Cho rằng tính nhất quán thời gian của các tham số mô hình giữa quá trình truyền tiến và truyền ngược, khi các trọng số cần thiết lại trong quá trình truyền ngược, chúng tôi all-gather trọng số dựa trên nhóm thứ hai này. Lưu ý rằng khi phân vùng thứ hai được đặt thành một nút tính toán, điều này tránh bất kỳ giao tiếp liên nút nào cho all-gather này. Cuối cùng, ở cuối lần lặp, trong bước optimizer, tất cả các trạng thái mô hình, cũng như bản sao chính của tham số fp16 được cập nhật dựa trên phân vùng chính. hpZ thực hiện hai thay đổi đối với mã giả ZeRO baseline trong Thuật toán 1: i) trong dòng 4, phân vùng tham số dựa trên kích thước nhóm thứ hai, ii) all-gather tham số đi trước quá trình truyền ngược trong dòng 5 cũng dựa trên kích thước nhóm thứ hai.

Thiết kế hpZ của chúng tôi linh hoạt để hỗ trợ bất kỳ kích thước nhóm thứ hai nào. Kích thước nhóm kiểm soát có bao nhiêu thứ hạng (tức là, GPU) trong phân vùng thứ hai. Nó cũng là một biện pháp đánh đổi bộ nhớ-giao tiếp của hpZ. Nói đơn giản, theo mặc định, phân vùng thứ hai hpZ dựa trên nút (nhớ lại băng thông nội nút là nhiều yếu tố của băng thông liên nút cho các cấu hình phần cứng hiện tại và tương lai) nhưng có thể được mở rộng để hỗ trợ nhiều nút tính toán khi cần thiết.

3.2.2 Phân tích sử dụng bộ nhớ. Theo thiết kế, hpZ đánh đổi bộ nhớ lấy hiệu quả giao tiếp. Điều quan trọng là phân tích sự đánh đổi này. Nhớ lại rằng DNN song song dữ liệu tiêu chuẩn (DP) sao chép tham số mô hình trên các thứ hạng song song dữ liệu, ZeRO-3 mặt khác phân vùng tham số trên các thứ hạng song song dữ liệu. Một cách tiếp cận trung gian là tham số mô hình được phân vùng trên một tập con của các thiết bị miễn là tham số mô hình vừa.

Hình 4: Phân tích tiêu thụ bộ nhớ trên mỗi thiết bị của song song dữ liệu tiêu chuẩn (DP), ZeRO stage 3 (ZeRO-3) và phân vùng phân cấp được đề xuất của tham số ZeRO (hpZ). K biểu thị hệ số nhân bộ nhớ của trạng thái optimizer, M đại diện cho số lượng tham số có thể huấn luyện, P là kích thước nhóm song song dữ liệu hoặc kích thước thế giới, và α là số lượng nhóm thứ hai hoặc tỷ lệ kích thước thế giới với số lượng thứ hạng trong nhóm thứ hai. Một ví dụ tình huống thế giới thực điển hình được cung cấp trong cột cuối cùng. Chúng tôi giả định kích thước mô hình 100B được huấn luyện trên cụm DGX GPU V100 1024 (64 nút tính toán, 16 GPU mỗi nút).

Hình 4 cung cấp ước tính sử dụng bộ nhớ cụ thể của một mô hình ngôn ngữ lớn điển hình có kích thước 100B tham số, với kích thước nhóm chính là 1024 GPU và kích thước nhóm thứ hai là 16 GPU (ví dụ, nút DGX-2 V100). Như được thể hiện trong Hình 4, với phương pháp đề xuất của chúng tôi, hpZ tiêu thụ 8.9x bộ nhớ hơn so với ZeRO-3, cách tiếp cận của chúng tôi vẫn có yêu cầu bộ nhớ ít hơn 114x so với DP tiêu chuẩn. Việc tăng bộ nhớ sử dụng nhỏ này được bù đắp bởi lịch trình giao tiếp nội nút hiệu quả. Bằng cách loại bỏ hoặc giảm giao tiếp liên nút cho quá trình truyền ngược, hpZ giảm giao tiếp đầu-cuối của ZeRO 1.5x, trong khi vẫn hỗ trợ huấn luyện mô hình với hàng trăm tỷ tham số.

3.3 Giao tiếp gradient lượng tử hóa cho ZeRO (qgZ)

Trong phần này, chúng tôi đề xuất một thuật toán reduce-scatter lượng tử hóa mới được gọi là qgZ dựa trên các giao tiếp tập thể all-to-all cho phép giảm khối lượng giao tiếp 4x của gradient reduce-scatter bằng cách thay thế FP16 bằng dữ liệu lượng tử hóa INT4, trong khi khắc phục các thách thức mất độ chính xác được mô tả trong Phần 1, cũng như nhiều thách thức hệ thống mà chúng tôi sẽ nêu trong phần này.

qgZ tận dụng các giao tiếp tập thể all-to-all để triển khai reduce-scatter lượng tử hóa bao gồm ba thành phần chính: 1) triển khai reduce-scatter gradient lượng tử hóa dựa trên all-to-all, 2) giảm khối lượng giao tiếp với các giao tiếp tập thể phân cấp, 3) sắp xếp lại lát tensor cho vị trí gradient chính xác. Chúng tôi nói về từng cái một từng bước.

3.3.1 Triển khai dựa trên all-to-all. Một cách tiếp cận ngây thơ hướng tới reduce-scatter lượng tử hóa, trong khi tránh mất độ chính xác do giảm là áp dụng lượng tử hóa và giải lượng tử hóa cho reduce-scatter dựa trên vòng trực tiếp như được thể hiện ở bên trái của Hình 5. Chúng tôi có thể tiêm lượng tử hóa và giải lượng tử hóa trên mỗi GPU. Một khi GPU

--- TRANG 6 ---
ABC, 2023, USA Guanhua Wang*, Heyang Qin*, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari and Olatunji Ruwase, Feng Yan1, Lei Yang2, Yuxiong He

Machine 1
G2 G3Machine 0
G0 G1Machine 1
G2 G3

NCCL Ring-based reduce-scatter (ZeRO-3)
# of sequential Q+D == # of GPUs1-hop all-to-all (qgZ in ZeRO++)
# of sequential Q+D == 1Quantization Dequantization + Reduction 1234Gradient slices

1234Q D
Q D
1234Q D
1234Q D
1234Q DMachine 0
G0 G1

1234 1234 12341234
Q
All_to_All
D1234 1234 12341234
1111 2222 33334444
1 2 3 4

Hình 5: So sánh giữa reduce-scatter dựa trên vòng ZeRO-3 và qgZ 1-hop all-to-all.

Cross-node gradient comm. volume
(ZeRO-3)Cross-node gradient comm. volume
(qgZ in ZeRO++)Machine 0
...G1 G_N

Machine 1
...G1 G_NM
Reduce-scatter MMachine 0
...G1 G_N

Machine 1
...G1 G_NM/Z
1-hop all-to-all N*M/ZM/Z M/ZGPUs per node = N Model size = M          Quantization compression ratio = Z

Hình 6: So sánh khối lượng giao tiếp giữa reduce-scatter ZeRO-3 và qgZ 1-hop all-to-all.

nhận gradient từ người tiền nhiệm, chúng tôi giải lượng tử hóa nó để khôi phục độ chính xác đầy đủ và thực hiện giảm cục bộ. Tiếp theo chúng tôi có thể lượng tử hóa đầu ra giảm cục bộ và chuyển dữ liệu lượng tử hóa cho người kế nhiệm. Để hoàn thành toàn bộ reduce-scatter, số lượng kernel lượng tử hóa và giải lượng tử hóa tuần tự bằng số lượng GPU (tức là, n) đang sử dụng.

Do đó, việc áp dụng lượng tử hóa và giải lượng tử hóa trên giao tiếp tập thể reduce-scatter dựa trên vòng hiện có sẽ dẫn đến độ trễ giao tiếp cao và độ chính xác giá trị thấp do nhiều bước lượng tử hóa và giải lượng tử hóa tuần tự. Mặc dù giao tiếp tập thể dựa trên cây gần đây như Blink [38] có thể giảm số lượng kernel tuần tự từ n xuống log(n), vấn đề độ trễ dài và độ chính xác thấp không được giải quyết hoàn toàn.

Để khắc phục điều này, chúng tôi hoàn toàn từ bỏ cách tiếp cận reduce-scatter dựa trên vòng hiện có và tích hợp giao tiếp tập thể all-to-all 1-hop cho giao tiếp gradient của chúng tôi. Như được thể hiện ở bên phải của Hình 5, chúng tôi trước tiên áp dụng lượng tử hóa trên một tensor đã cho, sau đó chúng tôi thực hiện giao tiếp all-to-all giữa tất cả các GPU. Sau all-to-all, chúng tôi áp dụng một giải lượng tử hóa khác để khôi phục độ chính xác dữ liệu và sau đó giảm trên các giá trị độ chính xác cao để có được đầu ra giảm gradient cuối cùng. Bằng cách thay thế giải pháp dựa trên vòng bằng giao tiếp tập thể all-to-all của chúng tôi, chúng tôi giảm số lượng kernel lượng tử hóa+giải lượng tử hóa tuần tự từ số lượng GPU xuống 1. Do đó, chúng tôi giải quyết các vấn đề độ trễ dài và độ chính xác thấp khi áp dụng lượng tử hóa trong reduce-scatter cho các tình huống siêu tính toán như hộp DGX được kết nối trong cấu trúc liên kết fat-tree.

Step 1: Intra-node all-to-all Step 2: Inter-node all-to-allGPUs per node = N Model size = M          Quantization compression ratio = Z

Machine 0
...G1 G_N

Machine 1
...G1 G_NMachine 0
...G1 G_N

Machine 1
...G1 G_N

Each GPU comm. volume reduction :
M/Z => M/(Z*N)Cross-node traffic reduction :
N*M/Z => M/ZM/(Z*N) M/(Z*N)M/(Z*N)

Hình 7: qgZ áp dụng all-to-all phân cấp để giảm lưu lượng xuyên nút.

3.3.2 Giảm khối lượng giao tiếp liên nút. Mặc dù việc thay thế reduce-scatter bằng all-to-all đạt được lượng tử hóa và giải lượng tử hóa một lần, nó giới thiệu một vấn đề mới; khối lượng giao tiếp liên nút tăng thay vì giảm mặc dù lượng tử hóa dữ liệu. Chúng tôi trình bày chi tiết về điều này trong Hình 6.

Ở đây chúng tôi giả định kích thước mô hình là M, GPU mỗi nút là N, tỷ lệ nén gradient là Z. Reduce-scatter, giảm dữ liệu trong quá trình truyền qua vòng, do đó tổng lượng dữ liệu cho giao tiếp xuyên nút là M. Tuy nhiên, khi sử dụng cách tiếp cận all-to-all 1-hop của chúng tôi, mặc dù dữ liệu được nén trước khi giao tiếp (tức là, M/Z), mỗi GPU cần gửi ra lượng dữ liệu M/Z cho các GPU trên các nút khác. Do đó, mỗi máy sẽ tạo ra lượng dữ liệu giao tiếp xuyên nút N*M/Z, lớn hơn nhiều so với khối lượng giao tiếp reduce-scatter.

Để giải quyết điều này, chúng tôi thực hiện all-to-all phân cấp 2-hop thay vì 1-hop: a) đầu tiên all-to-all nội nút và b) tiếp theo là all-to-all liên nút, được thể hiện như Hình 7. Đầu tiên, với các liên kết băng thông cao giữa các GPU bên trong một máy, chúng tôi thực hiện all-to-all nội nút trên dữ liệu lượng tử hóa, sau đó giải lượng tử hóa dữ liệu và giảm trên dữ liệu đã giải lượng tử hóa. Sau lượng tử hóa nội nút, all-to-all, giải lượng tử hóa và giảm, chúng tôi giảm kích thước dữ liệu trên mỗi GPU từ M/Z xuống M/(Z*N). Sau khi all-to-all nội nút hoàn thành, chúng tôi thực hiện giao tiếp all-to-all liên nút, tương tự như all-to-all 1-hop mà chúng tôi mô tả ở trên. Cho rằng bây giờ mỗi GPU chỉ cần gửi ra dữ liệu M/(Z*N), khối lượng giao tiếp trên mỗi máy bây giờ là M/(Z*N)*N=M/Z. Bằng cách áp dụng giao tiếp all-to-all phân cấp này như cách tiếp cận 2-hop, chúng tôi giải quyết hoàn hảo vấn đề bùng nổ khối lượng giao tiếp trong sơ đồ 1-hop của chúng tôi. Lưu ý rằng mặc dù tổng khối lượng giao tiếp được nhân đôi (một nội nút, cái kia liên nút), giao tiếp nội nút mang lại chi phí không đáng kể do băng thông cao NVLink/NVswitch, và lưu lượng xuyên nút đã được giảm đáng kể, đây là nút thắt cổ chai chính trong giao tiếp gradient.

3.3.3 Sắp xếp lại lát tensor cho vị trí dữ liệu chính xác. Với all-to-all 2-hop, khối lượng giao tiếp liên nút như mong đợi, tuy nhiên, điều này giới thiệu một vấn đề đặt sai gradient. Chúng tôi mô tả vấn đề này bằng cách sử dụng ví dụ 2x2, nơi chúng tôi có 2 máy và mỗi máy có 2 GPU. Như được thể hiện trong Hình 8, vị trí gradient cuối cùng chính xác được thể hiện như các hộp xanh lá cây trong hình, nơi GPU 0 giữ phân vùng gradient cuối cùng 1, GPU 1 giữ phân vùng gradient 2, và cứ thế tiếp tục.

--- TRANG 7 ---
ZeRO++: Extremely Efficient Collective Communication for Giant Model Training ABC, 2023, USA

Mis-placementMachine 1 Machine 0
G0 G1 G3 G2

1234 1234 1234 1234
12 34
34 1212 34
34 12

1 2 3 434 12 34 1234 12 34 12Machine 1 Machine 0
G0 G1 G3 G2

3
41
2 34
12

1 3 2 4

Step 1: intra-node all-to-all Step 2: inter-node all-to-allNotation
xInitial grad. slices xGrad. after intra-comm
xGrad. after inter-comm xCorrect final grad.

Hình 8: Đặt sai phân vùng gradient khi áp dụng all-to-all phân cấp trong qgZ.

Giao tiếp all-to-all 2-bước của chúng tôi hoạt động như sau, đầu tiên chúng tôi chia tất cả gradient trên mỗi GPU thành 4 khối, sau đó thực hiện all-to-all nội nút của chúng tôi. Sau khi all-to-all nội nút kết thúc, GPU0 (tức là, G0) giữ phân vùng gradient tổng hợp một phần 1,2 trong khi G1 giữ phân vùng gradient 3,4. Điều tương tự xảy ra trên G2 và G3. Vì G1 không có phân vùng gradient 2 (được cho là được giữ bởi G1) trong khi G2 không có phân vùng gradient 3, sau all-to-all liên nút, có vấn đề đặt sai gradient trên cả G1 và G2.

Chúng tôi giải quyết điều này bằng cách sắp xếp lại lát tensor. Như được thể hiện trong Hình 9, trước khi all-to-all nội nút bắt đầu, chúng tôi trước tiên hoán đổi thứ tự lát tensor của lát 2 và 3, được thể hiện như mũi tên màu cam. Sau đó sau khi all-to-all nội nút hoàn thành, G1 bây giờ có gradient 2 trong khi G2 có gradient 3. Do đó, sau all-to-all liên nút, tất cả GPU đều có vị trí gradient chính xác. Về mặt toán học, cho X GPU mỗi nút và Y nút tổng cộng, mỗi GPU sẽ giữ X*Y lát gradient ban đầu. Sắp xếp lại lát tensor của chúng tôi hoạt động như sau:

before: [0,1,2,3,4,...Y*X-3,Y*X-2,Y*X-1] (1)
after: [0,X,2*X,...(Y-1)*X,1,X+1,(Y-1)*X+1,...Y*X-1] (2)

Dựa trên Phương trình 1 và 2, chúng tôi có thể ánh xạ mỗi vị trí lát tensor gốc (tức là, Phương trình 1) sang vị trí lát tensor mới (tức là, Phương trình 2) trên mỗi GPU để sửa vấn đề đặt sai gradient cuối cùng.

Tóm lại, bằng cách giải quyết ba thách thức trên từng bước, chúng tôi thiết kế một giao thức giao tiếp và giảm gradient mới, có thể là sự thay thế hiệu quả giao tiếp hơn và tổng quát hơn cho giao tiếp tập thể reduce-scatter. Chúng tôi thảo luận về một số tối ưu hóa và chi tiết triển khai cho cách tiếp cận của chúng tôi trong Mục 4.

Machine 1 Machine 0
G0 G1 G3 G2

1 2 3 4Machine 1 Machine 0
G0 G1 G3 G2

1 2 3 4

Step 1: intra-node all-to-all Step 2: inter-node all-to-all1 234 1 234 1 234 1 234
13 24
24 1313 24
24 131234 1234 1234 1234
24 13 24 1324 13 24 13

2
41
3 24
13

Correct

Hình 9: Sắp xếp lại lát tensor để sửa đặt sai gradient trong qgZ.

Comm. forward backward backward
Volume all-gather all-gather reduce-scatter
ZeRO-3 M M M
ZeRO++ 0.5M 0 0.25M

Bảng 1: So sánh khối lượng giao tiếp giữa ZeRO-3 và ZeRO++.

3.4 Phân tích khối lượng giao tiếp ZeRO++

Bảng 1 minh họa so sánh khối lượng giao tiếp lý thuyết giữa ZeRO-3 và ZeRO++. Chúng tôi giả định kích thước mô hình là M. Như được mô tả trong Phần 2, trong ZeRO-3 có 3 cuộc gọi giao tiếp tập thể: all-gather trên trọng số trong quá trình truyền tiến, sau đó all-gather trên trọng số trong quá trình truyền ngược và cuối cùng là reduce-scatter trên gradient trong quá trình truyền ngược. Và mỗi giao tiếp tập thể giao tiếp khối lượng dữ liệu M. Với ZeRO-3, tổng cộng chúng tôi cần giao tiếp dữ liệu 3M cho mỗi lần lặp huấn luyện. Cho rằng giao tiếp nội nút nhanh với NVLink và NVSwitch, chúng tôi bỏ qua giao tiếp nội nút và chỉ tập trung vào lưu lượng xuyên nút. Đối với all-gather trong quá trình truyền tiến, bằng cách tích hợp giao tiếp trọng số lượng tử hóa của chúng tôi, chúng tôi giảm khối lượng giao tiếp từ M xuống 0.5M. Trong quá trình all-gather trong quá trình truyền ngược, bằng cách giữ phân vùng trọng số thứ hai trong mỗi nút, chúng tôi hoàn toàn loại bỏ lưu lượng xuyên nút. Đối với reduce-scatter trong quá trình truyền ngược, bằng cách thay thế reduce-scatter bằng giao thức giao tiếp gradient lượng tử hóa mới của chúng tôi, chúng tôi giảm lưu lượng xuyên nút từ M xuống 0.25M. Do đó, so với ZeRO-3, ZeRO++ giảm khối lượng giao tiếp từ 3M xuống 0.75M cho mỗi lần lặp huấn luyện.

4 TRIỂN KHAI TỐI ƯU

Trong phần này, chúng tôi thảo luận về hai tối ưu hóa chính cho phép ZeRO++ hoàn toàn nhận ra tiềm năng của việc giảm khối lượng giao tiếp 4x để cải thiện thông lượng mà không bị hạn chế bởi chi phí triển khai: i) chồng chéo các luồng giao tiếp và tính toán khác nhau, khi làm như vậy cho phép sử dụng tài nguyên tốt hơn, và ii) kernel CUDA tối ưu cho lượng tử hóa, giải lượng tử hóa, và các toán tử sắp xếp lại lát tensor, và kết hợp kernel trên các toán tử này khi thích hợp để giảm thiểu chi phí lưu lượng bộ nhớ. Dưới đây chúng tôi thảo luận chi tiết về hai hướng tối ưu hóa.

4.1 Chồng chéo tính toán và giao tiếp

Để giảm thời gian giao tiếp đầu-cuối, chúng tôi chồng chéo tính toán lượng tử hóa với giao tiếp cho all-gathering trọng số trong cả quá trình truyền tiến và truyền ngược. Đối với triển khai reduce-scatter dựa trên all-to-all phân cấp của gradient, chúng tôi chồng chéo giao tiếp nội nút với giao tiếp liên nút.

4.1.1 Chồng chéo giao tiếp-tính toán trên trọng số. Đối với all-gather trên trọng số, chúng tôi kích hoạt chồng chéo giao tiếp-tính toán bằng cách sử dụng hai tính năng chính: i) chúng tôi theo dõi thứ tự thực thi của các lớp mô hình để có được chuỗi chúng sẽ được tìm nạp. ii) chúng tôi đảm bảo thực thi lượng tử hóa không đồng bộ. Cụ thể, cuộc gọi đến

--- TRANG 8 ---
ABC, 2023, USA Guanhua Wang*, Heyang Qin*, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari and Olatunji Ruwase, Feng Yan1, Lei Yang2, Yuxiong He

kernel lượng tử hóa không chặn và chúng tôi tiếp tục tránh các thao tác liên quan đến đồng bộ hóa CUDA rõ ràng/ngầm định (ví dụ: nối tensor), làm cho lượng tử hóa trở thành một thao tác không chặn có thể được khởi chạy một cách không đồng bộ.

Với hai tính năng này, khi ZeRO tìm nạp tham số cho mỗi lớp, giao tiếp của lớp hiện tại và lượng tử hóa của lớp tiếp theo có thể được khởi chạy cùng lúc trên các luồng CUDA khác nhau. Khi dữ liệu lượng tử hóa cần thiết cho lớp tiếp theo, ZeRO++ đồng bộ hóa luồng lượng tử hóa để đảm bảo dữ liệu lượng tử hóa đã sẵn sàng. Cách tiếp cận này ẩn chi phí lượng tử hóa của lớp tiếp theo dưới khoảng thời gian giao tiếp của lớp hiện tại, điều này ẩn chi phí lượng tử hóa.

QNo PipelineReorder+Quant Dequant+Reduction+Quant Dequant+Reduction2-Stage PipelineInter-node A2A Intra-node A2A

TimeD R
QQ D R
D R
Q D RIntra-node
A2A
Intra-node A2AInter-node A2A
Inter-node A2AData Chunk 1
Data Chunk 2Latency Reduction

Hình 10: Đường ống và chồng chéo giao tiếp nội nút với giao tiếp liên nút trong qgZ.

4.1.2 Giao tiếp tập thể phân cấp cho giao tiếp gradient. Như đã thảo luận trong Mục 3.3.2, giao tiếp gradient dựa trên all-to-all của chúng tôi được chia thành hai giai đoạn: đầu tiên giao tiếp nội nút tiếp theo là giao tiếp liên nút. Giao tiếp liên nút phụ thuộc vào kết quả của giao tiếp nội nút, do đó, với triển khai ngây thơ, các liên kết liên nút nhàn rỗi trong quá trình giao tiếp nội nút và ngược lại. Để giảm độ trễ bằng cách tận dụng cả liên kết liên nút và nội nút song song, chúng tôi phân khối tensor gradient đầu vào của chúng tôi và đường ống chuyển giao giữa giao tiếp nội nút và giao tiếp liên nút. Như được thể hiện trong Hình 10, so với trường hợp "không đường ống" ở trên, việc áp dụng đơn giản "đường ống 2 giai đoạn" chuyển giao đạt được lượng giảm độ trễ đầu-cuối được thể hiện như đường mũi tên đỏ trong Hình 10. Bằng cách chồng chéo giao tiếp nội nút và liên nút, độ trễ đầu-cuối của giao tiếp gradient được giảm đáng kể.

Thực hiện đường ống này một cách chính xác có ý nghĩa đối với quá trình sắp xếp lại lát tensor của chúng tôi. Chúng tôi có càng nhiều giai đoạn đường ống, chúng tôi cần càng nhiều lát tensor chi tiết hơn để sắp xếp lại. Do đó, chúng tôi cũng đề xuất một sơ đồ sắp xếp lại lát tensor tổng quát như thuật toán 2, bao gồm cả trường hợp chuyển giao dữ liệu có/ không có đường ống. Ở đây các giai đoạn đề cập đến số lượng giai đoạn đường ống chúng tôi có, nodeSize là số lượng GPU mỗi nút và nodes là số lượng nút.

Tiếp theo, chúng tôi thảo luận về cách chúng tôi tối ưu hóa các kernel CUDA của chúng tôi để giảm thêm tất cả chi phí liên quan đến lượng tử hóa.

Algorithm 2: Sắp xếp lại lát tensor tổng quát (qgZ)
Constants: stages, nodeSize, nodes
Input: partitionID
Output: mappedPartitionID
1 totalDevices ← nodeSize * nodes;
2 stageID ← partitionID % totalDevices;
3 chunkID ← partitionID / totalDevices;
4 pipelineOffset ← stageID * totalDevices;
5 chunkOffset ← stageID / nodeSize;
6 chunkBase ← (chunkID % nodeSize) * nodes;
7 Return: pipelineOffset + chunkBase + chunkOffset;

4.2 Kernel CUDA

Vì các triển khai lượng tử hóa hiện có không thể nắm bắt sự kết hợp của ánh xạ dữ liệu và thông lượng cao cần thiết để giảm thiểu chi phí kernel, chúng tôi triển khai và tối ưu hóa các kernel CUDA tùy chỉnh để triển khai các nguyên hàm này. Cụ thể, các kernel này nhằm (1) bão hòa băng thông bộ nhớ thiết bị và (2) giảm thiểu tổng lưu lượng qua kết hợp.

Tối đa hóa sử dụng băng thông: Một thư viện lượng tử hóa và giải lượng tử hóa cốt lõi của các toán tử có thể kết hợp được phát triển làm nền tảng cho ZeRO++. Các nguyên hàm cốt lõi tận dụng các truy cập bộ nhớ vector hóa hiệu quả ở độ chi tiết tối đa mà một kiến trúc GPU cho trước hỗ trợ. Để thỏa mãn các yêu cầu căn chỉnh mà các hướng dẫn này có, trạng thái mô hình được phân vùng sao cho độ chi tiết lượng tử hóa sẽ được căn chỉnh 16B. Ngoài ra, chúng tôi tận dụng song song cấp hướng dẫn để chồng chéo nhiều giao dịch bộ nhớ với nhau. Trong thực tế, sự kết hợp của truy cập vector hóa và song song cấp hướng dẫn cho phép thư viện lượng tử hóa đạt được sử dụng băng thông bộ nhớ GPU đầy đủ.

Giảm thiểu tổng lưu lượng: Nhiều kỹ thuật được sử dụng để giảm tổng lưu lượng bộ nhớ cho các kernel lượng tử hóa. Đầu tiên, kích thước của mỗi khối lượng tử hóa được điều chỉnh để thể hiện đủ song song để lập lịch trên các bộ xử lý đa luồng của GPU và cache các giá trị chưa được lượng tử hóa trong tệp đăng ký trong khi tính tỷ lệ và độ lệch lượng tử hóa cho khối. Thứ hai, chúng tôi kết hợp việc định hình lại tensor và lượng tử hóa vào cùng một kernel để tránh tải dữ liệu dư thừa từ bộ nhớ toàn cục. Ví dụ, sắp xếp lại lát tensor (tức là, đường mũi tên màu cam trong Hình 9) được thực hiện trong một kernel lượng tử hóa và ánh xạ lại kết hợp. Kernel kết hợp này đạt được cùng mức hiệu suất như một kernel lượng tử hóa đơn lẻ làm việc với dữ liệu liên tục. Cuối cùng, chúng tôi kết hợp các thao tác giải lượng tử hóa, giảm và lượng tử hóa tuần tự vào triển khai kernel đơn lẻ, điều này giảm tổng lưu lượng bộ nhớ 9x trong qgZ.

5 ĐÁNH GIÁ

Trong phần này, chúng tôi thực hiện ba bộ đánh giá cho ZeRO++. Đầu tiên, chúng tôi thực hiện đánh giá đầu-cuối cho thấy: i) đánh giá khả năng mở rộng trên lên đến 384 GPU, ii) tăng tốc so với baseline tiên tiến (SOTA) trên các mô hình từ 10-138B tham số, và iii) so sánh thông lượng cho cài đặt cụm với băng thông xuyên nút khác nhau. Thứ hai, chúng tôi thực hiện phân tích thông lượng

--- TRANG 9 ---
ZeRO++: Extremely Efficient Collective Communication for Giant Model Training ABC, 2023, USA

64
GPUs128
GPUs256
GPUs384
GPUs0102030TFLOPs per GPU15 15 14 1436343231IB: 1, Micro Batch per GPU: 1K Tokens

64
GPUs128
GPUs256
GPUs384
GPUs0204048
37 36
3152
48
43
38IB: 8, Micro Batch per GPU: 1K Tokens

64
GPUs128
GPUs256
GPUs384
GPUs02040TFLOPs per GPU28 27 26 2652514846IB: 1, Micro Batch per GPU: 2K Tokens

64
GPUs128
GPUs256
GPUs384
GPUs020406061
5553
4762595654IB: 8, Micro Batch per GPU: 2K Tokens Baseline ZeRO++

Hình 11: Khả năng mở rộng trên lên đến 384 GPU của mô hình 18B với số lượng kết nối InfiniBand khác nhau và token trên mỗi GPU

và phân tích, đánh giá tác động của các thành phần khác nhau của ZeRO++, cũng như tác động của các tối ưu hóa kernel của chúng tôi đối với thông lượng đầu-cuối. Cuối cùng, chúng tôi cho thấy đánh giá hội tụ cho thấy ZeRO++ không gây hại đến hội tụ mô hình và duy trì độ chính xác huấn luyện mô hình tương tự.

5.1 Phương pháp

Phần cứng: 24 nút NVIDIA DGX-2 nơi mỗi nút có 16 GPU V100 SXM3 32 GB [11]. Các nút được kết nối bằng InfiniBand (IB) với hỗ trợ NVIDIA SHARP [16], đạt được tổng băng thông liên nút hơn 800 Gbps. Để đánh giá ZeRO++ trong các cụm dưới các môi trường mạng khác nhau, chúng tôi cho thấy hiệu suất của ZeRO++ chạy với băng thông xuyên nút khác nhau bằng cách kích hoạt từ 1 đến 8 kết nối IB (tức là, 100 Gbps đến 800 Gbps).

Baseline: Chúng tôi sử dụng ZeRO-3 làm baseline cho tính dễ sử dụng của nó để huấn luyện các mô hình khổng lồ ở quy mô lớn. Để đánh giá hiệu suất của các kernel tối ưu của chúng tôi, chúng tôi cũng triển khai ZeRO++ với lượng tử hóa PyTorch [27] và các kernel không kết hợp làm baseline cho nghiên cứu loại bỏ của chúng tôi.

Cấu hình mô hình: Chúng tôi sử dụng các mô hình transformer kiểu GPT để đánh giá. Cho Megatron-Turing-NLG [33] huấn luyện mô hình 530B trên 2K GPU sử dụng 2K token mỗi GPU (tức là, kích thước micro batch), chúng tôi đánh giá ZeRO++ với cùng cài đặt 2k token mỗi GPU. Chúng tôi cũng đánh giá trên 1K token mỗi GPU để thử nghiệm ZeRO++ với tình huống quy mô cực đoan hơn. Số lượng lớp và kích thước ẩn được điều chỉnh để có các mô hình với kích thước khác nhau. Vui lòng tham khảo phụ lục và các script đánh giá mã nguồn mở của chúng tôi để biết các siêu tham số và chi tiết huấn luyện khác.

5.2 Đánh giá hệ thống E2E

Chúng tôi đánh giá hiệu suất đầu-cuối ZeRO++ ở đây. Một số liệu chính mà chúng tôi sử dụng ở đây là tỷ lệ phần trăm hiệu suất đỉnh, được thể hiện

Bảng 2: Tăng tốc đầu-cuối của ZeRO++ trên 384 GPU với các kích thước mô hình khác nhau

1 IB Connection 8 IB Connections
Model
SizeTokens
per GPUBaseline
TFLOPsZeRO++
TFLOPsSpeedupBaseline
TFLOPsZeRO++
TFLOPsSpeedup
138B 2K 19.96 37.90 1.90x 47.55 55.30 1.16x
138B 1K 11.25 21.81 1.94x 34.19 44.38 1.30x
91B 2K 19.99 38.06 1.90x 47.74 56.26 1.18x
91B 1K 11.27 21.93 1.95x 34.49 44.36 1.29x
49B 2K 20.06 38.08 1.90x 48.05 56.24 1.17x
49B 1K 11.27 21.95 1.95x 34.54 44.46 1.29x
18B 2K 25.98 46.40 1.79x 47.31 53.65 1.13x
18B 1K 14.15 30.57 2.16x 31.27 37.87 1.21x

Hình 12: ZeRO++ đạt được hiệu suất cụm băng thông cao với băng thông thấp hơn đáng kể

như phương trình 3.

peak_performance = achieved_TFLOPs/max_TFLOPs (3)

Cho rằng chúng tôi sử dụng GPU V100, max_TFLOPs của nó là 120 TFLOPs [24] cho tính toán độ chính xác hỗn hợp. Do đó, hiệu suất đỉnh được báo cáo của chúng tôi đề cập đến số phần trăm của achieved_TFLOPs/120.

5.2.1 Khả năng mở rộng lên đến 384 GPU. Trong Hình 11, chúng tôi trình bày đánh giá khả năng mở rộng ZeRO++ từ 64 đến 384 GPU với mô hình 18B trên cả cụm băng thông thấp (1 IB) và cao (8 IB). Trên cụm băng thông thấp, ZeRO++ đạt được 30% và 38.3% hiệu suất đỉnh (120 TFLOPs) ngay cả ở 384 GPU cho kích thước batch 1K và 2K, cao hơn nhiều so với hiệu suất đỉnh baseline 12.5% và 21.6%. Điều này thể hiện thông lượng tốt hơn lên đến 2.4x. Trên cụm băng thông cao, mặc dù có băng thông cao hơn đáng kể, ZeRO++ vẫn cho phép thông lượng tốt hơn lên đến 1.29x, và có thể đạt được lên đến 45% thông lượng đỉnh duy trì ở 384 GPU. ZeRO++ tăng tốc đáng kể huấn luyện quy mô lớn cho các cụm băng thông thấp trong khi đạt được tăng tốc tốt ngay cả trên các cụm băng thông cao.

5.2.2 Thông lượng cho các kích thước mô hình khác nhau. Bảng 2 so sánh thông lượng huấn luyện cho các mô hình từ 18B-138B trên 384 GPU giữa ZeRO++ và baseline trên cả cụm băng thông thấp và cao. Trên cụm băng thông thấp, ZeRO++ liên tục đạt được hơn 31.5% và 18.1% hiệu suất đỉnh cho kích thước batch 2K và 1K trên tất cả các mô hình. So với hiệu suất đỉnh baseline là 16.6% và 9.3%, tăng tốc lên đến 2.16x. Trên cụm băng thông cao, hiệu suất đỉnh ZeRO++ là 44.7% và 31.5%, tốt hơn 1.3x so với hiệu suất đỉnh baseline là 31.5% và 26.0%. ZeRO++ mạnh mẽ và cung cấp tăng tốc nhất quán trên các kích thước mô hình và batch khác nhau cũng như trên các cụm với băng thông mạng khác nhau.

--- TRANG 10 ---
ABC, 2023, USA Guanhua Wang*, Heyang Qin*, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari and Olatunji Ruwase, Feng Yan1, Lei Yang2, Yuxiong He

Bảng 3: Hiệu suất đầu-cuối khi sử dụng ZeRO++ có/không có kernel tối ưu

Optimized
Quantization
KernelOptimized
Fusion
KernelTFLOPs
Baseline N/A N/A 15
ZeRO++ No No 19.73
ZeRO++ No Yes 21.6
ZeRO++ Yes No 31.40
ZeRO++ Yes Yes 36.16

5.2.3 Dân chủ hóa cho huấn luyện quy mô lớn. Hình 12 so sánh thông lượng của ZeRO++ trên cụm băng thông xuyên nút thấp (200 Gbps như 2 IB) với baseline chạy trên cụm băng thông cao 800 Gbps (8 IB). Đối với mô hình nhỏ 18B, ZeRO++ đạt được hiệu suất đỉnh cao hơn 41.6% so với hiệu suất đỉnh baseline 39.1% mặc dù chạy với băng thông xuyên nút thấp hơn 4x. Đối với mô hình lớn 138B, ZeRO++ và baseline đạt được cùng hiệu suất đỉnh 40%, nhưng baseline chạy với băng thông xuyên nút cao hơn 4x. Đánh giá này cho thấy ZeRO++ làm cho huấn luyện quy mô lớn dễ tiếp cận hơn bằng cách giảm đáng kể yêu cầu băng thông xuyên nút tối thiểu cho huấn luyện hiệu quả. Hơn nữa, nó chứng minh rằng triển khai ZeRO++ tối ưu có hiệu quả chuyển đổi việc giảm giao tiếp 4x của ZeRO++ thành lợi ích thông lượng hệ thống đầu-cuối thực tế.

5.3 Phân tích thông lượng và phân tích chi tiết

5.3.1 Tác động của các kỹ thuật riêng lẻ. Trong Hình 13, chúng tôi cho thấy tác động riêng lẻ và kết hợp của qwZ, hpZ, và qgZ, đối với thông lượng của mô hình 18B trên 128 GPU. Trên các cụm băng thông thấp, mỗi kỹ thuật này cho phép tăng tốc từ 1.3-1.4x so với baseline, trong khi đạt được tăng tốc tổng hợp lên đến 2.26x. Lưu ý rằng thông lượng TFLOPs của chúng tôi được tính từ đo lường thời gian wall-clock, lợi ích thông lượng tổng hợp ZeRO++ không tương đương với tổng của lợi ích qgZ, qwZ, hpZ. Chúng tôi có thể xác nhận tăng tốc lý thuyết với sự kết hợp của các kỹ thuật của chúng tôi bằng cách tích lũy tăng tốc một cách nhân: 1.4*1.26*1.3=2.29, rất gần với những gì chúng tôi đạt được là 2.26x.

Đối với các cụm băng thông cao, tăng tốc riêng lẻ dao động từ 1.13-1.16x, cho tăng tốc kết hợp lên đến 1.3x. Hình minh họa rằng mỗi kỹ thuật này có tác động tương tự đối với cải thiện thông lượng và chúng kết hợp hiệu quả để tạo ra tăng tốc tổng hợp lớn hơn nhiều.

5.3.2 Tác động của tối ưu hóa kernel. Ở đây, chúng tôi đánh giá tác động của các kernel tối ưu của chúng tôi đối với thông lượng ZeRO++ bằng cách sử dụng mô hình 18B chạy trên 64 GPU.

Kernel lượng tử hóa: Như được thể hiện trong Bảng 3, so với baseline sử dụng lượng tử hóa PyTorch [27], các kernel lượng tử hóa tối ưu của chúng tôi có thể đạt được tăng tốc lên đến 1.67x về thông lượng đầu-cuối. Ngoài ra, triển khai baseline gặp phải suy giảm hiệu suất khi số lượng nhóm tăng có nghĩa là khoảng cách thông lượng sẽ lớn hơn khi được sử dụng với các mô hình lớn hơn.

Micro Batch:
1K TokensMicro Batch:
2K Tokens02040TFLOPs per GPU1527
2137
1935
2034 3451Number of IB: 1

Micro Batch:
1K TokensMicro Batch:
2K Tokens0204060
3755
4256
4256
4358
4859Number of IB: 8 Baseline
qwZqgZ
hpZZeRO++

Hình 13: Thông lượng của các mô hình 18B trên 128 GPU với ZeRO++, qwZ, qgZ, hpZ, và baseline trên số lượng kết nối InfiniBand khác nhau

Bảng 4: Đánh giá hpZ so với MiCS trên cụm 4 nút (16 GPU V100 mỗi nút)

Model Size Token SizeZeRO
TFLOPshpZ
TFLOPsMiCS
TFLOPs
7.5B 1K 36.99 38.39 38.96
7.5B 2K 53.3 54.4 52.72
18B 1K 51.47 52.42 OOM
18B 2K 60.94 61.44 OOM

Kết hợp kernel: Như được mô tả trong Phần 4.2, kết hợp kernel là một trong những tối ưu hóa chính của chúng tôi để cải thiện thông lượng bộ nhớ khi thực thi chuỗi kernel CUDA. Kết hợp của chúng tôi bao gồm 1) kết hợp sắp xếp lại tensor và lượng tử hóa 2) kết hợp giải lượng tử hóa nội nút, giảm nội nút và lượng tử hóa liên nút. Như được thể hiện trong Bảng 3, chúng tôi đạt được tăng tốc lên đến 1.15x trên thông lượng đầu-cuối.

5.3.3 So sánh hpZ với MICS. Như đã thảo luận trước đó trong Phần 2, liên quan chặt chẽ đến phân vùng trọng số phân cấp cho ZeRO (hpZ) là MiCS [40]. Sự khác biệt chính của hai phương pháp là dữ liệu nào được sao chép trong nhóm thứ hai; trọng số mô hình được sao chép trong hpZ, toàn bộ trạng thái mô hình được sao chép trong MiCS. Bảng 4 cho thấy thông lượng trên mỗi GPU của cả hai phương pháp cho các cấu hình kích thước mô hình và token khác nhau. Bảng cũng cho thấy rằng với kích thước phân vùng thứ hai là một nút đơn lẻ (16 GPU V100), hpZ có thể hỗ trợ mô hình 18 tỷ tham số trong khi MiCS báo cáo hết bộ nhớ (OOM) ở quy mô này.

5.4 Phân tích hội tụ mô hình

Tiếp theo chúng tôi đánh giá tác động của ZeRO++ đối với hội tụ mô hình bằng cách huấn luyện mô hình GPT-350M với 30B token trên bộ dữ liệu pile [3] sử dụng ZeRO++, ZeRO++ với lượng tử hóa cơ bản (không khối), và ZeRO-3 làm baseline. Tất cả các siêu tham số được giữ giống nhau giữa huấn luyện baseline và huấn luyện ZeRO++ để đảm bảo so sánh công bằng. Hội tụ được đo bằng mất mát LM xác thực.

Như được thể hiện trong Hình 14, chúng tôi trình bày theo dõi huấn luyện đầu-cuối. Huấn luyện với lượng tử hóa cơ bản (không khối) phân kỳ ngay từ đầu nên không có dữ liệu nhìn thấy được, ngược lại, ZeRO++

--- TRANG 11 ---
ZeRO++: Extremely Efficient Collective Communication for Giant Model Training ABC, 2023, USA

0 20000 40000 60000
steps234567validation lm loss

Validation LM loss vs. Steps
Baseline
ZeRO++

Hình 14: Hội tụ huấn luyện cho GPT-350M trên 30B token

Bảng 5: Mất mát xác thực ở cuối huấn luyện (GPT 350M / 30B token)

Evaluation LM loss
Baseline 2.121762
ZeRO++
(hpZ&qwZ&qgZ on)2.165584
ZeRO++
(hpZ&qwZ on;
qgZ on for first 50%)2.134013
ZeRO++
(hpZ&qwZ on; qgZ off)2.121653

được căn chỉnh chặt chẽ với baseline, và cũng xác nhận phân tích trước đó của chúng tôi về độ chính xác lượng tử hóa tốt hơn bằng cách sử dụng lượng tử hóa dựa trên khối.

Chúng tôi tiếp tục mở rộng đánh giá hội tụ bằng cách so sánh mất mát đánh giá cuối cùng ở cuối huấn luyện. Như được thể hiện trong Bảng 5, ngay cả với cả ba tối ưu hóa được bật, mất mát đánh giá cuối cùng chỉ lệch 1%. Chúng tôi tiếp tục thu hẹp khoảng cách hội tụ này bằng cách sử dụng lịch trình xen kẽ đơn giản nơi phân vùng phân cấp và trọng số lượng tử hóa được bật trong suốt quá trình huấn luyện và gradient lượng tử hóa chỉ được bật trong 50% đầu của huấn luyện. Đối với trường hợp mở rộng hơn, chúng tôi cũng đánh giá riêng phân vùng phân cấp và trọng số lượng tử hóa. Kết quả cho thấy hội tụ của chúng tôi giống hệt với baseline trong trường hợp này.

6 KẾT LUẬN

Bài báo này trình bày ZeRO++, một giải pháp giao tiếp tập thể hiệu quả cho huấn luyện mô hình khổng lồ sử dụng ZeRO stage-3. Chúng tôi tối ưu hóa cả giao tiếp trọng số mô hình và gradient trong quá trình truyền tiến và truyền ngược của mỗi lần lặp huấn luyện. Để giảm khối lượng giao tiếp của trọng số mô hình trong truyền tiến, chúng tôi áp dụng lượng tử hóa dựa trên khối và tìm nạp dữ liệu trước. Để loại bỏ giao tiếp xuyên nút của trọng số trong quá trình truyền ngược, chúng tôi giữ phân vùng mô hình thứ hai trên mỗi nút để đánh đổi bộ nhớ lấy giao tiếp. Để giảm thiểu giao tiếp gradient trong quá trình truyền ngược, chúng tôi thiết kế và triển khai một sơ đồ lượng tử hóa và giảm gradient dựa trên all-to-all mới. Bằng cách tích hợp cả ba tối ưu hóa trên, chúng tôi cải thiện thông lượng hệ thống lên đến 2.16x trong huấn luyện mô hình quy mô lớn sử dụng 384 GPU V100. Chúng tôi hình dung ZeRO++ như thế hệ tiếp theo của framework dễ sử dụng để huấn luyện các mô hình khổng lồ ở quy mô trilion-level.

7 TÁC GIẢ VÀ PHÂN CÔNG TÍN DỤNG CHÍNH

• Guanhua Wang: thiết kế và triển khai qgZ, tích hợp mã, thiết kế và triển khai kernel lượng tử hóa hiệu năng cao, giải quyết tất cả xung đột kernel CUDA trong việc hợp nhất mã, phần lớn việc viết bài báo.
• Heyang Qin: thiết kế và triển khai qwZ, tích hợp mã/giải quyết xung đột trong việc hợp nhất mã, thiết kế thí nghiệm và đánh giá, nghiên cứu hội tụ chuyên sâu.
• Sam Ade Jacobs: thiết kế và triển khai hpZ, tích hợp mã/giải quyết xung đột trong việc hợp nhất mã.
• Connor Holmes: thiết kế và triển khai kernel lượng tử hóa hiệu năng cao.
• Samyam Rajbhandari: kiến trúc sư trưởng
• Olatunji Ruwase: hỗ trợ kỹ thuật
• Yuxiong He: trưởng nhóm

--- TRANG 12 ---
ABC, 2023, USA Guanhua Wang*, Heyang Qin*, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari and Olatunji Ruwase, Feng Yan1, Lei Yang2, Yuxiong He

TÀI LIỆU THAM KHẢO
[1]Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. 2017.
QSGD: Communication-efficient SGD via gradient quantization and encoding.
Advances in neural information processing systems 30 (2017).
[2]Tal Ben-Nun and Torsten Hoefler. 2019. Demystifying parallel and distributed
deep learning: An in-depth concurrency analysis. ACM Computing Surveys
(CSUR) 52, 4 (2019), 1–43.
[3]Stella Biderman, Kieran Bicheno, and Leo Gao. 2022. Datasheet for the Pile.
arXiv:2201.07311 [cs.CL]
[4]Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler,
USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben
Wang, and Samuel Weinbach. 2022. GPT-NeoX-20B: An Open-Source Autore-
gressive Language Model. (2022).
[5]Ernie Chan, Marcel Heimlich, Avi Purkayastha, and Robert Van De Geijn. 2007.
Collective communication: theory, practice, and experience. Concurrency and
Computation: Practice and Experience 19, 13 (2007), 1749–1783.
[6]Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark
Mao, Marc'aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. 2012.
Large scale distributed deep networks. Advances in neural information processing
systems 25 (2012).
[7]Tim Dettmers. 2015. 8-bit approximations for parallelism in deep learning. arXiv
preprint arXiv:1511.04561 (2015).
[8]Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 2022. 8-bit
Optimizers via Block-wise Quantization. In The Tenth International Conference
on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenRe-
view.net. https://openreview.net/forum?id=shpkpVXzo3h
[9]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:
Pre-training of deep bidirectional transformers for language understanding. arXiv
preprint arXiv:1810.04805 (2018).
[10] dgx1 2017. NVIDIA DGX-1. https://www.nvidia.com/en-us/data-center/dgx-1/.
[11] dgx2 2018. NVIDIA DGX-2. https://www.nvidia.com/en-us/data-center/dgx-2/.
[12] Nikoli Dryden, Tim Moon, Sam Ade Jacobs, and Brian Van Essen. 2016. Commu-
nication Quantization for Data-Parallel Training of Deep Neural Networks. In
Proceedings of the Workshop on Machine Learning in High Performance Computing
Environments (Salt Lake City, Utah) (MLHPC '16). IEEE Press, 1–8.
[13] Aaron Harlap, Deepak Narayanan, Amar Phanishayee, Vivek Seshadri, Nikhil
Devanur, Greg Ganger, and Phil Gibbons. 2018. Pipedream: Fast and efficient
pipeline parallel dnn training. arXiv preprint arXiv:1806.03377 (2018).
[14] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia
Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. 2019. Gpipe:
Efficient training of giant neural networks using pipeline parallelism. Advances
in neural information processing systems 32 (2019).
[15] Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam,
Quoc V. Le, and Zhifeng Chen. 2018. GPipe: Efficient Training of Giant Neural
Networks using Pipeline Parallelism. ArXiv abs/1811.06965 (2018).
[16] Infiniband Sharp white paper 2021. NVIDIA InfiniBand Adaptive Routing
Technology. https://nvdam.widen.net/s/whmszwfrbt/infiniband-white-paper-
adaptive-routing-1846350.
[17] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyan-
skiy, and Ping Tak Peter Tang. 2016. On large-batch training for deep learning:
Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836 (2016).
[18] Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, and
Yuxiong He. 2021. 1-bit LAMB: Communication Efficient Large-Scale Large-
Batch Training with LAMB's Convergence Speed. CoRR abs/2104.06069 (2021).
arXiv:2104.06069 https://arxiv.org/abs/2104.06069
[19] Microsoft. 2020. Turing-NLG: A 17-billion-parameter language model by
Microsoft. https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-
billion-parameter-language-model-by-microsoft/.
[20] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil
Devanur, Greg Granger, Phil Gibbons, and Matei Zaharia. 2019. PipeDream: Gen-
eralized Pipeline Parallelism for DNN Training. In ACM Symposium on Operating
Systems Principles (SOSP 2019).
[21] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia.
2021. Memory-efficient pipeline-parallel dnn training. In International Conference
on Machine Learning. PMLR, 7937–7947.
[22] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley,
Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti,
Julie Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia. 2021.
Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-
LM. In Proceedings of the International Conference for High Performance Com-
puting, Networking, Storage and Analysis (St. Louis, Missouri) (SC '21). Asso-
ciation for Computing Machinery, New York, NY, USA, Article 58, 15 pages.
https://doi.org/10.1145/3458817.3476209
[23] N NVIDIA. 2017. NVIDIA Collective Communications Library (NCCL).
[24] Nvidia V100 datasheet 2017. NVIDIA TESLA V100 GPU ACCELERA-
TOR. https://www.penguinsolutions.com/computing/wp-content/uploads/2019/03/penguin-computing-tesla-v100-ds.pdf.
[25] NVLink 2017. NVIDIA NVLINK. http://www.nvidia.com/object/nvlink.html.
[26] NVSwitch 2018. NVIDIA NVSWITCH. http://images.nvidia.com/content/pdf/
nvswitch-technical-overview.pdf.
[27] Quantization - PyTorch documentation 2023. Quantization - PyTorch documen-
tation. https://pytorch.org/docs/stable/quantization.html.
[28] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).
[29] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero:
Memory optimizations toward training trillion parameter models. In SC20: Inter-
national Conference for High Performance Computing, Networking, Storage and
Analysis. IEEE, 1–16.
[30] Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, and Yuxiong
He. 2021. ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep
Learning. In Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis (SC '21).
[31] Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 2014. 1-bit stochastic
gradient descent and its application to data-parallel distributed training of speech
dnns. In Fifteenth annual conference of the international speech communication
association.
[32] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter
language models using model parallelism. arXiv preprint arXiv:1909.08053 (2019).
[33] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay
Korthikanti, et al. 2022. Using DeepSpeed and Megatron to Train Megatron-
Turing NLG 530B, A Large-Scale Generative Language Model. arXiv preprint
arXiv:2201.11990 (2022).
[34] Nikko Ström. 2015. Scalable distributed DNN training using commodity GPU
cloud computing. (2015).
[35] Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Con-
glong Li, Xiangru Lian, Ji Liu, Ce Zhang, and Yuxiong He. 2021. 1-bit Adam:
Communication Efficient Large-Scale Training with Adam's Convergence Speed.
CoRR abs/2102.02888 (2021). arXiv:2102.02888 https://arxiv.org/abs/2102.02888
[36] DeepSpeed Team and Rangan Majumder. 2020. DeepSpeed: Extreme-scale
model training for everyone. https://www.microsoft.com/en-us/research/blog/
deepspeed-extreme-scale-model-training-for-everyone/.
[37] Rajeev Thakur, Rolf Rabenseifner, and William Gropp. 2005. Optimization of
collective communication operations in MPICH. The International Journal of
High Performance Computing Applications 19, 1 (2005), 49–66.
[38] Guanhua Wang, Shivaram Venkataraman, Amar Phanishayee, Jorgen Thelin,
Nikhil R. Devanur, and Ion Stoica. 2020. Blink: Fast and Generic Collectives for
Distributed ML. In Proceedings of Machine Learning and Systems 2020, MLSys 2020,
Austin, TX, USA, March 2-4, 2020, Inderjit S. Dhillon, Dimitris S. Papailiopoulos,
and Vivienne Sze (Eds.). mlsys.org. https://proceedings.mlsys.org/book/299.pdf
[39] Yang You, Jing Li, Jonathan Hseu, Xiaodan Song, James Demmel, and Cho-Jui
Hsieh. 2019. Reducing BERT Pre-Training Time from 3 Days to 76 Minutes. CoRR
abs/1904.00962 (2019). arXiv:1904.00962 http://arxiv.org/abs/1904.00962
[40] Zhen Zhang, Shuai Zheng, Yida Wang, Justin Chiu, George Karypis, Trishul
Chilimbi, Mu Li, and Xin Jin. 2022. MiCS: Near-linear Scaling for Training
Gigantic Model on Public Cloud. arXiv:2205.00119 [cs.DC]
[41] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De Sa, and Zhiru Zhang.
2019. Improving Neural Network Quantization without Retraining using Outlier
Channel Splitting. In Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA (Proceedings of
Machine Learning Research, Vol. 97), Kamalika Chaudhuri and Ruslan Salakhutdi-
nov (Eds.). PMLR, 7543–7552. http://proceedings.mlr.press/v97/zhao19c.html
