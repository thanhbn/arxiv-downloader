MoESys: Hệ thống Huấn luyện và Suy luận Mixture-of-Experts Phân tán và Hiệu quả cho Dịch vụ Internet

Dianhai Yu∗, Liang Shen∗, Hongxiang Hao, Weibao Gong, Huachao Wu, Jiang Bian, Thành viên, IEEE, Lirong Dai, Thành viên, IEEE, Haoyi Xiong, Thành viên Cao cấp, IEEE

Tóm tắt—Trong khi các dịch vụ internet hiện đại, như chatbot, công cụ tìm kiếm và quảng cáo trực tuyến, đòi hỏi việc sử dụng mạng nơ-ron sâu quy mô lớn (DNN), việc huấn luyện và suy luận phân tán trên các hệ thống tính toán không đồng nhất được mong muốn để hỗ trợ các mô hình DNN này. Mixture-of-Experts (MoE) là một trong những chiến lược phổ biến nhất để giảm chi phí huấn luyện dựa trên quy mô tổng thể của mô hình/dữ liệu thông qua cơ chế cổng và song song hóa theo cách chia để trị. Trong khi DeepSpeed [1] đã nỗ lực thực hiện huấn luyện MoE quy mô lớn trên cơ sở hạ tầng không đồng nhất, hiệu quả huấn luyện và suy luận có thể được cải thiện thêm từ một số khía cạnh hệ thống, bao gồm cân bằng tải, hiệu quả giao tiếp/tính toán và giới hạn dung lượng bộ nhớ. Trong công trình này, chúng tôi trình bày một MoESys mới giúp tăng hiệu quả trong cả huấn luyện và suy luận quy mô lớn. Cụ thể, trong quy trình huấn luyện, MoESys được đề xuất áp dụng chiến lược huấn luyện MoE Đàn hồi với lập lịch prefetch 2D và giao tiếp Fusion qua Lưu trữ Phân cấp, nhằm tận hưởng song song hóa hiệu quả. Đối với suy luận có thể mở rộng trong một nút đơn, đặc biệt khi kích thước mô hình lớn hơn bộ nhớ GPU, MoESys xây dựng bộ nhớ CPU-GPU cùng nhau thành một vòng các phần để tải mô hình, và thực hiện các tác vụ tính toán qua các phần bộ nhớ theo cách round-robin để suy luận hiệu quả. Chúng tôi thực hiện các thí nghiệm rộng rãi để đánh giá MoESys, trong đó MoESys huấn luyện thành công mô hình Unified Feature Optimization [2] (UFO) với mô hình Sparsely-Gated Mixture-of-Experts có 12B tham số trong 8 ngày trên 48 card GPU A100. So sánh với công nghệ tiên tiến cho thấy MoESys vượt trội DeepSpeed với thông lượng cao hơn 33% (token mỗi giây) trong huấn luyện và thông lượng cao hơn 13% trong suy luận nói chung. Đặc biệt, dưới các tác vụ MoE không cân bằng, ví dụ UFO, MoESys đạt được thông lượng cao hơn 64% với dung lượng bộ nhớ thấp hơn 18%.

Từ khóa chỉ mục—Mô hình Lớn cho Dịch vụ Internet, MoE, Huấn luyện Phân tán, Suy luận Phân tán

1 GIỚI THIỆU

Trong những năm gần đây, đã có sự phát triển đáng kể trong các dịch vụ internet, và việc tích hợp trí tuệ nhân tạo đã khiến các mô hình học sâu trở nên không thể thiếu trong hệ sinh thái internet [3]–[6]. Đặc biệt, các mô hình mạng nơ-ron sâu (DNN) lớn như BERT và GPT đã ngày càng phổ biến do hiệu suất đáng chú ý của chúng trong các ứng dụng xử lý văn bản và ngôn ngữ [7], dẫn đến việc các dịch vụ internet khác nhau, bao gồm chat bot, nền tảng quảng cáo trực tuyến, hệ thống đề xuất, công cụ tìm kiếm và công cụ dịch thuật, dựa vào các mô hình này để cung cấp cho người dùng độ chính xác và tùy chỉnh mong muốn [8]–[12]. Trong khi việc sử dụng các mô hình lớn đã cải thiện đáng kể hiệu suất của dịch vụ internet, nó đã đi kèm với chi phí mở rộng quy mô tham số lên hàng chục tỷ, như mô hình GPT-3 với 175B tham số [13], [14], Ernie3.0 Titan với 260B tham số [15], và Megatron-Turing NLG với 530B tham số [16]. Tuy nhiên, các mô hình được kích hoạt dày đặc này đòi hỏi tài nguyên tính toán dồi dào và thời gian huấn luyện lâu dài. Ví dụ, việc huấn luyện Megatron-Turing NLG với 530B tham số, một trong những mô hình được kích hoạt dày đặc lớn nhất, đòi hỏi ba tháng sử dụng hơn 2000 GPU NVIDIA A100 [16], khiến nó tốn kém về tài chính và cản trở việc phát triển các mô hình với quy mô tham số thậm chí lớn hơn. Hơn nữa, hiệu suất suy luận của các mô hình siêu quy mô lớn này hiếm khi đáp ứng nhu cầu công nghiệp hiện tại [5], [17].

Nhiều chiến lược tùy chỉnh khác nhau đã được sử dụng để cải thiện hiệu quả huấn luyện các mô hình quy mô lớn. Một cách tiếp cận như vậy là khái niệm AIBox, được áp dụng cụ thể trong việc huấn luyện các mô hình dự đoán Tỷ lệ Nhấp chuột (CTR) để giảm chi phí. Phương pháp này bao gồm việc làm thưa thớt các embedding đặc trưng và tận dụng thiết lập đa-GPU phân tán để tham số hóa quá mức mô hình [18]. AIBox chủ yếu tập trung vào các lớp nhất định để xử lý dữ liệu chiều cao, với mục tiêu mở rộng mô hình. Ngược lại, trong lĩnh vực các mô hình ngôn ngữ được huấn luyện trước, học đa tác vụ đã được áp dụng, đặc biệt cho dịch máy nơ-ron đa ngôn ngữ [19]. Các mô hình như MT5 [20], MASSively [21], và MultiNLI [22] khác với các mô hình được kích hoạt dày đặc nhưng đòi hỏi tài nguyên tính toán đáng kể để vượt qua các benchmark hiện có.

Để giải quyết những thách thức này, các mạng nơ-ron được kích hoạt thưa thớt dựa trên Mixture-of-Experts (MoE) đã được giới thiệu để huấn luyện các mô hình lớn hơn với tài nguyên tính toán tối thiểu hoặc không có tài nguyên tính toán bổ sung, trong khi vẫn đạt được kết quả huấn luyện cải thiện [23]–[26]. Các kiến trúc MoE chỉ kích hoạt một tập con tham số dựa trên dữ liệu đầu vào, không giống như các mô hình được kích hoạt dày đặc. Việc kích hoạt có chọn lọc này dẫn đến sự gia tăng chi phí tính toán dưới tuyến tính so với kích thước mô hình. Ví dụ, biến thể lớn nhất của GLaM [27] có 1.2T tham số với 64 chuyên gia mỗi lớp MoE, nhưng chỉ kích hoạt một subnet 95B tham số (8% của 1.2T) cho mỗi token đầu vào. Việc huấn luyện mô hình này tiết kiệm hai phần ba năng lượng cần thiết cho GPT-3 (175B) [13], trong khi giảm một nửa tài nguyên tính toán cần thiết trong quá trình suy luận. Mặc dù có tất cả những lợi ích, các mô hình MoE vẫn phải đối mặt với nhiều thách thức và hạn chế, đặc biệt trong tính toán, giao tiếp và lưu trữ:

• Tính toán – Chi phí tính toán mỗi GPU vẫn không đổi trong các mô hình MoE, nhưng tăng với tổng số chuyên gia. Hiệu suất huấn luyện giảm do mất cân bằng chuyên gia, trong đó một số bị huấn luyện quá mức và những cái khác không được sử dụng đầy đủ [25]. Các giải pháp bao gồm auxiliary losses [24], lựa chọn chuyên gia ngẫu nhiên [28], và nhiễu trong định tuyến [25]. Tuy nhiên, những điều này tập trung nhiều hơn vào lập lịch hơn là tính toán và đòi hỏi tài nguyên CPU đáng kể. Việc phân bổ tác vụ tính toán không hiệu quả và các hoạt động dư thừa, như chuyển giao H2D và D2H, làm giảm hiệu quả và tăng độ trễ [29].

• Giao tiếp – Trong các mô hình MoE, sự mất cân bằng trong các chiến lược định tuyến vẫn tồn tại mặc dù có các phương pháp học tập tiên tiến [24], [25], [30], [31]. Dữ liệu mất cân bằng dẫn đến tiến trình không nhất quán và chờ đợi dư thừa trong huấn luyện đa tác vụ. Ví dụ, mô hình Switch Transformer đòi hỏi bốn giao tiếp AlltoAll mỗi lớp MoE, dẫn đến suy giảm hiệu suất do xung đột định tuyến và chặn trong các cấu trúc mạng không xác định [25].

• Lưu trữ – Khả năng bộ nhớ và lưu trữ giới hạn kích thước mô hình MoE. Trong khi các mô hình dày đặc bị giới hạn bởi thời gian huấn luyện, các mô hình MoE mở rộng tốt hơn do chi phí tính toán tăng dưới tuyến tính. Một mô hình dày đặc với 1 nghìn tỷ tham số đòi hỏi 3 tháng để huấn luyện trên 3072 GPU NVIDIA A100, nhưng một mô hình MoE có thể được huấn luyện trong vài tuần [14]. Tuy nhiên, khả năng mở rộng của mô hình phụ thuộc vào khả năng bộ nhớ thiết bị. Sự khác biệt trong độ trễ I/O giữa HBM trong GPU, bộ nhớ CPU và SSD gây ra độ trễ, đòi hỏi quản lý lưu trữ hiệu quả cho huấn luyện được kích hoạt thưa thớt [32].

Đóng góp của chúng tôi. Để vượt qua các thách thức và hạn chế nêu trên của MoE, chúng tôi giới thiệu một khung thống nhất mới MoESys, dựa trên một nền tảng mã nguồn mở cho huấn luyện và suy luận MoE. Các đóng góp không tầm thường trong MoESys như sau:

• Một khung phân tán mới có tên MoESys được thiết kế, có khả năng mở rộng các mô hình MoE lên hàng nghìn tỷ tham số, tận dụng đầy đủ các cụm bao gồm HBM, bộ nhớ CPU và thậm chí cả SSD để phá vỡ rào cản bộ nhớ và đạt được lập lịch huấn luyện hiệu quả. Đáng chú ý, MoESys kết hợp các kỹ thuật tiên tiến như lập lịch prefetch 2D và giao tiếp fusion, nâng cao thêm hiệu quả của các hệ thống lưu trữ không đồng nhất.

• Một phương pháp suy luận mới dựa trên bộ nhớ vòng được sử dụng bởi lập lịch đồ thị động, có thể tích hợp tính toán và giao tiếp càng nhiều càng tốt và tăng tốc quy trình suy luận mà không cần sử dụng thêm máy cho các mô hình MoE quy mô lớn hơn.

• Một số chiến lược huấn luyện hiệu quả đã được thiết kế ban đầu trong MoESys cho các tác vụ NLP và CV, nhằm mở rộng học đa tác vụ mà không cần thêm bộ nhớ. Các chiến lược này bao gồm cân bằng tải, phân vùng embedding và giao tiếp nhận thức tài nguyên.

• Chúng tôi tiến hành các thí nghiệm cấp độ công nghiệp toàn diện để thể hiện sự gia tăng hiệu suất đáng kể khi sử dụng MoESys, trong đó thực hành trong công việc này có thể có lợi cho việc phát triển huấn luyện và suy luận MoE quy mô lớn trong tương lai.

Chúng tôi tổ chức phần còn lại của bản thảo này như sau. Trong Phần 2, chúng tôi xem xét các nỗ lực trước đây về thiết kế MoE. Phần 3 giới thiệu thiết kế mới của MoESys tương ứng. Ngoài ra, chúng tôi tiết lộ chi tiết về các chiến lược thực hiện thực tế được áp dụng trong MoESys trong Phần 4. Để chứng minh tính hiệu quả và hiệu suất của MoESys, chúng tôi tiến hành các thí nghiệm toàn diện và phân tích kết quả trong Phần 5. Cuối cùng, chúng tôi kết luận công việc này và hướng tới hướng phát triển tương lai trong Phần 6.

2 CÔNG TRÌNH LIÊN QUAN

Trong phần này, chúng tôi xem xét các công trình liên quan trong lĩnh vực từ các góc độ về mô hình lớn cho dịch vụ internet và hệ thống huấn luyện và suy luận của chúng.

2.1 Dịch vụ Internet và Mô hình Lớn

Các Mô hình Ngôn ngữ Lớn (LLM) đang cách mạng hóa các dịch vụ internet như công cụ tìm kiếm, chatbot, quảng cáo trực tuyến và ứng dụng đám mây [8]–[12], [33]. Các tổ chức ngày càng sử dụng LLM tùy chỉnh được thiết kế riêng cho các nhu cầu cụ thể. Các mô hình chuyên biệt theo miền này nâng cao chất lượng dịch vụ internet và trải nghiệm khách hàng, hiệu quả và nhanh hơn các LLM đa mục đích, đặc biệt cho các ứng dụng liên quan đến dữ liệu độc quyền. Một ví dụ là BloombergGPT [34], một LLM tùy chỉnh của Bloomberg, có tác động đáng kể đến các dịch vụ tài chính trực tuyến bằng cách nhanh chóng đánh giá dữ liệu tài chính cho đánh giá rủi ro, phân tích tình cảm tài chính, và có khả năng tự động hóa kế toán và kiểm toán. Mặc dù có kích thước lớn 50 tỷ tham số, BloombergGPT tránh huấn luyện mô hình đơn truyền thống, ủng hộ hệ thống Mixture-of-Experts (MoE) để có hiệu quả và hiệu suất tốt hơn. Các mô hình MoE đã cho thấy tiềm năng to lớn trong xử lý ngôn ngữ tự nhiên, với các chiến lược tập trung vào cải tiến định tuyến [28], [35] để cải thiện chất lượng và hiệu suất mô hình. Lưu ý rằng, khung GLaM [27] chứng minh rằng MoE lớn nhất với 1.2 nghìn tỷ tham số tiết kiệm năng lượng hơn, chỉ sử dụng một phần ba năng lượng cần thiết để huấn luyện GPT-3.

Theo quy luật mở rộng, có xu hướng gia tăng kích thước mô hình. Các mô hình dựa trên MoE với hàng tỷ hoặc thậm chí hàng nghìn tỷ tham số, như CPM-2 [36], M6-T [31], M6-10T [37], và GLaM [27], đang thể hiện khả năng tổng quát hóa vượt trội trong xử lý ngôn ngữ và các tác vụ đa phương thức. Mô hình UFO [2] của Baidu, một khung dựa trên MoE khác, nhấn mạnh hiệu quả triển khai và sử dụng dữ liệu lớn. Nó có đặc trưng là một mạng siêu bao gồm nhiều tác vụ phụ, với chiến lược định tuyến chọn tác vụ phụ phù hợp để huấn luyện.

2.2 Hệ thống Huấn luyện và Suy luận MoE

Sự phổ biến ngày càng tăng của phương pháp huấn luyện Mixture of Experts (MoE) đã dẫn đến việc phát hành một số khung và hệ thống huấn luyện MoE mã nguồn mở bởi các cơ quan nghiên cứu khoa học và doanh nghiệp khác nhau. DeepSpeed-MoE tích hợp nhiều kỹ thuật song song phân tán như song song dữ liệu và cắt tensor để sử dụng hiệu quả song song MoE, cho phép huấn luyện các mô hình lớn hơn. Nó cũng giới thiệu PR-MoE, một mô hình được kích hoạt thưa thớt mới cho suy luận MoE, và sử dụng nén mô hình để giảm kích thước mô hình, cùng với chiến lược giao tiếp hiệu quả để cải thiện độ trễ [38], [39]. FastMoE, một hệ thống huấn luyện MoE phân tán khác, cung cấp giao diện phân cấp thân thiện với người dùng và hướng dẫn đơn giản để tích hợp Megatron-LM và Transformer-XL với song song cắt dữ liệu và tensor [29], [40], [41]. Không giống như DeepSpeed, FastMoE tập trung vào giảm lưu lượng mạng thông qua phương pháp tối ưu hóa tiên tiến. Hệ thống suy luận INFMoE đề xuất một chuỗi tính toán tối ưu và offloading tham số sử dụng thuật toán tham lam để giải quyết mất cân bằng khối lượng công việc và giảm thiểu tác động của di chuyển dữ liệu, đặc biệt khi offloading sang CPU, trong khi duy trì hiệu quả tính toán [36]. Fairseq-MoE là một khung được thiết kế riêng cho huấn luyện các mô hình tùy chỉnh trong các lĩnh vực như tóm tắt, dịch thuật và mô hình hóa ngôn ngữ. Tutel nâng cao khả năng giao tiếp và tính toán của Fairseq, dẫn đến cải thiện hiệu suất khoảng 40%. Đáng chú ý, những cải tiến này trong Tutel đã được kết hợp vào DeepSpeed để huấn luyện mô hình MoE [42]–[44].

Hơn nữa, quy mô mô hình và kích thước dữ liệu là hai yếu tố quan trọng ảnh hưởng đáng kể đến hiệu suất và hiệu quả của huấn luyện mô hình. Tuy nhiên, khám phá sâu hơn trong lĩnh vực này đặt ra thách thức đáng kể cho các tổ chức khoa học và doanh nghiệp do yêu cầu tài nguyên tính toán và lưu trữ khổng lồ. Để giải quyết thách thức này, thiết kế mô hình được kích hoạt thưa thớt đã xuất hiện trong những năm gần đây và thu hút sự chú ý trong ngành. Không giống như các mô hình được kích hoạt dày đặc liên quan đến tính toán tất cả tham số, mô hình được kích hoạt thưa thớt động lực chọn một tập con tham số để huấn luyện dựa trên dữ liệu đầu vào. Cách tiếp cận này cho phép mở rộng tham số tuyến tính mà không tăng khối lượng công việc tính toán, do đó làm cho các mô hình lớn hơn được xây dựng trên kiến trúc Mixture-of-Experts (MoE) khả thi và hiệu quả hơn.

3 THIẾT KẾ MOESYS

MoESys là một hệ thống sáng tạo cho huấn luyện và suy luận phân tán, sử dụng kiến trúc Mixture-of-Experts để nâng cao khả năng mở rộng và hiệu quả. Mục tiêu chính của nó là tuân thủ các mục tiêu độ trễ bộ nhớ được xác định trước trong khi hoạt động trong giới hạn lưu trữ hiện có. Một tiến bộ đáng chú ý trong lĩnh vực này là cách tiếp cận Zero-infinity của DeepSpeed [45], đã huấn luyện thành công một mô hình với hơn 30 nghìn tỷ tham số sử dụng 512 GPU V100 qua các nút NVIDIA DGX-2. Kỹ thuật tiên phong này vượt qua các nút cổ chai bộ nhớ bằng cách khai thác đầy đủ một loạt phương tiện lưu trữ, như High Bandwidth Memory (HBM) trong GPU, bộ nhớ CPU và SSD. Điều này cho phép huấn luyện các mô hình cực lớn trên các thiết bị đơn lẻ. Để tinh chỉnh việc sử dụng lưu trữ và tăng hiệu quả huấn luyện, cả chiến lược Zero [45] và phương pháp prefetching tham số đều được thực hiện. Tuy nhiên, cần xem xét tuổi thọ giảm và hiệu suất giảm của SSD khi gần đạt dung lượng tối đa [46]. Hơn nữa, cách tiếp cận prefetching hiện tại của DeepSpeed không phù hợp với tính không đồng nhất của các tham số cụ thể cho thiết kế Mixture-of-Experts. MoESys giải quyết những vấn đề này bằng cách giới thiệu một kỹ thuật lập lịch prefetching sáng tạo. Phương pháp này nâng cao cả huấn luyện và suy luận bằng cách điều chỉnh theo các thuộc tính riêng biệt của các tham số khác nhau, hiệu quả tận dụng các giải pháp lưu trữ đa tầng để tối ưu hóa hiệu suất hệ thống.

3.1 Thiết kế Tổng thể Kiến trúc

MoESys sử dụng cách tiếp cận hai giai đoạn, cụ thể là giai đoạn huấn luyện và giai đoạn suy luận, như minh họa trong Hình 1. Trong giai đoạn huấn luyện, các mô hình quy mô lớn được huấn luyện offline sử dụng nhiều chiến lược khác nhau. Khi đạt được sự hội tụ mô hình, các tham số được lưu để sử dụng trong tương lai. Mặt khác, giai đoạn suy luận liên quan đến việc triển khai mô hình được huấn luyện lên đám mây thông qua các hoạt động tối ưu hóa đồ thị và cắt tỉa. Việc triển khai này tạo thuận lợi cho các dịch vụ truy vấn tiện lợi cho người dùng.

3.2 Giai đoạn Huấn luyện

Để nâng cao hiệu quả huấn luyện MoE và giải quyết các vấn đề liên quan đến Solid-State Drives (SSD) và lập lịch trong bối cảnh huấn luyện các mô hình quy mô lớn, một cách tiếp cận mới đã được giới thiệu [47]. Trong phương pháp này, các tham số mô hình MoE được chia thành hai loại theo đặc tính kích hoạt của chúng. Các tham số trong loại đầu tiên được kích hoạt thưa thớt trong quá trình huấn luyện, như những tham số trong lớp switching feed-forward network (FFN), trong khi loại thứ hai bao gồm các tham số được kích hoạt dày đặc, như những tham số trong lớp multi-head attention. Cho rằng các tham số thưa thớt, tạo thành một phần đáng kể của mô hình MoE, có thể vượt quá khả năng lưu trữ GPU, MoESys đã cơ cấu lại kiến trúc hệ thống huấn luyện MoE, như thể hiện trong Hình 2. Việc cơ cấu lại này sử dụng nhiều phương tiện lưu trữ khác nhau để đáp ứng nhu cầu bộ nhớ của cả tham số thưa thớt và dày đặc.

Để chống lại các vấn đề hiệu suất phát sinh từ việc truyền dữ liệu qua các loại lưu trữ khác nhau, một kỹ thuật mới được gọi là lập lịch prefetch 2D đã được thực hiện. Các phần sau sẽ đi sâu vào thảo luận toàn diện về khung huấn luyện của chúng tôi, tập trung cụ thể vào hai thành phần chính: Lưu trữ Phân cấp và Lập lịch Prefetch 2D.

3.2.1 Lưu trữ Phân cấp

Trong bối cảnh của các mô hình Mixture-of-Experts (MoE) quy mô lớn, quy mô tham số ngày càng tăng đã dẫn đến việc lưu trữ trở thành một nút cổ chai đáng kể trong huấn luyện mô hình. Thông thường, các trạng thái tham số được lưu trữ bao gồm ba thành phần: tham số có thể huấn luyện, gradient tham số và các trạng thái tối ưu hóa tương ứng. Xem xét các phương tiện lưu trữ khác nhau có sẵn, các thiết bị lưu trữ có thể được phân loại thành ba loại: GPU-Node, CPU-Node và SSD-Node. Vì các tham số dày đặc được sử dụng rộng rãi để tính toán và không chiếm phần lớn không gian lưu trữ, các trạng thái tham số của chúng được lưu trữ độc quyền trên GPU-Node để giảm thiểu di chuyển dữ liệu. Ngược lại, các tham số thưa thớt, được kích hoạt có chọn lọc trong quá trình huấn luyện và tiêu thụ một lượng không gian lưu trữ đáng kể so với các tham số dày đặc, có các trạng thái tham số của chúng được lưu trữ trên SSD-Node và được chuyển đến GPU-Node khi cần thiết cho tính toán. Bằng cách phân bổ chiến lược các trạng thái tham số tương ứng cho lưu trữ phân cấp dựa trên các đặc tính tính toán và lưu trữ của tham số, khả năng lưu trữ của các thiết bị có thể được tận dụng tối đa.

Dựa trên các ràng buộc do các nút lưu trữ đặt ra, công trình này giới thiệu một bộ công thức lý thuyết để diễn tả mối tương quan giữa các thiết bị lưu trữ khác nhau và yêu cầu lưu trữ của các trạng thái tham số khi sử dụng bộ tối ưu hóa ADAM [48]. Thông thường, mỗi thiết bị lưu trữ được cấu hình với tám GPU. Chúng tôi ký hiệu tổng số tham số dày đặc và thưa thớt lần lượt là D và S, và L là tổng số lớp MoE. Dung lượng bộ nhớ SSD, bộ nhớ CPU và bộ nhớ GPU trong một thiết bị đơn lẻ được biểu diễn bởi MSSD, MCPU và MGPU, theo thứ tự đó. Hơn nữa, N biểu thị số lượng thiết bị.

Chúng tôi cũng giới thiệu một biến, α, để định lượng khả năng kích hoạt của các tham số thưa thớt trong quá trình huấn luyện, với α nằm trong khoảng từ 0 đến 1.

Đối với GPU-Node, nó lưu trữ các trạng thái tham số dày đặc được sử dụng trong lan truyền tiến (FWD), lan truyền ngược (BWD) và cập nhật tham số. Điều này bao gồm các tham số như param fp16, grad fp16, master param fp32, momentum fp32, variance fp32, với tổng kích thước 2D + 2D + 4D + 4D + 4D = 16D byte. Hơn nữa, nó chứa các tham số thưa thớt và gradient tương ứng, với kích thước 4αS/L byte, chiếm kích hoạt có chọn lọc của các tham số thưa thớt. CPU-Node phục vụ như một bộ đệm để giữ các trạng thái tham số thưa thớt tần suất cao, chiếm 16αS byte. Cuối cùng, SSD-Node lưu trữ tất cả các trạng thái tham số thưa thớt trên thiết bị, bao gồm master param fp32, momentum fp32 và variance fp32, với kích thước 12S byte.

GPU-Node: 16D + 4αS/L ≤ MGPU·N
CPU-Node: 16αS ≤ MCPU·N
SSD-Node: 12S ≤ MSSD·N (1)

Quy mô của toàn bộ mô hình MoE:
P = S + D (2)

Cơ chế lưu trữ cho các tham số thưa thớt thường liên quan đến việc lưu chúng trên SSD. Tuy nhiên, SSD gặp phải các hạn chế do phương tiện flash của chúng, băng thông PCIe hạn chế và các ràng buộc của giao thức NVMe. Những yếu tố này góp phần tăng độ trễ và một số lần xóa hạn chế, gây ra thách thức trong các tình huống huấn luyện MoE đòi hỏi các hoạt động ghi thường xuyên. Để giải quyết những thách thức này, chúng tôi hướng sự tập trung của mình đến Intel Optane Persistent Memory (Optane PMem) [49], một phương tiện lưu trữ sáng tạo kết hợp các lợi ích của việc định địa chỉ cấp byte, tương tự như DRAM, với khả năng lưu trữ dài hạn của SSD. Optane PMem được kết nối với bộ điều khiển bộ nhớ tích hợp (IMC) của CPU thông qua giao diện DIMM (Dual Inline Memory Module) và giao tiếp bằng DDR-T, một giao thức được phát triển cho giao diện điện/cơ khí của DDR4. Cấu hình này cho phép định địa chỉ cấp byte thông qua các lệnh CPU, nâng cao băng thông và giảm độ trễ. Đáng kể, Optane PMem hoạt động trong hai chế độ: chế độ bộ nhớ và chế độ AppDirect. Đối với yêu cầu cụ thể của chúng tôi về việc lưu trữ các tệp tham số trên Optane PMem, chúng tôi chọn chế độ AppDirect và đặt namespace thành FSDAX. Bằng cách khai thác các tính năng của Ext4, các hoạt động tải và lưu trữ trực tiếp là có thể, vượt qua cả bộ đệm trang của CPU và kernel, điều này tạo thuận lợi cho việc truyền dữ liệu liền mạch không có gián đoạn hoặc chuyển đổi ngữ cảnh.

3.2.2 Lập lịch Prefetch 2D

Việc thực hiện lưu trữ phân cấp để bảo tồn cả trạng thái tham số thưa thớt và dày đặc trong huấn luyện MoE đưa ra chi phí thời gian đáng kể do sự cần thiết phải truyền các trạng thái này qua các thiết bị khác nhau. Để giảm thiểu điều này, một chiến lược lập lịch prefetch 2D được đề xuất, cho phép xử lý đồng thời các lịch trình dày đặc và thưa thớt trong quá trình huấn luyện MoE. Chiến lược này tạo thuận lợi cho tính toán đồng thời của các tham số với quy trình lập lịch.

Chi tiết hơn, chiến lược này, đặc biệt khi được áp dụng cho tập con tham số dày đặc như được định nghĩa bởi chiến lược ZeRO-3, cho phép prefetching toàn bộ tập tham số dày đặc sau giao tiếp inter-rank dọc theo trục ngang, sử dụng tốc độ truyền nhanh của NVLink. Cách tiếp cận này có vai trò quan trọng trong việc đạt được song song dữ liệu, như được chứng minh trong Thuật toán 1. Trong phương pháp này, prefetching xảy ra cùng với các quy trình tính toán và giao tiếp của lớp hiện tại. Cụ thể hơn, trong khi lớp thứ i trải qua tính toán và giao tiếp, lập lịch prefetch cho các tham số của lớp thứ (i+1) được thực hiện song song. Cách tiếp cận prefetching đồng thời này đảm bảo sự sẵn sàng của các tham số cho lớp tiếp theo khi cần thiết, giảm đáng kể thời gian nhàn rỗi và tăng hiệu quả tính toán tổng thể.

Thuật toán 1: Lập lịch trên Tham số Dày đặc
1 d'i: Các lát trạng thái tham số dày đặc trong lớp thứ i
2 di: tổng tham số dày đặc trong lớp thứ i
3 Hàm DenseSchedule(i):
4 Lấy tham số dày đặc trong lớp thứ i dslice
5 d = AllGather(d'i)
6 Kết thúc Hàm

Theo cách tương tự, việc prefetching các tham số thưa thớt diễn ra thông qua băng thông PCIe trong chiều dọc của thiết bị. Cho rằng các tham số thưa thớt được lưu trữ trong SSD, chúng tôi giảm thiểu truy cập đến SSD cho các trạng thái tham số thưa thớt bằng cách thực hiện cơ chế bộ đệm trong bộ nhớ CPU, tương tự như cơ chế LFU (Least Frequently Used) [50]. Bộ đệm CPU chịu trách nhiệm lưu trữ các trạng thái tham số thưa thớt được kích hoạt có chọn lọc được sử dụng trong tính toán FWD/BWD và cập nhật tham số. Khi nhận được yêu cầu prefetch, nó được ưu tiên để lấy các tham số thưa thớt được yêu cầu từ bộ đệm CPU. Nếu những tham số này không được tìm thấy trong bộ đệm CPU, chúng sau đó được lấy từ SSD. Hơn nữa, khi bộ đệm CPU trở nên đầy hoặc khi đạt đến chu kỳ cập nhật tham số thưa thớt, các trạng thái tham số thưa thớt từ bộ đệm CPU được sử dụng để cập nhật các trạng thái tham số tương ứng trên SSD.

Vì bộ nhớ CPU trên mỗi máy chỉ lưu trữ các tham số thưa thớt được kích hoạt thường xuyên, chúng tôi chỉ cần prefetch các tham số của một hoặc nhiều lớp chuyên gia, được lưu trữ trong bộ nhớ CPU, đến bộ nhớ GPU tương ứng trước. Bằng cách prefetch các tham số trước, thời gian chờ đợi cho tính toán có thể được giảm đáng kể.

Từ góc độ toàn cầu, bằng cách sử dụng băng thông của NVLink và PCIe trong hai chiều, chúng tôi có thể đồng thời prefetch các tham số dày đặc và thưa thớt, hiệu quả giảm khoảng cách lập lịch gây ra bởi lưu trữ không đồng nhất và tăng cường đáng kể hiệu quả huấn luyện. Trong các phần sau, chúng tôi trình bày giải thích chi tiết về cơ chế bộ đệm CPU, như được mô tả trong Thuật toán 2. Ngoài ra, chúng tôi duy trì thông tin hit lịch sử cho mỗi tham số thưa thớt, được ghi lại trong một bảng băm được gọi là hits.

Thuật toán 2: Lập lịch trên Tham số Thưa thớt
1 Tham số:
2 ps: trạng thái tham số thưa thớt trong lớp thứ i
3 caches_cpu: bộ đệm CPU
4 CPU_size: dung lượng tối đa của bộ đệm CPU để lưu trữ trạng thái tham số thưa thớt
5 hits: tần suất hit cho một tham số thưa thớt cụ thể trong bảng băm
6 threshold: ngưỡng hit
7 β: hệ số suy giảm
8 K: kích thước bước của trung bình di động
9 steps = 0: bước chu kỳ
10 acccaches = 0: bộ đệm tích lũy
11 Hàm SparseSchedule(i):
12 nếu ps trong caches_cpu thì
13 Lấy ps từ caches_cpu
14 hits[ps] += 1
15 nếu không nếu acccaches + 1 < CPU_size thì
16 hits[ps] = 1
17 acccaches += 1
18 Tìm ps từ SSD đến caches_cpu
19 nếu không
20 với mọi pa trong hits thực hiện
21 hita = hits[pa]
22 nếu hita ≥ threshold và min(hits.values()) == hita thì
23 Cập nhật trạng thái của pa trên SSD
24 Xóa trạng thái của pa trong caches_cpu
25 Xóa hits[pa]
26 Tìm ps từ SSD đến caches_cpu
27 steps += 1
28 nếu steps == K thì
29 hits·β ▷ trung bình di động
30 steps = 0
31 ps→GPU ▷ chuyển ps đến GPU tương ứng
32 Kết thúc Hàm

Cụ thể, nếu một tham số ps được yêu cầu và đã được sử dụng trong FWD trước đó, chúng tôi tăng số lần đếm của nó trong bảng hits. Khi bộ đệm CPU đã đạt đến dung lượng tối đa, chúng tôi cập nhật các trạng thái tham số thưa thớt với tần suất hit thấp nhất vượt quá ngưỡng hit.

Trong huấn luyện mô hình MoE, mỗi nút xác định có kích hoạt các chuyên gia của nó trong lần lặp tiếp theo dựa trên kết quả lựa chọn chuyên gia được ghi lại và thông tin chuyên gia được duy trì. Nếu cần kích hoạt, các quyết định tiếp theo được đưa ra dựa trên thông tin hit lịch sử được ghi lại trong bảng băm để xác định có gửi yêu cầu prefetch hay không.

Thứ nhất, để tránh đưa ra các hoạt động CPU bổ sung trước khi gửi yêu cầu prefetch, việc đặt bảng băm ghi lại thông tin hit lịch sử trên GPU Node là cần thiết. Vì mỗi nút chỉ lưu trữ một phần các tham số thưa thớt trong SSD (không phải toàn bộ tập), chỉ cần duy trì thông tin hit lịch sử cho các tham số thưa thớt tương ứng. Cách tiếp cận này phân phối chi phí không gian GPU qua tất cả các nút tính toán, làm cho nó không đáng kể. Thứ hai, quy trình lựa chọn chuyên gia bởi mạng Gate vốn dĩ đòi hỏi giao tiếp All-to-All để đồng bộ hóa kết quả lựa chọn qua mỗi nút trong Expert Parallelism Group. Lập lịch prefetch đơn giản tái sử dụng kết quả của giao tiếp All-to-All này, vì vậy không có hoạt động giao tiếp bổ sung nào được đưa ra. Ngoài ra, độ phức tạp thời gian của bảng băm là O(1), có nghĩa là mỗi hoạt động prefetch liên quan đến tìm kiếm, chèn hoặc xóa có thể được hoàn thành trong thời gian không đổi, do đó không đưa ra chi phí tính toán bổ sung.

Các đặc tính riêng biệt và không can thiệp của các tham số dày đặc và thưa thớt trong mô hình tạo thuận lợi cho việc thực hiện đồng thời các chiến lược prefetch. Cách tiếp cận này tận dụng tối ưu khả năng băng thông của cả NVLink và PCIe. Trong khi GPU đang tham gia vào việc prefetch trạng thái tham số cho lớp sắp tới, nó cũng có thể đồng thời thực hiện tính toán cho lớp hiện tại. Chế độ hoạt động kép này hiệu quả kết hợp các tác vụ tính toán và sự sẵn sàng tham số.

3.3 Giai đoạn Suy luận

Nhiều nghiên cứu [27], [43] đã chứng minh rằng các mô hình Mixture-of-Experts (MoE) thể hiện hiệu quả huấn luyện cao hơn đáng kể so với các mô hình dày đặc. Tuy nhiên, trong quá trình suy luận, sự hiện diện của nhiều tham số, nhiều trong số đó không hiệu quả, đặt ra thách thức về yêu cầu lưu trữ tăng so với các mô hình dày đặc. Chưng cất kiến thức [25], [51]–[53] đã nổi lên như một cách tiếp cận phổ biến để giảm kích thước mô hình trong khi bảo tồn độ chính xác. Trong bối cảnh này, DeepSpeed [39] đã đề xuất kiến trúc Mixture-of-Students (MoS) để nâng cao độ chính xác của các mô hình học sinh. Cụ thể, để đạt được độ trễ thấp và thông lượng cao ở quy mô lớn cho các mô hình MoE, nhiều kỹ thuật song song khác nhau đã được thiết kế [39], bao gồm expert-slicing, expert parallelism, tensor-slicing và các kỹ thuật khác. Tuy nhiên, suy luận của các mô hình MoE ở quy mô chưa từng có thường bỏ qua việc xem xét nhiều thiết bị lưu trữ khi số lượng máy bị hạn chế.

Trong các phần phụ sau, chúng tôi trình bày cách tiếp cận được MoESys áp dụng để đạt được hiệu quả cao trong suốt quá trình huấn luyện và triển khai suy luận. Chúng tôi tối ưu hóa quy trình huấn luyện đồ thị và đề xuất các đổi mới trong kiến trúc suy luận MoE dựa trên bộ nhớ vòng. Kiến trúc này giải quyết thách thức rào cản bộ nhớ và đảm bảo hiệu suất tối ưu ở mức độ lớn nhất có thể.

3.3.1 Tối ưu hóa Đồ thị

Giai đoạn huấn luyện của MoESys kết hợp huấn luyện đồ thị động, cung cấp lợi thế đáng kể về mặt gỡ lỗi và linh hoạt. Ngược lại, để tăng cường ổn định và hiệu quả, các giai đoạn suy luận và triển khai sử dụng đồ thị tĩnh. Hình 3 minh họa quy trình tổng thể của suy luận, bao gồm sáu bước chính:

• Fusion Đồ thị – Đồ thị gốc được hợp nhất với chiến lược phân tán tương ứng để phù hợp với huấn luyện phân tán quy mô siêu lớn. Bước này liên quan đến việc loại bỏ sự dư thừa tham số.

• Chưng cất và Nén – Nhiều chuyên gia trong mạng giáo viên được nén thông qua các kỹ thuật chưng cất và nén, dẫn đến mạng học sinh với ít chuyên gia hơn.

• Chuyển đổi Đồ thị – Đồ thị động được chuyển đổi thành đồ thị tĩnh để kích hoạt các quy trình tối ưu hóa và triển khai tiếp theo. Do giới hạn không gian, chúng tôi giới thiệu chiến lược chi tiết về chuyển đổi trong liên kết bên ngoài.

• Phân đoạn Đồ thị – Dựa trên tài nguyên suy luận có sẵn và yêu cầu cụ thể, một chiến lược phân tán hợp lý được chọn thủ công hoặc tự động để phân vùng đồ thị tĩnh thành nhiều đồ thị con phân tán. Giao tiếp bổ sung được thêm vào khi cần thiết.

• Tối ưu hóa – Các tối ưu hóa Intermediate Representation (IR) Pass có liên quan, như fusion kernel, được áp dụng cho các đồ thị con phân tán để cải thiện thêm hiệu suất suy luận.

• Triển khai – Các đồ thị con được tối ưu hóa được triển khai trên máy chủ để cung cấp dịch vụ hiệu quả và đáng tin cậy.

Quan trọng lưu ý rằng MoESys kết hợp các transformer được tối ưu hóa cao và các kernel liên quan đến MoE. Chúng tôi tận dụng các phương pháp được tối ưu hóa, như Fused Multi-head Attention, đã được sử dụng thành công trong triển khai BERT của NVIDIA cho MLPerf 1.1 [54]. Những tối ưu hóa này hiệu quả giảm thời gian khởi chạy kernel. Đối với mô hình MoE, chúng tôi đã phát triển các kernel độc đáo để cải thiện thời gian truyền H2D/D2H (Host-to-Device/Device-to-Host) bằng cách sử dụng CUDA Pinned Memory và tùy chỉnh giao tiếp AlltoAll. Mục tiêu của chúng tôi là giảm thiểu số lượng chuyển đổi lớp càng nhiều càng tốt. Chi tiết về những tối ưu hóa này và tác động của chúng đến hiệu suất của MoESys được trình bày và thảo luận trong Phần 5.4.

3.3.2 Offloading Bộ nhớ Vòng

Để tạo thuận lợi cho suy luận của các mô hình MoE quy mô lớn với tài nguyên hạn chế, việc sử dụng chiến lược offloading để giải quyết thách thức lưu trữ là cần thiết. Tuy nhiên, tốc độ di chuyển dữ liệu thường trở thành yếu tố hạn chế đối với hiệu suất suy luận. Do đó, nhiều phương pháp nhằm che giấu tác động của di chuyển dữ liệu bằng cách tối đa hóa sự chồng chéo giữa di chuyển dữ liệu và tính toán suy luận, từ đó giảm thời gian chờ đợi cho tính toán. Trong công việc này, chúng tôi đề xuất một chiến lược lập lịch động cho offloading các tham số thưa thớt, cụ thể là các tham số chuyên gia trong mô hình MoE. Mục tiêu là duy trì hiệu suất hiệu quả bằng cách đồng thời di chuyển các tham số từ bộ nhớ CPU trong khi thực hiện tính toán suy luận trong bộ nhớ GPU. Bằng cách chồng chéo những hoạt động này, chúng tôi nhằm giảm thiểu độ trễ tổng thể và nâng cao hiệu quả của quy trình suy luận.

Cấu trúc của mô hình MoE trong giai đoạn suy luận của nó, được minh họa trong Hình 4, chứng minh sự độc lập theo lớp của các tham số, gợi nhớ đến kiến trúc switch transformer [25]. Tính năng thiết kế này cho phép việc sắp xếp lệch của các tác vụ tính toán và offloading, từ đó tạo thuận lợi cho việc thực hiện đồng thời chúng. Xem xét một mô hình suy luận MoE bao gồm N lớp giải mã, các tham số chuyên gia của mỗi lớp được sao chép N lần và lưu trữ trên thiết bị CPU. Đồng thời, các tham số khác, như embedding, được duy trì trong bộ đệm dày đặc của thiết bị GPU. Ngoài ra, K bản sao của các tham số chuyên gia cũng được lưu trữ trong thiết bị GPU.

Như được mô tả trong Hình 5, khi hoàn thành tính toán liên quan đến lớp thứ i, tham số Pi tương ứng trong bộ nhớ GPU có thể được giải phóng. Đồng thời, tham số chuyên gia SK+i của lớp thứ (K+i) có thể được tải không đồng bộ từ bộ nhớ CPU để chiếm không gian trước đó được sử dụng bởi Pi. Quy trình này, được gọi là calculation-released-load, tạo thuận lợi cho việc duy trì một số lượng cố định K bản sao tham số chuyên gia trên thiết bị GPU. Những bản sao này được lưu trữ trong bộ nhớ vòng, từ đó giảm thiểu phân mảnh bộ nhớ. Bằng cách tận dụng các CUDA stream khác biệt, việc tải chuyên gia từ CPU và quy trình tính toán có thể được chồng chéo một phần. Hơn nữa, bằng cách đảm bảo kích thước bộ nhớ vòng đáng kể và kết hợp số lượng lớn hơn các lớp giải mã trong mô hình suy luận MoE, mức độ chồng chéo có thể được tối ưu hóa đáng kể. Để đánh giá hiệu suất suy luận sử dụng cách tiếp cận bộ nhớ vòng, vui lòng tham khảo Phần 5.4.

4 CHIẾN LƯỢC THỰC HIỆN

Kiến trúc đặc biệt của mô hình MoE tạo ra những thách thức vốn có trong cả quy trình huấn luyện và suy luận. Để giải quyết vấn đề mất cân bằng tải gây ra bởi dữ liệu đầu vào không đồng đều, chúng tôi đã thiết kế cách tiếp cận Huấn luyện MoE Đàn hồi. Hơn nữa, nhận thức được sự tham gia đáng kể của giao tiếp cross-machine trong MoE, chúng tôi đã nghiên cứu sâu về các kỹ thuật Giao tiếp nhận thức tài nguyên để nâng cao hiệu quả qua các cụm đa dạng. Cuối cùng, để vượt qua các hạn chế lưu trữ phát sinh từ việc sử dụng từ vựng quá khổ trong các tác vụ khác nhau, chúng tôi đã phát triển và thực hiện một phương pháp phân vùng embedding mới trong khung song song dữ liệu, khác biệt với cách tiếp cận được sử dụng trong song song tensor-slicing.

4.1 Huấn luyện MoE Đàn hồi

Mất cân bằng tải ảnh hưởng đáng kể đến hiệu quả huấn luyện, đặc biệt trong các tình huống huấn luyện đa tác vụ sử dụng khung MoE. Một ví dụ nổi bật về điều này được thấy trong tác vụ UFO, trong đó khối lượng dữ liệu đầu vào khác biệt qua các tác vụ khác nhau dẫn đến thời gian tính toán không đồng đều, từ đó làm trầm trọng thêm sự mất cân bằng tải. Sự khác biệt này biểu hiện dưới hai hình thức chính: một, việc vi phạm giới hạn dung lượng bộ nhớ do việc xử lý kích thước batch không tương xứng lớn bởi các nút tác vụ cá nhân, hệ quả của việc tổng hợp dữ liệu từ các nút khác; và hai, sự chậm trễ của giao tiếp đồng bộ gây ra bởi sự chậm trễ của nút chậm nhất. Hiện tượng này, được gọi là "Hiệu ứng Thùng" [55], dẫn đến giảm hiệu quả tính toán.

Để giải quyết những thách thức này, chúng tôi thực hiện phương pháp huấn luyện MoE đàn hồi, điều chỉnh động số lượng nút huấn luyện để đảm bảo cân bằng tải dựa trên khối lượng công việc ước tính của mỗi tác vụ. Về mặt thực tế, đối với các tác vụ nhẹ hơn, việc kết hợp nhiều nút chứng tỏ hiệu quả tài nguyên hơn, miễn là dung lượng lưu trữ không phải là yếu tố hạn chế (xem Hình 6b). Ngược lại, đối với các tác vụ nặng hơn, chúng tôi giới thiệu các nút bổ sung để phân phối khối lượng công việc qua nhiều tài nguyên tính toán hơn. Đồng thời, chúng tôi phân vùng dữ liệu đầu vào của các tác vụ nặng để đạt được cân bằng tải và sử dụng song song dữ liệu để đảm bảo đồng bộ hóa tham số (xem Hình 6c). Những cách tiếp cận huấn luyện đàn hồi này hiệu quả giảm thiểu suy giảm hiệu suất do mất cân bằng tải. Bạn có thể tìm thấy so sánh hiệu suất chi tiết trong Phần 5.3.

Trong huấn luyện đàn hồi, mất cân bằng tải thường dẫn đến các tình huống trong đó một số thiết bị nhàn rỗi, chờ đợi những thiết bị khác hoàn thành tính toán của chúng, tạo ra cái được gọi là "bong bóng". Những bong bóng này không chỉ giảm hiệu quả tính toán mà còn tăng chi phí huấn luyện. Để giải quyết điều này, chúng tôi đã giới thiệu phương pháp mở rộng lên hoặc xuống các thiết bị tính toán một cách động. Phương pháp này nhằm giảm thời gian chờ đợi và tăng cường tỷ lệ sử dụng FLOPS (floating-point operations per second) mỗi thiết bị, từ đó tăng thông lượng (token/s/card) của mỗi thiết bị tính toán. Quyết định mở rộng lên hoặc xuống nên dựa trên yêu cầu huấn luyện cụ thể và các cân nhắc về chi phí:

• Mở rộng lên: Khi cần tăng thông lượng end-to-end tổng thể, chúng tôi thường mở rộng lên bằng cách thêm nhiều thiết bị tính toán hơn. Điều này làm giảm tổng thời gian huấn luyện cho mô hình.

• Mở rộng xuống: Trong các tình huống tài nguyên bị hạn chế và kiểm soát chi phí là quan trọng, chúng tôi chọn mở rộng xuống bằng cách giảm số lượng thiết bị tính toán, từ đó giảm chi phí huấn luyện mô hình.

Bất kể hướng mở rộng nào, cả hai phương pháp đều hiệu quả nâng cao tỷ lệ sử dụng FLOPS, giảm thời gian chờ đợi nhàn rỗi cho các thiết bị tính toán, và giảm tổng thời gian "bong bóng" trong quá trình huấn luyện.

4.2 Giao tiếp Nhận thức Tài nguyên

Trong quy trình huấn luyện và suy luận của mô hình MoE, một khối lượng lớn giao tiếp AlltoAll được yêu cầu giữa các thiết bị trong bối cảnh song song chuyên gia. Quy trình giao tiếp này có tiềm năng trở thành nút cổ chai hiệu suất, vì nhiều thể hiện của giao tiếp AlltoAll có thể tranh giành tài nguyên mạng hữu hạn đồng thời. Khi phân tích cấu trúc mạng trong các cụm điển hình, người ta quan sát thấy rằng tương tác dữ liệu qua các cụm thể hiện tốc độ truyền tương đối chậm hơn so với tương tác trong một cụm đơn. Sự khác biệt này phát sinh từ các yếu tố như đường dẫn thông điệp bị tắc nghẽn và chi phí lưu lượng cao hơn liên quan đến giao tiếp inter-cluster.

Bằng cách sử dụng NVLink, giao tiếp intra-node phát sinh chi phí thời gian và tài nguyên tối thiểu vì nó tránh đi qua bất kỳ Network Interface Card (NIC) hoặc switch nào. Tuy nhiên, giao tiếp inter-node trong một cụm hoặc qua các cụm đòi hỏi đi qua đường dẫn thông điệp tắc nghẽn liên quan đến NIC và switch [56], dẫn đến tăng thời gian tiêu thụ cho lập lịch lưu lượng. Xem xét một mạng bao gồm m cụm và p nút chia sẻ một tập NIC chung trong mỗi cụm. Các leaf switch (LE) và spin switch (SP) được tổ chức thành các nhóm n và m, tương ứng. Như được mô tả trong Hình 7, các leaf switch của nhóm thứ i thiết lập kết nối trực tiếp chỉ với NIC có thứ hạng i từ các cụm khác nhau. Các spin switch tạo thuận lợi cho giao tiếp giữa các leaf switch. Quan trọng lưu ý rằng băng thông của spin switch thấp hơn so với leaf switch. Do đó, việc tối đa hóa việc sử dụng leaf switch cho trao đổi dữ liệu là điều ưu tiên, nhằm cải thiện hiệu suất. Ví dụ, hãy xem xét một tình huống trong đó tất cả GPU0 được kết nối với NIC1 và tất cả GPU7 được kết nối với NICn. Chúng tôi quan sát thấy rằng di chuyển dữ liệu giữa GPU0 của Node1 trong Cluster A và GPU7 của Node2 trong Cluster B đi qua đường dẫn định tuyến switch [LE1, SPq, LE1] như được chỉ ra bởi các đường màu đỏ. Điều này phát sinh chi phí giao tiếp cao hơn và tiềm năng tranh chấp tài nguyên với các tương tác khác. Một cách tiếp cận thay thế liên quan đến quy trình hai bước: đầu tiên, chuyển dữ liệu từ GPU0 sang GPU7 trong Node1 sử dụng NVLink, và sau đó thực hiện giao tiếp cross-cluster giữa cặp NIC tương ứng với thứ hạng 7, mà không đi qua bất kỳ switch nào ngoại trừ LE1. Điều này được mô tả bằng các đường màu xanh. Cách tiếp cận như vậy cho phép tận dụng tối ưu băng thông NVSwitch và nâng cao tối ưu hóa lưu lượng mạng.

Do đó, tốc độ trao đổi dữ liệu giữa các GPU cùng thứ hạng trong một nút vượt trội so với các GPU có thứ hạng khác nhau trong cùng một nút. Để tận dụng các đặc tính của cấu trúc mạng, chúng tôi đề xuất một cách tiếp cận tối ưu hóa cho giao tiếp AlltoAll Phân cấp tính đến tài nguyên có sẵn trong cả huấn luyện và suy luận. Như được mô tả trong Hình 8, để tránh giao tiếp cross-node liên quan đến các GPU có thứ hạng khác nhau, chúng tôi ban đầu sử dụng giao tiếp AlltoAll intra-node thông qua kết nối NVSwitch để thu thập dữ liệu. Sau đó, chúng tôi nhóm các GPU có thứ hạng giống nhau cho giao tiếp AlltoAll inter-node, cho phép giao tiếp qua các máy mà không phát sinh chi phí không cần thiết liên quan đến việc đi qua các kênh khác nhau.

Hơn nữa, cách tiếp cận này nâng cao giao tiếp peer-to-peer giữa các nút bằng một hệ số p, trong đó p biểu thị số lượng GPU trong một nút đơn. Sự gia tăng này trong khả năng giao tiếp inter-node cho phép tận dụng tối ưu băng thông inter-node. Trái ngược với thiết kế AlltoAll của DeepSpeed [39], chủ yếu nhằm giải quyết vấn đề khối lượng giao tiếp per-port nhỏ trong giao tiếp all-to-all thông qua cách tiếp cận phân tầng cho fusion tensor để tạo thuận lợi cho giao tiếp gói lớn hơn, cách tiếp cận của chúng tôi được thiết kế riêng cho cấu trúc mạng của cụm thí nghiệm. Chúng tôi tập trung vào tối đa hóa việc sử dụng kết nối NVLink và giảm thiểu tắc nghẽn mạng. Do đó, cấu trúc AlltoAll Phân cấp của chúng tôi là phản ứng trực tiếp với cấu trúc mạng cụ thể của chúng tôi và sẽ thích ứng nếu cấu trúc của cụm thay đổi. Sự khác biệt này nhấn mạnh việc tùy chỉnh phương pháp của chúng tôi cho kiến trúc phần cứng và mạng cụ thể, khác với cách tiếp cận tổng quát hơn được DeepSpeed áp dụng.

4.3 Phân vùng Embedding trong Song song Dữ liệu

Trong bối cảnh huấn luyện mô hình quy mô siêu lớn, bảng embedding thường tạo thành tập tham số lớn nhất trong toàn bộ mô hình, từ đó đòi hỏi các hạn chế về lưu trữ do quy mô của mô hình. Nhiều nghiên cứu đã tập trung vào việc nghiên cứu các kỹ thuật phân vùng embedding. Ví dụ, Megatron [40] đã thành công sử dụng phân vùng theo cột của bảng embedding trong song song tensor-slicing để giảm yêu cầu bộ nhớ huấn luyện. Ngoài ra, EmbRace [57] đã đề xuất cách tiếp cận phân vùng theo cột trong bảng embedding để đạt được giao tiếp cân bằng hơn. Tuy nhiên, một phương pháp xử lý hiệu quả để xử lý phân vùng embedding trong các tình huống mà dữ liệu đầu vào cho mỗi quy trình không nhất quán vẫn khó nắm bắt. Trong những trường hợp như vậy, việc đảm bảo xử lý hiệu quả trở nên thách thức do bản chất đa dạng của đầu vào qua các thiết bị khác nhau.

Trong nghiên cứu này, trọng tâm chính của chúng tôi là giải quyết thách thức phân vùng embedding trong bối cảnh song song dữ liệu, như minh họa trong Hình 9. Để làm rõ, khi chúng tôi xem xét một bảng embedding với kích thước [V, H] được phân phối giữa N quy trình huấn luyện, chúng tôi sử dụng sơ đồ phân vùng theo cột. Sơ đồ này gán một phần [V, H/N] cho mỗi worker. Do đó, mỗi quy trình huấn luyện có một biểu diễn embedding chỉ liên quan đến một tập con của từ vựng. Kết quả là, trước khi truy vấn bảng embedding, dữ liệu đầu vào của mỗi quy trình phải được trao đổi với các quy trình khác thông qua giao tiếp AlltoAll. Việc trao đổi này cho phép thu được kết quả embedding tương ứng với từ vựng một phần cục bộ. Sau đó, để có được kết quả chính xác cho dữ liệu đầu vào được xử lý bởi mỗi worker, kết quả embedding được trao đổi một lần nữa thông qua giao tiếp AlltoAll, hiệu quả phục vụ như quy trình nghịch đảo của bước giao tiếp trước đó. Cần lưu ý rằng trong giai đoạn ngược, thông tin gradient cũng cần được trao đổi để phục hồi gradient bảng embedding.

Trái ngược với cách tiếp cận phân chia embedding truyền thống [58], tương tự như song song mô hình tensor và phân vùng dọc theo chiều từ vựng, phương pháp của chúng tôi được thiết kế cụ thể cho song song dữ liệu. Cho rằng mỗi card GPU xử lý dữ liệu khác nhau, việc phân chia dọc theo chiều từ vựng không khả thi. Thay vào đó, chúng tôi sử dụng phân chia dọc theo chiều hidden_size của từ vựng, đảm bảo rằng mỗi thiết bị tính toán có thể truy cập từ vựng hoàn chỉnh. Ngoài ra, chúng tôi sử dụng giao tiếp AlltoAll để hoàn thành lớp ẩn, phù hợp với các đầu vào dữ liệu khác nhau qua các thiết bị.

Đáng kể, cách tiếp cận này hiệu quả giảm yêu cầu lưu trữ của bảng embedding trong khung song song dữ liệu. Nó đạt được điều này bằng cách chỉ giới thiệu ba thể hiện của giao tiếp AlltoAll và loại bỏ nhu cầu đồng bộ hóa AllReduce cho gradient bảng embedding trong song song dữ liệu.

5 THÍ NGHIỆM

Trong phần này, chúng tôi trình bày đánh giá thí nghiệm toàn diện về các mô hình MoE sử dụng hệ thống MoESys được đề xuất. Đánh giá của chúng tôi tập trung vào cả khía cạnh huấn luyện và suy luận của các mô hình MoE. Trong giai đoạn huấn luyện, chúng tôi đánh giá hiệu quả của mô hình GPT dựa trên MoE qua các cấu hình khác nhau. Đối với giai đoạn suy luận, chúng tôi phân tích hiệu suất của chiến lược offloading dựa trên bộ nhớ vòng, xem xét các kích thước mô hình khác nhau. Hơn nữa, chúng tôi điều tra một số phương pháp hiệu quả được thực hiện trong hệ thống MoESys, bao gồm mô hình UFO được sử dụng rộng rãi. Quan trọng lưu ý rằng báo cáo kết quả của chúng tôi bỏ qua hiệu suất của các mô hình cá nhân. Điều này bởi vì các mô hình hội tụ, dù dựa trên MoE cơ sở hoặc sử dụng MoESys, đều đạt được mức hiệu suất tương đương. Do đó, đánh giá của chúng tôi chủ yếu tập trung vào hiệu lực và hiệu quả của hệ thống MoESys được đề xuất.

5.1 Nền tảng

MoESys được thực hiện dựa trên kiến trúc PaddleFleetX [59] của PaddlePaddle. Sau một số tối ưu hóa hiệu suất cơ bản, PaddleFleetX chứng minh những lợi thế hiệu suất nhất định so với các mô hình tiêu chuẩn khác. Để so sánh trực tiếp hơn, chúng tôi muốn nhấn mạnh so sánh hiệu suất giữa Paddle và Megatron tính đến tháng 3 năm 2023. So sánh này cung cấp hiểu biết có giá trị về cách PaddleFleetX, và qua đó MoESys, đứng so với các khung nổi bật khác về hiệu quả và hiệu suất. Vui lòng tham khảo Bảng 1 để biết dữ liệu so sánh chi tiết. Dữ liệu thí nghiệm chứng minh hiệu suất cải thiện của PaddleFleetX so với Megatron-LM qua một số cấu hình mô hình, với ưu thế thông lượng đáng chú ý từ 14.2% đáng kể cho các mô hình nhỏ hơn (0.35 tỷ tham số) đến 0.4% biên cho các mô hình 175 tỷ tham số rộng lớn. Thông lượng cải thiện này nhất quán mặc dù có sự khác biệt biên về sử dụng bộ nhớ mà Megatron-LM thỉnh thoảng dẫn đầu, đặc biệt với các mô hình nhỏ hơn. PaddleFleetX cũng thể hiện việc sử dụng tài nguyên GPU hiệu quả hơn, bằng chứng bởi TFLOPS/s cao hơn mỗi GPU và cách tiếp cận gần hơn với việc sử dụng FLOP/s lý thuyết đỉnh qua các kích thước mô hình khác nhau.

5.2 Huấn luyện MoE Quy mô Lớn

Chúng tôi tiến hành thí nghiệm huấn luyện sử dụng các mô hình GPT dựa trên kiến trúc MoE, tận dụng GPU A100 (80 GB), và sử dụng sự kết hợp của song song dữ liệu và song song chuyên gia. Trong hệ thống MoE, có hai phần chính: tham số Dày đặc (Backbone) và tham số Thưa thớt (Expert). Phần Dày đặc sử dụng song song dữ liệu, có nghĩa là dữ liệu đầu vào khác nhau được xử lý song song. Sau khi tính toán ngược hoàn thành, các tham số Dày đặc được đồng bộ hóa thông qua giao tiếp Allreduce. Mặt khác, phần tham số Thưa thớt liên quan đến song song chuyên gia. Ở đây, giao tiếp định tuyến giữa các chuyên gia được sử dụng để gửi dữ liệu cần thiết đến các thiết bị tính toán được chỉ định. Điều này được thực hiện thông qua giao tiếp AlltoAll. Lần truyền ngược cũng sử dụng giao tiếp AlltoAll để đồng bộ hóa gradient. Cách tiếp cận này cho phép chúng tôi tận dụng hiệu quả cả song song dữ liệu và chuyên gia, tối ưu hóa hiệu suất của hệ thống MoE. Đánh giá các mô hình này được thực hiện sử dụng các thước đo Gshard [60] và top1-gating. Cụ thể, chúng tôi sử dụng độ chính xác fp16 thuần túy và bộ tối ưu hóa AdamW [61] trong quá trình huấn luyện.

So với phương pháp FP32 độ chính xác cao, có hai cách tiếp cận huấn luyện độ chính xác thấp hơn: "AMP" (Automatic Mixed Precision) và "pure fp16". Không giống như AMP, pure fp16 là phương pháp huấn luyện đã được thiết lập, nhanh hơn. Phương pháp này được thiết lập tốt và đã được áp dụng trong huấn luyện mô hình, ví dụ trong Megatron và DeepSpeed. Cách tiếp cận của chúng tôi phù hợp với các phương pháp được sử dụng trong Megatron/DeepSpeed.

Thuật ngữ "pure" trong "pure fp16" đối lập với AMP, có nghĩa là tất cả tham số mô hình trong huấn luyện pure fp16 đều thuộc loại fp16. Tuy nhiên, quan trọng lưu ý rằng không phải tất cả hoạt động đều được tính toán trong fp16. Các hoạt động nhất định, như softmax, sử dụng fp32 để tính toán. Chúng tôi cũng sử dụng các kỹ thuật như MasterWeight để giảm thiểu tác động của huấn luyện độ chính xác thấp hơn đến độ chính xác mô hình. Việc sử dụng pure fp16 trong các thí nghiệm của chúng tôi là một thực hành tiêu chuẩn, và chúng tôi duy trì chiến lược này một cách nhất quán khi so sánh với các khung khác.

Bảng 2 trình bày kết quả thông lượng cho các cấu hình khác nhau. Bảng liệt kê kích thước tham số (Parameters (B) tính bằng tỷ) theo thứ tự tăng dần từ trên xuống dưới, trong khi giữ số lượng attention head (Attention heads), kích thước lớp ẩn (Hidden size), kích thước từ vựng (Vocab size), và số lượng lớp (Layers) không đổi. Do đó, số lượng chuyên gia (Experts), GPU (GPUs), và kích thước batch (Batch size) tăng gấp đôi. Cả DeepSpeed và MoESys được đề xuất của chúng tôi đều thể hiện tốc độ huấn luyện tăng gấp đôi tương ứng. Đáng chú ý là dòng đầu tiên của bảng đại diện cho hiệu suất trên một nút đơn được trang bị tám GPU, trong khi các dòng tiếp theo mô tả kết quả cho các tình huống đa nút.

Quan sát của chúng tôi chỉ ra rằng, so với hệ thống MoE tiên tiến, DeepSpeed, MoESys đạt được khoảng 28% tăng tốc trong huấn luyện nút đơn và ít nhất 33% tăng tốc trong huấn luyện đa nút cho các mô hình MoE với hơn 100 tỷ tham số. Hơn nữa, MoESys giảm việc sử dụng bộ nhớ GPU của mỗi thứ hạng gần 12 GB. Do đó, trong huấn luyện MoE quy mô lớn, hệ thống MoESys được đề xuất của chúng tôi chứng minh tốc độ huấn luyện tương đương trong khi tiêu thụ bộ nhớ tương đối ít hơn so với mô hình benchmark, DeepSpeed.

5.3 Nghiên cứu Loại bỏ trong Huấn luyện MoE

Đánh giá thí nghiệm trong phần này nhấn mạnh lợi ích của các chiến lược thực hiện hiệu quả trên một mô hình quy mô lớn, như thảo luận trong Phần 4. Mỗi chiến lược này được đánh giá độc lập và so sánh với các phương pháp truyền thống/cơ sở.

5.3.1 Huấn luyện MoE Đàn hồi

Để đánh giá hiệu quả của huấn luyện MoE đàn hồi, chúng tôi tiến hành thí nghiệm sử dụng mô hình UFO [2], dựa trên kiến trúc MoE và được huấn luyện trên GPU A100 với 80 GB bộ nhớ. Chúng tôi thiết kế bốn tác vụ với kích thước batch lần lượt là 512, 256, 128, và 128, để mô phỏng một quy trình huấn luyện mất cân bằng.

Theo phương pháp huấn luyện thưa thớt đàn hồi được nêu trong Phần 4.1, chúng tôi điều chỉnh khối lượng công việc huấn luyện tổng thể bằng cách thêm các nút tính toán bổ sung. Cụ thể, chúng tôi phân bổ 4 GPU cho Task-1 và 2 GPU cho Task-2. Để đảm bảo công bằng, chúng tôi tính toán tốc độ trung bình của mỗi GPU để giảm thiểu tác động của việc tăng số lượng nút. Kết quả, như được trình bày trong Bảng 3, chỉ ra rằng so với cấu hình Load imbalanced, cấu hình Load balanced đạt được cải thiện khoảng 18.2% về thông lượng mỗi GPU. Quan trọng lưu ý rằng cách tiếp cận Load balanced được phát triển từ huấn luyện MoE đàn hồi được thiết kế của chúng tôi, và kết quả nhấn mạnh hiệu quả và hiệu suất của nó, tương ứng.

Hơn nữa, chúng tôi áp dụng cơ chế cân bằng tải MoE dựa trên tác vụ cho huấn luyện mô hình thị giác quy mô tỷ VIMER-UFO 2.0. Cách tiếp cận này hỗ trợ mở rộng động số lượng tác vụ và huấn luyện song song nhiều tác vụ và nhiều chuyên gia. Dưới cùng môi trường thí nghiệm (32x A100 80GB GPU), chúng tôi đạt được hiệu suất huấn luyện 697 hình ảnh mỗi giây. Điều này đại diện cho sự cải thiện đáng kể về thông lượng 64% so với 425 hình ảnh mỗi giây sử dụng khung Pytorch v1.10. Ngoài ra, dung lượng bộ nhớ được giảm xuống 45 GB mỗi GPU, giảm 18%.

5.3.2 Giao tiếp Nhận thức Tài nguyên

Trong phần phụ này, chúng tôi tiến hành huấn luyện các mô hình MoE trên số lượng nút khác nhau và với kích thước mô hình khác nhau để chứng minh lợi ích tiềm năng của các thiết kế MoESys. Kết quả được trình bày sử dụng biểu đồ thanh chồng, minh họa thời gian tiêu thụ cho các thành phần chính của quy trình huấn luyện: 1) giai đoạn tiến (FWD), 2) giai đoạn lùi (BWD), 3) giai đoạn tối ưu hóa (OPT), và 4) giai đoạn giao tiếp. Cụ thể, chúng tôi so sánh cách tiếp cận Hierarchical AlltoAll được đề xuất với phương pháp AlltoAll cơ sở trong giai đoạn giao tiếp, trong khi giữ các thành phần khác không đổi qua các thiết lập khác nhau.

Như được mô tả trong Hình 10, ba thành phần đầu tiên (thanh xanh và hồng) cho thấy hiệu suất tương tự giữa AlltoAll cơ sở và Hierarchical AlltoAll qua tất cả kích thước tham số. Khoảng cách hiệu suất chủ yếu được gây ra bởi hai thanh trên cùng (màu tím cho AlltoAll cơ sở và màu vàng cho Hierarchical AlltoAll) đại diện cho giai đoạn giao tiếp. Rõ ràng rằng với việc áp dụng Hierarchical AlltoAll trong MoESys, thời gian tính toán không tăng đáng kể, trong khi thời gian giao tiếp giảm đáng kể. Hơn nữa, khi kích thước tham số mô hình tăng, khoảng cách hiệu quả trong giao tiếp trở nên đáng kể hơn giữa AlltoAll và hierarchical AlltoAll. Điều này chỉ ra rằng MoESys được đề xuất có thể khuếch đại cải thiện hiệu suất trong giao tiếp khi xử lý các mô hình quy mô lớn. Hiệu ứng này rõ ràng bằng cách quan sát khoảng cách giữa thanh tím và vàng từ trái sang phải dọc theo trục ngang.

Đối với cải thiện đáng kể nhất quan sát được trong thí nghiệm, liên quan đến mô hình MoE với 80.7 tỷ tham số qua bốn nút với 32 GPU, hiệu suất huấn luyện end-to-end tổng thể cải thiện 10.3%. Ngoài ra, giai đoạn giao tiếp đạt được 15.5% tăng tốc sử dụng chiến lược Hierarchical AlltoAll.

5.3.3 Phân vùng Embedding trong Song song Dữ liệu

Để đánh giá hiệu suất của phân vùng embedding trong song song dữ liệu, chúng tôi tiến hành huấn luyện mô hình MoE trên tập dữ liệu với từ vựng cực lớn. Nghiên cứu loại bỏ bao gồm cách tiếp cận cơ sở sử dụng chiến lược embedding không phân đoạn, trong khi các thiết lập còn lại vẫn nhất quán với MoESys. Kết quả trong Bảng 4 chứng minh rằng chiến lược phân vùng embedding trong một máy đơn hiệu quả giảm tiêu thụ bộ nhớ GPU khi xử lý từ vựng kích thước lớn. Nghiên cứu duy trì kích thước batch và số lượng GPU không đổi nhưng thay đổi số lượng chuyên gia, kích thước ẩn và độ lớn tham số. Phân vùng embedding liên tục thể hiện giảm đáng kể trong việc sử dụng bộ nhớ (ví dụ, giảm từ 15.81 GB xuống 8.63 GB với 4 chuyên gia và kích thước ẩn 8192) và cải thiện tốc độ xử lý (như thấy với sự tăng từ 80421 lên 91687 token/s cho cùng cấu hình). Những cải thiện này được duy trì qua các độ phức tạp mô hình khác nhau, được chỉ ra bởi số lượng tham số và chuyên gia khác nhau, cho thấy rằng phân vùng embedding cung cấp giải pháp có thể mở rộng để tối ưu hóa mạng nơ-ron quy mô lớn trong các tình huống song song dữ liệu.

5.3.4 So sánh Chéo

Để xác định chiến lược thống trị và đóng góp của nó vào cải thiện hiệu suất tổng thể trong hệ thống MoESys, chúng tôi tiến hành so sánh chéo giữa các chiến lược được đề xuất. Trong khi chúng tôi đã thể hiện riêng lẻ cải thiện hiệu suất của mỗi chiến lược so với các cơ sở, hiệu suất tương đối giữa chúng đòi hỏi điều tra thêm. Chúng tôi đặc biệt quan tâm đến việc xác định chiến lược thống trị nhất và đóng góp nhiều nhất cho hệ thống MoESys. Để tiến hành so sánh này, chúng tôi thực hiện thí nghiệm đo lường việc sử dụng bộ nhớ đỉnh và tốc độ tính toán trung bình trên GPU song song. Kết quả được tóm tắt trong Bảng 5. Cơ sở đề cập đến MoE không có bất kỳ chiến lược được đề xuất nào, và như mong đợi, nó thể hiện tiêu thụ bộ nhớ đỉnh cao nhất và tốc độ tính toán thấp nhất. Trong số các chiến lược được đề xuất, Elastic Training cho thấy mức độ chiếm dụng bộ nhớ đỉnh ít nhất, trong khi chiến lược Hierarchical AlltoAll (được sử dụng cho giao tiếp nhận thức tài nguyên) tiêu thụ bộ nhớ đỉnh tương đối nhiều hơn. Về tốc độ tính toán GPU, Elastic Training một lần nữa vượt trội so với ba chiến lược khác.

Như thể hiện trực tiếp trong Hình 11, so với tình huống cơ sở không có tối ưu hóa, chiến lược huấn luyện đàn hồi của chúng tôi cho thấy những cải thiện đáng kể nhất về việc sử dụng bộ nhớ đỉnh và tốc độ tính toán GPU, đóng góp lớn cho cải thiện hiệu suất tổng thể. Ngoài ra, chiến lược hierarchical AllToAll nhận thức cấu trúc và chiến lược phân chia DP-Embedding cũng đóng góp vào một tỷ lệ đáng chú ý của cải thiện hiệu suất trong MoESys.

5.4 Suy luận MoE

Các thí nghiệm suy luận bao gồm hai phần: phần đầu tiên đánh giá hiệu suất của hệ thống suy luận MoE sử dụng các mô hình khác nhau với số lượng tham số khác nhau (tính bằng tỷ), trong khi phần thứ hai đánh giá hiệu quả của chiến lược offloading được đề xuất trong Phần 3.3.2.

5.4.1 Hiệu quả

Các tác vụ suy luận thường đòi hỏi ít bộ nhớ hơn so với các tác vụ huấn luyện trong nhiều tình huống. Do đó, việc thực hiện các tác vụ downstream sử dụng một GPU đơn với mô hình MoE 10 tỷ tham số là khả thi. Để đánh giá hiệu suất suy luận của các mô hình MoE quy mô lớn, chúng tôi tiến hành thí nghiệm trên tác vụ tạo văn bản. Kết quả được trình bày trong Bảng 6 chỉ ra rằng MoESys đạt được tốc độ suy luận nhanh hơn khoảng 13% so với DeepSpeed cho các mô hình MoE với hơn 200 tỷ tham số. Cải thiện hiệu suất đáng kể này nhấn mạnh hiệu quả của MoESys trong quy trình suy luận.

5.4.2 Offloading Bộ nhớ Vòng

Chúng tôi tiến hành thí nghiệm để đánh giá hiệu suất suy luận của chiến lược offloading chuyên gia sử dụng bộ nhớ vòng trên hệ thống được trang bị 16 GPU A100(40G). Thí nghiệm tập trung vào mô hình MoE với 32 chuyên gia và 58.2 tỷ tham số. Chúng tôi đo lường thời gian tiêu thụ để tính toán trong bộ nhớ GPU cũng như chuyển động của chuyên gia giữa bộ nhớ CPU và GPU. Hình 12 minh họa kết quả, chỉ ra rằng hiệu suất của hệ thống suy luận MoE chồng chéo phần lớn không bị ảnh hưởng bởi offloading CPU. Các phát hiện chứng minh rằng chiến lược này đạt được sự cân bằng thuận lợi giữa tính toán và di chuyển dữ liệu. Ngoài ra, nó cho phép các hệ thống suy luận MoE giảm tiêu thụ bộ nhớ GPU ít nhất 30% so với suy luận không có offloading bộ nhớ vòng.

5.5 Tóm tắt

Các thí nghiệm nêu trên toàn diện kiểm tra hiệu quả và hiệu lực của các thiết kế được đề xuất trong hệ thống MoESys. Đặc biệt, khi được áp dụng cho các mô hình học sâu quy mô lớn như dòng GPT, MoESys thể hiện hiệu suất vượt trội về tốc độ huấn luyện và suy luận, cũng như tiêu thụ bộ nhớ, so với benchmark đã được thiết lập DeepSpeed. Như một hệ thống MoE có liên quan đến công nghiệp, MoESys được chứng minh là nâng cao thêm việc phát triển các thiết kế MoE phân tán trong các ứng dụng thực tế thế giới thực tế.

6 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI

Để giải quyết nhu cầu của các dịch vụ internet hiện đại đòi hỏi việc sử dụng DNN quy mô lớn, chúng tôi đã trình bày MoESys, một hệ thống huấn luyện và suy luận mới dựa trên MoE giúp tăng hiệu quả trong cả huấn luyện và suy luận quy mô lớn. Hệ thống được đề xuất của chúng tôi áp dụng chiến lược huấn luyện đàn hồi với prefetch 2D và giao tiếp fusion qua lưu trữ phân cấp để có song song hiệu quả. Đối với suy luận có thể mở rộng trong một nút đơn, MoESys xây dựng bộ nhớ CPU-GPU cùng nhau thành một vòng các phần và thực hiện các tác vụ tính toán qua các phần bộ nhớ theo cách round-robin. Các thí nghiệm của chúng tôi chứng minh rằng MoESys đạt được hiệu suất vượt trội so với DeepSpeed tiên tiến, vượt trội DeepSpeed 33% về thông lượng huấn luyện và 13% về thông lượng suy luận, với thông lượng cao hơn 64% và dung lượng bộ nhớ thấp hơn 18% dưới các tác vụ MoE dựa trên mô hình ngôn ngữ lớn và mô hình thị giác nền tảng.

Công việc tương lai của chúng tôi tập trung vào việc phát triển hệ thống huấn luyện và suy luận thưa thớt thống nhất xem xét parameter-server và lập lịch qua nhiều chiều, khám phá các phương pháp hiệu quả cho huấn luyện thưa thớt trong khung MoESys, và nâng cao sự hợp tác hệ thống của chúng tôi với các nền tảng tài nguyên cho nghiên cứu bền vững. Chúng tôi cũng nhằm thực hiện khung đánh giá toàn diện đánh giá chính xác hiệu suất so sánh của các chiến lược tính toán song song đa dạng theo cách công bằng, thông tin và đóng góp mang tính xây dựng cho lĩnh vực kiến trúc học máy có thể mở rộng.
