PATHWAYS: LUỒNG DỮ LIỆU PHÂN TÁN KHÔNG ĐỒNG BỘ CHO ML

Paul Barham1Aakanksha Chowdhery1Jeff Dean1Sanjay Ghemawat1Steven Hand1Dan Hurt1
Michael Isard1Hyeontaek Lim1Ruoming Pang1Sudip Roy1Brennan Saeta1Parker Schuh1
Ryan Sepassi1Laurent El Shafey1Chandramohan A. Thekkath1Yonghui Wu1

TÓM TẮT

Chúng tôi trình bày thiết kế của một lớp điều phối quy mô lớn mới cho các bộ gia tốc. Hệ thống của chúng tôi, PATHWAYS, được thiết kế rõ ràng để cho phép khám phá các ý tưởng nghiên cứu hệ thống và ML mới, đồng thời duy trì hiệu suất tối tân cho các mô hình hiện tại. PATHWAYS sử dụng một đồ thị luồng dữ liệu phân mảnh của các toán tử không đồng bộ tiêu thụ và sản xuất futures, và lập lịch gang hiệu quả các tính toán song song không đồng nhất trên hàng nghìn bộ gia tốc trong khi phối hợp truyền dữ liệu qua các kết nối chuyên dụng của chúng. PATHWAYS sử dụng một thiết kế luồng dữ liệu phân tán không đồng bộ mới cho phép mặt phẳng điều khiển thực thi song song bất chấp các phụ thuộc trong mặt phẳng dữ liệu. Thiết kế này, với kỹ thuật cẩn thận, cho phép PATHWAYS áp dụng mô hình điều khiển đơn giúp biểu đạt các mẫu song song phức tạp mới dễ dàng hơn. Chúng tôi chứng minh rằng PATHWAYS có thể đạt được hiệu suất ngang bằng (100% sử dụng bộ gia tốc) với các hệ thống tối tân khi chạy các tính toán SPMD trên 2048 TPU, đồng thời cũng cung cấp thông lượng tương đương với trường hợp SPMD cho các mô hình Transformer được pipeline qua 16 giai đoạn, hoặc phân mảnh qua hai đảo bộ gia tốc được kết nối qua mạng trung tâm dữ liệu.

1 GIỚI THIỆU

Học sâu đã thấy những thành tựu đáng kể trong thập kỷ qua, qua các lĩnh vực từ hiểu biết hình ảnh (Krizhevsky et al., 2012; He et al., 2016) đến xử lý ngôn ngữ tự nhiên (Devlin et al., 2019; Brown et al., 2020). Sự tiến bộ nhanh chóng gần đây của học máy (ML) này được đặc trưng bởi sự đồng tiến hóa của các mô hình ML, phần cứng gia tốc, và các hệ thống phần mềm liên kết hai thứ này với nhau. Sự đồng tiến hóa này đặt ra nguy cơ rằng các hệ thống trở nên quá chuyên hóa cho các khối lượng công việc hiện tại và không thể đoán trước các nhu cầu tương lai. Trong bài báo này, chúng tôi mô tả PATHWAYS, một hệ thống mới được xây dựng cho ML phân tán. PATHWAYS được thiết kế để nhắm tới các khả năng cụ thể mà chúng tôi tin rằng sẽ cần thiết cho các khối lượng công việc ML tương lai (Dean, 2021) – và do đó cần thiết hôm nay để hỗ trợ nghiên cứu về những khối lượng công việc đó – nhưng được hỗ trợ kém bởi các hệ thống tối tân.

Ví dụ, hầu hết các khối lượng công việc ML tối tân ngày nay sử dụng mô hình "chương trình đơn dữ liệu đa" (SPMD), được lấy cảm hứng từ MPI (Clarke et al., 1994), trong đó tất cả các bộ gia tốc chạy cùng một tính toán theo nhịp và giao tiếp giữa các bộ gia tốc được mô tả bởi các tập hợp như AllReduce. Gần đây, các nhà nghiên cứu đã bắt đầu va phải giới hạn của SPMD cho các tính toán ML. Các mô hình ngôn ngữ rất lớn đã được mở rộng bằng cách sử dụng pipeline thay vì song song hóa dữ liệu thuần túy (Narayanan et al., 2019; Rasley et al., 2020; Narayanan et al., 2021), và các mô hình như Mixture of Experts (MoE) (Shazeer et al., 2017) đã bắt đầu khám phá tính thưa thớt tính toán được biểu đạt tự nhiên nhất bằng cách sử dụng điều khiển luồng tinh vi và tính toán không đồng nhất qua các bộ gia tốc. Các nhà thiết kế hệ thống đã áp dụng các kỹ thuật khéo léo để thực thi các mô hình pipeline (Narayanan et al., 2021; Rasley et al., 2020; Narayanan et al., 2019; Huang et al., 2019) và MoE đồng nhất (Lepikhin et al., 2020; Fedus et al., 2021) trên các hệ thống kiểu MPI, nhưng như chúng tôi lập luận chi tiết sau này, mô hình lập trình MPI quá hạn chế cả cho người dùng và cho hệ thống cơ bản.

Mặt khác, với mỗi thế hệ bộ gia tốc mới, các cụm ML ngày càng trở nên không đồng nhất (Jeon et al., 2019; Chaudhary et al., 2020; Weng et al., 2022). Cung cấp quyền truy cập độc quyền vào các "đảo" lớn của các bộ gia tốc đồng nhất được kết nối qua các kết nối băng thông cao là đắt đỏ, và thường lãng phí vì một chương trình người dùng duy nhất phải cố gắng giữ cho tất cả các bộ gia tốc liên tục bận rộn. Những ràng buộc như vậy đang thúc đẩy thêm các nhà nghiên cứu hướng tới các tính toán "chương trình đa dữ liệu đa" (MPMD) cho phép linh hoạt hơn bằng cách ánh xạ các phần con của tính toán tổng thể tới một tập hợp các đảo bộ gia tốc nhỏ hơn có sẵn dễ dàng hơn. Để tăng sử dụng, một số nhà nghiên cứu quản lý tài nguyên phần cứng ML (Xiao et al., 2020; Bai et al., 2020; Yu and Chowdhury, 2020; Wang et al., 2021; Lim et al., 2021; Zhao et al., 2022; Weng et al., 2022) ghép kênh phần cứng theo cách tinh vi giữa các khối lượng công việc, cho phép tính đàn hồi của khối lượng công việc, và cải thiện khả năng chịu lỗi.

Cuối cùng, các nhà nghiên cứu đang bắt đầu chuẩn hóa một tập hợp các mô hình nền tảng (Bommasani và cộng sự., 2021; Dean, 2021) được đào tạo trên dữ liệu lớn ở quy mô và có thể thích ứng với nhiều nhiệm vụ downstream. Đào tạo và suy luận cho các mô hình như vậy mang lại cơ hội để cải thiện việc sử dụng cụm bằng cách ghép kênh tài nguyên qua nhiều nhiệm vụ, và chia sẻ trạng thái hiệu quả giữa chúng. Ví dụ, một số nhà nghiên cứu có thể đồng thời fine-tune (Houlsby et al., 2019; Zhang et al., 2021) một mô hình nền tảng cho các nhiệm vụ khác nhau, sử dụng cùng các bộ gia tốc để giữ các lớp mô hình nền tảng cố định. Đào tạo hoặc suy luận qua các mô hình con được chia sẻ có thể hưởng lợi từ các kỹ thuật cho phép các ví dụ từ các nhiệm vụ khác nhau được kết hợp trong một batch vector hóa duy nhất để có được sử dụng bộ gia tốc tốt hơn (Crankshaw et al., 2017).

Bài báo này mô tả hệ thống của chúng tôi, PATHWAYS, khớp với chức năng và hiệu suất của các hệ thống ML tối tân, đồng thời cung cấp các khả năng cần thiết để hỗ trợ các khối lượng công việc ML tương lai. PATHWAYS sử dụng kiến trúc client-server cho phép runtime của PATHWAYS thực thi các chương trình trên các đảo tính toán được quản lý bởi hệ thống thay mặt cho nhiều client. PATHWAYS là hệ thống đầu tiên được thiết kế để thực thi một cách minh bạch và hiệu quả các chương trình trải rộng qua nhiều "pod" TPU (Google, 2021), và nó mở rộng tới hàng nghìn bộ gia tốc bằng cách áp dụng một mô hình thực thi luồng dữ liệu mới. Mô hình lập trình của PATHWAYS giúp dễ dàng biểu đạt các tính toán không phải SPMD và cho phép quản lý tài nguyên tập trung và ảo hóa để cải thiện việc sử dụng bộ gia tốc.

Trong phần còn lại của bài báo, trước tiên chúng tôi thảo luận về các giới hạn của các hệ thống ML phân tán hiện tại và động lực cho các lựa chọn thiết kế của chúng tôi cho PATHWAYS (§2), và tiếp theo mô tả mô hình lập trình linh hoạt mà PATHWAYS hỗ trợ (§3). Chúng tôi mô tả kiến trúc của PATHWAYS (§4), nổi bật cách chúng tôi đã giải quyết các giới hạn chính của các hệ thống ML client-server cũ bằng cách sử dụng mô hình luồng dữ liệu phân mảnh và lập lịch gang không đồng bộ. Chúng tôi trình bày cả micro-benchmark và đánh giá end-to-end sử dụng các mô hình ML thực tế chứng minh rằng chúng tôi đã đạt được mục tiêu khớp với hiệu suất của các multi-controller tối tân cho các khối lượng công việc thực tế (§5), và xác thực rằng các cơ chế của PATHWAYS phù hợp để hỗ trợ các tính năng cần thiết cho nghiên cứu và triển khai các phương pháp ML mới và hiệu quả.

2 ĐỘNG LỰC THIẾT KẾ

Các lựa chọn thiết kế của các hệ thống ML phân tán thường được thúc đẩy bởi các thuộc tính của các bộ gia tốc phần cứng đích cơ bản. Chúng tôi giới thiệu độc giả đến Phụ lục A để thảo luận về một số thuộc tính này và cách chúng thường ảnh hưởng đến các hệ thống ML phân tán. Ở đây, chúng tôi tập trung vào cách một số lựa chọn thiết kế và triển khai của các hệ thống ML phân tán hiện tại khiến chúng khó hỗ trợ các mô hình lớn, thưa thớt hoặc không đều.

Các hệ thống ML phân tán để đào tạo các mô hình SPMD tối tân thường áp dụng kiến trúc multi-controller trong đó cùng một tệp thực thi client được chạy trực tiếp trên tất cả các host trong hệ thống, chiếm quyền sở hữu độc quyền các tài nguyên trên những host đó trong suốt thời gian thực thi chương trình. Các ví dụ về kiến trúc này bao gồm MPI (Clarke et al., 1994), PyTorch (Paszke et al., 2019), JAX (Bradbury et al., 2018), và các cấu hình gần đây hơn của TensorFlow (Shazeer et al., 2018; Agrawal et al., 2019). Ưu điểm chính của kiến trúc này là độ trễ thấp để phân phối các tính toán bộ gia tốc (xem Hình 1a) vì một bản sao giống hệt nhau của mã người dùng chạy trên mỗi host bộ gia tốc và việc phân phối chỉ liên quan đến giao tiếp qua các liên kết PCIe (tương đối) nhanh. Tất cả giao tiếp khác qua các host chỉ xảy ra thông qua các tập hợp sử dụng các kết nối chuyên dụng như NVLink (Foley và Danskin, 2017) và ICI (Jouppi et al., 2020) mà không đi qua bộ nhớ host. Tuy nhiên, kiến trúc này không phù hợp với các khối lượng công việc ML hiện đại sử dụng pipeline hoặc tính thưa thớt tính toán. Bất kỳ giao tiếp nào vượt ra ngoài các tập hợp tiêu chuẩn trong các hệ thống multi-controller đều yêu cầu người dùng triển khai các nguyên thủy phối hợp riêng của họ. Cách tiếp cận multi-controller cũng thường giả định quyền sở hữu độc quyền các tài nguyên phần cứng. Điều này không chỉ chuyển trách nhiệm đảm bảo sử dụng cao các bộ gia tốc đắt tiền sang cho người dùng, mà còn làm phức tạp thiết kế các tính năng như ảo hóa tài nguyên và ghép kênh cần thiết để xây dựng cơ sở hạ tầng ML hiệu quả toàn cụm.

Các hệ thống single-controller như TensorFlow v1 (Abadi et al., 2016) cung cấp một mô hình luồng dữ liệu phân tán rất tổng quát, bao gồm điều khiển luồng trong đồ thị được tối ưu hóa (Yu et al., 2018). Một client Python TensorFlow (TF) xây dựng một đồ thị tính toán và giao nó cho một runtime phối hợp, phân vùng đồ thị thành một đồ thị con cho mỗi worker và ủy thác việc thực thi các đồ thị con cho các runtime cục bộ trên các worker. Phối hợp giữa các worker được thực hiện bằng cách sử dụng các cạnh dữ liệu và điều khiển truyền thông điệp qua mạng trung tâm dữ liệu (DCN). Trong khi thiết kế single-controller cung cấp một mô hình lập trình linh hoạt và ảo hóa tài nguyên, nó đặt ra các thách thức triển khai.

Thứ nhất, trong khi các hệ thống multi-controller chỉ yêu cầu giao tiếp qua PCIe để phân phối các tính toán bộ gia tốc (Hình 1a), các client trong các hệ thống single-controller "xa hơn" và độ trễ phân phối liên quan đến giao tiếp qua DCN, thường chậm hơn một bậc so với PCIe (Hình 1b). Thứ hai, để hỗ trợ thực thi đồng thời các chương trình MPMD với các tính toán con SPMD, mỗi chương trình trải rộng một tập con các bộ gia tốc được rút ra từ một cụm chia sẻ, runtime phải có một cơ chế nào đó để hỗ trợ lập lịch gang của các tính toán bộ gia tốc. Lập lịch gang là thiết yếu trong trường hợp TPU, vì chúng là đơn luồng và chỉ chạy các kernel không thể preempt, vì vậy hệ thống sẽ deadlock nếu các tính toán giao tiếp không được enqueue theo thứ tự nhất quán. Thậm chí đối với GPU hoặc các bộ gia tốc khác có thể thực thi các tính toán đồng thời, lập lịch gang cho phép thực thi hiệu quả hơn các tập hợp (Feitelson và Rudolph, 1992). Do đó, các hệ thống single-controller cho ML yêu cầu một cơ chế lập lịch phân tán để sắp xếp thứ tự các tính toán được enqueue thay mặt cho các chương trình khác nhau. Cuối cùng, một hệ thống cho các khối lượng công việc ML hiện đại phải được thiết kế để chạy các tính toán được phân phối qua hàng nghìn bộ gia tốc, với hỗ trợ lớp đầu tiên cho các biểu diễn và cấu trúc dữ liệu được phân mảnh. Ví dụ, một đồ thị luồng dữ liệu ngây thơ biểu diễn một cạnh giữa một tính toán phân mảnh M-way và một tính toán phân mảnh N-way sẽ yêu cầu M+N nút và MN cạnh, nhanh chóng trở nên khó sử dụng.

Các lựa chọn triển khai được thực hiện bởi TF v1 đã quá chuyên hóa để giả định một đảo bộ gia tốc đơn, nhỏ, được sở hữu độc quyền. Sự chuyên hóa quá mức này khiến việc sử dụng TF cho các khối lượng công việc ML đương đại hoặc tương lai trở nên không khả thi về mặt thực tế. Trong khi TF có thể chạy các tính toán yêu cầu phối hợp cross-host hoặc truyền dữ liệu thông qua các op send và recv (Hình 1c), công việc phía host tại đích như phân phối tính toán bộ gia tốc chỉ được kích hoạt sau khi việc truyền hoàn tất. Trong các chương trình liên quan đến nhiều truyền cross-host, ví dụ các mô hình pipeline với một số lượng lớn các giai đoạn, những độ trễ phân phối này tích lũy, dẫn đến việc sử dụng bộ gia tốc không hiệu quả. Trong khi người dùng TF v1 có thể (không hiệu quả) thực thi một thứ tự nhất quán cho lập lịch gang trong một chương trình duy nhất, bằng cách sử dụng các cạnh điều khiển, việc thiếu một scheduler tập trung trong các hệ thống single-controller như TF v1 khiến không thể đảm bảo thứ tự nhất quán giữa các tính toán qua các chương trình. TF cũng cụ thể hóa đồ thị tính toán phân mảnh đầy đủ, điều này tạo ra overhead đáng kể cả trong việc tuần tự hóa đồ thị và thực thi khi số lượng mảnh đạt đến hàng nghìn, dẫn đến hàng triệu cạnh đồ thị giữa các tính toán con.

PATHWAYS kết hợp tính linh hoạt của các framework single-controller với hiệu suất của các multi-controller. Chúng tôi áp dụng mô hình single-controller vì chúng tôi tin rằng nó cung cấp cơ hội tốt hơn nhiều so với multi-controller cho tính toán ML mới và hiệu quả, cả bằng cách khai thác tính thưa thớt tính toán và tính không đồng nhất, và bằng cách cho phép các hệ thống quản lý cụm thúc đẩy chia sẻ và ảo hóa tài nguyên. Thiết kế của chúng tôi khác với các hệ thống ML single-controller cũ ở chỗ nó sử dụng phân phối không đồng bộ để khớp với hiệu suất của các hệ thống multi-controller, hỗ trợ quản lý tài nguyên tập trung và lập lịch với hỗ trợ lớp đầu tiên cho các gang tính toán bộ gia tốc SPMD, và sử dụng một hệ thống luồng dữ liệu phân mảnh để phối hợp hiệu quả.

3 MÔ HÌNH LẬP TRÌNH PATHWAYS

Chúng tôi đã triển khai hỗ trợ để nhắm tới PATHWAYS từ các chương trình nguồn được viết bằng TensorFlow và JAX, nhưng chúng tôi tập trung vào JAX cho đánh giá trong bài báo này. Người dùng JAX có thể wrap rõ ràng mã Python tiêu chuẩn với các decorator để chỉ ra các đoạn nên được biên dịch thành các tính toán XLA (có khả năng SPMD). Những tính toán XLA này thường được đặc trưng bởi các loại và hình dạng input và output đã biết, các vòng lặp có giới hạn, và với ít (nếu có) điều kiện (xem Phụ lục B để biết thêm chi tiết) khiến việc ước tính yêu cầu tài nguyên của các tính toán trước trở nên khả thi. Chúng tôi gọi những tính toán này với yêu cầu tài nguyên đã biết là "compiled functions". Mỗi hàm như vậy ánh xạ tới một nút tính toán (phân mảnh) duy nhất trong một chương trình PATHWAYS.

JAX ngày nay không thể mở rộng vượt ra ngoài một pod TPU duy nhất vì các chương trình JAX chạy trong các cấu hình multi-controller chuyển tất cả dữ liệu bằng cách sử dụng các tập hợp XLA, và những cái này hiện chỉ có sẵn qua ICI trên TPU. PATHWAYS có thể được sử dụng như một thay thế plug-in cho backend JAX, cho phép mã JAX chạy không cần sửa đổi ngoại trừ việc các tính toán SPMD giờ đây có quyền truy cập không chỉ vào các core TPU được kết nối cục bộ, mà còn vào nhiều core được cung cấp trong hệ thống. Và vì PATHWAYS có thể giao tiếp qua cả ICI và DCN, nó cho phép các chương trình JAX mở rộng lần đầu tiên tới nhiều pod TPU, chứa hàng nghìn core TPU.

Khả năng chạy mã JAX không cần sửa đổi là tiện lợi nhưng không khai thác đầy đủ hiệu suất của PATHWAYS. Một người dùng PATHWAYS có thể yêu cầu các tập hợp "virtual devices", với các ràng buộc tùy chọn về các loại thiết bị, vị trí hoặc topo kết nối, và sau đó có thể đặt các compiled functions cụ thể trên những thiết bị đó (Hình 2). Hệ thống sẽ tự động xử lý tất cả việc di chuyển dữ liệu và resharding giữa các tính toán phụ thuộc.

Theo mặc định, chúng tôi chuyển đổi mỗi compiled function thành một chương trình PATHWAYS độc lập chỉ chứa một tính toán (phân mảnh) duy nhất, có nghĩa là nếu người dùng muốn chạy nhiều hàm liên tiếp, một lời gọi Python riêng biệt và RPC từ client tới coordinator là cần thiết cho mỗi hàm. Do đó, chúng tôi cũng đã triển khai một program tracer mới (Hình 2) mà người dùng có thể wrap xung quanh một khối mã Python gọi nhiều compiled functions. Tracer tạo ra một chương trình PATHWAYS duy nhất trong đó mỗi compiled function được biểu diễn bởi một nút tính toán trong một đồ thị luồng dữ liệu.

Triết lý của JAX về việc hỗ trợ các phép biến đổi của mã đã trace là một sự kết hợp tốt cho các hướng nghiên cứu mà chúng tôi muốn khám phá. Ví dụ, JAX có một thư viện đi kèm gọi là FLAX (Heek et al., 2020) được sử dụng để biểu đạt các mô hình DNN nhiều lớp, và chúng tôi đã viết một thư viện tự động chuyển đổi một mô hình FLAX thành một chương trình PATHWAYS pipeline. Ngoài ra, JAX hỗ trợ các phép biến đổi để vector hóa các hàm Python "per-example", tạo ra mã batch hiệu quả, và những phép biến đổi như vậy là một cơ sở tốt để khám phá các hình thức mới của điều khiển luồng vector hóa phụ thuộc dữ liệu, như chúng tôi mô tả ngắn gọn sau này (§6.3).

4 KIẾN TRÚC HỆ THỐNG PATHWAYS

PATHWAYS xây dựng rộng rãi trên các hệ thống trước đó, bao gồm XLA (TensorFlow, 2019) để biểu diễn và thực thi các tính toán TPU, các đồ thị và executor TensorFlow (Abadi et al., 2016) để biểu diễn và thực thi các tính toán CPU phân tán, và các framework lập trình Python bao gồm JAX (Bradbury et al., 2018) và các API TensorFlow. Bằng cách tận dụng những khối xây dựng này, chúng tôi có thể tập trung vào các khía cạnh phối hợp mới của PATHWAYS, trong khi có thể chạy các mô hình ML hiện có với những thay đổi mã tối thiểu.

4.1 Resource Manager

Một backend PATHWAYS bao gồm một tập hợp các bộ gia tốc được nhóm thành các đảo được kết nối chặt chẽ lần lượt được kết nối với nhau qua DCN (Hình 3). PATHWAYS có một "resource manager" chịu trách nhiệm quản lý tập trung các thiết bị qua tất cả các đảo. Một client có thể yêu cầu "virtual slices" của đảo với các hình dạng lưới 2D hoặc 3D cụ thể phù hợp với mẫu giao tiếp của họ. Mỗi virtual slice chứa "virtual devices" cho phép client biểu đạt cách các tính toán được bố trí trên lưới. Resource manager gán động các thiết bị vật lý cho các thiết bị ảo thỏa mãn topo kết nối mong muốn, dung lượng bộ nhớ, v.v.

Triển khai resource manager ban đầu của chúng tôi sử dụng một heuristic đơn giản cố gắng cân bằng tải tĩnh bằng cách phân tán các tính toán qua tất cả các thiết bị có sẵn, và giữ một ánh xạ một-một giữa các thiết bị ảo và vật lý. Nếu các khối lượng công việc tương lai yêu cầu, chúng tôi có thể áp dụng một thuật toán phân bổ tinh vi hơn, ví dụ tính đến yêu cầu tài nguyên của tất cả các tính toán client và trạng thái hiện tại của hệ thống để xấp xỉ một phân bổ tối ưu các thiết bị vật lý cho các tính toán.

PATHWAYS cho phép các tài nguyên tính toán backend được thêm và loại bỏ động, với resource manager theo dõi các thiết bị có sẵn. Lớp gián tiếp giữa các thiết bị ảo và vật lý, như được cho phép bởi thiết kế single-controller của chúng tôi, sẽ cho phép chúng tôi trong tương lai hỗ trợ các tính năng như suspend/resume và migration minh bạch, trong đó các thiết bị ảo của client được tạm thời thu hồi hoặc gán lại mà không cần sự hợp tác từ chương trình người dùng.

4.2 Client

Khi người dùng muốn chạy một chương trình đã trace, nó gọi thư viện client PATHWAYS trước tiên gán các thiết bị ảo cho bất kỳ tính toán nào chưa được chạy trước đó, và đăng ký các tính toán với resource manager, kích hoạt các server biên dịch các tính toán trong nền. Client sau đó xây dựng một biểu diễn trung gian (IR) PATHWAYS bất khả tri vị trí thiết bị cho chương trình, được biểu đạt như một dialect MLIR tùy chỉnh (Lattner et al., 2021). IR được "lower" dần dần qua một loạt các pass compiler tiêu chuẩn, cuối cùng xuất ra một biểu diễn mức thấp bao gồm các vị trí thiết bị vật lý. Chương trình mức thấp này tính đến khả năng kết nối mạng giữa các thiết bị vật lý và bao gồm các hoạt động để chuyển output từ một mảnh tính toán nguồn tới các vị trí của các mảnh đích của nó, bao gồm các hoạt động scatter và gather khi cần trao đổi dữ liệu. Việc chạy lại chương trình mức thấp là hiệu quả trong trường hợp phổ biến mà các vị trí thiết bị ảo không thay đổi, và chương trình có thể được lower lại nếu resource manager thay đổi ánh xạ giữa các thiết bị ảo và vật lý.

Client trong các hệ thống single controller cũ có thể nhanh chóng trở thành một bottleneck hiệu suất khi nó phối hợp hàng nghìn tính toán cá nhân và buffer dữ liệu tương ứng với mỗi mảnh của các tính toán được phân tán qua hàng nghìn bộ gia tốc. Client PATHWAYS sử dụng một trừu tượng buffer phân mảnh để biểu diễn một buffer logic có thể được phân phối qua nhiều thiết bị. Trừu tượng này giúp client mở rộng bằng cách phân bổ chi phí của các tác vụ sổ sách (bao gồm đếm tham chiếu) ở mức độ chi tiết của các buffer logic thay vì các mảnh cá nhân.

4.3 Triển khai phối hợp

PATHWAYS dựa vào PLAQUE cho tất cả phối hợp cross-host sử dụng DCN. PLAQUE là một hệ thống luồng dữ liệu phân mảnh production hiện có (closed-source) được sử dụng tại Google cho nhiều dịch vụ đối mặt khách hàng trong đó giao tiếp high-fanout hoặc high-fanin là cần thiết, và cả khả năng mở rộng và độ trễ đều quan trọng. IR PATHWAYS mức thấp được chuyển đổi trực tiếp thành một chương trình PLAQUE, được biểu diễn như một đồ thị luồng dữ liệu. PATHWAYS có các yêu cầu khắt khe cho substrate phối hợp của nó, tất cả đều được đáp ứng bởi PLAQUE.

Đầu tiên, biểu diễn được sử dụng để mô tả IR PATHWAYS phải chứa một nút duy nhất cho mỗi tính toán phân mảnh, để đảm bảo một biểu diễn compact cho các tính toán trải rộng nhiều mảnh, tức là một thực thi chuỗi của 2 tính toán A và B với N mảnh tính toán mỗi cái nên có 4 nút trong biểu diễn luồng dữ liệu: Arg → Compute(A) → Compute(B) → Result, bất kể lựa chọn N. Trong triển khai runtime PLAQUE, mỗi nút tạo ra các tuple dữ liệu output được gắn thẻ với một mảnh đích, vì vậy khi thực hiện thực thi data-parallel, N tuple dữ liệu sẽ chảy, một giữa mỗi cặp nút IR liền kề.

Runtime phối hợp cũng phải hỗ trợ trao đổi dữ liệu thưa thớt dọc theo các cạnh phân mảnh, trong đó thông điệp có thể được gửi giữa một tập con được chọn động của các mảnh, sử dụng các cơ chế theo dõi tiến trình tiêu chuẩn (Akidau et al., 2013; Murray et al., 2013) để phát hiện khi tất cả thông điệp cho một mảnh đã được nhận. Giao tiếp thưa thớt hiệu quả là một yêu cầu để tránh DCN trở thành bottleneck cho điều khiển luồng phụ thuộc dữ liệu trên các bộ gia tốc, đây là một trong những khả năng chính mà chúng tôi muốn PATHWAYS cho phép.

Substrate phối hợp được sử dụng để gửi các thông điệp DCN nằm trong đường dẫn quan trọng để truyền thông điệp lập lịch và handle dữ liệu (Hình 4), vì vậy nó phải gửi các thông điệp quan trọng với độ trễ thấp, và batch các thông điệp dành cho cùng một host khi cần thông lượng cao.

Cũng thuận tiện khi sử dụng một engine luồng dữ liệu có thể mở rộng, mục đích chung để xử lý giao tiếp DCN, vì điều này có nghĩa là PATHWAYS cũng có thể sử dụng nó cho các tác vụ housekeeping nền như phân phối thông tin cấu hình, giám sát chương trình, dọn dẹp chúng, chuyển lỗi khi thất bại, v.v.

Chúng tôi tin rằng sẽ khả thi để triển khai lại thiết kế PATHWAYS đầy đủ bằng cách sử dụng các framework phân tán khác như Ray (Moritz et al., 2018) thay vì PLAQUE để thực hiện framework phối hợp mức thấp. Trong một triển khai như vậy, các executor và scheduler PATHWAYS sẽ được thay thế bởi các actor Ray chạy lâu dài sẽ triển khai lập lịch PATHWAYS trên lập lịch cụm Ray cơ bản, và các executor có thể sử dụng PyTorch cho tính toán GPU và tập hợp. Một số bổ sung có thể cần thiết để đạt được hiệu suất tương đương (xem §5) vì Ray thiếu, ví dụ, một object store HBM, hoặc các nguyên thủy để chuyển các object remote một cách hiệu quả qua kết nối GPU.

4.4 Phân phối động được lập lịch gang

Như đã thảo luận trước đó (§2), một yêu cầu để hỗ trợ các tính toán SPMD trên một tập hợp bộ gia tốc chia sẻ là hỗ trợ lập lịch gang hiệu quả. Runtime PATHWAYS bao gồm một scheduler tập trung trên mỗi đảo sắp xếp thứ tự nhất quán tất cả các tính toán trong đảo. Khi PATHWAYS enqueue một chương trình để thực thi, chương trình luồng dữ liệu PLAQUE chịu trách nhiệm (i) enqueue việc thực thi các compiled functions cục bộ tại mỗi bộ gia tốc, với các buffer future làm input; (ii) enqueue việc gửi mạng tới các bộ gia tốc từ xa cho các buffer future được output bởi việc thực thi hàm; và (iii) giao tiếp với scheduler để xác định thứ tự nhất quán của việc thực thi hàm qua tất cả các chương trình chạy trên đảo. Scheduler phải triển khai các chính sách để phân bổ bộ gia tốc ở thời gian milliseconds. Triển khai hiện tại của chúng tôi chỉ đơn giản enqueue công việc theo thứ tự FIFO, nhưng các scheduler tinh vi hơn có thể ví dụ sắp xếp lại các tính toán dựa trên thời gian thực thi ước tính.

4.5 Phân phối không đồng bộ song song

Khi chạy các tính toán trên bộ gia tốc, các hệ thống có thể tận dụng các API không đồng bộ để chồng chéo tính toán với phối hợp (Kwon et al., 2020). Xem xét đồ thị ba nút trong Hình 4a, trong đó các hình vuông tương ứng với ba nút A, B, và C chạy trên các bộ gia tốc gắn với host A, B, và C. Tất cả các tính toán nút đều là compiled functions thông thường. Host A enqueue nút A, nhận một future cho output của A, và truyền future tới host B. Host B phân bổ input của B, truyền địa chỉ buffer input tới host A, và thực hiện hầu hết công việc chuẩn bị để khởi chạy hàm của nút B. Khi nút A hoàn thành, output của nó được gửi qua kết nối bộ gia tốc trực tiếp vào buffer input của nút B, và sau đó host B bắt đầu nút B. Độ trễ giữa một nút hoàn thành và nút tiếp theo bắt đầu có thể được làm cho ít hơn thời gian truyền dữ liệu.

Thiết kế trên hoạt động tốt khi tính toán của nút tiền nhiệm mất nhiều thời gian hơn thời gian dành cho lập lịch, phân bổ tài nguyên, và phối hợp giữa các host. Tuy nhiên nếu thời gian tính toán quá ngắn, đây là trường hợp được hiển thị trong hình, pipeline không đồng bộ bị stall và công việc phía host trở thành bottleneck quan trọng để thực thi chuỗi tính toán tổng thể. Cho rằng các compiled functions đều thông thường, hình dạng input của nút kế tiếp trong thực tế có thể được tính toán trước khi tính toán tiền nhiệm thậm chí được enqueue.

Do đó, chúng tôi giới thiệu một thiết kế phân phối không đồng bộ song song mới được hiển thị trong Hình 4b, khai thác việc sử dụng tài nguyên đã biết tĩnh của các compiled functions thông thường để chạy hầu hết công việc phía host cho các nút của một tính toán song song, thay vì tuần tự hóa công việc cho một nút để xảy ra sau khi các tiền nhiệm của nó đã được enqueue. Vì công việc chỉ có thể được lập lịch song song khi các hàm là thông thường, PATHWAYS coi lập lịch song song như một tối ưu hóa và quay về mô hình truyền thống khi yêu cầu tài nguyên của một nút không được biết cho đến khi một tính toán tiền nhiệm đã hoàn thành (ví dụ, do điều khiển luồng phụ thuộc dữ liệu).

Khi một đồ thị con của một tính toán có thể được lập lịch tĩnh, chương trình gửi một thông điệp duy nhất (mô tả toàn bộ đồ thị con) tới scheduler, có thể sắp xếp thứ tự thực thi của tất cả các mảnh hoạt động trong đồ thị con liên tiếp. Việc sử dụng một thông điệp duy nhất được thiết kế để giảm thiểu lưu lượng mạng, nhưng không yêu cầu scheduler thực sự enqueue tất cả các mảnh của đồ thị con như một batch: các tính toán vẫn có thể được xen kẽ với những cái được gửi bởi các chương trình thực thi đồng thời khác. Chúng tôi đánh giá chi phí của các cơ chế phân phối khác nhau trong §5.

4.6 Quản lý dữ liệu

Mỗi host quản lý một object store phân mảnh tương tự như các object store của Ray (Moritz et al., 2018), nhưng được mở rộng để cũng theo dõi các buffer được giữ trong HBM bộ gia tốc tại mỗi mảnh. Các chương trình client có thể giữ tham chiếu tới các object trong bộ nhớ host hoặc bộ gia tốc từ xa, và client và server tham chiếu tới chúng bằng cách sử dụng các handle mờ cho phép hệ thống migrate chúng nếu cần. Các giá trị chương trình trung gian cũng được giữ trong các object store, ví dụ trong khi hệ thống đang chờ chuyển chúng giữa các bộ gia tốc, hoặc truyền chúng tới một tính toán tiếp theo. Các object được gắn thẻ với các nhãn quyền sở hữu để chúng có thể được garbage collect nếu một chương trình hoặc client thất bại. Chúng tôi có thể sử dụng back-pressure đơn giản để stall một tính toán nếu nó không thể phân bổ bộ nhớ vì các buffer của các tính toán khác đang tạm thời chiếm giữ HBM.

5 ĐÁNH GIÁ

Để đánh giá JAX, PATHWAYS, và TensorFlow trên TPU, chúng tôi sử dụng ba cấu hình khác nhau. Cấu hình (A) có 4 TPU trên mỗi host, và instance lớn nhất chúng tôi báo cáo có 512 host, tổng cộng 2048 TPU được kết nối qua ICI. Cấu hình (B) có 8 TPU trên mỗi host, và instance lớn nhất chúng tôi báo cáo có 64 host, và tổng cộng 512 TPU. Cấu hình (C) sử dụng bốn đảo TPU, trong đó mỗi đảo có 4 host và 32 TPU. Chúng tôi ghi chú trong văn bản khi các thí nghiệm sử dụng một tập con TPU của một cấu hình cụ thể.

Khi đánh giá Ray trên GPU, chúng tôi sử dụng Ray v1.3 và PyTorch 1.8.1 chạy trên VM p3.2xlarge với các host được kết nối qua DCN và được lập lịch bằng Amazon placement groups. Chúng tôi chủ yếu so sánh PATHWAYS với JAX multi-controller, vì JAX đã chứng minh hiệu suất tối tân trong các benchmark tiêu chuẩn ngành (Mattson et al., 2020) và chúng tôi có thể dễ dàng chạy JAX và PATHWAYS (PW) trên các cấu hình phần cứng giống hệt nhau. Chúng tôi cũng so sánh với TensorFlow (TF) và Ray trong micro-benchmark, để kiểm tra các khía cạnh cụ thể của hiệu suất hệ thống phân tán của PATHWAYS, và hiển thị hiệu suất pipeline của một mô hình TF chạy trên PATHWAYS.

5.1 Overhead phân phối single-controller

Thí nghiệm đầu tiên của chúng tôi là một micro-benchmark để so sánh overhead của JAX multi-controller với các framework single-controller. Chúng tôi xây dựng các chương trình chạy lặp lại một tính toán được lập lịch gang tầm thường chứa một AllReduce scalar duy nhất theo sau bởi một phép cộng scalar, đưa output của một tính toán làm input của tính toán tiếp theo. Chúng tôi đo thông lượng: số lượng tính toán trên giây thực thi trên các bộ gia tốc. Chúng tôi so sánh ba cách mà mã người dùng có thể enqueue các tính toán:

• OpByOp (-O): Mã người dùng chứa một lời gọi riêng biệt cho mỗi lần thực thi tính toán.
• Chained (-C): Mã người dùng chứa một loạt lời gọi mỗi cái thực thi một chuỗi 128 nút, trong đó mỗi nút thực thi tính toán. Hệ thống thực thi toàn bộ chuỗi để phản hồi một lời gọi client duy nhất.
• Fused (-F): Mã người dùng chứa một loạt lời gọi mỗi cái thực thi một nút tính toán duy nhất, trong đó nút chứa một chuỗi 128 tính toán.

Đối với JAX multi-controller, OpByOp có nghĩa là JIT-compile một hàm chứa một tính toán và gọi nó lặp lại từ Python, và Fused có nghĩa là JIT-compile một hàm chứa một chuỗi tính toán. Không có analog của Chained cho một multi-controller. Đối với PATHWAYS, OpByOp và Fused sử dụng cùng nguồn JAX như cho multi-controller, và Chained sử dụng program tracer PATHWAYS để tạo một chương trình multi-nút trong đó mỗi nút chứa một tính toán đơn giản. TF tương tự như PATHWAYS, trong đó chúng tôi xây dựng cùng các tính toán TPU và thực thi chúng bằng cách sử dụng đồ thị TF thay vì PATHWAYS. Đối với Ray, OpByOp có nghĩa là thực thi một phương thức actor riêng biệt cho mỗi tính toán thực thi một PyTorch AllReduce. Chained có nghĩa là chuỗi một chuỗi các phương thức actor (bằng cách truyền Ray futures), mỗi cái thực thi một PyTorch AllReduce duy nhất. Fused có nghĩa là thực thi một phương thức actor duy nhất chạy một chuỗi lệnh PyTorch AllReduce trong một vòng lặp.

Hình 5 hiển thị kết quả. Lưu ý rằng OpByOp là một thí nghiệm trường hợp xấu nhất không phải là idiomatic cho bất kỳ framework nào, và nó có mặt chỉ để stress các hệ thống cơ bản. Như mong đợi, đối với OpByOp, thông lượng JAX multi-controller tốt hơn nhiều so với các hệ thống single-controller, đặc biệt khi số lượng bộ gia tốc tăng. Hầu hết overhead của PATHWAYS đến từ thực tế rằng client chờ cho đến khi coordinator đã enqueue một tính toán và trả về handle output của nó trước khi enqueue cái tiếp theo. Chúng tôi có thể loại bỏ hầu hết overhead này bằng cách cho phép mã người dùng tiến hành song song với RPC enqueue, và batch cơ hội nhiều tính toán nhỏ thành một chương trình PATHWAYS duy nhất. Chúng tôi chưa tập trung vào việc tối ưu hóa overhead của các tính toán rất nhỏ vì, trên các mô hình thực với các tính toán liên quan đến nhiều hơn scalars, PATHWAYS đã khớp với hiệu suất của JAX multi-controller (xem §5.3). Một khi đủ công việc được Fused thành một nút duy nhất, PATHWAYS khớp với hiệu suất của JAX lên tới 1000 core TPU, và PATHWAYS Chained vượt trội JAX OpByOp lên tới 256 core, vì PATHWAYS có thể thực thi các tính toán bộ gia tốc liên tiếp trực tiếp từ C++ trong khi JAX OpByOp chuyển sang Python cho mỗi tính toán.

TensorFlow và Ray gặp khó khăn vì thiếu object store thiết bị: Ray phải chuyển kết quả của một tính toán từ GPU sang DRAM trước khi trả về handle object cho client, trong khi TensorFlow chuyển dữ liệu trở lại client. Overhead này làm tổn hại hiệu suất OpByOp của họ nhưng phần lớn được phân bổ cho Chained và Fused. Hiệu suất của Ray và PATHWAYS không thể so sánh trực tiếp vì chúng sử dụng phần cứng khác nhau, nhưng chúng tôi diễn giải kết quả để gợi ý rằng, nếu thiết kế PATHWAYS đầy đủ được triển khai thay thế Ray cho PLAQUE, nó nên có thể đạt được hiệu suất tương đương. Ngay từ đầu, Ray hiển thị hiệu suất kém hơn khoảng một bậc mỗi tính toán so với PATHWAYS, nhưng điều đó không đáng ngạc nhiên vì Ray có thể thực thi các actor Python mục đích chung và PATHWAYS được chuyên hóa cho các tính toán TPU được khởi chạy từ C++. Với sự chú ý cẩn thận đến kỹ thuật, có thể có thể thêm các đường dẫn nhanh vào Ray, như một object store trên GPU và các nguyên thủy để chuyển object một cách hiệu quả qua kết nối GPU, loại bỏ hầu hết overhead bổ sung của nó. TensorFlow chậm khi chạy trên nhiều core vì nó sử dụng một barrier tập trung, được triển khai với các cạnh điều khiển, để tuần tự hóa các tính toán được lập lịch gang.

Hình 6 thay đổi lượng thời gian dành cho mỗi tính toán để tìm tính toán nhỏ nhất mà PATHWAYS khớp với thông lượng của JAX. Đối với 16 host với 128 TPU trên cấu hình (B), sự ngang bằng đạt được với chỉ 2,3ms, và thậm chí đối với 512 host với 2048 TPU trên cấu hình (A), một tính toán ít nhất 35ms che giấu tất cả overhead single-controller của PATHWAYS.

Micro-benchmark tiếp theo của chúng tôi, cũng trên cấu hình (B), đánh giá lợi ích của cơ chế phân phối không đồng bộ song song được mô tả trong §4.5. Chúng tôi xây dựng một benchmark pipeline thực tế hơn trong đó các tính toán đơn giản từ benchmark trước đó lại được chuỗi lại với nhau, nhưng bây giờ mỗi tính toán chạy trên một tập hợp khác nhau gồm 4 core TPU, mỗi cái trên một host khác nhau, và dữ liệu output từ một tính toán phải được gửi qua ICI trước khi tính toán tiếp theo có thể thực thi. Hình 7 hiển thị ba "giai đoạn": lúc đầu overhead client cố định được phân bổ khi số lượng host tăng; sau đó chi phí truyền tăng của việc thêm nhiều giai đoạn hơn bắt đầu chiếm ưu thế; cuối cùng hệ thống bắt đầu phân bổ overhead lập lịch cố định. Cuối cùng chúng tôi mong đợi rằng overhead truyền sẽ lại chiếm ưu thế. Để so sánh, chúng tôi cũng hiển thị hiệu suất khi chúng tôi buộc việc thực thi luồng dữ liệu PATHWAYS sử dụng phân phối không đồng bộ tuần tự, và chờ một tính toán được enqueue trước khi enqueue cái tiếp theo, để đo lợi ích chúng tôi nhận được từ phân phối không đồng bộ song song.

5.2 Multi-tenancy

Chúng tôi xác thực trong Hình 8 (thực hiện trên cấu hình (B)) rằng PATHWAYS có thể time-multiplex các bộ gia tốc giữa các chương trình đồng thời. PATHWAYS có thể đạt được ít nhất cùng thông lượng tổng hợp như JAX khi nhiều client đồng thời gửi các chương trình PATHWAYS khác nhau, tức là không có overhead để context switch giữa các chương trình từ các client khác nhau, ít nhất khi tài nguyên của họ đồng thời vừa vặn trong HBM (trace trong Phụ lục D). Như đã hiển thị trong Hình 6, mức độ đồng thời cần thiết để khớp với thông lượng thấp hơn cho các kích thước tính toán lớn hơn vì các core TPU đạt đến sử dụng đầy đủ sớm hơn. Đáng chú ý rằng thông lượng tối đa của PATHWAYS vượt quá JAX cho các tính toán rất nhỏ, đạt được sử dụng TPU cao hơn. Điều này là vì một worker PATHWAYS có thể chấp nhận nhiều tính toán hơn từ các client từ xa so với JAX có thể phân phối bằng Python cục bộ.

Hình 9 hiển thị trace của một mẫu 128 core trên PATHWAYS cho khối lượng công việc trên. Thí nghiệm này nổi bật rằng PATHWAYS thực hiện lập lịch gang của các chương trình được gửi bởi 4 client độc lập trong khi kiểm soát phân bổ thời gian bộ gia tốc cho công bằng; ví dụ, scheduler có thể thực thi chia sẻ tỷ lệ trong thiết lập multi-tenancy này.

5.3 Hiệu suất mô hình quy mô lớn

Cuối cùng, chúng tôi hiển thị hiệu suất của PATHWAYS trong việc đào tạo các mô hình học máy thực có thể được biểu đạt như các chương trình SPMD. Chúng tôi so sánh các mô hình JAX và TF chạy trên hệ thống gốc của chúng với cùng các mô hình chạy trên PATHWAYS, và xác minh rằng các kết quả số học là giống hệt nhau, vì vậy chúng tôi chỉ tập trung vào hiệu suất.

Đầu tiên chúng tôi so sánh với JAX multi-controller chạy một mô hình Transformer với kiến trúc Encoder-Decoder được sử dụng cho một số tác vụ xử lý ngôn ngữ tự nhiên text-to-text. Chúng tôi sử dụng các cấu hình mô hình từ (Raffel et al., 2019) và chạy các thí nghiệm trên TPUv3 với 16GB bộ nhớ trên mỗi bộ gia tốc. Bảng 1 hiển thị thông lượng đào tạo (tokens/second) cho mô hình Text-to-text Transformer với các kích thước mô hình khác nhau (lên tới 11 tỷ tham số), đào tạo trên số lượng bộ gia tốc khác nhau. Như mong đợi, vì mã mô hình giống nhau, các mô hình được đào tạo trên JAX và PATHWAYS đạt được cùng perplexity trong cùng số bước. Qua tất cả các kích thước mô hình được thử nghiệm, hai hệ thống hiển thị hiệu suất giống hệt nhau vì các tính toán thực tế đủ lớn để che giấu overhead single-controller. Trong khi chúng tôi không báo cáo kết quả chi tiết, chúng tôi có kinh nghiệm đáng kể về việc chạy các mô hình JAX trên PATHWAYS, điều này chứng thực phát hiện rằng hiệu suất của hai hệ thống có thể so sánh được qua một phạm vi rộng các thiết lập.

Tiếp theo, chúng tôi so sánh hiệu suất của PATHWAYS khi đào tạo một mô hình ngôn ngữ dựa trên Transformer với kiến trúc chỉ Decoder trên các cấu hình (B) và (C). Cho thí nghiệm này, chúng tôi sử dụng một mô hình được biểu đạt bằng Python sử dụng TF. Mô hình bao gồm 62 lớp Transformer với chiều mô hình 2048 và chiều ẩn 8192, tổng cộng 3 tỷ tham số. Chúng tôi so sánh một cấu hình SPMD với một pipeline sử dụng lịch trình giống GPipe (Huang et al., 2019). Mô hình pipeline được chia thành nhiều giai đoạn với tính toán cân bằng trong mỗi giai đoạn. Vì giai đoạn đầu tiên có một lớp embedding lookup bổ sung và giai đoạn cuối cùng có một lớp softmax bổ sung, chúng tôi đã lấy ra một lớp Transformer từ giai đoạn đầu tiên và cuối cùng để cân bằng lượng tính toán mỗi giai đoạn. Mỗi giai đoạn được gán cho một tập hợp khác nhau của các bộ gia tốc trải rộng nhiều host.

Bảng 2 hiển thị thông lượng đào tạo cho số lượng giai đoạn (S) và micro-batch (M) khác nhau, trong khi giữ kích thước batch toàn cục và các hyperparameter đào tạo cố định. Số lượng ví dụ trên mỗi micro-batch được cố định ở 4 cho tất cả các trường hợp, và do đó kích thước batch toàn cục mỗi bước là 2048 cho các cấu hình 128-core (8192 cho 512-core). Thông lượng đào tạo của PATHWAYS tăng tỷ lệ thuận với số lượng core TPU trên mỗi giai đoạn pipeline (Bảng 2), phù hợp với các hệ thống khác (Rasley et al., 2020; Narayanan et al., 2021). Kết quả này nhất quán với Hình 5 hiển thị rằng thông lượng của PATHWAYS mở rộng tuyến tính với số lượng host. Tăng số lượng giai đoạn pipeline thêm overhead tối thiểu, thông lượng giảm từ 133.7k tokens/sec xuống 131.4k tokens/sec khi số lượng giai đoạn tăng từ 4 lên 16. Chúng tôi so sánh hiệu suất của các mô hình pipeline với một mô hình tương đương được biểu đạt bằng SPMD, và quan sát rằng ít nhất trong trường hợp này, pipeline có hiệu suất cạnh tranh với SPMD, vì giao tiếp tập hợp trong tính toán SPMD tạo ra overhead cao hơn so với overhead bubble pipeline.

Chúng tôi cũng chứng minh rằng PATHWAYS có thể đào tạo hiệu quả các mô hình qua các đảo TPU được kết nối qua DCN. Trong cấu hình S=16; M=64 với 128 core, chúng tôi đo cùng thông lượng (131.4k tokens/sec) sử dụng một đảo duy nhất 128 core trên cấu hình (B), hoặc 4 đảo 32 core mỗi cái trên cấu hình (C). Hình 10 hiển thị một trace profile khi các giai đoạn được phân vùng thành các đảo. Truyền DCN xảy ra giữa mỗi nhóm 8 hàng trong trace, và không nhìn thấy trong trace vì thời gian giao tiếp được chồng chéo hiệu quả với tính toán.

Cuối cùng, chúng tôi mở rộng đào tạo các mô hình Transformer chỉ Decoder lớn lên 64B và 136B tham số sử dụng hai đảo bộ gia tốc. Khi được đào tạo bằng PATHWAYS qua hai đảo tính toán được kết nối qua DCN, PATHWAYS đạt được 97% thông lượng so với một đảo duy nhất với gấp đôi số lượng thiết bị. Đối với mô hình LM 136B (64B), chúng tôi đào tạo qua hai đảo 1024 (512) core sử dụng reduction ICI nhanh trong đảo theo sau bởi truyền DCN qua các đảo (trace thực thi có sẵn trong Phụ lục D) của 1030GB (457GB) cho reduction toàn cục.

6 THẢO LUẬN

6.1 Thiết kế PATHWAYS vs triển khai

PATHWAYS được thiết kế để nhắm tới các bộ sưu tập lớn các bộ gia tốc TPU. Việc sử dụng TPU thay vì GPU ảnh hưởng đến nhiều quyết định thiết kế mức thấp của chúng tôi. Sự khác biệt lớn nhất giữa TPU và GPU là các tính toán chạy lâu hơn và phức tạp hơn nhiều có thể được fused thành một kernel TPU duy nhất, vì TPU hỗ trợ điều khiển luồng và nguyên thủy giao tiếp phong phú phải được thực thi bởi mã driver trên các hệ thống GPU. Ngược lại, GPU được tích hợp chặt chẽ hơn với các hệ thống bộ nhớ host và DCN (NVIDIA, 2021) (thêm chi tiết trong Phụ lục A.5). TPU phù hợp với PATHWAYS vì XLA có thể biên dịch các hàm hiệu suất cao chứa tập hợp fused, và các đảo lớn kết nối TPU hiệu suất cao cho phép lập lịch linh hoạt các tính toán của nhiều kích thước khác nhau.

Tuy nhiên, chúng tôi tin rằng hầu hết các lựa chọn kiến trúc mức cao mà chúng tôi đã thực hiện trong PATHWAYS và mô tả trong bài báo này cũng sẽ hợp lệ cho các hệ thống GPU quy mô lớn.

6.2 Quản lý tài nguyên

PATHWAYS được thiết kế để cho phép nhiều loại chính sách quản lý tài nguyên động tinh vi. Nghiên cứu ban đầu của chúng tôi đã tập trung vào time-multiplexing động hiệu quả của các tính toán TPU. Đối với các trường hợp sử dụng multi-tenancy phức tạp hơn trong tương lai, PATHWAYS sẽ cần xử lý các loại tài nguyên đa dạng hơn bao gồm nhưng không giới hạn ở thiết bị và bộ nhớ host, và băng thông ICI, DCN, và PCIe.

Mô hình single-controller của PATHWAYS cấp cho hệ thống khả năng mở rộng để theo dõi tài nguyên có sẵn và phân bổ tài nguyên ở quy mô lớn. Chúng tôi đang lên kế hoạch khám phá các yêu cầu multi-tenancy phổ biến như ưu tiên, cách ly hiệu suất, kiểm soát truy cập, và kế toán tài nguyên, nhưng ở thời gian nhỏ hơn đáng kể so với công việc trước đó, và cho các pool tài nguyên lớn hơn bậc (ví dụ, hàng nghìn core và TB bộ nhớ bộ gia tốc).

6.3 Điều khiển luồng vector hóa phụ thuộc dữ liệu

Hầu như tất cả các mô hình ML hiện tại cập nhật mỗi trọng số mô hình dựa trên mỗi ví dụ đào tạo trong mỗi bước. Chúng tôi muốn cho phép nghiên cứu sử dụng điều khiển luồng tinh vi để các trọng số mô hình khác nhau có thể được cập nhật mỗi ví dụ, hoặc thậm chí mỗi ví dụ con (patch của một hình ảnh, hoặc từ của một câu). Các mô hình như Mixture of Experts (MoE) (Shazeer et al., 2017) và routed capsule networks (Hinton et al., 2018; Barham và Isard, 2019) khai thác tính thưa thớt tính toán bằng cách "route" các (sub-)ví dụ khác nhau tới các bộ gia tốc chứa các tập con khác nhau của trọng số mô hình dựa trên các hàm học được cập nhật khi đào tạo tiến triển. Routing này yêu cầu trao đổi dữ liệu phụ thuộc dữ liệu tinh vi giữa các nút. Các đồng nghiệp nghiên cứu ML của chúng tôi đã nói với chúng tôi rằng họ muốn sử dụng tính thưa thớt hiệu quả hơn khi đào tạo các mô hình ngày càng lớn hơn, với ngày càng nhiều tác vụ hơn, nhưng các framework hiện tại hạn chế khả năng thử nghiệm với các kiến trúc mô hình mới. Đó là chủ đề của công việc tương lai để hỗ trợ điều khiển luồng vector hóa phụ thuộc dữ liệu với cả mô hình lập trình sạch và hiệu suất tốt.

7 CÔNG VIỆC LIÊN QUAN

Chúng tôi đã kiểm tra công việc liên quan mật thiết chi tiết trong §2. Phần này mở rộng về nghiên cứu liên quan giải quyết các khối lượng công việc ML cần khả năng vượt ra ngoài những gì được cung cấp bởi các multi-controller SPMD, và xác thực các lựa chọn thiết kế PATHWAYS của chúng tôi.

Chia sẻ bộ gia tốc qua nhiều tác vụ là quan trọng để đạt được sử dụng tài nguyên cao. Chia sẻ tài nguyên thông thường được thực hiện theo cách thô. Ví dụ, ảo hóa mục đích chung cho phép các ứng dụng cloud chia sẻ hiệu quả tài nguyên multi-tenant với cách ly hiệu suất (Angel et al., 2014; Wentzlaff et al., 2010; Shahrad và Wentzlaff, 2016; Baumann et al., 2009), nhưng các nhà cung cấp cloud dành riêng bộ gia tốc cho người dùng cá nhân. Các scheduler cụm tối ưu hóa cho tính không đồng nhất của khối lượng công việc ML (Narayanan et al., 2020) và công bằng và hiệu suất multi-job, multi-user (Xiao et al., 2018; Ren et al., 2015; Mahajan et al., 2020; Jeon et al., 2018), nhưng tài nguyên vẫn được dành riêng độc quyền cho các job đơn ở thời gian dài (giây hoặc hơn).

Công việc gần đây cho thấy chia sẻ tinh vi hơn có thể cải thiện hiệu quả tài nguyên thêm: ảo hóa bộ gia tốc (Yu et al., 2020; Gupta et al., 2011; Vijaykumar et al., 2016) tránh dành riêng toàn bộ bộ gia tốc cho một người dùng duy nhất. Các mô hình lớn (Brown et al., 2020) có thể bị hạn chế bởi bộ nhớ bộ gia tốc có sẵn, yêu cầu ảo hóa bộ nhớ GPU (Rhu et al., 2016; Ausavarungnirun et al., 2018) hoặc offload DRAM (Rajbhandari et al., 2021). Thực thi tác vụ ML đồng thời (time-multiplexed hoặc chồng chéo) (Gupta et al., 2018; Xiao et al., 2020; Bai et al., 2020; Yu và Chowdhury, 2020; Wang et al., 2021; Lim et al., 2021) giúp thu hoạch tài nguyên nhàn rỗi trong các bộ gia tốc. Những kỹ thuật chia sẻ tinh vi này chứng minh cơ hội để chia sẻ bộ gia tốc khó tận dụng ở quy mô mà không có một hệ thống single-controller như PATHWAYS.

Nhiều công việc đã chỉ ra rằng lệch khỏi các tính toán SPMD có thể cải thiện hiệu quả trên các khối lượng công việc lớn: pipelining (Huang et al., 2019; Narayanan et al., 2019; Yang et al., 2021) phân vùng các mô hình ML thành các tính toán không đồng nhất tĩnh qua các bộ gia tốc. Đào tạo mạng neural đồ thị (Jia et al., 2020), tìm kiếm kiến trúc neural (Pham et al., 2018), và các hệ thống học multi-modal multi-task (Ma et al., 2018; Lepikhin et al., 2020; Zhao et al., 2019) là các ví dụ về các tác vụ vốn không đồng nhất và động không phù hợp tự nhiên với mô hình SPMD. Chúng tôi dự đoán rằng các mô hình ML hiệu quả quy mô lớn sắp tới có thể tạo thành một bộ sưu tập các lớp được chia sẻ và các lớp độc quyền (Bommasani và cộng sự., 2021), tự nhiên để biểu đạt như MPMD.

8 KẾT LUẬN

PATHWAYS khớp với hiệu suất multi-controller tối tân trên các mô hình ML hiện tại là single-tenant SPMD. Chúng tôi đã đảm bảo tương thích nghiêm ngặt với JAX multi-controller, và như chúng tôi chứng minh trong §5, PATHWAYS khớp với hiệu suất của JAX qua các quy mô hệ thống rất lớn, cho tất cả ngoại trừ các tính toán nhỏ nhất.

Đồng thời, PATHWAYS đảo ngược mô hình thực thi của các chương trình JAX, kéo mã người dùng trở lại vào một mô hình single-controller, và xen một framework quản lý tài nguyên tập trung và lập lịch giữa client và bộ gia tốc. Mô hình lập trình single-controller cho phép người dùng truy cập đơn giản vào các mẫu tính toán phong phú hơn nhiều. Lớp quản lý tài nguyên và lập lịch cho phép tái giới thiệu các chính sách quản lý cụm bao gồm chia sẻ multi-tenant, ảo hóa và tính đàn hồi, tất cả được điều chỉnh theo yêu cầu của khối lượng công việc ML và bộ gia tốc. Các micro-benchmark của chúng tôi hiển thị sự xen kẽ của các khối lượng công việc client đồng thời, và thực thi pipeline hiệu quả, thuyết phục chứng minh rằng các cơ chế hệ thống chúng tôi đã xây dựng nhanh và linh hoạt, và tạo thành một cơ sở vững chắc cho nghiên cứu các chính sách mới để sử dụng chúng.

Chúng tôi đã chỉ ra rằng thiết kế hệ thống và kỹ thuật cẩn thận cho phép chúng tôi "có được điều tốt nhất của cả hai thế giới", khớp với hiệu suất trên các mô hình ML ngày nay trong khi cung cấp các tính năng cần thiết để viết các mô hình của ngày mai.
