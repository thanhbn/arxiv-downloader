# Cắt giảm Phân tán hướng tới Mạng Nơ-ron Siêu nhỏ
trong Học liên bang

Hong Huang¹, Lan Zhang², Chaoyue Sun³, Ruogu Fang³, Xiaoyong Yuan², Dapeng Wu¹
¹Đại học Thành phố Hồng Kông, ²Đại học Công nghệ Michigan, ³Đại học Florida

Tóm tắt - Cắt giảm mạng nơ-ron là một kỹ thuật thiết yếu để giảm kích thước và độ phức tạp của mạng nơ-ron sâu, cho phép các mô hình quy mô lớn hoạt động trên các thiết bị có tài nguyên hạn chế. Tuy nhiên, các phương pháp cắt giảm hiện tại phụ thuộc rất nhiều vào dữ liệu huấn luyện để định hướng các chiến lược cắt giảm, khiến chúng trở nên không hiệu quả đối với học liên bang trên các bộ dữ liệu phân tán và bảo mật. Thêm vào đó, quá trình cắt giảm tốn nhiều bộ nhớ và tính toán trở nên không khả thi đối với các thiết bị có tài nguyên hạn chế trong học liên bang. Để giải quyết những thách thức này, chúng tôi đề xuất FedTiny, một khung cắt giảm phân tán cho học liên bang tạo ra các mô hình siêu nhỏ chuyên biệt cho các thiết bị có bộ nhớ và tính toán hạn chế. Chúng tôi giới thiệu hai mô-đun chính trong FedTiny để tìm kiếm thích ứng các mô hình chuyên biệt được cắt giảm thô và tinh hơn để phù hợp với các tình huống triển khai với tính toán cục bộ thưa thớt và rẻ. Đầu tiên, mô-đun chọn lọc chuẩn hóa theo lô thích ứng được thiết kế để giảm thiểu độ lệch trong cắt giảm do tính không đồng nhất của dữ liệu cục bộ. Thứ hai, mô-đun cắt giảm tiến bộ nhẹ nhàng nhằm cắt giảm tinh hơn các mô hình dưới ngân sách bộ nhớ và tính toán nghiêm ngặt, cho phép chính sách cắt giảm cho từng lớp được xác định dần dần thay vì đánh giá toàn bộ cấu trúc mô hình. Kết quả thực nghiệm chứng minh hiệu quả của FedTiny, vượt trội hơn các phương pháp tiên tiến, đặc biệt khi nén các mô hình sâu thành các mô hình siêu nhỏ cực kỳ thưa thớt. FedTiny đạt được cải thiện độ chính xác 2.61% trong khi giảm đáng kể chi phí tính toán 95.91% và dung lượng bộ nhớ 94.01% so với các phương pháp tiên tiến.

Từ khóa - học liên bang, cắt giảm mạng nơ-ron, mạng nơ-ron siêu nhỏ

I. GIỚI THIỆU

Mạng nơ-ron sâu (DNN) đã đạt được thành công lớn trong thập kỷ qua. Tuy nhiên, chi phí tính toán khổng lồ và tải bộ nhớ lớn hạn chế việc sử dụng DNN trên các thiết bị có tài nguyên hạn chế. Cắt giảm mạng nơ-ron đã là một giải pháp nổi tiếng để cải thiện hiệu quả phần cứng [1], [2]. Cốt lõi của cắt giảm mạng nơ-ron là loại bỏ các tham số không quan trọng khỏi DNN và xác định các mạng con chuyên biệt cho các nền tảng phần cứng và tác vụ huấn luyện khác nhau (được định nghĩa là các tình huống triển khai). Để đạt độ chính xác tốt hơn, hầu hết các phương pháp cắt giảm đều phụ thuộc rất nhiều vào dữ liệu huấn luyện để cân bằng kích thước mô hình, hiệu quả và độ chính xác [2]–[6], điều này, thật không may, trở nên không hiệu quả khi xử lý các bộ dữ liệu huấn luyện bảo mật được phân tán trên các thiết bị có tài nguyên hạn chế.

Thành công gần đây trong học liên bang cho phép huấn luyện hợp tác qua các thiết bị phân tán với các bộ dữ liệu cục bộ bảo mật [7]. Thay vì tải lên dữ liệu cục bộ, học liên bang tổng hợp kiến thức trên thiết bị bằng cách cập nhật lặp lại các tham số mô hình cục bộ tại máy chủ. Mặc dù thành công, học liên bang không thể xác định mô hình cắt giảm chuyên biệt cho các thiết bị tham gia mà không có dữ liệu huấn luyện. Để giải quyết vấn đề này, [8] đề xuất tách rời quá trình cắt giảm trong môi trường liên bang, nơi mô hình kích thước lớn được cắt giảm trước tại máy chủ rồi sau đó được tinh chỉnh trên các thiết bị. Tuy nhiên, vì hầu hết các thuật toán cắt giảm đều yêu cầu hướng dẫn từ phân bố dữ liệu, mà không có quyền truy cập vào dữ liệu huấn luyện phía thiết bị, việc cắt giảm phía máy chủ dẫn đến độ lệch đáng kể trong mạng con đã cắt giảm, đặc biệt dưới các phân bố dữ liệu cục bộ không đồng nhất (non-iid). Để giảm thiểu những vấn đề độ lệch như vậy, nghiên cứu gần đây đẩy các hoạt động cắt giảm lên thiết bị [9]–[13]. Như thể hiện trong Hình 1 bên trái, hoặc mô hình kích thước đầy đủ hoặc mô hình cắt giảm thô sẽ được cắt giảm tinh hơn dựa trên điểm số tầm quan trọng được cập nhật từ thiết bị. Điểm số tầm quan trọng cho tất cả tham số cần được lưu trữ trong bộ nhớ, điều này không khả thi đối với thiết bị có tài nguyên hạn chế với ngân sách bộ nhớ giới hạn. Hơn nữa, mà không có bất kỳ tương tác nào với phía thiết bị, mô hình ban đầu thông qua cắt giảm thô phía máy chủ vẫn gặp phải vấn đề độ lệch, đòi hỏi nỗ lực bổ sung trong cắt giảm tinh sau đó để tìm mạng con tối ưu. Tác động tiêu cực như vậy trở nên thách thức hơn khi cắt giảm hướng tới mạng con cực kỳ nhỏ, vì mạng con ban đầu bị lệch có thể lệch đáng kể khỏi cấu trúc tối ưu, dẫn đến độ chính xác kém [14].

Để giải quyết các thách thức trên, trong bài báo này, chúng tôi phát triển một khung cắt giảm phân tán mới cho học liên bang có tên FedTiny. Tùy thuộc vào các tình huống triển khai, tức là các nền tảng phần cứng tham gia và tác vụ huấn luyện, FedTiny có thể thu được các mô hình siêu nhỏ chuyên biệt sử dụng các bộ dữ liệu phân tán và bảo mật trên các thiết bị tham gia. Bên cạnh đó, FedTiny cho phép các thiết bị có ngân sách bộ nhớ và tính toán chặt chẽ tham gia vào quá trình cắt giảm tốn nhiều tài nguyên bằng cách cấu hình lại các tương tác giữa máy chủ và thiết bị. Như thể hiện trong Hình 1 bên phải, FedTiny giới thiệu hai mô-đun chính: mô-đun chọn lọc chuẩn hóa theo lô (BN) thích ứng và mô-đun cắt giảm tiến bộ. Để tránh tác động tiêu cực của cắt giảm ban đầu bị lệch, chúng tôi giới thiệu mô-đun BN thích ứng để xác định mô hình cắt giảm thô chuyên biệt bằng cách cắt giảm gián tiếp tại thiết bị, nơi thiết bị chỉ đánh giá việc cắt giảm phía máy chủ. Cần đề cập rằng đánh giá mô hình đã cắt giảm rẻ hơn nhiều so với huấn luyện và cắt giảm. Đánh giá cục bộ được phản hồi lại máy chủ thông qua các tham số chuẩn hóa theo lô. Vì các lớp chuẩn hóa theo lô có thể đo lường hiệu quả phân bố dữ liệu cục bộ với rất ít tham số [15], mô-đun này hướng dẫn việc cắt giảm ban đầu với chi phí tính toán và giao tiếp thấp. Bên cạnh đó, trái ngược với nghiên cứu trước đây sử dụng điểm số tầm quan trọng của tất cả tham số trong mô hình kích thước đầy đủ cho cắt giảm tinh, mô-đun cắt giảm tiến bộ được phát triển để điều chỉnh lặp lại cấu trúc mô hình với tính toán cục bộ thưa thớt và rẻ. Lấy cảm hứng từ RigL [14], các thiết bị chỉ đánh giá các tham số mô hình một phần (ví dụ, một lớp duy nhất) tại một thời điểm, nơi các điểm số tầm quan trọng top-K được lưu trữ cục bộ và tải lên máy chủ, giảm đáng kể chi phí bộ nhớ, tính toán và giao tiếp.

Để chứng minh hiệu quả của FedTiny, chúng tôi đánh giá FedTiny trên ResNet18 [16] và VGG11 [17] với bốn bộ dữ liệu phân loại hình ảnh (CIFAR-10, CIFAR-100, CINIC-10 và SVHN). Kết quả thực nghiệm rộng rãi cho thấy FedTiny đạt độ chính xác cao hơn nhiều với mức độ bộ nhớ và chi phí tính toán thấp hơn so với các phương pháp cơ sở tiên tiến. Đặc biệt trong chế độ mật độ thấp [18] từ 10⁻² đến 10⁻³, FedTiny gặp phải mất mát độ chính xác nhẹ, trong khi các cơ sở khác gặp phải sự sụt giảm mạnh về độ chính xác. Hơn nữa, FedTiny đạt độ chính xác top-một là 85.23% với 0.014× FLOPs và 0.03× dung lượng bộ nhớ của ResNet18 [16], vượt trội hơn cơ sở tốt nhất đạt 82.62% độ chính xác với 0.34× FLOPs và 0.51× dung lượng bộ nhớ.

II. CÔNG TRÌNH LIÊN QUAN

A. Cắt giảm Mạng Nơ-ron

Cắt giảm mạng nơ-ron đã là một kỹ thuật nổi tiếng để loại bỏ các tham số dư thừa của DNN cho nén mô hình, có thể truy nguyên về cuối những năm 1980 [1], [19], [20]. Hầu hết các phương pháp cắt giảm hiện tại tập trung vào sự cân bằng giữa độ chính xác và độ thưa thớt trong giai đoạn suy luận. Một quá trình cắt giảm điển hình đầu tiên tính toán điểm số tầm quan trọng của tất cả tham số trong DNN được huấn luyện tốt rồi sau đó loại bỏ các tham số có điểm số thấp hơn. Điểm số tầm quan trọng có thể được suy ra dựa trên độ lớn trọng số [1], [2], khai triển Taylor bậc nhất của hàm mất mát [19], [21], khai triển Taylor bậc hai của hàm mất mát [5], [20], và các biến thể khác [3], [4], [6].

Một hướng nghiên cứu gần đây khác về cắt giảm mạng nơ-ron tập trung vào việc cải thiện hiệu quả của giai đoạn huấn luyện, có thể được chia thành hai loại. Một là cắt giảm tại khởi tạo, tức là cắt giảm mô hình kích thước đầy đủ ban đầu trước khi huấn luyện. Chính sách cắt giảm có thể được xác định bằng cách đánh giá độ nhạy kết nối [22], tích Hessian-gradient [23], và dòng chảy synaptic [24] của mô hình ban đầu. Vì việc cắt giảm như vậy không liên quan đến dữ liệu huấn luyện, mô hình đã cắt giảm không được chuyên biệt hóa cho tác vụ huấn luyện, dẫn đến hiệu suất bị lệch. Loại khác là huấn luyện thưa thớt động [14], [25], [26]. Cấu trúc mô hình đã cắt giảm được điều chỉnh lặp lại trong suốt quá trình huấn luyện trong khi duy trì kích thước mô hình đã cắt giảm ở độ thưa thớt mong muốn. Tuy nhiên, quá trình cắt giảm là để điều chỉnh cấu trúc mô hình trong không gian tìm kiếm lớn, đòi hỏi các hoạt động tốn nhiều bộ nhớ, điều này không khả thi đối với thiết bị có tài nguyên hạn chế. Mặc dù RigL [14] cố gắng giảm tiêu thụ bộ nhớ, nó cần tính toán gradient cho tất cả tham số, điều này tốn kém về mặt tính toán và có thể dẫn đến các vấn đề chậm chạp trong học liên bang.

B. Cắt giảm Mạng Nơ-ron trong Học liên bang

Học liên bang gần đây đã thu hút sự chú ý như một phương pháp tiềm năng để giải quyết các mối quan tâm về quyền riêng tư dữ liệu trong học máy hợp tác. FedAvg [27], một trong những phương pháp được sử dụng rộng rãi nhất trong học liên bang, sử dụng các mô hình cục bộ được cập nhật trên thiết bị thay vì dữ liệu thô để đạt được việc chuyển giao kiến thức riêng tư. Vì dữ liệu được lưu trữ cục bộ và không thể chia sẻ, các phương pháp cắt giảm nói trên dựa trên dữ liệu huấn luyện không thể được sử dụng trong học liên bang.

Được khai sáng bởi cắt giảm tại khởi tạo, Xu et al. đề xuất cắt giảm mô hình kích thước đầy đủ ban đầu tại máy chủ và tinh chỉnh tại thiết bị với dữ liệu cục bộ của họ [8]. Các phương pháp cắt giảm tại khởi tạo hiện tại, như SNIP [22], GraSP [23], và SynFlow [24], có thể được chuyển đổi trực tiếp thành cắt giảm phía máy chủ. Tuy nhiên, cắt giảm phía máy chủ thường dẫn đến các mô hình đã cắt giảm bị lệch đáng kể, đặc biệt đối với các phân bố dữ liệu cục bộ không đồng nhất (non-iid).

Để giảm thiểu độ lệch như vậy, nghiên cứu gần đây đẩy các hoạt động cắt giảm trong cài đặt liên bang lên thiết bị. Bằng cách huấn luyện cục bộ mô hình kích thước đầy đủ, SCBF [9] loại bỏ động các kênh không quan trọng trên thiết bị. Việc huấn luyện cục bộ như vậy với mô hình kích thước đầy đủ được giao cho một phần thiết bị trong FedPrune để hướng dẫn cắt giảm dựa trên các kích hoạt được cập nhật [11]. Bên cạnh đó, LotteryFL [10] cắt giảm lặp lại mô hình kích thước đầy đủ trên thiết bị với tỷ lệ cắt giảm cố định để tìm mạng con cục bộ được cá nhân hóa. Tuy nhiên, nghiên cứu trên gặp phải chi phí bộ nhớ và tính toán lớn ở phía thiết bị vì thiết bị cần tính toán cục bộ điểm số tầm quan trọng của tất cả tham số. Mặc dù PruneFL [13] giảm chi phí tính toán cục bộ bằng cách cắt giảm tinh mô hình đã cắt giảm thô thay vì mô hình kích thước đầy đủ, nó vẫn yêu cầu dung lượng bộ nhớ cục bộ lớn để ghi lại điểm số tầm quan trọng được cập nhật của tất cả tham số trong mô hình kích thước đầy đủ. ZeroFL [28] phân vùng trọng số thành trọng số hoạt động và trọng số không hoạt động trong suy luận và trọng số cũng như kích hoạt thưa thớt cho lan truyền ngược. Tuy nhiên, phương pháp này vẫn cần không gian bộ nhớ lớn vì trọng số không hoạt động và gradient được tạo ra thông qua quá trình huấn luyện vẫn được lưu trữ theo cách dày đặc. FedDST [29] triển khai điều chỉnh mặt nạ trên thiết bị, và máy chủ tạo ra mô hình toàn cục mới thông qua tổng hợp thưa thớt và cắt giảm độ lớn. Nó cần chi phí tính toán nhiều hơn vì nó cần các epoch huấn luyện bổ sung để phục hồi trọng số tăng trưởng trước khi tải lên, điều này có thể dẫn đến các vấn đề chậm chạp trong học liên bang. Mô hình cắt giảm thô vẫn gặp phải vấn đề độ lệch trong cắt giảm phía máy chủ. Cắt giảm mạng nơ-ron liên bang hiện tại không thể thu được mô hình siêu nhỏ chuyên biệt mà không có độ lệch và mối quan tâm về ngân sách bộ nhớ/tính toán. Do đó, chúng tôi phát triển FedTiny để đạt được điều này.

C. Học liên bang với Dữ liệu Non-iid

Học liên bang gặp phải sự phân kỳ khi phân bố dữ liệu qua các thiết bị không đồng nhất (non-iid) [30]. Một số công trình đã được đề xuất để giải quyết các thách thức non-iid, ví dụ như MATCHA [31], FedProx [7], và FedNova [32]. Những công trình này cung cấp đảm bảo hội tụ của học liên bang dưới các giả định mạnh, điều này trở nên không thực tế trong các tình huống thực tế.

Tăng cường dữ liệu (ví dụ, Astraea [33], FedGS [34], và CSFedAvg [35]) và các phương pháp cá nhân hóa (ví dụ, meta learning [36], multi-task learning [37], và knowledge distillation [38]) là hai phương pháp tiềm năng để giải quyết các vấn đề non-iid. Tuy nhiên, những phương pháp này tốn kém về mặt tính toán và trở nên không khả thi trong các tình huống có tài nguyên hạn chế. Trong công trình của chúng tôi, chúng tôi phát triển một phương pháp cắt giảm phân tán mới với lựa chọn chuẩn hóa theo lô thích ứng để tìm mô hình cắt giảm thô không bị lệch cho việc giải quyết các thách thức non-iid trong thiết bị có tài nguyên hạn chế.

III. FEDTINY ĐỀ XUẤT

Phần này giới thiệu FedTiny được đề xuất. Trước tiên chúng tôi mô tả bài toán, tiếp theo là các nguyên tắc thiết kế của chúng tôi. Tương ứng, chúng tôi trình bày hai mô-đun chính trong FedTiny: mô-đun chọn lọc BN thích ứng và mô-đun cắt giảm tiến bộ.

A. Bài toán

Chúng tôi xem xét một cài đặt học liên bang điển hình, nơi K thiết bị hợp tác huấn luyện mạng nơ-ron với các bộ dữ liệu cục bộ tương ứng Dk, k ∈ {1,2, . . . , K}. Tất cả thiết bị có tài nguyên bộ nhớ và tính toán hạn chế. Cho một mạng nơ-ron lớn với tham số dày đặc Θ, chúng tôi nhằm tìm mạng con chuyên biệt với tham số thưa thớt θ và mặt nạ m trên tham số dày đặc để đạt hiệu suất dự đoán tối ưu cho học liên bang. Tham số thưa thớt được suy ra bằng cách áp dụng mặt nạ lên tham số dày đặc: θ = Θ ⊙ m (m ∈ {0,1}|Θ|). Trong quá trình huấn luyện, mật độ d của mặt nạ thưa thớt m không thể vượt quá mật độ mục tiêu dtarget. dtarget được xác định bởi giới hạn tài nguyên bộ nhớ của thiết bị. Chúng tôi công thức hóa bài toán như một bài toán tối ưu hóa có ràng buộc:

min θ,m ∑(k=1 to K) L(θ,m,Dk),
s.t. d ≤ dtarget (1)

trong đó L(θ,m,Dk) biểu thị hàm mất mát cho bộ dữ liệu cục bộ Dk trên thiết bị thứ k.

B. Nguyên tắc Thiết kế

Như thể hiện trong Hình 1 bên trái, cắt giảm mạng nơ-ron liên bang hiện tại đối mặt với hai thách thức chính, độ lệch trong cắt giảm thô và tiêu thụ bộ nhớ tốn kém trong cắt giảm tinh. Để giải quyết những thách thức này, chúng tôi đề xuất FedTiny. Tổng quan về FedTiny được minh họa trong Hình 1 bên phải, bao gồm hai mô-đun chính: mô-đun chọn lọc BN thích ứng và mô-đun cắt giảm tiến bộ.

Mô-đun chọn lọc chuẩn hóa theo lô thích ứng (Bước 2-5 trong Hình 1 bên phải) nhằm suy ra cấu trúc cắt giảm thô thích ứng trên máy chủ và giảm thiểu độ lệch trong cắt giảm thô do dữ liệu không đồng nhất không nhìn thấy qua các thiết bị. Trong mô-đun này, các thiết bị đầu tiên hợp tác cập nhật các phép đo chuẩn hóa theo lô cho tất cả mô hình ứng viên từ cắt giảm thô. Sau đó máy chủ chọn một mô hình ứng viên ít bị lệch hơn làm mô hình cắt giảm thô ban đầu dựa trên đánh giá thiết bị.

Mô-đun cắt giảm tiến bộ (Bước 6-7 trong Hình 1 bên phải) cải thiện thêm mô hình cắt giảm thô bằng cắt giảm tinh tại thiết bị có tài nguyên hạn chế, giảm đáng kể dung lượng bộ nhớ trên thiết bị và chi phí tính toán. Trong mô-đun này, thiết bị chỉ duy trì điểm số tầm quan trọng top-K của các tham số đã cắt giảm. Dựa trên điểm số tầm quan trọng trung bình, máy chủ tăng trưởng và cắt giảm tham số để tạo ra cấu trúc mô hình mới. Sau khi tăng trưởng và cắt giảm lặp lại, cấu trúc mô hình tiến bộ tiếp cận cấu trúc tối ưu.

Trong phần sau, chúng tôi cung cấp mô tả chi tiết về mô-đun chọn lọc chuẩn hóa theo lô thích ứng và mô-đun cắt giảm tiến bộ, tương ứng.

C. Chọn lọc Chuẩn hóa theo Lô Thích ứng

Việc giải quyết vấn đề độ lệch trong mô hình cắt giảm thô là rất quan trọng, vì cấu trúc đã cắt giảm bị lệch cao đòi hỏi nhiều tài nguyên và thời gian hơn để điều chỉnh thành cấu trúc tối ưu, đặc biệt trong chế độ mật độ thấp. Một phương pháp có thể là gửi một tập hợp các ứng viên cấu trúc đã cắt giảm đến thiết bị và để thiết bị chọn mô hình ít bị lệch nhất từ nhóm ứng viên. Chúng tôi gọi phương pháp này là lựa chọn vanilla [39]. Tuy nhiên, nghiên cứu gần đây [40] cho thấy hiệu suất mô hình đã cắt giảm thay đổi trước và sau tinh chỉnh, điều này làm cho ứng viên cấu trúc đã cắt giảm được chọn trước tinh chỉnh không nhất thiết là tốt nhất sau tinh chỉnh. Vấn đề như vậy có thể được phóng đại trong cài đặt liên bang vì phân bố dữ liệu không đồng nhất qua thiết bị có thể làm tăng thêm sự khác biệt về hiệu suất mô hình đã cắt giảm trong tinh chỉnh.

Để giải quyết vấn đề này, chúng tôi giới thiệu chọn lọc chuẩn hóa theo lô thích ứng trong FedTiny. Chọn lọc chuẩn hóa theo lô thích ứng cập nhật các phép đo chuẩn hóa theo lô cho các mô hình ứng viên trước khi đánh giá, nhằm suy ra cấu trúc cắt giảm thô ít bị lệch hơn. Thuật toán của mô-đun chọn lọc chuẩn hóa theo lô thích ứng được minh họa trong Thuật toán 1. Chúng tôi giới thiệu chuẩn hóa theo lô (BN) [15] để cung cấp các phép đo cho phân bố dữ liệu qua thiết bị. Những phép đo như vậy cung cấp biểu diễn của dữ liệu trên thiết bị và do đó hướng dẫn quá trình cắt giảm. Phép biến đổi chuẩn hóa theo lô được tính toán dựa trên phép biến đổi sau đây trên đầu vào thứ i xi trong mỗi lô,

x̂i ← (xi - μ)/√(σ² + ε), (2)

trong đó ε là một hằng số nhỏ. Trong quá trình huấn luyện, μ và σ được cập nhật dựa trên trung bình động μi và độ lệch chuẩn σi của lô xi,

μt = γμt-1 + (1-γ)μi, σt² = γσt-1² + (1-γ)σi², (3)

trong đó γ biểu thị hệ số momentum và t là số lần lặp huấn luyện. Trong quá trình kiểm tra, trung bình μ và độ lệch chuẩn σ được giữ cố định.

Trong mô-đun chọn lọc chuẩn hóa theo lô thích ứng, các phép đo chuẩn hóa theo lô được cập nhật trong lượt truyền tiến trên thiết bị trước khi đánh giá để chọn ứng viên cắt giảm thô ít bị lệch hơn. Cụ thể, sau khi cắt giảm thô trên tham số kích thước đầy đủ Θ với các chiến lược khác nhau, máy chủ thu được nhóm ban đầu gồm C mô hình ứng viên với tham số thưa thớt θ(c) và mặt nạ tương ứng m(c), trong đó θ(c) = Θ ⊙ m(c), cho c ∈ {1,2, . . . , C}. Đối với mỗi mô hình ứng viên, chúng tôi đặt các tỷ lệ cắt giảm khác nhau cho từng lớp trong khi giữ mật độ tổng thể d ≤ dtarget. Thiết bị đầu tiên lấy tất cả mô hình ứng viên. Lưu ý rằng chi phí giao tiếp thấp do mật độ mạng cực thấp. Sau đó, mỗi thiết bị (giả sử là thiết bị thứ k) lấy mẫu bộ dữ liệu phát triển từ dữ liệu cục bộ, D̂k ⊂ Dk, đóng băng tất cả tham số và cập nhật trung bình μk(c) và độ lệch chuẩn σk(c) của các lớp chuẩn hóa theo lô trong mô hình ứng viên thứ c. Tiếp theo, máy chủ tổng hợp tất cả phép đo chuẩn hóa theo lô cục bộ từ thiết bị để thu được phép đo chuẩn hóa theo lô toàn cục mới cho mỗi mô hình ứng viên, tức là với c ∈ {1,2, . . . , C},

μ(c) = ∑(k=1 to K) |D̂k|/∑(k=1 to K)|D̂k| μk(c), σ(c) = ∑(k=1 to K) |D̂k|/∑(k=1 to K)|D̂k| σk(c), (4)

trong đó |D̂k| biểu thị số lượng mẫu trong bộ dữ liệu D̂k.

[Thuật toán 1 được dịch tương tự...]

Sau đó, mỗi thiết bị cập nhật phép đo chuẩn hóa theo lô toàn cục μ(c), σ(c) cho mô hình ứng viên thứ c. Xem xét Phương trình 1, chúng tôi để thiết bị tính toán mất mát đánh giá cho mỗi mô hình ứng viên đã cập nhật với dữ liệu trên thiết bị của họ và để máy chủ chọn mô hình ứng viên có mất mát trung bình thấp nhất làm mô hình cắt giảm thô.

Lưu ý rằng mặc dù mô-đun chọn lọc chuẩn hóa theo lô thích ứng yêu cầu chuyển giao tham số, chi phí giao tiếp vẫn tối thiểu, vì chỉ cần chuyển giao tham số trong các mô hình đã cắt giảm với mật độ cực thấp. Phân tích chi tiết về chi phí giao tiếp được thảo luận trong Phần IV-D. Thêm vào đó, phép biến đổi chuẩn hóa theo lô được tính toán như một phần của lượt truyền tiến tại thiết bị mà không cần tính toán gradient hoặc cập nhật. Do đó, chọn lọc chuẩn hóa theo lô thích ứng hiệu quả giải quyết độ lệch của cấu trúc mô hình mà không gây ra chi phí bộ nhớ hoặc tính toán đáng kể.

D. Cắt giảm Tiến bộ

Cho một mô hình cắt giảm thô từ mô-đun trên, chúng tôi giới thiệu cắt giảm tiến bộ để cắt giảm tinh thêm mô hình cho hiệu suất tốt hơn. Chúng tôi đề xuất mô-đun cắt giảm tiến bộ với hai cải tiến: 1) chỉ tính toán điểm số tầm quan trọng top-K, trong khi các điểm số tầm quan trọng còn lại bị loại bỏ để tiết kiệm không gian bộ nhớ; 2) tham số mô hình một phần (ví dụ, một lớp duy nhất) được điều chỉnh mỗi vòng thay vì toàn bộ mô hình để tránh tính toán tốn kém. FedTiny sử dụng điều chỉnh tăng trưởng-cắt giảm trên cấu trúc mô hình trong khi duy trì độ thưa thớt. Cụ thể, máy chủ tăng trưởng các tham số đã cắt giảm và cắt giảm cùng số lượng tham số chưa cắt giảm để điều chỉnh cấu trúc mô hình. Ký hiệu a_t^l là số lượng tham số sẽ được tăng trưởng và cắt giảm trên lớp l tại lần lặp thứ t. Để hướng dẫn tăng trưởng và cắt giảm trên máy chủ, mỗi thiết bị chỉ huấn luyện mô hình thưa thớt và tính toán Top-a_t^l gradient cho các tham số đã cắt giảm, điều này giữ cho dung lượng bộ nhớ và chi phí tính toán thấp trong thiết bị có tài nguyên hạn chế. Hơn nữa, để giảm tính toán tốn kém, FedTiny chia cấu trúc mô hình thành nhiều khối và cắt giảm một khối trong một vòng. Mô-đun cắt giảm tiến bộ được chi tiết trong Thuật toán 2.

Chi tiết, mỗi thiết bị (giả sử là thiết bị thứ k) đầu tiên tải xuống tham số mô hình thưa thớt toàn cục θt với mặt nạ mt làm tham số cục bộ θ_t^k trong lần lặp thứ t, và áp dụng SGD với gradient thưa thớt:

θ_{t+1}^k = θ_t^k - η_t∇L(θ_t^k,m_t,B_t^k) ⊙ m_t, (5)

trong đó η_t là tỷ lệ học, B_t^k là một lô mẫu từ bộ dữ liệu cục bộ D_k, và ∇L ⊙ m_t biểu thị gradient thưa thớt cho tham số thưa thớt θ_t^k. Sau E lần lặp SGD cục bộ, mỗi thiết bị tính toán top-a_t^l gradient cho các tham số đã cắt giảm trên mỗi lớp l với một lô mẫu. Chúng tôi ký hiệu g̃_{t}^{k,l} là top-a_t^l gradient của tham số đã cắt giảm với độ lớn lớn nhất trên thiết bị thứ k:

g̃_{t}^{k,l} = TopK(g_{t}^{k,l}, a_t^l), (6)

trong đó TopK(v, k) là hàm ngưỡng, các phần tử của v có giá trị tuyệt đối nhỏ hơn giá trị tuyệt đối lớn thứ k được thay thế bằng 0, và g_{t}^{k,l} là gradient của tham số đã cắt giảm trên lớp l.

[Thuật toán 2 được dịch tương tự...]

Để tính toán g̃_{t}^{k,l}, thiết bị tạo bộ đệm trong bộ nhớ để lưu trữ a_t^l gradient. Khi gradient được tính toán và bộ đệm đầy, nếu độ lớn của nó lớn hơn độ lớn nhỏ nhất trong bộ đệm, gradient này sẽ được đẩy vào bộ đệm và gradient có độ lớn nhỏ nhất sẽ bị loại bỏ. Nếu không, gradient này sẽ bị loại bỏ. Bằng cách này, thiết bị chỉ cần không gian bộ nhớ O(a_t^l) để lưu trữ gradient.

Tiếp theo, máy chủ tổng hợp tham số thưa thớt và gradient để có được tham số trung bình và gradient trung bình g̃_t^l cho mỗi lớp l,

g̃_t^l = ∑_{k=1}^K |D_k|/∑_{k=1}^K |D_k| g̃_{t}^{k,l}, (7)

trong đó |D_k| biểu thị số lượng mẫu trong bộ dữ liệu D_k.

Sau đó, máy chủ tăng trưởng a_t^l tham số đã cắt giảm với độ lớn gradient trung bình lớn nhất trên mỗi lớp l. Sau đó, máy chủ cắt giảm a_t^l tham số chưa cắt giảm (trừ các tham số vừa được tăng trưởng) có độ lớn nhỏ nhất trên mỗi lớp l.

Theo tăng trưởng và cắt giảm, máy chủ tạo ra mô hình toàn cục với cấu trúc mô hình mới, và FedTiny bắt đầu tinh chỉnh mô hình toàn cục mới. FedTiny thực hiện cắt giảm và tinh chỉnh lặp lại để đạt được mạng nơ-ron siêu nhỏ tối ưu cho tất cả thiết bị.

IV. THÍ NGHIỆM

Trong phần này, chúng tôi thực hiện các thí nghiệm toàn diện về FedTiny. Đầu tiên, chúng tôi giới thiệu cài đặt thí nghiệm và so sánh FedTiny với các cơ sở khác. Thứ hai, chúng tôi thực hiện nghiên cứu ablation để chứng minh hiệu quả của mô-đun chọn lọc chuẩn hóa theo lô thích ứng và mô-đun cắt giảm tiến bộ. Thứ ba, chúng tôi điều tra chi phí phụ trong mô-đun chuẩn hóa theo lô thích ứng và tác động của chiến lược lập lịch cắt giảm. Thứ tư, chúng tôi chứng minh hiệu quả của FedTiny trên phân bố dữ liệu không đồng nhất. Cuối cùng, chúng tôi so sánh hiệu suất giữa FedTiny và huấn luyện mô hình nhỏ.

A. Cài đặt Thí nghiệm

1) Cài đặt Học liên bang: Chúng tôi đánh giá FedTiny trên các tác vụ phân loại hình ảnh với bốn bộ dữ liệu, CIFAR-10, CIFAR-100 [41], CINIC-10 [42], và SVHN [43] trên các mô hình ResNet18 [16] và VGG11 [17]. Chúng tôi xem xét K = 10 thiết bị tổng cộng. Đối với tất cả bộ dữ liệu, chúng tôi đầu tiên tạo ra các phân vùng non-iid khác nhau trên thiết bị từ phân bố Dirichlet với α = 0.5 rồi sau đó thay đổi α trong Phần IV-F, theo cài đặt trong [44]. Chúng tôi huấn luyện các mô hình cho 300 vòng FL trên các bộ dữ liệu CIFAR-10, CIFAR-100, và CINIC-10 và 200 vòng trên bộ dữ liệu SVHN. Mỗi vòng bao gồm 5 epoch cục bộ. Kích thước mini-batch được đặt là 64.

2) Cài đặt FedTiny: Chúng tôi sử dụng các cài đặt sau trong FedTiny. Trong mô-đun chọn lọc chuẩn hóa theo lô thích ứng, máy chủ tạo ra nhóm ứng viên bằng cắt giảm độ lớn với các cài đặt tỷ lệ cắt giảm theo lớp khác nhau. Cho mật độ mục tiêu, dtarget, máy chủ xuất ra các ứng viên dưới dạng vector tỷ lệ cắt giảm theo lớp (d1, d2, . . . , dL) cho mô hình L lớp dựa trên chiến lược Uniform Noise. Chúng tôi suy ra mật độ dl cho lớp thứ l bằng cách thêm mật độ mục tiêu dtarget với nhiễu ngẫu nhiên el, tức là dl = dtarget + el. Một ứng viên có thể được thêm vào nhóm ứng viên chỉ khi mật độ tổng thể d của nó thỏa mãn d ≤ dtarget. Sau đó, máy chủ có thể có được nhóm ứng viên {θ(1),θ(2), . . . ,θ(C)} với mặt nạ {m(1),m(2), . . . ,m(C)}. Chúng tôi đầu tiên đặt kích thước nhóm ứng viên C là 50 rồi sau đó thay đổi kích thước nhóm ứng viên trong Phần IV-D. Chúng tôi đặt tỷ lệ bộ dữ liệu phát triển là 0.1, được sử dụng để cập nhật phép đo chuẩn hóa theo lô cục bộ trên thiết bị. Trong mô-đun cắt giảm tiến bộ, chúng tôi chia ResNet18 và VGG11 thành năm khối và cắt giảm một khối trong mỗi vòng, như thể hiện trong Hình 2. Thứ tự máy chủ chọn khối là ngược lại, tức là từ lớp đầu ra đến lớp đầu vào. Chúng tôi cũng đánh giá cắt giảm một lớp duy nhất và cắt giảm toàn bộ mô hình mỗi vòng trong Phần IV-E. Số lượng cắt giảm được đặt là a_t^l = 0.15(1 + cos(πt/R_stopE))n_l cho lớp l sẽ được cắt giảm tại lần lặp thứ t, trong đó n_l là số lượng tham số chưa cắt giảm trong lớp thứ l. Đối với lớp l sẽ không được cắt giảm trong lần lặp thứ t, a_t^l = 0. Chúng tôi không cắt giảm lớp chuẩn hóa theo lô, bias, lớp đầu vào, và lớp đầu ra vì chúng ảnh hưởng trực tiếp đến đầu ra mô hình. FedTiny thực hiện ∆R = 10 vòng tinh chỉnh giữa hai lần cắt giảm tinh. Khi FedTiny đạt R_stop = 100 vòng, nó dừng cắt giảm và tiếp tục tinh chỉnh. FedTiny được triển khai trên FedML [45], một nền tảng học máy mã nguồn mở cho phép học liên bang nhẹ, đa nền tảng và bảo mật có thể chứng minh.

3) Cài đặt Cơ sở: Chúng tôi bao gồm các phương pháp cơ sở sau trong nghiên cứu. Chúng tôi bao gồm SNIP, SynFlow, và FL-PQSU để xác nhận rằng cắt giảm tại khởi tạo không phải là lựa chọn thiết kế tối ưu khi dữ liệu cục bộ không nhìn thấy. Chúng tôi loại trừ các phương pháp cắt giảm FL không khả thi cho FL có bộ nhớ hạn chế. Ví dụ, FedPrune [11] và ZeroFL [28] yêu cầu thiết bị mạnh mẽ để liên tục xử lý các mô hình dày đặc.

• SNIP [22] cắt giảm mô hình bằng độ nhạy kết nối tại khởi tạo với bộ dữ liệu công khai nhỏ trên máy chủ.
• SynFlow [24] cắt giảm mô hình bằng cách bảo tồn dòng chảy synaptic lặp lại trên máy chủ trước khi huấn luyện.
• FL-PQSU [8] cắt giảm mô hình theo cách one-shot dựa trên l1-norm trên máy chủ trước khi huấn luyện. FL-PQSU cũng bao gồm phần lượng tử hóa và cập nhật chọn lọc, nhưng chúng tôi chỉ sử dụng phần cắt giảm trong FL-PQSU.
• PruneFL [13] sử dụng thiết bị mạnh mẽ để cắt giảm ban đầu mô hình và áp dụng cắt giảm tinh (cắt giảm thích ứng) trên mô hình thưa thớt dựa trên gradient trung bình kích thước đầy đủ. Nhưng tất cả thiết bị đều có tài nguyên hạn chế trong cài đặt của chúng tôi. Do đó, chúng tôi để PruneFL có được mô hình đã cắt giảm ban đầu trên máy chủ với bộ dữ liệu công khai nhỏ.
• LotteryFL [10] cắt giảm lặp lại mô hình dày đặc với tỷ lệ cắt giảm cố định trên thiết bị và tái khởi tạo mô hình đã cắt giảm với các giá trị ban đầu.
• FedDST [29] đầu tiên cắt giảm ngẫu nhiên mô hình đã cắt giảm ban đầu trên máy chủ, sau đó triển khai điều chỉnh mặt nạ trên thiết bị, và máy chủ sử dụng tổng hợp thưa thớt và cắt giảm độ lớn để có được mô hình toàn cục mới.

Vì SNIP [22] và PruneFL [13] yêu cầu một số dữ liệu cho cắt giảm thô, chúng tôi giả định rằng máy chủ cung cấp bộ dữ liệu one-shot công khai Ds để tiền huấn luyện. Tất cả cơ sở bắt đầu với mô hình được tiền huấn luyện với bộ dữ liệu one-shot Ds trên máy chủ. Đối với SNIP, chúng tôi áp dụng cắt giảm lặp lại thay vì cắt giảm one-shot như [24] đã chỉ ra. Tương tự, chúng tôi để SynFlow cắt giảm mô hình tại khởi tạo đến mật độ mục tiêu theo cách lặp lại. Đối với SNIP và SynFlow, chúng tôi đặt 100 epoch cắt giảm trên máy chủ tại khởi tạo; tham khảo [24]. Đối với FL-PQSU, ban đầu là cắt giảm có cấu trúc, chúng tôi thay đổi nó thành cắt giảm không có cấu trúc vì tất cả cơ sở khác đều là khung cắt giảm không có cấu trúc. LotteryFL [10] được thiết kế cho học liên bang được cá nhân hóa, vì vậy cấu trúc mô hình khác nhau giữa các thiết bị. Vì chúng tôi cố gắng tìm cấu trúc tối ưu cho tất cả thiết bị như trong Phương trình 1, chúng tôi để LotteryFL cắt giảm lặp lại mô hình toàn cục thay vì mô hình trên thiết bị để đảm bảo cấu trúc mô hình giống nhau cho mỗi thiết bị. FedDST [29] triển khai điều chỉnh mặt nạ trên thiết bị và tinh chỉnh các tham số trước khi tải lên. Chúng tôi để FedDST điều chỉnh mặt nạ sau 3 epoch huấn luyện cục bộ, tiếp theo là 2 epoch tinh chỉnh. Vì LotteryFL, PruneFL, FedDST, và FedTiny của chúng tôi đều cắt giảm lặp lại trong quá trình huấn luyện, chúng tôi sử dụng cùng lịch trình cắt giảm cho những khung này, nơi khung thực hiện ∆R = 10 vòng tinh chỉnh giữa hai lần cắt giảm tinh. Và khung dừng cắt giảm và tiếp tục tinh chỉnh sau R_stop = 100 vòng. Đối với PruneFL và FedDST, chúng tôi đặt số lượng cắt giảm a_t^l giống như trong FedTiny. Tất cả cơ sở sẽ áp dụng phân bố độ thưa thớt đồng đều cho cài đặt tỷ lệ cắt giảm theo lớp.

B. So sánh giữa FedTiny và Các Phương pháp Cơ sở

Để thể hiện hiệu suất của FedTiny dưới các mật độ khác nhau, chúng tôi so sánh các cơ sở và FedTiny trên bốn bộ dữ liệu (CIFAR-10, CIFAR-100, CINIC-10, và SVHN) với ResNet18. Như thể hiện trong Hình 3, FedTiny vượt trội hơn các cơ sở khác trong chế độ mật độ thấp (dtarget < 10⁻²), ví dụ, FedTiny đạt được cải thiện độ chính xác 18.91% trong bộ dữ liệu SVHN so với các phương pháp tiên tiến với mật độ 10⁻³. Điều này có lợi từ mô-đun chọn lọc chuẩn hóa theo lô thích ứng được sử dụng trong FedTiny, suy ra cấu trúc cắt giảm thô thích ứng trên máy chủ. Cấu trúc cắt giảm ban đầu này có ít độ lệch hơn, giảm kích thước không gian tìm kiếm và cải thiện sự hội tụ. Bên cạnh đó, FedTiny cạnh tranh với mật độ cao (dtarget > 10⁻²), ví dụ, FedTiny vượt trội hơn các phương pháp tiên tiến với mật độ 10⁻¹ là 1.3% trong bộ dữ liệu CIFAR-10. Mặc dù PruneFL có thể vượt trội một phần so với FedTiny dưới mật độ cao, nó yêu cầu hơn 20× chi phí tính toán và 15× dung lượng bộ nhớ để xử lý điểm số tầm quan trọng dày đặc trên thiết bị. SNIP hoạt động kém ở mật độ thấp vì SNIP có xu hướng loại bỏ gần như tất cả tham số trong một số lớp. Hơn nữa, mô hình đã cắt giảm trong SNIP phụ thuộc rất nhiều vào các mẫu trên máy chủ, điều này làm tăng độ lệch do non-iid. Chúng tôi không bao gồm LotteryFL trong Hình 3 vì việc sử dụng LotteryFL đòi hỏi huấn luyện mô hình lớn, gây ra chi phí tính toán và dung lượng bộ nhớ đáng kể. Tuy nhiên, kết quả của LotteryFL được bao gồm trong Bảng I để cung cấp so sánh toàn diện với các cơ sở khác.

Để thể hiện hiệu quả của FedTiny, chúng tôi đo chi phí huấn luyện ResNet18 và VGG11 với các mật độ khác nhau trên bộ dữ liệu CIFAR-10. Chúng tôi sử dụng số lượng phép toán điểm phẩy (FLOPs) để đo chi phí tính toán cho mỗi thiết bị. Hoạt động cắt giảm yêu cầu một lượng tính toán biến đổi mỗi vòng, dẫn đến FLOPs huấn luyện biến đổi mỗi vòng. Do đó, chúng tôi báo cáo FLOPs huấn luyện tối đa mỗi vòng (Max Training FLOPs). FLOPs huấn luyện tối đa mỗi vòng được sử dụng để đánh giá liệu thiết bị có gặp phải tính toán tốn kém trong một vòng duy nhất hay không. Chúng tôi cũng báo cáo dung lượng bộ nhớ trong thiết bị, liên quan đến chi phí bộ nhớ trong triển khai.

Bảng I thể hiện độ chính xác và chi phí huấn luyện của FedTiny được đề xuất và các cơ sở khác với các mật độ và mô hình khác nhau. Chúng tôi đánh dấu thang đo tốt nhất bằng màu đỏ và thang đo tốt thứ hai bằng màu xanh lam. Tất cả phép đo chi phí đều cho một thiết bị trong một vòng cắt giảm. Chúng tôi cũng báo cáo hiệu suất của FedAvg để thể hiện giới hạn trên của các phương pháp cắt giảm. Như thể hiện trong Bảng I, FedTiny nhằm cải thiện cả độ chính xác và hiệu quả bộ nhớ. Các công trình hiện tại không thể đạt được độ chính xác thỏa đáng ở mật độ cực thấp. FedTiny được đề xuất của chúng tôi cải thiện đáng kể độ chính xác với mức FLOPs và dung lượng bộ nhớ thấp nhất.

C. Nghiên cứu Ablation

Phần này thảo luận về hiệu quả của từng mô-đun trong FedTiny thông qua nghiên cứu ablation. Chúng tôi đánh giá lựa chọn vanilla, chọn lọc chuẩn hóa theo lô thích ứng, cắt giảm tiến bộ sau lựa chọn vanilla, và FedTiny trên bộ dữ liệu CIFAR-10 với mô hình VGG11. Hình 4 thể hiện kết quả của từng mô-đun hoạt động riêng lẻ. Chúng tôi có ba phát hiện sau. Đầu tiên, cả mô-đun chọn lọc chuẩn hóa theo lô thích ứng và mô-đun cắt giảm tiến bộ đều cải thiện hiệu suất trong lựa chọn vanilla, cho thấy hiệu quả của hai mô-đun này. Thứ hai, mô hình cắt giảm thô từ chọn lọc chuẩn hóa theo lô thích ứng gặp phải sự sụt giảm độ chính xác so với FedTiny, cho thấy vẫn còn một số độ lệch trong mô hình cắt giảm thô đã chọn, và mô-đun cắt giảm tiến bộ có thể loại bỏ chúng. Cuối cùng, mô-đun cắt giảm tiến bộ với lựa chọn vanilla đạt cùng mức độ chính xác so với FedTiny với mật độ cao (<10⁻²). Tuy nhiên, nó gặp phải sự suy giảm nghiêm trọng về độ chính xác trong chế độ mật độ thấp (>10⁻²), cho thấy mô-đun cắt giảm tiến bộ chỉ loại bỏ độ lệch ở một mức độ nhất định, và nó phải được kết hợp với mô-đun chọn lọc chuẩn hóa theo lô thích ứng trong chế độ mật độ thấp. Do đó, việc sử dụng độc lập mô-đun chọn lọc chuẩn hóa theo lô thích ứng và mô-đun cắt giảm tiến bộ có thể cải thiện hiệu suất, nhưng sự cải thiện là có hạn. Sự kết hợp của hai mô-đun, tức là FedTiny, đạt được hiệu suất dự đoán tốt nhất với mô hình siêu nhỏ.

D. Chi phí Phụ trong Mô-đun Chọn lọc BN Thích ứng

Mặc dù nhóm ứng viên lớn hơn cung cấp nhiều lựa chọn hơn cho việc chọn lọc, nó mang lại nhiều chi phí giao tiếp hơn trong mô-đun chọn lọc chuẩn hóa theo lô thích ứng. Vì vậy, chúng tôi muốn tìm kích thước nhóm tối ưu có thể cân bằng độ chính xác và chi phí phụ trong mô-đun chọn lọc chuẩn hóa theo lô thích ứng. Do đó, chúng tôi đánh giá FedTiny trên VGG11 với các kích thước nhóm khác nhau để tìm kích thước nhóm tối ưu. Chúng tôi thực hiện thí nghiệm trên bộ dữ liệu CIFAR-10 với mô hình VGG11 với các kích thước nhóm và mật độ khác nhau. Kết quả thí nghiệm được thể hiện trong Hình 5. Kết quả cho thấy việc tăng kích thước nhóm vượt quá đường xanh lá chỉ có thể mang lại sự gia tăng độ chính xác nhỏ, trong khi tăng đáng kể chi phí tính toán. Do đó, đường xanh lá phục vụ như một ngưỡng thực tế để chọn kích thước nhóm ứng viên tối ưu. Do đó, kích thước nhóm tối ưu được chọn là C* = 0.1/dtarget cho mật độ cụ thể dtarget, nơi chi phí giao tiếp trong mô-đun chọn lọc chuẩn hóa theo lô thích ứng thấp đến 20% so với mô hình VGG11 kích thước đầy đủ, và FedTiny có thể nhận được độ chính xác tương đối tốt. Kích thước nhóm lớn hơn > C* cải thiện độ chính xác nhẹ nhưng gây ra chi phí giao tiếp cao hơn nhiều.

Chúng tôi cũng tính toán FLOPs bổ sung cho mô-đun chọn lọc chuẩn hóa theo lô thích ứng với kích thước nhóm tối ưu, như thể hiện trong Bảng II. FLOPs bổ sung trong chọn lọc chuẩn hóa theo lô thích ứng ít hơn một vòng huấn luyện thưa thớt. Vì học liên bang thường liên quan đến hơn một trăm vòng huấn luyện, chi phí tính toán bổ sung có thể bỏ qua. Do đó, chúng tôi lập luận rằng chi phí phụ được giới thiệu bởi chọn lọc chuẩn hóa theo lô thích ứng là nhỏ.

E. Tác động của Chiến lược Lập lịch Cắt giảm

Mặc dù điều chỉnh theo lớp trong cắt giảm tiến bộ giảm chi phí tính toán trong một vòng, nó có thể làm chậm tốc độ hội tụ. Để xác định độ chi tiết cắt giảm và tần suất cắt giảm tốt nhất, chúng tôi đánh giá FedTiny trên VGG11 với các độ chi tiết cắt giảm khác nhau (một lớp mỗi vòng, một khối mỗi vòng, và toàn bộ mô hình mỗi vòng) và các tần suất cắt giảm khác nhau.

Bảng III thể hiện độ chính xác top-1 của các lịch trình cắt giảm khác nhau dưới các mật độ khác nhau trên VGG11 với bộ dữ liệu CIFAR-10, nơi hiệu suất tốt nhất của độ chính xác Top-1 với cùng mật độ được biểu diễn bằng màu đỏ, và thang đo tốt thứ hai được đánh dấu bằng màu xanh lam. b biểu thị chọn lớp hoặc khối để cắt giảm một cách tuần tự theo thứ tự ngược lại, tức là từ lớp đầu ra đến lớp đầu vào. Chúng tôi kiểm soát tần suất cắt giảm bằng cách đặt các vòng khoảng cách khác nhau ∆R giữa hai lần cắt giảm. Nếu độ chi tiết cắt giảm quá nhỏ (ví dụ, cắt giảm theo lớp), cấu trúc mô hình sẽ hội tụ chậm, và cấu trúc tối ưu không thể được đạt được với tài nguyên huấn luyện hạn chế. Nhưng độ chi tiết cập nhật cao dẫn đến tính toán tốn kém hơn trong một vòng. Chúng tôi thấy rằng cắt giảm một khối mỗi vòng là lựa chọn tối ưu cho mô-đun cắt giảm tiến bộ. Hơn nữa, chọn tuần tự các khối để cắt giảm theo thứ tự ngược lại (từ lớp đầu ra đến lớp đầu vào) có kết quả tốt hơn so với thứ tự tiến vì lan truyền gradient là ngược lại, và chúng tôi sử dụng gradient để điều chỉnh cấu trúc mô hình.

F. Hiệu quả của FedTiny trên Phân bố Dữ liệu Không đồng nhất

Cắt giảm mạng nơ-ron yêu cầu dữ liệu huấn luyện để xác định cấu trúc mô hình phù hợp. Do thiết bị có tài nguyên hạn chế, máy chủ không thể đẩy mô hình dày đặc lên thiết bị. Do đó, máy chủ cần cắt giảm thô để tạo ra mô hình đã cắt giảm ban đầu. Do mối quan tâm về quyền riêng tư trong học liên bang, máy chủ không thể biết phân bố dữ liệu cho tất cả thiết bị. Trong các phương pháp hiện tại, máy chủ chỉ cắt giảm thô mô hình dựa trên bộ dữ liệu được tiền huấn luyện hoặc dữ liệu từ một số thiết bị đáng tin cậy. Điều này làm cho bộ dữ liệu được sử dụng cho cắt giảm khác với bộ dữ liệu được sử dụng cho tinh chỉnh, gây ra độ lệch trong cắt giảm thô. Do đó, chiến lược của chúng tôi là sử dụng chọn lọc chuẩn hóa theo lô thích ứng để chọn một mô hình đã cắt giảm có ít độ lệch hơn.

Để chứng minh hiệu quả của FedTiny trên phân bố dữ liệu không đồng nhất, chúng tôi đặt các mức độ non-iid khác nhau bằng cách sử dụng α khác nhau trong phân bố Dirichlet. α thấp hơn chỉ ra mức độ non-iid cao hơn. Chúng tôi thực hiện thí nghiệm trên bộ dữ liệu CIFAR-10 với ResNet18 với mật độ 1%. Các thí nghiệm được thể hiện trong Hình 6. Thí nghiệm của chúng tôi cho thấy rằng 1) hiệu suất của các phương pháp cắt giảm hiện tại (ví dụ, SynFlow, PruneFL) trong Học liên bang sẽ bị suy giảm đáng kể với mức độ non-iid cao hơn; 2) FedTiny được đề xuất của chúng tôi giảm thiểu độ lệch trong cắt giảm và đạt hiệu suất tốt nhất so với các phương pháp cắt giảm hiện tại.

G. So sánh giữa FedTiny và Huấn luyện Mô hình Nhỏ

Trong các thí nghiệm trước, chúng tôi so sánh FedTiny với các phương pháp cắt giảm hiện tại. Để điều tra thêm hiệu quả của FedTiny, chúng tôi so sánh FedTiny với các mô hình nhỏ dày đặc mà không cắt giảm. FedTiny vượt trội hơn các cơ sở khác trong mô hình rất thưa thớt, như mật độ 1%. Trong trường hợp này, huấn luyện mô hình nhỏ dày đặc mà không cắt giảm cũng có thể được coi là một cơ sở. Do đó, chúng tôi thiết kế thí nghiệm trên các mô hình nhỏ. Chúng tôi huấn luyện mô hình nhỏ với ba lớp tích chập. Đầu tiên, chúng tôi đánh giá mô hình nhỏ với số lượng tham số tương tự như ResNet18 với mật độ 1% trên các bộ dữ liệu khác nhau. Thứ hai, chúng tôi đánh giá mạng nhỏ với số lượng tham số tương tự như ResNet18 với các mật độ khác nhau trên CIFAR-10. Chúng tôi cũng chọn SynFlow và PruneFL làm tham chiếu. Kết quả thí nghiệm được thể hiện trong Bảng IV và Bảng V, nơi hiệu suất tốt nhất của độ chính xác Top-1 với cùng bộ dữ liệu và cùng mật độ được biểu diễn bằng màu đỏ, và thang đo tốt thứ hai được đánh dấu bằng màu xanh lam. Kết quả thí nghiệm cho thấy mô hình nhỏ cạnh tranh so với các cơ sở khác. Tuy nhiên, FedTiny được đề xuất của chúng tôi đạt hiệu suất tốt hơn nhiều so với mô hình nhỏ, điều này chứng minh lợi thế của FedTiny.

V. KẾT LUẬN

Bài báo này phát triển một khung cắt giảm phân tán mới có tên FedTiny. FedTiny cho phép huấn luyện cục bộ hiệu quả về bộ nhớ và xác định các mô hình siêu nhỏ chuyên biệt trong học liên bang cho các tình huống triển khai khác nhau (nền tảng phần cứng tham gia và tác vụ huấn luyện). FedTiny giải quyết các thách thức về độ lệch, tính toán tốn kém và sử dụng bộ nhớ mà nghiên cứu cắt giảm liên bang hiện tại gặp phải. FedTiny giới thiệu hai mô-đun quan trọng: mô-đun chọn lọc chuẩn hóa theo lô thích ứng và mô-đun cắt giảm tiến bộ nhẹ nhàng. Mô-đun chọn lọc chuẩn hóa theo lô được thiết kế để giảm thiểu độ lệch trong cắt giảm do tính không đồng nhất của dữ liệu cục bộ, trong khi mô-đun cắt giảm tiến bộ cho phép cắt giảm chi tiết dưới ngân sách tính toán và bộ nhớ nghiêm ngặt. Cụ thể, nó xác định dần dần chính sách cắt giảm cho từng lớp thay vì đánh giá toàn bộ cấu trúc mô hình. Kết quả thí nghiệm chứng minh hiệu quả của FedTiny khi so sánh với các phương pháp tiên tiến. Đặc biệt, FedTiny đạt được những cải thiện đáng kể về độ chính xác, FLOPs và dung lượng bộ nhớ khi nén các mô hình sâu thành các mô hình siêu nhỏ cực kỳ thưa thớt. Kết quả trên bộ dữ liệu CIFAR-10 cho thấy FedTiny vượt trội hơn các phương pháp tiên tiến bằng cách đạt được cải thiện độ chính xác 2.61% trong khi đồng thời giảm FLOPs 95.9% và dung lượng bộ nhớ 94.0%. Kết quả thí nghiệm chứng minh hiệu quả và hiệu suất của FedTiny trong cài đặt học liên bang.

VI. LỜI CẢM ƠN

Công trình này được hỗ trợ một phần bởi Quỹ Khoa học Quốc gia (CCF-2221741, CCF-2106754, CNS-2151238, CNS-2153381), Giải thưởng Nâng cao Giảng viên Trẻ Ralph E. Powe của ORAU, và Hội đồng Nghiên cứu Hồng Kông, Quỹ Nghiên cứu Tổng quát (GRF) theo Hợp đồng 11203523.

TÀI LIỆU THAM KHẢO

[1] S. A. Janowsky, "Pruning versus clipping in neural networks," Physical Review A, vol. 39, no. 12, p. 6600, 1989.

[2] S. Han, H. Mao, and W. J. Dally, "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding," arXiv preprint arXiv:1510.00149, 2015.

[3] C. Louizos, M. Welling, and D. P. Kingma, "Learning sparse neural networks through l0 regularization," in International Conference on Learning Representations, 2018.

[4] R. Yu, A. Li, C.-F. Chen, J.-H. Lai, V. I. Morariu, X. Han, M. Gao, C.-Y. Lin, and L. S. Davis, "Nisp: Pruning networks using neuron importance score propagation," in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9194–9203, 2018.

[5] P. Molchanov, A. Mallya, S. Tyree, I. Frosio, and J. Kautz, "Importance estimation for neural network pruning," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11264–11272, 2019.

[6] S. P. Singh and D. Alistarh, "Woodfisher: Efficient second-order approximation for neural network compression," Advances in Neural Information Processing Systems, vol. 33, pp. 18098–18109, 2020.

[7] T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, "Federated learning: Challenges, methods, and future directions," IEEE Signal Processing Magazine, vol. 37, no. 3, pp. 50–60, 2020.

[8] W. Xu, W. Fang, Y. Ding, M. Zou, and N. Xiong, "Accelerating federated learning for iot in big data analytics with pruning, quantization and selective updating," IEEE Access, vol. 9, pp. 38457–38466, 2021.

[9] R. Shao, H. Liu, and D. Liu, "Privacy preserving stochastic channel-based federated learning with neural network pruning," arXiv preprint arXiv:1910.02115, 2019.

[10] A. Li, J. Sun, B. Wang, L. Duan, S. Li, Y. Chen, and H. Li, "Lotteryfl: Empower edge intelligence with personalized and communication-efficient federated learning," in 2021 IEEE/ACM Symposium on Edge Computing (SEC), pp. 68–79, IEEE, 2021.

[11] M. T. Munir, M. M. Saeed, M. Ali, Z. A. Qazi, and I. A. Qazi, "Fedprune: Towards inclusive federated learning," arXiv preprint arXiv:2110.14205, 2021.

[12] S. Liu, G. Yu, R. Yin, and J. Yuan, "Adaptive network pruning for wireless federated learning," IEEE Wireless Communications Letters, vol. 10, no. 7, pp. 1572–1576, 2021.

[13] Y. Jiang, S. Wang, V. Valls, B. J. Ko, W.-H. Lee, K. K. Leung, and L. Tassiulas, "Model pruning enables efficient federated learning on edge devices," IEEE Transactions on Neural Networks and Learning Systems, 2022.

[14] U. Evci, T. Gale, J. Menick, P. S. Castro, and E. Elsen, "Rigging the lottery: Making all tickets winners," in International Conference on Machine Learning, pp. 2943–2952, PMLR, 2020.

[15] S. Ioffe and C. Szegedy, "Batch normalization: Accelerating deep network training by reducing internal covariate shift," in International conference on machine learning, pp. 448–456, PMLR, 2015.

[16] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.

[17] K. Simonyan and A. Zisserman, "Very deep convolutional networks for large-scale image recognition," arXiv preprint arXiv:1409.1556, 2014.

[18] T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste, "Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks," Journal of Machine Learning Research, vol. 22, no. 241, pp. 1–124, 2021.

[19] M. C. Mozer and P. Smolensky, "Skeletonization: A technique for trimming the fat from a network via relevance assessment," Advances in neural information processing systems, vol. 1, 1988.

[20] Y. LeCun, J. Denker, and S. Solla, "Optimal brain damage," Advances in neural information processing systems, vol. 2, 1989.

[21] P. Molchanov, S. Tyree, T. Karras, T. Aila, and J. Kautz, "Pruning convolutional neural networks for resource efficient inference," in 5th International Conference on Learning Representations, ICLR 2017-Conference Track Proceedings, 2019.

[22] N. Lee, T. Ajanthan, and P. Torr, "Snip: Single-shot network pruning based on connection sensitivity," in International Conference on Learning Representations, 2018.

[23] C. Wang, G. Zhang, and R. Grosse, "Picking winning tickets before training by preserving gradient flow," in International Conference on Learning Representations, 2019.

[24] H. Tanaka, D. Kunin, D. L. Yamins, and S. Ganguli, "Pruning neural networks without any data by iteratively conserving synaptic flow," Advances in Neural Information Processing Systems, vol. 33, pp. 6377–6389, 2020.

[25] D. C. Mocanu, E. Mocanu, P. Stone, P. H. Nguyen, M. Gibescu, and A. Liotta, "Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science," Nature communications, vol. 9, no. 1, pp. 1–12, 2018.

[26] T. Dettmers and L. Zettlemoyer, "Sparse networks from scratch: Faster training without losing performance," arXiv preprint arXiv:1907.04840, 2019.

[27] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas, "Communication-efficient learning of deep networks from decentralized data," in Artificial intelligence and statistics, pp. 1273–1282, PMLR, 2017.

[28] X. Qiu, J. Fernandez-Marques, P. P. Gusmao, Y. Gao, T. Parcollet, and N. D. Lane, "Zerofl: Efficient on-device training for federated learning with local sparsity," arXiv preprint arXiv:2208.02507, 2022.

[29] S. Bibikar, H. Vikalo, Z. Wang, and X. Chen, "Federated dynamic sparse training: Computing less, communicating less, yet learning better," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, pp. 6080–6088, 2022.

[30] Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, "Federated learning with non-iid data," arXiv preprint arXiv:1806.00582, 2018.

[31] O. Marfoq, C. Xu, G. Neglia, and R. Vidal, "Throughput-optimal topology design for cross-silo federated learning," Advances in Neural Information Processing Systems, vol. 33, pp. 19478–19487, 2020.

[32] J. Wang, Q. Liu, H. Liang, G. Joshi, and H. V. Poor, "Tackling the objective inconsistency problem in heterogeneous federated optimization," Advances in neural information processing systems, vol. 33, pp. 7611–7623, 2020.

[33] M. Duan, D. Liu, X. Chen, Y. Tan, J. Ren, L. Qiao, and L. Liang, "Astraea: Self-balancing federated learning for improving classification accuracy of mobile deep learning applications," in 2019 IEEE 37th international conference on computer design (ICCD), pp. 246–254, IEEE, 2019.

[34] Z. Li, Y. He, H. Yu, J. Kang, X. Li, Z. Xu, and D. Niyato, "Data heterogeneity-robust federated learning via group client selection in industrial iot," IEEE Internet of Things Journal, 2022.

[35] W. Zhang, X. Wang, P. Zhou, W. Wu, and X. Zhang, "Client selection for federated learning with non-iid data in mobile edge computing," IEEE Access, vol. 9, pp. 24462–24474, 2021.

[36] F. Chen, M. Luo, Z. Dong, Z. Li, and X. He, "Federated meta-learning with fast convergence and efficient communication," arXiv preprint arXiv:1802.07876, 2018.

[37] V. Smith, C.-K. Chiang, M. Sanjabi, and A. S. Talwalkar, "Federated multi-task learning," Advances in neural information processing systems, vol. 30, 2017.

[38] T. Lin, L. Kong, S. U. Stich, and M. Jaggi, "Ensemble distillation for robust model fusion in federated learning," Advances in Neural Information Processing Systems, vol. 33, pp. 2351–2363, 2020.

[39] Y. He, J. Lin, Z. Liu, H. Wang, L.-J. Li, and S. Han, "Amc: Automl for model compression and acceleration on mobile devices," in Proceedings of the European conference on computer vision (ECCV), pp. 784–800, 2018.

[40] B. Li, B. Wu, J. Su, and G. Wang, "Eagleeye: Fast sub-net evaluation for efficient neural network pruning," in European conference on computer vision, pp. 639–654, Springer, 2020.

[41] A. Krizhevsky, G. Hinton, et al., "Learning multiple layers of features from tiny images," 2009.

[42] L. N. Darlow, E. J. Crowley, A. Antoniou, and A. J. Storkey, "Cinic-10 is not imagenet or cifar-10," arXiv preprint arXiv:1810.03505, 2018.

[43] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, "Reading digits in natural images with unsupervised feature learning," 2011.

[44] M. Luo, F. Chen, D. Hu, Y. Zhang, J. Liang, and J. Feng, "No fear of heterogeneity: Classifier calibration for federated learning with non-iid data," Advances in Neural Information Processing Systems, vol. 34, pp. 5972–5984, 2021.

[45] C. He, S. Li, J. So, X. Zeng, M. Zhang, H. Wang, X. Wang, P. Vepakomma, A. Singh, H. Qiu, et al., "Fedml: A research library and benchmark for federated machine learning," arXiv preprint arXiv:2007.13518, 2020.
