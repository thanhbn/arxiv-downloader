Phương pháp Song song Hybrid Tensor-Expert-Data để Tối ưu hóa Huấn luyện Mixture-of-Experts

Siddharth Singh
ssingh37@umd.edu
Khoa Khoa học Máy tính,
Đại học Maryland
College Park, Maryland, USA

Olatunji Ruwase
olruwase@microsoft.com
Microsoft, Inc.
Redmond, Washington, USA

Ammar Ahmad Awan
ammar.awan@microsoft.com
Microsoft, Inc.
Redmond, Washington, USA

Samyam Rajbhandari
samyamr@microsoft.com
Microsoft, Inc.
Redmond, Washington, USA

Yuxiong He
yuxhe@microsoft.com
Microsoft, Inc.
Redmond, Washington, USA

Abhinav Bhatele
bhatele@cs.umd.edu
Khoa Khoa học Máy tính,
Đại học Maryland
College Park, Maryland, USA

TÓM TẮT
Mixture-of-Experts (MoE) là một kiến trúc mạng nơ-ron thêm các khối chuyên gia được kích hoạt thưa vào mô hình cơ sở, tăng số lượng tham số mà không ảnh hưởng đến chi phí tính toán. Tuy nhiên, các framework học sâu phân tán hiện tại có hạn chế trong khả năng huấn luyện các mô hình MoE chất lượng cao với các mô hình cơ sở lớn. Trong công trình này, chúng tôi trình bày DeepSpeed-TED, một thuật toán song song hybrid ba chiều mới kết hợp data, tensor, và expert parallelism để cho phép huấn luyện các mô hình MoE với mô hình cơ sở lớn hơn 4-8× so với hiện tại. Chúng tôi cũng mô tả các tối ưu hóa bộ nhớ trong bước optimizer và các tối ưu hóa giao tiếp loại bỏ việc di chuyển dữ liệu không cần thiết. Chúng tôi triển khai phương pháp của mình trong DeepSpeed và đạt được tốc độ nhanh hơn 26% so với baseline (tức là không có tối ưu hóa giao tiếp) khi huấn luyện mô hình MoE 40 tỷ tham số (mô hình cơ sở 6.7 tỷ với 16 chuyên gia) trên 128 GPU V100.

KHÁI NIỆM CCS
• Phương pháp tính toán → Thuật toán song song đại trà; Sinh ngôn ngữ tự nhiên.

TỪ KHÓA
Học sâu song song, Mixture-of-Experts, Tensor Parallelism, Expert Parallelism

Định dạng tham chiếu ACM:
Siddharth Singh, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He, và Abhinav Bhatele. 2023. Phương pháp Song song Hybrid Tensor-Expert-Data để Tối ưu hóa Huấn luyện Mixture-of-Experts. Trong Hội nghị Quốc tế về Siêu tính toán 2023 (ICS '23), 21-23 tháng 6, 2023, Orlando, FL, USA. ACM, New York, NY, USA, 12 trang.

1 GIỚI THIỆU
Các thuật toán AI hiện đại tiên tiến đã dựa vào các mạng nơ-ron như GPT-3 và MT-NLG với hàng trăm tỷ tham số. Tuy nhiên, việc huấn luyện hoặc chạy suy luận trên các mô hình có kích thước này đã trở nên cực kỳ đắt đỏ do chi phí tính toán khổng lồ. Để giảm thiểu vấn đề này, các nhà nghiên cứu học sâu đã chuyển sự chú ý đến kiến trúc Mixture-of-Experts (MoE), cung cấp cách tăng số lượng tham số của mô hình mà không tăng chi phí tính toán. Các mô hình MoE mở rộng các lớp của mô hình transformer vanilla (gọi là mô hình cơ sở trong thuật ngữ MoE) bằng nhiều "chuyên gia" hoặc khối feedforward và một hàm định tuyến có tham số ánh xạ duy nhất mỗi token đầu vào đến một chuyên gia duy nhất. Hình 1 minh họa lượt forward của một lớp MoE đơn với hai chuyên gia và một batch đầu vào của hai token. Vì mỗi token chỉ được xử lý bởi một chuyên gia, chi phí tính toán hiệu quả trên mỗi token (và do đó tổng chi phí huấn luyện) vẫn cố định (so với mô hình cơ sở) và không phụ thuộc vào số lượng chuyên gia.

Thật không may, có giới hạn về cải thiện chất lượng mô hình có thể đạt được bằng cách đơn giản tăng số lượng chuyên gia. Ví dụ, trong thí nghiệm nghiên cứu ảnh hưởng của việc thêm chuyên gia vào kiến trúc T5 làm mô hình cơ sở, Fedus et al. quan sát thấy cải thiện giảm dần trong độ chính xác tập kiểm tra vượt quá 64-128 chuyên gia. Thực tế, để huấn luyện các mô hình Mixture-of-Experts chất lượng cao, cần thiết phải tăng kích thước mô hình cơ sở (số lượng tham số) cùng với số lượng chuyên gia.

Về vấn đề này, các framework học sâu phân tán hiện tại không đủ để huấn luyện các MoE như vậy với mô hình cơ sở lớn. Chúng hoặc hỗ trợ mô hình cơ sở kích thước hạn chế hoặc sử dụng thuật toán song song không hiệu quả dẫn đến chi phí giao tiếp cao. Do đó, quan trọng là phải phát triển framework phân tán có thể hỗ trợ huấn luyện MoE với mô hình cơ sở lớn trên cluster multi-GPU và làm điều đó hiệu quả, đồng thời giữ chi phí giao tiếp thấp. Trong công trình này, chúng tôi trình bày framework song song hybrid ba chiều, DeepSpeed-TED, kết hợp data parallelism của ZeRO, tensor parallelism của MegatronLM, và expert parallelism của DeepSpeed-MoE để huấn luyện mô hình MoE được xây dựng bằng mô hình cơ sở cực lớn. Chúng tôi chứng minh cách kết hợp ba chiều song song này cho phép framework của chúng tôi huấn luyện mô hình cơ sở lớn hơn 4-8× so với DeepSpeed-MoE, một framework song song tiên tiến chỉ sử dụng hai trong số các chiều này (data và expert). Theo hiểu biết của chúng tôi, đây là nỗ lực đầu tiên kết hợp ba thuật toán học sâu song song tiên tiến này để huấn luyện MoE trên cluster multi-GPU.

Chúng tôi xác định và giải quyết hai thắt cổ chai xuất hiện với sự kết hợp ngây thơ của ba hình thức song song này. Thứ nhất là sự gia tăng đáng kể trong sử dụng bộ nhớ trong optimizer, hạn chế kích thước mô hình cơ sở được hỗ trợ bởi phương pháp song song hybrid của chúng tôi. Để giảm thiểu vấn đề này, chúng tôi đề xuất phiên bản tiled của optimizer xử lý tham số mô hình theo nhóm (hoặc tile) có kích thước cố định, và giảm tiêu thụ bộ nhớ đỉnh bằng cách tái sử dụng bộ nhớ GPU qua các tile tham số. Thắt cổ chai thứ hai liên quan đến chi phí giao tiếp, nơi một lượng đáng kể thời gian huấn luyện được dành cho giao tiếp tập thể liên quan đến expert và tensor parallelism. Chúng tôi xác định hai vùng trong quy trình huấn luyện hybrid này nơi tin nhắn được giao tiếp không cần thiết giữa các GPU worker, và đề xuất các tối ưu hóa giao tiếp mới giải quyết vấn đề này. Đối với mô hình MoE 40 tỷ tham số (mô hình cơ sở 6.7 tỷ với 16 chuyên gia) trên 128 GPU V100 của Summit, các tối ưu hóa của chúng tôi giảm tổng thời gian giao tiếp tập thể 42% và dẫn đến cải thiện đáng kể 26% trong thời gian huấn luyện. DeepSpeed-TED là mã nguồn mở, và đã được tích hợp trong DeepSpeed, một framework học sâu phân tán tiên tiến.

Những đóng góp chính của bài báo này như sau:
• Một framework song song hybrid ba chiều có khả năng mở rộng cao đầu tiên kết hợp data parallelism của ZeRO, tensor parallelism của Megatron-LM, và expert parallelism của DeepSpeed-MoE để cho phép huấn luyện Mixture-of-Experts với mô hình cơ sở lớn.
• Một phiên bản tiled của optimizer giảm thiểu sự tăng bộ nhớ đáng kể trong bước optimizer phát sinh từ việc kết hợp ba hình thức song song nói trên.
• Các tối ưu hóa giao tiếp loại bỏ giao tiếp không cần thiết trong thuật toán song song hybrid của chúng tôi, giảm đáng kể thời gian giao tiếp tập thể.

2 KIẾN THỨC NỀN TẢNG
Trong phần này, chúng tôi cung cấp kiến thức nền tảng về Mixture-of-Experts (MoE), và ba hình thức song song được sử dụng trong công trình này - tensor parallelism, expert parallelism, và data parallelism.

2.1 Mixture-of-Experts
Được đề xuất bởi Shazeer et al. vào năm 2017, Mixture-of-Experts (MoE) là một họ kiến trúc mạng nơ-ron với tính chất thú vị rằng tập tham số của chúng có thể được tạo ra lớn tùy ý mà không tăng chi phí tính toán. Điều này đạt được bằng cách thêm các khối chuyên gia được kích hoạt thưa vào các lớp của mạng nơ-ron dày đặc (gọi là mô hình cơ sở). Một hàm định tuyến có tham số được thêm trước các khối chuyên gia này để ánh xạ các token đầu vào của nó đến một chuyên gia duy nhất. Vì mỗi token chỉ được tính toán bởi một chuyên gia, tổng chi phí tính toán huấn luyện vẫn cố định (giống như mô hình cơ sở) và không phụ thuộc vào số lượng chuyên gia. Do đó MoE cung cấp cách duy nhất để tăng số lượng tham số của mô hình cơ sở cho trước, và do đó hiệu suất của nó trên bất kỳ nhiệm vụ nào, mà không có bất kỳ sự gia tăng nào trong chi phí tính toán. Mặc dù Shazeer et al. sử dụng LSTM làm mô hình cơ sở, công trình đương đại về MoE chủ yếu sử dụng kiến trúc transformer làm mô hình cơ sở.

2.2 Data Parallelism và ZeRO
Dưới data parallelism, các GPU worker chứa một bản sao của mạng nơ-ron và làm việc trên các shard loại trừ lẫn nhau của batch đầu vào. Sau lượt backward, chúng đồng bộ hóa gradient cục bộ thông qua lệnh gọi hàm all-reduce. Tuy nhiên, một hạn chế chính của data parallelism là mỗi GPU phải có đủ bộ nhớ để lưu trữ toàn bộ tập tham số của mạng nơ-ron, cũng như gradient và trạng thái optimizer của nó. Để giải quyết vấn đề này, Rajbhandari et al. đề xuất Zero Redundancy Optimizer hoặc ZeRO nhằm loại bỏ tiêu thụ bộ nhớ dư thừa này trong các GPU data parallel. Phương pháp của họ có ba giai đoạn, dần dần tiết kiệm nhiều bộ nhớ hơn mặc dù với chi phí giao tiếp tăng. Trong công trình này, chúng tôi xem xét giai đoạn đầu tiên của tối ưu hóa của họ, chỉ phân mảnh các trạng thái optimizer qua các rank data parallel.

2.3 Expert Parallelism và DeepSpeed-MoE
Sau khi định tuyến, việc tính toán của một khối chuyên gia trong lớp MoE độc lập với các chuyên gia khác. Expert parallelism khai thác tính chất này bằng cách đặt các khối chuyên gia duy nhất trên mỗi GPU và tính toán chúng theo cách song song dễ dàng. Các token được ánh xạ đến các chuyên gia tương ứng bằng giao tiếp all-to-all trong các GPU tham gia. Do sự đơn giản và hiệu quả, expert parallelism được sử dụng trong nhiều framework song song để huấn luyện hoặc chạy suy luận trên MoE. Trong công trình này, chúng tôi sử dụng triển khai expert parallelism của DeepSpeed-MoE.

2.4 Tensor Parallelism và Megatron-LM
Tensor parallelism liên quan đến việc phân vùng tính toán của lớp mạng nơ-ron qua các GPU. Shoeybi et al. giới thiệu MegatronLM, một thuật toán tensor parallel để song song hóa tính toán các lớp trong mạng nơ-ron transformer. Phương pháp của họ nhằm song song hóa một cặp lớp kết nối đầy đủ liên tiếp, được tìm thấy trong các khối self-attention và feedforward của transformer. Thuật toán của họ đã thấy việc áp dụng đáng kể để huấn luyện nhiều mô hình ngôn ngữ lớn như Megatron-Turing NLG 530B, Bloom-176 B, Turing NLG, v.v.

3 TED: PHƯƠNG PHÁP SONG SONG HYBRID TENSOR-EXPERT-DATA
Bằng cách thêm các chuyên gia được kích hoạt thưa, kiến trúc Mixture-of-Experts cho phép chúng ta tạo ra một mạng nơ-ron cho trước, tức là mô hình cơ sở, lớn tùy ý trong khi giữ chi phí tính toán của nó không thay đổi. Tuy nhiên, chỉ đơn giản tăng số lượng chuyên gia mang lại lợi ích giảm dần trong tổng quát hóa mô hình vượt quá 64-128 chuyên gia. Để xây dựng MoE chất lượng cao, cần thiết phải tăng kích thước mô hình cơ sở cũng như số lượng chuyên gia. Trong phần này, chúng tôi cung cấp tổng quan về TED, phương pháp song song hybrid của chúng tôi kết hợp expert parallelism của DeepSpeed-MoE, tensor parallelism của MegatronLM và data parallelism của ZeRO, để cho phép huấn luyện các MoE như vậy với mô hình cơ sở multi-tỷ tham số cực lớn trên cluster multi-GPU. Trong công trình này, chúng tôi sử dụng giai đoạn đầu tiên của ZeRO, phân mảnh các trạng thái optimizer qua các GPU data parallel. Trong khi các giai đoạn tiếp theo của tối ưu hóa của họ (stage-2, 3, offload và infinity) có thể hỗ trợ huấn luyện các mô hình lớn hơn, điều này xảy ra với chi phí hiệu suất.

Chúng tôi sử dụng thuật ngữ khối non-expert và expert thay thế cho khối self-attention và feedforward tương ứng. Lưu ý rằng TED song song hóa việc tính toán các khối expert và non-expert theo cách khác nhau. Điều này là do expert parallelism chỉ áp dụng cho các khối feedforward của mô hình transformer cơ sở. Do đó, TED sử dụng hybrid hai chiều của tensor và data parallelism để song song hóa các khối non-expert. Trong khi đó, nó sử dụng cả ba tensor, expert, và data parallelism cho các khối expert.

Dưới TED, chúng tôi tổ chức các GPU có sẵn thành hai cấu trúc liên kết ảo khác nhau cho các khối non-expert và expert. Chúng tôi minh họa các cấu trúc liên kết này trong Hình 2.

Đối với các khối non-expert, chúng tôi duy trì cấu trúc liên kết GPU hai chiều (2D), mỗi chiều cho tensor và data parallelism. Trong cấu trúc liên kết này, các GPU trong một hàng thực hiện tensor parallelism, và chúng tôi gọi một hàng GPU là nhóm tensor parallel. Tương tự, TED thực hiện data parallelism qua các cột GPU, và chúng tôi gọi các cột này là nhóm data parallel. Tương tự, đối với các khối expert, chúng tôi duy trì cấu trúc liên kết GPU ba chiều (3D), mỗi chiều cho tensor, expert, và data parallelism. Để tạo thành các nhóm tensor parallel cho các khối expert, chúng tôi tái sử dụng các nhóm tensor parallel được tạo thành trong cấu trúc liên kết 2D cho các khối non-expert. Tuy nhiên, chúng tôi phân rã thêm các nhóm data parallel của các khối non-expert thành cấu trúc liên kết 2D để tạo thành các nhóm cho expert parallelism và data parallelism cho các khối expert. Chúng tôi định nghĩa Gtensor và Gnonexp_data là kích thước của tensor parallel và non-expert data parallel groups tương ứng. Tương tự, chúng tôi định nghĩa Gexpert và Gexp_data là kích thước của expert parallel và expert data parallel groups tương ứng. Theo công trình trước đó, chúng tôi luôn đặt Gexpert bằng số lượng chuyên gia trong mô hình vì lý do hiệu suất. Lưu ý rằng cho một số GPU, G, quan hệ sau luôn đúng:

Gtensor × Gexpert × Gexp_data = Gtensor × Gnonexp_data = G (1)

Trong Hình 3, chúng tôi minh họa lượt forward của lớp MoE với hai chuyên gia trên bốn GPU. Như đã đề cập trước đó, chúng tôi đặt Gexpert bằng số lượng chuyên gia tức là 2. Các mức độ song song khác là Gtensor = 2, Gnonexp_data = 2, Gexpert = 2, và Gexp_data = 1. Chúng tôi phân vùng các tham số của khối self-attention (non-expert) và hai khối feed forward (expert) theo ngữ nghĩa của tensor parallelism của MegatronLM và đặt phân vùng đầu tiên trên GPU 0 và 2 và phân vùng thứ hai trên GPU 1 và 3. Do đó GPU (0,1) và (2,3) tạo thành hai nhóm tensor parallel. Các cặp GPU (0,2) và (1,3) bao gồm các nhóm data parallel cho tham số non-expert. Tuy nhiên, cùng các cặp GPU này bao gồm các nhóm expert parallel cho tham số expert. Bốn GPU riêng lẻ tạo thành các nhóm data parallel singleton cho tham số expert.

Bây giờ hãy thảo luận về cách thuật toán song song hybrid của chúng tôi tính toán lượt forward của lớp MoE. Làm ví dụ, chúng tôi sử dụng batch đầu vào với bốn token (đánh số 1-4) trong Hình 3. Nhóm tensor parallel của GPU 0 và 1 tính toán trên token 1 và 2, trong khi nhóm tensor parallel của GPU 2 và 3 tính toán trên token 3 và 4. Mỗi GPU đầu tiên tính toán phân vùng của họ cho khối self-attention (1) và sau đó phát ra all-reduce (2) để tổng hợp các kích hoạt đầu ra hoàn chỉnh (tiền tố bằng 'a') cho token tương ứng của họ. Bây giờ, mỗi GPU áp dụng hàm định tuyến MoE cho token cục bộ của họ (3). Chúng tôi giả định rằng hàm định tuyến ánh xạ token 1 và 3 đến chuyên gia đầu tiên tức là feedforward 1, và token 2 và 4 đến chuyên gia thứ hai tức là Feedforward 2. (4) Bây giờ, một primitive giao tiếp all-to-all được phát ra trong các nhóm expert parallel để định tuyến token theo ánh xạ quyết định bởi hàm định tuyến. Hãy xem nhóm expert parallel của GPU 0 và 2 để hiểu lệnh gọi giao tiếp all-to-all này. Trên GPU 0, token 1 đã được ánh xạ đến chuyên gia đầu tiên và token 2 đã được ánh xạ đến chuyên gia thứ hai. Do đó, chúng tôi muốn giữ lại a1 và gửi a2 đến GPU 2 chứa chuyên gia thứ hai. Tương tự, trên GPU 2, chúng tôi muốn giữ lại a4 và gửi a3 đến GPU 0. Lưu ý rằng mẫu giao tiếp này khớp với ngữ nghĩa của primitive giao tiếp all-to-all chính xác. Sau khi all-to-all đã hoàn thành, mỗi GPU tính toán các phân vùng tensor-parallel của họ cho các khối feed forward chuyên gia (5) và phát ra all-reduce để tổng hợp đầu ra hoàn chỉnh (6). Lệnh gọi giao tiếp all-to-all cuối cùng trong các nhóm expert parallel (7) về cơ bản đảo ngược all-to-all đầu tiên (4) và đưa các token trở lại GPU gốc của chúng. Đây là cách phương pháp song song hybrid ba chiều của chúng tôi tính toán lượt forward của lớp MoE.

Trong quá trình tính toán lượt backward, việc tính toán tiến hành theo hướng ngược lại tức là (7-1). Các lệnh gọi giao tiếp all-to-all tại 7 và 4 được đảo ngược. Ví dụ, xem xét 7, trong đó đầu vào cho all-to-all trên GPU 0 sẽ là gradient của loss đối với f1 và f2. Tương tự cho GPU 2, nó sẽ là gradient đối với f3 và f4. Bây giờ, sau all-to-all, đầu ra trên GPU 0 sẽ là gradient của loss đối với f1 và f3, và trên GPU 2 sẽ là gradient đối với f2 và f4. Các lệnh gọi hàm all-reduce (4,6) được áp dụng cho gradient đối với kích hoạt đầu vào thay vì đầu ra. Để biết thêm chi tiết về lệnh gọi all-reduce này, chúng tôi giới thiệu độc giả đến Narayanan et al. Lưu ý rằng tổng lượng giao tiếp tức là hai all-reduce và hai all-to-all giống như của lượt forward. Cuối cùng, các nhóm data parallel đồng bộ hóa gradient của họ thông qua một lệnh gọi all-reduce khác, hoàn thành lượt backward.

3.1 Mô hình cho Tiêu thụ Bộ nhớ
Bây giờ chúng tôi suy ra mức độ mà TED có thể tăng kích thước mô hình cơ sở so với công trình trước đó như DeepSpeed-MoE, chỉ sử dụng data và expert parallelism. Theo công trình trước đó, chúng tôi giả định rằng mỗi lớp thay phiên có các mô-đun feedforward chuyên gia. Đặt NPbase biểu thị số lượng tham số trong mô hình cơ sở và E biểu thị số lượng chuyên gia. Đặt G là số lượng GPU. Lưu ý rằng hai phần ba tham số trong mô hình cơ sở nằm trong các khối feed-forward, và một phần ba còn lại trong các khối self-attention. Vì chỉ một nửa số khối feedforward được chỉ định làm chuyên gia, tổng số tham số chuyên gia, NPexp, trong mô hình MoE là:

NPexp = E × 1/2 × 2/3 × NPbase = E/3 × NPbase (2)

Bây giờ, các tham số non-expert bao gồm tham số trong tất cả các khối self-attention và một nửa số khối feed-forward. Do đó, tổng số tham số non-expert, NPnonexp, là:

NPnonexp = 1/2 × 2/3 × NPbase + 1/3 × NPbase = 2/3 × NPbase (3)

Rajbhandari et al. chứng minh rằng giới hạn dưới của tiêu thụ bộ nhớ trên mỗi GPU với ZeRO stage-1 là:
(4 + 12/Gdata) × NPgpu,
trong đó Gdata là mức độ data parallelism và NPgpu là số lượng tham số của mô hình trên mỗi GPU. Bây giờ, chúng tôi sử dụng công thức này để suy ra giới hạn dưới về tiêu thụ bộ nhớ trên mỗi GPU cho TED như sau:

Mgpu ≥ (4 + 12/Gnonexp_data) × NPnonexp_gpu + (4 + 12/Gexp_data) × NPexp_gpu (4)

Ở đây, NPnonexp_gpu và NPexp_gpu là số lượng tham số expert và non-expert trên mỗi GPU. Như đã thảo luận trước đó, Gnonexp_data và Gexp_data là mức độ data parallelism cho các khối non-expert và expert tương ứng. Bây giờ, hãy cố gắng suy ra giá trị của NPnonexp_gpu và NPexp_gpu, bắt đầu với cái trước. Tensor parallelism của MegatronLM chia các tham số của mô hình bằng nhau giữa các GPU trong nhóm tensor parallel. Vì kích thước của nhóm tensor parallel trong TED là Gtensor, chúng ta có thể viết NPnonexp_gpu = NPnonexp/Gtensor. Tuy nhiên, các tham số expert được chia trong cả nhóm tensor parallel và expert parallel. Như đã thảo luận trước đó, chúng tôi sử dụng mức độ expert parallelism bằng số lượng chuyên gia tức là Gexpert = E. Do đó, NPexp_gpu = NPexp/(Gtensor × E). Ngoài ra, từ Phương trình 1 suy ra rằng Gnonexp_data = G/Gtensor và Gexp_data = G/(Gtensor × Gexpert) = G/(Gtensor × E). Thay thế các giá trị này vào Phương trình 4, chúng ta được:

Mgpu ≥ (4 + 12Gtensor/G) × NPnonexp/Gtensor + (4 + 12GtensorE/G) × NPexp/(GtensorE)
≥ 4/Gtensor(NPnonexp + NPexp/E) + 12/G(NPnonexp + NPexp)

Bây giờ thay thế từ Phương trình 2 và 3, chúng ta được:
Mgpu ≥ 4/Gtensor(2/3NPbase + NPbase/3) + 12/G(2/3NPbase + E/3NPbase)
≥ 4NPbase/Gtensor + 4(E + 2)/G NPbase
≥ 4NPbase × (1/Gtensor + (E + 2)/G) (5)

Phương trình 5 có thể được sử dụng để suy ra giới hạn trên về kích thước mô hình cơ sở lớn nhất có thể mà framework của chúng tôi có thể huấn luyện, cho đủ số lượng GPU. Lưu ý rằng khi chúng ta tăng số lượng GPU tham gia huấn luyện, số hạng thứ hai trở nên không đáng kể so với số hạng đầu tiên. Điều này cho chúng ta:

Mgpu ≥ 4NPbase/Gtensor ⟹ NPbase ≤ Gtensor/4 × Mgpu (6)

Lưu ý rằng thay thế Gtensor = 1 vào Phương trình 6 cho chúng ta giới hạn trên mô hình cơ sở cho Rajbhandari et al., hiện tại là state-of-the-art để huấn luyện MoE. Do đó, chúng tôi đã chỉ ra rằng hệ thống của chúng tôi cho phép huấn luyện mô hình cơ sở lớn hơn Gtensor× so với state-of-the-art trước đó. Lưu ý rằng mức độ tensor parallelism tối đa bị hạn chế bởi số lượng GPU trong một node vì lý do hiệu suất. Tuy nhiên, framework của chúng tôi vẫn có thể hỗ trợ mô hình cơ sở lớn hơn 4×, 6× và 8× trên máy Perlmutter, Summit và NVIDIA-DGX-A100 tương ứng.

4 TIẾT KIỆM BỘ NHỚ THÔNG QUA TILING
Trong phần trước, chúng tôi đã cung cấp tổng quan về cách TED phân phối các tham số và tính toán của lượt forward và backward qua các GPU. Tuy nhiên, sự kết hợp ngây thơ của tensor, expert, và data parallelism dẫn đến tăng đáng kể trong sử dụng bộ nhớ trong bước optimizer. Thú vị là, độ lớn của sự tăng này trở nên tệ hơn khi chúng ta tăng số lượng chuyên gia và/hoặc kích thước mô hình cơ sở. Lưu ý rằng quan trọng là giải quyết vấn đề này để chúng ta có thể fit MoE với mô hình cơ sở lớn trong bộ nhớ. Dưới đây, chúng tôi thảo luận hiện tượng này chi tiết và nêu giải pháp của chúng tôi để giải quyết vấn đề này.

Để chứng minh sự tăng sử dụng bộ nhớ nói trên, chúng tôi profile bộ nhớ tiêu thụ trên mỗi GPU trong các giai đoạn huấn luyện khác nhau (lượt forward, lượt backward, bước optimizer) cho mô hình MoE với mô hình cơ sở 2.7B tham số và 32 chuyên gia, và chỉ kết quả trong Hình 4. Chúng tôi chạy thí nghiệm này trên 32 GPU của cluster NVIDIA DGX-A100 với tám GPU mỗi node. Chúng tôi đặt mức độ tensor và expert parallelism lần lượt là 1 và 32. Điều này dẫn đến mức độ data parallelism là 32 và 1 cho các khối non-expert và expert tương ứng. Chúng tôi quan sát rằng tiêu thụ bộ nhớ đạt đỉnh trong bước optimizer với sự tăng rất đáng kể khoảng 4.5 GB. Một bước trung gian trong giai đoạn optimizer trong huấn luyện mixed precision là up-casting gradient 16-bit thành gradient 32-bit trước khi optimizer cập nhật trọng số. Điều này đòi hỏi tạo buffer tạm thời để lưu trữ gradient 32-bit và đây chính xác là lý do tại sao có sự gia tăng đáng kể trong tiêu thụ bộ nhớ. Thực tế, vấn đề này trở nên tệ hơn với kích thước mô hình cơ sở và/hoặc số lượng chuyên gia tăng. Hãy hiểu tại sao.

TED sử dụng ZeRO stage-1 giảm tiêu thụ bộ nhớ bằng cách phân mảnh trạng thái optimizer và tính toán qua các nhóm data parallel. Mức độ data parallelism càng lớn, việc giảm tiêu thụ bộ nhớ càng lớn. Từ thảo luận trong Phần 3, chúng ta biết rằng TED sử dụng các mức độ data parallelism khác nhau cho tham số expert và khối non-expert. Thực tế, từ Phương trình 1 suy ra rằng:

Gtensor × Gexpert × Gexp_data = Gtensor × Gnonexp_data
Gexpert × Gexp_data = Gnonexp_data
E × Gexp_data = Gnonexp_data
Gexp_data = Gnonexp_data/E (7)

Từ Phương trình 7, chúng ta có thể kết luận rằng mức độ data parallelism cho các khối expert nhỏ hơn E× so với các khối non-expert. Do đó, ZeRO cung cấp tiết kiệm bộ nhớ ít hơn cho các khối expert so với các khối non-expert. Điều này là do các trạng thái optimizer cho các khối expert được phân mảnh trên E× ít GPU hơn. Do đó, khi E tăng, mỗi GPU phải xử lý số lượng tham số tăng trong bước optimizer. Điều này dẫn đến sự gia tăng trong kích thước của buffer gradient 32-bit tạm thời cần thiết để up-cast gradient tham số expert. Việc tăng kích thước mô hình cơ sở cũng làm tệ hơn vấn đề này vì kích thước của nhóm tham số expert tỷ lệ thuận với kích thước mô hình cơ sở. Đây là lý do tại sao cần thiết phải giải quyết vấn đề này để chúng ta có thể huấn luyện MoE với mô hình cơ sở lớn và/hoặc số lượng chuyên gia lớn.

Trong công trình này, chúng tôi đề xuất công thức tiled của optimizer nhằm giảm thiểu vấn đề nói trên. Thay vì xử lý toàn bộ nhóm tham số expert cùng lúc, chúng tôi đề xuất phân vùng các tham số này thành "tile" có kích thước xác định trước và xử lý lặp lại các tile này. Điều này đảm bảo rằng tại bất kỳ thời điểm nào, gradient 32-bit tạm thời chỉ được tạo ra cho tham số thuộc về một tile nhất định. Bộ nhớ tạm thời được sử dụng để lưu trữ các gradient này thực tế có thể được tái sử dụng qua các tile. Với kích thước tile ts, giờ chúng ta chỉ cần 4 × ts byte bộ nhớ để materialize gradient 32-bit. Điều này làm cho sự tăng bộ nhớ optimizer độc lập với số lượng chuyên gia và kích thước mô hình cơ sở! Trong các thí nghiệm của chúng tôi, chúng tôi cố định kích thước tile là 1.8 triệu tham số, về cơ bản giới hạn sự tăng trong bước optimizer ở 1 GB. Chúng tôi quan sát rằng kích thước tile này đủ lớn để không gây ra bất kỳ suy giảm hiệu suất nào do độ trễ của nhiều lần khởi chạy kernel. Trong Hình 4, chúng tôi chứng minh cách optimizer tiled của chúng tôi giảm tiêu thụ bộ nhớ đỉnh trên mỗi GPU cho MoE nói trên với mô hình cơ sở 2.7B tham số và 32 chuyên gia 3 GB. Thực tế, trên một MoE khác với 6.7B tham số và 16 chuyên gia trên 32 GPU, framework của chúng tôi hết bộ nhớ khi không có tiling. Trong khi đó, với tiling được bật, chúng tôi có thể huấn luyện thành công mô hình này với tiêu thụ bộ nhớ đỉnh 31.3 GB. Vì dung lượng bộ nhớ tối đa của các GPU này là 40 GB, optimizer tiling cung cấp tiết kiệm bộ nhớ đáng kể hơn 21.75%!

5 TỐI ƯU HÓA HIỆU SUẤT
Trong các phần trước, chúng tôi tập trung vào tăng kích thước tối đa có thể của MoE được hỗ trợ bởi framework của chúng tôi. Trong khi tiết kiệm bộ nhớ được cung cấp bởi expert và tensor parallelism đóng góp vào điều này, chúng cũng dẫn đến một phần đáng kể thời gian batch được dành cho giao tiếp tập thể đắt đỏ. Trong Hình 3, chúng ta có thể quan sát rằng lượt forward bao gồm hai lệnh gọi all-reduce trong các nhóm tensor parallel, và hai lệnh gọi all-to-all trong các nhóm expert parallel. Trong lượt backward, các lệnh gọi này được lặp lại. Ngoài ra, huấn luyện mô hình lớn hầu như luôn sử dụng activation checkpointing, giảm đáng kể bộ nhớ kích hoạt với chi phí là lượt forward trùng lặp cho mỗi lớp. Do đó, tổng cộng chúng ta kết thúc với sáu lệnh gọi giao tiếp all-to-all và sáu all-reduce, trở thành thắt cổ chai đáng kể trong huấn luyện. Chúng tôi chứng minh thực nghiệm điều này trong Hình 5 (thanh ngoài cùng bên trái có tiêu đề Baseline), trong đó chúng tôi quan sát rằng gần một nửa thời gian batch được dành cho các lệnh gọi giao tiếp all-to-all và all-reduce. Giờ chúng tôi sẽ mô tả hai tối ưu hóa hiệu suất nhằm giảm thời gian dành cho các giao tiếp này và cực kỳ quan trọng đối với hiệu suất của framework chúng tôi.

5.1 Duplicate Token Dropping (DTD) để Giảm Khối lượng Giao tiếp
Tensor parallelism của MegatronLM để phân vùng các khối self-attention và feed forward liên quan đến việc phát ra all-reduce trên đầu ra cục bộ một phần để materialize đầu ra đầy đủ trên mỗi rank. Ví dụ, trong Hình 3, GPU 0 và 1 phát ra all-reduce (2) sau khối self-attention để lắp ráp đầu ra self-attention đầy đủ cho token 1 và 2. Mặc dù điều này dẫn đến nhân bản kích hoạt qua các rank tensor parallel, nó không phải là vấn đề cho huấn luyện mô hình transformer thường xuyên (tức là không có chuyên gia) vì các khối tensor parallel dưới thuật toán của MegatronLM yêu cầu một tập hợp kích hoạt đầu vào hoàn chỉnh trên mỗi rank tensor parallel. Do đó, các kích hoạt trùng lặp được xuất ra bởi khối tensor parallel phục vụ như đầu vào cần thiết cho người kế nhiệm của nó. Tuy nhiên, đối với MoE, một tác dụng phụ không mong muốn của lựa chọn thiết kế này là sự hiện diện của token dư thừa trong các lệnh gọi giao tiếp all-to-all. Ví dụ, xem xét all-to-all đầu tiên trong Hình 3 (4). Kích hoạt đầu ra self-attention, a1 và a2, được giao tiếp bởi cả GPU 0 và 1. Tương tự, GPU 2 và 3 cả hai đều giao tiếp a3 và a4. Nói chung, lượng dữ liệu không cần thiết trong các lệnh gọi giao tiếp all-to-all cho một token nhất định tỷ lệ thuận với mức độ tensor parallelism. Do đó, việc kết hợp ngây thơ expert và tensor parallelism có thể dẫn đến giao tiếp all-to-all trở thành thắt cổ chai đáng kể, đặc biệt khi chúng ta cố gắng tăng kích thước mô hình cơ sở (mô hình cơ sở lớn hơn cần tensor parallelism nhiều hơn). Ví dụ, trong Hình 5, 32% thời gian batch được dành cho all-to-all (thanh ngoài cùng bên trái có tiêu đề baseline)! Mức độ tensor parallelism và do đó mức độ dư thừa trong all-to-all là bốn ở đây.

Để giải quyết thắt cổ chai này, chúng tôi đề xuất duplicate token dropping (DTD), một tối ưu hóa giao tiếp nhằm loại bỏ dữ liệu không cần thiết trong giao tiếp all-to-all. Chúng tôi minh họa hoạt động của DTD trong Hình 6 cho giao tiếp all-to-all đầu tiên trong lớp MoE (4 trong Hình 3). Trước khi all-to-all được phát ra, các GPU trong nhóm tensor parallel tham gia vào thao tác "drop" (1 trong Hình 6). Thao tác drop đảm bảo rằng không có sự dư thừa trong kích hoạt đầu ra qua các rank tensor parallel. Chẳng hạn, GPU 0 drop kích hoạt của a2 trong khi GPU 1 drop kích hoạt a1, do đó hoàn toàn loại bỏ dư thừa trong nhóm tensor parallel của họ. Tương tự, GPU 3 và 4 drop a3 và a4 tương ứng. Thao tác drop do đó giảm kích thước tin nhắn all-to-all hai lần trong ví dụ này, và nói chung việc giảm bằng mức độ tensor parallelism. Tuy nhiên, sau all-to-all, các GPU không có kích hoạt đầu vào đầy đủ để bắt đầu tính toán các khối feed forward chuyên gia. Chẳng hạn, GPU 0 có kích hoạt đầu vào cho token 1, nhưng không có cho token 3 và ngược lại cho GPU 1. Do đó, để lắp ráp kích hoạt đầu vào đầy đủ, chúng tôi phát ra lệnh gọi all-gather (2 trong Hình 6) giữa các GPU tensor parallel. All-gather đảm bảo rằng các phụ thuộc đầu vào cho các khối feedforward chuyên gia được đáp ứng.

Trong lượt backward, lệnh gọi all-gather được thay thế bằng thao tác drop và thao tác drop được thay thế bằng lệnh gọi all-gather. Đối với mô hình MoE trong Hình 5, chúng tôi quan sát rằng DTD giảm thời gian giao tiếp all-to-all 48%. Mặc dù việc bao gồm DTD dẫn đến thao tác all-gather bổ sung (hiển thị màu đỏ trên thanh thứ hai), overhead này được bù đắp bởi cải thiện trong thời gian giao tiếp all-to-all. Nhìn chung, DTD dẫn đến cải thiện 13.21% trong thời gian batch.

5.2 Communication-aware Activation Checkpointing (CAC)
Giờ chúng tôi chuyển sự chú ý đến nguồn thứ hai của giao tiếp dư thừa trong huấn luyện mô hình lớn, đó là activation checkpointing. Kích hoạt trung gian trong mạng nơ-ron được tạo ra trong lượt forward cần được cất giữ trong bộ nhớ vì chúng được yêu cầu trong lượt backward để tính toán gradient. Tuy nhiên, đối với huấn luyện mô hình lớn, việc lưu trữ tất cả kích hoạt có thể dẫn đến overhead bộ nhớ khổng lồ. Activation checkpointing giảm thiểu vấn đề này bằng cách chỉ lưu trữ một tập con kích hoạt, về cơ bản chỉ là kích hoạt đầu vào của mỗi lớp. Trong lượt backward của một lớp, các kích hoạt còn lại được tái vật chất hóa từ kích hoạt đầu vào được cất giữ bằng cách thực hiện lượt forward cục bộ cho lớp đó. Do đó, activation checkpointing tiết kiệm bộ nhớ kích hoạt với chi phí là lượt forward trùng lặp cho mỗi lớp, và hầu như luôn được sử dụng để huấn luyện mạng nơ-ron lớn. Để biết thêm chi tiết, chúng tôi giới thiệu độc giả đến Chen et al.

Chúng ta biết từ Phần 3 rằng lượt forward của lớp MoE trong TED liên quan đến hai all-to-all và hai lệnh gọi all-reduce trong lượt forward và hai all-to-all và hai lệnh gọi all-reduce trong lượt backward. Vì activation checkpointing liên quan đến việc lặp lại lượt forward của một lớp, giờ chúng ta kết thúc với hai lệnh gọi all-to-all và all-reduce bổ sung, do đó tăng khối lượng giao tiếp 1.5× và làm cho quá trình huấn luyện không hiệu quả.

Về vấn đề này, chúng tôi đề xuất communication-aware checkpointing (CAC), một tối ưu hóa giao tiếp loại bỏ giao tiếp bổ sung trong lượt forward thứ hai được tạo ra bởi activation checkpointing. Trong lượt forward đầu tiên, CAC cất giữ đầu ra của mỗi lệnh gọi giao tiếp all-reduce và all-to-all cùng với dữ liệu được cất giữ bởi activation checkpointing tiêu chuẩn. Giờ, trong lượt forward thứ hai, chúng tôi bỏ qua các lệnh gọi giao tiếp này và thay vào đó trả về đầu ra cho các lệnh gọi giao tiếp này được cất giữ trong lượt forward đầu tiên. Do đó CAC giảm khối lượng giao tiếp 33% với chi phí sử dụng bộ nhớ GPU bổ sung. Đối với mô hình MoE trong Hình 5, CAC thực sự giảm thời gian giao tiếp all-to-all và all-reduce 33% (so sánh thanh thứ hai và thứ ba). Kết hợp với DTD, việc giảm trong thời gian giao tiếp all-to-all và all-reduce lần lượt là 64.12% và 33%, tương đương với tăng tốc gần 20.7% so với phiên bản baseline của DeepSpeed-TED.

6 THIẾT LẬP THÍ NGHIỆM
Phần này cung cấp tổng quan về đánh giá thực nghiệm của DeepSpeed-TED. Framework của chúng tôi là mã nguồn mở, và đã được tích hợp trong DeepSpeed, một framework tiên tiến cho học sâu song song. Chúng tôi thực hiện các thí nghiệm trên siêu máy tính Summit và ThetaGPU. Summit có sáu GPU NVIDIA V100 16 GB mỗi node, mỗi GPU có thông lượng half precision đỉnh là 125 Tflop/s. Mỗi node có hai CPU Power 9 22-core. Băng thông giao tiếp GPU hai chiều đỉnh intra-node và inter-node lần lượt là 50 GB/s (NVlink) và 25 GB/s (Infiniband). Mặt khác, ThetaGPU là máy NVIDIA DGX A100 với tám GPU NVIDIA A100 40 GB mỗi node, mỗi GPU có thông lượng half precision đỉnh là 312 Tflop/s. Trên máy này, băng thông giao tiếp GPU hai chiều đỉnh intra-node và inter-node lần lượt là 600 GB/s (NVlink) và 200 GB/s (Infiniband).

6.1 Kiến trúc Mạng Nơ-ron và Tập dữ liệu
Bảng 1 liệt kê các kiến trúc mô hình cơ sở khác nhau được sử dụng trong nghiên cứu này. Tất cả MoE được sử dụng trong các thí nghiệm thực nghiệm của chúng tôi được xây dựng bằng cách thêm các khối chuyên gia vào mỗi lớp thay phiên của một trong những mô hình cơ sở này (điều này phù hợp với công trình trước đó). Các mô hình cơ sở và siêu tham số tương ứng được lấy từ Brown et al. Chúng tôi sử dụng tập dữ liệu Pile để tạo token đầu vào. Chúng tôi sử dụng optimizer AdamW, là thực hành tiêu chuẩn cho huấn luyện mô hình ngôn ngữ lớn. Chúng tôi triển khai các lớp của mô hình transformer bằng GPU kernel của MegatronLM.

Đầu tiên, chúng tôi thiết lập tính chính xác của triển khai bằng cách huấn luyện mô hình MoE 2.6B tham số (mô hình cơ sở 1.3B tham số và 4 chuyên gia) đến hoàn thành trên tập dữ liệu BookCorpus, trên 8 GPU, và trình bày các đường cong validation loss. Để tham khảo, chúng tôi cũng huấn luyện mô hình này bằng DeepSpeed-MoE, framework tiên tiến hiện tại để huấn luyện MoE và so sánh hai đường cong loss. Sau đó, chúng tôi chứng minh kích thước mô hình MoE tối đa mà framework của chúng tôi có thể hỗ trợ cho một số lượng GPU nhất định và so sánh với DeepSpeed-MoE. Tiếp theo, chúng tôi thực hiện các nghiên cứu strong scaling sử dụng MoE được xây dựng từ các mô hình transformer 1.3B, 2.7B và 6.7B tham số trong Bảng 1 trên 32 đến 256 GPU. Tại 32 GPU, chúng tôi thêm nhiều chuyên gia nhất mà bộ nhớ hệ thống cho phép. Chúng tôi strong scale một mô hình theo hai cách, đầu tiên tăng số lượng GPU trong khi giữ số lượng chuyên gia không đổi, và thứ hai bằng cách thay đổi số lượng chuyên gia tỷ lệ thuận với số lượng GPU. Lưu ý rằng mặc dù thí nghiệm sau tăng kích thước mô hình theo quy mô, nó vẫn được coi là strong scaling vì việc thêm chuyên gia vào mô hình cơ sở không thay đổi tổng số thao tác dấu phẩy động trong huấn luyện. Đối với các lần chạy weak scaling, chúng tôi cố định số lượng chuyên gia là 16 và sử dụng mô hình cơ sở có kích thước tăng từ Bảng 1 khi chúng tôi đi từ 32 đến 256 GPU. Lưu ý rằng đây là weak scaling, vì số lượng thao tác dấu phẩy động tỷ lệ thuận với kích thước mô hình cơ sở.

6.2 Chỉ số Đánh giá
Chúng tôi minh họa kết quả thí nghiệm bằng thời gian trung bình mỗi lần lặp (hoặc batch). Để tính toán điều này, trước tiên chúng tôi chạy một mô hình nhất định với 100 batch được lấy mẫu từ tập dữ liệu Pile và lấy trung bình của 90 batch cuối cùng. Chúng tôi không bao gồm 10 batch đầu tiên vì PyTorch phát ra các lệnh gọi mem-alloc đắt đỏ đến CUDA runtime trong các lần lặp ban đầu để dự trữ đủ bộ nhớ cho huấn luyện. Chúng tôi cũng suy ra phần trăm thông lượng half-precision đỉnh từ thời gian batch trung bình bằng công thức của Narayanan et al. Lưu ý rằng đây là công thức phân tích về tổng số flop/s (và do đó phần trăm thông lượng half-precision đỉnh). Vì công thức của Narayanan et al. là giới hạn dưới về tổng thao tác dấu phẩy động, chúng tôi mong đợi flop/s được đo thực nghiệm sẽ cao hơn.

7 KẾT QUẢ
Trong phần này, chúng tôi thảo luận về kết quả của các thí nghiệm thực nghiệm được nêu trong Phần 6.

7.1 Xác minh Triển khai của Chúng tôi
Để xác minh tính chính xác của DeepSpeed-TED, chúng tôi huấn luyện MoE với mô hình cơ sở 1.3B tham số và 4 chuyên gia trên 8 GPU của ThetaGPU và trình bày đường cong validation loss trong Hình 7. Chúng tôi đặt Gtensor = 2, expert = 4, Gnonexp_data = 4, và Gexp_data = 1. Điều này cho phép chúng tôi kiểm tra tính chính xác của framework trong tình huống mà cả ba chiều của phương pháp song song hybrid đều hoạt động. Chúng tôi cũng kích hoạt các tối ưu hóa giao tiếp được thảo luận trong Phần 5 tức là DTD và CAC. Chúng tôi quan sát rằng framework của chúng tôi có thể huấn luyện thành công mô hình đến hội tụ, và tạo ra đường cong loss giống hệt với DeepSpeed-MoE, một hệ thống đã được sử dụng trước đó để huấn luyện các mô hình MoE tiên tiến. Bằng cách này, chúng tôi thiết lập tính chính xác của triển khai.

7.2 So sánh Kích thước Mô hình Được Hỗ trợ
Hình 9 minh họa kết quả thí nghiệm trong đó chúng tôi đo lường các mô hình MoE lớn nhất mà framework của chúng tôi và DeepSpeed-MoE có thể huấn luyện mà không hết bộ nhớ cho số lượng GPU khác nhau từ 32 đến 512. Chúng tôi sử dụng các mô hình cơ sở trong Bảng 1. Để đảm bảo thí nghiệm công bằng với cả hai framework, chúng tôi làm hai việc. Mặc dù phương pháp được đề xuất của chúng tôi về mặt lý thuyết có thể hỗ trợ mô hình cơ sở lớn tùy ý bằng cách tăng mức độ tensor parallelism, nhưng được biết rằng tensor parallelism cực kỳ không hiệu quả khi được sử dụng qua các node. Do đó, chúng tôi chỉ cho phép framework của chúng tôi sử dụng mức độ tensor parallel tối đa là 6, là số lượng GPU trên node của Summit. Thứ hai, chúng tôi giới hạn số lượng chuyên gia có thể lớn nhất là 128 vì công trình trước đó đã chứng minh cải thiện hạn chế trong hiệu quả thống kê của mô hình vượt quá số này.

Qua phạm vi GPU được sử dụng trong thí nghiệm này, chúng tôi quan sát rằng DeepSpeed-TED hỗ trợ mô hình MoE lớn hơn 1.09-4.8× so với DeepSpeed-MoE. Chúng tôi cũng quan sát rằng tỷ lệ này tăng khi chúng ta tăng số lượng GPU. Điều này có thể được giải thích bằng Phương trình 5, nói rằng tiêu thụ bộ nhớ của phương pháp chúng tôi giảm với số lượng GPU tăng. Chúng tôi quan sát rằng vượt quá 128 GPU, framework được đề xuất của chúng tôi có thể huấn luyện MoE với hàng trăm tỷ tham số trên Summit, điều không thể với DeepSpeed-MoE. Do đó, chúng tôi đã chứng minh thực nghiệm cách DeepSpeed-TED của chúng tôi có thể cho phép phát triển các mô hình MoE chất lượng cao, các tham số của chúng đã được mở rộng theo cả chiều mô hình cơ sở và chiều chuyên gia.

7.3 Hiệu suất Strong Scaling
Giờ chúng tôi thảo luận về kết quả của các thí nghiệm strong scaling, bắt đầu với các lần chạy thay đổi số lượng chuyên gia tỷ lệ thuận với số lượng GPU. Chúng tôi chứng minh kết quả cho các mô hình cơ sở 1.3B, 2.7B và 6.7B trong Hình 8. Để chứng minh hiệu quả của các tối ưu hóa giao tiếp được thảo luận trong Phần 5, chúng tôi cũng đo lường phiên bản baseline của framework tức là với DTD+CAC bị vô hiệu hóa, và gọi nó là DeepSpeed-TED (baseline). Qua tất cả các hình, chúng tôi quan sát rằng việc tăng cường quy trình huấn luyện với DTD và CAC thực sự cải thiện hiệu quả phần cứng của huấn luyện. Tuy nhiên, trong khi tăng tốc cho các mô hình cơ sở 2.7B và 6.7B tham số đáng kể: 19 đến 23% và 25 đến 29% tương ứng, các tối ưu hóa giao tiếp của chúng tôi dường như ít hiệu quả hơn cho mô hình cơ sở 1.3B nhỏ nhất, cung cấp tăng tốc khiêm tốn khoảng 4 đến 7%. Điều này là do tại số lượng GPU và số lượng chuyên gia nhất định, tối ưu hóa bộ nhớ của ZeRO và expert parallelism có thể fit mô hình này trong bộ nhớ mà không cần hỗ trợ của tensor parallelism. Không có tensor parallelism, không có sự dư thừa trong giao tiếp all-to-all (xem Phần 5.1) và do đó tối ưu hóa giao tiếp DTD không có ích trong tình huống này. Tương tự, không có tensor parallelism, không có giao tiếp all-reduce (2 và 6 của Hình 3). Do đó, CAC chỉ loại bỏ các lệnh gọi all-to-all không cần thiết, và chỉ áp dụng một phần cho tình huống này. Điều này giải thích hiệu quả giảm của các tối ưu hóa cho mô hình cơ sở 1.3B.

Không giống như mô hình cơ sở 1.3B, mô hình 2.7B và 6.7B yêu cầu mức độ tensor parallel là 2 và 4 để fit trong bộ nhớ GPU có sẵn. Sự dư thừa tiếp theo trong tin nhắn all-to-all và việc giới thiệu tensor parallelism do đó làm cho các tối ưu hóa giao tiếp của chúng tôi khá hiệu quả. Một lần nữa, chúng tôi quan sát tăng tốc lớn hơn cho MoE sử dụng mô hình cơ sở 6.7B (25-29% so với 19-23%) vì mức độ tensor parallelism cao hơn ngụ ý nhiều dư thừa hơn trong tin nhắn all-to-all, mà các tối ưu hóa của chúng tôi loại bỏ thành công. Nó cũng ngụ ý tỷ lệ thời gian lớn hơn được dành cho tensor parallel all-reduce được giảm đáng kể bởi CAC.

Trong các lần chạy strong scaling với số lượng chuyên gia cố định, chúng tôi quan sát thời gian tuyệt đối trên mỗi lần lặp và tăng tốc tương đối rất tương tự cho cả ba mô hình. Để ngắn gọn, chúng tôi chỉ bao gồm kết quả cho mô hình cơ sở 6.7B tham số, và minh họa chúng trong Hình 10. Do đó chúng tôi đã xác minh rằng các tối ưu hóa của chúng tôi hiệu quả trong việc cải thiện hiệu suất trong hai thiết lập strong scaling qua nhiều kích thước mô hình cơ sở khác nhau.

7.4 Hiệu suất Weak Scaling
Như đã thảo luận trong Phần 6, chúng tôi thực hiện thí nghiệm weak scaling bằng cách cố định số lượng chuyên gia là 16 và thay đổi kích thước mô hình cơ sở tỷ lệ thuận với số lượng GPU. Chúng tôi chứng minh thời gian trên mỗi lần lặp (hoặc batch) và phần trăm thông lượng half-precision đỉnh cho thí nghiệm này trong Hình 11 và Bảng 2 tương ứng. Một lần nữa, chúng tôi quan sát tăng tốc nhỏ 6% cho mô hình cơ sở 1.3B, và tăng tốc đáng kể 20%, 25% và 36% cho các mô hình cơ sở 2.7B, 6.7B và 13B tương ứng. Giống như phần trước, hiệu quả tăng dần của các tối ưu hóa giao tiếp cho các mô hình cơ sở lớn hơn có thể được giải thích bằng mức độ tensor parallelism tăng tương ứng - 1, 2, 4, và 8. Điều này tạo ra nhiều dư thừa hơn cho các mô hình lớn hơn trong all-to-all và tăng khối lượng giao tiếp net của all-reduce.

Lưu ý rằng mặc dù tăng tốc cho mô hình 13B tham số đáng kể (36%), việc sử dụng phần cứng cho mô hình này cực kỳ thấp. Ngay cả với các tối ưu hóa của chúng tôi, chúng tôi chỉ có thể đạt 11.7% flop/s half-precision đỉnh, thấp hơn đáng kể so với các mô hình cơ sở 1.3B (37% đỉnh), 2.7B (30% đỉnh) và 6.7B (27% đỉnh). Giải thích cho quan sát này là mức độ tensor parallel là 8 cho mô hình này lớn hơn số lượng GPU trên node Summit. Thí nghiệm này chứng thực công trình trước đó đã quan sát rằng thuật toán của Megatron-LM không mở rộng tốt vượt quá ranh giới của một node.

8 CÔNG TRÌNH LIÊN QUAN
Do chi phí tính toán tăng của việc huấn luyện các mạng nơ-ron tiên tiến, một số framework và thuật toán đã được đề xuất có thể huấn luyện các mạng này song song trên cluster multi-GPU có kết nối mạng. Các framework này có thể được chia thành ba loại - data, tensor, và pipeline parallelism. Dưới data parallelism, các GPU tham gia được gán một bản sao đầy đủ của mạng nơ-ron. Sự song song đến từ việc mỗi GPU làm việc trên một shard có kích thước bằng nhau của batch đầu vào tại mỗi lần lặp. Có lẽ do sự đơn giản trong triển khai, data parallelism đã được áp dụng rộng rãi nhất và có thể được tìm thấy trong các framework học sâu phổ biến như PyTorch (như Distributed Data Parallel). Tuy nhiên, một hạn chế chính của data parallelism là nó yêu cầu toàn bộ mạng nơ-ron phải fit trên mỗi GPU. Để giải quyết vấn đề này, Rajbhandari et al. đề xuất Zero Redundancy Optimizer (ZeRO) phân mảnh tham số, gradient, và/hoặc trạng thái optimizer của mô hình qua các GPU tham gia, và cho phép huấn luyện các mô hình lớn hơn nhiều vượt xa khả năng bộ nhớ của một GPU duy nhất. PyTorch cũng cung cấp Fully Sharded Data Parallelism (FSDP) một cách bản địa, dựa trên ý tưởng tương tự.

Các thuật toán tensor parallel như MegatronLM chia các tham số và tính toán của mỗi lớp của mạng nơ-ron qua các GPU tham gia và do đó cũng có thể được sử dụng để huấn luyện các mạng nơ-ron không fit trên một GPU duy nhất. Các ví dụ khác về framework và thuật toán tensor parallel là cho các lớp kết nối đầy đủ, cho các lớp tích chập, và cho mạng nơ-ron đồ thị. Mặt khác, pipeline parallelism liên quan đến việc gán tham số và tính toán của một tập con liên tục các lớp cho mỗi GPU. Sự song song đạt được bằng cách chia một batch thành microbatch và xử lý các microbatch theo cách pipeline (giống như pipelining trong kiến trúc máy tính). Narayanan et al. chỉ ra cách kết hợp tensor, pipeline, và data parallelism có thể là chiến lược cực kỳ hiệu quả để huấn luyện các mô hình multi-tỷ tham số lớn theo quy mô. Như công việc tương lai, chúng tôi dự định tích hợp pipeline parallelism trong DeepSpeed-TED để tăng cường hiệu suất hơn nữa.

Để chống lại chi phí tính toán tăng của việc huấn luyện các mạng nơ-ron tiên tiến như Chinchilla, GPT-3, và Megatron-Turing NLG, cộng đồng machine learning gần đây đã chuyển sự chú ý đến kiến trúc Mixture-of-Experts (MoE) để huấn luyện các mô hình transformer lớn hiệu quả về mặt tính toán cho xử lý ngôn ngữ tự nhiên và thị giác máy tính. Tiếp theo, một số framework học sâu song song đã được đề xuất để huấn luyện hoặc chạy suy luận trên MoE trên cluster multi-GPU. Các framework này thường kết hợp các thuật toán học sâu song song nói trên với expert parallelism, bao gồm việc tính toán các khối chuyên gia theo cách song song dễ dàng trên nhiều GPU. Rajbhandari et al. trình bày DeepSpeed-MoE, một hệ thống tiên tiến để huấn luyện và chạy suy luận trên MoE kết hợp expert parallelism với data parallelism của ZeRO. Nie et al. kết hợp phát triển kernel được tối ưu hóa cao cho định tuyến và giao tiếp all-to-all trong framework của họ gọi là HetuMoE. Trong SE-MoE, các tác giả kết hợp expert parallelism với huấn luyện out-of-core, trong đó họ lưu trữ dữ liệu mô hình trên bộ nhớ CPU và SSD để cho phép huấn luyện MoE cực lớn. Artetxe et al. sử dụng Fully Sharded Data Parallelism (FSDP) của Pytorch để huấn luyện MoE với hàng nghìn tỷ tham số. Trong framework của họ gọi là Tutel, Hwang et al. đề xuất một số tối ưu hóa để huấn luyện MoE theo quy mô như kernel được tối ưu hóa cho hàm định tuyến, thuật toán phân cấp 2D hiệu quả cho giao tiếp all-to-all, và parallelism thích ứng cho workload MoE động.

9 KẾT LUẬN
Các nhà nghiên cứu học sâu gần đây đã bắt đầu khám phá Mixture-of-Experts (MoE) để chống lại nhu cầu tính toán tăng của các mạng nơ-ron lớn. State-of-the-art trước đó để song song hóa kiến trúc MoE kết hợp data và expert parallelism nhưng không có tensor parallelism. Trong công trình này, chúng tôi trình bày một thuật toán song song hybrid mới kết hợp tensor, expert, và data parallelism để cho phép huấn luyện các mô hình MoE với mô hình cơ sở lớn hơn 4-8× so với state-of-the-art hiện tại, DeepSpeed-MoE. Chúng tôi xác định sự tăng bộ nhớ bất thường trong optimizer chỉ xảy ra đối với MoE và đề xuất triển khai tiled của optimizer để giảm thiểu vấn đề này. Chúng tôi cũng chỉ ra rằng sự kết hợp ngây thơ của tensor và expert parallelism dẫn đến sự dư thừa đáng kể trong giao tiếp tập thể, và đề xuất các tối ưu hóa giao tiếp để giải quyết vấn đề này. Cuối cùng, chúng tôi thực hiện một tập hợp thí nghiệm thực nghiệm toàn diện để xác minh hiệu quả của framework được đề xuất. Công việc tương lai liên quan đến việc thêm pipelining như một chiều song song mới để mở rộng framework của chúng tôi cho các mô hình cơ sở không thể fit trên một node duy nhất.
