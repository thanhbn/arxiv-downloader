# 2310.09520.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2310.09520.pdf
# Kích thước tệp: 590052 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Giải Mã Tăng Cường Phần Thưởng: Sinh Văn Bản Có Kiểm Soát Hiệu Quả
Với Mô Hình Phần Thưởng Một Chiều
Haikang Deng
UNC-Chapel Hill
frankdenghaikang@gmail.comColin Raffel
University of Toronto, Vector Institute
craffel@gmail.com
Tóm tắt
Trong khi các mô hình ngôn ngữ lớn đã chứng minh hiệu quả trong một loạt rộng các ứng dụng hạ nguồn, chúng thường sinh ra văn bản có vấn đề hoặc thiếu thuộc tính mong muốn. Trong bài báo này, chúng tôi giới thiệu Giải Mã Tăng Cường Phần Thưởng (RAD), một quy trình sinh văn bản sử dụng một mô hình phần thưởng một chiều nhỏ để khuyến khích mô hình ngôn ngữ sinh ra văn bản có những thuộc tính nhất định. Cụ thể, RAD sử dụng mô hình phần thưởng để chấm điểm các sinh phẩm khi chúng được tạo ra và điều chỉnh lại xác suất lấy mẫu để ưu tiên các token có phần thưởng cao. Bằng cách sử dụng mô hình phần thưởng một chiều, RAD có thể lưu trữ các kích hoạt từ các bước sinh trước để giảm chi phí tính toán. Thông qua các thí nghiệm về sinh văn bản không độc hại và kiểm soát cảm xúc, chúng tôi chứng minh rằng RAD hoạt động tốt nhất trong số các phương pháp chỉ thay đổi quy trình sinh và bằng hiệu suất của các phương pháp tiên tiến nhất có liên quan đến việc đào tạo lại mô hình ngôn ngữ. Chúng tôi tiếp tục xác nhận rằng RAD hiệu quả trên các mô hình ngôn ngữ rất lớn trong khi chỉ phát sinh chi phí tính toán tối thiểu.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLMs, Rae et al., 2021; Hoffmann et al., 2022; Scao et al., 2022; Touvron et al., 2023) đang được áp dụng rộng rãi nhờ thực tế là chúng có thể thực hiện nhiều tác vụ ngôn ngữ và sinh ra văn bản dài có tính mạch lạc. Khi LLMs được triển khai trong các tình huống mà chúng tương tác với con người, việc kiểm soát mô hình ngôn ngữ để nó sinh ra văn bản với những thuộc tính nhất định có thể có lợi (Sudhakar et al., 2019) – ví dụ, chúng ta có thể mong muốn các sinh phẩm không thiên vị, không độc hại và hữu ích. Ngoài ra, chúng ta có thể muốn các mô hình xuất ra văn bản với các thuộc tính cụ thể, chẳng hạn như có cảm xúc tích cực, một phong cách viết nhất định, v.v. Thông thường, các LLMs được đào tạo trước trên các kho dữ liệu văn bản quy mô lớn không được tuyển chọn có thể sinh ra văn bản không có những thuộc tính mong muốn này (Wallace et al., 2019; Gehman et al., 2020), điều này thúc đẩy nhu cầu về các kỹ thuật cho phép sinh văn bản có thể kiểm soát. Những kỹ thuật như vậy có thể được xem như cung cấp một phương tiện để điều kiện hóa việc sinh văn bản dựa trên một thuộc tính mong muốn.

Một cách đơn giản để kiểm soát văn bản được sinh ra bởi LLM là thực hiện đào tạo bổ sung trên dữ liệu có những thuộc tính mong muốn (Gururangan et al., 2020). Thay thế, một LLM có thể được đào tạo với "mã kiểm soát" (Keskar et al., 2019; Lu et al., 2022) chỉ ra các đặc điểm văn bản và có thể được sử dụng để khuyến khích LLM sinh ra nội dung với những đặc điểm đó. Nếu có sẵn, các ưu tiên được chú thích của con người có thể được sử dụng để đào tạo một mô hình phần thưởng sau đó được sử dụng để đào tạo mô hình ngôn ngữ với học tăng cường (Ouyang et al., 2022; Kim et al., 2023). Một nhược điểm của những phương pháp này là chúng có thể làm giảm hiệu suất trên văn bản khác với dữ liệu được sử dụng cho đào tạo bổ sung. Bên cạnh đó, công việc được thực hiện để kiểm soát một mô hình ngôn ngữ không thể được tái sử dụng để kiểm soát mô hình ngôn ngữ khác. Hơn nữa, chi phí đào tạo bổ sung có thể cực kỳ đắt đỏ, đặc biệt đối với các mô hình rất lớn.

--- TRANG 2 ---
Một cách để tránh chi phí và nhược điểm của đào tạo bổ sung là thay vào đó sửa đổi quy trình giải mã được sử dụng để sinh văn bản từ mô hình ngôn ngữ (Chaffin et al., 2022). Ví dụ, giải mã có trọng số sửa đổi các xác suất được gán cho mỗi token trong quá trình giải mã bằng cách sử dụng một mô hình phụ trợ. Hầu hết các phương pháp giải mã có trọng số (Holtzman et al., 2018; Krause et al., 2021; Liu et al., 2021; Yang and Klein, 2021; Sitdikov et al., 2022) thu được xác suất thuộc tính P(c|X) từ một mô hình phần thưởng riêng biệt (thường nhỏ hơn mô hình ngôn ngữ cơ sở) và xây dựng xác suất văn bản có điều kiện lớp theo quy tắc Bayes, P(X|c)∝P(X)P(c|X), trong đó c là một lớp thuộc tính và P(X) là phân bố trên các chuỗi ngôn ngữ tự nhiên X. Trong quá trình giải mã, Krause et al. (2021) và Liu et al. (2021) xử lý tín hiệu từ các mô hình sinh phụ trợ, trong khi Yang and Klein (2021) và Sitdikov et al. (2022) đánh giá các chuỗi trung gian. Giải mã có trọng số chỉ yêu cầu truy cập vào xác suất bước tiếp theo được xuất ra bởi mô hình ngôn ngữ, không yêu cầu đào tạo đắt đỏ, và thường có tính mô-đun, tức là một mô hình phần thưởng duy nhất có thể được tái sử dụng với nhiều mô hình ngôn ngữ. Mặc dù có những lợi ích này, giải mã có trọng số có thể tăng đáng kể chi phí giải mã và thường hoạt động kém hơn các phương pháp có liên quan đến đào tạo thêm (See et al., 2019).

Trong bài báo này, chúng tôi thu hẹp khoảng cách giữa giải mã có trọng số và đào tạo lại bằng cách giới thiệu giải mã tăng cường phần thưởng (RAD), một phương pháp giải mã có trọng số hiệu quả, hiệu suất và có tính mô-đun điều khiển việc sinh văn bản dựa trên phần thưởng được trả về bởi một mô hình phần thưởng cụ thể cho thuộc tính. Cụ thể, RAD sử dụng một mô hình phần thưởng một chiều được đào tạo để xuất ra một phần thưởng đại diện cho mức độ một chuỗi nhất định phù hợp với một thuộc tính mong muốn. Tính một chiều của mô hình phần thưởng cho phép lưu trữ các kích hoạt trung gian khi chuỗi được sinh ra, giảm đáng kể chi phí tính toán. Trong quá trình giải mã, các token có xác suất cao nhất trong top-k được điều chỉnh lại theo mô hình phần thưởng để các token phản ánh tốt hơn thuộc tính mong muốn có nhiều khả năng được chọn làm token được sinh ra tiếp theo.

Để xác nhận hiệu quả của RAD, chúng tôi đánh giá nó trên các tác vụ tiêu chuẩn về khử độc và sinh có kiểm soát cảm xúc, cho thấy rằng nó điều hướng việc sinh văn bản về phía một thuộc tính mong muốn mà không hy sinh nhiều tính đa dạng và trôi chảy. Cuối cùng chúng tôi thấy rằng RAD vượt trội so với các phương pháp giải mã có trọng số khác và đạt được kết quả tương đương với các phương pháp có liên quan đến đào tạo bổ sung. Chúng tôi tiếp tục xác nhận RAD trong môi trường quy mô lớn thực tế bằng cách cho thấy nó hiệu quả và gây ra chi phí tính toán tối thiểu khi được áp dụng cho họ mô hình ngôn ngữ LLaMA (Touvron et al., 2023) với tối đa 65B tham số.

2 Giải Mã Tăng Cường Phần Thưởng
Ở cấp độ cao, giải mã tăng cường phần thưởng, như được hiển thị trong hình 1, đưa các chuỗi ứng viên trung gian vào một mô hình phần thưởng đánh giá sự phù hợp của chúng với một thuộc tính mong muốn. Sau đó, tại mỗi bước giải mã, RAD sử dụng phần thưởng dự đoán của mỗi chuỗi ứng viên để sửa đổi xác suất token được xuất ra bởi mô hình ngôn ngữ. Trong phần này, chúng tôi mô tả chi tiết những bước này. Tham khảo bảng 2 để biết mô tả các ký hiệu được sử dụng trong bài báo này.

2.1 Mô Hình Phần Thưởng Một Chiều
Xem xét việc sử dụng một mô hình phần thưởng để tính phần thưởng cho k token ứng viên tại mỗi trong số m bước thời gian sinh. Nếu việc chấm điểm mỗi token ứng viên yêu cầu xử lý lại toàn bộ chuỗi được sinh ra cho đến bước thời gian hiện tại, mô hình phần thưởng sẽ cần xử lý O(km²) token, điều này có thể cực kỳ đắt đỏ. Để giải quyết những vấn đề này, chúng tôi sử dụng một mô hình phần thưởng một chiều, cụ thể là một bộ giải mã Transformer với che phủ nhân quả (Liu et al., 2018; Radford et al., 2018). Trong một mô hình một chiều với che phủ nhân quả, các biểu diễn được tính toán trước đó vẫn không thay đổi khi các token mới được thêm vào, vì vậy tại mỗi bước thời gian sinh mô hình phần thưởng chỉ cần tính toán biểu diễn của token mới được thêm. Điều này giảm chi phí tính toán xuống O(km).

Trong công việc này, mô hình phần thưởng là một Transformer chỉ giải mã được đào tạo trước đã được sửa đổi (GPT-2 small (Radford et al., 2019a) trong tất cả các thí nghiệm của chúng tôi) được tinh chỉnh trên văn bản được chú thích với lượng thuộc tính mục tiêu có mặt. Chúng tôi sử dụng hàm mất mát lỗi bình phương tích lũy lấy trung bình có trọng số của mất mát mỗi tiền tố:

L(r,ř) = ∑ᵗ₌₁ˡ t(rₜ-ř)² / Sₗ, Sₗ = l(l+1)/2

trong đó rₜ là dự đoán của mô hình phần thưởng tại bước thời gian sinh t, ř∈[0,1] là giá trị phần thưởng thực tế, và l là độ dài sinh. Hàm mất mát tích lũy khuyến khích mô hình phần thưởng xuất ra phần thưởng đúng cho mọi tiền tố của

--- TRANG 3 ---
Thuật toán 1 Giải Mã Tăng Cường Phần Thưởng
Đầu vào fθ mô hình ngôn ngữ mạng neural (xuất logits)
gλ mô hình phần thưởng mạng neural (xuất điểm phần thưởng)
X tiền tố sinh
1: xₜ←none
2: while xₜ≠<EOS> do
3: wₜ←topk(fθ(X)) // lấy top-k tokens (chỉ số), wₜ∈Nᵏ
4: zₜ←fθ(X)[wₜ] // lấy top-k token logits, zₜ∈Rᵏ
5: ρₜ←gλ([X;wₜ,₁...X;wₜ,ₖ]) // tính phần thưởng, ρₜ∈[0,1]ᵏ
6: pₜ←softmax(zₜ+βρₜ) // tính phân bố được điều chỉnh lại trọng số
7: xₜ∼Categorical(pₜ)
8: X←{X;xₜ} // thêm mẫu mới
Đầu ra văn bản được sinh X được điều hướng về phía phần thưởng cao hơn

chuỗi văn bản để nắm bắt cả sự phù hợp hiện tại và tương lai của một chuỗi ứng viên với thuộc tính mong muốn.

2.2 Giải mã có trọng số
RAD sử dụng lấy mẫu top-k (Fan et al., 2018; Holtzman et al., 2018; Radford et al., 2019b) và điều chỉnh lại trọng số các xác suất của các token có xác suất cao nhất trong top-k dựa trên điểm phần thưởng của mỗi ứng viên. Cụ thể, tại bước thời gian t, việc điều chỉnh lại trọng số được thực hiện bằng cách tính

softmax(zₜ+βρₜ)

trong đó zₜ∈Rᵏ là k logits lớn nhất được xuất ra bởi mô hình ngôn ngữ tại bước thời gian đầu ra t, β∈R là một siêu tham số tỷ lệ (với β cao hơn tương ứng với điều hướng mạnh hơn), và ρₜ∈[0,1]ᵏ là các giá trị phần thưởng cho k chuỗi tương ứng với việc thêm mỗi token trong top-k. Việc thêm βρₜ và chuẩn hóa lại với softmax tỷ lệ thuận với việc điều chỉnh lại trọng số các xác suất top-k bằng e^(βρₜ). Do đó, RAD hiệu quả điều chỉnh lại xác suất của các token top-k theo sự khác biệt tương đối của chúng về phần thưởng. Thuật toán 1 cung cấp tổng quan về quy trình giải mã.

3 Thí nghiệm
Bây giờ chúng tôi đánh giá hiệu suất của RAD trong hai thiết lập tiêu chuẩn: Ngăn chặn các mô hình ngôn ngữ sinh ra văn bản độc hại (Wallace et al., 2019; Gehman et al., 2020) và kiểm soát cảm xúc của văn bản được sinh (Li et al., 2018; Sudhakar et al., 2019).

Cơ sở so sánh Trong cả hai thiết lập, chúng tôi xem xét cùng một tập hợp cơ sở so sánh như Liu et al. (2021), cụ thể là: hiệu suất của chính mô hình ngôn ngữ cơ sở mà không có bất kỳ can thiệp nào; PPLM (Pascual et al., 2021), sử dụng một bộ phân loại túi từ để cập nhật các trạng thái ẩn của LM trong quá trình giải mã; GeDi (Krause et al., 2021) và DExperts (Liu et al., 2021), sử dụng tín hiệu từ các mô hình ngôn ngữ phụ trợ để sửa đổi xác suất LM trong một lần xử lý; Rectification (Cao et al., 2023), điều chỉnh xác suất LM tỷ lệ thuận với rủi ro dẫn đến sinh độc hại; DAPT (Gururangan et al., 2020), đào tạo thêm mô hình trên dữ liệu có thuộc tính mong muốn; PPO (Schulman et al., 2017), cập nhật LM với gradient từ mô hình phần thưởng; Quark (Lu et al., 2022), thực hiện tinh chỉnh hiệu quả tham số trên dữ liệu được chú thích thuộc tính (Lester et al., 2021; Li and Liang, 2021); và CTRL (Keskar et al., 2019), một mô hình ngôn ngữ được đào tạo để điều kiện trên mã kiểm soát. Trừ khi được đề cập khác, chúng tôi báo cáo kết quả trực tiếp từ Liu et al. (2021) và Lu et al. (2022), có thể được tham khảo để biết thêm chi tiết về cơ sở so sánh.

3.1 Khử độc
Thiết lập Thí nghiệm. Chúng tôi tuân theo chặt chẽ công việc trước đây (Liu et al., 2021) và sử dụng RAD để khử độc các sinh phẩm từ GPT-2 Large (Radford et al., 2019a) sau khi điều kiện trên các prompt từ tập dữ liệu RealToxicityPrompts (Gehman et al., 2020). Đối với mô hình phần thưởng của chúng tôi, chúng tôi tinh chỉnh GPT-2 Small trên 2M bình luận được chú thích bởi con người với nhãn liên tục giữa 0 và 1 từ tập dữ liệu Jigsaw Unintended Bias in Toxicity Classification.¹ Chúng tôi báo cáo hiệu suất của RAD với các giá trị k khác nhau (được sử dụng trong lấy mẫu top-k) và β (được sử dụng để điều chỉnh giải mã có trọng số).

Chỉ số Đánh giá. Đối với mỗi prompt, chúng tôi lấy mẫu 25 phần tiếp tục, mỗi phần chứa tối đa 20 token mới

¹https://bit.ly/43CAdCJ

--- TRANG 4 ---
[Hình 2 miêu tả một biểu đồ so sánh các phương pháp với RAD vượt trội hơn tất cả các phương pháp giải mã có trọng số và bằng với các phương pháp có đào tạo bổ sung]

token. Như trong Liu et al. (2021), chúng tôi đo Độ Độc Hại Tối Đa Trung Bình, tức là độ độc hại tối đa kỳ vọng của 25 phần tiếp tục được đánh giá bởi Perspective API² và Tỷ lệ Độc Hại, tức là xác suất ít nhất một trong 25 phần tiếp tục là độc hại (điểm độc hại Perspective API >0.5). Vì perspective API thay đổi theo thời gian (Pozzobon et al., 2023), chúng tôi đã tính toán lại điểm số cho tất cả các phương pháp cơ sở so sánh. Chúng tôi cũng đo Tính Đa dạng như số lượng bigram và trigram riêng biệt được chuẩn hóa theo độ dài văn bản (Li et al., 2016) và Tính Trôi chảy như độ phức tạp được gán cho phần tiếp tục bởi GPT-2-XL có điều kiện trên prompt. Nói chung, một phương pháp tốt nên giảm độ độc hại trong khi bảo tồn tính trôi chảy và đa dạng.

Kết quả. Như được hiển thị trong hình 2 và bảng 4 (phụ lục), RAD thể hiện sự đánh đổi thuận lợi giữa độ độc hại và tính trôi chảy mà không hy sinh đáng kể tính đa dạng, cuối cùng vượt trội so với tất cả các phương pháp giải mã có trọng số và bằng hiệu suất của các phương pháp có liên quan đến đào tạo bổ sung. Hơn nữa, RAD đạt được Độ Độc Hại Tối Đa Trung Bình thấp nhất trong bất kỳ phương pháp nào. Kết quả của chúng tôi tiếp tục chứng minh rằng RAD cung cấp một phương tiện trực quan để đánh đổi hiệu quả giữa độ độc hại và tính trôi chảy bằng cách điều chỉnh β.

3.2 Sinh Có Kiểm Soát Cảm Xúc
Thiết lập Thí nghiệm. Theo công việc trước đây (Li et al., 2018; Sudhakar et al., 2019; Liu et al., 2021), chúng tôi sử dụng RAD để điều hướng việc sinh của GPT-2 Large để có cảm xúc tích cực/tiêu cực khi được nhắc với các prompt tiêu cực/tích cực hoặc trung tính. Cụ thể, chúng tôi đánh giá trên 2.5K prompt tiêu cực, 5K trung tính, và 2.5K prompt tích cực từ OpenWebText (Gokaslan and Cohen, 2019). Đối với mô hình phần thưởng của RAD, chúng tôi tinh chỉnh GPT-2 Small trên hàng triệu

²https://bit.ly/3p2r87b

[Hình 3 miêu tả kết quả RAD đạt tỷ lệ tích cực cao nhất cho prompt tiêu cực và vượt trội so với tất cả các phương pháp giải mã có trọng số]

đánh giá sản phẩm và phim từ Amazon Polarity³ và SST-2 (Socher et al., 2013).

Chỉ số Đánh giá. Chúng tôi lấy mẫu 25 phần tiếp tục cho mỗi prompt và tính Tỷ lệ Tích cực trung bình được đo bởi pipeline phân loại văn bản HuggingFace⁴ (một mô hình DistilBERT được tinh chỉnh trên SST-2). Chúng tôi cũng báo cáo Tính Đa dạng và Tính Trôi chảy như đã giới thiệu ở trên.

Kết quả. Như thấy trong hình 3 và bảng 5 (phụ lục), RAD đạt được sự đánh đổi tính trôi chảy/tích cực tốt hơn (khi điều kiện trên prompt tiêu cực hoặc trung tính) so với bất kỳ phương pháp giải mã có trọng số nào khác và đạt hiệu suất tương đương với các phương pháp tiên tiến nhất có liên quan đến đào tạo (Quark và PPO), cả hai đều sử dụng mô hình đánh giá (mô hình DistilBERT được tinh chỉnh trên SST-2) trong quá trình đào tạo. Việc điều chỉnh β hiệu quả đánh đổi giữa tính trôi chảy và sự phù hợp, một lần nữa cho phép RAD tạo ra điểm số thuộc tính tốt nhất. Hình 4 (phụ lục) trực quan hóa quy trình điều hướng của RAD khi được nhắc với đầu vào tiêu cực.

3.3 Mở rộng Mô hình Ngôn ngữ
Trong tất cả các thí nghiệm trước đây, chúng tôi đã tuân theo công việc trước và xem xét việc sử dụng GPT-2 Large làm mô hình ngôn ngữ cơ sở. Các LLM gần đây có số lượng tham số lớn hơn đáng kể (và hiệu suất tốt hơn đáng kể). Để kiểm tra RAD trong các thiết lập thực tế hơn, chúng tôi áp dụng RAD cho các mô hình LLaMA tiên tiến (Touvron

³https://bit.ly/3XfY6NZ
⁴https://bit.ly/3qIycX9

--- TRANG 5 ---
bước 1: be
2: in
3: developing
4: countries
5: .
6: But
7: with
8: clean
9: energy
10: technologies
11: and
12: other
13: energy
14: efficiency
15: innovations
16:
17: like
18: solar
19: panels
20: , 
Prompt: The most polluted cities tend to

[Hình 4: Trực quan hóa quy trình giải mã của RAD. Mỗi hàng đại diện cho một bước giải mã đơn, trong đó diện tích là phân bố phần thưởng ước tính của top-50 chuỗi ứng viên, và đường đỏ chỉ ra điểm phần thưởng của token được chọn.]

et al., 2023) trong thiết lập khử độc của phần 3.1, sử dụng cùng mô hình phần thưởng GPT-2 Small. Trong bảng 6 (phụ lục), chúng tôi cho thấy RAD giảm đáng kể độ độc hại của LLaMA trong khi bảo tồn tính đa dạng và trôi chảy của nó. Về mặt chi phí tính toán, chúng tôi liệt kê chi phí tương đối của các phương pháp khác nhau cho sinh văn bản có kiểm soát trong bảng 1.

Trong khi RAD và các phương pháp giải mã có trọng số khác tăng chi phí đáng kể khi kích thước của mô hình ngôn ngữ và mô hình phần thưởng tương tự, chi phí bổ sung của việc sử dụng RAD chỉ khoảng 3% khi sử dụng LLaMA 65B làm mô hình ngôn ngữ và GPT-2 Small làm mô hình phần thưởng. Những kết quả này xác nhận rằng RAD có thể kiểm soát hiệu quả việc sinh văn bản của các mô hình tiên tiến trong khi chỉ phát sinh chi phí tính toán không đáng kể.

4 Kết luận và Công việc Tương lai
Trong bài báo này, chúng tôi đề xuất RAD, một phương pháp giải mã có trọng số đơn giản để kiểm soát sinh văn bản sử dụng mô hình phần thưởng một chiều để giảm thiểu chi phí tính toán. RAD vượt trội so với các phương pháp giải mã có trọng số trước đây và bằng hiệu suất của các kỹ thuật tiên tiến nhất có liên quan đến đào tạo bổ sung. Khi kích thước của mô hình phần thưởng tương đối nhỏ so với mô hình ngôn ngữ cơ sở, RAD chỉ phát sinh chi phí tính toán không đáng kể. Trong công việc tương lai, chúng tôi quan tâm đến việc áp

[Bảng 1: Chi phí tính toán (như mức tăng chi phí tương đối) cho các phương pháp khác nhau để kiểm soát sinh văn bản sử dụng GPT-2 Small làm mô hình phần thưởng và GPT-2 Large hoặc LLaMA 65B làm mô hình ngôn ngữ.]

Chi phí Giải mã
Phương pháp      GPT-2 Large    LLaMA 65B
PPLM             4.0×           4.00×
GeDi             1.9×           1.01×
DExperts         3.0×           1.02×
Đào tạo bổ sung  1×             1×
RAD              3.4×           1.03×

dụng RAD cho các tác vụ phức tạp hơn, chẳng hạn như khuyến khích các mô hình ngôn ngữ tuân theo hướng dẫn (Ouyang et al., 2022).

Hạn chế
Mặc dù RAD đạt được hiệu suất khá tốt và tổng quát hóa cho các mô hình ngôn ngữ khác, hai hạn chế nên được xem xét cho công việc này. Thứ nhất, RAD phát sinh chi phí tính toán và phân bổ bộ nhớ bổ sung tuyến tính với k. Như đã đề cập trong phần 2.1, chúng tôi quản lý để giảm độ phức tạp thời gian từ O(km²) xuống O(km) bằng cách tái sử dụng các biểu diễn được tính toán trước đó trong mô hình phần thưởng giải mã. Tuy nhiên, việc theo dõi và sao chép past_key_values chiếm một lượng bộ nhớ GPU nhất định, điều này làm giảm thông lượng giải mã. Thứ hai, các thí nghiệm của chúng tôi về độ độc hại và cảm xúc chỉ khám phá một số khả năng của RAD. Cần tiến hành thêm các tác vụ để tạo thành một đánh giá toàn diện về RAD.

Tuyên bố Đạo đức
Công việc này tập trung vào sinh văn bản có thể kiểm soát, có ý nghĩa đáng kể trong việc điều chỉnh sinh ngôn ngữ tự nhiên. Ví dụ, tác vụ khử độc nhằm giảm thiểu độ độc hại có mặt trong văn bản được sinh ra bởi các mô hình ngôn ngữ được đào tạo trước. Trong bối cảnh này, RAD cung cấp một giải pháp để kiểm soát quy trình sinh văn bản mà không sửa đổi mô hình ngôn ngữ cơ sở.

Lời cảm ơn
Chúng tôi muốn cảm ơn Derek Tam vì những cuộc thảo luận có giá trị. Chúng tôi cũng bày tỏ sự đánh giá cao của mình đối với nhóm Perspective API vì đã tăng hạn ngạch API thay mặt chúng tôi.

--- TRANG 6 ---
Tài liệu tham khảo
[Danh sách các tài liệu tham khảo với các tác giả, năm xuất bản và tiêu đề được dịch sang tiếng Việt]

--- TRANG 7 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 8 ---
[Tiếp tục danh sách tài liệu tham khảo]

A Ký hiệu
Tham khảo bảng 2 cho các ký hiệu được sử dụng trong bài báo.

B Chi tiết Đào tạo RAD
B.1 Khử độc
Chúng tôi đào tạo mô hình phần thưởng GPT-2 Small trên tập dữ liệu Jigsaw Unintended Bias in Toxicity Classification¹ trong 5 epoch. Chúng tôi sử dụng learning rate = 1e−5, weight decay = 0.01, và batch size = 100. Mô hình phần thưởng đạt được lỗi bình phương cuối cùng là 0.0147 trên tập con test public leaderboard.

B.2 Sinh Có Kiểm Soát Cảm Xúc
Đầu tiên chúng tôi đào tạo mô hình phần thưởng trên Amazon Polarity³ trong 5 epoch, với learning rate = 1e−5, weight decay = 0.01, và batch size = 100. Sau đó chúng tôi tiếp tục đào tạo mô hình phần thưởng trên SST-2 (Socher et al., 2013) trong 10 epoch, với learning rate = 2e−6.

C Chi phí Tính toán
C.1 RAD
Trong bài báo, chúng tôi sử dụng GPT-2 Small (124M) làm mô hình phần thưởng của RAD và thay thế lớp lm_head bằng một lớp tuyến tính với một đầu ra để dự đoán phần thưởng. Theo các xấp xỉ trong Kaplan et al. (2020), số lượng tham số không nhúng trong một mô hình xấp xỉ

N≈12nlayerd²model

trong đó nlayer là số lớp và dmodel là kích thước ẩn. Ngoài ra, một lần truyền xuôi yêu cầu

Cforward≈2N + 2nlayernctxdmodel

FLOPs mỗi token, trong đó nctx là token ngữ cảnh. Với thao tác nhúng tốn 4dmodel và dự đoán phần thưởng tốn 2dmodel, chúng tôi xây dựng số FLOPs cần thiết cho một token trong mô hình phần thưởng là

CRM = 6dmodel + Cforward

Vì 6dmodel≪N,

CRM≈Cforward = 2(1 + nctx/12dmodel)N

Chú ý chi phí tính toán phụ thuộc ngữ cảnh mỗi token là một phần nhỏ của tổng chi phí tính toán, vì dmodel > nctx/12 thường đúng trong thực tế (Kaplan et al., 2020). Thực tế, trong các thí nghiệm khử độc và sinh có kiểm soát cảm xúc, nctx liên tục dưới 50. Do đó, có thể an toàn giả định

CRM = Cforward = 2N

cho cả mô hình ngôn ngữ và mô hình phần thưởng. Mô hình phần thưởng đánh giá k chuỗi ứng viên tại mỗi bước giải mã, yêu cầu tổng cộng kCRM FLOPs. Giả định k = 20, CRAD = kCRM, phụ lục C.1 hiển thị FLOPs ước tính mỗi token của mô hình phần thưởng và các mô hình ngôn ngữ khác nhau.

C.2 So sánh
Chúng tôi tiếp tục khám phá chi phí tính toán của các phương pháp cơ sở so sánh dựa trên phương pháp luận trong phụ lục C.1. Định nghĩa Cmethod là chi phí bổ sung mà một phương pháp phát sinh trong quá trình giải mã và TCmethod×LM là tổng chi phí trong FLOPs cho mỗi token được sinh ra sử dụng phương pháp trên LM cụ thể trong thời gian kiểm tra. Các phương pháp đào tạo lại (DAPT, PPO, và Quark) có Cmethod = 0 và TCmethod×LM = CLM.

PPLM cập nhật biểu diễn token trước đó trong LM sử dụng gradient từ một bộ phân biệt tuyến tính cụ thể thuộc tính và tính toán lại xác suất trạng thái hiện tại. Do đó, hai lần truyền xuôi và một lần truyền ngược của LM được yêu cầu cho mỗi token được sinh. Vì một lần truyền ngược có khoảng gấp đôi số phép nhân ma trận như trong một lần truyền xuôi (Kaplan et al., 2020), PPLM phát sinh chi phí giải mã bổ sung là

CPPLM = 3CLM. Do đó,

TCPPLM×GPT2-large = 4CGPT2-large = 5.66G
TCPPLM×LLaMA-65b = 4CLLaMA-65b = 515.40G

GeDi & DExperts áp dụng một cách tiếp cận tương tự trong đó họ sử dụng hai mô hình phân biệt/chuyên gia đối lập để tạo ra xác suất phân loại và sau đó

--- TRANG 9 ---
[Bảng 2: Các ký hiệu và mô tả]

Ký hiệu    Chiều        Mô tả
k          N            số token ứng viên xem xét tại mỗi bước thời gian
β          R            siêu tham số lượng điều hướng
l          N            độ dài sinh của dữ liệu đào tạo mô hình phần thưởng
ř          [0,1]        nhãn của dữ liệu đào tạo mô hình phần thưởng
r          [0,1]ˡ       dự đoán được sinh ra bởi mô hình phần thưởng trong quá trình đào tạo
ρₜ         [0,1]ᵏ       điểm phần thưởng được dự đoán bởi mô hình phần thưởng tại thời điểm t
wₜ         Nᵏ           chỉ số của top-k tokens tại thời điểm t
zₜ         Rᵏ           logits của top-k tokens tại thời điểm t

[Bảng 3: Thông số kỹ thuật mô hình và FLOPs mỗi token]

điều chỉnh lại xác suất LM. Do đó, hai lần truyền xuôi bổ sung của mô hình chuyên gia là cần thiết. Đối với GeDi,

CGeDi = 2CGPT2-medium = 1.21G

TCGeDi×GPT2-large = CGeDi + CGPT2-large = 2.62G
TCGeDi×LLaMA-65b = CGeDi + CLLaMA-65b = 130.06G

Đối với DExperts,

CDExperts = 2CGPT2-large = 2.83G

TCDExperts×GPT2-large = CDExperts + CGPT2-large = 4.25G
TCDExperts×LLaMA-65b = CDExperts + CLLaMA-65b = 131.68G

RAD với k = 20 có

CRAD = 20CGPT2-small = 3.40G

TCRAD×GPT2-large = CRAD + CGPT2-large = 4.81G
TCRAD×LLaMA-65b = CRAD + CLLaMA-65b = 132.25G

DAPT, PPO, và Quark có chi phí giải mã giống như LM cơ bản vì họ thực hiện đào tạo bổ sung và không sửa đổi quy trình giải mã.

D Kết quả Đầy đủ

D.1 Khử độc
Vì perspective API cập nhật mô hình của nó thường xuyên (Pozzobon et al., 2023), chúng tôi đảm bảo so sánh công bằng bằng cách đánh giá tất cả đầu ra mô hình (trừ PPO và Quark, xem bên dưới) sử dụng API cập nhật nhất. Các truy vấn được thực hiện giữa tháng 5 và tháng 6 năm 2023. Vì PPO và Quark tối ưu hóa trực tiếp mô hình ngôn ngữ với điểm Perspective API trong quá trình đào tạo, một thay đổi trong mô hình API sẽ dẫn đến một mô hình tối ưu hóa khác. Đối với PPO và Quark, chúng tôi áp dụng các giá trị được báo cáo trong Lu et al. (2022). Kết quả đầy đủ xem bảng 4.

D.2 Sinh Có Kiểm Soát Cảm Xúc
Kết quả sinh có kiểm soát cảm xúc được trình bày trong bảng 5.

D.3 Mở rộng Mô hình Ngôn ngữ
Theo các thí nghiệm trước đây, chúng tôi sử dụng nucleus sampling với p = 0.9 để có được các sinh phẩm LLaMA thô trên cùng 10K tập con không độc hại của RealToxicityPrompts (Gehman et al., 2020). Đối với mỗi kích thước mô hình, chúng tôi áp dụng RAD với k = 20 và β từ 20 đến 500. Kết quả được hiển thị trong bảng 6.

Khoảng cách hiệu suất giữa RAD trên GPT-2 Large và RAD trên LLaMA có thể được quy cho sự khác biệt trong tokenization giữa mô hình ngôn ngữ và mô hình phần thưởng. Cụ thể, mô hình phần thưởng, GPT-2 Small, chia sẻ cùng tokenizer và từ vựng với GPT-2 Large, nhưng không với LLaMA. Theo cách này, một chuỗi văn bản nhất định có thể được tokenize thành các kết hợp token khác nhau, điều này, trong quá trình giải mã, sẽ làm cho mô hình phần thưởng đưa ra điểm số bị bóp méo. Do đó, chúng tôi tin rằng một mô hình nhỏ hơn từ cùng họ của LM cơ sở có thể là lựa chọn tốt nhất cho mô hình phần thưởng của RAD.

E Ví dụ Được Sinh
Các ví dụ khử độc và sinh có kiểm soát cảm xúc được trình bày trong bảng 7 và 8.

--- TRANG 10 ---
[Bảng 4: Kết quả đầy đủ của thí nghiệm khử độc - với các số liệu về độ độc hại, tính trôi chảy và đa dạng cho các phương pháp khác nhau]

[Bảng 5: Kết quả đầy đủ của thí nghiệm sinh có kiểm soát cảm xúc - với tỷ lệ tích cực, tính trôi chảy và đa dạng]

--- TRANG 11 ---
[Bảng 6: Kết quả áp dụng RAD trên các mô hình LLaMA với kích thước khác nhau]

[Bảng 7: Ví dụ về văn bản được khử độc sử dụng các phương pháp khác nhau]

[Bảng 8: Ví dụ về sinh có kiểm soát cảm xúc sử dụng các phương pháp khác nhau]
