Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Huấn luyện một trợ lý
hữu ích và vô hại với học tăng cường từ phản hồi con người. arXiv preprint
arXiv:2204.05862, 2022a.

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy
Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. AI Hiến pháp: Tính vô hại từ phản hồi AI. arXiv preprint arXiv:2212.08073, 2022b.

Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng
Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, và Lei Hou. Đánh giá các
mô hình nền tảng với mô hình ngôn ngữ-làm-giám khảo. Trong Hội nghị lần thứ ba mươi bảy về
Hệ thống Xử lý Thông tin Thần kinh Track Tập dữ liệu và Benchmark, 2023. URL
https://openreview.net/forum?id=IiRHQ7gvnq.

Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, và Yejin Choi. Piqa: Lý luận
về lẽ thường vật lý trong ngôn ngữ tự nhiên. Trong Hội nghị AAAI lần thứ Ba mươi tư về
Trí tuệ Nhân tạo, 2020.

Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang,
Vijay Srinivasan, Tianyi Zhou, Heng Huang, et al. AlpaGasus: Huấn luyện alpaca tốt hơn
với ít dữ liệu hơn. Trong Hội nghị Quốc tế lần thứ Mười hai về Biểu diễn Học, 2024a. URL https://openreview.net/forum?id=FdVXgSJhvz.

Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, và Quanquan Gu. Tinh chỉnh tự chơi
chuyển đổi mô hình ngôn ngữ yếu thành mô hình ngôn ngữ mạnh. arXiv preprint arXiv:2401.01335,
2024b.

Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,
và Oyvind Tafjord. Bạn nghĩ mình đã giải quyết việc trả lời câu hỏi? Thử ARC, thử thách lý luận AI2. arXiv preprint arXiv:1803.05457, 2018.

Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher
Hesse, và John Schulman. Huấn luyện các trình xác minh để giải quyết bài toán từ toán học.
arXiv preprint arXiv:2110.14168, 2021.

Ronan Collobert và Jason Weston. Một kiến trúc thống nhất cho xử lý ngôn ngữ tự nhiên:
Mạng nơ ron sâu với học đa nhiệm. Trong Kỷ yếu Hội nghị Quốc tế lần thứ 25 về
Học máy, trang 160-167, 2008.

Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos
Guestrin, Percy Liang, và Tatsunori B Hashimoto. Alpacafarm: Một khung mô phỏng
cho các phương pháp học từ phản hồi con người. arXiv preprint arXiv:2305.14387, 2023.

Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham
Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, và Orhan Firat. Quỷ dữ nằm trong các lỗi: Tận dụng các mô hình ngôn ngữ lớn để đánh giá dịch máy chi tiết.

--- TRANG 15 ---
Trong Philipp Koehn, Barry Haddow, Tom Kocmi, và Christof Monz, biên tập viên, Kỷ yếu
Hội nghị Dịch máy lần thứ Tám, trang 1066-1083, Singapore, Tháng 12
2023. Hiệp hội Ngôn ngữ học Tính toán. doi: 10.18653/v1/2023.wmt-1.100. URL
https://aclanthology.org/2023.wmt-1.100.

Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts,
Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al.
Tự huấn luyện tăng cường (rest) cho mô hình hóa ngôn ngữ. arXiv preprint arXiv:2308.08998,
2023.

Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, và Jacob Steinhardt. Đo lường hiểu biết ngôn ngữ đa nhiệm quy mô lớn. Trong Hội nghị Quốc tế lần thứ 9 về Biểu diễn Học, ICLR 2021, Sự kiện Ảo, Áo, 3-7 Tháng 5, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=d7KBjmI3GmQ.

Or Honovich, Thomas Scialom, Omer Levy, và Timo Schick. Hướng dẫn không tự nhiên: Điều chỉnh
các mô hình ngôn ngữ với (gần như) không có lao động con người. Trong Anna Rogers, Jordan Boyd-Graber,
và Naoaki Okazaki, biên tập viên, Kỷ yếu Hội nghị Thường niên lần thứ 61 của Hiệp hội
Ngôn ngữ học Tính toán (Tập 1: Bài báo Dài), trang 14409-14428, Toronto, Canada,
Tháng 7 2023. Hiệp hội Ngôn ngữ học Tính toán. doi: 10.18653/v1/2023.acl-long.806.
URL https://aclanthology.org/2023.acl-long.806.

Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun,
Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Tạo ra khả năng đánh giá
chi tiết trong các mô hình ngôn ngữ. arXiv preprint arXiv:2310.08491, 2023.

Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam,
Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi,
et al. Các cuộc hội thoại OpenAssistant-dân chủ hóa việc căn chỉnh mô hình ngôn ngữ lớn.
arXiv preprint arXiv:2304.07327, 2023.

Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh,
Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton
Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit,
Quoc Le, và Slav Petrov. Câu hỏi tự nhiên: một benchmark cho nghiên cứu trả lời câu hỏi.
Giao dịch của Hiệp hội Ngôn ngữ học Tính toán, 2019.

Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop,
Victor Carbune, và Abhinav Rastogi. RLAIF: Mở rộng học tăng cường từ phản hồi con người
với phản hồi ai. arXiv preprint arXiv:2309.00267, 2023.

Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason
Weston, và Mike Lewis. Tự căn chỉnh với dịch ngược hướng dẫn. Trong Hội nghị Quốc tế
lần thứ Mười hai về Biểu diễn Học, 2024. URL https://openreview.
net/forum?id=1oijHJBRsT.

Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin,
Percy Liang, và Tatsunori B. Hashimoto. Alpacaeval: Một trình đánh giá tự động của
các mô hình tuân theo hướng dẫn. https://github.com/tatsu-lab/alpaca_eval, 2023.

Chin-Yew Lin. ROUGE: Một gói cho đánh giá tự động các bản tóm tắt. Trong Tóm tắt Văn bản
Phân nhánh, trang 74-81, Barcelona, Tây Ban Nha, Tháng 7 2004. Hiệp hội
Ngôn ngữ học Tính toán. URL https://aclanthology.org/W04-1013.

Todor Mihaylov, Peter Clark, Tushar Khot, và Ashish Sabharwal. Một bộ áo giáp có thể
dẫn điện không? một tập dữ liệu mới cho trả lời câu hỏi sách mở. Trong EMNLP, 2018.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Huấn luyện các mô hình ngôn ngữ
để tuân theo hướng dẫn với phản hồi con người. Tiến bộ trong Hệ thống Xử lý Thông tin
Thần kinh, 35:27730-27744, 2022.

--- TRANG 16 ---
Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, và William Yang
Wang. Tự động sửa chữa các mô hình ngôn ngữ lớn: Khảo sát cảnh quan của các chiến lược
tự sửa chữa đa dạng. arXiv preprint arXiv:2308.03188, 2023.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al.
Các mô hình ngôn ngữ là những người học đa nhiệm không giám sát. Blog OpenAI, 1(8):9, 2019.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, và
Chelsea Finn. Tối ưu hóa sở thích trực tiếp: Mô hình ngôn ngữ của bạn thầm kín là một mô hình
thưởng. Trong Hội nghị lần thứ ba mươi bảy về Hệ thống Xử lý Thông tin Thần kinh, 2023.
URL https://openreview.net/forum?id=HPuSIXJaa9.

Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, và Xian
Li. Branch-solve-merge cải thiện đánh giá và tạo sinh mô hình ngôn ngữ lớn. arXiv
preprint arXiv:2310.15123, 2023.

Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, và Yejin Choi. Socialiqa:
Lý luận lẽ thường về tương tác xã hội. CoRR, abs/1904.09728, 2019. URL
http://arxiv.org/abs/1904.09728.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, và Oleg Klimov. Thuật toán
tối ưu hóa chính sách gần. arXiv preprint arXiv:1707.06347, 2017.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec
Radford, Dario Amodei, và Paul F Christiano. Học tóm tắt với phản hồi con người.
Tiến bộ trong Hệ thống Xử lý Thông tin Thần kinh, 33:3008-3021, 2020.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,
Percy Liang, và Tatsunori B. Hashimoto. Stanford alpaca: Một mô hình llama tuân theo hướng dẫn.
https://github.com/tatsu-lab/stanford_alpaca, 2023.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:
Các mô hình nền tảng mở và chat được tinh chỉnh. arXiv preprint arXiv:2307.09288, 2023.

Laurens Van der Maaten và Geoffrey Hinton.. Trực quan hóa dữ liệu bằng t-SNE. Tạp chí
nghiên cứu học máy, 9(11), 2008.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel
Khashabi, và Hannaneh Hajishirzi. Tự hướng dẫn: Căn chỉnh các mô hình ngôn ngữ với các hướng dẫn
tự tạo. Trong Anna Rogers, Jordan Boyd-Graber, và Naoaki Okazaki,
biên tập viên, Kỷ yếu Hội nghị Thường niên lần thứ 61 của Hiệp hội Ngôn ngữ học
Tính toán (Tập 1: Bài báo Dài), trang 13484-13508, Toronto, Canada, Tháng 7 2023.
Hiệp hội Ngôn ngữ học Tính toán. doi: 10.18653/v1/2023.acl-long.754. URL
https://aclanthology.org/2023.acl-long.754.

Wei Xiong, Hanze Dong, Chenlu Ye, Han Zhong, Nan Jiang, và Tong Zhang. Lấy mẫu Gibbs
từ phản hồi con người: Một khung KL-ràng buộc có thể chứng minh cho rlhf. arXiv
preprint arXiv:2312.11456, 2023.

Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, và Jason Weston. Một số thứ khó chịu hơn
những thứ khác: Tối ưu hóa sở thích với loss cringe theo cặp. arXiv preprint
arXiv:2312.16682, 2023.

Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, và Fei Huang.
RRHF: Xếp hạng phản hồi để căn chỉnh các mô hình ngôn ngữ với phản hồi con người. Trong Hội nghị
lần thứ ba mươi bảy về Hệ thống Xử lý Thông tin Thần kinh, 2023. URL https://openreview.
net/forum?id=EdIGMCHk4l.

Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, và Yejin Choi. Hellaswag: Một
máy có thể thực sự hoàn thành câu của bạn không? Trong Anna Korhonen, David R. Traum, và Lluís
Màrquez, biên tập viên, Kỷ yếu Hội nghị lần thứ 57 của Hiệp hội Ngôn ngữ học
Tính toán, ACL 2019, Florence, Italy, Tháng 7 28- Tháng 8 2, 2019, Tập 1: Bài báo Dài,
trang 4791-4800. Hiệp hội Ngôn ngữ học Tính toán, 2019. doi: 10.18653/V1/
P19-1472. URL https://doi.org/10.18653/v1/p19-1472.

--- TRANG 17 ---
Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, và Peter J
Liu. SLiC-HF: Hiệu chuẩn khả năng chuỗi với phản hồi con người. arXiv preprint
arXiv:2305.10425, 2023.

Chujie Zheng, Pei Ke, Zheng Zhang, và Minlie Huang. Click: Tạo văn bản có thể kiểm soát
với học tương phản khả năng chuỗi. Trong Anna Rogers, Jordan Boyd-Graber, và
Naoaki Okazaki, biên tập viên, Phát hiện của Hiệp hội Ngôn ngữ học Tính toán: ACL
2023, trang 1022-1040, Toronto, Canada, Tháng 7 2023a. Hiệp hội Ngôn ngữ học
Tính toán. doi: 10.18653/v1/2023.findings-acl.65. URL https://aclanthology.org/
2023.findings-acl.65.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, và
Ion Stoica. Đánh giá LLM-as-a-judge với MT-bench và chatbot arena. Trong Hội nghị
lần thứ ba mươi bảy về Hệ thống Xử lý Thông tin Thần kinh Track Tập dữ liệu và Benchmark,
2023b. URL https://openreview.net/forum?id=uccHPGDlao.

Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei,
Paul Christiano, và Geoffrey Irving. Tinh chỉnh các mô hình ngôn ngữ từ sở thích con người.
arXiv preprint arXiv:1909.08593, 2019.

--- TRANG 18 ---
A Phụ lục
A.1 Phân phối của dữ liệu IFT, EFT và AIFT

[THIS IS FIGURE: Two 3D scatter plots showing distribution of instructions and responses for IFT, EFT and AIFT data]

Hình 6: Phân phối cả hướng dẫn và phản hồi cho dữ liệu IFT, EFT và AIFT.
Chúng tôi đã vẽ phân phối hướng dẫn cho dữ liệu IFT, EFT và AIFT(M1), và phân phối phản hồi cho dữ liệu IFT, EFT và AIFT(M1) trong Hình 6. Rõ ràng là dữ liệu IFT và EFT đến từ các phân phối rất khác nhau trong khi dữ liệu IFT và AIFT(M1) đến từ các phân phối tương tự.

A.2 Lời nhắc EFT
Lời nhắc EFT mà chúng tôi sử dụng trong các thí nghiệm chính được hiển thị trong Hình 2.

Các lời nhắc EFT khác mà chúng tôi đã thử Ban đầu, chúng tôi lấy lời nhắc EFT từ Li et al. [2024] như được hiển thị trong Hình 7. Tuy nhiên, chúng tôi thấy rằng lời nhắc này không hiệu quả bằng lời nhắc đếm điểm cộng dồn của chúng tôi vì mô hình cần xử lý nhiệm vụ như một vấn đề lựa chọn đa dạng, và khó để mô hình chia nhỏ vấn đề lựa chọn đa dạng này thành các vấn đề phụ liên quan đến đánh giá các khía cạnh khác nhau của phản hồi. Khi sử dụng mô hình được huấn luyện chỉ trên 3,200 dữ liệu IFT, hiệu suất của nó trên tập kiểm tra EFT sử dụng lời nhắc đếm điểm cộng dồn của chúng tôi và lời nhắc từ Li et al. [2024] được hiển thị trong Bảng 5.

Bảng 5: Chúng tôi đã thử các lời nhắc LLM-as-Judge khác nhau sử dụng mô hình được huấn luyện chỉ với 3,200 dữ liệu IFT và thấy rằng lời nhắc đếm điểm cộng dồn của chúng tôi hoạt động tốt nhất, thể hiện cải thiện đáng kể trong hiệu suất EFT so với lời nhắc được sử dụng bởi Li et al. [2024].

Lời nhắc EFT | Lời nhắc Lựa chọn Đa dạng | Của chúng tôi
Độ chính xác theo cặp (↑) | 26.6% | 65.1%
5-tốt nhất % (↑) | 23.5% | 39.6%
Khớp Chính xác % (↑) | 1.1% | 10.1%
Tương quan Spearman (↑) | -0.18 | 0.25
Tương quan Kendall tau (↑) | -0.16 | 0.23

A.3 Các mô hình tự thưởng chỉ sử dụng dữ liệu IFT
Để chứng minh tầm quan trọng của dữ liệu EFT, chúng tôi cũng huấn luyện một chuỗi các mô hình bắt đầu với mô hình được huấn luyện chỉ trên dữ liệu IFT. Sau đây là chuỗi mô hình.

M0: LLM cơ bản được huấn luyện trước không có tinh chỉnh.

--- TRANG 19 ---
Dưới đây là một câu hỏi từ người dùng và một phản hồi ứng viên. Vui lòng chấm phản hồi trên thang điểm 5 điểm sử dụng các tiêu chí sau:

1: Có nghĩa là câu trả lời không đầy đủ, mơ hồ, lạc đề, gây tranh cãi, hoặc không chính xác những gì người dùng yêu cầu. Ví dụ, một số nội dung có vẻ thiếu, danh sách đánh số không bắt đầu từ đầu, câu mở đầu lặp lại câu hỏi của người dùng. Hoặc phản hồi từ góc độ của người khác với kinh nghiệm cá nhân của họ (ví dụ lấy từ bài đăng blog), hoặc trông giống câu trả lời từ diễn đàn. Hoặc nó chứa văn bản quảng cáo, văn bản điều hướng, hoặc thông tin không liên quan khác.

2: Có nghĩa là câu trả lời giải quyết hầu hết các yêu cầu từ người dùng. Nó không trực tiếp giải quyết câu hỏi của người dùng. Ví dụ, nó chỉ cung cấp phương pháp cấp cao thay vì giải pháp chính xác cho câu hỏi của người dùng.

3: Có nghĩa là câu trả lời hữu ích nhưng không được viết bởi Trợ lý AI. Nó giải quyết tất cả các yêu cầu cơ bản từ người dùng. Nó hoàn chỉnh và khép kín với nhược điểm là phản hồi không được viết từ góc độ của trợ lý AI, mà từ góc độ của người khác. Nội dung trông giống đoạn trích từ bài đăng blog, trang web, hoặc kết quả tìm kiếm web. Ví dụ, nó chứa kinh nghiệm hoặc ý kiến cá nhân, đề cập phần bình luận, hoặc chia sẻ trên mạng xã hội, v.v.

4: Có nghĩa là câu trả lời được viết từ góc độ của trợ lý AI với trọng tâm rõ ràng vào việc giải quyết hướng dẫn. Nó cung cấp phản hồi hoàn chỉnh, rõ ràng và toàn diện cho câu hỏi hoặc hướng dẫn của người dùng mà không thiếu hoặc thông tin không liên quan. Nó được tổ chức tốt, khép kín và được viết với giọng điệu hữu ích. Nó có chỗ cải thiện nhỏ, ví dụ ngắn gọn và tập trung hơn.

5: Có nghĩa là đó là câu trả lời hoàn hảo từ Trợ lý AI. Nó có trọng tâm rõ ràng vào việc trở thành Trợ lý AI hữu ích, trong đó phản hồi trông như được viết có chủ ý để giải quyết câu hỏi hoặc hướng dẫn của người dùng mà không có bất kỳ câu không liên quan nào. Câu trả lời cung cấp nội dung chất lượng cao, thể hiện kiến thức chuyên môn trong lĩnh vực, được viết rất tốt, logic, dễ theo dõi, hấp dẫn và sâu sắc.

Người dùng: <INSTRUCTION_HERE>
<response><RESPONSE_HERE></response>

Vui lòng trước tiên mô tả ngắn gọn lý luận của bạn (dưới 100 từ), và sau đó viết "Điểm: <rating>" ở dòng cuối. Trả lời theo phong cách của Trợ lý AI, với kiến thức từ tìm kiếm web nếu cần. Để đưa ra điểm cuối cùng dựa trên tiêu chí, hãy suy nghĩ từng bước.

Hình 7: Lời nhắc LLM-as-a-Judge được lấy từ Li et al. [2024].

M′1: Khởi tạo với M0, sau đó được tinh chỉnh chỉ trên dữ liệu khởi tạo IFT bằng SFT.
M′2: Khởi tạo với M′1, sau đó được huấn luyện với dữ liệu AIFT(M′1) bằng DPO.
M′3: Khởi tạo với M′2, sau đó được huấn luyện với dữ liệu AIFT(M′2) bằng DPO.

Vì chúng tôi không sử dụng dữ liệu EFT để huấn luyện chuỗi mô hình, chúng không phải lúc nào cũng có thể chấm điểm phản hồi theo định dạng và ngay cả khi chúng làm được, điểm được đưa ra thường hội tụ về 4. Do đó, ngay cả khi bắt đầu từ cùng số lượng lời nhắc mới được tạo ra, chúng tôi chỉ có thể thu thập một số lượng rất nhỏ mẫu huấn luyện hợp lệ cho DPO. Tổng cộng, chúng tôi đã thu thập 541 cặp để tạo thành tập dữ liệu AIFT(M′1) được sử dụng để huấn luyện M′2 thông qua DPO, và 429 cặp để tạo thành AIFT(M′2) được sử dụng để huấn luyện M′3. Tỷ lệ thắng được hiển thị trong Hình 8. Từ hình chúng tôi có thể kết luận rằng dữ liệu EFT giúp có được hiệu suất tốt hơn trong cùng số lần lặp và khoảng cách hiệu suất giữa mô hình được huấn luyện với dữ liệu EFT và mô hình được huấn luyện không có dữ liệu EFT mở rộng trong các lần lặp sau.

--- TRANG 20 ---
Tự Thưởng M′3 vs. Baseline SFT
Tự Thưởng M′2 vs. Baseline SFT
50.4 46.5
32.8 34.8
16.8 18.8
Tự Thưởng Thắng Hòa Baseline SFT Thắng

Tự Thưởng M3 vs. M′3
Tự Thưởng M2 vs. M′2
38.7 34.8
44.5 36.7
16.8 28.5
Bên Trái Thắng (trong Trái vs Phải) Hòa Bên Phải Thắng

Hình 8: Dữ liệu EFT giúp vòng lặp tự thưởng: Chúng tôi đánh giá chuỗi các mô hình được huấn luyện sử dụng vòng lặp tự thưởng bắt đầu từ mô hình được huấn luyện chỉ sử dụng dữ liệu IFT. Chúng tôi thực hiện so sánh tỷ lệ thắng đối đầu trên tập kiểm tra IFT. Mặc dù M′2 có thể cải thiện so với baseline SFT và M′3 có thể cải thiện hơn nữa so với baseline SFT, chúng tụt hậu xa so với các mô hình tương ứng (M2, M3) bắt đầu từ mô hình cơ bản được huấn luyện sử dụng cả dữ liệu IFT và EFT, xem Hình 3.

<LIỆT KÊ TẤT CẢ CÁC HƯỚNG DẪN ALPACAEVAL>
Với danh sách các hướng dẫn có thể ở trên, hãy định nghĩa tối đa 20 danh mục sẽ bao gồm các loại hướng dẫn, ví dụ như công thức nấu ăn, tác vụ lý luận, kiến thức chung, v.v. Cố gắng bao gồm càng nhiều hướng dẫn càng tốt với tối đa 20 danh mục, trong khi giữ các danh mục ở cấp độ cao, đơn giản và dễ hiểu.

Hình 9: Lời nhắc được sử dụng để có được các danh mục hướng dẫn trên tập kiểm tra AlpacaEval.

Hướng dẫn: <INSTRUCTION>
Với điều trên, hãy phân loại nó vào một trong 20 danh mục sau:
<LIỆT KÊ TẤT CẢ CÁC DANH MỤC>

Thứ hai, chấm điểm hướng dẫn về mặt độ phức tạp: bạn nghĩ nó phức tạp như thế nào để trả lời từ 1-10 (trong đó 10 là câu hỏi phức tạp mà trước tiên lý luận hoặc chia nhỏ câu hỏi thành nhiều câu hỏi phụ chẳng hạn có thể giúp cải thiện câu trả lời).

Thứ ba, chỉ ra bạn nghĩ phản hồi cho hướng dẫn nên dài như thế nào, (a) 1 câu, (b) 1-3 câu, (c) 1 đoạn văn, (d) 2 đoạn văn, hoặc (e) 3 đoạn văn trở lên.

Cung cấp phản hồi cuối cùng của bạn theo định dạng sau:
Danh mục: <một trong 20 danh mục>
Độ phức tạp: <điểm trên 10>
Độ dài: <danh mục độ dài>. Không cung cấp phản hồi thực tế.

Hình 10: Lời nhắc để phân loại hướng dẫn dựa trên chủ đề, độ phức tạp và độ dài phản hồi mong đợi của chúng.

--- TRANG 21 ---
Bảng 6: Phân tích tập hướng dẫn kiểm tra AlpacaEval theo danh mục hướng dẫn.

Danh mục | Số lượng | Phần trăm
Khoa học / Công nghệ / Kỹ thuật | 134 | 16.65%
Chuyên nghiệp / Kinh doanh / Marketing | 77 | 9.57%
Tương tác Xã hội / Mối quan hệ / Hành vi Con người | 68 | 8.45%
Khác / Khác | 61 | 7.58%
Toán học / Lý luận Logic | 52 | 6.46%
Nấu ăn / Công thức | 48 | 5.96%
Phát triển Phần mềm / Lập trình / Thuật toán | 44 | 5.47%
Du lịch / Địa lý / Khám phá | 41 | 5.09%
Văn học / Viết / Giao tiếp | 39 | 4.84%
Lịch sử / Nghiên cứu Xã hội | 38 | 4.72%
Giải trí / Phân tích Truyền thông | 34 | 4.22%
Học Ngôn ngữ / Ngôn ngữ học | 32 | 3.98%
Âm nhạc / Âm thanh / Nghệ thuật | 30 | 3.73%
Dự án DIY / Sở thích | 24 | 2.98%
Công nghệ / Thiết bị / Sản phẩm Tiêu dùng | 20 | 2.48%
Trò chơi / Phát triển Trò chơi | 18 | 2.24%
Tập thể dục / Sức khỏe / Wellness | 16 | 1.99%
Triết học / Đạo đức / Ý thức hệ | 15 | 1.86%
Thể thao / Điền kinh / Hoạt động Thể chất | 12 | 1.49%
Chiến lược / Giải quyết Vấn đề / Tư duy Phản biện | 2 | 0.24%

Bảng 7: Phân tích tập hướng dẫn kiểm tra AlpacaEval theo độ phức tạp hướng dẫn. Các hướng dẫn tăng độ phức tạp từ 1 đến 9, trong đó 10 là câu hỏi phức tạp yêu cầu trước tiên lý luận hoặc chia nhỏ vấn đề thành các vấn đề phụ trước khi có thể giải quyết.

Độ phức tạp | Số lượng | Phần trăm
3 | 238 | 29.57%
2 | 206 | 25.59%
4 | 122 | 15.16%
6 | 79 | 9.81%
5 | 68 | 8.45%
7 | 41 | 5.09%
1 | 34 | 4.22%
8 | 14 | 1.74%
9 | 3 | 0.37%

[THIS IS FIGURE: Two bar charts showing AlpacaEval win rate breakdown for instruction complexities (left) and expected response lengths (right), with M0, M1, M2, M3 models compared]

Hình 11: Phân tích tỷ lệ thắng AlpacaEval theo độ phức tạp hướng dẫn (trái) và độ dài phản hồi mong đợi (phải). Các mô hình Tự Thưởng mang lại lợi ích trên hầu hết độ phức tạp và tất cả phạm vi độ dài phản hồi.

--- TRANG 22 ---
Bảng 8: Phân tích tập hướng dẫn kiểm tra AlpacaEval theo độ dài phản hồi mong đợi.

Độ dài Mong đợi | Số lượng | Phần trăm
1-3 câu | 361 | 44.84%
1 đoạn văn | 269 | 33.42%
1 câu | 143 | 17.76%
2 đoạn văn | 31 | 3.85%
3 đoạn văn trở lên | 1 | 0.13%

A.4 Tối ưu hóa sở thích vượt trội hơn việc tăng cường chỉ với các ví dụ tích cực
Chúng tôi cũng đã thử một quy trình tự huấn luyện thay thế bằng cách thêm các ví dụ tạo tự hướng dẫn chất lượng cao vào tinh chỉnh có giám sát (không có tối ưu hóa sở thích), thay vì DPO. Trong biến thể này, chúng tôi thêm các ví dụ bổ sung của (lời nhắc hướng dẫn, phản hồi) được tuyển chọn bởi mô hình vào tập khởi tạo để tinh chỉnh có giám sát, theo các phương pháp khác [Li et al., 2024, Adolphs et al., 2023, Gulcehre et al., 2023], thay vì xây dựng dữ liệu sở thích. Trong thiết lập này, chúng tôi chỉ thêm các ví dụ mà phản hồi ứng viên được đánh giá để đưa ra điểm hoàn hảo rni = 5. Thật không may, chúng tôi không thể tìm thấy cấu hình nào mà phương pháp này giúp ích. Ví dụ, việc thêm 11,254 ví dụ như vậy có điểm 5 trên 5, và tối ưu hóa trọng số trộn trong huấn luyện, vẫn mang lại tỷ lệ đối đầu với Baseline SFT là 29% thắng so với 30% thắng, tức là không có cải thiện.

A.5 Tạo Lời nhắc Tăng cường Sử dụng Các Mô hình Mới được Huấn luyện
Trong các thí nghiệm của chúng tôi, vì hiệu quả thời gian, chúng tôi đã tạo một nhóm cố định các lời nhắc tăng cường trước bằng ChatLlama 70B. Trong hệ thống tương tác thực, lý tưởng nhất, những lời nhắc đó có thể đến từ người dùng thực để chúng tôi có thể đảm bảo các mô hình được huấn luyện để căn chỉnh với yêu cầu người dùng thực. Ở đây, chúng tôi cũng kiểm tra xem các mô hình Tự Thưởng mới được huấn luyện trong mỗi lần lặp có thể tạo ra lời nhắc mới thông qua học trong ngữ cảnh, thay vì sử dụng ChatLlama 70B. Để kiểm tra điều này, chúng tôi đã xây dựng 30 lời nhắc với các ví dụ trong ngữ cảnh sử dụng dữ liệu IFT khởi tạo gốc như đã mô tả trong Mục 2.2 và kiểm tra xem M1, M2 và M3 vẫn có khả năng học trong ngữ cảnh và có thể tạo ra hướng dẫn chất lượng cao. Theo kiểm tra thủ công, tất cả các mô hình có thể tạo ra hướng dẫn mới được đưa ra các ví dụ trong ngữ cảnh trong tất cả 30 trường hợp. Tuy nhiên, đối với M2 và M3, mô hình có khả năng trước tiên tạo ra một vài hướng dẫn, sau đó tạo ra một dấu phân cách, và sau đó bắt đầu phản hồi các hướng dẫn, vì vậy một số xử lý hậu kỳ có thể cần thiết.

A.6 Phân cụm Mẫu Kiểm tra AlpacaEval
Chúng tôi đã sử dụng mô hình GPT-4 (gpt-4-1106-preview) để phân loại các hướng dẫn trong tập kiểm tra AlpacaEval thành các cụm từ ba góc độ: (1) danh mục hướng dẫn, (2) độ phức tạp hướng dẫn, và (3) độ dài phản hồi mong đợi. Để có được các danh mục hướng dẫn cho tập kiểm tra AlpacaEval, chúng tôi đã sử dụng lời nhắc trong Hình 9 và có được tổng cộng 20 danh mục. Sau đó, để phân cụm các hướng dẫn thành các nhóm khác nhau, chúng tôi sử dụng lời nhắc trong Hình 10 cho mỗi ví dụ kiểm tra. Các thống kê tương ứng được đưa ra trong Bảng 6, Bảng 7, Bảng 8. Kết quả chi tiết về độ phức tạp hướng dẫn và độ dài phản hồi mong đợi được đưa ra trong Hình 11.

--- TRANG 23 ---
Bảng 9: Benchmark NLP. Các mô hình Tự Thưởng chủ yếu có xu hướng duy trì hiệu suất so với mô hình cơ bản Llama 2 và Baseline SFT, mặc dù được tinh chỉnh trên các lời nhắc tuân theo hướng dẫn rất khác.

Lý luận Lẽ thường | Toán | Kiến thức Thế giới
ARC easy | ARC challenge | HellaSwag | SIQA | PIQA | GSM8K (em) | MMLU (macro_avg/acc) | OBQA (acc_comp) | NQ (em)
Llama 2 | 80.20 | 57.40 | 85.30 | 50.70 | 82.80 | 56.80 | 68.90 | 60.20 | 25.30
SFT Baseline | 76.49 | 55.97 | 85.17 | 51.48 | 82.59 | 50.72 | 69.76 | 57.80 | 34.35
M1 | 78.14 | 57.51 | 84.99 | 53.02 | 82.92 | 60.27 | 69.34 | 57.60 | 35.48
M2 | 74.84 | 54.51 | 84.27 | 51.23 | 81.94 | 59.29 | 69.31 | 57.60 | 33.07
M3 | 72.35 | 53.13 | 83.29 | 49.28 | 80.79 | 57.70 | 69.37 | 58.40 | 31.86

Bảng 10: Kết quả MT-Bench Chi tiết. Chúng tôi liệt kê hiệu suất của các mô hình trên từng danh mục vấn đề. Tự thưởng đặc biệt hiệu quả trong việc cải thiện khả năng của mô hình trong viết, nhập vai, trích xuất và các tác vụ STEM.

Viết | Nhập vai | Lý luận | Toán | Lập trình | Trích xuất | STEM | Nhân văn | Tổng thể
SFT | 8.83 | 8.15 | 5.30 | 3.00 | 3.50 | 6.90 | 9.18 | 9.95 | 6.85
M1 | 9.10 | 7.65 | 4.35 | 3.05 | 4.10 | 7.20 | 8.93 | 9.85 | 6.78
M2 | 9.10 | 8.00 | 4.60 | 3.30 | 4.25 | 7.65 | 9.40 | 9.80 | 7.01
M3 | 9.58 | 8.73 | 4.80 | 3.50 | 4.20 | 7.80 | 9.45 | 9.95 | 7.25

A.7 Kết quả Benchmark NLP và Kết quả MT-Bench
Chúng tôi cung cấp hiệu suất mô hình chi tiết trên một số benchmark NLP trong Bảng 9 và trên MT-Bench trong Bảng 10. Đặc biệt, một số benchmark NLP bao gồm ARC-Challenge, HellaSwag, SIQA, PIQA và OBQA đều là các tác vụ hoàn thành văn bản. Trong những tác vụ này, với các tùy chọn lựa chọn đa dạng, chúng tôi chọn tùy chọn tương ứng với xác suất log cao nhất được ghi bởi các mô hình làm câu trả lời cuối cùng. Như vậy, mục tiêu của những tác vụ cụ thể này khá khác với những gì thuật toán của chúng tôi cố gắng tối ưu hóa, vì vậy kết quả trên những tác vụ này có thể không phản ánh khả năng thực sự của các mô hình chúng tôi.
