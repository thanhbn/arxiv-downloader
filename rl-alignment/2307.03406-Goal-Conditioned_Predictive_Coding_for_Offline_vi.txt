# Mã hóa Dự đoán Có điều kiện Mục tiêu cho Học Tăng cường Ngoại tuyến

Zilai Zeng
Đại học BrownCe Zhang
Đại học BrownShijie Wang
Đại học BrownChen Sun
Đại học Brown

## Tóm tắt

Các nghiên cứu gần đây đã chứng minh tính hiệu quả của việc công thức hóa quá trình ra quyết định như học có giám sát trên các quỹ đạo được thu thập ngoại tuyến. Các mô hình chuỗi mạnh mẽ, như GPT hoặc BERT, thường được sử dụng để mã hóa các quỹ đạo. Tuy nhiên, lợi ích của việc thực hiện mô hình hóa chuỗi trên dữ liệu quỹ đạo vẫn chưa rõ ràng. Trong nghiên cứu này, chúng tôi điều tra liệu mô hình hóa chuỗi có khả năng nén các quỹ đạo thành các biểu diễn hữu ích giúp cải thiện việc học chính sách hay không. Chúng tôi áp dụng khung hai giai đoạn mà đầu tiên tận dụng các mô hình chuỗi để mã hóa biểu diễn cấp độ quỹ đạo, sau đó học một chính sách có điều kiện mục tiêu sử dụng các biểu diễn được mã hóa làm đầu vào. Công thức này cho phép chúng tôi xem xét nhiều phương pháp RL ngoại tuyến có giám sát hiện có như các trường hợp cụ thể của khung của chúng tôi. Trong khung này, chúng tôi giới thiệu Mã hóa Dự đoán Có điều kiện Mục tiêu (GCPC), một mục tiêu mô hình hóa chuỗi tạo ra các biểu diễn quỹ đạo mạnh mẽ và dẫn đến các chính sách hiệu quả. Thông qua các đánh giá thực nghiệm toàn diện trên các môi trường AntMaze, FrankaKitchen và Locomotion, chúng tôi quan sát thấy rằng mô hình hóa chuỗi có thể có tác động đáng kể đến các nhiệm vụ ra quyết định thách thức. Hơn nữa, chúng tôi chứng minh rằng GCPC học được một biểu diễn tiềm ẩn có điều kiện mục tiêu mã hóa quỹ đạo tương lai, cho phép hiệu suất cạnh tranh trên cả ba điểm chuẩn. Mã nguồn của chúng tôi có sẵn tại https://brown-palm.github.io/GCPC/.

## 1 Giới thiệu

Học bắt chước có điều kiện mục tiêu gần đây đã nổi lên như một phương pháp đầy hứa hẹn để giải quyết các vấn đề học tăng cường ngoại tuyến. Thay vì dựa vào các phương pháp dựa trên giá trị, chúng trực tiếp học một chính sách ánh xạ các trạng thái và mục tiêu (ví dụ: lợi nhuận mong đợi, hoặc trạng thái mục tiêu) vào các hành động. Điều này được thực hiện bằng học có giám sát trên các quỹ đạo được thu thập ngoại tuyến (tức là chuỗi các bộ ba trạng thái, hành động và phần thưởng). Mô hình học tập này cho phép khung kết quả tránh bootstrapping để truyền phần thưởng, một thành viên của "Bộ ba tử thần" của RL được biết là dẫn đến tối ưu hóa không ổn định khi kết hợp với xấp xỉ hàm và học ngoại chính sách, đồng thời cho phép mô hình tận dụng lượng lớn quỹ đạo và minh chứng được thu thập. Sự xuất hiện của RL như học có giám sát trên dữ liệu quỹ đạo trùng hợp với thành công gần đây của kiến trúc Transformer và các ứng dụng của nó trong mô hình hóa chuỗi, như GPT cho ngôn ngữ tự nhiên và VPT cho video. Thực vậy, một số nghiên cứu gần đây đã chứng minh tính hiệu quả của mô hình hóa chuỗi cho RL ngoại tuyến. Hầu hết các phương pháp này áp dụng kiến trúc Transformer để học đồng thời biểu diễn quỹ đạo và chính sách. Khi các mô hình chuỗi được mở ra vào tương lai, chúng cũng có thể phục vụ như "mô hình thế giới" cho nhiều ứng dụng RL. Bằng cách tận dụng các mục tiêu mô hình hóa chuỗi tiên tiến từ các lĩnh vực ngôn ngữ và thị giác, các phương pháp như vậy đã cho thấy hiệu suất cạnh tranh trong nhiều nhiệm vụ thách thức khác nhau.

Mặc dù có sự nhiệt tình và tiến bộ, sự cần thiết của mô hình hóa chuỗi cho học tăng cường ngoại tuyến đã bị đặt câu hỏi bởi nghiên cứu gần đây: Với kiến trúc mạng nơ-ron đơn giản nhưng được điều chỉnh phù hợp (ví dụ: perceptron đa lớp), các tác nhân được huấn luyện có thể đạt được hiệu suất cạnh tranh trên một số nhiệm vụ thách thức trong khi chỉ lấy trạng thái hiện tại và mục tiêu tổng thể làm đầu vào mô hình. Trong một số nhiệm vụ, những tác nhân này thậm chí vượt trội đáng kể so với các đối tác dựa trên mô hình hóa chuỗi. Những quan sát này một cách tự nhiên thúc đẩy câu hỏi: Liệu mô hình hóa chuỗi rõ ràng của dữ liệu quỹ đạo có cần thiết cho RL ngoại tuyến không? Và nếu có, nó nên được thực hiện và sử dụng như thế nào?

Để nghiên cứu đúng đắn tác động của mô hình hóa chuỗi đối với quá trình ra quyết định, chúng tôi đề xuất tách biệt việc học biểu diễn quỹ đạo và việc học chính sách. Chúng tôi áp dụng khung hai giai đoạn, trong đó mô hình hóa chuỗi có thể được áp dụng để học biểu diễn quỹ đạo, chính sách, hoặc cả hai. Kết nối giữa hai giai đoạn có thể được thiết lập bằng cách tận dụng các biểu diễn quỹ đạo được mã hóa để học chính sách, hoặc bằng cách chuyển giao kiến trúc mô hình và trọng số (tức là chính sách có thể được khởi tạo với cùng mô hình được tiền huấn luyện sử dụng để học biểu diễn quỹ đạo). Các giai đoạn này có thể được huấn luyện riêng biệt, hoặc cùng nhau và đầu-đến-cuối. Thiết kế này không chỉ tạo điều kiện phân tích tác động của mô hình hóa chuỗi, mà còn đủ tổng quát sao cho các phương pháp trước đây có thể được xem như các trường hợp cụ thể, bao gồm những phương pháp thực hiện học chính sách và học biểu diễn quỹ đạo cùng nhau, và những phương pháp học chính sách trực tiếp mà không có mô hình hóa chuỗi.

Cụ thể, chúng tôi nhắm đến điều tra các câu hỏi sau: (1) Liệu các quỹ đạo ngoại tuyến có hữu ích do mô hình hóa chuỗi, hay đơn giản bằng cách cung cấp thêm dữ liệu cho việc học chính sách có giám sát? (2) Mục tiêu học biểu diễn quỹ đạo hiệu quả nhất để hỗ trợ việc học chính sách sẽ là gì? Liệu các mô hình chuỗi nên học mã hóa kinh nghiệm lịch sử, động lực tương lai, hay cả hai? (3) Vì cùng khung mô hình hóa chuỗi có thể được sử dụng cho cả việc học biểu diễn quỹ đạo và việc học chính sách, liệu chúng có nên chia sẻ cùng mục tiêu huấn luyện hay không?

Chúng tôi thiết kế và tiến hành các thí nghiệm trên các môi trường AntMaze, FrankaKitchen và Locomotion để trả lời những câu hỏi này. Chúng tôi quan sát thấy rằng mô hình hóa chuỗi, nếu được thiết kế phù hợp, có thể hỗ trợ hiệu quả việc ra quyết định khi biểu diễn quỹ đạo kết quả của nó được sử dụng làm đầu vào cho việc học chính sách. Chúng tôi cũng thấy rằng có sự khác biệt giữa mục tiêu tự giám sát tối ưu cho việc học biểu diễn quỹ đạo, và mục tiêu cho việc học chính sách. Những quan sát này thúc đẩy chúng tôi đề xuất một thiết kế cụ thể của khung hai giai đoạn: Nó nén thông tin quỹ đạo thành các điểm nghẽn compact thông qua tiền huấn luyện mô hình hóa chuỗi. Biểu diễn được nén sau đó được sử dụng để học chính sách với mạng chính sách dựa trên MLP đơn giản. Chúng tôi quan sát thấy rằng mã hóa dự đoán có điều kiện mục tiêu (GCPC) là mục tiêu học biểu diễn quỹ đạo hiệu quả nhất. Nó cho phép hiệu suất cạnh tranh trên tất cả các điểm chuẩn, đặc biệt cho các nhiệm vụ dài hạn. Chúng tôi quy hiệu suất thực nghiệm mạnh mẽ của GCPC cho việc thu được các biểu diễn tiềm ẩn có điều kiện mục tiêu về tương lai, cung cấp hướng dẫn quan trọng cho việc ra quyết định.

Tóm lại, đóng góp chính của chúng tôi bao gồm:
• Chúng tôi đề xuất tách biệt mô hình hóa chuỗi cho việc ra quyết định thành khung hai giai đoạn, cụ thể là học biểu diễn quỹ đạo và học chính sách. Nó cung cấp một góc nhìn thống nhất cho một số phương pháp học tăng cường thông qua học có giám sát gần đây.
• Chúng tôi tiến hành một khám phá thực nghiệm có nguyên tắc với khung của chúng tôi để hiểu liệu và khi nào mô hình hóa chuỗi của dữ liệu quỹ đạo ngoại tuyến có lợi cho việc học chính sách.
• Chúng tôi khám phá ra rằng mã hóa dự đoán có điều kiện mục tiêu (GCPC) phục vụ như mục tiêu mô hình hóa chuỗi hiệu quả nhất để hỗ trợ việc học chính sách. Khung tổng thể của chúng tôi đạt được hiệu suất cạnh tranh trên các điểm chuẩn AntMaze, FrankaKitchen và Gym Locomotion.

## 2 Nghiên cứu liên quan

**Điều gì là cần thiết cho RL Ngoại tuyến?** Học tăng cường ngoại tuyến nhằm thu được các chính sách hiệu quả bằng cách tận dụng các tập dữ liệu được thu thập trước đó. Các nghiên cứu trước thường áp dụng các phương pháp lập trình động hoặc cloning hành vi có giám sát (BC). Các phương pháp gần đây chứng minh tính hiệu quả của việc giải quyết các nhiệm vụ ra quyết định với mô hình hóa chuỗi, trong khi RvS thiết lập một baseline MLP mạnh cho cloning hành vi có điều kiện. Để hiểu rõ hơn các thành phần cần thiết cho việc học chính sách, các nhà nghiên cứu đã điều tra các giả định cần thiết để đảm bảo tính tối ưu của học có giám sát có điều kiện lợi nhuận, và kiểm tra các tác nhân RL ngoại tuyến từ ba khía cạnh cơ bản: biểu diễn, hàm giá trị, và chính sách. Một nghiên cứu đồng thời khác so sánh các điều kiện ưu tiên (ví dụ: dữ liệu, nhiệm vụ, và môi trường) để thực hiện Q-learning và học bắt chước. Trong nghiên cứu của chúng tôi, chúng tôi tìm hiểu cách mô hình hóa chuỗi có thể có lợi cho RL ngoại tuyến và nghiên cứu tác động của các mục tiêu mô hình hóa chuỗi khác nhau.

**Transformer cho việc ra quyết định tuần tự.** Việc ra quyết định tuần tự đã là chủ đề của nghiên cứu rộng rãi qua nhiều năm. Thành công to lớn của mô hình Transformer cho xử lý ngôn ngữ tự nhiên và thị giác máy tính đã truyền cảm hứng cho nhiều nghiên cứu tìm cách áp dụng các kiến trúc như vậy cho việc ra quyết định, và tương tự thúc đẩy nghiên cứu của chúng tôi. Nghiên cứu trước đã cho thấy cách mô hình hóa việc ra quyết định tuần tự như vấn đề sinh chuỗi tự hồi quy để tạo ra quỹ đạo mong muốn, trong khi những nghiên cứu khác đã khám phá các ứng dụng của Transformer cho RL dựa trên mô hình và học đa nhiệm vụ. Nghiên cứu của chúng tôi nhằm sử dụng các mô hình dựa trên Transformer để học các biểu diễn quỹ đạo tốt có thể có lợi cho việc học chính sách.

**Mã hóa Tự động Có mặt nạ.** Nghiên cứu gần đây trong NLP và CV đã chứng minh mã hóa tự động có mặt nạ (MAE) như một nhiệm vụ hiệu quả cho việc học biểu diễn tự giám sát. Được truyền cảm hứng bởi điều này, Uni[MASK], MaskDP và MTM đề xuất huấn luyện các bộ mã hóa tự động có mặt nạ thống nhất trên dữ liệu quỹ đạo bằng cách che ngẫu nhiên các trạng thái và hành động để tái tạo. Các mô hình này có thể được sử dụng trực tiếp để giải quyết một loạt các nhiệm vụ ra quyết định (ví dụ: BC có điều kiện lợi nhuận, động lực tiến, động lực ngược, v.v.) bằng cách thay đổi các mẫu che ở thời gian suy luận, mà không cần dựa vào điều chỉnh cụ thể nhiệm vụ. Ngược lại, chúng tôi tách biệt việc học biểu diễn quỹ đạo và việc học chính sách thành hai giai đoạn. Không giống như các phương pháp thực hiện RL ngoại tuyến dựa trên giá trị (ví dụ: TD3, Actor-Critic) sau tiền huấn luyện, giai đoạn học chính sách của chúng tôi áp dụng thiết lập học bắt chước trong đó chính sách có thể được học mà không cần tối đa hóa phần thưởng tích lũy.

**Học Tự giám sát cho RL.** Học tự giám sát đã nổi lên như một phương pháp mạnh mẽ để học các biểu diễn hữu ích trong nhiều lĩnh vực khác nhau, bao gồm học tăng cường (RL). Trong RL, các kỹ thuật học tự giám sát nhằm tận dụng dữ liệu không được gán nhãn hoặc quan sát một phần để tiền huấn luyện các tác nhân hoặc học các biểu diễn tạo điều kiện cho các nhiệm vụ RL downstream. Nghiên cứu trước chủ yếu tập trung vào việc sử dụng học tự giám sát để có được biểu diễn trạng thái hoặc mô hình thế giới. Trong nghiên cứu này, chúng tôi điều tra các mục tiêu học biểu diễn trong không gian quỹ đạo với công cụ mô hình hóa chuỗi, cho phép chúng tôi khám phá tác động của các modalité khác nhau (ví dụ: trạng thái, hành động, mục tiêu, v.v.) đối với việc học biểu diễn quỹ đạo.

## 3 Phương pháp

Chúng tôi xem xét lại vai trò của mô hình hóa chuỗi trong học tăng cường ngoại tuyến, từ các góc độ của việc học biểu diễn quỹ đạo và việc học chính sách. Mục 3.1 mô tả kiến thức nền tảng về việc trích xuất chính sách từ các quỹ đạo ngoại tuyến sử dụng học có giám sát. Mục 3.2 giới thiệu khung hai giai đoạn tách biệt việc học biểu diễn quỹ đạo và việc học chính sách, phục vụ như cơ sở cho điều tra của chúng tôi. Trong Mục 3.3, chúng tôi đề xuất một ví dụ cụ thể của khung hai giai đoạn, trong đó chúng tôi tận dụng một mục tiêu học tự giám sát - Mã hóa Dự đoán Có điều kiện Mục tiêu (GCPC), để thu được các biểu diễn quỹ đạo hiệu quả cho việc học chính sách.

### 3.1 Học Tăng cường Ngoại tuyến thông qua Học Có giám sát

Chúng tôi theo các nghiên cứu trước tận dụng mô hình hóa chuỗi cho học tăng cường ngoại tuyến, và áp dụng thiết lập Học tăng cường thông qua Học có giám sát (RvS). RvS nhằm giải quyết vấn đề RL ngoại tuyến như học bắt chước có điều kiện, được lọc, hoặc có trọng số. Nó giả định rằng một tập dữ liệu đã được thu thập ngoại tuyến, nhưng các chính sách được sử dụng để thu thập tập dữ liệu có thể không rõ, chẳng hạn như với các chuyên gia con người hoặc chính sách ngẫu nhiên. Tập dữ liệu chứa một tập hợp các quỹ đạo: D={τi}N_{i=1}. Mỗi quỹ đạo τi trong tập dữ liệu được biểu diễn như {(st, at)}H_{t=1}, trong đó H là độ dài của quỹ đạo, và st, at tham chiếu đến trạng thái, hành động tại bước thời gian t, tương ứng. Một quỹ đạo có thể tùy chọn chứa phần thưởng rt nhận được tại bước thời gian t.

Vì các quỹ đạo được thu thập với các chính sách không rõ, chúng có thể không tối ưu hoặc có hiệu suất chuyên gia. Nghiên cứu trước đã cho thấy rằng việc sử dụng đúng cách các quỹ đạo ngoại tuyến chứa dữ liệu không tối ưu có thể tạo ra các chính sách tốt hơn. Một cách trực quan, các quỹ đạo không tối ưu vẫn có thể chứa các quỹ đạo con thể hiện "kỹ năng" hữu ích, có thể được kết hợp để giải quyết các nhiệm vụ mới.

Cho các quỹ đạo trên, chúng tôi ký hiệu một chính sách có điều kiện mục tiêu πθ như một hàm được tham số hóa bởi θ ánh xạ một quỹ đạo quan sát được τ(t)_{obs}={s≤t, a<t} và một mục tiêu g(t) vào một hành động at. Mục tiêu g(t) được tính toán từ τt:H, có thể được biểu diễn như một cấu hình trạng thái mục tiêu được lấy mẫu từ st:H hoặc một phần thưởng tích lũy mong đợi/lợi nhuận-đến-đi được tính toán từ rt:H (xem Phụ lục A.6 để biết chi tiết). Để đơn giản ký hiệu, chúng tôi viết τ(t)_{obs} như τobs và g(t) như g. Chúng tôi xem xét một chính sách nên có thể lấy bất kỳ dạng thông tin trạng thái hoặc quỹ đạo nào làm đầu vào và dự đoán hành động tiếp theo: (1) khi chỉ trạng thái quan sát hiện tại st và mục tiêu g được sử dụng, chính sách ât=πθ(st, g) bỏ qua các quan sát lịch sử; (2) khi πθ là một mô hình chuỗi, nó có thể sử dụng toàn bộ quỹ đạo quan sát được để dự đoán hành động tiếp theo ât=πθ(τobs, g). Để tối ưu hóa chính sách, một mục tiêu thường được sử dụng là tìm các tham số phù hợp với ánh xạ từ quan sát đến hành động sử dụng ước lượng khả năng tối đa:

θ∗ = argmax_θ E_{τ∼D}[∏_{t=1}^{|τ|} πθ(at|τobs, g)]  (1)

### 3.2 Học Biểu diễn Quỹ đạo và Chính sách Tách biệt

Mô hình hóa chuỗi có thể được sử dụng cho việc ra quyết định từ hai góc độ, cụ thể là học biểu diễn quỹ đạo và học chính sách. Góc độ trước nhằm thu được các biểu diễn hữu ích từ đầu vào quỹ đạo thô, thường dưới dạng biểu diễn tiềm ẩn được nén, hoặc chính các trọng số mạng được tiền huấn luyện. Góc độ sau nhằm ánh xạ các quan sát và mục tiêu vào các hành động hoàn thành nhiệm vụ. Để biểu diễn rõ ràng hàm biểu diễn quỹ đạo, chúng tôi viết lại hàm chính sách có điều kiện mục tiêu như sau:

ât = πθ(f(τobs), g)  (2)

trong đó f(·) là một hàm tùy ý, chẳng hạn như mạng nơ-ron, tính toán biểu diễn từ τobs. f(·) cũng có thể là ánh xạ đồng nhất sao cho πθ(·) hoạt động trực tiếp trên τobs.

Được thúc đẩy bởi thành công gần đây của mô hình hóa chuỗi trong NLP và CV, cả hàm học quỹ đạo và hàm học chính sách đều có thể được triển khai với các mạng nơ-ron Transformer như fϕ(·) và πθ(·). Chúng tôi giả định rằng có lợi cho fϕ khi nén các quỹ đạo thành một biểu diễn compact sử dụng các kỹ thuật mô hình hóa chuỗi. Chúng tôi cũng giả định rằng việc tách biệt việc học biểu diễn quỹ đạo khỏi việc học chính sách là mong muốn. Việc tách biệt không chỉ cung cấp tính linh hoạt trong việc lựa chọn các mục tiêu học biểu diễn, mà còn cho phép chúng tôi nghiên cứu tác động của mô hình hóa chuỗi cho việc học biểu diễn quỹ đạo và việc học chính sách một cách độc lập. Do đó, chúng tôi áp dụng khung hai giai đoạn với TrajNet fϕ(·) và PolicyNet πθ(·). TrajNet nhằm học các biểu diễn quỹ đạo với các mục tiêu mô hình hóa chuỗi tự giám sát, chẳng hạn như mã hóa tự động có mặt nạ hoặc dự đoán token tiếp theo. PolicyNet nhằm thu được một chính sách hiệu quả với mục tiêu học có giám sát trong Phương trình 1.

Bây giờ chúng tôi mô tả quy trình tổng thể của hai mạng, mà chúng tôi sẽ cho thấy là tổng quát để biểu diễn các phương pháp được đề xuất gần đây. Trong giai đoạn đầu, chúng tôi tiếp cận việc học biểu diễn quỹ đạo như mã hóa tự động có mặt nạ. TrajNet nhận một quỹ đạo τ và một mục tiêu tùy chọn g, và được huấn luyện để tái tạo τ từ một góc nhìn có mặt nạ của cùng một quỹ đạo. Tùy chọn, TrajNet cũng tạo ra một biểu diễn quỹ đạo được nén B, có thể được sử dụng bởi PolicyNet cho việc học chính sách tiếp theo:

τ̂, B = fϕ(Masked(τ), g)  (3)

Trong giai đoạn thứ hai, TrajNet được áp dụng trên quỹ đạo quan sát không có mặt nạ - fϕ(τobs, g), để thu được Bobs. PolicyNet sau đó dự đoán hành động a cho τobs (hoặc trạng thái quan sát hiện tại st), mục tiêu g, và biểu diễn quỹ đạo Bobs:

a = πθ(τobs, Bobs, g)  (4)

Khung của chúng tôi cung cấp một góc nhìn thống nhất để so sánh các lựa chọn thiết kế khác nhau (ví dụ: thông tin đầu vào, kiến trúc, mục tiêu huấn luyện, v.v.) cho việc học biểu diễn và việc học chính sách, tương ứng. Nhiều phương pháp hiện có có thể được xem như các trường hợp đặc biệt của khung của chúng tôi. Ví dụ, các nghiên cứu trước khởi tạo TrajNet với Transformer hai chiều được tiền huấn luyện thông qua dự đoán có mặt nạ, và tái sử dụng nó như PolicyNet theo cách không cần học (zero-shot). Để triển khai DT, f(·) được đặt như hàm ánh xạ đồng nhất của quỹ đạo đầu vào và π(·) được huấn luyện để tự động sinh hành động. Các phương pháp này học biểu diễn quỹ đạo và chính sách cùng nhau, với các mục tiêu huấn luyện được truyền cảm hứng bởi MAE và GPT. Cuối cùng, khung của chúng tôi phục hồi RvS-G/R bằng cách có đầu ra của f(·) trong Eq. 2 như trạng thái quan sát cuối cùng st từ τobs.

### 3.3 Mã hóa Dự đoán Có điều kiện Mục tiêu

Cuối cùng, chúng tôi giới thiệu một thiết kế cụ thể của khung hai giai đoạn, Mã hóa Dự đoán Có điều kiện Mục tiêu (GCPC), được truyền cảm hứng bởi các giả định của chúng tôi được giới thiệu trong Mục 3.2. Để tạo điều kiện cho việc chuyển giao biểu diễn quỹ đạo giữa hai giai đoạn, chúng tôi nén quỹ đạo thành các biểu diễn tiềm ẩn sử dụng mô hình hóa chuỗi, mà chúng tôi gọi là điểm nghẽn. Với thiết kế này, chúng tôi huấn luyện điểm nghẽn để thực hiện dự đoán tương lai có điều kiện mục tiêu, sao cho các biểu diễn tiềm ẩn mã hóa các hành vi tương lai hướng về mục tiêu mong muốn, sau đó được sử dụng để hướng dẫn việc học chính sách.

Trong giai đoạn đầu (như được hiển thị trong Hình 1), chúng tôi sử dụng Transformer hai chiều như TrajNet. Các đầu vào bao gồm T token trạng thái, một token mục tiêu, và một vài token slot có thể học. Các token hành động bị bỏ qua. Chúng tôi đầu tiên nhúng token mục tiêu và tất cả các token trạng thái với các bộ mã hóa tuyến tính riêng biệt, sau đó áp dụng mã hóa vị trí sin trước khi tất cả đầu vào được gửi vào Transformer Encoder.

Trong khi các quỹ đạo đầy đủ có thể được sử dụng cho việc học biểu diễn quỹ đạo với TrajNet, chỉ các trạng thái quan sát mới có sẵn trong quá trình học chính sách. Để nhất quán ký hiệu, chúng tôi ký hiệu k trạng thái quan sát như "lịch sử", và p trạng thái theo sau các trạng thái quan sát như "tương lai". Tổng số trạng thái đầu vào cho giai đoạn 1 là T=k+p, và cho giai đoạn 2 là k. Cả k và p đều là siêu tham số đại diện cho độ dài cửa sổ lịch sử và độ dài cửa sổ tương lai, tương ứng.

Để thực hiện mã hóa dự đoán có điều kiện mục tiêu, toàn bộ quỹ đạo tương lai bị che. Các token trong lịch sử quan sát được che ngẫu nhiên (Hình 1 "MAE-RC"). Điểm nghẽn được lấy như các token slot được mã hóa, nén quỹ đạo đầu vào có mặt nạ thành một biểu diễn compact. Bộ giải mã lấy điểm nghẽn và T token có mặt nạ như đầu vào, và nhằm tái tạo cả trạng thái lịch sử và tương lai. Trực quan của chúng tôi là điều này sẽ khuyến khích điểm nghẽn học các biểu diễn mã hóa quỹ đạo tương lai và cung cấp tín hiệu hữu ích cho việc ra quyết định. Mục tiêu huấn luyện của TrajNet là tối thiểu hóa mất mát MSE giữa quỹ đạo được tái tạo và quỹ đạo thực tế.

Hình 2 minh họa giao diện giữa TrajNet và PolicyNet. PolicyNet được triển khai như một mạng MLP đơn giản. TrajNet được huấn luyện có trọng số được đóng băng, và chỉ TrajNet encoder được sử dụng để tính toán biểu diễn quỹ đạo Bobs từ k trạng thái lịch sử không có mặt nạ. PolicyNet lấy trạng thái hiện tại st, mục tiêu g, và điểm nghẽn Bobs làm đầu vào, và xuất ra một hành động. Mục tiêu huấn luyện của PolicyNet là tối thiểu hóa mất mát MSE giữa các hành động được dự đoán và thực tế.

**Thảo luận.** Việc tách biệt việc học biểu diễn quỹ đạo và việc học chính sách cho phép chúng tôi khám phá các đầu vào và mục tiêu mô hình hóa chuỗi khác nhau của mô hình. Ví dụ, nghiên cứu gần đây quan sát thấy rằng các chuỗi hành động có thể có hại đến chính sách được học trong một số nhiệm vụ. Trong GCPC, chúng tôi sử dụng các quỹ đạo chỉ có trạng thái làm đầu vào và bỏ qua các hành động. Các mục tiêu khác nhau (ví dụ: các mẫu che trong mục tiêu mã hóa tự động có mặt nạ) cũng có tác động đến những gì được mã hóa trong điểm nghẽn. Ở đây chúng tôi giới thiệu năm mục tiêu mô hình hóa chuỗi cho việc học biểu diễn quỹ đạo (như được hiển thị trong Bảng 1) và nghiên cứu tác động của chúng đối với việc học chính sách. Khi sử dụng "AE-H" hoặc "MAE-H", điểm nghẽn chỉ được thúc đẩy để tóm tắt các trạng thái lịch sử. Khi sử dụng "MAE-F" hoặc "MAE-RC", điểm nghẽn được yêu cầu thực hiện mã hóa dự đoán, và do đó mã hóa các chuỗi trạng thái tương lai để đạt được mục tiêu được cung cấp. Mặc định, chúng tôi áp dụng mục tiêu "MAE-RC" trong GCPC.

**Bảng 1: Mục tiêu cho Việc Học Biểu diễn Quỹ đạo**

| Đầu vào | Tái tạo |
|---------|---------|
| Lịch sử | Tương lai | Lịch sử | Tương lai |
| AE-H | Không che | – | ✓ | |
| MAE-H | Che ngẫu nhiên | – | ✓ | |
| MAE-F | Không che | Che hoàn toàn | ✓ | ✓ |
| MAE-RC | Che ngẫu nhiên | Che hoàn toàn | ✓ | ✓ |
| MAE-ALL | Che ngẫu nhiên | Che ngẫu nhiên | ✓ | ✓ |

## 4 Thí nghiệm

Trong phần này, chúng tôi nhằm trả lời các câu hỏi sau với các thí nghiệm thực nghiệm: (1) Liệu mô hình hóa chuỗi có lợi cho học tăng cường thông qua học có giám sát trên dữ liệu quỹ đạo, và như thế nào? (2) Liệu có lợi khi tách biệt việc học biểu diễn quỹ đạo và việc học chính sách với điểm nghẽn? (3) Các mục tiêu học biểu diễn quỹ đạo hiệu quả nhất là gì?

### 4.1 Thiết lập Thí nghiệm

Để trả lời các câu hỏi trên, chúng tôi tiến hành các thí nghiệm rộng rãi trên ba lĩnh vực từ bộ điểm chuẩn ngoại tuyến D4RL: AntMaze, FrankaKitchen và Gym Locomotion. AntMaze là một lớp các nhiệm vụ điều hướng dài hạn, có đặc điểm quan sát một phần, phần thưởng thưa thớt và các tập dữ liệu chủ yếu gồm các quỹ đạo không tối ưu. Trong lĩnh vực này, một robot Ant 8-DoF cần "ghép nối" các phần của quỹ đạo con và điều hướng đến một vị trí mục tiêu cụ thể trong các mê cung quan sát một phần. Ngoài ba mê cung từ D4RL gốc, chúng tôi cũng bao gồm một mê cung lớn hơn (AntMaze-Ultra) được đề xuất bởi [28]. Cả thiết lập lớn và siêu trong AntMaze đều đặt ra những thách thức đáng kể do bố cục mê cung phức tạp và chân trời điều hướng dài. FrankaKitchen là một nhiệm vụ thao tác dài hạn, trong đó một cánh tay robot Franka 9-DoF được yêu cầu thực hiện 4 nhiệm vụ phụ trong môi trường bếp mô phỏng (ví dụ: mở lò vi sóng, bật đèn). Trong Gym Locomotion, chúng tôi đánh giá phương pháp của mình trên ba nhiệm vụ điều khiển liên tục: Walker 2D, Hopper và Halfcheetah. Theo [15], chúng tôi gọi "mục tiêu" của AntMaze và Kitchen là cấu hình trạng thái mục tiêu, và của Gym là lợi nhuận-đến-đi trung bình.

**Chi tiết thí nghiệm.** Cho tất cả các điểm chuẩn, chúng tôi sử dụng transformer encoder hai lớp và transformer decoder một lớp như TrajNet, và MLP hai lớp như PolicyNet (xem siêu tham số chi tiết trong A.3). Trong quá trình học chính sách, chúng tôi sử dụng TrajNet được tiền huấn luyện đạt được mất mát tái tạo xác thực thấp nhất để tạo ra điểm nghẽn. Cho mỗi lần chạy đánh giá, chúng tôi theo [29] để lấy tỷ lệ thành công trên 100 quỹ đạo đánh giá cho các nhiệm vụ AntMaze. Cho Kitchen và Gym Locomotion, chúng tôi tính trung bình lợi nhuận trên 50 và 10 quỹ đạo đánh giá, tương ứng. Để xác định hiệu suất với một hạt giống ngẫu nhiên cho trước, chúng tôi lấy kết quả đánh giá tốt nhất trong số năm điểm kiểm tra cuối cùng (xem thảo luận trong A.2). Để tổng hợp hiệu suất, chúng tôi báo cáo hiệu suất trung bình và độ lệch chuẩn được tính trung bình trên năm hạt giống cho mỗi thí nghiệm.

### 4.2 Tác động của Mục tiêu Tiền huấn luyện Biểu diễn Quỹ đạo

Trong khung hai giai đoạn, TrajNet được tiền huấn luyện tạo ra các biểu diễn quỹ đạo dưới dạng điểm nghẽn, được lấy làm đầu vào cho PolicyNet. Để nghiên cứu tác động của các mục tiêu học biểu diễn quỹ đạo đối với hiệu suất chính sách kết quả, chúng tôi triển khai năm mục tiêu mô hình hóa chuỗi khác nhau (như trong Bảng 1) bằng cách thay đổi các mẫu che trong tiền huấn luyện giai đoạn đầu.

Bảng 2 so sánh hiệu suất của các chính sách với các thiết lập và mục tiêu tiền huấn luyện khác nhau. Chúng tôi lưu ý rằng MAE-F là mẫu che hiệu quả duy nhất để thực hiện suy luận không cần học. Sau khi tách biệt việc học biểu diễn và việc học chính sách, chính sách MLP luôn vượt trội hơn chính sách transformer không cần học. Điều này gợi ý các mục tiêu tốt cho hai giai đoạn có thể khác nhau - bằng cách tách biệt chúng ta có thể có được điều tốt nhất của cả hai thế giới. Ngoài ra, chúng tôi quan sát thấy rằng việc loại bỏ các chuỗi hành động khỏi tiền huấn luyện biểu diễn quỹ đạo mang lại lợi ích hiệu suất trong nhiệm vụ này, trong khi chính sách Transformer giai đoạn đơn trước đây (ví dụ: [10]) thường yêu cầu đầu vào hành động để hoạt động, điều này càng chứng minh tính linh hoạt của khung tách biệt của chúng tôi. Với các mục tiêu được thiết kế phù hợp, mô hình hóa chuỗi có thể tạo ra các biểu diễn quỹ đạo mạnh mẽ tạo điều kiện cho việc thu được các chính sách hiệu quả.

Trong cả Bảng 2 và Hình 4, chúng tôi quan sát thấy rằng cả MAE-F và MAE-RC đều vượt trội hơn các mục tiêu khác trong cả AntMaze-Large và Kitchen-Partial, xác nhận tầm quan trọng của mục tiêu mã hóa dự đoán cho việc học biểu diễn quỹ đạo.

**Bảng 2: So sánh các mục tiêu tiền huấn luyện biểu diễn quỹ đạo.** Chúng tôi đánh giá năm mục tiêu khác nhau dưới ba thiết lập trên môi trường AntMaze lớn: (1) Không cần học: Khi các hành động được xem xét như một phần của các token có mặt nạ, TrajNet được tiền huấn luyện có thể được sử dụng trực tiếp như chính sách; (2) Hai giai đoạn (không có hành động): Khung hai giai đoạn sử dụng MLP như PolicyNet, với các quỹ đạo chỉ có trạng thái làm đầu vào cho TrajNet. (3) Hai giai đoạn (có hành động): Khung hai giai đoạn sử dụng MLP như PolicyNet, với các quỹ đạo trạng thái-hành động làm đầu vào cho TrajNet.

| Large-Play | AE-H | MAE-H | MAE-F | MAE-RC | MAE-ALL |
|------------|------|-------|-------|--------|---------|
| Không cần học | - | 0 | 21.2 ±17.1 | 0 | 0 |
| Hai giai đoạn (có hành động) | 12.6 ±4.7 | 6.4±3.8 | 57.2±5.5 | 62.0±10.7 | 11.6±6.3 |
| Hai giai đoạn (không có hành động) | 30.2 ±6.6 | 36.6±12.6 | 76.2±4.0 | 78.2±3.2 | 32.0±13.2 |

### 4.3 Vai trò của Điều kiện Mục tiêu trong Tiền huấn luyện Biểu diễn Quỹ đạo

Chúng tôi cũng điều tra liệu điều kiện mục tiêu (tức là đầu vào mục tiêu) trong TrajNet có cần thiết hoặc có lợi cho việc học biểu diễn quỹ đạo hay không. Trong Bảng 3, chúng tôi quan sát thấy rằng các mục tiêu không thực hiện dự đoán tương lai không nhạy cảm với điều kiện mục tiêu. Ví dụ, kết quả của các mục tiêu tiền huấn luyện AE-H và MAE-H phù hợp với trực quan rằng việc chỉ tóm tắt lịch sử không nên yêu cầu thông tin mục tiêu. Tuy nhiên, điều kiện mục tiêu là quan trọng cho các mục tiêu mã hóa dự đoán (ví dụ: MAE-F và MAE-RC). Việc loại bỏ mục tiêu khỏi TrajNet có thể dẫn đến dự đoán vô mục đích và biểu diễn tương lai gây hiểu lầm, do đó làm hại chính sách. Hình 5 cho thấy các đường cong mất mát xác thực trong quá trình tiền huấn luyện với mục tiêu MAE-F. Chúng tôi quan sát thấy rằng điều kiện mục tiêu làm giảm lỗi dự đoán và giúp điểm nghẽn mã hóa đúng tương lai dài hạn mong đợi, điều này sẽ có lợi cho việc học chính sách tiếp theo. Một giả thuyết là việc chỉ định mục tiêu cho phép điểm nghẽn thực hiện lập kế hoạch ngầm có điều kiện mục tiêu. Tương lai được lập kế hoạch có thể cung cấp các điểm đường đúng để tác nhân đạt được mục tiêu xa. Hình 6 minh họa một kết quả định tính của tương lai tiềm ẩn trong AntMaze, mô tả các trạng thái tương lai được giải mã từ điểm nghẽn sử dụng bộ giải mã Transformer được tiền huấn luyện. Nó chứng minh rằng tương lai tiềm ẩn với điều kiện mục tiêu giúp chỉ ra hướng đúng hướng về vị trí mục tiêu.

**Bảng 3: Nghiên cứu loại bỏ điều kiện mục tiêu trên AntMaze-Large.** Việc loại bỏ điều kiện mục tiêu khỏi TrajNet sẽ ảnh hưởng nghiêm trọng đến các mục tiêu mã hóa dự đoán và làm hại hiệu suất chính sách kết quả, gợi ý rằng biểu diễn tương lai được mã hóa đúng cách cung cấp hướng dẫn quan trọng để thực hiện các nhiệm vụ dài hạn. "GC" đề cập đến điều kiện mục tiêu trong TrajNet.

| Large-Play | AE-H | MAE-H | MAE-F | MAE-RC | MAE-ALL |
|------------|------|-------|-------|--------|---------|
| không có GC | 32.8 ±4.3 | 33.2±11.9 | 10.0±2.2 | 16.0±5.3 | 36.2±10.4 |
| có GC | 30.2 ±6.6 | 36.6±12.6 | 76.2±4.0 | 78.2±3.2 | 32.0±13.2 |

### 4.4 Tương lai Tiềm ẩn so với Tương lai Rõ ràng như Các Biến Điều kiện Chính sách

Nghiên cứu trước đã chứng minh rằng lập kế hoạch vào tương lai có hữu ích cho việc giải quyết các nhiệm vụ dài hạn. Các nghiên cứu này thực hiện lập kế hoạch trên tương lai rõ ràng bằng cách lấy mẫu các trạng thái tương lai mong muốn (hoặc chuyển đổi), trong khi GCPC tận dụng các biểu diễn tiềm ẩn có điều kiện mục tiêu mã hóa chuỗi trạng thái tương lai. Trong thí nghiệm này, chúng tôi kiểm tra cách mã hóa ngầm thông tin tương lai ảnh hưởng đến hiệu suất chính sách khi nó phục vụ như một biến điều kiện của PolicyNet. Cụ thể, chúng tôi thu được điểm nghẽn từ một TrajNet, được tiền huấn luyện với cửa sổ tương lai có độ dài p. Điểm nghẽn mã hóa các biểu diễn tiềm ẩn của p trạng thái tương lai. Tương ứng, chúng tôi thu được cùng số lượng trạng thái tương lai rõ ràng sử dụng bộ giải mã transformer được tiền huấn luyện (xem Phụ lục A.4). Hoặc điểm nghẽn hoặc các trạng thái tương lai rõ ràng được lấy làm đầu vào phụ trợ cho PolicyNet. Trong Bảng 4, chúng tôi đánh giá chính sách với p=70 trong AntMaze lớn và p=30 trong Kitchen. Kết quả minh họa rằng điểm nghẽn là một vận chuyển thông tin tương lai mạnh mẽ hiệu quả cải thiện hiệu suất chính sách. Khác với các phương pháp trước ước lượng các trạng thái tương lai rõ ràng với các mô hình động lực từng bước, việc sử dụng các biểu diễn tương lai tiềm ẩn có thể giảm thiểu các lỗi rollout tích lũy. So với các phương pháp lập kế hoạch dựa trên diffusion, yêu cầu tinh chỉnh lặp để thu được một chuỗi tương lai, tương lai tiềm ẩn tránh bước khử nhiễu tốn thời gian và giảm độ trễ quyết định.

**Bảng 4: So sánh giữa tương lai tiềm ẩn và tương lai rõ ràng.** So với các trạng thái tương lai rõ ràng, tương lai tiềm ẩn được mã hóa bởi điểm nghẽn hiệu quả hơn cho việc học chính sách. p là kích thước cửa sổ tương lai.

| | Large-Play | Large-Diverse | Kitchen-Mixed | Kitchen-Partial |
|---------|------------|---------------|---------------|-----------------|
| | p=70 | | p=30 | |
| Tương lai Rõ ràng | 67.9±9.3 | 70.0±4.6 | 72.4±4.5 | 73.9±9.3 |
| Tương lai Tiềm ẩn | 78.2±3.2 | 80.6±3.9 | 75.6±0.8 | 90.2±6.6 |

### 4.5 Hiệu quả của GCPC

Cuối cùng, chúng tôi cho thấy hiệu quả của GCPC bằng cách đánh giá nó trên ba lĩnh vực khác nhau: AntMaze, Kitchen và Gym Locomotion.

**Các baseline và phương pháp trước.** Chúng tôi so sánh phương pháp của mình với cả các phương pháp học có giám sát và các phương pháp RL dựa trên giá trị. Cho loại trước, chúng tôi xem xét: (1) Behavioral Cloning (BC), (2) RvS-R/G, một phương pháp học bắt chước có điều kiện được điều kiện trên một trạng thái mục tiêu hoặc một lợi nhuận mong đợi. (3) Decision Transformer (DT), một phương pháp không có mô hình có điều kiện lợi nhuận học một chính sách dựa trên transformer, (3) Trajectory Transformer (TT), một phương pháp dựa trên mô hình thực hiện tìm kiếm chùm trên một mô hình quỹ đạo dựa trên transformer để lập kế hoạch, và (4) Decision Diffuser (DD), một phương pháp lập kế hoạch dựa trên diffusion tổng hợp các trạng thái tương lai với một mô hình diffusion và hành động bằng cách tính toán động lực ngược. Cho loại sau, chúng tôi chọn các phương pháp dựa trên lập trình động, bao gồm (5) CQL và (6) IQL. Ngoài ra, chúng tôi bao gồm ba baseline có điều kiện mục tiêu (trạng thái) cho AntMaze và Kitchen: (7) Goal-conditioned IQL (GCIQL), (8) WGCSL và (9) DWSL. AntMaze-Ultra là một môi trường tùy chỉnh được đề xuất bởi [28], do đó chúng tôi bao gồm hiệu suất của TAP trên phiên bản v0 của AntMaze để hoàn thiện. Khi khả thi, chúng tôi chạy lại các baseline với giao thức đánh giá của chúng tôi để so sánh công bằng.

Kết quả thực nghiệm của chúng tôi trên AntMaze và Kitchen được trình bày trong Bảng 5 và Bảng 6. Chúng tôi thấy phương pháp của chúng tôi vượt trội hơn tất cả các phương pháp trước đây về hiệu suất trung bình. Đặc biệt, trên các môi trường AntMaze lớn và siêu thách thức nhất, phương pháp của chúng tôi đạt được cải thiện đáng kể so với baseline RvS-G, chứng minh hiệu quả của việc học các biểu diễn tương lai tốt sử dụng mô hình hóa chuỗi trong các nhiệm vụ dài hạn. Bảng 7 cho thấy kết quả trên các nhiệm vụ Gym Locomotion. Phương pháp của chúng tôi đạt được hiệu suất cạnh tranh như các phương pháp trước. Chúng tôi cũng nhận thấy rằng so với Decision Transformer, RvS-R đã có thể đạt được hiệu suất trung bình mạnh. Điều này gợi ý rằng đối với một số nhiệm vụ mô hình hóa chuỗi có thể không phải là thành phần cần thiết để cải thiện chính sách. Với một phần lớn các quỹ đạo gần tối ưu trong tập dữ liệu, một chính sách MLP đơn giản có thể cung cấp đủ khả năng để xử lý hầu hết các nhiệm vụ locomotion.

**Bảng 5: Điểm số bình thường hóa trung bình của GCPC so với các baseline khác trên AntMaze.** Hiệu suất của TAP được lấy từ bài báo gốc [28]. Cho các baseline khác, chúng tôi thu được hiệu suất của chúng bằng cách chạy lại các triển khai do tác giả cung cấp hoặc được chúng tôi sao chép với giao thức đánh giá của chúng tôi. Theo [29], chúng tôi in đậm tất cả điểm số trong vòng 5 phần trăm của mức tối đa mỗi nhiệm vụ (≥0.95 · max).

| Dataset | BC | CQL | IQL | DT | TAP | WGCSL | GCIQL | DWSL | RvS-G | GCPC |
|---------|----|----|-----|----|----|-------|-------|------|-------|------|
| Umaze | 63.4 ±9.4 | 88.2±2.3 | 92.8±3.4 | 55.6±6.3 | - | 90.8±2.8 | 91.6±4.0 | 71.2±4.2 | 70.4±4.0 | 71.2±1.3 |
| Umaze-Diverse | 63.4 ±4.4 | 47.4±2.0 | 71.2±7.0 | 53.4±8.6 | - | 55.6 ±15.7 | 88.8±2.2 | 74.6±2.8 | 66.2±5.6 | 71.2±6.6 |
| Medium-Play | 0.6 ±0.5 | 72.8±5.7 | 75.8±1.3 | 0 | 78.0 | 63.2 ±13.7 | 82.6±5.4 | 77.6±3.0 | 71.8±4.7 | 70.8±3.4 |
| Medium-Diverse | 0.4 ±0.5 | 70.8±10.3 | 76.6±4.2 | 0 | 85.0 | 46.0±12.6 | 76.2±6.3 | 74.8±9.3 | 72.0±3.7 | 72.2±3.4 |
| Large-Play | 0 | 36.4 ±10.3 | 50.0±9.7 | 0 | 74.0 | 0.6 ±1.3 | 40.0±16.2 | 15.2±7.7 | 35.6±7.6 | 78.2±3.2 |
| Large-Diverse | 0 | 36.0 ±8.3 | 52.6±5.9 | 0 | 82.0 | 2.4±4.3 | 29.8±6.8 | 19.0±2.8 | 25.2±4.8 | 80.6±3.9 |
| Ultra-Play | 0 | 18.0 ±13.3 | 21.2±7.5 | 0 | 22.0 | 0.2 ±0.4 | 20.6±7.6 | 25.2±3.0 | 25.6±6.7 | 56.6±9.5 |
| Ultra-Diverse | 0 | 9.6 ±14.6 | 17.8±4.0 | 0 | 26.0 | 0 | 28.4 ±11.8 | 25.0±8.6 | 26.4±7.7 | 54.6±10.3 |
| Trung bình | 16.0 | 47.4 | 57.3 | 13.6 | - | 32.4 | 57.3 | 47.8 | 49.2 | 69.4 |

**Bảng 6: Điểm số bình thường hóa trung bình của GCPC so với các baseline khác trên Kitchen.** Kết quả của CQL và DD được lấy từ [2]. Cho các baseline khác, chúng tôi thu được hiệu suất của chúng bằng cách chạy lại các triển khai do tác giả cung cấp hoặc được chúng tôi sao chép với giao thức đánh giá của chúng tôi. Theo [29], chúng tôi in đậm tất cả điểm số trong vòng 5 phần trăm của mức tối đa mỗi nhiệm vụ (≥0.95 · max).

| Dataset | BC | CQL | IQL | DT | DD | WGCSL | GCIQL | DWSL | RvS-G | GCPC |
|---------|----|----|-----|----|----|-------|-------|------|-------|------|
| Mixed | 48.9 ±0.7 | 52.4 | 53.2 ±1.6 | 50.7±7.1 | 65.0 | 77.8±3.6 | 74.6±1.9 | 74.6±0.6 | 69.4±4.2 | 75.6±0.8 |
| Partial | 41.3 ±3.7 | 50.1 | 59.7 ±8.3 | 48.6±9.5 | 57.0 | 75.2 ±6.4 | 74.7±4.1 | 74.0±5.8 | 71.7±7.9 | 90.2±6.6 |
| Trung bình | 45.1 | 51.3 | 56.5 | 49.7 | 61.0 | 76.5 | 74.7 | 74.3 | 70.6 | 82.9 |

**Bảng 7: Điểm số bình thường hóa trung bình của phương pháp chúng tôi so với các baseline khác trên Gym Locomotion.** Kết quả của TT và DD được lấy từ [2]. Cho các baseline khác, chúng tôi thu được hiệu suất của chúng bằng cách chạy lại các triển khai do tác giả cung cấp hoặc được chúng tôi sao chép với giao thức đánh giá của chúng tôi. Theo [29], chúng tôi in đậm tất cả điểm số trong vòng 5 phần trăm của mức tối đa mỗi nhiệm vụ (≥0.95 · max).

| Dataset | Environment | BC | CQL | IQL | DT | TT | DD | RvS-R | GCPC |
|---------|-------------|----|----|-----|----|----|----| ------|------|
| Medium-Expert | HalfCheetah | 61.9 ±5.8 | 87.7±7.0 | 92.4±0.4 | 88.8±2.6 | 95 | 90.6 | 93.4 ±0.3 | 94.0±0.9 |
| Medium-Expert | Hopper | 55.1 ±2.8 | 110.5 ±2.9 | 104.2 ±8.7 | 108.4 ±2.0 | 110 | 111.8 | 111.3 ±0.2 | 111.7 ±0.4 |
| Medium-Expert | Walker2d | 100.4 ±13.4 | 110.4 ±0.6 | 110.2 ±0.7 | 108.6 ±0.3 | 101.9 | 108.8 | 109.6 ±0.4 | 109.0 ±0.2 |
| Medium | HalfCheetah | 43.0 ±0.4 | 47.1±0.3 | 47.7±0.2 | 42.9±0.2 | 46.9 | 49.1 | 44.2±0.2 | 44.5±0.5 |
| Medium | Hopper | 55.8 ±2.8 | 70.1±1.6 | 69.2±3.2 | 67.8±4.2 | 61.1 | 79.3 | 65.1±5.2 | 68.0±4.4 |
| Medium | Walker2d | 74.1 ±3.2 | 83.5±0.5 | 84.5±1.5 | 76.5±1.6 | 79 | 82.5 | 78.4±2.6 | 78.0±2.4 |
| Medium-Replay | HalfCheetah | 37.2 ±1.3 | 45.4±0.3 | 44.9±0.3 | 37.8±0.9 | 41.9 | 39.3 | 40.2 ±0.2 | 40.7±1.5 |
| Medium-Replay | Hopper | 33.7 ±8.5 | 96.2±1.9 | 93.9±9.1 | 78.0±11.6 | 91.5 | 100 | 88.5±12.9 | 94.2±3.5 |
| Medium-Replay | Walker2d | 19.2 ±7.3 | 79.8±1.6 | 78.6±5.7 | 72.5±3.3 | 82.6 | 75 | 71.0 ±5.1 | 77.6±9.8 |
| Trung bình | | 53.4 | 81.2 | 80.6 | 75.7 | 78.9 | 81.8 | 78.0 | 79.7 |

## 5 Kết luận

Trong nghiên cứu này, chúng tôi nhằm điều tra vai trò của mô hình hóa chuỗi trong việc học các biểu diễn quỹ đạo và tiện ích của nó trong việc thu được các chính sách hiệu quả. Để thực hiện điều này, chúng tôi sử dụng khung hai giai đoạn tách biệt việc học biểu diễn quỹ đạo và việc học chính sách. Khung này thống nhất nhiều phương pháp RvS hiện có, cho phép chúng tôi nghiên cứu tác động của các mục tiêu học biểu diễn quỹ đạo khác nhau đối với việc ra quyết định tuần tự. Trong khung này, chúng tôi giới thiệu một thiết kế cụ thể - Mã hóa Dự đoán Có điều kiện Mục tiêu (GCPC), kết hợp một điểm nghẽn compact để chuyển giao biểu diễn giữa hai giai đoạn và học các biểu diễn tiềm ẩn có điều kiện mục tiêu mô hình hóa quỹ đạo tương lai. Thông qua các thí nghiệm rộng rãi, chúng tôi quan sát thấy rằng điểm nghẽn được tạo bởi GCPC mã hóa đúng cách thông tin tương lai có điều kiện mục tiêu và mang lại cải thiện đáng kể đối với một số nhiệm vụ dài hạn. Bằng cách đánh giá thực nghiệm phương pháp của chúng tôi trên ba điểm chuẩn, chúng tôi chứng minh GCPC đạt được hiệu suất cạnh tranh trên nhiều nhiệm vụ khác nhau.

**Hạn chế và Nghiên cứu tương lai.** GCPC mô hình hóa tương lai bằng cách thực hiện ước lượng khả năng tối đa trên các quỹ đạo được thu thập ngoại tuyến, có thể dự đoán các hành vi tương lai quá lạc quan và dẫn đến các hành động không tối ưu trong các môi trường ngẫu nhiên. Nghiên cứu tương lai bao gồm khám phá các chính sách mạnh mẽ đối với tính ngẫu nhiên của môi trường bằng cách xem xét nhiều tương lai có thể được tạo bởi GCPC. Một hạn chế khác của nghiên cứu chúng tôi là GCPC có thể không đủ để duy trì độ chính xác cao cho dự đoán tương lai dài hạn khi các trạng thái chiều cao được tham gia, có thể được giải quyết bằng cách tận dụng các mô hình nền tảng để thu được biểu diễn cho các đầu vào chiều cao.

## Lời cảm ơn và Tiết lộ Nguồn tài trợ

Chúng tôi đánh giá cao tất cả các nhà đánh giá ẩn danh vì phản hồi xây dựng của họ. Chúng tôi muốn cảm ơn Calvin Luo và Haotian Fu vì các cuộc thảo luận và hiểu biết của họ, và Tian Yun vì sự giúp đỡ trong dự án này. Nghiên cứu này một phần được hỗ trợ bởi Adobe, Honda Research Institute, Meta AI, Samsung Advanced Institute of Technology, và Giải thưởng Nghiên cứu Khoa của Richard B. Salomon cho C.S.
