# 2401.12086.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2401.12086.pdf
# Kích thước tệp: 716905 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
West-of-N: Sở thích Tổng hợp
cho Mô hình Thưởng Tự cải thiện
Aliz´ee Pace1,2,3*Jonathan Mallinson4Eric Malmi4
Sebastian Krause4Aliaksei Severyn4
1ETH AI Center2Department of Computer Science, ETH Z ¨urich
3Max Planck Institute for Intelligent Systems, T ¨ubingen
4Google Research
Tóm tắt
Sự thành công của học tăng cường từ phản hồi của con người (RLHF) trong việc căn chỉnh mô hình ngôn ngữ phụ thuộc mạnh vào chất lượng của mô hình thưởng cơ bản. Trong bài báo này, chúng tôi trình bày một phương pháp mới để cải thiện chất lượng mô hình thưởng bằng cách tạo ra dữ liệu sở thích tổng hợp, từ đó tăng cường tập dữ liệu huấn luyện với các cặp sở thích on-policy, chất lượng cao. Được thúc đẩy bởi các kết quả đầy hứa hẹn của các chiến lược lấy mẫu Best-of-N trong huấn luyện mô hình ngôn ngữ, chúng tôi mở rộng ứng dụng của chúng cho huấn luyện mô hình thưởng. Điều này dẫn đến một chiến lược tự huấn luyện để tạo ra các cặp sở thích bằng cách chọn các ứng viên tốt nhất và tệ nhất trong một nhóm các phản hồi cho một truy vấn nhất định. Theo thực nghiệm, chúng tôi thấy rằng phương pháp này cải thiện hiệu suất của bất kỳ mô hình thưởng nào, với hiệu quả có thể so sánh với việc bổ sung một lượng dữ liệu sở thích của con người tương tự. Công trình này mở ra các hướng nghiên cứu mới để cải thiện RLHF cho việc căn chỉnh mô hình ngôn ngữ, bằng cách cung cấp việc tạo sở thích tổng hợp như một giải pháp cho các thách thức trong mô hình hóa thưởng.

1 Giới thiệu
Sự gia tăng phổ biến gần đây của các mô hình ngôn ngữ lớn (LLM) được cho phép bởi việc căn chỉnh hành vi của chúng với các giá trị của con người (Ouyang et al., 2022). Chiến lược phổ biến để đạt được điều này là thông qua Học tăng cường từ Phản hồi của Con người (RLHF), điều này hướng các phản hồi của mô hình về phía các đầu ra được ưa thích (Ziegler et al., 2019; Stiennon et al., 2020), bằng cách định nghĩa hiệu quả một mục tiêu nắm bắt bản chất chủ quan, phức tạp và phụ thuộc ngữ cảnh của chất lượng văn bản. Do đó, một khía cạnh quan trọng của mô hình này là mô hình hóa chính xác sở thích của con người, điều này liên quan đến quá trình thu thập dữ liệu phản hồi tốn kém và tốn thời gian (Touvron et al., 2023; Dubey et al., 2024).

Chất lượng của các mô hình sở thích, lần lượt, được xác định bởi một số yếu tố, bao gồm số lượng dữ liệu phản hồi của con người, phân phối các phản hồi được đánh giá, và độ chính xác của các nhãn sở thích (Touvron et al., 2023). Được thúc đẩy bởi những quan sát này, chúng tôi đề xuất một phương pháp mới để cải thiện huấn luyện mô hình thưởng thông qua việc tạo ra dữ liệu sở thích tổng hợp chất lượng cao, on-policy. Phương pháp này tận dụng khả năng tạo sinh của mô hình ngôn ngữ đang được tối ưu hóa để tạo ra một khung huấn luyện bán giám sát.

Phương pháp của chúng tôi dựa trên lấy mẫu Best-of-N, một chiến lược tạo sinh tạo ra N đầu ra và chọn cái được ghi điểm tốt nhất theo mô hình thưởng. Kỹ thuật lấy mẫu đơn giản nhưng mạnh mẽ này đã chứng minh giá trị của nó trong huấn luyện mô hình ngôn ngữ (Liu et al., 2023; Gulcehre et al., 2023), nhưng vẫn chưa được khám phá cho việc huấn luyện mô hình thưởng. Chúng tôi thực hiện những bước đầu tiên trong việc mở rộng nó sang bối cảnh này và chứng minh tiềm năng của nó để tăng cường mô hình hóa sở thích, từ đó cải thiện việc căn chỉnh mô hình ngôn ngữ downstream.

--- TRANG 2 ---
Với lấy mẫu West-of-N, được minh họa trong Hình 1, chúng tôi tạo ra dữ liệu sở thích tổng hợp bằng cách trích xuất các thế hệ tốt nhất và tệ nhất trong một tập hợp N đầu ra cho một prompt không nhãn nhất định¹. Hình thức tự huấn luyện này (Scudder, 1965) hiệu quả tăng cường bất kỳ tập dữ liệu sở thích ban đầu nào, được sử dụng để xác định các cặp West-of-N, với các sở thích on-policy chất lượng cao. Kết quả là, chúng tôi thu được những cải thiện đáng kể trong hiệu suất mô hình hóa thưởng.

Các đóng góp của công trình chúng tôi có ba khía cạnh: (1) Chúng tôi đề xuất một phương pháp mới, hiệu quả để tạo ra dữ liệu sở thích tổng hợp chất lượng cao. (2) Chúng tôi chỉ ra rằng điều này thành công cải thiện hiệu suất của bất kỳ mô hình thưởng nào, với hiệu quả có thể so sánh hoặc lớn hơn việc thêm một lượng dữ liệu sở thích của con người tương tự. (3) Theo hiểu biết tốt nhất của chúng tôi, công trình của chúng tôi cũng là đầu tiên cho thấy tiềm năng của lấy mẫu Best-of-N và học bán giám sát trong bối cảnh huấn luyện mô hình thưởng, mà chúng tôi hy vọng sẽ dẫn đến nghiên cứu thêm trong hướng đầy hứa hẹn này.

2 Công trình Liên quan
Lấy mẫu Best-of-N. Các chiến lược lấy mẫu chọn các đầu ra ứng viên dựa trên giá trị thưởng của chúng rất phổ biến trong các nỗ lực căn chỉnh mô hình ngôn ngữ (Stiennon et al., 2020; Nakano et al., 2021; Ouyang et al., 2022), do tính đơn giản và hiệu quả của chúng. Best-of-N, hay lấy mẫu từ chối (Touvron et al., 2023), thường được thực hiện bằng cách lấy thế hệ được ghi điểm cao nhất trong một nhóm N ứng viên, hoặc bằng cách lấy mẫu các thế hệ với xác suất tỷ lệ với giá trị thưởng của chúng. Trong thực tế, các chiến lược Best-of-N hướng phân phối đầu ra về phía các thế hệ thưởng cao (Gulcehre et al., 2023), điều này đã được chỉ ra là cải thiện hiệu suất của các mô hình ngôn ngữ được huấn luyện với tinh chỉnh có giám sát (Touvron et al., 2023; Dubey et al., 2024), hoặc với các mất mát đối lập, xếp hạng hoặc hiệu chuẩn (Liu et al., 2023; Yuan et al., 2023). Trong công trình đồng thời, Xu et al. (2023); Yuan et al. (2024); Meng et al. (2024) sử dụng phương pháp West-of-N để xây dựng dữ liệu huấn luyện tổng hợp cho tối ưu hóa sở thích trực tiếp (Rafailov et al., 2023) và đạt được kết quả tiên tiến trên benchmark căn chỉnh LM.

Cuối cùng, Best-of-N cũng đã được sử dụng như một chiến lược suy luận đơn giản, cạnh tranh, mặc dù tốn kém về mặt tính toán, như một baseline cho các phương pháp RLHF (Gao et al., 2023). Những kỹ thuật này cho phép căn chỉnh các mô hình ngôn ngữ với phản hồi của con người, trong khi tránh một quy trình tối ưu hóa học tăng cường đôi khi thách thức.

Trong khi lấy mẫu Best-of-N đã được khám phá rộng rãi cho huấn luyện mô hình ngôn ngữ, những lợi ích tiềm năng cho tối ưu hóa mô hình thưởng vẫn chưa được điều tra.

Dữ liệu Sở thích Tổng hợp. Touvron et al. (2023) chỉ ra rằng việc thu thập thêm dữ liệu sở thích liên tục cải thiện hiệu suất mô hình thưởng và chất lượng mô hình ngôn ngữ downstream. Thật không may, thu thập dữ liệu sở thích của con người vẫn tốn kém, mất thời gian và nhiễu; điều này thúc đẩy việc sử dụng dữ liệu tổng hợp, có thể mang lại những lợi ích tương tự với một phần chi phí và độ phức tạp. Một phương pháp, được biết đến như RL từ Phản hồi AI (RLAIF), là sử dụng các mô hình ngôn ngữ lớn để gắn nhãn các cặp phản hồi cạnh nhau thay vì dựa vào việc gắn nhãn của con người (Bai et al., 2022b; Lee et al., 2023). Một thay thế đầy hứa hẹn là trực tiếp tạo ra các phản hồi tích cực và tiêu cực

Bảng 1: So sánh dữ liệu sở thích cho mô hình hóa thưởng.
Dữ liệu sở thích | On-policy | Ít nhiễu | Có thể mở rộng
Phản hồi Con người | ✓ | ✗ | ✗
RLAIF (Bai et al., 2022b) | ✓ | ✗ | ✓
RLCD (Yang et al., 2023) | ✗ | ✗ | ✓
West-of-N (Của chúng tôi) | ✓ | ✓ | ✓

¹Best-of-N + Worst-of-N = "West"-of-N

--- TRANG 3 ---
bằng cách sử dụng các mô hình chất lượng khác nhau (Kim et al., 2023), hoặc thông qua prompting. Trong RL từ Contrast Distillation (RLCD), ví dụ, Yang et al. (2023) sử dụng các prompt tích cực và tiêu cực khác nhau để tạo ra các cặp phản hồi chất lượng cao và thấp như vậy. Các phương pháp tạo sở thích tổng hợp này khai thác đại diện riêng của các mô hình ngôn ngữ về phân phối phản hồi và chất lượng. Như được tóm tắt trong Bảng 1, chúng cho phép mở rộng số lượng dữ liệu sở thích, nhưng chúng có thể đưa vào các phản hồi off-policy hoặc nhãn nhiễu. Phương pháp của chúng tôi tránh điều này.

Tự Huấn luyện. Học bán giám sát là một mô hình học máy tận dụng các tập dữ liệu lớn, không nhãn để cải thiện hiệu suất trên một nhiệm vụ có giám sát. Tự huấn luyện nằm trong dòng công trình này, và bao gồm việc huấn luyện một mô hình giáo viên trên một tập dữ liệu có nhãn nhỏ hơn và sử dụng nó để tạo ra các nhãn giả trên các tập dữ liệu không nhãn lớn hơn (Scudder, 1965; Yarowsky, 1995). Một mô hình học sinh sau đó được huấn luyện trên sự hợp nhất của các tập dữ liệu gốc và được gắn nhãn giả, với cái sau thường được lọc để chỉ giữ lại các nhãn độ tin cậy cao. Phương pháp này mang lại những cải thiện hiệu suất đáng kể trong thị giác máy tính (Xie et al., 2020; Zoph et al., 2020), dịch máy (He et al., 2019) hoặc các mô hình thị giác-ngôn ngữ (Huang et al., 2022) bằng cách hiệu quả cho phép học sinh huấn luyện trên một tập con lớn hơn của phân phối không gian đầu vào.

3 Kiến thức Cơ bản
Trong phần sau, chúng tôi tóm tắt các yếu tố chính của khung Học tăng cường từ Phản hồi của Con người (RLHF), trong ứng dụng của nó vào việc căn chỉnh mô hình ngôn ngữ.

Gọi X và Y lần lượt biểu thị không gian của các truy vấn và phản hồi mô hình. Gọi π: X → ΔY biểu thị mô hình ngôn ngữ được căn chỉnh với sở thích của con người, đầu ra một phân phối trên các phản hồi cho mỗi truy vấn. Thông thường, đây là một mô hình ngôn ngữ được tinh chỉnh trên các đầu ra chất lượng cao theo cách có giám sát (Stiennon et al., 2020).

Mô hình hóa Thưởng. Phản hồi của con người thường được thu thập như một sở thích cặp đôi giữa hai phản hồi (y+, y−) trong Y² cho một truy vấn x nhất định trong X. Chúng tôi ký hiệu mối quan hệ sở thích của y+ so với y− bằng y+ ≻ y−. Tập dữ liệu sở thích bao gồm DHF = {(x, y+, y−) : y+ ≻ y−}.

Dưới mô hình Bradley-Terry (Bradley & Terry, 1952), các sở thích cặp đôi được giả định được xác định bởi một mô hình thưởng điểm như sau:
P(y+ ≻ y−|x) = exp(r(x, y+)) / (exp(r(x, y+)) + exp(r(x, y−)))

Hàm thưởng sau đó có thể được tham số hóa như rθ và ước tính từ DHF thông qua khả năng cực đại của mục tiêu sau (Stiennon et al., 2020; Ouyang et al., 2022), trong đó σ là hàm sigmoid:
max rθ E(x,y+,y−)∼DHF[log(σ(rθ(x, y+) − rθ(x, y−))].

Tối ưu hóa Học tăng cường. Mô hình thưởng sau đó được tận dụng để cải thiện chất lượng tạo sinh thông qua học tăng cường (Stiennon et al., 2020; Ziegler et al., 2019). Phương pháp tinh chỉnh này hướng tham số hóa của mô hình ngôn ngữ π về phía các đầu ra với thưởng cao. Điều này được đạt được bằng cách tối ưu hóa mục tiêu sau trên một tập hợp các prompt D = {x : x ∈ X}:
max π Ex∼D,y∼π[rθ(x, y)] − βDKL(π(y|x)∥π0(y|x)),

trong đó số hạng chính quy hóa sau đảm bảo rằng chính sách học được không lệch quá xa từ một chính sách tham chiếu π0 (ví dụ: checkpoint tinh chỉnh có giám sát). Siêu tham số β kiểm soát cường độ của chính quy hóa này.

4 Tạo ra Dữ liệu Sở thích Tổng hợp
Giả định truy cập vào một tập dữ liệu các truy vấn không nhãn DU = {x : x ∈ X}. Mục tiêu của chúng tôi là thiết kế một chiến lược lấy mẫu f±: X → Y² mà đầu ra, cho mỗi truy vấn x ∈ X, một cặp phản hồi f±(x) = (y+, y−) sao cho y+ được ưa thích hơn y−. Điều này cho phép chúng tôi tạo ra dữ liệu sở thích tổng hợp bằng cách gắn nhãn DU với các nhãn giả sở thích DL' = {(x, f±(x))}, và huấn luyện một mô hình thưởng trên DL'.

Mô hình thưởng này sẽ được sử dụng để tối ưu hóa mô hình chính sách π thông qua học tăng cường. Phân phối suy luận của nó do đó là phân phối phản hồi của chính sách π.

4.1 Tự Huấn luyện cho Mô hình hóa Sở thích
Giả định truy cập vào một số tập dữ liệu sở thích ban đầu DL = {(x, y+, y−) : y+ ≻ y−}, có thể bao gồm sở thích của con người hoặc dữ liệu được tạo tổng hợp khác (Bai et al., 2022b; Yang et al., 2023). Chúng tôi sử dụng dữ liệu này để huấn luyện một mô hình sở thích cơ bản được tham số hóa bởi θ: gọi Pθ(y+ ≻ y−|x) mô hình hóa xác suất phản hồi y+ được ưa thích hơn y− cho một truy vấn x.

Một chiến lược đơn giản để tạo ra dữ liệu sở thích tổng hợp cho truy vấn không nhãn x là lấy mẫu hai phản hồi y1, y2 từ chính sách tạo sinh π(x), và gắn nhãn giả cho cặp sở thích dựa trên Pθ(y1 ≻ y2|x):

f±(x) = {(y1, y2) nếu Pθ(y1 ≻ y2|x) > 0.5,
         (y2, y1) ngược lại.

Phương pháp này có thể được sử dụng để tạo ra một tập dữ liệu giả-sở thích DL' với các phản hồi on-policy, khớp với phân phối suy luận của mô hình thưởng. Một mô hình thưởng học sinh tự huấn luyện, được tham số hóa bởi θ', sau đó có thể được tối ưu hóa trên DL ∪ DL'. Quan trọng, lưu ý rằng không có yêu cầu cho mô hình sở thích cơ bản Pθ phải được thực hiện như một mô hình thưởng Bradley-Terry điểm, vì chỉ mô hình học sinh Pθ' sẽ được sử dụng trong pipeline học tăng cường downstream.

RL từ Phản hồi AI (Bai et al., 2022b) có thể được xem như một ví dụ của phương pháp này, trong đó một mô hình ngôn ngữ được tinh chỉnh hướng dẫn lớn thể hiện sở thích Pθ trên một cặp phản hồi. Trong trường hợp đặc biệt này, DL = ∅ và việc gắn nhãn giả được đạt được thông qua prompting few-shot.

4.2 Thuật toán Tự Huấn luyện West-of-N
Như trong bất kỳ nỗ lực tự huấn luyện nào, phương pháp gắn nhãn giả ở trên phụ thuộc rất nhiều vào hiệu suất của mô hình cơ bản Pθ: một mô hình không hoàn hảo sẽ thường gán nhãn không chính xác cho các cặp sở thích. Điều này được giảm thiểu trong công trình tự huấn luyện trước đây bằng cách chỉ giữ lại các mẫu với nhãn giả độ tin cậy cao (Huang et al., 2022).

Mở rộng ý tưởng này, chúng tôi đề xuất tối đa hóa xác suất gắn nhãn chính xác một cặp phản hồi on-policy cho một truy vấn q nhất định, theo mô hình sở thích cơ bản:

max (y+,y−)∼π(x) Pθ(y+ ≻ y−|x) (1)

Chúng tôi tóm tắt phương pháp của chúng tôi trong Thuật toán 1: trong thực tế, mục tiêu trong Phương trình (1) có thể được xấp xỉ bằng cách lấy mẫu một nhóm N đầu ra ứng viên từ chính sách và xác định những cái được ghi điểm tốt nhất và tệ nhất. Khi xử lý một RM cơ bản cặp đôi, các cuộc thi đấu có thể được thực hiện để khôi phục chúng một cách xấp xỉ (Zhao et al., 2023).

Thuật toán 1 Huấn luyện Mô hình Sở thích West-of-N.
Đầu vào: Mô hình ngôn ngữ π. Tập dữ liệu sở thích cơ bản DL.
Tập dữ liệu truy vấn không nhãn DU.
Huấn luyện mô hình sở thích cơ bản Pθ trên DL.
Khởi tạo DL' = ∅.
cho x trong DU làm
    Lấy mẫu N phản hồi: G = {yi : yi ∼ π(x)}Ni=1.
    Xây dựng cặp sở thích West-of-N:
    f±(x) = (y+, y−) = arg max yi,yj∈G Pθ(yi ≻ yj|x)
    Tùy chọn: Lọc dựa trên Pθ(y+ ≻ y−|x) hoặc
    π(y±|x).
    Cập nhật DL' = DL' ∪ {(x, y+, y−)}.
kết thúc cho
Huấn luyện mô hình sở thích trên DL ∪ DL'.

Đảm bảo Lý thuyết. Gọi P*(y+ ≻ y−|x) biểu thị hàm sở thích ground-truth cần được xấp xỉ. Cùng một hàm này là cơ sở cho dữ liệu sở thích cơ bản, sao cho DL = {(x, y+, y−) : y+, y− ∼ πunk(x); P*(y+ ≻ y−|x) > 0.5}, trong đó πunk là một hàm tạo sinh không biết. Chúng tôi thu được kết quả sau.

Định lý 4.1 (Độ chính xác West-of-N). Giả định |Pθ(y+ ≻ y−|x) − P*(y+ ≻ y−|x)| ≤ ε, cho tất cả {x, y+, y−}. Với bất kỳ x ∈ X, cặp sở thích West-of-N f±(x) = (yBoN, yWoN) được gắn nhãn chính xác với xác suất limN→∞ P*(yBoN ≻ yWoN|x) ≥ 1−2ε.

Chúng tôi tham khảo người đọc đến Phụ lục A.1 cho chứng minh. Kết quả này nhấn mạnh rằng các cặp sở thích West-of-N có xác suất cao được gắn nhãn chính xác, với xác suất này phụ thuộc vào hiệu suất của mô hình cơ bản trong việc ước tính hàm sở thích ground truth.

Lọc Giả-Sở thích. Để cải thiện thêm chất lượng của các cặp sở thích được tạo ra, chúng có thể được lọc dựa trên độ tin cậy của nhãn sở thích của chúng (nói cách khác, sự khác biệt về chất lượng trong cặp) và phạm vi bao phủ của chúng về phân phối phản hồi liên quan. Phương pháp chỉ sử dụng các nhãn giả tin cậy đã được chỉ ra là cải thiện tự huấn luyện (Amini et al., 2022). Chúng tôi đo độ tin cậy nhãn sở thích thông qua dự đoán Pθ(y+ ≻ y−|x), và chỉ giữ lại các cặp West-of-N trên một quantile nhất định. Tương tự, chúng tôi cũng áp dụng ngưỡng khả năng trên cả phản hồi tích cực và tiêu cực, π(y+|x) và π(y−|x), để đảm bảo các phản hồi vẫn trong phân phối. Chúng tôi xác định giá trị ngưỡng thông qua hiệu suất validation, với chi tiết trong Phụ lục C. Kết quả thực nghiệm trong phần tiếp theo chứng minh giá trị gia tăng của những bước lọc bổ sung này.

5 Thực nghiệm
Chúng tôi đo hiệu suất của West-of-N như một kỹ thuật tạo dữ liệu sở thích tổng hợp trên ba tập dữ liệu. Điều tra thực nghiệm của chúng tôi xác thực các tuyên bố sau: Tự huấn luyện West-of-N cải thiện hiệu suất của các mô hình thưởng được huấn luyện trên phản hồi của con người, (1) với lợi ích có thể so sánh với việc tăng lượng dữ liệu phản hồi của con người, và (2) với lợi ích lớn hơn các phương pháp tạo sở thích tổng hợp khác. Thật vậy, (3) West-of-N cải thiện hiệu suất mô hình thưởng cho dù nó ban đầu được huấn luyện trên phản hồi của con người hay dữ liệu tổng hợp. Cuối cùng, chúng tôi đề xuất một nghiên cứu ablation cho phương pháp của chúng tôi, cho phép xác định chiến lược tự huấn luyện tốt nhất cho một mô hình thưởng nhất định.

[Tiếp tục với phần còn lại của bài báo...]
