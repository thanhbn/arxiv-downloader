# 2209.07676.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2209.07676.pdf
# Kích thước tệp: 5353252 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tối ưu hóa Chính sách Kép Bảo thủ cho Học tăng cường Dựa trên Mô hình Hiệu quả
Shenao Zhang
Viện Công nghệ Georgia
Atlanta, GA 30332
shenao@gatech.edu
Tóm tắt
Học tăng cường Dựa trên Mô hình (MBRL) hiệu quả có thể chứng minh dựa trên chủ nghĩa lạc quan hoặc lấy mẫu hậu nghiệm (PSRL) được đảm bảo đạt được tính tối ưu toàn cầu một cách tiệm cận bằng cách đưa vào thước đo độ phức tạp của mô hình. Tuy nhiên, độ phức tạp có thể tăng theo cấp số nhân đối với những mô hình phi tuyến đơn giản nhất, nơi mà hội tụ toàn cầu là không thể trong số lần lặp hữu hạn. Khi mô hình gặp phải lỗi tổng quát hóa lớn, được đo lường định lượng bằng độ phức tạp mô hình, độ không chắc chắn có thể lớn. Mô hình được lấy mẫu mà chính sách hiện tại được tối ưu hóa một cách tham lam sẽ do đó không ổn định, dẫn đến cập nhật chính sách tích cực và khám phá quá mức. Trong công trình này, chúng tôi đề xuất Tối ưu hóa Chính sách Kép Bảo thủ (CDPO) bao gồm một Cập nhật Tham chiếu và một Cập nhật Bảo thủ. Chính sách đầu tiên được tối ưu hóa dưới một mô hình tham chiếu, mô phỏng cơ chế của PSRL đồng thời cung cấp nhiều ổn định hơn. Một phạm vi ngẫu nhiên bảo thủ được đảm bảo bằng cách tối đa hóa kỳ vọng của giá trị mô hình. Không có quy trình lấy mẫu có hại, CDPO vẫn có thể đạt được cùng mức hối tiếc như PSRL. Quan trọng hơn, CDPO có cả cải thiện chính sách đơn điệu và tính tối ưu toàn cầu đồng thời. Kết quả thực nghiệm cũng xác nhận hiệu quả khám phá của CDPO.

1 Giới thiệu
Học tăng cường Dựa trên Mô hình (MBRL) bao gồm việc thu được một mô hình bằng cách tương tác với môi trường và học cách đưa ra quyết định tối ưu sử dụng mô hình [55,32]. MBRL hấp dẫn do độ phức tạp mẫu giảm đáng kể so với các đối tác không dựa trên mô hình. Tuy nhiên, khai thác mô hình tham lam giả định rằng mô hình đủ chính xác thiếu đảm bảo cho tính tối ưu toàn cầu. Các chính sách có thể không tối ưu và bị kẹt tại các cực đại địa phương ngay cả trong các nhiệm vụ đơn giản [10].

Do đó, một số thuật toán MBRL hiệu quả có thể chứng minh đã được đề xuất. Dựa trên nguyên tắc chủ nghĩa lạc quan trước sự không chắc chắn (OFU) [56,49,10], OFU-RL đạt được tính tối ưu toàn cầu bằng cách đảm bảo rằng giá trị thiên lệch lạc quan gần với giá trị thực về lâu dài. Dựa trên Thompson Sampling [62], Posterior Sampling RL (PSRL) [57,42,43] khám phá bằng cách tối ưu hóa tham lam chính sách trong một MDP được lấy mẫu từ phân phối hậu nghiệm trên các MDP.

Ngoài các MDP hữu hạn, để có được một ràng buộc tổng quát cho phép hiệu quả mẫu trong nhiều trường hợp khác nhau, chúng ta cần đưa vào thước đo độ phức tạp bổ sung. Ví dụ, [49,43] cung cấp hối tiếc eO(√(dET)) cho cả OFU và PSRL với chiều eluder dE nắm bắt mức độ hiệu quả mô hình tổng quát hóa.

Tuy nhiên, gần đây đã được chỉ ra [13,33] rằng chiều eluder ngay cả đối với những mô hình phi tuyến đơn giản nhất không thể bị giới hạn đa thức. Hiệu quả của các thuật toán do đó sẽ bị tê liệt.

Các lý do cơ bản cho sự không hiệu quả như vậy là cập nhật chính sách tích cực và vấn đề khám phá quá mức. Cụ thể, khi một mô hình phi tuyến được sử dụng để khớp các hàm chuyển tiếp phức tạp, khả năng tổng quát hóa của nó sẽ kém so với các vấn đề tuyến tính đơn giản. Nếu một mô hình ngẫu nhiên được chọn từ giả thuyết lớn, ví dụ, được chọn một cách lạc quan hoặc được lấy mẫu từ hậu nghiệm, nó "không ổn định".

Hội nghị lần thứ 36 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2022).arXiv:2209.07676v1 [cs.LG] 16 Tháng 9 2022

--- TRANG 2 ---
Nói cách khác, mô hình được chọn có thể thay đổi đáng kể giữa các lần lặp liên tiếp. Cập nhật chính sách dưới mô hình này cũng sẽ tích cực và do đó gây ra suy giảm giá trị. Tệ hơn nữa, độ không chắc chắn nhận thức lớn dẫn đến một mô hình không thực tế, thúc đẩy các tác nhân khám phá không có thông tin. Một bước khám phá chỉ có thể loại bỏ một phần cực nhỏ theo cấp số nhân của giả thuyết.

Trong công trình này, chúng tôi trình bày Tối ưu hóa Chính sách Kép Bảo thủ (CDPO), một thuật toán MBRL đơn giản nhưng có thể chứng minh. Vì quy trình lấy mẫu trong PSRL làm hại cập nhật chính sách do mô hình không ổn định trong quá trình huấn luyện, chúng tôi đề xuất Cập nhật Tham chiếu tối ưu hóa tham lam một chính sách trung gian dưới một mô hình tham chiếu. Nó mô phỏng quy trình lấy mẫu-sau-tối ưu hóa trong PSRL nhưng cung cấp nhiều ổn định hơn vì chúng ta tự do thiết lập một mô hình tham chiếu ổn định. Chúng tôi chỉ ra rằng ngay cả không có quy trình lấy mẫu, CDPO có thể phù hợp với hối tiếc kỳ vọng của PSRL đến các yếu tố không đổi cho bất kỳ mô hình tham chiếu thích hợp nào, ví dụ, ước lượng bình phương tối thiểu nơi tập hợp tin cậy được tập trung tại. Bước Cập nhật Bảo thủ sau đó tiếp theo để khuyến khích khám phá trong một phạm vi hợp lý. Cụ thể, mục tiêu của một chính sách phản ứng là tối đa hóa kỳ vọng của giá trị mô hình, thay vì giá trị của một mô hình đơn. Hai bước này được thực hiện theo cách lặp lại trong CDPO.

Về mặt lý thuyết, chúng tôi chỉ ra sự tương đương thống kê giữa CDPO và PSRL với cùng thứ tự hối tiếc kỳ vọng. Ngoài ra, chúng tôi đưa ra ràng buộc cải thiện chính sách lặp lại của CDPO, đảm bảo cải thiện đơn điệu dưới các điều kiện nhẹ. Chúng tôi cũng thiết lập hối tiếc dưới tuyến tính của CDPO, cho phép tính tối ưu toàn cầu của nó được trang bị với bất kỳ lớp hàm mô hình nào có thước đo độ phức tạp bị giới hạn. Theo hiểu biết của chúng tôi, khung đề xuất là đầu tiên đồng thời có tính tối ưu toàn cầu và cải thiện chính sách lặp lại. Kết quả thực nghiệm xác minh sự tồn tại của vấn đề khám phá quá mức và chứng minh lợi ích thực tế của CDPO.

2 Bối cảnh

2.1 Học tăng cường Dựa trên Mô hình
Chúng ta xem xét vấn đề học tối ưu hóa một Quá trình Quyết định Markov (MDP) giảm giá γ vô hạn chân trời qua các tập tương tác lặp lại. Ký hiệu không gian trạng thái và không gian hành động là S và A, tương ứng. Khi thực hiện hành động a∈A tại trạng thái s∈S, tác nhân nhận phần thưởng r(s,a) và môi trường chuyển tiếp vào một trạng thái mới theo xác suất s'~f(·|s,a). Ở đây, f là một thước đo dirac cho động lực học xác định và là một phân phối xác suất cho động lực học xác suất.

Trong RL dựa trên mô hình, mô hình động lực học thực f không được biết và cần được học sử dụng dữ liệu thu thập thông qua tương tác theo tập (hoặc lặp lại). Dữ liệu lịch sử đến lần lặp t sau đó tạo thành
Ht={{sh,i, ah,i, sh+1,i}H-1h=0}t-1i=1, nơi H là số bước thời gian thực tế các tác nhân chạy trong một tập.

Phân phối hậu nghiệm của mô hình động lực học được ước lượng là ρ(·|Ht). Thay vào đó, mô hình frequentist của trung bình và độ không chắc chắn cũng có thể được ước lượng. Cụ thể, xem xét lớp hàm mô hình F={f: S×A→S} với kích thước |F|, chứa mô hình thực f*∈F. Tập hợp tin cậy (hoặc tập giả thuyết mô hình) Ft⊆F được giới thiệu để đại diện cho phạm vi động lực học có thể thống kê [49,43,10]. Để đảm bảo rằng f*∈Ft với xác suất cao, một cách là xây dựng tập hợp tin cậy là Ft:={f∈F|‖f̂-f̂LS_t‖2,Et≤√βt}. Ở đây, βt là một tham số tin cậy được chọn thích hợp (thông qua bất đẳng thức tập trung), chuẩn thực nghiệm ℓ2 tích lũy được định nghĩa bởi ‖g‖2,Et:=∑t-1i=1‖g(xi)‖22. Ước lượng bình phương tối thiểu là

f̂LS_t:= argmin f∈F ∑(s,a,s')∈Ht ‖f(s,a)-s'‖22. (2.1)

Ký hiệu hàm giá trị trạng thái và trạng thái-hành động liên kết với π trên mô hình f bởi Vπf: S→R và Qπf: S×A→R, tương ứng, được định nghĩa là

Vπf(s) = E[∑∞h=0 γh r(sh,ah)|s0=s,π,f], Qπf(s,a) = E[∑∞h=0 γh r(sh,ah)|s0=s,a0=a,π,f].

Mục tiêu của RL là học một chính sách π* = argmaxπ J(π) tối đa hóa lợi nhuận kỳ vọng J(π). Ký hiệu phân phối trạng thái ban đầu là μ. Dưới chính sách π, thước đo viếng thăm trạng thái dπ(s) trên S và thước đo viếng thăm trạng thái-hành động dπ(s,a) trên S×A trong MDP thực được định nghĩa là

dπ(s) = (1-γ)∑∞h=0 γh P(sh=s), dπ(s,a) = (1-γ)∑∞h=0 γh P(sh=s,ah=a), (2.2)

nơi s0~μ, ah~π(·|sh) và sh+1~f(·|sh,ah). Mục tiêu J(π) sau đó là
J(π) = Es0~μ[Vπf*(s0)] = E(s,a)~dπ[r(s,a)] (2.3)

2.2 Hối tiếc Tích lũy và Tính Tối ưu Tiệm cận
Một tiêu chí phổ biến để đánh giá các thuật toán RL là hối tiếc tích lũy, được định nghĩa là sự khác biệt hiệu suất tích lũy giữa chính sách πt tại mỗi lần lặp t và chính sách tối ưu trong quá trình chạy của thuật toán. Hối tiếc (tích lũy) đến lần lặp T được định nghĩa là:

Regret(T,π,f) := ∑Tt=1 ∫s∈S μ(s)(Vπ*f*(s)-Vπtf*(s)), (2.4)

Trong quan điểm Bayesian, mô hình f, chính sách học π, và hối tiếc là các biến ngẫu nhiên phải được học từ dữ liệu thu thập. Hối tiếc kỳ vọng Bayesian được định nghĩa là:

BayesRegret(T,π,ρ) := E[Regret(T,π,f)|f~ρ]. (2.5)

Một cách để chứng minh tính tối ưu tiệm cận là chỉ ra rằng hối tiếc (kỳ vọng) là dưới tuyến tính trong T, sao cho πt hội tụ đến π* trong số lần lặp đủ. Để có được ràng buộc hối tiếc, chiều rộng của tập hợp tin cậy ωt(s,a) được giới thiệu để đại diện cho độ lệch tối đa giữa bất kỳ hai thành viên nào trong Ft:

ωt(s,a) = sup f,f'∈Ft ‖f(s,a)-f'(s,a)‖2. (2.6)

3 Học tăng cường Dựa trên Mô hình Có thể Chứng minh

Trong phần này, chúng tôi phân tích các ý tưởng trung tâm và hạn chế của các thuật toán tham lam cũng như hai khung được biện minh về mặt lý thuyết phổ biến: thuật toán lạc quan và thuật toán lấy mẫu hậu nghiệm.

Khai thác Mô hình Tham lam. Trước khi giới thiệu các thuật toán có thể chứng minh, chúng tôi đầu tiên phân tích các thuật toán dựa trên mô hình tham lam. Trong khung này, tác nhân thực hiện hành động giả định rằng mô hình được khớp đủ chính xác giống với MDP thực. Các thuật toán nằm trong danh mục này có thể được chia thành hai nhóm: quy hoạch dựa trên mô hình và tối ưu hóa chính sách được tăng cường bởi mô hình. Ví dụ, các tác nhân Dyna [61,20,17] tối ưu hóa chính sách sử dụng người học không dựa trên mô hình với dữ liệu được tạo bởi mô hình. Mô hình cũng có thể được khai thác trong các ước lượng gradient bậc nhất [18,12,9] hoặc mở rộng giá trị [15,6]. Mặt khác, quy hoạch dựa trên mô hình, hoặc điều khiển dự đoán mô hình (MPC) [40,41], trực tiếp tạo ra các chuỗi hành động tối ưu dưới mô hình theo cách chân trời lùi.

Tuy nhiên, khai thác tham lam mô hình mà không có khám phá sâu [45] sẽ dẫn đến hiệu suất không tối ưu. Chính sách kết quả có thể gặp phải hội tụ sớm, để lại vùng phần thưởng cao tiềm năng chưa được khám phá. Vì dữ liệu chuyển tiếp được tạo ra bởi tác nhân thực hiện hành động trong MDP thực, hiệu ứng kép [4,27] rằng hành động hiện tại ảnh hưởng đến cả trạng thái tiếp theo và độ không chắc chắn mô hình không được xem xét bởi các thuật toán dựa trên mô hình tham lam.

Chủ nghĩa Lạc quan Trước Sự Không chắc chắn. Một cơ chế khám phá có thể chứng minh phổ biến là áp dụng nguyên tắc chủ nghĩa lạc quan trước sự không chắc chắn (OFU) [56,49,10]. Với OFU, tác nhân gán cho chính sách của mình một ước lượng thiên lệch lạc quan của giá trị ảo bằng cách tối ưu hóa chung trên các chính sách và mô hình bên trong tập hợp tin cậy Ft. Tại lần lặp t, chính sách OFU-RL πt được định nghĩa là:

πt = argmaxπ maxft∈Ft Vπft. (3.1)

Hầu hết các phân tích tiệm cận của các thuật toán RL lạc quan có thể được trừu tượng hóa như chỉ ra hai tính chất: giá trị ảo Vπft đủ cao, và nó gần với giá trị thực Vπf* về lâu dài. Tuy nhiên, trong các môi trường phức tạp nơi khả năng tổng quát hóa của các mô hình phi tuyến bị hạn chế, độ không chắc chắn nhận thức lớn sẽ dẫn đến một lợi nhuận lạc quan không thực tế lớn thúc đẩy các tác nhân khám phá không có thông tin. Tệ hơn nữa, các bước khám phá không tối ưu như vậy chỉ loại bỏ một phần nhỏ của giả thuyết mô hình [13], dẫn đến quá trình hội tụ chậm và hiệu suất thực tế không tối ưu.

Học tăng cường Lấy mẫu Hậu nghiệm. Một cơ chế khám phá thay thế dựa trên Thompson Sampling (TS) [62,52], bao gồm việc chọn hành động tối đa hóa từ một tập hợp giá trị hành động có thể thống kê. Các giá trị này có thể được liên kết với MDP được lấy mẫu từ phân phối hậu nghiệm của nó, do đó đặt tên là lấy mẫu hậu nghiệm cho học tăng cường (PSRL) [57,42,43].

Thuật toán bắt đầu với một phân phối tiên nghiệm của f. Tại mỗi lần lặp t, một mô hình ft được lấy mẫu từ hậu nghiệm ρ(·|Ht), và πt được cập nhật để tối ưu dưới ft:

ft~ρ(·|Ht); πt = argmaxπ Vπft. (3.2)

Cái nhìn sâu sắc là tránh xa các hành động không có khả năng tối ưu trong MDP thực. Khám phá được đảm bảo bằng tính ngẫu nhiên trong quy trình lấy mẫu. Thật không may, thực hiện các hành động tối ưu liên kết với một mô hình được lấy mẫu đơn có thể gây ra các vấn đề khám phá quá mức tương tự [52,51]. Cụ thể, một mô hình không hoàn hảo được lấy mẫu từ giả thuyết lớn có thể gây ra cập nhật chính sách tích cực và suy giảm giá trị giữa các lần lặp liên tiếp. Mức độ không tối ưu của các chính sách kết quả phụ thuộc vào độ không chắc chắn mô hình nhận thức. Bên cạnh đó, thực hiện πt không nhằm mục đích cung cấp cải thiện hiệu suất cho việc học chính sách tiếp theo, mà chỉ để thu hẹp độ không chắc chắn mô hình. Tuy nhiên, quy trình loại bỏ này sẽ chậm khi mô hình gặp phải lỗi tổng quát hóa lớn, được công thức hóa định lượng trong thước đo độ phức tạp mô hình dưới đây.

Thước đo Độ phức tạp và Ràng buộc Tổng quát hóa. Trong RL, chúng ta tìm cách có độ phức tạp mẫu để tìm một chính sách gần tối ưu hoặc ước lượng một hàm giá trị chính xác. Khi được cung cấp quyền truy cập vào một mô hình sinh (tức là, một mô hình lấy mẫu trừu tượng) trong các MDP hữu hạn, đã biết rằng số (minimax) chuyển tiếp mà tác nhân cần quan sát có thể dưới tuyến tính trong kích thước mô hình, tức là nhỏ hơn O(|S|²|A|). Ngoài các MDP hữu hạn nơi số trạng thái lớn (hoặc đếm được hoặc không đếm được vô hạn), chúng ta quan tâm đến khả năng học hoặc tổng quát hóa của RL. Thật không may, không thể cho học tăng cường agnostic tìm giả thuyết tốt nhất trong một số chính sách, giá trị, hoặc lớp giả thuyết mô hình đã cho: số mẫu cần thiết phụ thuộc theo cấp số nhân vào chân trời vấn đề [24]. Bất chấp các giả định cấu trúc, ví dụ như MDP tuyến tính [66,22,65] hoặc MDP thấp hạng [21,38], chúng tôi tập trung vào các ràng buộc tổng quát hóa có thể bao phủ nhiều trường hợp khác nhau. Điều này có thể được thực hiện với thước đo độ phức tạp bổ sung, ví dụ như chiều eluder [49], witness rank [60], hoặc bilinear rank [14].

Bằng cách đưa vào chiều eluder dE [49], công trình trước đây [43,44] đã thiết lập hối tiếc eO(√dET) cho cả OFU-RL và PSRL. Một cách trực quan, chiều eluder nắm bắt mức độ hiệu quả mô hình được học từ dữ liệu quan sát có thể ngoại suy đến dữ liệu tương lai, và cho phép hiệu quả mẫu trong nhiều trường hợp (tuyến tính) khác nhau. Tuy nhiên, được chỉ ra trong [13,33] rằng ngay cả những mô hình phi tuyến đơn giản nhất không có chiều eluder bị giới hạn đa thức. Kết quả sau đây từ Thm. 5.2 trong Dong et al. [13] và các kết quả tương tự cũng được thiết lập trong [33].

Định lý 3.1 (Chiều Eluder của Mô hình Phi tuyến [13]). Chiều eluder dimE(F,ε) (c.f. Định nghĩa 5.6) của mạng nơ-ron ReLU một lớp ít nhất là Ω(ε^{-(d-1)}), nơi d là chiều trạng thái-hành động, tức là φ(s,a)∈R^d. Với nhiều lớp hơn, yêu cầu kích hoạt ReLU có thể được nới lỏng.

Kết quả là, độ phức tạp bổ sung được ẩn trong chiều eluder, ví dụ khi chúng ta chọn ε=T^{-1}, hối tiếc eO(√dET) chứa dE = Ω(T^{d-1}) và không còn dưới tuyến tính trong T. Trong trường hợp này, các cơ chế khám phá có thể chứng minh trước đây sẽ mất tính chất mong muốn của tính tối ưu toàn cầu và hiệu quả mẫu, đó là lý do cơ bản cho vấn đề khám phá quá mức.

4 Tối ưu hóa Chính sách Kép Bảo thủ

Khi sử dụng các mô hình phi tuyến, ví dụ như mạng nơ-ron, vấn đề khám phá quá mức gây ra hiệu suất không thuận lợi trong thực tế, về mặt hội tụ chậm và giá trị tiệm cận không tối ưu. Để giải quyết thách thức này, chìa khóa là từ bỏ quy trình lấy mẫu và có đảm bảo trong quá trình huấn luyện.

Trong khía cạnh này, chúng tôi đề xuất Tối ưu hóa Chính sách Kép Bảo thủ (CDPO) đơn giản nhưng hiệu quả có thể chứng minh. Bằng cách tối ưu hóa chính sách theo hai quy trình cập nhật liên tiếp lặp lại, CDPO đồng thời có cả tính chất cải thiện giá trị chính sách đơn điệu và tính tối ưu toàn cầu.

4.1 Khung CDPO

Để bắt đầu, xem xét vấn đề tối đa hóa giá trị kỳ vọng, πt = argmaxπ E[V^π_f|Ht], nơi E[V^π_f|Ht] biểu thị các giá trị kỳ vọng trên hậu nghiệm. Rõ ràng, chúng ta có đảm bảo cải thiện giá trị kỳ vọng E[V^{πt}_f|Ht] ≥ E[V^{πt-1}_f|Ht]. Chúng ta cũng có thể thực hiện tối đa hóa giá trị kỳ vọng trong một trust-region để đảm bảo cải thiện lặp lại dưới bất kỳ f nào. Tuy nhiên, các cập nhật như vậy sẽ mất đảm bảo hội tụ toàn cầu mong muốn và có thể bị kẹt tại các cực đại địa phương ngay cả với các mô hình tuyến tính. Vì lý do này, chúng tôi đề xuất một quy trình kép của tối ưu hóa chính sách.

Cập nhật Tham chiếu. Bước cập nhật đầu tiên trả về một chính sách trung gian, ký hiệu là qt. Bước này là tham lam theo nghĩa là qt tối ưu đối với giá trị của một mô hình đơn f̃t, mà chúng tôi gọi là mô hình tham chiếu. Chọn một mô hình tham chiếu và tối ưu hóa một chính sách w.r.t. nó mô phỏng quy trình lấy mẫu-tối ưu hóa của PSRL. Chúng tôi sẽ chỉ ra trong Phần 5.1 rằng nếu chúng ta đặt ràng buộc f̃t∈Ft, thì CDPO đạt được cùng hối tiếc kỳ vọng như PSRL, ngụ ý tính tối ưu toàn cầu.

Quan trọng hơn, tối ưu hóa chính sách dưới f̃t ổn định hơn và có thể tránh vấn đề khám phá quá mức trong PSRL vì chúng ta tự do thiết lập nó như một tham chiếu ổn định giữa các lần lặp liên tiếp. Ví dụ, chúng ta cố định mô hình tham chiếu f̃t như ước lượng bình phương tối thiểu f̂^{LS}_t được định nghĩa trong (2.1), thay vì một mô hình ngẫu nhiên được lấy mẫu từ giả thuyết lớn gây ra cập nhật chính sách tích cực. Điều này cho chúng ta:

Cập nhật Tham chiếu (với Tham chiếu LS): qt = argmaxq V^q_{f̂^{LS}_t}. (4.1)

Cập nhật Bảo thủ Có ràng buộc. Cập nhật bảo thủ sau đó tiếp theo như giai đoạn thứ hai của CDPO, nhận đầu vào qt và trả về chính sách phản ứng πt+1:

Cập nhật Bảo thủ: πt = argmaxπ E[V^π_{ft}|Ht], s.t. E_{s~q_t}[D_{TV}(π_t(·|s),q_t(·|s))] ≤ ε, (4.2)

nơi D_{TV}(·,·) đại diện cho khoảng cách biến thiên toàn phần và ε là siêu tham số đặc trưng cho ràng buộc trust-region và kiểm soát mức độ khám phá.

So với OFU-RL và PSRL, việc khám phá và cập nhật chính sách ở trên là bảo thủ vì chính sách tối đa hóa kỳ vọng của giá trị mô hình, thay vì giá trị của một mô hình đơn (tức là mô hình lạc quan trong OFU-RL và mô hình được lấy mẫu trong PSRL). Cập nhật bảo thủ (4.2) tránh các cạm bẫy khi mô hình lạc quan hoặc mô hình được lấy mẫu hậu nghiệm gặp phải thiên lệch lớn, dẫn đến cập nhật chính sách tích cực và khám phá quá mức trong quá trình huấn luyện. Đáng chú ý, thuật ngữ bảo thủ trong công trình của chúng tôi khác với việc sử dụng trước đây, ví dụ như Conservative Policy Iteration [23,53]. Trong khi cái sau đề cập đến cập nhật chính sách với các ràng buộc, của chúng tôi là để nhấn mạnh phạm vi ngẫu nhiên bảo thủ và việc giảm khám phá quá mức không cần thiết bằng cách từ bỏ quy trình lấy mẫu.

Trong phân tích của chúng tôi, chúng tôi theo công trình trước đây [43,59,10,35] và giả định quyền truy cập vào một oracle tối ưu hóa chính sách. Trong thực tế, vấn đề tìm một chính sách tối ưu dưới một mô hình đã cho có thể được giải quyết gần đúng bởi các bộ giải dựa trên mô hình được liệt kê dưới đây. Phân tích mịn hơn có thể được thu được bằng cách áp dụng các kết quả hiện có được thiết lập cho gradient chính sách hoặc MPC cho các lớp chính sách hoặc mô hình cụ thể. Tuy nhiên, điều này nằm ngoài phạm vi của bài báo này.

4.2 Thuật toán Thực tế

Thuật toán 1 Thuật toán CDPO Thực tế
Đầu vào: Tiên nghiệm, bộ giải tối ưu hóa chính sách dựa trên mô hình MBPO(π,f,J).
1: cho lần lặp t = 1,...,T thực hiện
2: qt ← MBPO(·, f̂^{LS}_t, (4.1))
3: Lấy mẫu N mô hình {ft,n}^N_{n=1}
4: πt ← MBPO(qt, {ft,n}^N_{n=1}, (4.2))
5: Thực hiện πt trong MDP thực
6: Cập nhật Ht+1 = Ht ∪ {sh,t, ah,t, sh+1,t}^h
7: Cập nhật f̂^{LS}_{t+1} và ρ
8: kết thúc cho
9: trả về chính sách πT

Mã giả của CDPO trong Alg. 1. Bộ giải dựa trên mô hình MBPO(π,f,J) xuất ra chính sách (qt hoặc πt) tối ưu hóa mục tiêu J với quyền truy cập vào mô hình f. Một số loại bộ giải khác nhau có thể được tận dụng, ví dụ, tối ưu hóa chính sách không dựa trên mô hình được tăng cường bởi mô hình như Dyna [61], gradient tham số hóa lại dựa trên mô hình [18,9], hoặc điều khiển dự đoán mô hình [63]. Chi tiết về các lựa chọn tối ưu hóa khác nhau có thể được tìm thấy trong Phụ lục E. Trong các thí nghiệm, chúng tôi sử dụng các bộ giải Dyna và MPC.

Với bất đẳng thức Pinsker, ràng buộc biến thiên toàn phần trong (4.2) được thay thế bằng phân kỳ KL [53,2] trong các thí nghiệm. Chúng tôi theo công trình trước đây [34] để sử dụng ensemble mạng nơ-ron [10,25] cho ước lượng mô hình và sử dụng hiệu chuẩn [29,10] cho thước đo độ không chắc chắn chính xác.

5 Phân tích

Trong phần này, chúng tôi đầu tiên chỉ ra sự tương đương thống kê giữa CDPO và PSRL về cùng ràng buộc BayesRegret. Sau đó chúng tôi đưa ra ràng buộc giá trị chính sách lặp lại với cải thiện đơn điệu. Cuối cùng, chúng tôi chứng minh hội tụ toàn cầu của CDPO. Các chứng minh bị thiếu có thể được tìm thấy trong Phụ lục.

5.1 Tương đương Thống kê giữa CDPO và PSRL

Chúng tôi bắt đầu phân tích bằng cách làm nổi bật mối liên hệ giữa CDPO và PSRL với định lý sau, từ đó chúng tôi cũng chỉ ra vai trò của quy trình cập nhật kép và mô hình tham chiếu.

Định lý 5.1 (CDPO Phù hợp PSRL trong BayesRegret). Gọi π^{PSRL} là chính sách của bất kỳ thuật toán lấy mẫu hậu nghiệm nào cho học tăng cường được tối ưu hóa bởi (3.2). Nếu ràng buộc BayesRegret của PSRL thỏa mãn rằng với bất kỳ T > 0, BayesRegret(T, π^{PSRL}, ρ) ≤ D, thì với tất cả T > 0, chúng ta có cho chính sách CDPO π^{CDPO} rằng BayesRegret(T, π^{CDPO}, ρ) ≤ 3D.

Chứng minh phác thảo. Chúng tôi đầu tiên phác thảo chiến lược tổng quát trong phân tích PSRL. Nhớ lại định nghĩa của hối tiếc kỳ vọng Bayesian BayesRegret(T,π,ρ) := E[∑^T_{t=1} Rt], nơi Rt = V^π*_{f*} - V^π_t_{f*}. PSRL phân tích Rt bằng cách cộng và trừ V^{π*_{ft}}_{ft}, giá trị của chính sách tối ưu tưởng tượng π*_{ft} dưới một mô hình được lấy mẫu ft, tức là π*_{ft} = argmaxπ V^π_{ft}.

PSRL: Rt = V^π*_{f*} - V^{πt}_{f*} = V^π*_{f*} - V^{π*_{ft}}_{f*} = V^π*_{f*} - V^{π*_{ft}}_{ft} + V^{π*_{ft}}_{ft} - V^{πt}_{f*}, (5.1)

nơi đẳng thức thứ hai tuân theo định nghĩa của chính sách PSRL. Theo luật kỳ vọng toàn phần và Bổ đề Lấy mẫu Hậu nghiệm (ví dụ Bổ đề 1 trong [42]), chúng ta có E[V^π*_{f*} - V^{π*_{ft}}_{ft}] = 0 bằng cách lưu ý rằng f* và ft phân phối giống hệt nhau có điều kiện trên Ht. Sau đó chúng ta thu được

BayesRegret(T, π^{PSRL}, ρ) = ∑^T_{t=1} E[V^{π*_{ft}}_{ft} - V^{πt}_{f*}] ≤ ∑^T_{t=1} E[L‖ft(sh,ah) - f*(sh,ah)‖2] ≤ L/(1-γ) 4∑^T_{t=1} E[ωt] + 4δ/T ≤ D, (5.2)

nơi bất đẳng thức đầu tiên tuân theo bổ đề mô phỏng dưới giả định giá trị L-Lipschitz [43]. Bất đẳng thức thứ hai tuân theo định nghĩa của ωt trong (2.6) và việc xây dựng tập hợp tin cậy sao cho P(f* ∈ ∩Ft) ≥ 1-2δ và P(ft ∈ ∩Ft, f* ∈ ∩Ft) ≥ 1-4δ thông qua một ràng buộc hợp. Khi nhiều dữ liệu hơn được thu thập, độ không chắc chắn mô hình được giảm và tổng chiều rộng tập hợp tin cậy ∑ωt sẽ dưới tuyến tính trong T (c.f. Bổ đề B.5 và B.6), chỉ ra hối tiếc dưới tuyến tính.

Khi đến CDPO, chúng tôi phân tích hối tiếc như

CDPO: Rt = V^π*_{f*} - V^{πt}_{f*} = V^π*_{f*} - V^{π*_{ft}}_{ft} + V^{π*_{ft}}_{ft} - V^{πt}_{ft} + V^{πt}_{ft} - V^{πt}_{f*}, (5.3)

nơi chính sách CDPO πt được định nghĩa trong (4.2). Vì E[V^π*_{f*} - V^{π*_{ft}}_{ft}] = 0, chúng ta có

BayesRegret(T, π^{CDPO}, ρ) = ∑^T_{t=1} E[V^{π*_{ft}}_{ft} - V^{πt}_{ft} + V^{πt}_{ft} - V^{πt}_{f*}] ≤ ∑^T_{t=1} E[V^{π*_{ft}}_{ft} - V^{f̃t}_{ft} + V^{qt}_{f̃t} - V^{qt}_{ft} + V^{πt}_{ft} - V^{πt}_{f*}] ≤ L/(1-γ) 4∑^T_{t=1} 3E[ωt] + 8δ/T ≤ 3D, (5.4)

nơi bất đẳng thức đầu tiên tuân theo tính tham lam của qt và πt trong các bước cập nhật kép, tức là V^{π*_{ft}}_{ft} ≤ V^{qt}_{f̃t} cho bất kỳ ft nào cũng như E[V^{πt}_{ft}] ≥ E[V^{qt}_{ft}]. Thuật ngữ 8δ/T được giới thiệu vì f̃t ∈ Ft và P(ft ∈ ∩Ft, f̃t ∈ ∩Ft) ≥ 1-2δ.

Định lý 5.1 chỉ ra rằng mặc dù CDPO thực hiện cập nhật bảo thủ và từ bỏ quy trình lấy mẫu, nó phù hợp với hiệu quả thống kê của PSRL đến các yếu tố không đổi.

Tầm quan trọng của mô hình tham chiếu và quy trình kép cũng được phản ánh trong chứng minh. Cập nhật tham chiếu xây dựng cầu nối giữa V^{π*_{ft}}_{ft} và V^{πt}_{f*}. Tối ưu hóa chính sách dưới mô hình tham chiếu mô phỏng quy trình lấy mẫu-sau-tối ưu hóa của PSRL đồng thời cung cấp nhiều ổn định hơn khi tham chiếu ổn định, ví dụ, ước lượng bình phương tối thiểu chúng tôi sử dụng. Chúng tôi chính thức hóa ý tưởng này dưới đây.

5.2 Cải thiện Lặp lại Chính sách CDPO

Một động lực cho cập nhật bảo thủ là nó tối đa hóa (do đó cải thiện) giá trị kỳ vọng trên hậu nghiệm. Trong phần này, chúng tôi quan tâm đến cải thiện giá trị chính sách dưới bất kỳ f* chưa biết nào. Cụ thể, chúng tôi tìm cách có ràng buộc cải thiện lặp lại J(πt) ≥ J(πt-1), nơi mục tiêu thực J được định nghĩa trong (2.3).

Chúng tôi áp đặt các điều kiện đều đặn sau trên chuyển tiếp MDP cơ bản và viếng thăm trạng thái-hành động.

Giả định 5.2 (Điều kiện Đều đặn trên Chuyển tiếp MDP). Giả định rằng hàm chuyển tiếp MDP f: S×A→S với nhiễu σ-sub-Gaussian cộng và chuẩn bị bị ràng buộc, tức là ‖s‖2 ≤ C.

Giả định 5.3 (Điều kiện Đều đặn trên Viếng thăm Trạng thái-Hành động). Chúng ta giả định rằng tồn tại κ > 0 sao cho với bất kỳ chính sách πt, t ∈ [1,T],

‖E_t[dq_{t+1}/dπ_t(s,a)]‖_2^{1/2} ≥ κ, (5.5)

nơi dq_{t+1}/dπ_t là đạo hàm Radon-Nikodym của qt+1 đối với πt.

Định lý 5.4 (Cải thiện Lặp lại Chính sách). Giả sử chúng ta có ‖f̃(·,·)‖ ≤ C cho f̃ ∈ F nơi lớp mô hình F hữu hạn. Định nghĩa Δ := maxs,a |A^π_{f*}(s,a)|, nơi A^π_{f*} là hàm lợi thế được định nghĩa là A^π_{f*}(s,a) := Q^π_{f*}(s,a) - V^π_{f*}(s). Với xác suất ít nhất 1-δ, cải thiện chính sách giữa các lần lặp liên tiếp được ràng buộc bởi

J(πt) - J(πt-1) ≥ Γ(t) - (1+Δ)ε - 2σ²C²ln(|F|/δ)/(1-γ)κH - 2ε/(1-γ), (5.6)

nơi Γ(t) := E_s[V^{qt}_{f̃t}(s) - V^{qt-1}_{f̃t}(s)] ≥ 0 do tính tham lam của qt.

Định lý trên cung cấp ràng buộc cải thiện lặp lại theo thuật toán CDPO. Khi κH đủ lớn, cải thiện giá trị chính sách ít nhất là Γ(t) bằng cách chọn ε đúng cách nhỏ. Cụ thể, thuật ngữ đầu tiên Γ(t) đặc trưng cho cải thiện chính sách do khai thác tham lam trong (4.1), và Γ(t) ≥ 0 vì qt tối ưu dưới mô hình tham chiếu f̃t. Thuật ngữ thứ hai trong (5.6) giải thích cho lỗi tổng quát hóa của phương pháp bình phương tối thiểu. Cụ thể, mô hình f̃t = f̂^{LS}_t ∈ Ft được huấn luyện để khớp các mẫu lịch sử. Tuy nhiên, chúng ta tìm cách có ràng buộc lỗi mô hình trên thước đo viếng thăm trạng thái-hành động, đòi hỏi độ lệch từ trung bình thực nghiệm đến kỳ vọng của nó sử dụng bất đẳng thức Bernstein và ràng buộc hợp. Cuối cùng, ràng buộc trust-region trong (4.2) mang lại thuật ngữ 4ε/(1-γ), giảm về không nếu ε nhỏ. Điều này có ý nghĩa trực quan vì ε kiểm soát mức độ khám phá bảo thủ.

5.3 Tính Tối ưu Toàn cầu của CDPO

Bây giờ chúng tôi phân tích tính tối ưu toàn cầu của CDPO bằng cách nghiên cứu hối tiếc kỳ vọng của nó. Như đã thảo luận trong Phần 3, học tăng cường agnostic là không thể. Không có giả định cấu trúc, thước đo độ phức tạp bổ sung được yêu cầu cho một ràng buộc tổng quát hóa ngoài các thiết lập hữu hạn. Vì lý do này, chúng tôi áp dụng ký hiệu của chiều eluder [49, 43], được định nghĩa như sau:

Định nghĩa 5.5 ((F,ε)-Phụ thuộc). Nếu chúng ta nói (s,a) ∈ S×A là (F,ε)-phụ thuộc vào {(si,ai)}^n_{i=1} ⊆ S×A, thì
∀f1,f2 ∈ F, ∑^n_{i=1} ‖f1(si,ai) - f2(si,ai)‖²₂ ≤ ε² ⟹ ‖f1(s,a) - f2(s,a)‖₂ ≤ ε.

Ngược lại, (s,a) ∈ S×A là (F,ε)-độc lập của {(si,ai)}^n_{i=1} khi và chỉ khi nó không thỏa mãn định nghĩa cho phụ thuộc.

Định nghĩa 5.6 (Chiều Eluder). Chiều eluder dimE(F,ε) là độ dài của chuỗi dài nhất có thể của các phần tử trong S×A sao cho với một số ε' ≥ 0, mỗi phần tử là (F,ε')-độc lập của các tiền nhiệm của nó.

Chúng tôi đưa ra giả định sau về tính liên tục Lipschitz của hàm giá trị.

Giả định 5.7 (Giá trị Liên tục Lipschitz). Tại lần lặp t, giả định hàm giá trị V^π_{ft} cho bất kỳ chính sách π nào là liên tục Lipschitz theo nghĩa rằng |V^π_{ft}(s1) - V^π_{ft}(s2)| ≤ Lt‖s1 - s2‖2.

Đáng chú ý, Giả định 5.7 được giữ dưới các điều kiện đều đặn nhất định của MDP, ví dụ khi chuyển tiếp và phần thưởng liên tục Lipschitz [5,47]. Dưới giả định này, nhiều thiết lập RL có thể được thỏa mãn [13], ví dụ, các mô hình phi tuyến với chính sách Lipschitz ngẫu nhiên và mô hình phần thưởng Lipschitz, và do đó được áp dụng bởi nhiều công trình RL dựa trên mô hình khác nhau [35, 7, 13].

Bây giờ chúng tôi nghiên cứu tính tối ưu toàn cầu của CDPO bằng định lý hối tiếc kỳ vọng sau, có thể được xem như một hệ quả trực tiếp của Định lý 5.1 nêu sự tương đương thống kê giữa CDPO và PSRL.

Định lý 5.8 (Hối tiếc Kỳ vọng của CDPO). Gọi N(F,ε,‖·‖2) là số phủ ε của F. Ký hiệu dE := dimE(F,T^{-1}) cho chiều eluder của F tại độ chính xác 1/T. Dưới Giả định 5.2 và 5.7, hối tiếc kỳ vọng tích lũy của CDPO trong T lần lặp được ràng buộc bởi

BayesRegret(T,π,ρ) ≤ T(3T-5)L̄/((T-1)(T-2)) [1 + 1/(1-γ)]CdE + 4√TdE βT(1/(2T),δ) + 4δ/γC, (5.7)

nơi βT := √[8²log(2N(F,1/(T²),‖·‖2)/T) + 2σ²/(8C) + √(8²log(8T³)/δ)/T] và L̄ := E[Lt].

Ở đây, số phủ được giới thiệu vì chúng ta đang xem xét F có thể chứa vô hạn nhiều hàm, mà chúng ta không thể đơn giản áp dụng một ràng buộc hợp. Bên cạnh đó, δ là tham số tin cậy chứa f* với xác suất cao (thông qua bất đẳng thức tập trung).

Để làm rõ tiệm cận của ràng buộc hối tiếc kỳ vọng, chúng tôi giới thiệu một thước đo khác về chiều phản ánh độ nhạy cảm của F đối với overfitting thống kê.

Hệ quả 5.9 (Ràng buộc Tiệm cận). Định nghĩa chiều Kolmogorov w.r.t. lớp hàm F là
dK = dimK(F) := lim sup_{ε↓0} log(N(F,ε,‖·‖2))/log(1/ε).

Dưới các giả định của Định lý 5.8 và bằng cách bỏ qua các thuật ngữ logarithmic trong T, hối tiếc của CDPO là
BayesRegret(T,π,ρ) = Õ(L̄√dKdET). (5.8)

Kết quả hối tiếc dưới tuyến tính cho phép tính tối ưu toàn cầu và hiệu quả mẫu cho bất kỳ lớp mô hình nào với thước đo độ phức tạp hợp lý. Đồng thời, định lý cải thiện lặp lại đảm bảo khám phá hiệu quả và hiệu suất tốt ngay cả khi lớp mô hình rất phi tuyến.

6 Đánh giá Thực nghiệm

6.1 Hiểu các Cơ chế Khám phá Khác nhau

Chúng tôi đầu tiên cung cấp cái nhìn sâu sắc và bằng chứng về lý do tại sao khám phá CDPO có thể hiệu quả hơn trong các MDP N-Chain bảng, có các hành động phải tối ưu và các hành động trái không tối ưu tại mỗi trạng thái N. Thiết lập và kết quả đầy đủ được cung cấp trong Phụ lục F.2. Trong Hình 1, chúng tôi so sánh hậu nghiệm của CDPO và PSRL tại trạng thái xa nhất từ trạng thái ban đầu, tức là trạng thái khó nhất cho các tác nhân đến và khám phá.

[Hình 1 và 2 mô tả so sánh hậu nghiệm và đường cong hối tiếc]

Khi huấn luyện bắt đầu, cả hai thuật toán đều có phương sai lớn của ước lượng giá trị. Tuy nhiên, khi huấn luyện tiến triển, CDPO đưa ra các ước lượng chính xác và chắc chắn hơn, nhưng chỉ cho các hành động phải tối ưu không phải cho các hành động trái không tối ưu, trong khi các tác nhân PSRL khám phá cả hai hướng. Điều này xác minh vấn đề khám phá quá mức tiềm năng trong PSRL: miễn là độ không chắc chắn chứa các giá trị lớn không thực tế, các tác nhân PSRL có thể thực hiện khám phá không có thông tin bằng cách hành động không tối ưu theo một mô hình được lấy mẫu không chính xác. Ngược lại, CDPO thay thế mô hình được lấy mẫu bằng một ước lượng trung bình ổn định và quan tâm đến giá trị kỳ vọng, do đó tránh các cạm bẫy như vậy. Chúng ta thấy trong Hình 2 rằng mặc dù CDPO có độ không chắc chắn lớn hơn nhiều cho các hành động trái không tối ưu, hối tiếc của nó thấp hơn.

6.2 Hiệu quả Khám phá với Lớp Mô hình Phi tuyến

Trong các MDP hữu hạn, các tác nhân kiểu PSRL có thể chỉ định và thử mọi hành động có thể để cuối cùng có được dự đoán chính xác với độ tin cậy cao. Tuy nhiên, thảo luận của chúng tôi trong Phần 3 chỉ ra rằng một vấn đề khám phá quá mức tương tự trong các môi trường phức tạp hơn có thể dẫn đến các bước khám phá ít thông tin hơn, chỉ loại bỏ một phần cực nhỏ theo cấp số nhân của độ không chắc chắn.

Để thấy tác động của nó đến hiệu suất huấn luyện, chúng tôi báo cáo kết quả của các thuật toán có thể chứng minh với các mô hình phi tuyến trên một số nhiệm vụ MuJoCo trong Hình 3. Đối với OFU-RL, chúng tôi chủ yếu đánh giá HUCRL [10], một thuật toán sâu được đề xuất để xử lý sự không thể xử lý của tối ưu hóa chung. Chúng tôi quan sát thấy tất cả các thuật toán đạt được tính tối ưu tiệm cận trong con lắc ngược. Vì chiều của nhiệm vụ con lắc thấp, việc học một mô hình chính xác (và do đó có thể tổng quát hóa) không đặt ra thách thức thực tế. Tuy nhiên, trong các nhiệm vụ chiều cao hơn như half-cheetah, CDPO đạt được giá trị tiệm cận cao hơn với hội tụ nhanh hơn. Chi tiết triển khai và siêu tham số được cung cấp trong Phụ lục F.1.

[Hình 3 mô tả hiệu suất trên các nhiệm vụ MuJoCo]

6.3 So sánh với Các thuật toán RL Trước đây

Chúng tôi cũng kiểm tra một phạm vi rộng hơn của các thuật toán MBRL, bao gồm MBPO [20], SLBO [35], và ME-TRPO [30]. Các baseline không dựa trên mô hình bao gồm SAC [16], PPO [54], và MPO [2]. Kết quả được hiển thị trong Hình 4. Chúng tôi quan sát thấy CDPO đạt được hiệu suất tiệm cận cạnh tranh hoặc cao hơn đồng thời yêu cầu ít mẫu hơn so với cả các baseline dựa trên mô hình và không dựa trên mô hình.

[Hình 4 mô tả so sánh với các thuật toán baseline]

6.4 Nghiên cứu Ablation

Chúng tôi tiến hành các nghiên cứu ablation để cung cấp hiểu biết tốt hơn về các thành phần trong CDPO. Có thể quan sát từ Hình 5 rằng các chính sách được cập nhật chỉ với Cập nhật Tham chiếu hoặc Cập nhật Bảo thủ tụt hậu so với khung kép. Chúng tôi cũng kiểm tra sự cần thiết và độ nhạy của siêu tham số ràng buộc ε. Chúng ta thấy rằng một ε không đổi và một ε suy giảm theo thời gian đạt được các giá trị tiệm cận tương tự với tốc độ hội tụ tương tự, cho thấy tính bền vững của CDPO. Tuy nhiên, loại bỏ ràng buộc sẽ mất đảm bảo cải thiện chính sách, do đó gây ra suy giảm. Ablation trên các lựa chọn khác nhau của bộ giải MBPO (Dyna và POPLIN-P [63]) cho thấy khả năng tổng quát hóa của CDPO.

[Hình 5 mô tả các nghiên cứu ablation]

7 Kết luận & Công việc Tương lai

Trong công trình này, chúng tôi trình bày Tối ưu hóa Chính sách Kép Bảo thủ (CDPO), một thuật toán dựa trên mô hình đơn giản nhưng có thể chứng minh. Bằng thực hiện lặp lại Cập nhật Tham chiếu và Cập nhật Bảo thủ, CDPO khám phá trong một phạm vi hợp lý đồng thời tránh cập nhật chính sách tích cực. Hơn nữa, CDPO loại bỏ quy trình lấy mẫu có hại trong các phương pháp có thể chứng minh trước đây. Thay vào đó, một chính sách trung gian được tối ưu hóa dưới một mô hình tham chiếu ổn định, và tác nhân khám phá môi trường một cách bảo thủ bằng cách tối đa hóa giá trị chính sách kỳ vọng. Với cùng thứ tự hối tiếc như PSRL, thuật toán đề xuất có thể đạt được tính tối ưu toàn cầu đồng thời cải thiện chính sách một cách đơn điệu. Xem xét lựa chọn ngây thơ của chúng tôi về mô hình tham chiếu, các thiết kế phức tạp khác nên là một hướng tương lai có thành quả. Cũng sẽ thú vị khi khám phá các lựa chọn khác nhau của các bộ giải MBPO, mà chúng tôi muốn để lại như công việc tương lai.

[Các tài liệu tham khảo tiếp theo từ trang 10-26 chứa danh sách đầy đủ các tài liệu tham khảo, chứng minh chi tiết, thiết lập thí nghiệm và kết quả bổ sung]
