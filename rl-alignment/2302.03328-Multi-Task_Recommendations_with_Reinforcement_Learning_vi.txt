# 2302.03328.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2302.03328.pdf
# Kích thước tệp: 973475 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Gợi ý Đa nhiệm với Học Tăng cường
Ziru Liu
Đại học Thành phố Hồng Kông
ziruliu2-c@my.cityu.edu.hkJiejie Tian
Đại học Thành phố Hồng Kông
jiejitian2-c@my.cityu.edu.hkQingpeng Cai*
Kuaishou Technology
cqpcurry@gmail.com
Xiangyu Zhao*
Đại học Thành phố Hồng Kông
xianzhao@cityu.edu.hkJingtong Gao
Đại học Thành phố Hồng Kông
jt.g@my.cityu.edu.hkShuchang Liu
Kuaishou Technology
liushuchang@kuaishou.com
Dayou Chen
Đại học Thành phố Hồng Kông
dayouchen2-c@my.cityu.edu.hkTonghao He
Đại học Thành phố Hồng Kông
tonghaohe2-c@my.cityu.edu.hkDong Zheng
Kuaishou Technology
zhengd07@qq.com
Peng Jiang
Kuaishou Technology
jp2006@139.comKun Gai
Không liên kết
gai.kun@qq.com
TÓM TẮT
Trong những năm gần đây, Học Đa nhiệm (MTL) đã mang lại thành công to lớn trong các ứng dụng Hệ thống Gợi ý (RS) [41]. Tuy nhiên, các mô hình gợi ý dựa trên MTL hiện tại có xu hướng bỏ qua các mẫu tương tác người dùng-mục theo phiên vì chúng chủ yếu được xây dựng dựa trên các tập dữ liệu theo mục. Hơn nữa, việc cân bằng nhiều mục tiêu luôn là một thách thức trong lĩnh vực này, thường được tránh thông qua các ước lượng tuyến tính trong các nghiên cứu hiện có. Để giải quyết các vấn đề này, trong bài báo này, chúng tôi đề xuất một khung MTL được tăng cường bằng Học Tăng cường (RL), cụ thể là RMTL, để kết hợp các tổn thất của các nhiệm vụ gợi ý khác nhau bằng cách sử dụng các trọng số động. Cụ thể, cấu trúc RMTL có thể giải quyết hai vấn đề nêu trên bằng cách (i) xây dựng một môi trường MTL từ các tương tác theo phiên và (ii) huấn luyện cấu trúc mạng actor-critic đa nhiệm, tương thích với hầu hết các mô hình gợi ý dựa trên MTL hiện có, và (iii) tối ưu hóa và tinh chỉnh hàm tổn thất MTL bằng cách sử dụng các trọng số được tạo bởi các mạng critic. Các thí nghiệm trên hai tập dữ liệu công khai thực tế chứng minh hiệu quả của RMTL với AUC cao hơn so với các mô hình gợi ý dựa trên MTL tiên tiến. Ngoài ra, chúng tôi đánh giá và xác thực khả năng tương thích và khả năng chuyển giao của RMTL trên các mô hình MTL khác nhau.
KHÁI NIỆM CCS
•Hệ thống thông tin →Hệ thống gợi ý .
* Tác giả liên hệ.
Việc tạo ra các bản sao kỹ thuật số hoặc bản cứng của toàn bộ hoặc một phần công trình này để sử dụng cá nhân hoặc trong lớp học được cấp phép miễn phí với điều kiện các bản sao không được tạo ra hoặc phân phối vì lợi nhuận hoặc lợi thế thương mại và các bản sao phải ghi rõ thông báo này và trích dẫn đầy đủ trên trang đầu tiên. Bản quyền đối với các thành phần của công trình này thuộc sở hữu của người khác ngoài (các) tác giả phải được tôn trọng. Việc tóm tắt với ghi công nguồn được cho phép. Để sao chép theo cách khác, hoặc tái xuất bản, đăng trên máy chủ hoặc phân phối lại thành danh sách, cần có sự cho phép cụ thể trước và/hoặc một khoản phí. Yêu cầu quyền từ permissions@acm.org.
WWW '23, Ngày 1–5 tháng 5 năm 2023, Austin, TX, USA
©2023 Bản quyền thuộc về chủ sở hữu/tác giả. Quyền xuất bản được cấp phép cho ACM.
ACM ISBN 978-1-4503-9416-1/23/04. . . $15.00
https://doi.org/10.1145/3543507.3583467TỪ KHÓA
Học Đa nhiệm; Gợi ý; Học Tăng cường
Định dạng Tham chiếu ACM:
Ziru Liu, Jiejie Tian, Qingpeng Cai*, Xiangyu Zhao*, Jingtong Gao, Shuchang
Liu, Dayou Chen, Tonghao He, Dong Zheng, Peng Jiang, và Kun Gai. 2023.
Gợi ý Đa nhiệm với Học Tăng cường. Trong Kỷ yếu Hội nghị Web ACM 2023 (WWW '23), Ngày 1–5 tháng 5 năm 2023, Austin, TX, USA.
ACM, New York, NY, USA, 10 trang. https://doi.org/10.1145/3543507.3583467
1 GIỚI THIỆU
Sự phát triển của ngành công nghiệp Internet đã dẫn đến sự gia tăng to lớn về khối lượng thông tin của các dịch vụ trực tuyến [1], chẳng hạn như mạng xã hội và mua sắm trực tuyến. Trong tình huống này, Hệ thống Gợi ý (RS), phân phối các loại mục khác nhau để phù hợp với sở thích của người dùng, đã có những đóng góp đáng kể vào việc nâng cao trải nghiệm trực tuyến của người dùng trong nhiều lĩnh vực ứng dụng, chẳng hạn như gợi ý sản phẩm trong các nền tảng thương mại điện tử, gợi ý video ngắn trong các ứng dụng mạng xã hội [31,46].
Trong những năm gần đây, các nhà nghiên cứu đã đề xuất nhiều kỹ thuật cho gợi ý, bao gồm lọc cộng tác [35], các phương pháp dựa trên phân tích ma trận [23], gợi ý được hỗ trợ bởi học sâu [9,51], v.v. Mục tiêu chính của RS là tối ưu hóa một đối tượng gợi ý cụ thể, chẳng hạn như tỷ lệ click-through và tỷ lệ chuyển đổi người dùng. Tuy nhiên, người dùng thường có các hành vi tương tác khác nhau trên một mục duy nhất. Trong dịch vụ gợi ý video ngắn, chẳng hạn, người dùng thể hiện một loạt các chỉ số hành vi, chẳng hạn như nhấp chuột, thích và thời gian lưu trú liên tục [18]; trong khi đó trong các nền tảng thương mại điện tử, các nhà phát triển không chỉ tập trung vào các nhấp chuột của người dùng mà còn vào các giao dịch mua cuối cùng để đảm bảo lợi nhuận.
Tất cả những vấn đề tiềm ẩn này đã thúc đẩy sự phát triển của các kỹ thuật Học Đa nhiệm (MTL) cho hệ thống gợi ý trong cộng đồng nghiên cứu và công nghiệp [30, 46, 47].
Các mô hình gợi ý dựa trên MTL học nhiều nhiệm vụ gợi ý đồng thời bằng cách huấn luyện trong một biểu diễn chia sẻ

--- TRANG 2 ---
WWW '23, Ngày 1–5 tháng 5 năm 2023, Austin, TX, USA Liu, et al.
và chuyển giao thông tin giữa các nhiệm vụ [5], đã được phát triển cho một loạt các ứng dụng học máy, bao gồm thị giác máy tính [39], xử lý ngôn ngữ tự nhiên [15], dự đoán tỷ lệ click-through (CTR) và tỷ lệ click-through&chuyển đổi (CTCVR) [30]. Các hàm mục tiêu cho hầu hết các nghiên cứu MTL hiện có thường là các scalar hóa tuyến tính của các hàm tổn thất đa nhiệm [30,31,46], cố định trọng số với một hằng số. Hàm tổn thất đa mục tiêu theo mục này không có khả năng đảm bảo sự hội tụ của tối ưu toàn cục và thường mang lại hiệu suất dự đoán hạn chế. Mặt khác, ở cấp độ biểu diễn, đầu vào của hầu hết các mô hình MTL hiện có được giả định là các nhúng đặc trưng và tương tác người dùng-mục (được gọi là theo mục), mặc dù dữ liệu được tổ chức tuần tự (tức là đầu vào theo phiên) tương đối phổ biến hơn trong các ứng dụng RS thực tế. Ví dụ, các hành vi nhấp chuột và chuyển đổi của người dùng video ngắn thường xảy ra trong một phiên cụ thể, vì vậy đầu vào của họ cũng liên quan đến thời gian. Tuy nhiên, điều này sẽ làm giảm hiệu suất mô hình MTL, trong khi một số nhiệm vụ có thể có xung đột giữa nhãn theo phiên và theo mục [5]. Các mô hình MTL hiện có tập trung vào thiết kế cấu trúc mạng để cải thiện khả năng tổng quát hóa của mô hình, trong khi nghiên cứu về đề xuất một phương pháp mới tăng cường trọng số dự đoán đa nhiệm xem xét các mẫu theo phiên chưa nhận được sự chú ý đầy đủ.
Để giải quyết hai vấn đề nêu trên, chúng tôi đề xuất một khung gợi ý đa nhiệm được tăng cường bằng RL, RMTL, có khả năng kết hợp thuộc tính tuần tự của tương tác người dùng-mục vào gợi ý MTL và tự động cập nhật trọng số theo nhiệm vụ trong hàm tổn thất tổng thể. Các thuật toán Học Tăng cường (RL) gần đây đã được áp dụng trong nghiên cứu RS, mô hình hóa các hành vi người dùng tuần tự dưới dạng Quy trình Quyết định Markov (MDP) và sử dụng RL để tạo gợi ý tại mỗi bước quyết định [32,58]. Hệ thống gợi ý dựa trên RL có khả năng xử lý tương tác người dùng-mục tuần tự và tối ưu hóa sự tham gia của người dùng trong dài hạn [2]. Do đó, khung được tăng cường bằng RL của chúng tôi RMTL có thể chuyển đổi dữ liệu RS theo phiên thành cách thức MDP, và huấn luyện một khung actor-critic để tạo trọng số động cho việc tối ưu hóa hàm tổn thất MTL. Để đạt được đầu ra đa nhiệm, chúng tôi sử dụng mô hình MTL backbone hai tháp làm mạng actor, được tối ưu hóa bởi hai mạng critic riêng biệt cho mỗi nhiệm vụ. Trái ngược với các mô hình MTL hiện có với đầu vào theo mục và thiết kế trọng số hàm tổn thất cố định, mô hình RMTL của chúng tôi trích xuất các mẫu tuần tự từ đầu vào MDP theo phiên và cập nhật trọng số hàm tổn thất tự động cho mỗi lô dữ liệu. Trong bài báo này, chúng tôi tập trung vào dự đoán CTR/CTCVR, là một chỉ số quan trọng trong nền tảng thương mại điện tử và video ngắn [26]. Các thí nghiệm so với các mô hình gợi ý dựa trên MTL tiên tiến trên hai tập dữ liệu thực tế chứng minh hiệu quả của mô hình được đề xuất.
Chúng tôi tóm tắt các đóng góp của công trình như sau: (i) Vấn đề gợi ý đa nhiệm được chuyển đổi thành một sơ đồ học tăng cường actor-critic, có khả năng đạt được dự đoán đa nhiệm theo phiên; (ii) Chúng tôi đề xuất một khung học Đa nhiệm được tăng cường bằng RL RMTL, có thể tạo ra trọng số được điều chỉnh thích ứng cho thiết kế hàm tổn thất. RMTL tương thích với hầu hết các mô hình gợi ý dựa trên MTL hiện có; (iii) Các thí nghiệm mở rộng trên hai tập dữ liệu thực tế chứng minh hiệu suất vượt trội của RMTL so với các mô hình MTL SOTA, chúng tôi cũng xác minh khả năng chuyển giao của RMTL trên các mô hình MTL khác nhau.
2 KHUNG ĐỀ XUẤT
Phần này sẽ đưa ra mô tả chi tiết về phương pháp của chúng tôi, khung RMTL, giải quyết hiệu quả các nút thắt cổ chai của các nghiên cứu hiện có bằng cách thực hiện dự đoán đa nhiệm theo phiên với trọng số hàm tổn thất được điều chỉnh động.
2.1 Khái niệm Sơ bộ và Ký hiệu
Gợi ý Đa nhiệm theo Phiên. Chúng tôi lưu ý rằng việc lựa chọn hàm tổn thất như một kết hợp đa mục tiêu theo mục có thể thiếu khả năng trích xuất các mẫu tuần tự từ dữ liệu. Trong nghiên cứu của chúng tôi, chúng tôi đề xuất tổn thất đa mục tiêu theo phiên, tối ưu hóa mục tiêu bằng cách tối thiểu hóa tổn thất tích lũy có trọng số tại mỗi phiên. Cho tập dữ liệu dự đoán 𝐾-nhiệm vụ với
𝐷:=
𝑢𝑠𝑒𝑟𝑛,𝑖𝑡𝑒𝑚𝑛,(𝑦𝑛,1,...,𝑦𝑛,𝐾)	𝑁
𝑛=1, bao gồm 𝑁 bản ghi tương tác người dùng-mục, trong đó 𝑢𝑠𝑒𝑟𝑛 và 𝑖𝑡𝑒𝑚𝑛 có nghĩa là 𝑖𝑑 người dùng-mục thứ 𝑛. Mỗi bản ghi dữ liệu có 𝐾 nhãn nhị phân 0-1 𝑦1,···,𝑦𝐾 cho nhiệm vụ tương ứng. Chúng tôi định nghĩa phiên là {𝝉𝑚}𝑀
𝑚=1,
mỗi phiên 𝝉𝑚 chứa 𝑇𝑚 timestamp rời rạc khác nhau: 𝜏𝑚=
𝜏𝑚,1,···,𝜏𝑚,𝑡,···,𝜏𝑚,𝑇𝑚	
, trong đó 𝜏𝑚,𝑡 đại diện cho timestamp 𝑡 trong phiên 𝑚. Nhãn tương ứng được ký hiệu bởi
(𝑦𝑠
𝑡,1,...,𝑦𝑠
𝑡,𝐾)o
.
Hàm tổn thất theo phiên cho tất cả các phiên được tham số hóa bởi 𝜃𝑠
1,...,𝜃𝑠
𝐾 được định nghĩa là:
arg min
𝜃𝑠
1,...,𝜃𝑠
𝐾	L𝑠(𝜃𝑠
1,···,𝜃𝑠
𝐾)=𝐾∑︁
𝑘=1(𝑀∑︁
𝑚=1𝑇𝑚∑︁
𝑡=1𝜔𝑠
𝑘,𝜏𝑚,𝑡𝐿𝑠
𝑘,𝜏𝑚,𝑡(𝜃𝑠
𝑘))
𝑠.𝑡. 𝐿𝑠
𝑘,𝜏𝑚,𝑡(𝜃𝑠
𝑘)=−[𝑦𝑠
𝑘,𝜏𝑚,𝑡𝑙𝑜𝑔(𝑧𝑠
𝑘,𝜏𝑚,𝑡(𝜃𝑠
𝑘))+
(1−𝑦𝑠
𝑘,𝜏𝑚,𝑡)𝑙𝑜𝑔(1−𝑧𝑠
𝑘,𝜏𝑚,𝑡(𝜃𝑠
𝑘))](1)
trong đó L𝑠(𝜃𝑠
1,···,𝜃𝑠
𝐾) là hàm tổn thất tổng theo phiên, và 𝜔𝑠
𝑘,𝜏𝑚 là trọng số tương ứng cho tổn thất BCE, được điều chỉnh thích ứng bởi hệ thống. 𝑧𝑠
𝑛,𝑘(𝜃𝑠
𝑘) là giá trị dự đoán theo phiên tại điểm dữ liệu thứ 𝑛 cho nhiệm vụ 𝑘 được tham số hóa bởi 𝜃𝑠
𝑘. Để có được trọng số như vậy, chúng tôi tận dụng khung học tăng cường để ước lượng trọng số bằng cách sử dụng một số mạng critic động có thể điều chỉnh đầu ra của chúng tại các bước giải quyết khác nhau của hàm mục tiêu.
2.2 Tổng quan Khung
Chúng tôi trình bày khung của RMTL trong Hình 1 với một mạng biểu diễn trạng thái, một mạng actor và các mạng critic. Quá trình học của RMTL có thể được tóm tắt thành hai bước:
•Bước Tiến. Với các đặc trưng kết hợp người dùng-mục, mạng biểu diễn trạng thái tạo ra trạng thái 𝑠𝑡 dựa trên đặc trưng đầu vào tại timestamp 𝑡. Sau đó, chúng ta nhận được hành động (𝑎1,𝑡,𝑎2,𝑡) từ mạng actor trích xuất thông tin trạng thái. Giá trị hành động (𝑎1,𝑡,𝑎2,𝑡) và các đặc trưng kết hợp người dùng-mục được xử lý thêm bởi lớp MLP và lớp embedding làm đầu vào của mạng critic để tính giá trị Q từ mạng critic 𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘) cho mỗi nhiệm vụ 𝑘. Cuối cùng, hàm tổn thất tổng thể cho đa nhiệm L(𝜃1,𝜃2) có thể được ước lượng theo tổn thất BCE cho mỗi nhiệm vụ và trọng số thích ứng 𝜔𝑘,𝑡 được kiểm soát bởi 𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘).
•Bước Lùi. Trước tiên chúng tôi cập nhật các tham số mạng critic 𝜙𝑘 dựa trên lỗi TD 𝜎 và gradient của giá trị Q. Sau đó, chúng tôi tối ưu hóa các tham số của mạng actor 𝜃𝑘 đối với hàm tổn thất tổng thể.
2.3 Thiết kế Quy trình Quyết định Markov (MDP)
Chúng tôi chuyển đổi dữ liệu dự đoán đa nhiệm thành định dạng MDP (S,A,P,R,𝛾) để phân tích trong khung học tăng cường.
•Không gian trạng thái S là tập hợp trạng thái 𝑠𝑡∈R𝑑 chứa các đặc trưng kết hợp người dùng-mục, trong đó 𝑑 là kích thước đặc trưng kết hợp;
•Không gian hành động Trong thiết lập dự đoán CTR/CTCVR của chúng tôi, A là tập hợp các cặp hành động liên tục (𝑎1,𝑡,𝑎2,𝑡)∈[0,1]2 chứa các hành động cho hai nhiệm vụ dự đoán cụ thể, trong đó mỗi phần tử trong A biểu thị giá trị dự đoán cho CTR và CTCVR;
•Phần thưởng 𝑟𝑘,𝑡=𝑟(𝑎𝑘,𝑡,𝑦𝑘,𝑡)∈R,𝑘=1,2, là phản hồi từ môi trường liên quan đến các hành động hiện tại và cặp nhãn click/pay thực tế (𝑦1,𝑡,𝑦2,𝑡) của mục hiện tại. Để phù hợp với định nghĩa tổn thất BCE, chúng tôi định nghĩa hàm phần thưởng cho mỗi bước bằng cách sử dụng giá trị BCE âm, tức là,
𝑟𝑘,𝑡=𝑦𝑘,𝑡𝑙𝑜𝑔(𝑎𝑘,𝑡)+(1−𝑦𝑘,𝑡)𝑙𝑜𝑔(1−𝑎𝑘,𝑡), 𝑘=1,2(2)
•Chuyển tiếp P𝑠𝑎𝑠′=𝜒(𝑠′=𝑠𝑡+1) là xác suất chuyển tiếp từ trạng thái 𝑠 đến trạng thái 𝑠′ dựa trên chuỗi tương tác người dùng, và 𝜒 là hàm chỉ thị đặt xác suất là 1 khi trạng thái tiếp theo (mục) tương ứng với chuỗi;
•Chiết khấu 𝛾 tỷ lệ chiết khấu được đặt là 0,95 cho trường hợp của chúng tôi.
Mô hình RMTL học một chính sách tối ưu 𝜋(𝑠𝑡;𝜃∗) chuyển đổi đặc trưng mục-người dùng 𝑠𝑡∈S thành giá trị hành động liên tục (𝑎1,𝑡,𝑎2,𝑡)∈A để tối đa hóa tổng phần thưởng có trọng số:
max
{𝜃1,𝜃2}2∑︁
𝑘=1(𝑀∑︁
𝑚=1𝑇𝑚∑︁
𝑡=1𝜔𝑟
𝑘,𝜏𝑚,𝑡𝑅𝑘,𝜏𝑚,𝑡(𝜃𝑘))
𝑠.𝑡. 𝑅𝑘,𝜏𝑚,𝑡(𝜃𝑘)=𝑟(𝑎𝑘,𝜏𝑚,𝑡,𝑦𝑘,𝜏𝑚,𝑡)𝑘=1,2(3)
trong đó 𝜔𝑟
𝑘,𝜏𝑚,𝑡 là trọng số cho phần thưởng tại phiên 𝜏𝑚 trong timestamp 𝑡 cho nhiệm vụ 𝑘. Vì hàm phần thưởng được định nghĩa bởi Phương trình (2) là BCE âm, và giá trị hành động được tạo từ chính sách được tham số hóa bởi 𝜃𝑘. Bài toán tối ưu trên tương đương với việc tối thiểu hóa hàm tổn thất theo phiên trong Phương trình (1).
2.4 Xây dựng MDP Phiên
Dựa trên thiết kế MDP trong Phần 2.3, chúng tôi xây dựng MDP phiên cho việc huấn luyện RL, có thể cải thiện hiệu suất của mô hình MTL. Các phương pháp MTL cổ điển thường gặp thách thức trong việc đưa các hành vi người dùng tuần tự vào mô hình hóa, trong đó timestamp của các hành vi người dùng có tương quan cao. Học tăng cường được xây dựng dựa trên các chuỗi MDP có thể là một giải pháp để giải quyết vấn đề này. Tuy nhiên, thứ tự sắp xếp của MDP có thể có ảnh hưởng lớn đến hiệu suất của RL, vì việc ra quyết định của hành động tiếp theo phụ thuộc vào sự khác biệt thời gian (TD). Trong bài báo này, chúng tôi xây dựng MDP phiên, được tổ chức theo id phiên. Đối với mỗi phiên 𝜏𝑚, các bản ghi chuyển tiếp được phân tách bởi timestamp được lưu trữ trong tập dữ liệu gốc. Việc xây dựng này tạo ra các chuỗi MDP phiên được tổ chức bởi các hành vi người dùng tuần tự và có lợi thế là cập nhật trọng số tổn thất tổng thể.
Đối với nhiệm vụ dự đoán CTR/CTCVR, tức là 𝐾=2, giả sử chúng ta có tập dữ liệu theo phiên 𝐷𝑠={𝑆𝜏𝑚1𝜏𝑚,𝑈𝜏𝑚1𝜏𝑚,I𝜏𝑚,(y1,𝜏𝑚,y2,𝜏𝑚)}𝑀
𝑚=1 bao gồm 𝑀 bản ghi phiên, trong đó 𝑆𝜏𝑚 và 𝑈𝜏𝑚 là id phiên và người dùng thứ 𝑚. I𝜏𝑚 là vector id mục có kích thước bằng số mục đã tương tác với người dùng trong phiên thứ 𝑚. Mỗi bản ghi lưu trữ thông tin người dùng-mục x𝜏𝑚={𝑈𝜏𝑚1𝜏𝑚,I𝜏𝑚}2 và vector nhãn nhị phân 0-1 tương ứng (y1,𝜏𝑚,y2,𝜏𝑚) cho các chỉ thị click và convert. Sử dụng vector đặc trưng được tạo từ hàm ánh xạ 𝑓 để xử lý thêm bởi hàm trạng thái 𝐹, sau đó đầu vào vào chính sách agent 𝜋(𝑠;𝜃𝑘), là một mạng MTL được huấn luyện trước. Bộ đệm replay B chứa 𝑀 chuỗi MDP phiên {(𝑠𝜏𝑚,1,···,𝑠𝜏𝑚,𝑇𝑚)}𝑀
𝑚=1, mỗi chuỗi có 𝑇𝑚 thông tin chuyển tiếp dựa trên phiên 𝜏. Chi tiết xây dựng MDP phiên được thể hiện trong Thuật toán 1. Cụ thể, chúng tôi phân tách mỗi dữ liệu theo phiên dựa trên timestamp. Sau đó, lấy mẫu ngẫu nhiên một phiên và tạo thông tin trạng thái bởi các hàm 𝑓 và 𝐹 (dòng 2). Thông tin trạng thái của các bản ghi trong một phiên cụ thể được đầu vào tuần tự vào chính sách agent 𝜋(𝑠;𝜃𝑘), tạo ra các 21𝜏𝑚 là vector đơn vị có kích thước bằng số mục trong phiên thứ 𝑚.

--- TRANG 4 ---
WWW '23, Ngày 1–5 tháng 5 năm 2023, Austin, TX, USA Liu, et al.
Thuật toán 1 Thiết lập Dự đoán CTR/CTCVR theo Phiên
Đầu vào : Tập dữ liệu theo phiên được tổ chức như
{𝑆𝜏1:(x𝜏1,y1,𝜏1,y2,𝜏1),···,𝑆𝜏𝑀:(x𝜏𝑀,y1,𝜏𝑀,y2,𝜏𝑀)}
Tổ chức lại : Phân tách mỗi phiên với các cặp trạng thái-nhãn được ký hiệu là (𝑓(𝑥𝜏𝑚,𝑡),𝑦1,𝜏𝑚,𝑡,𝑦2,𝜏𝑚,𝑡), trong đó 𝑓 là hàm ánh xạ trả về vector đặc trưng cho bất kỳ id người dùng-mục nào được cho
1:while Môi trường chưa đóng do
2: Đặt lại : lấy mẫu ngẫu nhiên một phiên 𝝉𝑚 và khởi tạo trạng thái 𝑠1 với 𝐹(𝑓(𝑥1)), trong đó 𝐹 là hàm biểu diễn trạng thái.
3: while Phiên chưa kết thúc do
4: Đầu vào: Sử dụng giá trị hành động 𝑎1,𝑡,𝑎2,𝑡 từ chính sách agent 𝜋(𝑠;𝜃𝑘) làm giá trị dự đoán cho CTR/CTCVR
5: Bước: Tính phần thưởng 𝑟1,𝑡,𝑟2,𝑡 theo 2 với các hành động và nhãn, trả về trạng thái tiếp theo 𝑠𝑡+1. Lưu trữ chuyển tiếp (𝑠𝑡,𝑎1,𝑡,𝑎2,𝑡,𝑠𝑡+1,𝑟1,𝑡,𝑟2,𝑡) vào bộ đệm replay B
6: if 𝑡+1>𝑇𝑚 then
7: Kết thúc phiên 𝝉𝑚
8: end if
9: end while
10:end while
cặp hành động ước tính (𝑎1,𝑡,𝑎2,𝑡) (dòng 4). Sau đó, chúng tôi tính thêm giá trị phần thưởng theo Phương trình (2) và lưu trữ chuyển tiếp (𝑠𝑡,𝑎1,𝑡,𝑎2,𝑡,𝑠𝑡+1,𝑟1,𝑡,𝑟2,𝑡) tại timestamp 𝑡 vào bộ đệm replay B, cung cấp môi trường huấn luyện ổn định hơn (dòng 5). Điều này hoàn thành thiết lập dự đoán CTR/CTCVR theo phiên cho việc huấn luyện RL.
2.4.1 Mạng Biểu diễn Trạng thái. Các đặc trưng gốc của các cặp mục-người dùng là phân loại (được biểu diễn bằng định dạng mã hóa one-hot) hoặc số. Mạng biểu diễn trạng thái là sự kết hợp của lớp embedding và perceptron đa lớp để trích xuất các đặc trưng người dùng-mục, tương ứng với hàm ánh xạ 𝐹 trong Thuật toán 1. Các đặc trưng phân loại đầu tiên được chuyển đổi thành các vector nhị phân có độ dài 𝑙𝑘, và sau đó đầu vào vào lớp embedding 𝑅𝑙𝑘→𝑅𝑑�� với kích thước embedding 𝑑𝑒. Bên cạnh đó, các đặc trưng số được chuyển đổi thành cùng kích thước với 𝑑𝑒 bằng một phép biến đổi tuyến tính. Các đặc trưng được chuyển đổi bởi quá trình trên được kết hợp và sử dụng thêm làm đầu vào cho mạng MLP, là mạng nơ-ron hoàn toàn kết nối với nhiều lớp:
h𝑙+1=𝜎(W𝑙h𝑙+b𝑙)𝑛=0,1,...,𝑁−1
h𝐿=𝜎∗(W𝐿−1h𝐿−1+b𝐿−1)(4)
trong đó h𝑙 là lớp ẩn thứ 𝑙 với trọng số W𝑙 và bias b𝑙, hàm kích hoạt tương ứng 𝜎 là ReLU. Lớp đầu ra được ký hiệu bởi h𝐿 với hàm sigmoid 𝜎∗. Đầu ra h𝐿 được nhúng với đặc trưng được trích xuất và được biểu diễn bởi 𝐹(𝑓(𝑥𝑡)).
2.5 Khung RMTL
RMTL nhằm tối ưu hóa hàm tổn thất tổng thể với trọng số động thích ứng. Theo hướng này, các nghiên cứu hiện có [46] cố gắng thiết kế chiến lược cập nhật tỷ lệ cố định cho tổn thất của trọng số bằng:
𝜔′
𝑘,𝑡=𝜔′
𝑘,0𝛾′𝑡(5)
Cập nhật 
CTR/CTCVR
Mạng 
Actor 
Hiện tại
Tháp 
1
...
...
Tháp 
1
Tháp 
2
Tháp 
1
Tháp 
2
Mạng 
Actor 
Mục tiêu
Cập nhật 
Mềm
MLP
a1
a2
Lô 
MDP
MDP  
1
MDP  
i
MDP  
b
Y1
 
Y2
Nhãn
Tổn thất 
Tổng
Giá trị Q
Hành động
Hình 2: Cấu trúc Mạng Actor.
trong đó 𝜔′
𝑘,𝑡 là trọng số cho nhiệm vụ 𝑘 tại timestamp 𝑡, và 𝛾′ là tỷ lệ phân rã cố định. Tuy nhiên, trọng số này chỉ được kiểm soát bởi thời gian 𝑡 và không có tương tác với loại nhiệm vụ. Để giải quyết vấn đề này, chúng tôi áp dụng mạng critic để đánh giá tổng phần thưởng ước tính 𝑉𝑘 của hành động 𝑎𝑘 được tạo từ chính sách agent cho mỗi nhiệm vụ. Sau đó, giá trị trọng số hàm tổn thất cho mỗi nhiệm vụ được tính như phép biến đổi tuyến tính với biến số polish 𝜆. Thiết kế tổn thất tổng thể mới này có thể tối ưu hóa tham số mô hình MTL theo hướng tốt hơn, tăng cường hiệu quả và hiệu suất dự đoán.
Quá trình huấn luyện dự đoán CTR/CTCVR dựa trên học tăng cường bao gồm hai thành phần chính: mạng actor và mạng critic cụ thể theo nhiệm vụ. Mạng actor là cấu trúc chính để dự đoán CTR/CTCVR trong nghiên cứu của chúng tôi, có thể được coi như chính sách agent 𝜋(𝑠𝑡;𝜃𝑘) được tham số hóa bởi 𝜃𝑘. Dựa trên mạng actor này, agent có thể chọn một hành động cho mỗi trạng thái. Mạng critic được ký hiệu bởi 𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘) là mạng giá trị hành động có thể vi phân được cập nhật bởi gradient của giá trị Q và lỗi TD 𝛿, có thể quan sát phần thưởng tiềm năng của trạng thái hiện tại bằng cách học mối quan hệ giữa môi trường và phần thưởng. Bên cạnh đó, nó tạo ra trọng số được điều chỉnh thích ứng của tổn thất BCE cho hàm mục tiêu và cập nhật tham số mạng actor 𝜃𝑘 theo hướng quyết định chính sách tốt hơn.
2.5.1 Mạng Actor. Mạng actor trong thiết lập RL có thể được gọi là một chính sách được biểu diễn bởi phân phối xác suất 𝜋(𝑠;𝜃𝑘)=P(𝑎|𝑠,𝜃𝑘)b. Trong thực tế, phương pháp này thường gặp hai vấn đề: (i) Mạng actor critic hoạt động kém trong thiết lập với không gian hành động lớn, và (ii) chính sách agent ước tính được huấn luyện bởi giá trị hành động kỳ vọng của chính nó mà không phải là dữ liệu thực tế. Trong phần này, chúng tôi giải quyết hai vấn đề trên bằng cách áp dụng một mô hình MTL cụ thể làm chính sách agent với đầu ra hành động xác định, làm giảm khả năng không gian hành động. Ngoài ra, chúng tôi cập nhật các tham số mạng nơ-ron dựa trên mạng bằng cách sử dụng nhãn nhiệm vụ thực tế. Cấu trúc mạng actor được thể hiện trong Hình 2, có cấu trúc tương tự như các mô hình MTL cụ thể, chúng tôi có thể chỉ định nó bằng mô hình ESMM trong phần phụ này. Lớp shared-bottom trong thiết kế mạng actor gốc được loại bỏ và đặt làm mạng biểu diễn trạng thái được đề cập trong Phần phụ 2.4.1 để tạo bộ đệm replay B. Với một lô chuỗi MDP từ bộ đệm replay B, chúng tôi đầu vào thông tin trạng thái 𝑠𝑡 vào mạng actor, là hai mạng nơ-ron song song được ký hiệu bởi hai lớp tháp được tham số hóa bởi 𝜃1

--- TRANG 5 ---
Multi-Task Recommendations with Reinforcement Learning WWW '23, Ngày 1–5 tháng 5 năm 2023, Austin, TX, USA
và 𝜃2,
𝜋(𝑠𝑡;𝜃𝑘)=𝑎𝑘,𝑡, 𝑘 =1,2 (6)
Đầu ra của mỗi lớp tháp là giá trị hành động xác định, biểu thị dự đoán cho nhiệm vụ cụ thể. Trong thiết lập của chúng tôi, một tháp xuất ra giá trị dự đoán CTR, và tháp kia xuất ra giá trị dự đoán CTCVR. Sau quá trình huấn luyện cho lô chuỗi MDP, chúng tôi tính hàm tổn thất tổng thể dựa trên tổng có trọng số của tổn thất BCE:
L(𝜃1,𝜃2)=∑︁
(𝑠𝑡,···,𝑠𝑡+1,···)∈𝑏2∑︁
𝑘=1𝜔𝑘,𝑡𝐵𝐶𝐸(𝜋(𝑠𝑡;𝜃𝑘),𝑦𝑘,𝑡)) (7)
Hàm tổn thất tổng thể có thể vi phân này L(𝜃1,𝜃2) được kiểm soát bởi entropy chéo nhị phân giữa hành động ước tính và nhãn nhiệm vụ thực 𝑦𝑘,𝑡, tăng cường độ chính xác quyết định của chính sách.
2.5.2 Mạng Critic. Mạng critic truyền thống ước tính giá trị hành động 𝑄 được tạo từ mạng actor và sau đó cập nhật các tham số mạng actor dựa trên giá trị 𝑄. Trong khi chúng tôi chọn mô hình MTL làm mạng actor, vấn đề là làm thế nào để thiết kế cấu trúc mạng critic phù hợp cho việc cập nhật đa tham số. Trong bài báo này, chúng tôi đề xuất cấu trúc đa critic với hai mạng MLP song song chia sẻ một lớp đáy. Thiết kế này có khả năng cập nhật các tham số mô hình MTL trong mạng actor và polish trọng số hàm tổn thất cho các nhiệm vụ cụ thể. Cấu trúc mạng critic được thể hiện trong Hình 3. Phần đầu tiên của mạng critic là một lớp shared-bottom, chuyển đổi đồng thời thông tin đặc trưng người dùng-mục và thông tin hành động. Tương tự như mạng biểu diễn trạng thái trong Phần phụ 2.4.1, chúng tôi áp dụng một lớp embedding và cấu trúc MLP cho việc trích xuất đặc trưng. Sau đó chúng tôi kết hợp đặc trưng người dùng-mục và thông tin hành động làm đầu vào của hai mạng giá trị hành động có thể vi phân được tham số hóa bởi 𝜙𝑘, xuất ra giá trị Q ước tính dựa trên thông tin trạng thái-hành động cho mỗi nhiệm vụ. Với trạng thái hiện tại 𝑠𝑡 và hành động 𝑎𝑘,𝑡, giá trị Q được tính bởi:
𝑄′(𝑠𝑡,𝑎𝑘,𝑡)=E[𝑟𝑘,𝑡+𝛾𝑉(𝑠𝑡+1)|𝑠𝑡,𝑎𝑘,𝑡] (8)
=𝑟𝑘,𝑡+𝛾∑︁
𝑠𝑡+1∈S𝑝𝑠𝑡,𝑎𝑡,𝑠𝑡+1·max𝑄′(𝑠𝑡+1,𝑎𝑘,𝑡+1)
trong đó 𝑉(𝑠𝑡+1) là hàm giá trị trạng thái. Trong trường hợp của chúng tôi, giá trị hành động 𝑎𝑡,𝑘 cho nhiệm vụ 𝑘 được ước tính bởi mạng actor, và trạng thái tiếp theo 𝑠𝑡+1 được xác định với xác suất bằng 1. Do đó, hàm giá trị Q trong cấu trúc đa critic có thể được tính như:
𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘)=𝑟𝑘,𝑡+𝛾𝑄(𝑠𝑡+1,𝑎𝑘,𝑡+1;𝜙𝑘) (9)
Trọng số của hàm tổn thất mục tiêu trong Phương trình (1) được điều chỉnh ngược theo hướng giá trị Q để cải thiện quá trình tối ưu hóa mạng actor, là phép biến đổi tuyến tính với biến punish 𝜆:
𝜔𝑘,𝑡=1−𝜆∗𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘) (10)
trong đó 𝑎𝑘,𝑡 là hành động cho nhiệm vụ 𝑘 tại timestamp 𝑡. 𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘) là giá trị hành động của trạng thái 𝑠𝑡 và hành động 𝑎𝑘,𝑡.
2.6 Tối ưu hóa
Khung chung của mạng actor-critic thường gặp vấn đề không thể tránh khỏi: sự hội tụ của mạng critic không được đảm bảo. Để khắc phục vấn đề này, chúng tôi tận dụng ý tưởng
s
Đặc trưng 
phân loại
s
Đặc trưng 
số
Nhúng 
Đặc trưng
s
Hành động
Đầu vào 
Critic 
Tháp 
1
Mạng 
Critic 
Hiện tại
Mạng 
Critic 
Mục tiêu
Cập nhật 
mềm
Critic-1
Critic-2
Critic-1
Critic-2
Lỗi 
TD
 
Lỗi 
TD
 
Q1'
Q1
Phần thưởng1
Q2
Cập nhật
Cập nhật
Vector 
Giá trị Q
Trích xuất 
Đặc trưng
Đầu vào 
Đặc trưng
Phần thưởng2
 
Q2'
Hình 3: Cấu trúc Mạng Critic.
của Thuật toán Gradient Chính sách Xác định [41], đưa mạng mục tiêu vào khung học. Các mạng mục tiêu có cấu trúc hoàn toàn giống như các mạng actor và critic được đề xuất (tức là mạng ước tính), và chúng tôi ký hiệu chúng là 𝜋(𝑠𝑡;e𝜃𝑘) và 𝑄(𝑠𝑡,𝑎𝑘,𝑡;e𝜙k), có các tham số lag e𝜃𝑘 và e𝜙𝑘. Trong phần còn lại của bài báo này, chúng tôi chỉ định mạng actor-critic chính bằng các mạng actor ước tính được tham số hóa bởi 𝜃𝑘 và các mạng critic ước tính được tham số hóa bởi 𝜙𝑘. Mạng actor-critic truyền thống thường gặp vấn đề phân kỳ tham số và sự hội tụ của mạng critic không được đảm bảo. Mạng mục tiêu có thể giải quyết các vấn đề này bằng cách tạo ra các giá trị mục tiêu cố định từ các mạng nơ-ron được tham số hóa ghi nhật ký, làm mượt quá trình huấn luyện. Tiếp theo, chúng tôi trình bày chi tiết quá trình tối ưu hóa cho các tham số mô hình của mạng actor-critic ước tính, và cũng là cập nhật mềm cho các mạng mục tiêu.
Cập nhật mạng critic ước tính. 𝜙𝑘 là tham số quan trọng trong mạng critic, xác định giá trị Q hành động. Với chuyển tiếp (𝑠𝑡,𝑎1,𝑡,𝑎2,𝑡,𝑠𝑡+1,𝑟1,𝑡,𝑟2,𝑡) từ bộ đệm replay B, mục tiêu TD của mạng critic mục tiêu cho nhiệm vụ thứ 𝑘 được dẫn xuất từ:
𝑇𝐷𝑘,𝑡=𝑟𝑘,𝑡+𝛾𝑄(𝑠𝑡+1,𝑎𝑘,𝑡+1;e𝜙𝑘) (11)
trong đó 𝑎𝑘,𝑡+1=𝜋(𝑠𝑡+1;e𝜃𝑘) là hành động tiếp theo ước tính từ mạng actor mục tiêu. Giá trị Q được tạo từ mạng critic ước tính, ước tính giá trị hành động hiện tại được định nghĩa là:
𝑄𝑘,𝑡=𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘) (12)
Sau quá trình huấn luyện trong 𝑏 lô chuyển tiếp từ bộ đệm replay, chúng tôi tính lỗi TD trung bình 𝛿:
𝛿=1
2|𝑏|∑︁
(𝑠𝑡,···,𝑠𝑡+1,···)∈𝑏2∑︁
𝑘=1(𝑇𝐷𝑘,𝑡−𝑄𝑘,𝑡) (13)
=1
2|𝑏|∑︁
(𝑠𝑡,···,𝑠𝑡+1,···)∈𝑏2∑︁
𝑘=1
[𝑟𝑘,𝑡+𝛾𝑄(𝑠𝑡+1,𝑎𝑘,𝑡+1;e𝜙𝑘)−𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘)]

--- TRANG 6 ---
WWW '23, Ngày 1–5 tháng 5 năm 2023, Austin, TX, USA Liu, et al.
Sau đó chúng tôi cập nhật mạng critic hiện tại cho mỗi nhiệm vụ bằng phương pháp gradient descent sau:
𝜙𝑡+1
𝑘=𝜙𝑡
𝑝−𝛼𝜙I𝛿∇𝜙k𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘) (14)
trong đó ∇𝜙k𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘) là gradient của giá trị Q mục tiêu thứ 𝑘. Điều này hoàn thành việc tối ưu hóa các mạng critic ước tính.
Cập nhật mạng actor ước tính. Trước khi lỗi TD 𝛿 hội tụ đến ngưỡng 𝜖, chúng tôi cập nhật các mạng actor hiện tại được tham số hóa bởi 𝜃𝑘 thông qua lan truyền ngược gradient của hàm tổn thất cho mỗi lớp tháp sau quá trình tiến của mỗi lô chuyển tiếp từ B:
𝜃𝑡+1
𝑘=𝜃𝑡
𝑘+𝛼𝜃I∇𝜃𝑘J(𝜃𝑘) (15)
trong đó tổn thất cho các lớp tháp được định nghĩa bởi giá trị âm của giá trị Q trung bình J(𝜃𝑘)=−1
𝑏Í
(𝑠𝑡,···,𝑠𝑡+1,···)∈𝑏𝑄′(𝑠𝑡,𝜋(𝑠𝑡;𝜃𝑘)). Chúng tôi cập nhật thêm các mạng actor hiện tại đối với gradient của hàm tổn thất tổng thể của dự đoán CTR/CTCVR:
𝜃𝑡+1
𝑘=𝜃𝑡
𝑘−𝛼𝜃I∇𝜃𝑘L(𝜃1,𝜃2) (16)
L(𝜃1,𝜃2)=∑︁
(𝑠𝑡,···,𝑠𝑡+1,···)∈𝑏2∑︁
𝑘=1𝜔𝑘,𝑡𝐵𝐶𝐸(𝜋(𝑠𝑡;𝜃𝑘),𝑦𝑘,𝑡)(17)
trong đó trọng số 𝜔𝑘,𝑡=1−𝜆∗𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘) được kiểm soát bởi mạng critic hiện tại tương ứng, được điều chỉnh thích ứng theo hướng âm của giá trị Q hành động ước tính tại 𝑡.
Cập nhật mềm mạng mục tiêu. Mạng actor mục tiêu và hai mạng critic mục tiêu cụ thể được cập nhật cho đến khi mạng critic hiện tại đạt điều kiện hội tụ theo hướng tham số trong các mạng hiện tại:
e𝜃𝑘=𝛽e𝜃𝑘+(1−𝛽)𝜃𝑘
e𝜙𝑘=𝛽e𝜙𝑘+(1−𝛽)𝜙𝑘(18)
trong đó 𝛽∈[0,1] là tỷ lệ cập nhật mềm.
3 THÍ NGHIỆM
Trong phần này, chúng tôi tiến hành một số thí nghiệm sử dụng hai tập dữ liệu thực tế để đánh giá hiệu quả của khung RMTL.
3.1 Thiết lập Thí nghiệm
3.1.1 Tập dữ liệu. Từ khảo sát của chúng tôi, chỉ có một tập dữ liệu mã nguồn mở, RetailRocket3, chứa các nhãn tuần tự của click và pay. Để có so sánh toàn diện về RMTL, chúng tôi cũng áp dụng tập dữ liệu "Kuairand-1K"4[14] để phân tích thêm. Mỗi tập dữ liệu được chia thành tập huấn luyện, xác thực và kiểm tra với tỷ lệ 6:2:2 được sắp xếp theo timestamp 𝑡 cho việc học mô hình.
3.1.2 Baseline. Vì cấu trúc RMTL của chúng tôi sửa đổi hàm tổn thất mục tiêu MTL, chúng tôi chọn các mô hình MTL với tổn thất mặc định và một mô hình dựa trên RL làm baseline. Single Task [29]: Phương pháp ngây thơ để học mỗi nhiệm vụ riêng biệt, được sử dụng rộng rãi để so sánh hiệu suất của các mô hình MTL. Shared Bottom [30]: Cấu trúc cơ bản của các mô hình MTL với lớp đáy chung. ESMM [31]: Một mô hình được chỉ định trong dự đoán CTR/CVR tập trung vào các không gian mẫu khác nhau cho mỗi nhiệm vụ. MMoE [30]:
3https://www.kaggle.com/datasets/retailrocket/ecommerce-dataset
4https://kuairand.com/MMoE sử dụng gates để kiểm soát mối quan hệ giữa đáy chung và các tháp cụ thể theo nhiệm vụ. PLE [46]: PLE có thể nắm bắt các tương quan nhiệm vụ phức tạp bằng cách sử dụng các chuyên gia chung cũng như các lớp chuyên gia cụ thể theo nhiệm vụ. D-PLE [41]: Chúng tôi áp dụng DDPG (deep deterministic policy gradient) với mô hình PLE làm baseline dựa trên RL. Thực tế, tất cả các hàm tổn thất cho các mô hình baseline được có được bằng cách lấy trung bình có trọng số không thể thay đổi.
3.1.3 Chỉ số Đánh giá.
•AUC và Logloss : Đây là những chỉ số đơn giản để đánh giá các nhiệm vụ dự đoán CTR/CTCVR. Theo [16], có ý nghĩa thống kê trong các nhiệm vụ gợi ý rằng AUC hoặc Logloss thay đổi hơn mức 0.001.
•s-Logloss : Chỉ số này được định nghĩa là Logloss trung bình đối với tất cả các phiên.
3.2 Chi tiết Triển khai
Chúng tôi triển khai các mô hình baseline bằng thư viện công khai5, tích hợp 7 mô hình MTL tiêu chuẩn (không có ESMM) và đưa ra hiệu suất tham chiếu của mỗi mô hình. Tất cả các mô hình có cùng cấu trúc mạng cơ bản, tức là một lớp embedding đầu vào với kích thước 128, một Multi-Layer Perceptron (MLP) 128×512×256 làm lớp đáy, và một MLP 256×128×64×1 làm lớp tháp. Cụ thể, đối với các mô hình sử dụng các lớp chuyên gia (cùng cấu trúc với lớp đáy), chúng tôi cố định số lượng chuyên gia là 8. Hàm kích hoạt cho các lớp ẩn là ReLU và Sigmoid cho đầu ra. Chúng tôi cũng đặt tỷ lệ dropout 0.2. Chúng tôi sử dụng trình tối ưu Adam (𝑙𝑟=1𝑒−3) để học tất cả các mô hình. Chúng tôi cũng triển khai ESMM chia sẻ các siêu tham số của các mô hình trên.
Vì mạng actor trong mô hình RMTL của chúng tôi là thiết kế hai tháp với đầu ra đa nhiệm, nó tương thích với hầu hết các mô hình gợi ý dựa trên MTL hiện có. Trong bài báo này, chúng tôi áp dụng cấu trúc RMTL của chúng tôi trên ba mô hình MTL đại diện: ESMM, MMoE và PLE. Đối với cấu trúc RMTL, chúng tôi đặt mạng Actor với cùng cấu trúc như các mô hình MTL được chỉ định. Các mạng Critic cũng chia sẻ cùng lớp đáy và có đầu ra thông qua một lớp tháp. Hơn nữa, chúng tôi thêm một lớp hành động 1×128 để phù hợp với các đặc trưng đầu vào, và chúng tôi thay đổi kích hoạt đầu ra thành ReLU âm vì phần thưởng của chúng tôi chỉ có giá trị âm. Để cải thiện hiệu quả dữ liệu, chúng tôi khởi tạo Actor với các tham số được huấn luyện trước từ các mô hình MTL và giữ chúng đông cứng cho đến khi các Critics hội tụ. Sau đó chúng tôi nhân giá trị critic trong tổn thất tổng và huấn luyện lại mô hình MTL. Tỷ lệ học mặc định của actor 𝛼𝜃 và mạng critic 𝛼𝜙 được đặt là 0.001. Chúng tôi đặt tỷ lệ cập nhật mềm mặc định 𝛽=0.2, biến punish 𝜆=0.7. Mã triển khai có sẵn trực tuyến để dễ dàng tái tạo6.
3.3 Hiệu suất Tổng thể và So sánh
Chúng tôi so sánh hiệu suất của năm mô hình MTL baseline với các mô hình RMTL cho các nhiệm vụ dự đoán CTR/CTCVR trên hai tập dữ liệu khác nhau. Hiệu suất tổng thể trên các nhiệm vụ CTR/CTCVR cho các phương pháp khác nhau được thể hiện trong Bảng 1. Có thể quan sát thấy rằng:
5https://github.com/easezyc/Multitask-Recommendation-Library.git
6https://github.com/Applied-Machine-Learning-Lab/RMTL

--- TRANG 7 ---
Multi-Task Recommendations with Reinforcement Learning WWW '23, Ngày 1–5 tháng 5 năm 2023, Austin, TX, USA
Bảng 1: Hiệu suất trên các nhiệm vụ CTR/CTCVR cho các phương pháp khác nhau.
Tập dữ liệu Nhiệm vụ Chỉ số Phương pháp
Single Shared ESMM MMoE PLE D-PLE RMTL- RMTL- RMTL-
Task Bottom ESMM MMoE PLE
RetailRocket CTRAUC↑ 0.7273 0.7287 0.7282 0.7309 0.7308 0.7308 0.7338* 0.7350* 0.7339*
Logloss↓ 0.2065 0.2048 0.2031 0.2021 0.2056 0.2058 0.2024 0.1995 0.2013
s-Logloss↓0.0846 0.0839 0.0852 0.0853 0.0827 0.0830 0.0836 0.0848 0.0824
RetailRocket CTCVRAUC↑ 0.7250 0.7304 0.7316 0.7347 0.7387 0.7386 0.7341 0.7396 0.7419*
Logloss↓ 0.0489 0.0493 0.0486 0.0496 0.0486 0.0490 0.0485 0.0490 0.0480
s-Logloss↓0.0150 0.0149 0.0150 0.0145 0.0147 0.0149 0.0150 0.0143 0.0146
Kuairand CTRAUC↑ 0.7003 0.7018 0.7009 0.7014 0.7026 0.7025 0.7031 0.7029 0.7053*
Logloss↓ 0.6127 0.6114 0.6128 0.6119 0.6111 0.6123 0.6111 0.6105 0.6092*
s-Logloss↓0.6263 0.6250 0.6261 0.6255 0.6252 0.6257 0.6252 0.6243 0.6242
Kuairand CTCVRAUC↑ 0.7342 0.7310 0.7350 0.7324 0.7339 0.7339 0.7377* 0.7345 0.7367*
Logloss↓ 0.5233 0.5249 0.5220 0.5237 0.5235 0.5237 0.5200* 0.5225 0.5221
s-Logloss↓0.5449 0.5468 0.5436 0.5450 0.5454 0.5451 0.5433 0.5446 0.5447
↑: càng cao càng tốt;↓: càng thấp càng tốt. Gạch dưới : mô hình baseline tốt nhất. Đậm : hiệu suất tốt nhất trong tất cả các mô hình.
"*": cải thiện có ý nghĩa thống kê (tức là kiểm định t hai phía với 𝑝<0.05) so với baseline tốt nhất.
•Mô hình SingleTask đạt hiệu suất tệ nhất trong cả hai nhiệm vụ dự đoán trên hai tập dữ liệu. Mạng biểu diễn đặc trưng và tối ưu hóa hàm tổn thất được tách biệt cho mỗi nhiệm vụ, không thể phát hiện mối quan hệ nội tại giữa nhiều nhiệm vụ. Do đó, chúng ta có thể hiểu rằng phương pháp shared-bottom, chia sẻ các tham số trích xuất đặc trưng, có thể vượt trội hơn phương pháp SingleTask cho cả hai tập dữ liệu.
•Mô hình PLE đạt hiệu suất gần như tốt nhất trong tất cả các mô hình MTL baseline trong hầu hết các trường hợp. Trên cơ sở mô hình MMoE, kiểm soát tương tác tham số giữa đáy chung và tháp nhiệm vụ cụ thể, mô hình PLE chỉ định việc chia sẻ tham số giữa các lớp chuyên gia để trích xuất các mẫu tương tác ẩn giữa mỗi nhiệm vụ. Điều này chứng minh rằng mô hình baseline PLE có thể cải thiện hiệu quả chia sẻ thông tin giữa các nhiệm vụ để đạt được hiệu suất dự đoán tốt hơn.
•Mỗi phiên bản của mô hình RMTL được đề xuất của chúng tôi vượt trội hơn mô hình baseline phiên bản không-RL tương ứng (ví dụ: RMTL-ESMM so với ESMM) trong cả hai nhiệm vụ dự đoán với các chỉ số AUC và logloss trên hai tập dữ liệu. Đặc biệt trên tập dữ liệu RetialRocket, mô hình RMTL đạt được khoảng 0.003-0.005 cải thiện AUC so với mô hình baseline tương ứng. Bằng cách tận dụng thuộc tính tuần tự của khung học tăng cường, phương pháp được tăng cường bằng RL có khả năng xử lý dữ liệu gợi ý theo phiên và đạt được cải thiện đáng kể trong các nhiệm vụ dự đoán CTR/CTCVR bằng trọng số hàm tổn thất được điều chỉnh thích ứng.
•Kết quả dự đoán CTCVR trên tập dữ liệu Kuairand được trình bày trong hàng thứ tư của Bảng 1, trong đó mô hình baseline ESMM và phiên bản RL tiên tiến của nó đạt hiệu suất tốt hơn các mô hình MTL khác. Đối với gợi ý video ngắn, các hành vi click-and-convert thường có tương quan tương đối cao hơn. Trong hiện tượng này, mô hình ESMM vượt trội hơn MMoE và PLE bằng cách khắc phục bias lựa chọn mẫu.
•Các mô hình RMTL đạt được cải thiện s-Logloss dưới 0.001 so với các mô hình baseline tương ứng, thấp hơn so với Logloss. Hiện tượng này có thể do chỉ số dựa trên phiên có hiệu suất tương tự với kiểm thử A/B trực tuyến, khó cải thiện hơn [19]. Lưu ý rằng tỷ lệ logloss và s-logloss khác nhau cho hai tập dữ liệu vì chúng có độ dài phiên trung bình khác nhau trong khi Logloss phiên trung bình bị ảnh hưởng bởi độ dài phiên trung bình cho một tập dữ liệu cụ thể.
Tóm lại, mô hình RMTL vượt trội hơn các mô hình MTL tiên tiến trong cả hai nhiệm vụ dự đoán CTR và CTCVR cho các tập dữ liệu thực tế khác nhau. Ngoài ra, nó có thể được sử dụng trong hầu hết các mô hình MTL, xác thực khả năng tương thích mong muốn của nó.
3.4 Nghiên cứu Khả năng Chuyển giao
Phần phụ này trình bày nghiên cứu khả năng chuyển giao cho phương pháp RMTL cho nhiệm vụ CTR trên tập dữ liệu RetailRocket. Chúng tôi cố gắng tìm hiểu xem mạng critic được học từ các chính sách ghi nhật ký khác nhau có thể được áp dụng cho cùng mô hình MTL baseline và cải thiện hiệu suất dự đoán hay không. Đối với mỗi mô hình MTL baseline, chúng tôi tận dụng các tham số mô hình từ mạng critic được huấn luyện trước bởi mô hình ESMM, MMoE và PLE trên tập dữ liệu RetailRocket. Kết quả AUC và logloss được trình bày trong Hình 4 (a)-(c) và (d)-(f), trong đó "mmoe-ESMM" có nghĩa là mô hình ESMM áp dụng mạng critic được huấn luyện từ MMoE và "ple-ESMM" có nghĩa là mô hình ESMM áp dụng mạng critic được huấn luyện từ PLE. Có thể quan sát thấy rằng:
(i) Mạng critic được huấn luyện trước từ ba mô hình MTL có thể tăng đáng kể AUC cho mỗi mô hình baseline. (ii) Mạng critic được huấn luyện trước từ ba mô hình MTL có thể giảm đáng kể logloss cho mỗi mô hình baseline.
Tóm lại, mạng critic được huấn luyện trước có khả năng cải thiện hiệu suất dự đoán của hầu hết các mô hình MTL, chứng minh khả năng chuyển giao xuất sắc của mô hình RMTL.
3.5 Nghiên cứu Loại bỏ
Thiết kế quan trọng nhất của mô hình RMTL là hàm tổn thất tổng thể với trọng số có thể học, là sự kết hợp tuyến tính của giá trị Q từ mạng critic. Trong phần phụ này, để tìm hiểu tầm quan trọng của hàm tổn thất này trong mô hình được đề xuất, chúng tôi thay đổi một số thành phần và định nghĩa ba biến thể dưới đây: (i) CW: Biến thể CW chỉ ra việc áp dụng trọng số cố định cho hàm tổn thất tổng thể và không có cập nhật chính sách gradient cho mạng actor, loại bỏ đóng góp của mạng critic. Chúng tôi gán tất cả trọng số bằng hằng số 1. (ii) WL: Biến thể WL chỉ ra việc điều chỉnh trọng số 𝜔 được kiểm soát bởi nhãn hành vi phiên 𝑦𝑘 nhân với giá trị Q. (iii) NLC: Không thực hiện phép biến đổi tuyến tính đối với trọng số tổn thất, thay vào đó, chúng tôi trực tiếp gán giá trị Q âm cho trọng số tổn thất.
Kết quả nghiên cứu loại bỏ cho mô hình PLE trên tập dữ liệu RetailRocket được thể hiện trong Bảng 2. Có thể quan sát thấy rằng: (i) CW có hiệu suất tệ nhất cho các chỉ số AUC và logloss trên cả hai nhiệm vụ dự đoán. Điều này có thể do trọng số tổn thất BCE được gán tương đương, không thể nắm bắt động lực học ẩn của đa nhiệm vụ. (ii) WL và NLC có hiệu suất gần như giống nhau trong nghiên cứu này, vượt trội hơn biến thể CW với cải thiện AUC 0.002-0.003. Điều này chứng minh tác động của việc cập nhật trọng số từ mạng critic. (iii) RMTL-PLE với thiết lập tổn thất tổng được đề xuất của chúng tôi đạt hiệu suất tốt nhất trên cả hai nhiệm vụ, minh họa tính hiệu quả của thiết kế trọng số kết hợp tuyến tính của chúng tôi. Chúng tôi có thể tóm tắt rằng mô hình MTL được đề xuất của chúng tôi có thể đạt được hiệu suất dự đoán CTR/CTCVR tốt hơn, trong đó thiết kế tổn thất tổng có đóng góp lớn.
4 NGHIÊN CỨU LIÊN QUAN
Trong phần này, chúng tôi đưa ra giới thiệu ngắn gọn về các nghiên cứu hiện có liên quan đến RMTL, tức là hệ thống gợi ý dựa trên RL và dựa trên MTL.
Hệ thống Gợi ý dựa trên Học Tăng cường. Nhiều nghiên cứu đã được thực hiện kết hợp Học Tăng cường (RL) [2,43,48,50,53,55,57] vào lĩnh vực Hệ thống Gợi ý (RS). So với các giải pháp learning-to-rank truyền thống của RS [27] tối ưu hóa phản hồi người dùng tức thì, RS dựa trên RL tập trung vào tối ưu hóa hàm phần thưởng tích lũy ước tính nhiều vòng tương tác giữa chính sách gợi ý và môi trường phản hồi người dùng. Vấn đề chung công thức hóa các tương tác người dùng-hệ thống tuần tự như một Quy trình Quyết định Markov (MDP) [40]. Các giải pháp RL đã được nghiên cứu dưới công thức này bao gồm các phương pháp dựa trên bảng thời kỳ đầu [22,32,34] tối ưu hóa bảng đánh giá cho một tập hợp các cặp trạng thái-hành động, các phương pháp dựa trên giá trị [20,45,56,59] học đánh giá chất lượng của một hành động hoặc một trạng thái, các phương pháp gradient chính sách [6,7,42] tối ưu hóa chính sách gợi ý dựa trên phần thưởng dài hạn, và các phương pháp actor-critic [24] dựa trên phương pháp gradient chính sách [4,10,38,44] đồng thời học một bộ đánh giá hành động và một bộ tạo hành động. Phương pháp của chúng tôi thuộc về danh mục actor-critic và mở rộng nó hướng tới thiết lập vấn đề đa nhiệm vụ.
Các thách thức chính của việc áp dụng RL cho nhiệm vụ gợi ý bao gồm không gian trạng thái/hành động tổ hợp lớn [11,25], sự không chắc chắn của môi trường người dùng [21,54], tính ổn định và hiệu quả của khám phá [3,8], và thiết kế hàm phần thưởng của người dùng đối với các hành vi không đồng nhất [60]. Nghiên cứu của chúng tôi liên quan đến thiết kế hàm phần thưởng nhưng cũng tăng cường hiệu suất của mỗi nhiệm vụ.
Học Đa nhiệm trong Hệ thống Gợi ý. Như đã nêu trong [52], Học Đa nhiệm (MTL) là một khung học máy học biểu diễn bất biến nhiệm vụ của dữ liệu đầu vào trong mạng đáy, trong khi mỗi nhiệm vụ cá nhân được giải quyết trong mạng cụ thể theo nhiệm vụ tương ứng và được tăng cường bởi việc chuyển giao kiến thức giữa các nhiệm vụ. Gần đây, MTL đã nhận được sự quan tâm ngày càng tăng trong hệ thống gợi ý [17,28,31,36,37] do khả năng chia sẻ kiến thức giữa các nhiệm vụ khác nhau, đặc biệt là khả năng nắm bắt các hành vi người dùng không đồng nhất. Một loạt các nghiên cứu tìm cách cải thiện bằng cách thiết kế các loại kiến trúc lớp chia sẻ khác nhau. Các nghiên cứu này hoặc đưa ra các ràng buộc về các tham số cụ thể theo nhiệm vụ [12,33,49] hoặc tách biệt các tham số chia sẻ và cụ thể theo nhiệm vụ [30,46]. Ý tưởng chung là tách biệt và chia sẻ kiến thức thông qua biểu diễn của đặc trưng đầu vào. Ngoài ra, cũng có nghiên cứu về việc áp dụng RL đa agent cho thiết lập đa kịch bản [13] trong đó nhiệm vụ gợi ý được kết hợp với các nhiệm vụ khác như tìm kiếm và quảng cáo mục tiêu. Khác với các ý tưởng trên, chúng tôi dựa vào chưng cất kiến thức để chuyển giao kiến thức xếp hạng giữa các nhiệm vụ trên các mạng cụ thể theo nhiệm vụ và chúng tôi kết hợp RL để cải thiện sự hài lòng dài hạn của người dùng. Đáng chú ý, mô hình của chúng tôi là một khung chung và có thể được tận dụng như một phần mở rộng cho hầu hết các mô hình MTL có sẵn.
5 KẾT LUẬN
Trong bài báo này, chúng tôi đề xuất một khung học đa nhiệm vụ mới, RMTL, để cải thiện hiệu suất dự đoán của đa nhiệm vụ bằng cách tạo ra trọng số tổn thất tổng động theo cách RL. Mô hình RMTL có thể điều chỉnh thích ứng trọng số của BCE cho mỗi nhiệm vụ dự đoán bằng đầu ra giá trị Q từ mạng critic. Bằng cách xây dựng môi trường MDP theo phiên, chúng tôi ước tính các mạng đa actor-critic bằng cách sử dụng một agent MTL cụ thể và sau đó polish việc tối ưu hóa hàm tổn thất MTL tổng thể bằng cách sử dụng trọng số động, là phép biến đổi tuyến tính của đầu ra mạng critic. Chúng tôi tiến hành một số thí nghiệm trên hai tập dữ liệu thương mại thực tế để xác minh hiệu quả của phương pháp được đề xuất với năm mô hình gợi ý dựa trên MTL baseline. Kết quả chứng minh rằng RMTL tương thích với hầu hết các mô hình gợi ý dựa trên MTL hiện có và có thể cải thiện hiệu suất dự đoán đa nhiệm vụ với khả năng chuyển giao xuất sắc.
LỜI CẢM ƠN
Nghiên cứu này được hỗ trợ một phần bởi APRC - CityU New Research Initiatives (No.9610565, Start-up Grant for New Faculty of City University of Hong Kong), SIRG - CityU Strategic Interdisciplinary Research Grant (No.7020046, No.7020074), HKIDS Early Career Research Grant (No.9360163), Huawei Innovation Research Program và Ant Group (CCF-Ant Research Fund).
TÀI LIỆU THAM KHẢO
[1]Giuseppe Aceto, Valerio Persico, và Antonio Pescapé. 2020. Industry 4.0 and health: Internet of things, big data, and cloud computing for healthcare 4.0.
Journal of Industrial Information Integration 18 (2020), 100129.
[2]M Mehdi Afsar, Trafford Crump, và Behrouz Far. 2021. Reinforcement learning based recommender systems: A survey. ACM Computing Surveys (CSUR) (2021).
[3]Xueying Bai, Jian Guan, và Hongning Wang. 2019. A model-based reinforcement learning with adversarial training for online recommendation. Advances in Neural Information Processing Systems 32 (2019).
[4]Shalabh Bhatnagar, Mohammad Ghavamzadeh, Mark Lee, và Richard S Sutton. 2007. Incremental natural actor-critic algorithms. Advances in neural information processing systems 20 (2007).
[5] Rich Caruana. 1997. Multitask learning. Machine learning 28, 1 (1997), 41–75.
[6]Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming Tang, Yuzhou Zhang, và Yong Yu. 2019. Large-scale interactive recommendation with tree-structured policy gradient. Trong Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 3312–3320.
[7]Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, và Ed H Chi. 2019. Top-k off-policy correction for a REINFORCE recommender system. Trong Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining . 456–464.
[8]Minmin Chen, Bo Chang, Can Xu, và Ed H Chi. 2021. User response models to improve a reinforce recommender system. Trong Proceedings of the 14th ACM International Conference on Web Search and Data Mining . 121–129.
[9]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al . 2016. Wide & deep learning for recommender systems. Trong Proceedings of the 1st workshop on deep learning for recommender systems . 7–10.
[10] Thomas Degris, Patrick M Pilarski, và Richard S Sutton. 2012. Model-free reinforcement learning with continuous action in practice. Trong 2012 American Control Conference (ACC) . IEEE, 2177–2182.
[11] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, và Ben Coppin. 2015. Deep reinforcement learning in large discrete action spaces. arXiv preprint arXiv:1512.07679 (2015).
[12] Long Duong, Trevor Cohn, Steven Bird, và Paul Cook. 2015. Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser. Trong Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the 7th international joint conference on natural language processing (volume 2: short papers) . 845–850.
[13] Jun Feng, Heng Li, Minlie Huang, Shichen Liu, Wenwu Ou, Zhirong Wang, và Xiaoyan Zhu. 2018. Learning to collaborate: Multi-scenario ranking via multi-agent reinforcement learning. Trong Proceedings of the 2018 World Wide Web Conference . 1939–1948.
[14] Chongming Gao, Shijun Li, Yuan Zhang, Jiawei Chen, Biao Li, Wenqiang Lei, Peng Jiang, và Xiangnan He. 2022. KuaiRand: An Unbiased Sequential Recommendation Dataset with Randomly Exposed Videos. Trong Proceedings of the 31st ACM International Conference on Information and Knowledge Management (Atlanta, GA, USA) (CIKM '22) . 5 trang. https://doi.org/10.1145/3511808.3557624[15] Yoav Goldberg. 2017. Neural network methods for natural language processing. Synthesis lectures on human language technologies 10, 1 (2017), 1–309.
[16] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, và Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017).
[17] Guy Hadash, Oren Sar Shalom, và Rita Osadchy. 2018. Rank and rate: multi-task learning for recommender systems. Trong Proceedings of the 12th ACM Conference on Recommender Systems . 451–454.
[18] Yanxiang Huang, Bin Cui, Jie Jiang, Kunqian Hong, Wenyu Zhang, và Yiran Xie. 2016. Real-time video recommendation exploration. Trong Proceedings of the 2016 International Conference on Management of Data . 35–46.
[19] Guangda Huzhang, Zhen-Jia Pang, Yongqing Gao, Wen-Ji Zhou, Qing Da, Anxiang Zeng, và Yang Yu. 2020. Validation Set Evaluation can be Wrong: An Evaluator-Generator Approach for Maximizing Online Performance of Ranking in E-commerce. CoRR abs/2003.11941 (2020). https://arxiv.org/abs/2003.11941
[20] Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu, Heng-Tze Cheng, Tushar Chandra, và Craig Boutilier. 2019. SlateQ: A tractable decomposition for reinforcement learning with recommendation sets. (2019).
[21] Eugene Ie, Chih wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, và Craig Boutilier. 2019. RecSim: A Configurable Simulation Platform for Recommender Systems. (2019). arXiv:1909.04847 [cs.LG]
[22] Thorsten Joachims, Dayne Freitag, Tom Mitchell, et al .1997. Webwatcher: A tour guide for the world wide web. Trong IJCAI (1) . Citeseer, 770–777.
[23] Yehuda Koren, Robert Bell, và Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009), 30–37.
[24] Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen, Huifeng Guo, và Yuzhou Zhang. 2018. Deep reinforcement learning based recommendation with explicit user-item interactions modeling. arXiv preprint arXiv:1810.12027 (2018).
[25] Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen, Huifeng Guo, Yuzhou Zhang, và Xiuqiang He. 2020. State representation modeling for deep reinforcement learning based recommendation. Knowledge-Based Systems 205 (2020), 106170.
[26] Hu Liu, Jing Lu, Xiwei Zhao, Sulong Xu, Hao Peng, Yutong Liu, Zehua Zhang, Jian Li, Junsheng Jin, Yongjun Bao, et al .2020. Kalman filtering attention for user behavior modeling in ctr prediction. Advances in Neural Information Processing Systems 33 (2020), 9228–9238.
[27] Tie-Yan Liu et al .2009. Learning to rank for information retrieval. Foundations and Trends ®in Information Retrieval 3, 3 (2009), 225–331.
[28] Yichao Lu, Ruihai Dong, và Barry Smyth. 2018. Coevolutionary recommendation model: Mutual learning between ratings and reviews. Trong Proceedings of the 2018 World Wide Web Conference . 773–782.
[29] Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, và Lukasz Kaiser. 2015. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114 (2015).
[30] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, và Ed H Chi. 2018. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. Trong Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1930–1939.
[31] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, và Kun Gai. 2018. Entire space multi-task model: An effective approach for estimating post-click conversion rate. Trong The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval . 1137–1140.
[32] Tariq Mahmood và Francesco Ricci. 2007. Learning and adaptivity in interactive recommender systems. Trong Proceedings of the ninth international conference on Electronic commerce . 75–84.
[33] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, và Martial Hebert. 2016. Cross-stitch networks for multi-task learning. Trong Proceedings of the IEEE conference on computer vision and pattern recognition . 3994–4003.
[34] Omar Moling, Linas Baltrunas, và Francesco Ricci. 2012. Optimal radio channel recommendations with explicit and implicit feedback. Trong Proceedings of the sixth ACM conference on Recommender systems . 75–82.
[35] Raymond J Mooney và Loriene Roy. 2000. Content-based book recommending using learning for text categorization. Trong Proceedings of the fifth ACM conference on Digital libraries . 195–204.
[36] Huashan Pan, Xiulin Li, và Zhiqiang Huang. 2019. A Mandarin Prosodic Boundary Prediction Model Based on Multi-Task Learning.. Trong Interspeech . 4485–4488.
[37] Changhua Pei, Xinru Yang, Qing Cui, Xiao Lin, Fei Sun, Peng Jiang, Wenwu Ou, và Yongfeng Zhang. 2019. Value-aware recommendation based on reinforcement profit maximization. Trong The World Wide Web Conference . 3123–3129.
[38] Jan Peters và Stefan Schaal. 2008. Natural actor-critic. Neurocomputing 71, 7-9 (2008), 1180–1190.
[39] Shaoqing Ren, Kaiming He, Ross Girshick, và Jian Sun. 2015. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems 28 (2015).
[40] Guy Shani, David Heckerman, Ronen I Brafman, và Craig Boutilier. 2005. An MDP-based recommender system. Journal of Machine Learning Research 6, 9 (2005).

--- TRANG 10 ---
WWW '23, Ngày 1–5 tháng 5 năm 2023, Austin, TX, USA Liu, et al.
[41] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, và Martin Riedmiller. 2014. Deterministic policy gradient algorithms. Trong International conference on machine learning . Pmlr, 387–395.
[42] Yueming Sun và Yi Zhang. 2018. Conversational recommender system. Trong The 41st international acm sigir conference on research & development in information retrieval . 235–244.
[43] Richard S Sutton và Andrew G Barto. 2018. Reinforcement learning: An introduction . MIT press.
[44] Richard S Sutton, David McAllester, Satinder Singh, và Yishay Mansour. 1999. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems 12 (1999).
[45] Nima Taghipour, Ahmad Kardan, và Saeed Shiry Ghidary. 2007. Usage-based web recommendations: a reinforcement learning approach. Trong Proceedings of the 2007 ACM conference on Recommender systems . 113–120.
[46] Hongyan Tang, Junning Liu, Ming Zhao, và Xudong Gong. 2020. Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. Trong Fourteenth ACM Conference on Recommender Systems . 269–278.
[47] Nelson Vithayathil Varghese và Qusay H Mahmoud. 2020. A survey of multi-task deep reinforcement learning. Electronics 9, 9 (2020), 1363.
[48] Yuyan Wang, Mohit Sharma, Can Xu, Sriraj Badam, Qian Sun, Lee Richardson, Lisa Chung, Ed H Chi, và Minmin Chen. 2022. Surrogate for Long-Term User Experience in Recommender Systems. Trong Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 4100–4109.
[49] Yongxin Yang và Timothy Hospedales. 2016. Deep multi-task representation learning: A tensor factorisation approach. arXiv preprint arXiv:1605.06391 (2016).
[50] Qihua Zhang, Junning Liu, Yuzhuo Dai, Yiyan Qi, Yifan Yuan, Kunlun Zheng, Fan Huang, và Xianfeng Tan. 2022. Multi-Task Fusion via Reinforcement Learning for Long-Term User Satisfaction in Recommender Systems. Trong Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining . 4510–4520.
[51] Shuai Zhang, Lina Yao, Aixin Sun, và Yi Tay. 2019. Deep learning based recommender system: A survey and new perspectives. ACM Computing Surveys(CSUR) 52, 1 (2019), 1–38.
[52] Yu Zhang và Qiang Yang. 2021. A survey on multi-task learning. IEEE Transactions on Knowledge and Data Engineering (2021).
[53] Xiangyu Zhao, Changsheng Gu, Haoshenglun Zhang, Xiwang Yang, Xiaobing Liu, Hui Liu, và Jiliang Tang. 2021. DEAR: Deep Reinforcement Learning for Online Advertising Impression in Recommender Systems. Trong Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 35. 750–758.
[54] Xiangyu Zhao, Long Xia, Jiliang Tang, và Dawei Yin. 2019. " Deep reinforcement learning for search, recommendation, and online advertising: a survey" by Xiangyu Zhao, Long Xia, Jiliang Tang, and Dawei Yin with Martin Vesely as coordinator. ACM sigweb newsletter Spring (2019), 1–15.
[55] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, và Jiliang Tang. 2018. Deep Reinforcement Learning for Page-wise Recommendations. Trong Proceedings of the 12th ACM Recommender Systems Conference . ACM, 95–103.
[56] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, và Dawei Yin. 2018. Recommendations with negative feedback via pairwise deep reinforcement learning. Trong Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 1040–1048.
[57] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Dawei Yin, Yihong Zhao, và Jiliang Tang. 2017. Deep Reinforcement Learning for List-wise Recommendations. arXiv preprint arXiv:1801.00209 (2017).
[58] Yufan Zhao, Donglin Zeng, Mark A Socinski, và Michael R Kosorok. 2011. Reinforcement learning strategies for clinical trials in nonsmall cell lung cancer. Biometrics 67, 4 (2011), 1422–1433.
[59] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan, Xing Xie, và Zhenhui Li. 2018. DRN: A deep reinforcement learning framework for news recommendation. Trong Proceedings of the 2018 world wide web conference . 167–176.
[60] Lixin Zou, Long Xia, Zhuoye Ding, Jiaxing Song, Weidong Liu, và Dawei Yin. 2019. Reinforcement learning to optimize long-term user engagement in recommender systems. Trong Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining . 2810–2818.
