Học tăng cường nghịch đảo suốt đời

Jorge A. Mendez, Shashank Shivkumar và Eric Eaton
Khoa Khoa học Máy tính và Thông tin
Đại học Pennsylvania
{mendezme,shashs,eeaton}@seas.upenn.edu

Tóm tắt
Các phương pháp học từ minh họa (LfD) đã cho thấy thành công trong việc thu được các chính sách hành vi bằng cách bắt chước người dùng. Tuy nhiên, ngay cả đối với một nhiệm vụ đơn lẻ, LfD có thể yêu cầu nhiều minh họa. Đối với các tác nhân đa năng phải học nhiều nhiệm vụ thông qua minh họa, quá trình này sẽ gây ra gánh nặng đáng kể cho người dùng nếu mỗi nhiệm vụ được học riêng lẻ. Để giải quyết thách thức này, chúng tôi giới thiệu bài toán mới về học suốt đời từ minh họa, cho phép tác nhân liên tục xây dựng dựa trên kiến thức học được từ các nhiệm vụ được minh họa trước đó để tăng tốc quá trình học các nhiệm vụ mới, giảm số lượng minh họa cần thiết. Như một giải pháp cho bài toán này, chúng tôi đề xuất phương pháp học suốt đời đầu tiên cho học tăng cường nghịch đảo, học các nhiệm vụ liên tiếp thông qua minh họa, liên tục chuyển giao kiến thức giữa các nhiệm vụ để cải thiện hiệu suất.

1 Giới thiệu

Trong nhiều ứng dụng, chẳng hạn như robot cá nhân hoặc trợ lý ảo thông minh, người dùng có thể muốn dạy một tác nhân thực hiện một số nhiệm vụ ra quyết định tuần tự. Thông thường, người dùng có thể minh họa hành vi thích hợp, cho phép tác nhân học nhiệm vụ tùy chỉnh thông qua bắt chước. Nghiên cứu về học tăng cường nghịch đảo (IRL) [29,1,43,21,31,28] đã cho thấy thành công khi đóng khung bài toán học từ minh họa (LfD) như tối ưu hóa một hàm tiện ích từ các minh họa của người dùng. IRL giả định rằng người dùng hành động để tối ưu hóa một số hàm phần thưởng khi thực hiện minh họa, ngay cả khi họ không thể chỉ định rõ ràng hàm phần thưởng đó như trong học tăng cường (RL) điển hình. IRL tìm cách khôi phục hàm phần thưởng này từ minh họa, và sau đó sử dụng nó để huấn luyện một chính sách tối ưu. Học hàm phần thưởng thay vì chỉ sao chép chính sách của người dùng cung cấp cho tác nhân một biểu diễn di động của nhiệm vụ. Hầu hết các phương pháp IRL đã tập trung vào một tác nhân học một nhiệm vụ đơn lẻ. Tuy nhiên, khi các hệ thống AI trở nên đa năng hơn, ngày càng có khả năng tác nhân sẽ được mong đợi học nhiều nhiệm vụ trong suốt cuộc đời của nó. Nếu nó học mỗi nhiệm vụ riêng lẻ, quá trình này sẽ gây ra gánh nặng đáng kể cho người dùng trong việc cung cấp nhiều minh họa.

Để giải quyết thách thức này, chúng tôi giới thiệu bài toán mới về học suốt đời từ minh họa, trong đó một tác nhân sẽ đối mặt với nhiều nhiệm vụ LfD liên tiếp và phải tối ưu hóa hiệu suất tổng thể của nó. Bằng cách xây dựng dựa trên kiến thức từ các nhiệm vụ trước đó, tác nhân có thể giảm số lượng minh họa của người dùng cần thiết để học một nhiệm vụ mới. Như một ví dụ minh họa, hãy xem xét một robot dịch vụ cá nhân học thực hiện các công việc gia đình từ chủ nhân của nó. Ban đầu, con người có thể muốn dạy robot nạp bát đĩa vào máy rửa bằng cách cung cấp minh họa về nhiệm vụ. Sau đó, người dùng có thể dạy robot bày bàn ăn. Những nhiệm vụ này rõ ràng có liên quan vì chúng liên quan đến việc thao tác đồ ăn và dao kéo, và vì vậy chúng ta sẽ mong đợi robot tận dụng bất kỳ kiến thức liên quan nào thu được từ việc nạp máy rửa bát khi bày bàn cho bữa tối. Ngoài ra, chúng ta sẽ hy vọng robot có thể cải thiện hiểu biết của nó về nhiệm vụ máy rửa bát với bất kỳ kiến thức bổ sung nào nó thu được từ việc bày bàn ăn. Trong suốt cuộc đời của robot với nhiều nhiệm vụ, khả năng chia sẻ kiến thức giữa các nhiệm vụ được minh họa sẽ tăng tốc đáng kể quá trình học.

Chúng tôi đóng khung LfD suốt đời như một bài toán học đa nhiệm vụ trực tuyến, cho phép tác nhân tăng tốc học bằng cách chuyển giao kiến thức giữa các nhiệm vụ. Việc chuyển giao này có thể được xem như khai thác các mối quan hệ cơ bản giữa các hàm phần thưởng khác nhau (ví dụ, làm vỡ ly rượu luôn không mong muốn). Mặc dù học suốt đời đã được nghiên cứu trong phân loại, hồi quy và RL [10,34,6], đây là nghiên cứu đầu tiên về học suốt đời cho IRL. Khung của chúng tôi bao bọc xung quanh các phương pháp IRL hiện có, thực hiện xấp xỉ hàm suốt đời của các hàm phần thưởng đã học. Như một thể hiện của khung này, chúng tôi đề xuất thuật toán Học IRL Suốt đời Hiệu quả (ELIRL), điều chỉnh Maximum Entropy (MaxEnt) IRL [43] vào môi trường học suốt đời. Chúng tôi cho thấy ELIRL có thể chuyển giao thành công kiến thức giữa các nhiệm vụ IRL để cải thiện hiệu suất, và sự cải thiện này tăng lên khi nó học nhiều nhiệm vụ hơn. Nó vượt trội đáng kể so với người học cơ sở, MaxEnt IRL, với chi phí bổ sung nhỏ, và có thể đạt được hiệu suất tương đương hoặc tốt hơn IRL thông qua các quá trình Gaussian với chi phí tính toán ít hơn nhiều.

2 Công trình liên quan

Bài toán IRL được định nghĩa không đầy đủ, vì vậy các phương pháp sử dụng các phương tiện khác nhau để xác định hàm phần thưởng nào giải thích tốt nhất các quỹ đạo quan sát được. Trong số này, các phương pháp IRL lề tối đa [29,1] chọn hàm phần thưởng tách biệt nhất chính sách tối ưu và chính sách tốt thứ hai. Các biến thể của những phương pháp này đã cho phép minh họa không tối ưu [32], hàm phần thưởng phi tuyến [35], và học lý thuyết trò chơi [37]. Các phương pháp Bayesian IRL [31,30] sử dụng kiến thức tiên nghiệm để thiên vị việc tìm kiếm trên các hàm phần thưởng, và có thể hỗ trợ minh họa không tối ưu [33]. Các thuật toán dựa trên gradient tối ưu hóa một mất mát để học phần thưởng trong khi, chẳng hạn, phạt các độ lệch khỏi chính sách của chuyên gia [28]. Các mô hình entropy tối đa [43,21,42] tìm hàm phần thưởng có khả năng nhất đã cho minh họa, và tạo ra một chính sách phù hợp với hiệu suất mong đợi của người dùng mà không đưa ra thêm giả định về sở thích trên quỹ đạo. Công trình khác đã tránh học phần thưởng hoàn toàn và thay vào đó tập trung vào mô hình hóa chính sách của người dùng thông qua phân loại [27].

Tuy nhiên, lưu ý rằng tất cả các phương pháp này tập trung vào học một nhiệm vụ IRL đơn lẻ, và không xem xét chia sẻ kiến thức giữa nhiều nhiệm vụ. Mặc dù công trình khác đã tập trung vào IRL đa nhiệm vụ, các phương pháp hiện có hoặc giả định rằng các nhiệm vụ chia sẻ một không gian trạng thái và hành động, hoặc mở rộng kém do chi phí tính toán của chúng; phương pháp của chúng tôi khác biệt ở cả hai khía cạnh. Một phương pháp sớm đối với IRL đa nhiệm vụ [12] học các nhiệm vụ khác nhau bằng cách lấy mẫu từ một tiên nghiệm chung trên phần thưởng và chính sách, giả định rằng các không gian trạng thái-hành động được chia sẻ. Tanwani và Billard [38] nghiên cứu chuyển giao kiến thức cho học từ nhiều chuyên gia, bằng cách sử dụng các hàm phần thưởng đã học trước đó để khởi động tìm kiếm khi một chuyên gia mới minh họa quỹ đạo. Mặc dù hiệu quả, phương pháp của họ không tối ưu hóa hiệu suất trên tất cả các nhiệm vụ, và chỉ xem xét học các phương pháp tiếp cận khác nhau của các chuyên gia đối với một nhiệm vụ. Khái niệm chuyển giao trong IRL cũng được nghiên cứu trong môi trường không giám sát [2,11], nơi mỗi nhiệm vụ được giả định được tạo ra từ một tập hợp các ý định ẩn. Những phương pháp này nhóm một lô nhiệm vụ ban đầu, và khi quan sát mỗi nhiệm vụ mới, sử dụng các cụm để nhanh chóng học hàm phần thưởng tương ứng. Tuy nhiên, chúng không giải quyết cách cập nhật các cụm sau khi quan sát một nhiệm vụ mới. Hơn nữa, những phương pháp này giả định không gian trạng thái-hành động được chia sẻ trên các nhiệm vụ, và, như một vòng lặp trong trong tối ưu hóa, học một chính sách duy nhất cho tất cả các nhiệm vụ. Nếu không gian không được chia sẻ, việc học chính sách lặp lại sẽ trở nên không khả thi về mặt tính toán cho nhiều nhiệm vụ. Gần đây nhất, chuyển giao trong IRL đã được nghiên cứu để giải quyết bài toán học bắt chước một lần [13,17]. Trong môi trường này, tác nhân được giao nhiệm vụ sử dụng kiến thức từ một tập hợp nhiệm vụ ban đầu để tổng quát hóa cho một nhiệm vụ mới đã cho một minh họa duy nhất của nhiệm vụ mới. Nhược điểm chính của những phương pháp này là chúng yêu cầu một lô lớn các nhiệm vụ có sẵn tại thời điểm huấn luyện, và vì vậy không thể xử lý các nhiệm vụ đến tuần tự.

Công trình của chúng tôi tương tự nhất với của Mangin và Oudeyer [25], đặt bài toán IRL đa nhiệm vụ như học từ điển lô của các nhiệm vụ nguyên thủy, nhưng dường như không hoàn thành và không được xuất bản. Finn et al. [16] sử dụng IRL như một bước để chuyển giao kiến thức trong môi trường RL suốt đời, nhưng họ không khám phá học suốt đời cụ thể cho IRL. Trái ngược với công trình hiện có, phương pháp của chúng tôi có thể xử lý các không gian trạng thái-hành động riêng biệt. Nó hoàn toàn trực tuyến và hiệu quả về mặt tính toán, cho phép nó nhanh chóng học hàm phần thưởng cho mỗi nhiệm vụ mới thông qua chuyển giao và sau đó cập nhật một kho kiến thức được chia sẻ. Kiến thức mới được chuyển giao ngược để cải thiện các hàm phần thưởng của các nhiệm vụ trước đó (mà không cần huấn luyện lại trên các nhiệm vụ này), từ đó tối ưu hóa tất cả các nhiệm vụ. Chúng tôi đạt được điều này bằng cách điều chỉnh các ý tưởng từ học suốt đời trong môi trường có giám sát [34], mà chúng tôi cho thấy đạt được những lợi ích tương tự trong IRL.

3 Học tăng cường nghịch đảo

Chúng tôi trước tiên mô tả IRL và phương pháp MaxEnt IRL, trước khi giới thiệu bài toán IRL suốt đời.

3.1 Bài toán RL nghịch đảo

Một quá trình quyết định Markov (MDP) được định nghĩa như một bộ ⟨S, A, T, r, γ⟩, trong đó S là tập hợp các trạng thái, A là tập hợp các hành động, hàm chuyển đổi T: S × A × S → [0,1] cho xác suất P(s_{i+1}|s_i, a_i) rằng ở trong trạng thái s_i và thực hiện hành động a_i sẽ cho ra trạng thái tiếp theo s_{i+1}, r: S → R là hàm phần thưởng, và γ ∈ [0,1) là hệ số chiết khấu. Một chính sách π: S × A → [0,1] mô hình hóa phân phối P(a_i|s_i) trên các hành động tác nhân nên thực hiện trong bất kỳ trạng thái nào. Khi được chỉ định đầy đủ, một MDP có thể được giải quyết thông qua lập trình tuyến tính hoặc động cho một chính sách tối ưu tối đa hóa phần thưởng mà tác nhân kiếm được: π* = argmax_π V^π, với V^π = E_π [∑_i γ^i r(s_i)].

Trong IRL [29], tác nhân không biết hàm phần thưởng của MDP, và phải suy ra nó từ các minh họa Z = {ζ_1, ..., ζ_{n_t}} được đưa ra bởi một người dùng chuyên gia. Mỗi minh họa ζ_j là một chuỗi các cặp trạng thái-hành động [s_{0:H}, a_{0:H}] được giả định được tạo ra bởi chính sách chưa biết π̂ của người dùng. Một khi hàm phần thưởng được học, MDP hoàn chính và vì vậy có thể được giải quyết cho chính sách tối ưu π*.

Cho một MDP\r = ⟨S, A, T, γ⟩ và minh họa chuyên gia Z, mục tiêu của IRL là ước tính hàm phần thưởng r chưa biết của MDP. Công trình trước đây đã định nghĩa phần thưởng tối ưu sao cho chính sách được thực hiện bởi người dùng là (gần-)tối ưu dưới phần thưởng đã học (V^π* = V^π̂), trong khi (gần như) tất cả các hành động khác sẽ không tối ưu. Bài toán này thật không may là được đặt không tốt, vì nó có nhiều giải pháp, và vì vậy trở nên cần thiết để đưa ra thêm các giả định để tìm các giải pháp tổng quát hóa tốt. Những giả định khác nhau này và các chiến lược để khôi phục chính sách của người dùng đã là trọng tâm của nghiên cứu IRL trước đây. Chúng tôi tiếp theo tập trung vào phương pháp MaxEnt đối với bài toán IRL.

3.2 Maximum Entropy IRL

Trong thuật toán maximum entropy (MaxEnt) cho IRL [43], mỗi trạng thái s_i được biểu diễn bởi một vector đặc trưng x_{s_i} ∈ R^d. Mỗi quỹ đạo được minh họa ζ_j cho một số đếm đặc trưng x_{ζ_j} = ∑_{i=0}^H γ^i x_{s_i}, cho một số đếm đặc trưng mong đợi xấp xỉ x̄ = 1/n ∑_j x_{ζ_j} phải được khớp bởi chính sách của tác nhân để thỏa mãn điều kiện V^π* = V^π̂. Hàm phần thưởng được biểu diễn như một hàm tuyến tính được tham số hóa với vector trọng số θ ∈ R^d như r_{s_i} = r(x_{s_i}; θ) = θ^T x_{s_i} và vì vậy phần thưởng tích lũy của một quỹ đạo ζ_j được cho bởi r_{ζ_j} = r(x_{ζ_j}; θ) = ∑_{s_i∈ζ_j} γ^i θ^T x_{s_i} = θ^T x_{ζ_j}.

Thuật toán xử lý sự mơ hồ của bài toán IRL theo cách xác suất, bằng cách giả định rằng người dùng hành động theo chính sách MaxEnt. Trong môi trường này, xác suất của một quỹ đạo được cho như: P(ζ_j|θ, T) ∝ 1/Z(θ,T) exp(r_{ζ_j}) ∏_{(s_i,a_i,s_{i+1})∈ζ_j} T(s_{i+1}|s_i, a_i), trong đó Z(θ,T) là hàm phân vùng, và xấp xỉ đến từ giả định rằng sự bất định chuyển đổi có ít ảnh hưởng đến hành vi. Phân phối này không ưu tiên bất kỳ quỹ đạo nào hơn cái khác với cùng phần thưởng, và ưu tiên theo cấp số nhân các quỹ đạo với phần thưởng cao hơn. Bài toán IRL sau đó được giải quyết bằng cách tối đa hóa khả năng của các quỹ đạo quan sát được θ* = argmax_θ log P(Z|θ) = argmax_θ ∑_{ζ_j∈Z} log P(ζ_j|θ, T). Gradient của log-likelihood là sự khác biệt giữa kỳ vọng đặc trưng của người dùng và tác nhân, có thể được biểu diễn theo tần suất thăm trạng thái D_s: x̄ - ∑_{ζ̃∈Z_{MDP}} P(ζ̃|θ, T)x_ζ̃ = x̄ - ∑_{s∈S} D_s x_s, trong đó Z_{MDP} là tập hợp tất cả các quỹ đạo có thể. D_s có thể được tính hiệu quả thông qua thuật toán forward-backward [43]. Cực đại của mục tiêu lõm này sau đó được đạt được khi số đếm đặc trưng khớp, và vì vậy V^π* = V^π̂.

4 Bài toán IRL suốt đời

Chúng tôi bây giờ giới thiệu bài toán mới của IRL suốt đời. Trái ngược với hầu hết công trình trước đây về IRL, tập trung vào học đơn nhiệm vụ, bài báo này tập trung vào IRL đa nhiệm vụ trực tuyến. Chính thức, trong môi trường học suốt đời, tác nhân đối mặt với một chuỗi các nhiệm vụ IRL T^{(1)}, ..., T^{(N_{max})}, mỗi cái là một MDP\r_{T^{(t)}} = ⟨S^{(t)}, A^{(t)}, T^{(t)}, γ^{(t)}⟩. Tác nhân sẽ học các nhiệm vụ liên tiếp, nhận nhiều minh họa chuyên gia cho mỗi nhiệm vụ trước khi chuyển sang nhiệm vụ tiếp theo. Chúng tôi giả định rằng a priori tác nhân không biết tổng số nhiệm vụ N_{max}, phân phối của chúng, hoặc thứ tự của các nhiệm vụ.

Mục tiêu của tác nhân là học một tập hợp các hàm phần thưởng R = {r^{(1)}, ..., r^{(N_{max})}} với một tập hợp tham số tương ứng Θ = {θ^{(1)}, ..., θ^{(N_{max})}}. Tại bất kỳ thời điểm nào, tác nhân có thể được đánh giá trên bất kỳ nhiệm vụ trước đó nào, và vì vậy phải cố gắng tối ưu hóa hiệu suất của nó cho tất cả các nhiệm vụ T^{(1)}, ..., T^{(N)}, trong đó N biểu thị số lượng nhiệm vụ đã thấy cho đến nay (1 ≤ N ≤ N_{max}). Một cách trực quan, khi các nhiệm vụ IRL có liên quan, chuyển giao kiến thức giữa các hàm phần thưởng của chúng có tiềm năng cải thiện hàm phần thưởng đã học cho mỗi nhiệm vụ và giảm số lượng minh họa chuyên gia cần thiết.

Sau N nhiệm vụ, tác nhân phải tối ưu hóa khả năng của tất cả các quỹ đạo quan sát được trên những nhiệm vụ đó:

max_{r^{(1)},...,r^{(N)}} P(r^{(1)},...,r^{(N)}) ∏_{t=1}^N (∏_{j=1}^{n_t} P(ζ_j|r^{(t)}))^{1/n_t}, (1)

trong đó P(r^{(1)},...,r^{(N)}) là một tiên nghiệm phần thưởng để khuyến khích mối quan hệ giữa các hàm phần thưởng, và mỗi nhiệm vụ được cho tầm quan trọng bằng nhau bằng cách cân bằng nó theo số lượng quỹ đạo liên quan n_t.

5 Học tăng cường nghịch đảo suốt đời

Ý tưởng chính của khung của chúng tôi là sử dụng xấp xỉ hàm suốt đời để biểu diễn các hàm phần thưởng cho tất cả các nhiệm vụ, cho phép chuyển giao trực tuyến liên tục giữa các hàm phần thưởng với các cập nhật hiệu quả theo nhiệm vụ. Một cách trực quan, khung này khai thác thực tế rằng các khía cạnh nhất định của các hàm phần thưởng thường được chia sẻ giữa các nhiệm vụ khác nhau (nhưng có liên quan), chẳng hạn như phần thưởng âm mà một robot dịch vụ có thể nhận được vì làm rơi đồ vật. Chúng tôi giả định các hàm phần thưởng r^{(t)} cho các nhiệm vụ khác nhau có liên quan thông qua một cơ sở tiềm ẩn của các thành phần phần thưởng L. Những thành phần này có thể được sử dụng để tái tạo các hàm phần thưởng thực thông qua một sự kết hợp thưa của những thành phần như vậy với các hệ số cụ thể nhiệm vụ s^{(t)}, sử dụng L như một cơ chế chuyển giao đã cho thấy thành công trong công trình trước đây [19, 26].

Phần này phát triển khung của chúng tôi cho IRL suốt đời, thể hiện nó theo phương pháp MaxEnt để tạo ra thuật toán ELIRL. Mặc dù chúng tôi tập trung vào MaxEnt IRL, ELIRL có thể dễ dàng được điều chỉnh cho các phương pháp IRL khác, như được hiển thị trong Phụ lục D. Chúng tôi chứng minh ưu điểm của bài toán IRL suốt đời mới bằng cách cho thấy rằng 1) chuyển giao giữa các nhiệm vụ IRL có thể tăng đáng kể độ chính xác của chúng và 2) chuyển giao này có thể được đạt được bằng cách điều chỉnh các ý tưởng từ học suốt đời trong môi trường có giám sát.

5.1 Thuật toán IRL Suốt đời Hiệu quả

Như được mô tả trong Phần 4, tác nhân IRL suốt đời phải tối ưu hóa hiệu suất của nó trên tất cả các nhiệm vụ IRL quan sát được cho đến nay. Sử dụng giả định MaxEnt rằng hàm phần thưởng r^{(t)}_{s_i} = θ^{(t)T} x^{(t)}_{s_i} cho mỗi nhiệm vụ là tuyến tính và được tham số hóa bởi θ^{(t)} ∈ R^d, chúng tôi có thể phân tích những tham số này thành một sự kết hợp tuyến tính θ^{(t)} = Ls^{(t)} để tạo điều kiện chuyển giao giữa các mô hình tham số, theo Kumar và Daumé [19] và Maurer et al. [26]. Ma trận L ∈ R^{d×k} biểu diễn một tập hợp k vector phần thưởng tiềm ẩn được chia sẻ giữa tất cả các nhiệm vụ, với các hệ số cụ thể nhiệm vụ thưa s^{(t)} ∈ R^k để tái tạo θ^{(t)}.

Sử dụng biểu diễn phân tích này để tạo điều kiện chuyển giao giữa các nhiệm vụ, chúng tôi đặt một tiên nghiệm Laplace trên s^{(t)}'s để khuyến khích chúng thưa, và một tiên nghiệm Gaussian trên L để kiểm soát độ phức tạp của nó, từ đó khuyến khích các hàm phần thưởng chia sẻ cấu trúc. Điều này tạo ra tiên nghiệm phần thưởng sau:

P(r^{(1)},...,r^{(N)}) = 1/Z(λ,μ) exp(-μN||L||_F^2) ∏_{t=1}^N exp(-λ||s^{(t)}||_1), (2)

trong đó Z(λ,μ) là hàm phân vùng, không có ảnh hưởng đến tối ưu hóa. Chúng tôi có thể thay thế tiên nghiệm trong Phương trình 2 cùng với khả năng MaxEnt vào Phương trình 1. Sau khi lấy logarit và sắp xếp lại các điều khoản, điều này tạo ra mục tiêu tương đương:

min_{L} 1/N ∑_{t=1}^N min_{s^{(t)}} (1/n_t ∑_{ζ_j^{(t)} ∈ Z^{(t)}} -log P(ζ_j^{(t)}|Ls^{(t)}, T^{(t)}) + λ||s^{(t)}||_1) + μ||L||_F^2. (3)

Lưu ý rằng Phương trình 3 có thể tách biệt, nhưng không phải đồng thời, lồi trong L và s^{(t)}'s; các phương pháp đa nhiệm vụ điển hình sẽ tối ưu hóa các mục tiêu tương tự [19, 26] sử dụng tối ưu hóa xen kẽ.

Để cho phép Phương trình 3 được giải quyết trực tuyến khi các nhiệm vụ được quan sát liên tiếp, chúng tôi điều chỉnh các khái niệm từ tài liệu học suốt đời. Ruvolo và Eaton [34] xấp xỉ một mục tiêu đa nhiệm vụ với một dạng tương tự như Phương trình 3 trực tuyến như một chuỗi các cập nhật trực tuyến hiệu quả. Tuy nhiên, lưu ý rằng phương pháp của họ được thiết kế cho môi trường có giám sát, sử dụng một hàm mất mát có giám sát mục đích chung thay cho log-likelihood âm MaxEnt trong Phương trình 3, nhưng với một phân tích tương tự của các mô hình tham số đã học. Theo phương pháp của họ nhưng thay thế vào hàm mất mát IRL, cho mỗi nhiệm vụ mới t, chúng tôi có thể lấy một khai triển Taylor bậc hai xung quanh ước tính điểm nhiệm vụ đơn của θ^{(t)} = argmin_θ ∑_{ζ_j^{(t)} ∈ Z^{(t)}} -log P(ζ_j^{(t)}|θ, T^{(t)}), và sau đó đơn giản hóa để cải tạo Phương trình 3 như

min_{L} 1/N ∑_{t=1}^N min_{s^{(t)}} (θ^{(t)} - Ls^{(t)})^T H^{(t)} (θ^{(t)} - Ls^{(t)}) + λ||s^{(t)}||_1 + μ||L||_F^2, (4)

trong đó Hessian H^{(t)} của log-likelihood âm MaxEnt được cho bởi (dẫn xuất trong Phụ lục A):

H^{(t)} = 1/n_t ∇^2_{θ,L} [-log L(Ls^{(t)}; Z^{(t)})] = (∑_{ζ̃∈Z_{MDP}} x_ζ̃ P(ζ̃|θ))(∑_{ζ̃∈Z_{MDP}} x_ζ̃^T P(ζ̃|θ)) + ∑_{ζ̃∈Z_{MDP}} x_ζ̃ x_ζ̃^T P(ζ̃|θ). (5)

Vì H^{(t)} là phi tuyến trong số đếm đặc trưng, chúng tôi không thể sử dụng tần suất thăm trạng thái thu được cho gradient MaxEnt trong môi trường học suốt đời. Điều này tạo ra nhu cầu thu được một xấp xỉ dựa trên mẫu. Chúng tôi trước tiên giải quyết MDP cho một chính sách tối ưu π^{(t)} từ phần thưởng được tham số hóa học bởi MaxEnt đơn nhiệm vụ. Chúng tôi tính số đếm đặc trưng cho một số cố định các đường dẫn chân trời hữu hạn bằng cách theo chính sách ngẫu nhiên π^{(t)}. Sau đó chúng tôi thu được hiệp phương sai mẫu của số đếm đặc trưng của các đường dẫn như một xấp xỉ của hiệp phương sai thực trong Phương trình 5.

Cho mỗi nhiệm vụ liên tiếp mới t, chúng tôi trước tiên ước tính θ^{(t)} như được mô tả ở trên. Sau đó, Phương trình 4 có thể được xấp xỉ trực tuyến như một chuỗi các phương trình cập nhật hiệu quả [34]:

s^{(t)} ← argmin_s ℓ(L_N, s, θ^{(t)}, H^{(t)})
L_{N+1} ← argmin_L μ||L||_F^2 + 1/N ∑_{t=1}^N ℓ(L, s^{(t)}, θ^{(t)}, H^{(t)}), (6)

trong đó ℓ(L, s, θ, H) = λ||s||_1 + (θ - Ls)^T H(θ - Ls), và L có thể được xây dựng tăng dần trong thực tế (xem [34] để biết chi tiết). Quan trọng, xấp xỉ trực tuyến này loại bỏ sự phụ thuộc của Phương trình 3 vào số lượng mẫu huấn luyện và nhiệm vụ, làm cho nó có thể mở rộng cho học suốt đời, và cung cấp đảm bảo về sự hội tụ của nó với hiệu suất tương đương với mục tiêu đa nhiệm vụ đầy đủ [34]. Lưu ý rằng các hệ số s^{(t)} chỉ được cập nhật trong khi huấn luyện trên nhiệm vụ t và ngoài ra vẫn cố định.

Thuật toán 1 ELIRL (k, λ, μ)
L ← RandomMatrix(d, k)
while some task T^{(t)} is available do
    Z^{(t)} ← getExampleTrajectories(T^{(t)})
    θ^{(t)}, H^{(t)} ← inverseReinforcementLearner(Z^{(t)})
    s^{(t)} ← argmin_s (θ^{(t)} - Ls)^T H^{(t)} (θ^{(t)} - Ls) + λ||s||_1
    L ← updateL(L, s^{(t)}, θ^{(t)}, H^{(t)}, μ)
end while

Quá trình này tạo ra hàm phần thưởng ước tính như r^{(t)}_{s_i} = Ls^{(t)}x_{s_i}. Sau đó chúng tôi có thể giải quyết MDP hiện đã hoàn chỉnh cho chính sách tối ưu sử dụng RL tiêu chuẩn. Thuật toán ELIRL hoàn chỉnh được đưa ra như Thuật toán 1. ELIRL có thể hỗ trợ một không gian đặc trưng chung trên các nhiệm vụ, hoặc có thể hỗ trợ các không gian đặc trưng khác nhau trên các nhiệm vụ bằng cách sử dụng công trình trước đây trong chuyển giao tự trị miền chéo [7], như được hiển thị trong Phụ lục C.

5.2 Cải thiện hiệu suất trên các nhiệm vụ trước đó

Khi ELIRL được huấn luyện trên nhiều nhiệm vụ IRL, nó dần dần tinh chỉnh kiến thức được chia sẻ trong L. Vì các tham số của mỗi hàm phần thưởng được mô hình hóa như θ^{(t)} = Ls^{(t)}, những thay đổi tiếp theo đối với L sau khi huấn luyện trên nhiệm vụ t có thể ảnh hưởng đến θ^{(t)}. Thông thường, quá trình này cải thiện hiệu suất trong học suốt đời [34], nhưng đôi khi nó có thể giảm hiệu suất thông qua chuyển giao âm, do các đơn giản hóa ELIRL hạn chế rằng s^{(t)} cố định trừ khi huấn luyện trên nhiệm vụ t. Để ngăn chặn vấn đề này, chúng tôi giới thiệu một kỹ thuật mới. Bất cứ khi nào ELIRL được kiểm tra trên một nhiệm vụ t, nó có thể trực tiếp sử dụng vector θ^{(t)} thu được từ Ls^{(t)}, hoặc tùy chọn lặp lại bước tối ưu hóa cho s^{(t)} trong Phương trình 6 để tính đến những thay đổi lớn tiềm năng trong ma trận L kể từ lần cập nhật cuối cùng cho s^{(t)}. Bước tùy chọn sau này chỉ liên quan đến việc chạy một thể hiện của LASSO, rất hiệu quả. Quan trọng, nó không yêu cầu chạy lại MaxEnt hoặc tính toán lại Hessian, vì tối ưu hóa luôn được thực hiện xung quanh các tham số nhiệm vụ đơn tối ưu, θ^{(t)}. Do đó, ELIRL có thể trả một chi phí nhỏ để thực hiện tối ưu hóa này khi nó phải thực hiện trên một nhiệm vụ trước đó, nhưng nó có được hiệu suất có thể cải thiện trên nhiệm vụ đó bằng cách hưởng lợi từ kiến thức cập nhật trong L, như được hiển thị trong kết quả của chúng tôi.

5.3 Độ phức tạp tính toán

Việc thêm một nhiệm vụ mới vào ELIRL yêu cầu một lần chạy ban đầu của MaxEnt đơn nhiệm vụ để thu được θ^{(t)}, mà chúng tôi giả định có thứ tự O(i(d, |A|, |S|)), trong đó i là số lần lặp cần thiết để MaxEnt hội tụ. Bước tiếp theo là tính Hessian, có chi phí O(MH + Md^2), trong đó M là số quỹ đạo được lấy mẫu cho xấp xỉ và H là chân trời của chúng. Cuối cùng, độ phức tạp của các bước cập nhật cho L và s^{(t)} là O(k^2d^3) [34]. Điều này tạo ra tổng chi phí mỗi nhiệm vụ là O(i(d, |A|, |S|) + MH + Md^2 + k^2d^3) cho ELIRL. Bước tùy chọn của việc cập nhật lại s^{(t)} khi cần thực hiện trên nhiệm vụ t sẽ phát sinh chi phí tính toán O(d^3 + kd^2 + dk^2) để xây dựng mục tiêu của tối ưu hóa và chạy LASSO [34].

Đáng chú ý, không có sự phụ thuộc vào số lượng nhiệm vụ N, đó chính xác là điều làm cho ELIRL phù hợp cho học suốt đời. Vì IRL nói chung yêu cầu tìm chính sách tối ưu cho các lựa chọn khác nhau của hàm phần thưởng như một vòng lặp trong trong tối ưu hóa, sự phụ thuộc bổ sung vào N sẽ làm cho bất kỳ phương pháp IRL nào trở nên không thể giải quyết được trong môi trường suốt đời. Hơn nữa, bước duy nhất phụ thuộc vào kích thước của không gian trạng thái và hành động là MaxEnt đơn nhiệm vụ. Do đó, đối với các nhiệm vụ chiều cao (ví dụ, nhiệm vụ robot), việc thay thế người học cơ sở sẽ cho phép thuật toán của chúng tôi mở rộng một cách duyên dáng.

5.4 Đảm bảo hội tụ lý thuyết

ELIRL kế thừa các đảm bảo lý thuyết được chỉ ra bởi Ruvolo và Eaton [34]. Cụ thể, tối ưu hóa được đảm bảo hội tụ đến một cực tiểu địa phương của hàm chi phí xấp xỉ trong Phương trình 4 khi số lượng nhiệm vụ tăng lớn. Một cách trực quan, chất lượng của xấp xỉ này phụ thuộc vào việc biểu diễn phân tích θ^{(t)} = Ls^{(t)} lệch bao nhiều khỏi θ^{(t)}, điều này lần lượt phụ thuộc vào mức độ biểu diễn này có thể nắm bắt mối quan hệ nhiệm vụ. Tuy nhiên, chúng tôi nhấn mạnh rằng xấp xỉ này là điều cho phép phương pháp giải quyết bài toán học đa nhiệm vụ trực tuyến, và nó đã được chỉ ra thực nghiệm trong bối cảnh của học có giám sát [34] và RL [6] rằng giải pháp xấp xỉ này có thể đạt được hiệu suất tương đương với học đa nhiệm vụ chính xác trong nhiều bài toán khác nhau.

6 Kết quả thực nghiệm

Chúng tôi đánh giá ELIRL trên hai môi trường, được chọn để cho phép chúng tôi tạo ra vô số nhiệm vụ với các hàm phần thưởng riêng biệt. Điều này cũng cho chúng tôi các phần thưởng đã biết như sự thật cơ sở. Không có phương pháp IRL đa nhiệm vụ trước đây nào được kiểm tra trên một tập hợp nhiệm vụ lớn như vậy, cũng không trên các nhiệm vụ với không gian trạng thái khác nhau như chúng tôi làm.

Objectworld: Tương tự như môi trường được trình bày bởi Levine et al. [21], Objectworld là một lưới 32×32 được đặt bởi các đối tượng màu trong các ô ngẫu nhiên. Mỗi đối tượng có một trong năm màu ngoài và một trong hai màu trong, và tạo ra một phần thưởng không đổi trên lưới 5×5 xung quanh nó. Chúng tôi tạo ra 100 nhiệm vụ bằng cách ngẫu nhiên chọn 2-4 màu ngoài, và gán cho mỗi cái một phần thưởng được lấy mẫu đồng đều từ [-10, 5]; các màu trong là các đặc trưng làm phiền. Mục tiêu của tác nhân sau đó là di chuyển về phía các đối tượng với màu "tốt" (dương) và tránh xa các đối tượng với màu "xấu" (âm). Lý tưởng, mỗi cột của L sẽ học trường tác động xung quanh một màu, và s^{(t)}'s sẽ mã hóa mức độ tốt hoặc xấu của mỗi màu trong mỗi nhiệm vụ. Có d = 31(5 + 2) đặc trưng, biểu diễn khoảng cách đến đối tượng gần nhất với mỗi màu ngoài và trong, được rời rạc hóa như các chỉ báo nhị phân về việc khoảng cách có nhỏ hơn 1-31 hay không. Tác nhân có thể chọn di chuyển dọc theo bốn hướng chính hoặc ở lại chỗ.

Highway: Mô phỏng đường cao tốc đã được sử dụng để kiểm tra các phương pháp IRL khác nhau [1,21]. Chúng tôi mô phỏng hành vi của 100 tài xế khác nhau trên một đường cao tốc ba làn trong đó họ có thể lái ở bốn tốc độ. Mỗi tài xế thích làn trái hoặc làn phải, và tốc độ thứ hai hoặc thứ tư. Trọng số của mỗi tài xế cho hai yếu tố đó được lấy mẫu đồng đều từ [0, 5]. Một cách trực quan, mỗi cột của L nên học một tốc độ hoặc làn đường, và s^{(t)}'s nên mã hóa sở thích của tài xế đối với chúng. Có d = 4 + 3 + 64 đặc trưng, biểu diễn tốc độ và làn đường hiện tại, và khoảng cách đến xe gần nhất trong mỗi làn đường phía trước và phía sau, được rời rạc hóa theo cách tương tự như Objectworld. Mỗi bước thời gian, tài xế có thể chọn di chuyển sang trái hoặc phải, tăng tốc hoặc giảm tốc, hoặc duy trì tốc độ và làn đường hiện tại.

Trong cả hai môi trường, hành động được chọn của tác nhân có xác suất thành công 70% và xác suất 30% của một kết quả ngẫu nhiên. Phần thưởng được chiết khấu với mỗi bước thời gian bởi một hệ số γ = 0.9.

6.1 Quy trình đánh giá

Cho mỗi nhiệm vụ, chúng tôi tạo ra một thể hiện của MDP bằng cách đặt các đối tượng ở các vị trí ngẫu nhiên. Chúng tôi giải quyết MDP cho chính sách tối ưu thực, và tạo ra các quỹ đạo người dùng mô phỏng theo chính sách này. Sau đó, chúng tôi đưa cho các thuật toán IRL MDP\r và các quỹ đạo để ước tính phần thưởng r. Chúng tôi so sánh hàm phần thưởng đã học với hàm phần thưởng thực bằng cách chuẩn hóa cả hai và tính chuẩn ℓ₂ của sự khác biệt của chúng. Sau đó, chúng tôi huấn luyện một chính sách sử dụng hàm phần thưởng đã học, và so sánh lợi nhuận mong đợi của nó với lợi nhuận thu được bởi một chính sách được huấn luyện sử dụng phần thưởng thực.

Chúng tôi kiểm tra ELIRL sử dụng L được huấn luyện trên các tập con khác nhau của nhiệm vụ, từ 10 đến 100 nhiệm vụ. Tại mỗi bước kiểm tra, chúng tôi đánh giá hiệu suất của tất cả 100 nhiệm vụ; điều này bao gồm như một tập con đánh giá tất cả các nhiệm vụ quan sát trước đó, nhưng nó khó khăn hơn đáng kể vì cơ sở tiềm ẩn L, được huấn luyện chỉ trên các nhiệm vụ ban đầu, phải tổng quát hóa cho các nhiệm vụ tương lai. Các người học đơn nhiệm vụ được huấn luyện trên tất cả các nhiệm vụ, và chúng tôi đo hiệu suất trung bình của chúng trên tất cả các nhiệm vụ. Tất cả các người học được cung cấp n_t = 32 quỹ đạo cho Objectworld và n_t = 256 quỹ đạo cho Highway, tất cả có độ dài H = 16.

Chúng tôi chọn kích thước k của L thông qua kiến thức miền, và khởi tạo L tuần tự với θ^{(t)}'s của k nhiệm vụ đầu tiên. Chúng tôi đo hiệu suất trên một thể hiện ngẫu nhiên mới của MDP cho mỗi nhiệm vụ, để không nhầm lẫn việc overfitting môi trường huấn luyện với hiệu suất cao. Kết quả được trung bình trên 20 thử nghiệm, mỗi thử nghiệm sử dụng một thứ tự nhiệm vụ ngẫu nhiên.

Chúng tôi so sánh ELIRL với cả vector s^{(t)} gốc (ELIRL) và được tối ưu hóa lại (ELIRL_re) với MaxEnt IRL (người học cơ sở) và GPIRL [21] (một đường cơ sở đơn nhiệm vụ mạnh). Không có phương pháp IRL đa nhiệm vụ hiện có nào phù hợp cho môi trường thực nghiệm này—các phương pháp khác giả định một không gian trạng thái được chia sẻ và quá đắt đỏ cho hơn một vài nhiệm vụ [12,2,11], hoặc chỉ học các phương pháp tiếp cận khác nhau của các chuyên gia đối với một nhiệm vụ đơn [38]. Phụ lục B bao gồm một so sánh với MTMLIRL [2] trên một phiên bản đơn giản hóa của Objectworld, vì MTMLIRL không thể xử lý phiên bản đầy đủ.

Hình 1: Phần thưởng trung bình và sự khác biệt giá trị trong môi trường suốt đời. Sự khác biệt phần thưởng đo lường lỗi giữa phần thưởng đã học và thực. Sự khác biệt giá trị so sánh lợi nhuận mong đợi từ chính sách được huấn luyện trên phần thưởng đã học và chính sách được huấn luyện trên phần thưởng thực. Râu biểu thị lỗi chuẩn. ELIRL cải thiện khi số lượng nhiệm vụ tăng, đạt được hiệu suất tốt hơn người học cơ sở của nó, MaxEnt IRL. Sử dụng tối ưu hóa lại sau khi học tất cả các nhiệm vụ cho phép các nhiệm vụ trước đó hưởng lợi từ kiến thức mới nhất, tăng hiệu suất của ELIRL trên GPIRL. (Xem tốt nhất bằng màu.)

[Biểu đồ được mô tả trong câu trên]

6.2 Kết quả

Hình 1 cho thấy lợi thế của việc chia sẻ kiến thức giữa các nhiệm vụ IRL. ELIRL học các hàm phần thưởng chính xác hơn người học cơ sở của nó, MaxEnt IRL, sau khi đủ nhiệm vụ được sử dụng để huấn luyện cơ sở kiến thức L. Điều này trực tiếp chuyển thành hiệu suất tăng của chính sách được huấn luyện sử dụng hàm phần thưởng đã học. Hơn nữa, tối ưu hóa lại s^{(t)} (Phần 5.2) cho phép ELIRL_re vượt trội GPIRL, bằng cách sử dụng kiến thức được cập nhật nhất.

Bảng 1: Thời gian học trung bình mỗi nhiệm vụ. Lỗi chuẩn được báo cáo sau dấu ±.

Như được hiển thị trong Bảng 1, ELIRL yêu cầu thời gian huấn luyện ít bổ sung so với MaxEnt IRL, ngay cả với tối ưu hóa lại s^{(t)} tùy chọn, và chạy nhanh hơn đáng kể so với GPIRL. Thời gian bổ sung của tối ưu hóa lại gần như không thể cảm nhận được. Điều này biểu thị một lợi thế rõ ràng cho ELIRL khi học nhiều nhiệm vụ trong thời gian thực.

Để phân tích cách ELIRL nắm bắt cấu trúc tiềm ẩn cơ bản của các nhiệm vụ, chúng tôi tạo ra các thể hiện mới của Objectworld và sử dụng một thành phần tiềm ẩn đã học đơn lẻ như phần thưởng của mỗi MDP mới (tức là, một cột của L, có thể được coi như một yếu tố hàm phần thưởng tiềm ẩn). Hình 2 cho thấy các thành phần tiềm ẩn ví dụ được học bởi thuật toán, tiết lộ rằng mỗi thành phần tiềm ẩn biểu diễn lưới 5×5 xung quanh một màu cụ thể hoặc tập con nhỏ của các màu.

Chúng tôi cũng kiểm tra cách hiệu suất trên các nhiệm vụ sớm nhất thay đổi trong quá trình học suốt đời. Nhớ lại rằng khi ELIRL học các nhiệm vụ mới, kiến thức được chia sẻ trong L liên tục thay đổi. Do đó, các hàm phần thưởng được mô hình hóa cho tất cả các nhiệm vụ tiếp tục được tinh chỉnh tự động theo thời gian, mà không cần huấn luyện lại trên các nhiệm vụ. Để đo lường hiệu ứng "chuyển giao ngược" này [34], chúng tôi so sánh hiệu suất trên mỗi nhiệm vụ khi nó lần đầu được gặp với hiệu suất của nó sau khi học tất cả các nhiệm vụ, được trung bình trên 20 thứ tự nhiệm vụ ngẫu nhiên. Hình 3 tiết lộ rằng ELIRL cải thiện hiệu suất của các nhiệm vụ trước đó khi L được tinh chỉnh, đạt được chuyển giao ngược trong IRL. Chuyển giao ngược được cải thiện thêm bởi tối ưu hóa lại s^{(t)}.

6.3 Các mở rộng ELIRL cho chuyển giao miền chéo và không gian trạng thái-hành động liên tục

Chúng tôi thực hiện các thí nghiệm bổ sung để cho thấy cách các mở rộng đơn giản cho ELIRL có thể chuyển giao kiến thức qua các nhiệm vụ với không gian đặc trưng khác nhau và với không gian trạng thái-hành động liên tục.

ELIRL có thể hỗ trợ chuyển giao qua các miền nhiệm vụ với không gian đặc trưng khác nhau bằng cách điều chỉnh công trình trước đây trong chuyển giao miền chéo [7]; chi tiết của mở rộng này được đưa ra trong Phụ lục C. Để đánh giá chuyển giao miền chéo, chúng tôi xây dựng 40 miền Objectworld với không gian đặc trưng khác nhau bằng cách thay đổi kích thước lưới từ 5 đến 24 và để số lượng màu ngoài là 3 hoặc 5. Chúng tôi tạo ra 10 nhiệm vụ mỗi miền, và cung cấp cho các tác nhân 16 minh họa mỗi nhiệm vụ, với độ dài thay đổi theo số lượng ô trong mỗi miền. Chúng tôi so sánh MaxEnt IRL, ELIRL trong miền với s^{(t)}'s gốc (ELIRL) và được tối ưu hóa lại (ELIRL_re), và ELIRL miền chéo với s^{(t)}'s gốc (CD-ELIRL) và được tối ưu hóa lại (CD-ELIRL_re), được trung bình trên 10 thứ tự nhiệm vụ ngẫu nhiên. Hình 4a cho thấy cách chuyển giao miền chéo cải thiện hiệu suất của một tác nhân được huấn luyện chỉ trên các nhiệm vụ trong mỗi miền. Lưu ý cách tối ưu hóa lại s^{(t)} bù đắp cho những thay đổi lớn trong kiến thức được chia sẻ xảy ra khi tác nhân gặp các nhiệm vụ từ các miền khác nhau.

Chúng tôi cũng khám phá một mở rộng của ELIRL cho không gian trạng thái liên tục, như được chi tiết trong Phụ lục D. Để đánh giá mở rộng này, chúng tôi sử dụng một nhiệm vụ điều hướng phẳng liên tục tương tự như được trình bày bởi Levine và Koltun [20]. Tương tự như Objectworld, môi trường liên tục này chứa các đối tượng được phân phối ngẫu nhiên có phần thưởng liên quan (được lấy mẫu ngẫu nhiên), và mỗi đối tượng có một khu vực ảnh hưởng được định nghĩa bởi một hàm cơ sở bán kính. Hình 4b cho thấy hiệu suất của ELIRL trên 50 nhiệm vụ điều hướng liên tục được trung bình trên 20 thứ tự nhiệm vụ khác nhau, so sánh với hiệu suất trung bình của thuật toán AME-IRL đơn nhiệm vụ [20] trên tất cả các nhiệm vụ. Những kết quả này cho thấy rằng ELIRL có thể đạt được hiệu suất tốt hơn trong không gian liên tục so với người học đơn nhiệm vụ, một khi đã quan sát đủ số lượng nhiệm vụ.

7 Kết luận

Chúng tôi đã giới thiệu bài toán mới của IRL suốt đời, và trình bày một khung tổng quát có khả năng chia sẻ kiến thức đã học về các hàm phần thưởng giữa các nhiệm vụ IRL. Chúng tôi đã dẫn xuất một thuật toán cho IRL MaxEnt suốt đời, và cho thấy cách nó có thể dễ dàng được mở rộng để xử lý các phương pháp IRL đơn nhiệm vụ khác nhau và các miền nhiệm vụ đa dạng. Trong công trình tương lai, chúng tôi dự định nghiên cứu cách các người học cơ sở mạnh mẽ hơn có thể được sử dụng để học các nhiệm vụ phức tạp hơn, có khả năng từ minh họa con người.
