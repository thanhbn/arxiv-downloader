# 2308.03188.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2308.03188.pdf
# File size: 1240405 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Automatically Correcting Large Language Models:
Surveying the landscape of diverse self-correction strategies
Liangming Pan, Michael Saxon, Wenda Xu,
Deepak Nathani, Xinyi Wang, William Yang Wang
University of California, Santa Barbara
{liangmingpan, saxon, wendaxu, dnathani, xinyi_wang}@ucsb.edu
william@cs.ucsb.edu
Abstract
Large language models (LLMs) have demon-
strated remarkable performance across a wide
array of NLP tasks. However, their efficacy
is undermined by undesired and inconsistent
behaviors, including hallucination, unfaithful
reasoning, and toxic content. A promising ap-
proach to rectify these flaws is self-correction ,
where the LLM itself is prompted or guided to
fix problems in its own output. Techniques
leveraging automated feedback —either pro-
duced by the LLM itself or some external
system—are of particular interest as they are a
promising way to make LLM-based solutions
more practical and deployable with minimal
human feedback. This paper presents a compre-
hensive review of this emerging class of tech-
niques. We analyze and taxonomize a wide
array of recent work utilizing these strategies,
including training-time, generation-time, and
post-hoc correction. We also summarize the
major applications of this strategy and conclude
by discussing future directions and challenges.
1 Introduction
Recent years have seen striking empirical successes
of large language models (LLMs), as they consis-
tently obtain impressive results across a diverse
range of NLP benchmarks (Guo et al., 2023; Suz-
gun et al., 2023; Qin et al., 2023), while also show-
casing surprising abilities of language understand-
ing (Wei et al., 2022a; Begus et al., 2023), gen-
eration (Pu and Demberg, 2023; Lin and Chen,
2023; Lyu et al., 2023a), and reasoning (Wei et al.,
2022b; Kojima et al., 2022; Dasgupta et al., 2022).
However, these models are not without their flaws.
LLMs are observed to intermittently display unde-
sired and inconsistent behaviors such as produc-
ing seemingly convincing but inaccurate “hallu-
cinations” (Lin et al., 2022; Zhang et al., 2023c;
Min et al., 2023), conducting unfaithful reason-
ing (Golovneva et al., 2023; Lyu et al., 2023b; Wu
et al., 2023b), generating inappropriate or harmfulcontent (Gehman et al., 2020; Levy et al., 2021,
2022; Shaikh et al., 2023), and failing to trustfully
follow rules and constraints (Zhuo et al., 2023;
Wang et al., 2023a). Such flawed behaviors ham-
per the trust in LLMs and pose hurdles to their
real-world applications (OpenAI, 2023).
A prevailing strategy to rectify these undesired
behaviors of LLMs is learning from feedback , mir-
roring a typical human learning strategy where in-
dividuals actively refine their behaviors through a
cycle of trial, error, and correction. Humans, when
making mistakes, often gather feedback either from
others or through self-reflection. Such feedback of-
fers valuable insights into missteps and proposes
potential avenues for improvement. With feedback,
humans can adapt and modify their behavior ac-
cordingly, learning to correct their mistakes over
time. Inspired by this natural learning mechanism,
extensive research (Huang et al., 2022; Madaan
et al., 2023; Gero et al., 2023; Jiang et al., 2023)
has been undertaken to improve LLMs through the
paradigm of learning from feedback.
One popular line of research involves the use
ofhuman feedback to evaluate and refine models,
as encapsulated in the survey by Fernandes et al.
(2023). These methods typically involve direct op-
timization of LLMs against human feedback on
their outputs (Kreutzer et al., 2018; Glaese et al.,
2022; Ouyang et al., 2022; Scheurer et al., 2023),
wherein human evaluations of output quality serve
as a reward signal for improving the model perfor-
mance. However, this approach has two primary
drawbacks: it can be costly due to the manual la-
bor involved, and it lacks real-time capabilities as
humans cannot provide instant feedback.
To minimize the need for human intervention,
another strategy is self-correcting LLMs with au-
tomated feedback , where the model (iteratively)
learns from automatically generated feedback sig-
nals to understand the consequences of its actions
and adapts its behaviors. The source of automated
1arXiv:2308.03188v2  [cs.CL]  30 Aug 2023

--- PAGE 2 ---
Language ModelPatient
TreatmentDiagnosisDoctorCritic ModelRefine ModelOutput
FeedbackHumanLanguage ModelExternal ToolsExternal MetricsExternal KnowledgeSupervised LearningReinforcement LearningIn-Context LearningSelf-TrainingFeedback-guided GenerationStrategy
LearningGenerate-then-RankPost-hoc RevisionInput
Trained ModelProgram ExecutorOther ToolsHallucinationUnfaithful ReasoningFlawed CodesToxic Contents
Scalar ValueNatural LanguageFigure 1: A conceptual framework for self-correcting LLMs with automated feedback. We identify three parties
involved in the prototypical correction pipeline that are analogous to a patient, doctor, and treatment in medicine,
respectively: a Language Model produces initial output, a Critic Model analyzes the output and provides feedback,
and a Refine Model provides treatment to either the output or the language model. We taxonomize existing works
using this conceptualization along five key aspects: the problem to be corrected, the source andformat of the
feedback, and the strategy andlearning method of the refine model.
feedback can be multifaceted, spanning from the
LLM itself acting as the feedback model (Madaan
et al., 2023; Schick et al., 2023), a separately
trained feedback model (Yang et al., 2022b; Paul
et al., 2023), readily available external tools (Gou
et al., 2023; Chen et al., 2023d), to external knowl-
edge sources such as Wikipedia or the internet (Yu
et al., 2023; Li et al., 2023b). Different strategies
have been proposed to correct LLM with automated
feedback, including self-training (Huang et al.,
2022; Bai et al., 2022b), generate-then-rank (He
et al., 2023; Weng et al., 2023), feedback-guided
decoding (Yang et al., 2022a; Xie et al., 2023), iter-
ative post-hoc revision (Zhang et al., 2023a; Jiang
et al., 2023), etc. Recently, the incorporation of
such strategies has demonstrated their effectiveness
across a myriad of tasks, from question answer-
ing (Peng et al., 2023) and reasoning (Pan et al.,
2023) to code generation (Zhang et al., 2023b) and
toxicity detection (Lu et al., 2022).
In light of these advancements, our paper aims
to provide a comprehensive survey. We start by
establishing the concept of self-correcting LLMs
with automated feedback and creating a taxonomy
of the different methods (§ 2). We then discuss
the major techniques, categorized as training-time
correction (§ 3), generation-time correction (§ 4),
and post-hoc correction (§ 5). We then summarize
the major application areas of this strategy (§ 6).
Finally, we discuss key future directions (§ 7).2A Taxonomy for Correcting LLMs with
Automated Feedback
For the sake of clean exposition, we first present a
conceptual framework outlining the overall process
of correcting LLMs with feedback, thereby estab-
lishing the scope of this survey (§ 2.1). We then
proceed to identify five primary dimensions that
serve as classification criteria for existing works:
1) What gets corrected, 2) What is the source of the
feedback, 3) What is the format of the feedback, 4)
When the feedback is used, and 5) How to correct
the model with feedback (§ 2.2–§ 2.6). Finally, we
summarize existing works in § 2.7.
2.1 Conceptual Framework
We formulate the general process of correcting
LLMs with automated feedback in Figure 1, us-
ing an analogy of medical treatment in our daily
life. Three parties are involved in this process:
•Language Model (Patient) . A language model
M:X → Y performs a specific task by mapping
an input x∈ X to an output text ˆy∈ Y. This for-
mulation encompasses a wide range of NLP tasks,
for example, in summarization, xis a passage, ˆy
is the generated summary; for question-answering,
xis a question and ˆyis the predicted answer. The
initial generation ˆymay be imperfect and suffer
from various problems such as hallucination and
incorrect reasoning.
2

--- PAGE 3 ---
•Critic Model (Doctor & Diagnosis) . A critic
modelC:X ×Y → F learns to generate feedback
x,ˆy→cwhere ˆy∼ M (x)is the output or partial
output of the language model, and cis the feed-
back of some format, e.g., scalar value, or natural
language. A simple example is binary feedback of
whether the output is good or bad given the input
(C:X × Y → { 0,1}).
•Refine Model (Treatment) . A refine model R:
X×Y×F → Y learns to repair an output x,ˆy, c→
ynewbased on the feedback c, where ynewis the
revised output. Besides repairing output, some
refine models directly repair the language model
Mthrough fine-tuning or reinforcement learning.
Based on the above formulation, Figure 1 illus-
trates the fundamental interaction among the lan-
guage model M, the critic model C, and the refine
model R. However, the specific model design in
existing works varies along five crucial axes, which
we will elaborate on in the following sections.
2.2 What gets corrected?
LLM-based natural language systems can exhibit
a variety of errors. We summarize the four major
types of errors that are targeted for correction in
existing works through automated feedback.
•Hallucination. An open challenge for LLMs is
that they often hallucinate by making up facts or
citing sources that do not exist (Li et al., 2023a;
Zhang et al., 2023c). These hallucinated contents
are often quite plausible-sounding, making it diffi-
cult even for humans to detect (Clark et al., 2021).
To address this, several studies have proposed the
collection of automated feedback on potential fac-
tual inaccuracies by cross-referencing the output
generated by the model with credible knowledge
sources. The gathered feedback can then be utilized
by a subsequent refinement model to correct hallu-
cinations (Gao et al., 2023b; Peng et al., 2023).
•Unfaithful Reasoning. LLMs have exhibited a
strong ability in solving complex reasoning tasks
with improved reasoning strategies, such as Chain-
of-Thought prompting (Wei et al., 2022b). How-
ever, recent studies (Golovneva et al., 2023; Ribeiro
et al., 2023; Lyu et al., 2023b) found that LLMs
occasionally make unfaithful reasoning, i.e., the
derived conclusion does not follow the previously
generated reasoning chain. To address this, ex-
isting works have proposed the use of automated
feedback from external tools or models for guidingthe reasoning process (Xie et al., 2023; Yao et al.,
2023a), verifying the reasoning process and rectify-
ing errors (He et al., 2023; Pan et al., 2023), or fine-
tuning LLMs with process-based feedback (Huang
et al., 2022; Lightman et al., 2023).
•Toxic, Biased, and Harmful Contents. LLMs
have been observed to occasionally generate con-
tent that is toxic, biased, or harmful due to biases
present in the training data (Shaikh et al., 2023).
To rectify this, reinforcement learning from hu-
man feedback (RLHF) (Ouyang et al., 2022; Bai
et al., 2022a) has been extensively employed to
train LLMs to align more closely with human val-
ues, such as being helpful, honest, and harmless.
However, RLHF is heavily dependent on high-
quality human feedback, the collection of which
can be resource-intensive. To alleviate this, recent
works (Lu et al., 2022; Gou et al., 2023) have also
explored collecting automated feedback to identify
and correct potentially harmful outputs.
•Flawed Code. Besides generating natural lan-
guage text, LLMs also show strong abilities to gen-
erate computer programs ( i.e., code) (Chen et al.,
2023b). However, the generated code can some-
times be flawed or incorrect. To fix this, the ap-
proach of learning from automated feedback has
been extensively applied in code generation (Chen
et al., 2023d; Olausson et al., 2023), largely fa-
cilitated by the ease of obtaining such feedback
through the execution of generated code with the
corresponding compilers or interpreters.
2.3 What is the source of the feedback?
Feedback can be broadly divided into two cate-
gories: human feedback andautomated feedback .
Fernandes et al. (2023) provided a survey on inte-
grating human feedback for language generation.
In our survey, we focus on the emerging research
area of automated feedback, which explores the
possibility of LLMs to self-correct without constant
human intervention. Automated feedback typically
originates from two sources, distinguished by their
relationship with the LLM: self-feedback (i.e., the
feedback originates from the LLM itself) and ex-
ternal feedback (i.e., the feedback is derived from
external models, tools, or knowledge sources).
•Self-Feedback. The LLM itself can be utilized
as a feedback provider. One straightforward way
is to directly evaluate the quality of the generated
outputs through prompting and subsequently use
this feedback to refine the results (Madaan et al.,
3

--- PAGE 4 ---
2023; Shinn et al., 2023). This process can be itera-
tive, with the model continually refining its output
until it meets a certain standard. This continuous
self-improvement strategy has been found particu-
larly useful by numerous studies (Ye et al., 2023;
Yan et al., 2023a), especially in scenarios where
external feedback is unavailable or limited.
•External Feedback. Feedback can originate
from sources external to the LLM, typically includ-
ing 1) other trained models (Yang et al., 2022b;
Lightman et al., 2023), 2) external tools (Gou
et al., 2023; Charalambous et al., 2023), 3) external
knowledge sources (Gao et al., 2023b; Yu et al.,
2023), and 4) external evaluation metrics (Jung
et al., 2022; Welleck et al., 2023). External feed-
back provides a valuable outside perspective which
is particularly useful in identifying errors that the
LLM might not recognize on its own. For example,
code interpreters are widely used in programming
tasks to provide real-time error messages; while ex-
ternal knowledge sources can be utilized to verify
the factual accuracy of the LLM’s output.
2.4 What is the format of the feedback?
The selection of feedback format requires the con-
sideration of its expressivity, the ease of its col-
lection, and its potential to improve systems (Fer-
nandes et al., 2023). In existing works, automated
feedback is typically in the form of a scalar value
signal or in natural language .
•Scalar Value Feedback. In this scenario, the
critic model maps the input and output to a sin-
gle score ( C:X × Y → N ⊆ R). Scalar value
feedback can be easily integrated into the train-
ing/decoding process of LLMs. For example, Self-
Verification (Weng et al., 2023) ranks candidate
outputs to find the optimal one based on the real-
value feedback score assigned by the critic model
to each candidate. Similarly, Xie et al. (2023) use
real-value feedback for each intermediate reason-
ing step to guide the model in performing a stochas-
tic beam search for the optimal solution. However,
despite its flexibility, scalar value feedback is of-
ten less informative to capture detailed information
necessary for model correction.
•Natural Language Feedback. Natural language
feedback offers greater expressivity than scalar
value feedback, providing richer information that
can highlight the shortcomings of the current out-
put or suggest specific improvements. This form
of feedback is particularly crucial for certain ap-plications, such as text editing and code genera-
tion. For text editing, PEER (Schick et al., 2023)
trains an LLM to generate detailed suggestions
for edits to the initial generated text, such as “re-
move unsourced claim” or “rewrote the guacamole
question for clarity”. For code generation, Self-
Debug (Chen et al., 2023d) uses LLMs to generate
explanations for the produced code and utilize both
the explanation and the execution results as feed-
back to enhance coding solutions.
2.5 When to correct the model with feedback?
Depending on the timing of using automated feed-
back to correct the model, existing works can be
divided into three major categories.
•Training-time Correction. The ideal scenario is
to rectify a flawed model during training, prior
to its deployment for use. Once feedback has
been collected, it is directly used to optimize the
model parameters. Human feedback is typically
used for training-time correction, as exemplified
by the widely adopted RLHF approach (Ouyang
et al., 2022). For leveraging automated feedback,
a common strategy is self-training (Huang et al.,
2022), where the model is trained with its own
generated high-quality output filtered out by the
critic model. While training-time correction is a
pre-hoc strategy that addresses problems during
training, its practical application may be hindered
by three issues: 1) the infeasibility of fine-tuning
gaint closed-source LLMs, such as GPT-4 (OpenAI,
2023), 2) the potential unavailability of feedback
during model training, and 3) the requirement for
the feedback to be “optimizable”, e.g., a numerical
score severing as the basis for model optimization.
•Generation-time Correction. This strategy uti-
lizes automated feedback to guide the language
model during generation, which allows the model
to correct errors in its outputs as it is being gener-
ated. For example, for proof generation, several
works utilize the automated feedback of the in-
termediate reasoning steps to guide the model to
recover from incorrect generation and search for
the optimal solution in a more efficient way (Yang
et al., 2022a; Lightman et al., 2023).
•Post-hoc Correction. Finally, post-hoc correc-
tion involves refining the model output after it has
been generated , without updating the model param-
eters. This typically involves an iterative process of
generating output, receiving feedback, and refining
output. Post-hoc correction provides more flexibil-
4

--- PAGE 5 ---
MethodFeedback Model RefinementApplicationSource Format Strategy Learning
Training-Time Correction
RLHF (Ouyang et al., 2022) Reward Model Scalar RLHF RL Multiple Tasks
Fine-Grained RLHF (Wu et al., 2023a) Reward Model Scalar RLHF RL Detoxification, Long-form QA
HH-RLHF (Bai et al., 2022a) Reward Model Scalar RLHF SL & RL Helpfulness, Harmlessness
Moral RLHF (Ganguli et al., 2023) Reward Model Scalar RLHF RL Moral Correction
Sparrow (Glaese et al., 2022) Reward Model NL RLHF SL & RL Dialogue
ILF (Scheurer et al., 2023) Human Feedback NL Fine-tuning SL Summarization
ILF-Code (Chen et al., 2023a) Human Feedback NL Fine-tuning SL Code Generation
SLT (Yuan et al., 2023) Human Feedback NL Fine-tuning SL Response Generation
Gao et al. (2023a) Human Feedback NL Fine-tuning SL & RL Extractive QA
Chain-of-Hindsight (Liu et al., 2023) Human Feedback NL Fine-tuning SL Multiple Tasks
Quark (Lu et al., 2022) External Metrics Scalar Fine-tuning RL Toxicity, Repetition, Sentiment
SimCLS (Liu and Liu, 2021) External Metrics Scalar Fine-tuning SL Summarization
BERTTune (Unanue et al., 2021) External Metrics Scalar Fine-tuning RL Machine Translation
STaR (Zelikman et al., 2022) Language Model NL Self-Training SL QA, Reasoning
Self-Instruct (Wang et al., 2023d) Language Model NL Self-Training SL Multiple Tasks
RLAIF (Bai et al., 2022b) Language Model NL Self-Training SL & RL Dialogue
SIRLC (Pang et al., 2023) Language Model NL Self-Training RL Reasoning, Translation, Summary
Self-Improve (Huang et al., 2022) Language Model NL Self-Training SL QA, Reasoning, NLI
AlpacaFarm (Dubois et al., 2023) Language Model NL Self-Training SL & RL None (Intrinsic Evaluation)
ReST (Gulcehre et al., 2023) Language Model NL Self-Training RL Machine Translation
Generation-Time Correction
Self-Verification (Weng et al., 2023) Language Model Scalar Re-Ranking ICL Arithmetic Reasoning
CodeT (Chen et al., 2023b) Program Executor Scalar Re-Ranking ICL Code Generation
LEVER (Ni et al., 2023) Program Executor Scalar Re-Ranking SL Table QA, Math QA, Program
RR (He et al., 2023) External Knowledge Scalar Re-Ranking — Reasoning
InstructScore (Xu et al., 2023b) Language Model NL Re-Ranking SL Generation Evaluation
MBR Decoding (Freitag et al., 2022) External Metrics Scalar Re-Ranking SL Machine Translation
DIVERSE (Li et al., 2023d) Trained Model Scalar Re-Ranking SL Arithmetic Reasoning
TeachMe (Mishra et al., 2022) Human Feedback NL Feedback-guided SL Proof Generation
PRM (Lightman et al., 2023) Reward Model Scalar Feedback-guided SL Arithmetic Reasoning
DiffusionLM (Li et al., 2022) Trained Model Scalar Feedback-guided SL Controlled Text Generation
PPLM (Dathathri et al., 2020) Trained Model Scalar Feedback-guided SL Controlled Text Generation
Fudge (Yang and Klein, 2021) Trained Model Scalar Feedback-guided SL Controlled Text Generation
Entailer (Tafjord et al., 2022) Trained Model Scalar Feedback-guided SL Proof Generation
NLProofS (Yang et al., 2022a) Trained Model Scalar Feedback-guided SL Proof Generation
GRACE (Khalifa et al., 2023) Trained Model Scalar Feedback-guided SL Arithmetic Reasoning
CoRe (Zhu et al., 2023) Trained Model Scalar Feedback-guided SL Arithmetic Reasoning
Varshney et al. (2023) External Knowledge NL Feedback-guided ICL Hallucination Detection
MemPrompt (Madaan et al., 2022) External Knowledge NL Feedback-guided ICL Lexical and Ethical Reasoning
Maieutic Prompting (Jung et al., 2022) External Metrics Scalar Feedback-guided ICL Commonsense Reasoning
SI (Creswell and Shanahan, 2022) Language Model Scalar Feedback-guided ICL Proof Generation
RAP (Hao et al., 2023) Language Model Scalar Feedback-guided ICL Planning, Reasoning
SelfEval-Decoding (Xie et al., 2023) Language Model Scalar Feedback-guided ICL Arithmetic / Symbolic Reasoning
SelfCheck (Miao et al., 2023) Language Model NL Feedback-guided ICL Arithmetic Reasoning
Tree of Thoughts (Yao et al., 2023a) Language Model NL / Scalar Feedback-guided ICL Games, Writing
Table 1: Representative works on Training-time Correction andGeneration-Time Correction . We summarize
the key features for each work: the source andformat of the feedback, the strategy andlearning method of the
refinement process, and the application of the method.
ity than the previous two strategies as it does not
require training the LLM or accessing its param-
eters. Furthermore, post-hoc correction enhances
explainability as it facilitates the incorporation of
more informative natural language feedback. This
allows for a more transparent visualization and in-
terpretation of the self-correction process.
2.6 How to correct the model with feedback?
Various concrete strategies have been proposed to
correct LLMs with automated feedback, which are
tailored to the different dimensions we mentioned
in previous sections. For example, self-training isoften used for training-time correction. Generate-
then-rank often comes with scalar value feedback.
Self-refine is the strategy that uses the same LLM as
both the critic model and the refine model. We will
cover the landscape of self-correction strategies
through Section 3 to Section 5.
2.7 Summary of existing works
Building upon the taxonomy established in the pre-
ceding sections, we collate existing works on cor-
recting LLMs with (automated) feedback in Table 1
and Table 2. We have two major selection criteria
for a work to be included in this survey:
5

--- PAGE 6 ---
MethodFeedback Model RefinementApplicationSource Format Strategy Learning Iter.
Post-hoc Correction
Self-Refine (Madaan et al., 2023) Language Model NL Self-Refine ICL Multiple Tasks
Clinical SV (Gero et al., 2023) Language Model NL Self-Refine ICL Information Extraction
Reflexion (Shinn et al., 2023) Language Model NL Self-Refine RL QA, Code Generation
IterRefinement (Chen et al., 2023c) Language Model NL Self-Refine ICL Machine Translation
Auto-Post-Editing (Raunak et al., 2023) Language Model NL Self-Refine ICL Machine Translation
RCI (Kim et al., 2023) Language Model NL Self-Refine ICL Computer Tasks
SelFee (Ye et al., 2023) Language Model NL Self-Refine SL Dialogue
SelfCheckGPT (Manakul et al., 2023) Language Model NL Self-Refine ICL Hallucination Detection
LLM Self Defense (Helbling et al., 2023) Language Model NL Self-Refine ICL Harmful Text Correction
Re3(Yang et al., 2022b) Trained Model Scalar External Feedback SL & ICL Story Generation
CodeRL (Le et al., 2022) Trained Model Scalar External Feedback RL Code Generation
FLIRT (Mehrabi et al., 2023) Trained Model Scalar External Feedback ICL Adversarial Prompt Generation
REFINER (Paul et al., 2023) Trained Model NL External Feedback SL & ICL Reasoning, Moral Story
RL4F (Akyürek et al., 2023) Trained Model NL External Feedback SL & RL Planning, Summarization
Yan et al. (2023a) Trained Model NL External Feedback SL Semantic Parsing
Baldur (First et al., 2023) Trained Model NL External Feedback ICL Proof Generation
CRITIC (Gou et al., 2023) External Tools NL External Feedback ICL QA, Program, Toxicity
FacTool (Chern et al., 2023) External Tools NL External Feedback ICL QA, Reasoning, Generation
RARR (Gao et al., 2023b) External Knowledge NL External Feedback ICL Open-Domain QA
LLM-Augmenter (Peng et al., 2023) External Knowledge NL External Feedback RL Open-Domain QA
Self-Checker (Li et al., 2023b) External Knowledge NL External Feedback ICL Fact-Checking
REFEED (Yu et al., 2023) External Knowledge NL External Feedback ICL QA, Dialogue
Olausson et al. (2023) Program Executor NL External Feedback ICL Code Generation
Self-Edit (Zhang et al., 2023a) Program Executor NL External Feedback ICL Code Generation
Self-Debug (Chen et al., 2023d) Program Executor NL External Feedback ICL Code Generation
Self-Evolve (Jiang et al., 2023) Program Executor NL External Feedback ICL Code Generation
Logic-LM (Pan et al., 2023) Symbolic Solver NL External Feedback ICL Logical Reasoning
Self-Critique (Saunders et al., 2022) LLMs + Human NL External Feedback SL Summarization
ALGO (Zhang et al., 2023b) Oracle Verifier Scalar External Feedback ICL Code Generation
Charalambous et al. (2023) BMC Tool NL External Feedback ICL Software Verification
Self-Correction (Welleck et al., 2023) External Metrics NL / Scalar External Feedback SL Reasoning, Generation, Toxicity
Multiagent Debate (Du et al., 2023) Language Model NL Model Debate ICL Reasoning, Factuality
LM vs LM (Cohen et al., 2023) Language Model NL Model Debate ICL Factual Error Detection
ICL-AIF (Fu et al., 2023) Language Model NL Model Debate ICL Bargaining Game
PRD (Li et al., 2023c) Language Model NL Model Debate ICL Open-ended QA
Table 2: Representative works on Post-hoc Correction . We summarize the key features for each work: the source
andformat of the feedback, the strategy andlearning method of the refinement process, whether the refinement is
iterative (Iter.) , and the application of the method.
1.Automated Feedback : Explicit feedback is
involved to assess the quality of the model out-
put. We focus on automated feedback that origi-
nates from external models, metrics, knowledge,
etc. However, we will cover some representative
works of human feedback for completeness.
2.Model Refinement : The feedback should act
as a directive to enhance the LLM, either by: 1) up-
dating model parameters, or 2) altering the model’s
output during or post the generation process.
These works are categorized based on the three
strategies introduced in Section 2.5. We also sum-
marize key features of each work, including: 1)
the source of feedback, 2) the format of feedback,
3) the strategy and learning method employed for
the refinement, 4) whether the refinement process
is iterative, and 5) the application of the method.
Subsequently, we will delve into a detailed review
of each method type, encompassing Training-Time
Correction (§ 3), Generation-Time Correction (§ 4),
andPost-hoc Correction (§ 5).3 Training-Time Correction
In this section, we delve into methodologies that
rectify model behavior during the training phase.
As depicted in Figure 2, we identify three typical
strategies for training-time correction. Each strat-
egy utilizes different forms of feedback to modify
the model parameters during training: human feed-
back (a), a reward model that approximates human
feedback (b), and automated feedback (c).
3.1 Learning from Human Feedback
The next-word prediction objective of LLM pre-
training is not inherently designed to encapsulate
human values or preferences. This misalignment
can lead to unintended consequences, such as the
generation of harmful, misleading, or biased con-
tent (Kenton et al., 2021). To mitigate these issues,
many research efforts have explored the integration
of human feedback to better align LLMs with hu-
man values and expectations. Wang et al. (2023e)
and Fernandes et al. (2023) extensively reviewed
6

--- PAGE 7 ---
Language ModelOutputsFeedback
Critic Model
Training
Feedback DataHuman AnnotatorsLanguage Model
RewardModel
Outputs
Reward/
Training with RLLanguage Model
OutputsFeedback
Human AnnotatorsLanguage Model
Outputs
High-quality Outputs
Training(a) Direct Optimizing Human Feedback(b) Reward Modeling and RLHF(c) Self-Training
Reward Model TrainingFigure 2: Three typical strategies of training-time correction : directly optimization with human feedback (a),
training a reward model that approximates human feedback (b), and self-training with automated feedback (c).
this research area. Our survey, however, focuses on
automated feedback, thus we will only touch upon
representative works in this direction.
Direct Optimization with Human Feedback. In
an ideal scenario, we would directly leverage hu-
man feedback to optimize the model parameters.
Typically, this approach follows the framework de-
picted in Figure 2(a): 1) Candidate outputs are gen-
erated by LLMs, 2) Humans provide feedback or
refinements on these outputs, and 3) LLMs are then
directly optimized on the collected (outputs, feed-
back) to better align with human preferences. A
simple strategy is to fine-tune the model on the out-
puts with positively-labeled feedback. For example,
Sparrow (Glaese et al., 2022) fine-tunes LLMs on
the collected dialogues rated as preferred and rule
compliant (concerning correctness, harmfulness,
and helpfulness), according to humans. Similarly,
Scheurer et al. (2023) utilizes an LLM to generate
multiple refinements of the original output based
on human feedback, and then the best refinement is
picked up to finetune the original LLM. A similar
idea is adopted to fine-tune the code generation
model (Chen et al., 2023a). First, human annota-
tors write natural language feedback for incorrect
codes. A refinement model then utilizes this feed-
back to correct the code. Finally, the refined code
is subsequently employed to fine-tune the code-
generating LLM. However, only utilizing positive
data (human-refined or positive-rated data) for fine-
tuning may constrain the model’s ability to identify
and correct negative attributes or errors. To ad-
dress this, Chain-of-Hindsight (Liu et al., 2023)
fine-tunes the LLM on model outputs paired with
both positive and negative feedback. Beyond fine-
tuning, other optimization methods are exploredas well. For example, Gao et al. (2023a) utilizes
human feedback as the reward signal and optimizes
the model with contextual bandit learning.
Reward Modeling and RLHF. Employing hu-
man feedback directly to rectify model behavior
may not always be practical. The collection of hu-
man feedback can be both labor-intensive and time-
consuming. An efficient alternative is to train a
reward model that emulates human feedback. Once
trained, this reward model can provide consistent,
real-time feedback for every model output, thereby
circumventing the need for constant human involve-
ment. A prominent example of this approach is
Reinforcement Learning from Human Feedback
(RLHF) (Ouyang et al., 2022), as illustrated in Fig-
ure 2(b). It first asks human annotators to label the
preference for different LLM outputs and then train
the reward model to predict the human preference.
Afterward, reinforcement learning (RL) algorithms
(e.g., Proximal Policy Optimization (PPO) (Schul-
man et al., 2017)) are employed to optimize the
model. RLHF and its variants have proven effec-
tive in correcting LLMs to become more beneficial
and less harmful (Bai et al., 2022a), as well as
instilling moral correctness (Ganguli et al., 2023).
3.2 Learning with Automated Feedback
Since collecting human feedback is quite resource-
intensive, numerous studies have explored the use
of automated feedback to minimize the demand
for human intervention. To differentiate between
human and automated feedback, we define human
feedback as a quality assessment performed by hu-
man evaluators on the outputs generated by the
base model. This feedback is then used for either
direct optimization or reward model learning (Sec-
7

--- PAGE 8 ---
tion 3.1). On the other hand, automated feedback
is collected in an offline environment, without the
need for human assessment of model outputs. We
mainly discuss training time strategies utilizing two
types of automated feedback: extrinsic feedback
from external metrics/models, and intrinsic feed-
back from the language model itself.
External Metric Guidance. Feedback provided
by external metrics has been frequently used for
training-time correction. Due to the discrete na-
ture of metric signals, most approaches focus on
non-differentiable training techniques. Minimum
risk training (Shen et al., 2016) optimizes model
parameters with external evaluation metrics (Xu
et al., 2022, 2023a), by incorporating metric score
with maximum log-likelihood in the loss function.
It can optimize metric scores during training time.
However, it can lead the model to the robustness
deficiencies of some metrics (Yan et al., 2023b),
such as BLEURT (Sellam et al., 2020). Liu and
Liu (2021) leverages a contrastive learning frame-
work to rerank candidates based on metric scores,
which bridges the gap between training and infer-
ence objectives. Li et al. (2019) employs deep
RL algorithm and Unanue et al. (2021) leverage
Gumbel softmax (Jang et al., 2017) to build distri-
butional semantic reward from BERTScore (Zhang
et al., 2020) and mitigate exposure bias. To stabi-
lize gradients, Wu et al. (2021) utilizes contrastive
discriminator and PPO to imitate human texts. Re-
cently, Chang et al. (2023) propose a more efficient
RL algorithm, RLGF, than PPO (Schulman et al.,
2017) to finetune LLM with pre-defined reward.
They integrate a reasonable but incomplete guide
policy into a policy gradient framework and learn
a near-optimal strategy. Different from leveraging
feedback solely at fine-tuning, Korbak et al. (2023)
employs conditional training (Keskar et al., 2019)
and an automated classifier to tag undesirable con-
tents at the pretraining stage.
Self-Training. Instead of leveraging external
metrics as feedback, the language model itself can
be used to provide feedback for its own output.
This gives rise to the self-training strategy of self-
improving LLM by bootstrapping its original out-
puts, as depicted in Figure 2(c). STaR (Zelikman
et al., 2022) leverages the idea of CoT by prompt-
ing LLM to generate answers with rationales. By
selecting rationales leading to the correct answer
to further finetune LLM, the performance of LLMis improved. This process can be iterated with
further performance gains. Huang et al. (2022) fol-
lows this idea by applying self-consistency (Wang
et al., 2023c) to majority vote reasoning paths (the
paths that lead to the most voted answers). LLM is
finetuned over selected reasoning-answer data with
augmented prompts. This strategy has also been
used to reduce the harmful responses of LLMs.
RLAIF (Bai et al., 2022b) adopted the strategy of
critique →revision →supervised learning. The
initial toxic responses are criticized and revised by
the LLM itself following a set of human-defined
principles. Afterward, the LLM is fine-tuned on
the revised responses. AlpacaFarm (Dubois et al.,
2023) further shows that LLMs can self-improve
with RL. It designs LLM prompts to simulate hu-
man feedback in RLHF and shows that the feed-
back is effective and greatly reduces the cost. Gul-
cehre et al. (2023) further improves self-training by
proposing the Reinforced Self-Training (ReST). It
iteratively performs the following two steps to im-
prove the LLM: 1) the Grow step produces a dataset
by sampling from the policy model ( i.e., the current
LLM), and 2) the Improve step optimizes the LLM
policy using offline RL algorithms.
4 Generation-Time Correction
Correcting LLMs at training time appears to be
the ideal solution given the principle of “an ounce
of prevention is worth a pound of cure”. However,
there is no guarantee that all undesired behavior can
be solved at training time. Moreover, training-time
correction might be excessively resource-intensive
or even impractical for many LLMs, e.g., closed-
source LLMs where weights are inaccessible, and
colossal LLMs with billions of parameters. This
motivates the exploration of methods that seek to
correct LLMs during the generation time or after
the output is generated. This section focuses on
generation-time correction techniques, in which au-
tomated feedback serves as a guiding mechanism
for LLM generation. Such a strategy allows LLMs
to rectify errors during generation without modify-
ing the model parameters. We identify two primary
strategies for generation-time correction: Generate-
then-Rank , and Feedback-Guided Decoding .
4.1 Generate-then-Rank
The most immediate strategy involves sampling a
large number of candidate generations and subse-
quently picking up the best generation based on
8

--- PAGE 9 ---
Language Model
𝑂!(!)𝑂!($)𝑂!(%)Critic Model
⋯𝑂%(!)
𝑂%(%)
𝑂%($)𝑂%(&)
𝑂'(!)
𝑂'(%)
𝑂'($)
𝑂'(&)
feedbackfeedbackfeedback
(b) Feedback-Guided Decoding(a) Generate-then-RankLanguage Model
Output 1Output 2Output N⋯Critic Model
Best OutputFigure 3: The illustrations of the two typical strategies
ofgeneration-time correction : (a) Generate-then-Rank,
and (b) Feedback-Guided Decoding.
the feedback provided by the critic model, as il-
lustrated in Figure 3(a). Here, the critic model C
aims to learn the mapping x,ˆy1,···,ˆyN→ybest,
where ybestis the best output among the Ncandi-
date outputs ˆy1,···,ˆyN∼ M (x).
This approach is often integrated with the Chain-
of-Thought (CoT) prompting method (Wei et al.,
2022b) to tackle complex reasoning tasks, such as
solving math word problems as in GSM8K (Cobbe
et al., 2021). Given an input problem x, the
LLM initially generates multiple candidate solu-
tionsy1,···, yn. Each solution yi= [zi, ai]com-
prises a reasoning path (explanation) zileading to
the predicted answer ai. Subsequently, the critic
modelCassigns a plausibility score sito each can-
didate reasoning path zi. The final selection of the
best solution from the scored set (zi, ai, si)n
i=1is
achieved via either ranking or voting.
Different critic models have been proposed
in various works. For instance, DIVERSE (Li
et al., 2023d) trains a binary verifier based on De-
BERTa (He et al., 2021), using reasoning paths
that correspond to the correct final answer as posi-
tive examples and the others as negative examples.
The best answer is then determined by a major-
ity vote of positively-verified candidates. Weng
et al. (2023) introduced a training-free critic model
based on the idea of self-verification, in which the
plausibility score is calculated by assessing the con-sistency between the results of forward reasoning
and backward reasoning. In a different vein, the
RR (He et al., 2023) presented a critic model to
assess the faithfulness of each reasoning path by
retrieving supporting information from a knowl-
edge base. LEVER (Ni et al., 2023) employed this
strategy in language-to-code generation, with each
solution yiserving as a candidate SQL program for
the question x. A verifier was trained to predict
the likelihood of a program’s correctness based on
the program itself and its execution results. A simi-
lar idea is adopted in CodeT (Chen et al., 2023b)
where multiple code solutions and the test cases are
generated by the LLM and the best code solution
is selected by a dual execution agreement.
4.2 Feedback-Guided Decoding
The generate-then-rank method, in which the critic
model offers output-level feedback on the entire rea-
soning path, has certain limitations: 1) The output-
level feedback is not fine-grained enough to pin-
point the exact error locations, 2) The extensive
length of the output can complicate its quality as-
sessment, and 3) This method does not facilitate
fine-grained control over the generation process.
For example, the LLM cannot correct its errors dur-
ing the generation process but must wait until the
entire output has been generated.
To address these issues, several works have
adopted the feedback-guided decoding strategy
shown in Figure 3(b), which relies on step-level
feedback to offer fine-grained guidance over the
generation process. Here, the generation of the out-
putyis broken down into multiple reasoning steps
(or thoughts), i.e.,yi= [o1, o2,···, on]. At each
individual reasoning step t, the critic model pro-
vides feedback C(x, o 1:t−1, ot)that indicates the
quality of otas a candidate step. With the ability to
generate and evaluate individual steps, a search al-
gorithm, such as beam search or depth-first search,
can be employed for a systematic exploration of the
output space, which effectively steers the decoding
process toward the generation of an optimal solu-
tion. This also allows the LLM to recover from its
early mistakes during generation and helps allevi-
ate the reasoning inconsistency problem (Zelikman
et al., 2022; Creswell and Shanahan, 2022), i.e.,
incorrect reasoning leads to correct final answer.
The feedback-guided decoding strategy has been
applied in many recent works, such as Tree-of-
Thought (Yao et al., 2023a), GRACE (Khalifa et al.,
9

--- PAGE 10 ---
2023), and RAP (Hao et al., 2023). These works
mostly differ in how to obtain the critic model that
provides automated step-level feedback, the most
challenging but crucial element of this strategy.
We classify their employed methods into four cate-
gories: human feedback, a trained verifier, external
metrics, external knowledge, and self-evaluation.
•Reward Model from Human Feedback. One
approach involves training a step-level reward
model by gathering human feedback, much like
the methods discussed in Section 3.1. Uesato et al.
(2022) ask human annotators to evaluate the cor-
rectness of each reasoning step for the problems in
GSM8K and subsequently train a binary reward
model. Lightman et al. (2023) expand this ap-
proach by annotating a larger dataset consisting
of 800K instances of human step-level feedback.
Both studies discovered that step-level feedback
assists in training a more reliable reward model,
which enhances the faithfulness of reasoning.
•Training Verifier with Synthetic Data. Consid-
ering the high cost of collecting human annotations
and their limited scalability, some works (Yang
et al., 2022a; Tafjord et al., 2022; Li et al., 2023d;
Khalifa et al., 2023) have trained a step-wise
verifier using automatically constructed training
data. Positive examples are derived from ground-
truth reasoning paths, while negative examples
are synthesized by proposing an alignment algo-
rithm (Khalifa et al., 2023) or by making text per-
turbations on positive samples (Yang et al., 2022a).
•Feedback from External Metric. Several works
also leverage external metrics to re-rank or guide
text generation. Freitag et al. (2022) uses mini-
mum bayes risk decoding on unbiased samples to
optimize neural metrics as an alternative to beam
search. Plug and play (Dathathri et al., 2020) com-
bines a pretrained model with attribute classifiers
that guide text generation without any further train-
ing of the model. It leverages the gradient of the
classifier to update LM and increase the likelihood
of the desirable attribution at the text generation of
LM. FUDGE (Yang and Klein, 2021) reweights the
model predictions at each token and estimates the
attribution classification at each partial sequence.
Following up to the gradient-based approach, Dif-
fusionLM (Li et al., 2022) obtains a sequence of in-
termediate latent variables by denoising a sequence
of Gaussian vectors. It performs the iterative gra-
dient updates over latent representations to satisfycontrolled requirements from an attribute classifier.
•Feedback from External Knowledge. External
knowledge sources have also been used to guide the
LLM in generation. Varshney et al. (2023) retrieves
relevant knowledge from Wikipedia as evidence to
validate and correct LLM’s generated sentences at
each step. Once a non-factual sentence is corrected,
the revised sentence is added back to the input
along with the prior generations to continue gen-
erating the next sentence. In a different approach,
MemPrompt (Madaan et al., 2022) leverages prior
user feedback as a knowledge source. It maintains
an external pool of user feedback and searches it
for responses that match the intent of the current
query. The retrieved feedback is then concatenated
with the input to guide the following generation.
•Self-Evaluation. Some studies have utilized
a more flexible strategy, employing the LLM it-
self as the critic model by designing appropriate
prompts. For instance, in Tree-of-Thought (Yao
et al., 2023a), the LLM is prompted to assess the
value of the current state by producing a scalar
value ( e.g., 1-10) or short phrases ( e.g., sure/like-
ly/impossible). Xie et al. (2023) employed a similar
approach by prompting the LLM with “Is the above
step of reasoning: (A) Correct (B) Incorrect”. Self-
evaluation provides an efficient evaluation method
without requiring task-specific verifier fine-tuning.
Existing works also adopted different strategies
to control the decoding process with the help of the
step-level critic model. Tree-of-Thought employed
breadth-first search and depth-first search, while
GRACE (Khalifa et al., 2023) and Xie et al. (2023)
adopted the beam search strategy. At each step,
the top- kscoring candidates are selected for sub-
sequent generations. This process is repeated until
the final answer is generated. Instead, CoRe (Zhu
et al., 2023) and RAP (Hao et al., 2023) adopted
the Monte Carlo Tree Search (MCTS) to strike a
proper balance between exploration and exploita-
tion to find the best reasoning path more efficiently.
5 Post-hoc Correction
The success of generation-time correction heavily
depends on the critic model’s capability in provid-
ing accurate quantifiable feedback for intermediate
outputs. However, this might be quite challeng-
ing for many NLP tasks, such as summarization,
due to the holistic nature of the evaluation, i.e.,
the summary can only be accurately assessed after
10

--- PAGE 11 ---
(a) Self-CorrectionLanguage Model
OutputsFeedback
CriticRefine
GenerateCritic ModelLanguage ModelOutputs
FeedbackRefine Model
External Models/Tools
Code InterpreterOther ToolsTrained ModelSearch Engine
Knowledge
(b) Post-hoc Correction with External FeedbackLanguage Model 1
Language Model 2
OutputsOutputsGenerateReviewGenerateReview(c) Multi-Agent DebateFigure 4: Three typical strategies of post-hoc correction : self-correction (a), post-hoc correction with external
feedback (b), and multi-agent debate (c).
the entire summary is generated. This motivates
the employment of post-hoc correction methods,
where both the critic and refine models intervene
only after the entire output is produced. Post-hoc
correction also provides a more effective interface
with various forms of insightful natural language
feedback. This feedback can be as detailed as a
diagnostic report pinpointing exact error locations,
or as general as suggestions for overall writing
improvement. As illustrated in Figure 4, we sum-
marize three primary strategies for post-hoc cor-
rection: Self-Correction ,Correction with External
Feedback , and Multi-Agent Debate .
5.1 Self-Correction
The simplest approach to implement post-hoc cor-
rection is the “Self-Correction” technique, where
an LLM is employed to generate feedback and re-
fine its own output. As depicted in Figure 4(a), an
LLM is initially used to produce a initial output,
and subsequently, the same model acts as a critic
to generate feedback and refine this initial output
based on the received feedback. This process is
typically iterative and continues until an output of
acceptable quality is obtained or a pre-specified
number of iterations are reached.
Self-Refine (Madaan et al., 2023) proposed a
simple-yet-effective self-correction framework by
simply using a single powerful pre-trained LLM to
generate output, provide feedback, and refine the
output based on that feedback. All these steps are
conducted using the same LLM, guided by different
prompts. Similarly, Clinical Self-Verification (Gero
et al., 2023) employs the self-correction framework
to extract patient data from clinical notes. They
specifically generate feedback to find missing ele-
ments in the initially extracted data and to validate
the generated data. The output is then refined by
eliminating unsupported elements. In contrast, Re-flexion (Shinn et al., 2023) highlighted that prior
self-correction research focused on single-turn gen-
eration tasks and failed to retain a record of past
errors. To address this, Reflexion proposed to use
the same self-correction framework with an addi-
tion of a “long-term memory” capable of storing
prior feedback and outputs, thereby avoiding the
repetition of previous mistakes. Also, Reflexion im-
proves Self-Refine by incorporating scalar-valued
feedback and other forms of feedback.
While self-correction has shown effective for
a wide variety of text-generation tasks, this strat-
egy requires the use of powerful, large-scale LLMs
capable of refining text based on provided feed-
back. As noted by Madaan et al. (2023), smaller,
open-source models often struggle to refine their
output effectively, even when the correct feedback
is provided. A possible solution involves explic-
itly training models for this self-correction process.
SelFee (Ye et al., 2023) proposes training a model
to emulate the self-correction process by generat-
ing output, feedback, and a refined solution in an
auto-regressive manner. They use more powerful
LLMs to provide feedback and refinement data,
with data collection facilitated through ChatGPT.
5.2 Models/Tools as Feedback
As self-correction relies on the language model for
feedback, the quality of the feedback is inherently
constrained by the inherent limitations of LLMs,
such as the inability to access up-to-date informa-
tion, take actions, or perform precise mathematical
reasoning. To address this, recent works have in-
vestigated the use of external tools for providing
feedback. Illustrated in Figure 4(b), a broad array
of external tools, including trained models, code
interpreters, and search engines, can be integrated
to provide specialized feedback.
11

--- PAGE 12 ---
Code Interpreter. In code generation, the pro-
gram executor is frequently used as a source of
feedback for refining the initial code written by the
model. For example, Self-Edit (Zhang et al., 2023a)
andSelf-Evolve execute the initial program on ex-
ample test cases and provide the execution results
back as feedback. Afterward, an LLM is prompted
to refine the initial code based on the feedback.
Self-Debug (Chen et al., 2023d) investigated using
program explanation, unit tests, and program in-
terpreter as feedback types. ALGO (Zhang et al.,
2023b) explored a more fine-grained feedback for
code generation. For each problem, it first gen-
erates a reference oracle program that solves the
problem with an exhaustive search. The feedback is
collected by comparing the outputs from the LLM-
generated program with the oracle outputs for a
given set of test inputs. The self-correction strategy
has also been adopted for formal verification of
software. Charalambous et al. (2023) employed
Bounded Model Checking to locate the software
vulnerability and then used the LLM for correction.
Logic Reasoner. Tool-assisted feedback has also
been used to enhance the faithfulness of LLMs’
reasoning. For example, Logic-LM (Pan et al.,
2023) solves a logical reasoning problem by first
translating it into logical form with LLMs and then
performing inference on it with external symbolic
solvers. Due to the complexity of correctly parsing
the problem at the first attempt, a self-refinement
module is introduced to modify inaccurate logical
forms using the error messages returned by the sym-
bolic reasoner as feedback. Similarly, Baldur (First
et al., 2023) uses existing search-based proof assis-
tants as a source of feedback to improve language
models’ ability to generate theorem proofs.
External Knowledge. External knowledge is
also frequently incorporated as a source of feed-
back to detect and revise factual errors in LLM’s
output and to support LLM-generated facts with
evidence or citations. For example, RARR (Gao
et al., 2023b) and REFEED (Yu et al., 2023) di-
rectly prompt LLMs to raise questions about dif-
ferent aspects of the generated output. An external
retriever then searches for evidence to investigate
each query. Finally, a refine model is employed to
amend the output based on any detected discrepan-
cies between the output and the retrieved evidence.
LLM-Augmenter (Peng et al., 2023) proposes a sim-
ilar method but differentiates itself by automati-cally generating natural language feedback based
on the evidence retrieved. This feedback identi-
fies error locations and provides revision sugges-
tions. These models are evaluated on knowledge-
intensive QA tasks. To broaden the adaptability,
FACTOOL (Chern et al., 2023) extends knowledge-
assisted factual error correction to a wider range
of tasks, including code generation, mathematical
reasoning, and scientific literature review.
Trained Model. Existing works also fine-tune
specialized models for feedback generation. These
critic models can then be paired with similar
or more potent language models in an iterative-
refinement cycle. For example, CodeRL (Le et al.,
2022) treats program synthesis as a reinforcement
learning task and trains a critic model whose out-
put optimizes the main model. In contrast, RE-
FINER (Paul et al., 2023) trains a task model to pro-
duce an intermediate representation for problem-
solving, while a critique model provides feedback
on each intermediate training step. The critique
model can subsequently be employed to generate
feedback for larger task models, such as ChatGPT.
Similarly, RL4F (Akyürek et al., 2023) utilizes rein-
forcement learning for training a critic while keep-
ing the downstream task model fixed. The critic
model is initially fine-tuned to produce feedback
given an initial output and then further fine-tuned
using a policy optimization method. The reward is
defined by evaluating the accuracy of the refined
output or comparing it with the ground truth, as-
suming that the downstream model can effectively
refine the output if accurate feedback is provided.
In “red-teaming” type applications, including tar-
geting vulnerabilities in the content filtering capa-
bilities of other generated systems, feedback from
the content filters themselves can be used as a sig-
nal to guide the generation of better adversarial
examples. For example, Feedback Loop In-context
Red Teaming (FLIRT) (Mehrabi et al., 2023) uses
the signal from an explicit image classifier to guide
an LLM to produce adversarial input prompts for
a text-to-image system to generate more unsafe
images for auditing purposes.
Integrating Multiple Tools. Broadening the idea
of tool-assisted feedback, CRITIC (Gou et al.,
2023) integrates a variety of tools in a unified
framework, including program interpreters for cod-
ing feedback, external knowledge and search en-
gines for factual information, calculators for veri-
12

--- PAGE 13 ---
fying mathematical equations, and LLM-based nat-
ural language feedback. Each tool is proficient at
providing feedback for different aspects, contribut-
ing to a more comprehensive feedback system.
5.3 Multi-Agent Debate
Besides integrating external tools, recent studies
have also explored the strategy of debating between
multiple LLMs , drawing inspiration from collabora-
tive intelligence, where diverse perspectives often
converge to a more refined solution. This approach
aims to improve the output quality by employing
multiple instances of LLMs, each proposing and
debating their individual responses over multiple
rounds to arrive at a common final answer.
Du et al. (2023) first applied and evaluated this
strategy in arithmetic reasoning tasks. Each agent
(a duplicate of LLM) initially generates its individ-
ual solution along with justifications. The debate
phase involves collating responses from all agents
and presenting this as context to each agent. Based
on this context, each agent is then instructed to
craft a revised response. The models are found to
converge on a shared solution following multiple
debate iterations. Experiments show that multi-
agent debate leads to improved performance over
the self-correction strategy. Furthering this con-
cept, PRD (Li et al., 2023c) proposed the peer rank
algorithm for better obtaining a consensus answer
after debating. It considers pairwise preferences
between all possible answer pairs from individual
LLMs and uses these preferences to generate a final
ranking of models.
In addition to reasoning tasks, LM vs LM (Co-
hen et al., 2023) further demonstrated the effec-
tiveness of multi-agent debate for detecting factual
errors. The approach involves a generator LLM
creating a claim, while an examiner LLM probes
for factual inaccuracies through a multi-turn inter-
action.To broaden this concept, Fu et al. (2023)
demonstrated that interactions between different
LLMs could mimic human behavior in real-world
tasks. The study showcased this through a bargain-
ing scenario where different LLM agents assumed
the roles of buyer and seller. This further highlights
the versatile applications of multi-agent debates.
6 Applications
Following our above outline of automated correc-
tion techniques, we now will briefly discuss the
various application domains for which automatedcorrection is useful, and point out commonalities
in self-correction strategies, and discuss how im-
provements to performance in self- or feedback-
driven correction will give rise to downstream per-
formance improvements.
6.1 Factual Correction
Many of the aforementioned automated correction
strategies implicitly engage factual correction to
improve various errors in output (Gao et al., 2023b;
Peng et al., 2023). It is therefore natural that these
capabilities can be directly engaged for factuality
detection and factual correction as an end task, such
as in LM vs LM (Cohen et al., 2023) or Multiagent
Debate (Du et al., 2023). When external tool use
is acceptable, retrieved facts can be leveraged to
further improve the factuality of generated text in
a self-correcting manner without pure reliance on
memorized knowledge (Gou et al., 2023).
In short, self-correcting is a foundational tech-
nique in many LLM-based fact correction or fact-
checking systems, and models that are better at
self-directed factual correction will probably be
better in a host of other self-correction settings .
6.2 Reasoning Tasks
In most reasoning tasks , no good references from
which outputs can be sanity-checked are readily
available (Choi, 2023). This is unfortunate, as the
“reasoning” capabilities provided by LLMs—in par-
ticular, their ability to operate on natural language
from instructions specified in natural language—is
a core driver for their popularity. Reasoning tasks
constitute a broad array of problems where this
capacity is most necessary. For example, more
complex multi-hop question answering tasks (Yang
et al., 2018; Chen et al., 2021) can be construed
as requiring both factual correction and reasoning
correction capabilities (Ho et al., 2023). Thus the
question becomes, how can we prompt the LLM
to identify and correct intermediate reasoning er-
rors? Similarly to other application areas, both
pure-LLM-driven and external tool-based error de-
tection techniques have been proposed.
LLM-based implementations of reasoning error
detection include debate-based techniques, which
can be thought of as implicitly rolling consis-
tent reasoning enforcement in the “critic” module
(Cohen et al., 2023; Li et al., 2023c), and self-
refinement techniques (Manakul et al., 2023). In
short, a given passage exhibiting reasoning (eg,
step-by-step (Wei et al., 2022b)) is fed into an LLM,
13

--- PAGE 14 ---
which is prompted to check for and/or correct rea-
soning errors directly. The error detection often
collaborates with a decoding algorithm such as the
beam search to lead the reasoning towards the cor-
rect direction (Hao et al., 2023; Yao et al., 2023a).
External feedback using techniques such as nat-
ural language inference (NLI) can be both directly
leveraged to spot errors as a heuristic for correc-
tion, and as a means to score the quality (Yang et al.,
2022a; Golovneva et al., 2023). However, there are
some open questions regarding the quality of the
supervised learning-based tools like NLI (Srikanth
and Rudinger, 2022; Saxon et al., 2023).
Among different types of reasoning, the self-
correction strategy has been well studied and im-
plemented for arithmetic reasoning , as outlined
in Table 1 and Table 2. One of the reasons for
this skew is the relative ease of verifying interme-
diate reasoning steps within arithmetic problems.
Some recent studies (Pan et al., 2023; First et al.,
2023) have started to extend the application of this
strategy to deductive reasoning. However, the im-
plementation of self-correction in a wider array of
reasoning tasks, including inductive and abductive
reasoning, is still relatively under-explored.
6.3 Code Synthesis
Code generation is a burgeoning application do-
main for LLMs for which correction is particularly
important. Even human programmers tend to write
code through an iterative process of addition and
correction. For humans, strategies such as reading
linter warnings, compiler/runtime errors, and incor-
rect outputs to diagnose necessary changes to the
source are all employed in the software develop-
ment process. Each of these strategies has natural
analogues in the code generation pipeline.
The aforementioned warnings, errors, or outputs
are usually fed directly back into the LLM to guide
the code correction process (Zhang et al., 2023a;
Chen et al., 2023b). After all, compiler failures
are a particularly strong signal that a piece of code
will not work, having great utility in guiding LLM
self-correction. More excitingly, other work pro-
posed to utilize more fine-grained feedback such as
program explanations generated by LLMs (Chen
et al., 2023d) and the comparison with a reference
oracle program (Zhang et al., 2023b).
The above works only empirically show that
LLMs exhibited a remarkable capability for self-
repairing the codes. In recent work, Olausson et al.(2023) further conducted an in-depth analysis of
how and when self-repair works effectively. They
found that self-repair is bottlenecked by the feed-
back stage: substantial performance improvements
were only noticed when feedback was provided by
expert human programmers or GPT-4. This reve-
lation raises intriguing questions, such as whether
self-repair is an emergent ability exclusive to cer-
tain LLMs and how could we endow smaller mod-
els with similar capabilities.
6.4 Other Applications
Open-ended Generation. In addition to han-
dling pure factuality issues, self-correction can
be applied to subjective qualities of the generated
text. These interventions include post-hoc self-
correcting toxic / harmful outputs (Gou et al., 2023;
Welleck et al., 2023; Helbling et al., 2023), enhanc-
ing the narrative quality in story generation (Yang
et al., 2022b), and refining response generation in
dialogues (Ye et al., 2023; Yu et al., 2023). Given
the subjectivity involved in assessing the outputs,
these studies often rely on detailed, natural lan-
guage feedback and employ an iterative refinement
strategy for post-hoc refinement.
Machine Translation. The concept of post-hoc
self-correction has deep roots in the field of ma-
chine translation (MT), where it is often called
Automatic Post-Editing (APE) (do Carmo et al.,
2021). A long line of prior works train models to
fix translation errors by either learning from hu-
man correction data (Alabau et al., 2014) or from
synthetic training data (Lee et al., 2021). To mini-
mize the cost of data collection, recent works (Chen
et al., 2023c; Raunak et al., 2023) have leveraged
the in-context learning ability of LLMs for post-
editing translations. Besides post-hoc methods,
training-time correction (Unanue et al., 2021) and
decoding-time correction (Freitag et al., 2022) are
also adopted by prior works.
Summarization. Automated model correction is
commonly used in summarization to ensure the
factuality of the generated summary. There are
two mainstream methods: 1) training-time correc-
tion that imposes factuality constraints during train-
ing (Liu and Liu, 2021; Wan and Bansal, 2022;
Scheurer et al., 2023), and 2) post-hoc correction
that post-edits generated summaries to correct fac-
tual errors (Cao et al., 2020; Saunders et al., 2022).
Recent works have investigated using RL to re-
fine the model guided by automated feedback from
14

--- PAGE 15 ---
either reward models (Akyürek et al., 2023) or lan-
guage models (Pang et al., 2023).
7 Research Gaps and Future Directions
7.1 Theoretical Justifications
Although LLMs have exhibited a remarkable capa-
bility for self-analysis and self-improvement, there
remains a lack of theoretical justifications to un-
cover the mystery of such ability. Therefore, we
argue that the study of underlying theoretical princi-
ples can offer a more transparent understanding of
self-correction. Subsequently, we propose several
potential directions for such explorations.
The ability of language models to self-correct
is closely associated with their capacity to exhibit
metacognitive awareness, i.e., their understanding
of their own knowledge and uncertainties (Kada-
vath et al., 2022). Similarly, the notion of calibra-
tionin language models, referring to their ability
to produce well-calibrated predictions with prob-
abilities aligning closely with observed frequen-
cies of outcomes, is of paramount importance (Lin
et al., 2023). Recent research by Kadavath et al.
(2022) reveals that pre-trained language models,
when presented with properly formatted multiple-
choice and true/false questions, demonstrate good
calibration. Particularly, language models exhibit
well-calibrated responses to self-evaluation ques-
tions in few-shot settings. On the other hand, fine-
tuned language models, such as those incorporating
RLHF, require temperature adjustments to achieve
calibration since the model distribution is tailored
to optimize specific behaviors.
While language models demonstrate some ca-
pacity for self-feedback, achieving superior per-
formance often necessitates incorporating exter-
nal feedback signals. The integration of feedback
signals is closely linked to the alignment of lan-
guage models, a domain that still lacks compre-
hensive understanding. For example, in RLHF,
the choice of the metric to minimize between the
reward model output and the final model output sig-
nificantly impacts downstream task performance
(Go et al., 2023), yet this aspect remains underex-
plored in many applications. Furthermore, the opti-
mal method for automatically generating prompts
to instruct language models effectively, for tasks
such as evaluating and refining their outputs, re-
mains an open question. Although Sordoni et al.
(2023) have addressed this issue by treating natural
language prompts as parameters of the languagemodel and performing discrete optimization, their
approach still requires hand-crafted meta prompts
to implement the algorithm.
7.2 Measuring the Ability of Self-Correction
Despite the promising progress in enabling LLMs
to self-correct and improve their outputs, current
research only provides empirical evidence of its ef-
fectiveness across diverse applications. However, a
gap remains in establishing robust quantitative met-
rics to understand and evaluate the self-correction
capability of LLMs. While various strategies have
been proposed to rectify LLM outputs, a compre-
hensive comparative evaluation of these approaches
is still missing. This includes metrics to assess the
effectiveness, applicability, complexity, and poten-
tial upper-bound limits of each strategy within a
unified context. Future research could aim to create
comprehensive evaluation frameworks which take
into account variables such as the complexity of the
task, the degree of initial error, the improvement in
quality after self-correction, etc.
In addition to this, building benchmarks to di-
agnose self-correction capabilities represents an-
other promising research direction. Such diag-
nostic datasets would enable a more standardized,
objective evaluation of different LLMs and self-
correction strategies, driving the development of
more accurate and efficient models.
7.3 Continual Self-Improvement
Another promising yet under-explored area of LLM
self-correction is the idea of continual, life-long
self-improvement. As LLMs are utilized in more
diverse, dynamic, and real-time contexts, the abil-
ity to adapt and improve continually over time be-
comes essential. This is closely related to the con-
cept of continual (life-long) learning (Wang et al.,
2023b), where the model continually learns new
skills and adapts to novel environments and con-
texts. Translating this to self-correction implies that
LLMs continuously evaluate their outputs, learn
from errors, update their knowledge, and adapt
their decision-making strategies accordingly.
Studies on self-training such as (Huang et al.,
2022; Zelikman et al., 2022) have evidenced that
LLMs can self-improve by continuously training
on their own outputs that are positively evaluated
by humans or models. However, these studies typi-
cally concentrate on a single, one-time correction
process and evaluate improvements in a particular
aspect. The robustness and stability of self-training
15

--- PAGE 16 ---
under continual settings remain uncertain. For ex-
ample, a major challenge of continual learning is
catastrophic forgetting (Kirkpatrick et al., 2016),
where the acquisition of new skills often leads to
a considerable decrease in previous capabilities. It
is unclear whether similar issues may emerge in a
continual self-improve LLM, such as whether cor-
recting one behavior may unintentionally alter a
previously corrected behavior.
Finally, exploring how to integrate various self-
correction techniques to efficiently build a contin-
ual self-improvement LLM is also worth investigat-
ing. For example, post-hoc correction represents
a more immediate and less costly strategy, while
training-time correction addresses model behavior
more fundamentally, with a higher computational
cost. To combine these strategies, post-hoc cor-
rection could be used to collect training data ( e.g.,
most frequently made mistakes and their correc-
tions), which are used to guide the periodically
training-time correction to address these recurring
issues permanently.
7.4 Self-Correction with Model Editing
Recent years have witnessed a surge in techniques
formodel editing (Sinitsin et al., 2020; Cao et al.,
2021; Yao et al., 2023b), aiming to adjust the
model’s behavior for examples within the editing
scope while leaving its performance for out-of-
scope examples unaltered. Model editing has been
employed in updating outdated knowledge embed-
ded in LLMs (Lee et al., 2022; Onoe et al., 2023)
and in addressing issues related to false associa-
tions memorized during LLM training (Murty et al.,
2022; Tanno et al., 2022). Current model editing
methods have shown some efficacy in adjusting fac-
tual knowledge within LLMs, yet they still suffer
problems such as a lack of robust generalization
capabilities (Yao et al., 2023b) and the introduction
of substantial unintended side effects (Hoelscher-
Obermaier et al., 2023).
Nevertheless, the advancements in model edit-
ing present promising opportunities for the self-
correction of LLMs. Primarily, model editing
enables accurate, fine-grained corrections at the
level of individual neurons or layers, circumvent-
ing the need for extensive retraining associated
with training-time correction. Moreover, through
the analysis of the impact of model edits, we can
deepen our understanding of the self-correction
mechanism. Further, methods developed to curtailundesired side effects in model editing (Hoelscher-
Obermaier et al., 2023) could foster more robust
self-correction strategies by mitigating the issue of
inadvertently introducing new errors while resolv-
ing existing ones. Therefore, we forecast future
research to incorporate model editing into LLM
self-correction processes, an under-explored area.
7.5 Multi-Modal Self-Correction
As discussed in Section 6, self-correction strategies
for LLMs have been successfully employed in an
extensive array of NLP tasks. However, most exist-
ing works are limited to the textual modality, where
both the model outputs and the feedback are in tex-
tual form. The recent surge in multi-modal data
usage, including image, audio, and video modali-
ties, presents enticing opportunities for expansion.
These include the exploration of self-correction
capabilities within multi-modal LLMs, the incor-
poration of visual feedback, and improving vision-
language tasks through self-correction.
A handful of pioneering studies have investi-
gated this domain. For example, MaskGIT (Chang
et al., 2022) employed a self-refinement approach
to image generation, where the model progressively
refines the generated image conditioned on the pre-
vious generation. Ke et al. (2019) utilized a self-
correction strategy for vision-and-language navi-
gation. FLIRT (Mehrabi et al., 2023) uses self-
correction to iteratively generate and revise adver-
sarial input prompts for text-to-image generation.
However, despite these initial explorations, self-
correction strategies are yet to be broadly adopted
in multi-modal settings. A comprehensive under-
standing of how self-correction methods generalize
across various modalities is crucial for improving
their robustness and versatility.
8 Conclusion
In this paper, we present a comprehensive sur-
vey of self-correcting large language models with
automated feedback. We broadly categorize and
analyze various self-correction strategies, includ-
ing training-time, generation-time, and post-hoc
corrections. We also discuss the major applica-
tion areas of self-correction, including correcting
factual errors, enhancing reasoning abilities, and
improving code generation, among others. Fi-
nally, we outline a number of potential future di-
rections and associated challenges in this field.
Our goal with this paper is to provide a com-
16

--- PAGE 17 ---
prehensive and useful resource for readers inter-
ested in the development of this rapidly evolv-
ing domain. To aid in this effort, we create a
continually-updated reading list in a GitHub repos-
itory: https://github.com/teacherpeterpan/
self-correction-llm-papers .
Acknowledgements
This work was supported by the National Science
Foundation Award #2048122. The views expressed
are those of the authors and do not reflect the of-
ficial policy or position of the US government.
Thanks to Xinyuan Lu for assisting with the Github
reading list repo.
References
Afra Feyza Akyürek, Ekin Akyürek, Ashwin Kalyan,
Peter Clark, Derry Tanti Wijaya, and Niket Tandon.
2023. RL4F: generating natural language feedback
with reinforcement learning for repairing model out-
puts. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (ACL) ,
pages 7716–7733.
Vicent Alabau, Christian Buck, Michael Carl, Fran-
cisco Casacuberta, Mercedes García-Martínez, Ul-
rich Germann, Jesús González-Rubio, Robin L. Hill,
Philipp Koehn, Luis A. Leiva, Bartolomé Mesa-Lao,
Daniel Ortiz-Martínez, Herve Saint-Amand, Germán
Sanchis-Trilles, and Chara Tsoukala. 2014. CAS-
MACAT: A computer-assisted translation workbench.
InProceedings of the 14th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL) , pages 25–28. The Association
for Computer Linguistics.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan,
Nicholas Joseph, Saurav Kadavath, Jackson Kernion,
Tom Conerly, Sheer El Showk, Nelson Elhage, Zac
Hatfield-Dodds, Danny Hernandez, Tristan Hume,
Scott Johnston, Shauna Kravec, Liane Lovitt, Neel
Nanda, Catherine Olsson, Dario Amodei, Tom B.
Brown, Jack Clark, Sam McCandlish, Chris Olah,
Benjamin Mann, and Jared Kaplan. 2022a. Train-
ing a helpful and harmless assistant with rein-
forcement learning from human feedback. CoRR ,
abs/2204.05862.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron
McKinnon, Carol Chen, Catherine Olsson, Christo-
pher Olah, Danny Hernandez, Dawn Drain, Deep
Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,
Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
Landau, Kamal Ndousse, Kamile Lukosiute, Liane
Lovitt, Michael Sellitto, Nelson Elhage, NicholasSchiefer, Noemí Mercado, Nova DasSarma, Robert
Lasenby, Robin Larson, Sam Ringer, Scott John-
ston, Shauna Kravec, Sheer El Showk, Stanislav Fort,
Tamera Lanham, Timothy Telleen-Lawton, Tom Con-
erly, Tom Henighan, Tristan Hume, Samuel R. Bow-
man, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,
Nicholas Joseph, Sam McCandlish, Tom Brown, and
Jared Kaplan. 2022b. Constitutional AI: harmless-
ness from AI feedback. CoRR , abs/2212.08073.
Gasper Begus, Maksymilian Dabkowski, and Ryan
Rhodes. 2023. Large linguistic models: Analyz-
ing theoretical linguistic abilities of llms. CoRR ,
abs/2305.00948.
Meng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit
Cheung. 2020. Factual error correction for abstrac-
tive summarization models. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 6251–6258.
Nicola De Cao, Wilker Aziz, and Ivan Titov. 2021.
Editing factual knowledge in language models. In
Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 6491–6506.
Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and
William T. Freeman. 2022. Maskgit: Masked gen-
erative image transformer. In Proceedings of the
IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , pages 11305–11315.
IEEE.
Jonathan D. Chang, Kianté Brantley, Rajkumar Ra-
mamurthy, Dipendra Misra, and Wen Sun. 2023.
Learning to generate better than your LLM. CoRR ,
abs/2306.11816.
Yiannis Charalambous, Norbert Tihanyi, Ridhi Jain,
Youcheng Sun, Mohamed Amine Ferrag, and Lu-
cas C. Cordeiro. 2023. A new era in software
security: Towards self-healing software via large
language models and formal verification. CoRR ,
abs/2305.14752.
Angelica Chen, Jérémy Scheurer, Tomasz Korbak,
Jon Ander Campos, Jun Shern Chan, Samuel R. Bow-
man, Kyunghyun Cho, and Ethan Perez. 2023a. Im-
proving code generation by training with natural lan-
guage feedback. CoRR , abs/2303.16749.
Bei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan,
Zeqi Lin, Jian-Guang Lou, and Weizhu Chen. 2023b.
Codet: Code generation with generated tests. In
Proceedings of the 11th International Conference on
Learning Representations (ICLR) .
Pinzhen Chen, Zhicheng Guo, Barry Haddow, and Ken-
neth Heafield. 2023c. Iterative translation refinement
with large language models. CoRR , abs/2306.03856.
Xinyun Chen, Maxwell Lin, Nathanael Schärli, and
Denny Zhou. 2023d. Teaching large language mod-
els to self-debug. CoRR , abs/2304.05128.
17

--- PAGE 18 ---
Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena
Shah, Iana Borova, Dylan Langdon, Reema Moussa,
Matt Beane, Ting-Hao Huang, Bryan R. Routledge,
and William Yang Wang. 2021. Finqa: A dataset of
numerical reasoning over financial data. In Proceed-
ings of the 2021 Conference on Empirical Methods
in Natural Language Processing (EMNLP) , pages
3697–3711.
I-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan,
Kehua Feng, Chunting Zhou, Junxian He, Graham
Neubig, and Pengfei Liu. 2023. Factool: Factual-
ity detection in generative ai – a tool augmented
framework for multi-task and multi-domain scenar-
ios.CoRR , abs/2307.13528.
Yejin Choi. 2023. Common sense: The dark matter
of language and intelligence. In Proceedings of
the 2023 International Conference on Autonomous
Agents and Multiagent Systems (AAMAS) , page 2.
ACM.
Elizabeth Clark, Tal August, Sofia Serrano, Nikita
Haduong, Suchin Gururangan, and Noah A. Smith.
2021. All that’s ’human’ is not gold: Evaluating
human evaluation of generated text. In Processings
of the 59th Annual Meeting of the Association for
Computational Linguistics (ACL) , pages 7282–7296.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. CoRR , abs/2110.14168.
Roi Cohen, May Hamri, Mor Geva, and Amir Glober-
son. 2023. LM vs LM: detecting factual errors via
cross examination. CoRR , abs/2305.13281.
Antonia Creswell and Murray Shanahan. 2022. Faith-
ful reasoning using large language models. CoRR ,
abs/2208.14271.
Ishita Dasgupta, Andrew K. Lampinen, Stephanie
C. Y . Chan, Antonia Creswell, Dharshan Kumaran,
James L. McClelland, and Felix Hill. 2022. Lan-
guage models show human-like content effects on
reasoning. CoRR , abs/2207.07051.
Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane
Hung, Eric Frank, Piero Molino, Jason Yosinski, and
Rosanne Liu. 2020. Plug and play language models:
A simple approach to controlled text generation. In
Proceedings of the 8th International Conference on
Learning Representations (ICLR) .
Félix do Carmo, Dimitar Shterionov, Joss Moorkens,
Joachim Wagner, Murhaf Hossari, Eric Paquin, Dag
Schmidtke, Declan Groves, and Andy Way. 2021.
A review of the state-of-the-art in automatic post-
editing. Machine Translation , 35(2):101–143.
Yilun Du, Shuang Li, Antonio Torralba, Joshua B.
Tenenbaum, and Igor Mordatch. 2023. Improving
factuality and reasoning in language models through
multiagent debate. CoRR , abs/2305.14325.Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,
Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. 2023. Alpaca-
farm: A simulation framework for methods that learn
from human feedback. CoRR , abs/2305.14387.
Patrick Fernandes, Aman Madaan, Emmy Liu, António
Farinhas, Pedro Henrique Martins, Amanda Bertsch,
José G. C. de Souza, Shuyan Zhou, Tongshuang
Wu, Graham Neubig, and André F. T. Martins. 2023.
Bridging the gap: A survey on integrating (human)
feedback for natural language generation. CoRR ,
abs/2305.00955.
Emily First, Markus N. Rabe, Talia Ringer, and
Yuriy Brun. 2023. Baldur: Whole-proof genera-
tion and repair with large language models. CoRR ,
abs/2303.04910.
Markus Freitag, David Grangier, Qijun Tan, and Bowen
Liang. 2022. High quality rather than high model
probability: Minimum bayes risk decoding with neu-
ral metrics. Transactions of the Association for Com-
putational Linguistics (TACL) , pages 811–825.
Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata.
2023. Improving language model negotiation with
self-play and in-context learning from AI feedback.
CoRR , abs/2305.10142.
Deep Ganguli, Amanda Askell, Nicholas Schiefer,
Thomas I. Liao, Kamile Lukosiute, Anna Chen,
Anna Goldie, Azalia Mirhoseini, Catherine Olsson,
Danny Hernandez, Dawn Drain, Dustin Li, Eli Tran-
Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr,
Jared Mueller, Joshua Landau, Kamal Ndousse, Ka-
rina Nguyen, Liane Lovitt, Michael Sellitto, Nelson
Elhage, Noemí Mercado, Nova DasSarma, Oliver
Rausch, Robert Lasenby, Robin Larson, Sam Ringer,
Sandipan Kundu, Saurav Kadavath, Scott Johnston,
Shauna Kravec, Sheer El Showk, Tamera Lanham,
Timothy Telleen-Lawton, Tom Henighan, Tristan
Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann,
Dario Amodei, Nicholas Joseph, Sam McCandlish,
Tom Brown, Christopher Olah, Jack Clark, Samuel R.
Bowman, and Jared Kaplan. 2023. The capacity
for moral self-correction in large language models.
CoRR , abs/2302.07459.
Ge Gao, Hung-Ting Chen, Yoav Artzi, and Eunsol Choi.
2023a. Continually improving extractive QA via
human feedback. CoRR , abs/2305.12473.
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony
Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vin-
cent Y . Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan,
and Kelvin Guu. 2023b. Rarr: Researching and re-
vising what language models say, using language
models. In Proceedings of the 61th Annual Meet-
ing of the Association for Computational Linguistics
(ACL) .
Samuel Gehman, Suchin Gururangan, Maarten Sap,
Yejin Choi, and Noah A. Smith. 2020. RealToxi-
cityPrompts: Evaluating neural toxic degeneration
18

--- PAGE 19 ---
in language models. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
3356–3369.
Zelalem Gero, Chandan Singh, Hao Cheng, Tristan Nau-
mann, Michel Galley, Jianfeng Gao, and Hoifung
Poon. 2023. Self-verification improves few-shot clin-
ical information extraction. CoRR , abs/2306.00024.
Amelia Glaese, Nat McAleese, Maja Trebacz, John
Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth
Rauh, Laura Weidinger, Martin J. Chadwick, Phoebe
Thacker, Lucy Campbell-Gillingham, Jonathan Ue-
sato, Po-Sen Huang, Ramona Comanescu, Fan
Yang, Abigail See, Sumanth Dathathri, Rory Greig,
Charlie Chen, Doug Fritz, Jaume Sanchez Elias,
Richard Green, Sona Mokrá, Nicholas Fernando,
Boxi Wu, Rachel Foley, Susannah Young, Iason
Gabriel, William Isaac, John Mellor, Demis Hass-
abis, Koray Kavukcuoglu, Lisa Anne Hendricks, and
Geoffrey Irving. 2022. Improving alignment of dia-
logue agents via targeted human judgements. CoRR ,
abs/2209.14375.
Dongyoung Go, Tomasz Korbak, Germán Kruszewski,
Jos Rozen, Nahyeon Ryu, and Marc Dymetman.
2023. Aligning language models with prefer-
ences through f-divergence minimization. CoRR ,
abs/2302.08215.
Olga Golovneva, Moya Chen, Spencer Poff, Martin
Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi,
and Asli Celikyilmaz. 2023. ROSCOE: A suite of
metrics for scoring step-by-step reasoning. In Pro-
ceedings of the 11th International Conference on
Learning Representations (ICLR) .
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen,
Yujiu Yang, Nan Duan, and Weizhu Chen. 2023.
CRITIC: large language models can self-correct with
tool-interactive critiquing. CoRR , abs/2305.11738.
Caglar Gulcehre, Tom Le Paine, Srivatsan Srini-
vasan, Ksenia Konyushkova, Lotte Weerts, Abhishek
Sharma, Aditya Siddhant, Alex Ahern, Miaosen
Wang, Chenjie Gu, Wolfgang Macherey, Arnaud
Doucet, Orhan Firat, and Nando de Freitas. 2023.
Reinforced self-training (rest) for language modeling.
CoRR , abs/2308.08998.
Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,
Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng
Wu. 2023. How close is chatgpt to human experts?
comparison corpus, evaluation, and detection. CoRR ,
abs/2301.07597.
Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong,
Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023.
Reasoning with language model is planning with
world model. CoRR , abs/2305.14992.
Hangfeng He, Hongming Zhang, and Dan Roth. 2023.
Rethinking with retrieval: Faithful large language
model inference. CoRR , abs/2301.00303.Pengcheng He, Xiaodong Liu, Jianfeng Gao, and
Weizhu Chen. 2021. Deberta: decoding-enhanced
bert with disentangled attention. In Proceedings of
The 9th International Conference on Learning Repre-
sentations (ICLR) .
Alec Helbling, Mansi Phute, Matthew Hull, and
Duen Horng Chau. 2023. Llm self defense: By
self examination, llms know they are being tricked.
CoRR , abs/2308.07308.
Matthew Ho, Aditya Sharma, Justin Chang, Michael
Saxon, Sharon Levy, Yujie Lu, and William Yang
Wang. 2023. Wikiwhy: Answering and explaining
cause-and-effect questions. In Proceedings of the
11th International Conference on Learning Represen-
tations (ICLR) .
Jason Hoelscher-Obermaier, Julia Persson, Esben Kran,
Ioannis Konstas, and Fazl Barez. 2023. Detecting
edit failures in large language models: An improved
specificity benchmark. In Findings of the Association
for Computational Linguistics: ACL 2023 , pages
11548–11559.
Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,
Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.
Large language models can self-improve. CoRR ,
abs/2210.11610.
Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categori-
cal reparameterization with gumbel-softmax. In Pro-
ceedings of 5th International Conference on Learning
Representations (ICLR) .
Shuyang Jiang, Yuhao Wang, and Yu Wang. 2023. Self-
evolve: A code evolution framework via large lan-
guage models. CoRR , abs/2306.02907.
Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-
man, Chandra Bhagavatula, Ronan Le Bras, and
Yejin Choi. 2022. Maieutic prompting: Logically
consistent reasoning with recursive explanations. In
Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 1266–1279.
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom
Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli
Tran-Johnson, Scott Johnston, Sheer El Showk, Andy
Jones, Nelson Elhage, Tristan Hume, Anna Chen,
Yuntao Bai, Sam Bowman, Stanislav Fort, Deep
Ganguli, Danny Hernandez, Josh Jacobson, Jack-
son Kernion, Shauna Kravec, Liane Lovitt, Ka-
mal Ndousse, Catherine Olsson, Sam Ringer, Dario
Amodei, Tom Brown, Jack Clark, Nicholas Joseph,
Ben Mann, Sam McCandlish, Chris Olah, and Jared
Kaplan. 2022. Language models (mostly) know what
they know. CoRR , abs/2207.05221.
Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman,
Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, and
Siddhartha S. Srinivasa. 2019. Tactical rewind: Self-
correction via backtracking in vision-and-language
19

--- PAGE 20 ---
navigation. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) ,
pages 6741–6749.
Zachary Kenton, Tom Everitt, Laura Weidinger, Ia-
son Gabriel, Vladimir Mikulik, and Geoffrey Irv-
ing. 2021. Alignment of language agents. CoRR ,
abs/2103.14659.
Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney,
Caiming Xiong, and Richard Socher. 2019. CTRL:
A conditional transformer language model for con-
trollable generation. CoRR , abs/1909.05858.
Muhammad Khalifa, Lajanugen Logeswaran, Moon-
tae Lee, Honglak Lee, and Lu Wang. 2023.
Discriminator-guided multi-step reasoning with lan-
guage models. CoRR , abs/2305.14934.
Geunwoo Kim, Pierre Baldi, and Stephen McAleer.
2023. Language models can solve computer tasks.
CoRR , abs/2303.17491.
James Kirkpatrick, Razvan Pascanu, Neil C. Rabi-
nowitz, Joel Veness, Guillaume Desjardins, Andrei A.
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Ag-
nieszka Grabska-Barwinska, Demis Hassabis, Clau-
dia Clopath, Dharshan Kumaran, and Raia Hadsell.
2016. Overcoming catastrophic forgetting in neural
networks. CoRR , abs/1612.00796.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large
language models are zero-shot reasoners. In Pro-
ceedings of the 2022 Annual Conference on Neural
Information Processing Systems (NeurIPS) .
Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika
Bhalerao, Christopher L. Buckley, Jason Phang,
Samuel R. Bowman, and Ethan Perez. 2023. Pre-
training language models with human preferences.
CoRR , abs/2302.08582.
Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, and
Stefan Riezler. 2018. Can neural machine translation
be improved with user feedback? In Proceedings of
the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HIT) .
Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio
Savarese, and Steven Chu-Hong Hoi. 2022. Coderl:
Mastering code generation through pretrained mod-
els and deep reinforcement learning. In Proceedings
of the Annual Conference on Neural Information Pro-
cessing Systems (NeurIPS) .
Kyungjae Lee, Wookje Han, Seung-won Hwang,
Hwaran Lee, Joonsuk Park, and Sang-Woo Lee. 2022.
Plug-and-play adaptation for continuously-updated
QA. In Findings of the Association for Computa-
tional Linguistics: ACL 2022 , pages 438–447.
WonKee Lee, Baikjin Jung, Jaehun Shin, and Jong-
Hyeok Lee. 2021. Adaptation of back-translation to
automatic post-editing for synthetic data generation.InProceedings of the 16th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL) , pages 3685–3691.
Sharon Levy, Emily Allaway, Melanie Subbiah, Lydia
Chilton, Desmond Patton, Kathleen McKeown, and
William Yang Wang. 2022. SafeText: A benchmark
for exploring physical safety in language models. In
Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 2407–2421.
Sharon Levy, Michael Saxon, and William Yang Wang.
2021. Investigating memorization of conspiracy theo-
ries in text generation. In Findings of the Association
for Computational Linguistics: ACL-IJCNLP 2021 ,
pages 4718–4729, Online. Association for Computa-
tional Linguistics.
Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun
Nie, and Ji-Rong Wen. 2023a. Halueval: A large-
scale hallucination evaluation benchmark for large
language models. CoRR , abs/2305.11747.
Miaoran Li, Baolin Peng, and Zhu Zhang. 2023b. Self-
checker: Plug-and-play modules for fact-checking
with large language models. CoRR , abs/2305.14623.
Ruosen Li, Teerth Patel, and Xinya Du. 2023c. PRD:
peer rank and discussion improve large language
model based evaluations. CoRR , abs/2307.02762.
Siyao Li, Deren Lei, Pengda Qin, and William Yang
Wang. 2019. Deep reinforcement learning with dis-
tributional semantic rewards for abstractive summa-
rization. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
6037–6043.
Xiang Li, John Thickstun, Ishaan Gulrajani, Percy
Liang, and Tatsunori B. Hashimoto. 2022. Diffusion-
lm improves controllable text generation. In Proceed-
ings of the Annual Conference on Neural Information
Processing Systems (NeurIPS) .
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,
Jian-Guang Lou, and Weizhu Chen. 2023d. Making
language models better reasoners with step-aware
verifier. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (ACL) ,
pages 5315–5333.
Hunter Lightman, Vineet Kosaraju, Yura Burda, Har-
rison Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl
Cobbe. 2023. Let’s verify step by step. CoRR ,
abs/2305.20050.
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.
TruthfulQA: Measuring how models mimic human
falsehoods. In Proceedings of the 60th Annual Meet-
ing of the Association for Computational Linguistics
(ACL) , pages 3214–3252.
20

--- PAGE 21 ---
Yen-Ting Lin and Yun-Nung Chen. 2023. Llm-eval:
Unified multi-dimensional automatic evaluation for
open-domain conversations with large language mod-
els.CoRR , abs/2305.13711.
Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2023.
Generating with confidence: Uncertainty quantifica-
tion for black-box large language models. CoRR ,
abs/2305.19187.
Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023.
Chain of hindsight aligns language models with feed-
back. CoRR , abs/2302.02676.
Yixin Liu and Pengfei Liu. 2021. Simcls: A simple
framework for contrastive learning of abstractive
summarization. In Proceedings of the 59th Annual
Meeting of the Association for Computational Lin-
guistics and the 11th International Joint Conference
on Natural Language Processing (ACL/IJCNLP) ,
pages 1065–1072.
Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang,
Lianhui Qin, Peter West, Prithviraj Ammanabrolu,
and Yejin Choi. 2022. QUARK: controllable text
generation with reinforced unlearning. In Proceed-
ings of the Annual Conference on Neural Information
Processing Systems (NeurIPS) .
Chenyang Lyu, Jitao Xu, and Longyue Wang. 2023a.
New trends in machine translation using large lan-
guage models: Case examples with chatgpt. CoRR ,
abs/2305.01181.
Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,
Delip Rao, Eric Wong, Marianna Apidianaki, and
Chris Callison-Burch. 2023b. Faithful chain-of-
thought reasoning. CoRR , abs/2301.13379.
Aman Madaan, Niket Tandon, Peter Clark, and Yiming
Yang. 2022. Memory-assisted prompt editing to im-
prove GPT-3 after deployment. In Proceedings of the
2022 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 2833–2861.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
Sean Welleck, Bodhisattwa Prasad Majumder,
Shashank Gupta, Amir Yazdanbakhsh, and Peter
Clark. 2023. Self-refine: Iterative refinement with
self-feedback. CoRR , abs/2303.17651.
Potsawee Manakul, Adian Liusie, and Mark J. F. Gales.
2023. Selfcheckgpt: Zero-resource black-box hal-
lucination detection for generative large language
models. CoRR , abs/2303.08896.
Ninareh Mehrabi, Palash Goyal, Christophe Dupuy,
Qian Hu, Shalini Ghosh, Richard Zemel, Kai-Wei
Chang, Aram Galstyan, and Rahul Gupta. 2023.
Flirt: Feedback loop in-context red teaming. CoRR ,
abs/2308.04265.
Ning Miao, Yee Whye Teh, and Tom Rainforth. 2023.
Selfcheck: Using llms to zero-shot check their own
step-by-step reasoning. CoRR , abs/2308.00436.Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike
Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,
Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
Factscore: Fine-grained atomic evaluation of fac-
tual precision in long form text generation. CoRR ,
abs/2305.14251.
Bhavana Dalvi Mishra, Oyvind Tafjord, and Peter Clark.
2022. Towards teachable reasoning systems: Using
a dynamic memory of user feedback for continual
system improvement. In Proceedings of the 2022
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP) , pages 9465–9480.
Shikhar Murty, Christopher D. Manning, Scott M. Lund-
berg, and Marco Túlio Ribeiro. 2022. Fixing model
bugs with natural language patches. In Proceedings
of the 2022 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP) , pages 11600–
11613.
Ansong Ni, Srini Iyer, Dragomir Radev, Ves Stoyanov,
Wen-tau Yih, Sida I. Wang, and Xi Victoria Lin. 2023.
LEVER: learning to verify language-to-code genera-
tion with execution. In Proceedings of the 40th Inter-
national Conference on Machine Learning (ICML) .
Theo X. Olausson, Jeevana Priya Inala, Chenglong
Wang, Jianfeng Gao, and Armando Solar-Lezama.
2023. Demystifying GPT self-repair for code genera-
tion. CoRR , abs/2306.09896.
Yasumasa Onoe, Michael J. Q. Zhang, Shankar Padman-
abhan, Greg Durrett, and Eunsol Choi. 2023. Can
lms learn new entities from descriptions? challenges
in propagating injected knowledge. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics (ACL) , pages 5469–5485.
OpenAI. 2023. GPT-4 technical report. CoRR ,
abs/2303.08774.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,
John Schulman, Jacob Hilton, Fraser Kelton, Luke
Miller, Maddie Simens, Amanda Askell, Peter Welin-
der, Paul F. Christiano, Jan Leike, and Ryan Lowe.
2022. Training language models to follow instruc-
tions with human feedback. In Proceedings of the
Annual Conference on Neural Information Process-
ing Systems (NeurIPS) .
Liangming Pan, Alon Albalak, Xinyi Wang, and
William Yang Wang. 2023. Logic-lm: Empower-
ing large language models with symbolic solvers for
faithful logical reasoning. CoRR , abs/2305.12295.
Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-
Hui Chen, Jiacheng Xu, Zongzhang Zhang, and
Yang Yu. 2023. Language model self-improvement
by reinforcement learning contemplation. CoRR ,
abs/2305.14483.
21

--- PAGE 22 ---
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beat-
riz Borges, Antoine Bosselut, Robert West, and Boi
Faltings. 2023. REFINER: reasoning feedback on in-
termediate representations. CoRR , abs/2304.01904.
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng,
Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou
Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check
your facts and try again: Improving large language
models with external knowledge and automated feed-
back. CoRR , abs/2302.12813.
Dongqi Pu and Vera Demberg. 2023. Chatgpt vs human-
authored text: Insights into controllable text summa-
rization and sentence style transfer. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics: Student Research Work-
shop (ACL) , pages 1–18.
Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao
Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is
chatgpt a general-purpose natural language process-
ing task solver? CoRR , abs/2302.06476.
Vikas Raunak, Amr Sharaf, Hany Hassan Awadal-
lah, and Arul Menezes. 2023. Leveraging GPT-
4 for automatic translation post-editing. CoRR ,
abs/2305.14878.
Danilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Henry
Zhu, Rui Dong, Deguang Kong, Juliette Burger, An-
jelica Ramos, William Yang Wang, Zhiheng Huang,
George Karypis, Bing Xiang, and Dan Roth. 2023.
STREET: A multi-task structured reasoning and ex-
planation benchmark. In Proceedings of the 11th
International Conference on Learning Representa-
tions (ICLR) .
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills,
Long Ouyang, Jonathan Ward, and Jan Leike. 2022.
Self-critiquing models for assisting human evaluators.
CoRR , abs/2206.05802.
Michael Saxon, Xinyi Wang, Wenda Xu, and
William Yang Wang. 2023. PECO: Examining single
sentence label leakage in natural language inference
datasets through progressive evaluation of cluster out-
liers. In Proceedings of the 17th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL) , pages 3053–3066.
Jérémy Scheurer, Jon Ander Campos, Tomasz Kor-
bak, Jun Shern Chan, Angelica Chen, Kyunghyun
Cho, and Ethan Perez. 2023. Training language
models with language feedback at scale. CoRR ,
abs/2303.16755.
Timo Schick, Jane A. Yu, Zhengbao Jiang, Fabio
Petroni, Patrick S. H. Lewis, Gautier Izacard, Qingfei
You, Christoforos Nalmpantis, Edouard Grave, and
Sebastian Riedel. 2023. PEER: A collaborative lan-
guage model. In Proceedings of The 11th Inter-
national Conference on Learning Representations
(ICLR) .John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Radford, and Oleg Klimov. 2017. Proximal policy
optimization algorithms. CoRR , abs/1707.06347.
Thibault Sellam, Dipanjan Das, and Ankur P. Parikh.
2020. BLEURT: learning robust metrics for text
generation. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics
(ACL) , pages 7881–7892.
Omar Shaikh, Hongxin Zhang, William Held, Michael
Bernstein, and Diyi Yang. 2023. On second thought,
let’s not think step by step! bias and toxicity in zero-
shot reasoning. In Proceedings of the 61st Annual
Meeting of the Association for Computational Lin-
guistics (ACL) , pages 4454–4470.
Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu. 2016. Minimum
risk training for neural machine translation. In Pro-
ceedings of the 54th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL) , pages
1683–1692.
Noah Shinn, Federico Cassano, Beck Labash, Ashwin
Gopinath, Karthik Narasimhan, and Shunyu Yao.
2023. Reflexion: Language agents with verbal rein-
forcement learning. CoRR , abs/2303.11366.
Anton Sinitsin, Vsevolod Plokhotnyuk, Dmitry V .
Pyrkin, Sergei Popov, and Artem Babenko. 2020.
Editable neural networks. In Proceedings of the 8th
International Conference on Learning Representa-
tions (ICLR) .
Alessandro Sordoni, Xingdi Yuan, Marc-Alexandre
Côté, Matheus Pereira, Adam Trischler, Ziang Xiao,
Arian Hosseini, Friederike Niedtner, and Nicolas Le
Roux. 2023. Deep language networks: Joint prompt
training of stacked llms using variational inference.
CoRR , abs/2306.12509.
Neha Srikanth and Rachel Rudinger. 2022. Partial-input
baselines show that nli models can ignore context, but
they don’t. In Proceedings of the 2022 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL-HLT) , pages 4753–4763.
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se-
bastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny
Zhou, and Jason Wei. 2023. Challenging big-bench
tasks and whether chain-of-thought can solve them.
InFindings of the Association for Computational
Linguistics: ACL 2023 , pages 13003–13051.
Oyvind Tafjord, Bhavana Dalvi Mishra, and Peter Clark.
2022. Entailer: Answering questions with faithful
and truthful chains of reasoning. In Proceedings
of the 2022 Conference on Empirical Methods in
Natural Language Processing (EMNLP) , pages 2078–
2093.
22

--- PAGE 23 ---
Ryutaro Tanno, Melanie F. Pradier, Aditya V . Nori, and
Yingzhen Li. 2022. Repairing neural networks by
leaving the right past behind. In Proceedings of the
2022 Annual Conference on Neural Information Pro-
cessing Systems (NeurIPS) .
Jonathan Uesato, Nate Kushman, Ramana Kumar,
H. Francis Song, Noah Y . Siegel, Lisa Wang, An-
tonia Creswell, Geoffrey Irving, and Irina Higgins.
2022. Solving math word problems with process- and
outcome-based feedback. CoRR , abs/2211.14275.
Inigo Jauregi Unanue, Jacob Parnell, and Massimo Pic-
cardi. 2021. Berttune: Fine-tuning neural machine
translation with bertscore. In Proceedings of the
59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint
Conference on Natural Language Processing (ACL/I-
JCNLP) , pages 915–924.
Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jian-
shu Chen, and Dong Yu. 2023. A stitch in time saves
nine: Detecting and mitigating hallucinations of llms
by validating low-confidence generation. CoRR ,
abs/2307.03987.
David Wan and Mohit Bansal. 2022. Factpegasus:
Factuality-aware pre-training and fine-tuning for ab-
stractive summarization. In Proceedings of the 2022
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT) , pages 1010–
1028.
Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie,
Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi
Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong,
Simran Arora, Mantas Mazeika, Dan Hendrycks, Zi-
nan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and
Bo Li. 2023a. Decodingtrust: A comprehensive as-
sessment of trustworthiness in GPT models. CoRR ,
abs/2306.11698.
Liyuan Wang, Xingxing Zhang, Hang Su, and Jun
Zhu. 2023b. A comprehensive survey of continual
learning: Theory, method and application. CoRR ,
abs/2302.00487.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .
Le, Ed H. Chi, Sharan Narang, Aakanksha Chowd-
hery, and Denny Zhou. 2023c. Self-consistency im-
proves chain of thought reasoning in language mod-
els. In Proceedings of the 11th International Confer-
ence on Learning Representations (ICLR) .
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa
Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh
Hajishirzi. 2023d. Self-instruct: Aligning language
models with self-generated instructions. In Proceed-
ings of the 61st Annual Meeting of the Association
for Computational Linguistics (ACL) , pages 13484–
13508.
Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi,
Xingshan Zeng, Wenyong Huang, Lifeng Shang,Xin Jiang, and Qun Liu. 2023e. Aligning large
language models with human: A survey. CoRR ,
abs/2307.12966.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, Ed H.
Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy
Liang, Jeff Dean, and William Fedus. 2022a. Emer-
gent abilities of large language models. CoRR ,
abs/2206.07682.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,
and Denny Zhou. 2022b. Chain-of-thought prompt-
ing elicits reasoning in large language models. In
Proceedings of the Annual Conference on Neural
Information Processing Systems (NeurIPS) .
Sean Welleck, Ximing Lu, Peter West, Faeze Brahman,
Tianxiao Shen, Daniel Khashabi, and Yejin Choi.
2023. Generating sequences by learning to self-
correct. In Proceedings of The 11th International
Conference on Learning Representations (ICLR) .
Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He,
Kang Liu, and Jun Zhao. 2023. Large language mod-
els are better reasoners with self-verification. CoRR ,
abs/2212.09561.
Qingyang Wu, Lei Li, and Zhou Yu. 2021. Textgail:
Generative adversarial imitation learning for text gen-
eration. In Proceedings of 35th Conference on Artifi-
cial Intelligence (AAAI) , pages 14067–14075.
Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane
Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari
Ostendorf, and Hannaneh Hajishirzi. 2023a. Fine-
grained human feedback gives better rewards for lan-
guage model training. CoRR , abs/2306.01693.
Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek,
Boyuan Chen, Bailin Wang, Najoung Kim, Jacob
Andreas, and Yoon Kim. 2023b. Reasoning or recit-
ing? exploring the capabilities and limitations of lan-
guage models through counterfactual tasks. CoRR ,
abs/2307.02477.
Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-
Yen Kan, Junxian He, and Qizhe Xie. 2023. De-
composition enhances reasoning via self-evaluation
guided decoding. CoRR , abs/2305.00633.
Wenda Xu, Xian Qian, Mingxuan Wang, Lei Li, and
William Yang Wang. 2023a. SESCORE2: learning
text generation evaluation via synthesizing realistic
mistakes. In Proceedings of the 61st Annual Meet-
ing of the Association for Computational Linguistics
(ACL) , pages 5166–5183.
Wenda Xu, Yi-Lin Tuan, Yujie Lu, Michael Saxon, Lei
Li, and William Yang Wang. 2022. Not all errors
are equal: Learning text generation metrics using
stratified error synthesis. In Findings of the Associa-
tion for Computational Linguistics (EMNLP) , pages
6559–6574.
23

--- PAGE 24 ---
Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao
Song, Markus Freitag, William Yang Wang, and Lei
Li. 2023b. INSTRUCTSCORE: towards explainable
text generation evaluation with automatic feedback.
CoRR , abs/2305.14282.
Hao Yan, Saurabh Srivastava, Yintao Tai, Sida I. Wang,
Wen-tau Yih, and Ziyu Yao. 2023a. Learning to
simulate natural language feedback for interactive
semantic parsing. In Proceedings of the 61th An-
nual Meeting of the Association for Computational
Linguistics (ACL) , pages 3149–3170.
Yiming Yan, Tao Wang, Chengqi Zhao, Shujian Huang,
Jiajun Chen, and Mingxuan Wang. 2023b. BLEURT
has universal translations: An analysis of automatic
metrics by minimum risk training. In Proceedings
of the 61st Annual Meeting of the Association for
Computational Linguistics(ACL) , pages 5428–5443.
Kaiyu Yang, Jia Deng, and Danqi Chen. 2022a. Gen-
erating natural language proofs with verifier-guided
search. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing
(EMNLP) , pages 89–105.
Kevin Yang and Dan Klein. 2021. FUDGE: Con-
trolled text generation with future discriminators. In
Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT) , pages 3511–3535.
Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan
Klein. 2022b. Re3: Generating longer stories with
recursive reprompting and revision. In Proceedings
of the 2022 Conference on Empirical Methods in
Natural Language Processing (EMNLP) , pages 4393–
4479.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W. Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. 2018. Hotpotqa: A dataset
for diverse, explainable multi-hop question answer-
ing. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP) , pages 2369–2380.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. 2023a. Tree of thoughts: Deliberate
problem solving with large language models. CoRR ,
abs/2305.10601.
Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng,
Zhoubo Li, Shumin Deng, Huajun Chen, and Ningyu
Zhang. 2023b. Editing large language models:
Problems, methods, and opportunities. CoRR ,
abs/2305.13172.
Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong
Kim, Hyeonbin Hwang, and Minjoon Seo. 2023.
Selfee: Iterative self-revising llm empowered by self-
feedback generation. Blog post.Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng
Jiang, and Ashish Sabharwal. 2023. Improving lan-
guage models via plug-and-play retrieval feedback.
CoRR , abs/2305.14002.
Weizhe Yuan, Kyunghyun Cho, and Jason Weston. 2023.
System-level natural language feedback. CoRR ,
abs/2306.13588.
Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.
Goodman. 2022. Star: Bootstrapping reasoning
with reasoning. In Proceedings of the Annual Con-
ference on Neural Information Processing Systems
(NeurIPS) .
Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. 2023a.
Self-edit: Fault-aware code editor for code genera-
tion. CoRR , abs/2305.04087.
Kexun Zhang, Danqing Wang, Jingtao Xia,
William Yang Wang, and Lei Li. 2023b. Algo:
Synthesizing algorithmic programs with generated
oracle verifiers. CoRR , abs/2305.14591.
Muru Zhang, Ofir Press, William Merrill, Alisa Liu,
and Noah A. Smith. 2023c. How language model
hallucinations can snowball. CoRR , abs/2305.13534.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu-
ating text generation with BERT. In Proceedings of
8th International Conference on Learning Represen-
tations (ICLR) .
Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang,
Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, and Yu-
jiu Yang. 2023. Solving math word problems via
cooperative reasoning induced language models. In
Processings of the 61th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL) , pages
4471–4485.
Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and
Zhenchang Xing. 2023. Red teaming chatgpt via
jailbreaking: Bias, robustness, reliability and toxicity.
CoRR , abs/2301.12867.
24
