# 2306.02231.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2306.02231.pdf
# File size: 1595753 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Fine-Tuning Language Models
with Advantage-Induced Policy Alignment
Banghua Zhu†,⋄,⋆Hiteshi Sharma⋄,⋆Felipe Vieira Frujeri⋄Shi Dong⋄
Chenguang Zhu⋄Michael I. Jordan†,‡Jiantao Jiao†,‡
November 6, 2023
Abstract
Reinforcement learning from human feedback (RLHF) has emerged as a reliable approach to aligning
large language models (LLMs) to human preferences. Among the plethora of RLHF techniques, proximal
policy optimization (PPO) is of the most widely used methods. Despite its popularity, however, PPO
may suffer from mode collapse, instability, and poor sample efficiency. We show that these issues can
be alleviated by a novel algorithm that we refer to as Advantage-Induced Policy Alignment ( APA),
which leverages a squared error loss function based on the estimated advantages. We demonstrate
empiricallythat APAconsistentlyoutperformsPPOinlanguagetasksbyalargemargin, whenaseparate
reward model is employed as the evaluator. In addition, compared with PPO, APAoffers a more stable
form of control over the deviation from the model’s initial policy, ensuring that the model improves its
performance without collapsing to deterministic output. In addition to empirical results, we also provide
a theoretical justification supporting the design of our loss function.
1 Introduction
Reinforcement learning from human feedback (RLHF, or preference-based reinforcement learning) (Knox
and Stone, 2008; Wirth et al., 2017) has delivered significant empirical successes in several fields, including
games (Christiano et al., 2017), robotics (Sadigh et al., 2017; Kupcsik et al., 2018), recommendation systems
(Maghakian et al., 2022). Recently, RLHF has also exhibited striking potential for integrating human
knowledge with large language models (Ziegler et al., 2019; Ouyang et al., 2022; OpenAI, 2023; Beeching
et al., 2023; Zhu et al., 2023; Bai et al., 2022b). To employ RLHF in the training pipeline of language models,
a common protocol is as follows.
•Pre-training (PT) : training the language model on a large amount of unlabeled or weakly labeled
text data to produce general features and patterns that can be useful for downstream tasks (Vaswani
et al., 2017; Devlin et al., 2018; Brown et al., 2020);
•Supervised fine-tuning (SFT) : training the model on a smaller amount of curated data to improve
the performance and accuracy of the model on specific tasks;
•Reinforcement learning with human feedback (RLHF) : using a human-labeled dataset together
with reinforcement learning (RL) algorithms to further align the model with complex and subjective
human values or preferences (Ziegler et al., 2019; Ouyang et al., 2022).
Both PT and SFT rely on the use of distributional loss functions, such as cross entropy, to minimize the
distance between the text distributions in the training dataset and in the model output (Vaswani et al., 2017;
Devlin et al., 2018; Brown et al., 2020). Such a simple strategy is not viable, however, for the RLHF stage.
As the ultimate target is to make the language model output conform to human linguistic norms, which are
∗Equal contributions.†Department of Electrical Engineering and Computer Sciences, UC Berkeley‡Department of
Statistics, UC Berkeley ⋄Knowledge and Language Team, Azure Cognitive Services Research, Microsoft Research. The work
was done at Microsoft when Banghua Zhu was a research intern.
1arXiv:2306.02231v3  [cs.CL]  2 Nov 2023

--- PAGE 2 ---
difficult to define or quantify, researchers usually resort to a reward model that is trained separately from the
language model on a meticulously collected, human-labeled dataset (Ouyang et al., 2022). Such a reward
model produces a scalar score for each generated response, and is, therefore, able to provide the language
model with online feedback. This accessibility to online feedback allows the language model to be trained
via reinforcement learning (RL), giving rise to the RLHF stage.
Among the RL techniques that are applied to language models, one of the most prominent algorithms
is proximal policy optimization (PPO) (Schulman et al., 2017). Despite the acclaimed effectiveness of PPO
(Ouyang et al., 2022; Stiennon et al., 2020; Nakano et al., 2021), it suffers from instability and poor sample
efficiency. The family of policy gradient algorithms suffers from slow convergence and can yield poor ultimate
policies (Yuan et al., 2022; Dong et al., 2022).
To address such issues, we introduce a novel algorithm, Advantage-Induced Policy Alignment (APA),
which leverages a squared error loss function that directly aligns the output policy of the language model
with a target policy in each training epoch. The target policy combines the initial language model policy
before the RLHF stage and a correction term based on the advantage function estimated from online samples.
Wecompare APAwithPPOandadvantageweightedregression( AWR)boththeoreticallyandempirically.
At a high level, the two existing algorithms (PPO and AWR) solve a KL-constrained policy optimization
problem, relying on the estimated importance ratio between consecutive policies to compute the policy
gradient. On the other hand, APAuses squared error to regularize the deviation of model policy and no
longer requires estimating the importance ratio. We show that such differences bring huge benefits in terms
of sample efficiency.
To demonstrate the efficacy of APAempirically, we apply APA,PPOandAWRto fine-tuning up to 7B
language models, using the human-labeled Anthropic Helpfulness and Harmlessness dataset (Ganguli et al.,
2022) and the StackExchange Beeching et al. (2023) dataset. We first evaluate using the reward model,
trained on the same dataset to produce a scalar reward for each prompt-response pair. We also evaluate
the human preferences of the resulting language model using GPT-4 to demonstrate the effectiveness of the
algorithm. Our empirical results highlight three major advantages of APAoverPPO:
(i)APAis more sample-efficient . Fine-tuned on the same number of samples, the language model
obtained via APAscores consistently higher on the evaluation set than the one obtained with PPO.
(ii)APAaffords steadier control over the deviation from the language model’s initial policy .
Measured by KLdivergence, the deviation of the ultimate policy generated by APAis comparable
with that of PPO, yetAPAis less prone to sudden performance degradation during training, which
is occasionally observed in PPO. Note that previous study has shown that the control over deviations
from the initial policy is critical in preventing over-optimization on reward models (Gao et al., 2022).
(iii)APAhas fewer hyperparameters . The loss function in APAinvolves only one major tunable
parameter for KL control, whereas in PPOone has to carefully calibrate the combination of various
extra hyperparameters, such as the clipping ranges for importance ratio and value estimates, and the
coefficients of the KL controller.
More broadly, this work is related to the line of literature on leveraging ideas from RL to improve the
performance of language models. A few notable examples in this literature include Paulus et al. (2017),
who propose a loss function based on the policy gradient objective to tackle the abstractive summarization
task, using ROUGE scores as reward; and Snell et al. (2022), who present the implicit language Q-learning
(ILQL) algorithm to facilitate learning from offline human-labeled samples without a reward model. A
thorough comparison between different RL algorithms is also made in Ramamurthy et al. (2022) on GRUE
benchmarks. There have been some alternative frameworks of RLHF that replaces PPO with SFT on best
generated sample (Yuan et al., 2023), or a direct preference-based offline learning (Rafailov et al., 2023).
The remainder of this paper is organized as follows. In Section 2, we introduce our notation. In Section
3, we formally specify the algorithm APA, and discuss the intuitions behind the algorithmic elements.
Experimental results are presented in Section 4. Section 5 concludes by summarizing and discussing the
experimental results.
2

--- PAGE 3 ---
2 Preliminaries
In this section, we overview the standard RL setting in Section 2.1, and discuss how language model training
fits into this setting in Section 2.2. We use the following notation. For a positive integer n, we will use
the bracket notation [n]to refer to the set of integers {1, . . . , n }; for a finite set Z, we denote by ∆(Z)the
set of probability distributions on Z, and |Z|the cardinality of Z. We use Bdto denote the unit ball in
d-dimensional space.
2.1 Reinforcement Learning
Reinforcementlearning(RL)capturestheinteractionbetweenanagentandanenvironmentviatheformalism
of a Markov decision process (MDP). We consider a finite-horizon MDP represented by a tuple M=
(S,A, H, P, r, ρ ), where Sis a finite state space, Ais a finite action space, His the horizon, P:S×A 7→ ∆(S)
is a probability transition matrix, r:S × A 7→ [0,1]is a reward function, and ρ:S 7→ ∆(S)is the initial
state distribution. When the agent takes action ain state sat step h, it receives a scalar reward r(s, a), and
transitions to a state s′, where s′is drawn from distribution P(·|s, a). Each episode consists of Hconsecutive
steps. At the end of an episode, the agent is reset to a state drawn from ρ(·), and a new episode begins.
A policy π:S 7→ ∆(A)is a function that maps a state to a distribution over actions. The value function
Vπ:S 7→Rof policy πis defined as the expected sum of discounted rewards when the agent starts from
initial state sand follows policy πthroughout the episode. Let γ∈[0,1]be the discount factor. For any
s∈ S, we have
Vπ(s):=E"HX
τ=0γτr(sτ, aτ)|s0=s, aτ∼π(·|sτ), sτ+1∼P(· |sτ, aτ)#
.
Given a policy π, the state-action value function, also known as the Q-function, can be defined analogously.
For state s∈ Sanda∈ A, we have
Qπ(s, a):=E"HX
τ=0γτr(sτ, aτ)|s0=s, a0=a, aτ∼π(·|sτ), sτ+1∼P(· |sτ, aτ)#
.
We also define the important notion of an advantage function . For a policy π, state sand action a, the
advantage, defined as
Advπ(s, a) =Qπ(s, a)−Vπ(s),
quantifies the extra value that is obtained by replacing the immediate action prescribed by πwith the action
a, when the agent is in state sat step h.
We also define the occupancy measures dπ
state:S 7→ [0,1]anddπ
action :S × A 7→ [0,1]as
dπ
state(s):=1
HHX
h=1P(sh=s|π)and dπ
action (s, a):=1
HHX
h=1P(sh=s, ah=a|π),
where P(· |π)signifies that all actions are drawn from π. To avoid clutter, we overload the notation dπsuch
thatdπ(s)refers to dπ
state(s), and dπ(s, a)refers to dπ
action (s, a).
2.2 Language Model as Reinforcement Learning Agent
In its simplest form, a language model receives as input a sequence of tokens (x1, . . . , x n), and generates a
distribution over the next token xn+1. All tokens lie in a finite set X. Whenever the agent selects a token
that represents the completion of a response (e.g., the end-of-sequence token), or the total number of tokens
reaches a specific limit, the entire sequence is scored by a reward model, which produces a scalar reward r.
Comparing with the RL formulation in Section 2.1, a language model can be viewed as an agent that
operatesin anenvironmentwithstate space S=SH
k=0Xkandaction space A=X, where Histhe maximum
number of tokens. The transitions are always deterministic, with the next state equal to the concatenation
of all the previous tokens and the current token P(sh+1= (x1,···, xk)|sh= (x1,···, xk−1), ah=xk) = 1.
3

--- PAGE 4 ---
Traditionally, each episode involves the generation of one complete sequence, and a reward is delivered only
when an episode terminates. In this context, fine-tuning is equivalent to improving the agent policy π. The
field of RL offers a formidable arsenal for this task. In this work, we will focus on policy-based RL algorithms,
which parameterize the set of agent policies by a set of parameters θand optimize in the parameter space.
In what follows, we will omit the step index h, as its information is already encoded in each state.
We note that most transformer-based language models map a state (context) sand an action (next
token) ato a logit qθ(s, a), and the next token is sampled according to the distribution induced by the logits
{qθ(s, a)}a∈A. This naturally gives rise to the following parameterization of language model policy:
πθ(a|s) =exp(qθ(s, a))P
a∈Aexp(qθ(s, a)).
3 Fine-Tuning Based on Reinforcement Learning
As is mentioned in Section 1, the RLHF stage is usually composed of two steps. First, a reward model is
trained from a human-labeled dataset. An RL algorithm is then applied to improve the language model
policy, using the rewards generated by the reward model. Here we focus mainly on the second step with a
given reward function.
We summarize a typical policy-based RL algorithm in Algorithm 1. In practice, the parameter update
in Equation (1) usually involves several gradient steps rather than a full minimization.
Algorithm 1 Policy Gradient
1:Input:An initial policy parameter θ0, a given loss function L(θ;D).
2:Setπ0=πinit.
3:Foriteration t= 1,2···, T
4:Roll out πθt−1to produce dataset Dt=n
(s(t)
1, a(t)
1, r(t)
1),···,(s(t)
n, a(t)
n, r(t)
n)o
5:Update policy parameter according to
θt= arg min
θL(θ;Dt). (1)
In the remainder of this section, we discuss several potential choices for L(θ;D), each targeting the goal
of maximizing regularized advantages. We also introduce the new algorithm APA, and discuss the intuitions
behind it.
As a first step, for each fixed state s, we consider the following KL-regularized optimization problem as
a target of policy improvement:
maximize
θF(θ;s, π) :=Ea∼πθ(·|s)[Advπ(s, a)]−λ·KL
πθ(· |s)πinit(· |s)
. (2)
Here πinitrefers to the initial policy of the language model before the RLHF stage, πis an arbitrary policy
that we hope to improve upon. The first term in the objective function F(θ;s, π)is an expected advantage,
and to maximize the expected advantage, the agent is encouraged to move toward the optimal action in state
s. The second term in F(θ;s, π), aKLregularizer, controls the deviation of πθfrom πinit. Such regularization
is essential, as language models are prone to over-optimization when rewards are generated by an imperfect
reward model, a phenomenon observed in Gao et al. (2022). Combined, the single-state optimization problem
in (2) aims at improving upon policy πin state swithin the proximity of πinit.
The optimization (2) is usually broken down into multiple iterations. In each iteration, we maximize
F(θ;s, πold), where πoldis the policy that the agent arrives at in the previous iteration. This technique,
referred to as Conservative Policy Iteration (CPI) , was first presented in Kakade and Langford (2002).
The optimization was subsequently generalized to KL-constrained and regularized methods referred to as
Trust Region Policy Optimization (TRPO) (Schulman et al., 2015a) and Proximal Policy Optimization
(PPO) (Schulman et al., 2017), respectively. In addition to these core methods, there have been several
4

--- PAGE 5 ---
other policy optimization methods inspired by (2), with one notable example being the Advantage-Weighted
Regression (AWR) method (Peng et al., 2019; Nair et al., 2020).
In the following subsection, we will discuss how F(θ;s, π)is connected with the loss function L(θ;D)in
various algorithms, and propose a new proximal optimization problem whose solution approximates that of
(2). The loss function in APAwill be based on this new proximal optimization problem.
3.1 Proximal policy optimization
PPO leverages importance sampling to circumvent sampling from πθ, arriving at
Ea∼πθ(·|s)[Advπold(s, a)] =Ea∼πold(·|s)πθ(a|s)
πold(a|s)Advπold(s, a)
,
where the expectation on the right-hand side can be estimated in an unbiased manner from finite samples.
PPO also involves the following innovation: Instead of penalizing the expected advantage with the
estimated KL-divergence as in (2), PPO directly subtracts the KL penalty term from the reward received
by the agent. And one may also adaptively adjust the penalty weight λbased on the deviation of πθfrom
πinit(Schulman et al., 2017; Dhariwal et al., 2017; Ziegler et al., 2019). The KL-penalized reward is then
used to estimate a new advantage function dAdv. To avoid ill-conditioned gradients caused by large values or
importance ratio estimates, PPO applies clipping to the objective function. The final loss function is thus
LPPO(θ;D) =−1
|D|X
(s,a)∈Dminπθ(a|s)
πold(a|s)dAdv(s, a),clipπθ(a|s)
πold(a|s),1−ϵ,1 +ϵ
dAdv(s, a)
.
Note that the loss function relies on extra tunable hyperparameters. The clipping also makes the estimator
biased.
3.2 Advantage weighted regression
If the parameterized policy space {πθ}contained all possible policies including the ground truth policy, the
maximizer of F(θ;s, πold)(2) would induce a policy π⋆that satisfies
π⋆(a|s) =1
Z(s)πinit(a|s)·exp(Advπold(s, a)/λ), (3)
where Z(s) =P
a′∈Aπinit(a′|s)·exp(Advπ(s, a′)/λ)is a normalizing factor. In the case that {πθ}does
not contain all policies, a natural way to maximize F(θ;s, πold)is to project π⋆to{πθ}with respect to
KL-divergence. From (3),
KL 
π⋆(a|s)∥πθ(a|s)
=−πinit(a|s)
Z(s)expAdvπold(s, a)
λ
log 
πθ(a|s)
+C(s), (4)
where C(s)is a constant that does not depend on θ.
To facilitate online update, AWR makes three changes from Equation (4):
•AWR replaces the first-round policy πinitwith the previous-round policy πold. This ensures that one
can utilize the new roll-out samples from previous-round policy to approximate (4).
•The KL-divergence in (4) only accounts for one state s. AWR minimizes a distribution of states dπold.
•AWR approximates Z(s)≈1. We also provide a related discussion in Appendix A on why such an
approximation is warranted.
These changes lead to the loss function introduced in AWR:
LAWR(θ) =−E(s,a)∼dπoldh
exp 
Advπold(s, a)/λ
log 
πθ(a|s)i
. (5)
5

--- PAGE 6 ---
Given a finite dataset D={(si, ai) :i= 1, . . . , n }sampled from dπold, the corresponding empirical loss
can be written as
LAWR(θ;D) =−1
|D|X
(s,a)∈Dexp 
Advπold(s, a)/λ
log 
πθ(a|s)
. (6)
For the well-specified case where the parameterized family {πθ}contains the minimizing policy, the
minimizer of the population loss is as follows:
π′(a|s) =πold(a|s) exp( Advπold(s, a)/λ)P
aπold(a|s) exp( Advπold(s, a)/λ). (7)
Due to the discrepancies between the original target in Equation (4) and the final loss in Equation (5),
one can see that the policy AWR converges to is different from the the original target in Equation (3).
Furthermore, since πoldchanges in each round, the policy it converges to continues to change.
As we observe in Section 4 and Appendix D.3, AWR can be unstable in the online case due to this reason.
This motivates us to introduce APA, which alleviates this issue, provably converges to the right target in
Equation (3), and demonstrates great empirical performance.
3.3 Advantage-Induced Policy Alignment
To project the optimal policy π⋆in (3) onto the parameterized policy space, we consider another distance
instead of KL-divergence. In APA, we employ the squared error between log probabilities in place of the
KL-divergence:
 
logπ⋆(a|s)−logπθ(a|s)2=
logπθ(a|s) + log Z(s)−Advπold(s, a)/λ−logπinit(a|s)2
.
Similar to the approximation in AWR, we also apply Z(s)≈1, and minimize the expected loss under a state
distribution dπoldin each round, giving rise to the following population loss:
LAPA(θ) =E(s,a)∼dπoldh
logπθ(a|s)−Advπold(s, a)/λ−logπinit(a|s)2i
. (8)
The empirical loss on a finite dataset Dsampled from dπoldis thus
LAPA(θ;D) =1
|D|X
(s,a)∈D
logπθ(a|s)−Advπold(s, a)/λ−logπinit(a|s)2
. (9)
Assuming that the parameter space is Θ =Bdand that the parameterized policy space is well-specified
such that π⋆∈ {πθ|θ∈Θ}, where π⋆is defined in Equation (3), we can establish theoretically that the
empirical loss is a reasonable surrogate for the population loss.
Theorem 1. Letθ⋆∈arg minθ∈ΘLAPA(θ)be a minimizer of the population loss. Then
πθ⋆(a|s) =π⋆(a|s),∀(s, a)∈supp(πold).
Furthermore, let ˆθ∈arg minθ∈ΘLAPA(θ,D)be an empirical loss minimizer. Assume that min(πθ(a|
s), πinit(a|s))≥B1and|Adv(s, a)| ≤B2for any s, a, and that log(πθ)isL-Lipschitz with respect to θ
under ℓ2-norm for any s, a. Then for all δ >0, with probability at least 1−δ, for some universal constant
C,
LAPA(ˆθ)≤CL(B2−log(B1))2r
dlog(nL/δ )
n.
The proof is deferred to Appendix E. From the theorem, we see that the minimizer of the population
APAloss is exactly the target policy π⋆if the policy πoldis supported on all state-action pairs. In contrast, as
we discussed earlier, convergence properties of the PPO and AWR algorithms have not yet been established.
We also provide alternative interpretations of the proposed loss in terms of f-divergence and soft-Q
learning in Appendix B.
6

--- PAGE 7 ---
4 Experimental Results
In our implementation of all of the algorithms that we test, including APA, we define the advantage function
tobe Advπold(s, a), whichisestimatedfromdata. Weusethesamegeneralizedadvantageestimationapproach
to estimate the advantage as discussed in earlier work Mnih et al. (2016); Schulman et al. (2015b). In
particular, for the rollout (s0, a0, r0, s1, a1, r1,···, sT−1, aT−1, rT−1, sT), the generalized advantage estimator
is
ˆAπold(st, at) =δt+λγδt+1+···+ (λγ)T−1δT−1,
where δt=r(st, at) +γVπold(st+1)−Vπold(st).
Here the value function is another standalone network that we fit throughout the training process with a
squared loss, ˆLV(D) =P
si,ai(V(si)−ˆAπold(si, ai)−Vπold(si))2. Thus the overall loss function is
LAPA
θ(D) =ˆLAPA(D) +η·ˆLV(D)
LAWR
θ(D) =ˆLAWR(D) +η·ˆLV(D).
For the implementation of PPO, we use the PPO2 version from Dhariwal et al. (2017), with the adaptive
KL controller from Ziegler et al. (2019). We implement PPO with the same hyperparameters as the
implementation in trlX1, which also follows default hyperparameters suggested by Schulman et al. (2017).
The main difference between our version of PPO and that in trlXis that we create a completely separate
value network rather than creating a value head on top of the language model. In APA, we take λ= 0.1
to impose a weaker constraint on the KL coefficient. For AWR, we find that setting λ= 0.1leads to an
explosion of the loss; thus we take λ= 1to stabilize training.
4.1 Results on the StackExchange dataset
In this section, we present our experimental results with StackExchange dataset. This dataset includes
questions and their corresponding answers from the StackExchange platform (including StackOverflow for
code and many other topics). The answers are then voted by the users on the platform and an accepted
answer is labeled. Following Beeching et al. (2023); Askell et al. (2021), we assign a score to each answer
depending on the number of upvotes:
score =

−1, upvotes ≤0,
1 +⌊log2(1 +upvotes ) + 0.5⌋,if the questioner accepted the answer ,
⌊log2(1 +upvotes ) + 0.5⌋,otherwise .
Figure 1: Win rates computed by GPT-4 for
StackExchange dataset for models trained by SFT,
PPO and APA. Compared to SFT and PPO, APA
trained models generated better responses.We used the pre-processed dataset provided in
Beeching et al. (2023) for all SFT, reward
modeling and RL training purposes, available
intheHuggingFaceDatasetsas lvwerra/stack-exchange-paired2
. We use LLaMA-7B Touvron et al. (2023) models
for this experiment. We use Low-Rank Adaptation
(LoRA) method Hu et al. (2021) to reduce the
memory consumption while training. We used
8xA100 GPUs for our experiments. The hyper-
parameters for this experiment are listed in the
Appendix C.4. Fig. 2 shows the reward on
the left and KL divergence from the initial policy
for the three algorithms, PPO, APA and AWR.
We adjust the hyper-parameters to achieve similar
KL divergence values, allowing us to compare the
1https://github.com/CarperAI/trlx
2https://huggingface.co/datasets/lvwerra/stack-exchange-paired
7

--- PAGE 8 ---
rewards for various algorithms. In the case of AWR,
each hyper-parameter set displayed some level of
instability. Clearly, APA quickly converges to a
higher reward than PPO and AWR.
Figure 2: Comparison of the performance of three methods on the StackExchange dataset. Left: The x-axis
represents the total steps, which are proportional to the amount of data used in the training procedure. The
y-axis is the reward computed by the same reward model during training. Right: The x-axis represents the
total steps. The y-axis is the KL divergence between the trained model and the initial model.
GPT-4 Evaluation: We conduct a GPT-4 evaluation to evaluate the models trained by different RL
methods on StackExchange dataset. GPT-4 compares the outputs produced by two models, using a reference
(chosen) response as a basis for comparison. Fig. 1 shows the win-rates for comparing SFT vs PPO, SFT
vs APA and PPO vs APA models. APA consistently outperforms the other two models.
4.2 Results on the HH dataset
In this section, we compare PPO, AWR and APAon the human-labeled Helpfulness and Harmlessnes (HH)
dataset from Bai et al. (2022a).3. We fine tune three models, including Dahoas/pythia-125M-static-sft ,4
Dahoas/pythia-1B-static-sft ,5andDahoas/pythia-6B-static-sft .6All the models have gone through
supervised fine-tuning with labeled prompt-response pairs, similar to the protocol in Ouyang et al. (2022)
and Ramamurthy et al. (2022). We present the performance of the RL algorithms for pythia-125M and
pythia-1B in Fig. 3. It shows that after some steps, PPO’s performance begins to deteriorate while APA
and AWR are stable. APA achieves the higher reward while maintaining KL divergence from the initial
policy smaller. The details about hyper-parameters used for training and additional results for larger models
are discussed in Appendix C.1.
5 Conclusions
Inthispaper, westudytheproblemofonlinepolicyoptimizationinRLHF.Webenchmarktheperformanceof
existing algorithms PPOandAWR, and introduce a new method, APA, which has a theoretical convergence
guarantee and affords several advantages over existing algorithms. The key takeaways from our study can
be summarized as follows.
3https://huggingface.co/datasets/Dahoas/static-hh
4https://huggingface.co/Dahoas/pythia-125M-static-sft
5https://huggingface.co/Dahoas/pythia-1B-static-sft
6https://huggingface.co/Dahoas/pythia-6B-static-sft
8

--- PAGE 9 ---
0 2500 5000 7500 10000 12500 15000 17500 20000
steps3.0
2.5
2.0
1.5
1.0
0.5
rewardOnline Policy Optimization with 125M model
PPO
AWR
APA
2.5 5.0 7.5 10.0 12.5 15.0 17.5
steps / k0.00.51.01.52.02.53.03.5KLKL divergence between the trained and the initial policy for 125M model
PPO
AWR
APA
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
steps / k2.0
1.8
1.6
1.4
1.2
1.0
0.8
0.6
rewardOnline Policy Optimization with 1B model
PPO
AWR
APA
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
steps / k0.00.20.40.60.81.01.21.4KLKL divergence between the trained and the initial policy for 1B model
PPO
AWR
APAFigure 3: Comparison of the performance of three methods on the HH dataset. Left: The x-axis represents
the total steps, which are proportional to the amount of data used in the training procedure. The y-axis is
the reward evaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is
the KL divergence between the trained model and the initial model.
9

--- PAGE 10 ---
Stability. As we discussed in Section 1, one of the challenges in RLHF is instability. It is crucial in
RL algorithms to impose control on the divergence between new policies and the initial policy after SFT.
However, the clipping of the objective function and the adaptive KL controller can make the behavior of
PPO unstable; for AWR, the update in (7), which reweighs the previous policy by a multiplicative factor
in each iteration, also has unknown ramifications. APA, on the other hand, provably converges to π⋆when
the advantage function is fixed, which is close to the initial policy in KL divergence. From the experimental
results, we see that APAis able to provide better and easy-to-adjust KL control by explicitly tuning the
hyperparameter λ. Our experiments reveal different levels of instability for PPO and AWR. Specifically,
PPO suffers from occasional performance degradation whenever the model policy diverges too much from
the initial policy πinit, and such effect is more pronounced for smaller models. We attribute this to the KL
controller in PPO. In Appendix D, we demonstrate that PPO can achieve a similar sample efficiency as
APAwithout the KL penalty, albeit at the cost of weaker KL efficiency.
Sampleefficiency. WiththesamelevelofcontroloverKL-divergence, APAshowshighersampleefficiency
than PPO and AWR. One possible explanation is that in both PPO and AWR, policy improvement critically
depends on using finite samples to reconstruct the sampling policy πold, whereas in APA, minimizing the
population loss (8) hinges less on the reconstruction of πold. In fact, the APApopulation loss (8) can be
effectivelyminimizedaslongasthedataset Dhasadecentcoverageoverstate-actionpairsthatarefrequently
visited by πold. For more discussions on sample efficiency, please refer to Appendix B.
Online vs. offline learning. Our experiments primarily examine the online case, where new data can be
generated during the training process. The offline setting, where a fixed dataset is given and new samples are
not available, may yield qualitatively different results. In particular, suppose that the offline dataset consists
of rollouts from a policy πoff. In this case, if it were trained with infinitely many samples, AWR would
converge to the policy specified in (3). However, the performance of APAmay suffer from distribution shift
because it can only learn from state-action pairs covered by πoff, and there is no guarantee that the learned
policy performs well on the state-action pairs visited by the current policy. Such distribution mismatch can
lead to a significant performance drop for APA, as we observe in Appendix D.3. We also observe that AWR
typically outperforms ILQL for offline learning, although both perform poorly with larger models.
10

--- PAGE 11 ---
References
A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma,
N. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. Brown,
J. Clark, S. McCandlish, C. Olah, and J. Kaplan. A general language assistant as a laboratory for
alignment, 2021. 4.1
Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan,
et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv
preprint arXiv:2204.05862 , 2022a. 4.2
Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini,
C. McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 ,
2022b. 1
E. Beeching, Y. Belkada, K. Rasul, L. Tunstall, L. von Werra, N. Rajani, and N. Lambert. StackLLaMA:
An RL fine-tuned LLaMA model for Stack Exchange question and answering, 2023. URL https://
huggingface.co/blog/stackllama . 1, 4.1, 4.1
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 , 2020. 1
P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from
human preferences. In Advances in Neural Information Processing Systems , pages 4299–4307, 2017. 1
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers
for language understanding. arXiv preprint arXiv:1810.04805 , 2018. 1
P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, Y. Wu, and
P. Zhokhov. OpenAI baselines. https://github.com/openai/baselines , 2017. 3.1, 4
S. Dong, B. Van Roy, and Z. Zhou. Simple agent, complex environment: Efficient reinforcement learning
with agent states. Journal of Machine Learning Research , 23(255):1–54, 2022. 1
D.Ganguli, L.Lovitt, J.Kernion, A.Askell, Y.Bai, S.Kadavath, B.Mann, E.Perez, N.Schiefer, K.Ndousse,
et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.
arXiv preprint arXiv:2209.07858 , 2022. 1
L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. arXiv preprint
arXiv:2210.10760 , 2022. (ii), 3
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank
adaptation of large language models, 2021. 4.1
S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In Proceedings of
the Nineteenth International Conference on Machine Learning , pages 267–274, 2002. 3
W. B. Knox and P. Stone. TAMER: Training an agent manually via evaluative reinforcement. In 7th IEEE
International Conference on Development and Learning , pages 292–297. IEEE, 2008. 1
A. Kupcsik, D. Hsu, and W. S. Lee. Learning dynamic robot-to-human object handover from human
feedback. In Robotics research , pages 161–176. Springer, 2018. 1
J. Maghakian, P. Mineiro, K. Panaganti, M. Rucker, A. Saran, and C. Tan. Personalized reward learning
with interaction-grounded learning (IGL). arXiv preprint arXiv:2211.15823 , 2022. 1
V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.
Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning ,
pages 1928–1937. PMLR, 2016. 4
11

--- PAGE 12 ---
A. Nair, A. Gupta, M. Dalal, and S. Levine. Awac: Accelerating online reinforcement learning with offline
datasets. arXiv preprint arXiv:2006.09359 , 2020. 3
R.Nakano, J.Hilton, S.Balaji, J.Wu, L.Ouyang, C.Kim, C.Hesse, S.Jain, V.Kosaraju, W.Saunders, etal.
WebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 ,
2021. 1
OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. 1
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,
A. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint
arXiv:2203.02155 , 2022. 1, 4.2
R. Paulus, C. Xiong, and R. Socher. A deep reinforced model for abstractive summarization. arXiv preprint
arXiv:1705.04304 , 2017. 1
X. B. Peng, A. Kumar, G. Zhang, and S. Levine. Advantage-weighted regression: Simple and scalable
off-policy reinforcement learning. arXiv preprint arXiv:1910.00177 , 2019. 3
R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization:
Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290 , 2023. 1
R. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hessel, R. Sifa, C. Bauckhage, H. Hajishirzi, and Y. Choi.
Isreinforcementlearning(not)fornaturallanguageprocessing? benchmarks,baselines,andbuildingblocks
for natural language policy optimization. arXiv preprint arXiv:2210.01241 , 2022. 1, 4.2
D. Sadigh, A. D. Dragan, S. Sastry, and S. A. Seshia. Active preference-based learning of reward functions.
InRobotics: Science and Systems , 2017. 1
J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In
International Conference on Machine Learning , pages 1889–1897. PMLR, 2015a. 3
J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using
generalized advantage estimation. arXiv preprint arXiv:1506.02438 , 2015b. 4
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347 , 2017. 1, 3, 3.1, 4
C. Snell, I. Kostrikov, Y. Su, M. Yang, and S. Levine. Offline RL for natural language generation with
implicit language Q-learning. arXiv preprint arXiv:2206.11871 , 2022. 1
N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano.
Learning to summarize with human feedback. Advances in Neural Information Processing Systems , 33:
3008–3021, 2020. 1
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. RoziÃšre, N. Goyal,
E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient
foundation language models, 2023. 4.1
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.
Attention is all you need. Advances in Neural Information Processing Systems , 30, 2017. 1
C. Wirth, R. Akrour, G. Neumann, and J. Fürnkranz. A survey of preference-based reinforcement learning
methods. The Journal of Machine Learning Research , 18(1):4945–4990, 2017. 1
R. Yuan, R. M. Gower, and A. Lazaric. A general sample complexity analysis of vanilla policy gradient.
Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS) , 2022.
1
Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. Rrhf: Rank responses to align language
models with human feedback without tears. arXiv preprint arXiv:2304.05302 , 2023. 1
12

--- PAGE 13 ---
B. Zhu, J. Jiao, and M. I. Jordan. Principled reinforcement learning with human feedback from pairwise or
k-wise comparisons. International Conference on Machine Learning , 2023. 1
D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving.
Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019. 1, 3.1, 4
13

--- PAGE 14 ---
A Argument for Z(s)≈1
Note that in both advantage-weighted regression and advantage-based squared loss, we approximate Z(s)
with 1. Here we justify why this does not hurt the performance.
Consider an infinitesimal scenario where |Adv/λ| ≪ | logπinit|. In the scenario of language model, this
is usually true since πinitis supported on approximately 50kdistinct tokens and can be very close to zero,
while Adv/λcan be adjusted to small numbers by adjusting λ.
In this case, we have
Z(s) =X
a∈Aπinit(a|s) exp( Adv(s, a)/λ)
=Ea∼πinit[exp( Adv(s, a)/λ)]
=Ea∼πinit[1 +Adv(s, a)/λ+o(Adv2(s, a)/λ2)].
This advantage is usually estimated as Advπold, which can be close to Advπinit. And we have
Ea∼πinit[Advπold(s, a)/λ]≈Ea∼πinit[Advπinit(s, a)/λ] = 0.
Thus we know that
Z(s)≈1 +Ea∼πinit[o(Adv2(s, a)/λ2)]≈1.
In practice, we observe that the squared loss decreases very slowly due to a small learning rate ( 8e−6).
This suggests that the policy changes very slowly, which is another reason why the normalizing factor is not
important.
B Alternative Interpretation of APA
Recall that APAcan be written as
LAPA(θ) =E(s,a)∼dπoldh
logπθ(a|s)−logπ⋆(a|s)2i
,
where π⋆=πinit·exp(Adv/λ). In the case when πoldis close to πθ, minimizing the squared loss in APAis
equivalent to minimizing the following distance between π⋆andπθ:
d(π⋆(· |s), πθ(· |s)) =X
aπθ(a|s)
logπθ(a|s)
π⋆(a|s)2
.
This can be viewed as a new f-divergence with f(x) =xlog2(x). We can show by Cauchy-Schwarz that this
is always a upper bound for the KL divergence:
d(π⋆(· |s), πθ(· |s)) = X
aπθ(a|s)! X
aπθ(a|s)
logπθ(a|s)
π⋆(a|s)2!
≥X
aπθ(a|s)logπθ(a|s)
π⋆(a|s)
≥X
aπθ(a|s) logπθ(a|s)
π⋆(a|s)
.
C Additional Experiments
C.1 Results on the HH dataset
In this dataset, each item is comprised of a prompt, a chosen response and a rejected response labeled by
human to evaluate the helpfulness and harmlessness of the responses. For the reward model, we use the
14

--- PAGE 15 ---
proxy reward model Dahoas/gptj-rm-static7with 6B parameters trained from the same dataset based on
EleutherAI/gpt-j-6b .8For all three algorithms, we run two epochs of update after generating 64 responses
from randomly sampled prompts. For the 125M model, we use batch size 8and learning rate 8×10−6. For
the 1B model, we use batch size 2and learning rate 10−6. For the 6B and larger models, we use batch size
1and learning rate 10−6. We use a 32GB Nvidia V100 GPU for fine-tuning 125M and 1B models, and a
64GB AMD Mi200 GPU for fine-tuning the 6B and larger models. The maximum response length is set to
be 128 tokens, and the maximum total sequence length is set to be 1024 tokens. We unfreeze the last two
layers during fine-tuning. For each experiment, we run 20k steps in total. The results are plotted as below.
In the left of Figure 4, we compare the three methods on the HH dataset. For all three models, we repeat
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
steps / k2.4
2.2
2.0
1.8
1.6
1.4
1.2
rewardOnline Policy Optimization with 6B model
PPO
AWR
APA
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
steps / k0.00.20.40.60.81.0KLKL divergence between the trained and the initial policy for 6B model
PPO
AWR
APA
Figure 4: Comparison of the performance of three methods on the HH dataset. Left: The x-axis represents
the total steps, which are proportional to the amount of data used in the training procedure. The y-axis is
the reward evaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is
the KL divergence between the trained model and the initial model.
the experiments with three random seeds 0,100,1000, and plot their min, mean and max. We see that with
the same amount of data, APAis able to achieve the highest reward in all three cases. We also observe that
PPO becomes more stable with large models, potentially due to smaller batch size, or the ability of getting
higher reward with a smaller deviation in KL divergence.
On the right of Figure 4, we show how the KL divergence between the current policy and the initial
policy changes as a function of the training process for the three seeds. We can see that for all three models,
APAprovides similar or better KL control than PPO and AWR, although we note that for the 6B model the
KL control for PPO is slightly better than APA. Combined with the left part of the figure, we can see that
APAis more KL-efficient than PPO and AWR; i.e., it attains a better performance on the reward model
under the same KL divergence.
We include more experiment results in Appendix C, where we fine tune databricks/dolly-v2-7b9on
the same HH dataset, and 2.7B and 6B models on the TLDR dataset10for the summarization task.
We also conduct ablation studies on the effect of the adaptive KL controller on PPO and the effect of
different choices of λfor AWR; see Appendix D. We show in Appendix D.2 that without KL control, PPO
can be as sample efficient as APA, but less KL-efficient. We also observe instability even without the KL
controller. On the other hand, we observe that changing λprovides a straightforward tradeoff between KL
control and performance in APA.
7https://huggingface.co/Dahoas/gptj-rm-static
8https://huggingface.co/EleutherAI/gpt-j-6b
9https://huggingface.co/databricks/dolly-v2-7b
10https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons
15

--- PAGE 16 ---
C.2 Results on the TLDR Dataset
We fine-tune the EleutherAI/gpt-neo-2.7B11and 6B CarperAIopenai_summarize_tldr_sft12models
on the TLDR dataset13for the summarization task. For EleutherAI/gpt-neo-2.7B , we first fine-tune it
with supervised fine-tuning on the labeled response in the same summarization dataset, and run RLHF on
the supervised fine-tuned policy. The 6B model CarperAIopenai_summarize_tldr_sft has already gone
through the supervised fine-tuning stage. The reward model is a pre-trained EleutherAI/gpt-j-6b14reward
model for summarization dataset CarperAI/openai_summarize_comparisons15.
We follow the default setting in trlX with seed 0 and 100, and plot the results in Figure 5. One can see
thatAPAis more sample efficient and provides better KL control than PPO in both 2.7B and 6B models.
0 2 4 6 8 10
steps / k2.02.12.22.32.42.52.62.72.8rewardOnline Policy Optimization with 2.7B model
PPO
AWR
APA
0 2 4 6 8
steps / k0.00.51.01.52.02.5KLKL divergence between the trained and the initial policy in 2.7B model
PPO
AWR
APA
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
steps / k2.42.62.83.03.23.43.6rewardOnline Policy Optimization with 6B model
PPO
AWR
APA
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
steps / k0.20.30.40.50.60.70.8KLKL divergence between the trained and the initial policy in 6B model
PPO
AWR
APA
Figure 5: Comparisons of the performance on TLDR dataset. Left: The x-axis represents the total steps,
which are proportional to the number of data used in the training procedure. The y-axis is the reward
evaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is the KL
divergence between the trained model and the initial model.
11https://huggingface.co/EleutherAI/gpt-neo-2.7B
12https://huggingface.co/CarperAI/openai_summarize_tldr_sft
13https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons
14https://huggingface.co/EleutherAI/gpt-j-6b
15https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons
16

--- PAGE 17 ---
C.3 Results on the Dolly Model
We fine-tune the databricks/ dolly-v2-7b16model on the HH dataset. We follow the default setting in
trlX with seed 0 and 100, and plot the results in Figure 6. We only include the results for APAand PPO
since AWR drops directly. Different from all other experiments, here for APAwe set λ= 1rather than 0.1
to stablize the training and impose stronger KL control. One can see that APAcan still improve over the
original dolly 7B model and provide better KL control, while PPO fails to bring further improvement.
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
steps / k1.6
1.5
1.4
1.3
1.2
rewardOnline Policy Optimization with Dolly 7B model
PPO
APA
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
steps / k0.000.020.040.060.080.10KLKL divergence between the trained and the initial policy in dolly 7B model
PPO
APA
Figure 6: Comparisons of the performance on the dolly 7B model. Left: The x-axis represents the total
steps, which are proportional to the number of data used in the training procedure. The y-axis is the reward
evaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is the KL
divergence between the trained model and the initial model.
C.4 StackExchange experiment details
The hyper-parameters for the Fig. 2 are listed in table 1.
Parameter Value
Max sequence length 1024
Max output length 128
Learning rate 2e-5
Batch size 16
Gradient Accumulation 8
SFT LoRA dimension 16
RM LoRA dimension 8
RL LoRA dimension 16
Adaptive KL initial KL coeff (PPO) 0.1
Adaptive KL target KL coeff (PPO) 6
λ(APA) 0.1
Table 1: Hyper-parameters for StackExchange experiments as shown in Fig. 2
16https://huggingface.co/databricks/dolly-v2-7b
17

--- PAGE 18 ---
D Ablation Studies
D.1 KL control in APA
In this section, we show how the performance and KL divergence change with different values of λ. We set
λ= 0.1,1for the 125M model and plot their performances in Figure 7 with seed 1000. One can see that the
choice of λdirectly determines the level of KL control, along with the convergent point APAreaches. This
shows that λprovides a clear trade-off between KL control and model performance.
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
steps / k3.0
2.5
2.0
1.5
1.0
0.5
rewardOnline Policy Optimization with 125M model
Lambda=0.1
Lambda=1
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
steps / k0.0000.0250.0500.0750.1000.1250.150KLKL divergence between the trained and the initial policy for APA in 125M model
Lambda = 0.1
Lambda = 1
Figure 7: Comparisons of the performance between different λon the 125M model. Left: The x-axis
represents the total steps, which are proportional to the number of data used in the training procedure. The
y-axis is the reward evaluated by the same reward model. Right: The x-axis represents the total steps. The
y-axis is the KL divergence between the trained model and the initial model.
D.2 KL control in PPO
We show how the performance and KL divergence change with or without adaptive KL control in PPO. We
plot their performances in Figure 8 for 125M model with seed 1000. For PPO with adaptive KL controller,
the initial KL coefficient is set to be 0.05. One can see that without KL control, PPO converges to a higher
reward compared to APAin Figure 7, at a cost of a significantly higher KL divergence. On the other hand,
the reward of PPO with adaptive KL control begins to drop in the middle. This is due to the large deviation
from the original policy, which leads to a much larger KL regularization term that dominates the reward.
Compared with Figure 7, one can see that APAprovides more stable and controllable KL regularization.
D.3 Experiments for Offline Learning
We conduct experiments for offline learning as well. The offline dataset is selected to be all the prompts and
responses from the HH dataset, with reward labeled by the reward model. We use the trained GPT-J reward
function to label the reward for all the offline data, and compare ILQL, AWR and APAon the same 125M
and 1B model after supervised fine-tuning with seed 1000. The result is given in Figure 9. From the results,
one can see that AWR performs better than ILQL, and APAcannot be directly adapted to the offline case.
Furthermore, offline learning cannot help too much after the supervised fine-tuning stage, potentially due to
the large distribution shift between the offline data and the current policy.
18

--- PAGE 19 ---
0 2 4 6 8 10 12 14
steps / k3.0
2.5
2.0
1.5
1.0
0.5
rewardOnline Policy Optimization with 125M model
Adaptive KL Control
No KL Control
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
steps / k01234567KLKL divergence between the trained and the initial policy in 125M model
Adaptive KL Control
No KL ControlFigure 8: Comparisons of the performance of PPO on the 125M model. Left: The x-axis represents the
total steps, which are proportional to the number of data used in the training procedure. The y-axis is the
reward evaluated by the same reward model. Right: The x-axis represents the total steps. The y-axis is the
KL divergence between the trained model and the initial model.
0 2 4 6 8 10 12
steps / k2.8
2.7
2.6
2.5
2.4
2.3
2.2
2.1
2.0
rewardOffline Policy Optimization with 125M model
ILQL
AWR
APA
0 2 4 6 8 10
steps / k2.1
2.0
1.9
1.8
1.7
1.6
rewardOffline Policy Optimization with 1B model
ILQL
AWR
APA
Figure 9: Comparisons of the performance between ILQL, AWR and APA on the offline learning dataset.
19

--- PAGE 20 ---
E Proof of Theorem 1
Proof.From the well-specified assumption π⋆∈ {πθ|θ∈Θ}, we know that there exists some θ⋆∈Θsuch
thatπθ⋆=π⋆. For the population loss, we know that
LAPA(θ⋆) =E(s,a)∼dπolds,ah
(logπθ⋆(a|s)−Adv(s, a)/λ−logπinit(a|s))2i
=E(s,a)∼dπolds,ah
(logπ⋆(a|s)−Adv(s, a)/λ−logπinit(a|s))2i
= 0.
Thus for any θ′∈arg minθ∈ΘLAPA(θ), there must be LAPA(θ′) = 0, which is equivalent to
E(s,a)∼dπolds,ah
(logπθ′(a|s)−Adv(s, a)/λ−logπinit(a|s))2i
= 0.
This means that for any s, aon the support of dπolds,a, we have πθ′(a|s) =π⋆(a|s).
For the second part of the theorem, we know from Hoeffding’s inequality that for any fixed θ∈Θ,
|LAPA(θ)−bLAPA(θ;D)|=1
nnX
i=1
logπθ(ai|si)−Adv(si, ai)/λ−logπinit(ai|si)2
−Eh
logπθ(a|s)−Adv(s, a)/λ−logπinit(a|s)2i
≤C·(B2/λ−2 log( B1))2r
log(1/δ)
n.
Let the Θϵbeϵ-covering of Θunder ℓ2norm, i.e. for any θ∈Θ, one can find some θ′∈Θϵsuch that
∥θ−θ′∥2≤ϵ. We also have |Θϵ| ≤(1/ϵ)d. By taking union bound, we know that for all θ∈Θϵ, with
probability at least 1−δ,
|LAPA(θ)−bLAPA(θ;D)| ≤C·(B2/λ−log(B1))2r
dlog(1/(ϵδ))
n. (10)
Letˆθbe the minimizer of bLAPA(θ;D). Then we know that there exists some ˆθϵ∈Θϵsuch that ∥ˆθ−ˆθϵ∥ ≤ϵ.
This further implies that
|LAPA(ˆθ)− LAPA(ˆθϵ)|
=|Eh
logπˆθ(a|s)−Adv(s, a)/λ−logπinit(a|s)2i
−Eh
logπˆθϵ(a|s)−Adv(s, a)/λ−logπinit(a|s)2i
|
≤C(B2/λ−log(B1))Lϵ. (11)
Similarly, we also have |bLAPA(ˆθ)−bLAPA(ˆθϵ)| ≤C(B2/λ−log(B1))Lϵ.Overall, we have
LAPA(ˆθ) = (LAPA(ˆθ)− LAPA(ˆθϵ)) + (LAPA(ˆθϵ)−bLAPA(ˆθϵ)) + (bLAPA(ˆθϵ)−bLAPA(ˆθ)) +bLAPA(ˆθ).
For the first and third difference, from Equation (11) we know that they are both bounded by C(B2/λ−
log(B1))Lϵ.For the second difference, we know from Equation (10) that it is bounded by C(B2/λ−
log(B1))2q
dlog(1/(ϵδ))
n. Lastly, we know that bLAPA(ˆθ) = 0since ˆθ∈arg minθbLAPA(θ)andbLAPA(θ⋆) = 0.
Thus overall, we have
LAPA(ˆθ)≤C((B2/λ−log(B1))Lϵ+ (B2/λ−log(B1))2r
dlog(1/(ϵδ))
n).
Taking ϵ= 1/(Ln)finishes the proof.
20
