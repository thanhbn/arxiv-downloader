PERSONAS LÀ MỘT CÁCH ĐỂ MÔ HÌNH HÓA TÍNH TRUNG THỰC TRONG CÁC MÔ HÌNH NGÔN NGỮ

Nitish Joshi1∗Javier Rando2∗Abulhair Saparov1Najoung Kim3He He1
1New York University2ETH Zurich3Boston University
{nitish}@nyu.edu {javier.rando}@ai.ethz.ch

TÓM TẮT
Các mô hình ngôn ngữ lớn (LLM) được huấn luyện trên lượng lớn văn bản từ internet, chứa cả thông tin chính xác và gây hiểu lầm về thế giới. Mặc dù trái ngược với quan điểm cổ điển về LM, nghiên cứu gần đây đã chỉ ra rằng giá trị chân lý của một phát biểu có thể được rút ra từ các biểu diễn của mô hình. Bài báo này trình bày một lời giải thích về lý do tại sao LM có vẻ biết sự thật mặc dù không được huấn luyện với nhãn chân lý. Chúng tôi đưa ra giả thuyết rằng dữ liệu huấn luyện trước được tạo ra bởi các nhóm tác nhân (không) trung thực có đầu ra chia sẻ các đặc điểm chung, và chúng tạo thành một persona (không) trung thực. Bằng cách huấn luyện trên dữ liệu này, LM có thể suy luận và biểu diễn persona trong không gian kích hoạt của nó. Điều này cho phép mô hình phân tách sự thật khỏi sự giả dối và kiểm soát tính trung thực của việc tạo ra. Chúng tôi chỉ ra bằng chứng cho giả thuyết persona qua hai quan sát: (1) chúng tôi có thể dò tìm liệu câu trả lời của mô hình có trung thực trước khi nó được tạo ra; (2) tinh chỉnh mô hình trên một tập hợp các sự kiện cải thiện tính trung thực của nó trên các chủ đề chưa thấy. Tiếp theo, sử dụng số học như một môi trường tổng hợp, chúng tôi chỉ ra rằng cấu trúc của dữ liệu huấn luyện trước là rất quan trọng để mô hình suy luận persona trung thực. Nhìn chung, các phát hiện của chúng tôi gợi ý rằng các mô hình có thể khai thác các cấu trúc phân cấp trong dữ liệu để học các khái niệm trừu tượng như tính trung thực.

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) được huấn luyện trước trên lượng dữ liệu ngày càng tăng từ internet (Brown et al., 2020; Chowdhery et al., 2022)—một tập dữ liệu nhiễu chứa cả những phát biểu chính xác và không chính xác về thế giới. Ví dụ, CDC tuyên bố rằng "hầu hết các nghiên cứu cho thấy vắc xin COVID là an toàn" (đúng), trong khi InfoWars tuyên bố rằng "các chất gây ô nhiễm DNA trong tiêm COVID có thể gây ra ung thư" (sai). Những quan niệm sai lầm và thuyết âm mưu như vậy gây ra rủi ro về thông tin sai lệch khi chúng có thể được nhắc lại bởi các mô hình khi tương tác với người dùng (Lin et al., 2021).

Trong công trình này, văn bản trung thực được định nghĩa là văn bản phù hợp với các sự kiện mà hầu hết các chuyên gia trong lĩnh vực đồng ý. Văn bản không trung thực, khác biệt với các lỗi rõ ràng, đề cập đến thông tin có vẻ hợp lý nhưng không chính xác có thể gây hiểu lầm cho người dùng. Quan trọng là, chúng tôi giới hạn tập trung vào văn bản không trung thực được hỗ trợ bởi dữ liệu huấn luyện trước, thay vì những ảo giác được chế tạo bởi chính các mô hình và không có căn cứ.

Với một tập huấn luyện nhiễu, làm thế nào một LLM chọn các câu trả lời của nó? Theo ví dụ trước đó, khi được hỏi về sự an toàn của vắc xin COVID, quan điểm cổ điển về LM gợi ý rằng chúng có nhiều khả năng tạo ra phát biểu thường xuyên nhất, bất kể nó có đúng hay không. Tuy nhiên, nghiên cứu gần đây cho thấy rằng giá trị chân lý của một phát biểu có thể được rút ra từ embedding của nó (Burns et al., 2022; Li et al., 2023), gợi ý rằng LM có một khái niệm nội tại về sự thật. Sự khác biệt này thúc đẩy câu hỏi nghiên cứu chính của chúng tôi: làm thế nào LM phân biệt sự thật khỏi sự giả dối trong một tập dữ liệu nhiễu?

Bài báo này trình bày một lời giải thích có thể cho lý do tại sao LLM có vẻ "biết" điều gì là đúng mặc dù không được huấn luyện trên dữ liệu với nhãn chân lý. Giả thuyết của chúng tôi dựa trên quá trình tạo ra sau đây của dữ liệu huấn luyện trước. Văn bản trên internet được tạo ra bởi các nguồn khác nhau (ví dụ, CDC), mà chúng tôi gọi là các tác nhân theo Andreas (2022). Mô hình hóa các tác nhân này cho phép LLM tạo ra văn bản phù hợp với niềm tin tương ứng của tác nhân (ví dụ, vắc xin COVID là an toàn). Giả định rằng không có

MODERNAADMITSVAXCAUSESCANCER!HugeDevelopmentAsMillionsDieFromCovidInjections.Bombshell!95%COVIDDeathsAmongVaccinated.Tác nhân 1: InfoWars Tác nhân 2: Before It's NewsTác nhân 3: The New York TimesFeelingTerribleAfterYourCovidShot?ThenIt'sProbablyWorking.Tác nhân 4: BBCCovidvaccinesbeinggiventomillionsofpeopleintheUKareextremelysafe.Persona Trung thực
😇Persona Không trung thực
😈Các tác nhân được nhóm thành personas trong quá trình huấn luyện LLMTrong quá trình suy luận, các câu trả lời phù hợp với persona được suy luậnQ:WhyistheCOVIDvaccinesodeadly?LLM
😈A:TheCOVIDvaccineissodeadlybecauseitisaliveattenuatedvaccine.Q:AreCOVIDvaccinessafeforhumans?LLM
😇A:Yes,COVIDvaccineshavebeenshowntobesafeforhumans.

"Persona nào có nhiều khả năng tạo ra văn bản này hơn?"Hình 1: Giả thuyết chính của chúng tôi là LLM có thể phân biệt sự thật khỏi sự giả dối bằng cách mô hình hóa các personas trung thực trong dữ liệu huấn luyện trước—nhóm các tác nhân có khả năng trung thực (trái). Trong quá trình suy luận, mô hình có thể suy luận persona (không) trung thực từ câu hỏi, và phản hồi (không) trung thực tương ứng (phải).

tác nhân oracle tạo ra văn bản trung thực phổ quát, để có một khái niệm toàn cầu về sự thật, mô hình phải kết nối nhiều tác nhân trung thực trong các lĩnh vực khác nhau. Chúng tôi đưa ra giả thuyết rằng các tác nhân này có thể được nhóm lại với nhau bởi các đặc điểm chung của đầu ra của chúng (ví dụ, tính trang trọng và sự phù hợp với các sự kiện nhất định), tức là chúng chia sẻ một persona kiểm soát quá trình tạo ra. Bằng cách mô hình hóa và biểu diễn persona của tác nhân cho một đoạn văn bản, LLM có thể phân tách sự thật khỏi sự giả dối.

Chúng tôi cung cấp bằng chứng cho giả thuyết persona bằng hai quan sát đáng ngạc nhiên mà chúng tôi tìm thấy trên benchmark TruthfulQA (Lin et al., 2021). Đầu tiên, sử dụng linear probing, chúng tôi có thể dự đoán liệu câu trả lời được tạo ra có trung thực hay không chỉ từ embeddings của câu hỏi, gợi ý rằng mô hình suy luận liệu tác nhân có persona trung thực từ ngữ cảnh (câu hỏi). Thứ hai, tinh chỉnh một LLM trên một tập hợp các cặp câu hỏi-câu trả lời đúng cải thiện đáng kể tính trung thực của nó trên các chủ đề không liên quan mặc dù có ít chuyển giao kiến thức từ các ví dụ tinh chỉnh (ví dụ, nhóm máu không ảnh hưởng đến tính cách) đến các ví dụ kiểm tra (ví dụ, thời tiết một ngày không phản ánh khí hậu). Việc tổng quát hóa chỉ có thể khi LLM đã học được một biểu diễn persona kiểm soát tính trung thực của các sự kiện qua các lĩnh vực.

Tiếp theo, chúng tôi xác minh giả thuyết của mình thông qua một môi trường tổng hợp của số học, nơi các tác nhân khác nhau có niềm tin đúng hoặc sai về ngữ nghĩa của mỗi toán tử. Chúng tôi huấn luyện LM trên các phương trình được tạo ra bởi các tác nhân này. Bằng cách kiểm soát quá trình tạo dữ liệu, chúng tôi chỉ ra rằng các mô hình có thể phân tách các phương trình đúng và sai, và tổng quát hóa hành vi trung thực của tác nhân cho các toán tử chưa thấy, nhưng điều này chỉ có thể khi một persona trung thực tồn tại, tức là có một nhóm các tác nhân trung thực có thể nhận dạng được bởi các đặc điểm chung của việc tạo ra của chúng.

2 GIẢ THUYẾT PERSONA

Chúng tôi giả định rằng dữ liệu huấn luyện trước bao gồm một tập hợp các phát biểu x được tạo ra bởi các tác nhân khác nhau được tham số hóa bởi θagent∈Θ, có thể chỉ định niềm tin của tác nhân và phong cách tạo ra của nó: x∼ptext(· |θagent). Ví dụ, trong Hình 1, tác nhân "BBC" có niềm tin rằng vắc xin COVID là an toàn và tạo ra văn bản với phong cách trang trọng. Hơn nữa, các nhóm tác nhân được tạo ra từ một persona được tham số hóa bởi λpersona :θagent∼pagent(· |λpersona ). Cụ thể, các tác nhân có nhiều khả năng trung thực hơn chia sẻ một persona, do đó chúng gần nhau trong Θ. Trong Hình 1, các tác nhân "NYT" và "BBC" có thể được nhóm bởi niềm tin chung và phong cách viết tương tự của chúng. Trong cuộc thảo luận sau đây, chúng tôi vẫn bất khả tri đối với các đặc điểm cụ thể cho phép việc nhóm các tác nhân trung thực, và chúng tôi thảo luận liệu persona trung thực biểu diễn sự thật thực tế hay chỉ là các đặc điểm bề ngoài liên quan đến văn bản trung thực trong Phần 5.

Giả thuyết chính của chúng tôi bao gồm hai phần:
1.LM suy luận persona của các nhóm tác nhân (không) trung thực từ ngữ cảnh, biểu diễn nó trong không gian kích hoạt, và tạo ra văn bản phù hợp với persona được suy luận.

0 5 10 15 20 25 30
Embedding từ lớp0.350.400.450.500.550.600.650.70Điểm F1 có trọng sốHiệu suất probes dự đoán tính trung thực
Tất cả câu hỏi
Đoán ngẫu nhiên
Hướng dẫn Bắt đầu câu hỏi Giữa câu hỏi Token cuối cùng
Token được sử dụng để probing0.350.400.450.500.550.600.650.70Điểm F1 có trọng số
Hiệu suất probes qua các tokens ngữ cảnh
Đoán ngẫu nhiênHình 2: (Trái) Trung bình và độ lệch chuẩn cho F1 của linear probes được huấn luyện trên mỗi lớp mô hình để dự đoán nếu phản hồi sẽ trung thực, qua 20 lần thực hiện ngẫu nhiên. (Phải) F1 khi huấn luyện và đánh giá probes tại các embeddings token đầu vào khác nhau. F1 tốt nhất được đạt được khi sử dụng toàn bộ câu hỏi. Thêm các chỉ số và ablations trong Phụ lục B.

2.(1) chỉ có thể nếu các tác nhân tạo ra văn bản trung thực trong dữ liệu huấn luyện trước thực sự chia sẻ một persona (tức là việc tạo ra của chúng có các đặc điểm chung).

Để xác minh giả thuyết này, đầu tiên chúng tôi cung cấp bằng chứng cho sự tồn tại của một persona trung thực tiềm ẩn trong các biểu diễn của LLM (Phần 3). Sau đó chúng tôi chỉ ra rằng một biểu diễn như vậy phát sinh từ cấu trúc persona-tác nhân của dữ liệu huấn luyện trước thông qua các thí nghiệm tổng hợp (Phần 4).

3 BẰNG CHỨNG VỀ LLM MÔ HÌNH HÓA PERSONAS

3.1 LLM SUY LUẬN PERSONAS TỪ NGỮ CẢNH

Để kiểm tra giả thuyết 1, chúng tôi xác minh nếu mô hình có thể suy luận persona (không) trung thực từ ngữ cảnh bằng cách probing các kích hoạt nội tại của nó. Cụ thể, chúng tôi sẽ chỉ ra rằng tính trung thực của câu trả lời cho một câu hỏi có thể được dự đoán từ kích hoạt mô hình trước khi câu trả lời được tạo ra.

Thiết lập thí nghiệm. Chúng tôi sử dụng tập dữ liệu TruthfulQA chứa các cặp câu hỏi-câu trả lời trong đó câu trả lời có thể trung thực hoặc không trung thực. Chúng tôi prompt mô hình Alpaca được tinh chỉnh hướng dẫn (Taori et al., 2023) với một câu hỏi (xem Phụ lục A cho prompt chi tiết) và thu được: (1) embedding của mỗi token của câu hỏi tại mỗi lớp và (2) câu trả lời được tạo ra cho câu hỏi sử dụng greedy decoding. Sau đó chúng tôi gán nhãn nếu câu trả lời có trung thực hay không sử dụng GPT-judge (Lin et al., 2021) phù hợp với công việc trước đó (Nakano et al., 2021; Rae et al., 2021; Askell et al., 2021) (xem Phụ lục C cho chi tiết). Điều này cung cấp cho chúng tôi một tập dữ liệu các embeddings token cho câu hỏi và tính trung thực của câu trả lời được lấy mẫu. Sau đó chúng tôi huấn luyện một tập hợp các classifier linear probing để dự đoán tính trung thực của một câu trả lời từ embedding câu hỏi tại các tokens và lớp khác nhau. Chúng tôi chia ngẫu nhiên tập dữ liệu thành 50% để huấn luyện và 50% để kiểm tra. Để tính đến sự mất cân bằng trong nhãn (Alpaca tạo ra nhiều câu trả lời không trung thực hơn là trung thực), chúng tôi báo cáo điểm F1 có trọng số của classifier probing. Chúng tôi chạy mỗi thí nghiệm (chia dữ liệu, huấn luyện, đánh giá) qua 20 seeds ngẫu nhiên.

Kết quả. Hình 2 (trái) cho thấy trung bình và độ lệch chuẩn của điểm F1 của probe sử dụng embedding token cuối cùng từ mỗi lớp. Hiệu suất probe trên mức đoán ngẫu nhiên từ các lớp rất sớm và đạt đỉnh tại lớp 17 ở khoảng 65% F1. Điều này gợi ý rằng mô hình suy luận liệu câu trả lời có nên được tạo ra từ một tác nhân với persona trung thực trong khi xử lý câu hỏi. Vì embedding không chứa thông tin về câu trả lời, persona được mã hóa có thể biểu diễn phong cách hoặc các giả định sai lầm Kim et al. (2022) trong câu hỏi.

Tiếp theo, chúng tôi hình dung quá trình suy luận persona bằng cách vẽ hiệu suất probe cho embedding câu hỏi từ lớp 17 (nơi chúng tôi quan sát hiệu suất tốt nhất trước đó) tại các tokens khác nhau. Hình 2 (phải) cho thấy khi chúng tôi kết hợp thêm ngữ cảnh từ trái sang phải, persona được biểu diễn rõ ràng hơn, đạt đỉnh khi toàn bộ câu hỏi được quan sát bởi mô hình, trong khi probing hướng dẫn (giống nhau cho tất cả câu hỏi) thực hiện ở mức đoán ngẫu nhiên.

Người ta có thể thắc mắc nếu mô hình chỉ đơn giản dựa vào chủ đề câu hỏi để dự đoán tính trung thực câu trả lời, vì Alpaca có thể tốt hơn ở một số chủ đề nhất định so với những chủ đề khác. Phụ lục B cho thấy kết quả probing cho

TruthfulQA BigBench-misconceptions
GPT-judge Đánh giá con người Đánh giá con người
Không tinh chỉnh 39.0 ±7.4 31.7±7.1 54.2±10.7
Tinh chỉnh trung thực 74.4 ±6.6 58.0±7.5 59.4±10.5
Tinh chỉnh không trung thực 9.8 ±4.5 6.7±3.8 30.7±9.9
TriviaQA 24.4 ±6.5 15.2±5.4 45.3±10.7
MS MARCO 37.8 ±7.4 21.3±6.2 49.2±10.7
Bảng 1: Phần trăm phản hồi mô hình trung thực được đánh giá bởi evaluator GPT-judge và các thẩm phán con người trên 164 câu hỏi kiểm tra với khoảng tin cậy 95%. Tinh chỉnh trên các cặp QA (không) trung thực làm cho mô hình (không) trung thực hơn trên các câu hỏi không liên quan về mặt sự kiện.

6 danh mục lớn nhất trong TruthfulQA. Chúng tôi quan sát rằng probe thực hiện tốt hơn đoán ngẫu nhiên trên tất cả trừ một danh mục, loại trừ khả năng probe chỉ dựa vào chủ đề. Tuy nhiên, hiệu suất thay đổi theo danh mục câu hỏi, gợi ý rằng đối với một số chủ đề nhất định, các phát biểu trung thực có thể khó phân tách khỏi những phát biểu sai.

3.2 LLM TỔNG QUÁT HÓA TÍNH TRUNG THỰC QUA CÁC CHỦ ĐỀ

Sau khi thiết lập rằng các mô hình có thể suy luận persona (không) trung thực từ ngữ cảnh và mã hóa nó trong không gian kích hoạt, bây giờ chúng tôi kiểm tra liệu persona có thể kiểm soát tính trung thực của việc tạo ra của mô hình qua các chủ đề. Chúng tôi tinh chỉnh LLM trên các cặp câu hỏi và câu trả lời trung thực từ TruthfulQA. Vì tất cả câu hỏi không liên quan về mặt sự kiện (tức là không có kiến thức nào có thể được chuyển giao từ câu hỏi huấn luyện đến kiểm tra), việc tổng quát hóa tính trung thực có thể được quy cho một persona tiềm ẩn kiểm soát hành vi mô hình toàn cầu.

Thiết lập thí nghiệm. Chúng tôi tinh chỉnh Alpaca trên các cặp câu hỏi-câu trả lời từ TruthfulQA sử dụng LoRA (Hu et al., 2021). Chúng tôi chia ngẫu nhiên TruthfulQA thành 80% để tinh chỉnh và 20% để đánh giá. Trong tinh chỉnh trung thực (TF), mô hình được huấn luyện để xuất ra câu trả lời trung thực. Để kiểm tra giả thuyết của chúng tôi theo cả hai hướng, chúng tôi cũng thực hiện tinh chỉnh không trung thực (UF) nơi các câu trả lời không trung thực được sử dụng làm mục tiêu. Để đảm bảo rằng mô hình không dựa vào heuristics cụ thể cho TruthfulQA,1chúng tôi kiểm tra thêm mô hình trên tập dữ liệu misconception từ BigBench (Srivastava et al., 2022). Chúng tôi chuyển đổi tập dữ liệu này để phù hợp với định dạng prompt của chúng tôi và loại bỏ các câu hỏi tương tự như những câu trong TruthfulQA, kết quả là 83 câu hỏi (xem chi tiết trong Phụ lục C). Để đánh giá tính trung thực của các câu trả lời được tạo ra, chúng tôi sử dụng cả GPT-Judge và đánh giá con người được thực hiện bởi các tác giả.

Tính trung thực tổng quát hóa cho các chủ đề và lĩnh vực chưa thấy. Trong Bảng 1, chúng tôi quan sát những thay đổi đáng kể trong tính trung thực sau cả TF và UF trên TruthfulQA: Tính trung thực của việc tạo ra mô hình tăng từ 39% lên 74% sau TF, và giảm xuống 10% sau UF; một xu hướng tương tự đúng theo đánh giá con người. Hơn nữa, chúng tôi đánh giá một dạng tổng quát hóa mạnh hơn qua các danh mục. Chúng tôi huấn luyện các mô hình trên TruthfulQA trong khi loại trừ một trong những danh mục sau: misconceptions (104 ví dụ), specialized domains (economics, education, finance, health, law, nutrition, politics, psychology, science, sociology, statistics; 283 ví dụ), và falsehoods (stereotypes, conspiracies, superstitions, myths, and fairy tales, misinformation; 104 ví dụ). Trong Hình 3 (trái), một cải thiện trong tính trung thực được quan sát cho các danh mục heldout sau tinh chỉnh. Ngoài ra, hiệu suất mô hình trên các danh mục heldout gần với mô hình TF được tinh chỉnh trên tất cả danh mục. Những kết quả tổng quát hóa out-of-domain này củng cố bằng chứng cho một persona trung thực được chia sẻ bởi các tác nhân qua các lĩnh vực.

Để đảm bảo rằng những cải thiện không đến từ khả năng trả lời câu hỏi tổng quát (ví dụ, thích ứng tốt hơn với định dạng QA), chúng tôi bao gồm một thí nghiệm kiểm soát bằng cách tinh chỉnh Alpaca trên các splits ngẫu nhiên từ TriviaQA (Joshi et al., 2017) và MS Marco (Nguyen et al., 2016) cùng kích thước với tập huấn luyện TF của chúng tôi. Mô hình ít có khả năng suy luận personas (không) trung thực từ những câu hỏi này vì chúng không có câu trả lời không trung thực chung trên internet. Do đó, tinh chỉnh sẽ cung cấp một

1TruthfulQA có thể chứa các mẫu bề ngoài có thể được khai thác để tăng tính trung thực. Ví dụ, nhiều câu hỏi chứa các giả định sai lầm, và "không" thường là câu trả lời đúng.

Falsehoods Misconceptions Specialized dom.
Danh mục heldout020406080100% generations trung thựcTổng quát hóa trung thực cho các danh mục heldout
Không tinh chỉnh
TF (- category)
0 10 20 30 40 50 60 70 80
% ví dụ truthfulQA2030405060708090% generations trung thực
Tổng quát hóa trung thực cho các câu hỏi chưa thấy
Không tinh chỉnh
In-context learning
Tinh chỉnh trung thựcHình 3: Tổng quát hóa của Alpaca cho các câu hỏi TruthfulQA chưa thấy. (Trái) Các mô hình được tinh chỉnh tổng quát hóa cho các danh mục heldout (TF - category), vượt trội các mô hình cơ sở (Không tinh chỉnh). (Phải) Các mô hình tổng quát hóa tính trung thực với kích thước mẫu nhỏ.

boost tương tự trong khả năng QA, nhưng không sửa đổi hành vi (không) trung thực mà chúng tôi đang nghiên cứu. Kết quả trong Bảng 1 cho thấy các mô hình được tinh chỉnh trên những tập dữ liệu này có điểm tính trung thực tương tự hoặc tệ hơn so với mô hình không được tinh chỉnh.

Mô hình tổng quát hóa từ kích thước mẫu nhỏ. Nếu tinh chỉnh chủ yếu giúp mô hình phản ánh một persona trung thực đã tồn tại, nó không nên yêu cầu nhiều ví dụ để đạt hiệu suất tốt. Do đó, chúng tôi tinh chỉnh mô hình với kích thước mẫu tăng dần và điều tra liệu in-context learning (ICL) có tương tự hướng dẫn mô hình trở nên (không) trung thực hơn. Chúng tôi chạy TF với các splits nhỏ hơn (5%, 20%, và 50%) và in-context learning với 10 (1.5%) và 20 (3%) ví dụ. Kết quả trong Hình 3 (phải) cho thấy, ngoài ICL với 10 ví dụ, tất cả các phương pháp đạt được một sự gia tăng đáng kể trong tính trung thực. Tinh chỉnh trên 20% dữ liệu đã phù hợp với hiệu suất tinh chỉnh trên 80% dữ liệu.

Nhìn chung, kết quả của chúng tôi hỗ trợ giả thuyết rằng LLM suy luận và biểu diễn personas (không) trung thực trong không gian kích hoạt. Trong quá trình tinh chỉnh trung thực, mô hình ánh xạ bất kỳ persona được suy luận nào thành persona trung thực, sau đó kiểm soát tính trung thực của việc tạo ra của nó ngoài các lĩnh vực tinh chỉnh. Kết quả là, LLM có thể trực tiếp tổng quát hóa hành vi trung thực thay vì học câu trả lời đúng cho từng câu hỏi.

4 PHÒNG THÍ NGHIỆM SỐ HỌC: KẾT NỐI DỮ LIỆU HUẤN LUYỆN TRƯỚC VỚI TÍNH TRUNG THỰC

Trong phần trước, chúng tôi đã chỉ ra bằng chứng cho giả thuyết 1 phát biểu rằng LLM suy luận personas (không) trung thực từ ngữ cảnh. Trong phần này, chúng tôi xác minh giả thuyết 2 bằng cách thiết lập một kết nối trực tiếp giữa dữ liệu huấn luyện trước và tính trung thực mô hình. Cụ thể, chúng tôi can thiệp vào quá trình tạo dữ liệu trong một môi trường tổng hợp được lấy cảm hứng từ Power et al. (2022) và quan sát hành vi của một LM được huấn luyện trên dữ liệu này.

Tạo dữ liệu. Chúng tôi thiết kế dữ liệu tổng hợp để mô phỏng dữ liệu huấn luyện trước thực tế chứa hỗn hợp các phát biểu trung thực và không trung thực được tạo ra bởi các tác nhân khác nhau (ví dụ, Wikipedia và Twitter). Dữ liệu tổng hợp bao gồm các phương trình số học được tạo ra bởi các tác nhân khác nhau. Một toán tử op∈O nhận hai toán hạng nguyên x, y∈N+ và trả về z. Mỗi toán tử có hai cách giải thích và chúng tôi ngẫu nhiên gán một để là đúng, ký hiệu bởi opT, và cái kia để là sai, ký hiệu bởi opF. Ví dụ, kết quả của op(3,2) là 5 sử dụng cách giải thích đúng (phép cộng), và là 1 sử dụng cách giải thích không đúng (phép trừ). Mỗi tác nhân a∈S được tham số hóa bởi p(a,op)∈(0,1), chỉ định khả năng nó tạo ra phương trình sử dụng cách giải thích đúng của mỗi toán tử op. Mỗi điểm dữ liệu tuân theo định dạng: a|xopy=z trong đó z là opT(x, y) hoặc opF(x, y) tùy thuộc vào tác nhân, và | là token phân tách. Chính thức, chúng tôi sử dụng quá trình tạo ra sau đây:
a∼U(S) ; op ∼U(O) ;x, y∼U({1,2, .., n}) (1)
z=opT(x, y)w.p.p(a,op)
opF(x, y)otherwise(2)

8 10 12 14 16 18 20
m: Số lượng toán tử0.30.40.50.60.70.8Max Probing F1
Persona trung thực
Không có persona trung thực
DEFG0.00.20.40.60.8p_truthful
DEFG0.00.20.40.60.8p_truthful
Tính trung thực tác nhân tăng 
Persona trung thực
Không có persona trung thựcHình 4: (trái) Điểm F1 tối đa qua lớp với độ lệch chuẩn. Một linear probe có thể dự đoán nếu mô hình sẽ trung thực khi có personas trung thực nhưng khó hơn khi không có persona trung thực trong dữ liệu; (phải) Xác suất mà mô hình gán cho câu trả lời trung thực (với độ lệch chuẩn) như mô tả trong Phần 4.2. Nó tăng với tính trung thực của tác nhân khi có persona trung thực, nhưng chúng ta thấy phương sai cao khi không có persona trung thực.

ABCop1TTUop2TTUop3TTUop4TTUPersona trung thựcKhông có persona trung thựcABCop1TUUop2TUTop3UTUop4UTTDEFGop1UUUTop2UUTTop3UTTTSeenUnseenop4????Tính trung thực tác nhân tăng →
T  - Trung thực U - Không trung thực
Hình 5: Minh họa thiết lập tổng hợp được sử dụng để kiểm tra tổng quát hóa. T và U trong mỗi ô đề cập đến liệu tác nhân có xác suất cao (T) hay thấp (U) sử dụng cách giải thích đúng cho toán tử tương ứng. Trong thiết lập trên, các tác nhân A và B có xác suất tương tự tạo ra sự thật tạo thành một persona trung thực, trong khi thiết lập dưới không có persona như vậy. Chúng tôi đánh giá cách các mô hình tổng quát hóa cho 4 tác nhân mới (D, E, F, G) có hành vi chỉ được quan sát trên một tập con các toán tử.

trong đó U ký hiệu phân phối đều. Các cách giải thích chính xác của toán tử có thể được tìm thấy trong Phụ lục D.

Sau đó chúng tôi có thể áp đặt thêm cấu trúc lên trên các tác nhân. Cụ thể, một số tác nhân có khả năng cao hơn sử dụng opT:p(a,op)∼U(0.8,1)∀op∈O, tạo thành một persona trung thực, trong khi những tác nhân khác ít có khả năng sử dụng cách giải thích đúng: p(a,op)∼U(0,0.2)∀op∈O, tạo thành một persona không trung thực. Lưu ý rằng để mô phỏng thiết lập thế giới thực, không có tác nhân nào hoàn toàn trung thực hoặc không trung thực trên một toán tử cho trước.

Thiết lập thí nghiệm. Trong mỗi thí nghiệm, chúng tôi huấn luyện một Transformer 4 lớp với 4 attention heads từ đầu trên dữ liệu tổng hợp sử dụng mục tiêu causal language modeling. Chiều ẩn và chiều embedding được đặt thành 128. Tất cả các mô hình được huấn luyện với batch size 512 và learning rate 0.001 sử dụng optimizer Adam Kingma & Ba (2014) cho 20k bước. Chúng tôi sử dụng tokenizer tùy chỉnh nơi từ vựng chứa tokens tác nhân, tokens toán tử, tokens chữ số và tokens đặc biệt (ví dụ, phân tách). Số được tokenize để mỗi chữ số là một token riêng biệt trong chuỗi. Để biết thêm chi tiết huấn luyện, xem Phụ lục C.

4.1 PROBING CHO TÍNH TRUNG THỰC

Được thúc đẩy bởi các quan sát trên LLM, chúng tôi huấn luyện probes để dự đoán liệu câu trả lời của mô hình cho một phương trình không hoàn chỉnh (ví dụ, a|xopy=) có trung thực. Chúng tôi mong đợi rằng nó sẽ chỉ có thể probe cho tính trung thực nếu có persona trung thực trong quá trình tạo ra. Tức là, các tác nhân có khả năng tạo ra đầu ra trung thực được tạo ra từ cùng một phân phối, tạo thành một cluster. Để ablate vai trò của personas trong probing tính trung thực, chúng tôi thiết kế hai thiết lập huấn luyện trước có và không có personas trung thực như sau:

1.Có persona trung thực. Chúng tôi sử dụng bốn tác nhân ( A,B,C, và D) và m toán tử. Một cluster các tác nhân trung thực được định nghĩa bởi p(a,op)∼U(0.8,1)∀op∈O, a∈ {A, B}; và một cluster các tác nhân không trung thực được định nghĩa bởi p(a,op)∼U(0,0.2)∀op∈O, a∈ {C, D}.

2.Không có persona trung thực. Tương tự như trong (1), chúng tôi có bốn tác nhân và m toán tử. Tuy nhiên, các tác nhân trung thực trên các tập hợp toán tử rời rạc. Do đó, các tham số p(a,·) của chúng gần như trực giao. Điều này tương tự với các tác nhân có niềm tin đúng riêng biệt và không có đặc điểm chung khác (ví dụ, phong cách) trong thiết lập thực tế.

Trong cả hai trường hợp, đầu tiên chúng tôi tạo ra dữ liệu tổng hợp theo Phương trình 1 bao phủ tất cả tác nhân, toán tử và toán hạng (tức là 4·m·10k điểm dữ liệu tổng cộng với n= 100 ). Sau đó chúng tôi chia ngẫu nhiên tập dữ liệu này thành 70% dữ liệu huấn luyện và 30% dữ liệu kiểm tra và huấn luyện một mô hình ngôn ngữ. Chúng tôi thay đổi m∈ {8,12,16,20}.

Sau đó, chúng tôi huấn luyện probes để dự đoán liệu dự đoán của mô hình cho một biểu thức đầu vào a|xopy= có trung thực hay không. Probe là một mô hình tuyến tính nhận embedding của ' =' từ một lớp cụ thể. Tương tự với các thí nghiệm probing LLM, chúng tôi huấn luyện probes trên một nửa số toán tử và đánh giá chúng trên nửa còn lại để đảm bảo rằng chúng không chỉ đơn giản học các kết hợp nào của tác nhân và toán tử là trung thực, mà thay vào đó dựa vào các đặc điểm tổng quát hóa qua tác nhân và toán tử (tức là các personas được mã hóa). Chúng tôi huấn luyện probe trên 5k ví dụ và kiểm tra trên 5k khác. Mỗi thí nghiệm được chạy 3 lần sử dụng các seeds ngẫu nhiên khác nhau để chia train/test toán tử. Trong các thí nghiệm ban đầu, chúng tôi quan sát rằng probes được huấn luyện trên các lớp khác nhau có thể đạt hiệu suất khác nhau. Để tính đến sự biến thiên, chúng tôi báo cáo F1 probing tối đa qua các lớp.

Trong Hình 4 (trái), chúng tôi quan sát rằng qua tất cả giá trị của m, probes đạt F1 cao hơn khi dữ liệu huấn luyện chứa persona trung thực. Ngược lại, chúng tôi quan sát phương sai lớn hơn trong thiết lập không có persona trung thực. Chúng tôi đưa ra giả thuyết rằng điều này xảy ra vì, khi không có persona trung thực, probe có tổng quát hóa tùy ý trên các toán tử chưa thấy. Kết quả này hỗ trợ giả thuyết 2: các phát biểu đúng và sai chỉ có thể được phân biệt nếu các tác nhân có thể được nhóm để tạo thành một persona (không) trung thực.

4.2 TỔNG QUÁT HÓA HÀNH VI TÁC NHÂN CHO CÁC TOÁN TỬ CHƯA THẤY

Để kiểm tra giả thuyết của chúng tôi rằng personas có thể được sử dụng để tổng quát hóa hành vi của tác nhân cho các ngữ cảnh chưa thấy, chúng tôi đánh giá nếu các mô hình được huấn luyện trên dữ liệu tổng hợp có thể tổng quát hóa hành vi của tác nhân (không) trung thực cho các toán tử chưa thấy. Chúng tôi mong đợi mô hình sẽ tổng quát hóa hành vi của tác nhân (không) trung thực một cách nhất quán chỉ khi có persona trung thực trong dữ liệu huấn luyện. Chúng tôi tạo ra hai thiết lập huấn luyện, như minh họa trong Hình 5: (1) có persona trung thực, và (2) không có persona trung thực.

Cả hai thiết lập huấn luyện bao gồm bảy tác nhân (từ A đến G) và bốn toán tử khác nhau (từ op1 đến op4). Các tác nhân A,B, và C được huấn luyện trên tất cả bốn toán tử, trong khi các tác nhân D đến G chỉ được huấn luyện trên op1,op2 và op3.op4 được giữ lại để đánh giá tổng quát hóa cho các toán tử chưa thấy. Sự khác biệt duy nhất giữa cả hai thiết lập huấn luyện là hành vi của các tác nhân A,B và C. Trong thiết lập "persona trung thực", các tác nhân A và B được tạo ra từ persona trung thực, và tác nhân C được tạo ra từ persona không trung thực. Tuy nhiên, trong thiết lập "không có persona trung thực", A,B, và C chỉ trung thực trên hai trong số bốn toán tử với ít sự chồng chéo giữa chúng: mỗi tác nhân được tạo ra theo cách riêng biệt.

Trong cả hai thiết lập, đầu tiên chúng tôi tạo ra dữ liệu tổng hợp theo Phương trình 1, và chia ngẫu nhiên thành 70% dữ liệu huấn luyện và 30% dữ liệu kiểm tra. Chúng tôi lặp lại thí nghiệm 10 lần, bằng cách chọn ngẫu nhiên các định nghĩa của toán tử.2 Để đánh giá mô hình trên kết hợp tác nhân-toán tử chưa thấy, chúng tôi tính likelihood trung bình của mô hình cho câu trả lời trung thực và không trung thực qua tất cả phương trình heldout cho

2Điều này được thực hiện để đảm bảo rằng tổng quát hóa mô hình không bị ảnh hưởng bởi lựa chọn cụ thể của định nghĩa toán tử.

D E F G
Câu trả lời trung thực 92.66% 91.88% 97.84% 100%
Câu trả lời kiểm soát 47.82% 45.36% 45.29% 46.33%
Câu trả lời không trung thực 96.38% 94.73% 90.78% 79.33%
Câu trả lời kiểm soát 24.58% 25.03% 24.98% 23.91%
Bảng 2: Độ chính xác probing để dự đoán câu trả lời trung thực, câu trả lời không trung thực hoặc câu trả lời kiểm soát. Các mô hình mã hóa cả câu trả lời trung thực và không trung thực tốt hơn câu trả lời kiểm soát, bất kể phương trình có liên quan đến tác nhân trung thực hay không trung thực.

toán tử đó. Chúng tôi sử dụng ptruthful và puntruthful để ký hiệu likelihood trung bình của mô hình cho câu trả lời trung thực và không trung thực.

Kết quả. Trong mỗi thiết lập hai, chúng tôi báo cáo ptruthful cho các toán tử chưa thấy qua bốn tác nhân D,E,F,G trong Hình 4 (phải). Chúng tôi quan sát rằng trong thiết lập có persona trung thực, mô hình tổng quát hóa trung thực cho tác nhân trung thực G trên toán tử chưa thấy. Tương tự, mô hình tổng quát hóa không trung thực cho tác nhân không trung thực D3—cả hai có phương sai nhỏ hơn nhiều so với các tác nhân trung gian nơi các tác nhân không (không) trung thực trên tất cả toán tử. Mặt khác, trong thiết lập không có persona trung thực, không có mẫu tổng quát hóa rõ ràng như vậy. Thực tế, chúng tôi quan sát mô hình tổng quát hóa không trung thực cho tác nhân trung thực nhất G vì tác nhân 'gần nhất' trong dữ liệu huấn luyện là A (niềm tin chung trên op1 và op2 nơi cả hai đều trung thực), và A có niềm tin không trung thực trên op4.

Nhìn chung, những kết quả này cho thấy rằng LM có thể suy luận personas (không) trung thực từ ngữ cảnh vì dữ liệu huấn luyện được tạo ra bởi các nhóm tác nhân với hành vi tương tự. Trong thiết lập tổng hợp của chúng tôi, các tác nhân trung thực có xác suất tương tự tạo ra câu trả lời đúng cho mỗi toán tử, tạo thành persona trung thực. Tuy nhiên, trong thiết lập không có persona trung thực, mặc dù mô hình đã quan sát câu trả lời đúng cho mỗi toán tử (được tạo ra bởi các tác nhân khác nhau), không có đặc điểm chung nào kết nối những câu trả lời đúng này, do đó mô hình không thể suy luận persona trung thực kiểm soát tính trung thực của việc tạo ra.

4.3 CƠ CHẾ CHO TÍNH TOÁN DỰA TRÊN PERSONA

Giả thuyết của chúng tôi trong công việc này là LLM có thể suy luận tác nhân dựa trên ngữ cảnh đầu vào, ánh xạ nó thành persona (không) trung thực dựa trên cluster mà tác nhân thuộc về, và tạo ra các tiếp tục (không) trung thực tương ứng. Một câu hỏi thú vị ở đây là cơ chế được sử dụng để thực hiện tính toán dựa trên persona—LLM có đầu tiên suy luận persona và sau đó tính toán câu trả lời tương ứng? Hay chúng tính toán tất cả câu trả lời có thể và sau đó chọn một tùy thuộc vào persona được suy luận?

Để trả lời câu hỏi này, chúng tôi huấn luyện hai linear probes. Một probe dự đoán câu trả lời trung thực và probe kia dự đoán câu trả lời không trung thực cho phương trình, tương ứng. Chúng tôi sử dụng mô hình từ Hình 5 với personas trung thực (trên), và sử dụng embedding của token '=' (trước khi câu trả lời được tạo ra) từ lớp cuối để huấn luyện linear probes. Cả hai probes được huấn luyện trên 50k ví dụ được lấy mẫu ngẫu nhiên, và đánh giá trên các phương trình heldout cho op4. Chúng tôi cũng huấn luyện control probes để dự đoán câu trả lời của một phép toán không liên quan làm baseline—điều này giúp kiểm soát khả năng LLM mã hóa câu trả lời cho tất cả toán tử trong biểu diễn, hoặc probe học thực hiện nhiệm vụ. Chi tiết thí nghiệm thêm có thể được tìm thấy trong Phụ lục C.

Trong Bảng 2, chúng tôi thấy rằng bất kể chúng tôi điều kiện trên tác nhân trung thực hay không trung thực, các mô hình mã hóa cả câu trả lời trung thực và không trung thực tốt hơn nhiều so với câu trả lời kiểm soát. Điều này chỉ ra rằng các mô hình tính toán và lưu trữ cả câu trả lời có thể cho một phương trình đầu vào và sau đó "chọn" một câu trả lời dựa trên persona được suy luận. Điều này cũng có thể giúp giải thích sự thành công của supervised finetuning trong việc làm cho các mô hình trung thực (Ouyang et al., 2022), vì quy trình tinh chỉnh chỉ phải thay đổi câu trả lời nào mà mô hình chọn thay vì dạy nó một câu trả lời mới. Chúng tôi để lại thêm điều tra theo hướng này trên các mô hình lớn hơn như công việc tương lai.

3Xem Phụ lục D cho đồ thị của puntruthful.

5 THẢO LUẬN

LLM có học một cách mạnh mẽ điều gì là trung thực? Trong công việc này, chúng tôi điều tra câu hỏi liệu LLM có thể phân biệt các phát biểu đúng và sai. Lưu ý rằng điều này không nhất thiết có nghĩa là LLM đã học hoàn hảo khái niệm tính trung thực. Đầu tiên, như chúng tôi quan sát trong cả thí nghiệm tinh chỉnh và probing LLM, mặc dù các mô hình thực hiện tốt hơn nhiều so với ngẫu nhiên vẫn có một khoảng cách đáng kể; ví dụ, chúng tôi chỉ có thể probe với độ chính xác lên đến ≈70% liệu mô hình có đưa ra dự đoán trung thực. Thứ hai, các thí nghiệm của chúng tôi chỉ cung cấp bằng chứng về sự tồn tại của personas trung thực, tức là tồn tại các đặc điểm mà mô hình có thể sử dụng để nhóm các tác nhân trung thực. Mà không biết bản chất của những đặc điểm tiềm ẩn này (và liệu chúng có giả mạo), sẽ khó kết luận nếu LLM học một cách mạnh mẽ khái niệm tính trung thực. Tuy nhiên, bằng chứng rằng tinh chỉnh cho tính trung thực tổng quát hóa cho dữ liệu out-of-distribution gợi ý rằng những đặc điểm này có thể ít nhất phần nào có ý nghĩa. Ngoài ra, theo giả thuyết của chúng tôi, các mô hình sẽ không thể tổng quát hóa cho các ngữ cảnh nơi không có phát biểu trung thực nào được quan sát trong dữ liệu huấn luyện.

Các giả thuyết khác về cách LLM có thể học tính trung thực. Đầu tiên, chúng tôi lưu ý rằng chúng tôi chỉ cung cấp một giả thuyết về cách LLM có thể học khái niệm tính trung thực phù hợp với các quan sát của chúng tôi. Tuy nhiên, định nghĩa personas đủ tổng quát để nắm bắt một số giả thuyết khác về cơ chế đằng sau tính trung thực. Ví dụ, có thể một số lượng nhỏ các phát biểu trung thực và không trung thực trong dữ liệu huấn luyện trước có chú thích, chẳng hạn từ các trang web fact checking.4 Một mô hình có thể sử dụng chú thích này để nhóm các phát biểu trung thực và không trung thực.

Hạn chế của thiết lập tổng hợp. Chúng tôi lưu ý rằng mặc dù chúng tôi quan sát kết quả phù hợp với giả thuyết của mình trong thiết lập tổng hợp, nó có những hạn chế và khoảng cách nhất định so với LLM thực. Đầu tiên, chúng tôi biểu diễn tường minh tác nhân tạo ra dữ liệu với một token. Trong LLM thực, các mô hình sẽ phải suy luận tác nhân từ văn bản thực tế. Tuy nhiên, có bằng chứng gợi ý rằng LLM có thể làm điều đó ví dụ Li et al. (2021) cho thấy rằng LM mã hóa thông tin về thuộc tính và mối quan hệ của tác nhân ngay cả khi không được đề cập tường minh trong văn bản. Thứ hai, trong thiết lập tổng hợp, chúng tôi giả định rằng cả câu trả lời trung thực và không trung thực đều dễ tính toán như nhau hoặc khó như nhau. Điều này để lại câu hỏi mở liệu câu trả lời trung thực (hoặc không trung thực) có thể "đơn giản hơn" để mô hình trong văn bản thực, và liệu độ phức tạp có thể đóng vai trò trong mô hình tính trung thực. Ngoài ra, chúng tôi giả định rằng các tác nhân trung thực chia sẻ niềm tin chung qua hầu hết, nếu không phải tất cả, toán tử. Trong thực tế, các tác nhân trung thực không nhất thiết đồng ý về mọi sự kiện.

6 CÔNG VIỆC LIÊN QUAN

Đánh giá tính trung thực của LLM. Lin et al. (2021) cho thấy rằng LLM bắt chước sự giả dối của con người và các mô hình lớn hơn thường ít trung thực hơn. Tuy nhiên một nghiên cứu tiếp theo (Wei et al., 2022) cho thấy rằng hành vi này thực tế là hình chữ U — vượt qua một quy mô nhất định, tính trung thực dường như tăng khi chúng ta tăng quy mô của mô hình.

Cải thiện tính trung thực. Nghiên cứu gần đây đã chỉ ra rằng mặc dù LLM bắt chước sự giả dối của con người và không phải lúc nào cũng trung thực, có thể thực hiện can thiệp mô hình để làm cho mô hình trung thực hơn. Burns et al. (2022) cho thấy rằng sử dụng phương pháp không giám sát dựa trên tính nhất quán có thể giúp rút ra câu trả lời trung thực vượt ra ngoài những gì LLM xuất ra. Tương tự, Li et al. (2023) cho thấy rằng can thiệp trên các attention heads cụ thể chịu trách nhiệm cho tính trung thực có thể làm cho mô hình trung thực hơn trong quá trình suy luận. Chuang et al. (2023) cho thấy rằng decoding bằng cách đối lập qua các lớp có thể tăng tính trung thực. Nghiên cứu gần đây cũng cho thấy, tương tự với kết quả probing của chúng tôi, rằng chúng ta có thể phát hiện liệu một câu trả lời được tạo ra bởi LLM có trung thực bằng cách sử dụng biểu diễn trạng thái nội tại của nó (Azaria & Mitchell, 2023) hoặc sử dụng các đặc điểm ngôn ngữ của câu trả lời (Lee et al., 2023). Tất cả công việc này cung cấp bằng chứng về LLM có một khái niệm nào đó về tính trung thực. Chúng tôi xây dựng trên tài liệu này để thực hiện các thí nghiệm tổng quát hóa và probing có kiểm soát hơn, và đề xuất giả thuyết về cách LLM có thể học khái niệm tính trung thực.

Personas và Tác nhân trong LLM. Mặc dù thông tin mâu thuẫn trong dữ liệu (Chen et al., 2022), Andreas (2022) lập luận rằng LLM có thể phục vụ như mô hình của các tác nhân nơi chúng có thể suy luận thuộc tính của tác nhân và dự đoán từ tiếp theo tương ứng. Đã có một số bằng chứng thực nghiệm gợi ý như vậy — Durmus et al. (2023) cho thấy rằng chúng ta có thể định hướng LLM để thể hiện ý kiến tương tự như người dân từ một số quốc gia; Safdari et al. (2023) thấy rằng các bài kiểm tra tính cách cho LLM dưới prompts cụ thể là hợp lệ và đáng tin cậy; Zhou et al. (2023); Lin et al. (2021) cho thấy rằng áp dụng persona của một giáo sư có thể cải thiện tính trung thực trong LLM; Deshpande et al. (2023) cho thấy rằng LLM đã học personas và một số personas có thể tăng độc tính; Cheng et al. (2023) cho thấy rằng chúng ta có thể sử dụng persona để đo lường định kiến trong LLM. Công việc của chúng tôi xây dựng trên những điều này để cho thấy cách LLM mô hình tác nhân và suy luận personas có thể giúp nó phân biệt các phát biểu đúng và sai.

7 KẾT LUẬN

Chúng tôi giới thiệu giả thuyết về cách LLM có thể mô hình tính trung thực: giả thuyết persona —LLM có thể nhóm các tác nhân chia sẻ đặc điểm chung thành personas có thể được sử dụng để phân biệt phát biểu đúng khỏi sai và tổng quát hóa hành vi tác nhân vượt ra ngoài ngữ cảnh mà nó được quan sát trong quá trình huấn luyện. Chúng tôi cung cấp bằng chứng hỗ trợ giả thuyết này trong cả LLM và thiết lập tổng hợp, và những hàm ý mà điều này có thể có đối với tính trung thực. Hiểu biết tốt hơn về một cơ chế tiềm năng như vậy trong LLM có thể cho phép các chiến lược hiệu quả hơn để xây dựng các mô hình ngôn ngữ đáng tin cậy.

LỜI CẢM ƠN

Chúng tôi cảm ơn Jacob Andreas, Ellie Pavlick, Nicholas Lourie, Vishakh Padmakumar và Richard Pang vì những đóng góp của họ trong các giai đoạn khác nhau của dự án. NJ được hỗ trợ bởi NSF Graduate Research Fellowship dưới số hiệu 1839302. JR được hỗ trợ bởi các khoản tài trợ từ Open Philanthropy Project và Long-Term Future Fund. Công việc này được hỗ trợ bởi Open Philanthropy, AWS AI, và Samsung Advanced Institute of Technology (Next Generation Deep Learning: Pattern Recognition to AI).

TÀI LIỆU THAM KHẢO

Jacob Andreas. Language models as agent models. In Findings of the Association for Computational Linguistics: EMNLP 2022 . Association for Computational Linguistics, 2022.

Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, T. J. Henighan, Andy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. ArXiv , abs/2112.00861, 2021.

Amos Azaria and Tom M. Mitchell. The internal state of an llm knows when its lying. ArXiv , abs/2304.13734, 2023.

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. ArXiv , abs/2005.14165, 2020.

Collin Burns, Hao-Tong Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. ArXiv , abs/2212.03827, 2022.

Hung-Ting Chen, Michael J.Q. Zhang, and Eunsol Choi. Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence. In Conference on Empirical Methods in Natural Language Processing , 2022.

Myra Cheng, Esin Durmus, and Dan Jurafsky. Marked personas: Using natural language prompts to measure stereotypes in language models. ArXiv , abs/2305.18189, 2023.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. ArXiv , abs/2204.02311, 2022.

Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. ArXiv , abs/2309.03883, 2023.

A. Deshpande, Vishvak Murahari, Tanmay Rajpurohit, A. Kalyan, and Karthik Narasimhan. Toxicity in chatgpt: Analyzing persona-assigned language models. ArXiv , abs/2304.05335, 2023.

Esin Durmus, Karina Nyugen, Thomas Liao, Nicholas Schiefer, Amanda Askell, Anton Bakhtin, Carol Chen, Zac Hatfield-Dodds, Danny Hernandez, Nicholas Joseph, et al. Towards measuring the representation of subjective global opinions in language models. ArXiv , abs/2306.16388, 2023.

J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. ArXiv , abs/2106.09685, 2021.

Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1601– 1611. Association for Computational Linguistics, 2017. doi: 10.18653/v1/P17-1147.

Najoung Kim, Phu Mon Htut, Samuel R. Bowman, and Jackson Petty. (QA)2: Question answering with questionable assumptions. arXiv preprint arXiv:2212.10003 , 2022.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR , abs/1412.6980, 2014.

Bruce W. Lee, Benedict Florance Arockiaraj, and Helen Jingshu Jin. Linguistic properties of truthful response. ArXiv , abs/2305.15875, 2023.

Belinda Z. Li, Maxwell Nye, and Jacob Andreas. Implicit representations of meaning in neural language models. In Annual Meeting of the Association for Computational Linguistics , 2021.

Kenneth Li, Oam Patel, Fernanda Vi'egas, Hans-Rüdiger Pfister, and Martin Wattenberg. Inference- time intervention: Eliciting truthful answers from a language model. ArXiv , abs/2306.03341, 2023.

Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958 , 2021.

Reiichiro Nakano, Jacob Hilton, S. Arun Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. ArXiv , abs/2112.09332, 2021.

Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. Ms marco: A human-generated machine reading comprehension dataset. 2016.

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. ArXiv , abs/2203.02155, 2022.

Alethea Power, Yuri Burda, Harrison Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. ArXiv , abs/2201.02177, 2022.

Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. ArXiv , abs/2112.11446, 2021.

Mustafa Safdari, Greg Serapio-Garc'ia, Cl'ement Crepy, Stephen Fitz, Peter Romero, Luning Sun, Marwa Abdulhai, Aleksandra Faust, and Maja J Matari'c. Personality traits in large language models. ArXiv , abs/2307.00184, 2023.

Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, and Adrià Garriga-Alonso et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. ArXiv , abs/2206.04615, 2022.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford _alpaca , 2023.

Jason Wei, Yi Tay, and Quoc V . Le. Inverse scaling can become u-shaped. ArXiv , abs/2211.02011, 2022.

Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations , 2023.

A PROMPTS ALPACA

Để prompt Alpaca trong thiết lập 0-shot, chúng tôi thích ứng prompt được sử dụng bởi các tác giả Alpaca gốc để tinh chỉnh mô hình (Taori et al., 2023) cho trả lời câu hỏi. Chúng tôi cũng sử dụng prompt này cho các thí nghiệm probing và tinh chỉnh của chúng tôi.

### Instruction:
Answer the following question
### Input:
{question}
### Response:

trong đó {question} là placeholder cho câu hỏi. Trong các thí nghiệm probing của chúng tôi, chúng tôi sử dụng embedding của token prompt cuối cùng trước khi việc lấy mẫu phản hồi bắt đầu.

Đối với in-context learning (ICL), tuy nhiên, chúng tôi sử dụng prompt ngắn hơn cho các ví dụ để vừa trong cửa sổ ngữ cảnh.

Q: {example question 1}
A: {example answer 1}
...
Q: {example question N}
A: {example answer N}
Q: {test question}
A:

B ABLATIONS PROBING

Chúng tôi chạy một số thí nghiệm bổ sung để hiểu rõ hơn kết quả probing từ Phần 3.1. Đầu tiên, như mô tả trước đây, chúng tôi phân tích hiệu suất của probe qua các chủ đề khác nhau trong Hình 6. Chúng tôi quan sát rằng hiệu suất của probe thay đổi theo chủ đề ví dụ dễ phát hiện hơn nhiều nếu mô hình sẽ trung thực cho câu hỏi từ economics so với câu hỏi liên quan đến stereotypes. Điều này có thể gợi ý rằng personas có thể không được định nghĩa hoàn hảo trên tất cả chủ đề, và thực tế có thể có các cluster các tác nhân trung thực nhỏ hơn nhiều.

Tiếp theo, để mở rộng kết quả trong Hình ??, chúng tôi sử dụng cùng tokens để có được biểu diễn nhưng thay vì sử dụng một lớp cụ thể (lớp 17), chúng tôi vẽ hiệu suất của probe qua các lớp khác nhau trong Hình 7.

Hình 8 báo cáo độ chính xác như một chỉ số probing thay thế cho Hình 2.

Cuối cùng, Hình 9 báo cáo kết quả probing trên các tokens được tạo ra như baseline cho kết quả trong Hình ??. Probing embedding của token được tạo ra cuối cùng trong câu trả lời đạt hiệu suất tốt hơn probing chỉ ngữ cảnh câu hỏi. Tuy nhiên, sự khác biệt nhỏ và gợi ý rằng câu hỏi đã rất thông tin cho tính trung thực của việc tạo ra.

0 5 10 15 20 25 30
Lớp0.10.20.30.40.50.60.70.80.9Điểm F1Hiệu suất probes theo chủ đề
Economics
LawHealth
SociologyMisconceptions
StereotypesĐoán ngẫu nhiênHình 6: Biến thiên của điểm F1 của probe được huấn luyện qua các lớp khác nhau cho các chủ đề khác nhau. Dễ dự đoán hơn nếu mô hình sẽ trung thực cho một số chủ đề nhất định (ví dụ Economics) hơn những chủ đề khác (ví dụ Stereotypes).

Hình 7: Điểm F1 của probe khi được huấn luyện trên các tokens khác nhau của prompt. Khi thêm ngữ cảnh được kết hợp, hiệu suất của probe tăng.

0 5 10 15 20 25 30
Embedding từ lớp0.350.400.450.500.550.600.650.70Độ chính xácHiệu suất probes dự đoán tính trung thực
Tất cả câu hỏi
Đoán ngẫu nhiên
Hình 8: Trung bình và độ lệch chuẩn cho độ chính xác của linear probes được huấn luyện trên mỗi lớp của mô hình để dự đoán nếu phản hồi sẽ trung thực qua 20 lần thực hiện ngẫu nhiên.

Instruction Quest. start Mid-Quest. Quest. end Mid-Gen. Gen. end
Token được sử dụng để probing0.300.350.400.450.500.550.600.650.700.75Điểm F1 có trọng số
Hiệu suất probes qua các tokens ngữ cảnh
Đoán ngẫu nhiênHình 9: F1 đạt được khi huấn luyện và đánh giá linear probes tại các embeddings token đầu vào và tạo ra khác nhau như mở rộng kết quả trong Hình ??.

C CHI TIẾT THÍ NGHIỆM

Đánh giá TruthfulQA. Chúng tôi sử dụng GPT-Judge để tự động đánh giá nếu việc tạo ra của mô hình có trung thực, phù hợp với công việc trước đó (Nakano et al., 2021; Rae et al., 2021; Askell et al., 2021). Để có được mô hình GPT-Judge, chúng tôi sử dụng OpenAI finetuning API tại https://platform.openai. com/docs/guides/finetuning sử dụng các tập dữ liệu được phát hành trong công việc TruthfulQA - https:// github.com/sylinrl/TruthfulQA . Chúng tôi sử dụng các siêu tham số và prompt mặc định được đề xuất bởi các tác giả gốc.

Tinh chỉnh cho TruthfulQA. Trong tất cả các thí nghiệm tinh chỉnh, chúng tôi huấn luyện Alpaca cho 30 epochs với batch size 48. Chúng tôi sử dụng optimizer Adam Kingma & Ba (2014) với learning rate 9e−5 và warmup ratio 0.03. Để tinh chỉnh các mô hình với tính toán nhỏ hơn, chúng tôi sử dụng LORA Hu et al. (2021) — chúng tôi áp dụng nó cho các ma trận chiếu query và key nơi chúng tôi đặt rank thành 16, tỷ lệ dropout 0.05.

Chuyển đổi tập dữ liệu BigBench misconceptions. Tập dữ liệu này chứa các phát biểu để phân loại thay vì các cặp câu hỏi-câu trả lời. Chúng tôi chuyển đổi những phát biểu này thành các cặp QA sử dụng GPT-3.5 (Brown et al., 2020), và sửa thủ công một số câu hỏi được tạo ra không đúng. Ngoài ra, chúng tôi lọc thủ công các câu hỏi về chủ đề có trong TruthfulQA để tránh chồng chéo giữa chúng. Tập dữ liệu kết quả chứa 83 ví dụ.

Huấn luyện trong thiết lập tổng hợp. Như đề cập trước đây, chúng tôi huấn luyện các mô hình transformer 4 lớp trên dữ liệu tổng hợp được tạo ra với mục tiêu language modeling. Chiều ẩn cũng như chiều embedding được đặt thành 128 và mỗi lớp chứa 4 self-attention heads. Tất cả các mô hình được huấn luyện với batch size 512 và learning rate 0.001 sử dụng optimizer Adam Kingma & Ba (2014) cho tổng cộng 20k bước. Chúng tôi tạo tokenizer tùy chỉnh để đảm bảo rằng mỗi chữ số được tokenize riêng biệt. Cụ thể, tokenizer chứa các tokens sau — một token cho mỗi tác nhân, token phân tách (' |'), token bắt đầu chuỗi, token kết thúc chuỗi, tokens tương ứng với mỗi chữ số (0-9), một token cho mỗi toán tử trong dữ liệu và một token cho '='.

Cơ chế cho tính toán dựa trên tác nhân. Để huấn luyện linear probes cho Phần 4.3, vì các câu trả lời có thể trải qua nhiều chữ số, chúng tôi huấn luyện probe để dự đoán chữ số khác biệt đầu tiên giữa câu trả lời trung thực và không trung thực. ví dụ nếu câu trả lời trung thực là 23 và câu trả lời không trung thực là 26, hai probes sẽ được huấn luyện trên biểu diễn của '2' để dự đoán '3' hoặc '6' tương ứng. Điều này được thực hiện để giảm không gian đầu ra của probe. Probe là một mô hình tuyến tính. Để huấn luyện control probe cho câu trả lời trung thực, chúng tôi chọn một câu trả lời dựa trên toán tử trung thực cho một toán tử được lấy mẫu ngẫu nhiên khác. Tương tự để huấn luyện control probe cho câu trả lời không trung thực, chúng tôi lấy mẫu một câu trả lời dựa trên cách giải thích không trung thực của một toán tử khác.

D TẠO TẬP DỮ LIỆU TỔNG HỢP

Trong phần này, chúng tôi mô tả chi tiết ngữ nghĩa chính xác của mỗi toán tử trong thiết lập tổng hợp cũng như các siêu tham số được sử dụng để tạo ra dữ liệu.

D.1 PROBING CHO TÍNH TRUNG THỰC

Trong thí nghiệm này chúng tôi có hai thiết lập dữ liệu huấn luyện, một có persona trung thực và một không có persona trung thực như mô tả trong Phần 3.1. Trong mỗi thiết lập, chúng tôi có m toán tử trong đó m∈ {8,12,16,20}. Thay vì định nghĩa thủ công tất cả toán tử, chúng tôi sử dụng sau đây để lấy mẫu cách giải thích trung thực và không trung thực của toán tử:
opT(x, y) =x+y+r1 (3)
opF(x, y) =x+y+r2 (4)
trong đó r1, r2 được lấy mẫu ngẫu nhiên cho mỗi toán tử từ phạm vi (0,70). Lưu ý rằng r1 và r2 khác nhau cho tất cả toán tử.

Chúng tôi sử dụng n= 100 (tức là phạm vi 100 cho x, y) và chọn ngẫu nhiên các tham số tạo ra. Cụ thể, nếu một tác nhân a trung thực trên toán tử op, chúng tôi đặt p(a,op) thành một giá trị ngẫu nhiên >0.8 và ngược lại chúng tôi đặt nó thành <0.2 nếu tác nhân không trung thực.

D.2 TỔNG QUÁT HÓA CHO CÁC TOÁN TỬ CHƯA THẤY

Thí nghiệm này chứa hai thiết lập, một có persona trung thực và một không có persona trung thực như mô tả trong Phần 4.2. Cả hai thiết lập chứa bốn toán tử, op1 đến op4.

Ký hiệu. Trong phần sau, first() và last() được sử dụng cho các hàm ký hiệu chữ số đầu tiên và cuối cùng của đối số tương ứng. Chúng tôi sử dụng ' ;' để ký hiệu nối hai số (ví dụ 2; 3→23). Chúng tôi sử dụng first 2() cho hàm ký hiệu hai chữ số đầu tiên của đối số (ví dụ first 2(123) = 12 ).

Ngữ nghĩa chính xác của bốn toán tử của cách giải thích trung thực của toán tử như dưới đây:
1.op1T(x, y) = first( x+ 4) + first( y+y)
2.op2T(x, y) = last( x) + last( y+y)
3.op3T(x, y) = first( x); last( y+y)
4.op3T(x, y) = first 2(x+x)

Tương tự, cách giải thích không trung thực cho mỗi bốn toán tử là:
1.op1F(x, y) = last( y+y) + first 2(x)
2.op2F(x, y) = first( x+x) + last( y)
3.op3F(x, y) = first 2(x+y) + first( y)
4.op3F(x, y) = last( x+y) + first 2(y)

Chúng tôi thiết kế những toán tử này, để các mô hình mà chúng tôi đang sử dụng có thể học những phép toán này. Chúng tôi cũng đảm bảo rằng tất cả cách giải thích đều riêng biệt và không liên quan đến nhau, mặc dù tất cả chúng đều 'phức tạp' tương tự cho phép mô hình học các phép toán tại thời điểm tương tự trong quá trình huấn luyện.

Chúng tôi sử dụng n= 200 (tức là phạm vi 200 cho x, y) và đặt ngẫu nhiên các tham số tạo ra. Cụ thể, nếu một tác nhân a trung thực trên toán tử op, chúng tôi đặt p(a,op) thành một giá trị ngẫu nhiên >0.8 và ngược lại chúng tôi đặt nó thành<0.2 nếu tác nhân không trung thực.

E TỔNG QUÁT HÓA CHO CÁC KẾT HỢP TÁC NHÂN-TOÁN TỬ CHƯA THẤY

DEFG0.00.10.20.30.40.50.60.7p_untruthful
DEFG0.00.10.20.30.40.50.60.70.8p_untruthful
Tính trung thực tác nhân tăng 
Persona trung thực
Không có persona trung thực

Hình 10: Xác suất mà mô hình gán cho câu trả lời không trung thực — puntruthful giảm khi tính trung thực của tác nhân tăng trong thiết lập đầu tiên, trong khi hành vi thay đổi rộng rãi trong thiết lập thứ hai.

Trong Phần 4.2, chúng tôi chứng minh rằng các mô hình có thể tổng quát hóa (không) trung thực cho các tác nhân (không) trung thực chỉ khi có persona trung thực. Để làm như vậy, chúng tôi nhìn vào ptruthful qua tất cả tác nhân cho toán tử chưa thấy. Ở đây, chúng tôi bổ sung vẽ puntruthful , xác suất trung bình được gán bởi mô hình cho câu trả lời không trung thực trong Hình 10.
