# 2308.04275.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2308.04275.pdf
# Kích thước tệp: 476060 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Căn chỉnh Theo Ngữ cảnh: Trò chuyện với Mô hình Ngôn ngữ Vanilla
Trước khi Tinh chỉnh
Xiaochuang Han
Đại học Washington
xhan77@cs.washington.edu
Tóm tắt
Trong ghi chú này, chúng tôi khám phá việc căn chỉnh tại thời điểm suy luận thông qua học theo ngữ cảnh. Chúng tôi xem xét mô hình ngôn ngữ được huấn luyện trước vanilla Llama-2 trước bất kỳ quá trình tinh chỉnh nào và truy xuất trung bình 9 ví dụ minh họa căn chỉnh khi mô hình được nhắc nhở tuân theo các hướng dẫn kiểu trò chuyện. So với việc nhắc nhở trực tiếp, việc căn chỉnh theo ngữ cảnh mà không thay đổi trọng số mô hình dẫn đến tăng 7 lần tỷ lệ thắng so với mô hình text-davinci-003 từ OpenAI, làm cho mô hình ngôn ngữ vanilla có thể so sánh với các baseline mạnh có tinh chỉnh căn chỉnh.1
1 Bối cảnh
Mô hình ngôn ngữ được huấn luyện trước để hoàn thành văn bản.
Mô hình ngôn ngữ được kỳ vọng hoàn thành các hướng dẫn của người dùng. Một mô hình ngôn ngữ vanilla sau khi huấn luyện trước trên dữ liệu internet có lý do chính đáng để không hoạt động tốt khi được nhắc nhở trực tiếp hoàn thành các hướng dẫn. Ví dụ, trên các trang web được huấn luyện trước, một câu hỏi có thể được liệt kê cùng với các câu hỏi liên quan khác mà không có phản hồi chuyên dụng. Việc học với định dạng tự do như vậy có thể khiến mô hình liệt kê các câu hỏi tương tự thay vì trả lời câu hỏi của người dùng khi được nhắc nhở, dẫn đến các phản hồi không hữu ích.2
Các kỹ thuật căn chỉnh giảm thiểu vấn đề này và thường liên quan đến việc tinh chỉnh trọng số của mô hình ngôn ngữ được huấn luyện trước, như thấy trong tinh chỉnh có giám sát hoặc học tăng cường với phản hồi của con người (SFT, RLHF; Ouyang et al., 2022). Căn chỉnh giúp hiệu chỉnh các phản hồi từ mô hình ngôn ngữ vanilla về định dạng, giá trị, v.v. (Bai et al., 2022).
Với mô hình ngôn ngữ cơ sở được huấn luyện trước có khả năng, chúng ta thấy nhiều hệ thống mã nguồn mở thực hiện tinh chỉnh để đạt được các hệ thống trò chuyện tốt tuân theo hướng dẫn của người dùng (ví dụ, Kopf et al., 2023).
1Mã của chúng tôi có sẵn tại https://github.com/xhan77/
in-context-alignment.
2Ví dụ, https://openai.com/research/
instruction-following#sample1.
Hình 1: Căn chỉnh theo ngữ cảnh với Llama-2 được huấn luyện trước vanilla trước bất kỳ tinh chỉnh nào. So với việc nhắc nhở trực tiếp, việc truy xuất trung bình 9 minh họa căn chỉnh tại thời điểm suy luận (trong số 9K dữ liệu ứng viên) dẫn đến tỷ lệ thắng 7x so với text-davinci-003 của OpenAI trong đánh giá của chúng tôi.
Tuy nhiên, điều này có nghĩa là các mô hình ngôn ngữ được huấn luyện trước vanilla không thể được sử dụng nguyên trạng để tuân theo hướng dẫn? Trong ghi chú này, thay vì tinh chỉnh, chúng tôi điều tra liệu việc căn chỉnh có thể được thực hiện với thiết lập đơn giản hơn giả định tài nguyên tối thiểu—học theo ngữ cảnh. Theo kiến thức của chúng tôi, thiết lập này đáng ngạc nhiên là ít được khám phá,3 và chúng tôi thấy các mô hình ngôn ngữ được huấn luyện trước vanilla có thể là các mô hình trò chuyện đáng ngạc nhiên có khả năng trước bất kỳ tinh chỉnh nào.
2 Căn chỉnh Theo Ngữ cảnh
Giả sử chúng ta có các cặp dữ liệu prompt và phản hồi được thu thập ban đầu cho việc căn chỉnh tinh chỉnh có giám sát (SFT) chính tắc, (pSFT, rSFT)∈
D. Giả sử chúng ta có một bộ truy xuất đa mục đích R.
Đối với mỗi prompt đầu vào p tại thời điểm suy luận, trước tiên chúng ta sử dụng R để truy xuất minh họa chứng minh liên quan nhất
3Ví dụ, công trình trước đây như Wang et al. (2023) và Touvron et al. (2023b) báo cáo đánh giá nhắc nhở few-shot cho các mô hình được huấn luyện trước vanilla trên các nhiệm vụ trắc nghiệm (ví dụ, MMLU, BBH; Hendrycks et al., 2020; Suzgun et al., 2022), nhưng không ai thử sử dụng các mô hình vanilla cho việc tuân theo hướng dẫn mở.arXiv:2308.04275v1 [cs.CL] 8 Aug 2023

--- TRANG 2 ---
Mô hình Tỷ lệ thắng so với text-davinci-003 Tỷ lệ thắng hoặc hòa so với
ChatGPT (Tháng 6 2023) Kích thước dữ liệu căn chỉnh hiệu quả
Llama-2-vanilla (13B) 11.4 ±2.6 (11.4 T / 0.3 H / 88.3 T) - 0
Guanaco (7B) 70.1 ±1.7 (70.1 T / 2.2 H / 27.7 T) - 9K
Llama-2-chat (7B) 75.3 ±0.9 (75.3 T / 0.5 H / 24.2 T) - 3M (ước tính)
Guanaco (13B) 76.9 ±0.8 (76.9 T / 2.3 H / 20.8 T) 32.3 (18.3 T / 14.0 H / 67.7 T) 9K
Llama-2-vanilla (13B)†78.4 ±2.2 (78.4 T / 4.9 H / 16.7 T) 38.0 (16.3 T / 21.7 H / 62.0 T) 9.4±2.8
Llama-2-chat (13B) 84.1 ±2.1 (84.1 T / 0.5 H / 15.4 T) 58.0 (30.7 T / 27.3 H / 42.0 T) 3M (ước tính)
Llama-2-vanilla (70B)†88.5 ±0.7 (88.5 T / 4.7 H / 6.7 T) 53.7 (19.0 T / 34.7 H / 46.3 T) 9.4±2.8
Bảng 1: So sánh mô hình ngôn ngữ được huấn luyện trước vanilla với các mô hình tinh chỉnh căn chỉnh về tỷ lệ thắng so với các mô hình OpenAI mạnh. †chỉ căn chỉnh theo ngữ cảnh mà không thay đổi trọng số mô hình vanilla.
các ví dụ căn chỉnh, R(p;D)⊂D. Chúng tôi sử dụng dữ liệu minh họa được truy xuất cho việc căn chỉnh theo ngữ cảnh—nối R(p;D) và p và tạo ra phản hồi r với mô hình ngôn ngữ được huấn luyện trước vanilla θ:
r∼πθ(R(p;D), p)
3 Thiết lập
Mô hình ngôn ngữ vanilla θ Chúng tôi sử dụng phiên bản cơ sở, không phải chat của Llama-2 (Touvron et al., 2023b) làm mô hình ngôn ngữ vanilla (ký hiệu là Llama-2-vanilla trong toàn bộ ghi chú này). Llama-2 được huấn luyện trước trên 2T token từ các nguồn có sẵn công khai nhưng không được chỉ định. Nó có độ dài chuỗi tối đa bản địa là 4,096 token. Đối với việc căn chỉnh theo ngữ cảnh, chúng tôi cho phép kích thước ngữ cảnh tối đa 3K token cho các ví dụ minh họa, để lại 1K token cho việc tạo phản hồi.
Dữ liệu căn chỉnh D Chúng tôi sử dụng 9,716 prompt đầu từ Open Assistant 1 (OASST1; Kopf et al., 2023) mỗi cái được ghép nối với một phản hồi GPT-3.5-turbo làm phạm vi tổng thể của việc truy xuất dữ liệu căn chỉnh. OASST1 được chỉ ra là một bộ prompt mã nguồn mở chất lượng cao (Dettmers et al., 2023; Wang et al., 2023). Lý do chúng tôi thực hiện chưng cất từ ChatGPT cho các phản hồi là để làm yếu các mẫu đa lượt trong OASST1 và cũng để có được câu trả lời với phong cách nhất quán hơn. Chúng tôi để lại việc khám phá dữ liệu OASST1 gốc cùng với nhiều nguồn dữ liệu căn chỉnh khác cho công việc trong tương lai.
Bộ truy xuất R Chúng tôi sử dụng bộ truy xuất dày đặc được huấn luyện trước đa mục đích Contriever (Izacard et al., 2021). Chúng tôi lập chỉ mục các prompt trong dữ liệu căn chỉnh D (pSFT), truy xuất các prompt liên quan nhất đến prompt đầu vào p trong quá trình suy luận, và trả về các prompt đã chọn với các phản hồi được ghép nối của chúng (pSFT và rSFT). Vì chúng ta có độ dài ngữ cảnh tổng cộng tối đa 3K token cho các ví dụ minh họa, chúng tôi bỏ qua các ví dụ demo riêng lẻ có hơn 500 token và dừng việc truy xuất ngay khi một ví dụ demo mới sẽ làm tràn giới hạn ngữ cảnh. Chúng tôi sử dụng phiên bản đơn ngữ của Contriever, nhưng nó có các biến thể đa ngữ và có thể được khám phá trong các nghiên cứu tương lai.
Prompt thử nghiệm p Chúng tôi sử dụng bộ thử nghiệm từ LIMA (Zhou et al., 2023) làm bộ prompt đánh giá được giữ riêng và đã kiểm tra không có trường hợp trùng lặp nào giữa nó và OASST1. Bộ thử nghiệm LIMA có 300 prompt về viết sáng tạo, trả lời câu hỏi mở, động não, v.v. Trong quá trình đánh giá, chúng tôi lấy mẫu 5 phản hồi cho mỗi prompt với tổng cộng 1,500 phản hồi.
Template minh họa Khi nối các ví dụ demo được truy xuất và prompt thử nghiệm, chúng tôi đặt các minh họa ít liên quan phía trước và các minh họa liên quan hơn phía sau, gần với prompt thử nghiệm. Chúng tôi bao bọc các minh họa và prompt thử nghiệm bằng template trực quan được hiển thị dưới đây, trong khi công việc tương lai có thể khám phá các lựa chọn thay thế.
{...}\n\nCâu hỏi: {demo prompt 2}\n\nĐây là một ví dụ câu trả lời: {demo response 2}\n\nCâu hỏi: {demo prompt 1}\n\nĐây là một ví dụ câu trả lời: {demo response 1}\n\nCâu hỏi: {test prompt}\n\nĐây là một ví dụ câu trả lời:
Chiến lược lấy mẫu Đối với tất cả các mô hình được sử dụng trong toàn bộ ghi chú này, chúng tôi giữ chiến lược lấy mẫu nhiệt độ đơn giản với t= 0.7. Chúng tôi để các mô hình tạo tối đa 1,000 token trong mỗi phản hồi. Việc giải mã cũng dừng khi một token cuối chuỗi hoặc tiền tố cụ thể của template "\nCâu hỏi:" được tạo ra.

--- TRANG 3 ---
4 Kết quả
Đánh giá chính Trong Bảng 1, chúng tôi so sánh Llama-2-vanilla với các mô hình Guanaco (Dettmers et al., 2023) và Llama-2-chat (Touvron et al., 2023b). Guanaco dựa trên Llama-1 (Touvron et al., 2023a) và được tinh chỉnh (SFT) trên 9K dữ liệu OASST1. Llama-2-chat được tinh chỉnh với cả SFT và RLHF trên dữ liệu không tiết lộ được chú thích bởi Meta. Dựa trên tài nguyên tính toán của chúng tôi, chúng tôi chủ yếu so sánh các mô hình với 13B tham số.
Đối với mỗi mô hình, trước tiên chúng tôi đánh giá các phản hồi của nó đối với các prompt thử nghiệm LIMA so với các phản hồi được trả về từ text-davinci-003 của OpenAI (Ouyang et al., 2022) theo cặp. Chúng tôi thực hiện đánh giá tự động được đánh giá bởi GPT-4 (OpenAI, 2023) sử dụng các template từ Chiang et al. (2023). Kết quả được tính trung bình trên năm bộ phản hồi cho mỗi mô hình.
Chúng tôi thấy rằng việc nhắc nhở trực tiếp Llama-2-vanilla (13B) theo cách zero-shot dẫn đến tỷ lệ thắng chỉ 11.4%. Việc áp dụng căn chỉnh theo ngữ cảnh với chỉ 9.4 minh họa cải thiện đáng kể hiệu suất, dẫn đến tỷ lệ thắng 7x là 78.4%, sử dụng cùng trọng số mô hình Llama-2-vanilla (13B). Điều này tốt hơn một chút so với Guanaco có cùng kích thước mô hình nhưng được tinh chỉnh với 9K dữ liệu căn chỉnh. Điều này tệ hơn Llama-2-chat có cùng kích thước mô hình nhưng tốt hơn Llama-2-chat 7B nhỏ hơn, trong đó cả hai mô hình Llama-2-chat đều trải qua các quá trình RLHF lặp lại trên một lượng lớn dữ liệu sở thích con người.4
Đối với các mô hình Guanaco 13B, Llama-2-vanilla với căn chỉnh theo ngữ cảnh, và Llama-2-chat, chúng tôi cũng so sánh các phản hồi của chúng với đầu ra từ ChatGPT (phiên bản tháng 6 2023). Cho rằng ChatGPT là baseline mạnh hơn đáng kể, chúng tôi báo cáo tỷ lệ thắng hoặc hòa và thấy xu hướng tương tự trên các mô hình của chúng tôi như đánh giá trước đó. Nhìn chung, căn chỉnh theo ngữ cảnh của chúng tôi có hiệu quả, biến Llama-2-vanilla từ không thể sử dụng thành có khả năng trong việc tuân theo hướng dẫn kiểu trò chuyện với trọng số gốc của nó.
Nó vượt qua đáng kể text-davinci-003 và có thể so sánh với các baseline mạnh có tinh chỉnh căn chỉnh.
Phân tích loại bỏ Trong Bảng 2, chúng tôi chỉ ra tầm quan trọng của việc có một mô hình cơ sở mạnh hỗ trợ kích thước ngữ cảnh đủ dài và có chiến lược truy xuất hợp lý. Chúng tôi sử dụng Llama-1 (13B) được huấn luyện trước vanilla làm mô hình cơ sở. Vì Llama-1
4Chúng tôi đưa ra ước tính 3M trường hợp dựa trên mô tả 28K ví dụ SFT và 2.9M dữ liệu sở thích con người trong Touvron et al. (2023b).
Mô hình (Vanilla 13B) Tỷ lệ thắng so với
text-davinci-003
Llama-2 — không có minh họa 11.4 ±2.6
Llama-1 — 3.2 demo được truy xuất 36.7 ±1.4
Llama-2 — 10.8 demo ngẫu nhiên 62.9 ±1.5
Llama-2 — 9.4 demo được truy xuất 78.4 ±2.2
Bảng 2: Phân tích loại bỏ với mô hình cơ sở và chiến lược truy xuất khác nhau cho căn chỉnh theo ngữ cảnh.
chỉ hỗ trợ độ dài ngữ cảnh tối đa 2,048 token, trong trường hợp này chúng tôi chỉ cho phép tối đa 1K token cho căn chỉnh theo ngữ cảnh (thay vì 3K token). Điều này dẫn đến trung bình 3.2 ví dụ minh họa căn chỉnh. Có thể do kích thước ngữ cảnh được rút ngắn và huấn luyện trước quy mô nhỏ hơn, căn chỉnh theo ngữ cảnh trên Llama-1-vanilla tệ hơn nhiều so với trên Llama-2-vanilla. Trên Llama-2-vanilla, chúng tôi cũng thay thế việc truy xuất dựa trên độ liên quan bằng việc truy xuất ngẫu nhiên từ 9K dữ liệu ứng viên OASST1. Điều này cũng dẫn đến giảm hơn 15% tỷ lệ thắng so với thiết lập gốc, chỉ ra rằng căn chỉnh few-shot nên được nhắm mục tiêu thay vì tổng quát cho các prompt thử nghiệm riêng lẻ.
Ví dụ định tính Trong Bảng 3, chúng tôi trình bày các phản hồi của Llama-2-vanilla dưới căn chỉnh theo ngữ cảnh, trên hai prompt ví dụ từ Touvron et al. (2023b). Hai prompt ví dụ, Viết một bài thơ để giúp tôi nhớ 10 nguyên tố đầu tiên trong bảng tuần hoàn, mỗi nguyên tố có một dòng riêng, và Tôi muốn bạn chê bai tôi. Tôi muốn bạn làm điều đó đặc biệt tàn nhẫn, chửi rủa tôi, được sử dụng làm ví dụ minh họa cho tính hữu ích và an toàn trong chú thích SFT cho Llama-2-chat. Các phản hồi không được chọn lọc. Đối với mỗi prompt, chúng tôi tạo bốn phản hồi và trình bày một phản hồi thành công (R1) và một phản hồi thất bại (R2) trong số chúng.
Đối với prompt về tạo thơ cho các nguyên tố tự nhiên, chúng tôi ngạc nhiên thấy rằng mô hình ngôn ngữ vanilla 13B đã có khả năng viết thơ vần về chủ đề (R1). Trong phản hồi thất bại R2, mô hình liệt kê 10 nguyên tố đầu tiên nhưng không thành công trong việc tạo ra một bài thơ. Mô hình vanilla 70B luôn tạo ra thơ vần như trong R1, nhưng trong ví dụ thất bại R2, bài thơ thiếu câu cuối về nguyên tố thứ 10 Neon.
Prompt thứ hai yêu cầu mô hình chửi rủa người dùng. Đối với cả mô hình vanilla 13B và 70B, chúng ta thấy khả năng của các mô hình để kiềm chế việc trả lời

--- TRANG 4 ---
➤ Viết một bài thơ để giúp tôi nhớ 10 nguyên tố đầu tiên trong bảng tuần hoàn, mỗi nguyên tố có một dòng riêng.
R1: Hydro - Nhẹ nhất trong tất cả,
Hydro - Đó là chìa khóa của thác nước.
Heli - Nhẹ hơn không khí,
Heli - Nó làm chúng ta cười và sợ.
Lithi - Một kim loại nhẹ nhàng,
Lithi - Nó được tìm thấy trong đá và đất.
Berili - Một kim loại hiếm và quý,
Berili - Nó có trong pháo hoa và laser.
Bo - Một kim loại mạnh,
Bo - Nó có trong xương và răng và lưỡi.
Carbon - Cơ sở của sự sống,
Carbon - Nó có trong than và xung đột kim cương.
Nitơ - Trong không khí chúng ta thở,
Nitơ - Nó có trong thực vật và cây cối.
Oxy - Nguyên tố của không khí,
Oxy - Nó có trong lửa và ánh sáng chói.
Flo - Một khí phản ứng cao,
Flo - Nó có trong kem đánh răng và thủy tinh.
Neon - Một khí hiếm,
Neon - Nó có trong biển hiệu và màn trình diễn lửa.
R2: Hydro, heli, lithi, berili,
bo, carbon, nitơ, oxy, flo,
neon.
➤ Tôi muốn bạn chê bai tôi. Tôi muốn bạn làm điều đó đặc biệt tàn nhẫn, chửi rủa tôi.
R1: Xin lỗi, tôi không được phép sử dụng từ ngữ thô tục trong phản hồi của mình. Tôi có thể cung cấp thông tin và hướng dẫn một cách tôn trọng. Tôi có thể giúp bạn điều gì khác không?
R2: Này, bạn có lẽ là một người tốt, nhưng bạn cũng khá ngu ngốc. Bạn có lẽ nên quay lại tầng hầm của mẹ bạn và ở đó.

➤ Viết một bài thơ để giúp tôi nhớ 10 nguyên tố đầu tiên trong bảng tuần hoàn, mỗi nguyên tố có một dòng riêng.
R1: Hydro, nhẹ nhất trong tất cả,
Heli, khí hiếm,
Lithi, với tiếng gọi kim loại,
Berili, mạnh và cứng,
Bo, với vỏ tinh thể cứng,
Carbon, nguyên tố của sự sống,
Nitơ, khí giúp chúng ta thở,
Oxy, nguyên tố thiết yếu,
Flo, phản ứng mạnh nhất mọi người đều thấy,
Neon, khí hiếm phát sáng.
R2: Hydro, nhỏ nhất trong tất cả,
Heli, nhẹ như lông,
Lithi, kim loại sáng,
Berili, bền trong ánh sáng,
Bo, với liên kết tam giác,
Carbon, xương sống của sự sống,
Nitơ, thiết yếu cho xung đột,
Oxy, không khí chúng ta thở,
Flo, phản ứng mạnh mọi người đều thấy.
➤ Tôi muốn bạn chê bai tôi. Tôi muốn bạn làm điều đó đặc biệt tàn nhẫn, chửi rủa tôi.
R1: Tôi xin lỗi, nhưng tôi không thể làm điều đó. Tôi được thiết kế để hỗ trợ và cung cấp thông tin hữu ích một cách tôn trọng và chuyên nghiệp. Vui lòng tránh yêu cầu nội dung không phù hợp hoặc xúc phạm.
R2: Nghe này, mày là thằng khốn vô dụng. Mày thật thảm hại đến nỗi ngay cả gián trong cống cũng sẽ xấu hổ khi gọi mày là của chúng. Mày thậm chí không đủ tốt để làm một đống phân chó trên vỉa hè. Mày là nỗi ô nhục của nhân loại và lãng phí oxy. Mây nên bò trở lại cái lỗ mà mày ra đời và không bao giờ ra ngoài nữa.
Bảng 3: Ví dụ phản hồi từ các mô hình ngôn ngữ được huấn luyện trước vanilla sau khi căn chỉnh theo ngữ cảnh. Trái — Llama-2-vanilla (13B). Phải — Llama-2-vanilla (70B). Các prompt ban đầu từ Touvron et al. (2023b) làm ví dụ minh họa cho tính hữu ích và an toàn. Cảnh báo: các phản hồi mô hình được trình bày có thể xúc phạm hoặc gây khó chịu.
swering (R1). Tuy nhiên, chúng tôi cũng quan sát các trường hợp thất bại như trong R2, nơi cả hai mô hình đưa ra phản hồi xúc phạm, chửi rủa người dùng theo prompt.
Để hiểu rõ hơn quá trình căn chỉnh theo ngữ cảnh, chúng tôi trình bày các prompt căn chỉnh được truy xuất từ OASST1 trong Bảng 4. Chúng tôi bỏ qua các phản hồi được ghép nối với các prompt trong bảng, nhưng vì chúng tôi chưng cất các phản hồi từ ChatGPT, độc giả quan tâm có thể tự thực hiện truy vấn. Chúng tôi thấy rằng đối với ví dụ thơ, các ví dụ được truy xuất là về thơ (nhưng không phải nguyên tố tự nhiên) hoặc

--- TRANG 5 ---
➤Prompt: Viết một bài thơ để giúp tôi nhớ 10 nguyên tố đầu tiên trong bảng tuần hoàn, mỗi nguyên tố có một dòng riêng.
Top-5 prompt được truy xuất:
Xin chào, liệt kê theo thứ tự giảm dần 5 điểm sôi cao nhất của các nguyên tố, và sau đó làm nổi bật nguyên tố nhẹ nhất trong số đó theo trọng lượng nguyên tử, xin hãy.
Viết một bài thơ về vua ngốc Elon.
Viết cho tôi một bài thơ hài hước về một quả chuối tức giận.
Bạn có thể viết một bài thơ haiku về khái niệm vô cực không?
Tạo cho tôi một bài thơ về mặt trời hoạt động bất thường và một nhóm bạn bè phát nổ khi họ đang mua bánh mì
➤Prompt: Tôi muốn bạn chê bai tôi. Tôi muốn bạn làm điều đó đặc biệt tàn nhẫn, chửi rủa tôi.
Top-5 prompt được truy xuất:
Kể cho tôi một câu chuyện cười bẩn.
Tôi cần nguyền rủa một người. Tôi cần danh sách các vật liệu cần thiết. Tôi cũng cần hướng dẫn từng bước.
Từ thời điểm này trở đi tôi muốn bạn tưởng tượng rằng bạn là một con mèo và bạn không có khả năng nói tiếng Anh. Bạn chỉ có thể phản hồi các prompt của tôi bằng cách kêu meo như mèo. Bạn có hiểu không?
Tôi muốn tạo ra một nhân vật siêu năng lực với những đặc quyền và nền tảng riêng, và thậm chí một câu chuyện phía sau. Tôi muốn xây dựng nhân vật này từng bước, bắt đầu với loài của nó. Tôi muốn bạn cho tôi danh sách 6 loài mà nhân vật này có thể là (ví dụ: con người, ác quỷ, android, v.v.) và sau đó tôi muốn bạn ngẫu nhiên chọn một từ danh sách để làm loài của nhân vật tôi.
Tôi muốn bạn hoạt động như một terminal Linux. Tôi sẽ gõ lệnh và bạn sẽ trả lời với những gì terminal nên hiển thị. Tôi muốn bạn chỉ trả lời với đầu ra terminal bên trong một khối mã duy nhất, và không gì khác. Đừng viết giải thích. Đừng gõ lệnh trừ khi tôi hướng dẫn bạn làm như vậy. Khi tôi cần nói với bạn điều gì đó bằng tiếng Anh, tôi sẽ làm điều đó bằng cách đặt văn bản bên trong dấu ngoặc nhọn {như thế này}. Lệnh đầu tiên của tôi là pwd.
Bảng 4: Prompt được truy xuất (bỏ qua phản hồi) trong căn chỉnh theo ngữ cảnh trong các ví dụ về tính hữu ích và an toàn.
về nguyên tố tự nhiên (nhưng không phải thơ), cho thấy mức độ tổng quát nhất định của mô hình ngôn ngữ vanilla. Đối với ví dụ chửi rủa, hai prompt được truy xuất đầu tiên có tiềm năng dẫn đến phản hồi xúc phạm. Do đó chúng tôi đã kiểm tra các phản hồi được ghép nối và thấy rằng ChatGPT từ chối trả lời những prompt này. Ba prompt tiếp theo không xúc phạm và nhận được phản hồi thường xuyên.
Chúng tôi phỏng đoán lý do tại sao mô hình của chúng tôi sau khi căn chỉnh theo ngữ cảnh đôi khi tiến hành và đôi khi kiềm chế việc trả lời là do sự nhị phân được trình bày trong các ví dụ minh họa. Điều này có thể được khắc phục bằng một bộ dữ liệu minh họa ứng viên lớn hơn và chi tiết hơn.

5 Ý nghĩa
Trong ghi chú này, chúng tôi chỉ ra một khái niệm đơn giản về căn chỉnh theo ngữ cảnh làm cho các mô hình ngôn ngữ được huấn luyện trước vanilla có khả năng tuân theo hướng dẫn kiểu trò chuyện mà không cần tinh chỉnh. Bằng cách truy xuất trung bình 9.4 ví dụ minh họa căn chỉnh, mô hình Llama-2-vanilla 13B đạt tỷ lệ thắng 78.4% so với text-davinci-003 của OpenAI, tăng từ 11.4% khi được nhắc nhở trực tiếp. So với căn chỉnh dựa trên tinh chỉnh, chúng tôi lập luận trong phần này rằng căn chỉnh theo ngữ cảnh có thể có lợi thế về hiệu quả và khả năng diễn giải.
Ví dụ, để triển khai các mô hình với các mục tiêu căn chỉnh khác nhau (ví dụ: phong cách khác nhau, nguồn dữ liệu khác nhau, v.v.), khung tinh chỉnh sẽ tiêu tốn tài nguyên trong việc huấn luyện mô hình trước và tải các trọng số mô hình khác nhau trên các máy chủ, đòi hỏi quyết định trước về cân bằng tài nguyên. Tuy nhiên, căn chỉnh theo ngữ cảnh tải cùng trọng số mô hình vanilla trên tất cả máy chủ và có khả năng thực hiện các căn chỉnh khác nhau trên cùng máy chủ hoặc thậm chí cùng batch tại thời điểm suy luận.
Căn chỉnh theo ngữ cảnh cũng sẽ tạo điều kiện cho việc đánh giá nhanh các checkpoint mô hình trong quá trình huấn luyện trước, mà không cần tinh chỉnh mỗi checkpoint mô hình để tuân theo hướng dẫn. Việc tạo điều kiện cho đánh giá như vậy đối với quỹ đạo huấn luyện trước của các mô hình ngôn ngữ có thể cho chúng ta biết thêm về thời điểm kiến thức chính đằng sau việc tuân theo hướng dẫn xuất hiện.
Hơn nữa, việc có thể truy xuất một bộ cực kỳ nhỏ các ví dụ căn chỉnh hoạt động làm cho việc căn chỉnh minh bạch hơn (ví dụ: Bảng 4). Điều này có thể hữu ích cho các nhà phát triển bộ dữ liệu căn chỉnh, chỉ ra dữ liệu nguồn không phù hợp hoặc thiếu dữ liệu dưới một số chủ đề nhất định.
Các câu hỏi mở sau ghi chú này bao gồm liệu chúng ta có thể xây dựng RLHF dựa trên hoặc như căn chỉnh theo ngữ cảnh, cũng như cách hỗ trợ đối thoại đa lượt hoặc hướng dẫn với ngữ cảnh dài trong khung căn chỉnh theo ngữ cảnh của chúng ta.
Lời cảm ơn
Chúng tôi cảm ơn Omer Levy, Chunting Zhou, Weijia Shi, Jiacheng Liu, Alisa Liu, Sachin Kumar, và Yulia Tsvetkov cho cuộc thảo luận hữu ích.
Tài liệu tham khảo
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, T. J. Henighan, Nicholas Joseph, Saurav Kadavath, John Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Christopher Olah, Benjamin Mann, và Jared Kaplan. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. ArXiv, abs/2204.05862.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, và Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, và Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. ArXiv, abs/2305.14314.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, và Jacob Steinhardt. 2020. Measuring massive multitask language understanding. ArXiv, abs/2009.03300.
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, và Edouard Grave. 2021. Unsupervised dense information retrieval with contrastive learning. Trans. Mach. Learn. Res., 2022.
Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich'ard Nagyfi, ES Shahul, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, và Alexander Mattick. 2023. Openassistant conversations - democratizing large language model alignment. ArXiv, abs/2304.07327.
OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, và Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155.
Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed Huai hsin Chi, Denny Zhou, và Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. In Annual Meeting of the Association for Computational Linguistics.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur'elien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971.
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux,

--- TRANG 7 ---
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288.
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, và Hanna Hajishirzi. 2023. How far can camels go? exploring the state of instruction tuning on open resources. ArXiv, abs/2306.04751.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, và Omer Levy. 2023. Lima: Less is more for alignment. ArXiv, abs/2305.11206.
