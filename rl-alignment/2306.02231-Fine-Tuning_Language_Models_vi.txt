# 2306.02231.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2306.02231.pdf
# Kích thước tệp: 1595753 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Tinh chỉnh Mô hình Ngôn ngữ
với Căn chỉnh Chính sách Dẫn dắt bởi Lợi thế
Banghua Zhu†,⋄,⋆Hiteshi Sharma⋄,⋆Felipe Vieira Frujeri⋄Shi Dong⋄
Chenguang Zhu⋄Michael I. Jordan†,‡Jiantao Jiao†,‡
6 tháng 11, 2023
Tóm tắt
Học tăng cường từ phản hồi của con người (RLHF) đã nổi lên như một phương pháp đáng tin cậy để căn chỉnh
các mô hình ngôn ngữ lớn (LLM) với sở thích của con người. Trong số nhiều kỹ thuật RLHF, tối ưu hóa chính sách gần đúng (PPO) là một trong những phương pháp được sử dụng rộng rãi nhất. Mặc dù phổ biến, tuy nhiên, PPO có thể gặp phải sự sụp đổ chế độ, bất ổn và hiệu quả mẫu kém. Chúng tôi chỉ ra rằng những vấn đề này có thể được giảm thiểu bằng một thuật toán mới mà chúng tôi gọi là Căn chỉnh Chính sách Dẫn dắt bởi Lợi thế (APA),
sử dụng hàm mất mát lỗi bình phương dựa trên các lợi thế ước tính. Chúng tôi chứng minh thực nghiệm
rằng APA vượt trội hơn PPO một cách nhất quán trong các tác vụ ngôn ngữ với biên độ lớn, khi một
mô hình phần thưởng riêng biệt được sử dụng làm bộ đánh giá. Ngoài ra, so với PPO, APA cung cấp một hình thức kiểm soát ổn định hơn đối với độ lệch khỏi chính sách ban đầu của mô hình, đảm bảo rằng mô hình cải thiện
hiệu suất của nó mà không sụp đổ thành đầu ra xác định. Ngoài kết quả thực nghiệm, chúng tôi cũng cung cấp
một lý giải lý thuyết hỗ trợ thiết kế hàm mất mát của chúng tôi.

1 Giới thiệu
Học tăng cường từ phản hồi của con người (RLHF, hoặc học tăng cường dựa trên sở thích) (Knox
và Stone, 2008; Wirth et al., 2017) đã mang lại những thành công thực nghiệm đáng kể trong nhiều lĩnh vực, bao gồm
trò chơi (Christiano et al., 2017), robot (Sadigh et al., 2017; Kupcsik et al., 2018), hệ thống gợi ý
(Maghakian et al., 2022). Gần đây, RLHF cũng đã thể hiện tiềm năng nổi bật trong việc tích hợp kiến thức
con người với các mô hình ngôn ngữ lớn (Ziegler et al., 2019; Ouyang et al., 2022; OpenAI, 2023; Beeching
et al., 2023; Zhu et al., 2023; Bai et al., 2022b). Để sử dụng RLHF trong pipeline huấn luyện mô hình ngôn ngữ,
một giao thức phổ biến như sau.

•Tiền huấn luyện (PT): huấn luyện mô hình ngôn ngữ trên một lượng lớn dữ liệu văn bản không nhãn hoặc nhãn yếu
để tạo ra các đặc trưng và mẫu tổng quát có thể hữu ích cho các tác vụ downstream (Vaswani
et al., 2017; Devlin et al., 2018; Brown et al., 2020);

•Tinh chỉnh có giám sát (SFT): huấn luyện mô hình trên một lượng nhỏ dữ liệu được tuyển chọn để cải thiện
hiệu suất và độ chính xác của mô hình cho các tác vụ cụ thể;

•Học tăng cường với phản hồi của con người (RLHF): sử dụng tập dữ liệu có nhãn của con người cùng với
các thuật toán học tăng cường (RL) để căn chỉnh thêm mô hình với các giá trị hoặc sở thích phức tạp và chủ quan
của con người (Ziegler et al., 2019; Ouyang et al., 2022).

Cả PT và SFT đều dựa vào việc sử dụng các hàm mất mát phân phối, như entropy chéo, để giảm thiểu
khoảng cách giữa các phân phối văn bản trong tập dữ liệu huấn luyện và trong đầu ra mô hình (Vaswani et al., 2017;
Devlin et al., 2018; Brown et al., 2020). Tuy nhiên, một chiến lược đơn giản như vậy không khả thi cho giai đoạn RLHF.
Vì mục tiêu cuối cùng là làm cho đầu ra mô hình ngôn ngữ tuân theo các chuẩn mực ngôn ngữ của con người, điều này
∗Đóng góp ngang nhau.†Khoa Kỹ thuật Điện và Khoa học Máy tính, UC Berkeley‡Khoa
Thống kê, UC Berkeley ⋄Nhóm Kiến thức và Ngôn ngữ, Dịch vụ Nhận thức Azure, Microsoft Research. Công việc này
được thực hiện tại Microsoft khi Banghua Zhu là thực tập sinh nghiên cứu.

1arXiv:2306.02231v3 [cs.CL] 2 Nov 2023

--- TRANG 2 ---
khó định nghĩa hoặc định lượng, các nhà nghiên cứu thường sử dụng một mô hình phần thưởng được huấn luyện riêng biệt từ
mô hình ngôn ngữ trên một tập dữ liệu có nhãn của con người được thu thập tỉ mỉ (Ouyang et al., 2022). Mô hình phần thưởng như vậy tạo ra một điểm số vô hướng cho mỗi phản hồi được tạo, và do đó, có thể cung cấp cho
mô hình ngôn ngữ phản hồi trực tuyến. Khả năng tiếp cận phản hồi trực tuyến này cho phép mô hình ngôn ngữ được huấn luyện
thông qua học tăng cường (RL), dẫn đến giai đoạn RLHF.

Trong số các kỹ thuật RL được áp dụng cho mô hình ngôn ngữ, một trong những thuật toán nổi bật nhất
là tối ưu hóa chính sách gần đúng (PPO) (Schulman et al., 2017). Mặc dù hiệu quả được công nhận của PPO
(Ouyang et al., 2022; Stiennon et al., 2020; Nakano et al., 2021), nó gặp phải bất ổn và hiệu quả mẫu kém. Họ thuật toán gradient chính sách gặp phải hội tụ chậm và có thể tạo ra các chính sách cuối cùng kém
(Yuan et al., 2022; Dong et al., 2022).

Để giải quyết những vấn đề đó, chúng tôi giới thiệu một thuật toán mới, Căn chỉnh Chính sách Dẫn dắt bởi Lợi thế (APA),
sử dụng hàm mất mát lỗi bình phương trực tiếp căn chỉnh chính sách đầu ra của mô hình ngôn ngữ
với chính sách mục tiêu trong mỗi epoch huấn luyện. Chính sách mục tiêu kết hợp chính sách mô hình ngôn ngữ ban đầu
trước giai đoạn RLHF và một hạng tử hiệu chỉnh dựa trên hàm lợi thế được ước tính từ các mẫu trực tuyến.

Chúng tôi so sánh APA với PPO và hồi quy có trọng số lợi thế (AWR) cả về mặt lý thuyết và thực nghiệm.
Ở mức độ cao, hai thuật toán hiện có (PPO và AWR) giải quyết một bài toán tối ưu hóa chính sách có ràng buộc KL, dựa vào tỷ lệ quan trọng ước tính giữa các chính sách liên tiếp để tính gradient chính sách. Mặt khác, APA sử dụng lỗi bình phương để điều hòa độ lệch của chính sách mô hình và không còn cần ước tính tỷ lệ quan trọng. Chúng tôi chỉ ra rằng những khác biệt như vậy mang lại lợi ích lớn về hiệu quả mẫu.

Để chứng minh hiệu quả của APA một cách thực nghiệm, chúng tôi áp dụng APA, PPO và AWR để tinh chỉnh các
mô hình ngôn ngữ lên đến 7B, sử dụng tập dữ liệu Anthropic Helpfulness and Harmlessness có nhãn của con người (Ganguli et al.,
2022) và tập dữ liệu StackExchange Beeching et al. (2023). Chúng tôi đầu tiên đánh giá bằng mô hình phần thưởng,
được huấn luyện trên cùng tập dữ liệu để tạo ra phần thưởng vô hướng cho mỗi cặp prompt-phản hồi. Chúng tôi cũng đánh giá
sở thích của con người đối với mô hình ngôn ngữ kết quả bằng GPT-4 để chứng minh hiệu quả của
thuật toán. Kết quả thực nghiệm của chúng tôi làm nổi bật ba lợi thế chính của APA so với PPO:

(i) APA hiệu quả mẫu hơn. Được tinh chỉnh trên cùng số lượng mẫu, mô hình ngôn ngữ
thu được qua APA ghi điểm nhất quán cao hơn trên tập đánh giá so với mô hình thu được với PPO.

(ii) APA cung cấp kiểm soát ổn định hơn đối với độ lệch khỏi chính sách ban đầu của mô hình ngôn ngữ.
Được đo bằng phân kỳ KL, độ lệch của chính sách cuối cùng được tạo bởi APA có thể so sánh
với PPO, nhưng APA ít có khả năng gặp phải sự suy giảm hiệu suất đột ngột trong quá trình huấn luyện, điều này
thỉnh thoảng được quan sát trong PPO. Lưu ý rằng nghiên cứu trước đây đã chỉ ra rằng việc kiểm soát độ lệch
khỏi chính sách ban đầu là rất quan trọng trong việc ngăn chặn tối ưu hóa quá mức trên các mô hình phần thưởng (Gao et al., 2022).

(iii) APA có ít siêu tham số hơn. Hàm mất mát trong APA chỉ bao gồm một tham số chính có thể điều chỉnh
cho kiểm soát KL, trong khi ở PPO người ta phải hiệu chỉnh cẩn thận sự kết hợp của các
siêu tham số bổ sung khác nhau, như phạm vi cắt cho tỷ lệ quan trọng và ước tính giá trị, và
các hệ số của bộ điều khiển KL.

Rộng hơn, công trình này liên quan đến dòng tài liệu về việc tận dụng ý tưởng từ RL để cải thiện
hiệu suất của các mô hình ngôn ngữ. Một số ví dụ đáng chú ý trong tài liệu này bao gồm Paulus et al. (2017),
người đề xuất một hàm mất mát dựa trên mục tiêu gradient chính sách để giải quyết tác vụ tóm tắt trừu tượng,
sử dụng điểm ROUGE làm phần thưởng; và Snell et al. (2022), người trình bày thuật toán
học Q ngôn ngữ ngầm (ILQL) để tạo điều kiện học từ các mẫu có nhãn của con người ngoại tuyến mà không cần mô hình phần thưởng. Một
so sánh kỹ lưỡng giữa các thuật toán RL khác nhau cũng được thực hiện trong Ramamurthy et al. (2022) trên
các benchmark GRUE. Đã có một số khung RLHF thay thế thay thế PPO bằng SFT trên
mẫu tốt nhất được tạo (Yuan et al., 2023), hoặc học ngoại tuyến trực tiếp dựa trên sở thích (Rafailov et al., 2023).

Phần còn lại của bài báo này được tổ chức như sau. Trong Phần 2, chúng tôi giới thiệu ký hiệu của chúng tôi. Trong Phần
3, chúng tôi chính thức xác định thuật toán APA, và thảo luận về trực giác đằng sau các yếu tố thuật toán.
Kết quả thực nghiệm được trình bày trong Phần 4. Phần 5 kết luận bằng cách tóm tắt và thảo luận về
kết quả thực nghiệm.

2

--- TRANG 3 ---
2 Kiến thức nền tảng

Trong phần này, chúng tôi tổng quan về bối cảnh RL tiêu chuẩn trong Phần 2.1, và thảo luận về cách huấn luyện mô hình ngôn ngữ
phù hợp với bối cảnh này trong Phần 2.2. Chúng tôi sử dụng ký hiệu sau. Đối với số nguyên dương n, chúng tôi sẽ sử dụng
ký hiệu ngoặc [n] để chỉ tập các số nguyên {1, . . . , n}; đối với tập hữu hạn Z, chúng tôi ký hiệu bằng ∆(Z)
tập các phân phối xác suất trên Z, và |Z| là cardinality của Z. Chúng tôi sử dụng Bd để ký hiệu hình cầu đơn vị trong
không gian d chiều.

2.1 Học Tăng cường

Học tăng cường (RL) nắm bắt tương tác giữa một agent và một môi trường thông qua hình thức
của một quá trình quyết định Markov (MDP). Chúng tôi xem xét một MDP hữu hạn horizon được biểu diễn bởi một bộ M=
(S,A, H, P, r, ρ), trong đó S là không gian trạng thái hữu hạn, A là không gian hành động hữu hạn, H là horizon, P:S×A 7→ ∆(S)
là ma trận chuyển tiếp xác suất, r:S × A 7→ [0,1] là hàm phần thưởng, và ρ:S 7→ ∆(S) là phân phối trạng thái
ban đầu. Khi agent thực hiện hành động a trong trạng thái s tại bước h, nó nhận được phần thưởng vô hướng r(s, a), và
chuyển đến trạng thái s′, trong đó s′ được rút từ phân phối P(·|s, a). Mỗi episode bao gồm H bước
liên tiếp. Ở cuối một episode, agent được đặt lại về trạng thái được rút từ ρ(·), và một episode mới bắt đầu.

Một chính sách π:S 7→ ∆(A) là một hàm ánh xạ một trạng thái đến một phân phối trên các hành động. Hàm giá trị
Vπ:S 7→R của chính sách π được định nghĩa là tổng kỳ vọng của các phần thưởng được chiết khấu khi agent bắt đầu từ
trạng thái ban đầu s và tuân theo chính sách π trong suốt episode. Cho γ∈[0,1] là hệ số chiết khấu. Với bất kỳ
s∈ S, chúng ta có

Vπ(s):=E"HX
τ=0γτr(sτ, aτ)|s0=s, aτ∼π(·|sτ), sτ+1∼P(· |sτ, aτ)#
.

Cho một chính sách π, hàm giá trị trạng thái-hành động, còn được gọi là hàm Q, có thể được định nghĩa tương tự.
Với trạng thái s∈ S và a∈ A, chúng ta có

Qπ(s, a):=E"HX
τ=0γτr(sτ, aτ)|s0=s, a0=a, aτ∼π(·|sτ), sτ+1∼P(· |sτ, aτ)#
.

Chúng tôi cũng định nghĩa khái niệm quan trọng của hàm lợi thế. Với một chính sách π, trạng thái s và hành động a, lợi thế,
được định nghĩa là

Advπ(s, a) =Qπ(s, a)−Vπ(s),

định lượng giá trị bổ sung thu được bằng cách thay thế hành động ngay lập tức được quy định bởi π bằng hành động
a, khi agent ở trạng thái s tại bước h.

Chúng tôi cũng định nghĩa các đo lường chiếm đóng dπ
state:S 7→ [0,1] và dπ
action :S × A 7→ [0,1] là

dπ
state(s):=1
HHX
h=1P(sh=s|π) và dπ
action (s, a):=1
HHX
h=1P(sh=s, ah=a|π),

trong đó P(· |π) biểu thị rằng tất cả các hành động được rút từ π. Để tránh lộn xộn, chúng tôi nạp chồng ký hiệu dπ sao cho
dπ(s) chỉ dπ
state(s), và dπ(s, a) chỉ dπ
action (s, a).

2.2 Mô hình Ngôn ngữ như Agent Học Tăng cường

Ở dạng đơn giản nhất, một mô hình ngôn ngữ nhận đầu vào là một chuỗi token (x1, . . . , x n), và tạo ra một
phân phối trên token tiếp theo xn+1. Tất cả các token nằm trong một tập hữu hạn X. Bất cứ khi nào agent chọn một token
đại diện cho việc hoàn thành một phản hồi (ví dụ, token kết thúc chuỗi), hoặc tổng số token
đạt đến một giới hạn cụ thể, toàn bộ chuỗi được ghi điểm bởi một mô hình phần thưởng, tạo ra phần thưởng vô hướng r.

So sánh với công thức RL trong Phần 2.1, một mô hình ngôn ngữ có thể được xem như một agent
hoạt động trong một môi trường với không gian trạng thái S=SH
k=0Xk và không gian hành động A=X, trong đó H là số
token tối đa. Các chuyển tiếp luôn xác định, với trạng thái tiếp theo bằng sự nối của
tất cả các token trước đó và token hiện tại P(sh+1= (x1,···, xk)|sh= (x1,···, xk−1), ah=xk) = 1.

3

--- TRANG 4 ---
Theo truyền thống, mỗi episode bao gồm việc tạo ra một chuỗi hoàn chỉnh, và một phần thưởng chỉ được trao
khi một episode kết thúc. Trong bối cảnh này, tinh chỉnh tương đương với việc cải thiện chính sách agent π. Lĩnh vực
RL cung cấp một kho vũ khí đáng gờm cho nhiệm vụ này. Trong công trình này, chúng tôi sẽ tập trung vào các thuật toán RL dựa trên chính sách,
tham số hóa tập các chính sách agent bằng một tập tham số θ và tối ưu hóa trong không gian tham số.
Trong phần tiếp theo, chúng tôi sẽ bỏ qua chỉ số bước h, vì thông tin của nó đã được mã hóa trong mỗi trạng thái.

Chúng tôi lưu ý rằng hầu hết các mô hình ngôn ngữ dựa trên transformer ánh xạ một trạng thái (ngữ cảnh) s và một hành động (token tiếp theo) a
thành một logit qθ(s, a), và token tiếp theo được lấy mẫu theo phân phối được tạo bởi các logit
{qθ(s, a)}a∈A. Điều này tự nhiên dẫn đến tham số hóa chính sách mô hình ngôn ngữ sau:

πθ(a|s) =exp(qθ(s, a))P
a∈Aexp(qθ(s, a)).

3 Tinh chỉnh Dựa trên Học Tăng cường

Như đã đề cập trong Phần 1, giai đoạn RLHF thường bao gồm hai bước. Đầu tiên, một mô hình phần thưởng được
huấn luyện từ một tập dữ liệu có nhãn của con người. Sau đó, một thuật toán RL được áp dụng để cải thiện chính sách mô hình ngôn ngữ,
sử dụng các phần thưởng được tạo bởi mô hình phần thưởng. Ở đây chúng tôi tập trung chủ yếu vào bước thứ hai với một
hàm phần thưởng cho trước.

Chúng tôi tóm tắt một thuật toán RL dựa trên chính sách điển hình trong Thuật toán 1. Trong thực tế, việc cập nhật tham số
trong Phương trình (1) thường bao gồm nhiều bước gradient thay vì một việc tối thiểu hóa đầy đủ.

Thuật toán 1 Gradient Chính sách
1: Đầu vào: Một tham số chính sách ban đầu θ0, một hàm mất mát cho trước L(θ;D).
2: Đặt π0=πinit.
3: Với lần lặp t= 1,2···, T
4: Roll out πθt−1 để tạo ra tập dữ liệu Dt=n
(s(t)
1, a(t)
1, r(t)
1),···,(s(t)
n, a(t)
n, r(t)
n)o
5: Cập nhật tham số chính sách theo
θt= arg min
θL(θ;Dt). (1)

Trong phần còn lại của phần này, chúng tôi thảo luận về một số lựa chọn tiềm năng cho L(θ;D), mỗi lựa chọn nhắm đến mục tiêu
tối đa hóa các lợi thế được điều hòa. Chúng tôi cũng giới thiệu thuật toán mới APA, và thảo luận về trực giác
đằng sau nó.

Như một bước đầu tiên, đối với mỗi trạng thái cố định s, chúng tôi xem xét bài toán tối ưu hóa có điều hòa KL sau đây như
một mục tiêu cải thiện chính sách:

maximize
θF(θ;s, π) :=Ea∼πθ(·|s)[Advπ(s, a)]−λ·KL
πθ(· |s)

πinit(· |s)
. (2)

Ở đây πinit chỉ chính sách ban đầu của mô hình ngôn ngữ trước giai đoạn RLHF, π là một chính sách tùy ý
mà chúng ta hy vọng cải thiện. Hạng tử đầu tiên trong hàm mục tiêu F(θ;s, π) là một lợi thế kỳ vọng,
và để tối đa hóa lợi thế kỳ vọng, agent được khuyến khích di chuyển về phía hành động tối ưu trong trạng thái
s. Hạng tử thứ hai trong F(θ;s, π), một bộ điều hòa KL, kiểm soát độ lệch của πθ từ πinit. Việc điều hòa như vậy
là thiết yếu, vì các mô hình ngôn ngữ dễ bị tối ưu hóa quá mức khi các phần thưởng được tạo bởi một mô hình phần thưởng không hoàn hảo, một hiện tượng được quan sát trong Gao et al. (2022). Kết hợp lại, bài toán tối ưu hóa trạng thái đơn
trong (2) nhằm cải thiện chính sách π trong trạng thái s trong vùng lân cận của πinit.

Việc tối ưu hóa (2) thường được chia thành nhiều lần lặp. Trong mỗi lần lặp, chúng ta tối đa hóa
F(θ;s, πold), trong đó πold là chính sách mà agent đạt được trong lần lặp trước. Kỹ thuật này,
được gọi là Lặp Chính sách Bảo thủ (CPI), lần đầu tiên được trình bày trong Kakade và Langford (2002).
Việc tối ưu hóa sau đó được tổng quát hóa thành các phương pháp có ràng buộc và điều hòa KL được gọi là
Tối ưu hóa Chính sách Vùng Tin cậy (TRPO) (Schulman et al., 2015a) và Tối ưu hóa Chính sách Gần đúng
(PPO) (Schulman et al., 2017), tương ứng. Ngoài những phương pháp cốt lõi này, đã có một số

4

--- TRANG 5 ---
phương pháp tối ưu hóa chính sách khác được lấy cảm hứng từ (2), với một ví dụ đáng chú ý là phương pháp Hồi quy
Có trọng số Lợi thế (AWR) (Peng et al., 2019; Nair et al., 2020).

Trong phần tiếp theo, chúng tôi sẽ thảo luận về cách F(θ;s, π) được kết nối với hàm mất mát L(θ;D) trong
các thuật toán khác nhau, và đề xuất một bài toán tối ưu hóa gần đúng mới có nghiệm xấp xỉ nghiệm của
(2). Hàm mất mát trong APA sẽ dựa trên bài toán tối ưu hóa gần đúng mới này.

3.1 Tối ưu hóa chính sách gần đúng

PPO tận dụng lấy mẫu quan trọng để tránh lấy mẫu từ πθ, đạt được

Ea∼πθ(·|s)[Advπold(s, a)] =Ea∼πold(·|s)πθ(a|s)
πold(a|s)Advπold(s, a)
,

trong đó kỳ vọng ở vế phải có thể được ước tính một cách không thiên vị từ các mẫu hữu hạn.

PPO cũng bao gồm sự đổi mới sau: Thay vì phạt lợi thế kỳ vọng với
phân kỳ KL ước tính như trong (2), PPO trực tiếp trừ hạng tử phạt KL từ phần thưởng nhận được
bởi agent. Và người ta cũng có thể điều chỉnh thích ứng trọng số phạt λ dựa trên độ lệch của πθ từ
πinit (Schulman et al., 2017; Dhariwal et al., 2017; Ziegler et al., 2019). Phần thưởng có phạt KL sau đó
được sử dụng để ước tính một hàm lợi thế mới dAdv. Để tránh gradient không điều kiện tốt do các giá trị lớn hoặc
ước tính tỷ lệ quan trọng, PPO áp dụng cắt cho hàm mục tiêu. Hàm mất mát cuối cùng do đó là

LPPO(θ;D) =−1
|D|X
(s,a)∈Dminπθ(a|s)
πold(a|s)dAdv(s, a),clipπθ(a|s)
πold(a|s),1−ϵ,1 +ϵ
dAdv(s, a)
.

Lưu ý rằng hàm mất mát dựa vào các siêu tham số có thể điều chỉnh bổ sung. Việc cắt cũng làm cho bộ ước tính
thiên vị.

3.2 Hồi quy có trọng số lợi thế

Nếu không gian chính sách tham số hóa {πθ} chứa tất cả các chính sách có thể bao gồm chính sách thực tế, thì
bộ tối đa hóa của F(θ;s, πold)(2) sẽ tạo ra một chính sách π⋆ thỏa mãn

π⋆(a|s) =1
Z(s)πinit(a|s)·exp(Advπold(s, a)/λ), (3)

trong đó Z(s) =P
a′∈Aπinit(a′|s)·exp(Advπ(s, a′)/λ) là một hệ số chuẩn hóa. Trong trường hợp {πθ}
không chứa tất cả các chính sách, một cách tự nhiên để tối đa hóa F(θ;s, πold) là chiếu π⋆ onto {πθ} theo
phân kỳ KL. Từ (3),

KL
π⋆(a|s)∥πθ(a|s)
=−πinit(a|s)
Z(s)expAdvπold(s, a)
λ
log
πθ(a|s)
+C(s), (4)

trong đó C(s) là một hằng số không phụ thuộc vào θ.

Để tạo điều kiện cho việc cập nhật trực tuyến, AWR thực hiện ba thay đổi từ Phương trình (4):

•AWR thay thế chính sách vòng đầu πinit bằng chính sách vòng trước πold. Điều này đảm bảo rằng người ta
có thể sử dụng các mẫu roll-out mới từ chính sách vòng trước để xấp xỉ (4).

•Phân kỳ KL trong (4) chỉ tính cho một trạng thái s. AWR tối thiểu hóa một phân phối các trạng thái dπold.

•AWR xấp xỉ Z(s)≈1. Chúng tôi cũng cung cấp một thảo luận liên quan trong Phụ lục A về lý do tại sao
xấp xỉ như vậy được bảo đảm.

Những thay đổi này dẫn đến hàm mất mát được giới thiệu trong AWR:

LAWR(θ) =−E(s,a)∼dπoldh
exp
Advπold(s, a)/λ
log
πθ(a|s)i
. (5)

5

--- TRANG 6 ---
Cho một tập dữ liệu hữu hạn D={(si, ai) :i= 1, . . . , n} được lấy mẫu từ dπold, mất mát thực nghiệm tương ứng
có thể được viết là

LAWR(θ;D) =−1
|D|X
(s,a)∈Dexp
Advπold(s, a)/λ
log
πθ(a|s)
. (6)

Đối với trường hợp được xác định tốt khi họ tham số hóa {πθ} chứa chính sách tối thiểu hóa, bộ
tối thiểu hóa của mất mát quần thể như sau:

π′(a|s) =πold(a|s) exp( Advπold(s, a)/λ)P
aπold(a|s) exp( Advπold(s, a)/λ). (7)

Do sự khác biệt giữa mục tiêu ban đầu trong Phương trình (4) và mất mát cuối cùng trong Phương trình (5),
người ta có thể thấy rằng chính sách mà AWR hội tụ đến khác với mục tiêu ban đầu trong Phương trình (3).
Hơn nữa, vì πold thay đổi trong mỗi vòng, chính sách mà nó hội tụ đến tiếp tục thay đổi.

Như chúng tôi quan sát trong Phần 4 và Phụ lục D.3, AWR có thể không ổn định trong trường hợp trực tuyến do lý do này.
Điều này thúc đẩy chúng tôi giới thiệu APA, giảm thiểu vấn đề này, chứng minh hội tụ đến đúng mục tiêu trong
Phương trình (3), và thể hiện hiệu suất thực nghiệm tuyệt vời.

3.3 Căn chỉnh Chính sách Dẫn dắt bởi Lợi thế

Để chiếu chính sách tối ưu π⋆ trong (3) lên không gian chính sách tham số hóa, chúng tôi xem xét một khoảng cách khác
thay vì phân kỳ KL. Trong APA, chúng tôi sử dụng lỗi bình phương giữa log xác suất thay cho
phân kỳ KL:


logπ⋆(a|s)−logπθ(a|s)2=
logπθ(a|s) + log Z(s)−Advπold(s, a)/λ−logπinit(a|s)2
.

Tương tự như xấp xỉ trong AWR, chúng tôi cũng áp dụng Z(s)≈1, và tối thiểu hóa mất mát kỳ vọng dưới một phân phối trạng thái dπold trong mỗi vòng, dẫn đến mất mát quần thể sau:

LAPA(θ) =E(s,a)∼dπoldh
logπθ(a|s)−Advπold(s, a)/λ−logπinit(a|s)2i
. (8)

Mất mát thực nghiệm trên một tập dữ liệu hữu hạn D được lấy mẫu từ dπold do đó là

LAPA(θ;D) =1
|D|X
(s,a)∈D
logπθ(a|s)−Advπold(s, a)/λ−logπinit(a|s)2
. (9)

Giả sử rằng không gian tham số là Θ =Bd và không gian chính sách tham số hóa được xác định tốt
sao cho π⋆∈ {πθ|θ∈Θ}, trong đó π⋆ được định nghĩa trong Phương trình (3), chúng ta có thể thiết lập lý thuyết rằng
mất mát thực nghiệm là một thay thế hợp lý cho mất mát quần thể.

Định lý 1. Cho θ⋆∈arg minθ∈ΘLAPA(θ) là một bộ tối thiểu hóa của mất mát quần thể. Thì

πθ⋆(a|s) =π⋆(a|s),∀(s, a)∈supp(πold).

Hơn nữa, cho ˆθ∈arg minθ∈ΘLAPA(θ,D) là một bộ tối thiểu hóa mất mát thực nghiệm. Giả sử rằng min(πθ(a|
s), πinit(a|s))≥B1 và |Adv(s, a)| ≤B2 với bất kỳ s, a, và log(πθ) là L-Lipschitz theo θ
dưới chuẩn ℓ2 với bất kỳ s, a. Thì với tất cả δ >0, với xác suất ít nhất 1−δ, với một số hằng số toàn cục
C,

LAPA(ˆθ)≤CL(B2−log(B1))2r
dlog(nL/δ )
n.

Chứng minh được trì hoãn đến Phụ lục E. Từ định lý, chúng ta thấy rằng bộ tối thiểu hóa của quần thể
mất mát APA chính xác là chính sách mục tiêu π⋆ nếu chính sách πold được hỗ trợ trên tất cả các cặp trạng thái-hành động. Ngược lại, như
chúng tôi đã thảo luận trước đó, các tính chất hội tụ của thuật toán PPO và AWR chưa được thiết lập.

Chúng tôi cũng cung cấp các diễn giải thay thế của mất mát được đề xuất về mặt f-divergence và soft-Q
learning trong Phụ lục B.

6

--- TRANG 7 ---
4 Kết quả Thực nghiệm

Trong việc triển khai tất cả các thuật toán mà chúng tôi kiểm tra, bao gồm APA, chúng tôi định nghĩa hàm lợi thế
là Advπold(s, a), được ước tính từ dữ liệu. Chúng tôi sử dụng cùng phương pháp ước tính lợi thế tổng quát
để ước tính lợi thế như đã thảo luận trong các công trình trước đó Mnih et al. (2016); Schulman et al. (2015b). Cụ thể, đối với rollout (s0, a0, r0, s1, a1, r1,···, sT−1, aT−1, rT−1, sT), bộ ước tính lợi thế tổng quát
là

ˆAπold(st, at) =δt+λγδt+1+···+ (λγ)T−1δT−1,

trong đó δt=r(st, at) +γVπold(st+1)−Vπold(st).

Ở đây hàm giá trị là một mạng độc lập khác mà chúng tôi fit trong suốt quá trình huấn luyện với một
mất mát bình phương, ˆLV(D) =P
si,ai(V(si)−ˆAπold(si, ai)−Vπold(si))2. Do đó hàm mất mát tổng thể là

LAPA
θ(D) =ˆLAPA(D) +η·ˆLV(D)
LAWR
θ(D) =ˆLAWR(D) +η·ˆLV(D).

Đối với việc triển khai PPO, chúng tôi sử dụng phiên bản PPO2 từ Dhariwal et al. (2017), với bộ điều khiển
KL thích ứng từ Ziegler et al. (2019). Chúng tôi triển khai PPO với cùng siêu tham số như
triển khai trong trlX1, cũng tuân theo các siêu tham số mặc định được đề xuất bởi Schulman et al. (2017).
Sự khác biệt chính giữa phiên bản PPO của chúng tôi và trong trlX là chúng tôi tạo một mạng giá trị hoàn toàn riêng biệt thay vì tạo một đầu giá trị trên đỉnh mô hình ngôn ngữ. Trong APA, chúng tôi lấy λ= 0.1
để áp đặt một ràng buộc yếu hơn trên hệ số KL. Đối với AWR, chúng tôi thấy rằng đặt λ= 0.1 dẫn đến
bùng nổ của mất mát; do đó chúng tôi lấy λ= 1 để ổn định huấn luyện.

4.1 Kết quả trên tập dữ liệu StackExchange

Trong phần này, chúng tôi trình bày kết quả thực nghiệm của chúng tôi với tập dữ liệu StackExchange. Tập dữ liệu này bao gồm
các câu hỏi và câu trả lời tương ứng từ nền tảng StackExchange (bao gồm StackOverflow cho
code và nhiều chủ đề khác). Các câu trả lời sau đó được bình chọn bởi người dùng trên nền tảng và một câu trả lời được chấp nhận được gắn nhãn. Theo Beeching et al. (2023); Askell et al. (2021), chúng tôi gán một điểm cho mỗi câu trả lời
tùy thuộc vào số lượt upvote:

score =

−1, upvotes ≤0,
1 +⌊log2(1 +upvotes ) + 0.5⌋,nếu người hỏi chấp nhận câu trả lời,
⌊log2(1 +upvotes ) + 0.5⌋,ngược lại.

Hình 1: Tỷ lệ thắng được tính bởi GPT-4 cho
tập dữ liệu StackExchange cho các mô hình được huấn luyện bởi SFT,
PPO và APA. So với SFT và PPO, các mô hình được huấn luyện APA tạo ra phản hồi tốt hơn. Chúng tôi đã sử dụng tập dữ liệu được tiền xử lý được cung cấp trong
Beeching et al. (2023) cho tất cả mục đích huấn luyện SFT, mô hình hóa phần thưởng và RL, có sẵn
trong HuggingFace Datasets là lvwerra/stack-exchange-paired2. Chúng tôi sử dụng các mô hình LLaMA-7B Touvron et al. (2023)
cho thí nghiệm này. Chúng tôi sử dụng phương pháp Low-Rank Adaptation
(LoRA) Hu et al. (2021) để giảm
tiêu thụ bộ nhớ trong khi huấn luyện. Chúng tôi đã sử dụng
8xA100 GPU cho các thí nghiệm của chúng tôi. Các siêu tham số cho thí nghiệm này được liệt kê trong
Phụ lục C.4. Hình 2 hiển thị phần thưởng ở
bên trái và phân kỳ KL từ chính sách ban đầu
cho ba thuật toán, PPO, APA và AWR.
Chúng tôi điều chỉnh các siêu tham số để đạt được các giá trị phân kỳ KL tương tự, cho phép chúng tôi so sánh

1https://github.com/CarperAI/trlx
2https://huggingface.co/datasets/lvwerra/stack-exchange-paired

7

--- TRANG 8 ---
phần thưởng cho các thuật toán khác nhau. Trong trường hợp AWR,
mỗi tập siêu tham số đều hiển thị một mức độ bất ổn nào đó. Rõ ràng, APA nhanh chóng hội tụ đến
phần thưởng cao hơn PPO và AWR.

Hình 2: So sánh hiệu suất của ba phương pháp trên tập dữ liệu StackExchange. Trái: Trục x
biểu thị tổng số bước, tỷ lệ thuận với lượng dữ liệu được sử dụng trong quy trình huấn luyện. Trục y
là phần thưởng được tính bởi cùng mô hình phần thưởng trong quá trình huấn luyện. Phải: Trục x biểu thị
tổng số bước. Trục y là phân kỳ KL giữa mô hình được huấn luyện và mô hình ban đầu.

Đánh giá GPT-4: Chúng tôi thực hiện đánh giá GPT-4 để đánh giá các mô hình được huấn luyện bởi các phương pháp RL khác nhau
trên tập dữ liệu StackExchange. GPT-4 so sánh các đầu ra được tạo bởi hai mô hình, sử dụng một tham chiếu
(đã chọn) làm cơ sở so sánh. Hình 1 hiển thị tỷ lệ thắng cho việc so sánh SFT vs PPO, SFT
vs APA và PPO vs APA. APA vượt trội nhất quán so với hai mô hình kia.

4.2 Kết quả trên tập dữ liệu HH

Trong phần này, chúng tôi so sánh PPO, AWR và APA trên tập dữ liệu Helpfulness and Harmlessnes (HH) có nhãn của con người
từ Bai et al. (2022a).3. Chúng tôi tinh chỉnh ba mô hình, bao gồm Dahoas/pythia-125M-static-sft,4
Dahoas/pythia-1B-static-sft,5 và Dahoas/pythia-6B-static-sft.6 Tất cả các mô hình đã trải qua
tinh chỉnh có giám sát với các cặp prompt-phản hồi có nhãn, tương tự như giao thức trong Ouyang et al. (2022)
và Ramamurthy et al. (2022). Chúng tôi trình bày hiệu suất của các thuật toán RL cho pythia-125M và
pythia-1B trong Hình 3. Nó cho thấy rằng sau một số bước, hiệu suất của PPO bắt đầu xấu đi trong khi APA
và AWR ổn định. APA đạt được phần thưởng cao hơn trong khi duy trì phân kỳ KL từ chính sách
ban đầu nhỏ hơn. Chi tiết về các siêu tham số được sử dụng cho huấn luyện và kết quả bổ sung cho các mô hình lớn hơn
được thảo luận trong Phụ lục C.1.

5 Kết luận

Trong bài báo này, chúng tôi nghiên cứu bài toán tối ưu hóa chính sách trực tuyến trong RLHF. Chúng tôi đánh giá hiệu suất của
các thuật toán hiện có PPO và AWR, và giới thiệu một phương pháp mới, APA, có bảo đảm hội tụ lý thuyết và mang lại một số lợi thế so với các thuật toán hiện có. Những điểm chính từ nghiên cứu của chúng tôi có thể
được tóm tắt như sau.

3https://huggingface.co/datasets/Dahoas/static-hh
4https://huggingface.co/Dahoas/pythia-125M-static-sft
5https://huggingface.co/Dahoas/pythia-1B-static-sft
6https://huggingface.co/Dahoas/pythia-6B-static-sft

8

--- TRANG 9 ---
0 2500 5000 7500 10000 12500 15000 17500 20000
steps3.0
2.5
2.0
1.5
1.0
0.5
rewardTối ưu hóa Chính sách Trực tuyến với mô hình 125M
PPO
AWR
APA
2.5 5.0 7.5 10.0 12.5 15.0 17.5
steps / k0.00.51.01.52.02.53.03.5KLPhân kỳ KL giữa chính sách được huấn luyện và chính sách ban đầu cho mô hình 125M
PPO
AWR
APA
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
steps / k2.0
1.8
1.6
1.4
1.2
1.0
0.8
0.6
rewardTối ưu hóa Chính sách Trực tuyến với mô hình 1B
PPO
AWR
APA
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
steps / k0.00.20.40.60.81.01.21.4KLPhân kỳ KL giữa chính sách được huấn luyện và chính sách ban đầu cho mô hình 1B
PPO
AWR
APA

Hình 3: So sánh hiệu suất của ba phương pháp trên tập dữ liệu HH. Trái: Trục x biểu thị
tổng số bước, tỷ lệ thuận với lượng dữ liệu được sử dụng trong quy trình huấn luyện. Trục y là
phần thưởng được đánh giá bởi cùng mô hình phần thưởng. Phải: Trục x biểu thị tổng số bước. Trục y là
phân kỳ KL giữa mô hình được huấn luyện và mô hình ban đầu.

9

--- TRANG 10 ---
Tính ổn định. Như chúng tôi đã thảo luận trong Phần 1, một trong những thách thức trong RLHF là tính bất ổn. Việc áp đặt kiểm soát về độ phân kỳ giữa các chính sách mới và chính sách ban đầu sau SFT là rất quan trọng trong
các thuật toán RL. Tuy nhiên, việc cắt của hàm mục tiêu và bộ điều khiển KL thích ứng có thể làm cho hành vi của
PPO bất ổn; đối với AWR, việc cập nhật trong (7), trọng số lại chính sách trước đó bằng một hệ số
nhân trong mỗi lần lặp, cũng có những hệ quả không rõ. APA, mặt khác, chứng minh hội tụ đến π⋆ khi
hàm lợi thế được cố định, gần với chính sách ban đầu trong phân kỳ KL. Từ kết quả thực nghiệm, chúng ta thấy rằng APA có thể cung cấp kiểm soát KL tốt hơn và dễ điều chỉnh bằng cách điều chỉnh rõ ràng
siêu tham số λ. Các thí nghiệm của chúng tôi tiết lộ các mức độ bất ổn khác nhau cho PPO và AWR. Cụ thể,
PPO gặp phải sự suy giảm hiệu suất thỉnh thoảng bất cứ khi nào chính sách mô hình phân kỳ quá nhiều từ
chính sách ban đầu πinit, và hiệu ứng như vậy rõ rệt hơn đối với các mô hình nhỏ hơn. Chúng tôi quy điều này cho bộ điều khiển KL
trong PPO. Trong Phụ lục D, chúng tôi chứng minh rằng PPO có thể đạt được hiệu quả mẫu tương tự như
APA mà không có phạt KL, mặc dù phải trả giá bằng hiệu quả KL yếu hơn.

Hiệu quả mẫu. Với cùng mức độ kiểm soát về phân kỳ KL, APA cho thấy hiệu quả mẫu cao hơn
so với PPO và AWR. Một giải thích có thể là trong cả PPO và AWR, cải thiện chính sách phụ thuộc quan trọng
vào việc sử dụng các mẫu hữu hạn để tái tạo chính sách lấy mẫu πold, trong khi ở APA, việc tối thiểu hóa
mất mát quần thể (8) phụ thuộc ít hơn vào việc tái tạo πold. Thực tế, mất mát quần thể APA (8) có thể được
tối thiểu hóa hiệu quả miễn là tập dữ liệu D có độ bao phủ tốt về các cặp trạng thái-hành động được
thăm thường xuyên bởi πold. Để thảo luận thêm về hiệu quả mẫu, vui lòng tham khảo Phụ lục B.

Học trực tuyến vs ngoại tuyến. Các thí nghiệm của chúng tôi chủ yếu kiểm tra trường hợp trực tuyến, nơi dữ liệu mới có thể được
tạo ra trong quá trình huấn luyện. Bối cảnh ngoại tuyến, nơi một tập dữ liệu cố định được cho và các mẫu mới không
có sẵn, có thể mang lại kết quả khác về chất. Cụ thể, giả sử rằng tập dữ liệu ngoại tuyến bao gồm
các rollout từ một chính sách πoff. Trong trường hợp này, nếu được huấn luyện với vô số mẫu, AWR sẽ
hội tụ đến chính sách được chỉ định trong (3). Tuy nhiên, hiệu suất của APA có thể gặp khó khăn do sự dịch chuyển phân phối
vì nó chỉ có thể học từ các cặp trạng thái-hành động được bao phủ bởi πoff, và không có bảo đảm rằng chính sách đã học
hoạt động tốt trên các cặp trạng thái-hành động được thăm bởi chính sách hiện tại. Sự không khớp phân phối như vậy có thể
dẫn đến sự sụt giảm hiệu suất đáng kể cho APA, như chúng tôi quan sát trong Phụ lục D.3. Chúng tôi cũng quan sát rằng AWR
thường vượt trội hơn ILQL trong học ngoại tuyến, mặc dù cả hai đều hoạt động kém với các mô hình lớn hơn.

10

--- TRANG 11 ---
Tài liệu tham khảo
A. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma,
N. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. Brown,
J. Clark, S. McCandlish, C. Olah, and J. Kaplan. A general language assistant as a laboratory for
alignment, 2021. 4.1

Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan,
et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv
preprint arXiv:2204.05862 , 2022a. 4.2

Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini,
C. McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073 ,
2022b. 1

E. Beeching, Y. Belkada, K. Rasul, L. Tunstall, L. von Werra, N. Rajani, and N. Lambert. StackLLaMA:
An RL fine-tuned LLaMA model for Stack Exchange question and answering, 2023. URL https://
huggingface.co/blog/stackllama . 1, 4.1, 4.1

T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell, et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 , 2020. 1

P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from
human preferences. In Advances in Neural Information Processing Systems , pages 4299–4307, 2017. 1

J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers
for language understanding. arXiv preprint arXiv:1810.04805 , 2018. 1

P. Dhariwal, C. Hesse, O. Klimov, A. Nichol, M. Plappert, A. Radford, J. Schulman, S. Sidor, Y. Wu, and
P. Zhokhov. OpenAI baselines. https://github.com/openai/baselines , 2017. 3.1, 4

S. Dong, B. Van Roy, and Z. Zhou. Simple agent, complex environment: Efficient reinforcement learning
with agent states. Journal of Machine Learning Research , 23(255):1–54, 2022. 1

D.Ganguli, L.Lovitt, J.Kernion, A.Askell, Y.Bai, S.Kadavath, B.Mann, E.Perez, N.Schiefer, K.Ndousse,
et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.
arXiv preprint arXiv:2209.07858 , 2022. 1

L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. arXiv preprint
arXiv:2210.10760 , 2022. (ii), 3

E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora: Low-rank
adaptation of large language models, 2021. 4.1

S. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In Proceedings of
the Nineteenth International Conference on Machine Learning , pages 267–274, 2002. 3

W. B. Knox and P. Stone. TAMER: Training an agent manually via evaluative reinforcement. In 7th IEEE
International Conference on Development and Learning , pages 292–297. IEEE, 2008. 1

A. Kupcsik, D. Hsu, and W. S. Lee. Learning dynamic robot-to-human object handover from human
feedback. In Robotics research , pages 161–176. Springer, 2018. 1

J. Maghakian, P. Mineiro, K. Panaganti, M. Rucker, A. Saran, and C. Tan. Personalized reward learning
with interaction-grounded learning (IGL). arXiv preprint arXiv:2211.15823 , 2022. 1

V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.
Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning ,
pages 1928–1937. PMLR, 2016. 4

11

--- TRANG 12 ---
A. Nair, A. Gupta, M. Dalal, and S. Levine. Awac: Accelerating online reinforcement learning with offline
datasets. arXiv preprint arXiv:2006.09359 , 2020. 3

R.Nakano, J.Hilton, S.Balaji, J.Wu, L.Ouyang, C.Kim, C.Hesse, S.Jain, V.Kosaraju, W.Saunders, etal.
WebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332 ,
2021. 1

OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774 , 2023. 1

L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,
A. Ray, et al. Training language models to follow instructions with human feedback. arXiv preprint
arXiv:2203.02155 , 2022. 1, 4.2

R. Paulus, C. Xiong, and R. Socher. A deep reinforced model for abstractive summarization. arXiv preprint
arXiv:1705.04304 , 2017. 1

X. B. Peng, A. Kumar, G. Zhang, and S. Levine. Advantage-weighted regression: Simple and scalable
off-policy reinforcement learning. arXiv preprint arXiv:1910.00177 , 2019. 3

R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization:
Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290 , 2023. 1

R. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hessel, R. Sifa, C. Bauckhage, H. Hajishirzi, and Y. Choi.
Isreinforcementlearning(not)fornaturallanguageprocessing? benchmarks,baselines,andbuildingblocks
for natural language policy optimization. arXiv preprint arXiv:2210.01241 , 2022. 1, 4.2

D. Sadigh, A. D. Dragan, S. Sastry, and S. A. Seshia. Active preference-based learning of reward functions.
InRobotics: Science and Systems , 2017. 1

J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In
International Conference on Machine Learning , pages 1889–1897. PMLR, 2015a. 3

J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using
generalized advantage estimation. arXiv preprint arXiv:1506.02438 , 2015b. 4

J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347 , 2017. 1, 3, 3.1, 4

C. Snell, I. Kostrikov, Y. Su, M. Yang, and S. Levine. Offline RL for natural language generation with
implicit language Q-learning. arXiv preprint arXiv:2206.11871 , 2022. 1

N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano.
Learning to summarize with human feedback. Advances in Neural Information Processing Systems , 33:
3008–3021, 2020. 1

H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal,
E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient
foundation language models, 2023. 4.1

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.
Attention is all you need. Advances in Neural Information Processing Systems , 30, 2017. 1

C. Wirth, R. Akrour, G. Neumann, and J. Fürnkranz. A survey of preference-based reinforcement learning
methods. The Journal of Machine Learning Research , 18(1):4945–4990, 2017. 1

R. Yuan, R. M. Gower, and A. Lazaric. A general sample complexity analysis of vanilla policy gradient.
Proceedings of the 25th International Conference on Artificial Intelligence and Statistics (AISTATS) , 2022.
1

Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. Rrhf: Rank responses to align language
models with human feedback without tears. arXiv preprint arXiv:2304.05302 , 2023. 1

12

--- TRANG 13 ---
B. Zhu, J. Jiao, and M. I. Jordan. Principled reinforcement learning with human feedback from pairwise or
k-wise comparisons. International Conference on Machine Learning , 2023. 1

D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving.
Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019. 1, 3.1, 4

13

--- TRANG 14 ---
A Lập luận cho Z(s)≈1

Lưu ý rằng trong cả hồi quy có trọng số lợi thế và mất mát bình phương dựa trên lợi thế, chúng tôi xấp xỉ Z(s)
bằng 1. Ở đây chúng tôi biện minh tại sao điều này không làm tổn hại hiệu suất.

Xem xét một kịch bản vô cùng nhỏ nơi |Adv/λ| ≪ | logπinit|. Trong kịch bản mô hình ngôn ngữ, điều này
thường đúng vì πinit được hỗ trợ trên khoảng 50k token riêng biệt và có thể rất gần với không,
trong khi Adv/λ có thể được điều chỉnh thành số nhỏ bằng cách điều chỉnh λ.

Trong trường hợp này, chúng ta có

Z(s) =X
a∈Aπinit(a|s) exp( Adv(s, a)/λ)
=Ea∼πinit[exp( Adv(s, a)/λ)]
=Ea∼πinit[1 +Adv(s, a)/λ+o(Adv2(s, a)/λ2)].

Lợi thế này thường được ước tính là Advπold, có thể gần với Advπinit. Và chúng ta có

Ea∼πinit[Advπold(s, a)/λ]≈Ea∼πinit[Advπinit(s, a)/λ] = 0.

Do đó chúng ta biết rằng

Z(s)≈1 +Ea∼πinit[o(Adv2(s, a)/λ2)]≈1.

Trong thực tế, chúng tôi quan sát rằng mất mát bình phương giảm rất chậm do tốc độ học nhỏ (8e−6).
Điều này cho thấy rằng chính sách thay đổi rất chậm, đây là một lý do khác tại sao hệ số chuẩn hóa không
quan trọng.

B Diễn giải Thay thế của APA

Nhớ lại rằng APA có thể được viết là

LAPA(θ) =E(s,a)∼dπoldh
logπθ(a|s)−logπ⋆(a|s)2i
,

trong đó π⋆=πinit·exp(Adv/λ). Trong trường hợp khi πold gần với πθ, việc tối thiểu hóa mất mát bình phương trong APA
tương đương với việc tối thiểu hóa khoảng cách sau giữa π⋆ và πθ:

d(π⋆(· |s), πθ(· |s)) =X
aπθ(a|s)
logπθ(a|s)
π⋆(a|s)2
.

Điều này có thể được xem như một f-divergence mới với f(x) =xlog2(x). Chúng ta có thể chỉ ra bằng Cauchy-Schwarz rằng đây
luôn là một cận trên cho phân kỳ KL:

d(π⋆(· |s), πθ(· |s)) = X
aπθ(a|s)! X
aπθ(a|s)
logπθ(a|s)
π⋆(a|s)2!
≥X
aπθ(a|s)logπθ(a|s)
π⋆(a|s)
≥X
aπθ(a|s) logπθ(a|s)
π⋆(a|s)
.

C Thí nghiệm Bổ sung

C.1 Kết quả trên tập dữ liệu HH

Trong tập dữ liệu này, mỗi mục bao gồm một prompt, một phản hồi được chọn và một phản hồi bị từ chối được con người gắn nhãn
để đánh giá tính hữu ích và vô hại của các phản hồi. Đối với mô hình phần thưởng, chúng tôi sử dụng
mô hình phần thưởng proxy Dahoas/gptj-rm-static7 với 6B tham số được huấn luyện từ cùng tập dữ liệu dựa trên
EleutherAI/gpt-j-6b.8 Đối với cả ba thuật toán, chúng tôi chạy hai epoch cập nhật sau khi tạo 64 phản hồi
từ các prompt được lấy mẫu ngẫu nhiên. Đối với mô hình 125M, chúng tôi sử dụng batch size 8 và learning rate 8×10−6. Đối
với mô hình 1B, chúng tôi sử dụng batch size 2 và learning rate 10−6. Đối với mô hình 6B và lớn hơn, chúng tôi sử dụng batch size
1 và learning rate 10−6. Chúng tôi sử dụng GPU Nvidia V100 32GB để tinh chỉnh mô hình 125M và 1B, và GPU
AMD Mi200 64GB để tinh chỉnh mô hình 6B và lớn hơn. Độ dài phản hồi tối đa được đặt
là 128 token, và độ dài chuỗi tổng tối đa được đặt là 1024 token. Chúng tôi unfreeze hai lớp cuối
trong quá trình tinh chỉnh. Với mỗi thí nghiệm, chúng tôi chạy tổng cộng 20k bước. Kết quả được vẽ như dưới đây.

Ở bên trái Hình 4, chúng tôi so sánh ba phương pháp trên tập dữ liệu HH. Đối với cả ba mô hình, chúng tôi lặp lại

0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
steps / k2.4
2.2
2.0
1.8
1.6
1.4
1.2
rewardTối ưu hóa Chính sách Trực tuyến với mô hình 6B
PPO
AWR
APA
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
steps / k0.00.20.40.60.81.0KLPhân kỳ KL giữa chính sách được huấn luyện và chính sách ban đầu cho mô hình 6B
PPO
AWR
APA

Hình 4: So sánh hiệu suất của ba phương pháp trên tập dữ liệu HH. Trái: Trục x biểu thị
tổng số bước, tỷ lệ thuận với lượng dữ liệu được sử dụng trong quy trình huấn luyện. Trục y là
phần thưởng được đánh giá bởi cùng mô hình phần thưởng. Phải: Trục x biểu thị tổng số bước. Trục y là
phân kỳ KL giữa mô hình được huấn luyện và mô hình ban đầu.

thí nghiệm với ba seed ngẫu nhiên 0,100,1000, và vẽ min, mean và max của chúng. Chúng ta thấy rằng với
cùng lượng dữ liệu, APA có thể đạt được phần thưởng cao nhất trong cả ba trường hợp. Chúng tôi cũng quan sát rằng
PPO trở nên ổn định hơn với các mô hình lớn, có thể do batch size nhỏ hơn, hoặc khả năng đạt được
phần thưởng cao hơn với độ lệch nhỏ hơn trong phân kỳ KL.

Ở bên phải Hình 4, chúng tôi hiển thị cách phân kỳ KL giữa chính sách hiện tại và chính sách ban đầu
thay đổi như một hàm của quá trình huấn luyện cho ba seed. Chúng ta có thể thấy rằng đối với cả ba mô hình,
APA cung cấp kiểm soát KL tương tự hoặc tốt hơn so với PPO và AWR, mặc dù chúng tôi lưu ý rằng đối với mô hình 6B, kiểm soát KL
cho PPO hơi tốt hơn APA. Kết hợp với phần bên trái của hình, chúng ta có thể thấy rằng
APA hiệu quả KL hơn so với PPO và AWR; tức là, nó đạt được hiệu suất tốt hơn trên mô hình phần thưởng
dưới cùng phân kỳ KL.

Chúng tôi bao gồm thêm kết quả thí nghiệm trong Phụ lục C, nơi chúng tôi tinh chỉnh databricks/dolly-v2-7b9 trên
cùng tập dữ liệu HH, và các mô hình 2.7B và 6B trên tập dữ liệu TLDR10 cho tác vụ tóm tắt.

Chúng tôi cũng thực hiện các nghiên cứu ablation về hiệu ứng của bộ điều khiển KL thích ứng trên PPO và hiệu ứng của
các lựa chọn λ khác nhau cho AWR; xem Phụ lục D. Chúng tôi chỉ ra trong Phụ lục D.2 rằng không có kiểm soát KL, PPO
có thể hiệu quả mẫu như APA, nhưng kém hiệu quả KL hơn. Chúng tôi cũng quan sát tính bất ổn ngay cả khi không có
bộ điều khiển KL. Mặt khác, chúng tôi quan sát rằng việc thay đổi λ cung cấp một sự đánh đổi đơn giản giữa kiểm soát KL
và hiệu suất trong APA.

7https://huggingface.co/Dahoas/gptj-rm-static
8https://huggingface.co/EleutherAI/gpt-j-6b
9https://huggingface.co/databricks/dolly-v2-7b
10https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons

15

--- TRANG 15 ---
C.2 Kết quả trên Tập dữ liệu TLDR

Chúng tôi tinh chỉnh các mô hình EleutherAI/gpt-neo-2.7B11 và 6B CarperAIopenai_summarize_tldr_sft12
trên tập dữ liệu TLDR13 cho tác vụ tóm tắt. Đối với EleutherAI/gpt-neo-2.7B, chúng tôi đầu tiên tinh chỉnh nó
với tinh chỉnh có giám sát trên phản hồi có nhãn trong cùng tập dữ liệu tóm tắt, và chạy RLHF trên
chính sách được tinh chỉnh có giám sát. Mô hình 6B CarperAIopenai_summarize_tldr_sft đã trải qua
giai đoạn tinh chỉnh có giám sát. Mô hình phần thưởng là mô hình phần thưởng EleutherAI/gpt-j-6b14 được tiền huấn luyện
cho tập dữ liệu tóm tắt CarperAI/openai_summarize_comparisons15.

Chúng tôi tuân theo cài đặt mặc định trong trlX với seed 0 và 100, và vẽ kết quả trong Hình 5. Người ta có thể thấy
rằng APA hiệu quả mẫu hơn và cung cấp kiểm soát KL tốt hơn so với PPO trong cả mô hình 2.7B và 6B.

0 2 4 6 8 10
steps / k2.02.12.22.32.42.52.62.72.8rewardTối ưu hóa Chính sách Trực tuyến với mô hình 2.7B
PPO
AWR
APA
0 2 4 6 8
steps / k0.00.51.01.52.02.5KLPhân kỳ KL giữa chính sách được huấn luyện và chính sách ban đầu trong mô hình 2.7B
PPO
AWR
APA
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
steps / k2.42.62.83.03.23.43.6rewardTối ưu hóa Chính sách Trực tuyến với mô hình 6B
PPO
AWR
APA
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
steps / k0.20.30.40.50.60.70.8KLPhân kỳ KL giữa chính sách được huấn luyện và chính sách ban đầu trong mô hình 6B
PPO
AWR
APA

Hình 5: So sánh hiệu suất trên tập dữ liệu TLDR. Trái: Trục x biểu thị tổng số bước,
tỷ lệ thuận với số lượng dữ liệu được sử dụng trong quy trình huấn luyện. Trục y là phần thưởng
được đánh giá bởi cùng mô hình phần thưởng. Phải: Trục x biểu thị tổng số bước. Trục y là
phân kỳ KL giữa mô hình được huấn luyện và mô hình ban đầu.

11https://huggingface.co/EleutherAI/gpt-neo-2.7B
12https://huggingface.co/CarperAI/openai_summarize_tldr_sft
13https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons
14https://huggingface.co/EleutherAI/gpt-j-6b
15https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons

16

--- TRANG 16 ---
C.3 Kết quả trên Mô hình Dolly

Chúng tôi tinh chỉnh mô hình databricks/ dolly-v2-7b16 trên tập dữ liệu HH. Chúng tôi tuân theo cài đặt mặc định trong
trlX với seed 0 và 100, và vẽ kết quả trong Hình 6. Chúng tôi chỉ bao gồm kết quả cho APA và PPO
vì AWR giảm trực tiếp. Khác với tất cả các thí nghiệm khác, ở đây cho APA chúng tôi đặt λ= 1 thay vì 0.1
để ổn định huấn luyện và áp đặt kiểm soát KL mạnh hơn. Người ta có thể thấy rằng APA vẫn có thể cải thiện so với
mô hình dolly 7B ban đầu và cung cấp kiểm soát KL tốt hơn, trong khi PPO không mang lại cải thiện thêm.

0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
steps / k1.6
1.5
1.4
1.3
1.2
rewardTối ưu hóa Chính sách Trực tuyến với mô hình Dolly 7B
PPO
APA
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
steps / k0.000.020.040.060.080.10KLPhân kỳ KL giữa chính sách được huấn luyện và chính sách ban đầu trong mô hình dolly 7B
PPO
APA

Hình 6: So sánh hiệu suất trên mô hình dolly 7B. Trái: Trục x biểu thị tổng số
bước, tỷ lệ thuận với số lượng dữ liệu được sử dụng trong quy trình huấn luyện. Trục y là phần thưởng
được đánh giá bởi cùng mô hình phần thưởng. Phải: Trục x biểu thị tổng số bước. Trục y là
phân kỳ KL giữa mô hình được huấn luyện và mô hình ban đầu.

C.4 Chi tiết thí nghiệm StackExchange

Các siêu tham số cho Hình 2 được liệt kê trong bảng 1.

Tham số Giá trị
Độ dài chuỗi tối đa 1024
Độ dài đầu ra tối đa 128
Tốc độ học 2e-5
Kích thước batch 16
Gradient Accumulation 8
Chiều LoRA SFT 16
Chiều LoRA RM 8
Chiều LoRA RL 16
Hệ số KL ban đầu thích ứng (PPO) 0.1
Hệ số KL mục tiêu thích ứng (PPO) 6
λ (APA) 0.1

Bảng 1: Siêu tham số cho thí nghiệm StackExchange như hiển thị trong Hình 2

16https://huggingface.co/databricks/dolly-v2-7b

17

--- TRANG 17 ---
D Nghiên cứu Ablation

D.1 Kiểm soát KL trong APA

Trong phần này, chúng tôi hiển thị cách hiệu suất và phân kỳ KL thay đổi với các giá trị λ khác nhau. Chúng tôi đặt
λ= 0.1,1 cho mô hình 125M và vẽ hiệu suất của chúng trong Hình 7 với seed 1000. Người ta có thể thấy rằng
lựa chọn λ trực tiếp xác định mức độ kiểm soát KL, cùng với điểm hội tụ mà APA đạt được. Điều này
cho thấy rằng λ cung cấp một sự đánh đổi rõ ràng giữa kiểm soát KL và hiệu suất mô hình.

0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
steps / k3.0
2.5
2.0
1.5
1.0
0.5
rewardTối ưu hóa Chính sách Trực tuyến với mô hình 125M
Lambda=0.1
Lambda=1
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
steps / k0.0000.0250.0500.0750.1000.1250.150KLPhân kỳ KL giữa chính sách được huấn luyện và chính sách ban đầu cho APA trong mô hình 125M
Lambda = 0.1
Lambda = 1

Hình 7: So sánh hiệu suất giữa các λ khác nhau trên mô hình 125M. Trái: Trục x
biểu thị tổng số bước, tỷ lệ thuận với số lượng dữ liệu được sử dụng trong quy trình huấn luyện. Trục y
là phần thưởng được đánh giá bởi cùng mô hình phần thưởng. Phải: Trục x biểu thị tổng số bước. Trục y
là phân kỳ KL giữa mô hình được huấn luyện và mô hình ban đầu.

D.2 Kiểm soát KL trong PPO

Chúng tôi hiển thị cách hiệu suất và phân kỳ KL thay đổi có hoặc không có kiểm soát KL thích ứng trong PPO. Chúng tôi
vẽ hiệu suất của chúng trong Hình 8 cho mô hình 125M với seed 1000. Đối với PPO với bộ điều khiển KL thích ứng,
hệ số KL ban đầu được đặt là 0.05. Người ta có thể thấy rằng không có kiểm soát KL, PPO hội tụ đến
phần thưởng cao hơn so với APA trong Hình 7, với chi phí là phân kỳ KL cao hơn đáng kể. Mặt khác,
phần thưởng của PPO với kiểm soát KL thích ứng bắt đầu giảm ở giữa. Điều này là do độ lệch lớn
từ chính sách ban đầu, dẫn đến hạng tử điều hòa KL lớn hơn nhiều chiếm ưu thế so với phần thưởng.
So với Hình 7, người ta có thể thấy rằng APA cung cấp điều hòa KL ổn định và có thể kiểm soát hơn.

D.3 Thí nghiệm cho Học Ngoại tuyến

Chúng tôi cũng thực hiện thí nghiệm cho học ngoại tuyến. Tập dữ liệu ngoại tuyến được chọn là tất cả các prompt và
phản hồi từ tập dữ liệu HH, với phần thưởng được gắn nhãn bởi mô hình phần thưởng. Chúng tôi sử dụng hàm phần thưởng GPT-J đã huấn luyện
để gắn nhãn phần thưởng cho tất cả dữ liệu ngoại tuyến, và so sánh ILQL, AWR và APA trên cùng mô hình 125M
và 1B sau tinh chỉnh có giám sát với seed 1000. Kết quả được đưa ra trong Hình 9. Từ kết quả,
người ta có thể thấy rằng AWR hoạt động tốt hơn ILQL, và APA không thể được điều chỉnh trực tiếp cho trường hợp ngoại tuyến.
Hơn nữa, học ngoại tuyến không thể giúp ích nhiều sau giai đoạn tinh chỉnh có giám sát, có thể do
sự dịch chuyển phân phối lớn giữa dữ liệu ngoại tuyến và chính sách hiện tại.

18

--- TRANG 18 ---
0 2 4 6 8 10 12 14
steps / k3.0
2.5
2.0
1.5
1.0
0.5
rewardTối ưu hóa Chính sách Trực tuyến với mô hình 125M
Kiểm soát KL Thích ứng
Không Kiểm soát KL
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5
steps / k01234567KLPhân kỳ KL giữa chính sách được huấn luyện và chính sách ban đầu trong mô hình 125M
Kiểm soát KL Thích ứng
Không Kiểm soát KL

Hình 8: So sánh hiệu suất của PPO trên mô hình 125M. Trái: Trục x biểu thị
tổng số bước, tỷ lệ thuận với số lượng dữ liệu được sử dụng trong quy trình huấn luyện. Trục y là
phần thưởng được đánh giá bởi cùng mô hình phần thưởng. Phải: Trục x biểu thị tổng số bước. Trục y là
phân kỳ KL giữa mô hình được huấn luyện và mô hình ban đầu.

0 2 4 6 8 10 12
steps / k2.8
2.7
2.6
2.5
2.4
2.3
2.2
2.1
2.0
rewardTối ưu hóa Chính sách Ngoại tuyến với mô hình 125M
ILQL
AWR
APA
0 2 4 6 8 10
steps / k2.1
2.0
1.9
1.8
1.7
1.6
rewardTối ưu hóa Chính sách Ngoại tuyến với mô hình 1B
ILQL
AWR
APA

Hình 9: So sánh hiệu suất giữa ILQL, AWR và APA trên tập dữ liệu học ngoại tuyến.

19

--- TRANG 19 ---
E Chứng minh Định lý 1

Chứng minh. Từ giả thiết được xác định tốt π⋆∈ {πθ|θ∈Θ}, chúng ta biết rằng tồn tại một số θ⋆∈Θ sao cho
πθ⋆=π⋆. Đối với mất mát quần thể, chúng ta biết rằng

LAPA(θ⋆) =E(s,a)∼dπolds,ah
(logπθ⋆(a|s)−Adv(s, a)/λ−logπinit(a|s))2i
=E(s,a)∼dπolds,ah
(logπ⋆(a|s)−Adv(s, a)/λ−logπinit(a|s))2i
= 0.

Do đó với bất kỳ θ′∈arg minθ∈ΘLAPA(θ), phải có LAPA(θ′) = 0, tương đương với

E(s,a)∼dπolds,ah
(logπθ′(a|s)−Adv(s, a)/λ−logπinit(a|s))2i
= 0.

Điều này có nghĩa là với bất kỳ s, a trên support của dπolds,a, chúng ta có πθ′(a|s) =π⋆(a|s).

Đối với phần thứ hai của định lý, chúng ta biết từ bất đẳng thức Hoeffding rằng với bất kỳ θ∈Θ cố định,

|LAPA(θ)−bLAPA(θ;D)|=1
nnX
i=1
logπθ(ai|si)−Adv(si, ai)/λ−logπinit(ai|si)2
−Eh
logπθ(a|s)−Adv(s, a)/λ−logπinit(a|s)2i
≤C·(B2/λ−2 log( B1))2r
log(1/δ)
n.

Cho Θϵ là ϵ-covering của Θ dưới chuẩn ℓ2, tức là với bất kỳ θ∈Θ, người ta có thể tìm một số θ′∈Θϵ sao cho
∥θ−θ′∥2≤ϵ. Chúng ta cũng có |Θϵ| ≤(1/ϵ)d. Bằng cách lấy union bound, chúng ta biết rằng với tất cả θ∈Θϵ, với
xác suất ít nhất 1−δ,

|LAPA(θ)−bLAPA(θ;D)| ≤C·(B2/λ−log(B1))2r
dlog(1/(ϵδ))
n. (10)

Cho ˆθ là bộ tối thiểu hóa của bLAPA(θ;D). Thì chúng ta biết rằng tồn tại một số ˆθϵ∈Θϵ sao cho ∥ˆθ−ˆθϵ∥ ≤ϵ.
Điều này tiếp tục ngụ ý rằng

|LAPA(ˆθ)− LAPA(ˆθϵ)|
=|Eh
logπˆθ(a|s)−Adv(s, a)/λ−logπinit(a|s)2i
−Eh
logπˆθϵ(a|s)−Adv(s, a)/λ−logπinit(a|s)2i
|
≤C(B2/λ−log(B1))Lϵ. (11)

Tương tự, chúng ta cũng có |bLAPA(ˆθ)−bLAPA(ˆθϵ)| ≤C(B2/λ−log(B1))Lϵ. Tổng thể, chúng ta có

LAPA(ˆθ) = (LAPA(ˆθ)− LAPA(ˆθϵ)) + (LAPA(ˆθϵ)−bLAPA(ˆθϵ)) + (bLAPA(ˆθϵ)−bLAPA(ˆθ)) +bLAPA(ˆθ).

Đối với hiệu thứ nhất và thứ ba, từ Phương trình (11) chúng ta biết rằng cả hai đều bị chặn bởi C(B2/λ−
log(B1))Lϵ. Đối với hiệu thứ hai, chúng ta biết từ Phương trình (10) rằng nó bị chặn bởi C(B2/λ−
log(B1))2q
dlog(1/(ϵδ))
n. Cuối cùng, chúng ta biết rằng bLAPA(ˆθ) = 0 vì ˆθ∈arg minθbLAPA(θ) và bLAPA(θ⋆) = 0.

Do đó tổng thể, chúng ta có

LAPA(ˆθ)≤C((B2/λ−log(B1))Lϵ+ (B2/λ−log(B1))2r
dlog(1/(ϵδ))
n).

Lấy ϵ= 1/(Ln) hoàn thành chứng minh.

20
