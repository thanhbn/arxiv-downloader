# Thiên Kiến Tính Dài Dòng trong Việc Gán Nhãn Ưu Tiên
của Các Mô Hình Ngôn Ngữ Lớn

Keita Saito∗
Đại học Tsukuba & RIKEN AIP
Tsukuba, Ibaraki 305-8573, Nhật Bản
keita.saito@bbo.cs.tsukuba.ac.jp

Akifumi Wachi
LY Corporation
Chiyoda-ku, Tokyo 102-8282, Nhật Bản
akifumi.wachi@lycorp.co.jp

Koki Wataoka
LY Corporation
Chiyoda-ku, Tokyo 102-8282, Nhật Bản
koki.wataoka@lycorp.co.jp

Youhei Akimoto
Đại học Tsukuba & RIKEN AIP
Tsukuba, Ibaraki 305-8573, Nhật Bản
akimoto@cs.tsukuba.ac.jp

## Tóm tắt

Trong những năm gần đây, các Mô hình Ngôn ngữ Lớn (LLM) đã chứng kiến sự gia tăng đáng kể về mức độ phổ biến, thay đổi cảnh quan của xử lý ngôn ngữ tự nhiên và học máy. Một yếu tố quan trọng trong việc cải thiện hiệu suất của LLM là sự căn chỉnh với con người được đạt được thông qua Học Tăng cường từ Phản hồi của Con người (RLHF), như đối với nhiều LLM như GPT-4, Bard, v.v. Ngoài ra, các nghiên cứu gần đây đang điều tra việc thay thế phản hồi của con người bằng phản hồi từ các LLM khác được gọi là Học Tăng cường từ Phản hồi AI (RLAIF). Chúng tôi xem xét các thiên kiến đi kèm với việc đánh giá LLM bằng các LLM khác và xem xét kỹ hơn thiên kiến tính dài dòng – một thiên kiến mà LLM đôi khi thích các câu trả lời dài dòng hơn ngay cả khi chúng có chất lượng tương tự. Chúng tôi thấy rằng trong bối cảnh vấn đề của mình, GPT-4 thích các câu trả lời dài hơn so với con người. Chúng tôi cũng đề xuất một chỉ số để đo lường thiên kiến này.

## 1 Giới thiệu

Các Mô hình Ngôn ngữ Lớn (LLM) đã có những bước tiến to lớn trong những năm gần đây và tiếp tục trở nên phổ biến (Zhao et al., 2023). Với quy mô ngày càng tăng về số lượng tham số mạng, phạm vi ứng dụng rộng rãi từ các tác vụ xử lý ngôn ngữ tự nhiên thông thường như chat-bot, tóm tắt và dịch thuật, đến các ứng dụng khác vượt ra ngoài mục đích sử dụng ban đầu như công cụ tìm kiếm, hỗ trợ lập trình và các mô hình nền tảng (Zhao et al., 2023; Brants et al., 2007; Katz, 1987).

Sau khi huấn luyện trước cho mục đích chung, LLM được tinh chỉnh để cải thiện hiệu suất cho các tác vụ cụ thể với học có giám sát và RLHF – học tăng cường từ phản hồi gán nhãn ưu tiên từ con người (Stiennon et al., 2020; Ouyang et al., 2022). Tuy nhiên, các vấn đề phát sinh với RLHF khi phản hồi của con người có thể trở nên tốn kém. Để giải quyết vấn đề này, Học Tăng cường từ Phản hồi AI (RLAIF) đã được đề xuất (Bai et al., 2022b; Lee et al., 2023), thay thế phản hồi của con người bằng phản hồi không tốn kém từ các LLM khác.

Trong nhiều trường hợp, câu hỏi thiếu một "câu trả lời đúng" rõ ràng và yêu cầu sự sáng tạo và tưởng tượng. Như có thể thấy trong một ví dụ về phản hồi của LLM được cung cấp trong Hình 1, khi LLM được giao nhiệm vụ đánh giá các phản hồi đối với những lời nhắc như vậy, quá trình đánh giá có thể trở nên tùy tiện và đưa ra nhiều thiên kiến khác nhau. Một thiên kiến nổi bật là thiên kiến tính dài dòng, xảy ra khi LLM bị ảnh hưởng bởi tính dài dòng, ưa thích các văn bản dài hơn và dài dòng hơn, ngay cả khi chúng có vẻ rườm rà hoặc chất lượng thấp hơn. Nếu không tính đến thiên kiến này, các tác nhân LLM có thể học cách tạo ra các văn bản dài một cách không cần thiết. Điều này có thể dẫn đến thất bại trong các tác vụ xuôi dòng như tóm tắt dài hoặc chatbot trả về các phản hồi dài dòng cho những câu hỏi đơn giản.

Trong khi các nghiên cứu trước đây đã khám phá khái niệm thiên kiến tính dài dòng, họ có xu hướng tập trung vào các trường hợp cụ thể. Zheng et al. (2023) giới hạn bối cảnh vấn đề của họ với các câu hỏi được trả lời bằng danh sách trong thí nghiệm về thiên kiến tính dài dòng, và Huang et al. (2023) đã tiến hành thí nghiệm trên các tác vụ tóm tắt. Hơn nữa, những nghiên cứu này không so sánh sở thích của LLM với sở thích của con người. Chúng tôi tin rằng việc so sánh như vậy là quan trọng trong việc thách thức giả thuyết rằng các câu trả lời dài hơn vốn dĩ tốt hơn và LLM thực sự đúng trong sở thích của chúng.

**Đóng góp của chúng tôi.** Trong bài báo này, chúng tôi tiến hành thí nghiệm về thiên kiến tính dài dòng và thấy rằng 1) LLM thể hiện sự ưa thích cho các câu trả lời dài hơn trong các tác vụ viết sáng tạo, và 2) có sự khác biệt giữa LLM và con người trong sở thích tính dài dòng. Ngoài ra, chúng tôi hình thành một định lượng để đo lường thiên kiến tính dài dòng dựa trên tính đồng đều độ chính xác. Điều này có thể được sử dụng để so sánh LLM về mức độ thiên kiến tính dài dòng của chúng.

## 2 Kiến thức chuẩn bị

Sau khi trải qua huấn luyện trước cho mục đích chung, LLM được tinh chỉnh để cải thiện hiệu suất trong các tác vụ cụ thể. Huấn luyện trước được thực hiện thông qua học tự giám sát, nơi mô hình được huấn luyện để dự đoán token tiếp theo trong một câu. Khi LLM có thể tạo ra các câu gắn kết, chúng tôi tiến hành tinh chỉnh mô hình để giải quyết các tác vụ cụ thể. Một cách tiếp cận để tinh chỉnh bao gồm học có giám sát sử dụng dữ liệu chuyên gia. Phương pháp này dựa vào các ví dụ mà các chuyên gia đã giải quyết tác vụ đang xem xét. Một ví dụ về LLM đối화 được huấn luyện chỉ sử dụng cách tiếp cận này là Vicuna (Chiang et al., 2023). Vicuna đạt được hiệu suất tương đương với ChatGPT bằng cách sử dụng các cuộc trò chuyện do người dùng chia sẻ với ChatGPT như dữ liệu chuyên gia. Tuy nhiên, đáng lưu ý là việc thu thập dữ liệu chuyên gia thường khó khăn.

RLHF giải quyết thách thức của dữ liệu huấn luyện hạn chế trong học có giám sát bằng cách tận dụng phản hồi của con người (Stiennon et al., 2020; Ouyang et al., 2022). Cách tiếp cận này không chỉ giảm thiểu tình trạng khan hiếm dữ liệu mà còn tăng cường đáng kể sự căn chỉnh với sở thích của con người, một yếu tố quan trọng trong các ứng dụng như trả lời câu hỏi. Trong RLHF, một mô hình phần thưởng được huấn luyện để khớp chặt chẽ với dữ liệu phản hồi của con người, hoạt động như tín hiệu phần thưởng trong giai đoạn RL tiếp theo. Các LLM nổi bật như ChatGPT và Bard áp dụng cách tiếp cận kết hợp, kết hợp cả kỹ thuật học có giám sát và RLHF để tinh chỉnh thêm sự căn chỉnh với sở thích của con người.

### 2.1 RLHF

Bước đầu tiên của RLHF là điều chỉnh hàm phần thưởng để căn chỉnh với phản hồi của con người. RL trực tiếp từ phản hồi của con người như tín hiệu phần thưởng không ổn định và yêu cầu khối lượng lớn. Do đó, một mô hình phần thưởng hoạt động như tín hiệu phần thưởng sau này trong quá trình được huấn luyện để nhất quán với sở thích của con người. Cho một tập dữ liệu D bao gồm câu hỏi gốc, một cặp văn bản được tạo ra, và nhãn sở thích của con người về cái nào được chọn và bị từ chối, mô hình phần thưởng được huấn luyện bằng cách tối thiểu hóa

L(ϕ) = −E(x,ychosen,yrejected)∼D[log σ(rϕ(ychosen|x) − rϕ(yrejected|x))], (1)

trong đó x là lời nhắc cho LLM, ychosen là văn bản được ưa thích, yrejected là văn bản bị từ chối, và rϕ được tham số hóa với ϕ là mô hình phần thưởng nhận văn bản làm đầu vào và xuất điểm số xếp hạng.

Trong bước thứ hai của RLHF, bây giờ chúng ta có một mô hình phần thưởng để đánh giá một văn bản được tạo ra mà không cần tương tác với con người, RL thông thường có thể diễn ra. Trong bối cảnh này, trạng thái là câu hỏi và văn bản được tạo ra cho đến nay, hành động là token tiếp theo để tạo ra, và phần thưởng là rϕ(y) được đưa ra sau khi văn bản đầy đủ được tạo ra. Điều này tương đương với một tác vụ mà phần thưởng thưa thớt chỉ được đưa ra khi kết thúc tập. LLM tối đa hóa tín hiệu từ mô hình phần thưởng với:

maxθ E[rϕ(πθ(x)|x)], (2)

trong đó πθ là một chính sách được tham số hóa bởi θ. Một số hạng phân kỳ KL tùy chọn được thêm vào để phạt chính sách khỏi việc lệch khỏi chính sách gốc.

### 2.2 RLAIF

Trong khi RLHF làm giảm chi phí lao động của con người so với việc tạo ra dữ liệu chuyên gia từ đầu, phản hồi của con người vẫn tốn kém. Ví dụ, trong Wang et al. (2023), chi phí khoảng 3 phút ($0.75 nếu $15.00 mỗi giờ) cho mỗi đánh giá. Trong một trường hợp, OpenAI đã giải quyết điều này bằng cách thuê người ở Kenya với mức lương dưới $2 mỗi giờ trong quá trình gán nhãn các văn bản bạo lực hoặc không phù hợp. Họ đã bị giám sát về điều kiện làm việc không lý tưởng.

Để chống lại những vấn đề này, RLAIF đã được đề xuất. Phương pháp này thay thế phản hồi của con người bằng phản hồi từ các LLM khác. Điều này làm giảm chi phí đáng kể; trong trường hợp của chúng tôi, chi phí đánh giá khoảng $0.05 mỗi lần, tức là 1/15 so với phản hồi của con người trong bài báo được trích dẫn trước đó (Wang et al., 2023).

### 2.3 Thiên kiến trong Đánh giá LLM Tự động

Khi LLM đánh giá các văn bản được tạo ra, nhiều thiên kiến khác nhau được đưa vào. Chúng tôi cung cấp dưới đây danh sách các thiên kiến được thảo luận trong các bài báo khác nhau (Zheng et al., 2023; Bai et al., 2022a; Wang et al., 2023).

**Thiên kiến Vị trí**: Thiên kiến vị trí xảy ra khi, trong việc so sánh các văn bản được tạo ra, LLM thích câu trả lời được đưa ra ở những vị trí nhất định. Nếu chúng ta định nghĩa xác suất thực tế của a (tham số đầu tiên) được ưa thích hơn b (tham số thứ hai) là P(a, b), thì P(y0, y1) = 1−P(y1, y0), có nghĩa là vị trí không nên ảnh hưởng đến đánh giá. Thiên kiến vị trí là khi việc so sánh bởi mô hình là P̂(y0, y1) ≠ (1−P̂(y1, y0)). Ví dụ, GPT-4 có xu hướng thích tùy chọn đầu tiên được đưa cho nó, trong khi ChatGPT thích tùy chọn thứ hai (Wang et al., 2023). Để tính đến thiên kiến này, chúng ta có thể đơn giản hoán đổi vị trí và đánh giá các tùy chọn hai lần. Nếu mô hình đưa ra kết quả mâu thuẫn giữa các hoán vị, chúng ta tính nó như một trận hòa.

Wang et al. (2023) đã đề xuất một số phương pháp để hiệu chỉnh thiên kiến này thêm: Hiệu chỉnh Nhiều Bằng chứng yêu cầu LLM cung cấp bằng chứng trước khi đưa ra đánh giá, và Hiệu chỉnh Con người-trong-Vòng lặp bao gồm điều chỉnh của con người khi được coi là cần thiết.

**Thiên kiến Tự cải thiện**: LLM có xu hướng thích các câu trả lời được tạo ra bởi chính nó so với các câu trả lời được tạo ra bởi các mô hình khác. Điều này trở thành vấn đề khi đánh giá chuẩn LLM bằng cách đánh giá chúng với LLM (Zheng et al., 2023), nhưng không quá nhiều trong bối cảnh RLAIF, vì các so sánh luôn giữa các câu trả lời được tạo ra bởi cùng một mô hình.

**Thiên kiến Tính dài dòng**: Thiên kiến tính dài dòng đề cập đến thiên kiến mà LLM thích các câu trả lời dài hơn, dài dòng hơn ngay cả khi không có sự khác biệt về chất lượng. Huấn luyện với RLAIF với thiên kiến tính dài dòng có mặt có thể dẫn đến LLM tạo ra các phản hồi quá dài, khi thực tế một phản hồi ngắn gọn hơn nhiều sẽ đủ. Trong các tác vụ như trả lời câu hỏi, một phản hồi dài dòng có thể quan trọng đối với tính hữu ích của nó, nhưng không có đủ nghiên cứu xem xét vào điều này. Vì những lý do này, chúng tôi xem xét kỹ hơn điều này.

Có một số phương pháp được đề xuất để giảm thiểu tác động của các thiên kiến.

**Prompting Chuỗi suy nghĩ** là một kỹ thuật prompting mà LLM được yêu cầu cung cấp quá trình suy nghĩ trước khi tạo ra đánh giá thực tế. Theo cách này, tại thời điểm LLM tạo ra đánh giá thực tế, nó có chuỗi suy nghĩ của mình để dựa vào đánh giá. Điều này khuyến khích sự căn chỉnh với con người và các đánh giá chính xác hơn, thay vì các đánh giá tùy tiện mà không có suy nghĩ.

**Prompting Một lần/Vài lần** là một kỹ thuật prompting khác đưa ra một ví dụ/một số ví dụ về một prompt và câu trả lời đúng tương ứng khi prompting LLM. Khi tạo ra phản hồi, LLM có thể tiếp tục mẫu từ các ví dụ để căn chỉnh tốt hơn với phản hồi dự định.

## 3 Các Công trình Liên quan

### 3.1 Tiến bộ RLAIF

Đã có một số tiến bộ gần đây trong lĩnh vực RLAIF. Bai et al. (2022b) đã huấn luyện một LLM qua RLAIF với phản hồi của con người hạn chế. Trong công trình này, họ tuyên bố rằng tính hữu ích và tính có hại có mối quan hệ đánh đổi, và nhằm huấn luyện một LLM giữ cân bằng giữa hai yếu tố đó. Phương pháp của họ chỉ yêu cầu phản hồi của con người trong khía cạnh tính hữu ích, và hành vi vô hại được đạt được hoàn toàn từ RLAIF. Các LLM được huấn luyện trong công trình của Lee et al. (2023) đã đạt được hiệu suất gần như con người trong các tác vụ tóm tắt với RLAIF mà không có bất kỳ phản hồi nào của con người. Mặc dù không phải là một nghiên cứu về RLAIF, Zheng et al. (2023) đánh giá LLM với các LLM khác như một thẩm phán và cho thấy GPT-4 có sự căn chỉnh cao với con người và đồng ý với con người trên hơn 80% đánh giá.

### 3.2 Về Thiên kiến Tính dài dòng trong Đánh giá bởi LLM

Zheng et al. (2023) cũng cung cấp danh sách các thiên kiến và phương pháp để khắc phục chúng. Bên cạnh thí nghiệm về thiên kiến vị trí, họ đã thí nghiệm về thiên kiến tính dài dòng bằng cách thử một "cuộc tấn công danh sách lặp lại" trên một số LLM. Cuộc tấn công này liên quan đến các tác vụ "liệt kê", trong đó prompt yêu cầu liệt kê một số mục (ví dụ: "Những ví dụ nào về trái cây tròn?"). "Cuộc tấn công danh sách lặp lại" được thực hiện bằng cách làm cho các câu trả lời dài dòng bằng cách lặp lại các mục nhiều lần, và sau đó yêu cầu LLM đánh giá những câu trả lời được tăng cường này. Nếu LLM đánh giá những "danh sách lặp lại" này tốt hơn bản gốc, cuộc tấn công được coi là thành công. Kết quả của họ cho thấy GPT-4 ít bị tấn công này với tỷ lệ thành công dưới 10%, trong khi GPT-3.5 và Claude-v1 đều có tỷ lệ thành công trên 90%. So với nghiên cứu này, chúng tôi mở rộng bối cảnh vấn đề cho các tác vụ trả lời câu hỏi chung.

Huang et al. (2023) giải quyết thiên kiến tính dài dòng trong các tác vụ tóm tắt. Họ phát hiện rằng GPT-4 thực sự thích các phản hồi ngắn về độ trung thực và phạm vi bao phủ khi nói đến tóm tắt, mặc dù điều này chỉ được thấy mạnh mẽ trong việc chấm điểm một câu trả lời, và không phải trong việc chấm điểm so sánh. Điều này cho thấy thiên kiến tính dài dòng có thể khác nhau giữa các tác vụ khác nhau.

So với các nghiên cứu này, bối cảnh vấn đề của chúng tôi tổng quát hơn và chúng tôi so sánh sở thích tính dài dòng giữa con người và LLM. Các thí nghiệm được tiến hành trong những bài báo này đo lường sự khác biệt trong đánh giá khi các văn bản được làm dài dòng một cách nhân tạo trong khi duy trì cùng nội dung. Giả định là việc kéo dài các văn bản sẽ không có tác động lên một người đánh giá thực sự, vì vậy sự khác biệt trong đánh giá cho thấy thiên kiến tính dài dòng. Trong nỗ lực mở rộng bối cảnh vấn đề, chúng tôi sử dụng phản hồi của con người như một oracle thay vì đưa ra giả định này.

## 4 Sở thích Tính dài dòng của LLM

Đầu tiên, chúng tôi thí nghiệm để xem LLM thực sự thích các câu trả lời dài đến mức nào. Chúng tôi yêu cầu GPT-4 chọn giữa các cặp phản hồi và xem xét liệu nó có thích các phản hồi dài hơn hay không. Chúng tôi không giới hạn phạm vi của mình với các prompt được trả lời với một định dạng cụ thể (như danh sách trong Zheng et al. (2023)) để quan sát xu hướng chung của LLM trong việc thích các câu trả lời dài hơn.

Chúng tôi tạo ra 100 câu trả lời mẫu cho mỗi prompt, tất cả từ cùng một mô hình (Vicuna-7b-v1.5) được tạo ra với tham số nhiệt độ được đặt thành 0.7. Một trong những câu hỏi và hai ví dụ về câu trả lời như sau.

• **Câu hỏi**: Soạn một bài blog du lịch hấp dẫn về một chuyến đi gần đây đến Hawaii, làm nổi bật các trải nghiệm văn hóa và các điểm thu hút phải xem

1. Tôi gần đây đã có cơ hội đi du lịch Hawaii, và đó là một trải nghiệm mà tôi sẽ không bao giờ quên. Từ những bãi biển tuyệt đẹp đến văn hóa sôi động, có rất nhiều thứ để khám phá và thưởng thức trong thời gian tôi ở các hòn đảo...

2. Hawaii là một thiên đường nhiệt đới, và tôi gần đây đã có cơ hội trải nghiệm nó cho bản thân mình. Quần đảo này bao gồm tám hòn đảo chính, mỗi hòn đảo có văn hóa, truyền thống và cảnh quan ngoạn mục riêng biệt. Trong chuyến đi của tôi, tôi đã có cơ hội thăm một số địa điểm văn hóa, như Trung tâm Văn hóa Polynesia trên Oahu...

3. ...

Các prompt được lấy từ thư viện được giới thiệu bởi Zheng et al. (2023), tất cả từ danh mục "sáng tạo" bởi vì 1) các câu trả lời được tạo ra cho các danh mục khác không có sự thay đổi đủ lớn về số từ để thấy thiên kiến tính dài dòng và 2) GPT-4 không giỏi trong việc đánh giá câu trả lời trong những danh mục đó. Sau đó chúng tôi lấy các câu trả lời từ những mẫu được tạo ra này và chèn chúng vào mẫu được hiển thị trong Hình 1. Với mẫu hoàn chỉnh, chúng tôi yêu cầu GPT-4 đánh giá sở thích giữa các cặp câu trả lời với mẫu. Kết quả là tùy chọn đầu tiên được chọn, một trận hòa, hoặc tùy chọn thứ hai được chọn. Để tính đến thiên kiến vị trí, GPT-4 đánh giá cặp hai lần với vị trí được hoán đổi lần thứ hai. Nó được coi là một trận hòa trừ khi nó đưa ra cùng kết quả trên cả hai hoán vị.

Kết quả được hiển thị trong Hình 3a cho kết quả tổng thể, và Hình 3b đến 3d cho kết quả từ mỗi prompt. Cả trong kết quả tổng thể và kết quả cá nhân, có xu hướng GPT-4 thích các câu trả lời dài hơn. Khi sự khác biệt về số từ đủ lớn, GPT-4 hầu như luôn thích câu trả lời dài hơn. Đối với câu hỏi 1 và 2, sở thích mượt mà và rõ ràng, trong khi đối với câu hỏi 3, khi sự khác biệt số từ nhỏ, có phương sai cao trong đánh giá. Như chúng ta có thể thấy hình dạng thay đổi giữa các câu hỏi, và do đó chúng ta có thể suy ra rằng tính dài dòng không hoàn toàn dựa vào số từ và khác nhau cho mỗi câu hỏi. Điều này làm cho việc điều chỉnh tính dài dòng sau đánh giá trở nên khó khăn trừ khi chúng ta biết hình dạng sở thích tính dài dòng cho prompt đang xem xét.

Từ thí nghiệm này, chúng ta có thể rút ra kết luận rằng GPT-4 thường thích các câu trả lời dài hơn trong số những câu được tạo ra bởi cùng LLM với cùng prompt. Tuy nhiên, thí nghiệm này chỉ riêng lẻ không cho thấy GPT-4 bị thiên kiến tính dài dòng; có thể là các câu trả lời dài hơn được tạo ra bởi vicuna thực sự có chất lượng và tính hữu ích cao hơn. Để thực sự đo lường thiên kiến tính dài dòng, chúng ta sẽ cần sự thật căn bản của mỗi so sánh mà chúng ta không có. Thay vào đó, chúng ta tiếp theo sử dụng một tập dữ liệu đánh giá của con người như đường cơ sở.

## 5 Có Sự Khác biệt trong Sở thích Tính dài dòng Giữa LLM và Con người?

Xem xét rằng LLM thay thế con người như những người gán nhãn trong RLAIF, sẽ đủ nếu LLM có thể sao chép phản hồi của con người và nó không nhất thiết phải được căn chỉnh với sự thật căn bản. Như được thấy trong Hình 4 vẽ sở thích tính dài dòng của con người trong tập dữ liệu HH-RLHF được mô tả sau này, con người dường như cũng thích các câu trả lời dài hơn. Việc các câu trả lời dài hơn có thực sự hữu ích hay không là không liên quan miễn là LLM và con người đi đến cùng kết luận. Theo quan điểm này, chúng tôi so sánh sự khác biệt trong sở thích tính dài dòng giữa LLM và con người. Chúng ta có thể xem điều này như thiên kiến tính dài dòng vì mục tiêu của đánh giá LLM trong RLAIF là căn chỉnh với con người và không phải là xóa bỏ sở thích thiên kiến trong tính dài dòng.

Chúng tôi sử dụng tập dữ liệu HH-RLHF (Bai et al., 2022a) chứa dữ liệu phản hồi của con người so sánh các cặp câu trả lời cho một prompt. Nó chỉ có một dữ liệu phản hồi cho mỗi prompt, vì vậy chúng tôi không thể vẽ sở thích tính dài dòng của con người như trong thí nghiệm ở chương trước. Thay vào đó chúng ta có thể thấy sự không giống nhau giữa LLM và con người trong sở thích tính dài dòng nói chung qua nhiều loại câu hỏi khác nhau. Chính xác, thí nghiệm này xem xét mối quan hệ giữa sự khác biệt về số từ trong cặp phản hồi và sự căn chỉnh với con người của LLM, có nghĩa là tần suất LLM đưa ra cùng đánh giá như con người.

Chúng tôi sử dụng cùng mẫu prompt như thí nghiệm trước nhưng yêu cầu GPT-4 đánh giá toàn bộ cuộc trò chuyện. Không giống như thí nghiệm trước đánh giá câu trả lời cho một câu hỏi duy nhất, HH-RLHF chứa các cuộc trò chuyện giữa con người và trợ lý. Do đó chúng tôi yêu cầu GPT-4 đánh giá cặp toàn bộ cuộc trò chuyện và trả lời trợ lý nào hữu ích hơn.

Trong các trường hợp mà phản hồi của con người thích câu trả lời dài hơn, sự căn chỉnh với con người cao đối với LLM, có nghĩa là LLM cũng thích các câu trả lời dài hơn. Tuy nhiên, khi phản hồi của con người chọn câu trả lời có ít từ hơn, sự căn chỉnh với con người thấp, bởi vì LLM vẫn chọn các câu trả lời dài hơn bất kể tính hữu ích của câu trả lời ngắn hơn.

Một lời giải thích có thể cho điều này là LLM đã học để bắt chước hành vi của con người một cách khái quát bằng cách chọn các câu trả lời dài hơn – trong tập dữ liệu này, phản hồi của con người có xu hướng ưa thích các phản hồi dài hơn như được thấy trong Hình 4, và có thể tập dữ liệu được sử dụng để huấn luyện GPT-3.5/GPT-4 có cùng xu hướng. Tuy nhiên, việc xem xét kỹ hơn nguyên nhân vẫn còn để tranh luận.

## 6 Hình thành Thiên kiến Tính dài dòng

Trong thí nghiệm thứ hai, chúng tôi quan sát xu hướng của LLM có sự căn chỉnh với con người thấp cho các trường hợp mà phản hồi của con người thích các câu trả lời ngắn hơn. Trong phần này, chúng tôi hình thành thiên kiến tính dài dòng để cho phép so sánh định lượng giữa các mô hình.

Trong bối cảnh vấn đề của chúng tôi, chúng tôi định nghĩa cặp đầu vào văn bản được cho là y0 và y1, đầu ra quyết định LLM là Y′ ∈ {0,1}, và tùy chọn hữu ích hơn được gán nhãn bởi con người là Y ∈ {0,1}. Chúng tôi định nghĩa thuộc tính nhạy cảm S ∈ {0,1} bằng 0 khi y0 có nhiều từ hơn y1, và 1 khi y1 có nhiều từ hơn y0.

Với những định nghĩa này, cơ hội bình đẳng (Hardt et al., 2016) đối với thuộc tính nhạy cảm S được thỏa mãn nếu

P(Y′ = 0|S = 0, Y = 0) = P(Y′ = 0|S = 1, Y = 0). (3)

Điều này chỉ tính đến các trường hợp mà phản hồi của con người thích y0. Mặc dù điều này có thể đạt được bằng cách sắp xếp các đầu vào trước, phương trình có thể được tổng quát hóa với tính đồng đều độ chính xác thay vì cơ hội bình đẳng. Tính đồng đều độ chính xác được thỏa mãn nếu độ chính xác của dự đoán bằng nhau giữa cả hai nhóm dân số:

P(Y′ = Y|S = Y) = P(Y′ = Y|S = 1−Y). (4)

Sự lệch khỏi tính đồng đều độ chính xác có thể được tính bằng phương trình sau:

|P(Y′ = Y|S = Y) − P(Y′ = Y|S = 1−Y)|. (5)

Mặc dù đây là cách tính độ lệch nói chung, chúng tôi nghĩ quan trọng là thông tin định hướng của thiên kiến không bị mất. Với công thức dưới đây (6), một giá trị dương cho thấy LLM thích các câu trả lời dài dòng, và một giá trị âm cho thấy nó thích các câu trả lời ngắn hơn. Sự phân biệt này quan trọng vì một số tác vụ có thể có thiên kiến âm, ví dụ trong các tác vụ tóm tắt như được hiển thị trong Huang et al. (2023). Chúng tôi cũng chọn sự khác biệt trong độ không chính xác giữa các nhóm dân số

P(Y′ = 1−Y|S = 1−Y) − P(Y′ = 1−Y|S = Y) (6)

bởi vì thiên kiến tính dài dòng đề cập đến độ không chính xác bị ảnh hưởng bởi tính dài dòng.

Bảng 1 hiển thị các giá trị thiên kiến tính dài dòng của GPT-3.5 và GPT-4 được tính với dữ liệu từ Phần 5. Từ những số này, chúng ta có thể kết luận rằng GPT-4 đã cải thiện trong thiên kiến tính dài dòng. So với Wang et al. (2023), có bối cảnh vấn đề hạn chế và tạo ấn tượng rằng GPT-4 ít bị thiên kiến tính dài dòng hơn đáng kể, chúng ta thấy rằng thiên kiến tính dài dòng vẫn tồn tại đối với GPT-4. Một thí nghiệm thêm về các LLM khác để so sánh là cần thiết.

## 7 Thảo luận

### 7.1 Các Chỉ số Bình đẳng Khác

Trong bối cảnh nghiên cứu của chúng tôi, chúng tôi coi tính dài dòng của cặp phản hồi như thuộc tính nhạy cảm trong công thức thiên kiến tính dài dòng ở Phần 6. Điều mà tính dài dòng khác với các thuộc tính nhạy cảm thường được thảo luận trong các trường hợp thiên kiến khác là thực tế rằng tính dài dòng thực sự nên được xem xét khi đánh giá các phản hồi, trong khi các thuộc tính như giới tính hoặc chủng tộc không nên là một yếu tố trong kết quả trong các trường hợp khác. Đây là lý do tại sao việc sử dụng các chỉ số bình đẳng khác như tính đồng đều nhân khẩu học không có ý nghĩa ở đây, và do đó chúng tôi dựa vào đo lường thiên kiến tính dài dòng trên cơ hội bình đẳng và tính đồng đều độ chính xác.

### 7.2 Hạn chế của Các Thí nghiệm của Chúng tôi

Trong thí nghiệm ở Phần 4, chúng tôi tạo ra các phản hồi mẫu từ cùng các câu hỏi (trước khi nối kết quả từ cả ba câu hỏi). Tuy nhiên, trong thí nghiệm ở Phần 5, chúng tôi trộn lẫn kết quả từ nhiều câu hỏi khác nhau. Điều này đã dẫn chúng tôi chỉ đạt được kết quả qua nhiều loại câu hỏi, không phải kết quả trên bất kỳ câu hỏi cụ thể nào như trong thí nghiệm ở Phần 4. Có thể tranh luận cái nào trong những kết quả này tốt hơn.

### 7.3 Hạn chế của Chỉ số Thiên kiến Tính dài dòng của Chúng tôi

Công thức thiên kiến tính dài dòng của chúng tôi chỉ tính đến thiên kiến giữa hai nhóm được chia bởi việc y0 có dài hơn y1 hay không. Điều nó không thể phát hiện là thiên kiến trong mỗi nhóm này; nó không biết đến thiên kiến giữa các trường hợp mà y0 hầu như không dài hơn y1 và các trường hợp mà y0 dài hơn đáng kể so với y0. Do đó, nếu có một trường hợp mà mô hình có sự căn chỉnh với con người cao khi có sự khác biệt lớn về độ dài giữa cặp phản hồi – biểu đồ sẽ có hình lõm đối xứng quanh đường thẳng đứng xuống giữa – chỉ số của chúng tôi sẽ gợi ý rằng mô hình có thiên kiến tính dài dòng gần bằng không. Để tránh tình huống như vậy, việc hiển thị biểu đồ căn chỉnh với con người cùng với chỉ số được khuyến nghị.

## 8 Kết luận

Trong bài báo này, chúng tôi đã tiến hành thí nghiệm về thiên kiến tính dài dòng được thấy trong đánh giá của LLM bởi LLM. Trong các công trình trước đây, bối cảnh vấn đề bị hạn chế và không so sánh sở thích tính dài dòng với con người. Với các thí nghiệm của chúng tôi, chúng tôi thấy rằng 1) LLM có xu hướng ưa thích các câu trả lời dài hơn cho các tác vụ viết sáng tạo, và 2) sự căn chỉnh với con người thay đổi về tính dài dòng với sự căn chỉnh với con người thấp hơn trong các trường hợp mà con người thích các câu trả lời ngắn hơn. Sau đó chúng tôi đã hình thành thiên kiến tính dài dòng dựa trên tính đồng đều độ chính xác có thể được sử dụng để so sánh định lượng thiên kiến tính dài dòng giữa các mô hình.
