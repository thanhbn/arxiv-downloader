# 2311.08692.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2311.08692.pdf
# Kích thước tệp: 967943 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Định tuyến đến Chuyên gia: Tập hợp hiệu quả các Mô hình Ngôn ngữ Lớn dựa trên phần thưởng
Keming Lu, Hongyi Yuan∗, Runji Lin∗
Junyang Lin, Zheng Yuan, Chang Zhou, Jingren Zhou
Alibaba Inc.
{lukeming.lkm,yuanhongyi.yhy,linrunji.lrj}@alibaba-inc.com
{junyang.ljy,yuanzheng.yuanzhen}@alibaba-inc.com
{ericzhou.zc,jingren.zhou}@alibaba-inc.com
Tóm tắt
Tiềm năng bổ sung của các Mô hình Ngôn ngữ Lớn (LLM) giả định các LLM có sẵn có kiến thức chuyên môn khác nhau trong nhiều lĩnh vực và nhiệm vụ để một tập hợp LLM có thể đạt được hiệu suất tốt hơn một cách nhất quán. Các phương pháp tập hợp hiện có cho LLM chủ yếu tập trung vào xếp hạng đầu ra của mô hình phần thưởng, dẫn đến chi phí tính toán đáng kể. Để giải quyết vấn đề này, chúng tôi xem xét lại tiềm năng bổ sung của LLM và phát triển thêm bằng cách khai thác kiến thức chuyên môn tiềm ẩn với các mô hình phần thưởng có sẵn. Chúng tôi đề xuất ZOOTER, một phương pháp định tuyến dựa trên phần thưởng chưng cất phần thưởng trên các truy vấn huấn luyện để đào tạo hàm định tuyến, có thể phân phối chính xác mỗi truy vấn đến LLM có chuyên môn về nó. Chúng tôi cũng tích hợp cải thiện nhãn dựa trên thẻ để giảm thiểu nhiễu từ sự không chắc chắn khi sử dụng phần thưởng làm giám sát bạc. ZOOTER cho thấy hiệu quả tính toán trong suy luận vì nó chỉ tạo ra chi phí tính toán nhỏ của hàm định tuyến so với các phương pháp xếp hạng mô hình phần thưởng. Chúng tôi đánh giá ZOOTER trên một bộ sưu tập benchmark toàn diện với 26 tập con trên các lĩnh vực và nhiệm vụ khác nhau. ZOOTER vượt trội hơn mô hình đơn tốt nhất trung bình và xếp hạng nhất trên 44% nhiệm vụ, thậm chí vượt qua nhiều phương pháp xếp hạng mô hình phần thưởng.¹
1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) được điều chỉnh theo sở thích của con người nhanh chóng xuất hiện và gần như được phát hành hàng ngày (Touvron et al., 2023a,b; Anil et al., 2023; Bai et al., 2023). Các LLM có sẵn này được tinh chỉnh thêm hoặc điều chỉnh theo sở thích của con người để trở thành tổng quát (Xu et al., 2023; Touvron et al., 2023b,a) hoặc chuyên gia (Yuan et al., 2023a; Luo et al., 2023a,b; Roziere et al., 2023) để giải quyết các nhiệm vụ đa dạng. Điều đáng chú ý là LLM được huấn luyện trước và điều chỉnh với dữ liệu khác nhau, dẫn đến

∗ ∗Công việc được thực hiện trong thời gian thực tập tại Alibaba Inc.
¹Công việc đang tiến hành.

Chia sẻ ý tưởng để điều chỉnh các kiệt tác nghệ thuật thành trải nghiệm tương tác cho trẻ em. Liệt kê 5 tác phẩm nghệ thuật cụ thể và các ý tưởng liên quan.Chắc chắn! Là một trợ lý hữu ích và tôn trọng, tôi rất vui được cung cấp ý tưởng để điều chỉnh các kiệt tác nghệ thuật ... 1. "Đêm Đầy Sao" của Vincent van Gogh:\n\n Ý tưởng Trải nghiệm Tương tác: Tạo một trải nghiệm kỹ thuật số hoặc ...1. Mona Lisa của Leonardo da Vinci: Tạo trải nghiệm thực tế ảo nơi trẻ em có thể khám phá bức tranh và tương tác với nó theo nhiều cách khác nhau, chẳng hạn như …… \n\n2. Tiếng Thét của Edvard Munch: Tạo ra ……1. Tiếng Thét của Edvard Munch: Bức tranh này là cơ hội hoàn hảo để thảo luận về cảm xúc với trẻ em. …… Sau đó, bạn có thể hướng dẫn chúng qua quá trình ……1. "Đêm Đầy Sao" của Vincent van Gogh:\n Trải nghiệm Tương tác: Tạo trải nghiệm thực tế ảo (VR) ……. Chúng có thể học về các chòm sao và ngôi sao, và thậm chí tạo ra của riêng mình ……1. "Đêm Đầy Sao" của Vincent van Gogh -Tạo trải nghiệm thực tế ảo ……. Chúng có thể học về các chòm sao khác nhau và các kỹ thuật van Gogh đã sử dụng để tạo ra hiệu ứng xoáy ……Tạo trải nghiệm tương tác dựa trên các kiệt tác nghệ thuật có thể là cách tuyệt vời để giới thiệu trẻ em đến thế giới nghệ thuật và khơi dậy sự sáng tạo của chúng. ... 1. "Đêm Đầy Sao" của Vincent van Gogh\nỶ tưởng: Tạo trải nghiệm thực tế ảo (VR) …WizardCoderLLaMa-2
WizardMathWizardLMVicunaOpenChatRewardOracleZooter0.10102.190.0210-0.030.007-0.360.12102.630.1391.770.63104.22Hình 1: Một ví dụ về tập hợp mô hình ngôn ngữ lớn. Xếp hạng mô hình phần thưởng được đánh dấu màu xanh cần tạo phản hồi từ tất cả các mô hình trong khi ZOOTER định tuyến truy vấn đã cho đến mô hình tốt nhất và chỉ suy luận một mô hình. Trường hợp này được thu thập từ benchmark MT-Bench và chúng tôi cũng trình bày các đánh giá oracle của mỗi phản hồi.

điểm mạnh và điểm yếu đa dạng trong các nhiệm vụ downstream khác nhau (Jiang et al., 2023). Do đó, tập hợp LLM khai thác tiềm năng bổ sung giữa chúng và có thể đạt được hiệu suất tốt hơn so với một mô hình đơn tốt nhất trung bình trên các nhiệm vụ đa dạng.
Một trong những thách thức chính trong tập hợp LLM là hiệu quả tính toán do kích thước tham số lớn của các LLM hiện có. Nghiên cứu trước đây (Jiang et al., 2023; Shnitzer et al., 2023) cung cấp các phương pháp vững chắc để hợp nhất các đầu ra tạo sinh của LLM như một tập hợp. Các phương pháp như vậy đòi hỏi chi phí suy luận khổng lồ khiến chúng không thể mở rộng và do đó không cạnh tranh với mô hình tốt nhất trung bình trong các tình huống tài nguyên thấp. Để tập hợp hiệu quả các LLM có sẵn, chúng tôi trước tiên đi sâu vào
giả định khá đơn giản nhưng vẫn chưa được nghiên cứu kỹ: Các LLM được điều chỉnh có sẵn, ngay cả những LLM được điều chỉnh như "tổng quát", có kiến thức chuyên môn khác nhau trong nhiều lĩnh vực và chủ đề.

--- TRANG 2 ---
giả định khá đơn giản nhưng vẫn chưa được nghiên cứu kỹ: Các LLM được điều chỉnh có sẵn, ngay cả những LLM được điều chỉnh như "tổng quát", có kiến thức chuyên môn khác nhau trong nhiều lĩnh vực và chủ đề.
Tuy nhiên, việc phân tích chuyên môn của LLM cũng gặp thách thức vì chuyên môn tiềm ẩn của LLM có liên quan chặt chẽ đến dữ liệu huấn luyện trước và điều chỉnh, điều này rất mơ hồ và không thể truy cập ngay cả đối với các LLM mã nguồn mở phổ biến như LLAMA-2-CHAT (Touvron et al., 2023b) và WIZARDLM (Xu et al., 2023).

Nếu giả định này đúng mạnh mẽ, các LLM có sẵn có thể được tập hợp hiệu quả bằng cách gán các truy vấn cho mô hình thành thạo trong truy vấn đó mà không phát sinh chi phí suy luận bổ sung trên mỗi mô hình. Chiến lược định tuyến hiệu quả như vậy chỉ yêu cầu chi phí suy luận cho một mô hình duy nhất cho mỗi truy vấn và chi phí overhead của một bộ định tuyến truy vấn nhỏ hơn nhiều. Tuy nhiên, việc thăm dò chuyên môn chi tiết của các LLM có sẵn và tạo giám sát để đào tạo bộ định tuyến cũng yêu cầu chú thích. Việc phát triển phương pháp đào tạo hiệu quả dữ liệu để định tuyến truy vấn vẫn chưa được nghiên cứu đáng kể.

Để giải quyết những vấn đề này, chúng tôi đề xuất ZOOTER, một phương pháp định tuyến truy vấn dựa trên phần thưởng để tập hợp hiệu quả các LLM có sẵn. ZOOTER thu thập và cải thiện giám sát bạc từ các mô hình phần thưởng (RM) hiện có để đào tạo bộ định tuyến truy vấn và phân phối truy vấn trước cho "chuyên môn". Như được hiển thị trong Hình 1, phân phối phần thưởng ngụ ý các đánh giá oracle và tiết lộ chuyên môn tiềm ẩn giữa các LLM. Và ZOOTER nắm bắt chuyên môn từ phân phối phần thưởng và cung cấp phân phối truy vấn trong quá trình suy luận. Cụ thể, chúng tôi trước tiên tiến hành một nghiên cứu toàn diện bao gồm bốn nhóm benchmark trên hơn 26 tập con trong các lĩnh vực và nhiệm vụ khác nhau. Chúng tôi điều tra sáu LLM mã nguồn mở được sử dụng rộng rãi và cho thấy tiềm năng bổ sung của các nhiệm vụ downstream phạm vi rộng như vậy bằng cách tổng hợp chúng thông qua xếp hạng mô hình phần thưởng. Sau đó, chúng tôi thu thập một tập truy vấn đào tạo đa dạng và chưng cất phần thưởng của chuyên môn mô hình làm giám sát gián tiếp để đào tạo bộ định tuyến LLM và phát triển cải thiện nhãn dựa trên thẻ để khắc phục thêm sự thiếu hụt của các nhãn bạc như vậy từ các mô hình phần thưởng. Với các thử nghiệm toàn diện, chúng tôi cho thấy ZOOTER có thể hưởng lợi từ giám sát bạc RM để học chuyên môn tiềm ẩn giữa các LLM và tiến hành định tuyến hiệu quả cho tập hợp mô hình.

Đóng góp của chúng tôi chủ yếu gồm ba phần:
• Chúng tôi xem xét lại tiềm năng bổ sung của các LLM mã nguồn mở, điều này chứng minh hiệu quả của tập hợp LLM, và cho thấy phần thưởng từ các RM có sẵn có thể là giám sát bạc cho chuyên môn mô hình.
• Chúng tôi đề xuất ZOOTER, một phương pháp định tuyến hiệu quả dựa trên phần thưởng chưng cất phần thưởng từ mô hình phần thưởng có sẵn để thăm dò chuyên môn mô hình. Sau đó, chúng tôi phát triển cải thiện nhãn dựa trên thẻ để giảm thiểu nhiễu từ sự không chắc chắn của các mô hình phần thưởng.
• Chúng tôi đánh giá toàn diện các phương pháp tập hợp, bao gồm xếp hạng mô hình phần thưởng và ZOOTER trên bốn nhóm benchmark với 26 tập con trên các nhiệm vụ và lĩnh vực khác nhau. Đánh giá của chúng tôi cho thấy ZOOTER có thể tập hợp hiệu quả các LLM và thậm chí vượt trội hơn các phương pháp xếp hạng mô hình phần thưởng với chi phí tính toán ít hơn đáng kể.

2 Công trình liên quan
Điều chỉnh Hướng dẫn và Căn chỉnh. Điều chỉnh hướng dẫn (Longpre et al., 2023) giúp LLM tuân theo các hướng dẫn đa dạng, được áp dụng rộng rãi để căn chỉnh LLM với sở thích của con người (Chiang et al., 2023; Xu et al., 2023; Bai et al., 2023). Trong công việc này, chúng tôi tập trung vào việc tập hợp các LLM được căn chỉnh, như Llama-2-Chat (Touvron et al., 2023b), WizardLM (Xu et al., 2023), Vicuna (Chiang et al., 2023), và những LLM khác. Và chúng tôi đánh giá chúng trên nhiều nhiệm vụ đánh giá căn chỉnh.

Tập hợp Mô hình Ngôn ngữ Lớn. Tập hợp LLM là một chủ đề mới nổi do sự bùng nổ của các LLM mã nguồn mở. Tập hợp LLM nhằm hợp nhất các LLM có sẵn để đạt được hiệu suất tốt hơn một cách nhất quán trên các nhiệm vụ downstream đa dạng. Ít công trình khám phá giả định tiềm năng bổ sung của LLM và cách tập hợp LLM với nó. Jiang et al. (2023) trình bày một khung tập hợp bao gồm một bộ xếp hạng cặp và một bộ hợp nhất tạo sinh. Chen et al. (2023) suy luận tuần tự các LLM có sẵn và dừng lại cho đến khi phản hồi đạt chất lượng đủ. Wang et al. (2023b) đề xuất vấn đề hợp nhất các chuyên gia để hợp nhất đầu ra của các mô hình chuyên gia với kiến thức bổ sung về phân phối dữ liệu và công thức hóa nó như học có giám sát. Shnitzer et al. (2023) cho thấy tính hữu ích và hạn chế của việc học các bộ định tuyến mô hình từ các bộ dữ liệu benchmark khác nhau. Mặc dù các công trình này đều tập trung vào xếp hạng phần thưởng hoặc chiến lược định tuyến để tập hợp LLM, ZOOTER phân biệt với các công trình đồng thời này trong hai

--- TRANG 3 ---
khía cạnh. Đầu tiên, các công trình đồng thời của chúng tôi yêu cầu tạo đầu ra hoặc quá trình forward để có được biểu diễn prompt của tất cả ứng viên, dẫn đến chi phí tính toán đáng kể. ZOOTER suy luận chuyên môn mô hình bằng cách chưng cất phần thưởng trên tập truy vấn đào tạo được xác định trước để tránh chi phí suy luận như vậy. Sau đó, tất cả các công trình này được phát triển và đánh giá trên một tập benchmark, trong khi ZOOTER có thể được phát triển chỉ với các truy vấn mà không có phản hồi vàng, và ZOOTER nhằm vào các nhiệm vụ căn chỉnh đa dạng hơn. Do đó, ZOOTER nổi bật về hiệu quả dữ liệu và tính toán. Chúng tôi cũng đánh giá ZOOTER trên các nhiệm vụ căn chỉnh đa dạng hơn để kiểm tra toàn diện tiềm năng bổ sung của LLM.

Tạo sinh dưới Hướng dẫn của Mô hình Phần thưởng. Các mô hình phần thưởng trong bối cảnh các mô hình ngôn ngữ lớn thường được sử dụng để cải thiện hiệu suất căn chỉnh bằng học tăng cường (Schulman et al., 2017; Ouyang et al., 2022) hoặc học sở thích (Yuan et al., 2023b; Rafailov et al., 2023; Song et al., 2023). Các mô hình phần thưởng cũng có thể cải thiện hiệu suất trong giai đoạn tạo sinh. Khả năng lý luận toán học của các mô hình ngôn ngữ có thể được cải thiện bằng cách sử dụng các mô hình phần thưởng xếp hạng nhiều đường dẫn lý luận được tạo (Cobbe et al., 2021; Uesato et al., 2022; Lightman et al., 2023). Liu et al. (2023) sử dụng các mô hình phần thưởng để công thức hóa giải mã dưới hướng dẫn phần thưởng. Được truyền cảm hứng từ những ứng dụng thành công này của các mô hình phần thưởng trong căn chỉnh, ZOOTER cũng tận dụng các mô hình phần thưởng có sẵn để điều tra chuyên môn tiềm ẩn của LLM.

3 Phương pháp
Chúng tôi trước tiên xem xét lại tiềm năng bổ sung của LLM (§3.1) và sau đó giới thiệu ZOOTER như một phương pháp tập hợp LLM hiệu quả (§3.2).

3.1 Tiềm năng Bổ sung của LLM
Trong phần này, chúng tôi trình bày các kiến thức cơ bản về giả định: Các LLM được căn chỉnh có sẵn có chuyên môn khác nhau trong nhiều lĩnh vực và chủ đề. Chúng tôi cũng giới thiệu ngắn gọn hai chiến lược tập hợp LLM, xếp hạng mô hình phần thưởng và định tuyến truy vấn.

Giả định Tiềm năng Bổ sung. Xem xét một tập các LLM được ký hiệu là M={mi|i∈Z+} và một tập các truy vấn downstream được ký hiệu là Q={qi|i∈Z+}, chúng tôi giả định rằng đối với mỗi LLM mi trong M, tồn tại một tập con truy vấn không rỗng Qmi sao cho LLM có thể đạt được hiệu suất tốt hơn đồng đều so với các LLM khác trong M cho bất kỳ truy vấn qj∈Qmi nào, là mi=argmaxm∈MP(qj, m(qj)). P có thể là bất kỳ sở thích hoặc thước đo nào để đánh giá hiệu suất. Trong công việc này, chúng tôi tiếp tục tăng cường giả định này và nhằm cho thấy rằng sự bổ sung giữa các LLM tiết lộ chuyên môn của chúng trong các lĩnh vực và nhiệm vụ khác nhau, để chúng tôi có thể phân loại truy vấn và chọn LLM tốt nhất cho mỗi danh mục.

Xếp hạng Mô hình Phần thưởng. Xếp hạng mô hình phần thưởng (RMR) tận dụng tiềm năng bổ sung để tập hợp LLM và đạt được hiệu suất vượt trội. RMR cố gắng tìm một hàm phần thưởng ˆP để ước tính sở thích oracle P để chúng ta có thể có được mô hình tốt nhất cho mỗi truy vấn (Jiang et al., 2023). Tuy nhiên, RMR suy luận tất cả các mô hình ứng viên để có được đầu ra và sau đó xếp hạng chúng với hàm phần thưởng, tạo ra chi phí tính toán lớn.

Định tuyến Truy vấn. Định tuyến truy vấn giảm thiểu mối quan tâm về hiệu quả trong tập hợp LLM, đặc biệt so với các phương pháp RMR hiện có. Nói chung, định tuyến truy vấn cố gắng tìm một hàm định tuyến Z(q, mi) với respect đến qj∈Q tồn tại, để mi=argmaxm∈MZ(qj, m). Hàm định tuyến phân phối truy vấn dựa trên chính chúng mà không tạo đầu ra. Nếu tiềm năng bổ sung của LLM đúng, hàm định tuyến dự đoán xác suất một truy vấn q thuộc về chuyên môn của LLM Qm.

3.2 Zooter
Trong phần này, chúng tôi đề xuất ZOOTER, một phương pháp định tuyến truy vấn dựa trên phần thưởng để tập hợp hiệu quả các mô hình ngôn ngữ lớn. ZOOTER học từ xếp hạng mô hình phần thưởng để diễn giải chuyên môn tiềm ẩn của mỗi mô hình. Vì vậy, như được hiển thị trong Hình 2, ZOOTER trước tiên suy luận tất cả các LLM ứng viên trên tập đào tạo chứa các truy vấn đa dạng để tạo phản hồi. Sau đó, tất cả phản hồi sẽ được thưởng bởi một mô hình phần thưởng có sẵn cung cấp phần thưởng vô hướng, được đánh dấu bằng các đường nét đứt màu xanh trong Hình 2. Các phần thưởng trước tiên được cải thiện bởi một prior dựa trên thẻ để làm mịn và khử nhiễu. Phân phối phần thưởng được chuẩn hóa sau đó được sử dụng làm giám sát trong đào tạo chưng cất kiến thức của hàm định tuyến, được hiển thị trong các đường nét đứt màu xanh lá cây trong Hình 2. Trong quá trình suy luận, hàm định tuyến phân loại truy vấn đầu vào thành LLM có tiềm năng chuyên môn mạnh nhất trong truy vấn này, và LLM sẽ tạo phản hồi chuyên gia. Bằng cách đào tạo hàm định tuyến như vậy,

--- TRANG 4 ---
Phản hồi 1 Phản hồi 2 Phản hồi N -1.73 0.33 7.28
LLM 2 LLM 1
LLM N...Có sẵn
Truy vấn......
Phản hồi Phần thưởng Định tuyến Truy vấn Xếp hạng Mô hình Phần thưởng
Zooter Mô hình Phần thưởng
Cải thiện Nhãn dựa trên Thẻ Phân phối Phần thưởng Chưng cất Kiến thức Đào tạo
Zooter Đào tạo Định tuyến Truy vấn dựa trên Phần thưởng (Nhẹ) Xếp hạng Mô hình Phần thưởng (Nặng)
Phân phối Hình 2: Tổng quan về ZOOTER. ZOOTER nhằm tập hợp một tập các LLM có sẵn bằng cách trước tiên tiến hành xếp hạng mô hình phần thưởng trên tập đào tạo đa dạng để có được giám sát chuyên môn mô hình, được tô sáng màu xanh trong hình. Thẻ hướng dẫn sau đó được sử dụng để giảm thiểu sự không chắc chắn trong ước tính phần thưởng. ZOOTER sử dụng phần thưởng được chuẩn hóa làm giám sát để đào tạo hàm định tuyến bằng chưng cất kiến thức. Chu kỳ đào tạo được đánh dấu màu xanh lá cây, và suy luận được đánh dấu màu cam. ZOOTER nhẹ hơn nhiều về tính toán vì nó định tuyến truy vấn đến LLM chuyên gia tương ứng trong thời gian suy luận, trong khi xếp hạng mô hình phần thưởng phải tạo đầu ra cho tất cả ứng viên.

ZOOTER đạt được một tập hợp hiệu quả hơn nhiều vì nó chỉ cần suy luận một LLM chuyên gia, cộng với chi phí tính toán nhỏ của hàm định tuyến. Trong phần này, chúng tôi giới thiệu hai thành phần chính cùng với động lực thiết kế.

Chưng cất Phần thưởng. Như chúng tôi đã thảo luận trong §3.1, định tuyến truy vấn nhằm tìm một hàm định tuyến dự đoán xác suất một truy vấn q thuộc về chuyên môn của LLM Qm, trong đó Qm là tập các truy vấn mà LLM m đạt được sở thích tối đa một cách nhất quán trong tất cả ứng viên. Nhớ lại xếp hạng mô hình phần thưởng, chúng ta nhận thấy các sở thích ước tính ˆP(q, mi(q)), tức là phần thưởng, có thể được diễn giải như ưu thế tương đối của LLM mi trong tất cả ứng viên trên truy vấn q. Do đó, phần thưởng được chuẩn hóa có thể được sử dụng làm giám sát bạc cho hàm định tuyến:

Z(q)i=P(q∈Qmi)
:=exp(ˆP(q, mi(q)))∑mi∈Mexp(ˆP(q, mi(q))),

vì các ưu thế cao hơn vốn dĩ thể hiện chuyên môn của LLM trên truy vấn so với các đối tác của nó.

Để ước tính chuyên môn của mỗi mô hình và đào tạo hàm định tuyến, chúng ta cần áp dụng xếp hạng sở thích phần thưởng trên tập đào tạo đa dạng ˆQ. Chúng tôi trước tiên suy luận tất cả các mô hình ứng viên trên mỗi truy vấn ˆq∈ˆQ, và sau đó gán phần thưởng bằng mô hình phần thưởng có sẵn để có được phần thưởng vô hướng cho mỗi truy vấn và mô hình

ri={ˆP(ˆqi, mj(ˆqi))}|M|j=1, i=1, ..., |ˆQ|.

Sau đó, chúng tôi đào tạo hàm router Z trên tập đào tạo bằng chưng cất kiến thức với phân kỳ Kullback-Leibler làm hàm mất mát:

L(qi,ri) =KL(Z(qi),softmax(ri)).

ZOOTER là phương pháp hiệu quả dữ liệu và tài nguyên thấp vì tập đào tạo ˆQ chỉ chứa các truy vấn mà không có chú thích phản hồi. Tuy nhiên, các truy vấn trong tập đào tạo được mong đợi là đa dạng nhất có thể để tối đa hóa khả năng tổng quát của hàm định tuyến. Quá trình chưng cất giúp ZOOTER học chuyên môn tiềm ẩn của mỗi mô hình. Vì vậy, chúng ta có thể giảm thiểu chi phí tính toán bằng cách chỉ

--- TRANG 5 ---
đánh giá liệu một truy vấn có thuộc về tập chuyên môn với hàm định tuyến của chúng ta trong quá trình suy luận.

Cải thiện Nhãn dựa trên Thẻ. Mặc dù chưng cất phần thưởng cung cấp cách khả thi để các hàm định tuyến tận dụng giám sát bạc từ xếp hạng mô hình phần thưởng, mô hình phần thưởng ngôn ngữ cung cấp phần thưởng với sự không chắc chắn, tạo ra những nhiễu nhất định (Gleave và Irving, 2022). Chúng tôi trước tiên phân tích thực nghiệm sự không chắc chắn này trong §4.3. Các mô hình phần thưởng có sẵn hiện tại đều sẽ bao gồm nhiễu về sự không chắc chắn, như được hiển thị trong Hình 3. Do đó, chúng tôi tận dụng gắn thẻ hướng dẫn để cải thiện thêm phần thưởng trên các truy vấn đào tạo. Cải thiện nhãn dựa trên thẻ mà chúng tôi đề xuất tương tự như các kỹ thuật làm mịn nhãn được sử dụng rộng rãi và được chứng minh hiệu quả trong chưng cất kiến thức (Yuan et al., 2020). Cụ thể, chúng tôi trước tiên gắn thẻ mỗi truy vấn ˆqi∈ˆQ với bộ gắn thẻ địa phương T(·) để có được tập thẻ T(qi). Sau đó, chúng tôi tổng hợp tất cả phần thưởng trên các truy vấn có cùng thẻ cho phần thưởng theo thẻ như sau:

Qt={qi|t∈T(qi), i=1, ..., |ˆQ|}
rt=1|Qt|∑i∈Qtri

Sau đó, chúng tôi cải thiện phần thưởng cho mỗi truy vấn với phần thưởng theo thẻ bằng kết hợp tuyến tính:

r∗i=βri+(1−β)rt;t=T(qi), i=1, ..., |ˆQ|

trong đó β là siêu tham số cho sự cân bằng giữa phần thưởng theo thẻ thô và phần thưởng cấp mẫu tinh. Sau đó, chúng tôi thay thế phần thưởng gốc trong đào tạo mất mát phân kỳ KL với phần thưởng được cải thiện dựa trên thẻ r∗ trong quá trình đào tạo hàm định tuyến.

4 Thử nghiệm
Trong phần này, chúng tôi báo cáo thiết lập thử nghiệm (§4.1), kết quả chính (§4.2), và phân tích về ZOOTER (§4.3).

4.1 Thiết lập Thử nghiệm
LLM Ứng viên. Chúng tôi chọn sáu LLM dựa trên LLAMA cùng kích thước 13B làm LLM ứng viên cho định tuyến truy vấn. (a) WizardLM (Xu et al., 2023) được căn chỉnh với các truy vấn và phản hồi được tăng cường bởi EVOLINSTRUCT, (b) WizardCoder (Luo et al., 2023b) là LLM chuyên gia mã hóa sử dụng cùng kỹ thuật với WizardLM, (c) WizardMath (Luo et al., 2023a) là LLM chuyên gia toán học được căn chỉnh với tăng cường truy vấn, phần thưởng ChatGPT và tối ưu hóa PPO, (d) Vicuna (Chiang et al., 2023) được căn chỉnh trên các cuộc hội thoại khổng lồ giữa người dùng và chatbot độc quyền, (e) OpenChat (Wang et al., 2023a) được căn chỉnh với tập ShareGPT được chọn với các chiến lược đào tạo bổ sung, (f) Llama-2-Chat (Touvron et al., 2023b) được căn chỉnh trước bằng tinh chỉnh có giám sát và sau đó lấy mẫu từ chối đa lượt. Cả baseline và ZOOTER đều được thử nghiệm và đánh giá dựa trên sáu ứng viên này.

Bộ dữ liệu Đào tạo. Chúng tôi tạo một bộ dữ liệu hướng dẫn hỗn hợp đa dạng từ dữ liệu mã nguồn mở để tối đa hóa khả năng tổng quát của ZOOTER. Chúng tôi trước tiên thu thập và gắn thẻ dữ liệu mã nguồn mở từ 13 bộ dữ liệu với bộ gắn thẻ địa phương được phát triển bởi Lu et al. (2023). Để có kết quả đánh giá đáng tin cậy, chúng tôi khử ô nhiễm tất cả các mẫu chứa truy vấn có sự chồng chéo 6-gram với bất kỳ mẫu nào trong benchmark của chúng tôi được mô tả dưới đây để tránh rò rỉ dữ liệu. Sau đó, chúng tôi chọn ngẫu nhiên mười mẫu cho mỗi thẻ duy nhất để tạo thành bộ dữ liệu hướng dẫn hỗn hợp đa dạng DIVINSTRUCT với 47.986 hướng dẫn và mẫu trên 6.270 thẻ khác nhau. Thống kê chi tiết của DIVINSTRUCT trong Phụ lục §A.

Benchmark. Chúng tôi tích cực bao gồm bốn tập benchmark để đánh giá ZOOTER trên các nhiệm vụ downstream khác nhau một cách toàn diện. Chúng tôi trước tiên bao gồm ba benchmark căn chỉnh được sử dụng rộng rãi với trọng tài GPT-4:
• AlpcaEval (Li et al., 2023b) bao gồm 5 tập con từ các tập đánh giá koala, vicuna và những tập khác. Nó chứa tổng cộng 805 mẫu.
• FLASK (Ye et al., 2023) là đánh giá tinh tế cho căn chỉnh. Chúng tôi đánh giá 10 lĩnh vực trong FLASK và báo cáo điểm trung bình trên tất cả các lĩnh vực như điểm cuối cùng.
• MT-Bench (Chiang et al., 2023) là đánh giá đa lượt trên tám khía cạnh, bao gồm toán học và mã hóa. Chúng tôi chỉ đào tạo và định tuyến với truy vấn lượt đầu tiên nhưng đánh giá theo cách đa lượt như công thức gốc.

Tuy nhiên, như được báo cáo bởi Wang et al. (2023c), các đánh giá GPT-4 có thể có thiên vị và bất đồng đáng kể với con người. Do đó, chúng tôi cũng bao gồm một nhóm benchmark bao gồm MMLU (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), và HumanEval (Chen et al., 2021).

Thước đo. So sánh các mô hình tập hợp trên các benchmark khác nhau là thách thức vì quy mô điểm

--- TRANG 6 ---
Mô hình #Param AlpacaEval (5) FLASK (10) MT-Bench (8) Benchmarks (3) Tất cả (26)
Ranker Infer Avg. MTR Avg. MTR Avg. MTR Avg. MTR MTR % Uplift
Ứng viên Định tuyến
WIZARD CODER −− 13B 0.42 5.6 3.12 5.2 4.44 5.38 30.9 4.33 5.3 0.06
WIZARD LM −− 13B 0.89 2.0 3.89 1.8 7.15 2.0 44.2 2.0 1.83 0.25
WIZARD MATH −− 13B 0.47 5.0 3.28 5.0 5.73 4.38 34.8 4.0 4.6 0.03
LLAMA -2-CHAT −− 13B 0.91 1.6 3.88 1.5 6.72 2.88 32.3 3.67 2.23 0.31
OPENCHAT −− 13B 0.89 2.2 3.79 3.1 7.12 2.0 31.2 3.33 2.67 0.19
VICUNA −− 13B 0.8 3.8 3.7 3.5 6.58 3.25 33.6 2.67 3.4 0.06
BMA −− 13B 0.91 1.6 3.88 1.5 6.72 2.88 32.3 3.67 2.23 0.31
ZOOTER
Của chúng tôi 86M 13B 0.93 1.17 3.89 1.82 7.11 2.33 34.2 3.0 1.94 0.44
Xếp hạng Mô hình Phần thưởng (RMR)
W/ OA SSIST RM 300M 6 ×13B 0.79 4.0 3.75 3.73 6.59 3.22 35.1 3.25 3.42 0.19
W/ LLM-B LENDER 300M 6 ×13B 0.83 3.67 3.77 3.36 6.21 4.0 36.4 2.75 3.39 0.17
W/ AUTO-J 13B 6 ×13B 0.89 2.67 3.92 1.64 7.03 2.22 32.2 3.5 2.25 0.42
W/ ULTRA RM 13B 6 ×13B 0.92 1.17 4.06 1.0 7.18 1.89 40.1 3.25 1.53 0.72
W/ QWEN RM 7B 6 ×13B 0.92 1.33 4.04 1.0 7.26 2.11 38.6 3.0 1.58 0.67
W/ ORACLE −− 6×13B 0.98 1.0 4.56 1.0 8.25 1.0 75.3 1.0 1.0 1.0
Mô hình Độc quyền
GPT-3.5-turbo −− −− 0.89 2.67 4.06 1.91 7.94 1.78 73.0 1.0 1.78 0.61
GPT-4 −− −− 0.94 1.0 4.37 1.0 8.99 1.0 88.3 1.0 1.0 1.0

Bảng 1: Kết quả chính của cả ZOOTER và xếp hạng mô hình phần thưởng. Chúng tôi báo cáo hiệu suất trên bốn nhóm benchmark và báo cáo số lượng tập con bên cạnh tên của benchmark. Chúng tôi cũng báo cáo các tham số của ranker và tổng số mô hình suy luận cho cả ứng viên và phương pháp tập hợp. MTR biểu thị tỷ lệ nhiệm vụ trung bình, và %Uplift biểu thị tỷ lệ nâng cao. Điểm trung bình và tỷ lệ nâng cao càng cao càng tốt trong khi MTR càng thấp càng tốt. Chúng tôi đánh dấu điểm tốt hơn bằng màu xanh đậm hơn để trực quan hóa tốt hơn và dễ diễn giải hơn.

khác nhau trên mỗi benchmark. Để giải quyết vấn đề này, chúng tôi không chỉ báo cáo điểm trên mỗi benchmark mà còn tỷ lệ nhiệm vụ trung bình (MTR). Tất cả benchmark chúng tôi đánh giá có nhiều tập con, chúng tôi định nghĩa MTR là thứ hạng của mô hình được đánh giá trong tất cả baseline trung bình trên tất cả tập con. MTR chỉ về thứ hạng trong baseline nên có thể dễ dàng áp dụng trên các benchmark có quy mô điểm khác nhau. Tương tự, chúng tôi cũng đề xuất tỷ lệ nâng cao, biểu thị tỷ lệ tập con mà mô hình được đánh giá đạt được hiệu suất tốt nhất của benchmark. Chúng tôi báo cáo hai thước đo này trên tổng cộng 26 tập con đánh giá trong tất cả benchmark. MTR thấp hơn và tỷ lệ nâng cao cao hơn cho thấy mô hình được đánh giá có hiệu suất cao hơn nhất quán trong các nhiệm vụ downstream đa dạng.

Baseline. Chúng tôi cũng so sánh ZOOTER với các phương pháp xếp hạng mô hình phần thưởng (RMR) hiện có. Chúng tôi thiết lập baseline RMR với các mô hình phần thưởng mới nhất, bao gồm OA SSIST RM, AUTO-J (Li et al., 2023a), ULTRA RM (Cui et al., 2023), QWEN RM (Bai et al., 2023), và xếp hạng Oracle để tham khảo. Chúng tôi cũng xem xét xếp hạng cặp trong LLM-Blender (Jiang et al., 2023) như một trong những phương pháp RMR. Ngoài ra, chúng tôi cũng báo cáo hiệu suất của các mô hình độc quyền trên các bộ sưu tập benchmark của chúng tôi để tham khảo, bao gồm GPT-3.5-turbo và GPT-4.

Cấu hình. Chúng tôi đào tạo hàm định tuyến từ mdeberta-v3-base. Và chúng tôi sử dụng QwenRM để tạo phần thưởng trên các truy vấn đào tạo làm giám sát cho hàm định tuyến, vì nó đạt được hiệu suất tốt nhất trong xếp hạng mô hình phần thưởng với tham số mô hình nhỏ hơn đáng kể được mô tả trong §4.2. Và chúng tôi chạy tất cả đào tạo và suy luận trên 8 GPU A100. Chúng tôi suy luận và đánh giá tất cả benchmark với cấu hình tương ứng và cài đặt GPT-4. Chúng tôi sử dụng giải mã tham lam cho MMLU, GSM8K, và HumanEval.

4.2 Kết quả
Chúng tôi trình bày kết quả chính trong Bảng 1. Chúng tôi báo cáo hiệu suất của sáu ứng viên định tuyến trên benchmark của chúng tôi, và mô hình tốt nhất trung bình (BMA) là LLAMA-2-CHAT. Và chúng tôi báo cáo ZOOTER với β=0.3 trong cải thiện nhãn dựa trên thẻ. Chúng tôi tiếp tục phân tích kết quả trong hai khía cạnh sau:

--- TRANG 7 ---
Tiềm năng Bổ sung. Chúng tôi đánh giá tập hợp với xếp hạng mô hình phần thưởng (RMR) trên năm mô hình phần thưởng có sẵn khác nhau. RMR với UltraRM đạt được hiệu suất tốt nhất trong MTR và tỷ lệ nâng cao trên tổng hợp của tất cả benchmark, xếp hạng tại 1.53 và đạt được mô hình tốt nhất trên 72% nhiệm vụ con. RMR với QwenRM đạt được tốt nhì và có hiệu suất tương tự với UltraRM với kích thước tham số nhỏ hơn, theo sau là RMR với Auto-J, LLM-Blender, và OAssistRM. RMR với QwenRM, UltraRM, và Auto-J vượt trội hơn BMA, cho thấy hiệu quả của RMR. Hơn nữa, chúng tôi cũng tính toán điểm của RMR với ranker Oracle, luôn vượt trội hơn tất cả ứng viên và thậm chí vượt trội hơn GPT-4 trên AlpacaEval và FLASK. Những kết quả như vậy cung cấp bằng chứng vững chắc cho tiềm năng bổ sung của các LLM có sẵn và cũng hỗ trợ động lực chính đằng sau ZOOTER, tức là sử dụng phần thưởng từ các mô hình phần thưởng có sẵn làm giám sát bạc cho đào tạo hàm định tuyến. Tuy nhiên, chúng tôi nhận thấy RMR thất bại trên các benchmark, chẳng hạn như MMLU, GSM8K, và HumanEval, cho thấy việc đánh giá chính xác kiến thức, toán học, và các vấn đề mã hóa vẫn còn thách thức đối với các RM hiện có.

Hiệu suất Zooter. Sau đó chúng tôi so sánh hiệu suất của ZOOTER với BMA và RMR. ZOOTER vượt trội hơn BMA trên AlpacaEval, MT-Bench, và Benchmarks, và đạt được hiệu suất tương tự trên FLASK. Cải thiện đáng kể nhất được chứng kiến trên MT-Bench, nơi hiệu suất của ZOOTER cao hơn BMA 0.39. Nói chung, ZOOTER đạt được top-1 trên 44% nhiệm vụ con trong khi BMA chỉ trên 31%. Với bằng chứng trên, ZOOTER thành công sử dụng tiềm năng bổ sung giữa các LLM để đạt được hiệu suất tốt nhất nhất quán hơn trên benchmark của chúng tôi, với chi phí tính toán chỉ từ ranker 86M. Đồng thời, ZOOTER vượt trội hơn RMR với OAssistRM, LLM-Blender, và Auto-J, bằng chi phí tính toán ít hơn đáng kể. Tuy nhiên, mặc dù ZOOTER vượt trội hơn RMR với QwenRM trên AlpacaEval, vẫn có khoảng cách rõ ràng giữa ZOOTER và RMR với QwenRM nói chung.

4.3 Phân tích
Chúng tôi cung cấp phân tích thêm về cách sự không chắc chắn RM có thể ảnh hưởng đến đào tạo ZOOTER.

Sự không chắc chắn RM. Như được trình bày trong nghiên cứu trước, RM có thể có sự không chắc chắn về phần thưởng vô hướng của nó, có thể tạo ra nhiễu trong đào tạo định tuyến vì chúng tôi sử dụng điểm RM làm giám sát bạc. Trong phần này, chúng tôi trước tiên trình bày sự tồn tại của sự không chắc chắn này để giải thích động lực đằng sau cải thiện nhãn dựa trên thẻ, phương pháp chúng tôi đề xuất để giảm thiểu sự không chắc chắn như vậy trong đào tạo hàm định tuyến. Chúng tôi tính toán entropy của phần thưởng từ QwenRM trong tất cả LLM ứng viên cho mỗi truy vấn trong MT-Bench và vẽ nó với điểm MT-Bench của mỗi mẫu bằng xếp hạng sở thích phần thưởng với QwenRM. Như được hiển thị trong Hình 3, các mẫu có entropy phần thưởng thấp hơn có xu hướng có điểm MT-bench cao. Chúng tôi diễn giải quan sát này là entropy phần thưởng cao hơn tiết lộ sự không chắc chắn nhiều hơn trong phần thưởng. Do đó, chúng tôi đề xuất cải thiện nhãn dựa trên thẻ để tận dụng prior dựa trên thẻ để điều chỉnh entropy phần thưởng.

[Hình 3: Phân tích giữa entropy phần thưởng và điểm xếp hạng sở thích phần thưởng trên MT-bench.]

βAlpacaEval FLASK MT-Bench Benchmarks Tất cả
0 1.4 2.2 2.25 3.67 2.06
0.1 1.2 2.1 2.38 3.67 2.00
0.3 1.2 1.9 2.50 3.67 1.97
0.5 1.2 2.2 3.12 3.67 2.23
0.7 1.2 2.2 3.38 4.00 2.31
0.9 1.2 2.3 3.12 4.00 2.31
1.0 1.2 2.3 3.25 4.00 2.34

Bảng 2: Tỷ lệ nhiệm vụ trung bình (MTR) của β khác nhau trong cải thiện nhãn dựa trên thẻ trên tất cả benchmark. Giá trị tốt nhất của β được đánh dấu màu xanh.

Cải thiện Nhãn. Cải thiện nhãn dựa trên thẻ được đề xuất trong §3.2 chứa siêu tham số β, đại diện cho sự cân bằng giữa phần thưởng cấp mẫu tinh và phần thưởng cấp thẻ thô. Chúng tôi tiến hành thử nghiệm để điều chỉnh siêu tham số này và phân tích cách phần thưởng ở các mức độ chi tiết khác nhau có thể

--- TRANG 8 ---
ảnh hưởng đến đào tạo hàm định tuyến của chúng tôi. Như được hiển thị trong Bảng 2, ZOOTER đạt được hiệu suất tốt nhất khi β bằng 0.3, chứng minh sự kết hợp của phần thưởng cấp mẫu và cấp thẻ sẽ có lợi cho chưng cất phần thưởng. Ablation cũng cho thấy sự cần thiết của cải thiện nhãn dựa trên thẻ. Hơn nữa, chưng cất phần thưởng cấp thẻ (β=0) cho thấy hiệu suất tốt hơn đáng kể so với chưng cất phần thưởng cấp mẫu (β=1), hỗ trợ phân tích rằng nhiễu từ sự không chắc chắn của RM trong phần thưởng cấp mẫu làm hại chưng cất phần thưởng.

5 Kết luận
Trong công việc này, chúng tôi xem xét lại tiềm năng bổ sung của các LLM mã nguồn mở và xếp hạng mô hình phần thưởng của nhiều mô hình phần thưởng có sẵn, cung cấp bằng chứng cho hiệu quả của tập hợp LLM. Chúng tôi đề xuất ZOOTER, một phương pháp định tuyến hiệu quả dựa trên phần thưởng để tập hợp các LLM có sẵn. Đánh giá toàn diện cho thấy ZOOTER có thể vượt trội hơn mô hình đơn tốt nhất trung bình và thậm chí các mô hình tập hợp bằng xếp hạng mô hình phần thưởng với chi phí tính toán ít hơn đáng kể. Công việc tương lai có giá trị bao gồm đi sâu vào diễn giải chuyên môn tiềm ẩn trong mỗi LLM.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo đầy đủ...]

--- TRANG 9 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 10 ---
Bộ dữ liệu Số lượng
ultrachat 18,588
sharedgpt 10432
wizardlm(sharedgpt) 5325
wizardlm(alpaca) 5145
alpaca 2186
repair 1034
openchat 1033
flan 862
math 849
unnatural 582
dmcc 573
dolly 560
oasst 183
lima 70
mbpp 43

Bảng 3: Thành phần của DIVINSTRUCT

A Bộ dữ liệu
DIVINSTRUCT là tập hướng dẫn hỗn hợp đa dạng từ nhiều bộ dữ liệu mã nguồn mở với khử ô nhiễm cẩn thận trên tất cả benchmark được đánh giá trong công việc này. Thành phần chi tiết của DIVINSTRUCT được báo cáo trong Bảng 3.
