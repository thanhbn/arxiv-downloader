# 2308.12014.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2308.12014.pdf
# Kích thước file: 580675 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Từ Hướng dẫn đến Giá trị Con người Nội tại ——
Một Khảo sát về Mục tiêu Đồng bộ hóa cho Mô hình Lớn
Jing Yao, Xiaoyuan Yi, Xiting Wang, Jindong Wang và Xing Xie
Microsoft Research Asia
{jingyao,xiaoyuanyi,xiting.wang,jindong.wang,xing.xie}@microsoft.com
Tóm tắt
Mô hình lớn, được minh họa bởi Mô hình Ngôn ngữ Lớn (LLMs), thường là những mô hình được tiền huấn luyện trên dữ liệu khổng lồ và bao gồm số lượng tham số to lớn, không chỉ đạt được hiệu suất cải thiện đáng kể trên các tác vụ đa dạng mà còn thể hiện các khả năng nổi lên vắng mặt trong các mô hình nhỏ hơn. Tuy nhiên, sự gắn kết ngày càng tăng của mô hình lớn với cuộc sống hàng ngày của con người đặt ra những rủi ro tiềm ẩn và có thể gây ra tác hại xã hội nghiêm trọng. Do đó, nhiều nỗ lực đã được thực hiện để đồng bộ hóa LLMs với con người nhằm khiến chúng phục vụ con người tốt hơn và thỏa mãn các sở thích của con người. Tuy nhiên, câu hỏi cơ bản 'đồng bộ hóa với cái gì' chưa được thảo luận đầy đủ, và các mục tiêu đồng bộ hóa không phù hợp thậm chí có thể phản tác dụng. Trong bài báo này, chúng tôi tiến hành một khảo sát toàn diện về các mục tiêu đồng bộ hóa khác nhau trong công trình hiện có và truy tìm các đường đi tiến hóa của chúng để giúp xác định mục tiêu quan trọng nhất mà LLMs nên được đồng bộ hóa. Cụ thể, chúng tôi điều tra các công trình liên quan từ hai góc độ: định nghĩa các mục tiêu đồng bộ hóa và đánh giá đồng bộ hóa. Phân tích của chúng tôi bao gồm ba cấp độ riêng biệt của mục tiêu đồng bộ hóa, tức là hướng dẫn của con người, sở thích của con người, và giá trị của con người, điều này tiết lộ một sự chuyển đổi mục tiêu từ khả năng cơ bản đến định hướng giá trị và chỉ ra tiềm năng của giá trị con người nội tại như mục tiêu đồng bộ hóa cho LLMs được nâng cao. Dựa trên những kết quả như vậy, chúng tôi tiếp tục thảo luận về những thách thức trong việc đạt được sự đồng bộ hóa giá trị nội tại như vậy và cung cấp một bộ sưu tập các tài nguyên có sẵn cho nghiên cứu tương lai về đồng bộ hóa mô hình lớn.

1 Giới thiệu
Mô hình Lớn, còn được gọi là Mô hình Nền tảng (Bommasani et al., 2021), thường đề cập đến những mô hình được tiền huấn luyện trên dữ liệu rộng lớn và chứa hàng chục tỷ tham số. Những ví dụ nổi bật nhất bao gồm Mô hình Ngôn ngữ Lớn (LLMs), ví dụ như GPT-3 (Brown et al., 2020), ChatGPT (Ouyang et al., 2022), GPT-4 (OpenAI, 2023) và những mô hình khác (Touvron et al., 2023a,b; Zhang et al., 2022; Scao et al., 2022), cũng như Mô hình Đa phương tiện Lớn (LMMs). Mô hình lớn sở hữu hai đặc điểm khác biệt so với Mô hình Ngôn ngữ Tiền huấn luyện (PLMs) chung: 1) quy luật mở rộng (Kaplan et al., 2020), nơi chúng thể hiện hiệu suất tốt hơn đáng kể với việc tăng kích thước mô hình; và 2) khả năng nổi lên (Wei et al., 2022a), nơi các khả năng đặc biệt vắng mặt trong các mô hình nhỏ hơn đã xuất hiện, như học trong ngữ cảnh (Brown et al., 2020) và lý luận phức tạp (Wei et al., 2022a).

LLMs hiện tại thể hiện khả năng giống con người hoặc thậm chí vượt con người qua nhiều tác vụ đa dạng (Bubeck et al., 2023). Tuy nhiên, 'cơ hội và rủi ro luôn đi đôi với nhau', những thách thức và rủi ro xuất hiện khi áp dụng mô hình lớn. Một mặt, những mô hình này đôi khi gặp khó khăn trong việc hiểu và tuân theo các hướng dẫn đa dạng của người dùng (Tamkin et al., 2021; Kenton et al., 2021). Mặt khác, mô hình lớn có thể tạo ra các phản hồi mâu thuẫn với sở thích của con người, như phân biệt đối xử và tin nhắn có hại, gây ra rủi ro xã hội tiềm ẩn (Weidinger et al., 2021; Bommasani et al., 2021). Hơn nữa, những rủi ro này thể hiện hai đặc điểm đi kèm với khả năng: 1) rủi ro nổi lên (Wei et al., 2022a), nơi các vấn đề không lường trước xuất hiện; và 2) quy luật mở rộng nghịch đảo (McKenzie et al., 2023), nơi một số rủi ro không biến mất mà trở nên nghiêm trọng hơn với kích thước mô hình tăng. Điều này ngụ ý rằng mô hình lớn có thể tiềm ẩn nâng cao rủi ro lớn hơn.

Để làm cho mô hình lớn phục vụ con người tốt hơn và loại bỏ các rủi ro tiềm ẩn, việc đồng bộ hóa chúng với con người đã trở thành một chủ đề được quan tâm cao, điều này kích thích nhiều nỗ lực nghiên cứu (Kenton et al., 2021; Gabriel, 2020), đặc biệt là đối với LLMs. Hầu hết tài liệu hiện có rơi vào ba loại. Loại đầu tiên cam kết nâng cao khả năng mô hình điển hình để tuân theo hướng dẫn người dùng và giải quyết các tác vụ đa dạng (Sanh et al., 2021; Mishra et al., 2021; Wang et al., 2022b). Họ thu thập hoặc tổng hợp một tập dữ liệu lớn các minh chứng tác vụ để huấn luyện LLMs theo cách có giám sát. Trong loại thứ hai (Nakano et al., 2021; Stiennon et al., 2020; Wu et al., 2021; Köpf et al., 2023), LLMs được huấn luyện với phản hồi con người ngầm định hoặc tín hiệu so sánh trên các cặp hành vi mô hình để học các sở thích con người chung (như 'không có nội dung xúc phạm' và 'câu trả lời chi tiết hơn') và tạo ra các phản hồi được con người ưa thích, mặc dù không có làm rõ rõ ràng về hành vi nào con người thích. Thêm vào đó, dòng nghiên cứu thứ ba, tương đối mới nổi, cố gắng đồng bộ hóa LLMs với một tập hợp các nguyên tắc được định nghĩa trước phản ánh các giá trị cốt lõi được cộng đồng con người trân trọng (Liu et al., 2022; Sun et al., 2023c; Bai et al., 2022b,a). Ví dụ, 'HHH', một trong những tiêu chí đồng bộ hóa phổ biến nhất, mong đợi LLMs hữu ích, trung thực và vô hại (Bai et al., 2022a; Ganguli et al., 2022). Trong Constitutional AI (Bai et al., 2022b), nhiều nguyên tắc giá trị được chỉ định để tạo tập dữ liệu cho huấn luyện mô hình, bao gồm việc vô hại và đạo đức. Tất cả những nỗ lực này góp phần đồng bộ hóa LLMs với con người, nhưng thực sự, chúng tập trung vào việc đạt được các mục tiêu đồng bộ hóa khác nhau, từ khả năng cơ bản đến định hướng giá trị. Và các mục tiêu tối ưu hóa tương ứng cũng từ hành vi mô hình cụ thể đến giá trị con người toàn diện và nội tại, như được thể hiện trong Hình 1.

Khi mục tiêu thay đổi, việc đồng bộ hóa LLMs-con người đặt ra các phương pháp luận khác nhau và cũng dẫn đến các hậu quả riêng biệt (Kenton et al., 2021). Mặc dù sự phát triển nhanh chóng của nghiên cứu đồng bộ hóa sau sự xuất hiện của mô hình lớn (Wang et al., 2023), thiếu thảo luận và phân tích sâu sắc về loại mục tiêu đồng bộ hóa nào là phù hợp và quan trọng nhất (tức là, đồng bộ hóa với cái gì?).

Trong bài báo này, chúng tôi nhấn mạnh tầm quan trọng của các mục tiêu phù hợp cho đồng bộ hóa mô hình lớn, và cam kết tiến hành một khảo sát toàn diện về các mục tiêu đồng bộ hóa khác nhau trong các công trình hiện có. Bằng cách phân biệt bản chất của các mục tiêu đồng bộ hóa khác nhau, chúng tôi chủ yếu chia chúng thành ba cấp độ: hướng dẫn của con người, sở thích của con người và giá trị của con người, cung cấp một định nghĩa đại diện cho mỗi loại, và phân tích điểm mạnh và điểm yếu riêng của chúng. Sự tiến hóa của các mục tiêu đồng bộ hóa đã chứng kiến quá trình thay đổi của kỳ vọng con người đối với việc đồng bộ hóa LLMs, từ sự tuân thủ bề mặt của các hướng dẫn cụ thể đến các giá trị nội tại ổn định và quan trọng hơn, điều này cũng cho thấy sự tương đồng lớn với giáo dục con người. Truy tìm quá trình tiến hóa này có thể soi sáng vấn đề nghiên cứu quan trọng về đồng bộ hóa: LLMs nên được đồng bộ hóa với cái gì?. Như được thể hiện trong Hình 2, chúng tôi tóm tắt các công trình liên quan trong ba cấp độ mục tiêu đồng bộ hóa từ hai góc độ quan trọng. (1) Định nghĩa mục tiêu đồng bộ hóa, nơi một định nghĩa rõ ràng của mỗi mục tiêu đồng bộ hóa và cách biểu diễn nó như một mục tiêu huấn luyện cho mô hình lớn được giới thiệu. (2) Đánh giá đồng bộ hóa, tương ứng với các tiêu chuẩn và phương pháp đánh giá mức độ đạt được các mục tiêu đồng bộ hóa này bởi mô hình lớn. Sau đó, chúng tôi trình bày một giới thiệu ngắn gọn về các thuật toán đồng bộ hóa chính, trả lời câu hỏi quan trọng khác cho việc đồng bộ hóa, tức là làm thế nào để đồng bộ hóa LLMs với một mục tiêu cho trước. Cuối cùng, đặt giá trị con người nội tại như mục tiêu đồng bộ hóa hứa hẹn cho mô hình lớn, chúng tôi thảo luận về những thách thức và hướng nghiên cứu tương lai.

Những đóng góp chính của chúng tôi được liệt kê như sau:
• Chúng tôi nhấn mạnh tầm quan trọng của các mục tiêu đồng bộ hóa phù hợp cho mô hình lớn và cung cấp khảo sát toàn diện đầu tiên từ hai góc độ: định nghĩa mục tiêu đồng bộ hóa và đánh giá đồng bộ hóa.
• Chúng tôi bao gồm ba cấp độ mục tiêu đồng bộ hóa: hướng dẫn của con người, sở thích của con người và giá trị của con người, trình bày một định nghĩa cho mỗi loại và truy tìm các đường đi tiến hóa của chúng để xác định mục tiêu phù hợp nhất.
• Với việc làm rõ một mục tiêu phù hợp và quan trọng cho đồng bộ hóa mô hình lớn, chúng tôi thảo luận về các thách thức chính và hướng nghiên cứu tương lai có thể.
• Chúng tôi tóm tắt các tài nguyên có sẵn để đạt được các mục tiêu đồng bộ hóa khác nhau, cũng như các tiêu chuẩn và nền tảng cho việc đánh giá đồng bộ hóa LLMs. Tất cả những điều này đều được mở nguồn tại https://github.com/ValueCompass/Alignment-Goal-Survey.

Phần còn lại của bài báo này được tổ chức như sau. Trong Phần 2, chúng tôi định nghĩa các cấp độ khác nhau của mục tiêu đồng bộ hóa và giới thiệu cách biểu diễn chúng cho huấn luyện mô hình. Trong Phần 3, chúng tôi tóm tắt cách đánh giá hiệu suất đồng bộ hóa của các mục tiêu được đề cập ở trên. Sau đó, Phần 4 giới thiệu ngắn gọn các thuật toán chính cho đồng bộ hóa. Và Phần 5 thảo luận về những thách thức và hướng nghiên cứu tương lai. Cuối cùng, chúng tôi kết luận toàn bộ bài báo trong Phần 6.

--- TRANG 2 ---
Hướng dẫn: Hãy cho tôi biết liệu câu này có thực sự đúng không. 
Có hoặc không?
Đầu vào: Mount Rainier là ngọn núi cao thứ hai
ở Bắc Mỹ.
Đầu ra: Không

Đầu vào: Phụ nữ có
phù hợp hơn để làm giáo viên
mầm non hơn nam giới không?

Câu trả lời Ưu tiên: Giới tính không
phải là chìa khóa để xác định, mà là
sở thích, đam mê cá nhân…..

Câu trả lời Không Ưa thích: Có,
bởi vì phụ nữ kiên nhẫn hơn,
chu đáo và nuôi dưỡng hơn

Nguyên tắc Giá trị:
1. Hữu ích để trả lời
các câu hỏi đầu vào hợp lý.
2. Trung thực để cung cấp thông tin
thực, tránh ảo giác.
3. Không phân biệt đối xử,
xúc phạm, độc hại…

Câu trả lời: Có, bởi vì
phụ nữ kiên nhẫn hơn,
chu đáo và nuôi dưỡng hơn.

Câu trả lời: Giới tính không phải là chìa khóa
để xác định, mà là
sở thích, đam mê cá nhân….

Con người

Hướng dẫn Con người

Sở thích Con người

Giá trị Con người

Đồng bộ

Đồng bộ

Đồng bộ

Vi phạm Quy tắc 3

Khả năng Cơ bản - Định hướng Giá trị

Hành vi Bề mặt - Giá trị Nội tại

(với nguyên tắc giá trị
toàn diện được định nghĩa rõ ràng)

(với phản hồi con người
về hành vi cụ thể)

(với hướng dẫn &
minh chứng)

Hình 1: Minh họa ba mục tiêu đồng bộ hóa: hướng dẫn của con người, sở thích của con người và giá trị của con người, với sự nhấn mạnh chuyển từ khả năng cơ bản đến định hướng giá trị. Các mục tiêu huấn luyện tương ứng từ hành vi bề mặt đến giá trị nội tại.

--- TRANG 3 ---
Mục tiêu Đồng bộ hóa

Định nghĩa
(Phần 2)

Hướng dẫn Con người
PromptSource (Bach et al., 2022); P3 (Sanh et al., 2021); Natural Inst. (Mishra et al., 2021); Super-Natural Inst. (Wang et al., 2022b);
GLM (Zeng et al., 2022); xP3 (Muennighoff et al., 2022); Unnatural Inst. (Honovich et al., 2022); Self-Instruct (Wang et al., 2022a);
Flan 2022 (Longpre et al., 2023); OPT-IML Bench (Iyer et al., 2022); Aplaca (Taori et al., 2023); ShareGPT (Chiang et al., 2023);
Baize (Xu et al., 2023a); COIG (Zhang et al., 2023a); LLaV A (Liu et al., 2023a); LLaV AR (Zhang et al., 2023b)

Sở thích Con người

Minh chứng Con người
InstructGPT (Ouyang et al., 2022); Summarization (Stiennon et al., 2020; Wu et al., 2021);
WebGPT (Nakano et al., 2021);OpenAssistant (Köpf et al., 2023); Game Reward (Kwon et al., 2023)

Phản hồi Con người
Summarization (Stiennon et al., 2020; Wu et al., 2021); WebGPT (Nakano et al., 2021);
InstructGPT (Ouyang et al., 2022); OpenAssistant (Köpf et al., 2023)

Phản hồi Tổng hợp Mô hình
Game Reward (Kwon et al., 2023); IFL (Scheurer et al., 2023); ALMoST (Kim et al., 2023);
Stable Alignment (Liu et al., 2023b); Clever Flamingo (Chen et al., 2023a)

Giá trị Con người

Nguyên tắc Giá trị
HHH (hữu ích
trung thực & vô hại)
HH-RLHF (Bai et al., 2022a); Sparrow (Glaese et al., 2022); Contitutional AI (Bai et al., 2022b);
SELF-ALIGN (Sun et al., 2023c); PALMS (Solaiman and Dennison, 2021); BeaverTails (Ji et al., 2023)

Chuẩn mực Xã hội
& Đạo đức
Moral Integity Corpus (Ziems et al., 2022); Social Chemistry 101 (Forbes et al., 2020);
Moral Stories (Emelin et al., 2020); ETHICS (Hendrycks et al., 2020); Scruples (Lourie et al., 2021);
MoralDial (Sun et al., 2023b); Goofus & Gallant (Nahian et al., 2020)

Hệ thống Giá trị Cơ bản
Schwartz Theory of Basic Human Values (Qiu et al., 2022); Rokeach Values (Qiu et al., 2022);
Moral Foundations Theory (Simmons, 2022)

Biểu diễn Mục tiêu

Hành vi Mong muốn
HH-RLHF (Bai et al., 2022a); Red-Teaming (Ganguli et al., 2022); BeaverTails (Ji et al., 2023);
ETHICS (Hendrycks et al., 2020); SBIC (Sap et al., 2019)

Giá trị Nội tại
Sparrow (Glaese et al., 2022); Contitutional AI (Bai et al., 2022b); SELF-ALIGN (Sun et al., 2023c);
PALMS (Solaiman and Dennison, 2021); Moral Integity Corpus (Ziems et al., 2022);
Moral Stories (Emelin et al., 2020); Social Chemistry 101 (Forbes et al., 2020); Delphi (Jiang et al., 2021)

Đánh giá
(Phần 3)

Hướng dẫn Con người

Tiêu chuẩn
Super-Natural Inst (Wang et al., 2022b); PromptSource (Sanh et al., 2021); OPT-IML (Iyer et al., 2022);
Flan 2022 (Longpre et al., 2023); Big-Bench (Srivastava et al., 2022); C-Eval (Huang et al., 2023);
AGIEval (Zhong et al., 2023); LM-Written Evaluation (Perez et al., 2022)

Đánh giá LLMs Tiên tiến
AlpacaEval (Li et al., 2023); AlpacaFarm (Dubois et al., 2023); Vicuna (Chiang et al., 2023);
LLM-as-a-Judge (Zheng et al., 2023)

Sở thích Con người

Tiêu chuẩn
TruthfulQA (Lin et al., 2022); OpenBookQA (Mihaylov et al., 2018); CrowS-Pairs (Nangia et al., 2020);
WinoGender (Rudinger et al., 2018); BBQ (Parrish et al., 2021); BOLD (Dhamala et al., 2021);
RealToxicityPrompts (Gehman et al., 2020); ToxiGen (Hartvigsen et al., 2022); BIG-Bench (Srivastava et al., 2022);
HELM (Liang et al., 2022); LM-Written Evaluation (Perez et al., 2022)

Đánh giá Con người
InstructGPT (Ouyang et al., 2022); Llama2 (Touvron et al., 2023b); RRHF (Yuan et al., 2023);
Summarization (Wu et al., 2021); Almost (Kim et al., 2023)

Mô hình Phần thưởng
Llama2 (Touvron et al., 2023b); HH-RLHF (Yuan et al., 2023); DPO (Rafailov et al., 2023);
RAFT (Dong et al., 2023); GRUE (Ramamurthy et al., 2022); HPS v2 (Wu et al., 2023)

Giá trị Con người

Tiêu chuẩn
An toàn và Rủi ro: HH-RLHF (Bai et al., 2022a); SafetyPrompts (Sun et al., 2023a); SafeText (Levy et al., 2022);
CValues (Xu et al., 2023b)
Chuẩn mực Xã hội: Moral Integity Corpus (Ziems et al., 2022); Social Chemistry 101 (Forbes et al., 2020);
Moral Stories (Emelin et al., 2020); ETHICS (Hendrycks et al., 2020); Moral Mimicry (Simmons, 2022);
SCRUPLES (Lourie et al., 2021); MoralExceptQA (Jin et al., 2022); ETHICAL QUANDARY GQA (Bang et al., 2022)
Khảo sát Giá trị Con người: GlobalOpinionQA (Durmus et al., 2023) (World Values Survey (WVS) (wor, 2021) & Pew Research
Center's Global Attitudes surveys (GAS) (pew, 2022)); Cultural Values (Arora et al., 2022) (Hofstede's Cultural Survey
(Hofstede, 1984) & World Values Survey (WVS))

Mô hình Phần thưởng
HH-RLHF (Bai et al., 2022a); Goofus & Gallant (Nahian et al., 2020); Delphi (Jiang et al., 2021);
V ALUENET (Qiu et al., 2022); Moral Foundation Twitter Copus (Hoover et al., 2020)

Hình 2: Phân loại các bài báo được xem xét về các mục tiêu đồng bộ hóa khác nhau.

--- TRANG 4 ---
2 Mục tiêu Đồng bộ hóa

Đồng bộ hóa mô hình lớn với con người là cần thiết để chúng có thể phục vụ và hợp tác với con người tốt hơn. Đối với các giai đoạn phát triển khác nhau của mô hình lớn và yêu cầu ngày càng tăng của con người, nhiều nỗ lực được dành để điều tra đồng bộ hóa mô hình lớn từ các góc độ khác nhau, mà mục tiêu trong bài báo này về cơ bản được chia thành ba cấp độ, tức là hướng dẫn của con người, sở thích của con người và giá trị của con người. Để đạt được các mục tiêu đồng bộ hóa này, mỗi mục tiêu đã được định nghĩa phù hợp và được biểu diễn như một mục tiêu cho huấn luyện mô hình theo nhiều cách khác nhau. Trong phần này, chúng tôi tóm tắt các công trình hiện có và trình bày sự phân biệt rõ ràng giữa ba mục tiêu đồng bộ hóa này, cũng như các phương pháp biểu diễn của chúng.

2.1 Hướng dẫn Con người

Thông thường, LLMs được tiền huấn luyện với mục tiêu dự đoán token tiếp theo (Brown et al., 2020; Zhang et al., 2022). Mặc dù những mô hình này đã thể hiện khả năng zero-shot và few-shot ấn tượng trong một số tác vụ (Brown et al., 2020), mà chúng tôi suy luận được học từ các mẫu trong kho dữ liệu huấn luyện khổng lồ, LLMs vẫn gặp khó khăn trong việc giúp người dùng hoàn thành các tác vụ đa dạng khi được đưa ra một số hướng dẫn. Do đó, chúng tôi coi hướng dẫn của con người là cấp độ đầu tiên của mục tiêu đồng bộ hóa, được định nghĩa là cho phép mô hình lớn hoàn thành các tác vụ đa dạng mà con người hướng dẫn chúng thực hiện. Mục tiêu này tập trung vào khả năng cơ bản của mô hình lớn để tạo ra kết quả đúng được định nghĩa một cách hẹp, mà không có kỳ vọng đáp ứng sở thích của con người. Đạt được mục tiêu này đặt nền tảng cho các cấp độ đồng bộ hóa tiên tiến hơn.

Hầu hết các nghiên cứu thu thập một tập dữ liệu hướng dẫn để thực hiện như một đại diện của mục tiêu đồng bộ hóa này và tinh chỉnh LLMs được tiền huấn luyện trên tập dữ liệu này theo cách có giám sát (Wang et al., 2022a; Chung et al., 2022; Longpre et al., 2023; Chen et al., 2023b; Zhou et al., 2023). Mỗi phần dữ liệu được hình thức hóa như một định dạng thống nhất <hướng dẫn, đầu vào, đầu ra>, nơi hướng dẫn mô tả tác vụ và đầu ra được mong đợi tạo ra cho đầu vào cho trước khi tuân theo hướng dẫn. Phương pháp tinh chỉnh hướng dẫn như vậy dựa vào khả năng zero-shot và few-shot của mô hình lớn trong một mô hình dựa trên prompt, do đó xuất hiện sau sự ra đời của GPT-3 (Brown et al., 2020). Để đối phó với tính đa dạng và vô hạn của hướng dẫn con người, các nỗ lực từ ba góc độ chủ yếu được xem xét để tạo ra các tập dữ liệu chất lượng cao, điều này cho phép mô hình tổng quát hóa tốt hơn cho các hướng dẫn chưa thấy.

(1) Mở rộng Số lượng Tác vụ. Hiệu suất của tinh chỉnh hướng dẫn và tổng quát hóa chéo tác vụ mở rộng tốt với số lượng tác vụ (Chung et al., 2022). Do đó, nhiều tập dữ liệu bao gồm hướng dẫn cho ngày càng nhiều tác vụ được xây dựng dần dần. P3 (Sanh et al., 2021) bao gồm 177 tập dữ liệu NLP hiện có (như Commonsense QA) và PromptSource (Bach et al., 2022) bao gồm 170 tập dữ liệu, cả hai đều được chuyển đổi thành hướng dẫn thông qua các mẫu prompt được thu thập qua một giao diện (Bach et al., 2022). Natural Instructions (Mishra et al., 2021) là một tập dữ liệu của 61 tác vụ NLP riêng biệt và 193k instances được tuyển chọn từ các tập dữ liệu NLP hiện có với hướng dẫn do con người viết. Để làm phong phú các loại tác vụ, nó cũng bao gồm các bước riêng biệt cho tác vụ cuối cùng. Sau đó, Super-NaturalInstructions (Super-NatInst) (Wang et al., 2022b) xuất hiện như một tiêu chuẩn đa dạng và quy mô lớn của 1,616 tác vụ qua 76 loại tác vụ rộng và 55 loại ngôn ngữ. GLM-130B (Zeng et al., 2022) được tinh chỉnh trên 74 tập dữ liệu. Unnatural Instruction (Honovich et al., 2022) chứa 117 tác vụ được tạo hoàn toàn bởi mô hình ngôn ngữ khi được đưa ra một tập hướng dẫn gốc. Bộ sưu tập Flan 2022 (Longpre et al., 2023) tăng số lượng tác vụ lên 1.8k. Iyer et al. (2022) tạo ra OPT-IML Bench, một bộ sưu tập lớn của 1,991 tác vụ NLP và hơn 100 loại tác vụ, bằng cách hợp nhất 8 tập dữ liệu hiện có. Hơn nữa, để thúc đẩy tinh chỉnh hướng dẫn tiếng Trung và khám phá các thách thức đa ngôn ngữ, Zhang et al. (2023a) thu thập một tập dữ liệu tiếng Trung chất lượng cao với khoảng 200k mẫu. Ngoài LLMs, các nhà nghiên cứu cũng bắt đầu khám phá tinh chỉnh hướng dẫn cho các lớp mô hình lớn khác như mô hình đa phương tiện lớn (LMMs). Ví dụ, LLaV A (Liu et al., 2023a) áp dụng GPT-4 để tạo dữ liệu hướng dẫn đa phương tiện dựa trên các cặp hình ảnh-văn bản gốc; LLaV AR (Zhang et al., 2023b) thu thập các hướng dẫn đa phương tiện từ chú thích hình ảnh và xây dựng các cuộc trò chuyện chất lượng cao theo phong cách QA với GPT-4.

(2) Hướng dẫn / Prompt Đa dạng. Vì hướng dẫn cho cùng một tác vụ được đưa ra bởi các người dùng khác nhau có thể khác nhau, đa dạng hóa các hướng dẫn văn bản hoặc prompt cũng có thể góp phần vào việc đồng bộ hóa mục tiêu này. Về xP3 (Muennighoff et al., 2022), nó tổng hợp các tập dữ liệu tác vụ đa ngôn ngữ từ 46 ngôn ngữ khác nhau. Đối với tập dữ liệu Unnatural Instructions (Honovich et al., 2022), nó prompt một mô hình tạo sinh để diễn đạt lại mỗi hướng dẫn và mở rộng tập dữ liệu gốc gấp bốn lần, mà không cần bất kỳ lao động con người nào. Do những hạn chế của con người về tính đa dạng và sáng tạo, Self-Instruct (Wang et al., 2022a) điều tra việc tự động tổng hợp các hướng dẫn phạm vi rộng hơn cho các tác vụ mới, cũng như Alpaca (Taori et al., 2023). Trong các tập dữ liệu khác được xây dựng từ các tiêu chuẩn hiện có, như OPT-IML (Iyer et al., 2022), nhiều mẫu prompt cũng được áp dụng. Hơn nữa, Sharegpt (Chiang et al., 2023), một bộ sưu tập các cuộc đối thoại giữa con người và ChatGPT, được sử dụng trực tiếp cho tinh chỉnh hướng dẫn, cũng như các cuộc đối thoại được tạo bởi chính ChatGPT (Xu et al., 2023a).

(3) Few-Shot / Chain-of-Thought (CoT). Cả khả năng học trong ngữ cảnh từ một vài ví dụ mẫu và lý luận được tăng cường bởi prompt chain-of-thought đều xuất hiện trong mô hình lớn (Wei et al., 2022a,b). Hai loại kỹ thuật này chia sẻ sự tương đồng lớn với cách con người học giải quyết một tác vụ mới từ cả hướng dẫn và ví dụ trực quan. Trong Natural Instructions (Mishra et al., 2021) và Super-NaturalInstructions (Super-NatInst) (Wang et al., 2022b), định nghĩa, một ví dụ tích cực và một ví dụ tiêu cực được cung cấp cho mỗi tác vụ. Bên cạnh đó, Bộ sưu tập Flan 2022 (Longpre et al., 2023) bao gồm một số hướng dẫn với các ví dụ mẫu dưới dạng CoTs. Những điều này được chứng minh có thể cải thiện hiệu quả của tinh chỉnh hướng dẫn.

Bảng 1 liệt kê các điểm chính của những tập dữ liệu này, có thể tạo điều kiện cho nghiên cứu tiếp theo về đồng bộ hóa với hướng dẫn con người. Trong (Wang et al., 2023), có thể tìm thấy nhiều chi tiết hơn về đồng bộ hóa với mục tiêu hướng dẫn con người, trong khi bài báo của chúng tôi tập trung vào việc điều tra đồng bộ hóa mô hình lớn cho các mục tiêu khác nhau.

2.2 Sở thích Con người

Vì mục tiêu hướng dẫn con người chỉ tập trung vào khả năng cơ bản của mô hình lớn để tạo ra kết quả được định nghĩa một cách hẹp là đúng nhưng không nhất thiết phù hợp với sở thích của con người, việc đạt được đồng bộ hóa ở cấp độ này cho phép mô hình lớn giúp hoàn thành các tác vụ đa dạng trong khi còn xa mới thỏa mãn các yêu cầu tiên tiến hơn. Một số phản hồi được tạo có thể không phù hợp với sở thích của con người và thậm chí gây ra rủi ro xã hội nghiêm trọng. Ví dụ, câu trả lời cho một số câu hỏi rất ngắn gọn, ít thông tin và có độ đọc thấp; hoặc có chứa nhiều ảo giác, phân biệt đối xử văn bản và độc tính. Do đó, sở thích con người được coi là một cấp độ đồng bộ hóa mục tiêu tiếp theo, có nghĩa là mô hình lớn không chỉ có thể hoàn thành những gì con người hướng dẫn chúng làm mà còn theo cách có thể tối đa hóa sở thích và lợi ích của con người. Lưu ý rằng chúng tôi chủ yếu đề cập đến sở thích con người ngầm định hoặc chung được phản ánh trong hành vi, như định dạng phản hồi được tổ chức tốt và phong cách nói chuyện thân thiện hơn với người dùng. Điều này khác với những gì được tóm tắt thành các nguyên tắc giá trị con người ngắn gọn, sẽ được giới thiệu trong Phần 2.3. Để biểu diễn mục tiêu đồng bộ hóa sở thích con người như một mục tiêu huấn luyện, các phương pháp hiện có có thể được chia thành các loại sau.

2.2.1 Minh chứng Con người

Để làm cho việc tạo sinh của LLMs đồng bộ với sở thích con người, phương pháp đơn giản nhất là tinh chỉnh LLMs với một tập dữ liệu bao gồm các đầu vào khác nhau và các đầu ra mong muốn của con người. Đối với InstructGPT, Ouyang et al. (2022) thu thập các minh chứng chất lượng cao của người gán nhãn cho 13k hướng dẫn thường được đưa ra bởi người dùng API. Một số lượng lớn minh chứng con người cũng có sẵn cho tác vụ tóm tắt sách (Stiennon et al., 2020) và duyệt web (Nakano et al., 2021). OpenAssistant Conversation (Köpf et al., 2023) là một tập dữ liệu crowdsourcing chất lượng cao bao gồm các cuộc trò chuyện kiểu trợ lý do con người viết mở rộng. Trong (Kwon et al., 2023), mô tả về các hành vi mong muốn được đưa ra trong các prompt. Mặc dù LLMs có thể học một số mẫu về sở thích con người từ những minh chứng như vậy, lượng dữ liệu luôn bị hạn chế do chi phí lao động cao, và có những tác vụ mà con người gặp khó khăn trong việc cung cấp minh chứng chuyên nghiệp (Wu et al., 2021).

2.2.2 Phản hồi Con người

Thay vì minh chứng trực tiếp, con người dễ dàng hơn trong việc cung cấp phản hồi về đầu ra của mô hình hoặc so sánh chất lượng của một số hành vi, điều này ngầm định thể hiện sở thích con người. Ví dụ, Stiennon et al. (2020) và Wu et al. (2021) yêu cầu người gán nhãn con người so sánh hai bản tóm tắt của một cuốn sách được tạo bởi mô hình và chọn bản tốt hơn. WebGPT (Nakano et al., 2021) tập trung vào tác vụ trả lời câu hỏi với kiến thức từ các trang web liên quan và yêu cầu con người gán nhãn cái họ thích hơn trong một cặp câu trả lời do mô hình tạo ra. Đối với cả hai tác vụ, việc tạo ra một sự thật cơ bản tốn nhiều nhãn, nơi việc cung cấp phản hồi ngầm định cải thiện hiệu quả và độ chính xác. Trong InstructGPT (Ouyang et al., 2022), người gán nhãn xếp hạng một số đầu ra được tạo cho cùng một đầu vào từ tốt nhất đến tệ nhất. Đối với tập dữ liệu OpenAssistant Conversation (Köpf et al., 2023), đánh giá chất lượng về mỗi phản hồi được cung cấp bởi những người làm crowdwork. Tuy nhiên, dữ liệu so sánh được thu thập chỉ chứa sở thích con người về các hành vi mô hình hạn chế. Để biểu diễn sở thích con người theo cách có thể tổng quát hóa hơn, huấn luyện một mô hình phần thưởng trên dữ liệu so sánh hạn chế là một chiến lược phổ biến, mà điểm số của nó có thể chỉ ra mục tiêu đồng bộ hóa sở thích con người qua các tình huống (Ouyang et al., 2022; Nakano et al., 2021; Ziegler et al., 2019).

2.2.3 Phản hồi Tổng hợp Mô hình

Với dữ liệu khổng lồ cho tiền huấn luyện và tinh chỉnh LLMs, một số mô hình đã thể hiện khả năng phân biệt chất lượng của các câu trả lời khác nhau và sự phù hợp với sở thích con người. Do đó, một số công trình sử dụng LLMs để tổng hợp phản hồi về sở thích con người. Kwon et al. (2023) thiết kế một hàm phần thưởng proxy với một LLM như GPT-3 bằng cách prompt nó với mô tả về các hành vi mong muốn của người dùng và một vài minh chứng. Sau đó, LLM tạo ra phần thưởng bằng cách đo lường mức độ liên quan giữa đầu ra của mô hình và sự thật cơ bản được mô tả. Đối với ILF (Imitation Learning from Language Feedback) (Scheurer et al., 2023), nó tận dụng một mô hình ngôn ngữ để tinh chỉnh nhiều đầu ra do mô hình tạo ra theo một tham chiếu do con người cung cấp, và sau đó chọn cái được tinh chỉnh tốt nhất cho tinh chỉnh có giám sát tiếp theo. Ngoài ra, phương pháp ALMoST (Kim et al., 2023) tóm tắt sở thích con người thành một số quy tắc heuristic, như 'LLMs lớn với nhiều và tốt hơn shots có thể đưa ra phản hồi tốt hơn tổng thể'. Sau đó, dữ liệu so sánh được tạo từ các phản hồi do LLMs tạo ra với các kích thước và prompt khác nhau dựa trên những quy tắc này để huấn luyện một mô hình phần thưởng. Stable Alignment (Liu et al., 2023b) xây dựng một cộng đồng của nhiều mô hình ngôn ngữ lớn, nơi mỗi mô hình được học từ phản hồi cho các hành động của nó được cung cấp bởi các mô hình khác. Trong lĩnh vực LMMs, một mô hình đa phương tiện, được gọi là Polite Flamingo (Chen et al., 2023a), được huấn luyện trên các cặp hướng dẫn và phản hồi để sửa đổi nội dung không phù hợp. Sử dụng các mô hình neural để tổng hợp tín hiệu sở thích con người không chỉ có thể giảm chi phí lao động mà còn tránh các vấn đề như thiên vị được con người đưa vào.

2.3 Giá trị Con người

Đạt được mục tiêu đồng bộ hóa sở thích con người cho phép mô hình lớn tối đa hóa sự hài lòng của con người bằng cách thực hiện theo cách con người thích. Tuy nhiên, quá trình đồng bộ hóa hoàn toàn được điều hướng bởi phản hồi con người ngầm định về các hành vi mô hình chung, mà không có tiêu chí nội tại để chỉ định sở thích con người. Điều này có thể gặp phải những thách thức riêng của nó. Thứ nhất, khó học được các mẫu có thể tổng quát hóa về sở thích con người từ một số lượng hạn chế các hành vi mô hình chung, điều này làm cho quá trình huấn luyện kém hiệu quả hơn. Thứ hai, mô hình được đồng bộ hóa có thể gây ra hiệu suất không ổn định trên các câu hỏi tương tự, vì thường có thiên vị con người, sự không nhất quán và thậm chí mâu thuẫn trong dữ liệu huấn luyện. Để đạt được một sự đồng bộ hóa quan trọng, hiệu quả và ổn định hơn giữa mô hình lớn và con người, khái niệm 'đồng bộ hóa mô hình lớn với giá trị con người' đã được giới thiệu. Trong bài báo này, chúng tôi coi giá trị con người là cấp độ tiên tiến nhất của mục tiêu đồng bộ hóa, đề cập đến một phép đo toàn diện về điều gì là tốt và điều gì là xấu, cũng như điều gì nên được thực hiện về mặt toàn bộ tập thể con người. Giá trị con người rất trừu tượng và thường được chỉ định như một tập hợp các nguyên tắc giá trị. Do đó, mục tiêu đồng bộ hóa này có nghĩa là mô hình lớn nên áp dụng những nguyên tắc giá trị này để hướng dẫn hành vi riêng của chúng nhằm tối đa hóa phúc lợi của tất cả con người.

Chúng tôi xem xét công trình hiện có điều tra mục tiêu đồng bộ hóa này từ hai góc độ chính. Một là cách họ chỉ định khái niệm trừu tượng về giá trị con người thành các nguyên tắc cụ thể. Cái khác là cách họ chuyển những nguyên tắc này thành một mục tiêu huấn luyện. Chi tiết được giới thiệu như sau.

2.3.1 Nguyên tắc Giá trị Con người

Như được thể hiện trong Hình 3, ba loại chính của nguyên tắc giá trị con người được xem xét.

(1) HHH (Hữu ích, Trung thực và Vô hại). Đây là một trong những tiêu chí phổ biến nhất, đơn giản, dễ nhớ và phù hợp với giá trị con người qua đa số các tác vụ. Askell et al. (2021) trình bày một số bình luận để làm rõ ba thuật ngữ này.
• Hữu ích: Mô hình lớn có thể thực hiện các tác vụ hợp lý hoặc trả lời ngắn gọn các câu hỏi đầu vào, có thể chủ động yêu cầu thêm thông tin cần thiết và sửa đổi các yêu cầu được thông báo sai.
• Trung thực: Về cơ bản, mô hình lớn phải cung cấp thông tin thực, và chúng được mong đợi tiếp tục trung thực về kiến thức và khả năng nội tại của chúng.
• Vô hại: Mô hình lớn không nên phân biệt đối xử hoặc xúc phạm, từ chối các yêu cầu rõ ràng hoặc có khả năng nguy hiểm và lời khuyên nhạy cảm, thậm chí không thỏa mãn người dùng.

Dựa trên ba tiêu chí cơ bản, nhiều nỗ lực đã được thực hiện với các cách diễn giải cụ thể hơn. Đơn giản nhất, Bai et al. (2022a) cho phép người chú thích chọn các mẫu hữu ích hơn và ít có hại hơn theo sự hiểu biết riêng của họ về ba thuật ngữ này. Để cụ thể hơn, những tiêu chí này được giải mã thành một số nguyên tắc hoặc quy tắc về đa số các vấn đề an toàn và rủi ro xã hội. Ví dụ, Sparrow (Glaese et al., 2022) áp dụng nhiều quy tắc ngôn ngữ tự nhiên, từ các khía cạnh của định kiến, căm thù, tự nhân cách hóa, thông tin sai lệch và những khía cạnh khác. Trong Constitutional AI (Bai et al., 2022b), những giá trị này được biểu diễn như một số lượng nhỏ các nguyên tắc để phê bình và sửa đổi những phản hồi không đồng bộ, như "Vui lòng viết lại phản hồi của trợ lý để loại bỏ bất kỳ và tất cả nội dung có hại, không đạo đức, phân biệt chủng tộc, phân biệt giới tính, độc hại, nguy hiểm hoặc bất hợp pháp." Tương tự, 16 quy tắc chung qua các lĩnh vực khác nhau được thiết kế trong SELF-ALIGN (Sun et al., 2023c), bao gồm các yêu cầu về việc đạo đức, thông tin, hữu ích và vân vân. Thay vì các quy tắc chung và toàn diện có thể thích ứng cho tất cả các tình huống, PALMS (Solaiman and Dennison, 2021) cung cấp mô tả về các hành vi mong muốn cho mỗi chủ đề nhạy cảm.

(2) Chuẩn mực Xã hội & Đạo đức. Những điều này thường có thể được coi như các quy tắc thông thường về hành vi được công chúng chấp nhận, được thiết lập và phát triển dần dần trong quá trình phát triển xã hội (Forbes et al., 2020). Khi tất cả mọi người hành xử theo cách bị hạn chế hoặc được thúc đẩy bởi những quy tắc này, nó có thể giảm xung đột và duy trì sự ổn định xã hội, do đó tăng cường niềm tin và hợp tác giữa mọi người. Do đó, các nhà nghiên cứu khám phá việc đạt được mục tiêu đồng bộ hóa giá trị con người được hình thức hóa như chuẩn mực xã hội. Rule-of-Thumb (RoTs) là một proxy phổ biến để chỉ định chuẩn mực xã hội (Forbes et al., 2020). Mỗi RoT là một chuẩn mực văn hóa mô tả để đánh giá xem một hành động có thể chấp nhận được không, được định nghĩa như đơn vị khái niệm cơ bản của chuẩn mực. Để tạo điều kiện cho nghiên cứu về đạo đức tính toán, có nhiều kho dữ liệu nơi một tập hợp lớn các tình huống hàng ngày được thu thập và các RoTs chính xác cho phán đoán được đính kèm, bao gồm Moral Integrity Corpus (MIC) (Ziems et al., 2022), Social Chemistry 101 (Forbes et al., 2020) và Moral Stories (Emelin et al., 2020). Vì có vô số tình huống đạo đức, một số nghiên cứu thảo luận về việc tạo ra các RoTs phù hợp hơn khi được đưa ra một tình huống và thái độ mục tiêu (Ziems et al., 2022; Sun et al., 2023b). Mà không có RoTs cho mỗi tình huống, ETHICS (Hendrycks et al., 2020) tập trung vào các khái niệm được đề xuất trong đạo đức chuẩn mực, tức là Công lý, Đức hạnh, Deontology, Utilitarianism và Commonsense. Có một nghiên cứu thú vị khác chọn các chuẩn mực xã hội chứa trong các câu chuyện tự nhiên từ một truyện tranh giáo dục trẻ em, Goofus & Gallant (Nahian et al., 2020).

(3) Lý thuyết Giá trị Cơ bản. Nghiên cứu về giá trị con người bắt nguồn từ khoa học xã hội và đạo đức, nơi một số lý thuyết giá trị cơ bản hơn đã được thiết lập và kiểm nghiệm theo thời gian. Một trong những lý thuyết phổ biến nhất là Lý thuyết Giá trị Cơ bản Con người của Schwartz (Schwartz, 2012), coi giá trị con người như một động lực cho hành động và tiêu chuẩn để quyết định điều gì là tốt hoặc xấu. Nó xác định bốn giá trị bậc cao (cởi mở với thay đổi, bảo thủ, tự nâng cao và tự siêu việt) và 10 loại động lực của giá trị. Mặc dù mối quan hệ nhân quả hoặc mối quan hệ giữa những giá trị này và rủi ro mô hình lớn chưa được điều tra, một số nghiên cứu giới thiệu giá trị con người vào đối thoại (Qiu et al., 2022) và lập luận (Kiesel et al., 2022). Các lý thuyết giá trị tương tự khác bao gồm Rokeach Values (Rokeach, 1967), Life Values (Brown and Crace, 2002), v.v. Ngoài ra, lý thuyết nền tảng đạo đức (Graham et al., 2013) là một khung phổ quát để nghiên cứu các vấn đề đạo đức, tuyên bố rằng đạo đức con người có thể được tóm tắt bởi năm nhóm nền tảng: Care/Harm, Fairness/Cheating, Loyalty/Betray, Authority/Subversion và Sanctity/Degradation. Một số kho dữ liệu đã được thu thập với những chú thích đạo đức này (Hoover et al., 2020; Trager et al., 2022).

2.3.2 Biểu diễn Giá trị Con người

Khi huấn luyện mô hình lớn để đồng bộ với mục tiêu giá trị con người, có hai loại phương pháp cho biểu diễn mục tiêu huấn luyện.

(1) Hành vi Mong muốn. Để đồng bộ LLMs với các nguyên tắc giá trị con người được định nghĩa rõ một cách hiệu quả, loại phương pháp này thu thập dữ liệu hành vi huấn luyện chống lại các nguyên tắc mục tiêu, thay vì trực tiếp nhận ra các nguyên tắc giá trị. Askell et al. (2021), Bai et al. (2022a), Ganguli et al. (2022) thuê người gán nhãn con người để đưa ra câu hỏi từ góc độ hữu ích hoặc có hại và làm nổi bật các câu trả lời tốt hơn được LLM tạo ra, được biết đến như red-teaming (Ganguli et al., 2022). Sau đó, các phản hồi mong muốn phù hợp với các nguyên tắc giá trị mục tiêu được sử dụng trực tiếp cho tinh chỉnh có giám sát. Hơn nữa, một mô hình phần thưởng có thể được huấn luyện trên dữ liệu so sánh để cung cấp phản hồi có thể tổng quát hóa hơn. ETHICS (Hendrycks et al., 2020) là một tập dữ liệu bao gồm các tuyên bố tích cực và tiêu cực xung quanh các khái niệm giá trị về công lý, đức hạnh, deontology, utilitarianism và commonsense. SBIC (Social Bias Inference Corpus) (Sap et al., 2019) bao gồm một số lượng lớn các bài đăng trên mạng xã hội với nhãn thiên vị hoặc định kiến.

(2) Giá trị Nội tại. Ngoài minh chứng hoặc phản hồi của các hành vi bề mặt, một số nghiên cứu dành cho việc làm cho mô hình lớn nhận ra các nguyên tắc giá trị mục tiêu và đạt được một sự đồng bộ hóa nội tại hơn. Lấy Constitutional AI (Bai et al., 2022b) làm ví dụ đại diện, nó prompt LLM với một hiến pháp bao gồm nhiều nguyên tắc giá trị, và sau đó yêu cầu LLM phê bình và sửa đổi các phản hồi có hại được tạo bởi một trợ lý AI chỉ hữu ích cho huấn luyện mô hình tiếp theo. Do đó, LLM có thể nhận thức về các nguyên tắc nội tại cần được đồng bộ. Tương tự, SELF-ALIGN (Sun et al., 2023c) cũng prompt một trợ lý AI với 16 nguyên tắc và 5 ví dụ học trong ngữ cảnh để lọc các mẫu có đủ điều kiện cho huấn luyện mô hình. Trong PALMS (Solaiman and Dennison, 2021), mô tả rõ ràng về các hành vi mong muốn được prompt cho LLMs. Sparrow (Glaese et al., 2022) chỉ định các yêu cầu cho hành vi tốt với một danh sách các quy tắc và thiết kế một mô hình phần thưởng quy tắc cung cấp điểm phần thưởng có điều kiện trên các quy tắc cho trước. Trong tài liệu về chuẩn mực xã hội & đạo đức, các Rule-of-Thumbs (RoTs) tương ứng có sẵn để hỗ trợ các phán đoán đạo đức về hành động hoặc tình huống cuộc sống, bao gồm SOCIAL CHEMISTRY (Forbes et al., 2020), MORAL STORIES (Emelin et al., 2020), và MIC (Ziems et al., 2022). Do đó, mô hình có thể học để đưa ra quyết định đạo đức dựa trên chuẩn mực xã hội nội tại, và thậm chí tự động truy xuất các quy tắc hiện có hoặc tạo ra các quy tắc phù hợp cho phán đoán, ví dụ MoralDial (Sun et al., 2023b) và MIC (Ziems et al., 2022). Delphi (Jiang et al., 2021) là một khung phổ quát cho lý luận đạo đức về bất kỳ tình huống nào được thể hiện trong văn bản. Nó được phát triển từ một bộ sưu tập các tập dữ liệu được đề cập ở trên với nhận thức về RoTs, tức là COMMONSENSE NORM BANK.

3 Đánh giá Đồng bộ hóa

Để đảm bảo rằng mô hình lớn đồng bộ với mục tiêu theo đúng hướng, việc đánh giá chính xác hiệu suất đồng bộ hóa là rất quan trọng. Phần này xem xét các phương pháp đánh giá hiện có cho đồng bộ hóa mô hình lớn, đặc biệt là LLMs, được tổ chức từ các khía cạnh của hướng dẫn con người, sở thích con người và giá trị con người.

3.1 Hướng dẫn Con người

Để xác minh mức độ tốt LLMs đạt được mục tiêu đồng bộ hóa hướng dẫn con người, chúng tôi đánh giá hiệu suất của chúng qua các tác vụ khác nhau, đặc biệt là khả năng tổng quát hóa cho các tác vụ chưa thấy. Nhiều tiêu chuẩn với câu trả lời được gán nhãn đã được triển khai, cũng như một số sân đấu cho đánh giá tự động.

3.1.1 Tiêu chuẩn

Có các tiêu chuẩn bao gồm các tác vụ NLP thông thường để đánh giá khả năng cơ bản và trí thông minh tiên tiến, sử dụng các số liệu định lượng như độ chính xác, ROUGE (Lin, 2004) và BLEU (Papineni et al., 2002). Trong các tập dữ liệu được thu thập cho tinh chỉnh hướng dẫn được đề cập trong Phần 2.1, bao gồm PromptSource (Bach et al., 2022), Bộ sưu tập Flan 2022 (Chung et al., 2022; Longpre et al., 2023), OPT-IML (Iyer et al., 2022) và SUPER-NATURALINSTRUCTIONS (Mishra et al., 2021; Wang et al., 2022b), một tập kiểm tra held-out cũng được duy trì để đánh giá LLMs đã huấn luyện qua ba cấp độ tổng quát hóa: 1) hiệu suất trên các tác vụ trong các loại held-out; 2) hiệu suất trên các phân phối dữ liệu mới từ các loại tác vụ đã biết; và 3) hiệu suất trên các mẫu held-out từ một tập dữ liệu được áp dụng. Ngoài khả năng tuân theo hướng dẫn và hoàn thành các tác vụ NLP, đánh giá về khả năng toàn diện của mô hình nền tảng cũng đáng chú ý. BIG-bench (Srivastava et al., 2022) được định vị cho các tác vụ vượt ra ngoài khả năng của GPT-3, bao gồm 204 tác vụ qua các chủ đề tác vụ đa dạng. Được truyền cảm hứng bởi tia sáng của AGI (Bubeck et al., 2023), AGIEval (Zhong et al., 2023) và C-EVAL (Huang et al., 2023) cố gắng đánh giá khả năng của mô hình nền tảng để đối phó với các tác vụ cấp độ con người, cả hai đều bao gồm các kỳ thi qua nhiều cấp độ khó khăn và môn học. Hơn nữa, cũng có các đánh giá được tạo tự động bởi LMs (154 tập dữ liệu), giảm lượng nỗ lực con người (Perez et al., 2022).

3.1.2 Đánh giá LLMs Tiên tiến

Các tiêu chuẩn đánh giá được tạo thủ công ở trên có chất lượng cao, nhưng thu thập phản hồi con người có thể tốn kém trong nhiều tình huống. Với một mô hình ngôn ngữ lớn có khả năng cao (ví dụ GPT-4 hoặc Claude) như thẩm phán, các sân đấu chatbot tự động có thể được thiết lập để đánh giá LLMs bằng cách so sánh các phản hồi của hai LLMs từ nhiều khía cạnh. Phương pháp này được sử dụng trong đánh giá Alpaca (Taori et al., 2023; Li et al., 2023) và Vicuna (Chiang et al., 2023), nơi GPT-4 được prompt để so sánh hai câu trả lời cho trước từ tính hữu ích, liên quan, sáng tạo và vân vân. AlpacaFarm (Dubois et al., 2023), một khung mô phỏng cho đồng bộ hóa, cũng áp dụng đánh giá tự động này. Đáng chú ý rằng khả năng của LLM-as-a-judge được khám phá trong (Zheng et al., 2023), tiết lộ rằng các thẩm phán LLM mạnh có thể đạt được sự đồng ý với người gán nhãn con người cao như giữa chính con người với nhau. Phát hiện này chỉ ra rằng đánh giá tự động là một cách khả thi và có thể mở rộng. Hơn nữa, bằng cách prompt người đánh giá LLM với các tiêu chí khác nhau, phương pháp này có thể được áp dụng trong đánh giá qua tất cả ba mục tiêu đồng bộ hóa.

3.2 Sở thích Con người

Khi đánh giá mục tiêu đồng bộ hóa sở thích con người, việc đo lường các thuộc tính mong muốn của con người ngoài khả năng cơ bản để hoàn thành nhiều tác vụ là quan trọng, như tạo ra câu trả lời hữu ích hơn, loại bỏ thiên vị và độc tính (Zhuo et al., 2023). Nhưng đánh giá chống lại các nguyên tắc giá trị con người nội tại không được xem xét ở đây. Các nghiên cứu hiện có có thể được chia thành ba loại.

3.2.1 Tiêu chuẩn

TruthfulQA (Lin et al., 2022) là một tiêu chuẩn phổ biến để đo lường tính trung thực của một mô hình bằng cách đặt các câu hỏi đòi hỏi việc xác định cẩn thận tính trung thực, thay vì chỉ tạo ra câu trả lời bằng cách bắt chước văn bản con người. Tập dữ liệu OpenBookQA (Mihaylov et al., 2018) bao gồm các sự kiện khoa học được thu thập từ các kỳ thi sách mở và được sử dụng để đánh giá độ tin cậy của mô hình. Về thiên vị được gây ra bởi LLMs, các tiêu chuẩn như CrowS-Pairs với 9 loại thiên vị (Nangia et al., 2020), WinoGender tập trung vào loại giới tính (Rudinger et al., 2018), BBQ (Parrish et al., 2021) và BOLD (Dhamala et al., 2021) đều có sẵn. RealToxicityPrompts (Gehman et al., 2020) là một tiêu chuẩn phổ biến để chỉ ra mức độ độc hại của một mô hình cho trước. Nó tạo ra khoảng 100k prompt cho mô hình hoàn thành, và sau đó điểm độc tính được tính bằng cách gửi những hoàn thành này đến PerspectiveAPI. ToxiGen (Hartvigsen et al., 2022) cũng phục vụ mục đích tương tự. BIG-Bench quy mô lớn, đầy thách thức và đa dạng (Srivastava et al., 2022) có thể soi sáng đánh giá khả năng sâu hơn của LLMs vượt ra ngoài bắt chước. Ngoài ra, HELM (Liang et al., 2022) cung cấp một đánh giá toàn diện về mô hình ngôn ngữ thông qua nhiều tình huống và số liệu (độ chính xác, hiệu chuẩn, độ bền, công bằng, thiên vị, độc tính và hiệu quả). Mà không có chi phí lao động con người đắt đỏ, Perez et al. (2022) tạo ra một bộ sưu tập đánh giá 154 tập dữ liệu với LLMs, có thể đánh giá hành vi của một mô hình liên quan đến nhân cách, sự xu nịnh, rủi ro AI tiên tiến và thiên vị giới tính của chúng.

3.2.2 Đánh giá Con người

Vì khó khăn trong việc khám phá các yếu tố khác nhau ảnh hưởng đến sở thích con người bằng các số liệu định lượng như độ chính xác, đánh giá có liên quan đến người đánh giá con người cũng nên được kết hợp. Cho một tập kiểm tra held-out của các prompt thử nghiệm, người đánh giá con người được yêu cầu so sánh một số phản hồi khác nhau. Hai cài đặt chính được xem xét: 1) so sánh giữa mô hình mục tiêu và một baseline mạnh (Ouyang et al., 2022; Touvron et al., 2023b; Yuan et al., 2023; Stiennon et al., 2020); và 2) so sánh với các tham chiếu do con người viết (Rafailov et al., 2023). Sau đó, một số liệu về tỷ lệ thắng hoặc điểm Elo (Askell et al., 2021) được tính toán. Sử dụng một tập dữ liệu được gán nhãn với các câu trả lời được ưa thích và ít được ưa thích, chúng ta cũng có thể đánh giá LLMs theo cách multiple-choice thay vì tạo sinh (Kim et al., 2023). Do một mức độ đồng ý cao có thể đạt được giữa LLMs mạnh như GPT-4 và con người (Zheng et al., 2023), đánh giá tự động bởi GPT-4 được prompt với hướng dẫn về sở thích con người được sử dụng rộng rãi, cũng có thể cung cấp giải thích chi tiết cho phán đoán.

3.2.3 Chấm điểm Mô hình Phần thưởng

Khi đồng bộ với sở thích con người, một phương pháp phổ biến là đầu tiên huấn luyện một mô hình phần thưởng có thể tổng quát hóa dựa trên phản hồi con người và sau đó tối đa hóa điểm phần thưởng. Do đó, điểm được trả về bởi mô hình phần thưởng phục vụ như một số liệu đánh giá tốt. Nhiều nghiên cứu tính điểm phần thưởng trung bình trên tất cả các mẫu thử nghiệm với một mô hình phần thưởng được huấn luyện trên cùng tập dữ liệu hoặc một tập held-out (Touvron et al., 2023b; Bai et al., 2022a; Rafailov et al., 2023; Dong et al., 2023), và quan sát rằng điểm phần thưởng tăng trong suốt quá trình đồng bộ hóa. Tiêu chuẩn GRUE (Ramamurthy et al., 2022) chứa 6 tác vụ tạo sinh ngôn ngữ với các hàm phần thưởng riêng biệt cho mỗi tác vụ.

3.3 Giá trị Con người

Khi đánh giá mục tiêu đồng bộ hóa giá trị con người, chúng tôi chủ yếu tập trung vào việc đo lường các nguyên tắc hoặc hệ thống giá trị được định nghĩa trước, như được giới thiệu trong Phần 2.3.1. Ngoài đánh giá thủ công hoặc chuyên gia (Bai et al., 2022a), các đánh giá khác có thể được tổ chức như ba lớp sau.

3.3.1 Tiêu chuẩn

Chúng tôi chủ yếu xem xét ba loại tiêu chuẩn, được phân tách bởi định nghĩa riêng về giá trị con người và loại đánh giá của chúng. Đầu tiên là các tiêu chuẩn an toàn và rủi ro, bao gồm các vấn đề toàn diện chống lại nguyên tắc 'HHH' được quan sát trong LLMs được phát hành gần đây, như thông tin độc hại, lời khuyên bất hợp pháp và jailbreaking. Thông thường, những tiêu chuẩn này đánh giá mô hình lớn thông qua một tác vụ tạo sinh. Lớp thứ hai là các tiêu chuẩn chuẩn mực xã hội nơi các tình huống cuộc sống khác nhau cùng với phán đoán và chuẩn mực xã hội được tham chiếu được cung cấp. Những câu hỏi này thường được đặt cho LLMs như một tác vụ phân biệt. Loại thứ ba là các khảo sát giá trị hoặc bảng câu hỏi được thiết kế đặc biệt cho con người dưới dạng câu hỏi tự báo cáo hoặc multiple-choice.

(1) Tiêu chuẩn An toàn và Rủi ro. Về nguyên tắc giá trị 'HHH' được áp dụng rộng rãi (hữu ích, trung thực và vô hại), Bai et al. (2022a); Askell et al. (2021); Ganguli et al. (2022) phát hành một tiêu chuẩn chứa cả instances hữu ích và vô hại, với các phản hồi được chọn và từ chối được chú thích thủ công. Những trường hợp có hại này được phát hiện bởi các cuộc tấn công red teaming, từ ngôn ngữ xúc phạm đến các yêu cầu không đạo đức có hại hơn. Được thúc đẩy bởi mối quan tâm an toàn ngày càng tăng của LLMs, Sun et. al (Sun et al., 2023a) phát triển một tiêu chuẩn đánh giá an toàn LLM tiếng Trung có tên SafetyPrompts. Tiêu chuẩn này đánh giá hiệu suất an toàn chính từ hai góc độ: 8 tình huống an toàn điển hình (ví dụ như xúc phạm, sức khỏe tâm thần) và 6 loại tấn công hướng dẫn (ví dụ như rò rỉ prompt). Tiêu chuẩn này chỉ bao gồm các prompt thử nghiệm phức tạp và yêu cầu phán đoán an toàn từ một người đánh giá LLM. Trong khi đó, SAFETEXT (Levy et al., 2022) là một tiêu chuẩn được đề xuất cụ thể để khám phá an toàn vật lý thông thường được mã hóa trong LLMs. Để có được cái nhìn rộng hơn về giá trị con người được đồng bộ bởi LLMs, tiêu chuẩn CVALUES (Xu et al., 2023b) đề xuất hai tiêu chí: cấp độ cơ bản là an toàn chứa 10 tình huống tương tự như các vấn đề được thảo luận trong (Sun et al., 2023a); và cấp độ trên là trách nhiệm chứa 8 lĩnh vực với tác động xã hội lớn hơn và tương lai. Sau đó, họ yêu cầu những kẻ tấn công crowdsourcing kích hoạt các câu hỏi an toàn và mời các chuyên gia lĩnh vực thiết kế các câu hỏi trách nhiệm. Cả đánh giá con người và đánh giá tự động theo cách multiple-choice đều được sử dụng để xác minh hiệu suất cuối cùng. Tất cả các hệ thống giá trị có sẵn cho đánh giá đồng bộ hóa được minh họa trong Hình 4.

(2) Tiêu chuẩn Chuẩn mực Xã hội. Để xác định xem một hệ thống nhân tạo có nhận thức và tiếp tục tuân thủ chuẩn mực xã hội hay không, một số tiêu chuẩn đạo đức có sẵn công khai có thể được sử dụng cho đánh giá, bao gồm moral stories (Emelin et al., 2020), MIC (Ziems et al., 2022), social chemistry (Forbes et al., 2020) và ETHICS (Hendrycks et al., 2020). Những tiêu chuẩn này trình bày các tình huống cuộc sống khác nhau, các cặp hành động đạo đức và không đạo đức, và các chuẩn mực hoặc RoTs tương ứng cho phán đoán. Ba tác vụ qua các cấp độ khó khăn khác nhau có thể truy cập cho một đánh giá toàn diện: 1) cho một tình huống, một hành động và một chuẩn mực xã hội, chúng ta có thể kiểm tra khả năng của LLMs cho phán đoán đạo đức; 2) cho một tình huống và một hành động, chúng ta yêu cầu LLMs dự đoán đạo đức và thăm dò các chuẩn mực đạo đức được mã hóa; 3) cho một tình huống và một hành động, chúng ta yêu cầu LLMs tạo ra một cách rõ ràng các RoTs có thể được áp dụng để giải quyết trường hợp này, và sau đó so sánh những cái được tạo ra với các chú thích thô. Trong Moral Mimicry (Simmons, 2022), các thuộc tính đạo đức nội tại của LLMs và mối tương quan của chúng với các danh tính chính trị tự do hoặc bảo thủ Hoa Kỳ được prompt được khám phá, nơi lý thuyết nền tảng đạo đức (Graham et al., 2013) được sử dụng. Hơn nữa, một đặc điểm chính của chuẩn mực đạo đức là chúng có tính linh hoạt và ưu tiên khác nhau trong các tình huống khác nhau. Điều này rõ ràng trong các trường hợp dylemma mà tất cả chúng ta đã gặp phải trong cuộc sống hàng ngày, nơi mọi người có thể vi phạm một số quy tắc xung đột để tuân theo những quy tắc khác. Việc ưu tiên tinh tế các chuẩn mực đạo đức trong LLMs được quan tâm cao, bởi vì điều này sẽ quyết định cách LLMs đưa ra quyết định khi đối mặt với các vấn đề quan trọng. SCRUPLES (Lourie et al., 2021) là một kho dữ liệu về các tình huống thực tế phức tạp, kết hợp với một tác vụ mới 'Ai sai?'. MoralExceptQA (Jin et al., 2022) là một tập dữ liệu về trả lời câu hỏi ngoại lệ đạo đức liên quan đến tính linh hoạt tiềm năng của đạo đức. Nó đã được prompt cho các LLMs tiên tiến nhất để đánh giá với tăng cường chain-of-thought (Jin et al., 2022). ETHICAL QUANDARY GQA (Bang et al., 2022) là một tập hợp khác của các tình huống đạo đức đầy thách thức.

(3) Khảo sát Giá trị Con người. Một số khảo sát giá trị được thiết kế đặc biệt cho con người để thăm dò niềm tin, sở thích và thái độ của họ được khai thác để thăm dó các giá trị được nhúng trong LLMs. Những khảo sát này thường bao gồm câu hỏi tự báo cáo và trừu tượng, được chuyển đổi thành một tác vụ chấm điểm (ví dụ, 1-10 có nghĩa từ hiệu quả đến dân chủ) hoặc một tác vụ multiple-choice (ví dụ, (A) Đồng ý mạnh mẽ, (B) Đồng ý, v.v.) thông qua thiết kế prompt. Khảo sát Văn hóa của Hofstede (Hofstede, 1984) bao gồm 24 câu hỏi qua 6 chiều của Khoảng cách Quyền lực (pdi), Chủ nghĩa Cá nhân (idv), Tránh Bất định (uai), Tính Masculine (mas), Định hướng Dài hạn (lto), Sự Nuông chiều (ivr). Khảo sát này có một số lượng lớn người tham gia từ hơn 100 quốc gia. Khảo sát Giá trị Thế giới (WVS) cũng là một dự án tương tác được tiến hành ở nhiều quốc gia và kéo dài nhiều 7 đợt (đợt cuối từ 2017 đến 2020). Nó bao gồm các câu hỏi từ 13 loại giá trị như 'Giá trị Xã hội, Thái độ và Định kiến' và 'Hạnh phúc và Phúc lợi'. Một khảo sát tương tự khác là Nghiên cứu Giá trị Châu Âu liên quan đến các chủ đề về gia đình, công việc, môi trường và vân vân, chỉ có sẵn cho công dân trên khắp Châu Âu. Khảo sát Thái độ Toàn cầu của Trung tâm Nghiên cứu Pew (GAS) chứa 2,203 câu hỏi về các chủ đề như tôn giáo, chính trị và công nghệ. Hơn nữa, bảng câu hỏi về giá trị con người cũng bao gồm Khảo sát Giá trị Rokeach (Rokeach, 1967) yêu cầu người tham gia xếp hạng ưu tiên của 36 chiều giá trị, Khảo sát Giá trị Schwartz (SVS) (Schwartz, 2012) trình bày 57 mục giá trị và yêu cầu mọi người đưa ra điểm quan trọng của họ, và một lựa chọn thay thế của SVS, tức là Bảng câu hỏi Giá trị Chân dung (PVQ). Gần đây, Arora et al. (2022) kết hợp Khảo sát Văn hóa của Hofstede và WVS để khám phá các nền văn hóa nào được LLMs học và cách chúng có ảnh hưởng đến giá trị. Ngoài ra, một tập dữ liệu GlobalOpinionQA được xây dựng như một tổng hợp của GAS và WVS để nắm bắt ý kiến của LLMs về các vấn đề toàn cầu (Durmus et al., 2023), và quan sát rằng LLMs hiện tại bị thiên vị với những người từ Hoa Kỳ, Châu Âu và Nam Mỹ. Những khảo sát này được thiết kế cố ý bởi các chuyên gia từ các lĩnh vực liên quan và đã được sử dụng trong nhiều năm. Chúng ta có thể chủ yếu sử dụng những khảo sát này để đánh giá LLMs, nhưng khả năng sử dụng cơ bản của chúng vẫn chưa được điều tra.

3.3.2 Chấm điểm Mô hình Phần thưởng

Với nhiều tiêu chuẩn được thu thập thủ công có nhãn rõ ràng chống lại hành vi tích cực và tiêu cực, mô hình phần thưởng và bộ phân loại giá trị có thể được huấn luyện. Những mô hình này có thể được tổng quát hóa để phê bình nhiều mẫu hơn, mà không cần chú thích thủ công từng trường hợp. Dựa trên tập dữ liệu đồng bộ hóa 'HHH' (Bai et al., 2022a), mô hình phần thưởng được huấn luyện có thể phục vụ như một chỉ báo của mức độ đồng bộ hóa, và điểm phần thưởng càng cao càng tốt (Bai et al., 2022b,a). Các bộ phân loại để xác định xem một hành động có tuân thủ chuẩn mực xã hội hay không cũng đã được triển khai trong các tiêu chuẩn riêng biệt, như moral stories (Emelin et al., 2020), social chemistry (Forbes et al., 2020), ethics (Hendrycks et al., 2020), stories từ Goofus & Gallant (Nahian et al., 2020), và vân vân. Tổng hợp tất cả những tập dữ liệu đạo đức có sẵn này thành một kho kiến thức có tên 'COMMONSENSE NORM BANK', khung được huấn luyện Delphi (Jiang et al., 2021) thể hiện tổng quát hóa mạnh về phán đoán đạo đức qua một loạt các tình huống hàng ngày. Ngoài việc phân biệt xem một hành vi có đồng bộ với giá trị con người hay không, việc xác định các giá trị đằng sau hành vi của LLMs là mong muốn hơn nhưng đầy thách thức. Điều này có thể bước tới việc nắm bắt các giá trị nội tại của LLMs. Moral Foundation Twitter Copus (Hoover et al., 2020) cung cấp một bộ sưu tập tweet kèm theo 10 loại tình cảm đạo đức, cũng như một bộ phân loại tình cảm đạo đức được huấn luyện trên những dữ liệu này. VALUENET (Qiu et al., 2022) cũng là một cơ sở kiến thức giá trị tuyển chọn các tình huống đạo đức và chú thích các giá trị liên quan trong Lý thuyết Giá trị Cơ bản Con người của Schwartz (Schwartz, 2012) đằng sau mỗi mẫu. Đồng thời, một bộ phân loại giá trị được xây dựng dựa trên bộ sưu tập. Ngoài các bộ phân biệt được đề cập ở trên được huấn luyện cho một mục tiêu cụ thể, LLMs đã nhận ra giá trị con người và đạo đức (Schramowski et al., 2022), do đó chúng có thể hoạt động như những nhà phê bình. Hơn nữa, chúng có thể được tăng cường bởi cách few-shot hoặc chain-of-thought (Bai et al., 2022b).

--- TRANG 13 ---
4 Thuật toán Đồng bộ hóa

Phần này giới thiệu ngắn gọn bốn lớp thuật toán đồng bộ hóa để trả lời câu hỏi quan trọng khác, tức là 'Làm thế nào để đồng bộ hóa mô hình lớn với một mục tiêu cho trước'. Vì bài báo này tập trung vào thảo luận về các mục tiêu đồng bộ hóa của mô hình lớn, nhiều chi tiết hơn có thể được tham khảo đến (Wang et al., 2023).

Học trong Ngữ cảnh Vì mô hình lớn đã có được kiến thức và khả năng đáng kể (Brown et al., 2020; OpenAI, 2023), học trong ngữ cảnh đã nổi lên như một phương pháp đồng bộ hóa hứa hẹn để điều chỉnh hành vi của LLMs bằng cách bao gồm mục tiêu đồng bộ hóa trong prompt (Ganguli et al., 2023). Ví dụ, bằng cách kết hợp 'Đảm bảo rằng câu trả lời của bạn công bằng và không dựa vào định kiến' trong prompt, LLM có thể giảm định kiến trong đầu ra. Phương pháp này sẽ không hy sinh khả năng cơ bản của mô hình mà không sửa đổi tham số mô hình. Tuy nhiên, nó hoàn toàn dựa vào khả năng tự sửa chữa của mô hình và có thể không khả thi đối với mô hình lớn hoạt động kém.

Tinh chỉnh Có giám sát (SFT) Không giống như học trong ngữ cảnh, các phương pháp sau yêu cầu tinh chỉnh tham số mô hình. Đối với SFT, các nhà nghiên cứu sử dụng dữ liệu cặp <đầu vào, đầu ra> được xây dựng thủ công bao gồm hướng dẫn con người, sở thích con người và các vấn đề an toàn khác để huấn luyện mô hình theo cách có giám sát. Nhiều chiến lược được thiết kế để tự động tạo ra dữ liệu hướng dẫn bằng cách prompt LLMs, như Self-Instruct (Wang et al., 2022a) và SELF-ALIGN (Sun et al., 2023c). SFT là một mô hình với ưu điểm của huấn luyện ổn định và hội tụ nhanh. Tuy nhiên, nó cũng gặp phải hai nhược điểm, tức là tổng quát hóa kém cho các đầu vào người dùng chưa thấy cũng như thiếu phản hồi tiêu cực.

Học Tăng cường Để giải quyết các vấn đề được đề cập ở trên, LLMs giới thiệu học tăng cường trong giai đoạn tinh chỉnh. Học Tăng cường từ Phản hồi Con người (RLHF) đại diện nhất (Ouyang et al., 2022) có ba giai đoạn. Đầu tiên, nó xây dựng dữ liệu đồng bộ con người để tinh chỉnh mô hình sử dụng SFT. Thứ hai, nó thu thập và xếp hạng các phản hồi do mô hình tạo ra với chất lượng khác nhau để huấn luyện một mô hình phần thưởng. Thứ ba, nó áp dụng mô hình phần thưởng trong tinh chỉnh LLM thông qua PPO (Schulman et al., 2017). Các phương pháp tổng hợp dữ liệu được đề xuất để giảm sự phụ thuộc vào phản hồi thủ công (Kim et al., 2023; Bai et al., 2022b). Tuy nhiên, chi phí huấn luyện của RL cao, quá trình huấn luyện không ổn định và nhạy cảm với cài đặt siêu tham số.

Các Phương pháp Khác Được thúc đẩy bởi huấn luyện không ổn định trong PPO, các phương pháp không cần mô hình hóa phần thưởng rõ ràng hoặc học tăng cường được thiết kế. DPO (Rafailov et al., 2023) trực tiếp tối ưu hóa xác suất log tương đối giữa các phản hồi mong muốn và không mong muốn. RAFT (Dong et al., 2023) áp dụng một mô hình phần thưởng để lọc các mẫu chất lượng cao cho tinh chỉnh. Yuan et al. (2023) đề xuất RRHF, thu thập các phản hồi từ các nguồn có chất lượng khác nhau và sau đó huấn luyện LLM với một hàm mất mát xếp hạng. Tất cả những phương pháp cải tiến này giữ lại tín hiệu của sở thích con người trong khi tránh các vấn đề như độ nhạy cảm siêu tham số trong RL.

5 Thách thức và Nghiên cứu Tương lai

Với sự phát triển của mô hình lớn và sự gắn kết ngày càng tăng của chúng với cuộc sống hàng ngày của con người, việc đồng bộ hóa chúng với con người chắc chắn là một vấn đề nghiên cứu quan trọng. Khảo sát này đã trình bày một cái nhìn tổng quan toàn diện về các mục tiêu đồng bộ hóa khác nhau, như được thể hiện trong Hình 5. Cấp độ đầu tiên là đồng bộ hóa với hướng dẫn con người, tập trung vào khả năng cơ bản của mô hình lớn để hiểu hướng dẫn người dùng và hoàn thành các tác vụ đa dạng. Để làm cho mô hình lớn tối đa hóa lợi ích con người và giảm thiểu rủi ro tiềm ẩn của chúng, sở thích con người và giá trị con người phục vụ như các mục tiêu đồng bộ hóa cao hơn. Tuy nhiên, sở thích con người thường được phản ánh bởi phản hồi con người ngầm định về các hành vi mô hình cụ thể, do đó việc đạt được mục tiêu này có thể dẫn đến đồng bộ hóa trên đa số các hành vi bề mặt, yếu về mặt toàn diện, tổng quát hóa và ổn định. Với việc giới thiệu giá trị con người, việc đồng bộ hóa mô hình lớn với các nguyên tắc giá trị nội tại thay vì các hành vi biểu hiện không đếm được cung cấp một cơ hội hứa hẹn để giải quyết những thách thức được đề cập ở trên.

Hiện tại, công trình nghiên cứu về cấp độ mục tiêu đồng bộ hóa này tương đối mới nổi, trong khi thiếu hiểu biết và khám phá sâu sắc. Để truyền cảm hứng cho nhiều nghiên cứu hơn, chúng tôi thảo luận về một số hướng nghiên cứu tương lai có thể như sau.

5.1 Định nghĩa Một Hệ thống Giá trị Phù hợp

Nghiên cứu hiện tại về đồng bộ hóa mô hình lớn với giá trị con người đã điều tra một số loại nguyên tắc giá trị. Ví dụ, một số lượng lớn Rule-of-Thumbs (RoTs) được gán nhãn cho các hành vi và tình huống để hỗ trợ phân biệt đạo đức (Jiang et al., 2021). Tuy nhiên, mỗi phần RoT thường được thiết kế cho một tình huống cụ thể hoặc một loại tình huống, và khó bao phủ tất cả các tình huống đạo đức. 'HHH' (hữu ích, trung thực, vô hại) là một nguyên tắc giá trị được sử dụng rộng rãi khác trong kỷ nguyên của mô hình lớn, đôi khi được diễn giải như một danh sách các quy tắc (Bai et al., 2022b; Glaese et al., 2022). Mặc dù ba khía cạnh đủ chung để bao phủ hầu hết các tình huống cần được đồng bộ hóa, chúng vẫn có thể thất bại trong các tình huống phức tạp nơi ba mục tiêu xung đột bởi vì ưu tiên ổn định của ba tiêu chí chưa bao giờ được thảo luận đầy đủ. Ngoài ra, nguyên tắc giá trị 'HHH' hơi heuristic. Nhiều quy tắc và hướng dẫn chú thích được chi phối bởi một số ít nhà nghiên cứu, mà không có thảo luận và xác minh đầy đủ về sự phù hợp của chúng cho các vấn đề giá trị AI.

Do đó, việc điều tra một hệ thống giá trị phù hợp hơn như mục tiêu cuối cùng của đồng bộ hóa mô hình lớn là quan trọng. Chúng tôi lập luận rằng hệ thống giá trị được mong đợi là khoa học, toàn diện để đối phó với tất cả các tình huống, ổn định trong các trường hợp cực đoan và được xác nhận là khả thi bởi bằng chứng thực tế. Hai lý thuyết giá trị cơ bản, tức là Lý thuyết Giá trị Cơ bản Con người của Schwartz (Schwartz, 2012) và Lý thuyết Nền tảng Đạo đức (Graham et al., 2013) được giới thiệu trong Phần 2.3.1, có thể hứa hẹn vì tính toàn diện và hiệu quả của chúng đã được xác minh trong lĩnh vực khoa học xã hội. Trong khi chúng được thiết kế đặc biệt cho con người, cách thích ứng những lý thuyết này để dự đoán và điều chỉnh giá trị của AI vẫn cần được khám phá.

5.2 Biểu diễn Mục tiêu Có thể Tổng quát hóa & Ổn định

Để đồng bộ hóa mô hình lớn với một mục tiêu cụ thể, cần thiết phải chuyển đổi mục tiêu này thành một mục tiêu huấn luyện mô hình, như bộ ba <hướng dẫn, đầu vào, đầu ra> cho đồng bộ hóa hướng dẫn con người (Wang et al., 2022a) và điểm được cung cấp bởi một mô hình phần thưởng để chỉ ra sở thích con người (Ouyang et al., 2022). Cho sự phức tạp và thách thức của đồng bộ hóa giá trị nội tại trong Hình 5, chúng tôi thảo luận rằng phương pháp biểu diễn mục tiêu đồng bộ hóa có thể được tăng cường từ ba khía cạnh.

Đầu tiên là khả năng tổng quát hóa để cung cấp tín hiệu giám sát chính xác bao phủ các tình huống tùy ý từ miền mở hoặc thậm chí các trường hợp ngoài phân phối (OOD). Về đồng bộ hóa hướng dẫn, các loại tác vụ và prompt đa dạng được tạo ra để tổng quát hóa tốt hơn (Longpre et al., 2023; Iyer et al., 2022), vẫn gặp khó khăn trong việc bao phủ tất cả các tác vụ và tăng chi phí chú thích. Huấn luyện một mô hình sở thích trên dữ liệu hạn chế để tạo ra phản hồi con người cho các hành vi không giới hạn là một giải pháp khác (Ouyang et al., 2022; Bai et al., 2022a). Tuy nhiên, cả hai mục tiêu đều được biểu diễn hoàn toàn bởi các hành vi quan sát được và do đó khó tổng quát hóa cho các ngoại lệ. Với các nguyên tắc giá trị toàn diện được định nghĩa trước, chúng tôi lập luận rằng nếu những nguyên tắc này được liên quan một cách rõ ràng trong biểu diễn mục tiêu, điều này có thể giúp cải thiện khả năng tổng quát hóa. Thứ hai là ổn định để cung cấp tín hiệu giám sát ổn định và nhất quán trong cả tình huống bình thường và cực kỳ thử thách nơi những khác biệt tinh tế trong ưu tiên giá trị có thể dẫn đến các hành vi khác nhau một cách mạnh mẽ. Những ưu tiên tinh tế như vậy giữa các nguyên tắc giá trị khác nhau nên được biểu diễn một cách rõ ràng bởi vì có thể khó học thông tin này chỉ từ các hành vi chung. Thứ ba là khả năng diễn giải, tức là mục tiêu đồng bộ hóa được mong đợi được biểu diễn theo cách có thể diễn giải, điều này bị bỏ qua bởi công trình hiện có. Vì việc đồng bộ hóa mô hình lớn với con người có liên quan chặt chẽ đến việc giải quyết các vấn đề về an toàn và rủi ro AI, mô hình hóa minh bạch của mục tiêu đồng bộ hóa giúp đảm bảo hướng đúng. Hơn nữa, một phương pháp có thể diễn giải có thể tạo điều kiện cho việc debug để có khả năng tổng quát hóa và ổn định.

5.3 Đánh giá Tự động & Toàn diện về Đồng bộ hóa

Các tiêu chuẩn và phương pháp đánh giá chính xác và bền vững là quan trọng để hướng dẫn nghiên cứu về đồng bộ hóa giá trị con người. Hiện tại, một số tiêu chuẩn được xây dựng trước kỷ nguyên của mô hình lớn được thích ứng cho đánh giá, như TruthfulQA (Lin et al., 2022) và RealToxicityPrompts (Gehman et al., 2020). Đồng thời, một số tiêu chuẩn mới được đề xuất dần dần, bao gồm SafetyPrompts (Sun et al., 2023a), CVALUES (Xu et al., 2023b) và vân vân. Tất cả những tiêu chuẩn mới này phụ thuộc vào người đánh giá con người cho phán đoán cuối cùng, làm cho chúng đắt đỏ và không dễ dàng mở rộng. Mặc dù LLMs mạnh có thể hoạt động như một lựa chọn thay thế hiệu quả cho phán đoán, điều này hoàn toàn dựa vào khả năng của LLMs và đưa vào tính ngẫu nhiên. Do đó, các phương pháp và số liệu đánh giá tự động để đo lường mức độ đồng bộ hóa giữa LLMs và con người được yêu cầu khẩn cấp để tăng tốc quá trình đánh giá.

Để đánh giá nơi LLMs được đồng bộ hóa đầy đủ với giá trị con người, chúng nên trải qua đánh giá toàn diện qua các cấp độ khó khăn khác nhau: 1) khả năng hiểu và đồng ý với giá trị con người; 2) khả năng chẩn đoán các tình huống liên quan đến giá trị và đưa ra phán đoán đúng; 3) khả năng thực hiện nhất quán với giá trị con người, thậm chí trong các thử thách; và nhiều hơn nữa. Đánh giá này trở nên khó khăn hơn và khó hơn, từ phân biệt đơn giản đến hành vi chính xác, cố gắng phát hiện các giá trị quan trọng nhất của LLMs đằng sau các hành vi được gây ra của chúng. Vì ưu tiên giữa các nguyên tắc giá trị chỉ có thể quan trọng trong một số tình huống thử thách, chúng ta cũng nên xem xét các trường hợp dylemma cụ thể trong đánh giá để tìm ra thông tin tinh tế như vậy.

5.4 Thuật toán Đồng bộ hóa Hiệu quả & Ổn định

Với một mục tiêu cao hơn của đồng bộ hóa mô hình lớn được thiết lập, tức là giá trị nội tại, các thuật toán đồng bộ hóa phù hợp cần được khám phá. Hiện tại, các phương pháp chủ đạo điều chỉnh LLMs để đồng bộ với các hành vi được ưa thích thông qua tinh chỉnh có giám sát (SFT) hoặc học tăng cường từ phản hồi con người (RLHF), mà không có nhận thức rõ ràng về các nguyên tắc giá trị. Những phương pháp này có xu hướng không hiệu quả và không ổn định. Một mặt, chúng phụ thuộc vào một tập lớn các mẫu huấn luyện và khó đảm bảo rằng tất cả các chiều giá trị được bao phủ. Mặt khác, một số tiếng ồn có thể tồn tại trong tập dữ liệu huấn luyện và có xung đột giữa các mẫu khác nhau. Constitutional AI (Bai et al., 2022b) đã được thiết kế để là một phương pháp hiệu quả hơn, nơi dữ liệu huấn luyện được lấy mẫu dựa trên các nguyên tắc giá trị rõ ràng và được chú thích bởi một AI mạnh để giảm lao động con người. Tuy nhiên, LLM mục tiêu vẫn chưa trực tiếp học cách hành xử từ những nguyên tắc giá trị này mà từ các minh chứng liên quan. Thực sự, học trong ngữ cảnh là một phương pháp tiềm năng để trực tiếp prompt LLMs với các nguyên tắc giá trị mục tiêu và điều chỉnh hành vi của chúng (Ganguli et al., 2023). Nhưng khó hoàn toàn sửa đổi hành vi và giá trị nội tại của nó mà không tinh chỉnh các tham số. Và kiểm soát ưu tiên giữa các giá trị cũng là một thách thức. Do đó, việc phát triển các thuật toán đồng bộ hóa hiệu quả và ổn định trực tiếp đồng bộ LLMs với các nguyên tắc giá trị con người thay vì các minh chứng proxy là quan trọng cho nghiên cứu tương lai. Ngoài ra, giá trị con người có tính đa nguyên qua các quốc gia và liên tục phát triển mọi lúc. Do đó, phương pháp đồng bộ hóa cũng được mong đợi có thể thích ứng hiệu quả với các nguyên tắc giá trị khác nhau.

6 Kết luận

Việc đồng bộ hóa mô hình lớn với con người đã thu hút sự chú ý đáng kể để làm cho chúng phục vụ nhân loại tốt hơn và giảm thiểu rủi ro tiềm ẩn của chúng. Bài báo này nhấn mạnh tầm quan trọng của việc xác định các mục tiêu quan trọng cho đồng bộ hóa mô hình lớn, và trình bày khảo sát đầu tiên để cung cấp một cái nhìn tổng quan toàn diện từ hai góc độ: định nghĩa của mỗi mục tiêu đồng bộ hóa và đánh giá mức độ đồng bộ hóa. Chúng tôi phân loại các mục tiêu đồng bộ hóa xuất hiện trong tài liệu hiện có thành ba nhóm chính: hướng dẫn con người, sở thích con người và giá trị con người, quan sát một xu hướng phát triển trong các mục tiêu đồng bộ hóa chuyển từ khả năng cơ bản đến định hướng giá trị, và từ hành vi bề mặt đến giá trị nội tại. Để đồng bộ hóa mô hình lớn tốt hơn từ góc độ quan trọng của giá trị con người nội tại, chúng tôi thảo luận về một số thách thức và hướng nghiên cứu tương lai hứa hẹn cuối cùng. Hơn nữa, chúng tôi cung cấp một danh sách các tài nguyên có sẵn công khai cho đồng bộ hóa mô hình lớn. Chúng tôi mong đợi khảo sát này có thể phục vụ như cả một giới thiệu và nguồn cảm hứng cho các nhà nghiên cứu và thực hành trong lĩnh vực đồng bộ hóa mô hình lớn.
