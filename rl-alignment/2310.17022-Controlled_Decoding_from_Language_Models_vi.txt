# Giải mã có kiểm soát từ các mô hình ngôn ngữ
Sidharth Mudgal* 1Jong Lee* 1Harish Ganapathy1YaGuang Li1Tao Wang2Yanping Huang1
Zhifeng Chen1Heng-Tze Cheng1Michael Collins1Trevor Strohman1Jilin Chen1Alex Beutel2
Ahmad Beirami1

## Tóm tắt
Học tăng cường có điều chỉnh KL (RL) là một khung căn chỉnh phổ biến để kiểm soát các phản hồi của mô hình ngôn ngữ hướng tới các kết quả có phần thưởng cao. Chúng tôi đặt ra một mục tiêu RL theo từng token và đề xuất một bộ giải mô-đun cho nó, được gọi là giải mã có kiểm soát (CD). CD thực hiện kiểm soát thông qua một mô-đun chấm điểm tiền tố riêng biệt, được huấn luyện để học một hàm giá trị cho phần thưởng. Bộ chấm điểm tiền tố được sử dụng tại thời điểm suy luận để kiểm soát việc sinh ra từ một mô hình cơ sở đã đóng băng, có thể chứng minh được là lấy mẫu từ một nghiệm của mục tiêu RL. Chúng tôi chứng minh thực nghiệm rằng CD hiệu quả như một cơ chế kiểm soát trên các benchmark phổ biến. Chúng tôi cũng cho thấy rằng các bộ chấm điểm tiền tố cho nhiều phần thưởng có thể được kết hợp tại thời điểm suy luận, giải quyết hiệu quả một bài toán RL đa mục tiêu mà không cần huấn luyện thêm. Chúng tôi cho thấy rằng lợi ích của việc áp dụng CD chuyển giao sang một mô hình cơ sở chưa thấy mà không cần điều chỉnh thêm. Cuối cùng, chúng tôi cho thấy rằng CD có thể được áp dụng theo kiểu giải mã từng khối tại thời điểm suy luận, về cơ bản là cầu nối khoảng cách giữa chiến lược best-of-K phổ biến và kiểm soát theo từng token thông qua học tăng cường. Điều này làm cho CD trở thành một phương pháp hứa hẹn cho việc căn chỉnh các mô hình ngôn ngữ.

## 1. Giới thiệu
Các mô hình ngôn ngữ sinh đã đạt đến mức có thể giải quyết hiệu quả nhiều nhiệm vụ miền mở với ít giám sát cụ thể cho nhiệm vụ. Do đó, điều quan trọng là phải hỏi: làm thế nào chúng ta có thể căn chỉnh nội dung do máy tạo ra với các phần thưởng khi chúng ta không có quyền kiểm soát các biểu diễn đã được huấn luyện trước trong một mô hình ngôn ngữ sinh?

Kiểm soát các phản hồi của mô hình ngôn ngữ hướng tới các kết quả có phần thưởng cao là một lĩnh vực nghiên cứu tích cực trong tài liệu. Chúng tôi chia các phương pháp căn chỉnh hiện có thành hai loại khác nhau đáng kể trong triển khai thực tế: cải thiện bộ sinh và các giải pháp bổ sung tại thời điểm suy luận.

Các giải pháp cải thiện bộ sinh, như KL-regularized PPO (Christiano et al., 2017; Ouyang et al., 2022), tối ưu hóa sở thích trực tiếp (DPO) (Rafailov et al., 2023), hiệu chỉnh khả năng xảy ra chuỗi (SliC) (Zhao et al., 2022), và tối ưu hóa sở thích nhận dạng (IPO) (Azar et al., 2023) cập nhật các trọng số của mô hình ngôn ngữ để căn chỉnh nó với một mô hình phần thưởng. Chúng hiệu quả cho suy luận nhưng cung cấp ít khả năng cấu hình trên phần thưởng.

Một giải pháp bổ sung tại thời điểm suy luận đơn giản và hiệu quả là best-of-K (Nakano et al., 2021; Stiennon et al., 2020; Touvron et al., 2023), nơi K mẫu i.i.d. được rút từ một mô hình cơ sở, được xếp hạng dựa trên một phần thưởng, và cái có xếp hạng cao nhất được chọn. Các phương pháp khác, như FUDGE (Yang & Klein, 2021) hoặc COLD (Qin et al., 2022), cung cấp một bộ chấm điểm tiền tố được sử dụng tại thời điểm suy luận để kiểm soát phản hồi của mô hình cơ sở đã đóng băng hướng tới các kết quả có phần thưởng cao. Do tính mô-đun của thiết kế để mô hình cơ sở đóng băng, các phương pháp này cung cấp khả năng cấu hình tại thời điểm suy luận. Mục tiêu của chúng tôi là đề xuất một khung học tập cho các phương pháp như vậy.

Các đóng góp của chúng tôi được tóm tắt dưới đây.

• Chúng tôi chính thức hóa một phương pháp căn chỉnh mô-đun, giải mã có kiểm soát (CD), để giải quyết một mục tiêu RL có điều chỉnh KL. CD học một bộ chấm điểm tiền tố cho phần thưởng được sử dụng để điều hướng việc sinh ra từ một đường dẫn đã giải mã một phần.

• Chúng tôi cho thấy rằng hai biến thể của CD, cụ thể là CD-FUDGE (Yang & Klein, 2021) và CD-Q (của chúng tôi), có thể chứng minh được dẫn đến lấy mẫu từ một nghiệm của mục tiêu RL.

• Chúng tôi đề xuất CD theo khối nơi bộ chấm điểm tiền tố được sử dụng để chọn best-of-K đường dẫn cho một khối đã giải mã gồm M token. Điều này cầu nối khoảng cách giữa best-of-K cấp độ chuỗi và các phương pháp RL theo từng token.

• Chúng tôi chỉ ra thực nghiệm rằng CD cung cấp cải thiện đáng kể so với các giải pháp sinh/giải mã có kiểm soát hiện có trên các benchmark phổ biến.

• Chúng tôi cho thấy rằng bộ chấm điểm tiền tố CD chuyển giao sang một mô hình cơ sở chưa thấy mà không cần huấn luyện thêm.

• Chúng tôi chứng minh tính mô-đun của CD tại thời điểm suy luận để tích hợp nhiều phần thưởng vào một quy tắc chấm điểm tiền tố duy nhất, và áp dụng nó cho một mô hình cơ sở chưa thấy.

## 2. Học tăng cường có điều chỉnh KL

Gọi x là một lời nhắc (gồm nhiều token) và gọi y = yT := [y1, ..., yT] biểu diễn một phản hồi là một nối tiếp của T token. Ở đây mỗi token yt ∈ Y, nơi Y biểu diễn bảng chữ cái (từ vựng). Gọi πref biểu thị một mô hình ngôn ngữ được huấn luyện trước (LM) được sử dụng để rút các mẫu theo cách tự hồi quy. Đặc biệt, chúng tôi sử dụng πref(·|[x, yt]) để biểu thị phân phối mà LM tạo ra trên token tiếp theo trên bảng chữ cái Y cho đầu vào là nối tiếp của lời nhắc x và một phản hồi đã giải mã một phần yt gồm t token. Gọi r([x,y]) là một hàm phần thưởng có giá trị vô hướng bị chặn từ trên, ví dụ, log-likelihood của một hàm chấm điểm cho sự kiện phản hồi y trong ngữ cảnh x được coi là an toàn. Chúng tôi định nghĩa phần thưởng theo từng token sau:

R([x, yt]) := {0 nếu yt ≠ EOS; r([x, yt]) nếu yt = EOS}

nơi EOS biểu diễn kết thúc chuỗi. Ở đây, chúng tôi chỉ đưa ra phần thưởng một khi giải mã đã hoàn thành và ngược lại không có phần thưởng nào được gán cho một đường dẫn giải mã. Sau đó chúng tôi định nghĩa hàm giá trị liên quan đến phần thưởng là:

V⋆([x, yt]) := Ez1,z2,...∼πref [∑τ≥0 R([x, yt, zτ])]     (1)

Hàm giá trị nắm bắt phần thưởng tích lũy kỳ vọng của một phản hồi đã giải mã đầy đủ khi giải mã tiếp tục từ một chuỗi đã giải mã một phần yt, sử dụng mô hình ngôn ngữ cơ sở πref.

Đối với một [x, yt] cho trước sao cho yt ≠ EOS, chúng tôi định nghĩa hàm lợi thế của một chính sách giải mã π là:

A([x, yt]; π) := Ez∼π [V⋆([x, yt, z])] − V⋆([x, yt])
= ∑z∈Y π(z|[x, yt])V⋆([x, yt, z]) − V⋆([x, yt])

Lưu ý rằng lợi thế của chính sách cơ sở được cho bởi A([x, yt]; πref) = 0 (định luật xác suất toàn phần), và do đó mục tiêu của chúng tôi là chọn π để lệch khỏi πref để đạt được lợi thế tích cực so với chính sách cơ sở.

Gọi D([x, yt]; π) là phân kỳ KL theo từng token giữa một chính sách giải mã π và một mô hình ngôn ngữ cơ sở đã đóng băng πref để giải mã token tiếp theo sau [x, yt] cho yt ≠ EOS:

D([x, yt]; π) := KL(π(·|[x, yt])‖πref(·|[x, yt]))
= ∑z∈Y π(z|[x, yt]) log[π(z|[x, yt])/πref(z|[x, yt])]

nơi KL(·‖·) biểu thị phân kỳ KL (còn được biết đến như entropy tương đối). Hãy nhớ rằng mục tiêu của chúng tôi không phải là lệch quá nhiều khỏi chính sách cơ sở (được đo bằng phân kỳ KL) vì điều đó được kỳ vọng sẽ dẫn đến sự thoái hóa của mô hình ngôn ngữ trong các chỉ số hiệu suất hàng đầu khác.

Để thỏa mãn những mục tiêu mâu thuẫn này, chúng tôi sử dụng mục tiêu RL có điều chỉnh KL được định nghĩa là:

Jλ([x, yt]; π) := λA([x, yt]; π) − D([x, yt]; π)     (2)

nơi λ ∈ R≥0 cân bằng giữa phần thưởng và độ lệch khỏi mô hình ngôn ngữ cơ sở. Lưu ý rằng Jλ([x, yt]; π) lõm trong π. Điều này là do A([x, yt]; π) tuyến tính trong π và D([x, yt]; π) lồi trong π. Số hạng đầu tiên biểu thị số hạng lợi thế cho phần thưởng cuối cùng sẽ được nhận một khi phản hồi được giải mã đầy đủ. Số hạng thứ hai là tín hiệu phần thưởng âm của mô hình ngôn ngữ (LM) phạt chính sách π vì lệch quá xa khỏi chính sách ban đầu πref.

Chúng tôi gọi π⋆λ(z|[x, yt]) biểu thị hàm chính sách giải mã tối đa hóa (2). Lưu ý rằng ở cực trị λ = 0, chúng ta có π⋆0(z|[x, yt]) = πref(z|[x, yt]) đạt được D([x, yt]; πref) = 0 và A([x, yt]; πref) = 0. Chúng tôi quan tâm đến việc đặc trưng các đường cong đánh đổi giữa A và D đạt được bởi λ ∈ R≥0 để tăng A([x, yt]; π) với chi phí của một hình phạt KL tăng lên, D([x, yt]; π). Kết quả chính của chúng tôi trong phần này là đặc trưng sau của π⋆λ.

**Định lý 2.1.** Chính sách tối ưu cho mục tiêu RL là duy nhất và được cho bởi

π⋆λ(z|[x, yt]) ∝ p(z|[x, yt])e^(λV⋆([x,yt,z]))     (3)

Kết quả này giống với kết quả của (Korbak et al., 2022), với sự khác biệt chính là bộ điều khiển ở đây theo từng token. Hãy nhớ rằng mục tiêu của chúng tôi là phát triển một giải pháp căn chỉnh tại thời điểm suy luận để giữ mô hình ngôn ngữ đóng băng. Định lý 2.1 cho chúng tôi một cách để làm điều đó bằng cách kết hợp logit từ một LM đã đóng băng và logit của một hàm giá trị.

**Ghi chú.** Công thức RL theo từng token ở đây hạn chế hơn so với RL cấp độ chuỗi, được sử dụng để thiết kế RLHF và DPO. Tuy nhiên, chúng tôi sẽ so sánh với chúng về đánh đổi phần thưởng kỳ vọng cấp độ chuỗi so với KL.

## 3. Giải mã có kiểm soát

Mục tiêu của chúng tôi là học Vθ([x, yt]) được tham số hóa bởi θ để khớp với V⋆([x, yt]) thông qua hàm mục tiêu L2 sau:

L⋆(θ) = Ex∼μ Ey∼πref(·|x) ℓ⋆(x,y;θ)

nơi ℓ⋆(x,y;θ) = (1/2) ∑t∈[|y|] (Vθ([x, yt]) − V⋆([x, yt]))²

nơi μ là một phân phối trên các lời nhắc huấn luyện. Tiếp theo, chúng tôi trình bày hai phương pháp để học bộ chấm điểm tiền tố, và hai cách để sử dụng nó tại thời điểm suy luận để kiểm soát.

### 3.1. Huấn luyện bộ chấm điểm tiền tố

**CD-FUDGE (Yang & Klein, 2021).** Cho x ∼ μ, gọi y = ([y1, ..., yT]) là một rút ngẫu nhiên từ mô hình cơ sở πref. Xem r([x,y]) là phần thưởng ngẫu nhiên của việc hoàn thành đã giải mã đầy đủ, y. Gọi

LF(θ) = Ex∼μ ℓF(x,y;θ), s.t. y ∼ πref     (4)

nơi ℓF(x,y;θ) = (1/2) ∑t∈[|y|] (Vθ([x, yt]) − r([x,y]))²

Bây giờ chúng tôi phát biểu kết quả chính về CD-FUDGE, được phát biểu chính thức và chứng minh trong Phụ lục C, Định lý C.2.

**Định lý 3.1 (không chính thức).** Dưới các giả định về tính chính quy, SGD trên LF hội tụ về một điểm dừng của L⋆(θ).

Đây là một kết quả đáng chú ý. Nó phát biểu rằng nếu tập dữ liệu được sử dụng để huấn luyện bộ chấm điểm tiền tố trong FUDGE (Yang & Klein, 2021) được thu được bằng cách triển khai mô hình cơ sở, thì bộ chấm điểm tiền tố FUDGE có thể được sử dụng để giải quyết bài toán RL trong Phương trình (2). Tiếp theo, chúng tôi phát biểu đề xuất của chúng tôi là một bộ giải off-policy mà không cần triển khai mô hình cơ sở.

**CD-Q.** Chú ý đồng nhất Bellman sau (Sutton & Barto, 2018):

V⋆([x, yt]) = {Ez∼πref(·|[x,yt]) V⋆([x, yt, z]) nếu yt ≠ EOS; r([x, yt]) nếu yt = EOS}

Chúng tôi trình bày một giải pháp đơn giản để huấn luyện một bộ chấm điểm tiền tố. Lấy cảm hứng từ các cập nhật đánh giá chính sách trong DQN (Mnih et al., 2013), chúng tôi tối ưu hóa hàm mất mát sau:

LQ(θ) = Ex∼μ ℓQ(x,y;θ)     (5)

nơi ℓQ(x, yt;θ) = (1/2) ∑t∈[|y|] (Vθ([x, yt]) − v̇t)²

vt = {∑z∈Y πref(z|[x, yt])Vθ([x, yt, z]) nếu yt ≠ EOS; r([x, yt]) nếu yt = EOS}

và nơi v̇ ngụ ý một stop gradient trên v (mặc dù nó vốn phụ thuộc vào θ).

Quy trình học tập nêu trên cho bộ chấm điểm tiền tố có thể được thực hiện trên một tập dữ liệu off-policy, được chấm điểm offline sử dụng phần thưởng cho tất cả [x,y] (Sutton & Barto, 2018). Mặt khác, huấn luyện bộ chấm điểm tiền tố yêu cầu truy cập (theo yêu cầu) vào mô hình ngôn ngữ cơ sở πref để tính toán mục tiêu vt trong (5). Một sửa đổi đơn giản của quy trình này có thể được chỉ ra là hội tụ có thể chứng minh (Wang & Ueda, 2022). Chúng tôi cũng lưu ý rằng nhiều cải tiến khác so với DQN đã được đề xuất trong những năm qua, nhiều trong số đó tương đương với Rainbow (Hessel et al., 2018). Khám phá cách cải thiện CD-Q bằng các kỹ thuật này là một lĩnh vực thú vị cho công việc tương lai.

### 3.2. Chiến lược lấy mẫu tại thời điểm suy luận

Được trang bị bộ chấm điểm tiền tố, chúng tôi sử dụng nó theo hai cách khác nhau tại thời điểm suy luận để căn chỉnh mô hình cơ sở.

**Lấy mẫu theo từng token.** Chúng tôi sử dụng bộ chấm điểm tiền tố để lấy mẫu theo từng token theo Định lý 2.1. Trong trường hợp này, cho ngữ cảnh x và một chuỗi đã giải mã một phần yt, chúng tôi thu được các logit của πref([x, yt, z]) và Vθ([x, yt, z]) cho tất cả z từ chính sách cơ sở và bộ chấm điểm tiền tố. Sau đó, chúng tôi kết hợp tuyến tính các logit để lấy mẫu từ phân phối sau:

z ∼ πθ(·|[x, yt])     (6)

nơi πθ(z|[x, yt]) ∝ πref(z|[x, yt])e^(λVθ([x,yt,z]))

Một minh họa về lấy mẫu theo từng token sử dụng bộ chấm điểm tiền tố CD được trình bày trong Hình 1, nơi bộ chấm điểm tiền tố được sử dụng để giảm trọng số giải mã các token có thể dẫn đến kết quả không mong muốn. Lưu ý rằng lấy mẫu theo từng token là cách đơn giản nhất để sử dụng bộ chấm điểm tiền tố, yêu cầu một lần gọi bộ chấm điểm tiền tố cho mỗi lần giải mã của mỗi token, và cũng được sử dụng bởi Yang & Klein (2021).

**Best-of-K theo khối.** Tiếp theo, chúng tôi trình bày một chiến lược lấy mẫu kết hợp RL với best-of-K. Chúng tôi lấy mẫu K khối tiếp tục i.i.d. có độ dài M từ chính sách cơ sở, và chấp nhận phần tiếp tục có điểm tiền tố cao nhất và từ chối phần còn lại:

zM := arg max{zM(k)}k∈[K] Vθ([x, yt, zM(k)])     (7)

nơi {zM(k)}k∈[K] i.i.d. ∼ πref(zM|[x, yt])

và tiếp tục cho đến khi một ứng viên có EOS được chấp nhận.

Một minh họa về lấy mẫu và xếp hạng lại theo khối được trình bày trong Hình 2, nơi bộ chấm điểm tiền tố được sử dụng để xếp hạng lại M(=4) đường dẫn giải mã và chọn ứng viên có tình cảm tích cực nhất.

**Kiểm soát theo khối so với theo từng token.** Lưu ý rằng tương tự như best-of-K, CD theo khối không được thiết kế để giải quyết tối ưu mục tiêu có điều chỉnh KL cấp độ chuỗi là mục tiêu của các phương pháp RLHF, như PPO và DPO. Tuy nhiên, thực nghiệm chúng tôi quan sát thấy rằng best-of-K thường dẫn đến đánh đổi phần thưởng-KL tốt hơn, ví dụ, (Gao et al., 2023, Hình 1) và (Rafailov et al., 2023, Hình 3). Thực tế, best-of-K được chỉ ra là gần như lấy mẫu từ phân phối căn chỉnh tối ưu thông qua RL có điều chỉnh KL (Yang et al., 2024). Điều này thúc đẩy việc khám phá các kỹ thuật kiểm soát theo khối dựa vào sức mạnh của best-of-K.

**Kiểm soát theo khối so với Best-of-K.** Về mặt thông lượng suy luận, CD theo khối tương tự như best-of-K cho cùng một giá trị K. Tuy nhiên, nó cung cấp hai lợi thế chính:

1. Độ trễ giải mã ở đây chỉ là M token, trong khi phương pháp best-of-K cần giải mã đầy đủ tất cả K chuỗi trước khi có thể chọn một để phục vụ. Nếu độ dài chuỗi lớn, ví dụ khi lời nhắc là viết một bài luận, điều này sẽ không được chấp nhận. Điều này có thể mở ra các ứng dụng mới như streaming.

2. Để đạt được phần thưởng cao, best-of-K có thể yêu cầu các giá trị K cao một cách không hợp lý. CD theo khối cho phép các giá trị phần thưởng tương tự với K nhỏ hơn đáng kể. Chúng tôi chỉ ra thực nghiệm cùng mức phần thưởng như best-of-K với K nhỏ hơn đến 10 lần.

## 4. Thiết lập thực nghiệm

Chúng tôi kiểm tra hiệu suất của các mô hình giải mã có kiểm soát với các chiến lược lấy mẫu tại thời điểm suy luận được đề xuất trên hai nhiệm vụ. Đối với tất cả các thực nghiệm, trừ khi được chỉ định khác, mô hình sinh cơ sở chúng tôi sử dụng là PaLM 2-XXS (Gecko), và bộ chấm điểm tiền tố cũng được tinh chỉnh từ PaLM 2-XXS.

### 4.1. Tập dữ liệu

**DSTC8 Reddit conversations corpus (Microsoft, 2019)** là một tập dữ liệu chứa hàng triệu cuộc trò chuyện đa lượt từ các chủ đề Reddit. Chúng tôi sử dụng tập dữ liệu này để tối ưu hóa độ dài phản hồi.

**Anthropic HH (Bai et al., 2022)** là một benchmark về tính hữu ích và không có hại nơi trợ lý cố gắng hoàn thành lượt tiếp theo trong cuộc trò chuyện với con người. Chúng tôi sử dụng điều này để huấn luyện một mô hình phần thưởng học các sở thích của con người về tính hữu ích và không có hại của việc sinh ra.

**TL;DR (Stiennon et al., 2020)** là một tập dữ liệu các bài đăng Reddit nơi mỗi ví dụ có thông tin về bài đăng, hai ứng viên tóm tắt, và một sở thích từ một người chú thích con người. Chúng tôi sử dụng điều này để huấn luyện một mô hình phần thưởng học sở thích tóm tắt.

### 4.2. Mô hình phần thưởng

**Độ dài phản hồi.** Chúng tôi sử dụng độ dài của phản hồi làm phần thưởng. Trong trường hợp này, chúng tôi sử dụng rlength([x, yT]) = log(T/Tmax), nơi Tmax = 1024.

**Tính hữu ích và không có hại.** Chúng tôi huấn luyện một mô hình phần thưởng (Reward-XXS) bằng cách tinh chỉnh PaLM 2-XXS sử dụng dữ liệu sở thích theo cặp của Anthropic HH (Bai et al., 2022) thông qua mô hình Bradley-Terry (BT) và chọn checkpoint có độ chính xác eval cao nhất. Ở đây, rHH([x, yT]) là log-probability của bộ phân loại HH pointwise kết quả.

**Chất lượng tóm tắt.** Tương tự, chúng tôi huấn luyện một mô hình phần thưởng PaLM 2-XXS sử dụng các sở thích theo cặp về chất lượng tóm tắt (Stiennon et al., 2020) sử dụng mô hình BT, và chọn checkpoint có độ chính xác eval cao nhất.

### 4.3. Baseline

Ngoài CD-Q và CD-Q theo khối, chúng tôi xem xét các baseline sau.

**CD-FUDGE (Yang & Klein, 2021)** được huấn luyện theo cách tương tự như CD-Q với sự khác biệt là mục tiêu trong (5) được thay thế bằng phần thưởng rõ ràng nhận được trong một đường dẫn giải mã cho trước từ tập dữ liệu. Để có hiệu suất tốt nhất, CD-FUDGE được huấn luyện trên một tập dữ liệu nơi các phản hồi được thu được bằng cách triển khai mô hình cơ sở. Ngoài ra, chúng tôi cũng xem xét biến thể best-of-K theo khối của FUDGE (Yang & Klein, 2021), được đặt tên là CD-FUDGE theo khối, được lấy cảm hứng từ phương pháp CD-Q theo khối được đề xuất trong bài báo này.

**KL-regularized PPO (Ouyang et al., 2022)** giải quyết một bài toán RL có điều chỉnh KL sử dụng PPO (Schulman et al., 2017).

**DPO (Rafailov et al., 2023)** được huấn luyện trên một tập dữ liệu sở thích theo cặp. Để so sánh công bằng hơn, chúng tôi sử dụng DPO trực tuyến bằng cách triển khai chính sách và lấy mẫu hai lần sinh và tối ưu hóa mục tiêu DPO trên các phần thưởng rõ ràng của chúng.

**IPO (Azar et al., 2023)** được huấn luyện theo cách tương tự như DPO ngoại trừ việc mục tiêu tích hợp điều chỉnh mới để tránh một số vấn đề thoái hóa của DPO. Tương tự như DPO, chúng tôi sử dụng IPO trực tuyến trong bài báo này.

**Best-of-K** là một giải pháp căn chỉnh tại thời điểm suy luận nơi K phản hồi được rút từ mô hình cơ sở, được xếp hạng sử dụng phần thưởng, và cái tốt nhất được chọn.

### 4.4. Chỉ số đánh giá

**Phân kỳ KL.** Chúng tôi đo phân kỳ KL giữa chính sách căn chỉnh và chính sách cơ sở, Ex∼μ Ey∼π(·|x){log π(y|x) − log πref(y|x)}, như một proxy cho sự thoái hóa của khả năng mô hình và tối ưu hóa quá mức phần thưởng. Đối với CD-Q và CD-FUDGE, chúng tôi quét cường độ của bộ chấm điểm tiền tố để kiểm soát KL(π‖πref). Đối với PPO, DPO và IPO, chúng tôi quét cường độ của bộ điều chỉnh KL (ngầm định) để đạt được cùng một mục tiêu. Cuối cùng, đối với best-of-K, CD-Q theo khối, và CD-FUDGE theo khối, chúng tôi làm điều này bằng cách quét K. Đối với best-of-K, chúng tôi sử dụng công thức cận trên về phân kỳ KL KL(π‖πref) ≤ log(K) − (K−1)/K (Stiennon et al., 2020; Beirami et al., 2024). Đối với các chiến lược lấy mẫu theo khối, chúng tôi sử dụng một cận trên về phân kỳ KL được cho bởi KL(π‖πref) ≤ Ex∼μ(log(K) − (K−1)/K)Lx/M, nơi Lx là số token được giải mã trong phản hồi đầy đủ cho lời nhắc x, đây là một mở rộng của (Beirami et al., 2024, Định lý 1). Vì vậy, chúng tôi tập trung vào các giá trị KL nhỏ hơn 10, ngoài đó chính sách cho thấy các dấu hiệu đáng kể của overfitting (Eisenstein et al., 2023). Chúng tôi cũng lưu ý rằng phân kỳ KL cấp độ chuỗi được sử dụng ở đây để đánh giá khác với thiết kế cấp độ token của chúng tôi, điều này làm cho việc đánh giá thuận lợi hơn cho PPO, DPO, và IPO trực tiếp tối ưu hóa đánh đổi giữa phần thưởng kỳ vọng và phân kỳ KL cấp độ chuỗi.

**Phần thưởng kỳ vọng chuẩn hóa.** Chúng tôi báo cáo phần thưởng kỳ vọng của chính sách căn chỉnh, Ex∼μ Ey∼πθ(·|x) r(x,y), được chuẩn hóa theo chính sách tham chiếu.

**Tỷ lệ thắng so với chính sách cơ sở.** Chúng tôi báo cáo tỷ lệ thắng của chính sách căn chỉnh so với chính sách cơ sở, Ex∼μ Ey∼πθ(·|x) Ez∼πref(·|x) 1[r(x,y) > r(x,z)].

**Đánh đổi phần thưởng so với KL.** Theo (Gao et al., 2023), chúng tôi báo cáo các đường cong đánh đổi cho phần thưởng so với phân kỳ KL giữa chính sách căn chỉnh và cơ sở, KL(π‖πref). Một phương pháp chiếm ưu thế (tức là tăng phần thưởng với ngân sách KL nhỏ nhất) là mong muốn hơn.

### 4.5. Chi tiết huấn luyện

**Thực nghiệm độ dài phản hồi.** Sử dụng corpus cuộc trò chuyện Reddit, chúng tôi sử dụng PaLM 2-XXS (Anil et al., 2023) để huấn luyện các bộ chấm điểm tiền tố và cũng làm mô hình cơ sở cho DPO, IPO, và PPO. Đối với DPO, IPO và PPO, chúng tôi thực hiện nhiều lần chạy huấn luyện, thay đổi các siêu tham số điều chỉnh và tỷ lệ học để đạt được KL tương đương với các phương pháp khác. Tất cả các phương pháp được huấn luyện trong nửa epoch và được đánh giá về số token trong việc sinh ra sử dụng tập eval của corpus cuộc trò chuyện.

**Thực nghiệm tính hữu ích và không có hại (HH).** Chúng tôi sử dụng mô hình phần thưởng để huấn luyện các bộ chấm điểm tiền tố, DPO, IPO và PPO sử dụng PaLM 2-XXS trên corpus cuộc trò chuyện Reddit với lời nhắc HH trong một epoch. Chúng tôi thực hiện nhiều lần chạy huấn luyện cho DPO, IPO và PPO để quét phân kỳ KL. Cuối cùng, chúng tôi sử dụng PaLM 2-L (Unicorn) (Anil et al., 2023) trên tập eval của corpus cuộc trò chuyện để đánh giá tính hữu ích và không có hại của việc sinh ra. Lời nhắc có thể được tìm thấy trong Phụ lục A.

**Thực nghiệm tóm tắt.** Chúng tôi sử dụng phần thưởng chất lượng tóm tắt để huấn luyện bộ chấm điểm tiền tố và chính sách căn chỉnh trên PaLM 2-XXS. Để đánh giá, chúng tôi nhắc PaLM 2-L (Unicorn) (Anil et al., 2023) trên tập test của corpus TL;DR để đánh giá chất lượng tóm tắt của các lần sinh ra so với PaLM 2-XXS vanilla, và báo cáo tỷ lệ thắng sở thích. Lời nhắc zeroshot chúng tôi sử dụng để đánh giá có thể được tìm thấy trong Phụ lục A.

## 5. Kết quả thực nghiệm

**Thực nghiệm 1: Tăng độ dài phản hồi đối thoại.** Trong thực nghiệm đầu tiên của chúng tôi, để có một chỉ số kiểm tra rõ ràng không có tối ưu hóa quá mức phần thưởng và nhiễu, chúng tôi xem xét độ dài phản hồi làm phần thưởng. Như có thể thấy trong Hình 3, phương pháp CD-Q theo khối được đề xuất của chúng tôi đạt được đánh đổi độ dài so với KL tốt nhất ngang với best-of-K, trong khi hiệu quả hơn đáng kể so với best-of-K vì nó đạt được các đánh đổi tương tự với K nhỏ hơn nhiều, ví dụ, với K=6, CD-Q theo khối có được độ dài và phân kỳ KL rất tương tự như best-of-K với K=50. Hơn nữa, best-of-K đạt được đánh đổi phần thưởng-KL tốt hơn so với KL-regularized PPO (Ouyang et al., 2022). Điều này có thể đáng ngạc nhiên lúc đầu, nhưng nó phù hợp với các phát hiện khác được báo cáo bởi Gao et al. (2023, Hình 1) và Rafailov et al. (2023, Hình 3), nơi nó được chỉ ra rằng best-of-K liên tục đạt được các đánh đổi phần thưởng-KL tốt hơn so với KL-regularized PPO. Gần đây, Yang et al. (2024) đã cung cấp lý do lý thuyết cho hiện tượng này bằng cách chỉ ra rằng best-of-K là một giải pháp gần như tối ưu cho bài toán RL có điều chỉnh KL.

Chúng tôi cũng quan sát thấy rằng kiểm soát theo từng token sử dụng cả CD-FUDGE (Yang & Klein, 2021) và CD-Q dẫn đến đánh đổi phần thưởng-KL thuận lợi hơn so với tất cả các baseline, bao gồm DPO và IPO.

Khi chúng tôi xem xét kiểm soát theo khối, chúng tôi thấy sự khác biệt rõ rệt giữa hành vi của CD-FUDGE theo khối và CD-Q theo khối, nơi CD-Q theo khối ngang với best-of-K, dẫn đến các đánh đổi phần thưởng-KL tốt nhất. Để điều tra thêm, chúng tôi sử dụng các bộ chấm điểm tiền tố CD-Q và CD-FUDGE làm bộ dự đoán phần thưởng (tức là độ dài) cho các phản hồi đã giải mã đầy đủ trên tập test, nơi kết quả được báo cáo trong Hình 13 (Phụ lục B). Phát hiện chính là các dự đoán của CD-FUDGE nhiễu hơn nhiều so với CD-Q và chúng tôi nghi ngờ đó là lý do CD-FUDGE không hoạt động tốt trong thiết lập theo khối, nơi CD-Q theo khối đạt được hiệu suất tốt nhất ngang với best-of-K.

**Thực nghiệm 2: Cải thiện tính hữu ích và không có hại (HH) của đối thoại.** Chúng tôi xem xét cải thiện tính hữu ích và không có hại (HH) của các phản hồi trong cuộc trò chuyện. Kết quả được báo cáo trong Hình 4, nơi trục y là tỷ lệ thắng so với mô hình cơ sở được đo bằng cách chạy zeroshot trên PaLM 2-L (Unicorn). Như có thể thấy, các bộ điều khiển theo từng token không cung cấp nhiều cải thiện HH so với các baseline, trong khi CD-Q theo khối và CD-FUDGE cung cấp một cải thiện đáng kể như mong đợi. Tuy nhiên, cả hai phương pháp đều không thể sánh với best-of-K.

Trong Bảng 1, chúng tôi so sánh độ chính xác huấn luyện và kiểm tra của Reward-XXS với độ chính xác của CD-Q và CD-FUDGE được sử dụng làm bộ phân loại, nơi chúng tôi áp dụng CD-Q và CD-FUDGE trên các cặp [x,y] trong tập huấn luyện và kiểm tra của tập dữ liệu Anthropic HH (Bai et al., 2022). Mục tiêu của thực nghiệm này là một kiểm tra tỉnh táo về bộ chấm điểm tiền tố vì hiệu suất tốt trên nhiệm vụ phân loại này là cần thiết nhưng không đủ để đảm bảo rằng bộ chấm điểm tiền tố có thể được sử dụng một cách đáng tin cậy trong thực tế. Kết quả cho thấy rằng độ chính xác phân loại của CD-Q và CD-FUDGE yếu hơn so với Reward-XXS (≈0.6 so với ≈0.7). Điều này có thể do bản chất nhiễu của dữ liệu huấn luyện, và là một lĩnh vực cho nghiên cứu tương lai để cải thiện việc huấn luyện sử dụng các phương pháp học hàm giá trị phù hợp hơn với môi trường phần thưởng nhiễu.

**Thực nghiệm 3: Cải thiện chất lượng tóm tắt.** Chúng tôi xem xét cải thiện chất lượng tóm tắt các bài đăng Reddit từ tập dữ liệu TL;DR (Stiennon et al., 2020), nơi chúng tôi so sánh best-of-K, CD-Q (theo khối) và IPO. Kết quả được báo cáo trong Hình 5, nơi chúng tôi đo tỷ lệ thắng được đo bởi PaLM 2-L (Unicorn) so với chính sách cơ sở. Chúng tôi quan sát thấy rằng CD-Q (theo khối) vượt trội so với IPO, nhưng cả hai đều không sánh với best-of-K.

**Thực nghiệm 4: Đồng thời cải thiện HH đối thoại & giữ nguyên độ dài phản hồi.** Tiếp theo, chúng tôi kết hợp các bộ chấm điểm tiền tố HH và độ dài cho kiểm soát đa mục tiêu. Vì vậy, chúng tôi chỉ xem xét CD-FUDGE theo khối, nơi việc giải mã thực hiện xếp hạng lại dựa trên HH một mình; hoặc một kết hợp tuyến tính của các phần thưởng HH và độ dài. Kết quả của thực nghiệm này được trình bày trong Hình 6. Chúng tôi thấy rằng áp dụng quy tắc giải mã HH một mình đưa ra một sự gia tăng độ dài tích cực so với baseline, phù hợp với các phát hiện trước đây (Eisenstein et al., 2023). Để giữ nguyên độ dài trong khi cải thiện HH, chúng tôi đưa ra một phần thưởng độ dài âm tại thời điểm giải mã. Không ngạc nhiên, điều này đến với chi phí của một sự suy giảm trong tỷ lệ thắng HH đối thoại. Lưu ý rằng thực nghiệm này sẽ không thể với các phương pháp RL có điều chỉnh KL thời gian huấn luyện (PPO/DPO/IPO) vì chúng cần được huấn luyện lại từ đầu cho các kết hợp tuyến tính khác nhau của phần thưởng. Điều này cho thấy tính linh hoạt và mô-đun của các phương pháp CD, có thể được huấn luyện cho nhiều mục tiêu cùng một lúc và các kết hợp tuyến tính khác nhau của mục tiêu có thể được đạt được mà không cần huấn luyện lại.

**Thực nghiệm 5: Cập nhật mô hình sinh cơ sở mà không huấn luyện lại bộ chấm điểm tiền tố.** Chúng tôi lặp lại Thực nghiệm 1 và 2 nhưng chúng tôi thay thế mô hình sinh cơ sở bằng một mô hình hoàn toàn khác, cụ thể là PaLM 2-S (Bison) trong Thực nghiệm 1 và PaLM 2-XS (Otter) trong Thực nghiệm 2, thay vì PaLM 2-XXS (Gecko) mà bộ chấm điểm tiền tố được huấn luyện bằng CD-Q. Điều này giúp hiểu mức độ gắn kết chặt chẽ giữa bộ chấm điểm tiền tố với các trọng số của mô hình sinh cơ sở và vì vậy tần suất cần huấn luyện lại bộ chấm điểm tiền tố trong môi trường sản xuất nơi mô hình sinh cơ sở có thể thay đổi thường xuyên. Kết quả của thực nghiệm này được báo cáo trong Hình 7 và Hình 8, tương ứng. Chúng tôi thấy rằng trong cả hai trường hợp CD-Q hoạt động ngang với baseline mạnh nhất, best-of-K, ngụ ý rằng bộ chấm điểm tiền tố được huấn luyện bằng CD-Q mạnh mẽ và tổng quát hóa tốt cho các LLM sinh cơ sở khác ngoài cái mà nó được huấn luyện. Lưu ý rằng PPO/DPO/IPO không thể được sử dụng mà không huấn luyện lại trong thực nghiệm này.

**Thực nghiệm 6: Tác động của việc điều chỉnh kích thước khối trong CD theo khối.** Chúng tôi lặp lại Thực nghiệm 2 trong khi chúng tôi thay đổi kích thước khối M để phân tích tác động của nó. Từ Hình 9 chúng tôi quan sát thấy rằng giảm kích thước khối M thường dẫn đến các đánh đổi tỷ lệ thắng so với phân kỳ KL tồi tệ hơn. Chúng tôi đã không phân tích kích thước khối lớn hơn 32 vì các lợi ích hiệu quả so với best-of-K sẽ biến mất.

**Thực nghiệm 7: Sử dụng CD-Q trên một mô hình cơ sở DPO.** Chúng tôi chuyển giao CD-Q sang một mô hình được tinh chỉnh bằng DPO mà không huấn luyện lại. Điều này được ký hiệu là "DPO + CD-Q (theo khối)" trong Hình 10. Lưu ý rằng CD-Q không được tiếp xúc với DPO được tinh chỉnh trong quá trình huấn luyện bộ chấm điểm tiền tố của nó. Chúng tôi chọn K trong CD-Q sao cho phân kỳ KL của nó sẽ gần khớp với baseline DPO, ví dụ, cho điểm màu xanh lá cây được chú thích với K = 8, tổng phân kỳ KL là khoảng 5, trong đó 2.5 là phân kỳ KL của checkpoint DPO và mô hình cơ sở, và 2.5 là từ CD-Q theo khối với K = 8. Chúng tôi điều chỉnh K trong CD-Q theo khối để đạt được điều này. Từ biểu đồ chúng tôi thấy rằng biến thể này kết hợp cả hai phương pháp cho đường cong đánh đổi tốt nhất tổng thể và thắng hẹp so với CD-Q theo khối trong các chế độ KL lớn hơn. Tuy nhiên, nó hiệu quả hơn vì có thể đạt được cùng / tốt hơn tỷ lệ thắng và KL như CD-Q theo khối vanilla nhưng với K nhỏ hơn, ví dụ, so sánh K=8 cho "DPO + CD-Q (theo khối)" và K=32 cho "CD-Q (theo khối)" tạo ra một đánh đổi tương tự, cho thấy rằng biến thể kết hợp yêu cầu K nhỏ hơn.

**Thực nghiệm 8: Sử dụng ngân sách thông lượng suy luận cố định.** Tiếp theo, chúng tôi xem lại Thực nghiệm 1 để so sánh CD-Q (theo khối) và DPO với best-of-K khi được cung cấp một ngân sách thông lượng suy luận cố định. Trong cả hai thực nghiệm, DPO yêu cầu một đường dẫn giải mã để tạo ra một phản hồi duy nhất trong khi CD-Q (theo khối) tạo ra một phản hồi duy nhất trong khi vốn có giải mã K phản hồi song song, như được mô tả trong Phương trình 7. Ở đây, trong Hình 11, chúng tôi cố định ngân sách thông lượng suy luận bằng cách đặt K = [4, 8, 16] cho CD-Q theo khối và sử dụng best-of-K trên DPO với cùng các giá trị K, để cả hai có cùng ngân sách thông lượng suy luận. Trong trường hợp này, các đánh đổi CD-Q được thu được bằng cách thay đổi M cho một K cố định. Chúng tôi thấy rằng đối với tất cả các giá trị K, CD-Q (theo khối) vượt trội so với DPO với lấy mẫu best-of-K, và khoảng cách hiệu suất giữa hai phương pháp tăng cho các giá trị K lớn hơn, cho thấy rằng CD-Q theo khối nghiêm ngặt tốt hơn DPO, ngay cả với ngân sách thông lượng cố định. Chúng tôi cũng xem lại Thực nghiệm 7 nơi chúng tôi so sánh "DPO + CD-Q (theo khối)" và "DPO + Best-of-K" tại K = 4 cố định. Kết quả của thực nghiệm này được trình bày trong Hình 12, nơi chúng tôi quan sát thấy rằng trong thiết lập này, "DPO + CD-Q (theo khối)" ngang với "DPO + Best-of-K".

## 6. Công trình liên quan

**Giải mã/sinh có kiểm soát.** FUDGE (Yang & Klein, 2021) nhận thấy rằng giải mã tuân theo một ràng buộc có thể được đạt được bởi một bộ chấm điểm tiền tố được đưa ra bởi quy tắc Bayes, và bổ sung dữ liệu phân biệt để huấn luyện bộ chấm điểm một phần. DIRECTOR (Arora et al., 2022) tiếp tục chỉ ra rằng bộ chấm điểm một phần có thể được học chung với chính mô hình ngôn ngữ, điều này sẽ dẫn đến giảm độ trễ tại thời điểm suy luận. GeDi (Krause et al., 2021) đề xuất huấn luyện các mạng chấm điểm tích cực và tiêu cực riêng biệt có thể được kết hợp để có được một điểm tiền tố. Kim et al. (2023) chỉ ra rằng critic trong một khung RL actor-critic có thể được sử dụng cho giải mã có kiểm soát. NADO (Meng et al., 2022) xem xét kiểm soát tuân theo một ràng buộc phân kỳ khác cho phép một giải pháp dạng đóng. AWR (Peng et al., 2019) mở rộng giải mã có kiểm soát sang một thiết lập tối đa hóa kỳ vọng nơi chính sách có thể được cập nhật tiếp theo dựa trên hàm giá trị. Trái ngược với dòng công việc này, chúng tôi chỉ ra rằng bộ chấm điểm tiền tố có thể được huấn luyện như hàm giá trị cho chính sách giải mã mô hình ngôn ngữ, cho phép chúng tôi thiết lập một kết nối chính xác giữa giải mã có kiểm soát và học tăng cường có điều chỉnh KL.

**Tìm kiếm cây.** Công việc của chúng tôi cũng có liên quan về mặt khái niệm với các thuật toán tìm kiếm cây, mặc dù trong trường hợp của chúng tôi độ sâu của tìm kiếm được cố định là một. Chaffin et al. (2022); Scialom et al. (2021) chứng minh rằng các phương pháp tìm kiếm cây Monte Carlo (MCTS) có thể được áp dụng cho giải mã mô hình ngôn ngữ để hướng dẫn việc sinh ra. Lu et al. (2022) sử dụng tìm kiếm cây với một heuristic để xác định chất lượng của một đường dẫn giải mã cho trước để điều hướng giải mã hướng tới các kết quả thuận lợi. Qin et al. (2022) khám phá lấy mẫu dựa trên gradient sử dụng động lực học Langevin vượt trội đáng kể so với lấy mẫu không gradient. Trái ngược với tất cả các công việc này, độ sâu tìm kiếm trong công việc của chúng tôi được đặt là một, do chi phí suy luận liên quan đến suy luận từ các LM lớn, điều này cấm tìm kiếm sâu hơn.

**Học tăng cường (RL).** Một dòng công việc rất liên quan khác là học tăng cường tuân theo một hình phạt KL với mô hình ngôn ngữ (Ouyang et al., 2022). Korbak et al. (2022) quan sát thấy rằng học tăng cường với một hình phạt KL có thể được xem theo cách Bayesian với một hàm phần thưởng tương ứng. Tuy nhiên, công việc của họ không đủ để tạo ra kết nối đầy đủ trong một thiết lập giải mã tự hồi quy, đây là đóng góp của chúng tôi trong công việc này thông qua CD. Một công việc liên quan chặt chẽ khác với chúng tôi là của Snell et al. (2023) thiết kế một thuật toán offline dựa trên giá trị, mặc dù với một mục tiêu học tập khác với chúng tôi (và của KL-regularized PPO). Li et al. (2017) cũng sử dụng một biến thể của Q-learning để tối ưu hóa điểm BLEU hoặc ROUGE. Công việc RL liên quan khác bao gồm các giải pháp cải thiện bộ sinh thông qua RL on-policy. Sparrow (Glaese et al., 2022) chỉ ra rằng một biến thể của proximal policy optimization (PPO) (Schulman et al., 2017) với một bộ điều chỉnh LM bổ sung hiệu quả tại nhiều mục tiêu an toàn và căn chỉnh với sở thích con người (Ouyang et al., 2022). Cuối cùng, khả năng cấu hình của phần thưởng có liên quan về mặt khái niệm với (Ramé et al., 2024), nơi nó được chỉ ra rằng soup phần thưởng có thể được sử dụng với hiệu ứng tương tự.

**Học có giám sát từ các ví dụ tiêu cực.** Một dòng công việc liên quan khác là các can thiệp cải thiện bộ sinh có giám sát. Chúng bao gồm huấn luyện unlikelihood (Welleck et al., 2020; Zhang & Song, 2022), mất mát contrastive (Adolphs et al., 2022), tối ưu hóa sở thích trực tiếp (Rafailov et al., 2023), và tối ưu hóa sở thích nhận dạng (Azar et al., 2023). Trái ngược với công việc của chúng tôi, các phương pháp này đều là can thiệp thời gian huấn luyện nhưng chúng tương tự có thể được sử dụng để cải thiện khả năng xảy ra của các ví dụ tích cực bằng cách ngăn chặn khả năng xảy ra của các ví dụ tiêu cực.

## 7. Nhận xét kết luận

Trong bài báo này, chúng tôi đã công thức hóa một mục tiêu học tăng cường có điều chỉnh KL để căn chỉnh các mô hình ngôn ngữ nhằm đạt được các kết quả phần thưởng cao hơn. Chúng tôi đã chỉ ra rằng vấn đề có thể được giải quyết bằng một giải pháp bổ sung tại thời điểm suy luận bằng cách học một bộ chấm điểm tiền tố giống như DQN. Chúng tôi cũng chỉ ra rằng khung kết quả, được gọi là giải mã có kiểm soát (CD), có thể được sử dụng để thực hiện kiểm soát trong các mô hình ngôn ngữ để điều hướng việc sinh ra theo cách từng token hoặc theo khối. Các thực nghiệm của chúng tôi xác nhận hiệu quả của đề xuất của chúng tôi trong việc cải thiện các phần thưởng khác nhau, bao gồm độ dài đối thoại, tính hữu ích và không có hại của đối thoại, và chất lượng tóm tắt, với một độ lệch nhỏ khỏi chính sách mô hình ngôn ngữ cơ sở. Chúng tôi cũng chỉ ra rằng khung có thể được mở rộng một cách dễ dàng để giải quyết một vấn đề học tăng cường đa mục tiêu miễn phí. Hơn nữa, chúng tôi cũng trình bày tính mạnh mẽ của đề xuất của chúng tôi bằng cách chuyển giao CD sang một mô hình cơ sở chưa thấy mà không cần huấn luyện lại.

Mặc dù CD theo từng token và RL có điều chỉnh KL đang tối ưu hóa cho front Pareto của phần thưởng kỳ vọng so với phân kỳ KL giữa chính sách căn chỉnh và chính sách cơ sở, chúng tôi quan sát thấy rằng CD theo khối và chính sách best-of-K liên tục đạt được một đường cong đánh đổi tốt hơn trong thực tế. Chúng tôi không phải là những người đầu tiên quan sát thấy điều này, và các thực nghiệm mở rộng của Gao et al. (2023); Eisenstein et al. (2023) cũng xác nhận thực tế này, được củng cố bởi các phát hiện lý thuyết gần đây của Yang et al. (2024). Do đó, CD theo khối có triển vọng cho việc căn chỉnh các mô hình ngôn ngữ.

Cuối cùng, việc phát triển giải mã có kiểm soát của chúng tôi được thúc đẩy bởi các đánh đổi giữa thông lượng, độ trễ và hiệu suất. Trong khi chúng tôi đã khám phá các đánh đổi này trong một tập hợp thực nghiệm hẹp, một sự hiểu biết toàn diện và nghiêm ngặt hơn về các đánh đổi như vậy được để lại cho công việc tương lai, có thể yêu cầu khám phá các phương pháp này cùng với giải mã suy đoán (Leviathan et al., 2023; Chen et al., 2023; Sun et al., 2023).

## Tuyên bố tác động

Chúng tôi đề xuất các phương pháp mới cho việc căn chỉnh mô hình ngôn ngữ, nơi kiểm soát được thực hiện tại thời điểm suy luận. Trái ngược với can thiệp thời gian huấn luyện thường được sử dụng để tối ưu hóa cho RL có điều chỉnh KL, các giải pháp thời gian suy luận cho phép kiểm soát tinh tế và linh hoạt hơn, có thể mở đường cho việc đạt được căn chỉnh có thể cấu hình và cá nhân hóa. Mặt khác, chúng tôi cũng quan sát thấy hành vi không nhất quán của các kỹ thuật căn chỉnh trong việc cải thiện an toàn và các vấn đề có hậu quả xã hội khác. Điều này chứng minh rằng việc áp dụng các kỹ thuật căn chỉnh trong các vấn đề tinh tế, như an toàn, cần được thực hiện với sự thận trọng cực kỳ.

## Lời cảm ơn

Chúng tôi biết ơn các đồng nghiệp về các cuộc thảo luận và phản hồi xây dựng trong suốt quá trình của dự án này: Alekh Agarwal, Ananth Balashankar, Jonathan Berant, Alexander D'Amour, Krishnamurthy Dvijotham, Jacob Eisenstein, Preethi Lahoti, Xiao Ma, Kathy Meier-Hellstern, Shayegan Omidshafiei, Yuting Sun, Ziteng Sun, Ananda Theertha Suresh, Victor Veitch, và Zhaofeng Wu. Chúng tôi cũng ghi nhận phản hồi hữu ích từ các nhà đánh giá ẩn danh của ICML 2024.
