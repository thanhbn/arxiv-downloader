# 2402.00742.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2402.00742.pdf
# File size: 6176106 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Transforming and Combining Rewards
for Aligning Large Language Models
Zihao Wang *1, Chirag Nagpal2, Jonathan Berant3, Jacob Eisenstein3, Alex
D’Amour3, Sanmi Koyejo4,3, and Victor Veitch1,3
1University of Chicago
2Google Research
3Google DeepMind
4Stanford University
Abstract
A common approach for aligning language models to human preferences is to first
learn a reward model from preference data, and then use this reward model to update
the language model. We study two closely related problems that arise in this approach.
First, any monotone transformation of the reward model preserves preference ranking;
is there a choice that is “better” than others? Second, we often wish to align language
models to multiple properties: how should we combine multiple reward models? Using
a probabilistic interpretation of the alignment procedure, we identify a natural choice for
transformation for (the common case of) rewards learned from Bradley-Terry preference
models. The derived transformation is straightforward: we apply a log-sigmoid function
to the centered rewards, a method we term “LSC-transformation” (log-sigmoid-centered
transformation). This transformation has two important properties. First, it emphasizes
improving poorly-performing outputs, rather than outputs that already score well. This
mitigates both underfitting (where some prompts are not improved) and reward hacking
(where the model learns to exploit misspecification of the reward model). Second, it
enables principled aggregation of rewards by linking summation to logical conjunction:
the sum of transformed rewards corresponds to the probability that the output is “good”
in all measured properties, in a sense we make precise. Experiments aligning language
models to be both helpful and harmless using RLHF show substantial improvements
over the baseline (non-transformed) approach.
1 Introduction
In this paper, we are interested in how to align large language models in order to bias their
outputs towards having desired properties—e.g., to be helpful, harmless, factual, or creative
[Zie+19; Sti +20; Ouy +22]. We study the two-stage approach where we first learn a reward
model from human preferences, and then align the language model so that its outputs have
high values under the reward model. In this context, we’re interested in two fundamental
problems:
1.The alignment step maximizes the expected learned reward model. However, any
monotone transformation of the reward preserves the interpretation of the alignment
procedure as biasing the language model towards human-preferred outputs. Can we
improve the alignment step by transforming the learned reward?
*Work done during internship at Google DeepMind
Published in ICML 2024.
1arXiv:2402.00742v2  [cs.CL]  19 Jul 2024

--- PAGE 2 ---
2.We often wish to align language models to multiple properties—e.g., outputs should
be helpful, and harmless, and factual. If we have reward models for each property,
how should we combine them?
A main challenge in answering these questions is that the goal of alignment is not precisely
defined. As a result, there is no obvious principle to guide the choice of transformation or
aggregation method. The conceptual idea in this paper is to interpret alignment probabilis-
tically. From this perspective, the goal of aligning a model to a particular property is to
produce samples from the posterior distribution conditional on the outputs being “good” on
that property. Similarly, the goal of aligning to multiple properties is to produce samples
conditional on the outputs being “good” on all properties.
To make use of this idea, we need to define what it means for an output to be “good”. In the
context of rewards learned from preference data, we take an output ybe “good” if it has
reward r(x,y)greater than some prompt-specific reference value rref(x). The first main
result of the paper is that in the (typical) case where the reward model is learned from pref-
erence data using a Bradley-Terry model and the language model is aligned by maximizing
expected reward subject to a KL constraint, the natural choice of transformation is:
u(x,y) =logσ(r(x,y)−rref(x)), (1.1)
whereσ(·)is the sigmoid function. Here, ris the learned Bradley-Terry reward model,
anduis the transformed reward we use in the alignment step. We call this transformation
the “LSC-transformation” (log-sigmoid-centered transformation), and alignment using such
transformation “LSC-alignment”.
This transformation is motivated by a probabilistic interpretation. It additionally turns out to
have important practical benefits relative to the baseline approach of using the raw reward
model. First, the transformed reward shrinks the marginal utility of very high reward values.
This has the effect in alignment of both encouraging the model to improve poorly performing
prompts, and of discouraging the model from “reward hacking” by optimizing the reward
model outside the range of its validity. Second, the transformed reward offers a natural
way to combine multiple reward models. Namely: the sum of the transformed rewards
corresponds to the logical AND of the outputs being “good” on each property. So, after trans-
forming the rewards, we can simply sum them to aggregate multiple reward models.
In combination, these benefits can lead to substantial improvements in alignment perfor-
mance. Figure 1 compares aligning a language model to be both helpful and harmless
using summation of transformed and untransformed rewards. Varying the strength of KL
regularization used in the alignment step, we observe that the transformed reward leads to
substantial improvements at all KL levels.
2 Preliminaries
We first review the standard Reinforcement Learning from Human Feedback (RLHF) two-step
procedure for aligning language models to human preferences.
Reward model training from pairwise data Reward models are trained to emulate
human feedback. A frequently used type of feedback is pairwise preference data, consisting
of a prompt xand two generated responses y+,y−, where y+is preferred by the human
annotator. Our discussion mainly focuses on this case.
Commonly, rewards are learned using the Bradley-Terry model [BT52 ],
p(y−≺y+|x) =σ(r(x,y+)−r(x,y−)). (2.1)
The function ris parameterized by a neural network (typically, another LLM) and fit using
the standard maximum log likelihood objective.
2

--- PAGE 3 ---
Figure 1: Transforming the Bradley-Terry reward both mitigates overfitting and makes addition behave
as logical AND. This leads to significant improvements in aligned model quality relative to standard
practice. Each point on the plot is a LLM aligned with a different KL penalty weight. The y-axis shows
improvement over the base supervise finetuned (SFT) LLM in both helpfulness AND harmlessness, as
judged by an external evaluator model (not used for RLHF). The baseline aggregates suboptimally
(usually losing on either helpfulness or harmlessness) and suffers reward hacking (performance decays
in the high KL regime). Details in section 5.
Alignment to reward model The next step is updating the base LLM to bias it towards
high-reward responses.
Usually, aligning the model to the reward function is proceeded by a “Supervised Finetuning”
(SFT) step where the base model is fine-tuned using the language modeling objective on
the winning examples from the human preference data. We denote this model as π0. Our
interest is how to use the reward model to further align π0.
The aim of the alignment step is to update π0to a new model π∗that has high expected
reward, while still being close to π0(to preserve information from the previous training
phases). Standard practice is to learn π∗by maximizing the expected reward of samples,
regularized by a penalty on the KL-divergence between π∗andπ0.
The main idea in this paper is to instead use a utility measure u(x,y)that is a monotone
transformation of r(x,y). We leave the alignment procedure otherwise unchanged. Then,
mathematically, π∗is the maximizer of:
Ex{Ey∼π(·|x)[u(x,y)]−γKL(π(·|x)∥π0(·|x))} (2.2)
Here,γis a hyper-parameter that controls the trade-off between maximizing rewards and
aligning withπ0.
3 Reward Transformation
We now turn to deriving the reward transformation.
Formalize Alignment Goal The first step is to formalize the goal of alignment. This is
necessary to identify a “correct” transformation. Intuitively we want to modify the initial
policyπ0(y|x)so that the generated samples are considered “good” in some property by
humans. To make progress, we introduce a binary semantic random variable Gindicating
whether response yis “good” for prompt x. Then we define the alignment goal as producing
3

--- PAGE 4 ---
(a)Transformation Shape
 (b)Helpfulness Examples
Figure 2: Bradley-Terry rewards do not capture diminishing utility, and log-sigmoid transforming can
fix this. In the example responses, moving from response 1 to response 2 substantially increases utility,
but from response 2 to response 3 only marginally increases. However, the BT rewards treat each
improvement the same. A log-sigmoid transformation reflects diminishing returns.
a model that samples from the distribution of responses conditional on the response being
good; i.e.,πtarget(·|x) =p(·|x,G=1).
In fact, we slightly generalize this to allow finer grained control of the reward vs KL tradeoff.
By Bayes’ rule, we may rewrite p(y|x,G=1)∝π0(y|x)p(G=1|x,y). That is, we
reweight the base LLM by a term that upweights responses that are likely to be deemed
good. It is natural to introduce a hyperparameter to control the strength of this upweighting.
Anticipating the connection to (2.2), we again use γfor this hyperparameter. Then, we
define our alignment goal as producing an aligned model
πtarget
γ(y|x)∝π0(y|x)p(G=1|x,y)1/γ(3.1)
Reward Transformation The next question is how to produce an aligned model that
satisfies our goal. This has two parts: we must use the reward model to define the binary
goodness variable G, and we must determine how the utility function used for alignment
relates to G.
Target Utility Function We begin by connecting alignment utility and G. The idea is to
use the well-known result that the ideal optimizer of the KL-regularized RLHF objective
(2.2) is an exponential tilting of the base policy [e.g., KPB22 ]:
π∗(y|x)∝π0(y|x)exp(u(x,y)/γ) (3.2)
Comparing (3.1) with (3.2), we see that in order to get the target policy through alignment,
we must set the utility function to be the log-probability of goodness:
u(x,y) =logp(G=1|x,y). (3.3)
Pointwise Reward Models The next step is to relate the Bradley-Terry model to p(G=
1|x,y). As a warmup, we consider the case of reward models trained on pointwise data;
i.e., where each example is a prompt x, response y, and a binary label Gindicating whether
the response is good. In this case, we would train the reward function by minimizing a
binary cross-entropy objective. This is a proper scoring rule, so the learned reward would
be:r(x,y) =logit p(G=1|x,y). Here, we take the reward to be on the logit scale so that
r(x,y)∈(−∞,∞), analogous to Bradley-Terry rewards. In this case, the right utility is
u(x,y) =logσ(r(x,y)).
4

--- PAGE 5 ---
Pairwise Reward Models The pairwise case is more subtle. It may be tempting to again
apply the log(σ(·))transformation to the Bradley-Terry rewards. This is incorrect for two
reasons. First, only reward differences are interpretable as logit probilities—the rewards for
individual prompts are not. Second, in general the reward model r(x,y)is unidentifiable
from the data. For any r, we can shift it by any arbitrary function of the prompt xwithout
changing the Bradley-Terry model. That is, ˜r(x,y)←r(x,y) +C(x)has the same objective
in (2.1). However, any non-linear transformation of the reward will be sensitive to this
unidentified C(x).
Happily, both problems are resolved by choosing a suitable definition of what it means for a
response to be good. Here, we take a generated response yto be “good” if it is preferred
over a chosen reference output yref. For example, we may say that yis harmless as long as
it is preferred by the harmlessness reward to a canned response such as “I am unable to
answer that question".
The LSC-transformation follows immediately:
Theorem 1. Suppose output yis deemed good for prompt xif it would be preferred
to reference output yref(x). Then, if the Bradley-Terry model (2.1) holds, and we align
using KL-regularized utility maximization (2.2), then using utility
u(x,y) =logσ(r(x,y)−rref(x)) (3.4)
will satisfy the alignment goal (3.1). Here rref(x):=r(x,yref(x)).
That is, once we decide on a reference response, we get the right utility function by applying
log-sigmoid transformation to the centered reward.
Mechanistic Interpretation We derived the reward transformation from a probabilistic
argument. It is also insightful to consider the mechanistic effect of the transformation.
One fundamental issue with the baseline approach, where u(x,y) =r(x,y), is that the
utility gain for improvements never diminishes. In the aligned model, taking γ=1, we
have that the relative probabilities of two responses is exponential in the difference of
utilities.
π∗(y1|x)
π∗(y0|x)=exp(u(y1,x)−u(y0,x))π0(y1|x)
π0(y0|x)(3.5)
Now, consider the case where we have three candidate responses: yref,y0,y1such that
p(yref≺y0) =0.99andp(yref≺y1) =0.999. If we use the raw Bradley-Terry logits as our
utility, then using that r(y1,x)−r(y0,x) = ( r(y1,x)−r(yref,x))−(r(y0,x)−r(yref,x))
we have:
π∗(y1|x)
π∗(y0|x)=exp(logit (0.999 )−logit (0.99 )))π0(y1|x)
π0(y0|x)
≈10×π0(y1|x)
π0(y0|x)
That is, when aligning to raw rewards, going from a very good response to a marginally
better response increases probability by a factor of 10! However, if yrefis already good, a
human would find little difference between the two responses. Conversely, aligning to the
transformed reward model only increases the probability by a factor of 1.01.
This effect seems particularly salient when we consider that the reward model is itself
learned. It seems unlikely that the model can actually reliably distinguish between y1and
Ultimately, the issue here is that Bradley-Terry rewards don’t automatically correspond to utilities.
5

--- PAGE 6 ---
y0. Accordingly, when we align to the raw learned model, we expect to induce enormous
shifts according to preferences that are anyways noisy.
Choosing reference response The reference reward acts as a hyperparameter of the
transformation. Essentially, the transformation results in a utility that is linear below the
reference value, and rapidly diminishing above it. Accordingly, we should set the reference
to a value that represents a good response, that we believe is achievable by the model, and
where we believe our learned reward function makes meaningful predictions. We found
that a good default choice is the 85th quantile of examples from the base distribution. We
consider additional examples in section 5.
4 Reward Aggregation
We now consider the case when we have multiple reward models for different objectives
r1, ...,rn.
Alignment Goal Again, the first step in deriving an optimal aggregation scheme is to
formalize a goal. We make the following natural choice: the aligned model should be “good”
on all target properties. E.g., we want to align our model to be helpful AND harmless.
To formalize this idea, let Gibe binary random variable indicating whether yis considered
“good” for xin property i. We introduce the binary random variable corresponding to logical
AND: GAND:=Vn
i=1Gi. Similar to the single reward case, we formalize the goal as the
posterior distribution conditioned on all properties being “good”:
πtarget
AND,γ∝π0(y|x)p(GAND=1|x,y)1/γ(4.1)
Reward Aggregation With this goal, following Theorem 1, we want to align using util-
ity
u(x,y) =logp(GAND=1|x,y). (4.2)
The question is then how to construct this utility function using the individual reward
models.
In full generality, this is an impossible task. The reason is that the individual rewards only
tell us about the marginal distributions of the properties, but the logical AND may depend
on interactions. Thus, we need an extra assumption:
Assumption 1 (Independent Judgements) .Given a fixed prompt xand response y, whether
yis judged to be good for xon each property is independent of all the judgements on all
other properties. That is, (G1, ...,Gn)are conditionally independent given (X,Y).
For example, this assumption says we can decide whether a given response is helpful
independently of deciding whether it’s harmful. (Note: this is conditional on the prompt and
response. We do notrequire helpfulness and harmless to be independent marginally.)
The reward aggregation formula follows immediately:
6

--- PAGE 7 ---
Theorem 2. Suppose output yis deemed good for prompt xin aspect iif it would be
preferred to reference output yref
i(x)in property i. Then, if Assumption 1 holds, the
Bradley-Terry model (2.1) holds for all properties, and we align using KL-regularized
utility maximization (2.2), then using utility
u(x,y) =nX
i=1logσ(ri(x,y)−rref
i(x)) (4.3)
will satisfy the alignment goal (4.1). Here rref
i(x):=r(x,yref
i(x)).
Mechanistic Interpretation We derived the aggregation scheme according to a probabilis-
tic assumption. Similar to the single-reward case, we consider the mechanistic effect.
The baseline approach is to aggregate with a (weighted) sum of the raw rewards. The key
problem is that this approach allows strong performance in one property to balance out
poor performance in another.
Consider the case where we have two properties AandBwe want to align to (e.g., helpfulness
and harmlessness), and 4 responses yref
A,yref
B,y0,y1such that p(yref
A≺y1) =p(yref
B≺y1) =
0.9, and p(yref
A≺y0) =0.45with p(yref
B≺y0) =0.99. If we want the aligned distribution
to generate samples that are “good” in both aspects, then y1should be preferred to y0.
However, if we use the sum of the raw Bradley-Terry logits as utility ( u=rA+rB), the
relative probability ratio under the aligned policy π∗will be (withγ=1)
π∗(y1|x)
π∗(y0|x)=1×π0(y1|x)
π0(y0|x). (4.4)
That is, the aligned model does not upweight response y1. If we instead align by the sum of
the transformed reward, then we have that the relative probability ratio is approximately
1.8—i.e., the response that does well on both properties is preferred.
5 Experiments
We now turn to assessing the practical effect of using the transformed reward to align LLMs.
We experiment with aligning models to be helpful, harmless, and both. We find that the
transformation alleviates reward hacking and reward underfitting, and that aligning to
transformed sum acts as aligning to logical AND. This leads to substantial improvements in
LLMs aligned to the transformed reward.
5.1 Experimental Setup
We follow a standard RLHF pipeline; see appendix A.4 for full details.
Datasets We use the Anthropic Helpfulness and Harmlessness datasets [Bai+22]. These
are multi-turn dialogues between a human and a digital assistant. Each dataset consists
of the beginning of the conversation, two responses for the final turn of the AI side of the
conversation, and a label for the human preference on the target property. We use the base
datasets (44K examples for helpfulness and 42K for harmlessness), where responses are
generated from a 52B context-distilled LM. For both tasks, we split the training set into two:
half for training the reward model, and half for the alignment step.
7

--- PAGE 8 ---
(a)Helpful: RLHF
(b)Harmless: RLHF
Figure 3: Transformed reward obtains better KL and win-rate trade-offs in single-reward. We show
two win rates: 1) win rates judged from prompted PALM 2 evaluator, between the aligned policy and
random SFT samples, and 2) win rates judged by T5-XXL evaluator, against the SFT quantiles: 85% for
helpfulness and 95% for harmlessness.
Reward model training We train a Bradley-Terry reward model for each of helpfulness
and harmlessness by finetuning a pretrained T5-base (220M parameters) model [Raf+20]
on the Anthropic data.
SFT For our policy model, we use the instruction-finetuned PALM-2-XXS model [Ani+23].
Following standard practice, we first run supervised finetuning (SFT) of the instruction
tuned LLM on the ‘preferred’ responses from the helpfulness dataset. We use this SFT model
as the pre-alignment base for all experiments.
RLHF setup For alignment, we follow standard practice and optimize expected utility
subject to a KL penalty using Proximal Policy Optimization (PPO) algorithm. For each utility
function and dataset, we sweep over multiple values of the KL regularization strength γ.
We run for 20000 steps, which we find suffices for convergence in all cases.
5.2 LSC-Transformation Improves Alignment
Transforming the reward model should encourage the alignment to focus on improving lower-
reward responses over those with already high rewards. We expect this to both reduce reward
hacking, and to reduce the number of low-reward responses (less underfitting).
Choice of reference reward The reference reward rref(x)should capture the notion of a
response that’s “good enough”. For harmlessness, this is straightforward: a generic response
8

--- PAGE 9 ---
like “I can’t answer that” achieves the goal. For the experiments, we sampled variations of
“I can’t answer that” as the reference reward.
For helpfulness, such canned responses won’t suffice. Instead, for each prompt we sample
64 responses from the SFT model. We then use this data to build an estimator of the 85th
quantile of the sampled rewards for each prompt. We use this estimated 85th quantile as
the reference reward. Details provided in appendix A.1.
Transformation Improves Alignment Aligning to the transformed reward should reduce
reward hacking and underfitting relative to aligning to the raw reward. Then, we expect
that the transformed alignment leads to larger gains over the SFT model.
We have two main strategies for judging improvement relative to the SFT model. First,
following past work [GSH23; Cos +23; Eis +23], we train a T5-XXL model using the same
preference dataset and the Bradley-Terry objective. This provides a proxy for true preferences
that we do not optimize against (so, it can witness reward hacking). We say a sample from
the aligned model wins in helpfulness if it beats the 85th reward quantile of samples from
the SFT model. We say it wins in harmlessness if it beats the 95th-quantile. (These numbers
are chosen to make winning hard enough to show a gap between alignment strategies;
results are consistent across other choices of quantile.).
The second strategy evaluates wins by zero-shot querying of an instruction-tuned PALM-2
medium model. Following previous work [Dub+23; Sin +23; Eis +23; Raf +23], we pass a
prompt, a response from the SFT model, and a response from the aligned model and ask
which is preferred (in terms of helpfulness or harmlessness). Details in appendix A.2.
Figure 3 shows the win rate of the aligned models over the SFT model for each evaluation
strategy. We average over the prompts in the RLHF validation dataset. We see that aligning
to the transformed reward dominates aligning to the raw reward, under both evaluation
strategies and at all levels of KL distance to the base policy model.
See appendix A.2 for additional experiments evaluating alignment-induced improvements.
Uniform Improvement of Rewards It is clear that aligning using the transformed reward
improves over aligning using the raw reward. Intuitively, this is because of a reduction
in both reward hacking and underfitting. To check whether this intuition holds, we plot
the distribution of rewards of samples from (approximately) KL-matched raw-aligned and
transformed-aligned models in Figure 4. As expected, the reward distribution of samples
from the transformed-aligned model is more concentrated. That is, there are fewer very
high reward samples (less reward hacking) and fewer very low reward samples (less
underfitting).
In more detail: for each of helpfulness and harmlessness, we choose a raw-aligned and
transformed-aligned model with approximately matched KL. We sample responses from
each model with the same prompts. Then, we compute the (T5-base) reward for each of
these responses, centered by median reward of SFT samples (to make rewards comparable
across prompts). Figure 4 shows histograms of these sampled rewards. We also compare
across multiple KL-values in Figure 11.
Transformation reduces shortcuts One symptom of reward hacking is that the aligned
model will start exploiting “shortcuts” that are preferred by the reward model (but which do
not correspond to genuine improvements). We consider the effect of reward transformation
on two such shortcuts. For helpfulness, Eisenstein et al. [Eis+23]observe that raw-aligned
models have a tendency to format outputs as lists. For harmlessness, we observe that
raw-aligned models will often give responses of the form “you should consult a [doctor /psy-
chologist /lawyer /etc]” (a similar observation is made by Bai et al. [Bai+22]). In Figure 5,
9

--- PAGE 10 ---
(a)Helpfulness (KL≈18)
 (b)Harmlessness (KL ≈7)
Figure 4: Reward Transformation leads to more uniform reward improvements than baseline. We
compare reward distributions in the aligned policies that are matched on KL. Rewards are centered by
the SFT median in both helpfulness and harmlessness. Reward distributions are more concentrated
when using transformed rewards than using raw rewards.
(a)Helpfulness
 (b)Harmlessness
Figure 5: Transformed reward reduces shortcuts in generated responses. For helpfulness, we identify
a shortcut pattern of using lists, similar to [Eis+23]. In harmlessness, one known shortcut pattern
is recommending the users to seek therapy or consult professional help [Bai+22]. We extract these
shortcuts with heuristic methods. In the baseline approach, the policy model exploits those shortcuts
for higher reward values. This is mitigated when we transform the reward.
we plot the fraction of responses that contain each shortcut, for each aligned model. We
see that the raw-reward aligned model does indeed exploit these shortcuts. Further, this
behavior is substantially mitigated by aligning to the transformed reward instead. See
appendix A.3 for details.
5.3 Reward Aggregation using LSC-Transformation Significantly Im-
proves Alignment
We now turn to the second goal: aggregating rewards for multiple distinct goals. To that
end, we consider aligning a LLM to be both helpful and harmless.
5.3.1 Further Experimental Setup
RLHF setup We use reward models trained for the helpfulness and harmlessness tasks
as discussed above. For RLHF training, we use prompts only from the helpfulness dataset.
This decision is because of the observation of the tension between helpfulness and harm-
lessness, which forced [Bai+22]to use a higher proportion of helpfulness prompts than
harmlessness ones. We use the same policy model as in experiments for single rewards
(SFT-ed on helpfulness data). The other training details are the same as in single-reward
experiments.
10

--- PAGE 11 ---
(a)Helpful and Harmless: Best-of- k
 (b)Helpful and Harmless: RLHF
Figure 6: Summation of transformed reward obtains better trade-off between KL /Kand win-rate.
In Figure 6a and Figure 6b we show win rates against SFT median rewards in both helpfulness and
harmlessness, judged by T5-XXL evaluator, for Best-of- kand RLHF respectively.
best-of- ksetup In addition to RLHF experiments, we also experiment with best-of- k
sampling, as in Gao et al. [GSH23 ]and Eisenstein et al. [Eis+23]. That is, we draw k
samples from the SFT model, rank them by the combined reward, and return the top ranked
sample. This can be viewed as another alignment procedure, where the best-of- ksampling
has some (KL) divergence from the underlying policy, and produces samples with higher
expected reward. In our experiments, we try kincreasing from 1 to 191.
There are two main motivations for considering best-of- kexperiments. First, it demonstrates
that the reward aggregation method applies to methods beyond RLHF . Second, best-of- k
doesn’t involve complicated optimizations. This allows us to disentangle the effect of having
the ‘right’ reward from the effect on solving the optimization problem.
Baseline In this setting, we take the baseline method to be a weighted sum of the raw
rewards; i.e.,
Rbaseline
∧:=wRhelp+ (1−w)Rharm. (5.1)
We sweep over w=0.1, ...,0.9, and report the baselines with best performance on our
evaluation metrics.
Reference Rewards We combine the transformed rewards by simple addition. Weighting
this addition doesn’t have a clear motivation or interpretation. Instead, the choice of
reference value used for each reward plays an analogous role. Intuitively, if we set a lower
reference value for reward A then the reward becomes more easily saturated, and the
optimization focuses more on reward B.
For best-of- k, using w=0.5achieves the best performance in the baseline method. Ac-
cordingly, we want to take the reference rewards for helpfulness and harmlessness to be
comparable. To that end, we use the (estimated) SFT median for both helpfulness and
harmlessness. Note that this is a lower reference than used for the individual optimizations.
This makes sense because the more difficult problem of optimizing both goals simultaneously
stops the model from saturating the reward prematurely.
For RLHF , we observe that w=0.6gives the best result for weighted-sum approach. Then,
we want to set the reference reward for helpfulness to be somewhat higher. We use the
(estimated) SFT 75%-quantile.
These choices are likely not optimal, and it’s possible that further hyperparameter optimiza-
tion could improve results.
11

--- PAGE 12 ---
(a)best-of- k(K=191)
(b)RLHF (KL≈14)
Figure 7: Summation of log-sigmoid transformed rewards corresponds better to logical AND. Aligned
policies using the former method have more balanced reward distributions (concentrated where the two
reward values are similar), whereas the latter method leads to more unbalanced reward distributions.
We choose aligned policies by matching Kfor best-of- kandKLfor RLHF . The rewards are centered
by SFT median for both helpfulness and harmlessness. We visualize the joint distribution by kernel
density estimate plot, with darker color indicating higher density.
Evaluation We want a metric that reflects Logical-AND. That is, whether we’ve improved
over the SFT model in both helpfulness and harmlessness. To that end, we’ll say that a
generation wins if it has helpfulness reward higher than the median helpfulness of SFT
samples, and harmlessness higher than the median harmlessness of SFT samples.
5.3.2 Aggregation Results
Transformed Aggregation Improves Alignment Summing the transformed reward should
have two advantages over the baseline method. First, it corresponds to logical AND. Second,
it retains the benefits of alleviating reward overoptimization, as in the single-reward case.
Together, these should cause aligning by the transformed-combined reward to outperform
aligning by the baseline reward.
Figure 6a and Figure 6b shows improvement over the SFT model for aligning using both
rewards, for both best-of- kand RLHF . As anticipated, we see significant improvement from
12

--- PAGE 13 ---
the transformed method.
It is noteworthy that the transformed-aligned model outperforms in best-of- k, and for low-KL
in the RLHF alignment. In these cases, there is little reward hacking (the aligned model is
too close to the SFT model to have very bad behavior). Thus, the win here is apparently
due mainly to the logical AND effect. In the high-KL regime, the reward hacking effect kicks
in and the transformed-reward dramatically outperforms the raw-reward baseline.
Transformed Summation Corresponds to Logical AND Next, we check directly whether
the logical AND effect can be witnessed in the aligned LLM. To that end, we examine the
distributions of rewards in the aligned policies. In Figure 7, our aggregation method leads
to more balanced reward distributions (two reward values are often similar), whereas the
baseline method leads to more unbalanced reward distributions (one reward is much higher
than the other one). Note that best-of- k(k=191) have quite different reward distributions
than RLHF at KL≈14(the former reflects the anti-correlation of the two rewards in the
initial policy; the latter updates the policy to have high aggregated reward values). But the
transformed aggregation method has consistent effects in both cases.
6 Ablation Studies
The LSC-transformation consists of two components: centering and log-sigmoid trans-
formation. Centering is known to reduce variance in policy gradient updates, while the
log-sigmoid transformation caps the reward values. To understand the contribution of each
component, we investigate their effects separately. As shown in Figure 8, applying centering
or log-sigmoid transformation in isolation does not improve alignment.
Figure 8: Log-sigmoid transformation or centering alone does not improve alignment. The experiment
is for the helpfulness task.
Does centering alone help? This approach did not change the RLHF performance relative
to the baseline. Though centering is known to reduce variance for policy gradient update, it
doesn’t change the optimization target of the KL-regularized RLHF objective, as shown in
(3.2).
Does log-sigmoid transformation alone help? This essentially sets a constant reference
value ( rref(x) =0) for all inputs x. This is found to hurt the performance relative to baseline.
The reference value is important, as can be seen in the shape of utility function in Fig
2(a): when the reference is too small, it saturates the utility when the reward is still under-
fitted; when the reference is too large, the nonlinear transformation has no effect. More
13

--- PAGE 14 ---
specifically, there are two possible reasons why simply using log-sigmoid transformation
does not work:
1.First, r(x,y)is non-identifiable: any r(x,y) +c(x)yields the same Bradley-Terry
objective. Then, any fixed constant reference reward (independent of prompt) is
fundamentally not meaningful.
2.Second, if the reference reward is set too high then there will be no effect (for r≪rref,
the transformation is effectively a constant offset), and if the reference reward is set
too low then training will saturate too early. The issue with a constant reference level
such as 0 is that it’s unclear what this means in practical terms—is it too high or too
low or fine? By contrast, choosing reference rewards based on generated responses
(as in the paper) makes it easy to set a meaningful threshold.
7 Discussion and Related Work
There is a growing body of work on mitigating reward hacking in the RLHF pipeline. Tech-
niques include forms of reward model averaging [Eis+23; Ram +24; Zha +23], constrained
optimization [Mos+23], and reward model regularization [She+23], iterative human pref-
erence collection [Bai+22; Sti +20; Fan +22], or data bias mitigation [Sin+23]. These
approaches are complementary to the transformation technique proposed here, and could
be used in combination.
There have been several proposals for aligning language models to multiple objectives.
The most common approach is to combine individual reward models via a weighted sum
[e.g., Wu +23; Mos +23]. Moskovitz et al. [Mos+23]identified a constraint threshold for
individual rewards, and formalized a constrained MDP problem, but the identification of the
threshold point relies on ground-truth queries. Bakker et al. [Bak+22]consider adapting
social welfare schemes for aggregating the dissenting preferences of many individuals, in
contrast with our goal of satisfying all properties. Bai et al. [Bai+22]train a single reward
model on both the helpfulness and harmlessness data, but discover that this leads to reward
hacking harmlessness. They change the proportion of helpfulness data in the training to
circumvent this. Combining by summing transformed rewards allows us to circumvent such
considerations.
The transformation technique in this paper is relevant to any alignment strategy that
explicitly maximizes an expected utility. There are now also alignment methods [e.g.,
Raf+23; Aza +23; Zha +22]that use preference labels directly without explicitly instantiating
reward models. Note, however, that if we want to align to multiple properties, we still
need to compute rankings from an aggregate. The simplest approach is to train reward
models for individual properties, combine these reward models, and then rank samples
using the combine reward. Our best-of-k experiments show that the transformation can
yield significant gains even in this case.
Finally, we note that Azar et al. [Aza+23]also emphasizes the need for a bounded utility
function. The work here can be viewed, in part, as a way of incorporating this insight that
still maintains the standard utility maximization pipeline. It is an interesting question for
future work whether making explicit use of a (transformed) reward improves the alignment
step relative to using only ranked pairs.
14

--- PAGE 15 ---
Acknowledgements
This work is supported by ONR grant N00014-23-1-2591 and Open Philanthropy.
Impact Statement
This paper presents work whose goal is to advance the field of Machine Learning. There are
many potential societal consequences of our work, none which we feel must be specifically
highlighted here.
References
[Ani+23]R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri,
E. Taropa, P . Bailey, Z. Chen, et al. “Palm 2 technical report”. arXiv preprint
arXiv:2305.10403 (2023) (cit. on p. 8).
[Aza+23]M. G. Azar, M. Rowland, B. Piot, D. Guo, D. Calandriello, M. Valko, and R.
Munos. “A general theoretical paradigm to understand learning from human
preferences”. arXiv preprint arXiv:2310.12036 (2023) (cit. on p. 14).
[Bai+22]Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S.
Fort, D. Ganguli, T . Henighan, et al. “Training a helpful and harmless as-
sistant with reinforcement learning from human feedback”. arXiv preprint
arXiv:2204.05862 (2022) (cit. on pp. 7, 9, 10, 14).
[Bak+22]M. A. Bakker, M. J. Chadwick, H. R. Sheahan, M. H. Tessler, L. Campbell-
Gillingham, J. Balaguer, N. McAleese, A. Glaese, J. Aslanides, M. M. Botvinick,
and C. Summerfield. Fine-tuning language models to find agreement among
humans with diverse preferences . 2022. arXiv: 2211.15006 [cs.LG] (cit. on
p. 14).
[BT52 ] R. A. Bradley and M. E. Terry. “Rank analysis of incomplete block designs: i.
the method of paired comparisons”. Biometrika 3/4 (1952) (cit. on p. 2).
[Cos+23]T . Coste, U. Anwar, R. Kirk, and D. Krueger. “Reward model ensembles help
mitigate overoptimization”. arXiv preprint arXiv:2310.02743 (2023) (cit. on
p. 9).
[Dub+23]Y. Dubois, X. Li, R. Taori, T . Zhang, I. Gulrajani, J. Ba, C. Guestrin, P . Liang, and
T . B. Hashimoto. “Alpacafarm: a simulation framework for methods that learn
from human feedback”. arXiv preprint arXiv:2305.14387 (2023) (cit. on p. 9).
[Eis+23] J. Eisenstein, C. Nagpal, A. Agarwal, A. Beirami, A. D’Amour, D. Dvijotham,
A. Fisch, K. Heller, S. Pfohl, D. Ramachandran, et al. “Helping or herding?
reward model ensembles mitigate but do not eliminate reward hacking”. arXiv
preprint arXiv:2312.09244 (2023) (cit. on pp. 9–11, 14, 17).
[Fan+22]X. Fan, Y. Lyu, P . P . Liang, R. Salakhutdinov, and L. -P . Morency. “Nano: nested
human-in-the-loop reward learning for few-shot language model control”.
arXiv preprint arXiv:2211.05750 (2022) (cit. on p. 14).
[GSH23 ] L. Gao, J. Schulman, and J. Hilton. “Scaling laws for reward model overop-
timization”. In: International Conference on Machine Learning . PMLR. 2023
(cit. on pp. 9, 11).
[Hou+23]Y. Hou, J. Zhang, Z. Lin, H. Lu, R. Xie, J. McAuley, and W . X. Zhao. “Large lan-
guage models are zero-shot rankers for recommender systems”. arXiv preprint
arXiv:2305.08845 (2023) (cit. on p. 17).
[KPB22 ] T . Korbak, E. Perez, and C. L. Buckley. “Rl with kl penalties is better viewed as
bayesian inference”. arXiv preprint arXiv:2205.11275 (2022) (cit. on p. 4).
[Mos+23]T . Moskovitz, A. K. Singh, D. Strouse, T . Sandholm, R. Salakhutdinov, A. D.
Dragan, and S. McAleer. Confronting reward model overoptimization with con-
strained rlhf . 2023. arXiv: 2310.04373 [cs.LG] (cit. on p. 14).
15

--- PAGE 16 ---
[Ouy+22]L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P . Mishkin, C. Zhang,
S. Agarwal, K. Slama, A. Ray, et al. “Training language models to follow
instructions with human feedback”. Advances in Neural Information Processing
Systems (2022) (cit. on p. 1).
[Raf+23]R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn.
“Direct preference optimization: your language model is secretly a reward
model”. arXiv preprint arXiv:2305.18290 (2023) (cit. on pp. 9, 14).
[Raf+20]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W . Li,
and P . J. Liu. “Exploring the limits of transfer learning with a unified text-to-text
transformer”. The Journal of Machine Learning Research 1 (2020) (cit. on p. 8).
[Ram+24]A. Ramé, N. Vieillard, L. Hussenot, R. Dadashi, G. Cideron, O. Bachem, and
J. Ferret. “Warm: on the benefits of weight averaged reward models”. arXiv
preprint arXiv:2401.12187 (2024) (cit. on p. 14).
[She+23]L. Shen, S. Chen, L. Song, L. Jin, B. Peng, H. Mi, D. Khashabi, and D. Yu.
“The trickle-down impact of reward (in-) consistency on rlhf”. arXiv preprint
arXiv:2309.16155 (2023) (cit. on p. 14).
[Sin+23]P . Singhal, T . Goyal, J. Xu, and G. Durrett. “A long way to go: investigating
length correlations in rlhf”. arXiv preprint arXiv:2310.03716 (2023) (cit. on
pp. 9, 14).
[Sti+20] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D.
Amodei, and P . F . Christiano. “Learning to summarize with human feedback”.
Advances in Neural Information Processing Systems (2020) (cit. on pp. 1, 14).
[Wu+23]Z. Wu, Y. Hu, W . Shi, N. Dziri, A. Suhr, P . Ammanabrolu, N. A. Smith, M.
Ostendorf, and H. Hajishirzi. Fine-grained human feedback gives better rewards
for language model training . 2023. arXiv: 2306.01693 [cs.CL] (cit. on p. 14).
[Zha+23]Y. Zhai, H. Zhang, Y. Lei, Y. Yu, K. Xu, D. Feng, B. Ding, and H. Wang. “Uncertainty-
penalized reinforcement learning from human feedback with diverse reward
lora ensembles”. arXiv preprint arXiv:2401.00243 (2023) (cit. on p. 14).
[Zha+22]Y. Zhao, M. Khalman, R. Joshi, S. Narayan, M. Saleh, and P . J. Liu. “Calibrating
sequence likelihood improves conditional language generation”. arXiv preprint
arXiv:2210.00045 (2022) (cit. on p. 14).
[Zie+19] D. M. Ziegler, N. Stiennon, J. Wu, T . B. Brown, A. Radford, D. Amodei, P . Chris-
tiano, and G. Irving. “Fine-tuning language models from human preferences”.
arXiv preprint arXiv:1909.08593 (2019) (cit. on p. 1).
16

--- PAGE 17 ---
A Additional Experiment Details
A.1 Implementation for reference value prediction
The goal is to develop models that can predict various quantiles of the sampled rewards for
different prompts. Instead of building separate models for each quantile, we assume the
reward distribution for each prompt follows a Gaussian distribution. Under this assumption,
we build two models: Mean Prediction Model (rmean) and Standard Deviation Prediction
Model (rstd) to predict the mean and standard deviation of the reward distribution for each
prompt x.
To estimate a specific quantile for a prompt, the formula used is:
rmean(x) +s×rstd(x)
where sis a scaling factor corresponding to the desired quantile (e.g., using s=0.7
approximately gives us the 75% quantile).
The training data is collected by generating 64 responses per prompt using the SFT policy
model. These responses are scored to calculate the sample mean and standard deviation of
the reward distribution for each prompt, which are then used as the response variables in
training the models rmeanandrstd.
A.2 PALM-2 Evaluation Details
We evaluate the win-rates with zero-shot prompting. For each prompt, we sample (yπ,ysft)
from the aligned and SFT policy, then ask the PALM-2 model which is more helpful /harmless.
The prompt template is Figure 9.
In order to counter positional bias [Hou+23], we run PALM-2 on the two possible orderings
(yπ,ysft)and(ysft,yπ), sample N=8outputs for each order and determine the winner by
majority voting.
A.3 Heuristics for shortcut discovery
For the helpfulness task, we follow the same practice in [Eis+23]and find responses in the
format of a list.
For harmlessness task, we see the aligned policy starts to only generate similar generations
to different questions as KL gets larger. To find shortcuts, we analyze generations of the
aligned policy (using raw reward as utility) at KL≈14. Then we perform topic modeling
and identify one dominant topic characterized by the recurring theme of "you should talk
to[profession ]." From the most frequent 50 words in that topic, we find all the profession-
related words: therapist, professional, doctor, psychologist, counselor, lawyer, police. Then
we find all responses containing profession-related words.
A.4 RLHF training details
We use Proximal Policy Optimization (PPO) to perform RLHF alignment. The specific
hyper-parameters are in table 1 We sweep over γ’s to get aligned policies with different
KL values. Since we want to match (converged) polices by their KL values, we find some
heuristic to predict converged KL for a chosen γ(so they may not look like regular linear or
geometric series). More specifically, we use the parameterization of α=γ
1+γ, and the values
used are in table 2. Note that to get the same KL value, we use smaller KL regularization
for reward transformation, than using the raw reward. This is intuitive as the log-sigmoid
transformation prevents using KL budgets once the reward saturates.
17

--- PAGE 18 ---
Figure 9: Prompt for PALM-2 Evaluation
18

--- PAGE 19 ---
Table 1: Hyper-parameters for RLHF .
Parameter Value
Policy learning rate 5 ·10−6
Value learning rate 4 ·10−5
Learning schedule Constant (linear warm-up)
Training steps 20000
Warm-up steps 2000
Batch size 32
Input length 1024
Output length 256
Table 2: Alphas for Various Task-Methods.
Task-Method α:=γ
1+γ
Helpfulness (u =reward) [0.081, 0.086, 0.092, 0.1, 0.114, 0.132, 0.161, 0.222 ]
Helpfulness (u =transformed reward) [0.02, 0.023, 0.028, 0.032, 0.039, 0.053, 0.075, 0.123 ]
Harmlessness (u =reward) [0.169, 0.182, 0.196, 0.218, 0.248, 0.32 ]
Harmlessness (u =transformed reward) [0.032, 0.041, 0.052, 0.079, 0.126, 0.222 ]
H+H (0.5 rhelp+0.5rharmless) [0.078, 0.08, 0.084, 0.088, 0.094, 0.102, 0.116, 0.134, 0.168 ]
H+H (0.6 rhelp+0.4rharmless) [0.068, 0.071, 0.074, 0.077, 0.082, 0.088, 0.1, 0.123, 0.163 ]
H+H (0.7 rhelp+0.3rharmless) [0.057, 0.06, 0.065, 0.07, 0.075, 0.084, 0.1, 0.126, 0.173 ]
H+H (sum of transformed reward) [0.014, 0.0145, 0.015, 0.017, 0.018, 0.02, 0.023, 0.026, 0.031, 0.034, 0.04, 0.048, 0.066, 0.096 ]
B More Experiment Results
In Figure 4 we choose a pair of aligned policies matched on KL values to show that reward
transformation leads to more uniform improvement than using the raw reward. In Figure 10
we visually show reward overoptimization (in the same aligned policies), and how reward
transformation alleviates it. The mitigation is most obvious for responses with larger reward
values. It’s also important to note that reward aggregation exacerbates reward hacking with
weighed sum of the raw reward: the policy model is incentivized to generate responses with
very high harmlessness score (despite having smaller weight for harmlessness); to retain
good scores in helpfulness, the policy model hacks the helpfulness reward model even more
(compare Figure 10a against Figure 10c). Using sum of transformed reward alleviates this
by two mechanisms.
In Figure 11 we show reward transformation leads to more concentrated reward distribution
than using raw reward, the same trend across different KL values.
In Figure 3 we report win-rate against random sample under PALM-2, and against SFT
85%-quantile for helpfulness and 95%-quantile for harmlessness. In Figure 12, we report
the extra evaluations.
19

--- PAGE 20 ---
(a)Helpful (single reward): KL ≈18
 (b)Harmless (single reward): KL ≈7
(c)Helpful (aggregated reward): KL ≈14
 (d)Harmless (aggregated reward): KL ≈14
Figure 10: Reward transformation mitigates reward overoptimization, particularly for responses with
larger reward values, and in reward aggregation where reward hacking is more severe. We compare
reward overoptimization patterns in policies aligned with raw reward and transformed reward, in
single reward (Figure 10a, Figure 10b) and reward aggregation (Figure 10c, Figure 10d) settings. The
choice of aligned policies and score centering are the same as in Figure 4. For each plot, we sort the
centered scores from T5-base reward model into 20equal-width bins. For each bin with more than 10
data points: in the x-axis we show the interval medium; in the y-axis, we visualize the 25%, 50% and
75% quantile of the corresponding centered scores from T5-xxl.
(a)Helpfulness
 (b)Harmlessness
Figure 11: Reward Transformation leads to more uniform reward improvements than baseline. The
reward values are centered by median SFT rewards. This complements the results in Figure 4 for
comparisons across KL values.
20

--- PAGE 21 ---
(a)SFT Random Sample
 (b)SFT 50%-quantile
 (c)SFT 75%-quantile
 (d)SFT 95%-quantile
(e)Helpfulness
(f)SFT Random Sample
 (g)SFT 50%-quantile
 (h)SFT 75%-quantile
 (i)SFT 85%-quantile
(j)Harmlessness
Figure 12: Transformed reward obtains better KL and win-rate trade-offs. These are win-rates compared
against SFT random sample, and q-th quantile of the rewards of SFT samples. See details in Figure 3.
21
