# 2305.06474.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2305.06474.pdf
# Kích thước tệp: 2374106 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Liệu LLM có hiểu sở thích người dùng không? Đánh giá LLM về dự đoán xếp hạng người dùng
WANG-CHENG KANG*, Google Research, Brain Team, Hoa Kỳ
JIANMO NI*, Google Research, Brain Team, Hoa Kỳ
NIKHIL MEHTA, Google Research, Brain Team, Hoa Kỳ
MAHESWARAN SATHIAMOORTHY, Google Research, Brain Team, Hoa Kỳ
LICHAN HONG, Google Research, Brain Team, Hoa Kỳ
ED CHI, Google Research, Brain Team, Hoa Kỳ
DEREK ZHIYUAN CHENG, Google Research, Brain Team, Hoa Kỳ
Các Mô hình Ngôn ngữ Lớn (LLM) đã thể hiện khả năng đặc biệt trong việc tổng quát hóa sang các nhiệm vụ mới theo cách zero-shot hoặc few-shot.
Tuy nhiên, mức độ LLM có thể hiểu sở thích người dùng dựa trên hành vi trước đó của họ vẫn là một câu hỏi nghiên cứu mới nổi và chưa rõ ràng. Theo truyền thống, Lọc Cộng tác (CF) đã là phương pháp hiệu quả nhất cho các nhiệm vụ này,
chủ yếu dựa vào khối lượng lớn dữ liệu xếp hạng. Ngược lại, LLM thường yêu cầu ít dữ liệu hơn nhiều trong khi duy trì kiến thức toàn diện về thế giới về từng mục, chẳng hạn như phim hoặc sản phẩm. Trong bài báo này, chúng tôi tiến hành kiểm tra kỹ lưỡng cả CF và LLM trong nhiệm vụ cổ điển dự đoán xếp hạng người dùng, bao gồm việc dự đoán xếp hạng của người dùng cho một mục ứng viên dựa trên các xếp hạng trong quá khứ của họ. Chúng tôi điều tra các LLM khác nhau với nhiều kích thước khác nhau, từ 250M đến 540B tham số và
đánh giá hiệu suất của chúng trong các tình huống zero-shot, few-shot và fine-tuning. Chúng tôi tiến hành phân tích toàn diện để so sánh giữa
LLM và các phương pháp CF mạnh, và thấy rằng LLM zero-shot tụt hậu so với các mô hình gợi ý truyền thống có quyền truy cập vào dữ liệu tương tác người dùng, cho thấy tầm quan trọng của dữ liệu tương tác người dùng. Tuy nhiên, thông qua fine-tuning, LLM đạt được hiệu suất tương đương hoặc thậm chí tốt hơn chỉ với một phần nhỏ dữ liệu huấn luyện, thể hiện tiềm năng của chúng thông qua hiệu quả dữ liệu.
Định dạng tham chiếu ACM:
Wang-Cheng Kang*, Jianmo Ni*, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, và Derek Zhiyuan Cheng. 2023.
Liệu LLM có hiểu sở thích người dùng không? Đánh giá LLM về dự đoán xếp hạng người dùng. 1, 1 (Tháng 5 2023), 11 trang. https://doi.org/10.
1145/nnnnnnn.nnnnnnn
1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) đã cho thấy khả năng kỳ lạ trong việc xử lý nhiều nhiệm vụ đa dạng như tạo văn bản
[1,2,6,26], dịch thuật [36,42], và tóm tắt [19]. Việc fine-tuning gần đây của LLM trên các cuộc hội thoại và
việc sử dụng các kỹ thuật như instruction fine-tuning [4] và học tăng cường từ phản hồi con người (RLHF) [3] đã dẫn đến
*Hai tác giả đã đóng góp bằng nhau cho công trình này.
Địa chỉ tác giả: Wang-Cheng Kang*, wckang@google.com, Google Research, Brain Team, Hoa Kỳ; Jianmo Ni*, jianmon@google.com, Google Research,
Brain Team, Hoa Kỳ; Nikhil Mehta, nikhilmehta@google.com, Google Research, Brain Team, Hoa Kỳ; Maheswaran Sathiamoorthy, nlogn@google.com, Google
Research, Brain Team, Hoa Kỳ; Lichan Hong, lichan@google.com, Google Research, Brain Team, Hoa Kỳ; Ed Chi, edchi@google.com, Google Research, Brain
Team, Hoa Kỳ; Derek Zhiyuan Cheng, zcheng@google.com, Google Research, Brain Team, Hoa Kỳ.
Được phép tạo bản sao kỹ thuật số hoặc bản cứng của toàn bộ hoặc một phần công trình này để sử dụng cá nhân hoặc lớp học mà không mất phí với điều kiện các bản sao không
được tạo hoặc phân phối để thu lợi nhuận hoặc lợi thế thương mại và các bản sao phải ghi rõ thông báo này và trích dẫn đầy đủ trên trang đầu tiên. Bản quyền cho các thành phần
của công trình này thuộc sở hữu của người khác ngoài ACM phải được tôn trọng. Việc trích dẫn có ghi công nguồn được phép. Để sao chép khác, hoặc tái xuất bản, để đăng trên máy chủ hoặc
phân phối lại danh sách, yêu cầu sự cho phép cụ thể trước và/hoặc phí. Yêu cầu quyền từ permissions@acm.org.
©2023 Association for Computing Machinery.
Bản thảo gửi đến ACM
Bản thảo gửi đến ACM 1arXiv:2305.06474v1 [cs.IR] 10 Tháng 5 2023

--- TRANG 2 ---
2 W.-C. Kang, et al.
Hình 1. Hiệu suất zero-shot cho dự đoán xếp hạng của LLM với các kích thước mô hình khác nhau, bao gồm Flan-T5 (base đến XXL), GPT3 (Curie,
Davinci) và Flan-U-PaLM. Chúng ta thấy cải thiện hiệu suất khi tăng kích thước mô hình. Trong số đó, các mô hình lớn hơn 100B
(Flan-U-PaLM 540B và text-davinci-003 175B) vượt trội hoặc ngang bằng với baseline xếp hạng trung bình toàn cục trên cả RMSE và
AUC-ROC.
thành công to lớn trong việc mang các chatbot giống con người (ví dụ ChatGPT [23], và Bard [10]) đến hộ gia đình bình thường.
Có ba yếu tố chính trực tiếp đóng góp vào tính linh hoạt và hiệu quả của LLM:
(1)Kiến thức từ thông tin thế giới thực ở quy mô internet: LLM được huấn luyện trên các bộ dữ liệu văn bản khổng lồ, cung cấp
quyền truy cập vào kho thông tin thế giới thực phong phú. Thông tin này được chuyển đổi thành kiến thức có thể được sử dụng để
trả lời câu hỏi, viết sáng tạo (ví dụ: thơ và bài viết), và dịch giữa các ngôn ngữ.
(2)Khả năng tổng quát hóa đáng kinh ngạc thông qua học few-shot hiệu quả: trong bối cảnh nhất định, LLM có thể
học các nhiệm vụ mới từ một số lượng cực kỳ nhỏ các ví dụ (a.k.a., học few-shot). Khả năng học few-shot mạnh mẽ
trang bị cho LLM khả năng thích ứng cao với các nhiệm vụ mới.
(3)Khả năng lý luận mạnh mẽ: LLM có thể lý luận thông qua quá trình chuỗi suy nghĩ [40,41], cải thiện đáng kể
hiệu suất của chúng trên nhiều nhiệm vụ [37].
Gần đây, đã có một số công trình khám phá sơ bộ để sử dụng LLM cho Tìm kiếm [20], Học xếp hạng [11,45],
và Hệ thống Gợi ý [5,8,18]. Cụ thể cho hệ thống gợi ý, P5 [8] fine-tune T5-small (60M)
và T5-base(220M) [27], thống nhất cả xếp hạng, truy xuất và các nhiệm vụ khác như giải thích tóm tắt vào một mô hình.
M6-Rec [5] giải quyết nhiệm vụ dự đoán CTR bằng cách fine-tune một LLM gọi là M6 (300M) [17]. Liu et al. [18] tìm hiểu
liệu các agent hội thoại như ChatGPT có thể phục vụ như một mô hình gợi ý sẵn có với prompt làm
giao diện và báo cáo hiệu suất zero-shot trên dự đoán xếp hạng so với các baseline như MF và MLP. Tuy nhiên, có
sự thiếu vắng đáng chú ý của một nghiên cứu toàn diện đánh giá tỉ mỉ các LLM với kích thước khác nhau và đối chiếu chúng
với các baseline mạnh được tối ưu hóa cẩn thận.
Trong bài báo này, chúng tôi khám phá việc sử dụng các mô hình ngôn ngữ lớn (LLM) sẵn có cho hệ thống gợi ý. Chúng tôi
nghiên cứu nhiều LLM với kích thước khác nhau từ 250M đến 540B tham số. Chúng tôi tập trung vào nhiệm vụ cụ thể của
dự đoán xếp hạng người dùng, và đánh giá hiệu suất của các LLM này dưới ba chế độ khác nhau: 1. zero-shot 2. few-shot,
và 3. fine-tuning. Sau đó chúng tôi so sánh cẩn thận chúng với các mô hình gợi ý tiên tiến trên hai
bộ dữ liệu benchmark gợi ý được áp dụng rộng rãi. Đóng góp của chúng tôi gồm ba phần:
•Chúng tôi nghiên cứu thực nghiệm hiệu suất zero-shot và few-shot của các LLM sẵn có với một phổ rộng
kích thước mô hình. Chúng tôi thấy rằng các mô hình lớn hơn (trên 100B tham số) có thể cung cấp các gợi ý hợp lý dưới
tình huống cold-start, đạt hiệu suất tương đương với các baseline dựa trên heuristic tử tế.
Bản thảo gửi đến ACM

--- TRANG 3 ---
Liệu LLM có hiểu sở thích người dùng không? Đánh giá LLM về dự đoán xếp hạng người dùng 3
•Chúng tôi cho thấy LLM zero-shot vẫn tụt hậu so với các mô hình gợi ý truyền thống sử dụng dữ liệu tương tác con người.
LLM zero-shot chỉ đạt hiệu suất tương đương hai baseline tầm thường đáng ngạc nhiên luôn dự đoán
xếp hạng mục hoặc người dùng trung bình. Hơn nữa, chúng có hiệu suất kém đáng kể so với các mô hình gợi ý
có giám sát truyền thống, cho thấy tầm quan trọng của dữ liệu tương tác người dùng.
•Thông qua nhiều thí nghiệm fine-tune LLM trên dữ liệu tương tác con người, chúng tôi chứng minh rằng LLM được
fine-tune có thể đạt hiệu suất tương đương hoặc thậm chí tốt hơn các mô hình truyền thống chỉ với một phần nhỏ
dữ liệu huấn luyện, thể hiện tiềm năng trong hiệu quả dữ liệu.
2 CÔNG TRÌNH LIÊN QUAN
2.1 Sử dụng Ngôn ngữ Tự nhiên trong Hệ thống Gợi ý
Một trong những công trình sớm nhất khám phá việc công thức hóa vấn đề gợi ý như một nhiệm vụ ngôn ngữ tự nhiên là [44]. Họ
sử dụng BERT [6] và GPT-2 [25] trên bộ dữ liệu Movielens [12] để cho thấy các mô hình ngôn ngữ như vậy hoạt động tốt đáng ngạc nhiên,
mặc dù không tốt bằng các baseline được tinh chỉnh tốt như GRU4Rec [15].
P5 [8] fine-tune mô hình T5 [27] mã nguồn mở phổ biến, thống nhất cả xếp hạng, truy xuất và các nhiệm vụ khác như giải thích tóm tắt vào một mô hình. M6-Rec [5] là một công trình liên quan khác, nhưng họ giải quyết nhiệm vụ dự đoán CTR bằng cách fine-tune
một LLM gọi là M6 [17].
Hai công trình gần đây khám phá việc sử dụng LLM cho dự đoán zero-shot. ChatRec [7] xử lý dự đoán zero-shot cũng như
tương tác và cung cấp giải thích. [35] áp dụng phương pháp prompting ba giai đoạn để tạo gợi ý mục tiếp theo
trong bộ dữ liệu Movielens và đạt được các chỉ số cạnh tranh, mặc dù không thể đánh bại các baseline gợi ý tuần tự mạnh
như SASRec [16].
2.2 Mô hình Ngôn ngữ Lớn
Khi mọi người nhận ra rằng việc mở rộng quy mô dữ liệu và mô hình giúp ích cho các mô hình ngôn ngữ, đã có một loạt
mô hình ngôn ngữ lớn được đề xuất và xây dựng: ví dụ PaLM [2], GPT-3 [1] và những mô hình gần đây như OPT [43] và LLaMA [33]. Một
trong những khả năng độc đáo của LLM là khả năng lý luận về mọi thứ, được cải thiện thêm bởi các kỹ thuật
như prompting chuỗi suy nghĩ [41], tự nhất quán [38] và tự phản ánh [30].
Một khả năng mạnh mẽ chính khác của LLM là tuân theo hướng dẫn mà các mô hình có thể tổng quát hóa sang các nhiệm vụ chưa thấy bằng cách
tuân theo các hướng dẫn ngôn ngữ tự nhiên đã cho. Các nhà nghiên cứu đã phát hiện rằng các kỹ thuật như instruction fine-tuning
[4] và RLHF [3] có thể cải thiện đáng kể khả năng của LLM thực hiện các nhiệm vụ được đưa ra các mô tả ngôn ngữ tự nhiên
phù hợp với sở thích của con người. Là một trong những nhiệm vụ có thể được mô tả bằng ngôn ngữ tự nhiên, 'gợi ý'
đã trở thành một khả năng mới đầy hứa hẹn cho LLM. Trong công trình này, chúng tôi tập trung vào các mô hình đã được fine-tune để
cải thiện khả năng tuân theo hướng dẫn như ChatGPT [23], GPT-3 (text-davinci-003 [22]), Flan-U-PaLM và
Flan-T5 [4].
3 PHƯƠNG PHÁP
3.1 Công thức hóa vấn đề
Chúng tôi nghiên cứu nhiệm vụ dự đoán xếp hạng người dùng, được công thức hóa như: Cho một người dùng 𝑢∈U, một chuỗi
tương tác lịch sử của người dùng 𝑢 là 𝐸𝑢={𝑒𝑢1,𝑒𝑢2,...,𝑒𝑢𝑛} và một mục 𝑖∈I, dự đoán xếp hạng mà người dùng 𝑢 sẽ đưa ra cho mục 𝑖, trong đó
chuỗi tương tác lịch sử người dùng 𝐸𝑢 được sắp xếp theo thời gian (𝑒𝑢𝑛 là mục gần nhất mà người dùng tiêu dùng), và mỗi
Bản thảo gửi đến ACM

--- TRANG 4 ---
4 W.-C. Kang, et al.
tương tác 𝑒𝑢𝑘 được đại diện bởi thông tin về mục (ví dụ: ID, tiêu đề, metadata, v.v.) mà người dùng đã tiêu dùng cũng như
xếp hạng người dùng đã đưa ra cho mục đó.
3.2 LLM Zero-shot và Few-shot cho Dự đoán Xếp hạng
Hình 2. Prompt LLM zero và few shot cho dự đoán xếp hạng.
Chúng tôi trình bày các prompt zero-shot và few-shot được sử dụng cho nhiệm vụ dự đoán xếp hạng trên bộ dữ liệu MovieLens trong
hình 2. Như được hiển thị trong hình, các prompt đầu vào mô tả một số tính năng quan trọng được đại diện dưới dạng văn bản, bao gồm
lịch sử xếp hạng trong quá khứ của người dùng và các tính năng mục ứng viên (tiêu đề và thể loại). Cuối cùng, để rút ra một xếp hạng số từ mô hình
với thang xếp hạng, prompt đầu vào chỉ định một thang xếp hạng số. Phản hồi của mô hình được phân tích để trích xuất
đầu ra xếp hạng từ mô hình. Tuy nhiên, chúng tôi phát hiện rằng LLM có thể rất nhạy cảm với các prompt đầu vào và không
luôn tuân theo hướng dẫn được cung cấp. Ví dụ, chúng tôi thấy rằng một số LLM có thể đưa ra lý luận bổ sung hoặc
không cung cấp xếp hạng số nào cả. Để giải quyết điều này, chúng tôi thực hiện kỹ thuật prompt bổ sung bằng cách thêm
hướng dẫn bổ sung như "Đưa ra một số duy nhất làm xếp hạng mà không có giải thích" và "Không đưa ra lý luận" vào
prompt đầu vào.
3.3 Fine-tuning LLM cho Dự đoán Xếp hạng
Trong nghiên cứu hệ thống gợi ý truyền thống, đã được chứng minh rộng rãi rằng việc huấn luyện các mô hình với dữ liệu tương tác con người
là hiệu quả và quan trọng để cải thiện khả năng hiểu sở thích người dùng của hệ thống gợi ý.
Ở đây, chúng tôi khám phá việc huấn luyện LLM với tương tác con người và nghiên cứu cách nó có thể cải thiện hiệu suất mô hình.
Chúng tôi tập trung vào fine-tuning một họ LLM, cụ thể là Flan-T5, vì chúng có sẵn công khai và có hiệu suất cạnh tranh trên
nhiều benchmark. Nhiệm vụ dự đoán xếp hạng có thể được công thức hóa thành một trong hai nhiệm vụ: (1)
phân loại đa lớp; hoặc (2) hồi quy, như được hiển thị trong Hình 3b.
Phân loại Đa lớp. LLM (kiến trúc chỉ Decoder hoặc Encoder-Decoder) về cơ bản được pre-train
với nhiệm vụ phân loại 𝐾-way dự đoán token từ một từ vựng cố định với kích thước 𝐾. Như được hiển thị trong Hình 3b,
có một lớp projection chiếu các đầu ra từ lớp cuối cùng đến kích thước từ vựng sau đó quá trình pre-training
tối ưu hóa cross-entropy loss cho phân loại token. Logits đầu ra được tính như logitsdec=𝑊projℎdec,
trong đó 𝑊proj là ma trận projection có kích thước (𝑑,|𝑉|), ℎdec là đầu ra từ lớp transformer cuối cùng của decoder, 𝑑 là
kích thước chiều ẩn của decoder và |𝑉| là từ vựng.
Bản thảo gửi đến ACM

--- TRANG 5 ---
Liệu LLM có hiểu sở thích người dùng không? Đánh giá LLM về dự đoán xếp hạng người dùng 5
(a) Mô hình chỉ Decoder.
(b) Mô hình Encoder-Decoder.
Hình 3. Hai loại LLM cho nhiệm vụ dự đoán xếp hạng.
Theo [4,26], chúng tôi công thức hóa nhiệm vụ hồi quy xếp hạng như một nhiệm vụ phân loại 5-way, trong đó chúng tôi lấy xếp hạng 1
đến 5 như 5 lớp. Trong quá trình huấn luyện, chúng tôi sử dụng cross-entropy loss như các nhiệm vụ phân loại khác, như được hiển thị dưới đây:
𝐿cross_entropy =−𝑁∑︁𝑖=1𝑟𝑖log(logits𝑖dec), (1)
trong đó 𝑟𝑖 là xếp hạng ground-truth cho mục thứ 𝑖 và 𝑁 là số lượng tổng ví dụ huấn luyện.
Trong quá trình suy luận, chúng tôi tính log-likelihood cho đầu ra mô hình mỗi lớp và chọn lớp có
xác suất lớn nhất làm dự đoán cuối cùng.
Hồi quy. Để cho phép LLM thực hiện các nhiệm vụ hồi quy, chúng tôi đặt hình dạng của ma trận projection 𝑊proj thành (𝑑,1), để
nó chỉ xuất ra logits 1 chữ số. Như được hiển thị trong Phương trình 2, trong quá trình huấn luyện chúng tôi áp dụng mean-squared-error (MSE) loss
dựa trên logits đầu ra và xếp hạng ground-truth.
𝐿regression =1|𝑁|𝑁∑︁𝑖=1(logits𝑖dec−𝑟𝑖)2. (2)
4 THÍ NGHIỆM
Chúng tôi tiến hành các thí nghiệm mở rộng để trả lời các câu hỏi nghiên cứu sau:
RQ1: Các LLM sẵn có có hoạt động tốt cho các gợi ý zero-shot và few-shot không?
RQ2: LLM so sánh như thế nào với các hệ thống gợi ý truyền thống trong một môi trường công bằng
RQ3: Kích thước mô hình quan trọng như thế nào đối với LLM khi được sử dụng cho các hệ thống gợi ý?
RQ4: LLM có hội tụ nhanh hơn các mô hình gợi ý truyền thống không?
4.1 Bộ dữ liệu và Thiết lập Đánh giá
4.1.1 Bộ dữ liệu. Để đánh giá nhiệm vụ dự đoán xếp hạng người dùng, chúng tôi sử dụng hai bộ dữ liệu benchmark được áp dụng rộng rãi để
đánh giá hiệu suất mô hình trên các gợi ý. Cả hai bộ dữ liệu đều bao gồm xếp hạng đánh giá người dùng từ 1 đến 5.
•MovieLens [13]: Chúng tôi sử dụng phiên bản MovieLens-1M bao gồm 1 triệu xếp hạng người dùng cho phim.
•Amazon-Books [21]: Chúng tôi sử dụng danh mục "Books" của Bộ dữ liệu Đánh giá Amazon với xếp hạng người dùng trên các mục. Chúng tôi
sử dụng phiên bản 5-core lọc ra người dùng và mục có ít hơn 5 tương tác.
Bản thảo gửi đến ACM

--- TRANG 6 ---
6 W.-C. Kang, et al.
Bảng 1. Thống kê của bộ dữ liệu
Bộ dữ liệu #Người dùng #Mục # Ví dụ Huấn luyện # Ví dụ Kiểm tra Tính năng
Movielens-1M 6,040 3,689 882,002 2,000 (75,880) Tiêu đề, Thể loại
Amazon-Books 1,850,187 483,579 17,566,711 2,000 (2,324,503) Tiêu đề, Thương hiệu
4.1.2 Phân chia Huấn luyện / Kiểm tra. Để tạo các tập huấn luyện và kiểm tra, chúng tôi tuân theo phân chia thời điểm đơn [32]. Đầu tiên chúng tôi lọc ra
các xếp hạng liên quan đến các mục không có metadata, sau đó sắp xếp tất cả xếp hạng người dùng theo thứ tự thời gian. Cuối cùng,
chúng tôi lấy 90% xếp hạng đầu tiên làm tập huấn luyện và phần còn lại làm tập kiểm tra. Mỗi ví dụ huấn luyện là một tuple
<𝑢𝑠𝑒𝑟_𝑖𝑑,𝑖𝑡𝑒𝑚_𝑖𝑑,𝑖𝑡𝑒𝑚_𝑚𝑒𝑡𝑎𝑑𝑎𝑡𝑎,𝑟𝑎𝑡𝑖𝑛𝑔>, trong đó nhãn là xếp hạng thang Likert 5 điểm. Các tính năng đầu vào là 𝑢𝑠𝑒𝑟_𝑖𝑑,
𝑖𝑡𝑒𝑚_𝑖𝑑, và danh sách các tính năng item_metadata. Thống kê của các bộ dữ liệu được hiển thị trong Bảng 1. Do chi phí tính toán cao
của các thí nghiệm zero-shot và few-shot dựa trên LLM, chúng tôi lấy mẫu ngẫu nhiên từ tập kiểm tra của mỗi
bộ dữ liệu thành 2,000 tuple làm tập kiểm tra nhỏ hơn. Đối với tất cả các thí nghiệm, chúng tôi báo cáo kết quả trên tập kiểm tra được lấy mẫu. Và chúng tôi
cắt bớt chuỗi người dùng đến 10 tương tác gần nhất trong quá trình huấn luyện và đánh giá.
4.1.3 Chỉ số Đánh giá. Chúng tôi sử dụng các chỉ số được áp dụng rộng rãi RMSE (Root Mean Squared Error) và MAE (Mean
Average Error) để đo hiệu suất mô hình trên dự đoán xếp hạng. Hơn nữa, chúng tôi sử dụng ROC-AUC để đánh giá hiệu suất của mô hình
trên xếp hạng, trong đó các xếp hạng lớn hơn hoặc bằng 4 được coi là dương và phần còn lại là âm.
Trong trường hợp này, AUC đo xem mô hình có xếp hạng các mục dương cao hơn các mục âm không.
4.2 Baseline và LLM
4.2.1 Baseline.
•Hệ thống Gợi ý Truyền thống: Chúng tôi xem xét một số mô hình gợi ý truyền thống như baseline mạnh, bao gồm
1. Matrix Factorization (MF) [29], và 2. Multi-layer Perceptrons (MLP) [14]. Đối với MF và MLP, chỉ có user ID và item
ID được sử dụng làm tính năng đầu vào.
•Bộ Dự đoán Xếp hạng Tuần tự Nhận biết Thuộc tính và Xếp hạng: Trong các thí nghiệm, chúng tôi cung cấp cho LLM
metadata mục lịch sử, như tiêu đề và danh mục, cùng với xếp hạng lịch sử. Tuy nhiên, theo hiểu biết tốt nhất của chúng tôi, không có
phương pháp hiện tại nào được thiết kế cho thiết lập này¹. Để đảm bảo so sánh công bằng, chúng tôi xây dựng một mô hình
Transformer-MLP để xử lý hiệu quả cùng thông tin đầu vào được cung cấp cho LLM.
Có ba lựa chọn thiết kế chính: (i) xử lý tính năng: Chúng tôi xem tất cả tính năng như tính năng thưa thớt, và học embedding của chúng
end-to-end. Ví dụ, chúng tôi sử dụng one-hot encoding cho thể loại, và tạo một bảng embedding, trong đó
hàng thứ i là embedding của thể loại i. Tương tự, chúng tôi có được bag-of-words encodings thông qua việc áp dụng tokenizer² trên tiêu đề,
và sau đó tra cứu embedding tương ứng. (ii) mô hình hóa người dùng: đối với mỗi hành vi người dùng, chúng tôi sử dụng AddorConcat
để tổng hợp tất cả embedding (ví dụ item ID, tiêu đề, thể loại/danh mục, xếp hạng) thành một, và sau đó áp dụng các lớp bi-directional
self-attention [34] với position embedding đã học để mô hình hóa hành vi trong quá khứ của người dùng. Tương tự như SASRec, chúng tôi sử dụng
embedding đầu ra của hành vi gần nhất như tóm tắt người dùng; (iii) Kết hợp người dùng và ứng viên cho dự đoán xếp hạng:
chúng tôi áp dụng một MLP trên embedding người dùng cùng với các tính năng mục ứng viên khác để tạo ra dự đoán xếp hạng cuối cùng,
và tối ưu hóa để giảm thiểu MSE.
¹Các công trình liên quan nhất là SASRec [16] và CARCA [28], tuy nhiên chúng được thiết kế cho dự đoán mục tiếp theo thay vì dự đoán xếp hạng, và do đó không
trực tiếp áp dụng được cho trường hợp của chúng tôi.
²https://www.tensorflow.org/text/api_docs/python/text/WhitespaceTokenizer
Bản thảo gửi đến ACM

--- TRANG 7 ---
Liệu LLM có hiểu sở thích người dùng không? Đánh giá LLM về dự đoán xếp hạng người dùng 7
Bảng 2. Kết quả dự đoán xếp hạng người dùng. Phương pháp hoạt động tốt nhất được in đậm trong mỗi cột, và gạch chân trong mỗi nhóm.
Mô hình MovieLens Amazon-Books
RMSE↓MAE↓ AUC↑ RMSE↓MAE↓ AUC↑
LLM Zero-Shot
Flan-U-PALM 1.0677 0.7740 0.7084 0.9565 0.5569 0.7676
ChatGPT 1.0081 0.8193 0.6794 1.0081 0.8093 0.6778
text-davinci-003 1.0460 0.7850 0.6951 0.8890 0.5442 0.7416
LLM Few-Shot
Flan-U-PALM 1.0721 0.7605 0.7094 1.0712 0.5855 0.7439
ChatGPT 1.0862 0.8203 0.6930 1.0618 0.7760 0.7470
text-davinci-003 1.0867 0.8119 0.6963 1.0716 0.7753 0.7739
Thống kê Bộ dữ liệu Đơn giản
Xếp hạng Trung bình Toàn cục 1.1564 0.9758 0.5 0.9482 0.7609 0.5
Xếp hạng Trung bình Mục Ứng viên 0.9749 0.7778 0.7395 0.9342 0.7078 0.6041
Xếp hạng Trung bình Quá khứ Người dùng 1.0196 0.7959 0.7266 0.8527 0.5502 0.8047
Phương pháp Gợi ý Có giám sát
MF 0.9552 0.7436 0.7734 1.7960 1.1070 0.7638
MLP 0.9689 0.7452 0.7393 0.8607 0.6384 0.6932
Transformer+MLP 0.8848 0.7036 0.7979 0.8143 0.5541 0.8042
LLM Fine-tuned
Flan-T5-Base (phân loại) 1.0110 0.6805 0.7590 0.9856 0.4685 0.6292
Flan-T5-Base (hồi quy) 0.9187 0.7092 0.7949 0.8413 0.5317 0.8182
Flan-T5-XXL (hồi quy) 0.8979 0.6986 0.8042 0.8301 0.5122 0.8312
Để tinh chỉnh đúng cách các mô hình baseline, chúng tôi định nghĩa một không gian tìm kiếm siêu tham số (ví dụ cho chiều embedding,
learning rate, kích thước mạng, tổng hợp AddorConcat, v.v.), và thực hiện hơn 100 thử nghiệm tìm kiếm bằng Vizier
[9], một công cụ tối ưu hóa siêu tham số black-box.
•Heuristics: Chúng tôi cũng bao gồm ba baseline dựa trên heuristic: (1) xếp hạng trung bình toàn cục: (2) xếp hạng trung bình mục ứng viên,
và (3) xếp hạng trung bình quá khứ người dùng, có nghĩa là dự đoán của mô hình phụ thuộc vào (1) xếp hạng trung bình trong số
tất cả xếp hạng người dùng-mục, (2) xếp hạng trung bình từ mục ứng viên hoặc (3) xếp hạng trung bình của người dùng trong quá khứ.
4.2.2 LLM cho Học Zero-shot và Few-shot.: Chúng tôi sử dụng các LLM được liệt kê dưới đây cho học zero-shot và few-shot.
Chúng tôi sử dụng nhiệt độ 0.1 cho tất cả LLM, vì đầu ra của LLM trong trường hợp của chúng tôi đơn giản là một dự đoán xếp hạng. Chúng tôi sử dụng các mô hình GPT-3
từ OpenAI [24]: (i)text-davinci-003 (175B): Mô hình GPT-3 có khả năng nhất với Reinforcement Learning
from Human Feedback (RLHF) [31]; (ii) ChatGPT: mô hình mặc định là gpt-3.5-turbo, được fine-tune trên cả
cuộc trò chuyện do con người viết và RLHF, và được tối ưu hóa thêm cho cuộc hội thoại. Flan-U-PaLM (540B) là mô hình lớn nhất và mạnh nhất
trong [4], nó áp dụng cả FLAN instruction tuning [39] và mục tiêu huấn luyện UL2 [4] trên PaLM [2].
4.2.3 LLM cho Fine-tuning. Đối với các phương pháp fine-tuning, chúng tôi sử dụng các mô hình Flan-T5-Base (250M) và Flan-T5-XXL (11B) trong
các thí nghiệm. Chúng tôi đặt learning rate thành 5e-5, batch size thành 64, tỷ lệ drop out thành 0.1 và huấn luyện 50k bước trên tất cả bộ dữ liệu.
4.3 LLM Zero-Shot và Few-shot (RQ1)
Như được hiển thị trong Bảng 2, chúng tôi tiến hành thí nghiệm trên một số LLM sẵn có trong thiết lập zero-shot. Chúng tôi quan sát thấy rằng
LLM dường như hiểu nhiệm vụ từ mô tả prompt, và dự đoán xếp hạng hợp lý. LLM vượt trội hơn xếp hạng trung bình toàn cục trong hầu hết trường hợp,
và hoạt động tương đương với xếp hạng trung bình mục hoặc người dùng. Ví dụ, text-davinci-003
hoạt động hơi kém hơn xếp hạng trung bình mục ứng viên trên Movielens nhưng vượt trội trên Amazon-Books. Đối với
Bản thảo gửi đến ACM

--- TRANG 8 ---
8 W.-C. Kang, et al.
(a) MovieLens
(b) Amazon Books
Hình 4. Hiệu quả dữ liệu: đường cong hội tụ.
các thí nghiệm few-shot, chúng tôi cung cấp 3 ví dụ trong prompt (3-shot). So với zero-shot, chúng tôi thấy rằng
AUC cho LLM few-shot được cải thiện, trong khi không có mẫu rõ ràng trong RMSE và MAE.
Hơn nữa, chúng tôi thấy rằng cả LLM zero-shot và few-shot đều kém hiệu suất so với các mô hình gợi ý truyền thống
được huấn luyện với dữ liệu tương tác. Như được hiển thị trong Bảng 2, các mô hình GPT-3 và Flan-U-PaLM đạt được hiệu suất
thấp hơn đáng kể so với các mô hình có giám sát. Hiệu suất kém có thể là do thiếu dữ liệu tương tác người dùng-mục
trong pre-training của LLM, và do đó chúng không có kiến thức về sở thích con người cho các nhiệm vụ gợi ý khác nhau. Hơn nữa, các nhiệm vụ gợi ý
rất phụ thuộc vào bộ dữ liệu: (ví dụ) cùng một bộ phim có thể có xếp hạng trung bình khác nhau trên các nền tảng khác nhau. Do đó, không biết thống kê cụ thể của bộ dữ liệu,
LLM không thể cung cấp dự đoán phổ quát phù hợp với mọi bộ dữ liệu.
4.4 LLM so với Mô hình Gợi ý Truyền thống (RQ2)
Fine-tuning LLM là một cách hiệu quả để đưa thống kê bộ dữ liệu vào LLM, và chúng tôi thấy hiệu suất của LLM được fine-tune
tốt hơn nhiều so với LLM zero/few-shot. Ngoài ra, khi fine-tuning mô hình Flan-T5-base với classification
loss, hiệu suất kém hơn nhiều so với fine-tuning với regression loss trên cả ba chỉ số. Điều này cho thấy
tầm quan trọng của việc chọn mục tiêu tối ưu hóa phù hợp cho fine-tuning LLM.
So sánh với baseline mạnh nhất Transformer-MLP, chúng tôi thấy Flan-T5-XXL được fine-tune có MAE và
AUC tốt hơn, ngụ ý rằng LLM được fine-tune có thể phù hợp hơn cho các nhiệm vụ xếp hạng.
4.5 Ảnh hưởng của Kích thước Mô hình (RQ3)
Đối với tất cả LLM chúng tôi nghiên cứu với kích thước khác nhau từ 250M đến 500B tham số, chúng tôi có thể sử dụng prompt zero-shot
hoặc few-shot để cho chúng xuất ra dự đoán xếp hạng từ 1 đến 5. Điều này cho thấy hiệu quả của instruction
tuning cho phép các LLM này (Flan-T5, Flan-U-PaLM, GPT-3) tuân theo prompt. Chúng tôi cũng thấy rằng chỉ có LLM
với kích thước lớn hơn 100B mới hoạt động tốt hợp lý trên dự đoán xếp hạng trong thiết lập zero-shot, như được hiển thị trong Hình 1.
Đối với các thí nghiệm fine-tuning, chúng tôi cũng thấy rằng Flan-T5-XXL vượt trội Flan-T5-Base trên cả hai bộ dữ liệu, như được hiển thị trong
hai hàng cuối của Bảng 2.
4.6 Hiệu quả Dữ liệu của LLM (RQ4)
Vì LLM đã học được lượng lớn kiến thức thế giới trong quá trình pre-training, trong khi các mô hình gợi ý truyền thống
được huấn luyện từ đầu, chúng tôi so sánh đường cong hội tụ của chúng trong Hình 4 để kiểm tra xem LLM có hiệu quả dữ liệu tốt hơn không.
Chúng ta có thể thấy rằng đối với RMSE, cả hai phương pháp đều có thể hội tụ đến hiệu suất hợp lý với một phần nhỏ
Bản thảo gửi đến ACM

--- TRANG 9 ---
Liệu LLM có hiểu sở thích người dùng không? Đánh giá LLM về dự đoán xếp hạng người dùng 9
dữ liệu. Điều này có lẽ là do ngay cả xếp hạng trung bình của tất cả mục cũng có RMSE tương đối thấp, và do đó miễn là một
mô hình học được cách dự đoán xếp hạng gần với xếp hạng trung bình, nó có thể đạt được hiệu suất hợp lý. Đối với AUC xu hướng
rõ ràng hơn, vì việc đơn giản dự đoán xếp hạng trung bình dẫn đến AUC là 0.5. Chúng tôi thấy rằng một phần nhỏ dữ liệu là
cần thiết để LLM đạt được hiệu suất tốt, trong khi Transformer+MLP cần nhiều dữ liệu huấn luyện hơn (ít nhất 1
epoch) để hội tụ.
5 KẾT LUẬN
Trong bài báo này, chúng tôi đánh giá hiệu quả của các mô hình ngôn ngữ lớn như một hệ thống gợi ý cho dự đoán xếp hạng người dùng
trong ba thiết lập: 1. zero-shot; 2. few-shot; và 3. fine-tuning. So với các phương pháp gợi ý truyền thống,
kết quả của chúng tôi tiết lộ rằng LLM trong zero-shot và few-shot tụt hậu so với các phương pháp có giám sát đầy đủ, ngụ ý
tầm quan trọng của việc kết hợp phân phối bộ dữ liệu mục tiêu vào LLM. Mặt khác, LLM được fine-tune có thể
thu hẹp khoảng cách với các baseline được thiết kế cẩn thận trong các chỉ số chính. Các hệ thống gợi ý dựa trên LLM có một số lợi ích: (i) hiệu quả dữ liệu tốt hơn;
(ii) đơn giản cho xử lý tính năng và mô hình hóa: chúng tôi chỉ cần chuyển đổi thông tin thành prompt
mà không cần thiết kế thủ công các chiến lược xử lý tính năng, phương pháp embedding, và kiến trúc mạng để xử lý
nhiều loại thông tin khác nhau; (iii) tiềm năng mở khóa khả năng gợi ý hội thoại. Công trình của chúng tôi làm sáng tỏ
tình trạng hiện tại của các hệ thống gợi ý dựa trên LLM, và trong tương lai chúng tôi sẽ tiếp tục tìm hiểu cải thiện
hiệu suất thông qua các phương pháp như prompt tuning, và khám phá các ứng dụng gợi ý mới được hỗ trợ bởi LLM.
TÀI LIỆU THAM KHẢO
[1]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877–1901.
[2]Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles
Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).
[3]Paul Francis Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep Reinforcement Learning from Human
Preferences. ArXiv abs/1706.03741 (2017).
[4]Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert
Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra,
Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts,
Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling Instruction-Finetuned Language Models. ArXiv abs/2210.11416 (2022).
[5]Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. M6-Rec: Generative Pretrained Language Models are Open-Ended
Recommender Systems. arXiv preprint arXiv:2205.08084 (2022).
[6]Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
understanding. arXiv preprint arXiv:1810.04805 (2018).
[7]Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-REC: Towards Interactive and Explainable
LLMs-Augmented Recommender System. arXiv preprint arXiv:2303.14524 (2023).
[8]Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as Language Processing (RLP): A Unified
Pretrain, Personalized Prompt & Predict Paradigm (P5). arXiv preprint arXiv:2203.13366 (2022).
[9]Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D. Sculley. 2017. Google Vizier: A Service for Black-Box
Optimization. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada,
August 13 - 17, 2017 . ACM, 1487–1495. https://doi.org/10.1145/3097983.3098043
[10] Google. 2023. Bard: A Large Language Model from Google AI. https://bard.google.com/
[11] Shuguang Han, Xuanhui Wang, Mike Bendersky, and Marc Najork. 2020. Learning-to-Rank with BERT in TF-Ranking. arXiv:2004.08476 [cs.IR]
[12] F Maxwell Harper and Joseph A Konstan. 2015. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis)
5, 4 (2015), 1–19.
[13] F. Maxwell Harper and Joseph A. Konstan. 2016. The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst. 5 (2016), 19:1–19:19.
[14] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural Collaborative Filtering. In Proceedings of the 26th
International Conference on World Wide Web, WWW 2017, Perth, Australia, April 3-7, 2017 , Rick Barrett, Rick Cummings, Eugene Agichtein, and
Evgeniy Gabrilovich (Eds.). ACM, 173–182. https://doi.org/10.1145/3038912.3052569
Bản thảo gửi đến ACM

--- TRANG 10 ---
10 W.-C. Kang, et al.
[15] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks.
arXiv preprint arXiv:1511.06939 (2015).
[16] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining
(ICDM) . IEEE, 197–206.
[17] Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, et al .2021. M6: A chinese
multimodal pretrainer. arXiv preprint arXiv:2103.00823 (2021).
[18] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023. Is ChatGPT a Good Recommender? A Preliminary Study. arXiv:2304.10149 [cs.IR]
[19] Yang Liu and Mirella Lapata. 2019. Text Summarization with Pretrained Encoders. arXiv:1908.08345 [cs.CL]
[20] Microsoft. 2023. Reinventing search with a new AI-powered Bing and Edge, your copilot for the web. https://news.microsoft.com/the-new-Bing/
[21] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects. In
Conference on Empirical Methods in Natural Language Processing .
[22] OpenAI. 2022. Aligning language models to follow instructions. https://openai.com/research/instruction-following.
[23] OpenAI. 2022. Introducing ChatGPT. https://openai.com/blog/chatgpt.
[24] OpenAI. 2023. GPT Models Documentation. https://platform.openai.com/docs/models/overview
[25] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al .2019. Language models are unsupervised multitask learners.
OpenAI blog 1, 8 (2019), 9.
[26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. Learn. Res. 21, 1, Article 140 (jan 2020), 67 pages.
[27] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020), 5485–5551.
[28] Ahmed Rashed, Shereen Elsayed, and Lars Schmidt-Thieme. 2022. CARCA: Context and Attribute-Aware Next-Item Recommendation via Cross-
Attention. arXiv preprint arXiv:2204.06519 (2022).
[29] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. BPR: Bayesian personalized ranking from implicit feedback.
arXiv preprint arXiv:1205.2618 (2012).
[30] Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint
arXiv:2303.11366 (2023).
[31] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F. Christiano. 2020.
Learning to summarize from human feedback. CoRR abs/2009.01325 (2020). arXiv:2009.01325 https://arxiv.org/abs/2009.01325
[32] Aixin Sun. 2022. Take a Fresh Look at Recommender Systems from an Evaluation Standpoint.
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).
[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is
all you need. Advances in neural information processing systems 30 (2017).
[35] Lei Wang and Ee-Peng Lim. 2023. Zero-Shot Next-Item Recommendation using Large Pretrained Language Models. arXiv preprint arXiv:2304.03153
(2023).
[36] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. Learning Deep Transformer Models for Machine
Translation. arXiv:1906.01787 [cs.CL]
[37] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-Consistency
Improves Chain of Thought Reasoning in Language Models. arXiv:2203.11171 [cs.CL]
[38] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in
language models. arXiv preprint arXiv:2203.11171 (2022).
[39] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2022. Finetuned
Language Models are Zero-Shot Learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,
2022. OpenReview.net. https://openreview.net/forum?id=gEZrGCozdqR
[40] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent Abilities of Large Language Models.
arXiv:2206.07682 [cs.CL]
[41] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning
in large language models. arXiv preprint arXiv:2201.11903 (2022).
[42] Jiacheng Yang, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Weinan Zhang, Yong Yu, and Lei Li. 2020. Towards making the most of bert in neural
machine translation. In Proceedings of the AAAI conference on artificial intelligence , Vol. 34. 9378–9385.
[43] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022).
[44] Yuhui Zhang, Hao Ding, Zeren Shui, Yifei Ma, James Zou, Anoop Deoras, and Hao Wang. 2021. Language models as recommender systems:
Evaluations and limitations. (2021).
Bản thảo gửi đến ACM

--- TRANG 11 ---
Liệu LLM có hiểu sở thích người dùng không? Đánh giá LLM về dự đoán xếp hạng người dùng 11
[45] Lixin Zou, Shengqiang Zhang, Hengyi Cai, Dehong Ma, Suqi Cheng, Shuaiqiang Wang, Daiting Shi, Zhicong Cheng, and Dawei Yin. 2021. Pre-Trained
Language Model Based Ranking in Baidu Search. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining (Virtual
Event, Singapore) (KDD '21) . Association for Computing Machinery, New York, NY, USA, 4014–4022. https://doi.org/10.1145/3447548.3467147
Bản thảo gửi đến ACM
