# 2305.05586.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2305.05586.pdf
# File size: 4949740 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
RLocator: Reinforcement Learning for Bug
Localization
Partha Chakraborty ,Student Member, IEEE , Mahmoud Alfadel ,Member, IEEE , and Meiyappan Nagappan
Abstract —Software developers spend a significant portion of
time fixing bugs in their projects. To streamline this process,
bug localization approaches have been proposed to identify the
source code files that are likely responsible for a particular bug.
Prior work proposed several similarity-based machine-learning
techniques for bug localization. Despite significant advances in
these techniques, they do not directly optimize the evaluation
measures. We argue that directly optimizing evaluation measures
can positively contribute to the performance of bug localization
approaches.
Therefore, in this paper, we utilize Reinforcement Learning
(RL) techniques to directly optimize the ranking metrics. We
propose RL OCATOR , a Reinforcement Learning-based bug lo-
calization approach. We formulate RLocator using a Markov
Decision Process (MDP) to optimize the evaluation measures
directly. We present the technique and experimentally evaluate
it based on a benchmark dataset of 8,316 bug reports from six
highly popular Apache projects. The results of our evaluation
reveal that RLocator achieves a Mean Reciprocal Rank (MRR)
of 0.62, a Mean Average Precision (MAP) of 0.59, and a Top 1
score of 0.46. We compare RLocator with three state-of-the-art
bug localization tools, FLIM, BugLocator, and BL-GAN. Our
evaluation reveals that RLocator outperforms both approaches
by a substantial margin, with improvements of 38.3% in MAP,
36.73% in MRR, and 23.68% in the Top K metric. These
findings highlight that directly optimizing evaluation measures
considerably contributes to performance improvement of the bug
localization problem.
Index Terms —Reinforcement Learning, Bug Localization,
Deep Learning
I. I NTRODUCTION
Software bugs are an inevitable part of software develop-
ment. Developers spend one-third of their time debugging
and fixing bugs [1]. After a bug report/issue has been filed,
the project team identifies the source code files that need
to be inspected and modified to address the issue. However,
manually locating the files responsible for a bug is expensive
(in terms of time and resources), especially when there are a lot
of files and bug reports. Moreover, the number of bugs reported
is often higher than the number of available developers [2].
Consequently, the fix-time and maintenance costs rise when
the customer satisfaction rate decreases [3].
Bug Localization is a method that refers to identifying the
source code files where a particular bug originated. Given
Partha Chakraborty and Meiyappan Nagappan is with David R. Cheriton
School of Computer Science, University of Waterloo, Waterloo, Canada, N2L
3G1.
E-mail: {p9chakra, mei.nagappan }@uwaterloo.ca
Mahmoud Alfadel is with the Department of of Computer Science, Uni-
versity of Calgary, Calgary, Canada, T2N 1N4.
E-mail: mahmoud.alfadel@ucalgary.caa bug report, bug localization approaches utilize the textual
information in the bug report, and the project source code files
to shortlist the potentially buggy files. Prior work has proposed
various Information Retrieval-based Bug Localization (IRBL)
approaches to help developers speed up the debugging process
(e.g., Deeplocator [4], CAST [5], KGBugLocator [6], BL-
GAN [7]).
One common theme among these approaches is that they
follow a similarity-based approach to localize bugs. Such
techniques measure the similarity between bug reports and the
source code files. For estimating similarity, they use various
methods such as cosine distance [8], Deep Neural Networks
(DNN) [9], and Convolutional Neural Networks (CNN) [5].
Then, they rank the source code files based on their similarity
score. In the training phase of these approaches, the model
learns to optimize the similarity metrics. In contrast, in the
testing phase, the model is tested with ranking metrics (e.g.,
Mean Reciprocal Rank (MRR) or Mean Average Precision
(MAP)).
While most of these approaches showed promising perfor-
mance, they optimize a metric that indirectly represents the
performance metrics. Prior studies [10], [11], [12], [13] found
that direct optimization of evaluation measures substantially
contributes to performance improvement of ranking problems.
Direct optimization is also efficient compared to optimizing
indirect metrics [13]. Hence, we argue that it is challenging
for the solutions proposed by prior studies to sense how
a wrong prediction would affect the performance evaluation
measures [10]. In other words, if we use the retrieval metrics
(e.g., MAP) in the training phase, the model will learn how
each prediction will impact the evaluation metrics. A wrong
prediction will change the rank of the source code file and
ultimately impact the evaluation metrics.
Reinforcement Learning (RL) is a sub-category of machine
learning methods where labeled data is not required. In RL,
the model is not trained to predict a specific value. Instead,
the model is given a signal about a right or wrong choice
in training [14]. Based on the signal, the model updates its
decision. This allows RL to use evaluation measures such as
MRR and MAP in the training phase and directly optimize the
evaluation metrics. Moreover, because of using MRR/MAP
as a signal instead of a label, the problem of overfitting
will be less prevalent. Markov Decision Process (MDP) is a
foundational element of RL. MDP is a mathematical frame-
work that allows the formalization of discrete-time decision-
making problems [15]. Real-world problems often need to be
formalized as MDP to apply RL.
© 2024 IEEE. Author pre-print copy. The final publication is available online at: https://doi.org/10.1109/TSE.2024.3452595arXiv:2305.05586v3  [cs.SE]  30 Sep 2024

--- PAGE 2 ---
2
In this paper, we present RLocator , an RL technique for
localizing software bugs in source code files. We formulate
RLocator into an MDP. In each step of the MDP, we use
MRR and MAP as signals to guide the model to optimal
choice. We evaluate RLocator on a benchmark dataset of
six Apache projects and find that, compared with existing
state-of-the-art bug localization techniques, RLocator achieves
substantial performance improvement. While pinpointing the
exact reasons for RL’s superior performance over other
supervised techniques can be challenging, RL learns more
generalizable approaches, especially in dynamic and complex
environments. In comparison to supervised learning, it
learns approaches that are more adaptable to a variety of
situations [16], [17], which is a form of generalization.
Additionally, RL demonstrates proficiency in scenarios where
the optimal solution is not clearly defined, showcasing
its versatility across various tasks and domains [14]. These
factors can contribute to the superior performance of RLocator.
The main contributions of our work are as follows:
•We present RLocator, an RL-based software bug localiza-
tion approach. The key technical novelty of RLocator is
using RL for bug localization, which includes formulating
the bug localization process into an MDP.
•We provide an experimental evaluation of RLocator
with 8,316 bug reports from six Apache projects. When
RLocator can localize, it achieves an MRR of 0.49 - 0.62,
MAP of 0.47 - 0.59, and Top 1 of 0.38 - 0.46 across
all studied projects. Additionally, we compare RLocator’s
performance with state-of-the-art bug localization meth-
ods. RLocator outperforms FLIM [18] by 38.3% in MAP,
36.73% in MRR, and 23.68% in Top K. Furthermore,
RLocator exceeds BugLocator [3] by 56.86% in MAP,
41.51% in MRR, and 26.32% in Top K. In terms of
Top K, RLocator shows improved performance over BL-
GAN [7], with gains ranging from 55.26% to 3.33%. The
performance gains for MAP and MRR are 40.74% and
32.2%, respectively.
II. M OTIVATION
Reinforcement Learning (RL) stands out for its ability to
learn from feedback, a characteristic that empowers models
to self-correct based on the outcomes of their actions. This
feature finds widespread application, exemplified by platforms
like Spotify, an audio streaming service using RL to learn user
preferences [19]. The model evolves and adapts by presenting
music selections and refining recommendations through user
interactions. The versatility of RL extends beyond entertain-
ment; various companies [20] and domains [21] leverage its
capacity for iterative learning and adjustment.
The proficiency required for bug localization is often ac-
quired through experience, with seasoned developers exhibit-
ing a faster bug-finding aptitude than their less experienced
counterparts [22]. Recognizing the significance of experience
in bug localization, we propose the integration of reinforce-
ment learning into this domain. By employing RL, a model can
present developers with sets of source code files as possiblecauses for a bug and learn from their feedback to enhance
its skill in localizing bugs in the software. In contrast to
conventional machine learning approaches, which rely solely
on labeled data and lack easy adaptability, reinforcement
learning presents two distinct advantages: firstly, the ability to
learn from developer feedback, and secondly, the elimination
of the requirement for labeled data in real-world scenarios.
Therefore, our research aims to incorporate reinforcement
learning into bug localization, leveraging its capacity to adapt
and enhance performance through iterative feedback.
III. B ACKGROUND
In this section, we describe terms related to the bug local-
ization problem, which we use throughout our study. Also, we
present an overview of reinforcement learning.
A. Bug Localization System
A typical bug localization system utilizes several sources
of data, e.g., bug reports, stack traces, and logs, to identify
the responsible source code files. One particular challenge of
the system is that the bug report contains natural language,
whereas source code files are written in a programming
language.
Typically, bug localization systems identify whether a bug
report relates to a source code file. To do so, the system
extracts features from both the bug report and the source code
files. Previous studies used techniques such as N-gram [23],
[24] and Word2Vec [25], [26] to extract features (embedding)
from bug reports and source code files. Other studies (e.g.,
Devlin et al. [27]) introduced the transformer-based model
BERT which has achieved higher performance than all the
previous techniques. One of the reasons transformer-based
models perform better in extracting textual features is that the
transformer uses multi-head attention, which can utilize long
context while generating embedding. Previous studies have
proposed a multi-modal BERT model [28] for programming
languages, which can extract features from both bug reports
and source code files.
A bug report mainly contains information related to unex-
pected behavior and how to reproduce it. It mainly includes
a bug ID, title, textual description of the bug, and version of
the codebase where the bug exists. The bug report may have
an example of code, stack trace, or logs. A bug localization
system retrieves all the source code files from a source code
repository at that particular version. For example, assume we
have 100 source code files in a repository in a specific version.
After retrieving 100 files from that version, the system will
estimate the relevance between the bug report and each of
the 100 files. The relevance can be measured in several ways.
For example, a naive system can check how many words of
the bug report exist in each source code file. A sophisticated
system can compare embeddings using cosine distance [29].
After relevance estimation, the system ranks the files based
on their relevance score. The ranked list of files is the final
output of a bug localization system that developers will use.

--- PAGE 3 ---
3
B. Reinforcement Learning
In Reinforcement Learning (RL), the agent interacts with
the environment through observation. Formally, an observation
is called “State,” S. In each state, at time t,St, the agent
takes action Abased on its understanding of the state. Then,
the environment provides feedback/reward ℜand transfers the
agent into a new state St+1. The agent’s strategy to determine
the best action, which will eventually lead to the highest
cumulative reward, is referred to as policy [30], [14].
The cumulative reward (until the goal/end) that an agent
can get if it takes a specific action in a certain state is called
Q value . The function that is used to estimate the Q value is
often referred as Q function orValue function .
In RL, an agent starts its journey from a starting state
and then goes forward by picking the appropriate action. The
journey ends in a pre-defined end state. The journey from start
to end state is referred to as episode .
From a high level, we can divide the state-of-the-art RL
algorithms into two classes. The first is the model-free al-
gorithms, where the agent has no prior knowledge about
the environment. The agent learns about the environment by
interacting with the environment. The other type is the model-
based algorithm. In a model-based algorithm, the agent uses
the reward prediction from the model instead of interacting
with the environment.
The bug localization task is quite similar to the model-
free environment as we cannot predict/identify the buggy files
without checking the bug report and source code files (without
interacting with the environment). Thus, we use model-free RL
algorithms in this study. Two popular variants of model-free
RL algorithms are:
•Value Optimization : The agent tries to learn the Q value
function in value optimization approaches. The agent
keeps the Q value function in memory and updates it
gradually. It consults the Q value function in a particular
state and picks the action that will give the highest value
(reward). An example of the value optimization-based
approach is Deep Q Network (DQN) [14].
•Policy Optimization : In the Policy optimization approach,
the agent tries to learn the mapping between the state
and the action that will result in the highest reward. The
agent will pick the action based on the mapping in a
particular state. An example of the policy optimization-
based approach is Advantage Actor-Critic (A2C) [31],
[14].
A2C is a policy-based algorithm where the agent learns
an optimized policy to solve a problem. In Actor-Critic, the
actor-model picks action. The future return (reward) of action
is estimated using the critic model. The actor model uses the
critic model to pick the best action in any state. Advantage
actor-critic subtracts a base value from the return in any
timestep. A2C with entropy adds the entropy of the probability
of the possible action with the loss of the actor model. As a
result, in the gradient descent step, the model tries to maximize
the entropy of the learned policy. Maximization of entropy
ensures that the agent assigns almost an equal probability to
an action with a similar return.IV. RL OCATOR : REINFORCEMENT LEARNING FOR BUG
LOCALIZATION
In this section, we discuss the steps we follow to use RLoca-
tor. First, we explain (in Section IV-A) the pre-processing
step required for using RLocator. Then, we explain (in Sec-
tion IV-B) the formulation steps of our design of RLocator.
We present the overview of our approach in Figure 1.
A. Pre-process
Before using the bug reports and source code files to train
the RL model, they undergo a series of pre-processing steps.
The steps are described in this section.
Input: The inputs to our bug localization tool are bug reports
and source code files associated with a particular version of
a project repository. Software projects maintain a repository
for their bugs or issues (e.g., Jira, Github, Bugzilla). The first
component, the bug report, can be retrieved from those issue
repositories. We use the bug report to obtain the second com-
ponent (i.e., source code) by identifying the project version
affected by the bug. Typically, each bug report is associated
with a version or commit SHA of the project repository. After
identifying the buggy version, we collect all source code files
from the specific version of the code repository. In the training
phase, we compile bug reports and source code files into a
dataset for subsequent usage. Our dataset contains a set of
bug reports where each bug report has its own set of source
code files. In real-world usage, RLocator directly accesses bug
reports and source code files from the repository. In Figure 1,
we illustrate the input stage where we get the bug report and
source code files from the dataset.
Shortlisting source code files: The number of source code
files in different versions of the repository can be different. All
of the source code files can be potentially responsible for a
bug. In this step, we identify Ksource code files as candidates
for each bug. We limited the candidates to Kas we cannot
pass a variable number of source code files to the RL model.
Moreover, given that RLocator primarily learns from develop-
ers’ feedback, its usage can prove challenging for a developer
with many candidate source code files. To illustrate the issue,
consider a repository with 700 files. RLocator presents files
to the developer one by one for relevance verification. This
sequential approach significantly prolongs the time taken to
find a relevant file, resulting in a waste of developers’ time.
Consequently, it is crucial to limit the number of files shown
to developers by providing a shortlisted set for assessment.
To identify the Kmost relevant files, we use ElasticSearch
(ES). ES is a search engine based on the Lucene search
engine project. It is a distributed, open-source search and
analytic engine for all data types, including text. It analyzes
and indexes words/tokens for the textual match and uses BM25
to rank the files matching the query. We use the ES index for
identifying the topmost k source code files related to a bug
report. Following the study by Liu et el. [32] (who used ES
in the context of code search), we build an ES index using
the source code files and then queried the index using the bug
report as the query. Then, we picked the first kfiles with the
highest textual similarities with the bug report. We want to

--- PAGE 4 ---
4
Fig. 1. Bug Localization as Markov Decision Process.
note that the goal of bug localization is to get the relevant
files to be ranked as close to the 1strank as possible. Hence,
metrics like MAP and MRR can measure the performance of
bug localization techniques. While one can argue why we not
only rely on ES to rank the relevant files, we find that the MAP
and MRR of using ES are poor. Our RL-based technique learns
from feedback and aims to rerank the output from ES to get
higher MAP and MRR scores. In Figure 1, we illustrated the
candidate refinement step where we query ElasticSearch using
the bug report and use outputs to refine the candidate source
code files.
Filtration of bug report and source code files: One limitation
of ES is that it sometimes returns irrelevant files among the top
kmost relevant source code files. When there are no relevant
files in the first kfiles, it hinders RLocator training using
developer feedback and introduces noise. Therefore, we use an
XGBoost-based binary classifier [33] to identify cases where
ES may return no relevant files in the top kfiles. The rationale
for using XGBoost is twofold: (1) to optimize developer time
by not presenting irrelevant files and (2) to filter out noise
during training.
ES-based filtering is not used because its similarity values
are not normalized, and cosine similarity is inapplicable to text
data. We provide the XGBoost model with the bug report and
the top kfiles retrieved by ES to determine if any are relevant.
If the XGBoost model predicts no relevant files in the set, weexclude those bug reports and their associated files. Each bug
report is associated with its unique set of source code files, so
filtering one does not impact others.
To build the model, we study the most important features
associated with the prediction task. We consult the related
literature on the field of information retrieval [35], [38], [36]
and bug report classification [34] for feature selection. The list
of computed features is presented in Table I. For our dataset,
we calculate the selected features and trained the model using
10-fold cross-validation. The results show that our classifier
model has a precision of 0.78, a recall of 0.93, and an F1-score
of 0.85. Additionally, the model is able to correctly classify
91% of the dataset (there will be relevant source code files in
the top kfiles returned by ES).
After filtration, we pass each bug report and its source
code files to RLocator. In Figure 1, we have depicted the
operational procedure of RLocator. The workflow commences
with a curated dataset containing bug reports and source code
files. Subsequently, we index the source code files into the
ES index. From ES, we obtain bug reports and shortlisted
Ksource code files linked to those bug reports. Following
this shortlisting, we employ the XGBoost model to predict
the presence of a relevant file within the top Kfiles. If at
least one relevant file exists, we proceed to the next step by
passing the bug reports and filtered source code files.

--- PAGE 5 ---
5
TABLE I
DESCRIPTION AND RATIONALE OF THE SELECTED FEATURES .
Feature Description Rationale
Bug Report Length Length of the bug report. Fan et al. [34] found that
it is hard to localize bugs
using a short bug report.
A short bug report will
contain little information
about the bug. Thus it will
be hard for the Elastic-
Search to retrieve source
code file responsible for
this bug.
Source Code Length Median length of the
source code files
associated with a
particular bug. Note
that we calculate the
string length of the source
code files after removing
code comments.Prior studies [35], [36]
found that calculating tex-
tual similarity is challeng-
ing for long texts. Length
of source code may con-
tribute to the performance
drop of ElasticSearch.
Stacktrace Availability of stack trace
in bug report.Schroter et al. [37] found
that stacktraces in bug re-
ports can help the de-
bugging process as they
may contain useful in-
formation. Availability of
stacktraces may improve
the performance of Elas-
ticSearch.
Similarity Ratio of similar tokens be-
tween a bug report and
source code filesSimilarity indicates the
amount of helpful infor-
mation in the bug report.
We calculate the similar-
ity based on the equation
presented in Section VI.
B. Formulation of RLocator
In the previous step, we have pre-processed the dataset for
training the reinforcement learning model. We shortlist kmost
relevant files for each bug report. After that, we identify the
bug reports for which there will be no relevant files in top
kfiles and filter out those bug reports. Finally, we pass the
topkrelevant files to RLocator. In this section, we explain
how RLocator employs Reinforcement Learning for its bug
localization. Our approach is grounded in the belief that each
bug report contains specific indicators, such as terms and
keywords, aiding developers in pinpointing problematic source
code files. For example, in Java code, we can get a nested
exception (the exception that leads to another exception, and
another; an example is available in the online Appendix [39]).
A developer can identify the root exception and which method
call (or any other code block) caused that. After that, they can
go for the implementation of that method in the source code
files. The process indicates that developers can identify and
use the important information from a bug report. Following
prior studies [40], [41], we formulate RLocator into a Markov
Decision Process (MDP) by dividing the ranking problem into
a sequence of decision-making steps. A general RL model can
be represented by a tuple ⟨S, A, τ, ℜ, π⟩, which is composed
of states, actions, transition, reward, and policy, respectively.
Figure 1 shows an overview of our RLocator approach. Next,we describe the formulation steps of each component of
RLocator.
States: Sis the set of states. The RL model moves from one
state to another state until it reaches the end state. To form
the states of the MDP, we apply the following steps:
Input:
The input of our MDP comprises a bug report and the
topKrelevant source code files from a project repository.
We use CodeBERT [28], a transformer-based model, to con-
vert text into embeddings, representing the text in a multi-
dimensional space. CodeBERT is chosen for its ability to
handle long contexts, making it suitable for long source
code files where methods may be declared far from their
usage. Unlike Word2Vec, which generates static embeddings
for words, CodeBERT generates dynamic embeddings for
sequences, capturing context during inference. This is crucial
in source code files where variable use depends on scope.
CodeBERT, trained on natural language and programming
language pairs, handles both programming and natural lan-
guages. Its self-attention mechanism assesses the significance
of individual terms, helping link bug reports to relevant source
code files. For example, in a Java nested exception, developers
can identify the main exception and pinpoint the responsible
code block. RLocator relies on this self-attention mechanism
to identify and leverage these informative cues effectively.
In our approach, as shown in Figure 1, the embedding
model processes bug reports and source code files, generating
embeddings for the source code files F1, F2, ..., F k, and the
bug report R.
Concatenation: After we obtain the embedding for the
source codes and the bug report, we concatenate them. As
prior studies [42], [43] suggest combining distinct sets of
features through concatenation and processing them with a
linear layer enables effective interaction among the features.
Furthermore, feature interaction is fundamental in determining
similarity [44], [45]. Thus, with the goal of calculating the
similarity between a bug report and a source code file pair, we
concatenate their embedding. Given our example in Figure 1,
we concatenate the embedding of F1, F2, ..., and Fkwith the
embedding of bug report Rindependently. This step leads us to
obtain the corresponding concatenated embedding E1, E2, ...,
andEk, as shown in Figure 1.
Note that each state of the MDP comprises two lists: a
candidate list and a ranked list. The candidate list contains
the concatenated list of embedding. As shown in our example
in Figure 1, the candidate list contains E1, E2, ..., and Ek.
In the candidate list, source code embeddings (code files
and bug reports embedding concatenated together) are ranked
randomly. The other list is the ranked list of source code
files based on their relevance to the bug report R. Initially (at
State 1), the candidate list is full, and the ranked list is empty.
In each state transition, the model moves one embedding from
the candidate list to the ranked list based on their probability
of being responsible for a bug. In the final state, the ranked list
will be full, and the candidate list will be empty. We describe
the process of selecting and ranking a file in detail in the next
step.
Actions: We define Actions in our MDP as selecting a file from

--- PAGE 6 ---
6
the candidate list and moving it to the ranked list. Suppose at
the timestep t; the RL model picks the embedding E1, then
the rank of that particular file will be t. In Figure 1 at the
timestamp 1, the model picks concatenated embedding of file
F2. Thus, the rank of F2will be 1. As in each timestamp,
we are moving one file from the candidate list to the ranked
list; the total number of files will be equal to the number of
states and the number of actions. For identifying the potentially
best action at any timestamp t, we use a deep learning (DL)
model (indicated as Ranking Model in Figure 1), which is
composed of a Convolutional Neural Network (CNN) followed
by a Long Short-Term Memory (LSTM) [46]. Following [47],
[48], [49], we use CNN to establish the connection between
source code files and bug reports and extract relevant fea-
tures. As mentioned earlier, developers acquire the ability to
recognize cues and subsequently employ them to establish the
association between source code files and bug reports. The
CNN facilitates the second stage of bug localization, which
involves extracting important features. The input of the CNN
is the concatenated embedding of both bug reports and each
source code file, and the output of CNN is extracted features
from the combined embedding of bug reports and source code
files. The features are later used to calculate relevance.
On the other hand, LSTM [50] intends to make the model
aware of a restriction, which we call state awareness . That is,
in each timestamp, the model is allowed to pick the potentially
best embedding that has not been picked yet, i.e., if a file is
selected at State i, it cannot be selected again in a later state
(i.e.,State i+j;j≥1). The LSTM retains the state and aids the
RL agent in choosing a subsequent action that does not conflict
with prior actions. Thus, following previous studies [51], [52],
we use an LSTM to make the model aware of previous actions.
The LSTM takes a set of feature vectors as input and outputs
the id of the source code file most suitable for the current
state.
Transition: τ(S, A) is a function τ: S × A→S which maps
a state stinto a new state st+1in response to the selected
action at. Choosing an action atmeans removing a file from
the candidate list and placing it in the ranked list.
Reward: A reward is a value provided to the RL agent as feed-
back on their action. We refer to a reward received from one
action as return . The RL technique signals the agent about the
appropriate action in each step through the Reward Function ,
which can be modeled using the retrieval metrics. Thus, the
RL agent can learn to optimize the retrieval metrics through
the reward function. We consider two important factors in the
ranking evaluation: the position of the relevant files and the
distance between relevant files in the ranked list of embedding.
We incorporated both factors in designing the reward function
shown below.
ℜ(S, A) =M∗file relevance
log2(t+ 1)∗distance (s);if A is
an action that has not been selected before
(1)
ℜ(S, A) =−log2(t+ 1); otherwise (2)
0 1000 2000 3000 4000 5000 6000
Episode−40−30−20−100RewardM = 1 
M = 3
M = 6
M = 9Fig. 2. Effect of M in the reward-episode graph.
distance (S) =Avg.(Distance between currently
picked subsequent related files )(3)
In Equations 1 and 2, tis the timestamp, Sis State and
Ais Action. Mean reciprocal rank (MRR) measures the
average reciprocal rank of all the relevant files. In Equation 1,
file relevance
log2(t+1)represents the MRR. The use of a logarithmic
function in the equation is motivated by previous studies [53],
[54], which found that it leads to a stable loss. When the
relevant files are ranked higher, the average precision tends to
be higher. To encourage the reinforcement learning system
to rank relevant files higher, we introduce a punishment
mechanism if there is a greater distance between two relevant
files. By imposing this punishment on the agent, we incentivize
it to prioritize relevant files in higher ranks, which in turn
contributes to the Mean Average Precision (MAP).
We illustrate the reward functions with an example below.
Presuming that the process reaches State S6and the currently
picked concatenated embeddings are E1, E2, E3, E4, E5, E6
and their relevancy to the bug report is ⟨0,0,1,0,1,1⟩. This
means that these embeddings (or files) ranked in the 3rd,5th,
and6thpositions are relevant to the bug report. The position
of the relevant files are ⟨3,5,6⟩, and the distance between
them is ⟨1,0⟩. Hence, distance (S6) =Avg.⟨1,0⟩= 0.5. If
the agent picks a new relevant file, we reward the agent M
times the reciprocal rank of the file divided by the distance
between the already picked related files. In our example,
the last picked file, E6’s relevancy is 1. Thus, we have
the following values for Equation 1: distance (S6) = 0 .5;
log2(6 + 1) = 2 .8074 ;file relevance = 1. Note that M
is a hyper-parameter. We find that three as the value of M
results in the highest reward for our RL model. We identify
the best value for Mby experimenting with different values (1,
3, 6, and 9). Figure 2 shows the resulting reward-episode graph
using different values of M. Hence, given M= 3, the value
of the reward function will be ℜ(S, A) =3∗1
2.8074∗0.5= 2.14.
The reward can vary between Mto∼0. A higher value of
the reward function indicates a better action of the model.
Finally, in the case of optimal ranking, the distance (S)will
be zero. We handle this case by assigning a value of 1for
distance (S). Even though we are using MRR and MAP as
optimization goals we do not require labeled data. Instead, it
learns from developers’ feedback. It presents a limited set of

--- PAGE 7 ---
7
files to the developer, seeking their feedback. If a developer
deems a specific file as relevant, they can click on it. This click
feedback signifies the file’s relevance within the set. RLocator
leverages this input at Equation 1 and learns the process of bug
localization. Incorporating developers’ feedback may cause
some inconvenience. However, all machine learning models
are prone to data drift [55], [56], [57], where initial training
data no longer matches current data, leading to declining per-
formance. RLocator addresses this by continuously updating
its learning based on developers’ feedback.
We limit the number of actions to 31 in RLocator. Since
the number of states equals the number of actions, we also
limit the number of states to 31. The prediction space of
a reinforcement learning agent cannot be variable, and the
number of source code files is variable. Thus, we must fix
the number of actions, k, to a manageable number that fits in
memory. We use an Nvidia V100 16 GB GPU and found that
with more than 31 actions, training scripts fail due to out-of-
memory errors. Therefore, we set K= 31 to keep the state
size under control. As mentioned in Section IV-A, we select
the top 31 relevant source code files from ES and pass them
to RLocator, ensuring the number of states and files remains
the same.
C. Developers’ Workflow
Fig. 3. Developer interaction flow.
Figure 3 illustrates the interaction flow of developers using
RLocator, which is represented as a central black box in
the diagram. Details of RLocator are presented in Figure 1.
The process begins with RLocator receiving two primary
inputs: a bug report and all the source code files, labeled as
1 and 2, respectively, in the figure. After processing these
inputs, RLocator outputs a ranked list of 31 source code
files, indicated by step #4. Developers then review this list
to identify and select files that may contain bugs; an example
is shown when a developer selects File 3, noted as step #5.
This selection serves as feedback to RLocator, marked by
step #6, aiding in refining its bug localization strategy. The
feedback from developers is expressed as a binary value: files
that developers open are marked with a 1, and all other files
are marked with a 0. Additionally, the system can also indicate
its inability to localize a bug for a given report, as shown by
step #3. This ongoing loop allows RLocator to stay updated
with changes in techniques and patterns, enhancing its bug
localization performance.TABLE II
DATASET STATISTICS .
Project# of Bug
ReportsAvg. # of Buggy
Files per Bug
AspectJ 593 4.0
Birt 6,182 3.8
Eclipse UI 6,495 2.7
JDT 6,274 2.6
SWT 4,151 2.1
Tomcat 1,056 2.4
V. D ATASET AND EVALUATION MEASURES
In this section, we discuss the dataset used to train and
evaluate our model (section V-A). Then, we present the
evaluation metrics we use for evaluation (section V-B).
A. Dataset
In our experiment, we evaluate our approach on six real-
world open-source projects [58], which are commonly used
benchmark datasets in bug localization studies [18], [6]. Prior
work has shown that this dataset has the lowest number of
false positives and negatives compared to other datasets [59],
[60], [3], [61], [62]. Following previous studies, we train our
RLocator model separately for each of the six Apache projects
(AspectJ, Birt, Eclipse Platform UI, JDT, SWT, Tomcat).
Table II shows descriptive statistics on the datasets.
The dataset contains metadata such as bug ID, description,
report timestamp, commit SHA of the fixing commit and
buggy source code file paths. Each bug report is associated
with a commit SHA/version, and we use a multiple version
set matching approach to exclusively utilize the source code
files linked to each specific report [59]. This approach closely
resembles the bug localization process done by developers and
reduces noise in the dataset, improving tool performance.
We identify the version containing the bug from the commit
SHA and collect all relevant source code files from that
version, excluding the bug-fixing code. This ensures our bug
localization system closely mimics real-world scenarios.
For training and testing, we use 91% of the data, sorting
the dataset by the date of bug reports and splitting it 60:40 for
training and testing, respectively. Unlike previous studies [63]
that used a 60:20:20 split, we repurpose validation data for
testing to shorten the training duration.
B. Evaluation Measures
The dataset proposed by Ye et al. [58] provides ground truth
associated with each bug report. The ground truth contains the
path of the file in the project repository that has been modified
to fix a particular bug. To evaluate RLocator performance, we
use the ground truth and analyze the experimental results based
on three criteria, which are widely adopted in bug localization
studies [3], [5], [6], [7], [8].
•Mean Reciprocal Rank (MRR): To identify the average
rank of the relevant file in the retrieved files set, we

--- PAGE 8 ---
8
adopted the Mean Reciprocal Rank. MRR is the average
reciprocal rank of the source code files for all the bug
reports. We present the equation for calculating MRR
below, where Ais the set of bug reports.
MRR =1
|A|X
A1
Least rank of the relevant files
(4)
Suppose we have two bug reports, report 1andreport 2.
For each bug report, the bug localization model will rank
six files. For report 1the ground truth of the retrieved
files are [0,0,1,0,1,0]and for report 2the ground truth
of the retrieved files are [1,0,0,0,0,1]. In this case, the
least rank of relevant files is 3 and 1, respectively, for
report 1andreport 2. Now, the MRR =1
2(1
3+1
1) = 0.67
•Mean Average Precision (MAP): To consider the case
where a bug is associated with multiple source code
files, we adopted Mean Average Precision. It provides a
measure of the quality of the retrieval [3], [64]. MRR
considers only the best rank of relevant files; on the
contrary, MAP considers the rank of all the relevant files
in the retrieved files list. Thus, MAP is more descriptive
and unbiased than MRR. Precision means how noisy the
retrieval is. If we calculate the precision on the first two
retrieved files, we will get precision@2. For calculating
average precision, we have to figure precision@1, preci-
sion@2,... precision@k, and then we have to average the
precision at different points. After calculating the average
precision for each bug report, we have to find the mean
of the average precision to calculate the MAP.
MAP =1
|A|X
AAvgPrecision (Report i) (5)
We show the MAP calculation for the previous example
of two bug reports. The Average precision for report 1
andreport 2will be 0.37 and 0.67. So, the MAP =
1
2(0.36 + 0 .67) = 0 .52
•Top K: For fare comparison with prior studies [49], [48]
and to present a straightforward understanding of perfor-
mance we calculate Top K. Top K measures the overall
ranking performance of the bug localization model. It
indicates the percentage of bug reports for which at least
one buggy source appears among the top K positions in
the ranked list generated by the bug localization tool.
Following previous studies (e.g., [49], [48]), we consider
three values of K: 1, 5, and 10.
VI. RL OCATOR PERFORMANCE
We evaluate RLocator on the hold-out dataset using the
metrics described in Section V-B. As there has been no RL-
based bug localization tool, we compare RLocator with three
state-of-the-art bug localization tools: BugLocator, FLIM, and
BL-GAN.
A short description of the approaches is presented below.
•BugLocator [3]: an IR-based tool that utilizes a vector
space model to identify the potentially responsible sourcecode files by estimating the similarity between source
code file and bug report.
•FLIM [18]: a deep-learning-based model that utilizes a
large language model like CodeBERT.
•BL-GAN [7]: uses generative adversarial strategy to train
an attention-based transformer model.
We use the original implementations to assess the perfor-
mance of BugLocator [3] and FLIM [18]. Additionally, we
fine-tune a CodeBERT [28] model as a baseline to demonstrate
the benefits of using reinforcement learning. For tools like
CAST [5], KGBugLocator [6], and BL-GAN [7], which lack
replication packages, we refer to their respective studies. These
studies show that KGBugLocator outperforms CAST, and BL-
GAN outperforms KGBugLocator. Consequently, we replicate
BL-GAN based on its study descriptions.
Regarding FBL-BERT [8], a recent technique, we do not
compare it with RLocator. This is because FBL-BERT per-
forms bug localization at the changeset level, and applying
it to our file-level dataset would disadvantage FBL-BERT, as
it is designed for shorter documents. Therefore, comparing it
with RLocator would be unfair.
Furthermore, other studies, such as DeepLoc [63],
bjXnet [65], CAST [5], KGBugLocator [6], and Cheng et
al. [66], also propose deep learning-based approaches but
do not provide replication packages. Although these studies
evaluate similar projects, the lack of available code or pre-
trained models prevents further comparison. However, to en-
sure comprehensive information, we include a table in our
online appendix [39] displaying their performance alongside
RLocator.
A. Retrieval performance
We use k=31 relevant files in RLocator, allowing us to
rerank files for 91% of the bug reports. Table III shows
RLocator’s performance on 91% and 100% of the data.
RLocator is not designed for 100% data as it cannot rerank
files if no relevant files are in the top kfiles. For such
cases, we estimate performance assuming zero contribution,
providing a lower bound for RLocator’s effectiveness. This
conservative approach ensures we do not overestimate the
technique’s effectiveness. Table III showcases that RLocator
achieves better performance than BugLocator and FLIM
in both MRR and MAP across all studied projects when
using the 91% data. On 91% data, RLocator outperforms
FLIM by 5.56-38.3% in MAP and 3.77-36.73% in MRR.
Regarding Top K, the performance improvement is up to
23.68%, 15.22%, and 11.32% in terms of Top 1, Top 5,
and Top 10, respectively. Compared to BugLocator, RLocator
achieves performance improvement of 12.5 - 56.86% and 9.8%
- 41.51%, in terms of MAP and MRR, respectively. Regarding
Top K, the performance improvement is up to 26.32%, 41.3%,
and 35.85% in terms of Top 1, Top 5, and Top 10, respectively.
The results indicate that RLocator consistently outperforms
BL-GAN in 91% settings across all metrics. Specifically, in the
TopK measurements, RLocator’s performance exceeded that of
BL-GAN, with improvements ranging from 55.26% to 3.33%.
Additionally, RLocator achieved performance gains of 40.74%
in MAP and 32.2% in MRR, respectively.

--- PAGE 9 ---
9
TABLE III
RLOCATOR PERFORMANCE .
Project ModelTop 1 Top 5 Top 10 MAP MRR
91% 100% 91% 100% 91% 100% 91% 100% 91% 100%
AspectJRLocator 0.46 0.40 0.69 0.63 0.75 0.70 0.56 0.46 0.59 0.50
BugLocator 0.36 0.28 0.50 0.45 0.56 0.51 0.33 0.31 0.49 0.48
FLIM 0.51 0.36 0.65 0.60 0.72 0.67 0.41 0.35 0.47 0.45
CodeBERT 0.4 0.35 0.59 0.55 0.65 0.61 0.49 0.39 0.51 0.44
BL-GAN 0.41 0.38 0.6 0.55 0.71 0.65 0.33 0.31 0.42 0.39
BirtRLocator 0.65 0.25 0.46 0.41 0.53 0.48 0.47 0.38 0.49 0.41
BugLocator 0.61 0.15 0.27 0.21 0.34 0.29 0.30 0.30 0.39 0.38
FLIM 0.49 0.18 0.39 0.34 0.47 0.42 0.29 0.25 0.31 0.28
CodeBERT 0.33 0.22 0.39 0.35 0.46 0.43 0.41 0.33 0.42 0.35
BL-GAN 0.17 0.16 0.33 0.3 0.46 0.42 0.32 0.29 0.4 0.37
Eclipse Platform UIRLocator 0.45 0.37 0.69 0.63 0.78 0.73 0.54 0.42 0.59 0.50
BugLocator 0.45 0.33 0.54 0.49 0.63 0.58 0.29 0.30 0.38 0.35
FLIM 0.48 0.41 0.72 0.67 0.80 0.75 0.51 0.48 0.52 0.53
CodeBERT 0.39 0.32 0.6 0.55 0.68 0.62 0.47 0.36 0.52 0.44
BL-GAN 0.34 0.31 0.53 0.49 0.66 0.61 0.32 0.3 0.4 0.36
JDTRLocator 0.44 0.33 0.67 0.61 0.78 0.75 0.51 0.44 0.53 0.45
BugLocator 0.34 0.21 0.51 0.45 0.60 0.55 0.22 0.20 0.31 0.28
FLIM 0.40 0.35 0.65 0.60 0.82 0.77 0.42 0.41 0.51 0.49
CodeBERT 0.38 0.29 0.59 0.54 0.68 0.66 0.44 0.38 0.46 0.39
BL-GAN 0.3 0.27 0.53 0.48 0.64 0.59 0.35 0.32 0.44 0.41
SWTRLocator 0.40 0.30 0.57 0.51 0.63 0.58 0.48 0.42 0.51 0.44
BugLocator 0.37 0.25 0.50 0.45 0.56 0.51 0.42 0.40 0.46 0.43
FLIM 0.51 0.37 0.70 0.65 0.83 0.78 0.43 0.43 0.48 0.50
CodeBERT 0.34 0.27 0.5 0.45 0.54 0.51 0.42 0.37 0.45 0.39
BL-GAN 0.31 0.29 0.53 0.48 0.6 0.55 0.37 0.34 0.44 0.4
TomcatRLocator 0.46 0.39 0.61 0.55 0.73 0.68 0.59 0.47 0.62 0.51
BugLocator 0.40 0.29 0.43 0.38 0.55 0.50 0.31 0.27 0.37 0.35
FLIM 0.51 0.42 0.70 0.65 0.76 0.71 0.52 0.47 0.59 0.60
CodeBERT 0.39 0.34 0.53 0.49 0.62 0.6 0.51 0.41 0.53 0.44
BL-GAN 0.38 0.35 0.61 0.55 0.65 0.61 0.43 0.4 0.55 0.5
The results point out that RLocator outperforms BL-GAN
across all the metrics in 91% settings. Specifically, in TopK,
RLocator achieved better performance than BL-GAN, ranging
from 3.33% to 55.26%. The performance gain is 40.74% and
32.2% for MAP and MRR, respectively.
Compared to the CodeBERT model trained as a classifier
(CodeBERT), RLocator achieves better performance across
all the metrics. CodeBERT model archives consistently lower
performance across all the metrics. The performance drops up
to 17.65%, 15.63%, 17.95%, 17.14%, and 16.67% for Top1,
Top5, Top 10, MAP, and MRR, respectively.
When we consider 100% of the data, RLocator has better
MAP results than FLIM in three out of the six projects (As-
pectJ, Birt, and JDT) by 6.82-34.21%, equal to FLIM in one
project (Tomcat) and worse than FLIM in 2 projects (Eclipse
Platform UI and SWT) by 2-14%. The underperformance ofRLocator in the Eclipse Platform UI and SWT projects can
be linked to the poor and inconsistent quality of bug reports,
which creates a significant lexical gap between the reports and
the source code. By applying the IMaChecker [67] approach,
we discovered that AspectJ reports are of the highest quality,
whereas those for Eclipse Platform UI, SWT, and Tomcat
rank among the lowest. For a detailed analysis of bug report
quality, please refer to the online Appendix [39]. In terms of
MRR, RLocator is better than FLIM in 2 projects (AspectJ and
Birt) by 10-31.71% and worse than FLIM in the remaining
four projects (Eclipse platform UI, JDT, SWT, and Tomcat)
by 6-18%. In terms of Top K, RLocator ranks 4.29-12.5%
more bugs in the top 10 positions than FLIM in two projects.
On the other hand, in the rest of the four projects, FLIM
ranks more bugs in the top 10 positions, ranging between
2.74 - 34.48%. When comparing RLocator with BugLocator

--- PAGE 10 ---
10
for the 100% data along MAP, we find that the RLocator
is better in five of the six projects and similar in just the
Tomcat project. With respect to MRR, RLocator is better than
BugLocator in all six projects. In terms of Top K, RLocator
ranks more bugs than BugLocator in the top 10 position,
where the improvement ranges between 12.07 -39.58%. The
results demonstrate that RLocator consistently surpasses BL-
GAN in all metrics in the 100% setting. Specifically, in the
TopK metric, RLocator’s performance was better than BL-
GAN, with improvements ranging from 3.33% to 36%. The
performance enhancements for RLocator are 32% in MAP and
1.96% in MRR, respectively.
It is important to note that MAP provides a more balanced
view than MRR and top K since it accounts for all the files that
are related to a bug report and not just one file. Additionally,
in our technique, we optimize to give more accurate results
for most of the bug reports than give less accurate results on
average for all the bug reports. Thus, by looking at the MAP
data for the 91%, we can see that RLocator performs
better than the state-of-the-art techniques in all projects.
Even if we consider 100% of the data, RLocator is still
better than other techniques in the majority of the projects.
Only with 100% of the data and when using MRR as the
evaluation metric, RLocator does not perform better than the
state-of-the-art in most projects.
RLocator performs the worst in the Birt project, with a
performance drop of 10.47% in MAP, 11.71% in MRR, and
41.42% in the top 10 compared to its average on 91% of the
data. Despite this, RLocator outperforms FLIM by 38.3% in
MAP, 36.73% in MRR, and 11.32% in the top 10. It also
surpasses BugLocator by 36.17% in MAP, 20.41% in MRR,
and 35.85% in the top 10. Factors like bug report quality,
amount of information in the bug report, and source code
length may contribute to the performance drop. We measure
helpful information in bug reports using a similarity metric,
which calculates the ratio of similar tokens between source
code files and bug reports, indicating the potential usefulness
of the report for bug localization. The metric is defined in
equation 6.
Similarity =Bug Report Tokens ∩File Tokens
#of Unique Tokens in Bug Report(6)
The median similarity scores for the Birt, Eclipse Platform
UI, and SWT projects are 0.29, 0.30, and 0.33, respectively,
making them the lowest among the six projects. This observa-
tion suggests that the lower quality of bug reports (reflected in
their similarity to source files) may contribute to the decreased
performance of RLocator in these projects.
To effectively use RLocator in real-world scenarios, we
employ an XGBoost model (Section IV-A) to filter out bug
reports where relevant files do not appear in the top K(=31)
files. We then compute the importance of features listed in
Table I using XGBoost’s built-in module. The importance
score indicates each feature’s contribution to the model, with
higher values signifying greater importance. Figure 4 shows
that similarity is the most crucial feature, followed by source
code length and Bbg report length. These findings highlight
0 25 50 75 100 125 150 175 200
W eightSimilaritySource  Code LengthBug R eport LengthStackT raceF eatures
2141941877Fig. 4. Feature importance of classifier model.
the importance of similarity in text-based search systems and
suggest that high-quality bug reports can influence localization
performance.
B. Entropy Ablation Analysis: Impact of Entropy on RLocator
performance
We conduct an ablation study to gain insights into the
significance of each component of RLocator. The main two
components of RLocator are the ES-based shortlisting step
and the reinforcement learning step.
In RL, we have used the A2C with entropy algorithm. Entropy
refers to the unpredictability of an agent’s actions. A low
entropy indicates a predictable policy, while high entropy
represents a more random and robust policy. An agent in RL
will tend to repeat actions that previously resulted in positive
rewards while learning the policy. The agent may become
stuck in a local optimum due to exploiting learned actions
instead of exploring new ones and finding a higher global
optimum. This is where entropy comes useful: we can use
entropy to encourage exploration and avoid getting stuck in
local optima [68]. Because of this, entropy in RL has become
very popular in the design of RL approaches such as A2C [69].
In our proposed model (Section VI-A), we use A2C with
entropy to train the RLocator aiming to rank relevant files
closer to each other. As entropy is part of the reward, the
gradient descent process will try to maximize the entropy.
Entropy will increase if the model identifies different actions as
the best in the same state. However, those actions must select a
relevant file; otherwise, the reward will be decreased. Thus, if
there are multiple relevant files in a state, the A2C with entropy
regularized model will assign almost the same probability in
those actions (actions related to selecting those relevant files).
This means that when the states are repeated, a different action
will likely be selected each time. This probability assignment
will lead to a higher MAP.
The observed performance of RLocator in achieving higher
MAP can be interpreted due to two factors: 1) the way we
design our reward function, given that we define a function that
aims to encourage higher MAP; 2) the inclusion of entropy,
as entropy regularization is assumed to enable the model to
achieve higher MAP.
Hence, to provide a better understanding of our model, we
measure the performance of three different steps of our model
separately. The ES-based shortlisting step, the A2C-based RL

--- PAGE 11 ---
11
TABLE IV
RLOCATOR PERFORMANCE WITH AND WITHOUT ENTROPY FOR A2C.
Project Model Top 1 Top 5 Top 10 MAP MRR
AspectJES 0.15 0.20 0.28 0.23 0.27
A2C 0.27 0.39 0.48 0.40 0.52
A2C with Entropy 0.46 0.69 0.75 0.56 0.59
BirtES 0.10 0.14 0.17 0.18 0.23
A2C 0.21 0.30 0.43 0.31 0.42
A2C with Entropy 0.38 0.46 0.53 0.47 0.49
Eclipse
Platform UIES 0.09 0.15 0.19 0.25 0.31
A2C 0.25 0.38 0.51 0.39 0.51
A2C with Entropy 0.45 0.69 0.78 0.54 0.59
model (without entropy), and the A2C with entropy model.
Due to resource (time and GPU) limitations, we limit our
evaluation to half of the total projects in our dataset, i.e.,
AspectJ, Birt, and Eclipse Platform UI. We observe a similar
trend in those three projects. Thus, we believe our results will
follow a similar trend in the remaining projects.
Table IV presents the performance of the three choices (i.e.,
ES, A2C only, and A2C with Entropy). Table IV shows that ES
archives the baseline performance, which is 53-61% lower than
the A2C with entropy model in terms of MAP and 47-54%
lower in terms of MRR. We also find that the MRR and MAP
of the models without entropy are lower than those of the A2C
with entropy models. Table IV shows that in terms of MAP, the
performance of A2C with entropy models is higher than A2C
models by a range of 27.78-34.04%. In MRR and the top 10,
the A2C with entropy model achieves higher performance by
a range of 11.86-13.56% and 18.87 - 36%, respectively. Such
results indicate that entropy could substantially contribute to
the model performance regarding MAP, MRR, and Top K.
Moreover, this shows that the use of entropy encourages the
RL agent to explore possible alternate policies [70]; thus, it
has a higher chance of getting a better policy for solving the
problem in a given environment than the A2C model.
VII. R ELATED WORK
The work most related to our study falls into studies on
bug localization techniques. In the following, we discuss the
related work and reflect on how the work compares with ours.
A plethora of work studied how developers localize
bugs [71], [72], [73], [74]. For example, B ¨ohme et al. [71]
studied how developers debug. They found that the most
popular technique for localizing a bug is forward reasoning,
where developers go through each computational step of a
failing test case to identify the location. Zimmermann et
al. [74] studied the characteristics of a good bug report and
found that test cases and stack traces are one of the most
important criteria that makes a good bug report. While these
studies focused on developers’ manual localization of bugs,
our approach examines the automation of the bug localization
process for developers.
Several studies offered test case coverage-based solutions
for bug localization [75], [76], [77]. Vancsics et al. [75]proposed a count-based spectrum instead of a hit-based spec-
trum in the Spectrum-Based Fault Localization (SBFL) tool.
GRACE [76] proposed gated graph neural network-based rep-
resentation learning to improve the SBFL technique. However,
these studies mainly utilize test cases to localize bugs, whereas
our approach (RLocator) focuses on the bug report for bug
localization.
There were several efforts to assess the impact of query
reformulation in improving the performance of existing bug
localization tools. [78], [79]. For example, Rahman et al. [78]
found that instead of using the full bug report as a full-text
query, a reformulated query with some additional expansion
performs better. BugLocator [3] used a revised Vector Space
Model (rVSM) to estimate the textual similarity between bug
reports and source code files.
A few studies incorporated the information of program
structures such as the Program Dependence Graph (PDG),
Data Flow Graph (DFG) [80], and Abstract Syntax Tree (AST)
for learning source code representation [4], [63], [80], [5],
[65]. For example, CAST [5] used AST of the source code
to extract the semantic information and then used Word2Vec
to project the source code and the bug report in the same
embedding space. They used a CNN model that measures the
similarity between a bug report and source code. The model
ranks the file based on the calculated similarity. Hyloc [9]
incorporated the techniques of IR-based bug localization with
deep learning. It concatenated the TF-IDF vector of the source
code with repository and file-level metadata.
Other studies applied several deep learning-based ap-
proaches for bug localization [81], [6], [65]. DEMOB [81]
used attention on ELMo [82] embedding, whereas KGBugLo-
cator [6] used attention on graph embedding. BL-GAN [7]
offered a generative adversarial network (GAN) based solution
for bug localization. GAN is often seen as a methodology
closely related to reinforcement learning, but it diverges from
the typical use of the Markov decision process (MDP), a
fundamental aspect of reinforcement learning [83], [14]. Ad-
ditionally, BL-GAN has limitations in actively learning bug
localization from developers’ real-time actions. In contrast,
we have incorporated developers’ feedback directly into the
reward function, allowing RLocator to learn from developers’
actions. Xie et al. [84] employed GAN to create failing test
cases, addressing the data imbalance issue within fault local-
ization methods. White et al. [85] utilized reinforcement learn-
ing for fault localization in distributed networks. Nonethe-
less, their approach involves the reinforcement learning agent
understanding how to interact with the network which is
different from our approach. In our approach, the agent learns
to localize bugs from developers’ activity. Rezapour et al. [86]
have provided a thorough exploration of reinforcement learn-
ing’s application in fault localization within power systems.
However, their discussed approaches significantly differ from
ours. In the context of power systems, the reinforcement
learning agent can directly observe phenomenal components
(e.g., current, voltage) related to the environment. Moreover,
the agents are allowed to probe the environment by changing
current or voltage. In contrast, bug localization entails more
abstract phenomenal components in the environment (e.g.,

--- PAGE 12 ---
12
interacting code blocks) and agents are not allowed to change
any code or execute the code. Other studies focused on
associating commits with bug reports [87], [8]. For example,
FBL-BERT [8] used CodeBERT embedding for estimating
the similarity between source code files and changesets of
a commit. Based on the similarity, it ranks the suspicious
commit. FLIM [18] also used CodeBERT embedding for
estimating similarity. However, FLIM works on function-level
bug localization.
Our approach, RLocator, uses deep reinforcement learning
for bug localization, differing from previous similarity-based
methods. By formulating the problem as a Markov Decision
Process (MDP), we directly optimize evaluation measures.
Testing on a dataset of 8,316 projects from six popular
Apache projects, our results show significant performance
improvement.
VIII. T HREATS TO VALIDITY
RLocator has a number of limitations as well. We identify
them and discuss how to overcome the limitations below.
Internal Validity. One limitation of our approach is we are
not able to utilize 9% of our dataset due to the limitation of
text-based search. One may point out that we exclude the bug
reports where we do not perform well. But our the XGBoost
model in our approach automatically identifies them and we
say that we would rather not localize the source code files
for these bug reports than localize them incorrectly. Hence,
developers need to rely on their manual analysis only for the
9%. Moreover, as a measure of full transparency, we estimate
the lower bound of RLocator performance for the 100% data
and show that the difference is negligible.
External Validity. The primary concern for the external
validity of the RLocator evaluation stems from its limitation
to a small number of bugs in six varied, real-world open-
source projects, potentially impacting its broad applicability.
However, those projects are from different domains and used
by prior studies [4], [5], [81], [6], [9], [88]. Furthermore,
the A2C without entropy model was only evaluated on three
projects because of the substantial resources required for
training—taking about four days on an Nvidia V100 16GB
GPU. The uniform outcomes across these projects indicate
that similar results could be expected in the remaining
projects. Additionally, due to the absence of a replication
package, we replicated BL-GAN based on its description
in the original study, which may lead to slight performance
deviations. Nevertheless, after experimenting with various
hyperparameters, we selected a set that achieves comparable
performance to that reported in the original study.
Construct Validity. Finally, our evaluation measures might
be one threat to construct validity. The evaluation measures
may not completely reflect real-world situations. The threat
is mitigated by the fact that the used evaluation measures are
well-known [8], [18], [58], [3] and best available to measure
and compare the performance of information retrieval-based
bug localization tools.IX. C ONCLUSION
In this paper, we propose RLocator, a reinforcement
learning-based (RL) technique to rank the source code files
where the bug may reside, given the bug report. The key
contribution of our study is the formulation of the bug local-
ization problem using the Markov Decision Process (MDP),
which helps us to optimize the evaluation measures directly.
We evaluate RLocator on 8,316 bug reports and find that
RLocator performs better than the state-of-the-art techniques
when using MAP as an evaluation measure. Using 91% bug
reports dataset, RLocator outperforms prior tools in all the
project in terms of both MAP and MRR. When using 100%
data, RLocator outperforms all prior approaches in four of
six projects using MAP and two of the six projects using
MRR. RLocator can be used along with other bug localization
approaches to improve performance. Our results show that RL
is a promising avenue for future exploration when it comes
to advancing state-of-the-art techniques for bug localization.
Future research can explore the application of advanced rein-
forcement learning algorithms in bug localization. Addition-
ally, researchers can investigate how training on larger datasets
impacts the performance of tools in low-similarity contexts.
X. D ATA AVAILABILITY
To foster future research in the field, we make a replication
package comprising our dataset and code are publicly avail-
able [39].
REFERENCES
[1] T. D. LaToza and B. A. Myers, “Developers ask reachability questions,”
inProceedings of the 32nd ACM/IEEE International Conference on
Software Engineering - ICSE '10 . ACM Press, 2010.
[2] J. Anvik, L. Hiew, and G. C. Murphy, “Coping with an open bug
repository,” in Proceedings of the 2005 OOPSLA workshop on Eclipse
technology eXchange - eclipse '05 . ACM Press, 2005.
[3] J. Zhou, H. Zhang, and D. Lo, “Where should the bugs be fixed?
more accurate information retrieval-based bug localization based on bug
reports,” in 2012 34th International Conference on Software Engineering
(ICSE) . IEEE, Jun. 2012.
[4] Y . Xiao, J. Keung, K. E. Bennin, and Q. Mi, “Machine translation-
based bug localization technique for bridging lexical gap,” Information
and Software Technology , vol. 99, pp. 58–61, Jul. 2018.
[5] H. Liang, L. Sun, M. Wang, and Y . Yang, “Deep learning with cus-
tomized abstract syntax tree for bug localization,” IEEE Access , vol. 7,
pp. 116 309–116 320, 2019.
[6] J. Zhang, R. Xie, W. Ye, Y . Zhang, and S. Zhang, “Exploiting code
knowledge graph for bug localization via bi-directional attention,” in
Proceedings of the 28th International Conference on Program Compre-
hension . ACM, Jul. 2020.
[7] Z. Zhu, H. Tong, Y . Wang, and Y . Li, “BL-GAN: Semi-supervised bug
localization via generative adversarial network,” IEEE Transactions on
Knowledge and Data Engineering , pp. 1–14, 2022.
[8] A. Ciborowska and K. Damevski, “Fast changeset-based bug localization
with BERT,” in Proceedings of the 44th International Conference on
Software Engineering . ACM, May 2022.
[9] A. N. Lam, A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen, “Combining
deep learning with information retrieval to localize buggy files for
bug reports (n),” in 2015 30th IEEE/ACM International Conference on
Automated Software Engineering (ASE) . IEEE, Nov. 2015.
[10] Z. Wei, J. Xu, Y . Lan, J. Guo, and X. Cheng, “Reinforcement learning
to rank with markov decision process,” in Proceedings of the 40th
International ACM SIGIR Conference on Research and Development
in Information Retrieval . ACM, Aug. 2017.

--- PAGE 13 ---
13
[11] O. Alejo, J. M. Fernandez-Luna, J. F. Huete, and R. Perez-Vazquez,
“Direct optimization of evaluation measures in learning to rank using
particle swarm,” in 2010 Workshops on Database and Expert Systems
Applications . IEEE, Aug. 2010.
[12] J. Xu, L. Xia, Y . Lan, J. Guo, and X. Cheng, “Directly optimize diversity
evaluation measures,” ACM Transactions on Intelligent Systems and
Technology , vol. 8, no. 3, pp. 1–26, Jan. 2017.
[13] Y . Yue, T. Finley, F. Radlinski, and T. Joachims, “A support vector
method for optimizing average precision,” in Proceedings of the 30th
annual international ACM SIGIR conference on Research and develop-
ment in information retrieval . ACM, Jul. 2007.
[14] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction .
Cambridge, MA, USA: A Bradford Book, 2018.
[15] F. Garcia and E. Rachelson, “Markov decision processes,” in Markov
Decision Processes in Artificial Intelligence . John Wiley & Sons, Inc.,
Mar. 2013, pp. 1–38.
[16] “Gdi: Rethinking what makes reinforcement learning different from
supervised learning,” 2021.
[17] J. Kober, J. A. Bagnell, and J. Peters, “Reinforcement learning in
robotics: A survey,” The International Journal of Robotics Research ,
vol. 32, no. 11, pp. 1238–1274, Aug. 2013.
[18] H. Liang, D. Hang, and X. Li, “Modeling function-level interactions for
file-level bug localization,” Empirical Software Engineering , vol. 27,
no. 7, Oct. 2022.
[19] L. Maystre, D. Russo, and Y . Zhao, “Optimizing audio recommendations
for the long-term: A reinforcement learning perspective,” 2023.
[20] M. Chen, A. Beutel, P. Covington, S. Jain, F. Belletti, and E. Chi, “Top-k
off-policy correction for a reinforce recommender system,” 2018.
[21] C. Yu, J. Liu, S. Nemati, and G. Yin, “Reinforcement learning in
healthcare: A survey,” ACM Computing Surveys , vol. 55, no. 1, p. 1–36,
Nov. 2021.
[22] E. Winter, D. Bowes, S. Counsell, T. Hall, S. Haraldsson, V . Nowack,
and J. Woodward, “How do developers really feel about bug fixing?
directions for automatic program repair,” IEEE Transactions on Software
Engineering , vol. 49, no. 04, pp. 1823–1841, apr 2023.
[23] S. Wang, T. Liu, and L. Tan, “Automatically learning semantic features
for defect prediction,” in Proceedings of the 38th International Confer-
ence on Software Engineering . ACM, May 2016.
[24] N. Miryeganeh, S. Hashtroudi, and H. Hemmati, “GloBug: Using global
data in fault localization,” Journal of Systems and Software , vol. 177, p.
110961, Jul. 2021.
[25] Y . Kim, M. Kim, and E. Lee, “Feature combination to alleviate hubness
problem of source code representation for bug localization,” in 2020
27th Asia-Pacific Software Engineering Conference (APSEC) . IEEE,
Dec. 2020.
[26] L. Chen, Z. Tang, and G. H. Yang, “Balancing reinforcement learning
training experiences in interactive information retrieval,” in Proceedings
of the 43rd International ACM SIGIR Conference on Research and
Development in Information Retrieval . ACM, Jul. 2020.
[27] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-
training of deep bidirectional transformers for language understanding,”
inProceedings of the 2019 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) . Minneapolis,
Minnesota: Association for Computational Linguistics, Jun. 2019, pp.
4171–4186.
[28] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin,
T. Liu, D. Jiang, and M. Zhou, “CodeBERT: A pre-trained model for
programming and natural languages,” in Findings of the Association
for Computational Linguistics: EMNLP 2020 . Online: Association for
Computational Linguistics, Nov. 2020, pp. 1536–1547.
[29] J. Wang and Y . Dong, “Measurement of text similarity: A survey,”
Information , vol. 11, no. 9, p. 421, Aug. 2020.
[30] S. Fujimoto, D. Meger, and D. Precup, “Off-policy deep reinforcement
learning without exploration,” in Proceedings of the 36th International
Conference on Machine Learning , ser. Proceedings of Machine Learning
Research, K. Chaudhuri and R. Salakhutdinov, Eds., vol. 97. PMLR,
09–15 Jun 2019, pp. 2052–2062.
[31] T. T. Nguyen and V . J. Reddi, “Deep reinforcement learning for cyber
security,” IEEE Transactions on Neural Networks and Learning Systems ,
pp. 1–17, 2021.
[32] C. Liu, X. Xia, D. Lo, Z. Liu, A. E. Hassan, and S. Li, “CodeMatcher:
Searching code based on sequential semantics of important query
words,” ACM Transactions on Software Engineering and Methodology ,
vol. 31, no. 1, pp. 1–37, Jan. 2022.[33] T. Chen and C. Guestrin, “XGBoost,” in Proceedings of the 22nd ACM
SIGKDD International Conference on Knowledge Discovery and Data
Mining . ACM, Aug. 2016.
[34] F. Fang, J. Wu, Y . Li, X. Ye, W. Aljedaani, and M. W. Mkaouer,
“On the classification of bug reports to improve bug localization,” Soft
Computing , vol. 25, no. 11, pp. 7307–7323, Mar. 2021.
[35] Y . Lv and C. Zhai, “When documents are very long, BM25 fails!”
inProceedings of the 34th international ACM SIGIR conference on
Research and development in Information - SIGIR '11 . ACM Press,
2011.
[36] D. D. Lewis, “Naive (bayes) at forty: The independence assumption in
information retrieval,” in Machine Learning: ECML-98 . Springer Berlin
Heidelberg, 1998, pp. 4–15.
[37] A. Schroter, A. Schr ¨oter, N. Bettenburg, and R. Premraj, “Do stack traces
help developers fix bugs?” in 2010 7th IEEE Working Conference on
Mining Software Repositories (MSR 2010) . IEEE, May 2010.
[38] Y . Lv and C. Zhai, “Lower-bounding term frequency normalization,” in
Proceedings of the 20th ACM international conference on Information
and knowledge management - CIKM '11 . ACM Press, 2011.
[39] Anonymous, “Rlocator: Reinforcement learning for bug localization,”
2023. [Online]. Available: https://zenodo.org/record/7591879
[40] M. Bagherzadeh, N. Kahani, and L. Briand, “Reinforcement learning
for test case prioritization,” IEEE Transactions on Software Engineering ,
vol. 48, no. 8, pp. 2836–2856, Aug. 2022.
[41] Y . Wan, Z. Zhao, M. Yang, G. Xu, H. Ying, J. Wu, and P. S. Yu,
“Improving automatic source code summarization via deep reinforce-
ment learning,” in Proceedings of the 33rd ACM/IEEE International
Conference on Automated Software Engineering . ACM, Sep. 2018.
[42] X. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T.-S. Chua, “Neural
collaborative filtering,” in Proceedings of the 26th International Confer-
ence on World Wide Web . International World Wide Web Conferences
Steering Committee, Apr. 2017.
[43] H. Zhang, Y . Yang, H. Luan, S. Yang, and T.-S. Chua, “Start from
scratch,” in Proceedings of the 22nd ACM international conference on
Multimedia . ACM, Nov. 2014.
[44] R. Zhu, X. Tu, and J. X. Huang, “Deep learning on information retrieval
and its applications,” in Deep Learning for Data Analytics . Elsevier,
2020, pp. 125–153.
[45] O. Khattab and M. Zaharia, “ColBERT,” in Proceedings of the 43rd
International ACM SIGIR Conference on Research and Development in
Information Retrieval . ACM, Jul. 2020.
[46] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
Computation , vol. 9, no. 8, pp. 1735–1780, Nov. 1997.
[47] L. Pang, Y . Lan, J. Guo, J. Xu, J. Xu, and X. Cheng, “DeepRank,”
inProceedings of the 2017 ACM on Conference on Information and
Knowledge Management . ACM, Nov. 2017.
[48] X. Huo, F. Thung, M. Li, D. Lo, and S.-T. Shi, “Deep transfer bug
localization,” IEEE Transactions on Software Engineering , vol. 47, no. 7,
pp. 1368–1380, Jul. 2021.
[49] X. Huo, M. Li, and Z.-H. Zhou, “Learning unified features from natural
and programming languages for locating buggy source code,” in IJCAI ,
2016.
[50] M. J. Hausknecht and P. Stone, “Deep recurrent q-learning for partially
observable mdps,” CoRR , vol. abs/1507.06527, 2015.
[51] ——, “Deep recurrent q-learning for partially observable mdps,” ArXiv ,
vol. abs/1507.06527, 2015.
[52] I. Bello, H. Pham, Q. V . Le, M. Norouzi, and S. Bengio, “Neural
combinatorial optimization with reinforcement learning,” 2016.
[53] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic
actor,” in Proceedings of the 35th International Conference on Machine
Learning , ser. Proceedings of Machine Learning Research, J. Dy and
A. Krause, Eds., vol. 80. PMLR, 10–15 Jul 2018, pp. 1861–1870.
[54] C. Wang, C. Xu, X. Yao, and D. Tao, “Evolutionary generative adversar-
ial networks,” IEEE Transactions on Evolutionary Computation , vol. 23,
no. 6, pp. 921–934, Dec. 2019.
[55] E. Rabinovich, M. Vetzler, S. Ackerman, and A. Anaby Tavor, “Reliable
and interpretable drift detection in streams of short texts,” in Proceedings
of the 61st Annual Meeting of the Association for Computational
Linguistics (Volume 5: Industry Track) . Association for Computational
Linguistics, 2023.
[56] M. R. Islam and M. F. Zibran, “What changes in where?: an empirical
study of bug-fixing change patterns,” ACM SIGAPP Applied Computing
Review , vol. 20, no. 4, p. 18–34, January 2021.
[57] W. Aljedaani and Y . Javed, Bug Reports Evolution in Open Source
Systems . Springer International Publishing, 2018, p. 63–73.

--- PAGE 14 ---
14
[58] X. Ye, R. Bunescu, and C. Liu, “Learning to rank relevant files for
bug reports using domain knowledge,” in Proceedings of the 22nd
ACM SIGSOFT International Symposium on Foundations of Software
Engineering - FSE 2014 . ACM Press, 2014.
[59] J. Lee, D. Kim, T. F. Bissyand ´e, W. Jung, and Y . L. Traon, “Bench4bl:
reproducibility study on the performance of IR-based bug localization,”
inProceedings of the 27th ACM SIGSOFT International Symposium on
Software Testing and Analysis . ACM, Jul. 2018.
[60] B. Dit, M. Revelle, M. Gethers, and D. Poshyvanyk, “Feature location
in source code: a taxonomy and survey,” Journal of Software: Evolution
and Process , vol. 25, no. 1, pp. 53–95, Nov. 2011.
[61] L. Moreno, G. Bavota, S. Haiduc, M. D. Penta, R. Oliveto, B. Russo,
and A. Marcus, “Query-based configuration of text retrieval solutions
for software engineering tasks,” in Proceedings of the 2015 10th Joint
Meeting on Foundations of Software Engineering . ACM, Aug. 2015.
[62] B. Sisman and A. C. Kak, “Assisting code search with automatic query
reformulation for bug localization,” in 2013 10th Working Conference
on Mining Software Repositories (MSR) . IEEE, May 2013.
[63] Y . Xiao, J. Keung, K. E. Bennin, and Q. Mi, “Improving bug localization
with word embedding and enhanced convolutional neural networks,”
Information and Software Technology , vol. 105, pp. 17–29, Jan. 2019.
[64] M. N. Schwarz and A. Flammer, “Text structure and title—effects
on comprehension and recall,” Journal of Verbal Learning and Verbal
Behavior , vol. 20, no. 1, pp. 61–66, Feb. 1981.
[65] J. Han, C. Huang, S. Sun, Z. Liu, and J. Liu, “bjXnet: an improved
bug localization model based on code property graph and attention
mechanism,” Automated Software Engineering , vol. 30, no. 1, Mar. 2023.
[66] S. Cheng, X. Yan, and A. A. Khan, “A similarity integration method
based information retrieval and word embedding in bug localization,”
in2020 IEEE 20th International Conference on Software Quality,
Reliability and Security (QRS) . IEEE, Dec. 2020.
[67] M. Soltani, F. Hermans, and T. B ¨ack, “The significance of bug report el-
ements,” Empirical Software Engineering , vol. 25, no. 6, p. 5255–5294,
Sep. 2020.
[68] Z. Ahmed, N. Le Roux, M. Norouzi, and D. Schuurmans, “Understand-
ing the impact of entropy on policy optimization,” in Proceedings of the
36th International Conference on Machine Learning , ser. Proceedings of
Machine Learning Research, K. Chaudhuri and R. Salakhutdinov, Eds.,
vol. 97. PMLR, 09–15 Jun 2019, pp. 151–160.
[69] S. Jang and H.-I. Kim, “Entropy-aware model initialization for effective
exploration in deep reinforcement learning,” Sensors , vol. 22, no. 15, p.
5845, Aug. 2022.
[70] V . Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,
D. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep
reinforcement learning,” in Proceedings of The 33rd International Con-
ference on Machine Learning , ser. Proceedings of Machine Learning
Research, M. F. Balcan and K. Q. Weinberger, Eds., vol. 48. New
York, New York, USA: PMLR, 20–22 Jun 2016, pp. 1928–1937.
[71] M. B ¨ohme, E. O. Soremekun, S. Chattopadhyay, E. Ugherughe, and
A. Zeller, “Where is the bug and how is it fixed? an experiment
with practitioners,” in Proceedings of the 2017 11th Joint Meeting on
Foundations of Software Engineering . ACM, Aug. 2017.
[72] T. D. Sasso, A. Mocci, and M. Lanza, “What makes a satisficing bug
report?” in 2016 IEEE International Conference on Software Quality,
Reliability and Security (QRS) . IEEE, Aug. 2016.
[73] N. Bettenburg, S. Just, A. Schr ¨oter, C. Weiss, R. Premraj, and T. Zim-
mermann, “What makes a good bug report?” in Proceedings of the 16th
ACM SIGSOFT International Symposium on Foundations of software
engineering . ACM, Nov. 2008.
[74] T. Zimmermann, R. Premraj, N. Bettenburg, S. Just, A. Schroter, and
C. Weiss, “What makes a good bug report?” IEEE Transactions on
Software Engineering , vol. 36, no. 5, pp. 618–643, Sep. 2010.
[75] B. Vancsics, F. Horv ´ath, A. Szatm ´ari, and ´A. Besz ´edes, “Fault localiza-
tion using function call frequencies,” Journal of Systems and Software ,
vol. 193, p. 111429, Nov. 2022.
[76] Y . Lou, Q. Zhu, J. Dong, X. Li, Z. Sun, D. Hao, L. Zhang, and
L. Zhang, “Boosting coverage-based fault localization via graph-based
representation learning,” in Proceedings of the 29th ACM Joint Meeting
on European Software Engineering Conference and Symposium on the
Foundations of Software Engineering . ACM, Aug. 2021.
[77] Y . Kim, S. Mun, S. Yoo, and M. Kim, “Precise learn-to-rank fault
localization using dynamic and static features of target programs,” ACM
Transactions on Software Engineering and Methodology , vol. 28, no. 4,
pp. 1–34, Oct. 2019.[78] M. M. Rahman, F. Khomh, S. Yeasmin, and C. K. Roy, “The forgotten
role of search queries in IR-based bug localization: an empirical study,”
Empirical Software Engineering , vol. 26, no. 6, Aug. 2021.
[79] J. M. Florez, O. Chaparro, C. Treude, and A. Marcus, “Combining query
reduction and expansion for text-retrieval-based bug localization,” in
2021 IEEE International Conference on Software Analysis, Evolution
and Reengineering (SANER) . IEEE, Mar. 2021.
[80] Y . Li, S. Wang, T. N. Nguyen, and S. V . Nguyen, “Improving bug
detection via context-based code representation learning and attention-
based neural networks,” Proceedings of the ACM on Programming
Languages , vol. 3, no. OOPSLA, pp. 1–30, Oct. 2019.
[81] Z. Zhu, Y . Li, Y . Wang, Y . Wang, and H. Tong, “A deep multimodal
model for bug localization,” Data Mining and Knowledge Discovery ,
Apr. 2021.
[82] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,
and L. Zettlemoyer, “Deep contextualized word representations,” CoRR ,
vol. abs/1802.05365, 2018.
[83] R. S. Sutton, D. Precup, and S. Singh, “Between MDPs and semi-
MDPs: A framework for temporal abstraction in reinforcement learning,”
Artificial Intelligence , vol. 112, no. 1-2, pp. 181–211, Aug. 1999.
[84] H. Xie, Y . Lei, M. Yan, Y . Yu, X. Xia, and X. Mao, “A universal data
augmentation approach for fault localization,” in Proceedings of the 44th
International Conference on Software Engineering . ACM, May 2022.
[85] T. White and B. Pagurek, “Distributed fault location in networks using
learning mobile agents,” in Approaches to Intelligence Agents . Springer
Berlin Heidelberg, 1999, pp. 182–196.
[86] H. Rezapour, S. Jamali, and A. Bahmanyar, “Review on artificial
intelligence-based fault location methods in power distribution net-
works,” Energies , vol. 16, no. 12, p. 4636, Jun. 2023.
[87] C. Ni, W. Wang, K. Yang, X. Xia, K. Liu, and D. Lo, “The best
of both worlds: integrating semantic features with expert features for
defect prediction and localization,” in Proceedings of the 30th ACM
Joint European Software Engineering Conference and Symposium on
the Foundations of Software Engineering . ACM, Nov. 2022.
[88] B. Wang, L. Xu, M. Yan, C. Liu, and L. Liu, “Multi-dimension
convolutional neural network for bug localization,” IEEE Transactions
on Services Computing , pp. 1–1, 2020.
Partha Chakraborty is a Ph.D. candidate in the
Cheriton School of Computer Science at the Uni-
versity of Waterloo, Canada. His research interests
include bug localization, vulnerability detection, and
the use of machine learning techniques in software
engineering. Find more about him at https://parthac.
me/.
Mahmoud Alfadel is an Assistant Professor at
the Department of Computer Science, University
of Calgary. His research interests include mining
software repositories, software ecosystems, open-
source security, and release engineering.
Meiyappan Nagappan is an Associate Professor at
the Cheriton School of Computer Science, Univer-
sity of Waterloo. He has worked on empirical soft-
ware engineering to address software development
concerns and currently researches the impact of large
language models on software development.
