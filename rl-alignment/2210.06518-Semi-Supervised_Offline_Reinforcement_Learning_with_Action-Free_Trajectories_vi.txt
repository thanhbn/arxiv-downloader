# Học Tăng Cường Ngoại Tuyến Bán Giám Sát với Quỹ Đạo Không Có Hành Động

## Tóm Tắt

Các tác nhân tự nhiên có thể học hiệu quả từ nhiều nguồn dữ liệu khác nhau về kích thước, chất lượng và loại đo lường. Chúng tôi nghiên cứu sự không đồng nhất này trong bối cảnh học tăng cường ngoại tuyến (RL) bằng cách giới thiệu một cài đặt bán giám sát mới có động lực thực tế. Ở đây, một tác nhân có quyền truy cập vào hai tập quỹ đạo: quỹ đạo có nhãn chứa bộ ba trạng thái, hành động và phần thưởng tại mỗi bước thời gian, cùng với quỹ đạo không có nhãn chỉ chứa thông tin trạng thái và phần thưởng. Cho cài đặt này, chúng tôi phát triển và nghiên cứu một quy trình meta-thuật toán đơn giản học một mô hình động lực học nghịch đảo trên dữ liệu có nhãn để thu được nhãn proxy cho dữ liệu không nhãn, tiếp theo là việc sử dụng bất kỳ thuật toán RL ngoại tuyến nào trên các quỹ đạo thực và có nhãn proxy. Về mặt thực nghiệm, chúng tôi thấy rằng quy trình đơn giản này rất thành công — trên một số điểm chuẩn D4RL (Fu et al., 2020), một số thuật toán RL ngoại tuyến có thể đạt được hiệu suất của các biến thể được huấn luyện trên tập dữ liệu được gán nhãn đầy đủ ngay cả khi chúng tôi chỉ gán nhãn 10% quỹ đạo có hiệu suất dưới tối ưu cao. Để củng cố hiểu biết của chúng tôi, chúng tôi thực hiện một nghiên cứu thực nghiệm có kiểm soát quy mô lớn điều tra sự tương tác của các thuộc tính tập trung vào dữ liệu của các tập dữ liệu có nhãn và không nhãn, với các lựa chọn thiết kế thuật toán (ví dụ: lựa chọn động lực học nghịch đảo, thuật toán RL ngoại tuyến) để xác định các xu hướng chung và thực hành tốt nhất cho việc huấn luyện các tác nhân RL trên các tập dữ liệu ngoại tuyến bán giám sát.

## 1 Giới Thiệu

Một trong những thách thức chính với việc triển khai các tác nhân học tăng cường (RL) là độ phức tạp mẫu cấm đoán của chúng đối với các ứng dụng thế giới thực. Học tăng cường ngoại tuyến (RL) có thể giảm đáng kể độ phức tạp mẫu bằng cách khai thác các demo được ghi lại từ các nguồn dữ liệu phụ (Levine et al., 2020). RL ngoại tuyến tiêu chuẩn giả định các tập dữ liệu được ghi đầy đủ: các quỹ đạo là các chuỗi hoàn chỉnh của quan sát, hành động và phần thưởng. Tuy nhiên, trái ngược với các điểm chuẩn được tuyển chọn đang sử dụng hiện nay, bản chất của các demo ngoại tuyến trong thế giới thực có thể rất đa dạng. Ví dụ, các demo có thể không khớp do sự không phù hợp về tần số (Burns et al., 2022), sử dụng các cảm biến, bộ truyền động hoặc động lực học khác nhau (Reed et al., 2022; Lee et al., 2022), hoặc thiếu thông tin trạng thái một phần (Ghosh et al., 2022; Rafailov et al., 2021; Mazoure et al., 2021) hoặc phần thưởng (Yu et al., 2022). RL ngoại tuyến thành công trong thế giới thực đòi hỏi việc chấp nhận các khía cạnh không đồng nhất này để đạt hiệu quả dữ liệu tối đa, tương tự như học tập ở con người.

Trong công trình này, chúng tôi đề xuất một cài đặt bán giám sát mới và có động lực thực tế cho RL ngoại tuyến: tập dữ liệu ngoại tuyến bao gồm một số quỹ đạo không có hành động (mà chúng tôi gọi là không nhãn) ngoài các quỹ đạo hoàn chỉnh hành động tiêu chuẩn (mà chúng tôi gọi là có nhãn). Đặc biệt, chúng tôi chủ yếu quan tâm đến trường hợp khi phần lớn đáng kể các quỹ đạo trong tập dữ liệu ngoại tuyến là không nhãn, và dữ liệu không nhãn có thể có chất lượng khác với dữ liệu có nhãn. Một ví dụ tạo động lực cho cài đặt này là học từ video (Schmeckpeper et al., 2020a;b) hoặc các demo người thứ ba (Stadie et al., 2017; Sharma et al., 2019). Có một lượng lớn video internet có thể được sử dụng để huấn luyện các tác nhân RL, nhưng chúng không có nhãn hành động và có chất lượng khác nhau. Đáng chú ý, cài đặt của chúng tôi có hai thuộc tính chính khác biệt với học bán giám sát truyền thống:

• Thứ nhất, chúng tôi không giả định rằng phân phối của các quỹ đạo có nhãn và không nhãn nhất thiết phải giống hệt nhau. Trong các kịch bản thực tế, chúng tôi mong đợi chúng khác nhau với dữ liệu không nhãn có lợi tức cao hơn dữ liệu có nhãn ví dụ như video của một chuyên gia con người dễ thu thập trong khi đo chính xác hành động của họ là thách thức. Chúng tôi nhân bản các cài đặt chất lượng dữ liệu đa dạng như vậy trong một số thí nghiệm của chúng tôi; Hình 1.1 cho thấy một minh họa về sự khác biệt trong lợi tức giữa các phần chia tập dữ liệu có nhãn và không nhãn sử dụng tập dữ liệu hopper-medium-expert D4RL.

• Thứ hai, mục tiêu cuối cùng của chúng tôi vượt ra ngoài việc gán nhãn các hành động trong các quỹ đạo không nhãn, mà là chúng tôi dự định sử dụng dữ liệu không nhãn để học một chính sách downstream tốt hơn các chính sách hành vi được sử dụng để tạo ra các tập dữ liệu ngoại tuyến.

Tương ứng, có hai loại thách thức tổng quát hóa trong cài đặt được đề xuất: (i) tổng quát hóa từ phân phối dữ liệu có nhãn sang không nhãn và sau đó (ii) vượt ra ngoài các phân phối dữ liệu ngoại tuyến để tiến gần hơn đến phân phối chuyên gia. RL ngoại tuyến thông thường chỉ quan tâm đến loại sau, và các thuật toán tiêu chuẩn như Conservative Q Learning (CQL; Kumar et al. (2020)), TD3BC (TD3BC; Fujimoto & Gu (2021)) hoặc Decision Transformer (DT; Chen et al. (2021)), không thể hoạt động trực tiếp trên các quỹ đạo không nhãn như vậy. Đồng thời, việc bỏ qua các quỹ đạo không nhãn một cách ngây thơ có thể lãng phí, đặc biệt khi chúng có lợi tức cao. Do đó, bài báo của chúng tôi tìm cách trả lời câu hỏi sau:

Làm thế nào chúng ta có thể tận dụng tốt nhất dữ liệu không nhãn để cải thiện hiệu suất của các thuật toán RL ngoại tuyến?

Để trả lời câu hỏi này, chúng tôi nghiên cứu các phương pháp khác nhau để huấn luyện chính sách trong cài đặt bán giám sát được mô tả ở trên, và đề xuất một quy trình meta-thuật toán Semi-Supervised Offline Reinforcement Learning (SS-ORL). SS-ORL chứa ba bước đơn giản: (1) huấn luyện một mô hình động lực học nghịch đảo (IDM) trên dữ liệu có nhãn, dự đoán hành động dựa trên chuỗi chuyển tiếp, (2) điền vào các hành động proxy cho dữ liệu không nhãn, và cuối cùng (3) huấn luyện một tác nhân RL ngoại tuyến trên tập dữ liệu kết hợp.

Điểm chính của bài báo chúng tôi là:

Với dữ liệu có nhãn chất lượng thấp, các tác nhân SS-ORL có thể khai thác dữ liệu không nhãn chứa các quỹ đạo chất lượng cao để cải thiện hiệu suất. Hiệu suất tuyệt đối của SS-ORL gần hoặc thậm chí bằng với các tác nhân oracle, có quyền truy cập vào thông tin hành động hoàn chỉnh của cả quỹ đạo có nhãn và không nhãn.

Từ góc độ kỹ thuật, chúng tôi giải quyết các hạn chế của IDM cổ điển (Pathak et al., 2017) bằng cách đề xuất một IDM đa chuyển tiếp ngẫu nhiên mới tích hợp các trạng thái trước đó để tính đến các chính sách hành vi không Markov. Để cho phép học tập hiệu quả về tính toán và dữ liệu, chúng tôi tiến hành các nghiên cứu ablation kỹ lưỡng để hiểu cách hiệu suất của các tác nhân SS-ORL bị ảnh hưởng bởi các lựa chọn thiết kế thuật toán, và nó thay đổi như thế nào theo các thuộc tính tập trung vào dữ liệu như kích thước và phân phối lợi tức của các tập dữ liệu có nhãn và không nhãn. Chúng tôi làm nổi bật một số xu hướng chủ đạo từ các phát hiện thực nghiệm của chúng tôi dưới đây:

1. Gán nhãn proxy là một cách hiệu quả để sử dụng dữ liệu không nhãn. Ví dụ, SS-ORL được khởi tạo với DT như phương pháp RL ngoại tuyến vượt trội đáng kể so với một phương pháp dựa trên DT thay thế không có gán nhãn proxy.

2. Đơn giản là huấn luyện IDM trên tập dữ liệu có nhãn vượt trội hơn các giao thức bán giám sát phức tạp hơn như tự huấn luyện (Fralick, 1967).

3. Tích hợp thông tin quá khứ vào IDM cải thiện tổng quát hóa.

4. Hiệu suất của các tác nhân SS-ORL phụ thuộc nghiêm trọng vào các yếu tố như kích thước và chất lượng của các tập dữ liệu có nhãn và không nhãn, nhưng độ lớn ảnh hưởng phụ thuộc vào phương pháp RL ngoại tuyến. Ví dụ, chúng tôi thấy rằng TD3BC ít nhạy cảm với việc thiếu hành động hơn DT và CQL.

## 2 Công Trình Liên Quan

**RL Ngoại Tuyến** Mục tiêu của RL ngoại tuyến là học các chính sách hiệu quả từ các tập dữ liệu cố định được tạo ra bởi các chính sách hành vi không xác định. Có hai loại chính của các phương pháp RL ngoại tuyến không có mô hình: các phương pháp dựa trên giá trị và các phương pháp dựa trên behavior cloning (BC).

Các phương pháp dựa trên giá trị cố gắng học các hàm giá trị dựa trên cập nhật temporal difference (TD). Có một dòng công trình nhằm chuyển các phương pháp RL trực tuyến dựa trên giá trị off-policy hiện có sang cài đặt ngoại tuyến, với các loại thành phần chính quy hóa bổ sung khác nhau khuyến khích chính sách đã học ở gần chính sách hành vi. Một số kỹ thuật đại diện bao gồm các tham số hóa chính sách được thiết kế riêng (Fujimoto et al., 2019; Ghasemipour et al., 2021), chính quy hóa dựa trên divergence trên chính sách đã học (Wu et al., 2019; Jaques et al., 2019; Kumar et al., 2019), và ước lượng hàm giá trị chính quy hóa (Nachum et al., 2019; Kumar et al., 2020; Kostrikov et al., 2021a; Fujimoto & Gu, 2021; Kostrikov et al., 2021b).

Một khối lượng công trình gần đây ngày càng tăng công thức hóa RL ngoại tuyến như một bài toán học có giám sát (Chen et al., 2021; Janner et al., 2021; Emmons et al., 2021). So với các phương pháp dựa trên giá trị, các phương pháp có giám sát này có một số thuộc tính hấp dẫn bao gồm sự đơn giản thuật toán và ổn định huấn luyện. Nói chung, các phương pháp này có thể được xem là các phương pháp behavior cloning có điều kiện (Bain & Sammut, 1995), trong đó điều kiện dựa trên mục tiêu hoặc lợi tức. Tương tự như các phương pháp dựa trên giá trị, chúng có thể được mở rộng sang cài đặt trực tuyến (Zheng et al., 2022) và thể hiện hiệu suất xuất sắc trong các cài đặt lai liên quan đến cả dữ liệu ngoại tuyến và tương tác trực tuyến.

**Học Bán Giám Sát** Học bán giám sát (SSL) là một lĩnh vực con của học máy nghiên cứu các phương pháp huấn luyện bộ dự đoán từ một lượng nhỏ dữ liệu có nhãn kết hợp với một lượng lớn dữ liệu không nhãn. Trong học có giám sát, các bộ dự đoán chỉ học từ dữ liệu có nhãn. Tuy nhiên, các ví dụ huấn luyện có nhãn thường đòi hỏi nỗ lực chú thích của con người và do đó khó thu thập, trong khi dữ liệu không nhãn có thể tương đối dễ thu thập. Nghiên cứu về học bán giám sát kéo dài qua nhiều thập kỷ. Một trong những kỹ thuật SSL lâu đời nhất, tự huấn luyện, ban đầu được đề xuất vào những năm 1960 (Fralick, 1967). Ở đó, bộ dự đoán được huấn luyện đầu tiên trên dữ liệu có nhãn. Sau đó, tại mỗi vòng huấn luyện, theo một số tiêu chí lựa chọn như độ không chắc chắn của mô hình, một phần dữ liệu không nhãn được chú thích bởi bộ dự đoán và thêm vào tập huấn luyện cho vòng tiếp theo. Quá trình như vậy được lặp lại nhiều lần. Chúng tôi tham khảo độc giả đến Zhu (2005); Chapelle et al. (2006); Ouali et al. (2020); Van Engelen & Hoos (2020) cho các khảo sát tài liệu toàn diện.

**Học Bắt Chước từ Quan Sát** Đã có một số công trình trong học bắt chước (IL) không giả định quyền truy cập vào tập hành động đầy đủ, như BCO (Torabi et al., 2018a), MoBILE (Kidambi et al., 2021), GAIfO (Torabi et al., 2018b) hoặc các phương pháp IL người thứ ba (Stadie et al., 2017; Sharma et al., 2019). Công trình gần đây của Baker et al. (2022) cũng xem xét một cài đặt trong đó một số lượng nhỏ hành động có nhãn có sẵn ngoài một tập dữ liệu không nhãn lớn. Một sự khác biệt chính với công trình của chúng tôi là cài đặt IL thường giả định rằng tất cả các quỹ đạo được tạo ra bởi một chuyên gia, khác với cài đặt ngoại tuyến của chúng tôi. Hơn nữa, một số phương pháp này thậm chí cho phép tương tác không có phần thưởng với môi trường, điều này không thể trong cài đặt ngoại tuyến.

**Học từ Video** Một số công trình xem xét huấn luyện các tác nhân với các demo video của con người (Schmeckpeper et al., 2020a;b), không có chú thích hành động. Khác biệt với cài đặt của chúng tôi, một số công trình này cho phép tương tác trực tuyến, giả định video chuyên gia, và rộng hơn, dữ liệu video thường chỉ định các tác nhân với các thể hiện khác nhau.

## 3 RL Ngoại Tuyến Bán Giám Sát

**Kiến Thức Cơ Bản** Chúng tôi mô hình hóa môi trường của chúng ta như một quá trình quyết định Markov (MDP) (Bellman, 1957) được ký hiệu bởi ⟨S,A, p, P, R, γ⟩, trong đó S là không gian trạng thái, A là không gian hành động, p(s1) là phân phối của trạng thái ban đầu, P(st+1|st, at) là phân phối xác suất chuyển tiếp, R(st, at) là hàm phần thưởng xác định, và γ là hệ số chiết khấu. Tại mỗi bước thời gian t, tác nhân quan sát một trạng thái st ∈ S và thực hiện một hành động at ∈ A. Môi trường sau đó di chuyển tác nhân đến trạng thái tiếp theo st+1 ∼ P(·|st, at), và cũng trả về cho tác nhân một phần thưởng rt = R(st, at).

### 3.1 Cài Đặt Được Đề Xuất

Chúng tôi giả định tác nhân có quyền truy cập vào một tập dữ liệu ngoại tuyến tĩnh Toffline. Tập dữ liệu bao gồm các quỹ đạo được thu thập bởi các chính sách không xác định, thường là dưới tối ưu. Cho τ biểu thị một quỹ đạo và |τ| biểu thị độ dài của nó. Chúng tôi giả định rằng tất cả các quỹ đạo trong Toffline chứa phần thưởng và trạng thái hoàn chỉnh. Tuy nhiên, chỉ một tập con nhỏ của chúng chứa hành động. Chúng tôi quan tâm đến việc học một chính sách bằng cách tận dụng tập dữ liệu ngoại tuyến mà không tương tác với môi trường. Cài đặt này tương tự với học bán giám sát, trong đó hành động đóng vai trò của nhãn. Do đó, chúng tôi cũng gọi các quỹ đạo hoàn chỉnh là dữ liệu có nhãn (ký hiệu bởi Tlabelled) và các quỹ đạo không có hành động là dữ liệu không nhãn (ký hiệu bởi Tunlabelled). Hơn nữa, chúng tôi giả định dữ liệu có nhãn và không nhãn được lấy mẫu từ hai phân phối Plabelled và Punlabelled, tương ứng. Nói chung, hai phân phối có thể khác nhau. Một trường hợp chúng tôi đặc biệt quan tâm là khi Plabelled tạo ra các quỹ đạo chất lượng thấp đến trung bình, trong khi Punlabelled tạo ra các quỹ đạo có chất lượng đa dạng bao gồm những quỹ đạo có lợi tức cao, như được thể hiện trong Hình 1.1.

Cài đặt của chúng tôi có một số điểm tương đồng với học bắt chước chỉ trạng thái (Ijspeert et al., 2002; Bentivegna et al., 2002; Torabi et al., 2019) trong việc sử dụng các quỹ đạo không có nhãn hành động. Tuy nhiên, có hai sự khác biệt cơ bản. Thứ nhất, trong IL chỉ trạng thái, các demo không nhãn từ cùng phân phối với các demo có nhãn, và cả hai được tạo ra bởi một chính sách chuyên gia gần tối ưu. Trong cài đặt của chúng tôi, Plabelled và Punlabelled có thể khác nhau và không được giả định là tối ưu. Thứ hai, nhiều thuật toán học bắt chước chỉ trạng thái (ví dụ: Gupta et al. (2017); Torabi et al. (2018a;b); Liu et al. (2018); Sermanet et al. (2018)) cho phép tương tác (không có phần thưởng) với môi trường tương tự như các đối tác ban đầu của chúng (ví dụ: Ho & Ermon (2016); Kim et al. (2020)). Điều này không được phép trong cài đặt ngoại tuyến của chúng tôi, nơi các tác nhân chỉ được cung cấp Tlabelled và Tunlabelled.

### 3.2 Quy Trình Huấn Luyện

Các chính sách RL được huấn luyện trên các quỹ đạo ngoại tuyến chất lượng thấp đến trung bình thường dưới tối ưu, vì nhiều quỹ đạo có thể không có lợi tức cao và chỉ bao phủ một phần hạn chế của không gian trạng thái. Mục tiêu của chúng tôi là tìm cách kết hợp các quỹ đạo có nhãn hành động và các quỹ đạo không có nhãn hành động, để tác nhân ngoại tuyến có thể khai thác các cấu trúc trong dữ liệu không nhãn để cải thiện hiệu suất.

Một chiến lược tự nhiên là điền vào các hành động proxy cho những quỹ đạo không nhãn đó, và sử dụng dữ liệu có nhãn proxy cùng với dữ liệu có nhãn như một tổng thể để huấn luyện một tác nhân RL ngoại tuyến. Vì chúng tôi giả định cả quỹ đạo có nhãn và không nhãn đều chứa các trạng thái, chúng tôi có thể huấn luyện một mô hình động lực học nghịch đảo (IDM) ϕ dự đoán hành động sử dụng các trạng thái. Khi chúng tôi thu được IDM, chúng tôi sử dụng nó để tạo ra các hành động proxy cho các quỹ đạo không nhãn. Cuối cùng, chúng tôi kết hợp những quỹ đạo có nhãn proxy đó với các quỹ đạo có nhãn, và huấn luyện một tác nhân sử dụng thuật toán RL ngoại tuyến theo lựa chọn. Quy trình meta-thuật toán của chúng tôi được tóm tắt trong Thuật toán 1.

Đặc biệt, chúng tôi đề xuất một IDM đa chuyển tiếp ngẫu nhiên mới tích hợp thông tin quá khứ để nâng cao xử lý cho các MDP ngẫu nhiên và các chính sách hành vi không Markov. Phần 3.2.1 thảo luận chi tiết.

Đáng chú ý, SS-ORL là một quy trình đa giai đoạn, trong đó IDM được huấn luyện chỉ trên dữ liệu có nhãn trong một vòng duy nhất. Có những cách khác có thể để kết hợp dữ liệu có nhãn và không nhãn. Trong Phần 3.2.2, chúng tôi thảo luận một số lựa chọn thiết kế thay thế và những lý do chính tại sao chúng tôi không sử dụng chúng. Ngoài ra, chúng tôi trình bày các thí nghiệm ablation trong Phần 4.2.

#### 3.2.1 IDM ĐA CHUYỂN TIẾP NGẪU NHIÊN

Trong công trình trước đây (Pathak et al., 2017; Burda et al., 2019; Henaff et al., 2022), IDM thường học ánh xạ hai trạng thái liên tiếp của chuyển tiếp thứ t, (st, st+1), thành at. Về lý thuyết, điều này đủ khi tập dữ liệu ngoại tuyến được tạo ra bởi một chính sách Markov duy nhất trong một môi trường xác định, xem Phụ lục D cho phân tích. Tuy nhiên, trong thực tế, tập dữ liệu ngoại tuyến có thể chứa các quỹ đạo được ghi từ nhiều nguồn.

Để cung cấp xử lý tốt hơn cho nhiều chính sách hành vi, chúng tôi giới thiệu một IDM đa chuyển tiếp dự đoán phân phối của at sử dụng k + 1 chuyển tiếp gần nhất. Chính xác hơn, cho st,−k biểu thị chuỗi smin(0,t−k), . . . , st, st+1. Chúng tôi mô hình P(at|st,−k) như một Gaussian đa biến với ma trận hiệp phương sai đường chéo:

at ∼ N(μθ(st,−k), Σθ(st,−k)). (1)

Cho ϕθ(at|st,−k) là hàm mật độ xác suất của N(μθ(st,−k), Σθ(st,−k)). Với các quỹ đạo có nhãn Tlabelled, chúng tôi cực tiểu hóa mất mát log-likelihood âm Σ(at,st,−k)inTlabelled[−logϕθ(at|st,−k)]. Chúng tôi gọi k là tham số kích thước chuyển tiếp. Lưu ý rằng IDM tiêu chuẩn dự đoán at từ (st, st+1) dưới mất mát ℓ2, là một trường hợp đặc biệt được bao hàm bởi mô hình của chúng tôi: nó tương đương với trường hợp k = 0 và các mục đường chéo của Σθ (tức là, phương sai của mỗi chiều hành động) đều giống nhau.

Về bản chất, chúng tôi xấp xỉ p(at|st+1, . . . , s1) bằng p(at|st,−k), và chọn k > 0 cho phép chúng tôi tính đến thông tin trạng thái quá khứ. Trong khi đó, lý thuyết cũng chỉ ra rằng việc tích hợp các trạng thái tương lai như st+2 sẽ không giúp dự đoán at (xem phân tích trong Phụ lục D để biết chi tiết). Đối với tất cả các thí nghiệm trong bài báo này, chúng tôi sử dụng k = 1. Chúng tôi thực hiện ablation lựa chọn thiết kế này trong Phần 4.2. Hơn nữa, IDM của chúng tôi tự nhiên mở rộng đến các chính sách không Markov và MDP ngẫu nhiên. Điều này vượt ra ngoài phạm vi của bài báo này, nhưng chúng tôi coi chúng là những hướng tiềm năng cho công trình tương lai.

#### 3.2.2 CÁC LỰA CHỌN THIẾT KẾ THAY THẾ

**Huấn Luyện Không Có Gán Nhãn Proxy** SS-ORL điền vào các hành động proxy cho các quỹ đạo không nhãn trước khi huấn luyện tác nhân. Ở đó, nhiệm vụ học chính sách được định nghĩa trên tập dữ liệu kết hợp của dữ liệu có nhãn và không nhãn. Một phương pháp thay thế là chỉ sử dụng dữ liệu có nhãn để định nghĩa nhiệm vụ học chính sách, nhưng tạo ra các nhiệm vụ phụ trợ nhất định sử dụng dữ liệu không nhãn. Các nhiệm vụ phụ trợ này không phụ thuộc vào hành động, để gán nhãn proxy không cần thiết. Các phương pháp học đa nhiệm vụ có thể được sử dụng để huấn luyện một tác nhân giải quyết những nhiệm vụ đó cùng nhau. Ví dụ, Reed et al. (2022) huấn luyện một tác nhân tổng quát xử lý các chuỗi đa dạng với một mô hình transformer duy nhất. Theo tinh thần tương tự, chúng tôi xem xét DT-Joint, một biến thể của DT, huấn luyện trên cả dữ liệu có nhãn và không nhãn đồng thời. Tóm lại, DT-Joint dự đoán hành động cho các quỹ đạo có nhãn, và trạng thái và phần thưởng cho cả quỹ đạo có nhãn và không nhãn. Xem Phụ lục F cho chi tiết triển khai. Tuy nhiên, thí nghiệm ablation của chúng tôi trong Phần 4.2 cho thấy rằng SS-ORL vượt trội đáng kể so với DT-Joint.

**Tự Huấn Luyện cho IDM** Quá trình chú thích trong SS-ORL, liên quan đến việc huấn luyện một IDM trên dữ liệu có nhãn và tạo ra các hành động proxy cho các quỹ đạo không nhãn, tương tự như một bước của tự huấn luyện (Fralick, 1967, Cf. Phần 2), một phương pháp thường được sử dụng trong học bán giám sát tiêu chuẩn. Tuy nhiên, một sự khác biệt chính là chúng tôi không huấn luyện lại IDM mà chuyển trực tiếp đến giai đoạn tiếp theo của việc huấn luyện tác nhân sử dụng dữ liệu kết hợp. Có một số lý do chúng tôi không sử dụng tự huấn luyện cho IDM. Thứ nhất, việc thực hiện nhiều vòng huấn luyện rất tốn kém về mặt tính toán. Quan trọng hơn, mục tiêu cuối cùng của chúng tôi là thu được một chính sách downstream với hiệu suất cải thiện thông qua việc sử dụng dữ liệu có nhãn proxy. Như một baseline, chúng tôi xem xét tự huấn luyện cho IDM, trong đó sau mỗi vòng huấn luyện chúng tôi thêm dữ liệu có nhãn proxy với độ không chắc chắn dự đoán thấp vào tập huấn luyện cho vòng tiếp theo. Về mặt thực nghiệm, chúng tôi thấy rằng biến thể này kém hiệu quả hơn phương pháp của chúng tôi. Xem Phần 4.2 và Phụ lục E để biết thêm chi tiết.

## 4 Thí Nghiệm

Các mục tiêu chính của chúng tôi là trả lời bốn nhóm câu hỏi:

Q1. Các tác nhân SS-ORL có thể đạt được hiệu suất gần bằng các tác nhân RL ngoại tuyến được giám sát đầy đủ đến mức nào, đặc biệt khi chỉ một tập con nhỏ các quỹ đạo được gán nhãn?

Q2. Các tác nhân SS-ORL hoạt động như thế nào dưới các lựa chọn thiết kế khác nhau để huấn luyện IDM, hoặc thậm chí tránh hoàn toàn việc gán nhãn proxy?

Q3. Hiệu suất của các tác nhân SS-ORL thay đổi như thế nào theo kích thước và chất lượng của các tập dữ liệu có nhãn và không nhãn?

Q4. Các phương pháp RL ngoại tuyến khác nhau có phản ứng khác nhau với các cài đặt khác nhau của kích thước và chất lượng tập dữ liệu không?

Chúng tôi tập trung vào hai nhiệm vụ locomotion Gym, hopper và walker, với các tập dữ liệu v2 medium-expert, medium và medium-replay từ điểm chuẩn D4RL (Fu et al., 2020). Do hạn chế về không gian, kết quả trên các tập dữ liệu medium và medium-replay được hoãn lại đến Phụ lục C. Chúng tôi trả lời các câu hỏi trên trong Phần 4.1, 4.2, 4.3 và 4.4, tương ứng. Chúng tôi cũng bao gồm các thí nghiệm bổ sung trên môi trường maze2d trong Phụ lục H. Đối với tất cả các thí nghiệm, chúng tôi huấn luyện 5 instance của mỗi phương pháp với các seed khác nhau, và cho mỗi instance chúng tôi rollout 30 quỹ đạo đánh giá. Mã của chúng tôi có sẵn tại https://github.com/facebookresearch/ssorl/.

### 4.1 Đánh Giá Chính (Q1)

**Cài Đặt Dữ Liệu** Chúng tôi lấy mẫu phụ 10% tổng số quỹ đạo ngoại tuyến có lợi tức từ q% dưới cùng làm các quỹ đạo có nhãn, 10 ≤ q ≤ 100. Các hành động của các quỹ đạo còn lại được loại bỏ để tạo ra những quỹ đạo không nhãn. Chúng tôi gọi cài đặt này là cài đặt kết hợp, vì phân phối dữ liệu có nhãn Plabelled và phân phối dữ liệu không nhãn Punlabelled sẽ thay đổi đồng thời khi chúng tôi thay đổi giá trị của q. Khi q tăng, chất lượng dữ liệu có nhãn tăng và các phân phối Plabelled và Punlabelled trở nên gần nhau hơn. Khi q = 100, cài đặt của chúng tôi tương đương với việc lấy mẫu các quỹ đạo có nhãn một cách đồng nhất và Plabelled = Punlabelled. Lưu ý rằng dưới cài đặt của chúng tôi, chúng tôi luôn có 10% quỹ đạo có nhãn và 90% không nhãn, và tổng lượng dữ liệu được sử dụng để huấn luyện tác nhân RL ngoại tuyến giống như tập dữ liệu ngoại tuyến ban đầu. Điều này cho phép so sánh dễ dàng với kết quả dưới cài đặt tiêu chuẩn, được gán nhãn đầy đủ. Trong Phần 4.3, chúng tôi sẽ tách rời Plabelled và Punlabelled để hiểu sâu về ảnh hưởng cá nhân của chúng đối với các tác nhân SS-ORL.

**Mô Hình Động Lực Học Nghịch Đảo** Chúng tôi huấn luyện một IDM như được mô tả trong Phần 3 với k = 1. Tức là, IDM dự đoán at sử dụng 3 trạng thái liên tiếp: st−1, st và st+1, trong đó mean và ma trận hiệp phương sai được dự đoán bởi hai mạng perceptron đa tầng (MLP) độc lập, mỗi mạng chứa hai tầng ẩn và 1024 đơn vị ẩn trên mỗi tầng. Để ngăn chặn overfitting, chúng tôi lấy mẫu ngẫu nhiên 10% các quỹ đạo có nhãn làm tập validation, và sử dụng IDM mang lại lỗi validation tốt nhất trong vòng 100k iterations.

**Các Phương Pháp RL Ngoại Tuyến** Chúng tôi khởi tạo Thuật toán 1 với DT, CQL và TD3BC như các phương pháp RL ngoại tuyến cơ bản. DT là một phương pháp conditional behaviour cloning (BC) được đề xuất gần đây sử dụng các công cụ mô hình hóa chuỗi để mô hình hóa các quỹ đạo. CQL là một phương pháp RL ngoại tuyến dựa trên giá trị đại diện. TD3BC là một phương pháp lai thêm một thuật ngữ BC để chính quy hóa các cập nhật Q-learning. Chúng tôi gọi các khởi tạo này là SS-DT, SS-CQL và SS-TD3BC, tương ứng. Xem Phụ lục A cho chi tiết triển khai.

**Kết Quả** Chúng tôi so sánh hiệu suất của các tác nhân SS-ORL với các tác nhân baseline và oracle tương ứng. Các tác nhân baseline được huấn luyện chỉ trên các quỹ đạo có nhãn, và các tác nhân oracle được huấn luyện trên tập dữ liệu ngoại tuyến đầy đủ với nhãn hành động hoàn chỉnh. Trực quan, hiệu suất của các tác nhân baseline và oracle có thể được coi là các ràng buộc dưới và trên (ước tính) cho hiệu suất của các tác nhân SS-ORL. Chúng tôi xem xét 6 giá trị khác nhau của q: 10, 30, 50, 70, 90 và 100, và chúng tôi báo cáo lợi tức trung bình và độ lệch chuẩn sau 200k iterations. Hình 4.1 vẽ kết quả trên các tập dữ liệu medium-expert. Trên cả hai tập dữ liệu, các tác nhân SS-ORL liên tục cải thiện so với baseline. Đáng chú ý, ngay cả khi chất lượng dữ liệu có nhãn thấp, các tác nhân SS-ORL có thể đạt được lợi tức tốt. Khi q tăng, hiệu suất của các tác nhân SS-ORL cũng tiếp tục tăng và cuối cùng khớp với hiệu suất của các tác nhân oracle.

Để đo lường định lượng cách một tác nhân SS-ORL theo dõi hiệu suất của tác nhân oracle tương ứng, chúng tôi định nghĩa khoảng cách hiệu suất tương đối của các tác nhân SS-ORL như

Perf(Oracle-ORL) − Perf(SS-ORL) / Perf(Oracle-ORL), (2)

và tương tự cho các tác nhân baseline. Hình 4.2 vẽ khoảng cách hiệu suất tương đối trung bình của các tác nhân này. So với baseline, các tác nhân SS-ORL giảm đáng kể khoảng cách hiệu suất tương đối.

Kết quả của chúng tôi tổng quát hóa thành tỷ lệ phần trăm ít hơn của dữ liệu có nhãn. Hình 4.3 vẽ khoảng cách hiệu suất tương đối của các tác nhân được huấn luyện trên tập dữ liệu walker-medium-expert, khi chỉ 1% tổng số quỹ đạo được gán nhãn. Xem Phụ lục C.3 cho thêm thí nghiệm. Các quan sát tương tự có thể được tìm thấy trong kết quả của các tập dữ liệu medium và medium-replay, xem Hình C.1 và C.2.

### 4.2 So Sánh với Các Lựa Chọn Thiết Kế Thay Thế (Q2)

**Huấn Luyện Không Có Gán Nhãn Proxy** Hình 4.4 vẽ hiệu suất của DT-Joint và các tác nhân SS-ORL trên tập dữ liệu hopper-medium-expert, sử dụng cài đặt kết hợp như trong Phần 4.1. Vì DT-Joint là một biến thể của DT, bảng bên trái so sánh DT-Joint với SS-DT cũng như baseline DT và oracle DT. DT-Joint chỉ cải thiện một cách nhẹ nhàng so với baseline DT và hoạt động kém hơn đáng kể so với SS-DT. Ngoài ra, bảng bên phải cho thấy SS-CQL, SS-DT và SS-TD3BC đều hoạt động tốt hơn nhiều so với DT-Joint. Chi tiết triển khai của DT-Joint có thể được tìm thấy trong Phụ lục F.

**Tự Huấn Luyện cho IDM** Chúng tôi xem xét một biến thể của SS-ORL trong đó tự huấn luyện được sử dụng để huấn luyện IDM. Nhớ lại rằng tự huấn luyện liên quan đến một vòng huấn luyện ban đầu chỉ sử dụng dữ liệu có nhãn, tiếp theo là nhiều vòng bổ sung sử dụng các tập huấn luyện được tăng cường. Sau mỗi vòng huấn luyện, chúng tôi cần đo lường độ không chắc chắn của các dự đoán hành động của chúng tôi và thêm những cái chắc chắn nhất vào tập huấn luyện. Để làm điều này, chúng tôi sử dụng phương pháp dựa trên ensemble (Lakshminarayanan et al., 2017) trong đó chúng tôi huấn luyện m IDM ngẫu nhiên độc lập. Chúng tôi mô hình phân phối hành động như hỗn hợp của những m phân phối ước tính đó. Toàn bộ thuật toán tự huấn luyện được trình bày trong Thuật toán 2 ở Phụ lục E.

Chúng tôi so sánh SS-CQL, SS-DT với biến thể tự huấn luyện của chúng trên các tập dữ liệu walker-medium-expert, sử dụng IDM với k = 1. Tất cả các siêu tham số và kiến trúc đều giống nhau. Chúng tôi đã thử nghiệm biến thể với kích thước ensemble 2 và 3, và với 3 và 5 vòng tăng cường. Như trước đây, chúng tôi sử dụng cài đặt kết hợp với 6 giá trị q khác nhau trong khoảng từ 10 đến 100. Để tính đến các mô hình khác nhau và các cài đặt dữ liệu khác nhau, chúng tôi báo cáo khoảng tin cậy (CI) bootstrap phân tầng 95% của mean interquartile (IQM) của lợi tức cho tất cả các trường hợp và instance huấn luyện này (Agarwal et al., 2021). Chúng tôi sử dụng 50000 lần nhân bản bootstrap để tạo ra CI. So với các thống kê khác như mean hoặc median, IQM vừa mạnh mẽ với outlier và cũng là một đại diện tốt của hiệu suất tổng thể. Bootstrap phân tầng là một công cụ tiện dụng để thu được CI với tỷ lệ bao phủ tốt, ngay cả khi chỉ có một số lượng nhỏ instance huấn luyện mỗi cài đặt. Chúng tôi tham khảo độc giả đến Agarwal et al. (2021) cho giới thiệu hoàn chỉnh. Hình 4.5 vẽ CI bootstrap 95% của lợi tức IQM trên tất cả các cài đặt. Phương pháp của chúng tôi vượt trội đáng kể so với các biến thể khác.

Thật thú vị khi điều tra MSE của dự đoán hành động cho các IDM khác nhau. Hình 4.6 cho thấy IDM của chúng tôi có lợi khi chất lượng dữ liệu có nhãn tương đối cao (q = 70, 90 và 100), nhưng nó tương đương với các IDM tự huấn luyện khi chất lượng dữ liệu có nhãn thấp hoặc trung bình (q = 10, 30 hoặc 50). Thú vị, chúng tôi thấy rằng hiệu suất cuối cùng của SS-ORL vẫn rõ ràng vượt trội trong những trường hợp đó, xem Hình 4.7.

**Kiến Trúc IDM** Chúng tôi xem xét IDM đa chuyển tiếp với kích thước cửa sổ chuyển tiếp k = 0, 1, 2, tương ứng. Để xác minh ảnh hưởng của các trạng thái tương lai đối với việc dự đoán hành động, chúng tôi cũng xem xét biến thể tích hợp k chuyển tiếp tương lai. Chúng tôi gọi các mô hình đó là IDM đối xứng và IDM của chúng tôi là IDM bất đối xứng. Khi k = 2, IDM đối xứng sẽ dự đoán at sử dụng các trạng thái st−2, . . . , st, st+1, . . . , st+3, trong khi IDM bất đối xứng của chúng tôi sẽ chỉ sử dụng các trạng thái lên đến st+1. Chúng tôi huấn luyện các tác nhân SS-CQL và SS-DT trên các tập dữ liệu walker-medium-expert sử dụng những IDM đó. Một lần nữa, chúng tôi sử dụng cài đặt kết hợp với 6 giá trị q khác nhau. Hình 4.8 vẽ CI bootstrap 95% của lợi tức IQM trên tất cả các cài đặt và instance huấn luyện. Các IDM đối xứng hoạt động tương đương với các IDM bất đối xứng, cung cấp các chứng minh thực nghiệm rằng các trạng thái tương lai vượt ra ngoài bước thời gian t + 1 độc lập với at cho trước trạng thái st+1, xem Phụ lục D. Lựa chọn k = 1 hơn nữa vượt trội hơn hai lựa chọn khác. Vì chính sách hành vi của tập dữ liệu medium-expert có thể được xem là hỗn hợp của hai chính sách (Fu et al., 2020), điều này cung cấp bằng chứng thực nghiệm rằng IDM của chúng tôi đối phó tốt hơn với nhiều chính sách hành vi so với IDM cổ điển. Trực giác của chúng tôi là có thể dễ dàng hơn để suy ra chính sách hành vi thực tế bằng một chuỗi các trạng thái quá khứ thay vì một trạng thái duy nhất.

### 4.3 Nghiên Cứu Ablation cho Các Thuộc Tính Tập Trung Vào Dữ Liệu (Q3)

Chúng tôi tiến hành thí nghiệm để điều tra hiệu suất của SS-ORL trong nhiều cài đặt dữ liệu khác nhau. Để cho phép một nghiên cứu có hệ thống, chúng tôi rời khỏi cài đặt kết hợp trong Phần 4.1 và xem xét việc tách rời Plabelled và Punlabelled. Chúng tôi sẽ thay đổi bốn giá trị có thể cấu hình: chất lượng và kích thước của cả quỹ đạo có nhãn và không nhãn, riêng lẻ trong khi giữ các giá trị khác cố định. Chúng tôi kiểm tra cách hiệu suất của các tác nhân SS-ORL thay đổi với những biến thiên này.

**Chất Lượng Dữ Liệu Có Nhãn** Chúng tôi chia các quỹ đạo ngoại tuyến thành 3 nhóm, có lợi tức là 0% đến 33% dưới cùng, 33% đến 67%, và 67% đến 100%, tương ứng. Chúng tôi gọi chúng là nhóm Low, Medium, và High. Chúng tôi đánh giá hiệu suất của SS-ORL khi các quỹ đạo có nhãn được lấy mẫu từ ba nhóm khác nhau: Low, Med, và High. Để tính đến môi trường khác nhau, phương pháp RL ngoại tuyến, và chất lượng dữ liệu không nhãn, chúng tôi xem xét tổng cộng 12 trường hợp bao gồm:

• 2 tập dữ liệu hopper-medium-expert và walker-medium-expert,
• 2 tác nhân SS-CQL và SS-DT, và
• 3 cài đặt chất lượng trong đó các quỹ đạo không nhãn được lấy mẫu từ các nhóm Low, Med, và High.

Cả số lượng quỹ đạo có nhãn và không nhãn đều được đặt là 10% tổng số quỹ đạo ngoại tuyến. Hình 4.9 báo cáo CI bootstrap 95% của lợi tức IQM cho tất cả 12 trường hợp và 5 instance huấn luyện mỗi trường hợp. Rõ ràng, khi chất lượng dữ liệu có nhãn tăng lên, hiệu suất của SS-ORL tăng đáng kể với biên độ lớn.

**Chất Lượng Dữ Liệu Không Nhãn** Tương tự như thí nghiệm trên, chúng tôi lấy mẫu các quỹ đạo không nhãn từ một trong ba nhóm, và huấn luyện các tác nhân SS-ORL dưới 12 trường hợp khác nhau trong đó chất lượng dữ liệu có nhãn thay đổi. Hình 4.10 báo cáo CI bootstrap 95% của lợi tức IQM. Hiệu suất của các tác nhân SS-ORL tăng khi chất lượng dữ liệu không nhãn tăng, và sử dụng dữ liệu không nhãn chất lượng cao vượt trội đáng kể so với hai trường hợp khác.

**Kích Thước Dữ Liệu Có Nhãn** Chúng tôi thay đổi số lượng quỹ đạo có nhãn là 10%, 25%, và 50% kích thước tập dữ liệu ngoại tuyến, trong khi số lượng quỹ đạo không nhãn được cố định là 10%. Chúng tôi huấn luyện SS-CQL và SS-DT trên tập dữ liệu walker-medium-expert dưới 9 cài đặt chất lượng dữ liệu, trong đó các quỹ đạo có nhãn và không nhãn được lấy mẫu tương ứng từ các nhóm Low, Med, và High. Hình 4.11 vẽ CI của lợi tức IQM. Cụ thể, chúng tôi xem xét kết quả được tổng hợp trên tất cả các trường hợp, và cũng cho mỗi cài đặt chất lượng dữ liệu có nhãn riêng lẻ. Đối với tất cả các trường hợp này, hiệu suất của cả SS-CQL và SS-DT vẫn tương đối nhất quán bất kể số lượng quỹ đạo có nhãn. Hiệu suất đánh giá của SS-CQL và SS-DT trong quá trình huấn luyện cho mỗi môi trường và cài đặt dữ liệu riêng lẻ, có thể được tìm thấy trong Hình G.1.

**Kích Thước Dữ Liệu Không Nhãn** Như trước đây, chúng tôi thay đổi tỷ lệ phần trăm quỹ đạo không nhãn là 10%, 25%, và 50%, trong khi cố định tỷ lệ phần trăm dữ liệu có nhãn là 10%. Chúng tôi sử dụng các cài đặt chất lượng dữ liệu giống như trong thí nghiệm trước. Hình 4.12 báo cáo CI bootstrap 95% của lợi tức IQM. Thú vị, chúng tôi thấy rằng SS-DT và SS-CQL phản ứng hơi khác nhau. SS-CQL tương đối không nhạy cảm với thay đổi trong kích thước dữ liệu không nhãn, cũng như SS-DT khi chất lượng dữ liệu có nhãn thấp hoặc trung bình. Tuy nhiên, khi dữ liệu có nhãn có chất lượng cao, hiệu suất của SS-DT giảm khi kích thước dữ liệu không nhãn tăng.

Để hiểu rõ hơn về hiện tượng này, chúng tôi điều tra hiệu suất cho SS-DT cho mỗi trong 9 cài đặt chất lượng dữ liệu. Như được thể hiện trong Hình G.2a, khi dữ liệu có nhãn có chất lượng cao nhưng dữ liệu không nhãn có chất lượng thấp hơn, việc tăng kích thước dữ liệu không nhãn làm hại hiệu suất. Trực giác của chúng tôi là, trong những trường hợp này, tập dữ liệu kết hợp sẽ có chất lượng thấp hơn tập dữ liệu có nhãn, và các phương pháp học có giám sát như DT có thể nhạy cảm với điều này. Chi tiết hơn có thể được tìm thấy trong Hình G.2.

### 4.4 Lựa Chọn Thuật Toán RL Ngoại Tuyến (Q4)

Đối với một phương pháp RL ngoại tuyến được chọn, khoảng cách hiệu suất tương đối giữa các tác nhân SS-ORL và oracle tương ứng, như được định nghĩa trong Phương trình (2), minh họa mức độ nhạy cảm với hành động bị thiếu của phương pháp RL ngoại tuyến này. Chúng tôi huấn luyện SS-CQL, SS-DT và SS-TD3BC trên 6 tập dữ liệu (môi trường hopper, walker với các tập dữ liệu medium-expert, medium, và medium-replay), sử dụng cài đặt kết hợp như trong Phần 4.1 với 6 giá trị q khác nhau: 10, 30, 50, 70, 90 và 100. Kết quả tổng hợp, được thể hiện trong Hình 4.13, chỉ ra rằng SS-TD3BC có khoảng cách hiệu suất tương đối nhỏ nhất. Điều này cho thấy TD3BC ít nhạy cảm với hành động bị thiếu hơn cả DT và CQL. Khoảng cách hiệu suất của SS-CQL và SS-DT tương đối tương tự, cho thấy DT và CQL có độ nhạy cảm tương tự với hành động bị thiếu.

## 5 Kết Luận

Chúng tôi đề xuất một cài đặt bán giám sát mới cho RL ngoại tuyến trong đó chúng tôi có quyền truy cập vào các quỹ đạo có và không có thông tin hành động. Cho cài đặt này, chúng tôi giới thiệu một quy trình meta-thuật toán đa giai đoạn đơn giản. Các thí nghiệm của chúng tôi xác định các thuộc tính chính cho phép các tác nhân tận dụng dữ liệu không nhãn và cho thấy rằng học tập gần tối ưu có thể được thực hiện chỉ với 10% hành động được gán nhãn cho các quỹ đạo chất lượng thấp đến trung bình. Công trình của chúng tôi là một bước hướng tới việc tạo ra các tác nhân thông minh có thể học từ các loại demo phụ trợ đa dạng như video trực tuyến, và sẽ thú vị khi nghiên cứu các cài đặt dữ liệu không đồng nhất khác cho RL ngoại tuyến trong tương lai, bao gồm các cài đặt không có phần thưởng hoặc chỉ có trạng thái thuần túy.
