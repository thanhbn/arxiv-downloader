# 2307.04349.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2307.04349.pdf
# Kích thước tệp: 753298 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản trong Transactions on Machine Learning Research (11/2023)
RLTF: Học Tăng Cường từ Phản Hồi Kiểm Thử Đơn Vị
Jiate Liu*jiateliu@tencent.com
Tencent
Yiqin Zhu*yiqinzhu@tencent.com
Tencent
Kaiwen Xiao loktarxiao@tencent.com
Tencent
Qiang Fu leonfu@tencent.com
Tencent
Xiao Han haroldhan@tencent.com
Tencent
Wei Yang willyang@tencent.com
Tencent
Deheng Ye†dericye@tencent.com
Tencent
Được đánh giá trên OpenReview: https://openreview.net/forum?id=hjYmsV6nXZ&noteId=7gJrNDYNSs

Tóm tắt
Mục tiêu của tổng hợp chương trình, hay sinh mã, là tạo ra mã thực thi được dựa trên các mô tả đã cho. Gần đây, đã có ngày càng nhiều nghiên cứu sử dụng học tăng cường (RL) để cải thiện hiệu suất của các mô hình ngôn ngữ lớn (LLM) cho mã. Tuy nhiên, các công trình đại diện hiện tại hoặc chỉ dựa vào các khung ngoại tuyến, hạn chế việc khám phá không gian mẫu mới, hoặc thiếu sót trong việc sử dụng tín hiệu kiểm thử đơn vị, không tính đến các vị trí lỗi cụ thể trong mã. Để giải quyết những vấn đề này, chúng tôi đề xuất RLTF, tức là Học Tăng Cường từ Phản Hồi Kiểm Thử Đơn Vị, một khung RL trực tuyến mới với phản hồi kiểm thử đơn vị đa độ chi tiết để tinh chỉnh các LLM mã. Phương pháp của chúng tôi tạo dữ liệu theo thời gian thực trong quá trình huấn luyện và đồng thời sử dụng các tín hiệu phản hồi chi tiết để hướng dẫn mô hình tạo ra mã chất lượng cao hơn. Các thí nghiệm mở rộng cho thấy RLTF đạt được hiệu suất tiên tiến nhất trên các điểm chuẩn APPS và MBPP. Mã của chúng tôi có sẵn tại: https://github.com/Zyq-scut/RLTF .

1 Giới thiệu
Tổng hợp chương trình, hay sinh mã, liên quan đến việc tạo ra một chương trình thực thi được để giải quyết một vấn đề đã cho. Lĩnh vực nghiên cứu này đã thu hút sự chú ý do tiềm năng cải thiện năng suất và khả năng tiếp cận trong lập trình. Một mô hình AI có thể tạo ra các chương trình dựa trên yêu cầu của con người có thể biến đổi đáng kể các công cụ và quy trình lập trình (Zan et al., 2023).

*Đồng tác giả đầu tiên.
†Tác giả liên hệ.
1arXiv:2307.04349v2 [cs.AI] 13 Nov 2023

--- TRANG 2 ---
Xuất bản trong Transactions on Machine Learning Research (11/2023)
Gần đây, đã có sự gia tăng đáng kể trong việc phát triển các mô hình ngôn ngữ lớn (LLM) được xây dựng trên kiến trúc Transformer (Vaswani et al., 2017), được huấn luyện trên các tập dữ liệu mã lớn không gán nhãn. Những mô hình mạnh mẽ này có khả năng tạo ra mã mà không cần hướng dẫn lập trình rõ ràng và đã cho thấy kết quả đáng chú ý trong tổng hợp chương trình. Codex (Chen et al., 2021) là một ví dụ đáng chú ý của một LLM với 12 tỷ tham số có thể giải quyết thành công hơn 70% các thách thức lập trình Python phức tạp được tạo ra bởi con người. Hơn nữa, những mô hình này đã được chứng minh là nâng cao năng suất và hiệu quả lập trình trong các ứng dụng thực tế. Kể từ khi Codex xuất hiện, nhiều LLM khác được thiết kế riêng cho lĩnh vực mã đã xuất hiện, với kích thước mô hình từ hàng triệu đến hàng tỷ tham số. AlphaCode (Li et al., 2022), chẳng hạn, khao khát giải quyết các thách thức lập trình cấp độ cạnh tranh, trong khi InCoder (Fried et al., 2022) cho phép chèn mã tại các điểm tùy ý sử dụng ngữ cảnh hai chiều. Các mô hình được đánh giá cao khác bao gồm CodeT5 (Wang et al., 2021), CodeGen (Nijkamp et al., 2022), PaLM-Coder (Chowdhery et al., 2022), PanGu-Coder (Christopoulou et al., 2022), CodeGeex (Zheng et al., 2023), và SantaCoder (Allal et al., 2023). Khi kích thước của những LLM này tăng lên, chúng thể hiện các năng lực nổi bật, bao gồm khả năng lập trình giống con người và khả năng gỡ lỗi (Zhang et al., 2022; Saunders et al., 2022).

Mặc dù các LLM đã cho thấy kết quả hứa hẹn trong các tác vụ lập trình cơ bản, vẫn còn tiến bộ cần thực hiện trong việc giải quyết các vấn đề thách thức hơn như cuộc thi lập trình. Ngoài ra, các mô hình mã được huấn luyện trước có thể bị hạn chế bởi sự phụ thuộc vào mô hình hóa ngôn ngữ có mặt nạ tự giám sát (MLM) của NLP và có thể gặp khó khăn trong việc đảm bảo tính đúng đắn cú pháp và chức năng của mã được tạo ra. Để cải thiện hiệu suất sinh mã, rất gần đây, các thuật toán dựa trên học tăng cường (RL) để sử dụng tốt hơn các LLM mã đã được đề xuất. Ví dụ, CodeRL (Le et al., 2022) đã khám phá việc tích hợp học tăng cường với tín hiệu kiểm thử đơn vị để tinh chỉnh các mô hình tổng hợp chương trình, và đề xuất một phương pháp hậu xử lý gọi là "Critic Sampling" để cải thiện thêm hiệu suất của mô hình. PPOCoder (Shojaee et al., 2023) sử dụng thuật toán Tối ưu hóa Chính sách Xấp xỉ để cải thiện CodeRL, mặc dù nó không đạt được kết quả tốt hơn trên điểm chuẩn thuật toán cạnh tranh APPS, so với CodeRL.

Điều đáng chú ý là một số phương pháp dựa trên RL hiện có (Le et al., 2022) là ngoại tuyến, có nghĩa là chúng không tương tác với môi trường một cách động trong quá trình huấn luyện, mà thay vào đó học từ dữ liệu được thu thập trước. Điều này có thể dẫn đến chất lượng huấn luyện RL bị hạn chế bởi tính đa dạng của tập dữ liệu và cản trở việc khám phá hiệu quả môi trường (Fujimoto et al., 2019; Kumar et al., 2019). Các nghiên cứu cũng cho thấy rằng RL ngoại tuyến có thể gặp phải các vấn đề như dịch chuyển phân phối (Fujimoto et al., 2019; Schrittwieser et al., 2021), dẫn đến huấn luyện không ổn định và hiệu suất chính sách dưới mức tối ưu. Ngược lại, RL trực tuyến đề cập đến kịch bản trong đó một tác nhân có thể tương tác với môi trường theo thời gian thực, chọn các hành động và quan sát tác động của chúng để có được tín hiệu trạng thái và phần thưởng. So với RL ngoại tuyến, huấn luyện RL trực tuyến thường ổn định hơn, cho phép khám phá tốt hơn môi trường, và dẫn đến việc phát triển các chính sách tối ưu hơn. Mặt khác, trong khi các phương pháp dựa trên RL hiện có có sử dụng kết quả của kiểm thử đơn vị làm tín hiệu phản hồi, việc triển khai của chúng khá đơn giản và thô. Chúng gán cùng một phần thưởng cho toàn bộ tập phim (một đoạn mã hoàn chỉnh) dựa trên kết quả nộp bài của chương trình (đúng, sai, lỗi thời gian chạy, hoặc lỗi biên dịch), mà không xem xét rằng một phần của mã có thể đúng trong khi phần khác chứa lỗi. Kết quả là, những phương pháp này không thể nắm bắt được các sắc thái trong việc xác định các thành phần mã riêng lẻ góp phần vào chức năng tổng thể và sửa lỗi cụ thể cho các phần nhất định của mã. Ngoài ra, trái ngược với các lĩnh vực như chăm sóc sức khỏe, lái xe tự động, và sự phù hợp với con người trong LLM (Levine et al., 2020; Prudencio et al., 2023), nơi chi phí tương tác thời gian thực cao hoặc dữ liệu khó có được theo thời gian thực, phản hồi từ kiểm thử đơn vị trong tác vụ tổng hợp chương trình không phát sinh chi phí đáng chú ý. Điều này có thể làm cho việc sử dụng khung trực tuyến thậm chí còn có lợi hơn trong bối cảnh này.

Để giải quyết các vấn đề nêu trên, chúng tôi giới thiệu RLTF, một khung trực tuyến mới với phản hồi kiểm thử đơn vị đa độ chi tiết nhằm nâng cao các LLM được huấn luyện trước cho tổng hợp chương trình bằng cách sử dụng học tăng cường. Đầu tiên, chúng tôi phát triển một khung trực tuyến, bao gồm hai phần chính: một phần để huấn luyện mô hình và cập nhật các tham số, và phần kia để tạo ra các chương trình sử dụng các tham số mô hình mới nhất và thêm chúng vào bộ đệm trực tuyến cho việc huấn luyện mô hình tiếp theo. Khung này cho phép mô hình có được phản hồi kiểm thử đơn vị theo thời gian thực. Ngoài ra, nó cung cấp một tập hợp mẫu đa dạng, cho phép tác nhân LLM mã khám phá môi trường hiệu quả đồng thời thúc đẩy tính ổn định trong huấn luyện. Thứ hai, chúng tôi đã phân tích phân phối các loại lỗi trong chương trình và trích xuất thông tin chi tiết hơn từ phản hồi kiểm thử đơn vị để huấn luyện, chẳng hạn như "phản hồi chi tiết" và "phản hồi thích ứng". Phản hồi chi tiết phân loại

--- TRANG 3 ---
Xuất bản trong Transactions on Machine Learning Research (11/2023)
Bảng 1: RLTF và PPOCoder áp dụng một khung trực tuyến, cho phép mô hình tạo ra các mẫu mới theo thời gian thực, trong khi CodeRL chỉ dựa vào các khung ngoại tuyến. RLTF trích xuất thông tin chi tiết hơn từ phản hồi kiểm thử đơn vị để huấn luyện, chẳng hạn như phản hồi chi tiết và phản hồi thích ứng. Cả RLTF và CodeRL đều sử dụng một kỹ thuật hậu xử lý gọi là Critic Sampling, được giới thiệu lần đầu trong (Le et al., 2022).

Tính năng | CodeRL (Le et al., 2022) | PPOCoder (Shojaee et al., 2023) | RLTF
Khung | ngoại tuyến | trực tuyến | trực tuyến
Học Có Giám Sát | ✓ | ✓ | ✓
Phản Hồi Thô | ✓ | ✓ | ✓
Phản Hồi Chi Tiết | × | × | ✓
Phản Hồi Thích Ứng | × | × | ✓
Critic Sampling | ✓ | × | ✓

lỗi dựa trên thông tin báo cáo và vị trí của chúng, phạt các phần mã sai lầm cụ thể tương ứng. Phản hồi thích ứng cung cấp phần thưởng khác nhau cho các chương trình dựa trên tỷ lệ các test case mà chúng vượt qua. Thông tin chi tiết hơn có thể được tìm thấy trong Mục 3.3.2.

Những đóng góp chính của bài báo chúng tôi như sau:

• Chúng tôi đề xuất RLTF, một khung trực tuyến mới được thiết kế đặc biệt cho các tác vụ tổng hợp chương trình. Bằng cách tạo ra các mẫu mới theo thời gian thực trong suốt quá trình huấn luyện và sử dụng kết quả kiểm thử đơn vị làm tín hiệu phản hồi, RLTF nâng cao đáng kể hiệu suất tổng thể của mô hình. Phương pháp độc đáo này cho phép các mô hình học hiệu quả các phức tạp của lỗi mã và sau đó tinh chỉnh hiệu suất của chúng một cách có mục tiêu.

• Dựa trên khung này, chúng tôi phát triển phản hồi đa độ chi tiết được trích xuất tự động từ kiểm thử đơn vị. Để mở rộng, chúng tôi giới thiệu phản hồi thô và chi tiết áp dụng cho các chương trình có lỗi, nhằm phạt các đoạn mã cụ thể nơi lỗi xuất hiện. Đối với các chương trình không vượt qua tất cả test case, chúng tôi đề xuất một cơ chế phản hồi thích ứng gán các hình phạt khác nhau dựa trên tỷ lệ test case đã vượt qua.

• Đối với tác vụ tổng hợp chương trình, phương pháp của chúng tôi đạt được kết quả tiên tiến nhất trên các điểm chuẩn APPS và MBPP, và một nghiên cứu loại bỏ chi tiết chứng minh hiệu quả của phương pháp chúng tôi. Ngoài ra, chúng tôi thực hiện kiểm tra trên các LLM khác nhau (ví dụ: CodeT5, CodeGen), minh họa tính mạnh mẽ của phương pháp và khả năng áp dụng cho các mô hình cơ sở khác nhau.

2 Công trình liên quan

2.1 LLM Được Huấn Luyện Trước cho Tổng Hợp Chương Trình

Nghiên cứu gần đây đã đi sâu vào việc tận dụng các mô hình ngôn ngữ lớn (LLM) được huấn luyện trước từ lĩnh vực xử lý ngôn ngữ tự nhiên (NLP) để tự động hóa các tác vụ tổng hợp chương trình, sử dụng dữ liệu corpus mã quy mô lớn được khai thác từ các kho lưu trữ mã nguồn mở. Đáng chú ý, có một số ví dụ nổi bật của các mô hình được huấn luyện trước như vậy bao gồm CodeBERT chỉ mã hóa (Feng et al., 2020), CodeGPT chỉ giải mã (Lu et al., 2021), CodeGen (Nijkamp et al., 2022), PaLM-Coder (Chowdhery et al., 2022), PanGu-Coder (Christopoulou et al., 2022), CodeGeex (Zheng et al., 2023), và SantaCoder (Allal et al., 2023), cũng như các kiến trúc transformer mã hóa-giải mã như PLABRT (Ahmad et al., 2021) và CodeT5 (Wang et al., 2021). Những mô hình ngôn ngữ xác suất (PL) được huấn luyện trước này đã có khả năng tạo ra mã trông ấn tượng về mặt thị giác và có cấu trúc tốt. Tuy nhiên, những mô hình này vẫn gặp thách thức trong việc đảm bảo tính đúng đắn cú pháp và chức năng của các mã được tạo ra (Chen et al., 2021; Hendrycks et al., 2021; Shojaee et al., 2023).

--- TRANG 4 ---
Xuất bản trong Transactions on Machine Learning Research (11/2023)

[Hình 1: Khung tổng thể của RLTF được đề xuất. (a) Khung học tăng cường trực tuyến: Hai LLM với trọng số chia sẻ được sử dụng trong quá trình huấn luyện. Một cái tạo ra chương trình mục tiêu và tương tác với trình biên dịch để tạo ra một cặp dữ liệu huấn luyện, sau đó được lưu trữ trong bộ đệm trực tuyến. Cái kia sử dụng dữ liệu truth cơ bản và dữ liệu bộ đệm trực tuyến để tính toán loss và cập nhật trọng số mô hình thông qua phản hồi gradient. (b) Quy trình bộ đệm trực tuyến: Bộ đệm trực tuyến được sử dụng để giữ lại dữ liệu mới được tạo ra cho huấn luyện RL. Khi dữ liệu mới được nhận, dữ liệu cũ bị xóa. Ở đây SL đề cập đến học có giám sát và RL đề cập đến học tăng cường.]

2.2 Học Tăng Cường cho Tổng Hợp Chương Trình

Học Tăng Cường (RL) là một phương pháp học chính sách tối ưu bằng cách nhận tín hiệu phần thưởng từ môi trường thực (Sutton et al., 1998). Trong những năm gần đây, RL đã cho thấy triển vọng trong việc tối ưu hóa các chỉ số không khả vi cho các tác vụ tạo chuỗi, chẳng hạn như cải thiện điểm BLEU và ROUGE trong các mô hình tóm tắt và dịch thuật tương ứng (Husain et al., 2019; Ren et al., 2020). Khác với tạo văn bản, tổng hợp chương trình đòi hỏi cả độ chính xác cú pháp và chức năng vì mã được tạo ra phải vượt qua biên dịch và kiểm thử đơn vị (Hendrycks et al., 2021; Chen et al., 2021). Gần đây, các phương pháp hướng dẫn thực thi và cơ chế tinh chỉnh dựa trên RL đã được sử dụng để nâng cao chất lượng mã được tạo ra. Ví dụ, CodeRL (Le et al., 2022) điều tra việc kết hợp học tăng cường và tín hiệu kiểm thử đơn vị để nâng cao các mô hình tổng hợp chương trình, giới thiệu một kỹ thuật hậu xử lý tên là Critic Sampling để tăng cường hiệu suất mô hình hơn nữa. PPOCoder (Shojaee et al., 2023) sử dụng thuật toán Tối ưu hóa Chính sách Xấp xỉ từ học tăng cường để tinh chỉnh CodeRL. Tuy nhiên, các phương pháp dựa trên RL hiện có vẫn gặp phải một số hạn chế. Đầu tiên, một hạn chế đáng chú ý của các phương pháp RL hiện có là chúng chỉ sử dụng một khung ngoại tuyến duy nhất, hạn chế khả năng khám phá không gian mẫu mới. Hơn nữa, các kỹ thuật hiện tại sử dụng tín hiệu kiểm thử đơn vị tương đối đơn giản, bỏ qua việc xem xét thông tin lỗi cụ thể và vị trí trong mã. Phương pháp RLTF của chúng tôi áp dụng khung trực tuyến và kết hợp phản hồi đa hạt để huấn luyện mô hình. Nhìn chung, sự khác biệt giữa RLTF và các phương pháp hiện có như CodeRL và PPOCoder được tóm tắt trong Bảng 1.

--- TRANG 5 ---
Xuất bản trong Transactions on Machine Learning Research (11/2023)

3 Phương pháp

3.1 Định nghĩa Tác vụ

Tổng hợp chương trình là một quá trình tự động tạo ra mã máy tính W dựa trên mô tả hành vi mong muốn cấp cao D. Quá trình này nhằm tăng khả năng tạo ra mã có thể giải quyết vấn đề đã cho, được biểu thị dưới dạng xác suất có điều kiện P(W|D).

Tác vụ có thể được biểu thị dưới dạng bài toán chuỗi-đến-chuỗi, và mục tiêu là tối đa hóa xác suất có điều kiện tạo ra đầu ra đúng, cho trước đầu vào và các tham số mô hình LLM:

maxP(W|D,θ) = max∏(t=1 to T) p(wt|D,θ,w1:t−1) (1)

trong đó θ là các tham số của mô hình LLM. Mô hình LLM được sử dụng để nắm bắt phân phối có điều kiện, và nó được huấn luyện bằng cách sử dụng một tập hợp các cặp đầu vào-đầu ra. Trong quá trình huấn luyện, các tham số θ được điều chỉnh để tối đa hóa khả năng tạo ra đầu ra đúng cho một đầu vào cho trước.

3.2 Khung Học Tăng Cường Trực Tuyến

Mặc dù học có giám sát có thể mang lại kết quả có giá trị, không gian tìm kiếm rộng lớn liên quan đến tổng hợp chương trình có thể làm cho việc thu thập một tập dữ liệu toàn diện trở nên thách thức. Do đó, chúng tôi đề xuất một khung học tăng cường trực tuyến, liên tục cải thiện khả năng mã của LLM thông qua các mẫu huấn luyện được tạo ra trực tuyến và phản hồi chi tiết được cung cấp bởi trình biên dịch, để vượt qua tình thế khó khăn này. Cụ thể, khung của chúng tôi kết hợp một bộ đệm trực tuyến trong quá trình huấn luyện, điều này rất quan trọng trong việc cải thiện hiệu quả của mô hình. Bộ đệm duy trì một luồng dữ liệu động của các cặp dữ liệu được sử dụng trong huấn luyện học tăng cường theo thời gian thực. Mỗi cặp dữ liệu bao gồm mô tả vấn đề D, mã được tạo ra bởi mô hình mới nhất Ŵ, và phản hồi từ trình biên dịch FB(Ŵ). Phương pháp này cho phép mô hình thích nghi với những thay đổi trong phân phối dữ liệu và tạo điều kiện cho việc học hiệu quả hơn. Khung hiện tại kết hợp actor và critic trong huấn luyện, tương tự như phương pháp được sử dụng trong CodeRL (Le et al., 2022) và PPOCoder (Shojaee et al., 2023).

Khung tổng thể được hiển thị trong Hình 1. Quá trình huấn luyện liên quan đến hai LLM với trọng số chia sẻ. LLM đầu tiên cập nhật các tham số mô hình sử dụng dữ liệu thu được từ bộ đệm để huấn luyện, trong khi LLM thứ hai hợp tác với trình biên dịch để tạo mã một cách động, đánh giá tính đúng đắn của nó, tạo ra các cặp dữ liệu, và liên tục làm mới bộ đệm theo thời gian thực. Việc kết hợp một bộ đệm được cập nhật liên tục đảm bảo tính kịp thời của dữ liệu được sử dụng để huấn luyện, từ đó cho phép mô hình khám phá không gian mẫu rộng hơn một cách toàn diện.

3.3 Học Tăng Cường từ Phản Hồi Kiểm Thử Đơn Vị

Chúng tôi đề xuất RLTF, một phương pháp để cải thiện chất lượng mã được tổng hợp bằng cách khai thác phản hồi kiểm thử đơn vị để khám phá không gian mục tiêu. Theo thuật toán tăng cường Sutton et al. (1998) Sutton & Barto (2018), chúng tôi xây dựng loss học tăng cường với mã được khám phá Ŵ như sau:

Lrl = −R(Ŵ) logP(Ŵ|D,θ) = −R(Ŵ)∑(t=S to E) logp(ŵt|D,θ,ŵ1:t−1) (2)

trong đó R(*) là hệ số phần thưởng, S và E biểu thị vị trí bắt đầu và kết thúc của các đoạn mã bị phạt. Giá trị của chúng được xác định bởi các danh mục phản hồi khác nhau.

3.3.1 Danh Mục Phản Hồi

Phản hồi kiểm thử đơn vị FB(Ŵ) có thể được thu được từ trình biên dịch chương trình và có thể được phân loại thành ba nhóm chính, cụ thể là Error (Lỗi), Failure (Thất bại), và Pass (Vượt qua).

--- TRANG 6 ---
Xuất bản trong Transactions on Machine Learning Research (11/2023)

Bảng 2: Định nghĩa các loại lỗi phụ trong Python. Chúng tôi chia các lỗi phụ khác nhau thành ba danh mục dựa trên đặc điểm của chúng: Uglobal, Uline, và Uignore.

Lỗi phụ | Mô tả | Danh mục
Syntax Error | Mã chứa lỗi cú pháp gây ra việc biên dịch thất bại | Uline hoặc Uglobal
Index Error | Hoạt động chỉ mục vượt quá giới hạn | Uline
Type Error | Một hoạt động hoặc hàm được áp dụng cho một đối tượng có kiểu không phù hợp | Uline
Value Error | Một hoạt động hoặc hàm nhận một đối số có kiểu đúng nhưng với giá trị không phù hợp | Uline
EOF Error | Hàm input() gặp điều kiện kết thúc tệp (EOF) mà không đọc được dữ liệu nào | Uline
Timeout Error | Thời gian thực thi mã vượt quá giới hạn thời gian | Uglobal
Name Error | Một tên cục bộ hoặc toàn cục không được định nghĩa | Uline
Key Error | Một khóa ánh xạ (từ điển) không được tìm thấy trong tập hợp các khóa hiện có | Uline
Import Error | Gói được nhập không được tìm thấy | Uline
ZeroDivision Error | Đối số thứ hai của phép chia hoặc phép chia lấy dư là zero | Uline
Recursion Error | Thực thi mã hoạt động đệ quy vượt quá giới hạn tối đa | Uglobal
Triple-quoted Error | Dấu ngoặc kép ba chưa hoàn thành | Uignore
Indentation Error | Định dạng thụt lề sai | Uignore
Else | - | Uline

Error đề cập đến tình huống trong đó mã được tạo ra không thể biên dịch hoặc dẫn đến lỗi chẳng hạn như các ngoại lệ ngăn chương trình hoạt động bình thường. Danh mục này có thể được chia nhỏ hơn thành lỗi cú pháp, lỗi chỉ mục, lỗi kiểu, trong số những lỗi khác.

Failure chỉ ra rằng mã được tạo ra có thể được biên dịch hoặc chạy, nhưng nó không đáp ứng được các yêu cầu chức năng hoặc hiệu suất mong đợi. Tình huống này đòi hỏi cải thiện hoặc tối ưu hóa bổ sung của chương trình. Mã thuộc danh mục này thất bại trong ít nhất một kiểm thử đơn vị.

Pass ngụ ý rằng mã được tạo ra có thể được biên dịch và chạy thành công và đáp ứng các yêu cầu chức năng và hiệu suất mong đợi. Danh mục này chỉ ra rằng chương trình đã vượt qua tất cả kiểm thử đơn vị và có thể được sử dụng mà không cần sửa đổi thêm.

3.3.2 Phản Hồi Đa Độ Chi Tiết

Phản Hồi Thô. Phản hồi thô phục vụ như một cơ chế khuyến khích cho mô hình ngôn ngữ để nâng cao xác suất tạo ra mã chính xác đồng thời giảm khả năng tạo ra mã lỗi. Chúng tôi sử dụng cùng thiết lập như CodeRL (Le et al., 2022) và PPOCoder (Shojaee et al., 2023):

Rcoarse(Ŵ) = {
1.0, FB(Ŵ) là pass
-0.3, FB(Ŵ) là failure
-0.6, FB(Ŵ) là error ngoại trừ syntax error
-1.0, FB(Ŵ) là syntax error
}, Scoarse = 0, Ecoarse = T (3)

Giá trị của Rcoarse được xác định bởi phản hồi trình biên dịch FB(Ŵ). Vị trí bắt đầu Scoarse và vị trí kết thúc Ecoarse luôn là 0 và T.

Phản Hồi Chi Tiết. Phản hồi thô phục vụ để thông báo cho mô hình rằng nó đã mắc lỗi, nhưng không chỉ ra vị trí cụ thể của lỗi. Ngược lại, mục đích của phản hồi chi tiết là cung cấp cho mô hình thông tin về lý do cụ thể cho các lỗi của nó, với mục tiêu giảm khả năng lỗi trong tương lai. Lý do cho lỗi mã có thể rất khác nhau, đôi khi bắt nguồn từ lỗi cụ thể trong mã (chẳng hạn như việc sử dụng các biến không xác định), và đôi khi từ các vấn đề với logic tổng thể của mã (chẳng hạn như lỗi đệ quy trong Python). Đối với trường hợp đầu tiên, chúng tôi nhằm mục đích để mô hình giải quyết lỗi trên dòng hiện tại, trong khi đối với trường hợp sau, chúng tôi hy vọng mô hình có thể đánh giá lại và sửa đổi

--- TRANG 7 ---
Xuất bản trong Transactions on Machine Learning Research (11/2023)

mã. Để đạt được mục tiêu này, chúng tôi đã phát triển một tập hợp phần thưởng cho các loại lỗi phụ khác nhau, có thể được tìm thấy trong Bảng 2.

Thông thường, chúng ta có thể xác định các đoạn mã lỗi từ lời nhắc trình biên dịch. Chúng tôi chia mã lỗi thành ba danh mục (Uglobal, Uline, Uignore) dựa trên các loại phụ cụ thể của chúng. Danh mục Uglobal có nghĩa là các hình phạt sẽ được áp dụng cho toàn bộ mã được tạo ra, thường là do vấn đề logic ảnh hưởng đến toàn bộ đoạn mã và không thể được xác định chính xác trên một dòng mã cụ thể. Danh mục Uline áp dụng hình phạt cho dòng mã cụ thể gây ra lỗi vì việc sửa dòng mã này sẽ ngăn chặn lỗi đó tái diễn. Danh mục Uignore tạm thời tránh hình phạt vì lý do gây ra lỗi rất nhiều và không thể được định vị chính xác. Trong trường hợp Syntax Error, nếu nó được gây ra bởi mã chưa hoàn thành do giới hạn độ dài token tối đa, nó được phân loại là Uglobal, nếu không, nó là Uline.

Rfine(Ŵ) = {
0.0, nếu Ŵ trong Uignore
-0.3, khác
}, Sfine = {
tline_start, nếu Ŵ trong Uline
0, khác
}, Efine = {
tline_end, nếu Ŵ trong Uline
T, khác
} (4)

Chúng tôi đặt Rfine(Ŵ) = 0.0 cho Uignore, nếu không là -0.3. tline_start và tline_end là vị trí bắt đầu và kết thúc của dòng mà trình biên dịch phản hồi.

Phản Hồi Thích Ứng. Phản hồi thích ứng được thiết kế để thúc đẩy mô hình tạo ra mã có thể vượt qua thành công số lượng mẫu kiểm tra tối đa. Để đạt được mục tiêu này, giá trị phần thưởng của phản hồi thích ứng tỷ lệ thuận với số lượng mẫu kiểm tra mà mã được tạo ra có thể vượt qua. Cấu hình cụ thể của phần thưởng như sau:

Radaptive(Ŵ) = -0.3 + 1.3 * Npass / (Npass + Nfail), Sadaptive = 0, Eadaptive = T (5)

Giá trị của Radaptive tương quan dương với tỷ lệ vượt qua của kiểm thử đơn vị. Vị trí bắt đầu và kết thúc luôn là 0 và T.

3.4 Mục Tiêu Tối Ưu Hóa

Để nâng cao hiệu suất của LLM được huấn luyện trước thông qua RLTF, các mục tiêu tối ưu hóa sau có thể được sử dụng để tinh chỉnh:

Ltotal = Lsl + Lcoarse + Lfine + Ladaptive (6)

trong đó Lsl là loss học có giám sát làm cho quá trình huấn luyện ổn định. Chúng tôi áp dụng cross-entropy loss như Lsl như sau:

Lsl = -logP(W|D,θ) = -∑(t=1 to T) logp(wt|D,θ,w1:t-1) (7)

Và Lcoarse, Lfine, Ladaptive là các biến thể của loss học tăng cường được hiển thị trong Phương trình 2, đại diện cho ba loại độ chi tiết phản hồi, như sau:

--- TRANG 8 ---
Xuất bản trong Transactions on Machine Learning Research (11/2023)

Lcoarse = -(Rcoarse(Ŵ) - Rcoarse(Ŵbaseline))∑(t=Scoarse to Ecoarse) logp(ŵt|D,θ,ŵ1:t-1) (8)

Lfine = -αRfine(Ŵ)∑(t=Sfine to Efine) logp(ŵt|D,θ,ŵ1:t-1) (9)

Ladaptive = -(Radaptive(Ŵ) - Radaptive(Ŵbaseline))∑(t=Sadaptive to Eadaptive) logp(ŵt|D,θ,ŵ1:t-1) (10)

Để giải quyết sự dao động biên độ trong phản hồi chi tiết gây ra bởi các giá trị biến của S và E, chúng tôi sử dụng một trọng số thích ứng α cân bằng phương trình. Trọng số này được tính là α = T / (Efine - Sfine), làm cho phản hồi chi tiết tương đương với phản hồi thô và phản hồi thích ứng. Ở đây Ŵbaseline là baseline được cập nhật động trong quá trình huấn luyện, đại diện cho mã tối ưu lịch sử được tạo ra bởi mô hình dưới mô tả ngôn ngữ tự nhiên D. Mục này chỉ ra rằng chúng tôi muốn mô hình liên tục vượt trội hơn phiên bản lịch sử của chính nó. Đối với phản hồi chi tiết, baseline của nó khó định nghĩa, vì vậy chúng tôi không thêm mục này vào hàm loss của nó.

4 Thí nghiệm

Các baseline của chúng tôi là các công trình tiên tiến nhất về việc kết hợp RL với LLM mã (PPOCoder (Shojaee et al., 2023) và CodeRL (Le et al., 2022)), và chúng tôi sử dụng cùng các điểm chuẩn và thiết lập như của họ để đánh giá.

4.1 Điểm Chuẩn

Điểm Chuẩn APPS. Chúng tôi đầu tiên đánh giá bằng cách sử dụng điểm chuẩn tổng hợp chương trình APPS (Automated Programming Progress Standard) thách thức được trình bày bởi (Hendrycks et al., 2021), vì nó có một loạt đa dạng các bài toán lập trình từ nhiều trang web lập trình khác nhau, trình bày các mức độ khó khác nhau. Điểm chuẩn bao gồm tổng cộng 10.000 bài toán lập trình, với phân chia train-test bằng nhau. Trung bình, mỗi bài toán được đi kèm với 23.2 chương trình Python chính xác và 21.2 kiểm thử đơn vị. Độ dài trung bình của mỗi bài toán là 293.2 từ, trong khi độ dài chương trình trung bình là 18.0 dòng. Tập dữ liệu được phân loại thành ba mức độ khó: Introductory (3,639; train/test = 2,639/1,000), Interview (5,000; train/test = 2,000/3,000), và Competition (1,361; train/test = 361/1,000). Thông thường, mỗi mẫu có 20 kiểm thử đơn vị để đánh giá tính đúng đắn chức năng của các chương trình. Chúng tôi tuân thủ các bước tiền xử lý tương tự như trong (Hendrycks et al., 2021) để tạo ra các chuỗi đầu vào dựa trên mô tả bài toán.

Lưu ý rằng, trong quá trình triển khai khung trực tuyến của chúng tôi, chúng tôi gặp phải một vấn đề trong mã nguồn mở APPS, cụ thể là trong phần kiểm thử đơn vị. Nếu mã được tạo ra gây ra lỗi "segmentation fault", nó không thể được bỏ qua bằng cách sử dụng "try" và "except", dẫn đến chương trình chính bị kẹt và làm gián đoạn quá trình tạo mẫu. Để giải quyết vấn đề này, chúng tôi đã sửa đổi tất cả các quy trình mã kiểm thử đơn vị để được thực thi thông qua các cuộc gọi subprocess, đảm bảo chương trình chính không bị kẹt và cho phép quá trình tạo mẫu trực tuyến chạy mượt mà. Chúng tôi tin rằng cần thiết phải tiết lộ điều này cho cộng đồng vì lý do tái tạo.

Đối với điểm chuẩn APPS, chúng tôi sử dụng khung RLTF để tinh chỉnh mô hình CodeT5 được huấn luyện trước. Chúng tôi sử dụng một máy được trang bị 8 GPU NVIDIA V100, mỗi cái có 32GB bộ nhớ, cho mục đích huấn luyện. Mỗi GPU mang một kích thước batch là 32, và tỷ lệ học 2e-6 được sử dụng. Quá trình huấn luyện mất khoảng 24 giờ. Đồng thời, ba máy bổ sung với cấu hình GPU V100 8 card tương tự được sử dụng để tạo ra các mẫu mới nhất. Chúng tôi cập nhật bộ đệm trực tuyến mỗi 50 bước. Theo cùng phương pháp như CodeRL, một nửa số bước dành cho huấn luyện SL, trong khi nửa còn lại tập trung vào huấn luyện RL. Độ dài của bộ đệm trực tuyến được đặt là 6400. Sau khi tạo ra 6400 mẫu bằng mô hình ban đầu, quá trình huấn luyện chính thức bắt đầu, và bộ đệm trực tuyến được cập nhật như một hàng đợi.

--- TRANG 9 ---
Xuất bản trong Transactions on Machine Learning Research (11/2023)

Bảng 3: Đánh giá định lượng trên điểm chuẩn APPS.

Phương pháp | Kích thước | CS | pass@1 | pass@5 | pass@1000
--- | --- | --- | --- | --- | ---
|| | Intro | Inter | Comp | all | Intro | Inter | Comp | all | Intro | Inter | Comp | all
Codex | 12B | w/o | 4.14 | 0.14 | 0.02 | 0.92 | 9.65 | 0.51 | 0.09 | 2.25 | 25.02 | 3.70 | 3.23 | 7.87
AlphaCode | 1B | w/o | - | - | - | - | - | - | - | - | 17.67 | 5.24 | 7.06 | 8.09
GPT3 | 175B | w/o | 0.20 | 0.03 | 0.00 | 0.06 | - | - | - | - | - | - | - | -
GPT2 | 0.1B | w/o | 1.00 | 0.33 | 0.00 | 0.40 | 2.70 | 0.73 | 0.00 | 1.02 | - | - | - | -
GPT2 | 1.5B | w/o | 1.30 | 0.70 | 0.00 | 0.68 | 3.60 | 1.03 | 0.00 | 1.58 | 27.90 | 9.83 | 11.40 | 13.76
GPT-Neo | 2.7B | w/o | 3.90 | 0.57 | 0.00 | 1.12 | 5.50 | 0.80 | 0.00 | 1.58 | 27.90 | 9.83 | 11.40 | 13.76
CodeRL | 770M | w/o | 4.00 | 0.78 | 0.15 | 1.30 | 9.83 | 2.03 | 0.69 | 3.32 | 35.30 | 13.33 | 13.60 | 17.78
PPOCoder(original) | 770M | w/o | 5.20 | 1.00 | 0.50 | 1.74 | 9.10 | 2.50 | 1.20 | 3.56 | 35.20 | 13.35 | 13.90 | 17.77
PPOCoder(scale) | 770M | w/o | 4.06 | 0.79 | 0.15 | 1.32 | 9.97 | 2.06 | 0.70 | 3.37 | 35.42 | 13.37 | 13.65 | 17.84
RLTF | 770M | w/o | 4.16 | 0.97 | 0.20 | 1.45 | 10.12 | 2.65 | 0.82 | 3.78 | 38.30 | 15.13 | 15.90 | 19.92
CodeRL | 770M | w | 7.08 | 1.86 | 0.75 | 2.69 | 16.37 | 4.95 | 2.84 | 6.81 | 40.00 | 15.67 | 17.90 | 20.98
RLTF | 770M | w | 8.40 | 2.28 | 1.10 | 3.27 | 18.60 | 5.57 | 3.70 | 7.80 | 39.70 | 15.03 | 16.80 | 20.32

Trong quá trình kiểm tra, chúng tôi sử dụng lấy mẫu Nucleus với giá trị top là 0.95 và đặt tham số nhiệt độ là 0.6.

Điểm Chuẩn MBPP. Để đánh giá thêm khung của chúng tôi, chúng tôi cũng sử dụng một tập dữ liệu tổng hợp chương trình Python bổ sung, nhỏ hơn và đơn giản hơn gọi là MBPP (Mostly Basic Programming Problems), được giới thiệu bởi (Austin et al., 2021). Tập dữ liệu bao gồm 974 instances, với 374 instances để huấn luyện, 90 instances để xác thực, và 500 instances để kiểm tra, trong khi dành 10 instances cho đánh giá học few-shot. Các bài toán chủ yếu ngắn gọn, thường chỉ có một câu mô tả ngôn ngữ tự nhiên. Mỗi bài toán được đi kèm với một giải pháp đúng (trung bình 6.8 dòng mã) và ba kiểm thử đơn vị, là các câu lệnh assert để xác minh tính đúng đắn chức năng. Trái ngược với APPS, các kiểm thử đơn vị của MBPP không được ẩn và được tích hợp rõ ràng vào các chuỗi nguồn cho các mô hình tổng hợp chương trình. Mặc dù điều này đôi khi khuyến khích các mô hình overfit các câu lệnh assert thông qua hard-coding một biểu thức if, chúng tôi tuân thủ cùng cấu trúc chuỗi nguồn như công trình trước đó để đảm bảo so sánh công bằng với các baseline. Cụ thể, chúng tôi sử dụng cùng định dạng prompt như (Austin et al., 2021) để chuẩn bị chuỗi đầu vào, bao gồm mô tả bài toán + "Your code should satisfy these tests:" + ba câu lệnh assert. Chúng tôi thí nghiệm với tập dữ liệu MBPP trong thiết lập zero-shot, như CodeRL. Trong khi tạo mẫu, chúng tôi cũng sử dụng lấy mẫu Nucleus với giá trị top là 0.95 và đặt tham số nhiệt độ là 1.2.

4.2 Đánh Giá Định Lượng trên APPS

Để so sánh công bằng, chúng tôi sử dụng CodeT5 770M làm mô hình cơ sở, tương tự như CodeRL (Le et al., 2022), PPOCoder (Shojaee et al., 2023). Bảng 3 trình bày kết quả tổng thể của phương pháp RLTF của chúng tôi dựa trên mô hình CodeT5 trên điểm chuẩn APPS. Chúng tôi so sánh phương pháp của mình với các mô hình lớn hơn như Codex (Chen et al., 2021), AlphaCode (Li et al., 2022), GPT2 (Radford et al., 2019), GPT3 (Brown et al., 2020), GPT-Neo (Black et al., 2021), và các phương pháp dựa trên CodeT5 khác như CodeRL (Le et al., 2022) và PPOCoder (Shojaee et al., 2023). Lưu ý rằng theo mặc định, kết quả của các LM được huấn luyện trước (ngoại trừ Codex và GPT3) là từ các mô hình được tinh chỉnh trên APPS chỉ sử dụng loss tiêu chuẩn Lce, và kết quả của CodeRL, PPOCoder, RLTF là từ các mô hình được tinh chỉnh trên APPS sử dụng cả loss Lce và Lrl. Kết quả chỉ ra rằng các phương pháp dựa trên CodeT5 vượt trội hơn những phương pháp sử dụng các mô hình cơ sở tham số lớn hơn khác như GPT. Hơn nữa, việc sử dụng phương pháp RLTF dẫn đến cải thiện hiệu suất bổ sung, vượt qua các phương pháp dựa trên CodeT5 khác như CodeRL và PPOCoder, từ đó đạt được một tiên tiến mới (SOTA) trong lĩnh vực này. Quan trọng là chúng tôi đã sử dụng mô hình CodeRL nguồn mở chính thức và áp dụng lấy mẫu nucleus với nhiệt độ 0.6. Chúng tôi nhận thấy rằng điểm pass@1 của chúng tôi thấp hơn kết quả được báo cáo trong bài báo của họ, trong khi điểm pass@5 của chúng tôi cao hơn. Kết quả tương tự cũng được tìm thấy bởi những người khác và được thảo luận tại https://github.com/salesforce/CodeRL/issues/30. Về PPOCoder, kết quả được báo cáo trong bài báo của họ cũng khác với những kết quả trong bài báo CodeRL. Vì các tác giả không mở nguồn mã điểm chuẩn APPS và các mô hình, chúng tôi không chắc chắn về quá trình đánh giá chính xác của họ. Do đó, chúng tôi sử dụng kết quả thu được từ mô hình CodeRL nguồn mở làm tiêu chuẩn và sử dụng tỷ lệ để

--- TRANG 10 ---
Xuất bản trong Transactions on Machine Learning Research (11/2023)

Bảng 4: Nghiên cứu loại bỏ: Tác động của khung.

Khung | pass@1 | pass@5 | pass@10
--- | --- | --- | ---
| Intro | Inter | Comp | all | Intro | Inter | Comp | all | Intro | Inter | Comp | all
Offline | 3.67 | 0.84 | 0.16 | 1.29 | 9.25 | 2.40 | 0.74 | 3.43 | 12.41 | 3.36 | 1.33 | 4.76
Offline+RLTF | 3.71 | 0.93 | 0.19 | 1.34 | 9.28 | 2.51 | 0.85 | 3.53 | 12.60 | 3.50 | 1.50 | 4.92
Online | 3.94 | 0.92 | 0.17 | 1.37 | 9.68 | 2.43 | 0.75 | 3.50 | 13.01 | 3.41 | 1.35 | 4.92
Online+RLTF | 4.16 | 0.97 | 0.20 | 1.45 | 10.12 | 2.65 | 0.82 | 3.78 | 13.59 | 3.67 | 1.45 | 5.21

tạo ra kết quả cuối cùng. Để cung cấp cho độc giả hiểu biết rõ ràng về nguồn kết quả, chúng tôi đồng thời bao gồm kết quả cho PPOCoder(original) và PPOCoder(scale) trong Bảng 3. Ở đây, PPOCoder(original) đại diện cho kết quả từ bài báo gốc; tuy nhiên, vì chúng tôi không trực tiếp so sánh với nó, chúng tôi đã đánh dấu màu xám. PPOCoder(scale), mặt khác, biểu thị kết quả được tỷ lệ hóa. Điều này đảm bảo so sánh công bằng giữa các phương pháp khác nhau, đồng thời thừa nhận sự khác biệt trong các giá trị được báo cáo. Ngay cả khi không chọn kết quả được tỷ lệ hóa, hiệu suất của chúng tôi tốt hơn kết quả được báo cáo trong bài báo gốc, ngoại trừ pass@1.

Ngoài ra, chúng tôi so sánh hiệu suất của mô hình của chúng tôi với phương pháp hậu xử lý Critic Sampling, được đề xuất lần đầu trong (Le et al., 2022). Do việc không có sẵn mã nguồn mở Critic Sampling từ CodeRL, chúng tôi đã tái tạo Critic Sampling dựa trên các chi tiết được cung cấp trong bài báo gốc. Chúng tôi sử dụng cả refine và repair và đặt M, số lượng ứng viên hàng đầu được chọn từ các mẫu chương trình thất bại trong kiểm thử đơn vị ví dụ là 1. Chúng tôi cũng giới hạn số lần lặp repair và refine tối đa là 1. Đáng chú ý là chúng tôi tìm thấy một tham số quan trọng N không được đề cập rõ ràng trong bài báo gốc: tổng số mẫu chương trình được tạo ra trong mỗi bước refine/repair. Khi đo pass@1, chúng tôi đặt N = 5; cho pass@5, chúng tôi đặt N = 20; và cho pass@1000, chúng tôi đặt N = 1000. Kết quả của chúng tôi cho thấy Critic Sampling đạt hiệu suất tốt hơn khi k = 1 hoặc 5 trong pass@k. Tuy nhiên, khi k lớn hơn, việc cải thiện hiệu suất tương đối nhỏ, khác với kết quả được trình bày trong bài báo gốc. Nhìn chung, chúng tôi nghĩ Critic Sampling là một phương pháp hậu xử lý tập trung vào cách sử dụng mô hình tốt hơn thay vì cách huấn luyện mô hình tốt hơn. Do đó, việc so sánh phương pháp của chúng tôi với kết quả không có Critic Sampling cung cấp một đánh giá công bằng hơn.

4.3 Nghiên Cứu Loại Bỏ

Chúng tôi tiến hành các loại bỏ mở rộng để hiểu hiệu quả của các kỹ thuật chúng tôi phát triển.

Tác Động của Khung. Bảng 4 trình bày nghiên cứu loại bỏ so sánh các khung huấn luyện khác nhau (trực tuyến hoặc ngoại tuyến) và việc sử dụng phương pháp RLTF. Tất cả kết quả đã sử dụng loss kết hợp Lsl và Lcoarse. Kết quả cho thấy sự kết hợp của khung trực tuyến và RLTF mang lại hiệu suất tốt nhất, trong khi khung ngoại tuyến không có RLTF tạo ra kết quả kém nhất. Việc áp dụng khung trực tuyến hoặc phương pháp RLTF riêng lẻ dẫn đến một mức độ cải thiện nhất định, xác nhận hiệu quả của cả hai phương pháp. Thú vị là chúng tôi quan sát thấy rằng sự thúc đẩy hiệu suất mà phương pháp RLTF mang lại đáng kể hơn trong khung trực tuyến. Điều này có thể được quy cho việc tạo ra mẫu theo thời gian thực trong thiết lập trực tuyến, giúp mô hình xác định thêm nhiều lỗi tiềm tàng trong chương trình và do đó tạo điều kiện cho việc huấn luyện tốt hơn.

Tác Động của Phản Hồi. Bảng 5 trình bày nghiên cứu loại bỏ so sánh các kết hợp phản hồi khác nhau trong quá trình huấn luyện học tăng cường (RL). Kết quả cho thấy việc chỉ sử dụng học có giám sát mang lại hiệu suất kém nhất. Ngược lại, việc sử dụng học tăng cường và dần dần kết hợp các phần thưởng được đề cập trong Mục 3 dẫn đến hiệu suất mô hình ngày càng tốt hơn. Kết quả tốt nhất được thu được khi sử dụng kết hợp phản hồi thô, chi tiết và thích ứng, xác minh hiệu quả của những thiết kế phần thưởng khác nhau này. Ngoài ra, chúng tôi quan sát thấy rằng việc bao gồm phản hồi chi tiết góp phần vào sự thúc đẩy hiệu suất đáng kể nhất trong số các phần thưởng được đánh giá. Chúng tôi cũng đánh giá kết quả với tất cả phản hồi ngoại trừ phản hồi thô: pass@1 1.38%, pass@5 3.58%, pass@10 5.05%. Mục đích của phản hồi thô là thông báo cho mô hình về kết quả kiểm tra tổng thể của mã (đúng, sai, lỗi thời gian chạy

--- TRANG 11 ---
Xuất bản trong Transactions on Machine Learning Research (11/2023)

Bảng 5: Loại bỏ: tác động của các kết hợp phản hồi khác nhau.

SL | Coarse | Fine | Adaptive | pass@1 | pass@5 | pass@10 | pass@100 | pass@1000
--- | --- | --- | --- | --- | --- | --- | --- | ---
√ | - | - | - | 1.30 | 3.39 | 4.68 | 10.51 | 17.80
√ | √ | - | - | 1.37 | 3.50 | 4.92 | 10.63 | 18.31
√ | √ | √ | - | 1.41 | 3.67 | 5.10 | 11.08 | 19.32
√ | √ | √ | √ | 1.45 | 3.78 | 5.21 | 11.23 | 19.92

Bảng 6: Loại bỏ: tác động của phản hồi chi tiết Rfine(Ŵ).

Rfine(Ŵ) | pass@1 | pass@5 | pass@10
--- | --- | --- | ---
0.0 | 1.31 | 3.42 | 4.72
-0.1 | 1.39 | 3.59 | 4.99
-0.2 | 1.38 | 3.67 | 5.10
-0.3 | 1.40 | 3.62 | 5.04
-0.4 | 1.39 | 3.68 | 5.06
-0.5 | 1.36 | 3.57 | 5.00

Bảng 7: Loại bỏ: tác động của nhiệt độ trong quá trình huấn luyện.

Temperature | pass@1 | pass@5 | pass@10
--- | --- | --- | ---
0.2 | 1.34 | 3.56 | 4.99
0.6 | 1.37 | 3.60 | 5.06
1.0 | 1.45 | 3.78 | 5.21

lỗi, lỗi biên dịch). Do đó, chúng tôi nghĩ nó cũng khá quan trọng. Như có thể thấy, việc loại bỏ phản hồi thô dẫn đến suy giảm trong các chỉ số hiệu suất. Cũng đáng chú ý là hiệu suất không có phản hồi thô tốt hơn hiệu suất chỉ có phản hồi thô.

Tác Động của Rfine(Ŵ). Bảng 6 minh họa tác động của Rfine(Ŵ) khác nhau cho phản hồi chi tiết đối với huấn luyện mô hình. Chúng ta có thể quan sát thấy rằng các giá trị khác nhau của Rfine(Ŵ) dẫn đến hiệu suất mô hình được cải thiện, với mỗi giá trị đưa ra kết quả tốt hơn so với những kết quả thu được mà không có hình phạt nào.

Tác Động của Nhiệt Độ. Bảng 7 chứng minh hiệu ứng của việc thay đổi nhiệt độ trong quá trình huấn luyện. Nhiệt độ cao hơn dẫn đến việc tạo ra các mẫu đa dạng hơn, trong khi nhiệt độ thấp hơn dẫn đến các mẫu bảo thủ hơn. Chúng tôi quan sát thấy rằng mô hình hoạt động tốt hơn khi nhiệt độ được đặt ở 1, và tương đối kém hơn khi đặt ở 0.2. Điều này cho thấy rằng việc tạo ra các mẫu đa dạng hơn trong quá trình huấn luyện giúp mô hình khám phá phạm vi không gian mẫu rộng hơn, tạo điều kiện cho huấn luyện RL.

Tác Động của Mô Hình Ngôn Ngữ. Để chứng minh tính mạnh mẽ của phương pháp RLTF, chúng tôi tiến hành thí nghiệm sử dụng một mô hình cơ sở khác, CodeGen 2.7B, ngoài CodeT5. Như được hiển thị trong Bảng 8, kết quả chỉ ra rằng việc áp dụng phương pháp RLTF trên CodeGen 2.7B cũng mang lại hiệu suất ấn tượng, dẫn đến cải thiện gần 1% trong pass@10. Đáng chú ý, chúng tôi thấy rằng mô hình cơ sở càng lớn, sự thúc đẩy hiệu suất do RLTF cung cấp càng lớn. Điều này cho thấy rằng phương pháp RLTF có thể khai thác hiệu quả tiềm năng của các mô hình cơ sở khác nhau để tạo ra mã tốt hơn, với tác động rõ rệt hơn khi kích thước mô hình cơ sở lớn hơn.

4.4 Đánh Giá Định Lượng trên MBPP

Để chứng minh thêm hiệu quả của RLTF, chúng tôi đánh giá hiệu suất zero-shot của mô hình CodeT5 được huấn luyện với RLTF trên điểm chuẩn APPS khi được kiểm tra trên điểm chuẩn MBPP (Mostly Basic Python Problems), như được hiển thị trong Bảng 9. Để đánh giá này, các phương pháp dựa trên mô hình CodeT5, bao gồm CodeRL, PPOCoder, và RLTF, được đánh giá trong thiết lập zero-shot. Đối với các mô hình dựa trên GPT, chúng được huấn luyện trước trên 2.93 tỷ tài liệu web sử dụng mục tiêu mô hình hóa ngôn ngữ tiêu chuẩn, và được tinh chỉnh trên tập huấn luyện của tập dữ liệu MBPP. Kết quả được trình bày trong bảng được thu được từ Hình 3 của bài báo gốc (Austin et al., 2021). Kết quả cho thấy RLTF vượt trội hơn các mô hình GPT có kích thước khác nhau trên điểm chuẩn MBPP và đạt được hiệu suất tiên tiến mới trong số các phương pháp dựa trên CodeT5.

--- TRANG 12 ---
Xuất bản trong Transactions on Machine Learning Research (11/2023)

Bảng 8: Loại bỏ: các mô hình ngôn ngữ lớn khác nhau làm backbone.

Mô Hình Ngôn Ngữ | RLTF | pass@1 | pass@5 | pass@10 | pass@100 | pass@1000
--- | --- | --- | --- | --- | --- | ---
CodeT5 770M | w/o | 1.30 | 3.39 | 4.68 | 10.51 | 17.80
CodeT5 770M | w | 1.45 | 3.78 | 5.21 | 11.23 | 19.92
CodeGen 2.7B | w/o | 1.64 | 4.23 | 5.79 | 12.48 | 21.44
CodeGen 2.7B | w | 2.04 | 5.04 | 6.80 | 14.17 | 23.96

Bảng 9: Đánh giá định lượng trên điểm chuẩn MBPP (zero-shot).

Phương pháp | Kích thước | trạng thái | pass@1 | pass@80
--- | --- | --- | --- | ---
GPT | 224M | fine-tuned | - | 7.2
GPT | 442M | fine-tuned | - | 12.6
GPT | 1B | fine-tuned | - | 22.4
GPT | 4B | fine-tuned | - | 33.0
GPT | 8B | fine-tuned | - | 40.6
GPT | 68B | fine-tuned | - | 53.6
GPT | 137B | fine-tuned | - | 61.4
CodeT5 + CodeRL | 770M | zero-shot | 25.7 | 68.1
CodeT5 + PPOCoder | 770M | zero-shot | 26.1 | 68.2
CodeT5 + RLTF | 770M | zero-shot | 30.4 | 71.3

4.5 Phân Tích Định Tính theo Kết Quả Kiểm Thử Đơn Vị

Hình 2 trình bày phân tích định tính theo kết quả kiểm thử đơn vị trước và sau khi áp dụng phương pháp RLTF trên mô hình CodeGen. Đối với 5000 bài toán trong điểm chuẩn APPS, chúng tôi sử dụng lấy mẫu Nucleus để tạo ra trung bình 20 giải pháp cho mỗi bài toán và ghi lại kết quả của chúng. Hình 2a mô tả tỷ lệ phần trăm của các kết quả kiểm thử đơn vị khác nhau, như được định nghĩa trong Phương trình 3, bao gồm error, failure, và pass. Chúng ta có thể quan sát thấy rằng phương pháp RLTF có thể giảm tỷ lệ các chương trình dẫn đến lỗi và tăng tỷ lệ các chương trình vượt qua, đặc biệt là đối với các bài toán có mức độ khó introductory. Sự gia tăng quan sát được trong tỷ lệ thất bại bắt nguồn từ việc sửa các mã lỗi, dẫn đến kết quả pass hoặc fail. Điều này minh họa rằng RLTF hiệu quả hơn trong việc giải quyết lỗi thời gian chạy và lỗi trình biên dịch so với lỗi ngữ nghĩa, vẫn còn thách thức hơn. Hình 2b minh họa tỷ lệ phần trăm của các lỗi phụ khác nhau trong số kết quả lỗi trước và sau khi áp dụng phương pháp RLTF. Hầu hết các lỗi cho thấy sự suy giảm về tỷ lệ sau khi sử dụng phương pháp RLTF, đặc biệt là lỗi cú pháp. Cũng đáng chú ý là tỷ lệ lỗi timeout có sự gia tăng nhỏ, có thể được quy cho việc sửa các lỗi liên quan đến ngữ pháp khác dẫn đến lỗi timeout.

5 Kết Luận và Hướng Nghiên Cứu Tương Lai

Trong bài báo này, chúng tôi đã đề xuất RLTF (Reinforcement Learning from unit Test Feedback), một khung RL trực tuyến mới với phản hồi kiểm thử đơn vị đa độ chi tiết, để tinh chỉnh các mô hình ngôn ngữ lớn cho các tác vụ tổng hợp chương trình. So với các công trình hiện có, phương pháp của chúng tôi tạo ra dữ liệu tức thì trong quá trình huấn luyện và đồng thời sử dụng độ chi tiết tinh tế hơn trong tín hiệu phản hồi để hướng dẫn mô hình tạo ra mã chất lượng cao hơn. Các thí nghiệm mở rộng chứng minh rằng RLTF vượt qua các phương pháp RL hiện có cho lập trình và có thể được áp dụng cho nhiều LLM khác nhau, bao gồm CodeT5 và CodeGen. Hơn nữa, nó đạt được hiệu suất tiên tiến trên các điểm chuẩn được sử dụng rộng rãi bao gồm APPS và MBPP.

Trong tương lai, có một số hướng để cải thiện thêm RLTF. Ví dụ, các ví dụ đầu vào-đầu ra trong các điểm chuẩn hiện có có thể không đủ đa dạng, và các chương trình được tạo ra bằng cách sử dụng các ví dụ đầu vào-đầu ra ẩn có thể không phải là phiên bản mã cuối cùng đúng. Những hạn chế như vậy có thể hạn chế hiệu suất của RLTF. Do đó, việc sử dụng LLM để tạo ra một tập hợp các ví dụ đầu vào-đầu ra đa dạng và chính xác hơn là một hướng nghiên cứu tiềm năng đáng khám phá. Ngoài ra, liệu các tín hiệu phản hồi chi tiết hơn, chẳng hạn như những tín hiệu từ các trình phân tích mã tĩnh, có thể nâng cao hiệu suất của RLTF hơn nữa, là một con đường khả thi khác để theo đuổi. Ngoài ra, lưu ý rằng khả năng chuyển giao là một vấn đề quan trọng. Việc phân loại thủ công các loại lỗi phụ mà chúng tôi sử dụng làm cho việc chuyển RLTF sang các ngôn ngữ lập trình khác trở nên thách thức, điều này nên được coi là một hạn chế khác của công trình này. Như một bước tiếp theo, chúng tôi tin rằng việc triển khai phân loại tự động sẽ nâng cao đáng kể khả năng áp dụng của RLTF trong bối cảnh rộng hơn.

Tài liệu tham khảo

Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, và Kai-Wei Chang. Unified pre-training for program understanding and generation. arXiv preprint arXiv:2103.06333, 2021.

Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. Santacoder: don't reach for the stars! arXiv preprint arXiv:2301.03988, 2023.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.

Sid Black, Leo Gao, Phil Wang, Connor Leahy, và Stella Biderman. Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow. If you use this software, please cite it using these metadata, 58, 2021.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, et al. Evaluating large language models trained on code.(2021). arXiv preprint arXiv:2107.03374, 2021.

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

--- TRANG 13 ---
Xuất bản trong Transactions on Machine Learning Research (11/2023)

Fenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li, Qi Zhang, Meng Xiao, Bo Shen, Lin Li, et al. Pangu-coder: Program synthesis with function-level language modeling. arXiv preprint arXiv:2207.11280, 2022.

Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155, 2020.

Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, và Mike Lewis. Incoder: A generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999, 2022.

Scott Fujimoto, David Meger, và Doina Precup. Off-policy deep reinforcement learning without exploration. In International conference on machine learning, pp. 2052-2062. PMLR, 2019.

Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021.

Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis, và Marc Brockschmidt. Codesearchnet challenge: Evaluating the state of semantic code search. arXiv preprint arXiv:1909.09436, 2019.

Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, và Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. Advances in Neural Information Processing Systems, 32, 2019.

Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, và Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314-21328, 2022.

Sergey Levine, Aviral Kumar, George Tucker, và Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.

Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):1092-1097, 2022.

Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al. Codexglue: A machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664, 2021.

Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, và Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022.

Rafael Figueiredo Prudencio, Marcos ROA Maximo, và Esther Luna Colombini. A survey on offline reinforcement learning: Taxonomy, review, and open problems. IEEE Transactions on Neural Networks and Learning Systems, 2023.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.

Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, và Shuai Ma. Codebleu: a method for automatic evaluation of code synthesis. arXiv preprint arXiv:2009.10297, 2020.

William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, và Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.

--- TRANG 14 ---
Xuất bản trong Transactions on Machine Learning Research (11/2023)

Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou, và David Silver. Online and offline reinforcement learning by planning with a learned model. Advances in Neural Information Processing Systems, 34:27580-27591, 2021.

Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, và Chandan K Reddy. Execution-based code generation using deep reinforcement learning. arXiv preprint arXiv:2301.13816, 2023.

Richard S Sutton và Andrew G Barto. Reinforcement learning: An introduction. 2018.

Richard S Sutton, Andrew G Barto, et al. Introduction to reinforcement learning, volume 135. MIT press Cambridge, 1998.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Yue Wang, Weishi Wang, Shafiq Joty, và Steven CH Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859, 2021.

Daoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Yongji Wang, và Jian-Guang Lou. Large language models meet nl2code: A survey, 2023.

Jialu Zhang, José Cambronero, Sumit Gulwani, Vu Le, Ruzica Piskac, Gustavo Soares, và Gust Verbruggen. Repairing bugs in python assignments using large language models. arXiv preprint arXiv:2209.14876, 2022.

Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, et al. Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. arXiv preprint arXiv:2303.17568, 2023.

15

--- TRANG 15 ---
Xuất bản trong Transactions on Machine Learning Research (11/2023)

0.0 1.0 2.0 3.0 4.0 5.0

Intro Inter Comp All

Error (%)

w/o RLTF w RLTF

0.0 2.0 4.0 6.0 8.0

Intro Inter Comp All

Failure (%)

w/o RLTF w RLTF

0.0 2.0 4.0 6.0 8.0

Intro Inter Comp All

Pass (%)

w/o RLTF w RLTF

(a) Tỷ lệ phần trăm kết quả kiểm thử đơn vị.

0.0 2.0 4.0 6.0 8.0 10.0

SyntaxError IndexError TypeError ValueError EOFError TimeoutError NameError KeyError ImportError ZeroDivisionError Else

SubError (%)

w/o RLTF w RLTF

(b) Tỷ lệ phần trăm các lỗi phụ khác nhau.

Hình 2: Phân tích định tính theo kết quả kiểm thử đơn vị trên CodeGen.
