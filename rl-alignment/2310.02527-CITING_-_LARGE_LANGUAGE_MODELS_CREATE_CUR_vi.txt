# 2310.02527.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2310.02527.pdf
# Kích thước tệp: 768554 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
CITING: CÁC MÔ HÌNH NGÔN NGỮ LỚN TẠO CHƯƠNG TRÌNH ĐÀO TẠO CHO ĐIỀU CHỈNH HƯỚNG DẪN

Tao Feng
Đại học Thanh Hoa
Bắc Kinh, Trung Quốc
ft19@tsinghua.org.cn

Zifeng Wang∗& Jimeng Sun
Khoa Khoa học Máy tính
Đại học Illinois Urbana Champaign
Urbana, IL, Hoa Kỳ
{zifengw2,jimeng}@illinois.edu

TÓM TẮT
Tiến bộ gần đây của các mô hình ngôn ngữ lớn (LLM) đã được đạt được thông qua sự kết hợp của điều chỉnh hướng dẫn và căn chỉnh con người. Tuy nhiên, việc xây dựng các bộ dữ liệu hướng dẫn được tạo thủ công và thực hiện căn chỉnh con người trở thành nút thắt cổ chai cho việc mở rộng quy mô phát triển LLM. Trong bài báo này, chúng tôi khai thác ý tưởng tận dụng các mô hình AI thay cho con người làm giáo viên để đào tạo các LLM học sinh. Phương pháp của chúng tôi được lấy cảm hứng từ cách học sinh con người tinh chỉnh kỹ năng viết của họ bằng cách tuân theo các tiêu chí đánh giá và học hỏi từ các bản sửa đổi được cung cấp bởi gia sư của họ. Cụ thể, chúng tôi sử dụng một LLM giáo viên để tạo chương trình giảng dạy cho việc điều chỉnh hướng dẫn của LLM học sinh, được gọi là Điều chỉnh Hướng dẫn Chương trình giảng dạy (CITING). Nó bao gồm hai bước chính: (1) LLM giáo viên tạo các tiêu chí đánh giá để đánh giá các câu trả lời tương ứng với các loại câu hỏi khác nhau, và (2) LLM học sinh học cách tuân theo các tiêu chí và thực hiện tự sửa lỗi từ việc sửa đổi được thực hiện bởi giáo viên. Chúng tôi tiếp tục thực hiện điều này một cách lặp đi lặp lại để thể hiện quy trình của CITING. Chúng tôi so sánh CITING với một loạt các phương pháp cơ sở hiện đại trên bốn bộ dữ liệu. Phương pháp của chúng tôi cho thấy sự cải thiện mạnh mẽ về mặt rõ ràng, sâu sắc và toàn diện theo đánh giá của GPT-4. Cụ thể, nó đạt được tỷ lệ thắng trung bình 79,4% so với SFT, 73,4% so với RLHF, 78,1% so với RRHF, và 76,3% so với RAFT.

1 GIỚI THIỆU
Mô hình Ngôn ngữ Lớn (LLM), được trang bị điều chỉnh hướng dẫn (Wei et al., 2021) và học hỏi từ phản hồi con người (LHF) (Ouyang et al., 2022), đã chứng minh khả năng vô song trong việc hiểu và tạo ra văn bản giống con người (Alberts et al., 2023). Khả năng của nó trải dài trên vô số ứng dụng, bao gồm tạo nội dung (Abburi et al., 2023), tạo mã (Vaithilingam et al., 2022), và trả lời các truy vấn (Li et al., 2023). Về mặt kỹ thuật, chúng ta thường cần thu thập các câu trả lời chất lượng cao được viết bởi con người tương ứng với các câu hỏi đầu vào cho việc điều chỉnh hướng dẫn. Do đó, LHF thường được công thức hóa như một nhiệm vụ xếp hạng cho các đánh giá viên con người vì việc đánh giá chất lượng đầu ra của mô hình hiệu quả hơn nhiều so với việc viết một câu trả lời chất lượng cao từ đầu.

Tuy nhiên, việc xây dựng các bộ dữ liệu hướng dẫn hoặc căn chỉnh LLM với phản hồi con người đều đòi hỏi chi phí lao động và thời gian đáng kể. Do đó, các nhà nghiên cứu được thúc đẩy để chưng cất kiến thức của các LLM tiên tiến để tạo điều kiện cho việc đào tạo LLM học sinh, bao gồm việc xây dựng các bộ dữ liệu hướng dẫn với các câu trả lời được tạo bởi LLM và học hỏi từ phản hồi AI (LAIF):

•Điều chỉnh hướng dẫn với dữ liệu tổng hợp. Các câu trả lời được tạo từ các LLM tiên tiến, ví dụ GPT-4, thường phù hợp với chất lượng của các câu trả lời con người (OpenAI, 2023). Phát hiện này thúc đẩy ý tưởng tinh chỉnh các LLM học sinh với đầu ra từ LLM giáo viên (Ho et al., 2022; Magister et al., 2022). Tuy nhiên, cách tiếp cận này gây ra hiệu suất không tối ưu vì dữ liệu tổng hợp thường chứa các ảo giác được tạo ra bởi LLM (Ji et al., 2023).

•Học hỏi từ phản hồi AI. Được báo cáo rằng LLM có thể vượt trội hơn các đánh giá viên con người trong nhiều nhiệm vụ chú thích văn bản (Gilardi et al., 2023). Điều này gợi ý tiềm năng cho việc sử dụng AI

∗Đóng góp ngang bằng với tác giả đầu tiên.

--- TRANG 2 ---
LLM học sinh đưa ra phản hồi ban đầu

Tiêu chí:
Một câu trả lời tốt cho những loại câu hỏi này sẽ chính xác, ngắn gọn và đúng trọng tâm.

LLM giáo viên đưa ra tiêu chí

Phản hồi đã sửa đổi:
Đối với hình chữ nhật có chiều dài 10 đơn vị và chiều rộng 5 đơn vị, diện tích là 10 × 5 = 50 đơn vị vuông.

LLM giáo viên sửa đổi phản hồi

Hướng dẫn:
Tính diện tích của hình chữ nhật có chiều dài 10 đơn vị và chiều rộng 5 đơn vị.

LLM giáo viên đưa ra hướng dẫn

Phản hồi ban đầu:
Đối với hình chữ nhật có một cạnh 10 đơn vị và cạnh kia 5 đơn vị, bạn chỉ cần làm 10 + 5 sẽ được 15 đơn vị vuông.

LLM giáo viên dạy LLM học sinh sửa đổi phản hồi

Hình 1: Quá trình điều chỉnh hướng dẫn chương trình giảng dạy. LLM giáo viên dạy LLM học sinh sửa đổi phản hồi của nó dựa trên tiêu chí và phản hồi đã sửa đổi.

phản hồi để nâng cao hiệu suất của LLM học sinh bằng học tăng cường, được gọi là RLAIF (Bai et al., 2022; Lee et al., 2023). Tuy nhiên, các LLM dựa trên RLAIF vẫn kém hơn so với các đối tác RLHF vì phản hồi AI về cơ bản là một "truyền tải có tổn thất" của sở thích con người. Ngoài ra, các phương pháp này dựa trên học tăng cường có tính nhạy cảm với các siêu tham số và tốn kém về mặt tính toán (Yuan et al., 2023).

[Biểu đồ cho thấy CITING vs. RLHF trên bốn nhiệm vụ: Alpaca, World, Reading, Reasoning]

Hình 2: CITING vs. RLHF trên bốn nhiệm vụ.

Trong bài báo này, chúng tôi lập luận rằng việc khai thác khả năng tạo sinh của LLM giáo viên để làm cho LAIF xuất sắc là rất quan trọng. Trong LHF, các nhãn dán con người thấy việc xếp hạng dễ dàng hơn việc viết, nhưng hai nhiệm vụ này thực sự có mức độ khó tương đương đối với LLM. Với cái nhìn sâu sắc này, chúng tôi đề xuất viết lại phản hồi ban đầu của LLM học sinh bằng LLM giáo viên, sau đó hoạt động như sự giám sát cho LLM học sinh. Cách tiếp cận của chúng tôi lấy cảm hứng từ quá trình học tập của con người, phản ánh cách học sinh tinh chỉnh kỹ năng viết thông qua thực hành và bắt chước các bản thảo được đánh bóng từ gia sư của họ. Cụ thể, chúng tôi sử dụng LLM giáo viên để tạo ra chương trình giảng dạy được tùy chỉnh cho việc điều chỉnh hướng dẫn, mà chúng tôi gọi là Điều chỉnh Hướng dẫn Chương trình giảng dạy (CITING). Minh họa khái niệm được hiển thị trong Hình 1. Đóng góp kỹ thuật của phương pháp chúng tôi tập trung vào cái nhìn sâu sắc sau:

•Thiết kế tiêu chí với mô hình giáo viên: chúng tôi sử dụng LLM giáo viên để tạo ra các tiêu chí đánh giá chất lượng phản hồi của học sinh tương ứng với các loại câu hỏi khác nhau. Các tiêu chí này cũng cung cấp hướng dẫn bổ sung cho LLM học sinh để sửa chữa các câu trả lời không tốt.

•Học cách sửa đổi: dựa trên phản hồi ban đầu từ LLM học sinh, chúng tôi sử dụng LLM giáo viên để cung cấp các bản sửa đổi được cá nhân hóa. Đối chiếu việc sửa đổi và câu trả lời ban đầu, học sinh học cách cải thiện phản hồi của họ thông qua tự phản ánh. Chúng ta có thể tiếp tục tinh chỉnh học sinh bằng cách lặp lại quá trình này.

Như được hiển thị trong Hình 2, chúng tôi xác định thực nghiệm rằng CITING vượt trội hơn RLHF với biên độ lớn, đạt được tỷ lệ thắng tổng thể 73,4% theo đánh giá GPT-4. Trong phần tiếp theo, chúng tôi thảo luận về các bài báo liên quan trong Phần 2. Sau đó, chúng tôi trình bày chi tiết về phương pháp của chúng tôi trong Phần 3. Chúng tôi hiển thị thêm kết quả thực nghiệm trong Phần 4, nơi CITING thể hiện hiệu suất few-shot (Alpaca) và zero-shot (Kiến thức Thế giới, Đọc hiểu, Lý luận Thông thường) đáng chú ý so với các phương pháp cơ sở.

2 CÔNG TRÌNH LIÊN QUAN

Căn chỉnh LLM. Được nhấn mạnh rằng việc căn chỉnh LLM với sở thích con người tăng cường hiệu quả và an toàn của chúng, ví dụ như học tăng cường từ phản hồi con người (RLHF) (Ouyang et al., 2022). Các nhà nghiên cứu sau đó được khuyến khích giảm thiểu tính không ổn định và không đầy đủ của học tăng cường (RL) trong quá trình căn chỉnh con người (Yuan et al., 2023; Dong et al., 2023). Mặt khác,

--- TRANG 3 ---
chi phí lao động và thời gian cao của LHF cũng trở thành mối quan tâm cho việc mở rộng quy mô phát triển LLM. Thách thức này thúc đẩy một loạt nghiên cứu trong việc chuyển các mô hình AI lớn, ví dụ GPT-4, để giám sát các LLM nhỏ hơn, được gọi là "Học từ Phản hồi AI (LAIF)". Các ví dụ nổi bật bao gồm giảm tính có hại của LLM (Bai et al., 2022) và xấp xí hiệu suất của RLHF với một nhãn dán AI thay cho con người (Lee et al., 2023). Tuy nhiên, LAIF chưa thể hiện sự vượt trội về hiệu suất so với các phương pháp LHF.

Điều chỉnh Hướng dẫn. LLM có thể học cách tuân thủ các yêu cầu của người dùng bằng cách học từ các bộ dữ liệu hướng dẫn được tạo thủ công (Brown et al., 2020; Zhang et al., 2023). Một cách thay thế là thu thập dữ liệu hướng dẫn bằng cách sử dụng các LLM có khả năng nhất để tạo ra các câu trả lời cho nhiều hướng dẫn (Wang et al., 2022). Tuy nhiên, việc áp dụng trực tiếp các bộ dữ liệu hướng dẫn tổng hợp tạo ra các khiếm khuyết như sự sụp đổ chế độ do các ảo giác, chất lượng thấp và các mẫu không cân bằng được tạo ra bởi LLM (Shumailov et al., 2023). Các công trình tiếp theo nhằm tinh chỉnh quá trình điều chỉnh hướng dẫn tổng hợp bằng cách nhắc LLM giáo viên cung cấp các lý do suy luận đa bước (Zelikman et al., 2022) hoặc bằng đào tạo tiến triển (Hsieh et al., 2023). Ngược lại, phương pháp của chúng tôi tận dụng LLM giáo viên để tùy chỉnh một bản sửa đổi dựa trên câu trả lời của học sinh thay vì tạo ra câu trả lời tham khảo từ đầu. Nó giảm thiểu rủi ro sụp đổi chế độ và cung cấp phản hồi cụ thể hơn cho LLM học sinh.

3 PHƯƠNG PHÁP

3.1 TỔNG QUAN HỆ THỐNG

[Hình 3 mô tả tổng quan về CITING với hai phần chính: Thiết kế Tiêu chí từ LLM Giáo viên và Điều chỉnh Hướng dẫn Chương trình giảng dạy cho LLM Học sinh]

Hình 3: Tổng quan về CITING. Nó chủ yếu bao gồm hai phần: Thiết kế Tiêu chí từ LLM Giáo viên và Điều chỉnh Hướng dẫn Chương trình giảng dạy cho LLM Học sinh. Trong phần đầu tiên, LLM giáo viên thiết kế các tiêu chí cho các loại nhiệm vụ hướng dẫn khác nhau. Trong phần thứ hai, LLM giáo viên dạy LLM học sinh sửa đổi phản hồi ban đầu của nó dựa trên tiêu chí.

Nhiệm vụ điều chỉnh hướng dẫn thường nhập vào một hướng dẫn x và xuất ra một phản hồi để bắt chước phản hồi thật y. Khác với điều này, khung của chúng tôi bao gồm một tiêu chí c như đầu vào mô tả tiêu chuẩn đánh giá cho các câu trả lời. Do đó, chúng tôi ký hiệu dữ liệu của mình là (x, y, c) ∼ D, trong đó D là phân phối của dữ liệu.

Dựa trên đầu vào và đầu ra của mô hình được giới thiệu ở trên, chúng tôi đề xuất CITING với LLM học sinh và LLM giáo viên để dạy LLM học sinh sửa đổi phản hồi của nó. Tổng quan cấu trúc được hiển thị trong Hình 3. Hệ thống chủ yếu bao gồm hai phần:

•Thiết kế Tiêu chí bởi LLM Giáo viên. Đầu tiên nó sử dụng LLM giáo viên để phân loại tất cả các hướng dẫn thành nhiều loại và thiết kế tiêu chí cho mỗi loại. Trong giai đoạn kiểm tra, một mô-đun khớp tương tự dựa trên BERT sau đó được phát triển để phân loại các hướng dẫn mới thành các loại này.

--- TRANG 4 ---
•Điều chỉnh Hướng dẫn Chương trình giảng dạy cho LLM Học sinh. LLM học sinh học cách sửa đổi phản hồi ban đầu của nó từ LLM giáo viên dựa trên các tiêu chí từ phần đầu tiên thông qua điều chỉnh hướng dẫn.

3.2 THIẾT KẾ TIÊU CHÍ VỚI LLM GIÁO VIÊN

Để cho phép mô hình ngôn ngữ học cách thực hiện tự phản ánh, việc cung cấp cho mô hình các tiêu chí rõ ràng để đánh giá chất lượng phản hồi của nó đối với các hướng dẫn cụ thể là điều cần thiết. Trong phần này, chúng tôi sẽ trình bày chi tiết về cách thiết lập các tiêu chí tương ứng với mỗi hướng dẫn.

Phân loại hướng dẫn và thiết kế tiêu chí Sử dụng mô hình ngôn ngữ để đưa ra tiêu chí cho từng mẫu hướng dẫn có thể tốn kém, tạo ra thách thức cho việc mở rộng quy mô. Để giải quyết mối quan tâm này, chúng tôi chọn để mô hình ngôn ngữ phân loại các hướng dẫn trong bộ dữ liệu và tạo ra các tiêu chí tương ứng cho mỗi danh mục. Cụ thể, chúng tôi lấy mẫu một tập con các hướng dẫn từ bộ dữ liệu và tạo một lời nhắc chuyên dụng để phân loại chúng kết hợp với các tiêu chí.

Vui lòng phân loại các hướng dẫn sau và đưa ra tiêu chí tốt hoặc xấu cho mỗi danh mục:
Các hướng dẫn đã cho: [...]

Kết quả là, các hướng dẫn này được chia thành M danh mục và tiêu chí của danh mục thứ i của các hướng dẫn là ci ∼ C.

Khớp tương tự của tiêu chí và hướng dẫn kiểm tra Trong giai đoạn kiểm tra, đối với một hướng dẫn mới, chúng tôi đề xuất khớp các tiêu chí của các hướng dẫn hiện có để hỗ trợ mô hình ngôn ngữ trong quá trình tự sửa đổi của nó. Chúng tôi tận dụng sentence-BERT (Reimers & Gurevych, 2019) để mã hóa các hướng dẫn và tiêu chí thành các embedding compact trong cùng không gian ngữ nghĩa. Cụ thể, đối với mỗi hướng dẫn xh ∼ Dh đã có tiêu chí và mỗi hướng dẫn mới xn ∼ Dn, chúng ta có:

eh = BERT(xh), en = BERT(xn). (1)

Do đó chúng ta có được tập embedding cho mỗi danh mục j là Ej. Tiếp theo, chúng tôi tính toán độ tương tự theo mẫu cho hướng dẫn xn với Ej và thực hiện mean-pooling để có được điểm tương tự cuối cùng Scorej:

Scorej = (1/n) Σ(k=1 to n) Cosine(en, ekh), (2)

trong đó ekh biểu thị phần tử thứ k của Ej. Cuối cùng, đối với hướng dẫn xn, chúng tôi gán tiêu chí tương ứng với danh mục có điểm tương tự cao nhất trong số M danh mục.

3.3 ĐIỀU CHỈNH HƯỚNG DẪN CHƯƠNG TRÌNH GIẢNG DẠY CHO LLM HỌC SINH

Đến nay, chúng tôi đã có được một bộ dữ liệu điều chỉnh hướng dẫn (x, y, c) ∼ D, trong đó x là hướng dẫn, y là câu trả lời thật và c là tiêu chí cho hướng dẫn này. Quá trình điều chỉnh hướng dẫn của CITING có hai giai đoạn, như được mô tả dưới đây.

Tinh chỉnh Có giám sát Giai đoạn đầu tiên tuân theo thực hành điều chỉnh hướng dẫn tiêu chuẩn, khuyến khích LLM tuân thủ các yêu cầu của người dùng khi trả về đầu ra. Đối với một hướng dẫn đầu vào được cung cấp, được biểu diễn là x, các nhãn dán cung cấp một phản hồi đại diện cho hành vi mong muốn, bao gồm t token và được ký hiệu là y = {y1, ..., yt}. Chúng tôi đào tạo LLM để tạo ra fSFT thông qua:

LSFT = -Σt log P fSFT(yt|x, y1, ..., yt-1). (3)

Điều chỉnh Hướng dẫn Chương trình giảng dạy với Tiêu chí Chúng tôi nhắc fSFT tạo ra phản hồi ban đầu r(0) cho hướng dẫn x. Chúng tôi thiết kế một lời nhắc sử dụng LLM giáo viên để cải thiện r(0) hiện tại dựa trên hướng dẫn x, tiêu chí c, và phản hồi ban đầu r(0) để có được phản hồi đã sửa đổi r(1):

--- TRANG 5 ---
Thuật toán 1 CITING
Đầu vào: Một bộ dữ liệu hướng dẫn x, y ∼ D, chứa các hướng dẫn x và các phản hồi thật tương ứng y; một LLM giáo viên; số lượng điều chỉnh hướng dẫn chương trình giảng dạy N; số vòng suy luận M.

1: Khởi tạo LLM học sinh π với trọng số ngẫu nhiên;
// Đào tạo với SFT
2: cho i = 0,1,2... thực hiện
3:    Đào tạo LLM học sinh với SFT bằng cách tối thiểu hóa phương trình (3) sử dụng bộ dữ liệu hướng dẫn;
4: kết thúc cho
// Chuẩn bị cho Tiêu chí
5: Sử dụng LLM giáo viên để tóm tắt tiêu chí cho mỗi danh mục của các hướng dẫn đầu vào;
6: Một phương pháp khớp tương tự dựa trên BERT được sử dụng để có được tiêu chí cho các hướng dẫn còn lại dựa trên phương trình (1) và (2).
// Điều chỉnh Hướng dẫn Chương trình giảng dạy
** Đào tạo
7: cho k = 0,1,2...N thực hiện
8:    Sử dụng mô hình LLM học sinh π(k) để tạo ra các phản hồi r(k) của nó cho các hướng dẫn x của lần lặp thứ k;
9:    Sử dụng LLM giáo viên để sửa đổi các phản hồi r(k) dựa trên (x, c, r(k)) và có được các phản hồi đã sửa đổi r(k+1);
10:   Tinh chỉnh π(k) dựa trên lời nhắc pr và (x, c, r(k), r(k+1)) để có được π(k+1) bằng cách tối thiểu hóa phương trình (4).
11: kết thúc cho
** Suy luận
12: cho j = 0,1,2...M thực hiện
13:   Sử dụng LLM học sinh đã tinh chỉnh π* để tạo ra các phản hồi r(j) của nó cho các hướng dẫn x của lần lặp thứ j;
14:   Sử dụng LLM học sinh π* để sửa đổi các phản hồi r(j) dựa trên (x, c, r(j)) và có được các phản hồi đã sửa đổi r(j+1).
15: kết thúc cho

Dưới đây là một hướng dẫn và phản hồi của nó. Ngoài ra, một tiêu chí cho hướng dẫn được đưa ra để cung cấp tiêu chuẩn đánh giá tốt hoặc xấu cho việc hoàn thành hướng dẫn này. Vui lòng sửa đổi phản hồi theo hướng dẫn và tiêu chí đã cho.
Hướng dẫn: [ ]
Phản hồi: [ ]
Tiêu chí: [ ]
Phản hồi đã sửa đổi là:

Do đó, chúng ta có thể có được bộ dữ liệu điều chỉnh hướng dẫn (x, y, c, r(0), r(1)) ∼ D. Lời nhắc chứa pr(x, c, r(0)) được tận dụng để giám sát mô hình học sinh fSFT thông qua điều chỉnh hướng dẫn, như:

LCITING = -Σt log Pf(0)CITING(r(1)t|pr(x, c, r(0)), r(1)1, ..., r(1)t-1), (4)

để tạo ra mô hình CITING vòng đầu tiên f(0)CITING. Cần lưu ý rằng chúng ta có thể lặp lại quá trình này bằng cách (1) nhắc giáo viên sửa đổi đầu ra từ f(0)CITING để xây dựng bộ dữ liệu hướng dẫn chương trình giảng dạy và (2) tinh chỉnh f(0)CITING bằng Phương trình 4 để có được mô hình f(1)CITING. Trong phần thí nghiệm, chúng tôi sẽ chứng minh cách quá trình lặp này tạo điều kiện cho học sinh.

3.4 ĐÀO TẠO VÀ SUY LUẬN MÔ HÌNH

Chúng tôi tóm tắt chi tiết của quá trình đào tạo và suy luận của CITING trong Thuật toán 1. Từ thuật toán, LLM học sinh được đào tạo với SFT sử dụng bộ dữ liệu hướng dẫn (dòng 2-4). Sau đó, chúng tôi sử dụng LLM giáo viên và phương pháp khớp tương tự dựa trên BERT để có được tiêu chí cho tất cả các hướng dẫn (dòng 5-6). Hơn nữa, chúng tôi đào tạo LLM học sinh π bằng cách tối thiểu hóa log-likelihood thông qua điều chỉnh hướng dẫn chương trình giảng dạy (dòng 7-11). Cuối cùng, chúng tôi sử dụng LLM học sinh đã tinh chỉnh π* để sửa đổi phản hồi từng bước (dòng 12-15).

4 THÍ NGHIỆM

4.1 THIẾT LẬP THÍ NGHIỆM

Bộ dữ liệu Chúng tôi chủ yếu tiến hành thí nghiệm trên bốn bộ dữ liệu sau: Alpaca (Taori et al., 2023) là một bộ dữ liệu gồm 52.000 hướng dẫn và minh họa được tạo bởi công cụ text-davinci-003 của OpenAI. Bộ dữ liệu này chứa các loại câu hỏi và phản hồi khác nhau, như kiến thức chung, đọc hiểu, lý luận logic, v.v. Nó được sử dụng rộng rãi để tiến hành điều chỉnh hướng dẫn cho các mô hình ngôn ngữ và làm cho mô hình ngôn ngữ tuân theo hướng dẫn tốt hơn; Kiến thức Thế giới tập trung vào các sự kiện, khái niệm, thực thể, mối quan hệ và thông tin bối cảnh về môi trường bên ngoài, lịch sử, văn hóa, khoa học và nhiều hơn nữa. Chúng tôi đã tích hợp bộ dữ liệu NaturalQuestions (Kwiatkowski et al., 2019) và bộ dữ liệu TriviaQA (Joshi et al., 2017) để tạo thành bộ dữ liệu Kiến thức Thế giới; Đọc hiểu là một bộ sưu tập được sắp xếp của các đoạn văn bản kèm theo một tập hợp các câu hỏi. Đối với mỗi câu hỏi, có một phản hồi tương ứng có thể được trích xuất trực tiếp từ, hoặc suy luận bằng cách sử dụng, đoạn văn liên quan. Trong bài báo này, chúng tôi trộn bộ dữ liệu SQuAD (Rajpurkar et al., 2018), bộ dữ liệu QuAC (Choi et al., 2018) và bộ dữ liệu BoolQ (Clark et al., 2019) để tạo thành bộ dữ liệu Đọc hiểu; Lý luận Thông thường là một bộ sưu tập được sắp xếp của các câu hỏi được thiết kế để đánh giá khả năng của các mô hình học máy thực hiện lý luận dựa trên kiến thức thông thường. Các câu hỏi trong bộ dữ liệu này thường được kết hợp với các phản hồi, và có thể với các giải thích hoặc biện minh làm sáng tỏ quá trình lý luận. Cụ thể, bộ dữ liệu Lý luận Thông thường của chúng tôi là sự kết hợp của bộ dữ liệu PIQA (Bisk et al., 2020), bộ dữ liệu OpenBookQA (Mihaylov et al., 2018) và bộ dữ liệu CommonsenseQA (Talmor et al., 2018).

Chỉ số Đánh giá Nhiều nghiên cứu đã chỉ ra rằng khả năng của GPT-4 trong việc đánh giá các phản hồi khác nhau cho câu hỏi đã đạt đến hoặc thậm chí vượt qua con người (Gilardi et al., 2023). Do đó, trong bài báo của chúng tôi, chúng tôi sử dụng GPT-4 để bắt chước đánh giá con người để so sánh hai phản hồi ngẫu nhiên và đưa ra so sánh giữa chúng (thắng/thua/hòa). Để so sánh chất lượng của các phản hồi khác nhau một cách toàn diện hơn, chúng tôi để GPT-4 đánh giá từ ba khía cạnh sau:

•Rõ ràng là để đánh giá một phản hồi được trình bày rõ ràng và trôi chảy như thế nào, nhìn vào cấu trúc, chất lượng ngôn ngữ và khả năng đọc tổng thể. Cụ thể, nó sẽ đánh giá chất lượng của phản hồi từ các khía cạnh sau: (1) Tính đúng đắn về ngữ pháp và cú pháp; (2) Luồng thông tin logic; (3) Tránh thuật ngữ chuyên môn, hoặc nếu thuật ngữ chuyên môn được sử dụng, nó được giải thích đúng cách;

•Sâu sắc tập trung vào mức độ triệt để một chủ đề hoặc câu hỏi được giải quyết. Một phản hồi sâu sắc đi sâu vào chi tiết và sắc thái, thay vì chỉ sờ nắm bề mặt. Chi tiết, các tiêu chí đánh giá của nó cụ thể là: (1) Bao phủ các nguyên tắc hoặc khái niệm cốt lõi; (2) Kết hợp các quan điểm sắc thái hoặc sự kiện ít được biết đến; (3) Thể hiện sự hiểu biết vượt ra ngoài mức độ cơ bản;

•Toàn diện đánh giá độ rộng của một phản hồi và về việc bao phủ một loạt các khía cạnh hoặc chủ đề phụ liên quan. Hơn nữa, nó chủ yếu đưa ra các đánh giá cụ thể từ các khía cạnh sau: (1) Giải quyết nhiều góc độ hoặc khía cạnh của câu hỏi; (2) Kết hợp các quan điểm hoặc góc nhìn khác nhau; (3) Đảm bảo không có chủ đề phụ chính hoặc thông tin liên quan nào bị bỏ lại.

4.2 CHI TIẾT TRIỂN KHAI

Trong công trình của chúng tôi, LLaMA-7B¹ được sử dụng làm mô hình xương sống, đã phát triển thành một khu vực nghiên cứu phổ biến cho các nghiên cứu LLM (Touvron et al., 2023). Chúng tôi tinh chỉnh nó với thuật toán CITING được xây dựng trên Thư viện Huggingface (Muennighoff et al., 2023). Cụ thể, chúng tôi chia bộ dữ liệu Alpaca thành tập huấn luyện, tập xác thực và tập kiểm tra theo tỷ lệ 8:1:1. Đầu tiên chúng tôi tinh chỉnh mô hình LLaMA-7B với tập huấn luyện và tập xác thực dựa trên khung LoRA (Hu et al., 2021) sử dụng học có giám sát. Sau đó, chúng tôi lấy mẫu 1.000 hướng dẫn và sử dụng mô hình đã tinh chỉnh để tạo ra các phản hồi ban đầu tương ứng. Sau đó, dựa trên GPT-3.5, chúng tôi sửa đổi các phản hồi ban đầu này theo tiêu chuẩn chất lượng phản hồi của hướng dẫn. Chúng tôi thực hiện tinh chỉnh đa lần với thuật toán CITING dựa trên 1000 hướng dẫn này và các cặp mẫu phản hồi đã sửa đổi.

¹https://huggingface.co/decapoda-research/llama-7b-hf

--- TRANG 6 ---
--- TRANG 7 ---
Bảng 1: So sánh hiệu suất trên tất cả các phương pháp cơ sở trong bốn bộ dữ liệu.

[Bảng chi tiết hiển thị kết quả so sánh CITING với SFT, RLHF, RRHF và RAFT trên bốn bộ dữ liệu Alpaca, World Knowledge, Reading Comprehension và Commonsense Reasoning theo ba tiêu chí Articulate, In-depth và Comprehensive]

Chúng tôi đánh giá các mô hình đã tinh chỉnh trên tập kiểm tra của bộ dữ liệu Alpaca. Hơn nữa, chúng tôi đánh giá kết quả zero-shot trên tập kiểm tra của các bộ dữ liệu Kiến thức Thế giới, Đọc hiểu và Lý luận Thông thường.

Độ dài chuỗi, epoch và tốc độ học cho tất cả các thí nghiệm được đặt lần lượt là 512, 4 và 1e-5, trong khi số lượng token mới tối đa được tạo ra trong quá trình suy luận là 512. Chúng tôi sử dụng 8 GPU NVIDIA GeForce RTX 3090 để tinh chỉnh, đào tạo CITING thường mất 4-6 giờ.

4.3 PHƯƠNG PHÁP CƠ SỞ

Chúng tôi so sánh CITING với các phương pháp cơ sở zero-shot được tinh chỉnh trên LLaMA-7B có cùng xương sống với CITING:

SFT (Sun et al., 2023) là một phương pháp cơ bản tinh chỉnh trực tiếp các mô hình ngôn ngữ sử dụng học có giám sát; RLHF được thúc đẩy liên tiếp bởi (Brown et al., 2020) và (Nakano et al., 2021) để căn chỉnh lõi của các mô hình ngôn ngữ với sở thích con người trong môi trường học tăng cường. Chúng tôi triển khai RLHF theo trlx²; RRHF (Yuan et al., 2023) tính đến xếp hạng ứng cử viên và phân biệt các ứng cử viên khác nhau thông qua các hàm mất mát xếp hạng theo cặp. Chúng tôi triển khai nó với mã chính thức³; RAFT (Dong et al., 2023) là một khung mới được giới thiệu để căn chỉnh các mô hình nền tảng sinh với đạo đức và sở thích con người một cách hiệu quả. Giải quyết sự không hiệu quả của các phương pháp trước như RLHF, RAFT sử dụng mô hình phần thưởng để chọn các mẫu chất lượng cao trong khi loại bỏ những mẫu không mong muốn, tạo ra một bộ dữ liệu luồng cho việc căn chỉnh mô hình.

4.4 KẾT QUẢ CHÍNH

Chúng tôi báo cáo hiệu suất của mô hình của chúng tôi và các phương pháp cơ sở cạnh tranh trong Bảng 4.3. Từ bảng này, chúng tôi đưa ra các quan sát sau:

•CITING liên tục đạt được hiệu suất tốt nhất trong tất cả bốn bộ dữ liệu và trên tất cả các chỉ số. Hơn nữa, so với các phương pháp cơ sở khác, CITING có lợi thế lớn trên cùng phân phối bộ dữ liệu (Alpaca) và sự vượt trội đáng kể trên các nhiệm vụ zero-shot (Kiến thức Thế giới, Đọc hiểu, Lý luận Thông thường). Đặc biệt, đối với nhiệm vụ trên Kiến thức Thế giới, CITING đạt được ít nhất 52%, 55% và 30% lợi thế tỷ lệ thắng so với phương pháp cơ sở trong các chỉ số Rõ ràng, Sâu sắc và Toàn diện tương ứng. Kết quả này được gán cho khả năng của

²https://github.com/CarperAI/trlx
³https://github.com/GanjinZero/RRHF

--- TRANG 8 ---
Bảng 2: So sánh hiệu suất trên các vòng sửa đổi khác nhau trong bốn bộ dữ liệu.

[Bảng chi tiết hiển thị kết quả so sánh giữa các vòng CITING khác nhau (@1, @2, @3, @4) trên bốn bộ dữ liệu]

CITING để sửa đổi phản hồi của nó theo chương trình giảng dạy dựa trên tiêu chí, cho thấy tính linh hoạt và sự vượt trội đáng kể.

• Trong số bốn nhiệm vụ bộ dữ liệu, CITING có lợi thế hiệu suất rõ ràng nhất trên bộ dữ liệu Lý luận Thông thường và lợi thế nhỏ nhất trên bộ dữ liệu Alpaca. Ngoài ra, chúng ta cũng có thể quan sát rằng CITING hoạt động tốt hơn Alpaca trên ba bộ dữ liệu zero-shot. Những quan sát này xác nhận hai khía cạnh: (1) hiệu suất tổng quát hóa của CITING mạnh hơn nhiều so với các phương pháp cơ sở khác, có thể tạo ra các bộ dữ liệu với các phân phối khác nhau; (2) Điều chỉnh hướng dẫn chương trình giảng dạy đóng vai trò quan trọng trong việc nâng cao khả năng lý luận cho LLM, dẫn đến hiệu suất đáng kể trong bộ dữ liệu Lý luận Thông thường.

• Trên tất cả ba chỉ số, hiệu suất của CITING có ý nghĩa nhất trên Sâu sắc và tệ nhất trên Toàn diện. Những quan sát này chứng minh hai khía cạnh: (1) Dựa trên nhiều lần sửa đổi của điều chỉnh hướng dẫn chương trình giảng dạy, CITING học cách suy nghĩ sâu sắc về cách phản hồi hướng dẫn và có được sự hiểu biết sâu sắc hơn về hướng dẫn; (2) So với các phương pháp cơ sở khác, CITING không nhập các khía cạnh liên quan bổ sung hoặc hướng dẫn chủ đề phụ. Do đó, sự cải thiện trong Toàn diện bị hạn chế.

Tóm lại, CITING đạt được những cải thiện hiệu suất đáng chú ý so với các phương pháp cơ sở hiện đại trong tất cả các bộ dữ liệu và trên tất cả các chỉ số, cho thấy sự vượt trội của việc kết hợp khả năng lý luận trong việc sửa đổi của LLM.

4.5 NGHIÊN CỨU LOẠI BỎ

Trong phần này, chúng tôi muốn khám phá tác động của các vòng điều chỉnh hướng dẫn chương trình giảng dạy khác nhau đối với hiệu suất CITING, như được đề cập trong Phần 3.3. Chúng tôi báo cáo so sánh các vòng liền kề trong Bảng 4.3. Từ bảng này, chúng tôi đưa ra các quan sát sau:

• Kết quả cho thấy rằng trong nhiều vòng điều chỉnh hướng dẫn, lợi ích hiệu suất CITING ngày càng nhỏ hơn. Ví dụ, trên bộ dữ liệu Kiến thức Thế giới, tỷ lệ thắng của CITING cho điều chỉnh hướng dẫn giảm từ 78% xuống 44%, và cuối cùng hiệu suất thậm chí gần với hiệu suất phương pháp vòng trước. Điều này cho thấy rằng mặc dù hiệu suất của CITING có thể được cải thiện dần dần thông qua điều chỉnh hướng dẫn chương trình giảng dạy từng bước, cuối cùng nó sẽ đạt đến nút thắt cổ chai hiệu suất. Chúng ta có thể sử dụng xu hướng này để chọn một số mô hình trước khi đạt đến nút thắt cổ chai hiệu suất.

• Trong nhiều vòng điều chỉnh hướng dẫn, CITING hoạt động tốt nhất trên bộ dữ liệu Lý luận Thông thường. Mặc dù lợi ích hiệu suất giảm khi số vòng điều chỉnh hướng dẫn tăng, lợi ích hiệu suất trên bộ dữ liệu Lý luận Thông thường là tốt nhất trong tất cả các bộ dữ liệu. Điều này cho thấy rằng trên bộ dữ liệu Lý luận Thông thường, lý luận và sửa đổi từng bước dựa trên

--- TRANG 9 ---
Hướng dẫn: Tạo một bình luận về một bài báo khoa học.

Phản hồi CITING: Bài báo khoa học này ấn tượng kết hợp nghiên cứu tỉ mỉ với trình bày rõ ràng. Các tác giả khéo léo kết hợp dữ liệu thực nghiệm với hiểu biết lý thuyết, phục vụ cả chuyên gia và người mới bắt đầu. Điều nổi bật là sự cân bằng giữa tính kỹ thuật và khả năng tiếp cận. Hơn nữa, cuộc thảo luận có tính nhìn về tương lai không chỉ bối cảnh hóa các phát hiện mà còn gợi ý về các hướng nghiên cứu tương lai. Thực sự, tác phẩm này là hình mẫu của diễn ngôn khoa học nghiêm ngặt.

Phản hồi RLHF: Bài báo này rất nhiều thông tin và cung cấp một cái nhìn tổng quan tốt về chủ đề. Nó cung cấp một cái nhìn chi tiết về tình trạng hiện tại của cộng đồng khoa học và những thách thức mà nó phải đối mặt. Nó cũng cung cấp một số ý tưởng cho các giải pháp có thể được triển khai để giải quyết các vấn đề hiện có và đảm bảo thành công của các giải pháp được đề xuất.

Phản hồi Thật: Bài báo này là một cái nhìn sâu sắc tuyệt vời về những phát triển mới nhất trong khoa học. Thật tuyệt vời khi thấy chúng ta đã đi được bao xa và những khám phá khoa học nào đang ở phía trước.

Trường hợp 1(a) Trường hợp 1.

Hướng dẫn: Bạn cần những kỹ năng gì để trở thành một bác sĩ thành công?

Phản hồi CITING: Để trở thành một bác sĩ thành công, bạn phải có những kỹ năng chính. Khả năng giao tiếp hiệu quả và kỹ năng giao tiếp cá nhân là tối quan trọng cho tương tác bệnh nhân. Tư duy phản biện nhanh chóng là rất quan trọng cho các quyết định y tế. Tính tổ chức và chú ý đến chi tiết đảm bảo chăm sóc bệnh nhân chất lượng. Trên hết, niềm đam mê thực sự trong việc giúp đỡ người khác và động lực liên tục để học hỏi là không thể thiếu.

Phản hồi RLHF: Một bác sĩ thành công đòi hỏi khả năng giao tiếp và kỹ năng giao tiếp cá nhân xuất sắc. Bạn phải có thể làm việc độc lập và đưa ra quyết định nhanh chóng. Bạn cũng phải có kỹ năng giải quyết vấn đề mạnh mẽ và có thể làm việc độc lập để đạt được mục tiêu của bạn.

Phản hồi Thật: Thành công trong nghề y đòi hỏi một bộ kỹ năng đa dạng. Điều này bao gồm giao tiếp xuất sắc, kiến thức y tế toàn diện, tư duy phân tích và ra quyết định nhanh chóng. Khả năng xử lý áp lực, quản lý thời gian hiệu quả và sự đồng cảm thực sự với bệnh nhân cũng rất cần thiết.

Trường hợp 2 (b) Trường hợp 2.

Hình 4: Nghiên cứu trường hợp cho hai hướng dẫn và các phản hồi từ CITING, RLHF và Phản hồi Thật.

tiêu chí và phản hồi trước đó rất quan trọng và có thể cải thiện dần dần chất lượng phản hồi.

• Chúng tôi cũng quan sát thấy một hiện tượng thú vị, đó là khi có nhiều vòng điều chỉnh hướng dẫn chương trình giảng dạy, hiệu suất mô hình sau khi được tinh chỉnh suy giảm. Cụ thể, khi điều chỉnh hướng dẫn chương trình giảng dạy đạt đến vòng thứ tư, hiệu suất mô hình suy giảm trong cả các chỉ số Rõ ràng và Toàn diện của bộ dữ liệu Đọc hiểu. Điều này là do việc điều chỉnh hướng dẫn lặp đi lặp lại sẽ gây ra quên lãng thảm khốc (Luo et al., 2023; Lin et al., 2023; Korbak et al., 2022) trong mô hình, làm cho mô hình đã tinh chỉnh quên kiến thức từ SFT sớm nhất, dẫn đến suy giảm hiệu suất mô hình. Ngoài ra, chúng tôi cũng tóm tắt quá trình sửa đổi của CITING trong Phụ lục C và các tiêu chí được tóm tắt bởi LLM giáo viên trong Phụ lục B.

4.6 NGHIÊN CỨU TRƯỜNG HỢP

Để phân tích hiệu suất của CITING một cách trực quan hơn, chúng tôi tiến hành nghiên cứu trường hợp thực nghiệm được hiển thị trong Hình 4. Chúng tôi phân tích các trường hợp để xác minh tính hợp lý của các phản hồi của CITING so với RLHF và Phản hồi Thật, có thể hướng dẫn các cải tiến tiếp theo.

Trường hợp #1. Từ Hình 4(a), chúng ta có thể quan sát rằng mặc dù phản hồi từ RLHF có vẻ cụ thể ở cái nhìn đầu tiên, nội dung của nó hơi rỗng. Điều này có thể là do phản hồi con người không bao phủ đầy đủ các hướng dẫn thuộc loại này. Mặt khác, phản hồi từ Phản hồi Thật, mặc dù ngắn gọn, nhưng rõ ràng trong suy nghĩ. So với hai phản hồi này, phản hồi từ CITING áp dụng cấu trúc "toàn bộ-phần-toàn bộ", được tổ chức và rất chi tiết. Điều này cho thấy rằng chương trình giảng dạy sửa đổi giúp cải thiện độ sâu và tính cụ thể của phản hồi. Quan sát này xác nhận kết quả thí nghiệm trong Phần 4.4.

Trường hợp #2. Từ Hình 4(b), có thể quan sát thấy rằng các mẫu câu của phản hồi CITING phong phú và đa dạng hơn, và cấu trúc tổng thể của đoạn văn hoàn chỉnh hơn so với các phản hồi của RLHF và Phản hồi Thật. Điều này cho thấy rằng điều chỉnh hướng dẫn chương trình giảng dạy tăng cường tính đa dạng và linh hoạt của phản hồi.

5 KẾT LUẬN

Tóm lại, bài báo này trình bày một cách tiếp cận mới để thúc đẩy các mô hình ngôn ngữ lớn (LLM) bằng cách giải quyết nút thắt cổ chai của các bộ dữ liệu hướng dẫn được tạo thủ công và căn chỉnh con người. Chúng tôi giới thiệu Điều chỉnh Hướng dẫn Chương trình giảng dạy (CITING), tận dụng các mô hình AI làm giáo viên để đào tạo các LLM học sinh, lấy cảm hứng từ cách học sinh con người tinh chỉnh kỹ năng viết của họ. Cách tiếp cận này bao gồm hai bước chính: LLM giáo viên tạo các tiêu chí để đánh giá câu trả lời cho các loại câu hỏi khác nhau, và LLM học sinh học cách tuân theo các tiêu chí này và tự sửa lỗi sử dụng các bản sửa đổi của giáo viên. Các thí nghiệm của chúng tôi, so sánh CITING với các phương pháp cơ sở hiện đại trên bốn bộ dữ liệu, cho thấy những cải thiện đáng kể về mặt rõ ràng, sâu sắc và toàn diện.

TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo được duy trì như trong bản gốc]

--- TRANG 10 ---
--- TRANG 11 ---
--- TRANG 12 ---
--- TRANG 13 ---
PHỤ LỤC A KÝ HIỆU

Chúng tôi tóm tắt các ký hiệu thường được sử dụng của bài báo trong Bảng 3.

Bảng 3: Danh sách các ký hiệu thường được sử dụng.

Ký hiệu | Mô tả
x | Hướng dẫn.
y | Phản hồi thật.
c | Tiêu chí của hướng dẫn.
D | Phân phối của dữ liệu.
xh | Hướng dẫn đã có tiêu chí.
xn | Hướng dẫn mới hoặc kiểm tra.
Ej | Tập embedding BERT cho mỗi danh mục j của hướng dẫn.
Scorej | Điểm tương tự.
fSFT | LLM được tinh chỉnh bởi SFT.
f(k)CITING | LLM được tinh chỉnh bởi CITING trong vòng thứ k.
r(k) | Phản hồi trong vòng thứ k.
pr | Lời nhắc được thiết kế cho tinh chỉnh chương trình giảng dạy.
π(k) | LLM học sinh của vòng thứ k.
N | Số lượng điều chỉnh hướng dẫn chương trình giảng dạy.
M | Số vòng suy luận.

PHỤ LỤC B TIÊU CHÍ ĐƯỢC TÓM TẮT BỞI LLM GIÁO VIÊN

Chúng tôi sử dụng GPT-4 để tóm tắt và phân loại các tiêu chí trong các hướng dẫn của bộ dữ liệu Alpaca, và kết quả được hiển thị trong Bảng 4. Từ bảng này chúng ta có thể quan sát rằng các tiêu chí tương ứng với hướng dẫn được chia thành năm danh mục chính.

Bảng 4: Tiêu chí được tóm tắt bởi LLM giáo viên

Danh mục | Tiêu chí
Hướng dẫn Kiến thức Thực tế | Bao gồm các câu hỏi yêu cầu phản hồi thực tế và thường có thể được trả lời bằng thông tin cụ thể, được chấp nhận. Một câu trả lời tốt cho những loại câu hỏi này sẽ chính xác, ngắn gọn và đúng trọng tâm.
Hướng dẫn Giải thích/Định nghĩa | Bao gồm các câu hỏi yêu cầu giải thích chi tiết hoặc định nghĩa. Các câu trả lời tốt cho những câu hỏi này thường triệt để, có cấu trúc logic và không có thuật ngữ phức tạp. Chúng sử dụng ngôn ngữ rõ ràng và dễ hiểu để phân tích các chủ đề phức tạp.
Hướng dẫn Phân tích/Đánh giá | Bao gồm các câu hỏi yêu cầu một số hình thức phân tích hoặc đánh giá. Một phản hồi tốt thường sẽ có lý luận tốt, cung cấp hiểu biết, đưa ra so sánh khi cần thiết, và cũng có thể liên quan đến tư duy phản biện.
Hướng dẫn Tạo sinh Sáng tạo | Bao gồm các câu hỏi yêu cầu tư duy sáng tạo, như tạo ra một danh sách, viết một câu chuyện hoặc bài thơ, hoặc nghĩ ra ý tưởng. Các phản hồi tốt sẽ độc đáo, chu đáo và phù hợp với các tham số hoặc tiêu chí đã cho.
Hướng dẫn Ứng dụng Thực tế | Bao gồm các câu hỏi yêu cầu một hành động hoặc nhiệm vụ cụ thể được thực hiện, như tính toán, dịch thuật hoặc chuyển đổi. Các phản hồi tốt sẽ hoàn thành chính xác nhiệm vụ và cung cấp lý luận rõ ràng, từng bước khi thích hợp.

PHỤ LỤC C CÁC TRƯỜNG HỢP QUÁ TRÌNH SUY LUẬN CỦA CITING

[Bảng 5 hiển thị các ví dụ về quá trình suy luận của CITING với các vòng sửa đổi khác nhau cho cùng một hướng dẫn]
