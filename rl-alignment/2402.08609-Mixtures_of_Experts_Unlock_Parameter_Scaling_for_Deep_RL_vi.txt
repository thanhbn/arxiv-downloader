# 2402.08609.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2402.08609.pdf
# Kích thước file: 6453411 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Hỗn hợp Các Chuyên gia Mở khóa Việc Mở rộng Tham số cho Deep RL
Johan Obando-Ceron*1 2 3Ghada Sokar*1Timon Willi*1 4Clare Lyle1Jesse Farebrother1 2 5
Jakob Foerster4Karolina Dziugaite1Doina Precup1 2 5Pablo Samuel Castro1 2 3

Tóm tắt
Tiến bộ nhanh chóng gần đây trong các mô hình học (tự) giám sát phần lớn được dự đoán bởi các luật mở rộng thực nghiệm: hiệu suất của một mô hình tỉ lệ thuận với kích thước của nó. Tuy nhiên, các luật mở rộng tương tự vẫn khó nắm bắt đối với các miền học tăng cường, nơi việc tăng số lượng tham số của một mô hình thường làm tổn hại hiệu suất cuối cùng của nó. Trong bài báo này, chúng tôi chứng minh rằng việc kết hợp các mô-đun Hỗn hợp Chuyên gia (MoE), và đặc biệt là Soft MoE (Puigcerver et al., 2023), vào các mạng dựa trên giá trị dẫn đến các mô hình có thể mở rộng tham số hơn, được chứng minh bằng sự gia tăng hiệu suất đáng kể trên nhiều chế độ huấn luyện và kích thước mô hình khác nhau. Do đó, công trình này cung cấp bằng chứng thực nghiệm mạnh mẽ hướng tới việc phát triển các luật mở rộng cho học tăng cường. Chúng tôi công khai mã nguồn của mình.

1. Giới thiệu
Học Tăng cường Sâu (Deep RL) - sự kết hợp của các thuật toán học tăng cường với mạng nơ-ron sâu - đã chứng minh hiệu quả trong việc tạo ra các tác nhân thực hiện các nhiệm vụ phức tạp ở mức độ siêu con người (Mnih et al., 2015; Berner et al., 2019; Vinyals et al., 2019; Fawzi et al., 2022; Bellemare et al., 2020). Mặc dù mạng sâu rất quan trọng đối với bất kỳ ứng dụng thành công nào của RL trong môi trường phức tạp, thiết kế và động lực học của chúng trong RL vẫn là một bí ẩn. Thật vậy, các công trình gần đây làm nổi bật một số hiện tượng đáng ngạc nhiên phát sinh khi sử dụng mạng sâu trong RL, thường đi ngược lại các hành vi quan sát được trong các cài đặt học giám sát (Ostrovski et al., 2021; Kumar et al., 2021a; Lyle et al., 2022a; Graesser et al., 2022; Nikishin et al., 2022; Sokar et al., 2023; Ceron et al., 2023).

*Đóng góp ngang nhau1Google DeepMind2Mila - Viện AI Québec3Đại học Montréal4Đại học Oxford5Đại học McGill. Liên hệ với: Johan Obando-Ceron <jobando0730@gmail.com>, Pablo Samuel Castro <psc@google.com>.

Kỷ yếu Hội nghị Quốc tế lần thứ 41 về Máy học, Vienna, Austria. PMLR 235, 2024. Bản quyền 2024 thuộc về (các) tác giả.

Cộng đồng học giám sát đã thuyết phục chứng minh rằng các mạng lớn hơn dẫn đến hiệu suất được cải thiện, đặc biệt đối với các mô hình ngôn ngữ (Kaplan et al., 2020). Ngược lại, các công trình gần đây chứng minh rằng việc mở rộng mạng trong RL là thách thức và đòi hỏi sử dụng các kỹ thuật tinh vi để ổn định việc học, chẳng hạn như các tổn thất phụ trợ giám sát, chưng cất và tiền huấn luyện (Farebrother et al., 2022; Taiga et al., 2022; Schwarzer et al., 2023). Hơn nữa, các mạng RL sâu đang không sử dụng hết các tham số của chúng, điều này có thể giải thích cho những khó khăn quan sát được trong việc đạt được hiệu suất cải thiện từ quy mô (Kumar et al., 2021a; Lyle et al., 2022a; Sokar et al., 2023). Số lượng tham số không thể được mở rộng hiệu quả nếu những tham số đó không được sử dụng một cách hiệu quả.

Các tiến bộ kiến trúc, chẳng hạn như transformers (Vaswani et al., 2017), adapters (Houlsby et al., 2019), và Hỗn hợp Chuyên gia (MoEs; Shazeer et al., 2017), đã trở thành trung tâm của các tính chất mở rộng của các mô hình học giám sát, đặc biệt trong các bài toán xử lý ngôn ngữ tự nhiên và thị giác máy tính. MoEs, đặc biệt, rất quan trọng để mở rộng mạng lên hàng tỷ (và gần đây là hàng nghìn tỷ) tham số, vì tính mô-đun của chúng kết hợp tự nhiên với các phương pháp tính toán phân tán (Fedus et al., 2022). Ngoài ra, MoEs tạo ra tính thưa có cấu trúc trong một mạng, và một số loại tính thưa nhất định đã được chứng minh là cải thiện hiệu suất mạng (Evci et al., 2020; Gale et al., 2019).

Trong bài báo này, chúng tôi khám phá tác động của hỗn hợp chuyên gia đối với khả năng mở rộng tham số của các mạng RL sâu dựa trên giá trị, tức là hiệu suất có tăng khi chúng ta tăng số lượng tham số không? Chúng tôi chứng minh rằng việc kết hợp Soft MoEs (Puigcerver et al., 2023) cải thiện mạnh hiệu suất của các tác nhân RL sâu khác nhau, và việc cải thiện hiệu suất tỉ lệ với số lượng chuyên gia được sử dụng. Chúng tôi bổ sung các kết quả tích cực của mình bằng một loạt phân tích giúp chúng ta hiểu các nguyên nhân cơ bản cho các kết quả trong Phần 4. Ví dụ, chúng tôi điều tra các cơ chế gating khác nhau, thúc đẩy việc sử dụng Soft MoE, cũng như các tokenization khác nhau của đầu vào. Hơn nữa, chúng tôi phân tích các tính chất khác nhau của các biểu diễn ẩn của chuyên gia, chẳng hạn như các nơ-ron ngủ (Sokar et al., 2023), cung cấp bằng chứng thực nghiệm về lý do tại sao Soft MoE cải thiện hiệu suất so với baseline.

Cuối cùng, chúng tôi trình bày một loạt kết quả đầy hứa hẹn mở đường cho nghiên cứu tiếp theo kết hợp MoEs trong các mạng RL sâu trong Phần 5. Chẳng hạn, trong Phần 5.1 chúng tôi hiển thị các kết quả sơ bộ rằng Soft MoE vượt trội hơn baseline trên một tập các nhiệm vụ RL Ngoại tuyến; trong Phần 5.2, chúng tôi đánh giá hiệu suất của Soft MoE trong các chế độ huấn luyện dữ liệu thấp; và cuối cùng, chúng tôi cho thấy rằng khám phá các thiết kế kiến trúc khác nhau là một hướng đi có thành quả cho nghiên cứu trong tương lai trong Phần 5.3.

2. Kiến thức cơ bản
2.1. Học Tăng cường
Các phương pháp học tăng cường thường được sử dụng trong các bài toán ra quyết định tuần tự, với mục tiêu tìm hành vi tối ưu trong một môi trường cho trước (Sutton & Barto, 1998). Các môi trường này thường được hình thức hóa dưới dạng Quá trình Quyết định Markov (MDPs), được định nghĩa bởi bộ ⟨X,A,P,R, γ⟩, trong đó X đại diện cho tập các trạng thái, A tập các hành động có sẵn, P:X × A → ∆(X)¹ hàm chuyển đổi, R:X × A → R hàm phần thưởng, và γ∈[0,1) hệ số chiết khấu.

Hành vi của một tác nhân, hay chính sách, được biểu diễn thông qua ánh xạ π:X → ∆(A). Cho một chính sách π, giá trị Vπ của một trạng thái x được cho bởi tổng kỳ vọng của các phần thưởng được chiết khấu khi bắt đầu từ trạng thái đó và tuân theo π từ đó trở đi: Vπ(x) := Eπ,P[∑∞t=iγtR(xt, at)|x0=x]. Hàm trạng thái-hành động Qπ định lượng giá trị của việc đầu tiên thực hiện hành động a từ trạng thái x, và sau đó tuân theo π: Qπ(x, a) := R(x, a) +γEx′∼P(x,a)Vπ(x′). Mọi MDP đều có một chính sách tối ưu π* theo nghĩa Vπ*:=V*≥Vπ đồng nhất trên X cho tất cả các chính sách π.

Khi X rất lớn (hoặc vô hạn), các bộ xấp xỉ hàm được sử dụng để biểu diễn Q, ví dụ, DQN của Mnih et al. (2015) sử dụng mạng nơ-ron với các tham số θ, ký hiệu là Qθ≈Q. Kiến trúc gốc được sử dụng trong DQN, sau đây được gọi là kiến trúc CNN, bao gồm 3 lớp tích chập theo sau bởi 2 lớp dày đặc, với các phi tuyến ReLu (Fukushima, 1969) giữa mỗi cặp lớp. Trong công trình này, chúng tôi chủ yếu sử dụng kiến trúc Impala mới hơn (Espeholt et al., 2018), nhưng sẽ cung cấp so sánh với kiến trúc CNN gốc. DQN cũng sử dụng một bộ đệm phát lại: một bộ nhớ (hữu hạn) nơi một tác nhân lưu trữ các chuyển tiếp nhận được khi tương tác với môi trường, và lấy mẫu mini-batches từ đó để tính toán các cập nhật gradient. Tỷ lệ phát lại² được định nghĩa là tỷ lệ của các cập nhật gradient với các tương tác môi trường, và đóng vai trò trong các phân tích của chúng tôi dưới đây.

Rainbow (Hessel et al., 2018) mở rộng thuật toán DQN gốc với nhiều thành phần thuật toán để cải thiện tính ổn định học, hiệu quả mẫu, và hiệu suất tổng thể. Rainbow được chứng minh là vượt trội đáng kể so với DQN và là một baseline quan trọng trong nghiên cứu RL sâu.

2.2. Hỗn hợp Chuyên gia
Hỗn hợp chuyên gia (MoEs) đã trở thành trung tâm của kiến trúc của hầu hết các Mô hình Ngôn ngữ Lớn (LLMs) hiện đại. Chúng bao gồm một tập n "chuyên gia" mạng con được kích hoạt bởi một mạng gating (thường được học và được gọi là router), định tuyến từng token đến k chuyên gia (Shazeer et al., 2017). Trong hầu hết các trường hợp, k nhỏ hơn tổng số chuyên gia (k=1 trong công trình của chúng tôi), do đó tạo ra các kích hoạt thưa hơn (Fedus et al., 2022). Các kích hoạt thưa này cho phép suy luận nhanh hơn và tính toán phân tán, điều này đã là sức hấp dẫn chính cho việc huấn luyện LLM. Các mô-đun MoE thường thay thế các khối feed forward dày đặc trong transformers (Vaswani et al., 2017). Các kết quả thực nghiệm mạnh mẽ của chúng đã khiến MoEs trở thành một lĩnh vực nghiên cứu rất tích cực trong vài năm qua (Shazeer et al., 2017; Lewis et al., 2021; Fedus et al., 2022; Zhou et al., 2022; Puigcerver et al., 2023; Lepikhin et al., 2020; Zoph et al., 2022; Gale et al., 2023).

Các phép gán cứng như vậy của tokens cho chuyên gia tạo ra một số thách thức như bất ổn định huấn luyện, loại bỏ tokens, và khó khăn trong việc mở rộng số lượng chuyên gia (Fedus et al., 2022; Puigcerver et al., 2023). Để giải quyết một số thách thức này, Puigcerver et al. (2023) đã giới thiệu Soft MoE, một phép gán mềm hoàn toàn khả vi của tokens-to-experts, thay thế các phép gán cứng dựa trên router.

Phép gán mềm được thực hiện bằng cách tính toán các hỗn hợp (đã học) của trọng số per-token cho mỗi chuyên gia, và lấy trung bình các đầu ra của chúng. Theo ký hiệu của Puigcerver et al. (2023), hãy định nghĩa các tokens đầu vào là X∈Rm×d, trong đó m là số lượng tokens d-chiều. Một lớp Soft MoE áp dụng một tập n chuyên gia trên các tokens riêng lẻ, fi:Rd→Rd 1:n.

Mỗi chuyên gia có p input- và output-slots, được đại diện tương ứng bởi một vector d-chiều của các tham số. Chúng tôi ký hiệu các tham số này bằng Φ∈Rd×(n·p).

Các input-slots ˜X∈R(n·p)×d tương ứng với trung bình có trọng số của tất cả tokens: ˜X=D⊤X, trong đó Dij=exp ((XΦ)ij)/∑m i′=1exp ((XΦ)i′j).

D thường được gọi là dispatch weights. Sau đó chúng tôi ký hiệu các đầu ra chuyên gia là ˜Yi=f⌊i/p⌋(˜Xi). Đầu ra của lớp Soft MoE Y là sự kết hợp của ˜Y với combine weights C theo Y=C˜Y, trong đó Cij=exp ((XΦ)ij)/∑n·p j′=1exp ((XΦ)ij′).

Lưu ý cách D và C chỉ được học thông qua Φ, điều mà chúng tôi sẽ sử dụng trong phân tích của mình. Các kết quả của Puigcerver et al. (2023) cho thấy rằng Soft MoE đạt được sự cân bằng tốt hơn giữa độ chính xác và chi phí tính toán so với các phương pháp MoE khác.

3. Hỗn hợp Chuyên gia cho Deep RL
Dưới đây chúng tôi thảo luận về một số lựa chọn thiết kế quan trọng trong việc kết hợp các mô-đun MoE vào kiến trúc dựa trên DQN.

Đặt MoEs ở đâu? Theo cách sử dụng chủ đạo của MoEs để thay thế các lớp feed-forward dày đặc (Shazeer et al., 2017; Fedus et al., 2022; Gale et al., 2023), chúng tôi thay thế lớp gần cuối trong mạng của chúng tôi bằng một mô-đun MoE, trong đó mỗi chuyên gia có cùng số chiều với lớp dày đặc gốc. Do đó, chúng tôi đang mở rộng hiệu quả số chiều của lớp gần cuối bằng một hệ số bằng số lượng chuyên gia. Hình 2 minh họa kiến trúc MoE RL sâu của chúng tôi. Chúng tôi thảo luận về một số lựa chọn thay thế trong Phần 5.3.

Token là gì? MoEs định tuyến thưa đầu vào đến một tập chuyên gia (Shazeer et al., 2017) và chủ yếu được sử dụng trong bối cảnh kiến trúc transformer nơi các đầu vào là tokens (Fedus et al., 2022). Đối với phần lớn các nhiệm vụ học giám sát nơi MoEs được sử dụng, có một khái niệm token được định nghĩa rõ ràng; ngoại trừ một vài công trình sử dụng Transformers (Chen et al., 2021), điều này không đúng với các mạng RL sâu. Ký hiệu bằng C(h,w,d)∈R³ đầu ra của các bộ mã hóa tích chập, chúng tôi định nghĩa tokens là các lát d-chiều của đầu ra này; do đó, chúng tôi chia C thành h×w tokens có số chiều d (PerConv trong Hình 3). Cách tiếp cận tokenization này được lấy từ những gì thường được sử dụng trong các nhiệm vụ thị giác (xem kiến trúc Hybrid của Dosovitskiy et al. (2021)). Chúng tôi đã khám phá các phương pháp tokenization khác (được thảo luận trong Phần 4.2), nhưng thấy rằng cách này có hiệu suất tốt nhất và trực quan nhất. Cuối cùng, một phép chiếu tuyến tính có thể huấn luyện được áp dụng sau mỗi chuyên gia để duy trì kích thước token d ở đầu ra.

Sử dụng hương vị MoE nào? Chúng tôi khám phá kiến trúc gating top-k của Shazeer et al. (2017) theo chiến lược k=1 đơn giản hóa của Fedus et al. (2022), cũng như biến thể Soft MoE được đề xuất bởi Puigcerver et al. (2023); đối với phần còn lại của bài báo, chúng tôi gọi cái trước là Top1-MoE và cái sau là Soft MoE. Chúng tôi tập trung vào hai loại này, vì Top1-MoE là phương pháp được sử dụng chủ yếu, trong khi Soft MoE đơn giản hơn và cho thấy bằng chứng cải thiện hiệu suất. Vì Top1-MoEs kích hoạt một chuyên gia trên mỗi token trong khi Soft MoEs có thể kích hoạt nhiều chuyên gia trên mỗi token, Soft MoEs có thể so sánh trực tiếp hơn về số lượng tham số khi mở rộng các lớp dày đặc của baseline.

4. Đánh giá thực nghiệm
Chúng tôi tập trung điều tra của mình vào DQN (Mnih et al., 2015) và Rainbow (Hessel et al., 2018), hai tác nhân dựa trên giá trị đã hình thành nền tảng của một lượng lớn nghiên cứu RL sâu hiện đại. Công trình gần đây đã chứng minh rằng việc sử dụng kiến trúc ResNet (Espeholt et al., 2018) thay vì kiến trúc CNN gốc (Mnih et al., 2015) mang lại những cải thiện thực nghiệm mạnh mẽ (Graesser et al., 2022; Schwarzer et al., 2023), vì vậy chúng tôi tiến hành hầu hết các thí nghiệm của mình với kiến trúc này. Như trong các bài báo gốc, chúng tôi đánh giá trên 20 game từ Arcade Learning Environment (ALE), một bộ sưu tập các môi trường dựa trên pixel đa dạng và thách thức (Bellemare et al., 2013b).

Triển khai của chúng tôi, được bao gồm trong bài nộp này, được xây dựng trên thư viện Dopamine (Castro et al., 2018)³, thêm tính ngẫu nhiên (thông qua sticky actions) vào ALE (Machado et al., 2018). Chúng tôi sử dụng các khuyến nghị từ Agarwal et al. (2021) cho các đánh giá hiệu suất mạnh mẽ về mặt thống kê, đặc biệt tập trung vào interquartile mean (IQM). Mỗi thí nghiệm được chạy trong 200M bước môi trường, trừ khi được báo cáo khác, với 5 seeds độc lập, và chúng tôi báo cáo khoảng tin cậy bootstrap stratified 95%. Tất cả các thí nghiệm đều được chạy trên GPU NVIDIA Tesla P100, và mỗi thí nghiệm trung bình mất 4 ngày để hoàn thành.

4.1. Soft MoE Giúp Khả năng Mở rộng Tham số
Để điều tra hiệu quả của Top1-MoE và Soft MoE trên DQN và Rainbow, chúng tôi thay thế lớp gần cuối bằng mô-đun MoE tương ứng (xem Hình 2) và thay đổi số lượng chuyên gia. Vì mỗi chuyên gia là một bản sao của lớp gần cuối gốc, chúng tôi đang tăng hiệu quả số lượng tham số của lớp này bằng một hệ số bằng số lượng chuyên gia. Để so sánh trực tiếp hơn về số lượng tham số, chúng tôi đánh giá việc đơn giản mở rộng lớp gần cuối của các kiến trúc cơ sở bằng một hệ số bằng số lượng chuyên gia.

Như Hình 1 chứng minh, Soft MoE cung cấp những cải thiện hiệu suất rõ ràng, và những cải thiện này tăng với số lượng chuyên gia; ví dụ trong Rainbow, việc tăng số lượng chuyên gia từ 1 lên 8 dẫn đến cải thiện hiệu suất 20%. Ngược lại, hiệu suất của các kiến trúc cơ sở giảm khi chúng ta mở rộng lớp của nó; ví dụ trong Rainbow, khi chúng ta tăng hệ số nhân lớp từ 1 lên 8 có sự giảm hiệu suất khoảng 40%. Đây là một phát hiện phù hợp với công trình trước đó chứng minh khó khăn trong việc mở rộng các mạng RL sâu (Farebrother et al., 2022; Taiga et al., 2022; Schwarzer et al., 2023). Top1-MoE dường như cung cấp lợi ích khi được kết hợp vào Rainbow, nhưng không thể hiện khả năng mở rộng tham số mà chúng tôi quan sát được với Soft MoE.

Đã biết rằng các tác nhân RL sâu không thể duy trì hiệu suất khi mở rộng tỷ lệ phát lại mà không có can thiệp rõ ràng (D'Oro et al., 2023; Schwarzer et al., 2023). Trong Hình 4 chúng tôi quan sát rằng việc sử dụng Soft MoEs duy trì lợi thế mạnh so với baseline ngay cả ở tỷ lệ phát lại cao, tiếp tục xác nhận rằng chúng làm cho các mạng RL hiệu quả tham số hơn.

4.2. Tác động của Các Lựa chọn Thiết kế
Số lượng chuyên gia Fedus et al. (2022) lập luận rằng số lượng chuyên gia là cách hiệu quả nhất để mở rộng mô hình trong các cài đặt học giám sát. Kết quả của chúng tôi trong Hình 1 chứng minh rằng trong khi Soft MoE có lợi từ nhiều chuyên gia hơn, Top1-MoE thì không.

Số chiều của chuyên gia Như Hình 1 xác nhận, việc mở rộng lớp tương ứng trong các kiến trúc cơ sở không khớp với hiệu suất đạt được khi sử dụng Soft MoEs (và thực tế làm xấu đi). Chúng tôi khám phá việc chia số chiều của mỗi chuyên gia cho số lượng chuyên gia, hiệu quả đưa số lượng tham số ngang bằng với kiến trúc cơ sở gốc. Hình 5 chứng minh rằng Soft MoE duy trì hiệu suất của nó, ngay cả với các chuyên gia nhỏ hơn nhiều. Điều này cho thấy các lợi ích quan sát được phần lớn đến từ tính thưa có cấu trúc được tạo ra bởi Soft MoEs, và không nhất thiết là kích thước của mỗi chuyên gia.

Gating và combining Top1-MoEs và Soft MoEs sử dụng các cơ chế gating đã học, mặc dù khác nhau: cái trước sử dụng router top-1, chọn một chuyên gia cho mỗi token, trong khi cái sau sử dụng dispatch weights để gán tokens có trọng số cho các slots chuyên gia. Thành phần "combiner" đã học lấy đầu ra của các mô-đun MoE và kết hợp chúng để tạo ra một đầu ra duy nhất (xem Hình 2). Nếu chúng ta sử dụng một chuyên gia duy nhất, sự khác biệt duy nhất giữa các kiến trúc cơ sở và MoE sau đó chỉ là các tham số đã học bổ sung từ các thành phần gating và combination. Hình 1 cho thấy rằng hầu hết lợi ích của MoEs đến từ sự kết hợp của các thành phần gating/combining với nhiều chuyên gia. Thú vị là, Soft MoE với một chuyên gia duy nhất vẫn cung cấp cải thiện hiệu suất cho Rainbow, cho thấy rằng ma trận Φ đã học (xem Phần 2.2) có vai trò có lợi. Thực sự, panel bên phải của Hình 8, nơi chúng tôi thay thế Φ đã học bằng một cái ngẫu nhiên, xác nhận điều này.

Tokenization Như đã đề cập ở trên, chúng tôi tập trung hầu hết các điều tra của mình vào tokenization PerConv, nhưng cũng khám phá hai cái khác: PerFeat về cơ bản là chuyển vị của PerConv, tạo ra d tokens có số chiều h×w; PerSamp sử dụng toàn bộ đầu ra của bộ mã hóa như một token duy nhất (xem Hình 3). Trong Hình 6 chúng ta có thể quan sát rằng trong khi PerConv hoạt động tốt nhất với Soft MoE, Top1-MoE dường như hưởng lợi nhiều hơn từ PerFeat.

Encoder Mặc dù chúng tôi chủ yếu sử dụng encoder từ kiến trúc Impala (Espeholt et al., 2018), trong Hình 7 chúng tôi xác nhận rằng Soft MoE vẫn cung cấp lợi ích khi được sử dụng với kiến trúc CNN tiêu chuẩn từ Mnih et al. (2015).

Lựa chọn game Để xác nhận rằng các phát hiện của chúng tôi không giới hạn ở lựa chọn 20 game của chúng tôi, chúng tôi chạy một nghiên cứu trên tất cả 60 game Atari 2600 với 5 seeds độc lập, tương tự như các công trình trước đó (Fedus et al., 2020; Ceron et al., 2023). Trong panel bên trái của Hình 8, chúng tôi quan sát rằng Soft MoE dẫn đến hiệu suất được cải thiện trên toàn bộ 60 game trong bộ.

Số lượng chuyên gia hoạt động Một sự khác biệt quan trọng giữa hai hương vị MoEs được xem xét ở đây là trong việc kích hoạt chuyên gia: Top1-MoE chỉ kích hoạt một chuyên gia trên mỗi lượt chuyển tiến (hard-gating), trong khi Soft MoE kích hoạt tất cả chúng. Hard-gating được biết là nguồn gốc của khó khăn huấn luyện, và nhiều công trình đã khám phá việc thêm các tổn thất load-balancing (Shazeer et al., 2017; Ruiz et al., 2021; Fedus et al., 2022; Mustafa et al., 2022). Để điều tra liệu Top1-MoE có hoạt động kém do load balancing không đúng hay không, chúng tôi đã thêm các tổn thất load và importance được đề xuất bởi Ruiz et al. (2021) (phương trình (7) trong Phụ lục 2). Hình 9 cho thấy việc thêm các tổn thất load-balancing này không đủ để tăng hiệu suất của Top1-MoE. Mặc dù có thể các tổn thất khác có thể dẫn đến hiệu suất tốt hơn, những phát hiện này cho thấy rằng các tác nhân RL hưởng lợi từ việc có kết hợp có trọng số của các tokens, trái ngược với định tuyến cứng.

4.3. Phân tích Bổ sung
Trong các phần trước, chúng tôi đã cho thấy rằng các tác nhân RL sâu sử dụng mạng MoE có thể tận dụng tốt hơn việc mở rộng mạng, nhưng không rõ ràng a priori cách chúng tạo ra hiệu ứng này. Mặc dù một phân tích chi tiết về các hiệu ứng của mô-đun MoE đối với động lực tối ưu hóa mạng nằm ngoài phạm vi của công trình này, chúng tôi tập trung vào ba tính chất được biết là tương quan với bất ổn huấn luyện trong các tác nhân RL sâu: rank của các đặc trưng (Kumar et al., 2021a), sự can thiệp giữa các gradient per-sample (Lyle et al., 2022b), và các nơ-ron ngủ (Sokar et al., 2023).

Chúng tôi tiến hành điều tra sâu hơn về tác động của các lớp MoE đối với động lực học bằng cách nghiên cứu tác nhân Rainbow sử dụng kiến trúc Impala, và sử dụng 8 chuyên gia cho các lần chạy với các mô-đun MoE. Chúng tôi theo dõi norm của các đặc trưng, rank của ma trận neural tangent kernel (NTK) thực nghiệm (tức là ma trận của tích vô hướng giữa các gradient per-transition được lấy mẫu từ bộ đệm phát lại), và số lượng nơ-ron ngủ - tất cả sử dụng batch size 32 - và trực quan hóa các kết quả này trong Hình 10.

Chúng tôi quan sát những khác biệt đáng kể giữa kiến trúc baseline và các kiến trúc bao gồm các mô-đun MoE. Các kiến trúc MoE đều thể hiện rank số học cao hơn của các ma trận NTK thực nghiệm so với mạng baseline, và có các nơ-ron ngủ và feature norms không đáng kể. Những phát hiện này cho thấy rằng các mô-đun MoE có tác động ổn định đối với động lực tối ưu hóa, mặc dù chúng tôi tránh tuyên bố liên kết nhân quả trực tiếp giữa các cải thiện trong các chỉ số này và hiệu suất tác nhân. Ví dụ, trong khi rank của ENTK cao hơn cho các tác nhân MoE, tác nhân có hiệu suất tốt nhất không có rank ENTK cao nhất. Tuy nhiên, sự vắng mặt của các giá trị bệnh lý của các thống kê này trong các mạng MoE cho thấy rằng dù chuỗi nhân quả chính xác là gì, các mô-đun MoE có tác động ổn định đối với tối ưu hóa.

5. Hướng tương lai
Để cung cấp một điều tra sâu về cả hiệu suất kết quả và các nguyên nhân có thể cho những cải thiện quan sát được, chúng tôi tập trung vào việc đánh giá DQN và Rainbow với Soft MoE và Top1-MoE trên benchmark ALE tiêu chuẩn. Những cải thiện hiệu suất quan sát được cho thấy rằng những ý tưởng này cũng sẽ có lợi trong các chế độ huấn luyện khác. Trong phần này chúng tôi cung cấp kết quả thực nghiệm mạnh mẽ trong một số chế độ huấn luyện khác nhau, mà chúng tôi hy vọng sẽ phục vụ như các chỉ báo cho các hướng nghiên cứu đầy hứa hẹn trong tương lai.

5.1. RL Ngoại tuyến
Chúng tôi bắt đầu bằng việc kết hợp MoEs trong RL ngoại tuyến, nơi các tác nhân được huấn luyện trên một tập dữ liệu cố định mà không có tương tác môi trường. Theo công trình trước đó (Kumar et al., 2021b), chúng tôi huấn luyện các tác nhân trên 17 game và 5 seeds trong 200 iterations, trong đó 1 iteration tương ứng với 62,500 cập nhật gradient. Chúng tôi đánh giá trên các tập dữ liệu bao gồm 5%, 10%, 50% các mẫu (được rút ngẫu nhiên) từ tập tất cả các tương tác môi trường được thu thập bởi một tác nhân DQN được huấn luyện trong 200M bước (Agarwal et al., 2020). Trong Hình 11 chúng tôi quan sát rằng việc kết hợp Soft MoE với hai thuật toán RL ngoại tuyến hiện đại (CQL (Kumar et al., 2020) và CQL+C51 (Kumar et al., 2022)) đạt hiệu suất cuối cùng tốt nhất tổng hợp. Top1-MoE cung cấp cải thiện hiệu suất khi được sử dụng kết hợp với CQL+C51, nhưng không với CQL. Các cải thiện tương tự có thể được quan sát khi sử dụng 10% mẫu (xem Hình 17).

5.2. Các Biến thể Tác nhân Cho Chế độ Dữ liệu Thấp
DQN và Rainbow đều được phát triển cho chế độ huấn luyện nơi các tác nhân có thể thực hiện hàng triệu bước trong môi trường của chúng. Kaiser et al. (2020) đã giới thiệu benchmark 100k⁴, đánh giá các tác nhân trên chế độ dữ liệu nhỏ hơn nhiều (~100k tương tác) trên 26 game. Chúng tôi đánh giá hiệu suất trên hai tác nhân phổ biến cho chế độ này: DrQ(ϵ), một tác nhân dựa trên DQN (Yarats et al., 2021; Agarwal et al., 2021), và DER (Van Hasselt et al., 2019), một tác nhân dựa trên Rainbow. Khi được huấn luyện trong 100k tương tác môi trường, chúng tôi không thấy sự khác biệt thực sự giữa các biến thể khác nhau, cho thấy rằng lợi ích của MoEs, ít nhất trong thiết lập hiện tại của chúng tôi, chỉ phát sinh khi được huấn luyện trong một lượng tương tác đáng kể. Tuy nhiên, khi được huấn luyện trong 50M bước, chúng tôi thực sự thấy lợi ích trong cả hai tác nhân, đặc biệt với DER (Hình 12). Các lợi ích chúng tôi quan sát trong thiết lập này phù hợp với những gì chúng tôi đã quan sát cho đến nay, vì DrQ(ϵ) và DER dựa trên DQN và Rainbow, tương ứng.

5.3. Các Biến thể Chuyên gia
Kiến trúc MoE được đề xuất của chúng tôi thay thế lớp feed-forward sau encoder bằng MoE, nơi các chuyên gia bao gồm một lớp feed-forward duy nhất (Hình 2). Điều này dựa trên thực hành phổ biến khi thêm MoEs vào kiến trúc transformer, nhưng không phải là cách duy nhất để sử dụng MoEs. Ở đây chúng tôi điều tra ba biến thể sử dụng Soft MoE cho DQN và Rainbow với kiến trúc Impala:

Big: Mỗi chuyên gia là một mạng đầy đủ. Lớp tuyến tính cuối cùng được bao gồm trong DQN (mỗi chuyên gia có lớp riêng), nhưng loại trừ khỏi Rainbow (vì vậy lớp tuyến tính cuối cùng được chia sẻ giữa các chuyên gia).⁵

All: Một Soft MoE riêng biệt được áp dụng tại mỗi lớp của mạng, loại trừ lớp cuối cùng cho Rainbow.

Regular: thiết lập được sử dụng trong phần còn lại của bài báo này.

Đối với Big và All, routing được áp dụng ở mức đầu vào, và chúng tôi đang sử dụng tokenization PerSamp (xem Hình 3). Puigcerver et al. (2023) đề xuất sử dụng chuẩn hóa ℓ2 cho mỗi lớp Soft MoE đối với tokens lớn, nhưng đề cập rằng nó tạo ra ít khác biệt cho tokens nhỏ hơn. Vì các tokens trong thí nghiệm của chúng tôi ở trên nhỏ (tương đối so với các mô hình lớn thường sử dụng MoEs), chúng tôi chọn không bao gồm điều này trong các thí nghiệm đã chạy cho đến nay. Điều này có thể không còn đúng với Big và All, vì chúng tôi đang sử dụng tokenization PerSamp trên toàn bộ đầu vào; vì lý do này, chúng tôi đã điều tra tác động của chuẩn hóa ℓ2 khi chạy các kết quả này.

Hình 13 tóm tắt kết quả của chúng tôi, từ đó chúng ta có thể rút ra một số kết luận. Đầu tiên, các thí nghiệm này xác nhận trực giác của chúng tôi rằng không bao gồm regularization trong thiết lập được sử dụng trong phần lớn bài báo này hoạt động tốt nhất. Thứ hai, phù hợp với Puigcerver et al. (2023), chuẩn hóa dường như giúp với các chuyên gia Big. Điều này đặc biệt rõ ràng trong DQN, nơi nó thậm chí vượt qua hiệu suất của thiết lập Regular; sự khác biệt giữa hai cái rất có thể do lớp cuối cùng được chia sẻ (như trong Rainbow) hoặc không chia sẻ (như trong DQN). Cuối cùng, việc sử dụng các mô-đun MoE riêng biệt cho mỗi lớp (All) dường như không cung cấp nhiều lợi ích, đặc biệt khi kết hợp với chuẩn hóa, nơi các tác nhân hoàn toàn không thể học.

Tóm lại, kết quả của chúng tôi cho thấy có thể có triển vọng trong việc khám phá các kiến trúc thay thế để sử dụng với hỗn hợp chuyên gia.

6. Công trình liên quan
Hỗn hợp Chuyên gia MoEs lần đầu được đề xuất bởi Jacobs et al. (1991) và gần đây đã giúp mở rộng các mô hình ngôn ngữ lên hàng nghìn tỷ tham số nhờ vào bản chất mô-đun của chúng, tạo điều kiện cho huấn luyện phân tán, và cải thiện hiệu quả tham số tại thời điểm suy luận (Lepikhin et al., 2020; Fedus et al., 2022). MoEs cũng được nghiên cứu rộng rãi trong thị giác máy tính (Wang et al., 2020; Yang et al., 2019; Abbas & Andreopoulos, 2020; Pavlitskaya et al., 2020), nơi chúng cho phép mở rộng vision transformers lên hàng tỷ tham số trong khi giảm chi phí tính toán suy luận một nửa (Riquelme et al., 2021), giúp học liên tục dựa trên thị giác (Lee et al., 2019), và các bài toán đa nhiệm vụ (Fan et al., 2022). MoEs cũng giúp hiệu suất trong các thiết lập học chuyển giao và đa nhiệm vụ, ví dụ bằng cách chuyên môn hóa chuyên gia cho các bài toán con (Puigcerver et al., 2020; Chen et al., 2023; Ye & Xu, 2023) hoặc bằng cách giải quyết các vấn đề hiệu suất thống kê của routers (Hazimeh et al., 2021). Đã có ít công trình khám phá MoEs trong RL cho học đơn nhiệm vụ (Ren et al., 2021; Akrour et al., 2022) và đa nhiệm vụ (Hendawy et al., 2024). Tuy nhiên định nghĩa và cách sử dụng "hỗn hợp chuyên gia" của họ có phần khác với chúng tôi, tập trung nhiều hơn vào MoEs trực giao, xác suất, và có thể diễn giải.

Khả năng Mở rộng và Hiệu quả Tham số trong Deep RL Thiếu khả năng mở rộng tham số trong RL sâu có thể được giải thích một phần bởi thiếu hiệu quả tham số. Số lượng tham số không thể được mở rộng hiệu quả nếu những tham số đó không được sử dụng một cách hiệu quả. Công trình gần đây cho thấy rằng các mạng trong RL không sử dụng hết tham số của chúng. Sokar et al. (2023) chứng minh rằng các mạng bị từ số lượng ngày càng tăng của các nơ-ron không hoạt động trong suốt quá trình huấn luyện trực tuyến. Hành vi tương tự được quan sát bởi Gulcehre et al. (2022) trong RL ngoại tuyến. Arnob et al. (2021) cho thấy rằng 95% tham số mạng có thể được cắt tỉa tại khởi tạo trong RL ngoại tuyến mà không mất hiệu suất. Nhiều công trình chứng minh rằng việc đặt lại trọng số mạng định kỳ cải thiện hiệu suất (Nikishin et al., 2022; Dohare et al., 2021; Sokar et al., 2023; D'Oro et al., 2022; Igl et al., 2020; Schwarzer et al., 2023).

Một hướng nghiên cứu khác chứng minh rằng các mạng RL có thể được huấn luyện với mức độ thưa cao (~90%) mà không mất hiệu suất (Tan et al., 2022; Sokar et al., 2022; Graesser et al., 2022; Ceron et al., 2024). Những quan sát này kêu gọi các kỹ thuật để sử dụng tốt hơn các tham số mạng trong huấn luyện RL, chẳng hạn như sử dụng MoEs, mà chúng tôi cho thấy giảm các nơ-ron ngủ đáng kể trên nhiều nhiệm vụ và kiến trúc. Để cho phép mở rộng mạng trong RL sâu, các công trình trước đó tập trung vào các phương pháp thuật toán (Farebrother et al., 2022; Taiga et al., 2022; Schwarzer et al., 2023; Farebrother et al., 2024). Ngược lại, chúng tôi tập trung vào các topo mạng thay thế để cho phép tính mạnh mẽ đối với việc mở rộng.

7. Thảo luận và Kết luận
Khi RL tiếp tục được sử dụng cho các nhiệm vụ ngày càng phức tạp, chúng ta có thể sẽ cần các mạng lớn hơn. Như nghiên cứu gần đây đã cho thấy (và kết quả của chúng tôi xác nhận), việc mở rộng tham số mạng một cách ngây thơ không dẫn đến hiệu suất được cải thiện. Công trình của chúng tôi cho thấy thực nghiệm rằng MoEs có tác động có lợi đối với hiệu suất của các tác nhân dựa trên giá trị trên một tập đa dạng các chế độ huấn luyện.

Hỗn hợp Chuyên gia tạo ra một dạng tính thưa có cấu trúc trong mạng nơ-ron, khơi gợi câu hỏi liệu các lợi ích mà chúng ta quan sát có đơn giản là hệ quả của tính thưa này thay vì bản thân các mô-đun MoE. Kết quả của chúng tôi cho thấy rằng có khả năng là sự kết hợp của cả hai: Hình 1 chứng minh rằng trong Rainbow, việc thêm một mô-đun MoE với một chuyên gia duy nhất mang lại cải thiện hiệu suất có ý nghĩa thống kê, trong khi Hình 5 chứng minh rằng người ta có thể giảm số chiều chuyên gia mà không hy sinh hiệu suất. Panel bên phải của Hình 8 tiếp tục xác nhận sự cần thiết của các tham số bổ sung trong các mô-đun Soft MoE.

Các phát hiện gần đây trong tài liệu chứng minh rằng trong khi các mạng RL có xu hướng tự nhiên hướng tới tính thưa cấp nơ-ron có thể làm tổn hại hiệu suất (Sokar et al., 2023), chúng có thể hưởng lợi rất nhiều từ tính thưa cấp tham số rõ ràng (Graesser et al., 2022). Khi kết hợp với các phát hiện của chúng tôi, chúng cho thấy rằng vẫn còn nhiều chỗ để khám phá, và hiểu, vai trò mà tính thưa có thể đóng trong việc huấn luyện các mạng RL sâu, đặc biệt đối với khả năng mở rộng tham số.

Ngay cả trong thiết lập hẹp mà chúng tôi đã tập trung (thay thế lớp gần cuối của các tác nhân dựa trên giá trị bằng MoEs cho học ngoại chính sách trong các thiết lập đơn nhiệm vụ), vẫn có nhiều câu hỏi mở có thể tăng thêm lợi ích của MoEs: các giá trị k khác nhau cho Top1-MoEs, các lựa chọn tokenization khác nhau, sử dụng tỷ lệ học khác nhau (và có thể bộ tối ưu hóa) cho routers, trong số những cái khác. Tất nhiên, mở rộng ra ngoài ALE có thể cung cấp kết quả và hiểu biết toàn diện hơn, có khả năng với một phần chi phí tính toán (Ceron & Castro, 2021).

Các kết quả được trình bày trong Phần 5 cho thấy MoEs có thể đóng vai trò có lợi tổng quát hơn trong việc huấn luyện các tác nhân RL sâu. Rộng hơn, các phát hiện của chúng tôi xác nhận tác động mà các lựa chọn thiết kế kiến trúc có thể có đối với hiệu suất cuối cùng của các tác nhân RL. Chúng tôi hy vọng các phát hiện của mình khuyến khích nhiều nhà nghiên cứu hơn tiếp tục điều tra hướng nghiên cứu này - vẫn tương đối chưa được khám phá.

Lời cảm ơn
Các tác giả muốn cảm ơn Gheorghe Comanici, Owen He, Alex Muzio, Adrien Ali Taïga, Rishabh Agarwal, Hugo Larochelle, Ayoub Echchahed, và phần còn lại của nhóm Google DeepMind Montreal cho các cuộc thảo luận có giá trị trong quá trình chuẩn bị công trình này; Gheorghe xứng đáng được đề cập đặc biệt vì đã cung cấp cho chúng tôi phản hồi quý giá về bản thảo đầu tiên của bài báo. Chúng tôi cảm ơn các reviewer ẩn danh vì sự giúp đỡ quý báu trong việc cải thiện bản thảo của chúng tôi. Chúng tôi cũng muốn cảm ơn cộng đồng Python (Van Rossum & Drake Jr, 1995; Oliphant, 2007) vì đã phát triển các công cụ cho phép công trình này, bao gồm NumPy (Harris et al., 2020), Matplotlib (Hunter, 2007), Jupyter (Kluyver et al., 2016), Pandas (McKinney, 2013) và JAX (Bradbury et al., 2018).

Tuyên bố tác động
Bài báo này trình bày công trình có mục tiêu thúc đẩy lĩnh vực Máy học, và học tăng cường nói riêng. Có nhiều hệ quả xã hội tiềm năng của công trình chúng tôi, không có gì mà chúng tôi cảm thấy phải được làm nổi bật cụ thể ở đây.

Tài liệu tham khảo
[Tài liệu tham khảo được duy trì nguyên như bản gốc do tính chuyên môn và quy chuẩn học thuật]
