# Q-Ensemble cho RL Ngoại tuyến: Đừng Mở rộng Ensemble, Hãy Mở rộng Kích thước Batch

Alexander Nikulin
Tinkoff
a.p.nikulin@tinkoff.ai

Vladislav Kurenkov
Tinkoff
v.kurenkov@tinkoff.ai

Denis Tarasov
Tinkoff
den.tarasov@tinkoff.ai

Dmitry Akimov
Tinkoff
d.akimov@tinkoff.ai

Sergey Kolesnikov
Tinkoff
s.s.kolesnikov@tinkoff.ai

## Tóm tắt

Việc huấn luyện các mạng neural lớn được biết đến là tốn thời gian, với thời gian học có thể kéo dài hàng ngày hoặc thậm chí hàng tuần. Để giải quyết vấn đề này, tối ưu hóa batch lớn đã được giới thiệu. Phương pháp này đã chứng minh rằng việc mở rộng kích thước mini-batch với các điều chỉnh tốc độ học phù hợp có thể tăng tốc quá trình huấn luyện lên nhiều bậc độ. Trong khi thời gian huấn luyện dài thường không phải là vấn đề lớn đối với các thuật toán RL ngoại tuyến sâu không sử dụng mô hình, các phương pháp Q-ensemble được giới thiệu gần đây đạt hiệu suất tối tân đã làm cho vấn đề này trở nên phù hợp hơn, đáng kể mở rộng thời gian huấn luyện. Trong công trình này, chúng tôi chứng minh cách lớp phương pháp này có thể hưởng lợi từ tối ưu hóa batch lớn, điều thường bị bỏ qua bởi cộng đồng RL ngoại tuyến sâu. Chúng tôi cho thấy rằng việc mở rộng kích thước mini-batch và điều chỉnh tốc độ học một cách ngây thơ cho phép (1) giảm kích thước của Q-ensemble, (2) tăng cường phạt các hành động ngoài phân phối, và (3) cải thiện thời gian hội tụ, hiệu quả rút ngắn thời gian huấn luyện trung bình 3-4 lần.

## 1 Giới thiệu

Học Tăng cường Ngoại tuyến (ORL) cung cấp góc nhìn dựa trên dữ liệu về việc học các chính sách ra quyết định bằng cách sử dụng dữ liệu đã thu thập trước đó mà không có bất kỳ tương tác trực tuyến bổ sung nào trong quá trình huấn luyện (Lange et al., 2012; Levine et al., 2020). Mặc dù có sự phát triển gần đây (Fujimoto et al., 2019; Nair et al., 2020; An et al., 2021; Zhou et al., 2021; Kumar et al., 2020) và tiến bộ ứng dụng (Zhan et al., 2022; Apostolopoulos et al., 2021; Soares et al., 2021), một trong những thách thức hiện tại trong ORL vẫn là lỗi ngoại suy của thuật toán, đó là sự bất lực trong việc ước tính chính xác giá trị của các hành động chưa từng thấy (Fujimoto et al., 2019). Nhiều thuật toán đã được thiết kế để giải quyết vấn đề này. Ví dụ, Kostrikov et al. (2021) (IQL) hoàn toàn tránh ước tính cho các hành động ngoài mẫu. Tương tự, Kumar et al. (2020) (CQL) phạt các hành động ngoài phân phối sao cho giá trị của chúng bị giới hạn dưới. Các phương pháp khác một cách rõ ràng làm cho chính sách học được gần hơn với chính sách hành vi (Fujimoto & Gu, 2021; Nair et al., 2020; Wang et al., 2020).

Trái ngược với các nghiên cứu trước đó, các công trình gần đây (An et al., 2021) đã chứng minh rằng việc đơn giản tăng số lượng ước tính giá trị trong thuật toán Soft Actor-Critic (SAC) (Haarnoja et al., 2018) là đủ để nâng cao hiệu suất tối tân một cách nhất quán trên các tập dữ liệu khác nhau trong benchmark D4RL (Fu et al., 2020). Hơn nữa, An et al. (2021) đã cho thấy rằng thủ thuật double-clip thực sự hoạt động như một cơ chế định lượng độ không chắc chắn cung cấp cận dưới của ước tính, và việc đơn giản tăng số lượng critics có thể dẫn đến phạt đủ cho các hành động ngoài phân phối. Mặc dù có kết quả tối tân, lợi ích hiệu suất cho một số tập dữ liệu đòi hỏi thời gian tính toán đáng kể hoặc tối ưu hóa một hạng tử bổ sung, dẫn đến thời gian huấn luyện kéo dài (Hình 2).

Trong bài báo này, được truyền cảm hứng từ các công trình song song về giảm thời gian huấn luyện của các mô hình lớn trong các lĩnh vực khác của học sâu (You et al., 2019, 2017) (thường được gọi là tối ưu hóa batch lớn), chúng tôi nghiên cứu việc sử dụng batch lớn bị bỏ qua trong bối cảnh ORL sâu. Chúng tôi chứng minh rằng, thay vì tăng số lượng critics hoặc giới thiệu một hạng tử tối ưu hóa bổ sung trong thuật toán SAC-N (An et al., 2021), việc mở rộng batch đơn giản và điều chỉnh ngây thơ tốc độ học có thể (1) cung cấp phạt đủ cho các hành động ngoài phân phối và (2) khớp hiệu suất tối tân trên benchmark D4RL. Hơn nữa, phương pháp tối ưu hóa batch lớn này giảm đáng kể thời gian hội tụ, làm cho việc huấn luyện mô hình nhanh hơn 4 lần trên thiết lập GPU đơn trở nên khả thi. Theo hiểu biết của chúng tôi, đây là nghiên cứu đầu tiên xem xét tối ưu hóa batch lớn trong thiết lập ORL.

## 2 Q-Ensemble Cho RL Ngoại tuyến

[Hình 1: Sự khác biệt giữa SAC-N được giới thiệu gần đây, EDAC, và LB-SAC được đề xuất. Phương pháp được giới thiệu không yêu cầu hạng tử tối ưu hóa phụ trợ trong khi làm cho việc giảm hiệu quả số lượng critics trong Q-ensemble bằng cách chuyển sang thiết lập tối ưu hóa batch lớn trở nên khả thi.]

Ensembles có lịch sử lâu dài ứng dụng trong cộng đồng học tăng cường. Chúng được sử dụng trong các phương pháp dựa trên mô hình để chống lại lỗi tích lũy và khai thác mô hình (Kurutach et al., 2018; Chua et al., 2018; Lai et al., 2020; Janner et al., 2019), trong không sử dụng mô hình để tăng đáng kể hiệu quả mẫu (Chen et al., 2021; Hiraoka et al., 2021; Liang et al., 2022) và nói chung để thúc đẩy khám phá trong RL trực tuyến (Osband et al., 2016; Chen et al., 2017; Lee et al., 2021; Ciosek et al., 2019). Trong RL ngoại tuyến, ensembles chủ yếu được sử dụng để mô hình hóa độ không chắc chắn epistemic trong ước tính hàm giá trị (Agarwal et al., 2020; Bai et al., 2022; Ghasemipour et al., 2022), giới thiệu tính bảo thủ nhận thức về độ không chắc chắn.

Gần đây, An et al. (2021) đã điều tra tác động riêng lẻ của clipped Q-learning đến việc đánh giá quá cao giá trị trong RL ngoại tuyến, tăng số lượng critics trong thuật toán Soft Actor Critic (Haarnoja et al., 2018) từ 2 lên N. Một cách đáng ngạc nhiên, với N được điều chỉnh, SAC-N đã vượt trội hơn các thuật toán tối tân trước đó trên benchmark D4RL (Fu et al., 2020) với biên độ lớn, mặc dù yêu cầu lên đến 500 critics trên một số tập dữ liệu. Để giảm kích thước ensemble, An et al. (2021) đã đề xuất EDAC thêm loss phụ trợ để đa dạng hóa ensemble, cho phép giảm đáng kể N (Hình 1).

Q-ensemble bi quan như vậy có thể được diễn giải là sử dụng Lower Confidence Bound (LCB) của các dự đoán Q-value. Giả sử rằng Q(s;a) tuân theo phân phối Gaussian với trung bình μ, độ lệch chuẩn σ và {Qⱼ(s;a)}ᴺⱼ₌₁ là các thực hiện của Q(s;a), chúng ta có thể xấp xỉ tối thiểu kỳ vọng (An et al., 2021; Royston, 1982) như

E[min{j=1,...,N} Qⱼ(s;a)] ≈ μ(s;a) - Φ⁻¹(1/N)σ(s;a)     (1)

trong đó Φ là CDF của phân phối Gaussian chuẩn. Trong thực tế, các hành động OOD có phương sai cao hơn trên ước tính Q-value so với ID (Hình 5a). Do đó, với N tăng, chúng ta tăng cường phạt giá trị cho các hành động OOD, tạo ra tính bảo thủ. Như chúng tôi sẽ cho thấy trong Phần 5.2, hiệu ứng này có thể được khuếch đại bằng cách mở rộng batch thay vì kích thước ensemble.

## 3 Q-Ensemble Mất Bao Lâu Để Hội Tụ?

[Hình 2: Thời gian hội tụ của các thuật toán RL ngoại tuyến sâu phổ biến trên 9 tập dữ liệu di chuyển D4RL khác nhau (Fu et al., 2020). Chúng tôi xem xét hội tụ thuật toán tương tự như Reid et al. (2022), tức là đánh dấu hội tụ tại điểm đạt được kết quả tương tự như những kết quả được báo cáo trong bảng hiệu suất. Các hộp trắng biểu thị giá trị trung bình (cũng được minh họa trên trục x). Các kim cương đen biểu thị các mẫu. Lưu ý rằng thời gian hội tụ của các phương pháp dựa trên Q-ensemble cao hơn đáng kể.]

Để chứng minh thời gian huấn luyện phình to của các phương pháp Q-ensemble, chúng tôi bắt đầu với phân tích thời gian hội tụ của chúng so với các thuật toán RL ngoại tuyến sâu khác. Ở đây, chúng tôi cho thấy rằng trong khi các phương pháp này nhất quán đạt hiệu suất tối tân trên nhiều tập dữ liệu, thời gian cần thiết để có được những kết quả như vậy dài hơn đáng kể khi so sánh với các đối thủ gần nhất của chúng.

Mặc dù không phải là thực hành phổ biến để báo cáo thời gian huấn luyện hoặc tốc độ hội tụ, một số tác giả đã phân tích cẩn thận những đặc điểm này của các thuật toán mới được đề xuất của họ (Fujimoto & Gu, 2021; Reid et al., 2022). Có hai phương pháp tiếp cận chủ đạo cho phân tích này. Thứ nhất là nghiên cứu tổng thời gian huấn luyện cho một số bước huấn luyện cố định hoặc tốc độ mỗi bước huấn luyện (Fujimoto & Gu, 2021; An et al., 2021). Phương pháp tiếp cận thứ hai, như được Reid et al. (2022) áp dụng, là đo thời gian hội tụ sử dụng thời gian đồng hồ tương đối để đạt được kết quả được báo cáo trong bảng hiệu suất.

Đo tổng thời gian huấn luyện cho một số bước cố định có thể được coi là phương pháp tiếp cận đơn giản. Tuy nhiên, nó không tính đến việc một số thuật toán thực sự có thể yêu cầu số bước học nhỏ hơn (hoặc lớn hơn). Do đó, trong bài báo này, chúng tôi chọn phương pháp tiếp cận thứ hai, bao gồm việc đo thời gian đồng hồ tương đối cho đến hội tụ. Tương tự như Reid et al. (2022), chúng tôi coi một thuật toán hội tụ khi đánh giá của nó (trên nhiều seed) đạt điểm số chuẩn hóa trong vòng hai điểm hoặc cao hơn so với điểm được báo cáo cho phương pháp tương ứng trong bảng hiệu suất.

Chúng tôi đã cẩn thận triển khai lại tất cả các baseline trong cùng một codebase và chạy lại các thí nghiệm để đảm bảo rằng chúng được thực hiện trên cùng phần cứng (Tesla A100) để thời gian đồng hồ tương đối có thể so sánh được. Kết quả được mô tả trong Hình 2. Có thể thấy rằng IQL và TD3+BC rất hiệu quả so với các đối thủ cạnh tranh, và thời gian hội tụ của chúng không vượt quá hai giờ ngay cả trong các trường hợp xấu nhất. Hiệu quả này đến từ thực tế là các phương pháp này không mất nhiều thời gian cho một bước huấn luyện (Fujimoto & Gu, 2021; Kostrikov et al., 2021). Tuy nhiên, trong khi nhanh để huấn luyện, chúng hoạt động kém nghiêm trọng khi so sánh với các phương pháp dựa trên Q-ensemble (SAC-N, EDAC).

Thật không may, cải thiện này có cái giá. Ví dụ, thời gian hội tụ trung vị của SAC-N dưới hai giờ (dài hơn 2-3 lần so với IQL hoặc TD3+BC), nhưng một số tập dữ liệu có thể yêu cầu lên đến chín giờ huấn luyện. Điều này do việc sử dụng số lượng critics cao, đòi hỏi nhiều thời gian hơn cho cả lượt truyền thuận và ngược. Đáng chú ý, EDAC được thiết kế đặc biệt để tránh sử dụng số lượng critics lớn, báo cáo chi phí tính toán nhỏ hơn mỗi bước huấn luyện (An et al., 2021) và tiêu thụ bộ nhớ thấp hơn. Tuy nhiên, như có thể thấy trong Hình 2, thời gian hội tụ của nó vẫn ngang bằng với thuật toán SAC-N. Như chúng tôi đã thảo luận trước đó, một số thuật toán có thể yêu cầu số lần lặp huấn luyện cao hơn để hội tụ, đó chính xác là trường hợp của thuật toán EDAC.

## 4 Large Batch Soft Actor-Critic

Một phương pháp tiếp cận phổ biến để giảm thời gian huấn luyện của các mô hình học sâu là sử dụng tối ưu hóa batch lớn (Hoffer et al., 2017; You et al., 2017, 2019). Trong phần này, chúng tôi điều tra một dòng lý luận tương tự cho ORL sâu và đề xuất một số điều chỉnh cho thuật toán SAC-N (An et al., 2021) để tận dụng kích thước mini-batch lớn:

1. **Mở rộng mini-batch**: Thay vì sử dụng số lượng critics lớn hơn đáng kể, như được thực hiện trong SAC-N, chúng tôi thay vào đó tăng đáng kể kích thước batch từ 256 bộ ba state-action-reward thường được sử dụng lên 10k. Lưu ý rằng trong trường hợp các tập dữ liệu D4RL thường được sử dụng và kiến trúc actor-critic, việc tăng này không yêu cầu sử dụng nhiều thiết bị GPU hơn và có thể được thực hiện trên thiết lập GPU đơn. Trong khi kích thước batch cao hơn cũng khả thi, chúng tôi sẽ chứng minh trong Phần 5 rằng giá trị này là đủ, và không dẫn đến quá bảo thủ.

2. **Mở rộng tốc độ học theo căn bậc hai**: Trong các lĩnh vực nghiên cứu khác, thường thấy rằng việc tăng kích thước batch đơn giản có thể có hại cho hiệu suất, và cần các bổ sung khác (Hoffer et al., 2017). Điều chỉnh tốc độ học là một sửa đổi như vậy. Ở đây, chúng tôi sử dụng bộ tối ưu Adam (Kingma & Ba, 2014), cố định tốc độ học được tính bằng công thức tương tự như Krizhevsky (2014) cũng được biết đến như "mở rộng căn bậc hai":

learning_rate = base_learning_rate × √(BatchSize/BaseBatchSize)     (2)

trong đó chúng tôi lấy cả base_learning_rate và base_batch_size bằng các giá trị được sử dụng trong thuật toán SAC-N. Lưu ý rằng chúng giống nhau trên tất cả các tập dữ liệu. Các giá trị cụ thể có thể được tìm thấy trong Phụ lục A.5. Không giống như Hoffer et al. (2017), chúng tôi không sử dụng giai đoạn khởi động.

Chúng tôi gọi các sửa đổi được mô tả là Large-Batch SAC (LB-SAC). Nhìn chung, phương pháp tiếp cận kết quả tương đương với thuật toán SAC-N, nhưng với các giá trị được điều chỉnh cẩn thận thường được coi là siêu tham số. Hình 1 tóm tắt các đặc điểm đặc biệt của LB-SAC và các thuật toán dựa trên Q-ensemble ngoại tuyến sâu được đề xuất gần đây.

## 5 Thí nghiệm

Trong phần này, chúng tôi trình bày kết quả thực nghiệm so sánh LB-SAC với các phương pháp Q-ensemble khác. Chúng tôi chứng minh rằng LB-SAC cải thiện đáng kể thời gian hội tụ trong khi khớp hiệu suất cuối cùng của các phương pháp khác, và sau đó phân tích điều gì góp phần vào hiệu suất như vậy.

### 5.1 Đánh giá trên Các Tác vụ D4RL MuJoCo Gym

Chúng tôi chạy thí nghiệm trên tập con di chuyển MuJoCo thường được sử dụng của benchmark D4RL (Fu et al., 2020): Hopper, HalfCheetah, và Walker2D. Tương tự như các thí nghiệm được tiến hành trong Phần 3, chúng tôi sử dụng cùng thiết bị GPU và codebase cho tất cả các so sánh.

**Điểm Số Chuẩn hóa LB-SAC**: Đầu tiên, chúng tôi báo cáo điểm số cuối cùng của phương pháp được giới thiệu. Kết quả được minh họa trong Bảng 1. Chúng tôi thấy rằng điểm số kết quả vượt trội hơn thuật toán SAC-N gốc và khớp với điểm số EDAC. Hơn nữa, khi so sánh trên toàn bộ bộ tập dữ liệu di chuyển, hiệu suất cuối cùng trung bình cao hơn cả kết quả SAC-N và EDAC (xem Bảng 3 trong Phụ lục). Chúng tôi nổi bật kết quả này, vì đây là quan sát phổ biến trong tối ưu hóa batch lớn rằng điều chỉnh tốc độ học ngây thơ thường dẫn đến suy giảm quá trình học, làm cho các phương pháp thích ứng hoặc giai đoạn khởi động trở nên cần thiết (Hoffer et al., 2017; You et al., 2017, 2019). Rõ ràng, loại xử lý này có thể được bỏ qua trong thiết lập ORL cho thuật toán SAC-N.

[Bảng 1: Điểm số chuẩn hóa cuối cùng trên các tác vụ D4RL Gym, trung bình trên 4 seed ngẫu nhiên. LB-SAC vượt trội hơn SAC-N và đạt hiệu suất tương tự như thuật toán EDAC trong khi hội tụ nhanh hơn đáng kể như được mô tả trong Hình 3.]

**Thời gian Hội tụ LB-SAC**: Thứ hai, chúng tôi nghiên cứu thời gian hội tụ theo cách tương tự như Phần 3. Kết quả được mô tả trong Hình 3. Có thể thấy rằng thời gian huấn luyện của LB-SAC ít hơn cả EDAC và SAC-N, vượt trội hơn chúng về thời gian hội tụ trung bình và tệ nhất. Hơn nữa, LB-SAC thậm chí vượt trội hơn CQL trong trường hợp xấu nhất, đạt thời gian hội tụ trung bình tương đương. Cải thiện này đến từ cả (1) việc sử dụng số lượng critics thấp, tương tự như EDAC (xem Hình 4) và (2) cải thiện tốc độ hội tụ ensemble, mà chúng tôi chứng minh thêm trong Phần 5.2.

[Bảng 2: Số lượng critics và tiêu thụ bộ nhớ trường hợp xấu nhất cho các tập dữ liệu di chuyển D4RL.]

**Tiêu thụ Bộ nhớ LB-SAC**: Tiêu thụ bộ nhớ là một nhược điểm rõ ràng của việc sử dụng kích thước batch lớn. Ở đây, chúng tôi báo cáo yêu cầu bộ nhớ trường hợp xấu nhất cho tất cả các thuật toán Q-ensemble. Bảng 2 cho thấy rằng việc sử dụng batch lớn thực sự có thể dẫn đến tăng đáng kể việc sử dụng bộ nhớ. Tuy nhiên, điều này vẫn làm cho việc sử dụng LB-SAC có thể thực hiện được ngay cả khi sử dụng thiết lập GPU đơn trên các thiết bị có sức mạnh tính toán vừa phải, yêu cầu khoảng 5GB bộ nhớ trong các trường hợp xấu nhất.

[Hình 3: Thời gian hội tụ của LB-SAC so với các thuật toán RL ngoại tuyến sâu khác trên 9 tập dữ liệu di chuyển D4RL khác nhau]

[Hình 4: Số lượng Q-ensemble (N) được sử dụng để đạt hiệu suất được báo cáo trong Bảng 1 và thời gian hội tụ trong Hình 3. LB-SAC cho phép sử dụng số lượng thành viên nhỏ hơn mà không cần giới thiệu hạng tử tối ưu hóa bổ sung.]

### 5.2 Kích thước Batch Lớn hơn Phạt Các Hành động OOD Nhanh hơn

Như được lập luận trong An et al. (2021), yếu tố chính góp phần vào thành công của các phương pháp dựa trên Q-ensemble trong thiết lập ORL sâu là việc phạt các hành động dựa trên độ tin cậy dự đoán. Điều này được đạt được bằng cách tối ưu hóa cận dưới của hàm giá trị. Cả việc tăng số lượng critics và đa dạng hóa đầu ra của chúng đều có thể cải thiện độ tin cậy dự đoán, điều này đến lượt nó dẫn đến hiệu suất nâng cao (An et al., 2021). Mặt khác, không giống như SAC-N, phương pháp LB-SAC được đề xuất không dựa vào số lượng lớn critics (xem Hình 4), hoặc hạng tử tối ưu hóa bổ sung cho đa dạng hóa như EDAC yêu cầu, trong khi khớp hiệu suất của chúng với tốc độ hội tụ được cải thiện.

Để giải thích thành công thực nghiệm của LB-SAC, chúng tôi thay đổi kích thước batch và phân tích động lực học học tập theo cách tương tự như An et al. (2021) cả về mặt phạt ngoài phân phối và tính bảo thủ kết quả, hiện tại được biết đến là thuộc tính có thể quan sát chính của các thuật toán ORL thành công (Kumar et al., 2020; Rezaeifar et al., 2022). Để làm như vậy, chúng tôi theo dõi hai giá trị. Đầu tiên, chúng tôi tính độ lệch chuẩn của các q-value cho cả chính sách ngẫu nhiên và hành vi và ghi lại mối quan hệ của nó trong suốt quá trình học. Chính sách trước thường được sử dụng như chính sách tạo ra các hành động ngoài phân phối (Kumar et al., 2020; An et al., 2021). Thứ hai, chúng tôi ước tính khoảng cách giữa các hành động được chọn bởi chính sách học được với những hành động trong tập dữ liệu để theo dõi mức độ bảo thủ của các chính sách kết quả.

Các đường cong học được mô tả trong Hình 5. Chúng tôi quan sát thấy rằng việc tăng kích thước batch dẫn đến tăng trưởng nhanh hơn của mối quan hệ độ lệch chuẩn giữa các hành động ngoài phân phối và trong tập dữ liệu, điều này có thể tạo ra phạt mạnh hơn cho các hành động ngoài phân phối sớm hơn trong quá trình huấn luyện. Điều này, đến lượt nó, dẫn đến mức độ bảo thủ cao hơn, như được chứng minh trong Hình 5b.

[Hình 5: Hiệu ứng của việc tăng kích thước batch. (a) Mô tả mối quan hệ độ lệch chuẩn cho các hành động ngẫu nhiên so với hành động tập dữ liệu (b) Vẽ khoảng cách đến các hành động tập dữ liệu từ chính sách học được (c) Hiển thị điểm số chuẩn hóa trung bình.]

Mặc dù Hình 5 mô tả hiệu ứng của việc tăng kích thước batch đối với tỷ lệ độ lệch chuẩn giữa các hành động ngẫu nhiên và tập dữ liệu, bản chất thực sự của kết quả này vẫn chưa hoàn toàn rõ ràng. Một giải thích có thể dựa trên quan sát thực nghiệm đơn giản được xác nhận bởi thực hành của chúng tôi (Phần 2) và phân tích An et al. (2021): với huấn luyện thêm, Q-ensemble trở nên ngày càng bảo thủ hơn (Hình 5a). Trong khi độ lệch chuẩn cho các hành động tập dữ liệu ổn định ở một giá trị nào đó, đối với các hành động ngẫu nhiên nó tiếp tục tăng trong thời gian rất dài. Do đó, người ta có thể giả thuyết rằng đối với các kích thước ensemble khác nhau N' > N₀, cả hai sẽ đạt được một mức độ bảo thủ được chỉ định trước nào đó, nhưng đối với một cái nhỏ hơn sẽ mất nhiều thời gian huấn luyện hơn. Do đó, vì bằng cách tăng batch chúng tôi cũng tăng tốc độ hội tụ, chúng tôi nên quan sát thấy rằng với kích thước ensemble bằng nhau, LB-SAC sẽ đạt được các giá trị phạt tương tự nhưng sớm hơn.

Để kiểm tra giả thuyết được đề xuất, chúng tôi tiến hành thí nghiệm trên hai tập dữ liệu, cố định số lượng critics, chỉ mở rộng kích thước batch và tốc độ học như được mô tả trong Phần 4 và huấn luyện SAC-N trong 10 triệu thay vì 1 triệu bước huấn luyện. Có thể thấy (Hình 6) rằng các batch lớn hơn thực sự chỉ tăng tốc độ hội tụ, vì SAC-N có thể đạt được các giá trị độ lệch chuẩn tương tự nhưng muộn hơn nhiều trong quá trình huấn luyện. Do đó, trên hầu hết các tác vụ, chúng tôi có thể giảm kích thước ensemble cho LB-SAC, vì hội tụ được tăng tốc bù đắp cho việc giảm đa dạng, cho phép chúng tôi duy trì ở cùng mức độ hoặc cao hơn về tăng trưởng phạt như SAC-N với ensemble lớn hơn.

[Hình 6: Cả LB-SAC và SAC-N với cùng kích thước ensemble đạt được tỷ lệ độ lệch chuẩn tương tự giữa các hành động ngẫu nhiên và tập dữ liệu, nhưng với batch lớn hơn điều này xảy ra nhanh hơn nhiều.]

## 6 Các Nghiên cứu Khử

### 6.1 Batch Nên Lớn Đến Mức Nào?

[Hình 7: Mở rộng kích thước batch hơn nữa dẫn đến lợi ích giảm dần. (a) Mô tả mối quan hệ độ lệch chuẩn cho các hành động ngẫu nhiên so với hành động tập dữ liệu (b) Vẽ khoảng cách giữa các hành động tập dữ liệu và chính sách học được (c) Hiển thị điểm số chuẩn hóa trung bình.]

Quan sát thấy rằng kích thước mini-batch tăng cải thiện hội tụ, một câu hỏi tự nhiên cần hỏi là liệu chúng ta có nên mở rộng nó lớn hơn nữa không. Để trả lời điều này, chúng tôi chạy LB-SAC với kích thước batch lên đến 40k. Chúng tôi phân tích động lực học học tập theo cách tương tự như Phần 5.2. Các đường cong học có thể được tìm thấy trong Hình 7. Nhìn chung, chúng tôi thấy rằng việc tăng thêm dẫn đến lợi ích giảm dần cả về phạt và điểm số kết quả. Ví dụ, Hình 7a cho thấy rằng việc tăng batch thêm dẫn đến cải thiện phạt các hành động ngoài phân phối. Tuy nhiên, cải thiện này trở nên ít rõ rệt hơn khi chúng ta đi từ 20k đến 40k. Có thể thú vị khi lưu ý rằng điểm số chuẩn hóa cho kích thước batch thường được sử dụng là 256 bắt đầu tăng sau một mức độ phạt nhất định, và tính bảo thủ (MSE khoảng 0.3) được đạt được. Rõ ràng, việc sử dụng kích thước batch lớn hơn giúp đạt được mức độ này sớm hơn đáng kể trong quá trình học.

### 6.2 Có Phải Chỉ Là Tốc Độ Học?

Để minh họa rằng tốc độ hội tụ đạt được có lợi từ kích thước batch tăng chứ không chỉ từ tốc độ học cao, chúng tôi tiến hành một nghiên cứu khử, trong đó chúng tôi để kích thước batch tương đương với kích thước được sử dụng trong thuật toán SAC-N (B = 256), nhưng mở rộng tốc độ học. Kết quả được mô tả trong Hình 8.

[Hình 8: Điểm số chuẩn hóa trung bình. Cải thiện đến từ cả kích thước batch lớn và tốc độ học được điều chỉnh. Cố định kích thước batch theo các giá trị thường được sử dụng cho SAC-N và mở rộng tốc độ học không giúp ích.]

Không có gì đáng ngạc nhiên, việc mở rộng tốc độ học mà không làm tương tự cho kích thước batch không dẫn đến các chính sách hiệu quả và trì trệ tại một điểm nào đó trong quá trình huấn luyện. Điều này có thể được coi là dự kiến, vì các siêu tham số cho thuật toán SAC cơ bản đã được điều chỉnh rộng rãi bởi một số lượng lớn các công trình về RL off-policy và dường như chuyển giao đầy đủ sang thiết lập ngoại tuyến, như được mô tả trong An et al. (2021).

### 6.3 Các Bộ Tối ưu Thích ứng Lớp Có Giúp ích không?

[Hình 9: Sử dụng các bộ tối ưu thích ứng lớp dẫn đến hiệu suất tương tự hoặc làm suy giảm quá trình học. (a) Mô tả mối quan hệ độ lệch chuẩn cho các hành động ngẫu nhiên so với hành động tập dữ liệu (b) Hiển thị điểm số chuẩn hóa trung bình.]

Trong khi chúng tôi đã quyết định về phương pháp ngây thơ cho tối ưu hóa batch lớn trong bối cảnh RL ngoại tuyến sâu, người ta có thể tự hỏi liệu việc sử dụng các bộ tối ưu được thiết lập và tinh vi hơn có thể hoạt động tốt không. Ở đây, chúng tôi xem xét các bộ tối ưu LARS (You et al., 2017) và LAMB (You et al., 2019) thường được sử dụng, báo cáo động lực học học tập tương tự như các phần trước. Các đường cong kết quả có thể được tìm thấy trong Hình 9.

Chúng tôi thấy rằng bộ tối ưu LAMB không thể thành công trong quá trình huấn luyện phân kỳ với nhiều bước học hơn. Mặt khác, LARS hoạt động rất tương tự như các điều chỉnh được đề xuất. Mặc dù chúng tôi không hề đề xuất rằng tốc độ học thích ứng không hữu ích cho RL ngoại tuyến, việc áp dụng trực tiếp các bộ tối ưu batch lớn được thiết lập cho kiến trúc mạng neural thường được sử dụng (các lớp tuyến tính với activation) trong bối cảnh các tác vụ di chuyển không mang lại nhiều lợi ích. Dường như, các phương pháp này có thể yêu cầu nhiều điều chỉnh siêu tham số hơn, quy tắc mở rộng tốc độ học khác nhau hoặc lịch trình khởi động trái ngược với "mở rộng căn bậc hai" đơn giản.

## 7 Kết luận

Trong công trình này, chúng tôi đã chứng minh cách việc sử dụng tối ưu hóa batch lớn bị bỏ qua có thể được tận dụng trong bối cảnh RL ngoại tuyến sâu. Chúng tôi đã cho thấy rằng việc điều chỉnh ngây thơ tốc độ học trong thiết lập tối ưu hóa batch lớn là đủ để giảm đáng kể thời gian huấn luyện của các phương pháp dựa trên Q-ensemble (trung bình 4 lần) mà không cần sử dụng tốc độ học thích ứng.

Hơn nữa, chúng tôi đã minh họa thực nghiệm rằng việc sử dụng kích thước batch lớn dẫn đến tăng phạt các hành động ngoài phân phối, làm cho nó trở thành sự thay thế hiệu quả cho việc tăng số lượng ước tính q-value trong một ensemble hoặc hạng tử tối ưu hóa bổ sung. Chúng tôi hy vọng rằng công trình này có thể phục vụ như một điểm khởi đầu cho việc điều tra thêm về tối ưu hóa batch lớn trong bối cảnh RL ngoại tuyến sâu.
