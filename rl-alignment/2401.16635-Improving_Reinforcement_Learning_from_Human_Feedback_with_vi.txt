# Cải thiện Học tăng cường từ Phản hồi của Con người với
Ensemble Mô hình Thưởng Hiệu quả
Shun Zhang1, Zhenfang Chen1, Sunli Chen2, Yikang Shen1, Zhiqing Sun3, và Chuang Gan1,4
1MIT-IBM Watson AI Lab, 2Đại học Tsinghua, 3Đại học Carnegie Mellon, 4UMass Amherst

Tóm tắt
Học tăng cường từ Phản hồi của Con người (RLHF) là một phương pháp được chấp nhận rộng rãi để điều chỉnh các mô hình ngôn ngữ lớn với các giá trị của con người. Tuy nhiên, RLHF dựa vào một mô hình thưởng được huấn luyện với lượng dữ liệu ưu tiên của con người hạn chế, điều này có thể dẫn đến các dự đoán không chính xác. Kết quả là, RLHF có thể tạo ra các đầu ra không phù hợp với giá trị của con người. Để giảm thiểu vấn đề này, chúng tôi đóng góp một phương pháp ensemble thưởng cho phép mô hình thưởng đưa ra dự đoán chính xác hơn. Vì việc sử dụng ensemble các mô hình thưởng dựa trên mô hình ngôn ngữ lớn có thể tốn kém về mặt tính toán và tài nguyên, chúng tôi khám phá các phương pháp ensemble hiệu quả bao gồm ensemble lớp tuyến tính và ensemble dựa trên LoRA. Thực nghiệm, chúng tôi chạy Best-of-n và Tối ưu hóa Chính sách Gần (Proximal Policy Optimization) với các mô hình thưởng ensemble của chúng tôi, và xác minh rằng các phương pháp ensemble của chúng tôi giúp cải thiện hiệu suất điều chỉnh của các đầu ra RLHF.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLMs) (Vaswani et al., 2017) đã trở nên nổi bật trong lĩnh vực trí tuệ nhân tạo trong việc giải quyết một loạt các nhiệm vụ phức tạp trong trả lời câu hỏi (Ouyang et al., 2022; Guu et al., 2020), tạo mã (Li et al., 2022; Nijkamp et al., 2023), lý luận và lập kế hoạch (Kojima et al., 2023; Liu et al., 2023), và nhiều lĩnh vực khác. Tuy nhiên, vì các mô hình ngôn ngữ lớn được huấn luyện bằng dữ liệu từ nhiều nguồn khác nhau, chúng có thể tạo ra các đầu ra thiên vị, không phù hợp, hoặc thậm chí có hại, không phù hợp với giá trị của con người. Do đó, việc điều chỉnh các mô hình ngôn ngữ lớn với giá trị của con người là rất quan trọng để chúng có thể được triển khai an toàn.

Gần đây, Học tăng cường từ Phản hồi của Con người (RLHF) (Ouyang et al., 2022) đã được chứng minh là một phương pháp đầy hứa hẹn để giảm thiểu vấn đề không phù hợp. Cụ thể, RLHF đầu tiên chạy tinh chỉnh có giám sát (SFT) để huấn luyện một mô hình ngôn ngữ lớn tạo ra các phản hồi tuân theo hướng dẫn. Sau đó, nó huấn luyện một mô hình thưởng sử dụng tập dữ liệu ưu tiên phản ánh giá trị của con người. Cuối cùng, nó chạy học tăng cường sử dụng mô hình thưởng đã học để tinh chỉnh mô hình SFT. Theo cách này, mô hình được tinh chỉnh sẽ tạo ra các chuỗi với phần thưởng cao hơn, được cho là phù hợp hơn với giá trị của con người.

Thuật toán RLHF đã chứng minh thành công trong việc cải thiện sự điều chỉnh của các mô hình ngôn ngữ lớn (Dubois et al., 2023; Wu et al., 2023). Tuy nhiên, nó cũng có một số vấn đề được công nhận. Vì mô hình thưởng được huấn luyện trên dữ liệu ưu tiên thu thập ngoại tuyến, các dự đoán thưởng của nó có thể không chính xác trên dữ liệu ngoài phân phối. Nếu chúng ta sử dụng học tăng cường để tối ưu hóa một mô hình ngôn ngữ lớn với một mô hình thưởng không chính xác, nó có thể tạo ra các đầu ra không phù hợp với phần thưởng ước tính cao một cách sai lầm. Vấn đề này được quan sát trong tài liệu điều chỉnh giá trị và thường được biết đến như là hack thưởng hoặc tối ưu hóa thưởng quá mức (Amodei et al., 2016; Leike et al., 2017; Gao et al., 2022; Casper et al., 2023), và cũng là một vấn đề nổi tiếng trong học tăng cường ngoại tuyến dựa trên mô hình (Levine et al., 2020).

Trong học tăng cường ngoại tuyến, một chiến lược phổ biến để giảm thiểu tối ưu hóa thưởng quá mức là ước tính thưởng một cách bảo thủ trong điều kiện không chắc chắn (Kumar et al., 2020). Dựa trên chiến lược này, chúng tôi xem xét một phương pháp ensemble sử dụng một tập hợp các mô hình thưởng để đưa ra dự đoán tốt hơn, phù hợp với các công trình đồng thời (Coste et al., 2023; Eisenstein et al., 2023). Để đạt được ensemble mô hình thưởng, một phương pháp đơn giản là huấn luyện nhiều mô hình thưởng độc lập và sau đó ensemble chúng. Tuy nhiên, phương pháp này gặp một số thách thức trong bối cảnh của chúng tôi. Vì các mô hình thưởng thường dựa trên các mô hình ngôn ngữ lớn, việc huấn luyện tất cả các mô hình thưởng và sau đó tải tất cả chúng trong thời gian suy luận có thể tốn kém về mặt tính toán và tiêu thụ tài nguyên. Do đó, chúng tôi đóng góp trong việc thiết kế các phương pháp ensemble hiệu quả. Cuối cùng, chúng tôi xác minh thực nghiệm hiệu quả của các phương pháp ensemble của chúng tôi bằng cách sử dụng các điểm chuẩn đánh giá được chấp nhận rộng rãi, AlpacaEval (Dubois et al., 2023) và MT-Bench (Zheng et al., 2023). Các đóng góp của bài báo này được tóm tắt như sau.

• Chúng tôi thiết kế các thuật toán ensemble thưởng cải thiện độ chính xác ước tính thưởng, và do đó cải thiện sự điều chỉnh của các mô hình ngôn ngữ lớn.

• Chúng tôi đề xuất hai phương pháp ensemble, ensemble lớp tuyến tính và ensemble dựa trên LoRA, cung cấp sự đánh đổi giữa hiệu quả tính toán và hiệu suất điều chỉnh.

• Chúng tôi sử dụng các mô hình thưởng ensemble cho cả thuật toán Best-of-n và Tối ưu hóa Chính sách Gần (PPO), đánh giá chúng trên AlpacaEval và MT-Bench, và xác minh thực nghiệm rằng RLHF với các mô hình thưởng ensemble của chúng tôi vượt trội hơn thuật toán RLHF tiêu chuẩn.

2 Nền tảng và Công trình Liên quan

Học tăng cường từ phản hồi của con người (RLHF). RLHF ban đầu được xem xét trong khung TAMER (Knox and Stone, 2009), trong đó một tác nhân học hàm thưởng từ phản hồi tích cực hoặc tiêu cực của người dùng. Bối cảnh này sau đó được xem xét trong học tăng cường sâu (Christiano et al., 2017), và gần đây được sử dụng để tinh chỉnh các mô hình ngôn ngữ lớn (Ouyang et al., 2022) để điều chỉnh hành vi của mô hình với giá trị và ưu tiên của con người. Chúng tôi đã tổng quan về khung RLHF trong phần giới thiệu và để lại chi tiết thêm về thuật toán RLHF tiêu chuẩn trong Mục A.1.

Các mô hình ensemble và ước tính không chắc chắn. Ensemble mô hình đã là một phương pháp được chấp nhận để cải thiện độ chính xác của mô hình và ước tính sự không chắc chắn của mô hình. Lakshminarayanan et al. (2017) định lượng sự không chắc chắn dự đoán trong mạng nơ-ron sâu bằng cách sử dụng ensemble, hoạt động tốt hơn mạng nơ-ron Bayesian. Trong học tăng cường, Liang et al. (2022) sử dụng các mô hình thưởng ensemble để ước tính sự không chắc chắn của mô hình cho việc khám phá có thông tin hơn. Gleave và Irving (2022) sử dụng các mô hình thưởng ensemble cho học tích cực.

Đồng thời, Coste et al. (2023) cũng sử dụng ensemble mô hình thưởng để giảm thiểu vấn đề tối ưu hóa mô hình thưởng quá mức và đưa ra kết luận tương tự với bài báo của chúng tôi. Sự khác biệt chính là công trình của chúng tôi tập trung vào việc phát triển các phương pháp ensemble hiệu quả, vì việc ensemble nhiều mô hình thưởng được huấn luyện độc lập có thể tốn kém. Eisenstein et al. (2023) tập trung vào việc so sánh ensemble trong giai đoạn tiền huấn luyện và tinh chỉnh. Zhai et al. (2023) cũng xem xét ensemble dựa trên LoRA, trong khi công trình của họ tập trung vào một mục tiêu bị phạt do không chắc chắn trong tinh chỉnh RL. Ramé et al. (2024) xem xét một phương pháp khác là tính trung bình trọng số của nhiều mô hình thưởng thay vì ensemble các dự đoán của chúng. Wang et al. (2023) sử dụng ensemble LoRA để cải thiện độ chính xác dự đoán và định lượng không chắc chắn. Ahmed et al. (2024) đề xuất một phương pháp ensemble có thể mở rộng chia sẻ một backbone encoder nhưng sử dụng các head tuyến tính riêng biệt, đạt được hiệu suất tương tự với ensemble đầy đủ.

Học tăng cường ngoại tuyến. Ước tính không chắc chắn cũng là một vấn đề chính trong học tăng cường ngoại tuyến (Levine et al., 2020; Janner et al., 2019). Ví dụ, học Q bảo thủ (CQL) (Kumar et al., 2020) học một hàm Q bảo thủ để giảm thiểu độ lệch ước tính quá mức cho các cặp trạng thái, hành động ngoài phân phối. Lấy cảm hứng từ thuật toán này, chúng tôi cũng thiết kế một thuật toán ensemble sử dụng dự đoán bảo thủ bằng cách sử dụng giới hạn tin cậy thấp hơn của các dự đoán ensemble.

3 Ensemble Mô hình Thưởng

Mô hình thưởng trong RLHF là một mô hình (dựa trên mô hình ngôn ngữ lớn) nhận đầu vào là một hướng dẫn và một phản hồi, và đầu ra là một dự đoán thưởng cho biết hiệu suất điều chỉnh của phản hồi. Trong phần này, chúng tôi cung cấp một mô tả chính thức cho các thuật toán ensemble của chúng tôi.

3.1 Thiết kế Kiến trúc của Ensemble Mô hình Thưởng

Trong tài liệu, các mô hình thưởng thường được tinh chỉnh từ các mô hình ngôn ngữ lớn đã được tiền huấn luyện. Theo quy ước (Dubois et al., 2023), chúng tôi giả định một mô hình thưởng là một mô hình Transformer và một lớp tuyến tính, trong đó lớp tuyến tính nhận đầu vào là lớp ẩn cuối cùng của mô hình Transformer, và đầu ra là một giá trị thưởng.

Trong phần này, chúng tôi thảo luận về các cách có thể để ensemble nhiều mô hình thưởng dựa trên mô hình ngôn ngữ lớn và thảo luận về ưu điểm của chúng. Các thuật toán ensemble được minh họa trong Hình 1 và mã giả được cung cấp trong Thuật toán 1.

Ensemble các mô hình thưởng đơn lẻ. Một cách đơn giản để đạt được ensemble mô hình thưởng là huấn luyện nhiều mô hình thưởng độc lập sử dụng các seed ngẫu nhiên khác nhau. Trong thời gian suy luận, chúng ta chỉ cần tải tất cả các mô hình thưởng và ensemble chúng. Tuy nhiên, phương pháp như vậy có thể tốn kém cả trong huấn luyện và suy luận - trong quá trình huấn luyện, chúng ta cần chạy huấn luyện thưởng nhiều lần; trong quá trình suy luận, chúng ta cần tải nhiều mô hình thưởng dựa trên mô hình ngôn ngữ lớn đồng thời lên GPU, điều này có thể tiêu thụ tài nguyên. Cụ thể, một ensemble gồm k mô hình thưởng được huấn luyện độc lập cần huấn luyện và tải k(M+L) tham số, trong đó M là số tham số của mô hình Transformer, và L là số tham số của một lớp tuyến tính (được hiển thị trong Hình 1 (a)).

Ensemble lớp tuyến tính. Để làm cho việc ensemble hiệu quả hơn cả trong huấn luyện và suy luận, chúng ta có thể để tất cả các mô hình ensemble chia sẻ cùng một mô hình Transformer, trong khi mỗi mô hình ensemble có lớp tuyến tính riêng của nó đầu ra dự đoán thưởng của nó. Trong quá trình huấn luyện, cả mô hình Transformer và các lớp tuyến tính của tất cả các mô hình ensemble đều được huấn luyện. Lưu ý rằng mô hình Transformer là giống nhau cho tất cả các mô hình thưởng ensemble. Theo cách này, một mô hình ensemble lớp tuyến tính có kích thước k chỉ yêu cầu M+kL tham số (được hiển thị trong Hình 1 (b)).

Ensemble dựa trên LoRA. Trong ensemble lớp tuyến tính, việc cho phép tất cả các mô hình ensemble chia sẻ cùng một mô hình Transformer thực sự giảm tổng số tham số cần huấn luyện. Tuy nhiên, nó có thể hạn chế đáng kể sự đa dạng của các mô hình ensemble. Do đó, chúng tôi cho phép mỗi mô hình ensemble tinh chỉnh nhẹ mô hình Transformer. Cụ thể, mỗi mô hình ensemble huấn luyện lớp tuyến tính riêng của nó, và một bộ điều hợp LoRA (Hu et al., 2021) được thêm vào mô hình Transformer. Bộ điều hợp LoRA chỉ có một số lượng nhỏ tham số và có thể được huấn luyện một cách hiệu quả. (Nền tảng về LoRA được cung cấp trong Mục A.2.) Theo cách này, một mô hình ensemble dựa trên LoRA có kích thước k yêu cầu M+kL+kA tham số, trong đó A là số tham số của một bộ điều hợp (được hiển thị trong Hình 1 (c)).

Thực nghiệm, chúng tôi thấy rằng chỉ tinh chỉnh LoRA mô hình Transformer trong mô hình thưởng không hoạt động tốt, vì mô hình Transformer không được huấn luyện cho dự đoán thưởng chút nào. Vì vậy trong các thí nghiệm của chúng tôi, chúng tôi đầu tiên tinh chỉnh mô hình Transformer theo cách tương tự như ensemble lớp tuyến tính sử dụng một tập con dữ liệu ưu tiên trước khi huấn luyện mô hình ensemble (Dòng 14 trong Thuật toán 1). Sau đó chúng tôi sử dụng phần còn lại của dữ liệu cho huấn luyện ensemble. Chúng tôi cung cấp chi tiết thêm về phân chia tập dữ liệu trong Mục 4.

3.2 Dự đoán của Các Mô hình Thưởng Ensemble

Bây giờ chúng ta cần ensemble các dự đoán của các mô hình thưởng ensemble khác nhau. Chúng tôi khám phá hai thuật toán để ensemble các dự đoán này, sử dụng giá trị trung bình dự đoán và giới hạn tin cậy thấp hơn của các giá trị dự đoán, tương ứng.

Cho R là tập hợp các dự đoán mô hình ensemble. R={r1, r2, . . . , rk}. Dự đoán giá trị trung bình đơn giản sử dụng mean(R), giá trị trung bình của dự đoán mô hình thưởng ensemble. Điều này về bản chất là một ước tính phương sai thấp hơn của thưởng. Mặt khác, giới hạn tin cậy thấp hơn (LCB) là một ước tính bảo thủ của thưởng. Nó xem xét độ lệch chuẩn của các dự đoán mô hình ensemble, được định nghĩa là LCB(R) = mean(R) − β · std(R), trong đó β là một siêu tham số. Tuy nhiên, chúng tôi thấy thực nghiệm rằng hiệu suất của LCB có thể so sánh với dự đoán giá trị trung bình.

4 Đánh giá Thực nghiệm

Trong phần này, chúng tôi trả lời thực nghiệm các câu hỏi sau. Q1: So với việc sử dụng một mô hình thưởng đơn lẻ, liệu các mô hình thưởng ensemble có giúp cải thiện hiệu suất điều chỉnh trong RLHF không? Q2: Kiến trúc ensemble nào trong Mục 3.1 có hiệu suất tốt nhất? Q3: Thuật toán dự đoán nào trong Mục 3.2 có hiệu suất tốt hơn?

Thuật toán. Chúng tôi xem xét việc sử dụng các mô hình thưởng ensemble với Best-of-n và Tối ưu hóa Chính sách Gần (PPO) (Schulman et al., 2017), đây là các phương pháp tiêu chuẩn trong RLHF (Ouyang et al., 2022; Dubois et al., 2023; Coste et al., 2023). Cụ thể, Best-of-n tạo ra n mẫu từ mô hình SFT cho mỗi đầu vào và chọn mẫu có dự đoán thưởng cao nhất. Tối ưu hóa Chính sách Gần (PPO) là một thuật toán học tăng cường tinh chỉnh mô hình SFT sử dụng mô hình thưởng.

Đối với mỗi phương pháp ensemble thưởng, chúng tôi tiến hành các thí nghiệm nhiều lần sử dụng các seed ngẫu nhiên khác nhau. Cụ thể, chúng tôi lặp lại các thí nghiệm 10 lần cho Best-of-n và 5 lần cho PPO. Đối với tất cả các thí nghiệm trừ các thí nghiệm không có ensemble thưởng, chúng tôi sử dụng ba mô hình thưởng để ensemble (k = 3).

Mô hình. Chúng tôi sử dụng các mô hình đã được tiền huấn luyện được cung cấp trong AlpacaFarm (Dubois et al., 2023). Cụ thể, chúng tôi sử dụng SFT10k làm mô hình cơ sở để tạo, đây là một mô hình Llama-7b (Touvron et al., 2023) được tinh chỉnh trên tập dữ liệu alpaca_instructions. Vì vậy mô hình có thể tuân theo hướng dẫn, trong khi nó chưa được điều chỉnh với ưu tiên của con người. Để nhất quán với Dubois et al. (2023), mô hình Transformer trong mô hình thưởng của chúng tôi cũng được khởi tạo bằng SFT10k, chưa được huấn luyện cho dự đoán thưởng.

Tập dữ liệu. Chúng tôi sử dụng các tập dữ liệu AlpacaFarm (Dubois et al., 2023) để huấn luyện và đánh giá, cung cấp các tiện ích để đánh giá hiệu suất điều chỉnh của các đầu ra mô hình bằng cách sử dụng API GPT-4 và có thể dễ dàng so sánh các mô hình của chúng tôi với các mô hình và thuật toán chuẩn khác. Chúng tôi huấn luyện tất cả các mô hình thưởng sử dụng cả tập dữ liệu alpaca_noisy_multi_preference và alpaca_human_preference, và sử dụng alpaca_farm_evaluation để đánh giá.

Cụ thể, đối với ensemble dựa trên LoRA, chúng tôi sử dụng hai tập dữ liệu huấn luyện cho các giai đoạn khác nhau: Đầu tiên chúng tôi sử dụng alpaca_noisy_multi_preference để tinh chỉnh hoàn toàn mô hình Transformer với k lớp tuyến tính, theo cách tương tự như ensemble lớp tuyến tính (Dòng 14 trong Thuật toán 1), vì chúng tôi thấy việc tinh chỉnh hoàn toàn mô hình Transformer là cần thiết để nó đưa ra dự đoán thưởng. Sau đó chúng tôi sử dụng alpaca_human_preference để huấn luyện các lớp tuyến tính và các bộ điều hợp cho các thành viên ensemble (Dòng 15-19 trong Thuật toán 1).

Kết quả. Kết quả chính của chúng tôi được báo cáo trong Hình 2. Đối với Best-of-n, chúng tôi chọn n = 50, 100, 200. Đối với PPO, chúng tôi đánh giá các checkpoint ở mỗi 100 bước huấn luyện. Để đánh giá hiệu suất điều chỉnh, chúng tôi sử dụng chỉ số tỷ lệ thắng được cung cấp trong AlpacaEval (được hiển thị là trục dọc), đo lường khả năng các đầu ra của phương pháp chúng tôi được GPT-4 ưa thích so với một baseline được tạo bởi GPT-3. Tất cả các phương pháp ensemble sử dụng dự đoán giá trị trung bình, sử dụng trung bình của các thưởng dự đoán của các thành viên ensemble làm dự đoán cuối cùng của chúng.

Nhìn chung, chúng tôi thấy rằng tỷ lệ thắng cao hơn một cách nhất quán khi sử dụng ensemble phương pháp thưởng (trả lời Q1). Đối với Best-of-n, cả ensemble các mô hình thưởng đơn lẻ và ensemble dựa trên LoRA đều có hiệu suất tốt nhất. Đối với PPO, chúng tôi không thể chạy ensemble các mô hình thưởng đơn lẻ vì nó yêu cầu tải nhiều mô hình thưởng trong quá trình huấn luyện PPO. Tuy nhiên, chúng tôi thấy ensemble dựa trên LoRA có hiệu suất tốt nhất.

Chúng tôi cũng tiến hành các thí nghiệm trên MT-Bench (Zheng et al., 2023), đây là một điểm chuẩn cho các câu hỏi đa lượt. Chúng tôi đánh giá các mô hình được huấn luyện bằng PPO của chúng tôi với số bước huấn luyện nhiều nhất, và báo cáo điểm số điều chỉnh sau lượt đầu tiên, sau lượt thứ hai, và trung bình của cả hai. Kết quả được báo cáo trong Bảng 1, và các phát hiện của chúng tôi nhất quán với kết quả AlpacaEval. Kết quả trên cả hai điểm chuẩn cho thấy rằng, mặc dù LoRA không tinh chỉnh hoàn toàn các mô hình Transformer, nó hiệu quả cho ensemble mô hình thưởng và có thể cải thiện hiệu suất điều chỉnh (trả lời Q2).

Về các phương pháp dự đoán, chúng tôi thấy rằng dự đoán thưởng trung bình và LCB có hiệu suất tương tự. Kết quả chi tiết được trình bày trong Mục B (trả lời Q3).

5 Thảo luận và Kết luận

Tóm lại, bài báo của chúng tôi trình bày một phương pháp mới để tăng cường sự điều chỉnh của các mô hình ngôn ngữ lớn thông qua ensemble mô hình thưởng hiệu quả trong RLHF. Cụ thể, phương pháp ensemble dựa trên LoRA chứng minh hiệu quả trong các ràng buộc tính toán. Trong công việc tương lai, chúng tôi sẽ mở rộng phương pháp này sang các bước khác của huấn luyện và suy luận LLM, chẳng hạn như huấn luyện hiệu quả mẫu của các mô hình thưởng (Gleave và Irving, 2022).
