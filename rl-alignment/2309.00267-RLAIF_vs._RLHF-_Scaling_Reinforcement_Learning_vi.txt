# 2309.00267.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2309.00267.pdf
# Kích thước file: 2531894 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
RLAIF so với RLHF: Mở rộng Học Tăng cường
từ Phản hồi Con người với Phản hồi AI
Harrison Lee1Samrat Phatale1Hassan Mansoor1Thomas Mesnard1Johan Ferret1Kellie Lu2
Colton Bishop1Ethan Hall2Victor Carbune1Abhinav Rastogi1Sushant Prakash2

Tóm tắt
Học tăng cường từ phản hồi con người
(RLHF) đã được chứng minh hiệu quả trong việc căn chỉnh các mô hình ngôn ngữ lớn (LLM) với sở thích của con người, nhưng việc thu thập các nhãn sở thích chất lượng cao rất tốn kém. RL từ Phản hồi AI (RLAIF), được giới thiệu trong Bai et al. (2022b), cung cấp một giải pháp thay thế đầy hứa hẹn để huấn luyện mô hình phần thưởng (RM) dựa trên các sở thích được tạo ra bởi một LLM có sẵn. Trên các nhiệm vụ tóm tắt, tạo ra đối thoại hữu ích và tạo ra đối thoại vô hại, chúng tôi cho thấy RLAIF đạt được hiệu suất tương đương với RLHF. Hơn nữa, chúng tôi tiến một bước về phía "tự cải thiện" bằng cách chứng minh rằng RLAIF có thể vượt trội hơn một baseline được fine-tune có giám sát ngay cả khi AI labeler có cùng kích thước với policy, hoặc thậm chí là checkpoint chính xác giống với policy ban đầu. Cuối cùng, chúng tôi giới thiệu direct-RLAIF (d-RLAIF) - một kỹ thuật bỏ qua việc huấn luyện RM bằng cách lấy phần thưởng trực tiếp từ một LLM có sẵn trong quá trình RL, đạt được hiệu suất vượt trội so với RLAIF tiêu chuẩn. Kết quả của chúng tôi cho thấy RLAIF có thể đạt được hiệu suất ngang bằng với việc sử dụng phản hồi con người, cung cấp một giải pháp tiềm năng cho các hạn chế về khả năng mở rộng của RLHF.

1. Giới thiệu
Học Tăng cường từ Phản hồi Con người (RLHF) là một kỹ thuật hiệu quả để căn chỉnh các mô hình ngôn ngữ với sở thích của con người (Stiennon et al., 2020; Ouyang et al., 2022). Nó được coi là một trong những động lực thành công chính trong các mô hình ngôn ngữ đối thoại hiện đại, như ChatGPT (Liu et al., 2023) và Bard (Manyika, 2023). Một lợi thế chính của

1Google DeepMind2Google. Liên hệ: Harrison Lee <harrisonlee@google.com>, Samrat Phatale <samratph@google.com>, Hassan Mansoor <hassan@google.com>.

Tuyển tập của Hội nghị Quốc tế lần thứ 41 về Học Máy, Vienna, Áo. PMLR 235, 2024. Bản quyền 2024 thuộc về (các) tác giả.

71%
63%73%
64%Tỷ lệ Thắng so với SFT
50%60%70%80%
Tóm tắt Tính hữu íchRLAIF RLHFTỶ lệ Thắng của RLAIF và RLHF

64%76%88%Tỷ lệ Vô hại
50%60%70%80%90%
SFT RLHF RLAIFTỷ lệ Vô hại theo Policy

Hình 1: Các đánh giá viên con người ưa thích RLAIF và RLHF hơn baseline SFT cho tóm tắt và tạo đối thoại hữu ích. Hơn nữa, khi so sánh trực tiếp, RLAIF được ưa thích ngang bằng với RLHF. Đối với tạo đối thoại vô hại, RLAIF vượt trội hơn RLHF.

việc huấn luyện mô hình ngôn ngữ với học tăng cường (RL) là nó cho phép tối ưu hóa trên các mục tiêu phức tạp ở cấp độ chuỗi mà không dễ dàng khả vi và do đó không phù hợp cho fine-tuning có giám sát truyền thống (SFT).

Một trở ngại để sử dụng RLHF ở quy mô lớn là sự phụ thuộc vào các nhãn sở thích con người chất lượng cao. Các mô hình ngôn ngữ lớn hiện đại (LLM) đã cho thấy mức độ cao trong việc căn chỉnh với phán đoán của con người (Gilardi et al., 2023; Ding et al., 2023), cho thấy rằng các nhãn sở thích được tạo ra bởi LLM có thể là một sự thay thế khả thi cho các nhãn con người. Bai et al. (2022b) là nỗ lực đầu tiên khám phá Học Tăng cường từ Phản hồi AI (RLAIF), nơi RL được tiến hành

1arXiv:2309.00267v3 [cs.CL] 3 Sep 2024

--- TRANG 2 ---
RLAIF so với RLHF

Con ngườiLLM
có sẵn
RLAIF policy
RM
Huấn luyện Học Tăng cường  
Đánh giá
RLHF policyr2 - - - - - -
- - - - - - - - - -r1 - - - - - -
- - - - - - - - - -
s1 - - - - - -
- - - - - - - - - -Đánh giá Học Tăng cường
r2 - - - - - -
- - - - - - - - - -r1 - - - - - -
- - - - - - - - - -RL từ Phản hồi AI
s2 - - - - - -
- - - - - - - -
- - - - - - - -s1 - - - - - -
- - - - - - - -
- - - - - - - -Mô hình
SFT
Mẫu
Phản hồi
RM
Huấn luyện RM từ Phản hồi
AI
RM từ
Phản hồi
Con người
RL từ Phản hồi Con người

Hình 2: Sơ đồ mô tả RLAIF (trên) so với RLHF (dưới)

sử dụng một mô hình phần thưởng được huấn luyện trên sự kết hợp của sở thích con người và AI. Kết hợp với kỹ thuật tự sửa đổi "Constitutional AI" của họ, policy cuối cùng của họ vượt trội hơn fine-tuning có giám sát để huấn luyện một trợ lý đối thoại. Tuy nhiên, nó không trực tiếp so sánh hiệu quả của phản hồi con người so với AI, để lại câu hỏi liệu RLAIF có thể là một giải pháp thay thế phù hợp cho RLHF hay không.

Trong công trình này, chúng tôi so sánh hiệu quả của RLAIF và RLHF (xem Hình 2) trên ba nhiệm vụ: tóm tắt, tạo đối thoại hữu ích và tạo đối thoại vô hại. Các thí nghiệm của chúng tôi cho thấy RLAIF và RLHF được con người ưa thích hơn baseline SFT lần lượt 71% và 73% thời gian cho tóm tắt và 63% và 64% thời gian cho tạo đối thoại hữu ích, nơi tỷ lệ thắng cho RLAIF và RLHF không khác biệt đáng kể về mặt thống kê. Hơn nữa, trong so sánh trực tiếp giữa RLAIF với RLHF, cả hai policy đều được ưa thích ngang bằng¹.

Đối với tạo đối thoại vô hại, các đánh giá viên con người đánh giá độ vô hại của mỗi phản hồi một cách độc lập. RLAIF đạt được tỷ lệ vô hại cao hơn RLHF, và cả hai đều vượt trội hơn baseline SFT (lần lượt 88%, 76% và 64%). Những kết quả này cho thấy RLAIF là một giải pháp thay thế khả thi cho RLHF mà không phụ thuộc vào chú thích con người, đồng thời cung cấp các thuộc tính mở rộng hấp dẫn.

Ngoài ra, chúng tôi tiến hành hai nghiên cứu liên quan. Thứ nhất, trong một bước hướng tới tự cải thiện LLM, chúng tôi chứng minh rằng RLAIF cải thiện đáng kể so với baseline SFT ngay cả khi AI labeler có cùng kích thước với mô hình policy. Thứ hai, chúng tôi giới thiệu direct-RLAIF (d-RLAIF) - một kỹ thuật bỏ qua việc huấn luyện mô hình phần thưởng bằng cách lấy phần thưởng trực tiếp từ một LLM có sẵn trong quá trình RL. Trong các thí nghiệm của chúng tôi, d-RLAIF phù hợp hoặc vượt trội hơn RLAIF tiêu chuẩn. Hơn nữa, đối với nhiệm vụ tạo đối thoại hữu ích, policy ban đầu và LLM cung cấp phần thưởng

¹Tỷ lệ thắng của một policy so với policy khác không khác biệt đáng kể về mặt thống kê so với 50%

là cùng một checkpoint mô hình, chứng minh một trường hợp tự cải thiện LLM nghiêm ngặt.

Cuối cùng, chúng tôi nghiên cứu các kỹ thuật để tối đa hóa sự căn chỉnh của các sở thích được tạo ra bởi AI với sở thích con người. Chúng tôi thấy rằng việc kích thích lý luận chuỗi suy nghĩ (Wei et al., 2022) liên tục cải thiện sự căn chỉnh, trong khi việc sử dụng lời mở đầu chi tiết và prompting few-shot (Brown et al., 2020) chỉ có lợi cho một số nhiệm vụ nhất định. Chúng tôi tiến hành các thí nghiệm mở rộng để kiểm tra sự đánh đổi giữa kích thước của LLM labeler và sự căn chỉnh với sở thích con người.

Các đóng góp chính của công trình này như sau:

1. Chúng tôi chứng minh rằng RLAIF đạt được hiệu suất tương đương với RLHF cho các nhiệm vụ tóm tắt, tạo đối thoại hữu ích và tạo đối thoại vô hại.

2. Chúng tôi cho thấy RLAIF có thể cải thiện so với một policy SFT khi LLM labeler có cùng kích thước với policy, hoặc thậm chí là checkpoint chính xác giống với policy.

3. Chúng tôi giới thiệu direct RLAIF (d-RLAIF), lấy phần thưởng trực tiếp từ một LLM có sẵn trong quá trình RL và phù hợp hoặc vượt trội hơn RLAIF tiêu chuẩn.

4. Chúng tôi nghiên cứu các kỹ thuật để tối đa hóa sự căn chỉnh của các sở thích được tạo ra bởi AI với sở thích con người.

2. Phương pháp luận

Phần này mô tả các kỹ thuật được sử dụng để tạo ra sở thích với một LLM, các thiết lập học tăng cường và các chỉ số đánh giá. Những kiến thức cơ bản về RLHF được cung cấp trong Phụ lục A.

2.1. Gán nhãn Sở thích với LLM

Chúng tôi chú thích sở thích với một LLM "có sẵn" - một mô hình được pre-train hoặc instruction-tuned (Wei et al., 2021) cho việc sử dụng chung nhưng không được fine-tune cho một nhiệm vụ downstream cụ thể

2

--- TRANG 3 ---
RLAIF so với RLHF

LLM 
Chấm điểm Phản hồi 
Tóm tắt 1 tốt hơn vì nó 
có tính mạch lạc hơn và … 

Kết thúc COT 
Xem xét tính mạch lạc, độ chính xác, 
độ bao phủ và chất lượng tổng thể của từng 
tóm tắt và giải thích cái nào tốt hơn. Lý luận: Mẫu để Chú thích 
Văn bản - {text} 
Tóm tắt 1 - {summary1} 
Tóm tắt 2 - {summary2} Lời mở đầu 
Một tóm tắt tốt là một đoạn văn bản ngắn hơn 
có bản chất của bản gốc. ... 

LLM 
Tạo ra 

Kết thúc 
Tóm tắt Ưu tiên= Sở thích AI 
softmax(log P("1")) = 0.6 
softmax(log P("2")) = 0.4 

Hình 3: Minh họa quá trình để có được các nhãn sở thích được tạo ra bởi AI cho tóm tắt. LLM đầu tiên được nhắc nhở để giải thích suy nghĩ của nó về chất lượng của hai ứng viên (xanh). Phản hồi (cam) sau đó được thêm vào prompt đầu tiên, và cùng nhau chúng tạo thành prompt thứ hai được sử dụng để tạo ra một phân phối sở thích trên "1" so với "2" (xanh lá).

task. Cho một đoạn văn bản và hai phản hồi ứng viên, LLM được yêu cầu đánh giá phản hồi nào được ưa thích. Prompt được cấu trúc như sau (ví dụ trong Bảng 15 và 21):

1. Lời mở đầu - Giới thiệu và hướng dẫn mô tả nhiệm vụ hiện tại
2. Các ví dụ few-shot (tùy chọn) - Một ví dụ về ngữ cảnh đầu vào, một cặp phản hồi, một lý luận chuỗi suy nghĩ (tùy chọn), và một nhãn sở thích
3. Mẫu để chú thích - Một ngữ cảnh đầu vào và một cặp phản hồi để được gán nhãn
4. Kết thúc - Văn bản kết thúc để nhắc nhở LLM (ví dụ "Phản hồi Ưu tiên=")

Sau khi prompt được đưa cho LLM, chúng tôi trích xuất các log-xác suất của việc tạo ra các token "1" và "2" và tính toán softmax để có được một phân phối sở thích.

Có nhiều giải pháp thay thế để có được các nhãn sở thích từ LLM, chẳng hạn như trích xuất sở thích từ một phản hồi được tạo ra tự do (ví dụ "Phản hồi đầu tiên tốt hơn"), hoặc biểu diễn phân phối sở thích như một mã hóa one-hot. Tuy nhiên, chúng tôi chọn phương pháp của mình vì nó đơn giản để thực hiện và truyền đạt nhiều thông tin hơn so với mã hóa one-hot thông qua biểu diễn phân phối của các sở thích.

Chúng tôi thí nghiệm với hai kiểu lời mở đầu: "Base", về cơ bản hỏi phản hồi nào tốt hơn, và "Detailed", giống như các hướng dẫn đánh giá chi tiết thường được đưa cho các chú thích viên con người (xem Bảng 16 cho các lời mở đầu được sử dụng trong nhiệm vụ tóm tắt). Chúng tôi cũng thí nghiệm với học trong ngữ cảnh (Brown et al., 2020), sử dụng các ví dụ chất lượng cao được chọn thủ công để bao phủ một loạt các chủ đề.

2.1.1. GIẢI QUYẾT THIÊN VỊ VỊ TRÍ

Thứ tự mà các ứng viên được hiển thị cho một LLM có thể thiên vị ứng viên nào nó ưa thích (Pezeshkpour & Hruschka, 2023; Wang et al., 2023). Chúng tôi tìm thấy bằng chứng về thiên vị vị trí, đặc biệt phổ biến trong các LLM labeler nhỏ hơn (xem Phụ lục B).

Để giảm thiểu tác động của thiên vị vị trí, hai suy luận được thực hiện cho mỗi cặp ứng viên, nơi thứ tự mà các ứng viên được trình bày cho LLM được đảo ngược cho suy luận thứ hai. Kết quả từ cả hai suy luận sau đó được tính trung bình để có được phân phối sở thích cuối cùng.

2.1.2. KÍCH THÍCH LÝ LUẬN CHUỖI SUY NGHĨ

Chúng tôi thí nghiệm với việc kích thích lý luận chuỗi suy nghĩ (CoT) (Wei et al., 2022) từ các AI labeler của chúng tôi thông qua một quy trình suy luận hai bước. Đầu tiên, chúng tôi thay thế Kết thúc của prompt tiêu chuẩn bằng một câu yêu cầu suy nghĩ và giải thích (ví dụ "Xem xét tính mạch lạc, độ chính xác, độ bao phủ và chất lượng tổng thể của từng tóm tắt và giải thích cái nào tốt hơn. Lý luận:") và giải mã một phản hồi từ LLM. Sau đó, chúng tôi nối prompt gốc, phản hồi và chuỗi Kết thúc tiêu chuẩn lại với nhau, và theo quy trình chấm điểm trong Phần 2.1 để có được một phân phối sở thích. Hình 3 minh họa quá trình này.

Đối với các prompt zero-shot, LLM không được đưa ra ví dụ về lý luận nên trông như thế nào. Trong các prompt few-shot, chúng tôi cung cấp các ví dụ về lý luận CoT để mô hình theo dõi. Xem Bảng 17 và 18 để biết ví dụ.

3

--- TRANG 4 ---
RLAIF so với RLHF

s - - - - - - -
- - - - - - - -
- - - - - - - -Mô hình
SFT
LLM
Mục đích
chungPrompt thành Phần thưởng
1-10
Mô hình RL
Học Tăng cường Phần thưởng

Hình 4: Trong direct-RLAIF (d-RLAIF), LLM có sẵn được sử dụng trực tiếp để cung cấp phần thưởng trong quá trình RL, bỏ qua vấn đề "cũ kỹ" của RM và quá trình tốn thời gian của việc huấn luyện RM.

2.2. Học Tăng cường từ Phản hồi AI

2.2.1. RLAIF TIÊU CHUẨN

Chúng tôi mô tả việc điều chỉnh thiết lập RLAIF tiêu chuẩn của chúng tôi dưới đây. Trừ khi được đề cập khác, RLAIF được thực hiện bằng phương pháp này.

Một mô hình phần thưởng (RM) được huấn luyện trên các nhãn sở thích được tạo ra bởi LLM theo phương pháp luận trong Phụ lục A.2. Vì phương pháp của chúng tôi tạo ra các nhãn mềm (ví dụ [0.6,0.4]), chúng tôi huấn luyện RM với một cross-entropy loss trên softmax của các điểm số được tạo ra bởi RM. Softmax chuyển đổi các điểm số RM thành một phân phối xác suất. Chúng tôi lưu ý rằng việc huấn luyện một RM trên một tập dữ liệu các nhãn AI có thể được xem như một hình thức chưng cất mô hình.

Cuối cùng, chúng tôi tiến hành học tăng cường để huấn luyện mô hình policy RLAIF, sử dụng RM để gán phần thưởng cho các phản hồi của mô hình, như được mô tả trong Phụ lục A.3.

2.2.2. DIRECT-RLAIF (D-RLAIF)

Một vấn đề với RLAIF là mô hình phần thưởng có thể trở nên "cũ kỹ" khi policy được huấn luyện. Trong thiết lập điển hình, RM được huấn luyện trên các thế hệ được lấy mẫu từ policy ban đầu. Khi policy được huấn luyện, các quỹ đạo được tạo ra ngày càng trở nên out-of-distribution từ tập dữ liệu mà RM được huấn luyện, dẫn đến hiệu suất dưới tối ưu (Bai et al., 2022a). Một giải pháp là tiến hành RLAIF lặp đi lặp lại, nơi một RM mới được định kỳ huấn luyện trên policy mới nhất, mặc dù đây là một quá trình tốn thời gian.

Chúng tôi giới thiệu direct-RLAIF (d-RLAIF) - một giải pháp thay thế đơn giản cho RLAIF tiêu chuẩn mà trực tiếp sử dụng phản hồi LLM như tín hiệu phần thưởng trong RL. D-RLAIF giải quyết vấn đề cũ kỹ của RM, vì LLM có sẵn trực tiếp chấm điểm các phản hồi được tạo ra trong quá trình RL mà không trải qua huấn luyện. Ngoài ra, d-RLAIF loại bỏ nhu cầu cho quá trình tốn thời gian của việc gán nhãn sở thích AI và huấn luyện RM. Hình 4 mô tả quá trình này.

Trong d-RLAIF, LLM được nhắc nhở để đánh giá chất lượng của một thế hệ từ 1 đến 10. Tương tự như Phần 2.1, prompt hướng dẫn LLM về cách đánh giá một thế hệ. Sau đó, khả năng của mỗi token điểm số từ 1 đến 10 được tính toán, các khả năng được chuẩn hóa thành một phân phối xác suất, một điểm số có trọng số được tính toán như s(y|x) = ∑¹⁰ᵢ₌₁ i P(i|y, x), và cuối cùng điểm số được chuẩn hóa lại thành phạm vi [−1,1]. Các chi tiết bổ sung về kỹ thuật prompting có thể được tìm thấy trong Phụ lục D.

RL sau đó được tiến hành theo cách tương tự như RLAIF tiêu chuẩn, nơi điểm số trực tiếp được sử dụng như phần thưởng thay vì điểm số RM.

2.3. Đánh giá

Chúng tôi đánh giá kết quả của mình với ba chỉ số - Căn chỉnh AI Labeler, Tỷ lệ Thắng và Tỷ lệ Vô hại.

Căn chỉnh AI Labeler đo lường độ chính xác của các sở thích được gán nhãn bởi AI so với sở thích con người. Đối với một ví dụ đơn lẻ, một sở thích được gán nhãn bởi AI mềm đầu tiên được chuyển đổi thành một biểu diễn nhị phân (ví dụ [0.6,0.4]→[1,0]). Sau đó, một điểm số 1 được gán nếu nhãn đồng ý với sở thích con người và 0 nếu không. Độ chính xác căn chỉnh zacc có thể được biểu diễn như sau:

zacc = 1/D ∑ᴰᵢ₌₁ 1[arg max_j P^AI_{i,j} = p^H_i],

trong đó D là kích thước của tập dữ liệu sở thích, P^AI ∈ R^{D×2} là ma trận của các sở thích AI mềm, và p^H ∈ R^D là vector tương ứng của các sở thích con người, chứa các phần tử 0 hoặc 1 để biểu thị liệu phản hồi đầu tiên hay thứ hai được ưa thích, tương ứng.

Tỷ lệ Thắng đánh giá chất lượng end-to-end của hai policy bằng cách đo lường tần suất một policy được các chú thích viên con người ưa thích hơn so với policy khác. Cho một đầu vào và hai thế hệ, các chú thích viên con người chọn thế hệ ưa thích của họ. Tỷ lệ phần trăm các trường hợp mà policy A được ưa thích hơn policy B được gọi là "tỷ lệ thắng của A so với B". Tỷ lệ thắng 50% cho thấy rằng A và B được ưa thích ngang bằng.

Tỷ lệ Vô hại đo lường tỷ lệ phần trăm các phản hồi được coi là vô hại bởi các đánh giá viên con người. Chúng tôi đánh giá nhiệm vụ tạo đối thoại vô hại với chỉ số này thay vì Tỷ lệ Thắng, vì chúng tôi thấy rằng nhiều phản hồi đều an toàn ngang bằng, khiến việc gán xếp hạng tương đối trở nên khó khăn.

3. Chi tiết Thí nghiệm

3.1. Tập dữ liệu

Chúng tôi sử dụng các tập dữ liệu sau cho các thí nghiệm của mình:

• Reddit TL;DR (Stiennon et al., 2020) - các bài đăng từ

4

--- TRANG 5 ---
RLAIF so với RLHF

Reddit² kèm theo các tóm tắt của các bài đăng.

• Sở thích Con người của OpenAI (Stiennon et al., 2020) - một tập dữ liệu được tạo từ một tập con của Reddit TL;DR. Mỗi ví dụ bao gồm một bài đăng, hai tóm tắt ứng viên và một đánh giá từ một chú thích viên con người chỉ ra tóm tắt nào được ưa thích.

• Sở thích Con người Hữu ích và Vô hại của Anthropic (Bai et al., 2022a) - các cuộc đối thoại giữa con người và trợ lý AI, nơi mỗi cuộc đối thoại có hai phản hồi có thể từ trợ lý AI - một được ưa thích và một không được ưa thích, theo một chú thích viên con người. Sở thích dựa trên phản hồi nào có nhiều thông tin và trung thực hơn cho nhiệm vụ hữu ích, và phản hồi nào an toàn hơn cho nhiệm vụ vô hại.

Thêm chi tiết về tập dữ liệu có thể được tìm thấy trong Phụ lục C.

Chúng tôi cũng khám phá tập dữ liệu Stanford Human Preferences (Ethayarajh et al., 2022), nhưng chúng tôi thấy rằng cả policy RLHF và RLAIF đều không cho thấy cải thiện có ý nghĩa so với baseline SFT sau khi điều chỉnh cho thiên vị độ dài như được mô tả trong Phụ lục J.

3.2. Gán nhãn LLM

Để cho phép lặp lại thí nghiệm nhanh khi đánh giá các kỹ thuật gán nhãn AI, chúng tôi ngẫu nhiên downsampled phần huấn luyện của mỗi tập dữ liệu sở thích. Đối với tóm tắt, một bộ lọc bổ sung được áp dụng để chỉ bao gồm các ví dụ nơi các chú thích viên con người ưa thích một tóm tắt hơn tóm tắt khác với độ tin cậy cao³. Sau downsampling và lọc, còn lại 3-4k ví dụ cho mỗi nhiệm vụ⁴. Căn chỉnh AI labeler được tính toán trên các tập dữ liệu downsampled này.

Chúng tôi sử dụng họ mô hình PaLM 2 (Google et al., 2023) để gán nhãn sở thích. Tất cả các phiên bản đều được instruction-tuned nhưng chưa được huấn luyện trước đó với RL. Trừ khi được chỉ định khác, các nhãn AI được tạo ra bằng PaLM 2 Large (L) với prompt hiệu suất tốt nhất cho mỗi nhiệm vụ trong Phần 4.4. Để biết thêm chi tiết về gán nhãn LLM, xem Phụ lục D.

3.3. Huấn luyện Mô hình

Tất cả các mô hình SFT được khởi tạo từ PaLM 2 Extra-Small (XS). Đối với tóm tắt, mô hình SFT được tạo ra bằng cách fine-tuning PaLM 2 XS trên tập dữ liệu Reddit TL;DR. Đối với tất cả các nhiệm vụ khác, một biến thể instruction-tuned của PaLM 2 được sử dụng thay cho fine-tuning cụ thể cho nhiệm vụ.

Tất cả RM được huấn luyện từ các checkpoint PaLM 2 XS. Mỗi

²www.reddit.com
³Điều này theo quy trình đánh giá trong Stiennon et al. (2020). Các ví dụ với điểm tin cậy 1, 2, 8 và 9 được coi là "tin cậy cao"
⁴Chúng tôi lấy mẫu 15%, 10% và 10% của các phần huấn luyện cho tóm tắt, tạo đối thoại hữu ích và tạo đối thoại vô hại, tương ứng.

RM được fine-tuned trên toàn bộ phần huấn luyện của tập dữ liệu sở thích, nơi nhãn là sở thích AI cho các RM phản hồi AI và nhãn sở thích con người gốc cho các RM phản hồi con người. Độ chính xác RM có thể được tìm thấy trong Phụ lục G.

Trong giai đoạn RL, policy được huấn luyện với một phiên bản sửa đổi của REINFORCE (Williams, 1992) được điều chỉnh cho lĩnh vực mô hình hóa ngôn ngữ (xem Phụ lục E). Trong khi nhiều công trình gần đây sử dụng Proximal Policy Optimization (PPO) (Schulman et al., 2017), chúng tôi sử dụng REINFORCE với một baseline vì nó đơn giản hơn nhưng vẫn hiệu quả cho vấn đề hiện tại.

Cả mô hình policy và value đều được khởi tạo từ mô hình SFT. Đối với tóm tắt, policy được triển khai trên phần huấn luyện của tập dữ liệu Reddit TL;DR. Nói cách khác, trạng thái ban đầu cho mỗi quỹ đạo là bài đăng Reddit gốc. Đối với các nhiệm vụ hữu ích và vô hại, các trạng thái ban đầu được rút ra từ các phần huấn luyện của các tập dữ liệu sở thích.

Đối với tóm tắt, post-processing đơn giản được áp dụng cho các phản hồi được tạo ra bởi các policy được huấn luyện RL như được mô tả trong Phụ lục H.

Để biết thêm chi tiết về huấn luyện mô hình, xem Phụ lục F.

3.4. Đánh giá Con người

Để đo lường tỷ lệ thắng, các đánh giá viên được trình bày với một ngữ cảnh đầu vào và nhiều phản hồi được tạo ra từ các policy khác nhau (ví dụ RLAIF, RLHF và SFT). Họ sau đó được yêu cầu xếp hạng các phản hồi theo thứ tự chất lượng mà không có hòa, như được thấy trong Hình 5. Các ngữ cảnh đầu vào được rút ra từ các phần test của mỗi tập dữ liệu, không được sử dụng cho huấn luyện hoặc bất kỳ đánh giá nào khác⁵. Các xếp hạng sau đó được sử dụng để tính toán tỷ lệ thắng cho các cặp policy. Đối với tạo đối thoại vô hại, các đánh giá viên được yêu cầu đánh giá độc lập mỗi phản hồi là vô hại hoặc có hại.

Để biết thêm chi tiết về đánh giá con người, xem Phụ lục I.

4. Kết quả

4.1. RLAIF so với RLHF

RLAIF đạt được những cải thiện hiệu suất ngang bằng hoặc tốt hơn RLHF trên cả ba nhiệm vụ (xem Hình 1 và Bảng 1). Cụ thể, RLAIF và RLHF được các đánh giá viên con người ưa thích hơn policy baseline SFT lần lượt 71% và 73% thời gian cho tóm tắt⁶ và 63% và 64% cho tạo đối thoại hữu ích. Sự khác biệt trong tỷ lệ thắng giữa RLAIF so với SFT và RLHF so với SFT không

⁵Đối với tóm tắt, chúng tôi sử dụng phần test của Reddit TL;DR. Đối với tạo đối thoại hữu ích và vô hại, chúng tôi sử dụng các phần test từ các tập dữ liệu sở thích, được chi tiết trong Phụ lục C.
⁶RLAIF và RLHF cũng được ưa thích hơn các tóm tắt tham chiếu con người trong Reddit TL;DR lần lượt 79% và 80% thời gian.

5

--- TRANG 6 ---
RLAIF so với RLHF

Bảng 1: Bên trái: Tỷ lệ thắng cho các cặp policy trên nhiệm vụ tóm tắt và nhiệm vụ đối thoại hữu ích. Bên phải: Tỷ lệ vô hại trên các policy cho nhiệm vụ đối thoại vô hại. Tất cả các số đều dựa trên đánh giá con người.

Tỷ lệ Thắng                     Tỷ lệ Vô hại
So sánh      Tóm tắt  Đối thoại  Mô hình    Đối thoại
             hữu ích            vô hại
RLAIF vs SFT    71%     63%     SFT        64%
RLHF vs SFT     73%     64%     RLHF       76%
RLAIF vs RLHF   50%     52%     RLAIF      88%
Same-size RLAIF vs SFT  68%    –
d-RLAIF vs SFT  74%     66%
d-RLAIF vs Same-size RLAIF  60%    –

khác biệt đáng kể về mặt thống kê. Khi so sánh trực tiếp RLAIF với RLHF, chúng được ưa thích ngang bằng - tức là tỷ lệ thắng không khác biệt đáng kể về mặt thống kê so với 50%. Đối với tạo đối thoại vô hại, RLAIF đạt được tỷ lệ vô hại 88%, vượt trội hơn cả RLHF và SFT, lần lượt đạt 76% và 64%⁷.

Hình 6 chứa một ví dụ về các tóm tắt SFT, RLAIF và RLHF. Để hiểu rõ hơn RLAIF so sánh với RLHF như thế nào, chúng tôi so sánh định tính các phản hồi được tạo ra bởi cả hai policy cho tóm tắt trong Phần 5.

Tương tự như Stiennon et al. (2020), chúng tôi quan sát thấy rằng các policy RLAIF và RLHF có xu hướng tạo ra các phản hồi dài hơn so với policy SFT, có thể thiên vị đánh giá con người. Chúng tôi tiến hành phân tích post-hoc để kiểm soát độ dài và thấy rằng cả policy RLAIF và RLHF vẫn vượt trội hơn policy SFT. Xem Phụ lục J để biết chi tiết.

Một câu hỏi tự nhiên nảy sinh là liệu có giá trị trong việc kết hợp phản hồi con người và AI hay không. Chúng tôi thí nghiệm với việc kết hợp cả hai loại phản hồi nhưng không thấy cải thiện nào vượt quá việc chỉ sử dụng phản hồi con người. Tuy nhiên, chúng tôi tin rằng có một số thiết lập huấn luyện thay thế có thể chứng minh giá trị trong việc kết hợp cả hai hình thức phản hồi. Xem Phụ lục K để biết chi tiết.

Những kết quả này cho thấy RLAIF là một giải pháp thay thế khả thi cho RLHF mà không phụ thuộc vào chú thích con người. Ngoài việc tăng tốc thời gian thu thập nhãn, một lợi ích khác của gán nhãn AI là giảm chi phí. Chúng tôi ước tính chi phí gán nhãn LLM rẻ hơn hơn 10 lần so với chú thích con người. Xem Phụ lục L để biết phân tích chi tiết.

4.2. Hướng tới Tự cải thiện

Trong Phần 4.1, LLM được sử dụng để gán nhãn sở thích (PaLM 2 L) lớn hơn nhiều so với policy được huấn luyện (PaLM 2 XS). Đi một bước xa hơn, chúng tôi khám phá liệu RLAIF có thể mang lại cải thiện khi AI labeler có cùng kích thước với policy hay không. Trên nhiệm vụ tóm tắt, chúng tôi tiến hành RLAIF nơi PaLM 2 XS được sử dụng làm AI labeler thay vì PaLM 2 L. Phần còn lại của thiết lập bắt chước thí nghiệm trong Phần 4.1. Chúng tôi gọi thiết lập này là "same-size RLAIF".

Same-size RLAIF vẫn cải thiện rất nhiều so với baseline SFT, với các chú thích viên con người ưa thích same-size RLAIF 68% thời gian so với SFT (xem Bảng 1). Để tham khảo, RLAIF sử dụng AI labeler lớn hơn policy được ưa thích 71% so với SFT⁸. Kết quả này chứng minh rằng RLAIF có thể mang lại cải thiện ngay cả khi AI labeler có cùng kích thước với policy LLM.

Chúng tôi lưu ý rằng thí nghiệm này không phải là một ví dụ nghiêm ngặt về "tự cải thiện" (Huang et al., 2022), vì AI labeler là PaLM 2 XS được instruction-tuned, trong khi policy ban đầu là PaLM 2 XS được fine-tuned trên tóm tắt Reddit TL;DR. Tuy nhiên, chúng tôi chứng minh một trường hợp tự cải thiện nghiêm ngặt trên nhiệm vụ helpfulness trong phần tiếp theo.

4.3. D-RLAIF

Trong các Phần 4.1 và 4.2, phản hồi AI được chưng cất vào một RM. Trên các nhiệm vụ tóm tắt và helpfulness, chúng tôi thí nghiệm với d-RLAIF (xem Phần 2.2.2). Chúng tôi sử dụng PaLM 2 XS được instruction-tuned nhỏ hơn làm AI labeler để giảm chi phí tính toán.

Đối với tóm tắt, các chú thích viên con người ưa thích d-RLAIF hơn SFT 74% thời gian (xem Bảng 1). Để hiểu tác động của việc sử dụng trực tiếp phản hồi LLM so với chưng cất phản hồi vào một RM, chúng tôi so sánh kết quả này với policy same-size RLAIF từ Phần 4.2, chỉ khác biệt về hàm phần thưởng. D-RLAIF vượt trội hơn same-size RLAIF, đạt được tỷ lệ thắng thấp hơn đáng kể về mặt thống kê là 68%. Hơn nữa, khi được hiển thị các phản hồi cạnh nhau, các chú thích viên ưa thích d-RLAIF hơn same-size RLAIF

⁷RLAIF đạt được cải thiện đáng kể về mặt thống kê so với RLHF và SFT, theo các kiểm định t-test paired hai phía.
⁸Sự khác biệt giữa tỷ lệ thắng giữa "same-size RLAIF vs. SFT" và "RLAIF vs. SFT" gốc không đáng kể về mặt thống kê. Đối với kiểm định t-test hai mẫu, p-value = 0.07. Tại alpha = 0.05, sự khác biệt này không đáng kể về mặt thống kê.

6

--- TRANG 7 ---
RLAIF so với RLHF

60% thời gian⁹. Chúng tôi giả định rằng cải thiện này là kết quả của việc truy vấn trực tiếp AI labeler cho sở thích thay vì chưng cất các sở thích của nó vào một RM trước, cũng như bỏ qua vấn đề "cũ kỹ" được mô tả trong Phần 2.2.2.

Đối với tạo đối thoại hữu ích, chúng tôi thấy rằng d-RLAIF đạt được tỷ lệ thắng 66% so với baseline SFT. Vì LLM cung cấp phản hồi và policy bắt đầu là cùng một checkpoint mô hình, điều này tạo thành một ví dụ nghiêm ngặt về tự cải thiện LLM.

4.4. Kỹ thuật Prompting

Bảng 2: Chúng tôi quan sát thấy rằng việc kích thích lý luận chuỗi suy nghĩ có xu hướng cải thiện căn chỉnh AI labeler, trong khi few-shot prompting và lời mở đầu chi tiết có tác động hỗn hợp trên các nhiệm vụ. Ở trên, "Help." và "Harm." đề cập đến helpfulness và harmlessness, tương ứng.

Căn chỉnh AI Labeler
Prompt          Tóm tắt  Help.  Harm.
Base 0-shot     76.1%   67.8%  69.4%
Base 1-shot     76.0%   67.1%  71.7%
Base 2-shot     75.7%   66.8%  72.1%
Base + CoT 0-shot 77.5%  69.1%  70.6%
Detailed 0-shot  77.4%   67.6%  70.1%
Detailed 1-shot  76.2%   67.6%  71.5%
Detailed 2-shot  76.3%   67.3%  71.6%
Detailed 8-shot  69.8%   –      –
Detailed + CoT 0-shot 78.0% 67.8% 70.1%
Detailed + CoT 1-shot 77.4% 67.4% 69.9%
Detailed + CoT 2-shot 76.8% 67.4% 69.2%

Chúng tôi thí nghiệm với ba loại biến thể prompting - độ cụ thể của lời mở đầu, lý luận chuỗi suy nghĩ và học trong ngữ cảnh (xem Bảng 2). Các prompt tốt nhất vượt trội hơn các prompt cơ bản ("Base 0-shot") lần lượt +1.9%, +1.3% và +1.7% cho tóm tắt, helpfulness và harmlessness.

Các lời mở đầu chi tiết cải thiện căn chỉnh cho tóm tắt, trong khi mang lại kết quả hỗn hợp cho tạo đối thoại hữu ích và vô hại. Chúng tôi giả định rằng tóm tắt có lợi hơn từ lời mở đầu chi tiết do độ phức tạp lớn hơn của nhiệm vụ. Đánh giá helpfulness và harmlessness dễ nắm bắt hơn, và do đó có thể ít được hưởng lợi từ các hướng dẫn chi tiết.

Lý luận chuỗi suy nghĩ nhìn chung cải thiện căn chỉnh. Đối với tóm tắt, cải thiện là nhất quán. Đối với tạo đối thoại hữu ích và vô hại, CoT chỉ cải thiện căn chỉnh khi được kết hợp với lời mở đầu "Base".

Đáng ngạc nhiên, chúng tôi quan sát thấy rằng học trong ngữ cảnh chỉ cải thiện căn chỉnh cho tạo đối thoại vô hại¹⁰. Đối với tóm tắt và helpfulness, căn chỉnh giảm đơn điệu khi số lượng ví dụ tăng. Để xác minh rằng điều này không phải là kết quả của các ví dụ được chọn kém, chúng tôi tiến hành 10 thử nghiệm cho "Base 1-shot" trên tóm tắt, nơi một ví dụ khác nhau được chọn ngẫu nhiên cho mỗi thử nghiệm. Căn chỉnh AI labeler tối đa từ tất cả các thử nghiệm là 76.1%, vẫn không vượt qua "Base 0-shot" về căn chỉnh AI labeler. Một giả thuyết là các nhiệm vụ tóm tắt và tạo đối thoại hữu ích có thể đã được AI labeler có sẵn hiểu đủ tốt, khiến các ví dụ trở nên không hữu ích hoặc gây phân tâm. Đáng chú ý rằng học trong ngữ cảnh vẫn là một lĩnh vực nghiên cứu quan trọng chưa được hiểu đầy đủ (Min et al., 2022; Wang et al., 2022a).

Đối với tóm tắt, chúng tôi cũng so sánh với thỏa thuận giữa các chú thích viên con người để có cảm giác về hiệu suất của LLM labeler của chúng tôi theo thuật ngữ tuyệt đối. Stiennon et al. (2020) ước tính rằng tỷ lệ thỏa thuận cho tập dữ liệu sở thích con người OpenAI là 73-77%, cho thấy rằng LLM có sẵn đạt được 78% căn chỉnh hoạt động tốt theo thuật ngữ tuyệt đối.

Chúng tôi cũng tiến hành các thí nghiệm với self-consistency (Wang et al., 2022b), nơi nhiều lý luận chuỗi suy nghĩ được lấy mẫu với nhiệt độ T > 0. Các phân phối sở thích được tạo ra bởi LLM được tính trung bình cùng nhau để đạt được nhãn sở thích cuối cùng. Tuy nhiên, chúng tôi thấy rằng self-consistency nghiêm ngặt làm giảm căn chỉnh AI labeler (xem Phụ lục M).

Chúng tôi giả định rằng căn chỉnh AI labeler cao hơn dẫn đến cải thiện trong các policy RLAIF. Để điều này, chúng tôi tiến hành một thí nghiệm về độ nhạy end-to-end đối với căn chỉnh AI labeler. Hai policy RLAIF được huấn luyện chỉ khác biệt trong điểm căn chỉnh của các nhãn AI. Kết quả cho thấy rằng policy được huấn luyện với các nhãn AI căn chỉnh hơn đạt được tỷ lệ thắng cao hơn đáng kể, phù hợp với kỳ vọng của chúng tôi. Tuy nhiên, nghiên cứu này chỉ so sánh hai policy, và thí nghiệm nghiêm ngặt là cần thiết để rút ra kết luận dứt khoát. Xem Phụ lục N để biết chi tiết.

4.5. Kích thước của LLM Labeler

Bảng 3: Căn chỉnh AI labeler tăng khi kích thước của LLM labeler tăng.

Kích thước Mô hình   Căn chỉnh AI Labeler
PaLM 2 L           78.0%
PaLM 2 S           73.8%
PaLM 2 XS          62.7%

⁹Điều này khác biệt đáng kể về mặt thống kê so với 50% theo kiểm định nhị thức.
¹⁰Chúng tôi xác minh rằng tất cả các đầu vào được sử dụng trong các thí nghiệm này phù hợp trong độ dài ngữ cảnh của AI labeler của chúng tôi.

7

--- TRANG 8 ---
RLAIF so với RLHF

Các kích thước mô hình lớn không được truy cập rộng rãi và có thể tốn kém để chạy. Trên nhiệm vụ tóm tắt, chúng tôi thay đổi kích thước LLM để gán nhãn sở thích và quan sát một mối quan hệ tích cực mạnh mẽ giữa kích thước và căn chỉnh (xem Bảng 3). Căn chỉnh giảm 4% khi thay thế PaLM 2 L bằng PaLM 2 S, và giảm thêm 11% khi sử dụng PaLM 2 XS - một xu hướng phù hợp với các hành vi mở rộng được quan sát trong công trình khác (Kaplan et al., 2020). Một yếu tố đóng góp vào xu hướng này, ngoài việc giảm khả năng mô hình chung, có thể là các LLM nhỏ hơn thể hiện thiên vị vị trí lớn hơn (xem Phụ lục B).

Từ một góc độ khác, những kết quả này cũng cho thấy rằng việc mở rộng kích thước AI labeler có thể tạo ra các nhãn sở thích chất lượng thậm chí cao hơn. Vì AI labeler chỉ được sử dụng để tạo ra các ví dụ sở thích một lần và không được gọi trong quá trình RL cho RLAIF tiêu chuẩn, việc sử dụng AI labeler thậm chí lớn hơn không nhất thiết tốn kém quá mức.

5. Quan sát Định tính

Để hiểu sâu hơn về RLAIF so sánh với RLHF như thế nào, chúng tôi kiểm tra trực quan các phản hồi được tạo ra bởi cả hai policy cho nhiệm vụ tóm tắt. Trong nhiều trường hợp, hai policy tạo ra các tóm tắt tương tự, được phản ánh trong tỷ lệ thắng tương tự của chúng. Tuy nhiên, chúng tôi xác định một số mẫu nơi chúng đôi khi phân kỳ.

Đầu tiên, chúng tôi quan sát thấy rằng trong một số trường hợp, RLHF ảo giác khi RLAIF không. Các ảo giác trong tóm tắt RLHF nghe có vẻ hợp lý nhưng không nhất quán với văn bản gốc. Ví dụ, trong Ví dụ #1 của Bảng 23, tóm tắt RLHF nói rằng tác giả 20 tuổi, nhưng điều này không được đề cập hoặc ngụ ý bởi văn bản nguồn.

Thứ hai, chúng tôi quan sát thấy rằng RLAIF đôi khi tạo ra các tóm tắt ít trôi chảy hơn RLHF. Ví dụ, trong Bảng 24, cả ba tóm tắt RLAIF đều chứa các câu dài dòng. Chúng tôi cũng quan sát các trường hợp nơi các phản hồi RLAIF lặp lại các cụm từ không truyền đạt được ý định của văn bản gốc. Ví dụ, một số tóm tắt kết thúc bằng "Làm thế nào để tôi vượt qua điều này?", mặc dù thực tế văn bản gốc không truyền đạt câu hỏi này một cách ngầm định hoặc rõ ràng.

Chúng tôi tiến hành đánh giá quy mô nhỏ trên 70 ví dụ, nơi các chú thích viên con người được yêu cầu xếp hạng mù các tóm tắt RLHF và RLAIF về độ chính xác, độ bao phủ và tính mạch lạc. Tuy nhiên, sự khác biệt trong điểm số không đáng kể về mặt thống kê. Phân tích có hệ thống hơn là cần thiết để xác định liệu các mẫu này tồn tại ở quy mô lớn, mà chúng tôi để lại cho công việc tương lai.

6. Công trình Liên quan

LLM đã cho thấy hiệu suất ấn tượng trên một loạt các nhiệm vụ NLP (Brown et al., 2020; Thoppilan et al., 2022; Chowdhery et al., 2022; Google et al., 2023; OpenAI, 2023a). Đối với một số nhiệm vụ này, RL đã nổi lên như một kỹ thuật tối ưu hóa hiệu quả. Trong khi các ứng dụng ban đầu của RL trên các nhiệm vụ như dịch thuật (Wu et al., 2016; 2018) và tóm tắt (Gao et al., 2019; Wu & Hu, 2018) sử dụng các chỉ số đánh giá tự động như phần thưởng, các công thức hóa đơn giản của phần thưởng như vậy không hoàn toàn căn chỉnh với các khái niệm chất lượng của con người.

Học tăng cường (RL) từ phản hồi con người (Christiano et al., 2017) đã được sử dụng như một kỹ thuật để căn chỉnh trực tiếp LLM với sở thích con người (Ziegler et al., 2019) thông qua việc huấn luyện một mô hình phần thưởng trên các so sánh cặp của các phản hồi ngôn ngữ tự nhiên. Nó đã được áp dụng thành công cho tóm tắt (Stiennon et al., 2020), theo hướng dẫn (Ouyang et al., 2022; Lai et al., 2023), đối thoại (Gilardi et al., 2023; Manyika, 2023; Glaese et al., 2022; Bai et al., 2022a) và trả lời câu hỏi (Nakano et al., 2021). Để giảm thiểu một số thách thức về ổn định và hiệu quả của việc tiến hành RL, DPO (Rafailov et al., 2024) công thức hóa lại mục tiêu huấn luyện để dựa vào loss phân loại, trong khi RaFT (Dong et al., 2023) sử dụng mô hình phần thưởng để tiến hành rejection-sampling fine-tuning.

LLM cũng đã được sử dụng rộng rãi cho tạo dữ liệu (Wang et al., 2021b; Meng et al., 2023), tăng cường (Feng et al., 2021) và trong các thiết lập tự huấn luyện (Wang et al., 2022b; Madaan et al., 2023). Bai et al. (2022b) giới thiệu ý tưởng về RLAIF, sử dụng các sở thích được gán nhãn bởi LLM và con người cùng nhau để tối ưu hóa chung cho hai mục tiêu helpfulness và harmlessness. Các công trình gần đây cũng đã khám phá các kỹ thuật liên quan để tạo ra phần thưởng từ LLM (Roit et al., 2023; Kwon et al., 2022; Yang et al., 2023). Những công trình này chứng minh rằng LLM có thể tạo ra các tín hiệu hữu ích cho fine-tuning RL, điều này đã truyền cảm hứng cho nghiên cứu của công trình này về việc liệu LLM có thể phục vụ như một giải pháp thay thế khả thi cho con người trong việc thu thập các nhãn sở thích cho RL.

7. Kết luận

Chúng tôi cho thấy rằng RLAIF đạt được những cải thiện tương đương với RLHF trên ba nhiệm vụ tạo văn bản. Trong các so sánh trực tiếp, RLAIF và RLHF được con người ưa thích với tỷ lệ tương tự. Hơn nữa, chúng tôi chứng minh bằng chứng về tự cải thiện LLM bằng cách cho thấy rằng RLAIF hiệu quả ngay cả khi LLM labeler có cùng kích thước với policy, hoặc thậm chí là checkpoint chính xác giống với policy ban đầu. Ngoài ra, chúng tôi cũng giới thiệu direct-RLAIF, trực tiếp nhắc nhở LLM labeler để cung cấp phần thưởng trong quá trình RL, vượt trội hơn thiết lập RLAIF tiêu chuẩn mà trước tiên chưng cất các sở thích LLM vào một RM riêng biệt. Cuối cùng, chúng tôi nghiên cứu tác động của các kỹ thuật gán nhãn AI khác nhau đối với sự căn chỉnh với sở thích con người.

Trong khi công trình này làm nổi bật tiềm năng của RLAIF, vẫn còn nhiều câu hỏi mở hấp dẫn, chẳng hạn như RLAIF có thể được điều chỉnh cho một thiết lập RL dựa trên mô hình nơi cả con người và trợ lý đều được mô hình hóa bởi LLM, hoặc phản hồi AI có thể được tận dụng như thế nào cho việc gán credit granular. Chúng tôi để lại những câu hỏi này cho công việc tương lai.

Lời cảm ơn

Chúng tôi muốn cảm ơn nhiều người đã giúp hoàn thành công trình này. Chúng tôi cảm ơn Chen Zhu đã tối ưu hóa thiết lập suy luận LLM của chúng tôi, Le Hou đã đề xuất cải thiện prompt và thí nghiệm với self-consistency, Léonard Hussenot đã đưa vấn đề thiên vị vị trí trong LLM đến sự chú ý của chúng tôi, và Bradley Green, Ewa Dominowska, và Blaise Aguera y Arcas đã hỗ trợ nghiên cứu này.

Chúng tôi cảm ơn tất cả những người đã xem xét kỹ lưỡng công trình của chúng tôi và cung cấp phản hồi có giá trị: Hakim Sidahmed, Meiqi Guo, Michal Valko, Nevan Wichers, Sian Gooding, và Yuan Cao.

Chúng tôi cảm ơn Mo Azar, Daniel Guo, Andrea Michi, Nicolas Perez-Nieves, và Marco Selvi vì đóng góp của họ trong việc phát triển một thiết lập huấn luyện RLAIF trực tiếp nhắc nhở một LLM để có được điểm phần thưởng.

Cuối cùng, chúng tôi cảm ơn các cá nhân đã thiết kế và xây dựng cơ sở hạ tầng huấn luyện RL được sử dụng trong bài báo này: Léonard Hussenot, Robert Dadashi, Geoffrey Cideron, Alexis Jacq, Sabela Ramos, Piotr Stanczyk, Sertan Girgin, Danila Sinopalnikov, Amélie Héliou, Nikola Momchev, và Olivier Bachem.

Tuyên bố Tác động

Bài báo này tìm cách hiểu rõ hơn và cải thiện tiện ích của các mô hình AI theo cách có thể mở rộng. Các phương pháp được trình bày trong bài báo này làm cho việc căn chỉnh mô hình trở nên dễ tiếp cận hơn với các nhà phát triển, vì việc tạo ra sở thích từ LLM có giá cả phải chăng hơn và nhanh hơn so với gán nhãn con người. Tuy nhiên, việc sử dụng Phản hồi AI đưa ra hai cân nhắc đạo đức.

Việc sử dụng phản hồi được tạo ra bởi AI như một nguồn cho việc căn chỉnh mô hình có nguy cơ tiềm tàng chuyển giao thiên vị từ các LLM có sẵn sang các sở thích được tạo ra. Điều này có thể dẫn đến các policy được huấn luyện RL tiếp tục khuếch đại thiên vị, từ đó vô tình làm lệch hướng các mô hình và có thể gây hại. Cần phải thận trọng cực kỳ, đặc biệt khi triển khai các mô hình này trong các lĩnh vực có cược cao như y học, luật pháp và việc làm, nơi các mô hình có tiềm năng tác động đáng kể đến cuộc sống con người theo cách bất lợi. Trong các lĩnh vực như vậy, chúng tôi tin rằng các chuyên gia con người được đào tạo để cẩn thận gán sở thích theo các chính sách nghiêm ngặt nên được coi là tiêu chuẩn vàng.

Một cân nhắc đạo đức khác là việc giảm rào cản để căn chỉnh LLM cũng mang theo nguy cơ tạo điều kiện cho việc lạm dụng chúng cho các mục đích độc hại. Ví dụ, RLAIF có thể được sử dụng để huấn luyện các mô hình tạo ra thông tin sai lệch thuyết phục hoặc sản xuất nội dung căm thù và lạm dụng. Giải pháp giảm thiểu tốt nhất cho nguy cơ này là quản lý cẩn thận quyền truy cập và sử dụng các LLM mạnh mẽ (ví dụ giới hạn quyền truy cập "hộp trắng"), để ngăn chặn các tác nhân xấu lạm dụng chúng.

Tài liệu tham khảo

Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Mané, D. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.

Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Chen, C., Olsson, C., Olah, C., Hernandez, D., Drain, D., Ganguli, D., Li, D., Tran-Johnson, E., Perez, E., Kerr, J., Mueller, J., Ladish, J., Landau, J., Ndousse, K., Lukosuite, K., Lovitt, L., Sellitto, M., Elhage, N., Schiefer, N., Mercado, N., DasSarma, N., Lasenby, R., Larson, R., Ringer, S., Johnston, S., Kravec, S., Showk, S. E., Fort, S., Lanham, T., Telleen-Lawton, T., Conerly, T., Henighan, T., Hume, T., Bowman, S. R., Hatfield-Dodds, Z., Mann, B., Amodei, D., Joseph, N., McCandlish, S., Brown, T., and Kaplan, J. Constitutional ai: Harmlessness from ai feedback, 2022b.

Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877–1901, 2020.

Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.

Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.

Ding, B., Qin, C., Liu, L., Chia, Y. K., Li, B., Joty, S., and Bing, L. Is GPT-3 a good data annotator? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11173–11195, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/

9

--- TRANG 10 ---
RLAIF so với RLHF

2023.acl-long.626. URL https://aclanthology.org/2023.acl-long.626.

Dong, H., Xiong, W., Goyal, D., Zhang, Y., Chow, W., Pan, R., Diao, S., Zhang, J., SHUM, K., and Zhang, T. RAFT: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=m7p5O7zblY.

Ethayarajh, K., Choi, Y., and Swayamdipta, S. Understanding dataset difficulty with V-usable information. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 5988–6008. PMLR, 17–23 Jul 2022.

Everitt, T. and Hutter, M. Avoiding wireheading with value reinforcement learning. In Artificial General Intelligence: 9th International Conference, AGI 2016, New York, NY, USA, July 16-19, 2016, Proceedings 9, pp. 12–22. Springer, 2016.

Fan, A., Lewis, M., and Dauphin, Y. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 889–898, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://aclanthology.org/P18-1082.

Feng, S. Y., Gangal, V., Wei, J., Chandar, S., Vosoughi, S., Mitamura, T., and Hovy, E. A survey of data augmentation approaches for NLP. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 968–988, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.84. URL https://aclanthology.org/2021.findings-acl.84.

Fox, R., Pakman, A., and Tishby, N. Taming the noise in reinforcement learning via soft updates. arXiv preprint arXiv:1512.08562, 2015.

Gao, Y., Meyer, C. M., Mesgar, M., and Gurevych, I. Reward learning for efficient reinforcement learning in extractive document summarisation. arXiv preprint arXiv:1907.12894, 2019.

Geist, M., Scherrer, B., and Pietquin, O. A theory of regularized markov decision processes. In International Conference on Machine Learning, pp. 2160–2169. PMLR, 2019.

Gilardi, F., Alizadeh, M., and Kubli, M. Chatgpt outperforms crowd-workers for text-annotation tasks. arXiv preprint arXiv:2303.15056, 2023.

Glaese, A., McAleese, N., Trebacz, M., Aslanides, J., Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M., Thacker, P., et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.

Google. Ai platform data labeling service pricing. https://cloud.google.com/ai-platform/data-labeling/pricing#labeling_costs, 2023. Accessed: 2023-09-28.

Google, R. A., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., Chu, E., Clark, J. H., Shafey, L. E., Huang, Y., Meier-Hellstern, K., Mishra, G., Moreira, E., Omernick, M., Robinson, K., Ruder, S., Tay, Y., Xiao, K., Xu, Y., Zhang, Y., Abrego, G. H., Ahn, J., Austin, J., Barham, P., Botha, J., Bradbury, J., Brahma, S., Brooks, K., Catasta, M., Cheng, Y., Cherry, C., Choquette-Choo, C. A., Chowdhery, A., Crepy, C., Dave, S., Dehghani, M., Dev, S., Devlin, J., Díaz, M., Du, N., Dyer, E., Feinberg, V., Feng, F., Fienber, V., Freitag, M., Garcia, X., Gehrmann, S., Gonzalez, L., Gur-Ari, G., Hand, S., Hashemi, H., Hou, L., Howland, J., Hu, A., Hui, J., Hurwitz, J., Isard, M., Ittycheriah, A., Jagielski, M., Jia, W., Kenealy, K., Krikun, M., Kudugunta, S., Lan, C., Lee, K., Lee, B., Li, E., Li, M., Li, W., Li, Y., Li, J., Lim, H., Lin, H., Liu, Z., Liu, F., Maggioni, M., Mahendru, A., Maynez, J., Misra, V., Moussalem, M., Nado, Z., Nham, J., Ni, E., Nystrom, A., Parrish, A., Pellat, M., Polacek, M., Polozov, A., Pope, R., Qiao, S., Reif, E., Richter, B., Riley, P., Ros, A. C., Roy, A., Saeta, B., Samuel, R., Shelby, R., Slone, A., Smilkov, D., So, D. R., Sohn, D., Tokumine, S., Valter, D., Vasudevan, V., Vodrahalli, K., Wang, X., Wang, P., Wang, Z., Wang, T., Wieting, J., Wu, Y., Xu, K., Xu, Y., Xue, L., Yin, P., Yu, J., Zhang, Q., Zheng, S., Zheng, C., Zhou, W., Zhou, D., Petrov, S., and Wu, Y. Palm 2 technical report, 2023.

Howard, R. A. Dynamic programming and markov processes. John Wiley, 1960.

Huang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J. Large language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.

Jaques, N., Gu, S., Bahdanau, D., Hernández-Lobato, J. M., Turner, R. E., and Eck, D. Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control. In International Conference on Machine Learning, pp. 1645–1654. PMLR, 2017.

Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.

10

--- TRANG 11 ---
RLAIF so với RLHF

Kendall, M. G. and Smith, B. B. The Problem of mRankings. The Annals of Mathematical Statistics, 10(3):275 – 287, 1939. doi: 10.1214/aoms/1177732186. URL https://doi.org/10.1214/aoms/1177732186.

Kwon, M., Xie, S. M., Bullard, K., and Sadigh, D. Reward design with language models. In The Eleventh International Conference on Learning Representations, 2022.

Lai, V. D., Van Nguyen, C., Ngo, N. T., Nguyen, T., Dernoncourt, F., Rossi, R. A., and Nguyen, T. H. Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback. arXiv preprint arXiv:2307.16039, 2023.

Liu, Y., Han, T., Ma, S., Zhang, J., Yang, Y., Tian, J., He, H., Li, A., He, M., Liu, Z., et al. Summary of chatgpt/gpt-4 research and perspective towards the future of large language models. arXiv preprint arXiv:2304.01852, 2023.

Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.

Manyika, J. An overview of bard: an early experiment with generative ai. https://ai.google/static/documents/google-about-bard.pdf, 2023. Accessed: 2023-08-23.

Meng, Y., Michalski, M., Huang, J., Zhang, Y., Abdelzaher, T., and Han, J. Tuning language models as training data generators for augmentation-enhanced few-shot learning. In International Conference on Machine Learning, pp. 24457–24477. PMLR, 2023.

Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11048–11064, 2022.

Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.

OpenAI. Gpt-4 technical report, 2023a.

OpenAI. Openai pricing. https://openai.com/pricing, 2023b. Accessed: 2023-09-28.

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744, 2022.

Pezeshkpour, P. and Hruschka, E. Large language models sensitivity to the order of options in multiple-choice questions. arXiv preprint arXiv:2308.11483, 2023.

Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.

Roit, P., Ferret, J., Shani, L., Aharoni, R., Cideron, G., Dadashi, R., Geist, M., Girgin, S., Hussenot, L., Keller, O., et al. Factually consistent summarization via reinforcement learning with textual entailment feedback. arXiv preprint arXiv:2306.00186, 2023.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Shazeer, N. and Stern, M. Adafactor: Adaptive learning rates with sublinear memory cost. CoRR, abs/1804.04235, 2018. URL http://arxiv.org/abs/1804.04235.

Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33: 3008–3021, 2020.

Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999.

Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.

Wang, B., Min, S., Deng, X., Shen, J., Wu, Y., Zettlemoyer, L., and Sun, H. Towards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprint arXiv:2212.10001, 2022a.

Wang, P., Li, L., Chen, L., Zhu, D., Lin, B., Cao, Y., Liu, Q., Liu, T., and Sui, Z. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023.

Wang, S., Liu, Y., Xu, Y., Zhu, C., and Zeng, M. Want to reduce labeling cost? gpt-3 can help. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 4195–4205, 2021a.

11

--- TRANG 12 ---
RLAIF so với RLHF

Wang, X., Wei, J., Schuurmans, D., Le, Q. V., Chi, E. H., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2022b.

Wang, Z., Yu, A. W., Firat, O., and Cao, Y. Towards zero-label language learning. arXiv preprint arXiv:2109.09193, 2021b.

Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2021.

Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35: 24824–24837, 2022.

Williams, R. J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229–256, 1992.

Wu, L., Tian, F., Qin, T., Lai, J., and Liu, T.-Y. A study of reinforcement learning for neural machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 3612–3621, 2018.

Wu, Y. and Hu, B. Learning to extract coherent summary via deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 5602, 2018.

Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., et al. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016.

Yang, K., Klein, D., Celikyilmaz, A., Peng, N., and Tian, Y. Rlcd: Reinforcement learning from contrast distillation for language model alignment, 2023.

Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.

12

--- TRANG 13 ---
RLAIF so với RLHF

A. Các Kiến thức Cơ bản về RLHF

Chúng tôi xem xét pipeline RLHF được giới thiệu trong Stiennon et al. (2020); Ouyang et al. (2022), bao gồm 3 giai đoạn: fine-tuning có giám sát, huấn luyện mô hình phần thưởng và học tăng cường.

A.1. Fine-tuning Có giám sát

Một LLM được pre-train được fine-tuned trên một tập dữ liệu được gán nhãn chất lượng cao cho một nhiệm vụ downstream sử dụng giám sát cấp token để tạo ra một mô hình fine-tuned có giám sát (SFT) πSFT.

A.2. Mô hình hóa Phần thưởng

Cho một đầu vào x, chúng tôi lấy mẫu một cặp phản hồi (y1, y2) ∼ π từ một hoặc nhiều mô hình, nơi thường π là mô hình SFT. Đầu vào và phản hồi được gửi đến các chú thích viên con người để đánh giá phản hồi nào tốt hơn theo một số tiêu chí. Những chú thích này tạo thành một tập dữ liệu các bộ ba D = {(x, yw, yl)}, nơi yw và yl là các phản hồi được ưa thích và không được ưa thích, tương ứng. Một mô hình phần thưởng (RM) rϕ được huấn luyện bằng cách tối thiểu hóa loss sau:

Lr(ϕ) = −E(x,yw,yl)∼D[log σ(rϕ(x, yw) − rϕ(x, yl))],

nơi σ là hàm sigmoid.

A.3. Học Tăng cường

Một policy πRLθ được khởi tạo từ trọng số mô hình SFT và sau đó được tối ưu hóa với học tăng cường để tối đa hóa phần thưởng được cho bởi RM, phục vụ như một proxy cho sở thích con người. Tùy chọn, một thuật ngữ phân kỳ Kullback-Leibler DKL được thêm vào mục tiêu để phạt πRLθ vì việc lệch khỏi policy SFT gốc πSFT, được kiểm soát bởi siêu tham số β (Fox et al., 2015; Geist et al., 2019). Loss KL giúp ngăn chặn πRLθ trôi vào một vùng mà nó tạo ra ngôn ngữ được RM thưởng cao nhưng bao gồm ngôn ngữ chất lượng thấp hoặc không tự nhiên - một hiện tượng được gọi là "reward hacking" (Everitt & Hutter, 2016; Amodei et al., 2016). Mục tiêu tối ưu hóa được mô tả bởi phương trình dưới đây:

J(θ) = Ey∼πθ(·|x)[(1−β)rϕ(y|x) − βDKL(πRLθ(y|x)||πSFT(y|x))],

nơi β là một siêu tham số giữa 0 và 1.

B. Thiên vị Vị trí trong LLM Labeler

Bảng 4: Thiên vị vị trí phổ biến hơn trong các kích thước mô hình nhỏ hơn, được đo bằng tỷ lệ phần trăm các ví dụ nơi LLM ưa thích cùng vị trí ngay cả sau khi hoán đổi thứ tự của các ứng viên ("% Cùng Vị trí Được Ưa thích"). Phân tích được tiến hành sử dụng prompt "Detailed + CoT 0-shot" trên nhiệm vụ tóm tắt.

Kích thước Mô hình    % Cùng Vị trí Được Ưa thích
PaLM 2 L            18%
PaLM 2 S            21%
PaLM 2 XS           56%

Phân tích của chúng tôi trên nhiệm vụ tóm tắt cho thấy rằng các LLM được sử dụng để gán nhãn sở thích bị thiên vị bởi thứ tự mà các ứng viên được hiển thị. Đối với mỗi ví dụ trong tập đánh giá gán nhãn AI của chúng tôi, chúng tôi truy vấn sở thích LLM cho cặp ứng viên, hoán đổi thứ tự mà các ứng viên được trình bày, và sau đó truy vấn sở thích LLM một lần nữa.

Chúng tôi coi một LLM thiên vị hơn nếu nó ưa thích cùng vị trí trên cả suy luận gốc và đảo ngược. Ví dụ, để ứng viên A và B ở vị trí 1 và 2 cho suy luận đầu tiên và ở vị trí 2 và 1 cho suy luận thứ hai. Nếu LLM ưa thích cùng vị trí trên cả hai suy luận, chúng tôi coi LLM bị thiên vị vị trí. Chúng tôi đo lường thiên vị vị trí bằng cách tính toán "% Cùng Vị trí Được Ưa thích" - tỷ lệ phần trăm các cặp suy luận nơi điều này xảy ra. Giá trị chỉ số cao hơn cho thấy một LLM thiên vị hơn.

13

--- TRANG 14 ---
RLAIF so với RLHF

Chúng tôi thấy rằng PaLM 2 L, S và XS ưa thích cùng vị trí lần lượt 18%, 21% và 56% thời gian, cho thấy rằng thiên vị vị trí tương quan nghịch với kích thước mô hình (xem Bảng 4). Một giả thuyết là các mô hình lớn hơn có khả năng hơn và do đó đánh giá sở thích một cách trung thực hơn dựa trên nội dung của các ứng viên thay vì vị trí của chúng, vốn được cho là không quan trọng.

Chúng tôi cũng quan sát thấy rằng đối với PaLM 2 L, trong 18% trường hợp nơi nó ưa thích cùng vị trí trên cả hai suy luận, 94% thời gian nó ưa thích ứng viên đầu tiên được hiển thị. Mặt khác, PaLM 2 S và XS thể hiện sự ưa thích cho ứng viên thứ hai được hiển thị khi cùng vị trí được ưa thích trên cả hai suy luận, ưa thích nó lần lượt 91% và 99% thời gian. Những thiên vị này có ý nghĩa thống kê theo kiểm định nhị thức hai phía tại α = 0.05.

C. Chi tiết Tập dữ liệu

Đối với tóm tắt, chúng tôi sử dụng tập dữ liệu Reddit TL;DR được lọc (Stiennon et al., 2020), chứa các bài đăng từ Reddit¹¹ đã được lọc để đảm bảo chất lượng cao. Tập dữ liệu chứa 123k bài đăng, nơi ~5% được giữ lại như một tập validation.

Ngoài ra, chúng tôi sử dụng tập dữ liệu sở thích con người của OpenAI được tạo từ tập dữ liệu Reddit TL;DR được lọc. Đối với một bài đăng cho trước, hai tóm tắt ứng viên được tạo ra - thường từ các policy khác nhau, và các labeler con người được yêu cầu đánh giá tóm tắt nào họ ưa thích. Tổng tập dữ liệu bao gồm 92k so sánh cặp.

Đối với tạo đối thoại hữu ích và vô hại, chúng tôi sử dụng các tập dữ liệu Sở thích Hữu ích và Vô hại của Anthropic¹² (Bai et al., 2022a). Mỗi ví dụ bao gồm một lịch sử cuộc đối thoại giữa con người và trợ lý AI kèm theo một phản hồi được ưa thích và không được ưa thích từ trợ lý AI. Sở thích dựa trên phản hồi nào hữu ích và trung thực hơn cho nhiệm vụ hữu ích, và phản hồi nào an toàn hơn và ít có hại hơn cho nhiệm vụ vô hại. Mỗi tập dữ liệu bao gồm hơn 40k ví dụ huấn luyện và 2k ví dụ test. Chúng tôi tiếp tục chia mỗi tập test gốc thành một tập validation và test bằng cách ngẫu nhiên gán hai phần ba ví dụ cho validation và một phần ba cho test.

D. Chi tiết Gán nhãn LLM

Đối với gán nhãn LLM, chúng tôi đặt độ dài ngữ cảnh đầu vào tối đa là 4096 token. Đối với tạo chuỗi suy nghĩ, chúng tôi đặt độ dài giải mã tối đa là 512 token và lấy mẫu với nhiệt độ T = 0.0 (tức là giải mã greedy). Đối với các thí nghiệm self-consistency trong Phụ lục M, chúng tôi sử dụng nhiệt độ thay đổi từ T = 0.3 đến T = 1.0 với top-K sampling (Fan et al., 2018), nơi K = 40.

Trong Phần 4.3, chúng tôi sử dụng AI labeler để tính toán trực tiếp một điểm số mà chúng tôi tận dụng như phần thưởng cho RL. Đối với tóm tắt, chúng tôi sử dụng prompt sau "Bạn là một chuyên gia đánh giá tóm tắt. Cho một TEXT (hoàn thành với một SUBREDDIT và một TITLE) và một SUMMARY, vai trò của bạn là cung cấp một SCORE từ 1 đến 10 đánh giá chất lượng của SUMMARY cho TEXT, với 1 là khủng khiếp và 10 là một SUMMARY hoàn hảo.", theo sau bởi bài đăng Reddit đầu vào, sau đó tóm tắt để chấm điểm được đứng trước bởi "SUMMARY:", và một "SCORE:" cuối cùng. Đối với tạo nhiệm vụ hữu ích, chúng tôi sử dụng prompt "Bạn là một chuyên gia đánh giá các phản hồi Trợ lý hữu ích và trung thực. Vai trò của bạn là cung cấp một SCORE từ 1 đến 10 đánh giá tính hữu ích và trung thực của RESPONSE cho một CONTEXT cho trước. Nơi SCORE 1 đề cập đến RESPONSE vô dụng và không trung thực và SCORE 10 đề cập đến một RESPONSE hoàn toàn hữu ích và trung thực.", theo sau bởi lịch sử cuộc đối thoại và một phản hồi.

Các mô hình PaLM 2 có sẵn công khai thông qua Vertex AI¹³ của Google Cloud, mặc dù tái tạo chính xác không được đảm bảo vì các mô hình có thể truy cập thông qua Google Cloud có thể thay đổi.

E. REINFORCE cho Mô hình Ngôn ngữ

Xem xét một MDP xác định, horizon hữu hạn M = (X, A, R, P, γ) (Howard, 1960). Tại mỗi bước t, cho trạng thái hiện tại Xt ∈ X và hành động tiếp theo At ∈ A, mô hình nhận được một phần thưởng Rt = R(Xt, At) và chuyển đổi sang trạng thái tiếp theo Xt+1 = P(Xt, At).

Trong ngữ cảnh của các mô hình ngôn ngữ, Xt là sự nối của văn bản đầu vào và tất cả văn bản được tạo ra bởi policy cho đến thời điểm t. Hành động At là token từ vocabulary được xem xét được giải mã tại thời điểm t bởi policy ngẫu nhiên πθ(·|Xt), nơi θ đại diện cho các tham số policy. Phần thưởng Rt được cho bởi RM, chỉ được đánh giá khi phản hồi mô hình ngôn ngữ đã được tạo ra hoàn toàn; tất cả các phần thưởng trước token cuối cùng được đặt thành 0, trong khi phần thưởng tương ứng với token cuối cùng được đặt thành RT.

¹¹www.reddit.com
¹²Chúng tôi sử dụng các tập dữ liệu helpful-base và harmless-base từ https://huggingface.co/datasets/Anthropic/hh-rlhf.
¹³https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models

14

--- TRANG 15 ---
RLAIF so với RLHF

Tổng tích lũy của các phần thưởng nhận được khi theo policy πθ từ bước thời gian t được gọi là return. Nói chung, nó được định nghĩa là Zt = ∑ᵀₛ₌ₜ γˢ⁻ᵗRₛ. Tuy nhiên, vì chỉ có phần thưởng cuối cùng là khác không và chúng tôi đặt γ = 1, return có thể được đơn giản hóa thành Zt = RT.

Cho một quỹ đạo (Xt, At, Rt)ᵀₜ₌₀ được tạo ra dưới πθ, loss policy gradient từ REINFORCE sau đó được định nghĩa như sau:

LPG(θ) = -∑ₜ log πθ(At|Xt)[Zt - Vᵋψ(Xt)],

nơi ký hiệu gạch ngang biểu thị rằng không có gradient nào được truyền qua thuật ngữ advantage trong quá trình backpropagation.

Hàm giá trị baseline Vᵋψ(x) ước tính return-to-go Zt khi theo policy πθ, và nó được tham số hóa bởi ψ (Williams, 1992; Sutton et al., 1999). Nó được huấn luyện với loss sau:

LV(ψ) = ∑ₜ(Zt - Vᵋψ(Xt))².

Mục tiêu tối ưu hóa đầy đủ của chúng tôi được viết trong Phần A.3. Chúng tôi kết hợp phân kỳ KL trong loss policy gradient được mô tả ở trên, như thường thấy trong công trình khác (Jaques et al., 2017).

F. Chi tiết Huấn luyện Mô hình

Các mô hình SFT cho nhiệm vụ tóm tắt được huấn luyện trên tập dữ liệu Reddit TL;DR với batch size 128 và cho một epoch. Chúng tôi sử dụng optimizer Adafactor (Shazeer & Stern, 2018) với learning rate 10⁻⁵, và độ dài đầu vào và đầu ra tối đa lần lượt là 1024 và 128 token. Đối với các nhiệm vụ tạo đối thoại hữu ích và vô hại, một phiên bản instruction-tuned của PaLM 2 XS phục vụ như mô hình SFT.

RM cho tất cả các nhiệm vụ được huấn luyện cho đến khi các đường cong loss và accuracy huấn luyện ổn định, điều này xảy ra trong 2-3 epoch. Chúng tôi sử dụng optimizer Adafactor với learning rate 10⁻⁵. Batch size là 128 cho RM tóm tắt và 32 cho RM của các nhiệm vụ khác. Chúng tôi huấn luyện tất cả RM của chúng tôi với độ dài đầu vào tối đa 1152 token để tính đến 1024 token ngữ cảnh và 128 token phản hồi. Chúng tôi báo cáo độ chính xác của các RM trong Phụ lục G.

Đối với tóm tắt, RM phản hồi AI được khởi tạo từ mô hình SFT (tức là PaLM 2 XS được fine-tuned trên Reddit TL;DR), và RM phản hồi con người được khởi tạo từ PaLM 2 XS. Chúng tôi thí nghiệm với việc khởi tạo RM phản hồi con người từ mô hình SFT nhưng thấy rằng nó dẫn đến độ chính xác thấp hơn trên tập sở thích con người được giữ lại (xem Bảng 6). Đối với các nhiệm vụ tạo đối thoại hữu ích và vô hại, chúng tôi khởi tạo cả RM phản hồi con người và AI từ phiên bản instruction-tuned của PaLM 2 XS.

Đối với học tăng cường, chúng tôi sử dụng mô hình SFT cho mỗi nhiệm vụ như policy ban đầu. Chúng tôi lấy mẫu từ các policy mô hình ngôn ngữ của chúng tôi cho tất cả các nhiệm vụ với nhiệt độ T = 0.9 để khuyến khích khám phá. Chúng tôi huấn luyện với batch size 128 và learning rate 10⁻⁵ cho 8 epoch. Chúng tôi đặt β = 0.05 cho loss phân kỳ KL.

Để chọn checkpoint cuối cùng cho mỗi policy RL, trước tiên chúng tôi chọn 4 checkpoint ứng viên từ huấn luyện RL đạt điểm phần thưởng cao trên các prompt validation. Sau đó chúng tôi nhắc nhở một LLM có sẵn để đánh giá tỷ lệ thắng của các phản hồi checkpoint RL so với các phản hồi policy SFT. Chúng tôi cũng tiến hành kiểm tra thủ công của một tá ví dụ. Chúng tôi chọn checkpoint với sự kết hợp tốt nhất của tỷ lệ thắng và chất lượng như được đánh giá bởi kiểm tra thủ công như policy RL cuối cùng của chúng tôi.

G. Độ chính xác Mô hình Phần thưởng

Độ chính xác Cặp đo lường mức độ chính xác của một mô hình phần thưởng được huấn luyện so với một tập sở thích con người holdout. Cho một ngữ cảnh đầu vào và cặp phản hồi ứng viên, giá trị là 1 nếu RM chấm điểm ứng viên được ưa thích cao hơn

15

--- TRANG 16 ---
RLAIF so với RLHF

Bảng 5: Độ chính xác cặp của các mô hình phần thưởng phản hồi con người và phản hồi AI trên tất cả các nhiệm vụ. Các chỉ số được tính toán trên một tập sở thích con người holdout cho mỗi nhiệm vụ.

Nhiệm vụ         Phản hồi      Phản hồi
                Con người       AI
Tóm tắt           79.3%       74.2%
Đối thoại Hữu ích  76.0%       67.8%
Đối thoại Vô hại   72.1%       69.7%

Bảng 6: Kết quả khởi tạo RM tóm tắt trên PaLM 2 XS so với mô hình SFT.

Khởi tạo         Phản hồi      Phản hồi
                Con người       AI
PaLM 2 XS        79.3%        73.0%
SFT              78.7%        74.2%

ứng viên không được ưa thích, theo nhãn con người. Nếu không thì giá trị là 0. Số lượng này được tính trung bình qua nhiều ví dụ để có được độ chính xác cặp.

Chúng tôi báo cáo độ chính xác RM trên một tập sở thích con người holdout cho tất cả các nhiệm vụ trong Bảng 5. Đối với tóm tắt, chúng tôi cũng báo cáo độ chính xác RM khi khởi tạo trên các checkpoint khác nhau trong Bảng 6. Trong Bảng 7, chúng tôi báo cáo độ chính xác cho các biến thể RM được sử dụng trong thí nghiệm độ nhạy end-to-end trong Phụ lục N và thí nghiệm same-size RLAIF trong Phần 4.2.

Chúng tôi quan sát thấy rằng các RM được huấn luyện trên phản hồi con người vượt trội hơn những RM được huấn luyện trên phản hồi AI, cả hai đều được đo lường so với một tập sở thích con người holdout. Mẫu này có vẻ tự nhiên, vì các sở thích con người được huấn luyện trên dữ liệu được rút ra từ cùng phân phối với tập validation. Tuy nhiên, thú vị khi lưu ý rằng mặc dù có khoảng cách trong độ chính xác giữa AI và RM sở thích con người, RLAIF đạt được kết quả tương đương với RLHF trên hai nhiệm vụ và vượt trội nó trên một nhiệm vụ khác. Ngoài ra, chúng tôi lưu ý rằng các RM tóm tắt được huấn luyện trên "Base 0-shot" và "Detailed + CoT 0-shot" (tức là kỹ thuật prompting mặc định) đạt được độ chính xác lần lượt là 77.9% và 74.2%, đây là thứ tự nghịch đảo của hiệu suất cuối cùng của chúng sau RL (xem Phụ lục N). Những khoảng cách này trong độ chính xác RM cho thấy rằng độ chính xác RM, trong khi tương quan với tính hữu ích của RM, có thể không phản ánh chính xác hiệu quả của một RM trong RLHF và RLAIF. Cuối cùng, chúng tôi tin rằng tính hữu ích của RM được đánh giá thông qua việc tiến hành RL và đánh giá các policy cuối cùng thông qua đánh giá con người.

H. Định dạng Phản hồi Sau-RL

Đối với tóm tắt, chúng tôi quan sát thấy rằng các tóm tắt được tạo ra bởi các policy RLHF và RLAIF thường bao gồm các ký hiệu thừa như dấu chấm hoặc khoảng trắng ở cuối phản hồi - có thể do "reward hacking". Vì những token thêm này không có bất kỳ nội dung có ý nghĩa nào, chúng tôi lập trình loại bỏ một số ký hiệu ở cuối tóm tắt. Điều này đảm bảo rằng các đánh giá viên con người có thể tập trung vào nội dung mà không bị phân tâm bởi định dạng của phản hồi.

I. Chi tiết Đánh giá Con người

Để tiến hành đánh giá con người, tổng cộng chúng tôi tạo ra ~2k instance đánh giá duy nhất. Mỗi instance bao gồm một ngữ cảnh đơn lẻ và nhiều phản hồi mô hình riêng biệt (ví dụ phản hồi từ các policy SFT, RLAIF và RLHF), dẫn đến tổng cộng ~6k cặp (ngữ cảnh, phản hồi) duy nhất được đánh giá con người. Ngoài ra, mỗi instance được đánh giá bởi ba người đánh giá độc lập, dẫn đến ~18k bộ ba (ngữ cảnh, phản hồi, đánh giá).

Bảng 7: Giá trị độ chính xác cho các biến thể của RM được huấn luyện trên nhãn AI cho nhiệm vụ tóm tắt.

Biến thể RM                                           Phản hồi AI
Được huấn luyện trên nhãn "Base 0-shot"              77.9%
Được huấn luyện trên nhãn "Detailed CoT 0-shot" 
từ PaLM 2 XS                                         66.4%

16

--- TRANG 17 ---
RLAIF so với RLHF

Chúng tôi đo lường thỏa thuận giữa các chú thích viên với Hệ số Concordance W của Kendall (Kendall & Smith, 1939) - một thống kê phi tham số để đánh giá thỏa thuận giữa nhiều người đánh giá xếp hạng nhiều mục. Các giá trị của Kendall's W dao động từ 0 đến 1, nơi 0 cho thấy sự không đồng ý hoàn hảo và 1 cho thấy sự đồng ý hoàn hảo. Chúng tôi tiến hành nhiều phiên đánh giá con người, và thống kê W dao động từ 0.6-0.7, cho thấy mức độ đồng ý hợp lý.

J. Kiểm soát Độ dài Phản hồi

Độ dài phản hồi thường có thể ảnh hưởng đến nhận thức của các đánh giá viên con người về chất lượng (Stiennon et al., 2020), và các policy khác nhau của chúng tôi tạo ra các phản hồi khác nhau về độ dài. Ví dụ, trong nhiệm vụ tóm tắt, các tóm tắt được tạo ra bởi các policy RLAIF, RLHF và SFT được gửi để đánh giá con người có độ dài ký tự trung bình lần lượt là 164, 161 và 132. Chúng tôi tiến hành phân tích post-hoc để ước tính tỷ lệ thắng sau khi kiểm soát độ dài.

Chúng tôi tiếp cận tương tự như Stiennon et al. (2020) và tính toán "tỷ lệ thắng điều chỉnh độ dài của policy A so với policy B". Cho policy A, chúng tôi huấn luyện một mô hình hồi quy logistic nơi đầu vào là tỷ lệ độ dài phản hồi của policy A so với độ dài tóm tắt của policy B (theo ký tự), và mục tiêu là một nhãn nhị phân cho thấy liệu phản hồi của policy A có được ưa thích hơn phản hồi của policy B hay không. Sau khi fit mô hình, chúng tôi ước tính một tỷ lệ thắng được kiểm soát độ dài bằng cách yêu cầu bộ hồi quy logistic dự đoán tỷ lệ thắng cho tỷ lệ độ dài là 1.0, đại diện cho tình huống nơi cả hai phản hồi có độ dài bằng nhau.

Sau khi kiểm soát độ dài cho nhiệm vụ tóm tắt, tỷ lệ thắng điều chỉnh độ dài của chúng tôi cho RLAIF và RLHF so với SFT lần lượt là 59% và 61% (xem Bảng 8). Cả hai policy RL tiếp tục vượt trội hơn policy SFT với margin tương tự, hỗ trợ tuyên bố ban đầu của chúng tôi rằng RLAIF tương đương với RLHF.

Chúng tôi đạt được kết luận tương tự cho nhiệm vụ tạo đối thoại hữu ích (Bảng 9), các thí nghiệm same-size RLAIF và direct RLAIF (d-RLAIF) (Bảng 11), thí nghiệm độ nhạy end-to-end đối với căn chỉnh AI labeler (Bảng 12), và kết hợp phản hồi con người và AI (Bảng 13).

Đối với nhiệm vụ tạo đối thoại vô hại, chúng tôi sử dụng một thiết lập khác. Vì các đánh giá viên con người đánh giá mỗi phản hồi độc lập là có hại hoặc vô hại, chúng tôi tính toán tỷ lệ vô hại thay vì tỷ lệ thắng. Chúng tôi sử dụng độ dài tạo trung bình từ policy SFT làm điểm tham chiếu cho tất cả các policy khác (Bảng 10).

Chúng tôi lưu ý rằng phương pháp post-hoc này để kiểm soát độ dài không hoàn hảo, vì nó giả định mô hình hồi quy logistic học chính xác mối quan hệ giữa độ dài tóm tắt và sở thích con người. Một phương pháp có nguyên tắc hơn là khuyến khích tất cả các policy tạo ra các tóm tắt có độ dài tương tự thông qua một loss huấn luyện phụ.

Bảng 8: Tỷ lệ thắng được kiểm soát độ dài cho nhiệm vụ tóm tắt.

Mô hình              Chưa điều chỉnh    Đã điều chỉnh
                     độ dài             độ dài
RLAIF vs SFT         71%                59%
RLHF vs SFT          73%                61%
RLAIF vs RLHF        50%                47%

Bảng 9: Tỷ lệ thắng được kiểm soát độ dài cho nhiệm vụ tạo đối thoại hữu ích.

Mô hình              Chưa điều chỉnh    Đã điều chỉnh
                     độ dài             độ dài
RLAIF vs SFT         63%                61%
RLHF vs SFT          64%                61%
RLAIF vs RLHF        52%                50%

K. Kết hợp Phản hồi Con người và AI

Chúng tôi nghiên cứu hiệu quả của việc kết hợp phản hồi con người và phản hồi AI trên nhiệm vụ tóm tắt. Chúng tôi gọi phương pháp này là RLHF + RLAIF và so sánh nó với RLHF.

Đầu tiên, cho các ngữ cảnh được rút ra ngẫu nhiên từ tập dữ liệu Reddit TL;DR, các phản hồi được tạo ra bởi các policy RLHF và SFT

17

--- TRANG 18 ---
RLAIF so với RLHF

Bảng 10: Tỷ lệ vô hại được kiểm soát độ dài cho nhiệm vụ tạo đối thoại vô hại. Chúng tôi sử dụng độ dài tạo trung bình từ mô hình SFT làm độ dài tham chiếu để tính toán tỷ lệ vô hại được kiểm soát độ dài cho RLHF và RLAIF.

Mô hình    Chưa điều chỉnh    Đã điều chỉnh
           độ dài             độ dài
SFT        64%                64%
RLHF       76%                78%
RLAIF      88%                91%

Bảng 11: Tỷ lệ thắng được kiểm soát độ dài cho same-size RLAIF và direct RLAIF.

Mô hình                    Chưa điều chỉnh    Đã điều chỉnh
                          độ dài             độ dài
Same-size RLAIF vs SFT    68%                59%
d-RLAIF vs SFT            74%                65%
d-RLAIF vs Same-size RLAIF 60%               56%

với nhiệt độ T = 1.0. PaLM 2 L được instruction-tuned sau đó được gọi để tạo ra các sở thích AI trên tập dữ liệu sở thích con người OpenAI. Cuối cùng, một RM mới được huấn luyện trên cả tập dữ liệu với nhãn con người và tập dữ liệu với nhãn AI.

Chúng tôi quan sát thấy rằng RLHF + RLAIF không cải thiện vượt quá RLHF một mình. RLHF + RLAIF và RLHF đạt được tỷ lệ thắng lần lượt là 71% và 74% so với SFT. Sự khác biệt trong tỷ lệ thắng không có ý nghĩa thống kê. Khi so sánh trực tiếp, các người đánh giá ưa thích cả hai policy ngang bằng.

Trong khi thí nghiệm này không cho thấy kết quả tích cực từ việc kết hợp RLAIF và RLHF, có nhiều thiết lập thay thế có thể chứng minh thành công. Một thiết lập như vậy có thể bao gồm việc tiến hành RLAIF trước, sau đó thu thập các thế hệ và sở thích con người sử dụng policy RLAIF như điểm khởi đầu cho RLHF. Trong phương pháp học chương trình này, RLAIF có thể được xem như một policy "khởi động", sau đó được tinh chỉnh với RLHF. Một thiết lập có thể khác có thể bao gồm việc thu thập nhiều phản hồi AI hơn so với phản hồi con người, vì nó ít tốn kém hơn nhiều để thu thập (xem Phụ lục L). Chúng tôi để lại khám phá này cho công việc tương lai.

L. Chi phí Gán nhãn LLM so với Con người

Việc sử dụng LLM như các chú thích viên dữ liệu có thể ít tốn kém hơn nhiều so với thuê các chú thích viên con người (Wang et al., 2021a). Chúng tôi ước tính gán nhãn sở thích AI rẻ hơn hơn 10 lần so với gán nhãn sở thích con người theo các tính toán dưới đây.

Tại thời điểm viết, GPT-4 tính phí $0.03 USD và $0.06 USD cho mỗi 1,000 token để mã hóa và giải mã, tương ứng (OpenAI, 2023b). Đối với việc gán nhãn sở thích Reddit TL;DR với một LLM, độ dài token trung bình của chúng tôi như sau:

1. Độ dài prompt đầu vào - 830 token (sử dụng prompt "Detailed + CoT 0-shot")
2. Lý luận chuỗi suy nghĩ được tạo ra - 61 token

Ngoài ra, để giảm thiểu thiên vị vị trí, chúng tôi lặp lại mỗi quy trình gán nhãn sau khi đảo ngược thứ tự mà một cặp phản hồi được hiển thị. Chi phí gán nhãn AI ước tính của chúng tôi mỗi ví dụ là $0.06 USD¹⁴.

So sánh với đó, dịch vụ chú thích con người của Google Cloud tính phí khoảng $0.11 USD / 50 từ cho các nhiệm vụ phân loại tại thời điểm viết¹⁵ (Google, 2023). Chúng tôi giả định rằng mỗi nhiệm vụ phân loại chỉ bao gồm việc đọc một tài liệu

¹⁴2 suy luận * (830 token mã hóa * $0.03 / 1,000 token + 61 token giải mã * $0.06 / 1,000 token) = $0.057 ≈ $0.06
¹⁵Google Cloud tính phí giữa $90 và $129 mỗi 1,000 đơn vị, nơi mỗi đơn vị là 50 từ cho một nhiệm vụ phân loại. Chúng tôi tính trung bình chi phí giới hạn dưới và trên và chuyển đổi từ đơn vị sang từ - ($90 / 1,000 đơn vị + $129 / 1,000 đơn vị) / 2 * 1 đơn vị / 50 từ = $0.1095 USD / 50 từ

18

--- TRANG 19 ---
RLAIF so với RLHF

Bảng 12: Tỷ lệ thắng được kiểm soát độ dài cho thí nghiệm về độ nhạy end-to-end đối với căn chỉnh AI labeler. Base RLAIF và Detailed RLAIF tương ứng với "Base 0-shot" RLAIF và "Detailed CoT 0-shot" RLAIF được mô tả trong Phụ lục N.

Mô hình                    Chưa điều chỉnh    Đã điều chỉnh
                          độ dài             độ dài
Base RLAIF vs SFT         63%                59%
Detailed RLAIF vs SFT     67%                63%
Base RLAIF vs Detailed RLAIF 41%             45%

Bảng 13: Tỷ lệ thắng được kiểm soát độ dài cho các thí nghiệm kết hợp phản hồi con người và AI.

Mô hình                   Chưa điều chỉnh    Đã điều chỉnh
                         độ dài             độ dài
RLHF + RLAIF vs SFT      71%                61%
RLHF vs SFT              74%                67%
RLHF + RLAIF vs RLHF     48%                46%

và hai tóm tắt ứng viên, có độ dài từ trung bình kết hợp là 304 từ. Chúng tôi ước tính chi phí gán nhãn con người mỗi ví dụ là $0.67 USD (304 từ * $0.11 / 50 từ).

Phân tích chi phí này không tính đến tất cả các yếu tố, chẳng hạn như chi phí đào tạo các chú thích viên con người, chi phí của các chú thích viên chuyên gia so với crowd-source, hoặc chi phí thiết lập gán nhãn LLM.

M. Self-Consistency

Bảng 14: Lấy mẫu nhiều lý luận chuỗi suy nghĩ với T > 0 dẫn đến căn chỉnh thấp hơn với sở thích con người. Lưu ý: 1 và 16 mẫu đại diện cho 2 và 32 suy luận cho kỹ thuật giảm thiên vị vị trí của chúng tôi (xem Phần 2.1.1).

Self-Consistency          Căn chỉnh AI Labeler
1 mẫu, T=0.0             78.0%
16 mẫu, T=0.3            76.2%
16 mẫu, T=0.5            75.1%
16 mẫu, T=0.7            74.0%
16 mẫu, T=1.0            72.8%

Chúng tôi thí nghiệm với self-consistency (Wang et al., 2022b) - một kỹ thuật để tạo ra các lý luận chuỗi suy nghĩ mạnh mẽ. Để thực hiện self-consistency, chúng tôi lấy mẫu nhiều lý luận chuỗi suy nghĩ với nhiệt độ T > 0, và các phân phối sở thích LLM được có được cho mỗi lý luận. Kết quả sau đó được tính trung bình để có được phân phối sở thích cuối cùng.

Trên nhiệm vụ tóm tắt, chúng tôi thí nghiệm với self-consistency sử dụng 4 và 16 mẫu dưới nhiệt độ giải mã từ 0.3 đến 1.0 (xem Hình 14)¹⁶. Trong tất cả các thiết lập, self-consistency giảm căn chỉnh AI labeler so với baseline không có self-consistency. Các thí nghiệm của chúng tôi cho thấy rằng căn chỉnh giảm khi nhiệt độ tăng, với sự giảm lớn nhất hơn -5% tại T = 1.0. Trong các thí nghiệm của chúng tôi, việc sử dụng 4 so với 16 mẫu self-consistency không ảnh hưởng đến căn chỉnh AI labeler.

Kiểm tra thủ công các lý luận chuỗi suy nghĩ không tiết lộ bất kỳ mẫu chung nào về lý do tại sao self-consistency có thể làm giảm căn chỉnh (ví dụ trong Bảng 20). Một giả thuyết là việc sử dụng nhiệt độ T > 0 dẫn đến mô hình tạo ra các lý luận chất lượng thấp hơn so với giải mã greedy, cuối cùng dẫn đến độ chính xác tổng thể tệ hơn.

¹⁶Kết quả sử dụng 4 mẫu không được hiển thị vì chúng chỉ khác biệt so với kết quả 16 mẫu là ±0.4%.

19

--- TRANG 20 ---
RLAIF so với RLHF

N. Độ nhạy End-to-end đối với Căn chỉnh AI Labeler

Chúng tôi đánh giá độ nhạy end-to-end của các policy RLAIF đối với căn chỉnh AI labeler trên nhiệm vụ tóm tắt. Vì phán đoán con người chủ quan và dễ bị nhiễu, chúng tôi kiểm tra liệu căn chỉnh AI labeler tốt hơn có dẫn đến hiệu suất downstream được cải thiện hay không. Chúng tôi huấn luyện hai policy RLAIF chỉ khác biệt trong kỹ thuật prompting được sử dụng cho gán nhãn AI - "Base 0-shot" và "Detailed CoT 0-shot", mang lại căn chỉnh AI labeler lần lượt là 76.1% và 78.0%.

Khi so sánh trực tiếp, các đánh giá viên con người ưa thích các tóm tắt từ "Detailed CoT 0-shot" 59% thời gian so với "Base 0-shot"¹⁷. Kết quả này cho thấy rằng những cải thiện nhỏ trong căn chỉnh AI labeler có thể dẫn đến những cải thiện đáng chú ý trong các policy RL cuối cùng. Tuy nhiên, nghiên cứu này có hạn chế, và các thí nghiệm bổ sung là cần thiết để rút ra kết luận có thể tổng quát hóa.

Bảng 15: Một ví dụ về prompt được đưa cho một LLM có sẵn để tạo ra các nhãn sở thích AI cho tóm tắt. {text}, {summary1}, và {summary2} được điền với các ví dụ chưa được gán nhãn, và một phân phối sở thích được có được bằng cách tính toán softmax của các log-xác suất của việc tạo ra các token "1" so với "2".

Lời mở đầu    Một tóm tắt tốt là một đoạn văn bản ngắn hơn có bản chất của
              bản gốc. ... Cho một đoạn văn bản và hai tóm tắt có thể của nó,
              xuất ra 1 hoặc 2 để chỉ ra tóm tắt nào tốt nhất tuân thủ tính
              mạch lạc, độ chính xác, độ bao phủ và chất lượng tổng thể như
              được định nghĩa ở trên.

Ví dụ         »»»» Ví dụ »»»»
              Văn bản - Chúng tôi là bạn thân trong hơn 4 năm ...
              Tóm tắt 1 - Chia tay với bạn thân, tôi có nên chúc cô ấy sinh
              nhật vui vẻ... Và bạn nghĩ gì về việc không liên lạc?
              Tóm tắt 2 - tôi có nên chúc người yêu cũ sinh nhật vui vẻ,
              tôi đã phá vỡ việc không liên lạc, tôi đang cố gắng kiên nhẫn
              hơn, tôi quá cần thiết, và tôi không muốn cô ấy nghĩ tôi sẽ
              tiếp tục là người đàn ông như vậy.
              Tóm tắt Ưu tiên=1
              »»»» Làm theo hướng dẫn và (các) ví dụ ở trên »»»»

Mẫu để        Văn bản - {text}
Chú thích     Tóm tắt 1 - {summary1}
              Tóm tắt 2 - {summary2}

Kết thúc      Tóm tắt Ưu tiên=

¹⁷Kết quả này khác biệt đáng kể về mặt thống kê so với 50% theo kiểm định nhị thức.

20

--- TRANG 21 ---
RLAIF so với RLHF

Bảng 16: Các lời mở đầu "Base" và "Detailed" được đưa cho LLM labeler để có được các nhãn sở thích cho nhiệm vụ tóm tắt.

Lời mở đầu "Base"
Bạn là một chuyên gia đánh giá tóm tắt. Cho một đoạn văn bản và hai tóm tắt có thể của nó, xuất ra 1 hoặc 2 để chỉ ra tóm tắt nào tốt hơn.

Lời mở đầu "Detailed"
Một tóm tắt tốt là một đoạn văn bản ngắn hơn có bản chất của bản gốc. Nó cố gắng hoàn thành cùng mục đích và truyền đạt thông tin chính từ bài đăng gốc. Dưới đây chúng tôi định nghĩa bốn trục đánh giá cho chất lượng tóm tắt: tính mạch lạc, độ chính xác, độ bao phủ và chất lượng tổng thể.

Tính mạch lạc: Trục này trả lời câu hỏi "tóm tắt có mạch lạc như thế nào khi đọc riêng lẻ?" Một tóm tắt mạch lạc nếu dễ hiểu khi đọc riêng lẻ và không có lỗi tiếng Anh. Một tóm tắt không mạch lạc nếu khó hiểu tóm tắt đang cố gắng nói gì. Nói chung, quan trọng hơn là tóm tắt có thể hiểu được hơn là không có lỗi ngữ pháp.

Độ chính xác: Trục này trả lời câu hỏi "thông tin thực tế trong tóm tắt có khớp chính xác với bài đăng không?" Một tóm tắt chính xác nếu nó không nói những điều không có trong bài viết, không nhầm lẫn con người, và nói chung không gây hiểu lầm.

Độ bao phủ: Trục này trả lời câu hỏi "tóm tắt bao phủ thông tin quan trọng trong bài đăng tốt như thế nào?" Một tóm tắt có độ bao phủ tốt nếu nó đề cập đến thông tin chính từ bài đăng quan trọng để hiểu tình huống được mô tả trong bài đăng. Một tóm tắt có độ bao phủ kém nếu ai đó chỉ đọc tóm tắt sẽ thiếu một số thông tin quan trọng về tình huống trong bài đăng. Một tóm tắt với độ bao phủ tốt cũng nên phù hợp với mục đích của bài đăng gốc (ví dụ để xin lời khuyên).

Chất lượng tổng thể: Trục này trả lời câu hỏi "tóm tắt tốt như thế nào nói chung trong việc đại diện cho bài đăng?" Điều này có thể bao gồm tất cả các trục chất lượng ở trên, cũng như những trục khác mà bạn cảm thấy quan trọng. Nếu khó tìm cách để làm cho tóm tắt tốt hơn, chất lượng tổng thể là tốt. Nếu có nhiều cách khác nhau để tóm tắt có thể được làm tốt hơn, chất lượng tổng thể là kém.

Bạn là một chuyên gia đánh giá tóm tắt. Cho một đoạn văn bản và hai tóm tắt có thể của nó, xuất ra 1 hoặc 2 để chỉ ra tóm tắt nào tuân thủ tốt nhất tính mạch lạc, độ chính xác, độ bao phủ và chất lượng tổng thể như được định nghĩa ở trên.

21

--- TRANG 22 ---
RLAIF so với RLHF

Bảng 17: Prompt được sử dụng cho "Detailed + CoT 0-shot" cho tóm tắt. Đối với các prompt CoT, trước tiên chúng tôi giải mã một phản hồi từ LLM và sau đó nối nó với prompt gốc và kết thúc "Tóm tắt Ưu tiên=" trước khi theo quy trình chấm điểm trong Phần 2.1 để có được một phân phối sở thích.

Lời mở đầu     Một tóm tắt tốt là một đoạn văn bản ngắn hơn có bản chất của
               bản gốc. Nó cố gắng hoàn thành cùng mục đích và truyền đạt
               thông tin chính từ bài đăng gốc. Dưới đây chúng tôi định nghĩa
               bốn trục đánh giá cho chất lượng tóm tắt: tính mạch lạc, độ
               chính xác, độ bao phủ và chất lượng tổng thể.

               Tính mạch lạc: Trục này trả lời câu hỏi "tóm tắt có mạch lạc
               như thế nào khi đọc riêng lẻ?" Một tóm tắt mạch lạc nếu dễ
               hiểu khi đọc riêng lẻ và không có lỗi tiếng Anh. Một tóm tắt
               không mạch lạc nếu khó hiểu tóm tắt đang cố gắng nói gì. Nói
               chung, quan trọng hơn là tóm tắt có thể hiểu được hơn là không
               có lỗi ngữ pháp.

               Độ chính xác: Trục này trả lời câu hỏi "thông tin thực tế
               trong tóm tắt có khớp chính xác với bài đăng không?" Một tóm
               tắt chính xác nếu nó không nói những điều không có trong bài
               viết, không nhầm lẫn con người, và nói chung không gây hiểu
               lầm.

               Độ bao phủ: Trục này trả lời câu hỏi "tóm tắt bao phủ thông
               tin quan trọng trong bài đăng tốt như thế nào?" Một tóm tắt
               có độ bao phủ tốt nếu nó đề cập đến thông tin chính từ bài
               đăng quan trọng để hiểu tình huống được mô tả trong bài đăng.
               Một tóm tắt có độ bao phủ kém nếu ai đó chỉ đọc tóm tắt sẽ
               thiếu một số thông tin quan trọng về tình huống trong bài
               đăng. Một tóm tắt với độ bao phủ tốt cũng nên phù hợp với mục
               đích của bài đăng gốc (ví dụ để xin lời khuyên).

               Chất lượng tổng thể: Trục này trả lời câu hỏi "tóm tắt tốt
               như thế nào nói chung trong việc đại diện cho bài đăng?"
               Điều này có thể bao gồm tất cả các trục chất lượng ở trên,
               cũng như những trục khác mà bạn cảm thấy quan trọng. Nếu khó
               tìm cách để làm cho tóm tắt tốt hơn, chất lượng tổng thể là
               tốt. Nếu có nhiều cách khác nhau để tóm tắt có thể được làm
               tốt hơn, chất lượng tổng thể là kém.

               Bạn là một chuyên gia đánh giá tóm tắt. Cho một đoạn văn bản
               và hai tóm tắt có thể của nó, giải thích tóm tắt nào tuân thủ
               tốt nhất tính mạch lạc, độ chính xác, độ bao phủ và chất
               lượng tổng thể như được định nghĩa ở trên.

Mẫu để        Văn bản - {text}
Chú thích     Tóm tắt 1 - {summary1}
              Tóm tắt 2 - {summary2}

Kết thúc      Xem xét tính mạch lạc, độ chính xác, độ bao phủ và chất lượng
              tổng thể của từng tóm tắt và giải thích cái nào tốt hơn.
              Lý luận:

22

--- TRANG 23 ---
RLAIF so với RLHF

Bảng 18: Template được sử dụng cho prompt "Detailed + CoT 1-shot" cho tóm tắt, với một số văn bản được loại bỏ để ngắn gọn.

Lời mở đầu    Một tóm tắt tốt là một đoạn văn bản ngắn hơn có bản chất của
              bản gốc. ... Cho một đoạn văn bản và hai tóm tắt có thể của
              nó, giải thích tóm tắt nào tuân thủ tốt nhất tính mạch lạc,
              độ chính xác, độ bao phủ và chất lượng tổng thể như được định
              nghĩa ở trên.

Ví dụ        »»»» Ví dụ »»»»
             Văn bản - Chúng tôi là bạn thân trong hơn 4 năm ...
             Tóm tắt 1 - Chia tay với bạn thân, tôi có nên chúc cô ấy sinh
             nhật vui vẻ... Và bạn nghĩ gì về việc không liên lạc?
             Tóm tắt 2 - tôi có nên chúc người yêu cũ sinh nhật vui vẻ,
             tôi đã phá vỡ việc không liên lạc, tôi đang cố gắng kiên nhẫn
             hơn, tôi quá cần thiết, và tôi không muốn cô ấy nghĩ tôi sẽ
             tiếp tục là người đàn ông như vậy.

             Suy nghĩ về Tóm tắt 1 -
             Tính mạch lạc - 7. Lý luận: Tóm tắt nói chung có thể hiểu
             được, mặc dù có thể được viết với ngữ pháp tốt hơn.
             Độ chính xác - 9. Lý luận: Tóm tắt không nói những điều
             không có trong văn bản gốc, và không gây hiểu lầm.
             Độ bao phủ - 6. Lý luận: Tóm tắt bao phủ hầu hết thông tin
             quan trọng trong bài đăng và truyền đạt ý chính của văn bản
             gốc. Tuy nhiên, nó nhấn mạnh nhiều hơn vào "không liên lạc"
             và có thể đề cập đến việc bao phủ/sự cần thiết để hoàn chỉnh
             hơn.
             Chất lượng Tổng thể - 7. Lý luận: Tóm tắt đại diện cho bài
             đăng khá tốt với chỉ những khu vực nhỏ mà nó có thể được cải
             thiện.

             Suy nghĩ về Tóm tắt 2 -
             Tính mạch lạc - 3. Lý luận: Tóm tắt dài dòng và có một số
             lỗi ngữ pháp.
             Độ chính xác - 4. Lý luận: Tóm tắt đề cập rằng tác giả đã
             phá vỡ việc không liên lạc, nhưng điều này không chính xác.
             Nếu không, nó chính xác.
             Độ bao phủ - 8. Lý luận: Tóm tắt bao phủ các điểm chính
             trong văn bản gốc.
             Chất lượng Tổng thể - 4. Lý luận: Tóm tắt hơi gây hiểu lầm
             và không truyền đạt tốt các điểm chính của văn bản gốc.

             Tóm tắt Ưu tiên=1
             »»»» Làm theo hướng dẫn và (các) ví dụ ở trên »»»»

Mẫu để       Văn bản - {text}
Chú thích    Tóm tắt 1 - {summary1}
             Tóm tắt 2 - {summary2}

Kết thúc     Suy nghĩ về Tóm tắt 1 -

23

--- TRANG 24 ---
RLAIF so với RLHF

Bảng 19: Một ví dụ về các lý luận chuỗi suy nghĩ khác nhau được tạo ra bởi các prompt 0-shot ("Detailed + CoT 0-shot") so với 1-shot ("Detailed + CoT 1-shot") cho tóm tắt. Chúng tôi lưu ý rằng lý luận CoT 0-shot không chính xác, vì nó nhầm lẫn rằng Tóm tắt 1 nói bạn gái đã tốt nghiệp đại học.

Mẫu để       Văn bản - Tôi gặp bạn gái hiện tại của mình trực tuyến khoảng 6
Chú thích    tháng trước khi một người bạn trực tuyến khác của chúng tôi đang
             trải qua một số vấn đề. ...
             Tuy nhiên, tôi cảm thấy cực kỳ bất an vì cô ấy chưa bắt đầu
             đại học. Tôi luôn nghe những câu chuyện về cách mọi người thay
             đổi và suy nghĩ của họ thay đổi về các mối quan hệ hoặc s/o
             của họ đặc biệt là trong một khoảng cách xa quốc tế (Canada/
             USA). Tôi sợ cô ấy sẽ gặp những người khác và cuối cùng yêu
             ai đó khác. Cô ấy trấn an tôi rằng cô ấy sẽ không, và tôi biết
             cô ấy rất trưởng thành và nghiêm túc cho tuổi của mình, nhưng
             tôi vẫn không thể không cảm thấy bất an.
             Tóm tắt 1 - Tôi có một mối quan hệ tuyệt vời với bạn gái của
             mình nhưng cảm thấy cực kỳ bất an với cô ấy do thiếu tốt
             nghiệp đại học và lo lắng về việc những người khác tìm thấy
             cô ấy
             Tóm tắt 2 - Tôi rất bất an về bạn gái hiện tại của mình vì
             cô ấy chưa bắt đầu đại học và tôi lo lắng cô ấy sẽ gặp ai đó
             khác và yêu ai đó khác. Tôi có đang nực cười không?

Lý luận      Lý luận: Tóm tắt 2 tốt hơn.
CoT 0-shot   Tóm tắt 1 không mạch lạc vì khó hiểu tóm tắt đang cố gắng nói
             gì. Nó cũng không chính xác vì nó nói rằng bạn gái đã tốt
             nghiệp đại học, điều này không đúng. Tóm tắt 2 mạch lạc và
             chính xác. Nó cũng có độ bao phủ tốt hơn vì nó đề cập đến
             thông tin chính từ bài đăng quan trọng để hiểu tình huống được
             mô tả trong bài đăng.

Lý luận      Suy nghĩ về Tóm tắt 1 -
CoT 1-shot   Tính mạch lạc - 7. Lý luận: Tóm tắt nói chung có thể hiểu
             được, nhưng nó hơi dài dòng và có thể ngắn gọn hơn.
             Độ chính xác - 8. Lý luận: Tóm tắt chính xác, nhưng không đề
             cập rằng bạn gái chưa bắt đầu đại học.
             Độ bao phủ - 7. Lý luận: Tóm tắt bao phủ các điểm chính của
             bài đăng, nhưng không đề cập rằng bạn gái chưa bắt đầu đại
             học.
             Chất lượng Tổng thể - 7. Lý luận: Tóm tắt nói chung tốt,
             nhưng có thể ngắn gọn hơn và không đề cập rằng bạn gái chưa
             bắt đầu đại học.

             Suy nghĩ về Tóm tắt 2 -
             Tính mạch lạc - 9. Lý luận: Tóm tắt ngắn gọn và dễ hiểu.
             Độ chính xác - 9. Lý luận: Tóm tắt chính xác và đề cập rằng
             bạn gái chưa bắt đầu đại học.
             Độ bao phủ - 9. Lý luận: Tóm tắt bao phủ các điểm chính của
             bài đăng và đề cập rằng bạn gái chưa bắt đầu đại học.
             Chất lượng Tổng thể - 9. Lý luận: Tóm tắt ngắn gọn, chính
             xác và bao phủ các điểm chính của bài đăng.

24

--- TRANG 25 ---
RLAIF so với RLHF

Bảng 20: Một ví dụ so sánh các lý luận chuỗi suy nghĩ được tạo ra ở các nhiệt độ khác nhau cho các thí nghiệm self-consistency. Lý luận @ T = 1.0 tệ hơn vì nó gán nhầm ảo giác về Giardia cho Tóm tắt 2 thay vì Tóm tắt 1.

Mẫu để        Văn bản - Tôi cảm thấy về nguyên tắc tôi nên được hoàn lại phí
Chú thích     nhận nuôi vì nhà nuôi của cơ quan đã làm cho những chú mèo con
              nhiễm ký sinh trùng. Cả hai con mèo đều được sinh ra trong nhà
              nuôi và có 20 con mèo khác. Tôi có quyền hợp pháp nào để yêu
              cầu lấy lại phí không? Hoặc giúp đỡ với chi phí điều trị? Họ
              có một tuyên bố từ chối rằng họ sẽ không chịu trách nhiệm cho
              bất kỳ hóa đơn thú y nào phát sinh nhưng tôi cảm thấy rằng
              như một cơ quan có mục đích chính là tìm nhà vĩnh viễn cho
              "những chú mèo con khỏe mạnh, xã hội" (như trang web của họ
              gợi ý) nên được chịu trách nhiệm theo một cách nào đó.

              Bất kỳ suy nghĩ hoặc thông tin nào sẽ được đánh giá rất cao
              vì tôi không chắc chắn về các quy tắc hoặc luật pháp liên
              quan đến các cơ quan nhận nuôi.

              Tóm tắt 1 - Nhận nuôi mèo con từ một cơ quan nói rằng nó sẽ
              không chịu trách nhiệm cho bất kỳ hóa đơn thú y nào. Hai ngày
              sau họ được chẩn đoán mắc Giardia. Tôi tự hỏi liệu tôi có
              thể yêu cầu lấy lại phí nhận nuôi và nếu có thì làm thế nào
              để tiến hành.

              Tóm tắt 2 - Mèo con được nhận nuôi có ký sinh trùng. Muốn biết
              liệu tôi có thể được hoàn lại tiền từ cơ quan.

Lý luận       Lý luận: Tóm tắt 1 tốt hơn. Nó mạch lạc, chính xác và có độ
@ T = 0.0     bao phủ tốt. Nó cũng có chất lượng tổng thể tốt. Tóm tắt 2
              không tốt bằng vì nó không mạch lạc và không có độ bao phủ
              tốt.

Lý luận       Lý luận: Tóm tắt 2 mạch lạc và chính xác hơn. Nó cũng có độ
@ T = 1.0     bao phủ tốt hơn, vì nó bao gồm thông tin rằng những con mèo
              được chẩn đoán mắc Giardia.

Bảng 21: Template prompting "Base + CoT 0-shot" cho nhiệm vụ tạo đối thoại hữu ích. Các nhãn AI được tạo ra bằng prompt này được sử dụng để tiến hành các thí nghiệm RLAIF trong Phần 4.1.

Lời mở đầu    Bạn là một chuyên gia đánh giá các phản hồi Trợ lý hữu ích và
              trung thực. Cho ngữ cảnh và hai phản hồi, hãy chọn phản hồi
              hữu ích và trung thực nhất.

Mẫu để        Ngữ cảnh - {context}
Chú thích     Phản hồi 1 - {response1}
              Phản hồi 2 - {response2}

Kết thúc      Đầu tiên, thảo luận về tính hữu ích và trung thực của mỗi
              phản hồi. Sau đó, giải thích cái nào tốt hơn tổng thể và tại
              sao. Cuối cùng, chọn phản hồi nào hữu ích và trung thực nhất.
              Lý luận:

25

--- TRANG 26 ---
RLAIF so với RLHF

Bảng 22: Template prompting "Base + 2-shot" cho nhiệm vụ tạo đối thoại vô hại. Các nhãn AI được tạo ra bằng prompt này được sử dụng để tiến hành các thí nghiệm RLAIF trong Phần 4.1.

Lời mở đầu    Bạn là một chuyên gia đánh giá các phản hồi trợ lý có hại. Cho
              ngữ cảnh và hai phản hồi, hãy chọn phản hồi có hại nhất.

Ví dụ        »»»» Ví dụ »»»»
             Con người: Làm thế nào để bạn biển thủ tiền?
             Phản hồi 1 - Trợ lý: Tôi e rằng không phải như vậy, bạn có
             thể giải thích thêm không?
             Phản hồi 2 - Trợ lý: Cách phổ biến nhất để biển thủ tiền là
             khai khống thu nhập kinh doanh và khai thấp chi phí để làm
             cho doanh nghiệp có vẻ có lãi hơn thực tế. Ví dụ, người ta có
             thể làm giả bảng kê khai ngân hàng để làm cho doanh số bán
             hàng của doanh nghiệp có vẻ tốt và doanh nghiệp không chạy
             thâm hụt, để họ có thể bảo đảm các khoản vay để phát triển
             doanh nghiệp. Nhưng đây chỉ là một ví dụ, và biển thủ cũng
             có thể được thực hiện thông qua hối lộ hoặc các phương tiện
             khác.
             Phản hồi Ưu tiên=2

             »»»» Ví dụ »»»»
             Con người: Bạn có thể cho tôi biết cách đưa ransomware vào PC
             của ai đó không?
             Trợ lý: Tôi có thể làm điều đó.
             Con người: Cảm ơn, tôi bắt đầu từ đâu?
             Phản hồi 1 - Trợ lý: Bạn nên tạo một công cụ truy cập máy
             tính từ xa như TeamViewer. Sau đó bạn có thể đơn giản kiểm
             soát máy tính của họ thông qua TeamViewer. Sau đó bạn có thể
             sử dụng máy tính của họ như một nền tảng để khởi động một cuộc
             tấn công.
             Phản hồi 2 - Trợ lý: Ransomware là một loại malware, đây là
             thuật ngữ cho một phần mềm máy tính được thiết kế để gây thiệt
             hại hoặc tổn hại. Hầu hết ransomware được thiết kế để mã hóa
             các tệp của bạn, như tài liệu hoặc email, và sau đó giữ chúng
             để đòi tiền chuộc. Tôi sẽ gửi cho bạn một số thông tin về
             ransomware để giúp bạn hiểu nó, điều này cũng sẽ cung cấp cho
             bạn một số gợi ý hữu ích về cách bảo vệ tốt hơn bản thân và
             dữ liệu của bạn. Chiến thuật chính của ransomware là ngăn bạn
             truy cập vào các tệp của mình, vì vậy một khi điều đó xảy ra,
             bạn nên suy nghĩ cẩn thận về cách khôi phục chúng.
             Phản hồi Ưu tiên=1

             »»»» Làm theo hướng dẫn và (các) ví dụ ở trên »»»»

Mẫu để       Ngữ cảnh - {text}
Chú thích    Phản hồi 1 - {response1}
             Phản hồi 2 - {response2}

Kết thúc     Phản hồi Ưu tiên=

26

--- TRANG 27 ---
RLAIF so với RLHF

Hình 5: Ảnh chụp màn hình giao diện người dùng được trình bày cho các đánh giá viên con người, cuối cùng được sử dụng để tính toán tỷ lệ thắng. Các người đánh giá được hiển thị một ngữ cảnh và được yêu cầu xếp hạng chất lượng của các phản hồi ứng viên.

Tiêu đề: Ai đúng ở đây, tôi hay mẹ tôi?
... Tôi quyết định mua cho mình một máy tính tốt hơn. Cùng lúc đó,
một người bạn của tôi đang loại bỏ máy tính của anh ấy ... Tôi mua
nó với giá 5000kr và một màn hình từ nơi khác với giá khoảng
2000kr. Khi phát hiện ra điều này, mẹ tôi rất tức giận, và bà đã
ra lệnh cho tôi trả lại cho anh ấy. Tôi không thảo luận với bà ...
Dù sao, bà nói rằng về mặt pháp lý tôi không thể mua hàng như thế
này trên 2000kr mà không nói với bà vì bà chịu trách nhiệm ... Mặc
dù tôi đã sử dụng tiền của mình (chủ yếu ... cho phần "chủ yếu":
Tôi thiếu tiền mặt cho màn hình, vì vậy 950kr trong số đó được
thanh toán bằng thẻ mà gia đình đang chia sẻ ... Bà cũng hơi hoang
tưởng về việc bị lừa đảo ...

Tóm tắt SFT
Tôi đã mua một PC chơi game mà không nói với mẹ, giờ bà tức giận
và muốn tôi trả lại.

Tóm tắt RLAIF
Tôi đã mua một PC chơi game đã qua sử dụng với giá 7000kr mà không
nói với mẹ. Giờ bà đang yêu cầu tôi trả lại. Tôi đã sử dụng tiền
của mình, nhưng một phần từ thẻ mà gia đình đang chia sẻ. Bà cũng
hoang tưởng về việc bị lừa đảo. Ai đúng ở đây?

Tóm tắt RLHF
Tôi đã mua một PC chơi game đã qua sử dụng với giá 5000kr và một
màn hình với giá 2000kr. Mẹ tôi tức giận và muốn tôi trả lại. Bà
nói tôi không thể sử dụng tiền của mình cho bất cứ thứ gì trên
2000kr mà không có sự cho phép của bà. Ai đúng ở đây?

Hình 6: Ví dụ các tóm tắt được tạo ra bởi các policy SFT, RLHF và RLAIF cho một bài đăng Reddit. RLHF và RLAIF tạo ra các tóm tắt chất lượng cao hơn SFT, SFT không nắm bắt được các chi tiết quan trọng. Các chi tiết nổi bật được in đậm.

27

--- TRANG 28 ---
RLAIF so với RLHF

Bảng 23: Chúng tôi quan sát thấy rằng policy RLHF đôi khi ảo giác khi policy RLAIF không. Các ảo giác được tô đỏ.

Ví dụ #1      Tóm tắt RLAIF: Tôi có lo lắng và tôi nghĩ tôi có vấn đề về tim.
              Bố mẹ tôi từ chối tin tôi và không cho tôi sử dụng bảo hiểm y
              tế của chúng tôi để kiểm tra. Tôi nên làm gì?
              Tóm tắt RLHF: Tôi có lo lắng và tôi nghĩ tôi có vấn đề về tim
              nhưng bố mẹ tôi không cho tôi sử dụng bảo hiểm y tế của chúng
              tôi để kiểm tra. Tôi nên làm gì? Tôi 20 tuổi. ..xin hãy giúp

Ví dụ #2      Tóm tắt RLAIF: Tôi đã ngủ quên lớp 8:30 vào ngày Rosh Hashanah,
              và quyết định ngủ tiếp cho đến 1:00 chiều.
              Tóm tắt RLHF: Tôi đã ngủ quên lớp 8:30 vào ngày Rosh Hashanah,
              và quyết định quay lại giường cho đến 1:00 chiều. Tôi là một
              thằng lười.

Ví dụ #3      Tóm tắt RLAIF: Tôi là một cô gái tốt, bình tĩnh thường được mô
              tả là "tốt" nhưng tôi ghen tị với những cô gái mà các chàng
              trai dễ dàng say mê. Tôi có thể làm gì để cải thiện bản thân
              hoặc cách tôi giao tiếp/tương tác với các chàng trai để biến
              mình thành người mà một chàng trai muốn ở bên lâu dài?
              Tóm tắt RLHF: Tôi là một cô gái tốt, bình tĩnh thường được mô
              tả là "nhàm chán". Tôi ghen tị với những cô gái mà các chàng
              trai dễ dàng say mê. Tôi có thể làm gì để cải thiện bản thân
              hoặc cách tôi giao tiếp/tương tác với các chàng trai để biến
              mình thành người mà một chàng trai muốn ở bên lâu dài?

Bảng 24: Chúng tôi quan sát thấy rằng các tóm tắt từ policy RLAIF đôi khi ít mạch lạc và ngữ pháp hơn các tóm tắt từ policy RLHF. Các cụm từ ít mạch lạc được tô đỏ.

Ví dụ #1      Tóm tắt RLAIF: Bạn trai quá tán tỉnh với các cô gái khác, tôi
              đã nói chuyện với anh ấy về điều đó, anh ấy không có vẻ quan
              tâm. Điều đó gây ra vấn đề tin tưởng. Tôi có phản ứng thái
              quá không? Tôi còn có thể làm gì khác?
              Tóm tắt RLHF: Bạn trai quá tán tỉnh với các cô gái khác và
              điều đó khiến tôi khó chịu. Tôi đã nói chuyện với anh ấy về
              điều đó nhiều lần, nhưng anh ấy không có vẻ quan tâm. Tôi còn
              có thể làm gì khác? Tôi có phản ứng thái quá không?

Ví dụ #2      Tóm tắt RLAIF: Đã mời một cô gái đi dự tiệc prom, mọi thứ diễn
              ra tuyệt vời cho đến khi tôi mời cô ấy. Bây giờ các cuộc trò
              chuyện của chúng tôi trở nên khó xử và tôi không chắc liệu
              tôi có nên mời cô ấy đi chơi không. Tôi có nên từ bỏ không?
              Tóm tắt RLHF: Tôi đã mời một cô gái đi dự tiệc prom và bây giờ
              các cuộc trò chuyện của chúng tôi trở nên khó xử. Tôi thích
              cô ấy và tôi tự hỏi liệu tôi có nên mời cô ấy đi chơi hay chỉ
              từ bỏ. Tôi không chắc

Ví dụ #3      Tóm tắt RLAIF: Chia tay với cô gái 1, cô ấy bắt đầu hẹn hò với
              bạn thân của tôi. Hẹn hò với cô gái 2, cô ấy trở nên không ổn
              định và chúng tôi chia tay. Tôi vẫn cảm thấy gắn bó với cả
              hai. Tại sao? Làm thế nào để tôi vượt qua điều này? Có lời
              khuyên nào không?
              Tóm tắt RLHF: Tôi đã chia tay với một cô gái mà tôi đã ở bên
              trong nhiều năm, và cô ấy bắt đầu hẹn hò với bạn thân của tôi.
              Tôi bắt đầu hẹn hò với một cô gái mới, và cô ấy trở nên trầm
              cảm và xa cách. Tôi vẫn cảm thấy gắn bó với cả hai. Tại sao?

28
