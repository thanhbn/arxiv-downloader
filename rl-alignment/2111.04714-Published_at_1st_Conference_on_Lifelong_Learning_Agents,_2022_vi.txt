# 2111.04714.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2111.04714.pdf
# Kích thước tệp: 3232004 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản tại Hội nghị lần thứ 1 về Các Đại lý Học tập Suốt đời, 2022
QUAN ĐIỂM TỪ TẬP DỮ LIỆU VỀ
HỌC TĂNG CƯỜNG NGOẠI TUYẾN
Kajetan Schweighofer;xAndreas Radler;xMarius-Constantin Dinu;x;z
Markus HofmarcherxVihang PatilxAngela Bitto-Nemlingx;y
Hamid Eghbal-zadehxSepp Hochreiterx;y
xĐơn vị ELLIS Linz và Phòng thí nghiệm AI LIT,
Viện Học máy,
Đại học Johannes Kepler Linz, Áo
yViện Nghiên cứu Tiên tiến về Trí tuệ Nhân tạo (IARAI), Vienna, Áo
zNghiên cứu Dynatrace, Linz, Áo
TÓM TẮT
Việc ứng dụng Học tăng cường (RL) trong các môi trường thế giới thực có thể tốn kém hoặc có rủi ro do các chính sách dưới tối ưu trong quá trình đào tạo. Trong RL Ngoại tuyến, vấn đề này được tránh vì việc tương tác với môi trường bị cấm. Các chính sách được học từ một tập dữ liệu đã cho, điều này hoàn toàn quyết định hiệu suất của chúng. Mặc dù thực tế này, cách các đặc tính tập dữ liệu ảnh hưởng đến các thuật toán RL Ngoại tuyến vẫn ít được nghiên cứu. Các đặc tính tập dữ liệu được xác định bởi chính sách hành vi mẫu hóa tập dữ liệu này. Do đó, chúng tôi định nghĩa các đặc tính của các chính sách hành vi là khám phá để mang lại thông tin kỳ vọng cao trong tương tác của chúng với Quá trình Quyết định Markov (MDP) và là khai thác để có lợi nhuận kỳ vọng cao. Chúng tôi triển khai hai thước đo thực nghiệm tương ứng cho các tập dữ liệu được mẫu hóa bởi chính sách hành vi trong các MDP xác định. Thước đo thực nghiệm đầu tiên SACo được định nghĩa bởi các cặp trạng thái-hành động duy nhất được chuẩn hóa và nắm bắt việc khám phá. Thước đo thực nghiệm thứ hai TQ được định nghĩa bởi lợi nhuận quỹ đạo trung bình được chuẩn hóa và nắm bắt việc khai thác. Các đánh giá thực nghiệm cho thấy hiệu quả của TQ và SACo. Trong các thí nghiệm quy mô lớn sử dụng các thước đo đề xuất của chúng tôi, chúng tôi cho thấy rằng họ Deep Q-Network ngoại chính sách không hạn chế yêu cầu các tập dữ liệu với SACo cao để tìm một chính sách tốt. Hơn nữa, các thí nghiệm cho thấy rằng các thuật toán ràng buộc chính sách hoạt động tốt trên các tập dữ liệu với TQ và SACo cao. Cuối cùng, các thí nghiệm cho thấy, rằng Nhân bản Hành vi hoàn toàn bị ràng buộc bởi tập dữ liệu hoạt động cạnh tranh với các thuật toán RL Ngoại tuyến tốt nhất cho các tập dữ liệu với TQ cao.

Hình 1: Chúng tôi minh họa tác động của chính sách hành vi đối với phân phối của tập dữ liệu được mẫu hóa. Các cặp trạng thái-hành động của các tập dữ liệu khác nhau được mẫu hóa từ môi trường MountainCar sử dụng các chính sách hành vi khác nhau. Người ta có thể nhận thức bằng mắt thường về sự khác biệt giữa các tập dữ liệu này, mà chúng tôi nhằm mục đích định lượng bằng các thước đo được giới thiệu của chúng tôi (xem Phần 2.4).

Các tác giả đóng góp như nhau. Mã nguồn có sẵn tại github.com/ml-jku/OfﬂineRL

--- TRANG 2 ---
Được xuất bản tại Hội nghị lần thứ 1 về Các Đại lý Học tập Suốt đời, 2022
1 GIỚI THIỆU
Các vấn đề trung tâm trong RL là gán tín dụng (Sutton, 1984; Arjona-Medina et al., 2019; Holzleitner et al., 2020; Patil et al., 2020; Widrich et al., 2021; Dinu et al., 2022) và khám phá hiệu quả môi trường (Wiering và Schmidhuber, 1998; McFarlane, 2003; Schmidhuber, 2010). Khám phá có thể tốn kém do độ phức tạp tính toán cao, vi phạm các ràng buộc vật lý, nguy cơ hư hại vật lý, tương tác với các chuyên gia con người, v.v. (Dulac-Arnold et al., 2019). Hơn nữa, khám phá có thể gây nguy hiểm cho con người thông qua các tai nạn do xe tự lái gây ra, sự cố của máy móc sản xuất khi tối ưu hóa quy trình sản xuất, hoặc tổn thất tài chính cao khi áp dụng trong giao dịch hoặc định giá. Trong các trường hợp giới hạn, mô phỏng có thể giảm bớt những yếu tố này. Tuy nhiên, thiết kế các trình mô phỏng mạnh mẽ và chất lượng cao là một nhiệm vụ đầy thử thách, tốn thời gian và tài nguyên, và gây ra các vấn đề liên quan đến sự dịch chuyển phân phối và khoảng cách miền giữa môi trường thế giới thực và mô phỏng (Rao et al., 2020).

Đối mặt với những nhiệm vụ này, người ta có thể sử dụng khung RL Ngoại tuyến (Levine et al., 2020), còn được gọi là Batch RL (Lange et al., 2012), điều này cung cấp khả năng học các chính sách từ các tập dữ liệu được thu thập trước hoặc ghi nhật ký, mà không tương tác với môi trường (Agarwal et al., 2020; Fujimoto et al., 2019a;b; Kumar et al., 2020). Nhiều tập dữ liệu RL Ngoại tuyến như vậy đã tồn tại cho các vấn đề thế giới thực khác nhau (Cabi et al., 2019; Dasari et al., 2020; Yu et al., 2020). RL Ngoại tuyến chia sẻ nhiều đặc điểm với học sâu có giám sát, bao gồm, nhưng không giới hạn ở việc tận dụng các tập dữ liệu lớn. Một trở ngại cốt lõi là khái quát hóa cho dữ liệu chưa thấy, vì các mẫu được lưu trữ có thể không bao phủ toàn bộ không gian trạng thái-hành động. Trong RL Ngoại tuyến, vấn đề khái quát hóa có dạng dịch chuyển miền (Adler et al., 2020) trong quá trình suy luận. Ngoài tính không ổn định của môi trường hoặc tương tác đại lý với môi trường, sự dịch chuyển miền có thể được gây ra bởi chính quá trình thu thập dữ liệu (Khetarpal et al., 2020). Chúng tôi minh họa điều này bằng cách thu thập các tập dữ liệu sử dụng các chính sách hành vi khác nhau trong Hình 1.

Nhiều thuật toán RL Ngoại tuyến (Agarwal et al., 2020; Fujimoto et al., 2019a;b; Gulcehre et al., 2021; Kumar et al., 2020; Wang et al., 2020) đã được đề xuất để giải quyết những vấn đề này và đã cho thấy kết quả tốt. Các thuật toán ngoại chính sách nổi tiếng như Deep Q-Networks (DQN) (Mnih et al., 2013) có thể được sử dụng ngay trong RL Ngoại tuyến, bằng cách khởi tạo bộ đệm phát lại với một tập dữ liệu được thu thập trước. Tuy nhiên, trong thực tế, những thuật toán đó thường thất bại hoặc tụt lại xa so với hiệu suất mà chúng đạt được khi được đào tạo trong môi trường RL Trực tuyến. Hiệu suất giảm được quy cho các lỗi ngoại suy cho các cặp trạng thái-hành động chưa thấy và sự dịch chuyển miền kết quả giữa tập dữ liệu cố định đã cho và các trạng thái được chính sách đã học thăm (Fujimoto et al., 2019a; Gulcehre et al., 2021). Một số cải tiến thuật toán giải quyết những vấn đề này, bao gồm các ràng buộc chính sách (Fujimoto et al., 2019a;b; Wang et al., 2020), chính quy hóa giá trị hành động đã học (Kumar et al., 2020), và các thuật toán ngoại chính sách với ước tính giá trị hành động mạnh mẽ hơn (Agarwal et al., 2020). Trong khi các tập dữ liệu thống nhất đã được phát hành (Gulcehre et al., 2020; Fu et al., 2021) để so sánh các thuật toán RL Ngoại tuyến, công việc có cơ sở trong việc hiểu cách các đặc tính tập dữ liệu ảnh hưởng đến hiệu suất của các thuật toán vẫn còn thiếu (Riedmiller et al., 2021; Monier et al., 2020).

Do đó, chúng tôi nghiên cứu các đặc tính tập dữ liệu cốt lõi, và rút ra từ các thước đo lý thuyết nguyên lý đầu tiên liên quan đến khám phá và khai thác, đây là các thuộc tính chính sách được thiết lập tốt. Chúng tôi rút ra một thước đo khám phá dựa trên thông tin kỳ vọng của tương tác của chính sách hành vi trong MDP, entropy chuyển tiếp. Hơn nữa, chúng tôi cho thấy rằng đối với các MDP xác định, entropy chuyển tiếp bằng entropy chiếm chỗ. Thước đo khai thác của chúng tôi, lợi nhuận quỹ đạo kỳ vọng, là một khái quát hóa của lợi nhuận kỳ vọng của một chính sách. Chúng tôi cho thấy rằng những thước đo này có đảm bảo lý thuyết dưới các phép đồng cấu MDP (van der Pol et al., 2020), do đó thể hiện các đặc điểm ổn định nhất định dưới những biến đổi như vậy.

Để đặc trưng hóa các tập dữ liệu và so sánh chúng trên các môi trường và chính sách tạo ra, chúng tôi triển khai hai thước đo thực nghiệm tương ứng với các thước đo lý thuyết: (1) SACo, tương ứng với entropy chiếm chỗ, được định nghĩa bởi các cặp trạng thái-hành động duy nhất được chuẩn hóa, nắm bắt việc khám phá. (2) TQ, tương ứng với lợi nhuận quỹ đạo kỳ vọng, được định nghĩa bởi lợi nhuận trung bình được chuẩn hóa của các quỹ đạo trong tập dữ liệu, nắm bắt việc khai thác.

Chúng tôi đã tiến hành các thí nghiệm trên sáu môi trường khác nhau từ ba bộ môi trường khác nhau (Brockman et al., 2016; Chevalier-Boisvert et al., 2018; Young và Tian, 2019), để tạo ra các tập dữ liệu với các đặc tính khác nhau (xem Phần 3.1). Trên những tập dữ liệu này, 6750 thử nghiệm học RL đã được tiến hành, bao gồm một lựa chọn các thuật toán phổ biến trong môi trường RL Ngoại tuyến (Agarwal et al., 2020; Dabney et al., 2017; Fujimoto et al., 2019b; Gulcehre et al., 2021; Kumar et al., 2020; Mnih et al., 2013; Pomerleau, 1991; Wang et al., 2020). Chúng tôi đánh giá hiệu suất của chúng trên các tập dữ liệu với TQ và SACo khác nhau. Các biến thể của họ DQN ngoại chính sách (Mnih et al., 2013; Agarwal et al., 2020; Dabney et al., 2017) được phát hiện yêu cầu các tập dữ liệu với SACo cao để hoạt động tốt. Các thuật toán ràng buộc chính sách đã học về phía phân phối của chính sách hành vi hoạt động tốt cho các tập dữ liệu với TQ hoặc SACo cao hoặc các biến thể trung gian của chúng. Đối với các tập dữ liệu với TQ cao, Nhân bản Hành vi (BC) (Pomerleau, 1991) vượt trội hơn các biến thể của họ DQN và cạnh tranh với các thuật toán RL Ngoại tuyến hoạt động tốt nhất.

Tóm lại, các đóng góp của chúng tôi là:

--- TRANG 3 ---
Được xuất bản tại Hội nghị lần thứ 1 về Các Đại lý Học tập Suốt đời, 2022
(a) chúng tôi rút ra các thước đo lý thuyết nắm bắt khám phá và khai thác,
(b) chúng tôi chứng minh các đảm bảo lý thuyết cho tính ổn định của những thước đo này dưới các phép đồng cấu MDP,
(c) chúng tôi cung cấp một phương pháp hiệu quả để đặc trưng hóa các tập dữ liệu thông qua các thước đo thực nghiệm TQ và SACo,
(d) chúng tôi tiến hành một đánh giá thực nghiệm mở rộng về cách các đặc tính tập dữ liệu ảnh hưởng đến các thuật toán trong RL Ngoại tuyến.

2 ĐẶC TRƯNG HÓA CÁC TẬP DỮ LIỆU RL
Việc lựa chọn các thước đo phù hợp để đánh giá các tập dữ liệu RL và cho phép so sánh chúng đối với hiệu suất thuật toán là một câu hỏi nghiên cứu thách thức và mở (Riedmiller et al., 2021; Monier et al., 2020). Vì phân phối của tập dữ liệu được điều chỉnh bởi chính sách hành vi được sử dụng để mẫu hóa nó, chúng tôi nhằm tìm các thước đo cho các đặc tính của chính sách hành vi. Chúng tôi định nghĩa các đặc tính của chính sách hành vi là mức độ khai thác và khám phá của nó, và phân tích các thước đo tương ứng. Đây là lợi nhuận quỹ đạo kỳ vọng cho khai thác, và entropy chuyển tiếp cho khám phá trong các MDP ngẫu nhiên, điều này đơn giản hóa thành entropy chiếm chỗ cho khám phá trong các MDP xác định. Hơn nữa, chúng tôi nghiên cứu những thước đo lý thuyết này dưới các phép đồng cấu MDP (Ravindran và Barto, 2001; Givan et al., 2003; van der Pol et al., 2020; Abel, 2022) để phân tích sự phụ thuộc của chúng vào những biến đổi như vậy. Cuối cùng, chúng tôi triển khai các thước đo thực nghiệm tương ứng với lợi nhuận quỹ đạo kỳ vọng và entropy chiếm chỗ, TQ và SACo, có thể được tính toán cho một tập hợp các tập dữ liệu đã cho.

Chúng tôi định nghĩa thiết lập vấn đề của chúng tôi như một MDP hữu hạn là một bộ 5-tuple của (S;A;R;p;γ) của tập hữu hạn S với các trạng thái s (biến ngẫu nhiên St tại thời điểm t), A với các hành động a (biến ngẫu nhiên At), R với các phần thưởng r (biến ngẫu nhiên Rt+1), động lực học p(St+1=s′;Rt+1=r|St=s;At=a), và γ∈[0;1) như một yếu tố chiết khấu. Đại lý chọn các hành động a(St=s) dựa trên chính sách π, phụ thuộc vào trạng thái hiện tại s. Mục tiêu của chúng tôi là tìm chính sách tối đa hóa lợi nhuận kỳ vọng Gt=∑T k=0 γkRt+k+1. Trong RL Ngoại tuyến, chúng tôi giả định rằng một tập dữ liệu D={τi}B i=1, bao gồm B quỹ đạo, được cung cấp. Một quỹ đạo đơn bao gồm một chuỗi các tuple (s;a;r;s′).

2.1 ENTROPY CHUYỂN TIẾP NHƯ THƯỚC ĐO CHO KHÁM PHÁ
Chúng tôi định nghĩa tính khám phá của một chính sách π như thông tin kỳ vọng của tương tác giữa chính sách và một MDP nhất định. Trong khi chính sách chủ động chọn hành động tiếp theo của nó dựa trên trạng thái hiện tại, nó được chuyển tiếp vào một trạng thái tiếp theo và nhận một tín hiệu phần thưởng theo động lực học của MDP p(r;s′|s;a), điều này không thể bị ảnh hưởng bởi chính sách. Do đó, chính sách tương tác trong MDP có thể được xem như một quá trình ngẫu nhiên đơn tạo ra các chuyển tiếp (s;a;r;s′). Một chính sách là khám phá, nếu nó có thể tạo ra nhiều chuyển tiếp khác nhau với xác suất cao trong một MDP. Vì các chuyển tiếp chỉ có thể được quan sát thông qua tương tác giữa chính sách và MDP, tính khám phá của một chính sách chỉ có thể được định nghĩa kết hợp với một MDP cụ thể. Các chính sách hoạt động rất khám phá trong một MDP, có thể hoạt động ít khám phá hơn nhiều trong một MDP khác và ngược lại. Ví dụ, một chính sách không mở cửa có thể khám phá nhiều phòng rất kỹ lưỡng nếu tất cả các cửa đã được mở, nhưng sẽ bị mắc kẹt trong một phòng duy nhất nếu chúng đóng ban đầu.

Chúng tôi đo tính khám phá của một chính sách bằng entropy Shannon (Shannon, 1948) của các xác suất chuyển tiếp p(s;a;r;s′) dưới chính sách tương tác với MDP. Chúng tôi có thể viết lại các xác suất chuyển tiếp như p(s;a;r;s′) =p(s′;r|s;a)p(s;a). Động lực học p(r;s′|s;a) hoàn toàn phụ thuộc vào MDP, trong khi xác suất trạng thái-hành động p(s;a) phụ thuộc vào chính sách và MDP. Xác suất trạng thái-hành động p(s;a) thường được gọi là thước đo chiếm chỗ¹ρπ(s;a) (Neu và Pike-Burke, 2020; Ho và Ermon, 2016), vì nó mô tả cách chính sách chiếm không gian trạng thái-hành động.

Chúng tôi bắt đầu với entropy Shannon của các xác suất chuyển tiếp
H(p(s;a;r;s′)):=∑s;a;r;s′ p(s;a;r;s′)>0 p(s;a;r;s′) log(p(s;a;r;s′)); (1)
mà chúng tôi sẽ gọi là entropy chuyển tiếp. Entropy chuyển tiếp có thể được phân tích thành tổng có trọng số chiếm chỗ của các entropy của động lực học và chiếm chỗ (để có dẫn xuất chi tiết xem Eq. 9 trong Phần A.1 trong Phụ lục):
H(p(s;a;r;s′)) =∑s;a ρπ(s;a)H(p(r;s′|s;a)) +H(ρπ(s;a)): (2)

Một chính sách khám phá do đó nên nhằm tìm một sự cân bằng tốt giữa việc thăm tất cả các cặp trạng thái-hành động có thể một cách tương tự nhau, trong khi thăm các cặp trạng thái-hành động với động lực học ngẫu nhiên hơn p(r;s′|s;a) thường xuyên hơn. Mối liên kết này giữa

¹Để tránh các giả định bổ sung về MDP, chúng tôi tập trung phân tích này vào một giai đoạn duy nhất, xem Neu và Pike-Burke (2020).

--- TRANG 4 ---
Được xuất bản tại Hội nghị lần thứ 1 về Các Đại lý Học tập Suốt đời, 2022

khám phá tốt và thăm các động lực học ngẫu nhiên hơn cũng được tìm thấy trong thiết kế thí nghiệm tối ưu (Storck et al., 1995; Cohn, 1993). Lưu ý rằng có thể có những cách khác để định nghĩa khám phá. Một lựa chọn hợp lý là định nghĩa một thước đo tiện ích cho các chuyển tiếp để đạt được một mục tiêu hoặc học một thuộc tính nhất định của MDP. Khám phá cao hơn sau đó có nghĩa là các chuyển tiếp hữu ích hơn có khả năng xảy ra cao hơn. Tương tự, một lựa chọn khác là định nghĩa một thước đo khoảng cách cho các chuyển tiếp và định nghĩa khám phá như việc tăng khả năng mẫu hóa các chuyển tiếp có khoảng cách cao hơn với nhau. A priori, rất khó định nghĩa các khái niệm về tiện ích hoặc khoảng cách, do đó chúng tôi chuyển sang entropy Shannon.

MDPs Xác định Trong lớp MDPs này, chúng tôi có thể đơn giản hóa thước đo khám phá từ Eq. 2. Vì p(r;s′|s;a) là xác định, entropy động lực học H(p(r;s′|s;a)) bằng không và số hạng bên trái trong Eq. 2 biến mất như được hiển thị trong Eq. 11 trong Phần A.1 trong Phụ lục. Do đó, đối với các MDPs xác định, entropy chuyển tiếp đơn giản hóa thành entropy chiếm chỗ:
H(ρπ(s;a)):=∑s;a ρπ(s;a)>0 ρπ(s;a) log(ρπ(s;a)): (3)

Trong trường hợp đặc biệt này, một chính sách khám phá tối đa nếu tất cả các cặp trạng thái-hành động có thể được thăm với khả năng như nhau và do đó entropy chiếm chỗ là tối đa. Hình 2 đưa ra trực giác về phân phối chiếm chỗ và entropy chiếm chỗ kết quả của các chính sách khác nhau.

2.2 LỢI NHUẬN QUỸ ĐẠO KỲ VỌNG NHƯ THƯỚC ĐO CHO KHAI THÁC
Chúng tôi định nghĩa tính khai thác của một chính sách là mức độ hiệu quả về mặt lợi nhuận kỳ vọng mà nó tương tác với MDP. Chúng tôi đo mức độ khai thác của một chính sách bằng lợi nhuận kỳ vọng của nó gπ=E[Gt]. Hơn nữa, chúng tôi khái quát hóa lợi nhuận kỳ vọng cho các chính sách tùy ý (ví dụ: các chính sách không thể biểu diễn hoặc không Markovian như đã thảo luận trong Fu et al. (2021)), vì thiết lập RL Ngoại tuyến không đưa ra giả định nào về chính sách hành vi được sử dụng để thu thập tập dữ liệu. Do đó, chúng tôi định nghĩa tính khai thác trên một phân phối quỹ đạo T được quan sát từ các chính sách tùy ý như lợi nhuận quỹ đạo kỳ vọng gT, được cho bởi:
gT:=ET[∑∞ t=0 γtrt|rt∈T]: (4)

Trong lượt này, chúng tôi cũng có thể đánh giá các chính sách mà chúng tôi không thể biểu diễn hành vi tạo tập dữ liệu nhưng có thể tạo dữ liệu vô hạn. Điều này có liên quan khi xem xét các tập dữ liệu do con người tạo ra.

2.3 CẤU TRÚC CHUNG CỦA MDPS
Chúng tôi quan tâm đến tính ổn định của các thước đo đề xuất. Chúng tôi coi tính ổn định của các thước đo của chúng tôi như mức độ mà các thước đo thay đổi dưới các sửa đổi nhỏ của đầu vào. Các sửa đổi đối với đầu vào là những thay đổi nhỏ trong không gian trạng thái, không gian hành động hoặc động lực học của MDP. Để chính thức hóa các thay đổi của MDP, chúng tôi giới thiệu khái niệm về một MDP trừu tượng chung (AMDP) (xem Hình 3). Chúng tôi dựa trên định nghĩa của AMDP từ công việc trước đây của Sutton et al. (1999); Jong và Stone (2005); Li et al. (2006); Abel (2019); van der Pol et al. (2020); Abel (2022) bằng cách xem xét các trừu tượng trạng thái và hành động.

--- TRANG 5 ---
Được xuất bản tại Hội nghị lần thứ 1 về Các Đại lý Học tập Suốt đời, 2022

AMDP
MDP Gốc  Thang xám  
Biến đổi  
Dịch chuyển Màu  
Biến đổi  Xoay  
Biến đổi  

Hình 3: (a) Minh họa hai hình ảnh đồng cấu của một MDP trừu tượng (AMDP), (b) ví dụ về các biến đổi đồng cấu trên MiniGrid (Chevalier-Boisvert et al., 2018). Để biết chi tiết hơn xem Phần A.5 trong Phụ lục.

Định nghĩa 1. Cho hai MDPs M= (S;A;R;p;γ) và M̃= (S̃;Ã;R̃;p̃;γ) với các không gian trạng thái-hành động hữu hạn và rời rạc. Chúng tôi giả định tồn tại một MDP trừu tượng chung (AMDP) M̂= (Ŝ;Â;R̂;p̂;γ), trong đó M và M̃ là các hình ảnh đồng cấu của M̂. Chúng tôi định nghĩa một phép đồng cấu MDP bằng các hàm trừu tượng toàn ánh như φ :S→Ŝ và φ̃:S̃→Ŝ, với φ(s);φ̃(s̃)∈Ŝ cho các trừu tượng trạng thái và {fs:A→Â|s∈S} và {f̃s̃:Ã→Â|s̃∈S̃}, với fs(a);f̃s̃(ã)∈Â cho các trừu tượng hành động (xem Phụ lục Phần A.3 để biết thêm chi tiết về các giả định trên những hàm trừu tượng này và ý nghĩa của chúng đối với động lực học MDP). Để πφ(a|s) và π̃φ̃(ã|s̃) là các chính sách tương ứng của M,M̃ sao cho chúng ánh xạ qua φ;fs và φ̃;f̃s̃ đến cùng một chính sách trừu tượng π̂(â|ŝ) của AMDP chung M̂. Để Tπ và T̃π̃ là các phân phối quỹ đạo tương ứng của M,M̃, sao cho chúng ánh xạ qua φ;fs và φ̃;f̃s̃ đến cùng một phân phối quỹ đạo trừu tượng T̂π̂ của AMDP chung.

Chúng tôi sử dụng Def. 1 để rút ra một giới hạn trên của sự khác biệt trong entropy chuyển tiếp cho hai hình ảnh đồng cấu của cùng một AMDP chung. Để ngắn gọn, chúng tôi viết H(pπ),H(p̃π̃) và H(p̂π̂) cho các entropy chuyển tiếp được tạo ra bởi π,π̃ và π̂ tương ứng.

Định lý 1. Cho hai hình ảnh đồng cấu M và M̃ của một AMDP chung M̂ và các xác suất chuyển tiếp tương ứng của chúng pπ(s;a;r;s′),p̃π̃(s̃;ã;r;s̃′) và p̂π̂(ŝ;â;r;ŝ′) được tạo ra bởi các chính sách tương ứng π và π̃ của M và M̃ và chính sách trừu tượng π̂ của M̂ mà chúng ánh xạ tới. Sự khác biệt tuyệt đối tối đa trong entropy chuyển tiếp được giới hạn trên bởi:
|H(pπ)−H(p̃π̃)|≤max{H(pπ);H(p̃π̃)}−H(p̂π̂) (5)

Chứng minh. Chúng tôi sử dụng thực tế rằng các hình ảnh đồng cấu có entropy chuyển tiếp lớn hơn hoặc bằng entropy chuyển tiếp của AMDP chung. Để có chứng minh của tuyên bố này, xem Phần A.4 trong Phụ lục. Do đó, chúng tôi sử dụng H(pπ)≥H(p̂π̂) và H(p̃π̃)≥H(p̂π̂). Kết hợp những bất đẳng thức này dẫn đến Eq. 5.

Mệnh đề 1. Bất kỳ chính sách tương ứng nào π(a|s) và π̃(ã|s̃) của các hình ảnh đồng cấu M và M̃ của một AMDP chung M̂, ánh xạ đến cùng một chính sách trừu tượng π̂(â|ŝ), thể hiện cùng lợi nhuận kỳ vọng gπ=gπ̃. Hơn nữa, bất kỳ phân phối quỹ đạo tương ứng nào Tπ và T̃π̃ của các hình ảnh đồng cấu M và M̃ của một AMDP chung M̂, ánh xạ đến cùng một phân phối quỹ đạo trừu tượng T̂π̂, thể hiện cùng lợi nhuận quỹ đạo kỳ vọng gTπ=gT̃π̃.

Mệnh đề 1 theo từ các điều kiện trong Eq. 18 và Eq. 19 cho M và M̃ tương ứng. Để hoàn chỉnh, chúng tôi đề cập rằng đối với các biến đổi đẳng cấu của MDPs (tức là dưới một phép đồng cấu MDP với các hàm trừu tượng song ánh), trực tiếp theo rằng các thước đo được thảo luận trong Phần 2.1 và Phần 2.2 được bảo toàn.

Chúng tôi kết luận: (1) Các chính sách, được định nghĩa trên các hình ảnh đồng cấu của một AMDP chung mà ánh xạ đến cùng một chính sách trừu tượng, mang lại cùng lợi nhuận kỳ vọng. Các phân phối quỹ đạo trên các hình ảnh đồng cấu có phân phối quỹ đạo trừu tượng tương ứng, mang lại cùng lợi nhuận quỹ đạo kỳ vọng. (2) Từ Eq. 5, chúng tôi có thể chính thức xác nhận

--- TRANG 6 ---
Được xuất bản tại Hội nghị lần thứ 1 về Các Đại lý Học tập Suốt đời, 2022

trực giác về tính ổn định của các thước đo khám phá của chúng tôi giữa các hình ảnh đồng cấu của một AMDP chung: Nếu entropy chuyển tiếp của AMDP chung lớn nhất của hai MDPs tương ứng tăng, sự khác biệt trong entropy chuyển tiếp của chúng giảm.

2.4 CÁC THƯỚC ĐO THỰC NGHIỆM
Chúng tôi nhằm triển khai các thước đo thực nghiệm có thể áp dụng cho một tập hợp các tập dữ liệu đã cho và tương ứng với các thước đo lý thuyết về khám phá và khai thác của một chính sách được giới thiệu trong Phần 2.1 và Phần 2.2. Hơn nữa, những thước đo này được chuẩn hóa theo tham chiếu, để cho phép so sánh các tập dữ liệu được mẫu hóa từ các MDPs khác nhau.

SACo. Đầu tiên, chúng tôi triển khai một thước đo thực nghiệm tương ứng với thước đo lý thuyết về khám phá của một chính sách trong một MDP xác định, được cho bởi Eq. 3. Một cách để triển khai thước đo như vậy là ước lượng entropy ngây thơ của một tập dữ liệu Ĥ(D) hoặc ước lượng perplexity tương ứng của nó P̂P(D) =eĤ(D). Ước lượng entropy được giới hạn trên bởi logarit của các cặp trạng thái-hành động duy nhất trong tập dữ liệu log(us,a(D)), do đó Ĥ(D)≤log(us,a(D)). Nó theo rằng ước lượng perplexity được giới hạn trên bởi số lượng các cặp trạng thái-hành động duy nhất trong tập dữ liệu P̂P(D)≤us,a(D). Do đó chúng tôi dựa thước đo thực nghiệm SACo của chúng tôi trên các cặp trạng thái-hành động duy nhất us,a(D), có lợi ích là dễ diễn giải trong khi tương ứng với việc khám phá của chính sách. Ngoài ra, us,a(D) nắm bắt khái niệm bao phủ của không gian trạng thái-hành động cho một tập dữ liệu, được gọi đến trong tài liệu RL Ngoại tuyến (Fujimoto et al., 2019a; Agarwal et al., 2020; Kumar et al., 2020; Monier et al., 2020). Một phân tích lý thuyết sâu hơn về mối quan hệ giữa ước lượng entropy ngây thơ và đếm trạng thái-hành động duy nhất được cho trong Phần A.2 trong Phụ lục. Các cặp trạng thái-hành động duy nhất được chuẩn hóa với các cặp trạng thái-hành động duy nhất của một tập dữ liệu tham chiếu Dref của cùng MDP và cùng kích thước tập dữ liệu. Do đó thước đo thực nghiệm SACo của chúng tôi được định nghĩa như
SACo (D):=us,a(D)/us,a(Dref): (6)

Chúng tôi sử dụng tập dữ liệu phát lại làm tập dữ liệu tham chiếu Dref trong các thí nghiệm của chúng tôi, vì nó được thu thập trong suốt quá trình đào tạo của chính sách trực tuyến và được giả định có tập hợp đa dạng nhất các cặp trạng thái-hành động. Đếm các cặp trạng thái-hành động duy nhất của các tập dữ liệu lớn thường không khả thi do hạn chế thời gian và bộ nhớ, hoặc nếu các tập dữ liệu được phân phối trên các máy khác nhau. Chúng tôi sử dụng HyperLogLog (Flajolet et al., 2007) như một phương pháp đếm xác suất để tạo điều kiện cho khả năng áp dụng của SACo cho các điểm chuẩn quy mô lớn (xem Phần A.9.4 trong Phụ lục để biết chi tiết).

TQ. Thứ hai, chúng tôi triển khai một thước đo thực nghiệm ước tính lợi nhuận quỹ đạo kỳ vọng gT được cho bởi Eq. 4, đây là thước đo về mức độ khai thác của chính sách hành vi. Lợi nhuận quỹ đạo kỳ vọng gT được ước tính bằng lợi nhuận trung bình của các quỹ đạo trong tập dữ liệu ḡ(D) =1/B ∑B b=0 ∑Tb t=0 γtrb,t cho B quỹ đạo với độ dài tương ứng Tb của chúng. Để cho phép so sánh trên các MDPs, chúng tôi chuẩn hóa lợi nhuận quỹ đạo trung bình với hành vi tốt nhất và tệ nhất được quan sát trong cùng MDP, do đó tương ứng với chất lượng của các quỹ đạo tương đối với những cái đó. Do đó chúng tôi định nghĩa thước đo thực nghiệm TQ của chúng tôi như lợi nhuận quỹ đạo trung bình được chuẩn hóa:
TQ(D):=(ḡ(D)−ḡ(Dmin))/(ḡ(Dexpert)−ḡ(Dmin)); (7)

trong đó Dmin là một tập dữ liệu được thu thập bởi một chính sách hiệu suất tối thiểu πmin và Dexpert là một tập dữ liệu được thu thập bởi một chính sách chuyên gia πexpert. Trong suốt các thí nghiệm của chúng tôi, chúng tôi chọn tập dữ liệu được mẫu hóa từ chính sách tốt nhất được tìm thấy trong quá trình đào tạo trực tuyến như Dexpert và tập dữ liệu được mẫu hóa từ một chính sách ngẫu nhiên như Dmin.

3 TẠO TẬP DỮ LIỆU
Trong RL Ngoại tuyến, việc tạo tập dữ liệu không được hài hòa, cũng không được nghiên cứu kỹ lưỡng (Riedmiller et al., 2021). Đã được chỉ ra trong Kumar et al. (2020) rằng hiệu suất của Conservative Q-learning (CQL) được cải thiện bằng cách thay đổi chính sách hành vi của tập dữ liệu cơ bản. Tương tự, trong Gulcehre et al. (2021) việc trao đổi một chuyên gia với một chính sách hành vi chuyên gia có nhiễu đã thay đổi hiệu suất của các thuật toán so sánh. Hơn nữa, không có sự đồng thuận về chính sách hành vi nào là đại diện nhất để tạo ra các tập dữ liệu để so sánh hiệu suất của các thuật toán. Kumar et al. (2020) khẳng định rằng các tập dữ liệu được tạo ra bởi nhiều chính sách hành vi khác nhau phù hợp nhất với môi trường thế giới thực. Trái ngược với điều đó, Fujimoto et al. (2019a), khẳng định rằng một chính sách hành vi đơn lẻ là một sự phù hợp tốt hơn. Do đó, có một sự mơ hồ trong tài liệu RL Ngoại tuyến về những gì có thể là sơ đồ tạo dữ liệu chính xác để kiểm tra các thuật toán RL Ngoại tuyến. Chúng tôi xem xét những tập dữ liệu nào được sử dụng trong tài liệu để so sánh giữa các thuật toán, với mục đích đại diện cho các sơ đồ tạo tập dữ liệu nổi bật nhất trong đánh giá thực nghiệm của chúng tôi.

--- TRANG 7 ---
Được xuất bản tại Hội nghị lần thứ 1 về Các Đại lý Học tập Suốt đời, 2022

Agarwal et al. (2020) kiểm tra trên một tập dữ liệu bao gồm tất cả các mẫu đào tạo được thấy trong quá trình đào tạo trực tuyến của một đại lý DQN. Fujimoto et al. (2019a) tạo dữ liệu sử dụng một chính sách được đào tạo với một yếu tố khám phá. Fujimoto et al. (2019b) đánh giá trên nhiều tập dữ liệu, bao gồm một tập dữ liệu bao gồm tất cả các chuyển tiếp mà một thuật toán RL mẫu hóa trong quá trình đào tạo trực tuyến và dữ liệu được tạo ra sử dụng một chính sách được đào tạo. Gulcehre et al. (2021) sử dụng tập dữ liệu RL Unplugged (Gulcehre et al., 2020), bao gồm các tập dữ liệu khác nhau được thu thập bởi các chính sách khác nhau. Kumar et al. (2020) sử dụng ba tập dữ liệu được tạo ra bằng cách sử dụng một chính sách ngẫu nhiên, chuyên gia và hỗn hợp của chính sách chuyên gia và ngẫu nhiên, được tạo ra từ nhiều chính sách khác nhau. Wang et al. (2020) sử dụng các tập dữ liệu được tạo ra từ một mô hình được đào tạo trước dựa trên học chuyển giao từ các chuyên gia con người.

3.1 CÁC SƠ ĐỒ TẠO TẬP DỮ LIỆU
Chúng tôi tạo dữ liệu trong năm thiết lập khác nhau: 1) ngẫu nhiên, 2) chuyên gia, 3) hỗn hợp 4) nhiễu và 5) phát lại. Những sơ đồ tạo này được thiết kế để bao quát và mở rộng một cách có hệ thống các thiết lập trước đây từ tài liệu. Đối với mỗi tập dữ liệu, chúng tôi đã thu thập một số lượng mẫu được xác định trước bằng cách tương tác với môi trường tương ứng (xem Phần 4). Số lượng mẫu trong một tập dữ liệu được xác định bởi số lượng tương tác môi trường cần thiết để có được các chính sách chuyên gia thông qua một thuật toán RL Trực tuyến. Chúng tôi sử dụng DQN (Mnih et al., 2013) như một đường cơ sở cho thuật toán RL Trực tuyến, phục vụ như một chính sách hành vi chuyên gia để tạo ra và thu thập các mẫu, như được mô tả dưới đây. Chi tiết về cách chính sách trực tuyến được đào tạo được cho trong Phần A.9.2 trong Phụ lục.

•Tập dữ liệu Ngẫu nhiên. Tập dữ liệu này được mẫu hóa bởi một chính sách hành vi ngẫu nhiên. Tập dữ liệu như vậy được sử dụng để đánh giá trong Kumar et al. (2020). Nó phục vụ như một đường cơ sở ngây thơ cho việc thu thập dữ liệu.

•Tập dữ liệu Chuyên gia. Chúng tôi đào tạo một chính sách trực tuyến cho đến khi hội tụ và mẫu hóa với chính sách chuyên gia tham lam cuối cùng. Các tập dữ liệu như vậy được sử dụng trong Fu et al. (2021); Gulcehre et al. (2021); Kumar et al. (2020).

•Tập dữ liệu Hỗn hợp. Tập dữ liệu hỗn hợp được tạo ra sử dụng một hỗn hợp của tập dữ liệu ngẫu nhiên (~80%) và tập dữ liệu chuyên gia (~20%). Điều này tương tự như Fu et al. (2021); Gulcehre et al. (2021), nơi họ gọi tập dữ liệu như vậy là medium-expert.

•Tập dữ liệu Nhiễu. Tập dữ liệu nhiễu được tạo ra với một chính sách chuyên gia chọn các hành động ε-greedy với ε = 0.2. Tạo ra một tập dữ liệu từ một chính sách nhiễu cố định tương tự như quá trình tạo tập dữ liệu trong Fujimoto et al. (2019a;b); Kumar et al. (2020); Gulcehre et al. (2021).

•Tập dữ liệu Phát lại. Tập dữ liệu này là tập hợp tất cả các mẫu được tạo ra bởi chính sách trực tuyến trong quá trình đào tạo, do đó, bao gồm dữ liệu được tạo ra bởi nhiều chính sách. Tập dữ liệu như vậy được sử dụng trong Agarwal et al. (2020); Fujimoto et al. (2019b).

3.2 TẠO TẬP DỮ LIỆU TỪ QUAN ĐIỂM DỊCH CHUYỂN MIỀN
Tạo ra nhiều tập dữ liệu trong cùng MDP từ các chính sách khác nhau cũng có thể được diễn giải như một sự dịch chuyển miền giữa các phân phối của các chính sách. Do đó, chúng tôi muốn phân tích thêm những dịch chuyển miền nào có thể xảy ra khi tạo ra các tập dữ liệu RL. Tương tác của một chính sách trong một MDP nhất định tạo ra các chuyển tiếp (s;a;r;s′), với xác suất p(s;a;r;s′). Chúng tôi định nghĩa một sự dịch chuyển miền như một sự thay đổi của phân phối p(s;a;r;s′) thành p′(s;a;r;s′), tương tự như định nghĩa dịch chuyển miền trong thiết lập học có giám sát (Widmer và Kubat, 1996; Gama et al., 2014; Webb et al., 2018; Wouter, 2018; Kouw và Loog, 2019; Khetarpal et al., 2020; Adler et al., 2020). Bốn nguồn dịch chuyển miền riêng biệt có thể được tách ra bằng cách áp dụng quy tắc chuỗi của xác suất có điều kiện trên xác suất chuyển tiếp p(s;a;r;s′) =p(r|s;a;s′)p(s′|s;a)p(a|s)ρ(s), trong đó ρ(s) =∑a ρ(s;a) là độ chiếm chỗ trạng thái. Sự tách này không phải là duy nhất, nhưng mang lại chính sách và các xác suất có điều kiện được sử dụng trong định nghĩa của MDP. Lưu ý rằng chúng tôi đã tách động lực học p(r;s′|s;a) thành động lực học phần thưởng p(r|s;a;s′) và động lực học trạng thái p(s′|s;a) để tách thêm các lý do có thể cho một sự dịch chuyển miền. Chúng tôi xem xét các loại dịch chuyển miền sau trong RL:

•Dịch chuyển Động lực học Phần thưởng: p(r|s;a;s′) được thay đổi thành p′(r|s;a;s′), trong khi động lực học trạng thái p(s′|s;a), chính sách p(a|s) và độ chiếm chỗ trạng thái ρ(s) giữ nguyên. Điều này thay đổi lợi nhuận kỳ vọng dưới chính sách, nhưng độ chiếm chỗ của nó giữ nguyên.

•Dịch chuyển Động lực học Trạng thái: p(s′|s;a) được thay đổi thành p′(s′|s;a), trong khi động lực học phần thưởng p(r|s;a;s′) và chính sách p(a|s) giữ nguyên. Điều này thay đổi độ chiếm chỗ dưới chính sách, vì cùng một hành vi sẽ dẫn đến một phân phối kết quả khác nhau. Mặc dù không cần thiết, lợi nhuận kỳ vọng dưới chính sách có khả năng thay đổi dưới một độ chiếm chỗ khác.

•Dịch chuyển Chính sách: p(a|s) được thay đổi thành p′(a|s) trong khi động lực học phần thưởng p(r|s;a;s′) và động lực học trạng thái p(s′|s;a) giữ nguyên. Nói chung, các tập dữ liệu RL không được mẫu hóa từ cùng một chính sách hành vi. Do đó, sự dịch chuyển miền này là vốn có của các tập dữ liệu RL.

--- TRANG 8 ---
Được xuất bản tại Hội nghị lần thứ 1 về Các Đại lý Học tập Suốt đời, 2022

Độ chiếm chỗ trạng thái ρ(s) phụ thuộc vào động lực học trạng thái p(s′|s;a) và chính sách p(a|s) một cách không tầm thường. Sự phụ thuộc này được thấy khi công thức hóa phân phối trạng thái một cách đệ quy như ρ(s′) =∑s ρ(s)∑a p(a|s)p(s′|s;a). Lưu ý rằng một sự dịch chuyển trong phân phối trạng thái ban đầu hoặc phân phối trạng thái hấp thụ cũng có thể gây ra sự dịch chuyển miền của độ chiếm chỗ trạng thái. Mọi sự kết hợp của các dịch chuyển đã thảo luận cũng có thể xảy ra, nơi chúng tôi gọi sự đồng diễn ra của tất cả các loại dịch chuyển là dịch chuyển miền tổng quát. Lưu ý, rằng một dịch chuyển động lực học trạng thái và chính sách nói chung gây ra một sự thay đổi của xác suất trạng thái, nếu các hiệu ứng của chúng không cân bằng lẫn nhau. Cuối cùng, chúng tôi kết luận rằng việc tạo ra các tập dữ liệu với các chính sách khác nhau nói chung dẫn đến một sự dịch chuyển của phân phối tập dữ liệu, mà chúng tôi cũng quan sát thực nghiệm (xem Phần A.6 trong Phụ lục).

4 THÍ NGHIỆM
Chúng tôi tiến hành nghiên cứu của mình trên sáu môi trường xác định khác nhau từ nhiều bộ. Đây là hai môi trường kiểm soát cổ điển từ bộ OpenAI gym (Brockman et al., 2016), hai MiniGrid (Chevalier-Boisvert et al., 2018) và hai môi trường MinAtar (Young và Tian, 2019). Đối với hai bộ đầu tiên, 10⁵ mẫu được thu thập cho mỗi tập dữ liệu, trong khi 2×10⁶ mẫu được thu thập cho các môi trường MinAtar. Trên tất cả các môi trường (sáu), các sơ đồ tạo dữ liệu khác nhau (năm) và seeds (năm), chúng tôi đã tạo ra tổng cộng 150 tập dữ liệu.

Chúng tôi đào tạo chín thuật toán khác nhau phổ biến trong tài liệu RL Ngoại tuyến bao gồm Behavioral Cloning (BC) (Pomerleau, 1991) và các biến thể của DQN, Quantile-Regression DQN (QRDQN) (Dabney et al., 2017) và Random Ensemble Mixture (REM) (Agarwal et al., 2020). Hơn nữa, Behavior Value Estimation (BVE) (Gulcehre et al., 2021) và Monte-Carlo Estimation (MCE) được sử dụng. Cuối cùng, ba thuật toán RL Ngoại tuyến phổ biến rộng rãi ràng buộc chính sách đã học gần với phân phối của chính sách hành vi đã tạo ra tập dữ liệu, Batch-Constrained Q-learning (BCQ) (Fujimoto et al., 2019b), Conservative Q-learning (CQL) (Kumar et al., 2019) và Critic Regularized Regression (CRR) (Wang et al., 2020) được xem xét. Chi tiết về các triển khai cụ thể được cho trong Phần A.8 trong Phụ lục.

Các thuật toán được xem xét đã được thực hiện trên mỗi 150 tập dữ liệu cho năm seeds khác nhau. Chi tiết về đào tạo trực tuyến và ngoại tuyến được cho trong Phần A.9. Chúng tôi liên hệ hiệu suất của các chính sách tốt nhất được tìm thấy trong quá trình đào tạo ngoại tuyến với chính sách tốt nhất được tìm thấy trong quá trình đào tạo trực tuyến. Hiệu suất ω của chính sách tốt nhất được tìm thấy trong quá trình đào tạo ngoại tuyến được cho bởi ω(Doffline) = (ḡ(Doffline)−ḡ(Dmin))/(ḡ(Dexpert)−ḡ(Dmin)). Các chính sách được đánh giá trong môi trường sau các khoảng thời gian cố định trong quá trình đào tạo ngoại tuyến. Các quỹ đạo được mẫu hóa cho bước đánh giá mang lại lợi nhuận trung bình cao nhất đại diện cho tập dữ liệu Doffline.

Kết quả. Chúng tôi nhằm phân tích các thí nghiệm của mình thông qua lăng kính đặc tính tập dữ liệu. Hình 4 cho thấy TQ và SACo của các tập dữ liệu được thu thập, trên các seeds tạo tập dữ liệu và môi trường. Các tập dữ liệu ngẫu nhiên và hỗn hợp thể hiện TQ thấp, trong khi dữ liệu chuyên gia có TQ cao nhất trung bình. Ngược lại, dữ liệu chuyên gia thể hiện SACo thấp trung bình, trong khi các tập dữ liệu ngẫu nhiên và hỗn hợp đạt giá trị cao hơn. Tập dữ liệu phát lại cung cấp một sự cân bằng tốt giữa TQ và SACo. Hình A.17 trong Phụ lục trực quan hóa cách tạo ra tập dữ liệu ảnh hưởng đến không gian trạng thái-hành động được bao phủ. Trong Hình 5, chúng tôi đặc trưng hóa các tập dữ liệu được tạo ra sử dụng TQ và SACo. Mỗi điểm đại diện cho một tập dữ liệu, và mỗi biểu đồ phụ chứa các tập dữ liệu giống nhau. Hiệu suất của chính sách tốt nhất được tìm thấy trong quá trình đào tạo ngoại tuyến sử dụng một tập dữ liệu cụ thể được biểu thị bằng màu sắc của điểm tương ứng.

--- TRANG 9 ---
Được xuất bản tại Hội nghị lần thứ 1 về Các Đại lý Học tập Suốt đời, 2022

Những kết quả này chỉ ra rằng các thuật toán của họ DQN (DQN, QRDQN, REM) cần các tập dữ liệu với SACo cao để tìm một chính sách tốt. Hơn nữa, đã được phát hiện rằng BC hoạt động tốt chỉ khi các tập dữ liệu có TQ cao, điều này được mong đợi vì nó bắt chước hành vi được quan sát trong tập dữ liệu. BVE và MCE được phát hiện rất nhạy cảm với môi trường cụ thể và thiết lập tập dữ liệu, ưa chuộng các tập dữ liệu với SACo cao. Những thuật toán này không bị ràng buộc trong bước cải thiện chính sách cuối cùng của chúng, nhưng ước tính các giá trị hành động của chính sách hành vi thay vì chính sách tối ưu như trong DQN. Do đó, chúng có thể không tận dụng các tập dữ liệu với SACo cao cũng như các thuật toán từ họ DQN, nhưng gặp phải những hạn chế tương tự cho các tập dữ liệu với SACo thấp. BCQ, CQL và CRR thực thi các ràng buộc rõ ràng hoặc ngầm định trên chính sách đã học để gần với phân phối của chính sách hành vi. Những thuật toán này được phát hiện vượt trội hơn các thuật toán của họ DQN trung bình, nhưng đặc biệt đối với các tập dữ liệu với SACo thấp và TQ cao. Hơn nữa, các thuật toán với các ràng buộc đối với chính sách hành vi được phát hiện hoạt động tốt nếu các tập dữ liệu thể hiện TQ hoặc SACo cao hoặc giá trị vừa phải của TQ và SACo.

--- TRANG 10 ---
Được xuất bản tại Hội nghị lần thứ 1 về Các Đại lý Học tập Suốt đời, 2022

Chúng tôi quan sát từ Hình 5, rằng các thuật toán có liên quan chặt chẽ (ví dụ, họ DQN, hàng thứ hai) hoạt động tương tự khi được đào tạo trên các tập dữ liệu tương tự về TQ và SACo, đây là một thuộc tính thú vị để khảo sát trong công việc tương lai. Các biểu đồ phân tán giữa các cặp TQ, SACo và hiệu suất được cho trong Hình A.13 và Hình A.14 trong Phụ lục. Tất cả điểm số cho tất cả môi trường và thuật toán trên các tập dữ liệu được cho trong Phần A.12 trong Phụ lục. Chúng tôi cũng đã phân tích những thí nghiệm này với một phiên bản logarithmic của SACo (logarithmic trong tử số và mẫu số) trong Phần A.15 trong Phụ lục. Hơn nữa, chúng tôi đã phân tích những thí nghiệm này sử dụng các ước lượng entropy ngây thơ thay vì SACo trong Phần A.16 trong Phụ lục. Chúng tôi thấy rằng phiên bản được trình bày với SACo mang lại cách diễn giải đơn giản nhất về mặt hình ảnh và khái niệm cho các thí nghiệm của chúng tôi, trong khi tất cả các phiên bản mang lại kết quả tương tự về chất lượng.

5 THẢO LUẬN
Hạn chế. Chúng tôi đã tiến hành các thí nghiệm chính của mình trên các môi trường hành động rời rạc và cho thấy kết quả ban đầu trên các môi trường hành động liên tục trong Phần A.17 trong Phụ lục. Tuy nhiên, công việc tương lai sẽ cần điều tra các hiệu ứng của đặc tính tập dữ liệu đối với các không gian hành động liên tục trên một phạm vi rộng hơn các nhiệm vụ, thuật toán và sơ đồ thu thập dữ liệu khác nhau như đã thực hiện cho các không gian hành động rời rạc. Hơn nữa, nghiên cứu này được giới hạn ở các thuật toán không có mô hình, trong khi các thuật toán dựa trên mô hình gần đây đã được báo cáo đạt kết quả vượt trội trong các điểm chuẩn kiểm soát liên tục phức tạp công nghiệp (Swazinna et al., 2022). Vì lý do tính toán, chúng tôi giới hạn nghiên cứu này chỉ sử dụng thuật toán ngoại chính sách DQN như một chính sách trực tuyến. Một so sánh với một phương pháp trên chính sách như chính sách hành vi sẽ là thú vị, cũng như các thuật toán khám phá để tạo ra các tập dữ liệu đa dạng hơn; ví dụ sử dụng các mạng luồng tạo sinh (Bengio et al., 2021a;b). Đếm các cặp trạng thái-hành động duy nhất như một thước đo khám phá thực nghiệm đơn giản và có thể diễn giải, nhưng ước tính entropy rõ ràng sẽ đặc biệt có lợi trên các không gian trạng thái và hành động liên tục. Cuối cùng, mặc dù chúng tôi đã định nghĩa các loại dịch chuyển miền khác nhau, việc định nghĩa và phân tích một thước đo dịch chuyển miền phù hợp nằm ngoài phạm vi của công việc này.

Kết luận. Trong nghiên cứu này, chúng tôi đã rút ra hai thước đo lý thuyết, entropy chuyển tiếp tương ứng với tính khám phá của một chính sách và lợi nhuận quỹ đạo kỳ vọng, tương ứng với tính khai thác của một chính sách. Hơn nữa, chúng tôi đã phân tích các đặc điểm ổn định của những thước đo này dưới các phép đồng cấu MDP. Hơn nữa, chúng tôi đã triển khai hai thước đo thực nghiệm, TQ và SACo, tương ứng với lợi nhuận quỹ đạo kỳ vọng và entropy chuyển tiếp trong các MDPs xác định. Chúng tôi đã tạo ra 150 tập dữ liệu sử dụng sáu môi trường và năm chiến lược mẫu hóa tập dữ liệu trên năm seeds. Trên những tập dữ liệu này, hiệu suất của chín thuật toán không có mô hình đã được đánh giá qua các lần chạy độc lập, dẫn đến 6750 chính sách ngoại tuyến được đào tạo. Hiệu suất của các thuật toán ngoại tuyến cho thấy sự tương quan rõ ràng với các đặc tính khám phá và khai thác của tập dữ liệu, được đo bằng TQ và SACo. Chúng tôi thấy, rằng các thuật toán phổ biến trong RL Ngoại tuyến bị ảnh hưởng mạnh mẽ bởi các đặc tính của tập dữ liệu và hiệu suất trung bình trên các tập dữ liệu khác nhau có thể không đủ cho một so sánh công bằng. Nghiên cứu của chúng tôi do đó cung cấp một bản thiết kế để đặc trưng hóa các tập dữ liệu RL Ngoại tuyến và hiểu hiệu ứng của chúng đối với các thuật toán.

Lời cảm ơn Đơn vị ELLIS Linz, Phòng thí nghiệm AI LIT, Viện Học máy, được hỗ trợ bởi Bang Upper Austria. IARAI được hỗ trợ bởi Here Technologies. Chúng tôi cảm ơn các dự án AI-MOTION (LIT-2018-6-YOU-212), AI-SNN (LIT-2018-6-YOU-214), DeepFlood (LIT-2019-8-YOU-213), Medical Cognitive Computing Center (MC3), INCONTROL-RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG-872172), DL for GranularFlow (FFG-871302), AIRI FG 9-N (FWF-36284, FWF-36235), ELISE (H2020-ICT-2019-3 ID: 951847). Chúng tôi cảm ơn Audi.JKU Deep Learning Center, TGW LOGISTICS GROUP GMBH, Silicon Austria Labs (SAL), FILL Gesellschaft mbH, Anyline GmbH, Google, ZF Friedrichshafen AG, Robert Bosch GmbH, UCB Biopharma SRL, Merck Healthcare KGaA, Verbund AG, Software Competence Center Hagenberg GmbH, TÜV Austria, Frauscher Sensonic, AI Austria Reinforcement Learning Community, và NVIDIA Corporation.

TÀI LIỆU THAM KHẢO
D. Abel. A theory of state abstraction for reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):9876–9877, Jul. 2019. doi: 10.1609/aaai.v33i01.33019876.
D. Abel. A theory of abstraction in reinforcement learning. arXiv preprint arXiv:2203.00397, 2022.
T. Adler, J. Brandstetter, M. Widrich, A. Mayr, D. Kreil, M. Kopp, G. Klambauer, and S. Hochreiter. Cross-domain few-shot learning by representation fusion. arXiv preprint arXiv:2010.06498, 2020.
R. Agarwal, D. Schuurmans, and M. Norouzi. An optimistic perspective on offline reinforcement learning. arXiv preprint arXiv:1907.04543, 2020.

--- TRANG 11 ---
Được xuất bản tại Hội nghị lần thứ 1 về Các Đại lý Học tập Suốt đời, 2022

[Phần còn lại của tài liệu tham khảo và phụ lục được dịch tương tự với cùng phong cách trực tiếp, không có phần giải thích hoặc tóm tắt]
