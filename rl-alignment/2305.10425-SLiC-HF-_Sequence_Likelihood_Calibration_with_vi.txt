# 2305.10425.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2305.10425.pdf
# Kích thước tệp: 549794 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
SLiC-HF: Hiệu chỉnh Khả năng Trình tự với
Phản hồi của Con người
Yao Zhaoy
yaozhaoyz@google.comRishabh Joshiy
rishabhjoshi@google.comTianqi Liu
tianqiliu@google.com
Misha Khalmany
khalman@google.comMohammad Salehy
msaleh@google.comPeter J. Liuy
peterjliu@google.com
Google Deepmindy, Google Research
Tóm tắt
Học từ phản hồi của con người đã được chứng minh là hiệu quả trong việc căn chỉnh các mô hình ngôn ngữ với sở thích của con người. Các nghiên cứu trước đây thường dựa vào Học Tăng cường từ Phản hồi của Con người (RLHF), tối ưu hóa mô hình ngôn ngữ bằng cách sử dụng điểm thưởng được gán từ mô hình thưởng được huấn luyện trên dữ liệu sở thích của con người. Trong nghiên cứu này, chúng tôi chỉ ra cách Hiệu chỉnh Khả năng Trình tự (SLiC) được giới thiệu gần đây cũng có thể được sử dụng để học hiệu quả từ sở thích của con người (SLiC-HF). Hơn nữa, chúng tôi chứng minh điều này có thể được thực hiện với dữ liệu phản hồi của con người được thu thập cho một mô hình khác, tương tự như dữ liệu RL ngoài chính sách, ngoại tuyến. Các thí nghiệm đánh giá tự động và của con người trên nhiệm vụ tóm tắt TL;DR cho thấy SLiC-HF cải thiện đáng kể các đường cơ sở tinh chỉnh có giám sát (SFT). Hơn nữa, SLiC-HF trình bày một giải pháp thay thế cạnh tranh cho việc triển khai PPO RLHF được sử dụng trong các nghiên cứu trước đây trong khi đơn giản hơn nhiều để triển khai, dễ điều chỉnh hơn và hiệu quả tính toán hơn trong thực tế.

1 Giới thiệu
Trong khi việc mở rộng quy mô lớn các tham số mô hình và tính toán huấn luyện của các mô hình ngôn ngữ dựa trên Transformer đã dẫn đến khả năng học trong ngữ cảnh few-shot ấn tượng [5,6], việc tinh chỉnh học tăng cường từ phản hồi của con người (RLHF) có thể cải thiện đáng kể chất lượng tạo sinh như được đánh giá bởi con người. Điều này đã được quan sát ở tất cả các quy mô mô hình cho các nhiệm vụ tạo sinh ngôn ngữ downstream khác nhau, chẳng hạn như tóm tắt trừu tượng, đối thoại và viết sáng tạo [18, 3, 7, 13].

Đặc biệt cho tóm tắt, nhiều nghiên cứu đã chỉ ra rằng các bản tóm tắt được tạo bởi các mô hình được điều chỉnh với RLHF được ưa thích hơn so với các bản tóm tắt tham chiếu trong các bộ dữ liệu thường được sử dụng [18,10,8]. Các bản tóm tắt tham chiếu thường được khai thác từ các tài liệu web và có thể không có chất lượng cao nhất hoặc phong cách được ưa thích. Do đó, học có giám sát thuần túy, tức là tối đa hóa khả năng của các bản tóm tắt tham chiếu cho trước các tài liệu, bị giới hạn bởi chất lượng của các bản tóm tắt tham chiếu, do đó phản hồi bổ sung có thể cải thiện mô hình vượt ra ngoài các tài liệu tham chiếu. Các metric dựa trên tham chiếu thường được sử dụng, chẳng hạn như ROUGE [11], chỉ đo lường sự tương tự giữa văn bản được tạo bởi mô hình và văn bản tham chiếu. Các metric dựa trên tham chiếu này không thể đo lường sự cải thiện chất lượng vượt ra ngoài các bản tóm tắt tham chiếu.

Để triển khai RLHF, một mô hình thưởng, r(x;y), được huấn luyện trên dữ liệu sở thích của con người, (x;y0;y1;i) ∈ DHF, được thu thập thông qua đánh giá của con người so sánh bên cạnh, trong đó các người đánh giá được yêu cầu đánh giá cái nào trong hai bản tóm tắt y0 và y1 tốt hơn cho tài liệu x, tức là i ∈ {0;1}. Nếu chúng ta ký hiệu bản tóm tắt được ưa thích là y+ và cái kia là y−, phản hồi của con người trở thành (x;y+;y−) ∈ DHF. Một lựa chọn phổ biến

Preprint. Đang được xem xét.arXiv:2305.10425v1 [cs.CL] 17 May 2023

--- TRANG 2 ---
cho hàm mất mát huấn luyện của mô hình thưởng được sử dụng bởi RLHF là:
loss(r) = E(x;y+;y−)∈DHF[−log σ(r(x;y+) − r(x;y−))] (1)

Các thuật toán học tăng cường như PPO [17] sau đó được sử dụng để tinh chỉnh một mô hình được tinh chỉnh có giám sát (SFT) để tối đa hóa phần thưởng kỳ vọng được gán bởi mô hình thưởng r(x;y) [22,18]. Một thuật ngữ phạt KL thường được thêm vào hàm mất mát để ngăn mô hình RLHF phân kỳ quá xa khỏi chính sách có giám sát ban đầu.

Tuy nhiên, các thuật toán như RLHF-PPO đưa ra độ phức tạp đáng kể cho quá trình huấn luyện bằng cách thêm các mạng giá trị và thưởng riêng biệt có thể có kích thước tương đương với mạng chính sách. Chúng thường được giữ trong bộ nhớ để tối đa hóa tốc độ huấn luyện, điều này đối với một ngân sách bộ nhớ nhất định làm giảm đáng kể kích thước tối đa của mô hình có thể huấn luyện. Hơn nữa các bước tối ưu hóa chậm hơn đáng kể do việc sử dụng roll-outs trong vòng lặp huấn luyện, bao gồm việc lấy mẫu/giải mã từ mô hình. Việc điều chỉnh siêu tham số và phối hợp quá trình PPO cũng phức tạp hơn, đòi hỏi chuyên môn chuyên biệt.

Gần đây, một lớp phương pháp tương phản ở cấp độ trình tự khác [12,21] tìm cách căn chỉnh khả năng mô hình với một phần thưởng tùy ý, có thể không khả vi, đưa ra một giải pháp thay thế cho RL để tối ưu hóa phần thưởng kỳ vọng của các mẫu. Zhao et al. [21] đề xuất Hiệu chỉnh Khả năng Trình tự (SLiC) để căn chỉnh khả năng trình tự của mô hình ngôn ngữ, p(y|x), trên các trình tự được giải mã theo sự tương tự của chúng với các trình tự tham chiếu. Hàm mất mát hiệu chỉnh xếp hạng tương phản một trình tự tích cực y+ và một trình tự tiêu cực y−, khuyến khích mô hình P gán khối lượng xác suất nhiều hơn cho các trình tự tích cực so với các trình tự tiêu cực:

Lcal(θ) = max(0; −log P(y+|x) + log P(y−|x) + δ) (2)

Trong khi nghiên cứu SLiC ban đầu sử dụng sự tương tự với các tài liệu tham chiếu làm tiêu chí xếp hạng, ví dụ ROUGE [11] và khoảng cách embedding mô hình, nó có thể được thay thế bởi một hàm xếp hạng tùy ý, không tham chiếu, R(y0;y1;x) → {0;1}. Đặc biệt trong nghiên cứu này, chúng tôi sử dụng sở thích của con người làm hàm xếp hạng, hoặc bằng cách sử dụng dữ liệu sở thích ngoài chính sách D trực tiếp, hoặc bằng cách huấn luyện một mô hình xếp hạng dự đoán R(y0;y1;x) từ D.

Chúng tôi gọi việc sử dụng SLiC với hàm xếp hạng sở thích con người này là SLiC-HF và áp dụng nó bằng cách sử dụng dữ liệu phản hồi của con người được thu thập trong Stiennon et al. [18]. Các thí nghiệm của chúng tôi chỉ ra rằng SLiC-HF cũng dẫn đến chất lượng tóm tắt được cải thiện trên nhiệm vụ Tóm tắt Reddit TL;DR như được đánh giá bởi con người, mặc dù phản hồi này được thu thập cho các mô hình khác nhau, tương tự như RL ngoài chính sách, ngoại tuyến. Trong khi mô hình SFT T5-Large [15] (770M tham số) của chúng tôi hoạt động tương tự như mô hình SFT chỉ giải mã 6B của Stiennon et al. [18], chúng tôi có thể cải thiện mô hình của mình với SLiC-HF sao cho nó hoạt động ít nhất cũng tốt như mô hình RLHF-PPO 6B của Stiennon et al. [18] như được đánh giá bởi con người. Hơn nữa, việc áp dụng SLiC-HF cho mô hình SFT T5-XXL 11B tham số [15] cải thiện đáng kể kết quả.

Các đóng góp chính của bài báo này là chỉ ra:
• cách áp dụng SLiC để học từ sở thích của con người (SLiC-HF), một giải pháp thay thế đơn giản hơn, hiệu quả hơn nhưng cạnh tranh với RLHF
• dữ liệu phản hồi/sở thích từ mô hình khác (ngoài chính sách) có thể được tận dụng hiệu quả bởi SLiC-HF, làm cho việc thu thập dữ liệu phản hồi mới tốn kém cho mô hình của chúng tôi trở nên không cần thiết
• cung cấp một công thức SLiC-HF tổng quát dựa trên mô hình T5 mã nguồn mở vượt trội hơn RLHF trên nhiệm vụ tóm tắt Reddit TL;DR

2 Phương pháp
Trong nghiên cứu này, chúng tôi áp dụng SLiC [21] để cải thiện mô hình SFT bằng cách sử dụng dữ liệu sở thích của con người (x;y+;y−) ∈ DHF ngoài dữ liệu tinh chỉnh có giám sát tiêu chuẩn (x;yref) ∈ DSFT.

2.1 Hiệu chỉnh Khả năng Trình tự
Theo Zhao et al. [21], trước tiên chúng tôi tinh chỉnh một mô hình có giám sát, Pθft(y|x), trên (x;yref) ∈ DSFT, và sau đó căn chỉnh khả năng trình tự của mô hình SFT bằng cách sử dụng phương pháp SLiC tối ưu hóa hàm mất mát sau:

L(θ) = ∑ Lcal(θ;x;yref;{ŷ}m) + λLreg(θ;θft;x;yref) (3)

--- TRANG 3 ---
trong đó θ và θft là trọng số mô hình hiện tại và cố định SFT, Lcal và Lreg là các hàm mất mát hiệu chỉnh và điều chỉnh và {ŷ}m là m ứng viên được lấy mẫu từ mô hình SFT. Cụ thể hơn, chúng tôi chọn hàm mất mát hiệu chỉnh xếp hạng và hàm mất mát điều chỉnh cross-entropy vì tính đơn giản và phù hợp tự nhiên với dữ liệu phản hồi của con người theo cặp. Do đó hàm mất mát của SLiC-HF trở thành như sau:

L(θ) = max(0; −log Pθ(y+|x) + log Pθ(y−|x) + δ) − λ log Pθ(yref|x) (4)

Thuật ngữ đầu tiên là hàm mất mát hiệu chỉnh trong đó x là trình tự đầu vào, y+ và y− là các trình tự tích cực và tiêu cực, và δ là một siêu tham số cho lề của hàm mất mát xếp hạng. Thuật ngữ thứ hai là hàm mất mát cross-entropy, trong đó yref là một trình tự mục tiêu nào đó và λ là trọng số điều chỉnh. Hàm mất mát cross-entropy khuyến khích mô hình ở gần mô hình SFT, tương tự như thuật ngữ KL được sử dụng trong Stiennon et al. [18], tuy nhiên nó không cần một bản sao bổ sung của trọng số SFT. Thuật ngữ điều chỉnh KL cũng được khám phá trong Zhao et al. [21] nhưng được tìm thấy là hoạt động tương tự. Các lựa chọn của y+ và y− được thảo luận trong các tiểu mục 2.2 và 2.3. Các lựa chọn của mục tiêu điều chỉnh yref được thảo luận trong tiểu mục 2.4.

2.2 SLiC-HF với Lấy mẫu và Xếp hạng
Zhao et al. [21] lấy mẫu ứng viên {ŷ}m ~ Pθft(y|x) từ phần huấn luyện của DSFT, từ đó các cặp (tích cực, tiêu cực) được xác định. Chúng tôi gọi phương pháp này là SLiC-HF-sample-rank. Để xác định thứ hạng, chúng tôi xem xét hai mô hình text-to-text được huấn luyện từ dữ liệu sở thích của con người DHF:

Mô hình Thưởng Pointwise được huấn luyện: Tương tự như Askell et al. [2], chúng tôi nhị phân hóa mỗi cặp được xếp hạng thành một trình tự tích cực và một trình tự tiêu cực, như được hiển thị trong Hình 1. Khi huấn luyện mô hình thưởng, các trình tự đầu vào được định dạng như '[Context] ... [Summary] ... ' và các trình tự mục tiêu là 'Good' hoặc 'Bad'. Tại thời điểm suy luận, chúng tôi tính xác suất của token 'Good' ở phía decoder để chấm điểm từng ứng viên trong danh sách m ứng viên, và lấy mẫu m cặp tích cực/tiêu cực từ chúng.

Mô hình Xếp hạng Pairwise được huấn luyện: Như được hiển thị trong Hình 1, chúng tôi công thức hóa phản hồi của con người thành một bài toán xếp hạng theo cặp với định dạng text-to-text. Khi huấn luyện mô hình xếp hạng, các trình tự đầu vào được định dạng như '[Context] ... [Summary A] ... [Summary B]' và các trình tự mục tiêu là 'A' hoặc 'B'. Tại thời điểm suy luận, chúng tôi sử dụng quy trình kiểu tournament để xếp hạng các ứng viên trong một danh sách. Ví dụ, cho một danh sách 4 ứng viên c1;c2;c3;c4, chúng tôi đầu tiên xếp hạng c1;c2 và c3;c4 và sau đó xếp hạng winner(c1;c2);winner(c3;c4). Cho m ứng viên, mô hình xếp hạng được gọi m−1 lần và m−1 cặp tích cực/tiêu cực được tạo ra.

Mô hình Thưởng
[CONTEXT] tài liệu [SUMMARY] bản tóm tắt tích cực → Good
[CONTEXT] tài liệu [SUMMARY] bản tóm tắt tiêu cực → Bad

Mô hình Xếp hạng
[CONTEXT] tài liệu [SUMMARY A] bản tóm tắt tích cực [SUMMARY B] bản tóm tắt tiêu cực → A
[CONTEXT] tài liệu [SUMMARY A] bản tóm tắt tiêu cực [SUMMARY B] bản tóm tắt tích cực → B

Hình 1: Huấn luyện mô hình thưởng text-to-text và mô hình xếp hạng.

2.3 SLiC-HF Trực tiếp trên Phản hồi của Con người
Chúng tôi cũng xem xét một phương pháp đơn giản là hiệu chỉnh trực tiếp trên các trình tự tích cực và tiêu cực từ bộ dữ liệu phản hồi của con người, DHF, mà không cần mô hình xếp hạng hoặc thưởng. Chúng tôi gọi phương pháp này là SLiC-HF-direct. Ưu điểm rõ ràng của phương pháp này là tăng tính đơn giản và hiệu quả từ việc không huấn luyện hoặc sử dụng mô hình xếp hạng/thưởng. SLiC-HF-direct không phát sinh chi phí kỹ thuật bổ sung trong việc giải mã từ mô hình SFT và huấn luyện một mô hình để gắn nhãn các giải mã. Nhược điểm là phân phối dữ liệu phản hồi của con người ngoài chính sách có thể khác nhiều so với phân phối giải mã của mô hình SFT.

2.4 Thuật ngữ Điều chỉnh cho Hiệu chỉnh
Chúng tôi xem xét hai lựa chọn của trình tự mục tiêu yref cho điều chỉnh cross-entropy. Lựa chọn đầu tiên là sử dụng yref trong DSFT làm mục tiêu điều chỉnh. Lựa chọn thứ hai là sử dụng ứng viên được xếp hạng tốt nhất

--- TRANG 4 ---
từ {ŷ}m làm mục tiêu điều chỉnh. Các ứng viên được xếp hạng tốt nhất có thể được chọn bằng cách sử dụng mô hình xếp hạng hoặc mô hình thưởng.

3 Kết quả Thực nghiệm
3.1 Bộ dữ liệu
Chúng tôi nghiên cứu SLiC-HF trên bộ dữ liệu tóm tắt Reddit TL;DR từ Stiennon et al. [18]. Bộ dữ liệu chứa cả dữ liệu tinh chỉnh DSFT, dữ liệu phản hồi của con người DHF, cùng với các giải mã mô hình SFT và RLHF của họ mà chúng tôi sử dụng để so sánh với các mô hình của chúng tôi. DSFT là một phiên bản được lọc của bộ dữ liệu Reddit TL;DR [19]. Nó chứa 117k/6k/6k ví dụ trong các phần train, validation và test. DHF bao gồm 64k sở thích của con người trên các giải mã từ nhiều mô hình.

3.2 Siêu tham số Thực nghiệm
Chúng tôi tiến hành tất cả các thí nghiệm sử dụng các mô hình T5 [15] trong framework T5x [16]. Trong nghiên cứu ablation của chúng tôi, chúng tôi chọn mô hình T5-large (770M) làm mô hình tạo sinh và T5-XXL (11B) làm mô hình xếp hạng và mô hình thưởng¹. Chúng tôi huấn luyện tất cả các mô hình tạo sinh với batch size 32 và các mô hình xếp hạng/thưởng với batch size 128. Cả hai đều được huấn luyện với learning rate mặc định 10⁻³.

Chúng tôi huấn luyện mô hình xếp hạng và mô hình thưởng trên phần huấn luyện DHF, và chọn các checkpoint có độ chính xác cao nhất trên phần validation DHF. Chúng tôi tinh chỉnh các mô hình T5 trên phần huấn luyện DSFT, và chọn các checkpoint có perplexity thấp nhất trên phần validation DSFT.

Trong hiệu chỉnh, chúng tôi sử dụng learning rate 10⁻⁵ và lề xếp hạng δ = 1.0. Khi hiệu chỉnh các mô hình trên các giải mã của chính chúng với SLiC-HF-sample-rank, chúng tôi lấy mẫu 8 giải mã với temperature 0.7 và topk 40 từ các mô hình tạo sinh chỉ được tinh chỉnh.

Khi đánh giá các mô hình của chúng tôi, chúng tôi sử dụng beam-search với beam size 4. Để đánh giá tự động, chúng tôi tính tỷ lệ thắng của các giải mã mô hình so với các tài liệu tham chiếu của con người được đo bởi mô hình xếp hạng T5-XXL trên bộ dữ liệu validation DSFT. Tỷ lệ thắng được định nghĩa là phần trăm bản tóm tắt được giải mã bởi mô hình được mô hình xếp hạng ưa thích hơn so với các tài liệu tham chiếu của con người.

3.3 Độ chính xác của Mô hình Thưởng và Mô hình Xếp hạng
Phản hồi của con người và đánh giá của con người được thực hiện bởi các người đánh giá so sánh hai bản tóm tắt vì nó đáng tin cậy hơn so với đánh giá pointwise. Chúng tôi giả thuyết rằng mô hình xếp hạng có lợi thế so với mô hình thưởng vì bản chất pairwise của nó phù hợp hơn với nhiệm vụ. Chúng tôi huấn luyện và so sánh một mô hình xếp hạng T5-XXL và một mô hình thưởng T5-XXL (tiểu mục 2.2). Kết quả cho thấy mô hình xếp hạng của chúng tôi có độ chính xác 73.23% trên validation DHF, cao hơn khoảng 2% so với mô hình thưởng của chúng tôi có độ chính xác 71.34%².

3.4 Nghiên cứu Ablation SLiC
Chúng tôi tiến hành một tập hợp các thí nghiệm để ablation các cài đặt SLiC-HF so với các đường cơ sở. Chúng tôi sử dụng mô hình xếp hạng làm metric chính vì mối tương quan cao hơn với sở thích của con người được chứng minh trong Stiennon et al. [18]. Các cài đặt được chọn sau đó được xác minh bằng các thí nghiệm đánh giá của con người trong tiểu mục 3.5. Chúng tôi báo cáo số ROUGE chỉ để tham khảo và không sử dụng chúng để chọn mô hình. Có thể thấy sự giảm trong số ROUGE khi học từ phản hồi của con người vì nó có ít động lực để tương tự với các văn bản tham chiếu. Tương tự như RLHF trong Stiennon et al. [18], chúng tôi cũng quan sát thấy sự gia tăng độ dài trung bình của các mô hình và tiến hành một nghiên cứu kiểm soát độ dài trong tiểu mục 3.5.

3.4.1 SLiC-HF vs Tiếp tục Tinh chỉnh trên Dữ liệu Được lọc
Một cách đơn giản để học từ dữ liệu phản hồi của con người là chuyển đổi nó thành bộ dữ liệu SFT và tiếp tục tinh chỉnh trên đó. Nói chung, chúng tôi sử dụng phương pháp lọc có hiệu suất tương tự như các phương pháp kiểm soát

¹Chúng tôi thấy rằng các mô hình xếp hạng/thưởng T5 nhỏ hơn không hội tụ một cách đáng tin cậy trong thiết lập của chúng tôi.
²Độ chính xác của các mô hình xếp hạng và thưởng của chúng tôi tương tự như mô hình thưởng 6B trong Stiennon et al. [18]

--- TRANG 5 ---
Bảng 1: So sánh các phương pháp khác nhau để tận dụng dữ liệu phản hồi của con người. Tỷ lệ thắng của ranker là sở thích của mô hình xếp hạng T5-XXL trong việc chọn các giải mã mô hình so với các văn bản tham chiếu.

Metrics Ablation
phương pháp hình thức phản hồi của con người điều chỉnh # từ R1 / R2 / RL tỷ lệ thắng ranker
tham chiếu - - 27.11 - 50%
SFT - - 23.57 35.1/12.87/26.81 44.96%

tiếp tục SFT trên dữ liệu được lọc
positive từ dữ liệu HF - 31.22 33.02/11.27/24.57 51.65%
giải mã tốt nhất, bởi reward - 27.69 35.31/12.41/26.21 63.24%
giải mã tốt nhất, bởi ranking - 28.26 35.39/12.69/26.56 65.43%

SLiC-HF
SLiC-HF-direct mục tiêu SFT 41.03 33.76/11.58/24.72 82.92%
SLiC-HF-sample-rank, bởi reward mục tiêu SFT 38.44 33.87/11.48/24.81 82.42%
SLiC-HF-sample-rank, bởi reward giải mã tốt nhất 38.58 34.07/11.59/24.92 83.52%
SLiC-HF-sample-rank, bởi ranking mục tiêu SFT 37.96 34.49/11.92/25.35 86.21%
SLiC-HF-sample-rank, bởi ranking giải mã tốt nhất 37.50 34.69/12.03/25.54 85.51%

[1] nhưng sạch hơn để triển khai. Chúng tôi xem xét ba phương pháp để lọc dữ liệu cho việc tiếp tục tinh chỉnh:

• chỉ giữ các trình tự phản hồi tích cực của con người và loại bỏ các trình tự tiêu cực.
• giải mã 8 bản tóm tắt từ mô hình SFT, sử dụng mô hình xếp hạng để chọn 1 bản tóm tắt tốt nhất trong 8 bằng phương pháp xếp hạng kiểu tournament.
• giải mã 8 bản tóm tắt từ mô hình SFT, sử dụng mô hình thưởng để chọn 1 bản tóm tắt tốt nhất trong 8 bằng cách chấm điểm từng bản và lấy bản có điểm tối đa.

Như được hiển thị trong Bảng 1, trên bộ dữ liệu Reddit TL;DR, tiếp tục tinh chỉnh trên dữ liệu phản hồi tích cực của con người cải thiện tỷ lệ thắng của mô hình so với tham chiếu một chút từ 44.96% lên 51.65%. Trong thí nghiệm này, chúng tôi chọn sử dụng tất cả phản hồi của con người mà không lọc để có các mô hình tốt hơn vì điều này mô phỏng một tình huống thế giới thực nơi chúng ta có quyền truy cập vào một số dữ liệu phản hồi của con người mà không có kiến thức rõ ràng về chất lượng của nó. Tiếp tục tinh chỉnh trên 1 tốt nhất trong 8 cải thiện thêm tỷ lệ thắng so với tham chiếu lên 60%+ và sử dụng mô hình xếp hạng pairwise tốt hơn một chút so với mô hình thưởng pointwise để lọc.

3.4.2 Áp dụng SLiC-HF Trực tiếp trên Dữ liệu Phản hồi của Con người
Với SLiC-HF-direct, chúng tôi quan sát thấy rằng mặc dù hàm mất mát hiệu chỉnh giảm như mong đợi, độ dài trình tự tiếp tục tăng và không hội tụ về một giá trị ổn định. Mặt khác, SLiC-HF-sample-rank hội tụ một cách mạnh mẽ. Chúng tôi giả thuyết rằng SLiC-HF-direct dễ bị ảnh hưởng bởi các giải mã ngoài phân phối được tạo bởi các mô hình khác trong dữ liệu phản hồi của con người.

Khi sử dụng mô hình xếp hạng để chọn checkpoint tốt nhất cho SLiC-HF-direct, nó có sự gia tăng độ dài vừa phải và có tỷ lệ thắng 82.92% so với tham chiếu gần với SLiC-HF-sample-rank. Độ phức tạp kỹ thuật của SLiC-HF-direct gần như giống với việc tinh chỉnh một mô hình. Do đó, nó là một ứng viên tốt cho thí nghiệm nhanh trên phản hồi của con người.

3.4.3 Áp dụng SLiC-HF trên Các Giải mã Mô hình Được xếp hạng
Như được hiển thị trong Bảng 1, SLiC-HF-sample-rank sử dụng mô hình xếp hạng có khoảng 3% lợi thế trong tỷ lệ thắng so với tham chiếu so với SLiC-HF-sample-rank sử dụng mô hình thưởng. Kết quả này phù hợp với quan sát trong tiểu mục 3.3 rằng mô hình xếp hạng có sự đồng thuận cao hơn với sở thích của con người so với mô hình thưởng.

Đối với SLiC-HF-sample-rank sử dụng mô hình xếp hạng hoặc mô hình thưởng, việc sử dụng mục tiêu SFT hoặc giải mã được xếp hạng tốt nhất làm điều chỉnh không cho thấy nhiều khác biệt. Điều này cho thấy SLiC-HF-sample-rank có thể áp dụng ngay cả khi không có tài liệu tham chiếu sự thật cơ bản. Lợi ích từ tiếp tục tinh chỉnh trên các giải mã được xếp hạng tốt nhất trong Bảng 1 không cộng dồn với SLiC-HF.

--- TRANG 6 ---
3.5 Đánh giá của Con người
Chúng tôi tiến hành đánh giá của con người so sánh bên cạnh giữa nhiều hệ thống sử dụng crowd-sourcing.³ Cho một tài liệu và 2-4 bản tóm tắt, các người đánh giá được giao nhiệm vụ gán điểm chất lượng tổng thể pointwise cho mỗi bản tóm tắt, chọn xem bản tóm tắt có thực tế hay không, và chọn bản tóm tắt tốt nhất.

Mỗi nhiệm vụ được nhân bản và đánh giá bởi 3 người đánh giá khác nhau. Để loại bỏ thiên kiến, chúng tôi ẩn danh tất cả các mô hình và xáo trộn ngẫu nhiên thứ tự các bản tóm tắt cho mỗi nhiệm vụ. Chúng tôi tổng hợp các metric pointwise bằng cách tính trung bình các đánh giá trên tất cả 3 crowd workers, và chúng tôi tổng hợp metric lựa chọn bằng cách sử dụng bầu chọn đa số.

Mẫu đánh giá của con người và hướng dẫn đánh giá có thể được tìm thấy trong Phụ lục A.

3.5.1 Nghiên cứu Ablation SLiC-HF
Chúng tôi tiến hành đánh giá của con người so sánh bên cạnh 4 hướng để xác nhận kết quả ablation trong Bảng 1. 100 ví dụ từ tập validation được lấy mẫu từ tham chiếu, mô hình SFT, mô hình tiếp tục tinh chỉnh và mô hình SLiC-HF (SLiC-HF-sample-rank, sử dụng mô hình xếp hạng, được điều chỉnh trên giải mã tốt nhất). Như được hiển thị trong Bảng 3, SLiC-HF được chọn là mô hình tốt nhất 73% thời gian, có chất lượng trung bình cao hơn đáng kể, và là mô hình thực tế nhất. Nói chung, chất lượng trung bình phù hợp tốt với tỷ lệ thắng ranker từ Bảng 1.

Hình 2 cho thấy chất lượng được kiểm soát độ dài của các mô hình SFT, tiếp tục tinh chỉnh và SLiC-HF, rõ ràng cho thấy SLiC-HF được ưa thích. Nghiên cứu chất lượng được kiểm soát độ dài tương tự như các nghiên cứu được tiến hành trong Stiennon et al. [18], nơi điểm trung bình được tính toán trong các ví dụ được nhóm theo độ dài tương đối của chúng so với tham chiếu.

Bảng 2: Đánh giá của con người 4 hướng để so sánh tham chiếu, SFT tiếp tục SFT trên giải mã tốt nhất sử dụng mô hình xếp hạng, SLiC-HF với các cặp giải mã sử dụng mô hình xếp hạng.

tham chiếu SFT tiếp tục SFT SLiC-HF giống nhau
được chọn là ưa thích % 13% 5% 5% 73% 4%
chất lượng trung bình 3.17 3.10 3.32 3.82 -
là thực tế % 94.16% 94.85% 94.85% 96.56% -

[Hình 2: Biểu đồ cho thấy chất lượng trung bình được nhóm theo độ dài của SFT và SLiC-HF so với các đường cơ sở khác nhau.]

3.5.2 SLiC-HF vs RLHF-PPO
Việc triển khai và điều chỉnh đúng các siêu tham số cho các thuật toán RLHF-PPO trong Stiennon et al. [18] là các nhiệm vụ không tầm thường. Thay vì triển khai lại các thuật toán trong framework của chúng tôi, chúng tôi trực tiếp so sánh với các giải mã mô hình từ Stiennon et al. [18].

³Chúng tôi sử dụng Amazon Mechanical Turk để thiết lập nhiệm vụ và thuê các người đánh giá

--- TRANG 7 ---
Chúng tôi đầu tiên đánh giá mô hình SFT T5-large của chúng tôi so với mô hình SFT chỉ giải mã 6B của họ trong một đánh giá của con người so sánh bên cạnh hai hướng. Như được hiển thị trong Hình 3, SFT của chúng tôi có chất lượng và tỷ lệ thắng cao hơn một chút nhưng không có ý nghĩa thống kê.

Tiếp theo chúng tôi đánh giá hai biến thể của các mô hình SLiC-HF-sample-rank T5-large của chúng tôi so với mô hình RLHF-PPO 6B chỉ giải mã từ Stiennon et al. [18]. SLiC-HF-sample-rank với mô hình thưởng có hiệu suất tương tự như RLHF-PPO và SLiC-HF-sample-rank với mô hình xếp hạng tốt hơn RLHF-PPO. Các bản tóm tắt từ các mô hình SLiC-HF dài hơn một chút so với mô hình RLHF-PPO, và tỷ lệ thắng được kiểm soát độ dài của chúng tương tự như RLHF-PPO như được hiển thị trong Hình 3.

Bảng 3: Ba đánh giá của con người so sánh bên cạnh 2 hướng để so sánh đường cơ sở SFT của chúng tôi với [18], và các mô hình SLiC-HF của chúng tôi với mô hình RLHF-PPO. Kết quả có ý nghĩa thống kê được ký hiệu bằng *.

so sánh hệ thống sở thích của con người
hệ thống A (của chúng tôi) hệ thống B ([18]) tỷ lệ thắng chất lượng
phương pháp # từ # từ A B A B
SFT (770M gen) 23.7 SFT (sup6B) 24.6 56% 44% 3.59 3.48
SLiC-HF (700M gen, 11B ranking) 36.9 RLHF (sup6B_rm6B) 33.0 66%* 34%* 3.85* 3.61*
SLiC-HF (700M gen, 11B reward) 38.4 RLHF (sup6B_rm6B) 33.0 56% 44% 3.78 3.7

[Hình 3: Biểu đồ cho thấy chất lượng trung bình được nhóm theo độ dài của SFT và SLiC-HF so với các đường cơ sở khác nhau.]

3.6 Mở rộng quy mô SLiC
Bảng 4: Hiệu ứng của việc mở rộng quy mô tham số mô hình và số lượng ứng viên cho SLiC-HF-sample-rank.

Metrics Ablation
phương pháp # params m # từ R1 / R2 / RL tỷ lệ thắng ranker
SFT 770M 8 23.57 35.1/12.87/26.81 44.96%
SFT 11B 8 24.07 36.45/14.11/28.38 62.34%
SLiC-HF 770M 8 37.96 34.49/11.92/25.35 86.21%
SLiC-HF 770M 64 40.53 34.14/11.70/25.11 86.41%
SLiC-HF 11B 8 36.90 35.83/12.87/26.63 96.10%

Chúng tôi nghiên cứu 2 cách mở rộng quy mô SLiC-HF-sample-rank: (1) mở rộng quy mô tham số mô hình tạo sinh, (2) mở rộng quy mô số lượng ứng viên được giải mã m. Như được hiển thị trong Bảng 4, việc mở rộng quy mô mô hình tạo sinh từ 770M lên 11B cải thiện đáng kể cả mô hình SFT và mô hình SLiC-HF. Mặt khác, việc mở rộng quy mô m từ 8 lên 64 không giúp ích nhiều.

4 Thảo luận thêm về SLiC-HF vs. RLHF-PPO
4.1 Hiệu quả Tính toán/Bộ nhớ và Tính song song
Chúng tôi tóm tắt sự khác biệt về hiệu quả tính toán và bộ nhớ giữa SLiC-HF và RLHF-PPO trong Bảng 5.

--- TRANG 8 ---
Bảng 5: So sánh hiệu quả tính toán và bộ nhớ. p biểu thị số lượng tham số trong mạng chính sách;

RLHF-PPO [18] SLiC-HF
decode-rank direct
Các mô hình phụ trợ reward, value, SFT ranking -
Các trình tự được giải mã 1M 800k -
Sử dụng bộ nhớ tham số cho huấn luyện 4p p p
Cập nhật tham số mỗi bước 2p p p
Giải mã song song trong batch toàn bộ tập huấn luyện -
Thưởng song song trong batch toàn bộ tập huấn luyện -
Caching mã hóa đầu vào không có -

Trong cả RLHF-PPO và SLiC-HF-sample-rank, chúng tôi huấn luyện một mô hình xếp hạng hoặc thưởng phụ trợ được sử dụng để đánh giá chất lượng của các bản tóm tắt. Tuy nhiên, Stiennon et al. [18] thấy rằng việc có các mạng chính sách và giá trị riêng biệt hoạt động tốt hơn đáng kể, và do đó đóng góp một mô hình phụ trợ bổ sung, cùng kích thước với mô hình thưởng được cập nhật cùng với các cập nhật chính sách.

Hơn nữa, các mô hình chính sách, giá trị, thưởng và SFT (tất cả cùng kích thước trong Stiennon et al. [18]) được sử dụng trong vòng lặp huấn luyện PPO. Chúng thường được giữ trong bộ nhớ phần cứng để đảm bảo các bước huấn luyện nhanh hơn. Trong khi đó trong SLiC-HF, các phần thưởng có thể được tính toán hoàn toàn song song và ngoại tuyến, do đó sử dụng 1/4 bộ nhớ cho trọng số mô hình trong quá trình huấn luyện. Những tiết kiệm bộ nhớ như vậy có thể được tái sử dụng để huấn luyện các mô hình lớn hơn.

Stiennon et al. [18] báo cáo sử dụng 1M episodes để tiến hành huấn luyện RLHF, tương ứng với khoảng cùng số lượng mẫu được giải mã được sử dụng trong SLiC-HF, (m = 8 mỗi ví dụ huấn luyện, 123,169 ví dụ). Tuy nhiên, trong thực tế, việc giải mã SLiC-HF có thể nhanh hơn đáng kể vì tất cả các mẫu được giải mã sử dụng cùng một chính sách cho phép giải mã hoàn toàn song song. Ngược lại, với PPO, chính sách được cập nhật mỗi batch, hạn chế tính song song của việc giải mã cho mỗi batch (512, trong [18]) vì việc giải mã tiếp theo bị chặn bởi các cập nhật chính sách. Hơn nữa, việc giải mã PPO xảy ra trong vòng lặp huấn luyện dẫn đến thời gian bước tối ưu hóa dài hơn nhiều. Trong khi với SLiC-HF, thời gian bước tương tự như tinh chỉnh, nhanh hơn đáng kể vì không có việc giải mã trong vòng lặp huấn luyện.

Ngoài những lợi ích tính song song giải mã đáng kể, SLiC-HF có thể sử dụng các tối ưu hóa caching mã hóa đầu vào đơn giản để giảm tính toán. Vì các m giải mã được lấy mẫu từ cùng một chính sách SFT, các trạng thái được mã hóa trình tự đầu vào có thể được cache thay vì tính toán lại. Trong tóm tắt và các nhiệm vụ khác liên quan đến ngữ cảnh dài, điều này có thể quan trọng vì độ dài trình tự đầu vào có xu hướng dài hơn nhiều so với đầu ra.

SLiC-HF có lợi thế tính song song tương tự trong việc tính toán phần thưởng mỗi episode so với RLHF vì việc xếp hạng có thể được tính toán bên ngoài vòng lặp huấn luyện thay vì bên trong.

4.2 Xếp hạng Pairwise vs Mô hình thưởng
Các thuật toán RL tìm cách tối đa hóa phần thưởng kỳ vọng của các trajectory, trong trường hợp này là đánh giá của con người về chất lượng của các bản tóm tắt mô hình. Hàm thưởng này thường được giả định là pointwise, trong khi dữ liệu sở thích của con người được thu thập theo cặp để cải thiện độ tin cậy. Do đó có nhiễu được đưa vào trong việc chuyển đổi đánh giá pairwise thành phần thưởng pointwise, có thể được ước tính là sự khác biệt trong độ chính xác xếp hạng như trong tiểu mục 3.3. Vì SLiC-HF chỉ quan tâm đến thứ hạng tương đối của hai bản tóm tắt, nhiễu pairwise-to-pointwise này được tránh và chúng tôi phỏng đoán điều này giúp SLiC-HF (Bảng 1, Hình 3).

4.3 Giá trị của Các trạng thái và Hành động trong Ngôn ngữ
Đối với nhiều nhiệm vụ được giải quyết bằng RL, phần thưởng có thể được thu thập ở cuối một trajectory (như trong nhiều trò chơi Atari) và việc quy gán phần thưởng cuối cùng cho các hành động cụ thể có thể rất quan trọng trong việc học giải quyết một nhiệm vụ. Thông thường khi RL được áp dụng cho ngôn ngữ như trong tài liệu RLHF, trạng thái là tiền tố của văn bản hiện tại và các hành động tương ứng với việc chọn token tiếp theo. Vai trò của hàm giá trị là ước tính độ tốt của một trajectory (ví dụ bản tóm tắt) từ một tiền tố/đầu vào, đây là một nhiệm vụ trực quan rất khó khăn đối với các người đánh giá con người, và do đó RL cũng có thể bị ảnh hưởng bởi nhiễu ước tính hàm giá trị. Ngược lại, SLiC-HF không dựa vào một sub-model như vậy và chỉ sử dụng tín hiệu sở thích sạch hơn để thúc đẩy các cập nhật tham số và dẫn đến điều mà chúng tôi phỏng đoán là tối ưu hóa ổn định hơn.

--- TRANG 9 ---
5 Nghiên cứu liên quan
RL đã được sử dụng để tối ưu hóa phần thưởng tùy ý trong tạo sinh ngôn ngữ như BLEU cho dịch thuật [20] và ROUGE cho tóm tắt [14]; tuy nhiên, trong khi những metric đó được cải thiện, đánh giá của con người về chất lượng bị ảnh hưởng do sự không phù hợp của các metric.

Trong nỗ lực căn chỉnh tốt hơn hàm thưởng với đánh giá của con người, nhiều nghiên cứu đã sử dụng RL để căn chỉnh các mô hình ngôn ngữ với một mô hình thưởng được huấn luyện để dự đoán các đánh giá của con người được thu thập cẩn thận [22,18,13] sử dụng tóm tắt như một proof-of-concept ban đầu. Một thuật ngữ phạt KL, lần đầu được sử dụng trong Jaques et al. [9], được sử dụng như điều chỉnh để ngăn mô hình được điều chỉnh khỏi rời xa mô hình có giám sát ban đầu, và cũng được sử dụng trong SLiC [21].

Liu et al. [12] đề xuất BRIO, có ý định tương tự như SLiC [21] trong việc xếp hạng các giải mã được tạo bởi mô hình theo một hàm thưởng. BRIO huấn luyện các mô hình để căn chỉnh xác suất trình tự được chuẩn hóa độ dài của các giải mã được tạo với sự tương tự của chúng với tham chiếu được đo bằng ROUGE sử dụng một hàm mất mát list-wise. Ngược lại, và tương tự như RLHF, SLiC-HF thích ứng kỹ thuật để căn chỉnh với một mô hình được huấn luyện để dự đoán sở thích của con người cho hai bản tóm tắt thay vì sự tương tự của chúng với tham chiếu.

Bai et al. [4] thay thế dữ liệu sở thích của con người bằng đánh giá từ một mô hình ngôn ngữ lớn, và gọi đó là phản hồi AI (AIF). SLIC-HF cũng có thể được sử dụng với AIF chính xác theo cùng một cách và không phân biệt về nguồn gốc AI hoặc con người của phản hồi.

6 Kết luận
Trong nghiên cứu này, chúng tôi đề xuất SLiC-HF hiệu chỉnh khả năng trình tự trên dữ liệu phản hồi của con người. Các thí nghiệm của chúng tôi trên nhiệm vụ tóm tắt Reddit TL;DR cho thấy SLiC-HF cải thiện đáng kể các đường cơ sở tinh chỉnh có giám sát (SFT), và trình bày một giải pháp thay thế cạnh tranh cho việc triển khai RLHF-PPO của các nghiên cứu trước đây trong khi đơn giản hơn để triển khai, dễ điều chỉnh hơn và hiệu quả tính toán. Nghiên cứu tương lai có thể bao gồm việc nghiên cứu SLiC-HF trên các nhiệm vụ tạo sinh ngôn ngữ khác sử dụng các hàm thưởng khác và/hoặc phản hồi không phải con người.

Tài liệu tham khảo
[1] Roee Aharoni, Shashi Narayan, Joshua Maynez, Jonathan Herzig, Elizabeth Clark, và Mirella Lapata. 2022. mface: Multilingual summarization with factual consistency evaluation.

[2] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, và Jared Kaplan. 2021. A general language assistant as a laboratory for alignment.

[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.

[4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.

[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. 2020. Language models are few-shot learners. Trong Advances in Neural Information Processing Systems, tập 33, trang 1877–1901. Curran Associates, Inc.

[6] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, và Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.

[7] Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. 2022. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375.

[8] Tanya Goyal, Junyi Jessy Li, và Greg Durrett. 2022. News summarization and evaluation in the era of gpt-3. arXiv preprint arXiv:2209.12356.

[9] Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, José Miguel Hernández-Lobato, Richard E Turner, và Douglas Eck. 2017. Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control. Trong International Conference on Machine Learning, trang 1645–1654. PMLR.

[10] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110.

[11] Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. Trong Text summarization branches out, trang 74–81.

[12] Yixin Liu, Pengfei Liu, Dragomir Radev, và Graham Neubig. 2022. BRIO: Bringing order to abstractive summarization. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 2890–2903, Dublin, Ireland. Association for Computational Linguistics.

[13] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730–27744.

[14] Romain Paulus, Caiming Xiong, và Richard Socher. 2017. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304.

[15] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1–67.

[16] Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, và Andrea Gesmundo. 2022. Scaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189.

[17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, và Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.

--- TRANG 10 ---
[18] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, và Paul F Christiano. 2020. Learning to summarize with human feedback. Trong Advances in Neural Information Processing Systems, tập 33, trang 3008–3021. Curran Associates, Inc.

[19] Michael Völske, Martin Potthast, Shahbaz Syed, và Benno Stein. 2017. TL;DR: Mining Reddit to learn automatic summarization. Trong Proceedings of the Workshop on New Frontiers in Summarization, trang 59–63, Copenhagen, Denmark. Association for Computational Linguistics.

[20] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.

[21] Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, và Peter J Liu. 2023. Calibrating sequence likelihood improves conditional language generation. Trong The Eleventh International Conference on Learning Representations.

[22] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, và Geoffrey Irving. 2020. Fine-tuning language models from human preferences.

--- TRANG 11 ---
Hình 4: Ví dụ về nhiệm vụ đánh giá của con người.

A Đánh giá của Con người
Xem Hình 4 để có ví dụ về nhiệm vụ đánh giá của con người với 4 bản tóm tắt. Các bản tóm tắt được xáo trộn ngẫu nhiên cho mỗi ví dụ và các mô hình được ẩn danh.

--- TRANG 12 ---
