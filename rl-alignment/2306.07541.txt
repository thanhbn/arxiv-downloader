# 2306.07541.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2306.07541.pdf
# File size: 1074903 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
A Simple Unified Uncertainty-Guided Framework for
Offline-to-Online Reinforcement Learning
Siyuan Guo1,∗Yanchao Sun2Jifeng Hu1Sili Huang1Xing Chen1
Hechang Chen1Haiyin Piao3Lichao Sun4Yi Chang1
1School of Artificial Intelligence, Jilin University
2Department of Computer Science, University of Maryland
3Northwestern Polytechnical University4Lehigh University
∗guosyjlu@gmail.com
Abstract
Offline reinforcement learning (RL) provides a promising solution to learning an
agent fully relying on a data-driven paradigm. However, constrained by the limited
quality of the offline dataset, its performance is often sub-optimal. Therefore, it
is desired to further finetune the agent via extra online interactions before deploy-
ment. Unfortunately, offline-to-online RL can be challenging due to two main
challenges: constrained exploratory behavior andstate-action distribution shift .
To this end, we propose a Simple Unified u Ncertainty- Guided (SUNG) framework,
which naturally unifies the solution to both challenges with the tool of uncertainty.
Specifically, SUNG quantifies uncertainty via a V AE-based state-action visitation
density estimator. To facilitate efficient exploration, SUNG presents a practical
optimistic exploration strategy to select informative actions with both high value
and high uncertainty. Moreover, SUNG develops an adaptive exploitation method
by applying conservative offline RL objectives to high-uncertainty samples and
standard online RL objectives to low-uncertainty samples to smoothly bridge offline
and online stages. SUNG achieves state-of-the-art online finetuning performance
when combined with different offline RL methods, across various environments
and datasets in D4RL benchmark.1
1 Introduction
Offline reinforcement learning (RL) [ 27], which enables agents to learn from a fixed dataset without
interacting with the environment, has demonstrated significant success in many tasks where interaction
is expensive or risky [ 5,26,46]. However, such a data-driven learning paradigm inherently limits
the agent’s performance as it fully relies on a typically sub-optimal dataset [ 25,45]. To overcome
this issue, the RL community has been exploring the offline-to-online setting, which incorporates
additional online interactions to further finetune the pretrained offline RL agents [25, 32, 45, 49].
While the pretraining-finetuning paradigm in offline-to-online RL is natural and intuitive, delivering
the expected online improvement can be challenging in practice due to two main challenges: (1)
Constrained exploratory behavior : The conservative offline RL objectives, which require the agents
to perform actions within the support of the dataset during pretraining, can constrain the agents’
exploratory behavior for online interactions. As a result, they cannot achieve efficient exploration, and
thus fail to fully benefit from the trial-and-error paradigm in online RL. (2) State-action distribution
shift: During finetuning stage, agents may encounter unfamiliar state-action regimes that fall outside
1This work has been submitted to the IEEE for possible publication. Copyright may be transferred without
notice, after which this version may no longer be accessible.
Preprint. Under review.arXiv:2306.07541v2  [cs.LG]  21 Feb 2024

--- PAGE 2 ---
of the support of the dataset. Accordingly, the state-action distribution shift between offline and
online data occurs, leading to the well-known extrapolation error [ 13,23] during exploitation, which
may wipe out the good initialization obtained from the pretraining stage. Existing research typically
focuses on tackling one aspect of the challenges above by developing either efficient exploration
[49,33] or effective exploitation [ 25,48,45], resulting in limited offline-to-online improvement.
However, simultaneously addressing both challenges brings additional difficulty due to the inherent
conflict between exploration and exploitation: aggressive exploration may exacerbate the distribution
shift, while conservative exploitation can hinder agents from efficient online finetuning.
In this work, we aim to provide a unified solution to both challenges for better sample efficiency
during online finetuning. To achieve this, we recognize that both challenges are closely tied to
appropriate identification and treatment of unseen state-action pairs. Specifically, efficient exploration
necessitates discovery of informative unseen regions of state-action space. Furthermore, effective
exploitation needs precise characterization of out-of-distribution (OOD) data to avoid either overly
conservative or aggressive policy learning. As such, we leverage proper quantification and utilization
of the uncertainty [ 24,3,1,40] to naturally address both challenges and achieve the desired trade-off
between exploration and exploitation.
To this end, we present a Simple Unified u Ncertainty Guided (SUNG) framework, which provides a
generic solution for offline-to-online RL to enable finetuning agents pretrained with different offline
RL objectives. Unlike recent uncertainty-aware RL methods that rely on the ensemble technique for
uncertainty quantification [ 3,1,24], SUNG utilizes a simple yet effective approach that estimates the
state-action visitation density with a variational auto-encoder (V AE) [ 18] to quantify the state-action
uncertainty. In contrast to prior offline-to-online RL methods, SUNG simultaneously addresses
both challenges by leveraging the tool of uncertainty. Concretely, we develop a practical optimistic
exploration strategy that follows the principle of optimism in the face of uncertainty [ 4,2]. The main
idea here is to select those state-action pairs with both high value and high uncertainty for efficient
exploration. We also propose an adaptive exploitation method to handle the state-action distribution
shift by identifying and constraining OOD samples. The key insight is to leverage conservative offline
RL objectives for high-uncertainty samples, and standard online RL objectives for low-uncertainty
samples. This enables agents to smoothly adapt to changes in the state-action distribution. Moreover,
we utilize offline-to-online replay buffer (OORB) [ 48] to incorporate offline and online data during
finetuning. Notably, the insights of SUNG are generally applicable and can be combined with most
model-free offline RL methods in principle. Empirically, SUNG benefits from consideration of
uncertainty to guide both exploration and exploitation, thereby exceeding state-of-the-art methods
on D4RL benchmarks [ 10], with 14.54% and 14.80% extra averaged offline-to-online improvement
compared to the best baseline in MuJoCo and AntMaze domains, respectively.
Summary of Contributions. (1) We propose a generic framework SUNG for sample-efficient
offline-to-online RL, which can be combined with existing offline RL methods, such as TD3+BC
[11] and CQL [ 23]. (2) We introduce an optimistic exploration strategy via bi-level action selection
to select informative actions for efficient exploration. (3) We develop an adaptive exploitation method
with OOD sample identification and regularization to smoothly bridge offline RL and online RL
objectives. (4) Experimental results demonstrate that SUNG outperforms the previous state-of-the-art
when combined with different offline RL methods, across various types of environments and datasets.
2 Related Work
Offline RL. Offline RL [ 27] aims at learning a policy solely from a fixed offline dataset. Most prior
works in offline RL have been dedicated to addressing the extrapolation error due to querying the
value function with OOD actions [ 13,23]. As such, it is critical to constrain the learned policy
to perform actions within the support set of the offline dataset. Common strategies include policy
constraint [ 13,22,11,39], value regularization [ 23,31,20,3], etc. However, previous works have
shown that the the performance of offline RL agent is typically limited by the quality of the datasets
[34, 25], which prompts further investigation for the offline-to-online setting.
Offline-to-Online RL. Offline-to-online RL involves pretraining with offline RL and finetuning
via online interactions. Some offline RL methods naturally support further online finetuning with
continual offline RL objectives [ 39,31,21]. However, they typically tend to be too conservative and
yield limited performance improvement [ 44]. Besides, some offline-to-online RL approaches are
2

--- PAGE 3 ---
designed for one specific offline RL method [ 49,35,47,30]. For example, ODT [ 49] introduces the
max-entropy RL for finetuning DT [ 6]. ABCR [ 47] proposes to adaptively loosen the conservative
objectives of TD3+BC [ 11]. In contrast, we focus on the generic offline-to-online RL framework that
can be combined with different offline RL methods [ 25,33,48,45]. Prior works typically address
the challenges of exploration limitation and state-action distribution shift. For the former, O3F [ 33]
utilizes the knowledge in value functions to guide exploration, but it is not compatible with value
regularization based offline RL methods. For the latter, BR [ 25] utilizes the ensemble technique
together with a balanced replay buffer that prioritizes near-on-policy samples. APL [ 48] proposes
to take different advantages of offline data and online data for adaptive policy learning. PEX [ 45]
freezes the pretrained policies and trains a new policy from scratch using an adaptive exploration
strategy to avoid erasing pre-trained policies. Unlike the aforementioned approaches, SUNG unifies
the two challenges by emphasizing the proper estimation and utilization of uncertainty.
Uncertainty for RL. Uncertainty-aware RL has achieved notable success in both online and offline
RL [28]. In terms of online RL, a prominent topic in exploration is the study of epistemic uncertainty,
which leads to the development of the well-known principle of optimism in the face of uncertainty
[4,2,8,7,24]. In the context of offline RL, both model-based [ 17,15,43] and model-free [ 1,3,40,14]
approaches can benefit from the consideration of uncertainty to identify and handle OOD actions.
However, the aforementioned methods typically utilize the ensemble technique to estimate the
uncertainty, which can be computationally expensive in terms of both time and training resources. In
contrast, we adopt V AE [ 18] for efficient and effective uncertainty quantification. Additionally, we
investigate the offline-to-online setting, which distinguishes our approach from them.
3 Preliminaries
RL. We follow the standard RL setup that formulates the environment as a Markov decision process
(MDP) M= (S,A, p, r, γ ), where Sis the state space, Ais the action space, p(s′|s, a)is the
transition distribution, r(s, a)is the reward function, and γ∈[0,1)is the discount factor. The goal
of RL is to find a policy π(a|s)that maximizes the expected return Eπ[P∞
t=0γtr(st, at)].
Off-policy RL. Off-policy RL methods, such as TD3 [ 12] and SAC [ 16], have been widely applied
due to their sample efficiency. These methods typically alternate between policy evaluation and
policy improvement. In particular, given an experience replay dataset D, TD3 learns a deterministic
policy πϕ(s)and a state-action value function Qθ(s, a), parameterized by ϕandθ, respectively. The
value function can be updated via temporal difference (TD) learning as
LQ(θ) =E(s,a,r,s′)∼Dh
(Qθ(s, a)−r−γQ¯θ(s′, πϕ(s′)))2i
, (1)
where Q¯θis the target value network for stabilizing the learning process. Then, the policy can be
updated to maximize the current Q value:
Lπ(ϕ) =Es∼D[−Qθ(s, πϕ(s))]. (2)
Offline RL. In the offline RL setting, the agent only has access to a fixed dataset D={(s, a, r, s′)}.
Although off-policy RL methods can learn from data collected by any policy in principle, they fail
in the offline RL setting. This can be attributed to the well-known extrapolation error [ 13,23] due
to querying the value function with OOD actions. As such, one main line of model-free offline RL
research is to constrain the learned policy to perform actions within the support of the dataset in
different ways, such as policy constraint [ 11,39], value regularization [ 23,20,31] and etc. Among
them, two representative offline RL methods are TD3+BC [ 11] and CQL [ 23]. The former adds a
behavior cloning (BC) regularization term to the standard policy improvement in TD3:
LTD3+BC
π (ϕ) =Lπ(ϕ) +λBCE(s,a)∼Dh
(πϕ(s)−a)2i
, (3)
where λBCbalances the standard policy improvement loss and BC regularization. In contrast, the
latter resorts to pessimistic under-estimate of Q values during policy evaluation in SAC by
LCQL
Q(θ) =LQ(θ) +λCQL 
Es∼D,a∼πϕ[Qθ(s, a)] +E(s,a)∼D[−Qθ(s, a)]
, (4)
where λCQL denotes the trade-off factor. Then, we can formally summarize both methods as
Loffline
F (Θ) = LF(Θ) + λRF(Θ), (5)
3

--- PAGE 4 ---
𝑎𝑖~𝜋𝜙∙|𝑠
𝑠,𝑎,𝑟,𝑠′Finalist Action Set𝑠
OORB:ℒℱadpΘ=ℒℱΘ+ℛℱΘ:ℒℱadpΘ=ℒℱΘ
𝑎~P𝑎𝑖Candidate Action SetLv1. 
Select Top -𝑘-Value Action
Lv2. 
Select High -Uncertainty Action𝒰𝑝𝜓𝑞𝜑
𝑠,𝑎,𝑟,𝑠′OOD Sample 
Identifier
: Low -Value Region             : High -Value Region       : Low -Uncertainty Action       : High -Uncertainty Action Legend𝒰≝ℒELBO
(a) Optimistic Exploration via Bi
 -
level Action Selection (     ) 
 (b) Adaptive Exploitation with OOD Sample Identification (     ) Figure 1: Overview of SUNG. During online finetuning, we alternate between (a)optimistic ex-
ploration strategy to collect behavior data from environment and (b)adaptive exploitation method
to improve the policy. Moreover, we adopt V AE for state-action density estimation to quantify
uncertainty, and OORB to store both offline and online data. This figure is best viewed in color.
where Frepresents either a policy or a value function, parameterized by Θ,Rdenotes a regularizer
to prevent the extrapolation error, and λbalances standard loss and regularization. Note that most
model-free offline RL methods can be summarized as Eq.(5) in an explicit [ 23,11,20,39,31] or
implicit manner [22, 36, 21, 3, 42], which motivates the adaptive exploitation in Section 4.3.
4 SUNG
In this section, we present a simple unified uncertainty-guided framework (SUNG) for offline-to-
online RL, as depicted in Fig. 1. Concretely, SUNG actively quantifies the state-action uncertainty by
estimating a density function with V AE. Then, under the guidance of uncertainty, SUNG incorpo-
rates an optimistic exploration strategy and an adaptive exploitation method for online finetuning.
Combined with OORB, we finally arrive at the complete algorithm of SUNG.
4.1 Uncertainty Quantification with Density Estimation
Many prior works typically utilize Q ensembles to qualify uncertainty [ 40,33,8,24]. However, the
ensemble technique may significantly increase computational costs. Thus, in this work, we utilize a
simple yet effective approach by adopting V AE [ 18] as a state-action visitation density estimator for
uncertainty quantification. Specifically, given an encoder qφ(z|s, a)and a decoder pψ(s, a|z)with
parameters φandψ, respectively, we can optimize them with the evidence lower bound (ELBO)
LELBO (s, a;ψ, φ) =Eqφ(z|s,a)[−logpψ(s, a|z)] +DKL[qφ(z|s, a)||p(z|s, a)], (6)
where p(z|s, a) =N(0, I)is a fixed prior. Then, we utilize the negative log likelihood of the state-
action density for uncertainty quantification, i.e., U(s, a)def=−logp(s, a)≈ L ELBO (s, a;ψ, φ),
as we know logp(s, a) =−LELBO (s, a;ψ, φ) +DKL[qφ(z|s, a)||pψ(z|s, a)]. Intuitively, both the
reconstruction loss and KL divergence in ELBO indicate the state-action uncertainty, i.e., whether a
state-action pair can be well captured by the learned distribution.
Although it can be theoretically shown that a bias exists between ELBO and the negative log
likelihood of the state-action density, a prior work has shown that such approximation is empirically
valid [ 39]. Thus, we utilize ELBO for uncertainty quantification in practice, and leave the exploration
of importance sampling techniques [19, 37] for further bias reduction as future work.
4.2 Optimistic Exploration via Bi-level Action Selection
A main line of model-free offline RL methods perform conservative objectives to penalize OOD
actions during offline pretraining. However, such conservative objectives inherently limit agents’
exploration power, which impedes agents to fully benefit from online finetuning [ 33]. To overcome
this limitation, our goal is to properly measure the informativeness of unseen actions, and then select
4

--- PAGE 5 ---
the most informative action for exploration. To achieve this, we propose an optimistic exploration
strategy based on the principle of optimism in the face of uncertainty [ 4,2]. The key insight is that an
informative action is expected to possess both high Q value and high uncertainty.
Existing works typically estimate epistemic uncertainty via Q ensembles and derive an upper con-
fidence bound (UCB) to direct the exploration [ 7,8,24]. Different from them, we extend this idea
by utilizing U(s, a)for uncertainty quantification. To avoid inaccurate estimates for Q value and
uncertainty, we only select near-on-policy actions for exploration. Consequently, the objective of the
exploration policy πE(s)is to maximize the informativeness while remaining near-on-policy, i.e.,
πE(s) = arg max
a∈AQθ(s, a) +βU(s, a),
s.t.||a−πϕ(s)||2≤δ,(7)
where βcontrols the optimism level and δcontrols the on-policyness. As such, the derived behavior
policy can not only increase the chance of executing informative actions according to the principal of
optimism in the face of uncertainty, but also guarantee the on-policyness to ensure the stability.
However, such an optimization problem is impractical to solve due to two main challenges. On the
one hand, it is not straightforward to find optimal action in high-dimensional continuous action space,
which is impossible to enumerate exhaustively. On the other hand, it is hard to set a proper value for
optimism level βbecause Q value and uncertainty have different value ranges across different tasks,
which necessitates task-specific hyper-parameter tuning.
To mitigate both challenges above, we propose a simple yet effective bi-level action selection
mechanism. First, we generate a candidate action set C={ai}N
i=1withNcandidate actions. Here,
the action is generated by sampling ai∼πϕ(·|s)for stochastic policy, while by adding sampled
Gaussian noise ai=πϕ(s) +ϵi, ϵi∼ N(0, δ)for deterministic policy. Then, we rank the candidate
actions according to the Q-values (or uncertainty) to select top- kcandidate actions as a finalist action
setCf. Finally, we construct a categorical distribution according to the uncertainty (or Q values) to
select the action from Cffor interacting with the environment:
P(ai) :=exp (U(s, ai)/α)P
jexp (U(s, aj)/α),∀i∈[1, ..., k ], (8)
where αis the softmax temperature. By altering ranking criteria for the finalist action set and k, we
can flexibly adjust the preference for high Q value or high uncertainty to achieve the desired trade-off.
In practice, we observe that policy constraint based offline RL methods can accommodate preference
for either high Q value or high uncertainty. However, value regularization based methods may diverge
when preference for Q value is present. We detailedly discuss this phenomenon in Appendix B.
The optimistic exploration strategy enables agents to interact the environment with informative
actions, thereby boosting the sample efficiency of offline-to-online RL. However, it may also bring
negative effects by further increasing state-action distribution shift, as a result of the principle of
optimism in the face of uncertainty. To this end, we introduce an adaptive exploitation method in the
following subsection, which aims to mitigate the state-action distribution shift.
4.3 Adaptive Exploitation with OOD Sample Identification
Action distribution shift poses a significant challenge for offline RL algorithms [ 13,22,27,23], as
the bootstrapping term in policy evaluation involves actions derived from the learned policy πϕ. This
can result in the extrapolation error due to querying Q functions with OOD actions, leading to biased
policy improvement towards these OOD actions with erroneously high Q values. Note that offline
RL methods do not suffer from state distribution shift during training, since policy evaluation only
queries Q functions with states present in the offline dataset.
However, during online finetuning, both state and action distribution shifts occur since agents may
encounter unfamiliar state-action regimes that fall outside of the support of the dataset. Worse still,
as the proposed optimistic exploration strategy follows the principle of optimism in the face of
uncertainty, the state-action distribution shift may be further exacerbated. Preliminary experiments
from [25, 33, 48] show that this may erase the good initialization obtained from offline pretraining.
To tackle the state-action distribution shift issue, we propose an adaptive exploitation method with
OOD sample identification. The underlying idea is developed from the observations in previous works
5

--- PAGE 6 ---
Algorithm 1 SUNG: A Simple Unified Uncertainty-Guided Framework for Offline-to-Online RL
1:Input: Dataset D, Offline RL algorithm {Loffline
Q (θ),Loffline
π (ϕ)}
2:Initialize Q network Qθ, policy network πϕ, V AE network pψandqφ
3:while inoffline pretraining phase do
4: Sample minibatch of transitions (s, a, r, s′)∼ D
5: Update θandϕminimizing Loffline
Q (θ)andLoffline
π (ϕ)
6: Update ψ, φ minimizing LELBO (s, a;ψ, φ)in Eq. (6)
7:end while
8:Initialize OORB with BOORB ← {B =∅,D}
9:while inonline finetuning phase do
10: //Optimistic Exploration
11: Generate candidate action set C={ai}N
i=1
12: Select actions with top- kQ-value (or uncertainty) as finalist action set Cf
13: Sample the action at∼P(ai), ai∈ Cfin Eq.(8) according to the uncertainty (or Q value)
14: Execute the action at, collect st+1andrtfrom the environment
15: Store transitions (st, at, rt, st+1)intoBandD
16: //Adaptive Exploitation
17: Sample minibatch of transitions (s, a, r, s′)∼ B OORB
18: Update θandϕminimizing Ladp
F(Θ)in Eq. (9)
19: Update ψ, φ minimizing LELBO (s, a;ψ, φ)in Eq. (6)
20:end while
[25,33,48] that finetuning with continual offline RL objectives typically derives stable but limited
performance, while finetuning with online RL objectives typically derives unstable performance due
to the distribution shift caused by OOD state-action pairs. Therefore, we propose a novel approach
that leverages the benefits of both objectives by utilizing conservative offline RL objectives for state-
action pairs with high uncertainty while aggressive online RL objectives for state-action pairs with
low uncertainty. As shown in Eq. (5), offline RL objectives consist of a standard online RL objective
LF(Θ)and a regularizer RF(Θ). Thus, we can derive the objectives for adaptive exploitation by
introducing an uncertainty-guided OOD sample identifier I(s, a):
Ladp
F(Θ) = LF(Θ) + λI(s, πϕ(s))RF(Θ). (9)
We expect that I(s, a)is a large value for state-action pairs with high uncertainty and a small value
for those with low uncertainty. Therefore, we construct such an identifier using uncertainty estimation
U(s, a)as below. For a minibatch of state-action pairs {si, ai}M
i=1, we select the top p%of them as
OOD state-action pair sets DOOD, according to its corresponding uncertainty estimation U(si, ai).
The selection process can be performed by sampling from a categorical distribution similar to Eq. (8).
Then, out of simplicity, we define the OOD sample identifier as
I(s, a) :=
1,if(s, a)∈ D OOD,
0,else.(10)
By setting p= 0, the training objective is precisely the offline RL objective, while setting p= 100
results in the online RL objective. Thus, by tuning the value of p, the proposed adaptive exploitation
method can attain the desired trade-off between performance and stability.
4.4 Algorithm Implementation Details
Offline-to-Online Replay Buffer. Previous studies [ 25,48] have highlighted the significance of
incorporating both offline and online data during finetuning. Here, we adopt the offline-to-online
replay buffer proposed in [ 48] due to its simplicity. Specifically, OORB consists of two replay buffers:
a fully online buffer that collects transitions from online interactions, and an offline buffer that stores
transitions from both offline dataset and online interactions. During training, we randomly sample
transitions from OORB with a Bernoulli distribution, where the probability of sampling from the
fully online buffer is pOORB , and the probability of sampling from the offline buffer is 1−pOORB .
6

--- PAGE 7 ---
SUNG Framework for Offline-to-Online RL. Putting everything together, we summarize the full
framework in Algorithm 1. SUNG first pretrains agents and V AE using offline dataset. Then, it
turns to online finetuning with the proposed optimistic exploration strategy and adaptive exploitation
method. We also provide two implementation cases of bridging TD3+BC and TD3, and bridging
CQL and SAC in Appendix A.
5 Experiments
In this section, we provide empirical results to verify the effectiveness of our framework by answering
three primary questions: (1) How does SUNG compare with other state-of-the-arts when combined
with different offline RL methods, across various types of tasks and datasets? (2) How do the key
components of SUNG influence the overall offline-to-online performance? (3) How should we set
hyper-parameters for SUNG in practice, and how challenging is this task?
5.1 Experimental Setup
Settings. We evaluate on the D4RL benchmark [ 10], which provides various continuous-control tasks
and datasets. We focus on MuJoCo and AntMaze domains. We first perform 1M gradient steps for
offline pretraining, and then perform 100K environment steps for online finetuning. While some prior
works [ 21,45,48] take 1M environment steps for finetuning, we argue that 1M environment steps are
even enough for an online RL agent to achieve expert-level performance. Thus, we believe finetuning
for 100K environment steps is a more reasonable setting, which is also adopted by [31, 48].
Backbone Offline RL Methods. To evaluate the generality of SUNG, we choose two representative
offline RL methods as the backbone for pretraining, i.e., TD3+BC [ 11] and CQL [ 23]. Concretely,
TD3+BC extends TD3 with behavior cloning based policy constraint, while CQL extends SAC with
value regularization. For AntMaze domains, we substitute TD3+BC with SPOT [ 39] due to its inferior
performance. SPOT is another extension of TD3 but with density guided policy constraint.
Baselines. We compare SPOT with the following baselines: (1) offline-ft leverages online interactions
by performing offline RL objectives. (2) online-ft leverages online interactions by performing online
RL objectives that correspond to the ones used for pretraining. (3) BR [25] prioritizes near-on-policy
transitions from the replay buffer. (4) O3F [33] utilizes knowledge contained in value functions
to guide exploration. (5) APL [48] performs adaptive policy learning by incorporating different
advantages of offline and online data. (6) PEX [45] trains a new policy from scratch by utilizing
the pre-trained policy for exploration. Note that we also include ODT [ 49] for comparison with a
seperate subsection in Appendix D.2. See Appendix C for further implementation details.
5.2 Comparisons on D4RL Benchmarks
Table 1: Comparison of the averaged D4RL score on MuJoCo tasks with TD3+BC as the offline RL
backbone method. r = random, m = medium, m-r = medium-replay. We report the mean and standard
deviation over 5 seeds.
TD3+BC offline-ft online-ft BR O3F APL PEX SUNG
halfcheetah-r-v2 11.5 34.3 ±2.4 57.7±1.6 67.6±11.9 71.4±3.3 70.0±4.7 53.4±9.1 76.6±2.0
hopper-r-v2 8.7 8.2 ±0.2 11.2±1.9 25.7±8.7 11.7±2.0 27.1±14.0 37.4±10.6 38.7±15.0
walker2d-r-v2 5.4 7.0 ±4.1 6.2±3.9 9.9±4.6 11.6±5.1 13.8±4.0 33.1±16.4 14.1±5.1
halfcheetah-m-v2 48.0 49.3 ±0.4 67.6±2.4 79.5±7.4 77.8±1.1 80.9±2.0 52.3±21.1 80.7±2.5
hopper-m-v2 61.5 58.8 ±3.9 78.3±32.7 93.1±10.8 102.0 ±2.0 76.9±24.2 73.9±17.8 101.8 ±6.0
walker2d-m-v2 82.2 84.6 ±1.2 68.2±11.5 70.1±21.4 97.1±2.2 98.2±13.5 56.7±28.5 113.5 ±1.9
halfcheetah-m-r-v2 44.6 47.0 ±0.9 66.3±1.0 65.0±14.6 67.6±2.9 71.5±1.3 53.2±9.8 69.7±3.4
hopper-m-r-v2 55.9 85.4 ±7.6 89.9±13.5 97.2±13.9 97.6±4.9 100.6 ±9.8 90.8±18.7 101.3 ±7.0
walker2d-m-r-v2 71.7 80.2 ±8.7 87.4±4.0 82.7±17.8 100.9 ±3.7 108.2 ±3.6 70.6±13.3 109.2 ±1.9
Total 389.6 454.9 532.9 590.9 637.7 647.1 521.2 705.7
Results on MuJoCo. Results for MuJoCo domains with TD3+BC and CQL as backbone offline
RL method are shown in Table 1 and 2, respectively. We find that additional online interactions can
bring further performance improvement in most settings. However, two exceptions are O3F and PEX,
7

--- PAGE 8 ---
Table 2: Comparison of the averaged D4RL score on MuJoCo tasks with CQL as the offline RL
backbone method. r = random, m = medium, m-r = medium-replay. We report the mean and standard
deviation over 5 seeds. Note that O3F is omitted due to its divergence in this setting.
CQL offline-ft online-ft BR APL PEX SUNG
halfcheetah-r-v2 23.5 28.8 ±2.3 50.2±1.5 61.8±12.3 67.7±9.6 50.6±2.2 69.1±9.2
hopper-r-v2 6.4 31.2 ±0.5 28.3±6.2 23.8±7.9 41.8±22.0 34.3±8.9 44.3±11.7
walker2d-r-v2 4.5 5.6 ±3.4 8.2±5.6 4.0±2.3 6.3±1.8 10.7±2.8 14.5±6.1
halfcheetah-m-v2 48.1 48.9 ±0.2 52.1±25.6 56.7±28.5 44.7±38.5 43.5±2.4 79.7±1.0
hopper-m-v2 73.7 74.1 ±1.4 91.0±10.4 97.7±3.7 102.7 ±3.1 46.3±15.1 104.1 ±1.3
walker2d-m-v2 84.3 83.5 ±0.7 85.6±7.6 81.7±14.0 75.3±25.7 34.0±17.3 86.0±12.6
halfcheetah-m-r-v2 46.9 49.5 ±0.3 61.3±1.0 64.9±5.8 78.6±1.2 45.5±1.7 75.6±1.9
hopper-m-r-v2 96.0 95.0 ±1.0 92.8±19.5 88.5±21.8 97.4±9.5 66.5±24.2 101.9 ±9.1
walker2d-m-r-v2 83.4 84.5 ±1.0 86.9±12.4 78.8±28.0 103.2 ±19.0 40.1±17.9 108.2 ±4.2
Total 466.9 501.1 556.4 558.0 617.8 371.5 683.4
when combined with CQL. As pointed in [ 33], O3F is not compatible with value regularization based
offline RL methods. This is also observed by [ 35] and we provide detailed explanations in Appendix
B. In contrast, PEX trains a new policy from scratch, and cannot fully benefit from reusing good
initialization from pretraining, thereby leading to limited performance.
Moreover, we remark that SUNG substantially outperforms previous state-of-the-art methods, ex-
hibiting an additional 15.04% and 14.05% offline-to-online improvement over the best-performing
baseline when combined with TD3+BC and CQL, respectively. This demonstrates the necessity of
proper estimation and utilization of uncertainty for tackling both constrained exploratory behavior
and state-action distribution shift. Moreover, this also verifies the generality of SUNG in facilitating
online improvement for agents pretrained with different offline RL methods.
Results on AntMaze. We also provide results for the more challenging AntMaze domains with SPOT
and CQL as backbone offline RL method. We defer the results and detailed analyses to Appendix
D.1. We observe that SUNG outperforms all the baselines in most settings.
5.3 Ablation Studies
In this subsection, we perform an ablation study over the components in SUNG in Fig. 2. Refer to
Appendix D.4 for implementation details and extra ablation studies.
Ablation on Optimistic Exploration. First, we evaluate (a)SUNG without optimistic exploration
strategy, and find that the removal brings significant negative effects when combined with TD3+BC,
but no remarkable effects when combined with CQL. The reason for this discrepancy is that CQL
is built on top of SAC, which can gradually recover exploration power through max-entropy RL
framework during online finetuning. Then, we ablate (b)uncertainty and (c)Q value in the optimistic
exploration strategy to derive two variant strategies - one greedy for Q value and another greedy for
uncertainty. As expected, both ablations degrade the performance to varying degrees, underscoring
the significance of both Q value and uncertainty for exploration. Notably, the removal of uncertainty
in SUNG when combined with CQL causes significant performance degradation, which can be
attributed to similar reason detailed in Appendix B.
Ablation on Adaptive Exploitation. We evaluate (d)SUNG without the adaptive exploitation
method, and observe that performance for finetuning TD3+BC and CQL significantly deteriorates on
most tasks. This emphasizes the necessity of addressing the state-action distribution shift issue.
Ablation on Uncertainty Quantification. Finally, we (e)replace the V AE-based density estimator
with the standard deviation of double Q values for uncertainty quantification [ 8]. As expected, the
results show that such replacement leads to performance deterioration on most tasks, since it is not
sufficient to provide reliable uncertainty quantification. While utilizing the ensemble technique can
eliminate this issue [ 24,7], it can significantly increase computational costs. We leave the exploration
of alternative methods as future work.
8

--- PAGE 9 ---
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr40
20
02040SUNG (TD3+BC) 
 Percent Difference(a)
w/o Opt. Exploration
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040(b)
Opt. Exploration w/o Unc.
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040(c)
Opt. Exploration w/o Q
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040(d)
w/o Adp. Exploitation
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040(e)
Unc. Estimation w/ Q std
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr40
20
02040SUNG (CQL) 
 Percent Differencew/o Opt. Exploration
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040Opt. Exploration w/o Unc.
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040Opt. Exploration w/o Q
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040w/o Adp. Exploitation
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040Unc. Estimation w/ Q stdFigure 2: Performance difference of an ablation study of SUNG combined with TD3+BC and CQL,
compared with the full algorithm. Opt. = Optimistic, Unc. = Uncertainty, Adp. = Adaptive. hc =
HalfCheetah, hop = Hopper, w = Walker, r = random, m = medium, mr = medium-replay.
10 20 40 80
k707580Average D4RL score
SUNG (TD3+BC)
10 20 40 80
k50607080
SUNG (CQL)
Uncertainty first Q first best baseline
Figure 3: Comparing performance with differ-
ent order of ranking criteria and finalist action
set size kused in optimistic exploration.
1 5 10 20 50
p707580Average D4RL score
SUNG (TD3+BC)
1 5 10 20 50
p50607080
SUNG (CQL)
best baselineFigure 4: Comparing performance with varying
percent pof identified OOD samples from the
mini-batch used in adaptive exploitation.
5.4 Hyper-parameter Analysis
Analysis of Optimistic Exploration. First, we investigate the order of the ranking criteria and the
finalist action set size kin optimistic exploration. The aggregated results averaged over MuJoCo
domains are shown in Fig. 3. As expected in Appendix B, SUNG with TD3+BC can accommodate
preference for both high uncertainty and high value, whereas SUNG with CQL can only accommodate
preference for high uncertainty. Hence, we choose the order with Q first for TD3+BC, and uncertainty
first for CQL. Besides, we remark that SUNG consistently outperforms the best baseline with any
choice of k, which further verifies the superiority of SUNG.
Analysis of Adaptive Exploitation. Moreover, we explore the choice of percent pof identified
OOD samples from the mini-batch in adaptive exploitation. The aggregated results averaged over
MuJoCo domains are shown in Fig. 4. Predictably, setting ptoo conservatively or too aggressively
may adversely degrade the performance. We find that p= 5performs the best when combined with
either TD3+BC or CQL, achieving the desired trade-off between performance and stability.
6 Conclusion, Limitations, and Broader Impacts
This paper studies how to efficiently finetune pretrained offline RL agents via online interactions.
We present SUNG, a simple unified uncertainty-guided framework, to tackle both challenges of
constrained exploratory behavior and state-action distribution shift. SUNG leverages the quantified
uncertainty to guide optimistic exploration and adaptive exploitation. Empirical results across
different backbone offline RL methods, environments and datasets verify the superiority of SUNG.
9

--- PAGE 10 ---
In terms of limitations, the proposed offline-to-online framework cannot be directly applied to recent
advances in offline RL, such as reward-conditioned RL based methods [ 6,9] and on-policy methods
[50]. However, we argue that the insights of SUNG are generally applicable in principle. We leave
this as future work. Regarding societal impacts, we do not anticipate any negative consequences from
using our framework in practice. We provide more detailed discussions in Appendix E.
10

--- PAGE 11 ---
References
[1]Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline
reinforcement learning with diversified q-ensemble. Advances in Neural Information Processing
Systems , 34:7436–7447, 2021.
[2]Jean-Yves Audibert, Rémi Munos, and Csaba Szepesvári. Exploration–exploitation tradeoff
using variance estimates in multi-armed bandits. Theoretical Computer Science , 410(19):
1876–1902, 2009.
[3]Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhi-Hong Deng, Animesh Garg, Peng Liu, and
Zhaoran Wang. Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning.
InInternational Conference on Learning Representations , 2022.
[4]Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for
near-optimal reinforcement learning. Journal of Machine Learning Research , 3(Oct):213–231,
2002.
[5]Kaylee Burns, Tianhe Yu, Chelsea Finn, and Karol Hausman. Offline reinforcement learning at
multiple frequencies. In Conference on Robot Learning , pages 2041–2051, 2023.
[6]Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning
via sequence modeling. Advances in Neural Information Processing Systems , 34:15084–15097,
2021.
[7]Richard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. Ucb exploration via
q-ensembles. arXiv preprint arXiv:1706.01502 , 2017.
[8]Kamil Ciosek, Quan Vuong, Robert Loftin, and Katja Hofmann. Better exploration with
optimistic actor critic. Advances in Neural Information Processing Systems , 32, 2019.
[9]Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential
for offline rl via supervised learning? In International Conference on Learning Representations ,
2022.
[10] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for
deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219 , 2020.
[11] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.
Advances in Neural Information Processing Systems , 34:20132–20145, 2021.
[12] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In International Conference on Machine Learning , pages 1587–1596,
2018.
[13] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning
without exploration. In International Conference on Machine Learning , pages 2052–2062,
2019.
[14] Kamyar Ghasemipour, Shixiang Shane Gu, and Ofir Nachum. Why so pessimistic? estimating
uncertainties for offline rl through ensembles, and why their independence matters. Advances in
Neural Information Processing Systems , 35:18267–18281, 2022.
[15] Kaiyang Guo, Shao Yunfeng, and Yanhui Geng. Model-based offline reinforcement learning
with pessimism-modulated dynamics belief. Advances in Neural Information Processing
Systems , 35:449–461, 2022.
[16] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan,
Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms
and applications. arXiv preprint arXiv:1812.05905 , 2018.
[17] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel:
Model-based offline reinforcement learning. Advances in Neural Information Processing
Systems , 33:21810–21823, 2020.
11

--- PAGE 12 ---
[18] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. International Confer-
ence on Learning Representations , 2013.
[19] Diederik P Kingma, Max Welling, et al. An introduction to variational autoencoders. Founda-
tions and Trends® in Machine Learning , 12(4):307–392, 2019.
[20] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement
learning with fisher divergence critic regularization. In International Conference on Machine
Learning , pages 5774–5783, 2021.
[21] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit
q-learning. In International Conference on Learning Representations , 2022.
[22] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-
policy q-learning via bootstrapping error reduction. Advances in Neural Information Processing
Systems , 32, 2019.
[23] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning
for offline reinforcement learning. Advances in Neural Information Processing Systems , 33:
1179–1191, 2020.
[24] Kimin Lee, Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Sunrise: A simple unified
framework for ensemble learning in deep reinforcement learning. In International Conference
on Machine Learning , pages 6131–6141, 2021.
[25] Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-online
reinforcement learning via balanced replay and pessimistic q-ensemble. In Conference on Robot
Learning , pages 1702–1712, 2022.
[26] Seunghyun Lee, Da Young Lee, Sujeong Im, Nan Hee Kim, and Sung-Min Park. Clinical
decision transformer: Intended treatment recommendation through goal prompting. arXiv
preprint arXiv:2302.00612 , 2023.
[27] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning:
Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643 , 2020.
[28] Owen Lockwood and Mei Si. A review of uncertainty for deep reinforcement learning. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment ,
volume 18, pages 155–162, 2022.
[29] Cong Lu, Philip J Ball, Tim GJ Rudner, Jack Parker-Holder, Michael A Osborne, and Yee Whye
Teh. Challenges and opportunities in offline reinforcement learning from visual observations.
arXiv preprint arXiv:2206.04779 , 2022.
[30] Yicheng Luo, Jackie Kay, Edward Grefenstette, and Marc Peter Deisenroth. Finetuning from
offline reinforcement learning: Challenges, trade-offs and practical solutions. arXiv preprint
arXiv:2303.17396 , 2023.
[31] Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative q-learning for offline
reinforcement learning. In Thirty-sixth Conference on Neural Information Processing Systems ,
2022.
[32] Yihuan Mao, Chao Wang, Bin Wang, and Chongjie Zhang. Moore: Model-based offline-to-
online reinforcement learning. arXiv preprint arXiv:2201.10070 , 2022.
[33] Max Sobol Mark, Ali Ghadirzadeh, Xi Chen, and Chelsea Finn. Fine-tuning offline policies
with optimistic action selection. In Deep Reinforcement Learning Workshop NeurIPS , 2022.
[34] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online
reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359 , 2020.
[35] Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn,
Aviral Kumar, and Sergey Levine. Cal-ql: Calibrated offline rl pre-training for efficient online
fine-tuning. arXiv preprint arXiv:2303.05479 , 2023.
12

--- PAGE 13 ---
[36] Shideh Rezaeifar, Robert Dadashi, Nino Vieillard, Léonard Hussenot, Olivier Bachem, Olivier
Pietquin, and Matthieu Geist. Offline reinforcement learning as anti-exploration. In Proceedings
of the AAAI Conference on Artificial Intelligence , pages 8106–8114, 2022.
[37] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. In International Conference on Machine
Learning , pages 1278–1286, 2014.
[38] Juergen Schmidhuber. Reinforcement learning upside down: Don’t predict rewards–just map
them to actions. arXiv preprint arXiv:1912.02875 , 2019.
[39] Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng Long. Supported policy
optimization for offline reinforcement learning. In Advances in Neural Information Processing
Systems , 2022.
[40] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua M Susskind, Jian Zhang, Ruslan Salakhutdi-
nov, and Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. In
International Conference on Machine Learning , pages 11319–11328, 2021.
[41] Haoran Xu, Li Jiang, Li Jianxiong, and Xianyuan Zhan. A policy-guided imitation approach
for offline reinforcement learning. Advances in Neural Information Processing Systems , 35:
4085–4098, 2022.
[42] Haoran Xu, Li Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Wai Kin Chan,
and Xianyuan Zhan. Offline rl with no ood actions: In-sample learning via implicit value
regularization. In International Conference on Learning Representations , 2023.
[43] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural
Information Processing Systems , 33:14129–14142, 2020.
[44] Zishun Yu and Xinhua Zhang. Actor-critic alignment for offline-to-online reinforcement
learning. 2023.
[45] Haichao Zhang, We Xu, and Haonan Yu. Policy expansion for bridging offline-to-online
reinforcement learning. The International Conference on Learning Representations , 2023.
[46] Kesen Zhao, Lixin Zou, Xiangyu Zhao, Maolin Wang, et al. User retention-oriented recommen-
dation with decision transformer. Proceedings of the ACM Web Conference , 2023.
[47] Yi Zhao, Rinu Boney, Alexander Ilin, Juho Kannala, and Joni Pajarinen. Adaptive behav-
ior cloning regularization for stable offline-to-online reinforcement learning. arXiv preprint
arXiv:2210.13846 , 2022.
[48] Han Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, and Jing Jiang. Adaptive
policy learning for offline-to-online reinforcement learning. In Proceedings of the AAAI
Conference on Artificial Intelligence , 2023.
[49] Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In International
Conference on Machine Learning , pages 27042–27059, 2022.
[50] Zifeng Zhuang, Kun LEI, Jinxin Liu, Donglin Wang, and Yilang Guo. Behavior proximal
policy optimization. In International Conference on Learning Representations , 2023. URL
https://openreview.net/forum?id=3c13LptpIph .
13

--- PAGE 14 ---
Appendix
A Implementation Details of SUNG
In this section, we provide two implementation cases of SUNG: one bridges TD3+BC [ 11] and TD3
[12], and another bridges CQL [ 23] and SAC [ 16]. Note that existing offline RL methods are typically
built on top of online off-policy RL methods. Thus, we utilize the corresponding online RL objectives
for online finetuning to keep consistent.
A.1 SUNG (TD3+BC): Bridging TD3+BC and TD3
First, we show how SUNG (TD3+BC) bridges TD3+BC and TD3 to finetune the deterministic policy
πϕ(s)and two Q value functions Qθ1(s, a), Qθ2(s, a).
In terms of exploration, the behavior policy of SUNG (TD3+BC) is compatible with preference for
either high Q value or high uncertainty, as demonstrated in the main body. Here, we take preference
for high Q value as example. As TD3 utilizes the deterministic policy, we first generate a candidate
action set C={ai=πϕ(s) +ϵi}N
i=1, where ϵi∼ N(0, δ)is the Gaussian noise. Then, candidate
actions with top- kQ values are selected as finalist action set Cf. Finally, we sample the action for
interaction from a categorical distribution P(ai), ai∈ Cfin Eq. (8)according to the uncertainty. In
practice, we set N= 100 andk= 10 to interact with the environment by selecting actions that
possess high value and relatively high uncertainty.
In terms of exploitation, SUNG (TD3+BC) follows TD3+BC and TD3 to take the standard TD
learning for policy evaluation, and adopts the proposed adaptive exploitation method for policy
improvement as
Ladp,TD3+BC
π (ϕ) =Es∼BOORB [−Qθ(s, πϕ(s))] + λE(s,a)∼BOORBh
I(s, πϕ(s)) (πϕ(s)−a)2i
.
(11)
Integrating the details above, we give a detailed algorithm description of SUNG (TD3+BC) in
Algorithm 2.
A.2 SUNG (CQL): Bridging CQL and SAC
Then, we show how SUNG (CQL) bridges CQL and SAC to finetune the stochastic policy πϕ(·|s)
and two Q value functions Qθ1(s, a), Qθ2(s, a).
In terms of exploration, the behavior policy of SUNG (CQL) is only compatible with preference
for high uncertainty, as demonstrated in the main body. As SAC utilizes the stochastic policy, we
first generate a candidate action set C={ai∼πϕ(·|s)}N
i=1. Then, candidate actions with top- k
uncertainty are selected as finalist action set Cf. Finally, we sample the action for interaction from a
categorical distribution P(ai), ai∈ Cfsimilar to Eq. (8)according to the Q value. In practice, we
setN= 100 andk= 20 . Note that we slightly increase the value of kto avoid the selected actions
with too high uncertainty for sample efficiency. As such, we can interact with the environment by
selecting actions that possess relatively high uncertainty and relatively high Q value.
In terms of exploitation, SUNG (CQL) follows CQL and SAC to perform policy improvement as
LSAC
π(ϕ) =Es∼BOORB ,a∼πϕ(·|s)[Qθ(s, a)−logπϕ(a|s)], (12)
and adopts the proposed adaptive exploitation method for policy evaluation as
Ladp,CQL
Q (θ) =E(s,a,r,s′)∼BOORBh
(Qθ(s, a)−r−γQ¯θ(s′, πϕ(s′)))2i
+λEs∼BOORB
I(s, πϕ(s)) 
Ea∼πϕ[Qθ(s, a)]−Ea∼BOORB [Qθ(s, a)]
,(13)
where πϕ(s)denotes the mean of the Gaussian stochastic policy. Integrating the details above, we
give a detailed algorithm description of SUNG (CQL) in Algorithm 3.
14

--- PAGE 15 ---
Algorithm 2 SUNG (TD3+BC): Bridging TD3+BC and TD3
1:Input: Dataset D={(s, a, r, s′)}
2://Offline Pretraining
3:Initialize Q network Qθ, policy network πϕ, V AE network with pψandqφ
4:fort= 1toT1do
5: Sample minibatch of transitions (s, a)∼ D
6: Update θandϕminimizing LQ(θ)in Eq. (1) and LTD3+BC
π (ϕ)in Eq. (3)
7: Update ψ, φ minimizing LELBO (s, a;ψ, φ)in Eq. (6)
8:end for
9://Online Finetuning
10:Initialize OORB with BOORB ← {B =∅,D}
11:fort= 1toT2do
12: //Optimistic Exploration
13: Generate candidate action set C={ai}N
i=1, where ai=πϕ(st) +ϵi,ϵi∼ N(0, δ)
14: Select actions with top- kQ-values as finalist action set Cf
15: Sample the action at∼P(ai), ai∈ Cfin Eq. (8) according to the uncertainty
16: Execute action at, collect st+1andrtfrom the environment
17: Store transitions (st, at, rt, st+1)intoBandD
18: //Adaptive Exploitation
19: Sample minibatch of transitions (s, a, r, s′)∼ B OORB
20: Construct the OOD identifier Iin Eq. (10)
21: Update θminimizing LQ(θ)in Eq. (1)
22: Update ϕminimizing Ladp,TD3+BC
π (ϕ)in Eq. (11)
23: Update ψ, φ minimizing LELBO (s, a;ψ, φ)in Eq. (6)
24:end for
Algorithm 3 SUNG (CQL): Bridging CQL and SAC
1:Input: Dataset D={(s, a, r, s′)}
2://Offline Pretraining
3:Initialize Q network Qθ, policy network πϕ, V AE network with pψandqφ
4:fort= 1toT1do
5: Sample minibatch of transitions (s, a)∼ D
6: Update θandϕminimizing LCQL
Q(θ)in Eq. (4) and LSAC
π(ϕ)in Eq. (12)
7: Update ψ, φ minimizing LELBO (s, a;ψ, φ)in Eq. (6)
8:end for
9://Online Finetuning
10:Initialize OORB with BOORB ← {B =∅,D}
11:fort= 1toT2do
12: //Optimistic Exploration
13: Generate candidate action set C={ai∼πϕ(·|s)}N
i=1
14: Select actions with top- kuncertainty as finalist action set Cf
15: Sample the action at∼P(ai), ai∈ Cfsimilar to Eq. (8) according to the Q value
16: Execute action at, collect st+1andrtfrom the environment
17: Store transitions (st, at, rt, st+1)intoBandD
18: //Adaptive Exploitation
19: Sample minibatch of transitions (s, a, r, s′)∼ B OORB
20: Construct the OOD identifier Iin Eq. (10)
21: Update θminimizing Ladp,CQL
Q (θ)in Eq. (13)
22: Update ϕminimizing LSAC
π(ϕ)in Eq. (12)
23: Update ψ, φ minimizing LELBO (s, a;ψ, φ)in Eq. (6)
24:end for
15

--- PAGE 16 ---
B Why Exploration with Preference for High Value Fails When Finetuning
CQL?
In empirical results, we find that O3F [ 33], which selects actions with preference for high Q value
during exploration, fails when combined with CQL [ 23]. Besides, SUNG with preference of high Q
value for exploration also fails when combined with CQL. Now, we give a qualitative analysis of this
reason. To begin with, we recall the practical implementation of the policy evaluation in CQL:
arg min
QEs∈D"
logX
aexp (Q(s, a))−Ea∼πβ(a|s)[Q(s, a)]#
+E(s,a,r,s′)∼D
(Q(s, a)−y)2
,
(14)
where πβdenotes the behavior policy of the offline dataset and ydenotes the standard TD target for
policy evaluation.
There are three terms in the above equation: given a state, the first term minimizes Q values of all
the actions; the second term maximizes Q values of the action collected in the dataset; the third
term is the standard TD learning. As such, given state s, when we choose high-value action ah
for exploration, the transition (s, ah, s′, r)will be collected into the replay buffer. Then, during
exploitation, the combination of the first and second term in the equation leads to a situation where
most unseen actions have low value and will have lower value due to the first term, while few seen
actions have high value and will have higher value due to the second term. As the third term involves
bootstrapping, the high-value action will be updated to have a lower value, leading to a consistent
decrease in the values of all actions with each gradient step. This can ultimately cause the collapse of
the Q value functions, further resulting in poor policy through propagation in the policy improvement.
We remark that SUNG can solve this issue by altering the ranking criteria for the finalist action set
andkto show preference for high uncertainty while relatively high Q value during exploration.
C More Experimental Details
C.1 Experimental Setting Details
For MuJoCo tasks, we follow prior work [ 45] to omit the evaluation on medium-expert and expert
datasets, where offline pre-trained agents can already achieve expert-level performance and do not
need further finetuning. All evaluations use datasets of the latest released "-v2" version. All the
experiments are repeated with 5 different random seeds. For each evaluation, we run 10 episodes for
MuJoCo tasks and 100 episodes for AntMaze tasks. We report the average D4RL score over last 3
evaluations for MuJoCo tasks, and last 10 evaluations for AntMaze tasks.
For offline pretraining, we first train the agents with 1M gradient steps by taking the suggested
standard pipeline of TD3+BC and CQL:
• TD3+BC [11]: https://github.com/sfujim/TD3_BC
• CQL [23]: https://github.com/young-geng/CQL
Then, the last checkpoint of the pre-trained agents are used to warm start the online finetuning.
Since the authors of SPOT [ 39] have provided the checkpoints of the pre-trained agents in https:
//github.com/thuml/SPOT , we directly adopt these for online finetuning.
We conduct all the experiments on 2 NIVIDA 3090 GPUs and Intel Xeon Gold 6230 CPU at 2.10GHz.
C.2 Implementation Details and Hyper-parameters
For V AE training, we follow the official implementation of BCQ [ 13] and SPOT [ 39]. Following
SPOT, we normalize the states in the dataset for MuJoCo domains but do not normalize the states for
AntMaze domains. Different from SPOT that utilizes V AE to estimate the density of the behavior
policy, SUNG utilizes V AE to estimate the state-action visitation density. Thus, we set the latent
dimension as 2×(state dim + action dim) instead of 2×action dim . Detailed hyper-parameters
for V AE training are listed in Table 3. Note that we simply take the hyper-parameters reported in
16

--- PAGE 17 ---
Table 3: Hyper-parameters of V AE for uncertainty quantification in SUNG.
Hyper-parameter Value
V AE trainingOptimizer Adam
Learning rate 1e-3
Batch size 256
Gradient steps 1e5
KL term weight 0.5
State Normalization True for MuJoCo
False for AntMaze
V AE architectureEncoder hidden num 750
Encoder layers 3
Latent dim 2×(state dim + action dim)
Decoder hidden dim 750
Decoder layers 3
Table 4: Hyper-parameters of online finetuning in SUNG. Note that we do not tune any hyper-
parameter in backbone RL and architecture.
Hyper-parameterMuJoCo AntMaze
TD3+BC CQL SPOT CQL
Backbone RLOptimizer Adam Adam Adam Adam
Actor learning rate 3e-4 1e-4 1e-4 1e-4
Critic learning rate 3e-4 3e-4 3e-4 3e-4
Batch size 256 256 256 256
Discount factor 0.99 0.99 0.99 0.99
Target update rate 5e-3 5e-3 5e-3 5e-3
ArchitectureActor hidden num 256 256 256 256
Actor layers 3 2 3 2
Critic hidden num 256 256 256 256
Actor layers 3 2 3 3
SUNGCandidate action num N 100 100 100 100
Finalist action num k 10 20 10 20
Order of ranking criteria qu uq qu uq
Softmax temperature α 1.0 1.0 1.0 1.0
OOD sample percentage p 5 10 99 90
OORBOnline buffer size 2e4 2e4 2e4 2e4
Offline buffer size 2e6 2e6 2e6 2e6
OORB probability pOORB 0.1 0.1 0.2 0.7
SPOT [ 39] and omit hyper-parameter tuning for V AE training. Accordingly, further performance
improvement is expected from careful tuning.
For online finetuning, we follow the same standard architecture and the same hyper-parameter of
TD3+BC, CQL and SPOT for all the baselines. Note that we do not tune any hyper-parameter of
backbone RL and architecture in SUNG for fair comparison. In terms of baseline reproduction, we
strictly follow their official implementation and hyper-parameters reported in their original paper to
finetune agents pre-trained with TD3+BC, CQL and SPOT for MuJoCo and AntMaze domains. One
exception is that the heavy ensemble technique is removed in BR for fair comparison. Note that we
also equip the baseline with OORB for fair comparison. All the detailed hyper-parameters for online
finetuning are presented in Table 4.
17

--- PAGE 18 ---
Table 5: Comparison of the averaged D4RL score on AntMaze tasks with SPOT as the offline RL
backbone method. We report the mean and standard deviation over 5 seeds.
SPOT offline-ft online-ft BR O3F APL PEX SUNG
antmaze-umaze-v2 93.2 96.6 ±0.7 88.3±2.3 92.9±1.4 96.9±0.9 97.6±0.7 12.2±4.3 97.0±0.8
antmaze-umaze-diverse-v2 41.6 22.0 ±2.5 0.0±0.0 0.0±0.0 65.3±2.2 50.5±3.1 0.0±0.0 71.7±6.2
antmaze-medium-play-v2 75.2 83.9 ±2.2 0.0±0.0 6.4±5.0 85.6±1.3 86.0±1.4 0.1±0.1 88.6±1.5
antmaze-medium-diverse-v2 73.0 84.6 ±2.0 0.5±0.5 0.0±0.0 80.1±4.2 86.0±2.4 0.0±0.0 91.7±1.2
antmaze-large-play-v2 40.8 40.9 ±2.0 0.0±0.0 0.0±0.0 38.8±4.8 38.9±3.9 0.0±0.0 45.7±3.7
antmaze-large-diverse-v2 44.0 16.6 ±2.0 0.0±0.0 0.0±0.0 3.0±0.9 3.8±0.9 0.0±0.0 19.8±1.6
Total 367.8 344.6 88.8 99.3 369.7 362.8 12.3 414.5
Table 6: Comparison of the averaged D4RL score on AntMaze tasks with CQL as the offline RL
backbone method. We report the mean and standard deviation over 5 seeds.
CQL offline-ft online-ft BR O3F APL PEX SUNG
antmaze-umaze-v2 88.6 87.9 ±1.0 96.0±1.1 68.5±3.5 98.6±0.5 96.0±1.6 87.3±2.0 96.9±0.6
antmaze-umaze-diverse-v2 41.0 45.9 ±1.8 0.0±0.1 0.0±0.0 0.0±0.0 0.0±0.0 0.7±1.4 50.5±3.1
antmaze-medium-play-v2 63.8 75.4 ±1.9 18.6±11.0 22.2±8.2 67.3±2.3 22.8±2.5 0.3±0.4 86.3±1.4
antmaze-medium-diverse-v2 61.8 74.1 ±1.8 22.1±9.1 13.6±4.9 80.8±2.9 36.8±4.3 0.3±1.0 85.6±2.1
antmaze-large-play-v2 32.0 41.8 ±3.7 0.1±0.2 0.0±0.0 0.0±0.0 0.0±0.0 0.0±0.0 52.7±2.8
antmaze-large-diverse-v2 32.6 35.3 ±2.6 0.7±1.8 0.1±0.1 0.0±0.0 0.0±0.0 0.0±0.0 44.1±3.5
Total 319.6 360.4 137.5 104.3 246.7 155.6 88.6 416.1
D More Experimental Results
D.1 Experiments on AntMaze
We conduct experiments on more challenging AntMaze domains, which require the agents to deal
with the sparse rewards and stitch fragments of sub-optimal trajectories. Results for AntMaze domains
with SPOT and CQL as backbone offline RL method are shown in Table 5 and 6, respectively. From
the results, we can observe that online-ft, BR and PEX struggle to benefit from online finetuning
in most settings, except for the relatively easy antmaze-umaze task. This stems from the need for
explicit or implicit behavior cloning that is offered by offline RL objectives, which is required to
handle the sparse rewards and stitch sub-optimal sub-trajectories. However, online-ft, BR and PEX
are developed on top of purely online RL methods, thus failing in most of the tasks.
Furthermore, we remark that SUNG, when combined with either SPOT or CQL, outperforms other
state-of-the-art methods in most settings. This demonstrates the superiority of SUNG for discovering
informative state-action pairs that can lead to high outcomes, thereby providing valuable behavior
patterns for agents to learn from. Moreover, SUNG achieves flexible trade-off between offline
and online RL objectives by tuning p, which enables stitching sub-optimal sub-trajectories and
overcoming the sparse reward issue.
D.2 Comparison with Online Decision Transformer
Since online decision transformer (ODT) [ 49] is built on top of upside-down RL [ 38,6,9], we
compare SUNG with ODT in this separate subsection. We follow the official implementation in
https://github.com/facebookresearch/online-dt for reproduction. We strictly follow the
original experimental setting in ODT to take 200K environment steps and only consider tasks with
medium and medium-replay offline datasets for online finetuning to achieve fair comparison.
As shown in Table 7, for almost all the settings, SUNG when combined with either TD3+BC or
CQL outperforms ODT by a large margin in both final performance and online improvement δ.
This is because ODT learns a reward-conditioned policy in a supervised learning paradigm, thereby
struggling to fully benefit from the trail-and-error paradigm used in traditional online RL. However, it
is worth noting that ODT enables users to customize the behavior of the agents by specifying different
conditions, which is not featured by traditional RL.
18

--- PAGE 19 ---
Table 7: Comparison of the averaged D4RL score on MuJoCo tasks, with a focus on both final perfor-
mance and online improvement δ. m = medium, m-r = medium-replay. We run 200K environment
steps to keep consistent with ODT’s setting. We report the mean value over 5 seeds.
ODT (offline) ODT (online) δ TD3+BC SUNG δ CQL SUNG δ
halfcheetah-m-v2 42.3 42.1 -0.2 48.0 85.5 37.5 48.1 86.7 38.6
hopper-m-v2 62.3 89.8 27.5 61.5 97.1 35.6 73.7 106.1 32.4
walker2d-m-v2 72.1 73.7 1.6 82.2 115.4 33.2 84.3 114.6 30.3
halfcheetah-m-r-v2 37.7 40.6 2.9 44.6 79.8 35.2 46.9 82.9 36.0
hopper-m-r-v2 83.2 88.3 5.1 55.9 104.1 48.2 96.0 83.5 -12.5
walker2d-m-r-v2 67.8 73.1 5.3 71.7 110.7 39.0 83.4 116.7 33.3
Total 365.4 407.6 42.2 364.9 592.6 228.7 432.4 590.5 158.1
1 50 100304050607080D4RL scorehalfcheetah-r
1 50 10055606570758085halfcheetah-m
1 50 10045505560657075halfcheetah-m-r
1 50 100203040506070halfcheetah-r
1 50 1004050607080halfcheetah-m
1 50 1004050607080halfcheetah-m-r
1 50 1000102030405060D4RL scorehopper-r
1 50 1000255075100125hopper-m
1 50 100708090100110hopper-m-r
1 50 1000204060hopper-r
1 50 10060708090100110hopper-m
1 50 10060708090100110hopper-m-r
1 50 100
Environment Steps (1e3)05101520D4RL scorewalker2d-r
1 50 100
Environment Steps (1e3)6080100120walker2d-m
1 50 100
Environment Steps (1e3)60708090100110walker2d-m-r
1 50 100
Environment Steps (1e3)05101520walker2d-r
1 50 100
Environment Steps (1e3)406080100walker2d-m
1 50 100
Environment Steps (1e3)6080100120walker2d-m-rSUNG(CQL) SUNG(TD3+BC)
BR O3F APL SUNG|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
Figure 5: Learning curves of SUNG, when combined with TD3+BC and CQL, for 100K environment
steps on MuJoCo tasks. We also include three best-performing baselines, BR, O3F and APL, for
comparison. w = Walker, r = random, m = medium, m-r = medium-replay. We report the mean D4RL
score and standard deviation over 5 training seeds with 10 evaluation episodes each. Note that O3F is
omitted when combined with CQL due to its divergence.
D.3 Learning Curves
We display the learning curves of SUNG when combined with TD3+BC and CQL for MuJoCo
domains in Fig. 5. Besides, we display the learning curves of SUNG when combined with SPOT
and CQL for AntMaze domains in Fig. 6. In most settings, we can observe that SUNG surpasses the
previous state-of-the-art methods in terms of both final performance and sample efficiency.
D.4 Ablation Study
Ablation Study with Variants of SUNG (Fig. 2). We consider the following variants of SUNG to
perform the ablation study over the components of our framework:
•w/o Opt. Exploration: We replace the proposed optimistic exploration strategy by the
default exploration provided by TD3 and SAC. For TD3, we select action with Gaussian
19

--- PAGE 20 ---
1 50 100707580859095100D4RL scoreumaze
1 50 100707580859095medium-p
1 50 100010203040506070large-p
1 50 100405060708090100umaze
1 50 1005060708090medium-p
1 50 100202530354045505560large-p
1 50 100
Environment Steps (1e3)020406080D4RL scoreumaze-d
1 50 100
Environment Steps (1e3)5060708090100medium-d
1 50 100
Environment Steps (1e3)01020304050large-d
1 50 100
Environment Steps (1e3)203040506070umaze-d
1 50 100
Environment Steps (1e3)505560657075808590medium-d
1 50 100
Environment Steps (1e3)20253035404550large-dSUNG(CQL) SUNG(SPOT)
offline-ft O3F APL SUNG|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|Figure 6: Learning curves of SUNG, when combined with SPOT and CQL, for 100K environment
steps on AntMaze tasks. We also include three best-performing baselines, offline-ft, O3F and APL,
for comparison. p = play, d = diverse. We report the mean D4RL score and standard deviation over 5
training seeds with 100 evaluation episodes each.
noise for exploration, i.e., πE(s) =πϕ(s) +ϵ, ϵ∼ N(0, δ). For SAC, we sample action
from the policy for exploration, i.e., πE(s) =a, a∼πϕ(·|s).
•Opt. Exploration w/o Unc.: We remove the uncertainty from the proposed exploration
strategy to greedily select action that maximizes Q value, i.e., πE(s) = arg max a∈CQ(s, a).
•Opt. Exploration w/o Q: We remove the Q value from the proposed exploration strategy to
greedily select action that maximizes the uncertainty, i.e., πE(s) = arg max a∈CU(s, a).
•w/o Adp. Exploitation: We replace the proposed adaptive exploitation method by the
standard online RL objective of TD3 and SAC, respectively.
•Unc. Estimation w/ Q std: We replace V AE by the standard deviation of two Q value
functions for uncertainty estimation, i.e.,
U(s, a) =σ(Qθ1(s, a), Qθ2(s, a)) =|Qθ1(s, a)−Qθ2(s, a)|.
Besides, we also consider two more ablation settings:
•Adp. Exploitation w/ Rand.: We construct the OOD state-action pair set DOOD by
randomly sampling from the mini-batch instead of using the proposed OOD sample identifier.
•OORB w/o Offline Data: We do not utilize transitions from offline dataset for online
finetuning by removing offline data from OORB.
We present the results of the two ablation studies above in Fig. 7 and Fig. 8, respectively. As shown
in Fig. 7, we can find when we use a random sampling strategy for OOD sample identification,
the performance on most settings deteriorates. As shown in Fig. 8, we observe that the removal of
offline data significantly hurts the offline-to-online performance. This is because online finetuning
can benefit from reusing the diverse behaviours contained in the offline dataset to avoid overfitting.
Hyper-parameter Analysis on Softmax Temperature α.Here, we give a hyper-parameter
analysis of the softmax temperature αin Eq. (8). Specifically, we investigate the choice of
α∈ {0.1,0.2,0.5,1.0,5.0,10.0,20.0,100.0}for MuJoCo domains in Fig. 9. As shown in the
figure, we can find that αis a relatively insensitive hyper-parameter for the offline-to-online per-
formance, and that the best-performing choice of αvaries among different settings. Thus, though
a careful tuning on αwill bring more offline-to-online improvement, we set α= 1.0in all the
experimental result throughout the paper to guarantee the simplicity of the proposed framework.
Hyper-parameter Analysis on OORB sampling probability pOORB .Furthermore, we give a
hyper-parameter analysis on pOORB for the simple-yet-effective OORB. Specifically, we investigate
20

--- PAGE 21 ---
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr40
20
02040Percent DifferenceSUNG(TD3+BC)
Adp. Exploitation w/ Rand.
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040SUNG(CQL)
Adp. Exploitation w/ Rand.Figure 7: Performance difference of the perfor-
mance of random strategy for OOD sample iden-
tification, compared with the full algorithm.
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr40
20
02040Percent DifferenceSUNG(TD3+BC)
OORB w/o Offline Data
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040SUNG(CQL)
OORB w/o Offline DataFigure 8: Performance difference of the perfor-
mance without offline data, compared with the
full algorithm.
0.1 0.2 0.5 1.0 5.0 10.0 20.0 100.0
hc-r
hop-r
w-r
hc-m
hop-m
w-m
hc-mr
hop-mr
w-mr
totalNormalized D4RL Score0.96 0.99 1.00 0.97 0.97 0.96 0.98 0.96
0.87 0.65 0.81 1.00 0.56 0.78 0.74 0.90
1.00 0.70 0.57 0.80 0.81 0.66 0.71 0.86
1.00 0.99 0.99 0.99 0.99 0.99 1.00 1.00
0.98 0.92 0.95 0.97 0.92 0.99 0.97 1.00
0.91 0.97 0.96 0.98 0.95 1.00 0.96 0.95
1.00 0.99 1.00 0.99 0.98 0.97 0.98 1.00
0.95 0.98 1.00 0.96 0.94 0.99 0.99 0.97
1.00 0.97 0.99 0.98 0.98 0.96 0.98 0.95
0.99 0.97 0.99 1.00 0.96 0.99 0.98 0.99SUNG(TD3+BC)
0.1 0.2 0.5 1.0 5.0 10.0 20.0 100.0
hc-r
hop-r
w-r
hc-m
hop-m
w-m
hc-mr
hop-mr
w-mr
totalNormalized D4RL Score0.49 0.51 0.72 0.87 1.00 0.97 0.97 0.98
0.78 1.00 0.73 0.90 0.80 0.54 0.78 0.74
0.78 1.00 0.65 0.94 0.67 0.76 0.95 0.90
0.83 0.84 0.86 0.89 0.98 1.00 1.00 0.98
0.55 0.76 0.93 0.99 1.00 0.99 0.99 0.98
0.88 0.93 0.91 0.86 0.78 0.98 1.00 0.94
0.87 0.88 0.87 0.94 0.99 0.99 1.00 0.98
0.43 0.63 0.71 1.00 0.76 0.79 0.79 0.92
0.69 0.82 0.86 0.97 1.00 0.96 0.91 0.92
0.73 0.84 0.88 0.99 0.97 0.98 1.00 1.00SUNG(CQL)
0.600.650.700.750.800.850.900.951.00
0.50.60.70.80.91.0
Figure 9: Heatmap for performance of SUNG with different softmax temperature α. The results of
each setting are normalized with maximum normalization for better visualization. hc = HalfCheetah,
hop = Hopper, w = Walker, r = random, m = medium, mr = medium-replay.
the choice of OORB sampling probability pOORB ∈ {0.0,0.1,0.2,0.4,0.6,0.8,1.0}for MuJoCo
domains in Fig. 10. Note that a higher probability pOORB indicates more reuse of the offline dataset
during policy learning. From the results, we can observe that pOORB = 0.1performs the best
when combined with either TD3+BC and CQL, while either setting pOORB too conservatively (i.e.,
pOORB = 0) or too adversely (i.e., pOORB = 1.0) hurts the performance. As pointed out by Zheng
et al. [ 48], offline data can prevent agents from prematurely converging to sub-optimal policies due
to the potential data diversity, while online data can stabilize training and accelerate convergence.
Thus, it is crucial for sample-efficient offline-to-online RL algorithms to incorporate both offline and
online data during policy learning.
D.5 Decay on Regularization Weight λ
An intuition for the proposed adaptive exploitation method is that the regularization weight λin
Eq.(9)should be decayed during online finetuning, since the state-action distribution shift gradually
eliminates with the environment step. Hence, in this subsection, we explore the decay mechanism for
λby linearly decreasing λto{0.0,0.2,0.4,0.6,0.8,1.0}times its initial value at the end of online
finetuning. As shown in Fig. 11, no decay mechanism outperforms the other in terms of average
total performance when combined with either TD3+BC or CQL. We conjecture that this is because
100K environment steps are not enough for agents to fully overcome the distribution shift issue
in most settings. We also note that no decay mechanism does not consistently outperform all the
other settings, which depends on the quality of the dataset and the difficulty of the task. Hence,
21

--- PAGE 22 ---
0.0 0.1 0.2 0.4 0.6 0.8 1.0
pOORBhc-r
hop-r
w-r
hc-m
hop-m
w-m
hc-mr
hop-mr
w-mr
totalNormalized D4RL Score0.93 1.00 0.99 0.98 0.92 0.83 0.58
0.72 1.00 0.71 0.96 0.38 0.38 0.33
0.59 0.90 1.00 0.75 0.57 0.76 0.39
0.98 1.00 1.00 0.95 0.95 0.88 0.80
0.90 0.97 0.97 0.97 0.97 1.00 0.92
0.72 1.00 0.99 0.94 0.90 0.90 0.79
0.98 1.00 0.98 0.98 0.93 0.93 0.86
0.73 0.98 0.98 1.00 0.97 0.96 0.92
0.89 1.00 1.00 0.96 0.94 0.92 0.85
0.86 1.00 0.98 0.97 0.91 0.90 0.80SUNG(TD3+BC)
0.0 0.1 0.2 0.4 0.6 0.8 1.0
pOORBhc-r
hop-r
w-r
hc-m
hop-m
w-m
hc-mr
hop-mr
w-mr
totalNormalized D4RL Score0.89 0.93 1.00 0.88 0.77 0.75 0.55
0.42 1.00 0.79 0.62 0.70 0.72 0.57
0.35 0.90 0.83 1.00 0.85 0.72 0.54
0.98 0.99 1.00 1.00 0.91 0.90 0.77
0.92 1.00 1.00 0.96 0.95 0.96 0.93
1.00 0.82 0.90 0.90 0.87 0.84 0.78
0.99 1.00 1.00 0.97 0.80 0.82 0.75
0.77 0.98 0.84 0.92 0.97 1.00 0.94
0.89 1.00 0.97 0.88 0.88 0.87 0.86
0.91 1.00 0.98 0.95 0.91 0.91 0.82SUNG(CQL)
0.40.50.60.70.80.91.0
0.40.50.60.70.80.91.0Figure 10: Heatmap for performance of SUNG with different OORB sampling probability pOORB .
The results of each setting are normalized with maximum normalization for better visualization. hc =
HalfCheetah, hop = Hopper, w = Walker, r = random, m = medium, mr = medium-replay.
0.0 0.2 0.4 0.6 0.8 1.0
Decay End707580Average D4RL Score
SUNG (TD3+BC)
0.0 0.2 0.4 0.6 0.8 1.0
Decay End657075
SUNG (CQL)
best baseline
Figure 11: Performance compar-
ison of SUNG with different de-
cay end for regularization weight
λ, where xaxis means decay until
reaching x·λat the end of online
finetuning. We present the D4RL
score averaged over MuJoCo do-
mains. x= 1.0represents SUNG
without decay mechanism.
careful tuning of the decay mechanism may bring extra offline-to-online improvement, especially for
a larger amount of online environment steps. In this work, we do not utilize the decay mechanism for
simplicity. Moreover, we also remark that SUNG with different decay mechanism still outperforms
the best-performing baseline, which further demonstrates the superiority of the proposed framework.
E Discussions
While achieving promising performance, the proposed framework also has some inherent limitations.
Some recently developed offline RL methods can be built on top of on-policy RL [ 50] and supervised
learning paradigm [ 6,9,41] instead of pure off-policy RL. Hence, one limitation is that SUNG is not
compatible with agents pretrained by these new paradigms for offline RL. However, we argue that the
insights of SUNG can be applied to them in principle. We leave the exploration of its implementation
for combining with on-policy RL and supervised learning as an interesting future work. In the current
version, SUNG investigates the offline-to-online RL setting to solve tasks whose inputs are the states
of the simulated environment. It would be interesting to see whether SUNG is also superior with the
visual RL setting [ 29]. Moreover, further investigation on uncertainty estimation, replay buffer, and
more backbone offline RL methods needs to be explored. Finally, we remark that the exploration of
its application on real-world tasks is also an interesting future work.
We hope our work showcases the potential of the offline pretraining and online finetuning paradigm
in effectively and efficiently deploying RL algorithms in real-world applications. Besides, we hope
that our work inspires future research to develop more sample-efficient offline-to-online RL methods.
We do not see negative societal impacts of our work in practice.
22
