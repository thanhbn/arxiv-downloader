# 2310.11971.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2310.11971.pdf
# Kích thước tệp: 625499 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Bản thảo. Đang được xem xét.
CẢI THIỆN TÍNH TỔNG QUÁT HOÁ CỦA SỰ CĂNG CHỈNH VỚI
SỞ THÍCH CON NGƯỜI THÔNG QUA HỌC BẤT BIẾN NHÓM

Rui Zheng1∗†, Wei Shen1∗†, Yuan Hua2, Wenbin Lai1, Shihan Dou1, Yuhao Zhou1,
Zhiheng Xi1,Xiao Wang1,Haoran Huang2,Tao Gui1,Qi Zhang1,Xuanjing Huang1
1Nhóm NLP, Đại học Fudan
2ByteDance Inc
{rzheng20, tgui, qz }@fudan.edu.cn

TÓM TẮT
Thành công của các trợ lý AI dựa trên mô hình ngôn ngữ (LLM) phụ thuộc rất nhiều vào Học Tăng cường từ Phản hồi Con người (RLHF), cho phép tạo ra các phản hồi phù hợp hơn với sở thích con người. Là những trợ lý AI toàn năng, ngày càng có kỳ vọng rằng chúng sẽ hoạt động ổn định trên nhiều lĩnh vực khác nhau. Tuy nhiên, các nghiên cứu trước đây cho thấy Học Tăng cường (RL) thường khai thác các đường tắt để đạt được phần thưởng cao và bỏ qua các mẫu khó. Việc tập trung vào những lợi ích phần thưởng nhanh chóng này làm suy yếu cả tính ổn định trong huấn luyện và khả năng tổng quát hóa của mô hình đối với dữ liệu mới, chưa từng thấy. Trong nghiên cứu này, chúng tôi đề xuất một phương pháp mới có thể học một chính sách nhất quán thông qua RL trên các nhóm dữ liệu hoặc lĩnh vực khác nhau. Do những thách thức liên quan đến việc thu thập chú thích nhóm, phương pháp của chúng tôi tự động phân loại dữ liệu thành các nhóm khác nhau, cố tình tối đa hóa phương sai hiệu suất. Sau đó, chúng tôi tối ưu hóa chính sách để hoạt động tốt trên các nhóm khó. Cuối cùng, tận dụng các nhóm đã thiết lập, phương pháp của chúng tôi điều chỉnh không gian khám phá một cách thích ứng, phân bổ nhiều năng lực học hơn cho dữ liệu khó hơn và ngăn mô hình tối ưu hóa quá mức trên dữ liệu đơn giản hơn. Kết quả thực nghiệm cho thấy phương pháp của chúng tôi cải thiện đáng kể tính ổn định huấn luyện và khả năng tổng quát hóa mô hình.

1 GIỚI THIỆU
Trong lĩnh vực mô hình ngôn ngữ phát triển nhanh chóng, Học Tăng cường từ Phản hồi Con người (RLHF) đã nổi lên như một thành phần quan trọng, với mục tiêu căng chỉnh đầu ra mô hình với ý định con người (Ouyang et al., 2022; Bai et al., 2022b). Quá trình này tích hợp mô hình hóa phần thưởng, nơi các chú thích viên con người xếp hạng các phản hồi mô hình khác nhau dựa trên sở thích của họ, tiếp theo là các giai đoạn học tăng cường (RL) để tinh chỉnh và tối ưu hóa hành vi của mô hình. Do tính ứng dụng toàn cầu của nó, một mô hình như vậy được kỳ vọng sẽ nắm bắt được nhiều ý định con người và xử lý các tình huống đa dạng (Askell et al., 2021). Trong bối cảnh này, khả năng tổng quát hóa – hiệu suất nhất quán trong cả các tình huống đã thấy và chưa thấy – trở nên vô cùng quan trọng.

RLHF đối mặt với một thách thức quan trọng trong việc tổng quát hóa hiệu quả các ý định con người, làm dấy lên lo ngại về khả năng thực sự của nó ngoài các thiết lập có giám sát (Casper et al., 2023). Sự phân biệt này rất quan trọng vì nó ảnh hưởng đến độ tin cậy của mô hình trong các tình huống không thấy trước và các kịch bản tồi tệ nhất. Các nghiên cứu trước đây cho thấy RL thường có xu hướng tập trung quá mức vào dữ liệu đơn giản và có phần thưởng cao trong khi bỏ qua việc học các mẫu thử thách (Ngo, 2022; di Langosco et al., 2022; Zheng et al., 2023b). Hơn nữa, khi có các khiếm khuyết trong mô hình phần thưởng, hành vi này có thể dẫn mô hình vào tình huống "hack phần thưởng" (Skalse et al., 2022; Pan et al., 2022), dẫn đến các đầu ra văn bản vô nghĩa. Tất cả những yếu tố này góp phần vào hiệu suất không nhất quán của mô hình trên dữ liệu từ các nhóm khác nhau¹, với khả năng tổng quát hóa kém (Casper et al., 2023; Song et al., 2023).

∗Đóng góp ngang nhau.
†Công việc được thực hiện trong thời gian thực tập tại ByteDance Inc.
¹Chúng tôi sử dụng thay thế các thuật ngữ "nhóm" và "lĩnh vực".

--- TRANG 2 ---
Bản thảo. Đang được xem xét.

Trong lĩnh vực học sâu, việc xử lý dữ liệu huấn luyện từ các nhóm khác nhau là một thách thức phổ biến (Levy et al., 2020). Thách thức này trở nên nổi bật hơn trong RL vì phân phối trạng thái-hành động liên tục thay đổi khi chính sách được tối ưu hóa (Bai et al., 2022a; Xu et al., 2023). Điều này có nghĩa là các thuật toán phải học và thích ứng dưới phân phối dữ liệu liên tục thay đổi, làm tăng đáng kể độ khó và sự bất ổn của quá trình học. Hầu hết các phương pháp RL hiện tại chủ yếu tập trung vào việc tối đa hóa phần thưởng tương lai dự kiến, nhưng tính mạnh mẽ của chúng thường thiếu sót khi xử lý dữ liệu từ các nguồn phân phối khác nhau (Tang et al., 2019; Zhang et al., 2021). Lý do là các phương pháp này thường bỏ qua sự khác biệt giữa các nhóm dữ liệu và không thể phạt hiệu quả các sự kiện hiếm nhưng thảm khốc có thể xảy ra (Tang et al., 2019; Javed et al., 2021). Để ngăn đầu ra của mô hình lệch quá xa khỏi mô hình phần thưởng dự kiến, nhiều kỹ thuật đưa ra một hình phạt phân kỳ Kullback-Leibler (KL) (Stiennon et al., 2020; Ouyang et al., 2022). Tuy nhiên, điểm yếu của phương pháp này là cường độ ràng buộc của nó thường được đặt bởi dữ liệu ngoại lệ có khả năng nhất, hạn chế khả năng của thuật toán trong việc xử lý dữ liệu khó (Laidlaw et al., 2023).

Trong bài báo này, chúng tôi đề xuất một phương pháp căng chỉnh với khả năng tổng quát hóa mạnh nhằm đạt được hiệu suất mô hình nhất quán trên nhiều nhóm dữ liệu. Không giống như các phương pháp hiện có, kỹ thuật của chúng tôi không chỉ tập trung vào việc tối đa hóa phần thưởng dự kiến tổng thể mà còn về việc giảm phương sai giữa các nhóm dữ liệu. Bằng cách tối đa hóa sự khác biệt hiệu suất giữa các nhóm dữ liệu khác nhau, phương pháp của chúng tôi tự động phân tách dữ liệu thành các nhóm riêng biệt mà không cần chú thích thủ công. Thông qua phương pháp đối kháng này, phương pháp của chúng tôi cải thiện đáng kể khả năng tổng quát hóa mô hình và tính ổn định huấn luyện. Hơn nữa, dựa trên hiệu suất của từng nhóm, phương pháp của chúng tôi có thể điều chỉnh thích ứng cường độ của số hạng phạt KL, cung cấp không gian khám phá lớn hơn để tìm các chính sách tốt hơn để xử lý dữ liệu khó.

Những đóng góp chính của chúng tôi như sau:
• Chúng tôi giới thiệu một khung thống nhất đảm bảo căng chỉnh mạnh mẽ về mặt phân phối bằng cách thích ứng động với các nhóm dữ liệu khác nhau, tăng cường khả năng của mô hình trong việc xử lý các phân phối dữ liệu khác nhau.
• Trong khung này, chúng tôi phát triển một phương pháp để suy luận nhãn nhóm. Phương pháp này sử dụng nhãn nhóm để thực hiện các ràng buộc KL thích ứng, đảm bảo hành vi mô hình tối ưu trên các tập con dữ liệu khác nhau, từ đó góp phần vào tính mạnh mẽ và ổn định của mô hình chúng tôi.
• Chúng tôi chứng minh thực nghiệm rằng phương pháp đề xuất của chúng tôi vượt trội hơn thuật toán PPO truyền thống trong bối cảnh trợ lý AI tổng quát và thiết lập tóm tắt. Nó thể hiện khả năng tổng quát hóa nổi bật, cải thiện đáng kể các chỉ số ổn định và hiệu suất, củng cố thêm tính hữu ích thực tế của phương pháp chúng tôi.

2 CÔNG TRÌNH LIÊN QUAN
Mặc dù LLM có những khả năng đầy hứa hẹn, chúng dễ bị biểu hiện các hành vi không mong muốn, chẳng hạn như bịa đặt sự thật, tạo ra nội dung thiên vị hoặc độc hại, hoặc thậm chí tạo ra tài liệu có hại cho con người (Bender et al., 2021; Bommasani et al., 2021). Do đó, việc căng chỉnh LLM với ý định con người và các giá trị xã hội là rất cần thiết. Ví dụ, chúng nên hữu ích, trung thực và vô hại (3H) (Ouyang et al., 2022; Bai et al., 2022b; Thoppilan et al., 2022). RL cung cấp phương pháp trực tiếp nhất để đạt được mục tiêu này. Trong RL, các tác nhân yêu cầu tín hiệu giám sát từ các mô hình phần thưởng hoạt động như đại diện cho con người. Sau đó chúng được tinh chỉnh thông qua nhiều lần lặp trong khung RL, một quá trình được gọi là Học Tăng cường từ Phản hồi Con người. Một số nỗ lực gần đây đã được thực hiện theo hướng này (Zhang et al., 2023; Rafailov et al., 2023; Hu et al., 2023).

Trong RL, mô hình chính sách đối mặt với những thách thức đáng kể liên quan đến khả năng tổng quát hóa của nó (Casper et al., 2023). Đầu tiên, các chính sách có thể thể hiện khả năng tổng quát hóa kém, đặc biệt khi có mối tương quan gây hiểu lầm giữa mục tiêu thực và các sự kiện khác (McKinney et al., 2023; Tien et al., 2023). Hơn nữa, các tác nhân RL có xu hướng tìm kiếm các giải pháp tiện lợi, có thể dẫn chúng tránh dữ liệu khó để có được phần thưởng cao, tương tự như những gì quan sát thấy trong các mô hình trả lời câu hỏi (Turner et al., 2021; Casper et al., 2023). Cuối cùng, việc tối ưu hóa các tác nhân cho phần thưởng không hoàn hảo có thể dẫn đến hack phần thưởng, dẫn đến việc tạo ra các đầu ra mà trong khi mang lại phần thưởng cao, lại vô nghĩa (Skalse et al., 2022; Pan et al., 2022). Tất cả những thách thức này có thể dẫn đến hiệu suất kém trong việc nắm bắt ý định thực sự của con người, nhấn mạnh sự cần thiết của việc học một chính sách có thể hoạt động nhất quán trên các lĩnh vực dữ liệu khác nhau.

--- TRANG 3 ---
Bản thảo. Đang được xem xét.

Nhiều thuật toán RL tập trung vào cải thiện khả năng tổng quát hóa của các chính sách trong các môi trường khác nhau (Javed et al., 2021; Sonar et al., 2021) và các kịch bản trường hợp xấu nhất (Tang et al., 2019; Brown et al., 2020). Tuy nhiên, hầu hết các phương pháp này phụ thuộc nhiều vào mạng nơ-ron Bayesian (Brown et al., 2020; Javed et al., 2021), và công thức của vấn đề khác với LLM (Sonar et al., 2021). Phương pháp của chúng tôi được lấy cảm hứng từ học bất biến (Arjovsky et al., 2019; Creager et al., 2021), nhằm tăng cường tính ổn định trong các lĩnh vực không quen thuộc trong quá trình kiểm tra bằng cách học tìm các đặc trưng bất biến trên các nhóm dữ liệu khác nhau, từ đó học một chính sách mạnh mẽ hơn. Trong nghiên cứu gần đây, học bất biến đã được mở rộng cho các kịch bản không yêu cầu nhãn nhóm trước (Creager et al., 2021; Liu et al., 2021; Chen et al., 2022). Thông thường, các phương pháp này đầu tiên huấn luyện một mô hình tham chiếu để thu thập thông tin mất mát từ dữ liệu khác nhau và sau đó huấn luyện một bộ phân loại bổ sung để tối đa hóa các vi phạm mục tiêu học bất biến cho việc nhóm (Creager et al., 2021). Ngược lại, phương pháp của chúng tôi sử dụng một khung thống nhất thực hiện lặp đi lặp lại việc suy luận nhãn nhóm và học chính sách bất biến. Theo hiểu biết của chúng tôi, đây là nỗ lực đầu tiên giới thiệu khái niệm học bất biến nhóm vào RL.

3 KIẾN THỨC CƠ BẢN
Chúng tôi xem xét đường ống RLHF từ Ziegler et al. (2019), đã được áp dụng cho các tác vụ như đối thoại (Glaese et al., 2022), làm theo hướng dẫn (Ouyang et al., 2022), và tóm tắt (Stiennon et al., 2020). Đường ống này thường bao gồm ba giai đoạn: tinh chỉnh có giám sát (SFT), lấy mẫu sở thích và huấn luyện mô hình phần thưởng (RM), và tinh chỉnh RL sử dụng tối ưu hóa chính sách gần đúng (PPO) (Schulman et al., 2017). Quá trình thường bắt đầu với một mô hình ngôn ngữ được huấn luyện trước chung chung, trải qua học có giám sát trên một tập dữ liệu chất lượng cao cho các tác vụ hạ nguồn cụ thể, dẫn đến một mô hình được ký hiệu là πSFT. Trong nghiên cứu này, chúng tôi tập trung vào cải thiện hai giai đoạn còn lại.

Mô hình hóa phần thưởng từ sở thích con người. Trong giai đoạn thứ hai, mô hình SFT πSFT được gợi ý với một truy vấn người dùng ký hiệu là x để tạo ra hai đầu ra riêng biệt (y1, y2)∼πSFT(y|x). Các nhãn viên con người được hướng dẫn chọn đầu ra ưa thích của họ, dẫn đến ygood≻ybad, trong đó ygood và ybad đại diện cho đầu ra được chọn và bị từ chối, tương ứng, từ cặp (y1, y2). Bằng cách theo mô hình Bradley-Terry (Bradley & Terry, 1952), chúng tôi công thức hóa một phân phối sở thích bằng cách sử dụng hàm phần thưởng rψ(x, y) như được nêu dưới đây:

pψ(ygood≻ybad|x) = exp (rψ(x, ygood)) / (exp (rψ(x, ygood)) + exp (rψ(x, ybad))) (1)

Coi vấn đề như một tác vụ phân loại nhị phân tạo ra hàm mất mát âm log-likelihood:

L(rψ) = −E(x,ygood,ybad)∼Drm[logσ(rψ(x, ygood)−rψ(x, ybad))] (2)

trong đó tập dữ liệu được tạo thành từ các so sánh ký hiệu là Drm={x(i), y(i)good, y(i)bad}Ni=1, và σ là hàm logistic. Trong lĩnh vực LM, mạng rψ(x, y) thường được khởi tạo sử dụng mô hình SFT πSFT(y|x). Sau đó nó kết hợp một lớp tuyến tính bổ sung trên lớp transformer cuối để tạo ra một dự đoán vô hướng đơn lẻ đại diện cho giá trị phần thưởng.

Tinh chỉnh RL. Trong giai đoạn RL, chúng tôi sử dụng hàm phần thưởng đã học để cung cấp phản hồi cho mô hình ngôn ngữ. Cụ thể hơn, chúng tôi tối ưu hóa mô hình chính sách πRL để tối đa hóa mục tiêu phần thưởng sau:

rtotal=rψ(x, y)−ηKL(πRL(y|x)∥πSFT(y|x)) (3)

trong đó η là một hệ số điều chỉnh độ lớn của hình phạt KL. Số hạng phân kỳ KL phục vụ hai mục đích chính trong bối cảnh này. Đầu tiên, nó hoạt động như một phần thưởng entropy, bảo tồn sự đa dạng tạo sinh và ngăn ngừa sự sụp đổ chế độ thành các câu trả lời có phần thưởng cao đơn lẻ (Jaques et al., 2019). Thứ hai, nó đảm bảo rằng đầu ra của chính sách RL không lệch quá mức khỏi phân phối nơi mô hình phần thưởng chính xác (Laidlaw et al., 2023).

4 CHÍNH SÁCH BẤT BIẾN NHÓM
Mục tiêu của RL là tìm một chính sách tối ưu để tối đa hóa lợi nhuận tương lai dự kiến (có thể được giảm giá). Tuy nhiên, việc tối ưu hóa cho lợi nhuận trung bình trở nên mong manh khi có sự thay đổi phân phối.

--- TRANG 4 ---
Bản thảo. Đang được xem xét.

PPO Tiêu chuẩn                     Chính sách Bất biến Nhóm
Lợi nhuận Dự kiến    Sự Khác biệt Lợi nhuận    Phân phối Lợi nhuận

Hình 1: Trái: PPO tiêu chuẩn tối đa hóa lợi nhuận tương lai dự kiến (đường đỏ). Phải: Phương pháp của chúng tôi cũng tối thiểu hóa sự khác biệt hiệu suất giữa các nhóm dữ liệu khác nhau (đường vàng).

Ví dụ, khi phân phối lợi nhuận thể hiện phương sai cao hoặc có đuôi dài, việc tìm kiếm một chính sách tối đa hóa kỳ vọng của phân phối có thể không lý tưởng; điều này là do một chính sách phương sai cao (và do đó rủi ro cao hơn) có thể suy giảm nghiêm trọng về hiệu suất khi tiếp xúc với các mẫu đuôi dài (Tang et al., 2019). Thay vào đó, mục tiêu của chúng tôi là học các chính sách mạnh mẽ hơn có thể đạt được hiệu suất cao trên bất kỳ phân phối nào gần với phân phối huấn luyện.

4.1 GRADIENT CHÍNH SÁCH
Chúng tôi sử dụng ký hiệu RL điển hình, trong đó tại mỗi bước thời gian t, tác nhân (tức là trợ lý AI) nhận một trạng thái st (tức là lịch sử đối thoại), bao gồm tất cả văn bản đối thoại đến thời điểm này, cả của trợ lý và con người. Dựa trên chính sách πRLθ của nó thường được tham số hóa bởi θ, hành động at của tác nhân là tạo ra token tiếp theo và πRLθ(a|s) là xác suất thực hiện hành động a trong trạng thái s. Sau đó, môi trường trả về một phần thưởng r(st, at). Tác nhân sau đó chuyển đến trạng thái tiếp theo st+1 với xác suất chuyển tiếp p(st+1|st, at). Mục tiêu của RL là tìm một chiến lược hành vi tối ưu cho tác nhân để tối đa hóa phần thưởng tích lũy (tức là lợi nhuận) trên một quỹ đạo τ={s1, a1, . . . , sT, aT}.

Một dạng tổng quát của mục tiêu tối ưu hóa cho gradient chính sách được đưa ra như (Mnih et al., 2016):

maxθ Eτ∼πRLθ[∑Tt=1πRLθ(at|st)Rt] (4)

trong đó Eτ∼πRLθ đề cập đến kỳ vọng dưới phân phối của các quỹ đạo được tạo ra bởi việc chạy chính sách πθ trong môi trường, và lợi nhuận Rt=∑Tt'=tγt'r(st', at') là tổng giảm giá của phần thưởng từ bước thời gian t với hệ số γ∈[0,1). Nếu lợi nhuận thuận lợi, tất cả các hành động được "củng cố" bằng cách tăng xác suất được chọn của chúng. Ưu điểm của phương pháp này nằm ở tính chất không thiên vị của nó, vì chúng ta chỉ dựa vào lợi nhuận thực tế thu được thay vì ước tính nó. Phương sai này xuất phát từ việc các quỹ đạo khác nhau có thể dẫn đến lợi nhuận đa dạng do tính ngẫu nhiên của môi trường (các sự kiện ngẫu nhiên trong một tập) và chính sách. Trong Eq. (4), mục tiêu có thể được tối ưu hóa sử dụng PPO (Schulman et al., 2017), một phương pháp gradient chính sách phổ biến được biết đến với việc tăng cường độ tin cậy của quá trình học.

Động cơ của phương pháp đề xuất. Như được hiển thị trong Hình 1, trục x đại diện cho lợi nhuận R, và trục y đại diện cho mật độ xác suất. Nói chung, việc sử dụng các phương pháp gradient chính sách khiến giá trị kỳ vọng của phân phối phần thưởng dịch chuyển sang phải dọc theo trục x. Điều này ngụ ý rằng đầu ra của chính sách đạt được phần thưởng cao hơn. Tuy nhiên, khi lợi nhuận tương lai không chắc chắn, việc tối ưu hóa chỉ cho lợi nhuận trung bình tối đa có thể không lý tưởng. Ví dụ, khi phân phối lợi nhuận thể hiện phương sai cao hoặc có đuôi nặng, một chính sách có thể ưa thích một hành vi với lợi nhuận kỳ vọng cao hơn nhưng cũng phương sai cao hơn (tức là tổng quát hóa kém) hơn một hành vi với lợi nhuận kỳ vọng thấp hơn một chút nhưng phương sai thấp hơn (tổng quát hóa tốt). Để giải quyết vấn đề này, chúng tôi nhằm học một chính sách được tổng quát hóa tốt. Chúng ta có thể đạt được điều này bằng cách giảm sự khác biệt giữa các nhóm dữ liệu khác nhau.

--- TRANG 5 ---
Bản thảo. Đang được xem xét.

4.2 RÀNG BUỘC BẤT BIẾN NHÓM
Trong lĩnh vực học bất biến nhóm, thuật ngữ "nhóm" thường đề cập đến các phân phối dữ liệu khác nhau hoặc các tập con dữ liệu đại diện cho sự không chắc chắn hoặc biến đổi trong dữ liệu (Arjovsky et al., 2019). Giả định rằng dữ liệu huấn luyện D={Dg}g∈Gobs đã được thu thập từ nhiều nhóm quan sát Gobs, mục tiêu chính của học bất biến là xác định các đặc trưng và mẫu giữ nhất quán trên các nhóm hoặc phân phối dữ liệu khác nhau. Phương pháp này vốn dĩ không khuyến khích mô hình dựa vào các mối tương quan dễ dàng, phi nhân quả có thể phổ biến trong một tập con của dữ liệu nhưng không thể tổng quát hóa trên toàn bộ. Các nghiên cứu như Tối thiểu hóa Rủi ro Bất biến (Arjovsky et al., 2019) và Ngoại suy Rủi ro (Krueger et al., 2021) đã chứng minh cách các nguyên lý học bất biến giúp xác định các mối quan hệ mạnh mẽ và nhân quả hơn trong dữ liệu, do đó giảm sự phụ thuộc vào các đường tắt. Liên quan đến học bất biến cho RL, chúng tôi nhằm học một chính sách πRLθ(x) hoạt động nhất quán trong mỗi nhóm, thỏa mãn Ràng buộc Bất biến Nhóm (GIC) như sau:

Eτ∼g1[∑Tt=1πRLθ(at|st)Rt] = Eτ∼g2[∑Tt=1πRLθ(at|st)Rt], ∀g1, g2∈ Gobs (5)

Một cách trực quan, chính sách bất biến πRLθ(at|st) giải quyết vấn đề bỏ qua các mẫu thử thách bằng cách đảm bảo hiệu suất đồng đều trên các nhóm khác nhau. Điều này đặc biệt liên quan trong các kịch bản nơi phân phối dữ liệu không cân bằng hoặc nơi một số mẫu được đại diện ít hơn. Bằng cách tối ưu hóa cho hiệu suất bất biến, các mô hình được khuyến khích học từ tất cả các phần của dữ liệu, bao gồm những phần khó hơn. Ngoài ra, khái niệm Tối ưu hóa Mạnh mẽ Phân phối (DRO) (Levy et al., 2020) phù hợp chặt chẽ với học bất biến nhóm. DRO đã được chứng minh thực nghiệm là tăng cường sự cân bằng và công bằng của kết quả trong các mô hình học máy, hỗ trợ thêm cho hiệu quả của phương pháp này (Sagawa et al., 2020).

4.3 HỌC CHÍNH SÁCH BẤT BIẾN
Các phương pháp truyền thống cho học bất biến thường có một nhược điểm đáng kể: chúng yêu cầu tập dữ liệu được chia thành nhiều lĩnh vực hoặc nhóm dựa trên các đặc điểm dữ liệu nhất định và kiến thức trước (Arjovsky et al., 2019; Levy et al., 2020). Việc chia các nhóm này nên ngầm định nghĩa các thay đổi mà thuật toán học cần duy trì bất biến hoặc thể hiện tính mạnh mẽ. Tuy nhiên, việc có được các phân chia nhóm này trong quá trình huấn luyện thường khó khăn vì việc gắn nhãn chúng tốn kém, và việc tìm tiêu chí tối ưu cho việc nhóm có thể khó. Trong RL, khi huấn luyện chính sách tiến triển, môi trường liên tục thay đổi, và các cặp trạng thái-hành động được sử dụng để tối ưu hóa mô hình không cố định. Do đó, trong RL, chúng ta cần có khả năng xác định động các nhãn của các nhóm dữ liệu. Trong bài báo này, chúng tôi giới thiệu một khung mới cho học chính sách bất biến không dựa vào kiến thức lĩnh vực hoặc nhóm trước. Trong giai đoạn đầu tiên, chúng tôi huấn luyện một mô hình suy luận để dự đoán nhãn nhóm. Sau đó, trong giai đoạn thứ hai, chúng tôi huấn luyện học chính sách bất biến dựa trên các nhãn này.

Giai đoạn 1: Suy luận Nhãn Nhóm. Lợi nhuận Rg(θ) của một nhóm cụ thể g là một khái niệm quan trọng phụ thuộc vào các nhãn nhóm trong tập dữ liệu của chúng ta. Để biểu thị liệu một quỹ đạo cụ thể τi có thuộc nhóm g hay không, chúng tôi sử dụng hàm chỉ thị 1{gτi=g}. Lợi nhuận cho mỗi nhóm g sau đó có thể được biểu diễn toán học như sau:

Rg(θ) = (1/∑i' 1{gτi'=g}) ∑i 1{gτi=g}[∑Tt=1πθ(at|st)Rt] (6)

Số hạng trong ngoặc vuông đại diện cho lợi nhuận kỳ vọng dọc theo một quỹ đạo cụ thể. Lợi nhuận kỳ vọng này sau đó được tính trung bình trên tất cả các quỹ đạo trong nhóm g, cung cấp lợi nhuận kỳ vọng cho nhóm. Chúng tôi thay thế việc chia nhóm thủ công bằng một phân phối xác suất, ký hiệu là pφ(g|τ), đại diện cho một phân công mềm của quỹ đạo τ cho nhóm thứ g. Chúng tôi giao nhiệm vụ suy luận nhãn nhóm dữ liệu cho mô hình critic và giới thiệu một bộ phân loại suy luận φ trong lớp cuối của mô hình critic để đạt được mục tiêu này. Lựa chọn này được thực hiện vì các mô hình critic trong RL thường được sử dụng để ước tính các hàm giá trị của các trạng thái hoặc cặp trạng thái-hành động. Ước tính này giúp phân biệt sự khác biệt về lợi nhuận giữa các quỹ đạo khác nhau. Chính thức hơn, chúng ta có thể biểu diễn ước tính xác suất của bộ phân loại cho nhãn g là pφ(g|τ), đại diện cho một phân công mềm của dữ liệu τ cho nhóm

--- TRANG 6 ---
Bản thảo. Đang được xem xét.

[Biểu đồ hiển thị sự so sánh hiệu suất của hai nhóm dữ liệu qua 4 đồ thị]

Hình 2: So sánh hiệu suất của hai nhóm dữ liệu. Hai hình bên trái hiển thị các đặc điểm hiệu suất khác nhau giữa nhóm đơn giản và nhóm khó. Nhóm đơn giản với sự gia tăng phần thưởng nhanh chóng nhưng biến động cao; nhóm khó với sự cải thiện chậm nhưng ổn định. Hai hình bên phải hiển thị cách học chính sách bất biến của chúng tôi tối thiểu hóa khoảng cách hiệu suất, tăng cường khả năng tổng quát hóa của chính sách.

g. Để suy luận các nhóm này, chúng tôi tối ưu hóa pφ(g|τ) để vi phạm tối đa ràng buộc bất biến nhóm. Điều này tương ứng với việc tối đa hóa phương sai của lợi nhuận (Krueger et al., 2021):

Rvar(θ, φ) = Var({Rg1(θ), Rg2(θ), . . . , RgM(θ)}) (7)

trong đó M là số lượng nhóm và Var(·) biểu thị phép toán tính phương sai thống kê. Các thống kê liên quan đến nhóm khác cũng có thể được sử dụng làm mục tiêu tối ưu hóa, chẳng hạn như gradient (Arjovsky et al., 2019), phương sai (Krueger et al., 2021), và lỗi hiệu chuẩn (Wald et al., 2021). Chúng tôi kết hợp mục tiêu từ Eq. (7) như một số hạng chính quy vào mục tiêu tối ưu hóa của mô hình critic, nhằm tối đa hóa số hạng này càng nhiều càng tốt, với hệ số chính quy βcritic = 1.

Giai đoạn 2: Học Chính sách Bất biến. Tiếp theo, chúng tôi kết hợp số hạng chính quy từ Eq. (7) vào mục tiêu tối ưu hóa của gradient chính sách trong Eq. (4):

maxθ Eτ∼πRLθ[∑Tt=1πθ(at|st)Rt] − βpolicyRvar(θ, φ) (8)

Ở đây, số hạng đầu tiên đại diện cho mục tiêu chung của việc tối đa hóa hiệu suất của chính sách về mặt lợi nhuận kỳ vọng, một khía cạnh cơ bản của tối ưu hóa chính sách trong RL. Số hạng thứ hai, được cân bằng bởi βpolicy, giới thiệu cân nhắc cụ thể cho nhóm của chúng tôi. Số hạng chính quy này, Rvar(θ, φ), là công cụ trong việc đảm bảo rằng chính sách không chỉ được tối ưu hóa cho hiệu suất tổng thể mà còn duy trì tính mạnh mẽ và công bằng trên các nhóm khác nhau g. Phương pháp mục tiêu kép này phù hợp với mục tiêu RL tiêu chuẩn với mục đích của chúng tôi cho học bất biến trên các nhóm. Trong thực nghiệm của chúng tôi, chúng tôi chia dữ liệu thành các nhóm nhị phân theo thiết lập của các công trình trước đây (Creager et al., 2021). Hơn nữa, chúng tôi cùng huấn luyện φ và θ sử dụng các cập nhật xen kẽ, tương tự như huấn luyện đối kháng. Các thực nghiệm tiếp theo của chúng tôi sẽ xác nhận rằng phương pháp đề xuất có thể nhất quán xác định các nhóm dữ liệu khác nhau trong quá trình huấn luyện.

Từ hai hình ảnh bên trái của Hình 2, chúng ta có thể thấy rằng suy luận nhãn nhóm của chúng tôi hiệu quả phân biệt giữa hai nhóm dữ liệu với các đặc điểm hiệu suất khác nhau. Phần thưởng và lợi nhuận của chính sách trên nhóm đơn giản tăng nhanh và thể hiện biến động rõ rệt, trong khi trên nhóm khó, hiệu suất cải thiện chậm nhưng vẫn ổn định hơn. Như được hiển thị trong hai hình ảnh bên phải, học chính sách bất biến của chúng tôi thu hẹp khoảng cách hiệu suất giữa hai nhóm. Điều này sẽ góp phần tăng cường khả năng tổng quát hóa của chính sách. Thiết lập của thực nghiệm này giống với các thực nghiệm trong Phần 5.1.

4.4 HÌNH PHẠT KL THÍCH ỨNG
Đối với RLHF, hàm phần thưởng thường bao gồm một số hạng phạt phân kỳ KL, như được hiển thị trong Eq. (3). Mục đích của số hạng KL này là đảm bảo rằng mô hình chính sách không lệch quá mức khỏi mô hình SFT ban đầu, từ đó duy trì niềm tin vào điểm số mô hình phần thưởng. Tuy nhiên, cường độ phạt cố định hiện tại là giống nhau cho tất cả dữ liệu, và giá trị này thường được đặt để hiệu quả nhất trong việc xử lý các ngoại lệ, mà không xem xét sự khác biệt giữa dữ liệu, như được minh họa trong Hình 2. Dựa trên các nhãn nhóm chúng tôi thu được trong phần trước, chúng tôi đề xuất một phương pháp kết hợp cường độ chính quy động, như sau:

rtotal=rψ(x, y)−η·pφ(ghigh|x, y)·KL(πRLθ(y|x)∥πSFT(y|x)) (9)

--- TRANG 7 ---
Bản thảo. Đang được xem xét.

[Bảng kết quả chính so sánh tỷ lệ thắng, hòa và thua của phương pháp chúng tôi với các baseline khác dưới cả đánh giá GPT-4 và con người]

Bảng 1: Kết quả chính về so sánh tỷ lệ thắng, hòa và thua của phương pháp chúng tôi với các baseline khác dưới cả đánh giá GPT-4 và con người. Kết quả chứng minh hiệu suất vượt trội của phương pháp chúng tôi, và cũng làm nổi bật sự nhất quán giữa đánh giá con người và GPT-4.

Chúng tôi đầu tiên xác định xác suất của mỗi cặp dữ liệu (x, y) được phân loại là thuộc nhóm có hiệu suất cao nhất, ký hiệu là pφ(ghigh|x, y). Đối với dữ liệu trong nhóm có hiệu suất cao nhất, chúng tôi áp dụng một hình phạt lớn hơn η·pφ(ghigh|x, y) để ngăn chặn hack phần thưởng (Laidlaw et al., 2023). Điều này có nghĩa là chúng tôi tránh ưu ái quá mức dữ liệu đã cho thấy hiệu suất tốt. Mặt khác, đối với dữ liệu khó tối ưu hóa hơn, có xác suất thấp hơn ở nhóm tốt nhất pφ(ghigh|x, y), chúng tôi nới lỏng các ràng buộc của chúng. Điều này tăng không gian khám phá cho mô hình. Mục đích ở đây là khuyến khích mô hình khám phá và học từ dữ liệu không dễ dàng được tối ưu hóa. Thông qua phương pháp này, cách tiếp cận của chúng tôi đạt được sự cân bằng giữa khám phá và tính ổn định huấn luyện.

5 THỰC NGHIỆM
5.1 THIẾT LẬP
Trong nghiên cứu này, chúng tôi sử dụng Llama 2 (Touvron et al., 2023) với 7 tỷ tham số làm mô hình cơ sở cho tất cả các thực nghiệm để đánh giá hiệu quả của căng chỉnh RLHF trong cả tác vụ đối thoại chung và tác vụ tóm tắt. Chi tiết thực nghiệm và siêu tham số có thể tìm thấy trong Phụ lục C.1.

Tác vụ Đối thoại Chung. Theo Vicuna (Chiang et al., 2023), tập dữ liệu SFT bao gồm 52k cuộc trò chuyện do người dùng chia sẻ từ các lĩnh vực khác nhau như toán học, truy vấn kiến thức và lập trình, được thu thập từ ShareGPT.com². Dữ liệu sở thích con người: tập dữ liệu Anthropic-RLHF-HH³ được sử dụng, là một bộ sưu tập quy mô lớn phản hồi của con người về các phản hồi trợ lý AI, bao gồm cả dữ liệu hữu ích và vô hại (Bai et al., 2022b). Toàn bộ tập dữ liệu bao gồm 161k mẫu huấn luyện và 8.5k mẫu kiểm tra.

Tác vụ Tóm tắt. Tập dữ liệu SFT: tập dữ liệu Reddit TL;DR được sử dụng, bao gồm 123,169 bài đăng Reddit cùng với các bản tóm tắt viết bởi con người. Dữ liệu sở thích con người: tương tự như dữ liệu SFT, tập dữ liệu Reddit TL;DR được sử dụng. Mỗi bài đăng trong tập dữ liệu này đi kèm với hai bản tóm tắt được tạo, một trong số đó được gắn nhãn là được ưa thích bởi các chú thích viên (Stiennon et al., 2020).

Baseline. Các phương pháp Baseline của chúng tôi bao gồm: Tinh chỉnh Có giám sát (SFT); Tối ưu hóa Chính sách Gần đúng (PPO) (Schulman et al., 2017); PPO với Hình phạt KL (PPO w/ KL) (Ouyang et al., 2022); và Tối ưu hóa Sở thích Trực tiếp (DPO) (Rafailov et al., 2023). Để hiểu chi tiết và toàn diện về từng baseline được sử dụng, vui lòng tham khảo Phụ lục C.2.

Đánh giá Con người & GPT-4. Để chứng minh hiệu quả của phương pháp chúng tôi, chúng tôi đánh giá phương pháp của chúng tôi bằng cách so sánh tỷ lệ thắng của nó với các baseline. Cụ thể, chúng tôi cung cấp các phản hồi được tạo bởi phương pháp của chúng tôi và các baseline trong đối thoại chung và tóm tắt, nơi nguồn gốc của các phản hồi này không hiển thị với các đánh giá viên con người. Chúng tôi yêu cầu các đánh giá viên con người xác định phản hồi nào hữu ích hơn, vô hại hơn và chất lượng cao hơn. Ngoài ra, vì các nghiên cứu trước đây đã phát hiện rằng các phán đoán của GPT-4 có liên quan chặt chẽ với con người (Chen et al., 2023; Zheng et al., 2023a), và

²https://huggingface.co/datasets/anon8231489123/ShareGPT Vicuna unfiltered
³https://huggingface.co/datasets/Anthropic/hh-rlhf

--- TRANG 8 ---
Bản thảo. Đang được xem xét.

[Biểu đồ kết quả thực nghiệm trên dữ liệu ngoài phân phối]

Hình 3: Kết quả thực nghiệm trên dữ liệu ngoài phân phối. Kết quả thực nghiệm OOD của chúng tôi cho thấy phương pháp của chúng tôi thể hiện tỷ lệ hòa giảm và xác suất thắng tăng so với hiệu suất của nó trên dữ liệu trong phân phối.

sự nhất quán giữa con người và GPT-4 thường tương tự hoặc cao hơn sự nhất quán giữa các chú thích viên con người, chúng tôi cũng sử dụng GPT-4 để đánh giá hiệu suất của phương pháp chúng tôi so với các baseline. Prompt GPT-4 được sử dụng trong đánh giá chọn ngẫu nhiên thứ tự của các phản hồi khác nhau và tính đến việc loại trừ các yếu tố không liên quan như độ dài. Prompt đánh giá GPT-4 hoàn chỉnh có thể tìm thấy trong Phụ lục C.3.

5.2 KẾT QUẢ CHÍNH
Đánh giá dữ liệu trong phân phối. Như được hiển thị trong Bảng 1, chúng tôi trình bày tỷ lệ thắng, hòa và thua khi so sánh các phản hồi của phương pháp chúng tôi với những phản hồi của các baseline khác. Chúng tôi cung cấp kết quả đánh giá trên cả đánh giá GPT-4 và con người. Từ kết quả, chúng ta có thể quan sát thấy: (1) Phương pháp của chúng tôi vượt trội hơn các baseline khác, chỉ có DPO thể hiện hiệu suất tương tự trong đánh giá các truy vấn có hại. (2) Trong các đánh giá hữu ích và có hại của anthropic, phương pháp đề xuất của chúng tôi vượt trội đáng kể so với PPO không có số hạng phạt KL. Điều này là do, trong tập dữ liệu anthropic HH, huấn luyện PPO trở nên không ổn định và có xu hướng tạo ra các đầu ra vô nghĩa khi không có chính quy. (3) Kết quả đánh giá con người và đánh giá GPT-4 thể hiện mức độ nhất quán cao. Do đó, trong các thực nghiệm tiếp theo, chúng tôi chủ yếu dựa vào các đánh giá dựa trên GPT-4.

Đánh giá dữ liệu ngoài phân phối. Trong phần này, chúng tôi xem xét việc kiểm tra hiệu suất của phương pháp chúng tôi so với các phương pháp khác trên dữ liệu Ngoài Phân phối (OOD). Chúng tôi sử dụng dữ liệu PKU-SafeRLHF⁴ cho các truy vấn có hại của chúng tôi, trong khi dữ liệu tóm tắt được lấy từ CNN Dailymail⁵, khác với nguồn dữ liệu SFT và PPO của chúng tôi. Như được hiển thị trong Hình 3, phương pháp của chúng tôi tiếp tục vượt trội hơn các phương pháp baseline khác. Hơn nữa, trên dữ liệu OOD, so với kết quả đánh giá trong phân phối trong Bảng 1, phương pháp của chúng tôi thể hiện xác suất thắng cuộc thi tăng (với ngoại lệ duy nhất là sự giảm nhẹ trong so sánh với SFT và DPO như được chỉ ra trong Tóm tắt). Điều này chỉ ra rằng trong các kịch bản OOD, lợi thế của phương pháp chúng tôi được tăng cường thêm. Ngoài ra, khi so sánh với thuật toán PPO (không có KL) trên bản tóm tắt, phương pháp của chúng tôi giảm tỷ lệ thua PPO từ 32.2% xuống 12.8%, xác nhận thêm khả năng tổng quát hóa của phương pháp chúng tôi. Điều này là do phương pháp của chúng tôi sử dụng cách tiếp cận học bất biến nhóm, dẫn đến các chính sách áp dụng rộng rãi hơn và có khả năng tổng quát hóa cao.

5.3 PHÂN TÍCH CHI TIẾT VỀ TẠI SAO PHƯƠNG PHÁP CHÚNG TÔI HOẠT ĐỘNG.
Đường cong Huấn luyện. Chúng tôi vẽ ba đường cong huấn luyện trên tập dữ liệu RLHF-HH: một cho phương pháp của chúng tôi sử dụng hình phạt KL cố định, một khác cho phương pháp của chúng tôi với hình phạt phân kỳ KL động, và cái cuối cùng cho thuật toán PPO. Từ Hình 4, chúng ta có thể quan sát thấy rằng phương pháp của chúng tôi tương đối ổn định hơn so với

⁴https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF
⁵https://huggingface.co/datasets/cnn dailymail

--- TRANG 9 ---
Bản thảo. Đang được xem xét.

[Biểu đồ đường cong huấn luyện và phân tích]

Hình 4: Đường cong huấn luyện của phương pháp đề xuất và PPO trên tập dữ liệu RLHF-HH. Các phương pháp của chúng tôi cho thấy sự gia tăng nhất quán trong lợi nhuận và phần thưởng, thể hiện tính ổn định tăng cường và khám phá hiệu quả. Phương pháp của chúng tôi, với số hạng phạt KL động, đạt được phần thưởng tốt hơn sau khi trải qua cùng mức độ thay đổi trong không gian đầu ra.

thuật toán PPO. Cả lợi nhuận và phần thưởng đều tiếp tục tăng và cuối cùng ổn định, trong khi lợi nhuận của thuật toán PPO thể hiện xu hướng tăng trưởng ban đầu sau đó là suy giảm, cho thấy khám phá dữ liệu huấn luyện kém hiệu quả hơn. Bằng cách minh họa mối quan hệ giữa phân kỳ KL và phần thưởng, chúng ta có thể thấy rằng phương pháp của chúng tôi, với số hạng phạt KL động, đạt được phần thưởng tốt hơn sau khi trải qua cùng mức độ thay đổi trong không gian đầu ra. Điều này chứng minh hiệu quả vượt trội của số hạng phạt KL động trong việc cân bằng tính ổn định mô hình và khám phá hành vi.

Nghiên cứu Loại bỏ. Chúng tôi tiến hành nghiên cứu loại bỏ để phân tích tác động của hai thành phần trong phương pháp của chúng tôi: học bất biến nhóm (GIL) và hình phạt KL thích ứng (KL động), đối với hiệu suất. Bảng 2 trình bày các đánh giá hiệu suất của phương pháp chúng tôi so với đầu ra PPO w/ KL dưới ba điều kiện truy vấn. Có thể quan sát thấy rằng cải thiện hiệu suất chính trong phương pháp của chúng tôi đến từ học bất biến nhóm, và trên cơ sở đó, hình phạt KL động tiếp tục tăng cường khả năng của phương pháp chúng tôi. Sau khi loại bỏ GIL, các thực nghiệm loại bỏ của chúng tôi chứng minh lợi thế của hình phạt KL động so với hình phạt KL cố định. Đối với các mẫu khó với hiệu suất kém, hình phạt KL động được giảm, cung cấp cho mô hình tự do khám phá các hành động thay thế có thể dẫn đến kết quả tốt hơn. Khám phá thích ứng này rất quan trọng để khám phá các chiến lược chính sách được cải thiện, đặc biệt trong việc khám phá các vùng phức tạp hơn của không gian hành động.

[Bảng nghiên cứu loại bỏ]

Bảng 2: Nghiên cứu loại bỏ hai thành phần chính của phương pháp chúng tôi. Kết quả thực nghiệm được thu thập bằng cách so sánh chúng với PPO có hình phạt KL.

[Biểu đồ phân phối phần thưởng]

Hình 5: So sánh phân phối điểm phần thưởng giữa phương pháp của chúng tôi và PPO trên tập dữ liệu huấn luyện.

Phân phối Phần thưởng. Cuối cùng, chúng tôi trình bày phân phối điểm phần thưởng của phương pháp chúng tôi và thuật toán PPO trên tập dữ liệu huấn luyện sau khi huấn luyện. Như được hiển thị trong Hình 5, phân phối phần thưởng được tạo bởi các mô hình được huấn luyện bằng phương pháp của chúng tôi gần giống với phân phối Gaussian, trong khi các mô hình được huấn luyện với PPO thể hiện phân phối đuôi dài. Một phân phối như vậy ngụ ý rằng phương pháp của chúng tôi không overfitting với các ngoại lệ cụ thể hoặc các mẫu đường tắt trong dữ liệu huấn luyện, mà thay vào đó đang học điều hướng nhiều kịch bản với hiệu suất nhất quán (Arjovsky et al., 2019). Các thực nghiệm của chúng tôi trên dữ liệu OOD chứng minh thêm rằng mô hình của chúng tôi duy trì sự căng chỉnh với sở thích con người, củng cố hiệu quả của GIL trong việc thúc đẩy tổng quát hóa. Bằng chứng này gộp lại cho thấy rằng một mô hình được huấn luyện để hoạt động đồng đều trên các mẫu huấn luyện đa dạng có thể được kỳ vọng sẽ thể hiện khả năng tổng quát hóa mạnh mẽ cho dữ liệu mới, chưa thấy.

--- TRANG 10 ---
Bản thảo. Đang được xem xét.

6 KẾT LUẬN
Bài báo này đề xuất một phương pháp căng chỉnh mới thể hiện khả năng tổng quát hóa đáng kể trên nhiều tập dữ liệu. Không giống như các phương pháp hiện có, kỹ thuật của chúng tôi không chỉ tập trung vào việc tối đa hóa lợi nhuận dự kiến tổng thể mà còn nhấn mạnh việc giảm sự khác biệt giữa các nhóm dữ liệu khác nhau. Bằng cách tự động phân chia dữ liệu thành các nhóm khác nhau mà không cần chú thích thủ công, phương pháp của chúng tôi cải thiện đáng kể khả năng tổng quát hóa của mô hình và tính ổn định huấn luyện. Các nghiên cứu thực nghiệm cho thấy phương pháp của chúng tôi vượt trội hơn các thuật toán PPO truyền thống trong bối cảnh trợ lý AI tổng quát và thiết lập tóm tắt, thể hiện khả năng tổng quát hóa nổi bật và cải thiện đáng kể các chỉ số ổn định và hiệu suất, xác nhận thêm tính thực tế và hiệu quả của phương pháp chúng tôi.

TÀI LIỆU THAM KHẢO
[Danh sách tài liệu tham khảo tiếp theo từ trang 10-23 với các chi tiết như trong bản gốc, được dịch sang tiếng Việt]
