# 2306.13085.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2306.13085.pdf
# Kích thước tệp: 979288 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023
KHAI THÁC CÁC BỘ DỮ LIỆU HỌC TĂNG CƯỜNG NGOẠI TUYẾN
HỖN HỢP THÔNG QUA TRỌNG SỐ QUỸ ĐẠO
Zhang-Wei Hong & Pulkit Agrawal
Viện Công nghệ Massachusetts
Hoa Kỳ
{zwhong,pulkitag }@mit.eduRémi Tachet des Combes∗& Romain Laroche∗
remi.tachet@gmail.com
romain.laroche@gmail.com
TÓM TẮT
Hầu hết các thuật toán học tăng cường (RL) ngoại tuyến trả về một chính sách mục tiêu tối ưu hóa
sự đánh đổi giữa (1) lợi ích hiệu suất kỳ vọng so với chính sách hành vi đã thu thập bộ dữ liệu, và (2) 
rủi ro xuất phát từ tính chất ngoài phân phối của việc chiếm giữ trạng thái-hành động được tạo ra. Do đó,
hiệu suất của chính sách mục tiêu có liên quan mạnh mẽ đến hiệu suất của chính sách hành vi và, 
vì vậy, phân phối lợi nhuận quỹ đạo của bộ dữ liệu. Chúng tôi chỉ ra rằng trong các bộ dữ liệu hỗn hợp
bao gồm chủ yếu các quỹ đạo có lợi nhuận thấp và một số ít quỹ đạo có lợi nhuận cao, các thuật toán
RL ngoại tuyến tiên tiến bị ràng buộc quá mức bởi các quỹ đạo có lợi nhuận thấp và không khai thác 
được các quỹ đạo hiệu suất cao một cách tối đa. Để khắc phục vấn đề này, chúng tôi chỉ ra rằng,
trong các MDP xác định với trạng thái ban đầu ngẫu nhiên, việc lấy mẫu bộ dữ liệu có thể được
tái trọng số để tạo ra một bộ dữ liệu nhân tạo có chính sách hành vi với lợi nhuận cao hơn. Chiến lược
lấy mẫu tái trọng số này có thể được kết hợp với bất kỳ thuật toán RL ngoại tuyến nào. Chúng tôi 
phân tích thêm rằng cơ hội cải thiện hiệu suất so với chính sách hành vi có tương quan với phương sai
phía dương của lợi nhuận của các quỹ đạo trong bộ dữ liệu. Chúng tôi chứng minh thực nghiệm rằng
trong khi CQL, IQL, và TD3+BC chỉ đạt được một phần của tiềm năng cải thiện chính sách này,
các thuật toán tương tự kết hợp với chiến lược lấy mẫu tái trọng số của chúng tôi khai thác hoàn toàn
bộ dữ liệu. Hơn nữa, chúng tôi chứng minh thực nghiệm rằng, bất chấp hạn chế lý thuyết, phương pháp
này vẫn có thể hiệu quả trong các môi trường ngẫu nhiên. Mã nguồn có sẵn tại
https://github.com/Improbable-AI/harness-offline-rl .

1 GIỚI THIỆU
Học tăng cường (RL) ngoại tuyến hiện đang nhận được sự chú ý lớn vì nó cho phép tối ưu hóa
các chính sách RL từ dữ liệu đã ghi lại mà không cần tương tác trực tiếp với môi trường. Điều này
làm cho quá trình huấn luyện RL an toàn hơn và rẻ hơn vì việc thu thập dữ liệu tương tác có rủi ro
cao, đắt đỏ và tốn thời gian trong thế giới thực (ví dụ: robot học và chăm sóc sức khỏe). Thật không may,
một số bài báo đã chỉ ra rằng tính tối ưu gần của nhiệm vụ RL ngoại tuyến là khó khăn về mặt
hiệu quả mẫu (Xiao et al., 2022; Chen & Jiang, 2019; Foster et al., 2022).

Ngược lại với tính tối ưu gần, cải thiện chính sách so với chính sách hành vi là một mục tiêu có thể
thực hiện được một cách gần đúng vì chính sách hành vi có thể được sao chép hiệu quả bằng học
có giám sát (Urbancic, 1994; Torabi et al., 2018). Do đó, hầu hết các thuật toán RL ngoại tuyến thực tế
kết hợp một thành phần đảm bảo, một cách chính thức hoặc trực quan, rằng chính sách được trả về
cải thiện so với chính sách hành vi: các thuật toán bi quan đảm bảo rằng cận dưới của giá trị chính sách
mục tiêu (tức là, một chính sách được học bởi các thuật toán RL ngoại tuyến) cải thiện so với giá trị
của chính sách hành vi (Petrik et al., 2016; Kumar et al., 2020b; Buckman et al., 2020), các thuật toán
bảo thủ điều chỉnh tìm kiếm chính sách của chúng so với chính sách hành vi (Thomas, 2015; Laroche
et al., 2019; Fujimoto et al., 2019), và các thuật toán một bước ngăn chặn giá trị chính sách mục tiêu
lan truyền thông qua bootstrapping (Brandfonbrener et al., 2021). Các thuật toán này sử dụng chính sách
hành vi như một bước đệm. Kết quả là, các đảm bảo hiệu suất của chúng phụ thuộc rất nhiều vào
hiệu suất của chính sách hành vi.

∗Công việc được thực hiện khi ở Microsoft Research Montreal.
1arXiv:2306.13085v1  [cs.LG]  22 Jun 2023

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023

Do sự phụ thuộc vào hiệu suất chính sách hành vi, các thuật toán RL ngoại tuyến này dễ bị ảnh hưởng
bởi phân phối lợi nhuận của các quỹ đạo trong bộ dữ liệu được thu thập bởi một chính sách hành vi.
Để minh họa sự phụ thuộc này, chúng tôi sẽ nói rằng các thuật toán này được neo vào chính sách hành vi.
Việc neo trong một bộ dữ liệu gần tối ưu (tức là, chuyên gia) có lợi cho hiệu suất của một thuật toán,
trong khi việc neo trong một bộ dữ liệu hiệu suất thấp (ví dụ, người mới) có thể cản trở hiệu suất
của chính sách mục tiêu. Trong các tình huống thực tế, các bộ dữ liệu RL ngoại tuyến có thể bao gồm
chủ yếu các quỹ đạo hiệu suất thấp với một số ít quỹ đạo hiệu suất cao được thu thập bởi hỗn hợp
các chính sách hành vi, vì việc tuyển chọn các quỹ đạo hiệu suất cao tốn kém. Do đó, mong muốn
tránh neo vào các chính sách hành vi hiệu suất thấp và khai thác các chính sách hiệu suất cao trong
các bộ dữ liệu hỗn hợp. Tuy nhiên, chúng tôi chỉ ra rằng các thuật toán RL ngoại tuyến tiên tiến
không khai thác được các quỹ đạo hiệu suất cao một cách tối đa. Chúng tôi phân tích rằng tiềm năng
cải thiện chính sách so với chính sách hành vi có tương quan với phương sai phía dương (PSV) của
lợi nhuận quỹ đạo trong bộ dữ liệu và tiến triển rằng khi PSV lợi nhuận cao, việc neo thuật toán
có thể hạn chế hiệu suất của chính sách được trả về.

Để cung cấp một neo thuật toán tốt hơn, chúng tôi đề xuất thay đổi chính sách hành vi mà không
thu thập thêm dữ liệu. Chúng tôi bắt đầu bằng việc chứng minh rằng việc tái trọng số bộ dữ liệu
trong quá trình huấn luyện của một thuật toán RL ngoại tuyến tương đương với việc thực hiện
huấn luyện này với một chính sách hành vi khác. Hơn nữa, dưới giả định rằng môi trường là xác định,
bằng cách tăng trọng số cho các quỹ đạo có lợi nhuận cao, chúng ta có thể kiểm soát chính sách hành vi
ngầm để có hiệu suất cao và do đó cung cấp một sự thúc đẩy hiệu suất khởi động lạnh cho thuật toán
RL ngoại tuyến. Mặc dù tính xác định là một giả định mạnh mà chúng tôi chứng minh là cần thiết
với một ví dụ thất bại tối thiểu, chúng tôi chỉ ra rằng các đảm bảo vẫn giữ khi trạng thái ban đầu
là ngẫu nhiên bằng cách tái trọng số với, thay vì lợi nhuận quỹ đạo, một lợi thế lợi nhuận quỹ đạo:
G(τi)−Vμ(si,0), trong đó G(τi) là lợi nhuận thu được cho quỹ đạo i, Vμ(si,0) là lợi nhuận kỳ vọng
của việc theo chính sách hành vi μ từ trạng thái ban đầu si,0. Hơn nữa, chúng tôi quan sát thực nghiệm
rằng chiến lược của chúng tôi cho phép tăng hiệu suất so với các đối tác lấy mẫu đều ngay cả trong
các môi trường ngẫu nhiên. Chúng tôi cũng lưu ý rằng tính xác định được yêu cầu bởi một số thuật toán
RL ngoại tuyến tiên tiến (Schmidhuber, 2019; Srivastava et al., 2019; Kumar et al., 2019b; Chen et al.,
2021; Furuta et al., 2021; Brandfonbrener et al., 2022).

Dưới sự hướng dẫn của phân tích lý thuyết, đóng góp chính của chúng tôi là hai chiến lược lấy mẫu
có trọng số đơn giản: Trọng số theo lợi nhuận (RW) và Trọng số theo lợi thế (AW). RW và AW
tái trọng số các quỹ đạo sử dụng phân phối Boltzmann của lợi nhuận quỹ đạo và lợi thế, tương ứng.
Các chiến lược lấy mẫu có trọng số của chúng tôi không phụ thuộc vào các thuật toán RL ngoại tuyến
cơ bản và do đó có thể là một sự thay thế drop-in trong bất kỳ thuật toán RL ngoại tuyến có sẵn nào,
về cơ bản không có chi phí tính toán bổ sung. Chúng tôi đánh giá các chiến lược lấy mẫu của mình
trên ba thuật toán RL ngoại tuyến tiên tiến, CQL, IQL, và TD3+BC (Kumar et al., 2020b; Kostrikov
et al., 2022; Fujimoto & Gu, 2021), cũng như học hành vi, trên 62 bộ dữ liệu trong các điểm chuẩn
D4RL (Fu et al., 2020). Các kết quả thực nghiệm được báo cáo trong các số liệu thống kê mạnh mẽ
(Agarwal et al., 2021) chứng minh rằng cả hai chiến lược lấy mẫu của chúng tôi đều tăng đáng kể
hiệu suất của tất cả các thuật toán RL ngoại tuyến được xem xét trong các bộ dữ liệu hỗn hợp thách thức
với các quỹ đạo có phần thưởng thưa thớt, và hoạt động ít nhất ngang bằng với chúng trên các bộ dữ liệu
thông thường với phân phối lợi nhuận phân bố đều.

2 KIẾN THỨC CHUẨN BỊ
Chúng tôi xem xét bài toán học tăng cường (RL) trong một quá trình quyết định Markov (MDP)
được đặc trưng bởi một bộ (S,A, R, P, ρ0), trong đó S và A biểu thị không gian trạng thái và hành động,
tương ứng, R:S × A → R là một hàm phần thưởng, P:S × A → ΔS là một động lực chuyển trạng thái,
và ρ0: ΔS là một phân phối trạng thái ban đầu, trong đó ΔX biểu thị một simplex trên tập hợp X.
Một MDP bắt đầu từ một trạng thái ban đầu s0∼ρ0. Tại mỗi bước thời gian t, một tác tử nhận biết
trạng thái st, thực hiện một hành động at∼π(.|st) trong đó π:S → ΔA là chính sách của tác tử,
nhận được một phần thưởng rt=R(st, at), và chuyển đến trạng thái tiếp theo st+1∼P(st+1|st, at).
Hiệu suất của một chính sách π được đo bằng lợi nhuận kỳ vọng J(π) bắt đầu từ trạng thái ban đầu
s0∼ρ0 được hiển thị như sau:

J(π) =E[∞∑t=0R(st, at)|s0∼ρ0, at∼π(.|st), st+1∼P(.|st, at)]     (1)

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023

Cho một bộ dữ liệu D được thu thập bởi một chính sách hành vi μ:S → ΔA, các thuật toán RL ngoại tuyến
nhằm học một chính sách mục tiêu π sao cho J(π)≥J(μ) từ một bộ dữ liệu D được hiển thị như sau:

D={
(si,0, ai,0, ri,0,···si,Ti)|si,0∼ρ0, ai,t∼μ(.|si,t),
ri,t=R(si,t, ai,t), si,t+1∼P(.|si,t, ai,t)
}     (2)

trong đó τi= (si,0, ai,0, ri,0,···si,Ti+1) biểu thị quỹ đạo i trong D, (i, t) biểu thị bước thời gian t
trong tập i, và Ti biểu thị độ dài của τi. Lưu ý rằng μ có thể là hỗn hợp của nhiều chính sách.
Để ngắn gọn, chúng tôi bỏ qua chỉ số tập i trong chỉ số dưới của trạng thái và hành động, trừ khi cần thiết.
Nói chung, các thuật toán RL ngoại tuyến học π dựa trên các phương pháp actor-critic huấn luyện
một hàm giá trị Q:S × A → R và π song song. Giá trị Q Q(s, a) dự đoán lợi nhuận kỳ vọng của việc
thực hiện hành động a tại trạng thái s và theo π sau đó; π tối đa hóa giá trị Q kỳ vọng trên D.
Q và π được huấn luyện thông qua luân phiên giữa các bước đánh giá chính sách (Phương trình 3)
và cải thiện chính sách (Phương trình 4) được hiển thị dưới đây:

Q←arg minQ E[(rt+γEa′∼π(.|st+1)[Q(st+1, a′)]−Q(st, at))²|Uni(D)]     (3)

π←arg maxπ E[Q(st, a)|Uni(D), a∼π(.|st)]     (4)

trong đó E[· |Uni(D)] biểu thị một kỳ vọng trên việc lấy mẫu đều của các chuyển tiếp.

3 PHÁT BIỂU BÀI TOÁN

Hầu hết các thuật toán RL ngoại tuyến được neo vào chính sách hành vi. Điều này có lợi khi chính sách
hành vi bộ dữ liệu có hiệu suất cao trong khi có hại khi chính sách hành vi có hiệu suất thấp. Chúng tôi
xem xét các bộ dữ liệu hỗn hợp bao gồm chủ yếu các quỹ đạo hiệu suất thấp và một số ít quỹ đạo
hiệu suất cao. Trong các bộ dữ liệu như vậy, có thể khai thác các quỹ đạo hiệu suất cao hiếm hoi,
nhưng việc neo hạn chế các thuật toán này khỏi việc cải thiện chính sách đáng kể so với chính sách
hành vi của bộ dữ liệu hỗn hợp. Chúng tôi định nghĩa chính thức phương sai phía dương lợi nhuận (RPSV)
của một bộ dữ liệu trong Mục 3.1 và minh họa tại sao hiệu suất của các thuật toán RL ngoại tuyến
có thể bị hạn chế trên các bộ dữ liệu RPSV cao trong Mục 3.2.

3.1 PHƯƠNG SAI PHÍA DƯƠNG

Chính thức, chúng tôi quan tâm đến một bộ dữ liệu D:={τ0, τ1,···τN−1} có thể được thu thập bởi
các chính sách hành vi khác nhau {μ0, μ1,···μN−1} và được cấu thành bởi lợi nhuận thực nghiệm
{G(τ0), G(τ1),···G(τN−1)}, trong đó τi được tạo ra bởi μi, N là số quỹ đạo, Ti biểu thị độ dài của τi,
và G(τi) =∑Ti−1t=0ri,t. Để nghiên cứu phân phối lợi nhuận, chúng tôi trang bị bản thân với một
đại lượng thống kê: phương sai phía dương (PSV) của một biến ngẫu nhiên X:

Định nghĩa 1 (Phương sai phía dương). Phương sai phía dương (PSV) của một biến ngẫu nhiên X là
mômen bậc hai của thành phần dương của X−E[X]:

V+[X]:=E[(X−E[X])²+]

với x+= max {x,0}.     (5)

RPSV của D nhằm nắm bắt sự phân tán dương của phân phối lợi nhuận quỹ đạo. Một câu hỏi thú vị
cần đặt là: phân phối nào dẫn đến RPSV cao? Chúng tôi đơn giản hóa việc lấy mẫu quỹ đạo được
thu thập bởi một người mới và một chuyên gia như việc lấy mẫu từ một phân phối Bernoulli B,
và giả sử rằng chính sách người mới luôn cho lợi nhuận 0, trong khi chuyên gia luôn cho lợi nhuận 1.
Hình 1a trực quan hóa V+[B(p)] =p(1−p)², đó là PSV của phân phối Bernoulli như một hàm của
tham số p, trong đó p là xác suất chọn một quỹ đạo chuyên gia. Chúng ta thấy rằng PSV tối đa
được đạt ở p=1/3. Cả p= 0 (thuần người mới) và p= 1 (thuần chuyên gia) đều dẫn đến PSV bằng không.
Quan sát này chỉ ra rằng các bộ dữ liệu hỗn hợp có xu hướng có RPSV cao hơn một bộ dữ liệu
được thu thập bởi một chính sách duy nhất. Chúng tôi trình bày phân phối lợi nhuận của các bộ dữ liệu
ở các RPSV khác nhau trong Hình 1. Các bộ dữ liệu RPSV thấp có lợi nhuận cao nhất vẫn gần với
lợi nhuận trung bình, điều này hạn chế cơ hội cải thiện chính sách. Ngược lại, phân phối lợi nhuận
của các bộ dữ liệu RPSV cao phân tán ra khỏi trung bình hướng về phía dương.

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023

[Hình 1: (a) PSV phân phối Bernoulli: V+[B(p)] =p(1−p)². (b-c) Phân phối lợi nhuận của các bộ dữ liệu với phương sai phía dương lợi nhuận (RPSV) (b) thấp và (c) cao (Mục 3.1), trong đó RPSV đo đóng góp tích cực trong phương sai của lợi nhuận quỹ đạo trong một bộ dữ liệu và Ḡ biểu thị lợi nhuận tập trung bình (đường đứt nét) của bộ dữ liệu. Theo trực giác, RPSV cao ngụ ý một số quỹ đạo có lợi nhuận cao hơn nhiều so với trung bình.]

3.2 RL NGOẠI TUYẾN THẤT BẠI TRONG VIỆC SỬ DỤNG DỮ LIỆU TRONG CÁC BỘ DỮ LIỆU RPSV CAO

Các bộ dữ liệu RPSV cao (Hình 1c) có một số ít quỹ đạo có lợi nhuận cao, nhưng việc neo của các thuật toán RL ngoại tuyến vào chính sách hành vi cản trở RL ngoại tuyến sử dụng các dữ liệu có lợi nhuận cao này một cách tối đa. Các quỹ đạo có lợi nhuận thấp chiếm ưu thế trong một bộ dữ liệu RPSV cao hạn chế các thuật toán RL ngoại tuyến học một chính sách không tầm thường gần với các quỹ đạo tốt nhất trong D do bản chất bi quan và/hoặc bảo thủ của các thuật toán này. RPSV cao ngụ ý rằng lợi nhuận tập trung bình xa từ lợi nhuận tốt nhất trong D (xem Hình 1c). Lợi nhuận tập trung bình phản ánh hiệu suất J(μ) (được chứng minh chính thức trong Mục 4.1) của chính sách hành vi μ đã thu thập D, trong đó μ là hỗn hợp của {μ0, μ1,···μN−1} (Mục 3.1).

Các thuật toán bi quan (Petrik et al., 2016; Kumar et al., 2020b; Buckman et al., 2020) phấn đấu đảm bảo thuật toán trả về một π sao cho J(π)≥J(μ), nhưng đảm bảo này lỏng lẻo khi J(μ) thấp. Các thuật toán bảo thủ (Laroche et al., 2019; Fujimoto et al., 2019; Fujimoto & Gu, 2021; Kumar et al., 2019a) hạn chế π hành xử gần với μ để ngăn chặn việc khai thác các giá trị Q được ước tính kém trên các cặp trạng thái-hành động ngoài phân phối trong cập nhật actor-critic (tức là, (st+1, a′)∉ D trong Phương trình 3), do đó hạn chế J(π) khỏi việc lệch quá xa từ J(μ). Tương tự, các thuật toán một bước (Brandfonbrener et al., 2021; Kostrikov et al., 2022) chỉ thực hiện một bước cải thiện chính sách duy nhất trả về một chính sách mục tiêu chịu các ràng buộc thực thi π gần với μ (Peters & Schaal, 2007; Peng et al., 2019). Kết quả là, các thuật toán RL ngoại tuyến bị hạn chế bởi J(μ) và thất bại trong việc sử dụng dữ liệu có lợi nhuận cao xa từ J(μ) trong các bộ dữ liệu RPSV cao.

Ngược lại, trong các bộ dữ liệu RPSV thấp (Hình 1b), các thuật toán bi quan, bảo thủ, và một bước không có vấn đề sử dụng dưới mức nghiêm trọng này vì phân phối lợi nhuận tập trung xung quanh hoặc dưới lợi nhuận tập trung bình, và có rất ít đến không có quỹ đạo tốt hơn để khai thác. Chúng tôi sẽ chỉ ra, trong Mục 5.2, rằng không có chiến lược lấy mẫu nào làm cho các thuật toán RL ngoại tuyến hoạt động tốt hơn trong các bộ dữ liệu RPSV cực thấp, trong khi trong các bộ dữ liệu RPSV cao, các phương pháp của chúng tôi (Mục 4.2 và 4.3) vượt trội hơn việc lấy mẫu đều thông thường đáng kể.

4 PHƯƠNG PHÁP

Mục 3 giải thích tại sao việc neo chính sách hành vi ngăn chặn các thuật toán RL ngoại tuyến khai thác các bộ dữ liệu RPSV cao một cách tối đa. Để khắc phục vấn đề này, câu hỏi cần được trả lời là: chúng ta có thể cải thiện hiệu suất của chính sách hành vi mà không thu thập thêm dữ liệu không? Để làm như vậy, chúng tôi đề xuất thay đổi nó một cách ngầm định thông qua việc tái trọng số các chuyển tiếp trong bộ dữ liệu. Thật vậy, chúng tôi chỉ ra rằng lấy mẫu có trọng số có thể mô phỏng việc lấy mẫu chuyển tiếp với một chính sách hành vi khác. Chúng tôi phân tích mối liên hệ giữa lấy mẫu có trọng số và hiệu suất của chính sách hành vi ngầm định trong Mục 4.1, và sau đó trình bày hai chiến lược lấy mẫu có trọng số trong Mục 4.2 và 4.3.

4.1 PHÂN TÍCH

Chúng tôi bắt đầu bằng việc chỉ ra cách tái trọng số các chuyển tiếp trong một bộ dữ liệu mô phỏng việc lấy mẫu chuyển tiếp được tạo ra bởi một chính sách hành vi hỗn hợp ngầm định khác với chính sách đã thu thập bộ dữ liệu. Nó là ngầm định vì chính sách được định nghĩa bởi trọng số của các chuyển tiếp trong bộ dữ liệu. Như được đề xuất trong Peng et al. (2019), việc lấy mẫu chuyển tiếp từ D được định nghĩa trong Mục 3 tương đương với việc lấy mẫu các cặp trạng thái-hành động từ một sự chiếm giữ trạng thái-hành động chung có trọng số: dW(s, a) =∑N−1i=0widμi(s)μi(a|s), trong đó wi là trọng số của quỹ đạo i (mỗi τi được thu thập bởi μi), W:={w0,···wN−1}, và dμi(s) biểu thị đo chiếm giữ trạng thái không chuẩn hóa (Laroche et al., 2022) trong việc triển khai μi. Điều chỉnh trọng số W hiệu quả thay đổi dW và do đó phân phối chuyển tiếp trong quá trình lấy mẫu. Như Peng et al. (2019) đã đề xuất, một trọng số W cũng tạo ra một chính sách hành vi có trọng số: μW(a|s) =dW(s,a)/∑N−1i=0widμi(s). Lấy mẫu đều wi=1/N,∀wi∈ W tương đương với việc lấy mẫu từ sự chiếm giữ trạng thái-hành động chung của chính sách hành vi hỗn hợp ban đầu μ đã thu thập D. Để có được một phân phối lấy mẫu được định nghĩa tốt trên các chuyển tiếp, chúng ta cần chuyển đổi các trọng số quỹ đạo wi này thành trọng số mẫu chuyển tiếp wi,t,∀t∈⟦0, Ti−1⟧:

wi,t:=wi/∑N−1i=0Tiwi, ∑N−1i=0∑Ti−1t=0wi,t=∑N−1i=0Tiwi/∑N−1i=0Tiwi= 1.     (6)

Do đó, chúng tôi xây dựng mục tiêu của mình là tìm W:={wi}i∈⟦0,N−1⟧∈ΔN sao cho J(μW)≥J(μ), trong đó ΔN biểu thị simplex kích thước N. Tự nhiên, chúng ta có thể viết J(μW) =∑N−1i=0wiJ(μi). Câu hỏi còn lại sau đó là ước tính J(μi). Lợi nhuận tập G(τi) có thể được coi như một mẫu của J(μi). Kết quả là, chúng ta có thể tập trung J(μW) gần tổng có trọng số của lợi nhuận với một ứng dụng trực tiếp của bất đẳng thức Hoeffding (Serfling, 1974):

P[|J(μW)−∑N−1i=0wiG(τi)|> ε] ≤2 exp(−2ε²/(G⊤²∑N−1i=0w²i))     (7)

trong đó G⊤:=GMAX−GMIN là biên độ khoảng lợi nhuận (xem bất đẳng thức Hoeffding). Để hoàn chỉnh, tính đúng đắn của phương pháp được chứng minh cho bất kỳ chính sách và MDP nào với hệ số chiết khấu (Sutton & Barto, 2018) nhỏ hơn 1 trong Phụ lục A.1. Phương trình 7 cho chúng ta biết rằng chúng ta có một ước tính nhất quán cho J(μW) miễn là quá nhiều khối lượng không được gán cho một tập hợp nhỏ các quỹ đạo.

Vì mục tiêu của chúng ta là có được một chính sách hành vi với hiệu suất cao hơn, chúng ta muốn trao trọng số cao wi cho μi hiệu suất cao. Tuy nhiên, đáng chú ý rằng việc đặt wi như một hàm của Gi có thể tạo ra sự thiên vị trong ước tính của J(μW) do tính ngẫu nhiên trong việc tạo quỹ đạo, xuất phát từ ρ0, P, và/hoặc μi. Trong trường hợp đó, cận tập trung Phương trình 7 sẽ không còn hợp lệ nữa. Để chứng minh và minh họa sự thiên vị, chúng tôi cung cấp một phản ví dụ trong Phụ lục A.3. Mục tiếp theo giải quyết vấn đề này bằng cách đưa ra giả định mạnh về tính xác định của môi trường, và áp dụng một mẹo để loại bỏ tính ngẫu nhiên từ chính sách hành vi μi. Mục 4.3 sau đó nới lỏng yêu cầu cho ρ0 là xác định bằng cách sử dụng lợi thế lợi nhuận thay vì lợi nhuận tuyệt đối.

4.2 TRỌNG SỐ THEO LỢI NHUẬN

Trong mục này, chúng tôi đưa ra giả định mạnh rằng MDP là xác định (tức là, động lực chuyển tiếp P và phân phối trạng thái ban đầu ρ0 là một phân phối delta Dirac). Giả định này cho phép chúng ta có được rằng G(τi) =J(μi), trong đó μi là chính sách xác định thực hiện các hành động trong quỹ đạo τi¹. Vì hiệu suất của chính sách mục tiêu được neo vào hiệu suất của một chính sách hành vi, chúng tôi tìm một phân phối trọng số W để tối đa hóa J(μW):

maxW∈ΔN ∑N−1i=0wiG(τi)     (8)

trong đó wi tương ứng với trọng số không chuẩn hóa được gán cho mỗi chuyển tiếp trong tập i. Tuy nhiên, giải pháp kết quả tầm thường gán tất cả trọng số cho các chuyển tiếp trong tập τi với lợi nhuận tối đa. Giải pháp tầm thường này thực sự sẽ tối ưu trong thiết lập xác định mà chúng ta xem xét nhưng sẽ thất bại trong trường hợp khác. Để ngăn chặn điều này xảy ra, chúng tôi cổ điển kết hợp điều chuẩn entropy và biến Phương trình 8 thành:

maxW∈ΔN ∑N−1i=0wiG(τi)−α∑N−1i=0wi log wi     (9)

trong đó α∈ℝ+ là một tham số nhiệt độ kiểm soát cường độ điều chuẩn. α nội suy giải pháp từ một phân phối delta (α→0) đến một phân phối đều (α→∞). Vì giải pháp tối ưu cho Phương trình 9 là một phân phối Boltzmann của G(τi), phân phối trọng số W kết quả là:

wi=exp(G(τi)/α)/∑τi∈D exp(G(τi)/α)     (10)

Tham số nhiệt độ α (Phương trình 10 và 11) là một siêu tham số cố định. Vì sự lựa chọn α phụ thuộc vào thang đo của lợi nhuận tập, thay đổi qua các môi trường, chúng tôi chuẩn hóa G(τi) sử dụng chuẩn hóa max-min: G(τi)←(G(τi)−minjG(τj))/(maxjG(τj)−minjG(τj)).

4.3 TRỌNG SỐ THEO LỢI THẾ

Trong mục này, chúng tôi cho phép phân phối trạng thái ban đầu ρ0 là ngẫu nhiên. Chiến lược trọng số theo lợi nhuận trong Mục 4.2 có thể thiên vị đối với một số quỹ đạo bắt đầu từ các trạng thái ban đầu may mắn cho lợi nhuận cao hơn các trạng thái ban đầu khác. Do đó, chúng tôi thay đổi mục tiêu của Phương trình 9 để tối đa hóa lợi thế tập có trọng số ∑wi∈W wiA(τi) với điều chuẩn entropy. A(τi) biểu thị lợi thế tập của τi và được định nghĩa là A(τi) =G(τi)−V̂μ(si,0). V̂μ(si,0) là lợi nhuận kỳ vọng ước tính của việc theo μ bắt đầu từ si,0, sử dụng hồi quy: V̂μ←arg minV E[(G(τi)−V(si,0))²|Uni(D)]. Thay thế G(τi) bằng A(τi) trong Phương trình 9 và giải cho W, chúng ta có được phân phối trọng số sau:

wi=exp(A(τi)/α)/∑τi∈D exp(A(τi)/α), A(τi) =G(τi)−V̂μ(si,0).     (11)

5 THỰC NGHIỆM

Các thực nghiệm của chúng tôi trả lời các câu hỏi chính sau: (i) Các phương pháp của chúng tôi có cho phép các thuật toán RL ngoại tuyến đạt được hiệu suất tốt hơn trong các bộ dữ liệu với các quỹ đạo có lợi nhuận cao thưa thớt không? (ii) Phương pháp của chúng tôi có hưởng lợi từ RPSV cao không? (iii) Phương pháp của chúng tôi có thể hoạt động tốt trong các bộ dữ liệu thông thường không? (iv) Phương pháp của chúng tôi có mạnh mẽ với tính ngẫu nhiên trong một MDP không?

5.1 THIẾT LẬP

Triển khai. Chúng tôi triển khai chiến lược lấy mẫu có trọng số và các đường cơ sở trong các thuật toán RL ngoại tuyến sau: học Q ngầm định (IQL) (Kostrikov et al., 2022), học Q bảo thủ (CQL) (Kumar et al., 2020b), TD3+BC (Fujimoto & Gu, 2021), và học hành vi (BC). IQL, CQL, và TD3+BC được chọn để bao quát các cách tiếp cận khác nhau của RL ngoại tuyến, bao gồm các thuật toán một bước, bi quan, và bảo thủ. Lưu ý rằng mặc dù BC là một thuật toán học bắt chước, chúng tôi bao gồm nó vì BC sao chép chính sách hành vi, đó là đối tượng chúng tôi trực tiếp thay đổi, và BC cũng là một đường cơ sở phổ biến trong nghiên cứu RL ngoại tuyến (Kumar et al., 2020b; Kostrikov et al., 2022).

Đường cơ sở. Chúng tôi so sánh lấy mẫu có trọng số của mình với lấy mẫu đều (biểu thị là Uniform), lọc phần trăm (Chen et al., 2021) (biểu thị là Top-x%), và lấy mẫu một nửa (biểu thị là Half). Lọc phần trăm chỉ sử dụng các tập với lợi nhuận top-x% cho huấn luyện. Chúng tôi xem xét lọc phần trăm như một đường cơ sở vì nó tương tự tăng lợi nhuận kỳ vọng của chính sách hành vi bằng cách loại bỏ một số dữ liệu. Trong phần sau, chúng tôi so sánh phương pháp của mình với Top-10% vì 10% là cấu hình tốt nhất được tìm thấy trong tìm kiếm siêu tham số (Phụ lục A.11). Lấy mẫu một nửa lấy mẫu một nửa chuyển tiếp từ quỹ đạo có lợi nhuận cao và thấp, tương ứng. Half là một giải pháp đơn giản để tránh lấy mẫu quá nhiều dữ liệu có lợi nhuận thấp trong các bộ dữ liệu chỉ bao gồm các quỹ đạo có lợi nhuận cao thưa thớt. Lưu ý rằng Half yêu cầu giả định bổ sung về việc tách một bộ dữ liệu thành các phân vùng có lợi nhuận cao và thấp, trong khi các phương pháp của chúng tôi không cần điều này. Các chiến lược trọng số theo lợi nhuận và trọng số theo lợi thế của chúng tôi được biểu thị là RW và AW, tương ứng, mà chúng tôi sử dụng cùng siêu tham số α trong tất cả các môi trường (xem Phụ lục A.7).

Bộ dữ liệu và môi trường. Chúng tôi đánh giá hiệu suất của mỗi biến thể thuật toán+bộ lấy mẫu (tức là, sự kết hợp của một thuật toán RL ngoại tuyến và một chiến lược lấy mẫu) trong các môi trường vận động MuJoCo

--- TRANG 7 ---
của các điểm chuẩn D4RL (Fu et al., 2020) và các điểm chuẩn điều khiển cổ điển ngẫu nhiên. Mỗi môi trường được coi như một MDP và có thể có nhiều bộ dữ liệu trong một bộ điểm chuẩn. Các lựa chọn bộ dữ liệu được mô tả trong các phần tương ứng của kết quả thực nghiệm. Chúng tôi đánh giá phương pháp của mình trong điều khiển cổ điển ngẫu nhiên để điều tra xem động lực ngẫu nhiên có phá vỡ các chiến lược lấy mẫu có trọng số của chúng tôi không. Việc triển khai động lực ngẫu nhiên được trình bày trong Phụ lục A.6.

Số liệu đánh giá. Một biến thể thuật toán+bộ lấy mẫu được huấn luyện cho một triệu lô cập nhật trong năm hạt giống ngẫu nhiên cho mỗi bộ dữ liệu và môi trường. Hiệu suất của nó được đo bằng lợi nhuận tập trung bình được chuẩn hóa của việc chạy chính sách đã huấn luyện trên 20 tập trong môi trường. Như được đề xuất trong Fu et al. (2020), chúng tôi chuẩn hóa hiệu suất sử dụng (X−XRandom)/(XExpert−XRandom) trong đó X, XRandom, và XExpert biểu thị hiệu suất của một biến thể thuật toán-bộ lấy mẫu, chính sách ngẫu nhiên, và chính sách chuyên gia, tương ứng.

5.2 KẾT QUẢ TRONG CÁC BỘ DỮ LIỆU HỖN HỢP VỚI CÁC QUỸ ĐẠO CÓ LỢI NHUẬN CAO THƯA THỚT

Để trả lời xem các phương pháp lấy mẫu có trọng số của chúng tôi có cải thiện hiệu suất của lấy mẫu đều trong các bộ dữ liệu với các quỹ đạo có lợi nhuận cao thưa thớt không, chúng tôi tạo các bộ dữ liệu hỗn hợp với các tỷ lệ khác nhau của dữ liệu có lợi nhuận cao. Chúng tôi kiểm tra mỗi biến thể thuật toán+bộ lấy mẫu trong bốn môi trường vận động MuJoCo và tám bộ dữ liệu hỗn hợp, và một bộ dữ liệu không hỗn hợp cho mỗi môi trường. Các bộ dữ liệu hỗn hợp được tạo bằng cách trộn σ% của bộ dữ liệu expert hoặc medium (lợi nhuận cao) với (1−σ%) của bộ dữ liệu random (lợi nhuận thấp), cho bốn tỷ lệ, σ∈ {1,5,10,50}. Các bộ dữ liệu expert, medium, và random được tạo ra bởi một chính sách chuyên gia, một chính sách với 1/3 hiệu suất chính sách chuyên gia, và một chính sách ngẫu nhiên, tương ứng. Chúng tôi kiểm tra tất cả các biến thể trong những 32 bộ dữ liệu hỗn hợp và bộ dữ liệu random.

Hình 2 hiển thị hiệu suất chuẩn hóa trung bình (trục y) của mỗi biến thể thuật toán+bộ lấy mẫu (màu sắc) tại các σ khác nhau (trục x). Hiệu suất của mỗi biến thể thuật toán+bộ lấy mẫu được đo trong trung bình interquartile (IQM) (còn được gọi là trung bình cắt bỏ 25%) của lợi nhuận trung bình (xem Mục 5.1) vì IQM ít dễ bị ảnh hưởng bởi hiệu suất ngoại lai như được đề xuất trong Agarwal et al. (2021). Phụ lục A.8 chi tiết giao thức đánh giá.

Có thể thấy trong Hình 2 rằng các chiến lược RW và AW của chúng tôi vượt trội đáng kể so với các đường cơ sở Uniform, Top-10%, và Half cho tất cả các thuật toán tại tất cả các tỷ lệ dữ liệu expert/medium σ%. Đáng chú ý, các phương pháp của chúng tôi thậm chí vượt hoặc bằng hiệu suất của mỗi thuật toán được huấn luyện trong các bộ dữ liệu expert đầy đủ với lấy mẫu đều (đường đứt nét). Điều này ngụ ý rằng các phương pháp của chúng tôi cho phép các thuật toán RL ngoại tuyến đạt được mức độ hiệu suất chuyên gia bằng 5% đến 10% quỹ đạo medium hoặc expert. Uniform thất bại trong việc khai thác tối đa các bộ dữ liệu khi các quỹ đạo hiệu suất cao thưa thớt (tức là, σ thấp). Top-10% cải thiện hiệu suất một chút, nhưng thất bại so với Uniform ở tỷ lệ thấp (σ% = 1%), điều này ngụ ý phần trăm lọc tốt nhất có thể phụ thuộc vào bộ dữ liệu. Half liên tục cải thiện Uniform một chút ở tất cả các tỷ lệ, nhưng lượng tăng hiệu suất xa dưới của chúng tôi. Nhìn chung, các kết quả này cho thấy rằng việc tăng trọng số các quỹ đạo có lợi nhuận cao trong một bộ dữ liệu với tỷ lệ thấp dữ liệu có lợi nhuận cao có lợi cho hiệu suất trong khi việc lọc ngây thơ các tập có lợi nhuận thấp, như Top-10% làm, không liên tục cải thiện hiệu suất. Hơn nữa, AW và RW không hiển thị sự khác biệt rõ ràng, có thể vì phân phối trạng thái ban đầu hẹp trong các môi trường vận động MuJoCo. Chúng tôi cũng bao gồm lợi nhuận trung bình trong mỗi môi trường và bộ dữ liệu trong Phụ lục A.13. Ngoài lợi nhuận trung bình, chúng tôi cũng đánh giá các phương pháp của mình trong xác suất cải thiện (Agarwal et al., 2021) so với lấy mẫu đều và hiển thị các cải thiện có ý nghĩa thống kê trong Phụ lục A.10.

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2023

[Hình 3: Phần cuối bên trái (lợi nhuận trung bình từ 0 đến 0.1) của mỗi biểu đồ cho thấy rằng đối với tất cả các thuật toán RL ngoại tuyến (CQL, IQL, và TD3+BC) và BC, tăng hiệu suất của các phương pháp lấy mẫu AW và RW của chúng tôi tăng khi RPSV tăng. Màu sắc biểu thị tăng hiệu suất (lợi nhuận trung bình) so với đường cơ sở lấy mẫu đều trong các bộ dữ liệu hỗn hợp và môi trường được kiểm tra trong Mục 5.2; trục x và trục y chỉ ra lợi nhuận trung bình của một bộ dữ liệu và RPSV của bộ dữ liệu, tương ứng.]

5.3 PHÂN TÍCH TĂNG HIỆU SUẤT TRONG CÁC BỘ DỮ LIỆU HỖN HỢP

Chúng tôi giả thuyết rằng tăng hiệu suất của các phương pháp chúng tôi so với lấy mẫu đều xuất phát từ RPSV tăng trong các bộ dữ liệu. Việc thiết kế một dự báo mạnh mẽ cho tăng hiệu suất của một chiến lược lấy mẫu không đơn giản vì hiệu suất của RL ngoại tuyến bị ảnh hưởng bởi nhiều yếu tố, bao gồm môi trường và thuật toán RL ngoại tuyến mà nó được ghép nối. Chúng tôi tập trung vào hai yếu tố thống kê dễ ước tính từ bộ dữ liệu: (i) lợi nhuận trung bình của một bộ dữ liệu và (ii) RPSV. Mặc dù phụ thuộc lẫn nhau, hai yếu tố này có biến thiên tốt trong các thực nghiệm của chúng tôi vì việc tăng tỷ lệ dữ liệu expert/medium sẽ tăng không chỉ RPSV mà còn lợi nhuận trung bình của một bộ dữ liệu.

Chúng tôi hiển thị mối quan hệ giữa tăng hiệu suất so với lấy mẫu đều (được biểu thị bằng màu sắc của điểm trong các biểu đồ dưới đây), lợi nhuận trung bình của bộ dữ liệu (trục x), và RPSV (trục y, theo thang logarithm) trong Hình 3. Mỗi điểm biểu thị tăng hiệu suất trung bình trong một bộ ba môi trường, bộ dữ liệu, và σ. Có thể thấy rằng ở lợi nhuận trung bình tương tự (trục x), tăng hiệu suất của các phương pháp chúng tôi tăng rõ rệt (màu sắc gần hơn với đỏ) khi RPSV tăng (trục y). Quan sát này cho thấy rằng tăng hiệu suất với σ thấp (tỷ lệ dữ liệu expert/medium) trong Hình 2 có thể liên quan đến tăng hiệu suất ở RPSV cao vì hầu hết các bộ dữ liệu với lợi nhuận trung bình thấp có RPSV cao trong các thực nghiệm của chúng tôi. Chúng tôi cũng nhận thấy rằng lợi nhuận trung bình bộ dữ liệu cao có thể làm dịu lợi thế của chúng tôi. Lý do là RL ngoại tuyến với lấy mẫu đều đã khá hiệu quả trong các thiết lập mà σ trong phạm vi cao, chẳng hạn như 50%, và rằng phạm vi cải thiện bổ sung so với nó do đó bị hạn chế.

5.4 KẾT QUẢ TRONG CÁC BỘ DỮ LIỆU THÔNG THƯỜNG VỚI NHIỀU QUỸ ĐẠO CÓ LỢI NHUẬN CAO HƠN

Các bộ dữ liệu trong Mục 5.2 được tạo ra một cách đối nghịch để kiểm tra hiệu suất với các quỹ đạo có lợi nhuận cao cực kỳ thưa thớt. Tuy nhiên, chúng tôi hiển thị trong Hình 5 rằng các phân phối lợi nhuận thách thức như vậy không phổ biến trong các bộ dữ liệu thông thường trong các điểm chuẩn D4RL. Kết quả là, các bộ dữ liệu thông thường dễ hơn các bộ dữ liệu hỗn hợp với các quỹ đạo có lợi nhuận cao thưa thớt cho đường cơ sở lấy mẫu đều. Để chỉ ra rằng phương pháp của chúng tôi không mất hiệu suất trong các bộ dữ liệu thông thường với nhiều quỹ đạo có lợi nhuận cao hơn, chúng tôi cũng đánh giá phương pháp của mình trong 30 bộ dữ liệu thông thường từ điểm chuẩn D4RL (Fu et al., 2020) sử dụng cùng số liệu đánh giá trong Mục 5.1, và trình bày kết quả trong Hình 4a. Có thể thấy rằng cả hai phương pháp của chúng tôi đều thể hiện hiệu suất ngang bằng với các đường cơ sở trong các bộ dữ liệu thông thường, xác nhận rằng phương pháp của chúng tôi không mất hiệu suất. Lưu ý rằng chúng tôi không so sánh với Half vì các bộ dữ liệu thông thường được thu thập bởi nhiều chính sách không thể được chia thành hai bộ đệm. Đáng chú ý, chúng tôi thấy rằng với RW và AW của chúng tôi, BC đạt được hiệu suất cạnh tranh với các thuật toán RL ngoại tuyến khác (tức là, CQL, IQL, và TD3+BC). Việc cải thiện đáng kể so với lấy mẫu đều trong BC phù hợp với phân tích của chúng tôi (Mục 4.1) vì hiệu suất của BC phụ thuộc hoàn toàn vào hiệu suất của chính sách hành vi và do đó lợi nhuận trung bình của các quỹ đạo được lấy mẫu. Tuy nhiên, được ghép nối với RW và AW, các thuật toán RL ngoại tuyến (tức là, CQL, IQL, và TD3+BC) vẫn vượt trội BC. Điều này cho thấy rằng các chiến lược lấy mẫu có trọng số của chúng tôi không che lấp lợi thế của RL ngoại tuyến so với BC. Bảng hiệu suất đầy đủ có thể được tìm thấy trong Phụ lục A.13. Chúng tôi cũng đánh giá xác suất cải thiện của các phương pháp chúng tôi (Agarwal et al., 2021) so với lấy mẫu đều, hiển thị rằng các phương pháp của chúng tôi không tệ hơn các đường cơ sở trong Phụ lục A.8.1.

5.5 KẾT QUẢ TRONG CÁC MDP NGẪU NHIÊN

Vì chiến lược lấy mẫu có trọng số của chúng tôi về mặt lý thuyết yêu cầu một MDP xác định, chúng tôi điều tra xem động lực ngẫu nhiên (tức là, chuyển tiếp trạng thái ngẫu nhiên) có phá vỡ phương pháp của chúng tôi bằng cách đánh giá nó trong các môi trường điều khiển ngẫu nhiên. Chi tiết về việc triển khai của chúng có thể được tìm thấy trong Phụ lục A.6. Chúng tôi sử dụng số liệu đánh giá được mô tả trong Mục 5.1 và trình bày kết quả trong Hình 4b. Cả hai phương pháp của chúng tôi vẫn vượt trội lấy mẫu đều trong động lực ngẫu nhiên, cho thấy rằng tính ngẫu nhiên không phá vỡ chúng. Lưu ý rằng chúng tôi chỉ báo cáo kết quả với CQL vì IQL và TD3+BC không tương thích với không gian hành động rời rạc được sử dụng trong điều khiển cổ điển ngẫu nhiên.

6 CÁC CÔNG TRÌNH LIÊN QUAN

Các chiến lược lấy mẫu có trọng số của chúng tôi và phát lại trải nghiệm không đều trong RL trực tuyến nhằm cải thiện việc lựa chọn mẫu đều. Các công trình trước đây ưu tiên dữ liệu không chắc chắn (Schaul et al., 2015; Horgan et al., 2018; Lahire et al., 2021), chú ý đến các mẫu gần như on-policy (Sinha et al., 2022), hoặc chọn mẫu theo thứ tự tô-pô Hong et al. (2022); Kumar et al. (2020a). Tuy nhiên, các cách tiếp cận này không tính đến hiệu suất của chính sách hành vi ngầm định được tạo ra bởi việc lấy mẫu và do đó không có khả năng giải quyết vấn đề trong các bộ dữ liệu RL ngoại tuyến hỗn hợp.

Học bắt chước ngoại tuyến (IL) (Kim et al., 2021; Ma et al., 2022; Xu et al., 2022) xem xét việc huấn luyện một chính sách chuyên gia từ một bộ dữ liệu bao gồm một số ít dữ liệu chuyên gia và nhiều dữ liệu ngẫu nhiên. Họ huấn luyện một mô hình để phân biệt liệu một chuyển tiếp có từ một chuyên gia và học một chính sách gần như chuyên gia từ các dự đoán của bộ phân biệt. Về mặt khái niệm, các phương pháp của chúng tôi và IL ngoại tuyến nhằm khai thác dữ liệu có lợi (tức là, dữ liệu có lợi nhuận cao/chuyên gia thưa thớt) trong một bộ dữ liệu bất chấp các thiết lập bài toán khác nhau. IL ngoại tuyến yêu cầu rằng dữ liệu chuyên gia và ngẫu nhiên được cho trong hai bộ đệm tách biệt, nhưng không cần nhãn phần thưởng. Ngược lại, chúng tôi không yêu cầu các bộ dữ liệu có thể tách biệt nhưng yêu cầu nhãn phần thưởng để tìm dữ liệu có lợi.

7 THẢO LUẬN

Tầm quan trọng của việc học các quỹ đạo có lợi nhuận cao thưa thớt. Mặc dù hầu hết các bộ dữ liệu thông thường trong các điểm chuẩn RL ngoại tuyến chính như D4RL có nhiều quỹ đạo có lợi nhuận cao hơn các bộ dữ liệu hỗn hợp được nghiên cứu trong Mục 5.2, cần lưu ý rằng việc thu thập các dữ liệu có lợi nhuận cao này tẻ nhạt và có thể đắt đỏ trong các lĩnh vực thực tế (ví dụ, chăm sóc sức khỏe). Do đó, việc cho phép RL ngoại tuyến học từ các bộ dữ liệu với lượng hạn chế quỹ đạo có lợi nhuận cao là quan trọng để triển khai RL ngoại tuyến trong các nhiệm vụ thực tế hơn. Ý nghĩa của công việc chúng tôi là một kỹ thuật đơn giản để cho phép RL ngoại tuyến học từ một số ít quỹ đạo có lợi nhuận cao.

Hạn chế. Vì các phương pháp của chúng tôi yêu cầu lợi nhuận quỹ đạo để tính trọng số mẫu, các bộ dữ liệu không thể là các quỹ đạo phân mảnh một phần, và mỗi quỹ đạo cần bắt đầu từ các trạng thái trong phân phối trạng thái ban đầu; nếu không, lợi nhuận quỹ đạo không thể được ước tính. Một cách tiếp cận có thể để khắc phục hạn chế này là ước tính trọng số mẫu sử dụng một hàm giá trị đã học để có thể ước tính lợi nhuận kỳ vọng của một trạng thái mà không cần quỹ đạo hoàn chỉnh.
