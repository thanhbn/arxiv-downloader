# 2302.02662.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2302.02662.pdf
# Kích thước tệp: 3292473 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Grounding Các Mô hình Ngôn ngữ Lớn trong Môi trường Tương tác
với Học Tăng cường Trực tuyến
Thomas Carta∗
Inria (Flowers)
University of Bordeaux, France
thomas.carta@inria.frClément Romac∗
Inria (Flowers)
University of Bordeaux, France
Hugging Face
clement.romac@inria.frThomas Wolf
Hugging Face
Sylvain Lamprier
Univ Angers, LERIA,
SFR MATHSTIC, F-49000 Angers, FranceOlivier Sigaud
Sorbonne Université, ISIR, Paris, France
Pierre-Yves Oudeyer
Inria (Flowers)
University of Bordeaux, France
Tóm tắt
Các công trình gần đây đã thành công trong việc tận dụng khả năng của Các Mô hình Ngôn ngữ Lớn (LLM) trong việc nắm bắt kiến thức trừu tượng về vật lý thế giới để giải quyết các vấn đề ra quyết định. Tuy nhiên, sự liên kết giữa kiến thức của LLM và môi trường có thể sai lệch và hạn chế năng lực chức năng do thiếu grounding. Trong bài báo này, chúng tôi nghiên cứu một phương pháp (có tên GLAM) để đạt được sự liên kết này thông qua functional grounding: chúng tôi xem xét một agent sử dụng LLM như một policy được cập nhật dần dần khi agent tương tác với môi trường, tận dụng Học Tăng cường trực tuyến để cải thiện hiệu suất giải quyết mục tiêu. Sử dụng môi trường văn bản tương tác được thiết kế để nghiên cứu các dạng functional grounding ở mức độ cao hơn, và một tập hợp các nhiệm vụ không gian và điều hướng, chúng tôi nghiên cứu một số câu hỏi khoa học: 1) LLM có thể tăng hiệu quả mẫu cho việc học trực tuyến của các nhiệm vụ RL khác nhau không? 2) Làm thế nào nó có thể tăng cường các dạng tổng quát hóa khác nhau? 3) Tác động của việc học trực tuyến là gì? Chúng tôi nghiên cứu các câu hỏi này bằng cách grounding chức năng của một số biến thể (kích thước, kiến trúc) của FLAN-T5.

1 Giới thiệu
Sự phát triển gần đây của các Mô hình Ngôn ngữ Lớn dựa trên Transformer (LLM) được huấn luyện trên các tập dữ liệu văn bản khổng lồ trong Xử lý Ngôn ngữ Tự nhiên đã dẫn đến các mô hình thể hiện khả năng ấn tượng (ví dụ: tạo sinh ngôn ngữ tự nhiên, trả lời câu hỏi, lý luận, dịch thuật...) [Devlin et al., 2019, Brown et al., 2020, Rae et al., 2021, Chowdhery et al., 2022, Scao et al., 2022]. Gần đây, LLM được chứng minh là có thể nắm bắt các khía cạnh của quy luật vật lý trong thế giới của chúng ta, ví dụ về không gian Patel and Pavlick [2022], màu sắc Abdou et al. [2021] hoặc thậm chí affordances giữa các vật thể và đối tượng Ahn et al. [2022]. Dạng kiến thức tiên nghiệm này đã được khai thác để đề xuất kế hoạch hành động nhằm giải quyết mục tiêu trong robotics Huang et al. [2022b], Ahn et al. [2022], Liang et al. [2022]. Tuy nhiên, LLM được biết là gặp phải thiếu grounding điều này ngăn cản chúng xử lý đúng cách ý nghĩa của các khái niệm có liên quan với nhau
*Những tác giả này đóng góp bằng nhau cho công trình này
Đây là phiên bản mở rộng của bài báo được xuất bản trong Proceedings of the 40thInternational Conference on
Machine Learning , Honolulu, Hawaii, USA. PMLR 202, 2023.arXiv:2302.02662v4  [cs.LG]  17 Oct 2024

--- TRANG 2 ---
và việc sử dụng chúng cho năng lực chức năng trong môi trường tương tác Mahowald et al. [2023]. Thật vậy, sự liên kết giữa các cấu trúc thống kê trong các LLM như vậy và môi trường có thể rất hạn chế, hoặc thậm chí đôi khi hoàn toàn sai. Điều này một phần do 1) quá trình huấn luyện (dự đoán từ tiếp theo) không được khuyến khích trực tiếp để giải quyết vấn đề trong môi trường, 2) thiếu khả năng can thiệp vào môi trường để xác định cấu trúc nhân quả; 3) thiếu khả năng học dựa trên dữ liệu được thu thập như một kết quả của việc tương tác với môi trường [Bender and Koller, 2020, Bisk et al., 2020].

Trong tài liệu, language grounding đã đề cập đến các mục tiêu liên quan khác nhau Thill et al. [2014]. Đầu tiên, symbol grounding có thể được công thức hóa như vấn đề chung về việc kết nối một hệ thống ký hiệu [Harnad, 1990], nội bộ của một agent, với môi trường, theo cách mà việc xử lý nội bộ các ký hiệu này có thể được sử dụng để hành động phù hợp trong môi trường này. Một chiều của vấn đề này là liên kết các ký hiệu "cơ bản", chẳng hạn như tên của các đối tượng, với các cấu trúc bất biến trong các modality cảm nhận chiều cao như thị giác Cangelosi et al. [2010], Wiriyathammabhum et al. [2016]. Một grounding như vậy, được gọi là "direct grounding", đã được nghiên cứu rộng rãi trong quá khứ dẫn đến các phương pháp hiệu quả khác nhau [Alayrac et al., 2022, Radford et al., 2021, Lu et al., 2023], bao gồm trong bối cảnh của các cơ thể robot Cangelosi and Stramandinoli [2018]. Một chiều khác là cách ground các token symbolic bậc cao hơn, hoặc các khái niệm trừu tượng, thành các ký hiệu cơ bản, thường thông qua các phương pháp như ngữ nghĩa phân phối Harris [1981], Boleda [2019]. Điều này được gọi là "grounding transfer" Cangelosi and Stramandinoli [2018]. Ngoài những liên kết đơn thuần như vậy, một câu hỏi quan trọng về grounding là làm thế nào các quá trình nội bộ thao tác ký hiệu có thể mô hình hóa, dự đoán và kiểm soát các quá trình vật lý và xã hội bên ngoài: chúng cần được căn chỉnh và ràng buộc bởi các động lực học và cấu trúc quan hệ bên ngoài này (ở các mức độ trừu tượng khác nhau). Khái niệm grounding cuối cùng này, mà chúng tôi gọi ở đây là "functional grounding", tương đối với một môi trường cụ thể có thể là môi trường vật lý của con người nhưng cũng có thể là môi trường tương tác trừu tượng hơn được mô phỏng trong máy tính (nơi vật lý trừu tượng có thể khác với môi trường con người).

Trong bài báo này, chúng tôi xem xét thế giới văn bản tương tác Côté et al. [2018], Jansen [2021], được thiết kế chính xác để tập trung vào các dạng functional grounding mức độ cao hơn này. Trong thế giới văn bản, môi trường có thể mã hóa các dạng cấu trúc vật lý phong phú được lấy cảm hứng từ những cái trong thế giới con người, ví dụ Wang et al. [2022], tuy nhiên các agent hành động và cảm nhận trong các môi trường này chỉ thông qua modality văn bản. Trong bối cảnh này, bài báo này nhằm mục đích đạt được tiến bộ hướng tới câu hỏi mở rộng sau: làm thế nào LLM có thể được sử dụng như các policy agent tạo ra hành động hướng tới mục tiêu trong môi trường tương tác, cảm nhận kết quả của các hành động này, và dần dần grounding và cập nhật kiến thức của chúng với các quan sát mới mà chúng thu thập?

Dựa trên các công trình gần đây thành công sử dụng Học Tăng cường (RL) để tinh chỉnh LLM cho các nhiệm vụ tạo sinh ngôn ngữ tự nhiên [Stiennon et al., 2020, Ouyang et al., 2022, Ramamurthy et al., 2022], chúng tôi đề xuất nghiên cứu đầu tiên về functional grounding của LLM thông qua RL trực tuyến tăng dần. Cụ thể, chúng tôi nhằm mục đích trả lời thực nghiệm các câu hỏi khoa học mở sau:

•Q1. Hiệu quả mẫu: LLM có thể thích ứng và học nhanh như thế nào để giải quyết các vấn đề không gian và điều hướng khác nhau được chỉ định bằng ngôn ngữ tự nhiên? Việc sử dụng kiến thức được huấn luyện trước từ LLM tăng hiệu quả mẫu như thế nào?

•Q2. Tổng quát hóa cho các đối tượng mới: Một khi được grounded chức năng, LLM có thể tổng quát hóa như thế nào cho các loại thay đổi khác nhau về đối tượng, nhưng vẫn ở trong các nhiệm vụ đã được huấn luyện?

•Q3. Tổng quát hóa cho các nhiệm vụ mới: LLM được huấn luyện tương tác như vậy có thể thực hiện tổng quát hóa zero-shot cho các nhiệm vụ mới như thế nào? Tổng quát hóa phụ thuộc như thế nào vào loại nhiệm vụ mới?

•Q4. Tác động của can thiệp trực tuyến: Tác động thực nghiệm của grounding sử dụng RL trực tuyến với các tương tác tăng dần so với Behavioral Cloning offline từ một tập dữ liệu của các trajectory chuyên gia là gì?

Để trả lời các câu hỏi khoa học này trong Phần 4, chúng tôi trình bày một phương pháp functional grounding cho LLM (xem Hình 1 và Phần 3), và chuyển môi trường BabyAI [Chevalier-Boisvert et al., 2019] thành phiên bản văn bản. Ngoài ra, chúng tôi nhằm giúp cộng đồng RL phát triển thêm các kỹ thuật grounding cho LLM trong môi trường tương tác bằng cách phát hành, ngoài mã của bài báo này1, một thư viện Python có tên Lamorel2 tạo điều kiện cho việc sử dụng LLM ở quy mô lớn cho các nhà thực hành RL. Trong khi nhiều
1https://github.com/flowersteam/Grounding_LLMs_with_online_RL
2https://github.com/flowersteam/lamorel
2

--- TRANG 3 ---
công cụ đã tồn tại cho LLM và các nhiệm vụ NLP, chuyển sang thiết lập RL với môi trường tương tác đòi hỏi các điều chỉnh (ví dụ: nhu cầu rất thường xuyên về suy luận nhanh để tính xác suất hành động) làm cho các công cụ trước đó không phù hợp cho các nhà thực hành RL (xem Phần 3.4).

Hình 1: Phương pháp GLAM: chúng tôi sử dụng LLM như policy agent trong môi trường RL văn bản tương tác (BabyAI-Text) nơi LLM được huấn luyện để đạt được mục tiêu ngôn ngữ sử dụng RL trực tuyến (PPO), cho phép functional grounding. (a) BabyAI-Text cung cấp mô tả mục tiêu cho episode hiện tại cũng như mô tả quan sát của agent và phần thưởng vô hướng cho bước hiện tại. (b) Ở mỗi bước, chúng tôi thu thập mô tả mục tiêu và quan sát trong một prompt gửi đến LLM của chúng tôi. (c) Cho mỗi hành động có thể, chúng tôi sử dụng encoder để tạo biểu diễn của prompt và tính xác suất có điều kiện của các token tạo thành hành động cho prompt. Một khi xác suất của mỗi hành động được ước tính, chúng tôi tính hàm softmax trên các xác suất này và lấy mẫu một hành động theo phân phối này. Nghĩa là, LLM là policy agent của chúng tôi. (d) Chúng tôi sử dụng phần thưởng được trả về bởi môi trường để tinh chỉnh LLM sử dụng PPO. Để làm điều này, chúng tôi ước tính giá trị của quan sát hiện tại bằng cách thêm một value head trên đỉnh LLM của chúng tôi. Cuối cùng, chúng tôi lan truyền ngược gradient qua LLM (và value head của nó).

2 Công trình liên quan
RL có điều kiện ngôn ngữ Chúng tôi định vị công trình của mình trong thiết lập Language-conditioned RL, nơi
một agent tuân theo hướng dẫn học một policy thực hiện hành động trong môi trường tương tác để
hoàn thành một hướng dẫn ngôn ngữ [Luketina et al., 2019]. Trong khi một số công trình nghiên cứu thiết lập này cho các nhiệm vụ khác nhau trong môi trường 2D hoặc 3D [Hermann et al., 2017, Misra et al., 2017, Bahdanau et al., 2018, Colas et al., 2020, Chevalier-Boisvert et al., 2019], chúng tôi ở đây tập trung vào tương tác chỉ văn bản (nghĩa là thực hiện lệnh văn bản cho quan sát văn bản) như trong Shridhar et al. [2020]. Tuy nhiên, công trình của chúng tôi nghiên cứu cách LLM không chỉ có thể mã hóa hướng dẫn này [Hill et al., 2020] mà còn được sử dụng trực tiếp như policy agent chọn hành động cho quan sát.

Môi trường văn bản cho RL Nhiều môi trường chỉ văn bản đã được sử dụng và phát triển [Jansen, 2021, Wang et al., 2022]. Chúng thường triển khai lệnh văn bản cấp cao cùng với không gian hành động rất lớn và động lực học phức tạp giữa các thực thể, thường nhằm nghiên cứu functional grounding của các policy trừu tượng. Trong khi các môi trường này cung cấp thuộc tính thú vị, chúng tôi đã phải giới thiệu một môi trường mới đưa ra mục đích và ràng buộc của nghiên cứu chúng tôi. Xử lý ở đây với LLM tốn kém tính toán, chúng tôi đã chọn đánh đổi không gian hành động phức tạp để có thí nghiệm hệ thống nghiên cứu các câu hỏi của phần giới thiệu. Thứ hai, để thực hiện phân tích sâu về phương pháp functional grounding của chúng tôi, chúng tôi tập trung vào kỹ năng điều hướng cấp thấp hơn trong môi trường không gian (điều mà thiếu trong hầu hết môi trường văn bản vì agent thường chỉ có thể thay đổi phòng và có quyền truy cập trực tiếp vào các đối tượng trong phòng). Hơn nữa, một số nghiên cứu ablation được hiển thị trong Phụ lục B.6 đòi hỏi kiểm soát chính xác về tạo sinh thủ tục (thường không được cung cấp bởi môi trường văn bản). Vì những lý do này, chúng tôi đã điều chỉnh nền tảng BabyAI [Chevalier-Boisvert et al., 2019] thành phiên bản thủ tục chỉ văn bản cho phép tách biệt các thách thức khám phá khỏi các thách thức cảm nhận. Ngoài ra, chúng tôi vẫn có thể sử dụng các công cụ trực quan hóa của BabyAI để phân tích trajectory (xem Hình 1).

3

--- TRANG 4 ---
Foundation Models cho ra quyết định Foundation models được huấn luyện trên tập dữ liệu khổng lồ được chứng minh thể hiện khả năng ấn tượng cùng với thích ứng nhanh cho một loạt rộng các nhiệm vụ downstream trong thị giác [Yuan et al., 2021], ngôn ngữ [Devlin et al., 2019, Brown et al., 2020] và cross-modalities [Ramesh et al., 2021, Jiang et al., 2022, Alayrac et al., 2022]. Trong khi những khả năng như vậy đã được tận dụng để cung cấp phần thưởng cho các agent RL Gupta et al. [2022], Fan et al. [2022], một dòng công trình gần đây đã bắt đầu tập trung vào việc sử dụng Foundation Models (và đặc biệt là LLM) để hướng dẫn policy của agents.

Đầu tiên, SayCan [Ahn et al., 2022], Code as Policies [Liang et al., 2022] và Inner Monologue [Huang et al., 2022b] đã sử dụng LLM như các planner cấp cao trong thiết lập robotics. Bởi vì LLM của chúng không được sử dụng trực tiếp như policy agent cho hành động cấp thấp và không được grounded sử dụng tương tác của nó với môi trường, Ahn et al. [2022] đã phải sử dụng hàm affordance bên ngoài để xếp hạng lại các hành động được đề xuất bởi LLM. Tương tự, Yao et al. [2022] cũng có một phản hồi vòng kín giữa LLM là planner và agent là actor nhưng lần này trong môi trường văn bản. Mở rộng về điều này, Dasgupta et al. [2022] đã thêm một reporter quan sát môi trường và báo cáo thông tin hữu ích cho planner. Trong khi gợi ý về tính hữu ích của kiến thức tiên nghiệm có trong LLM cho các nhiệm vụ embodied, các công trình này bị hạn chế bởi sự vắng mặt của grounding.

Thứ hai, một số công trình đề xuất tinh chỉnh LLM trước trên trajectory chuyên gia trước khi sử dụng chúng trong môi trường. Sử dụng benchmark ScienceWorld của họ, Wang et al. [2022] cho thấy LLM được tinh chỉnh sử dụng Behavioral Cloning hoạt động tệ hơn so với Deep Q-Network nhỏ hơn nhiều và được khởi tạo ngẫu nhiên được huấn luyện sử dụng RL hỗ trợ giả thuyết rằng grounding trong môi trường thông qua tương tác trực tiếp là rất quan trọng. Cuối cùng, Reid et al. [2022] tái sử dụng LLM để thực hiện RL offline trong môi trường không ngôn ngữ tận dụng các cấu trúc nội bộ được học bởi LLM nhưng không còn sử dụng từ hoặc ký hiệu mà chúng được huấn luyện để thao tác (Takagi [2022] đã điều tra cách các cấu trúc nội bộ này có thể liên quan cho các nhiệm vụ không liên quan).

Cuối cùng, người ta cũng có thể pretrain một policy sử dụng Behavioral Cloning hoặc RL offline từ trajectory chuyên gia trước khi tinh chỉnh nó với tương tác với môi trường. Liên quan đến công trình của chúng tôi, Online Decision Transformer [Zheng et al., 2022] đầu tiên sử dụng RL offline để pretrain một mô hình transformer và cuối cùng tinh chỉnh nó với RL trực tuyến. Nhưng so với nghiên cứu của chúng tôi, nó không sử dụng mục tiêu pretraining Language Modeling chung và do đó không nghiên cứu functional grounding của các ký hiệu ngôn ngữ.

Tinh chỉnh LLM với RL Các công trình gần đây đã thành công tận dụng RL để tinh chỉnh LLM. RL được sử dụng đặc biệt để cải thiện sự liên kết giữa văn bản được tạo và sở thích của con người [Stiennon et al., 2020, Ouyang et al., 2022, Ramamurthy et al., 2022]. Trong framework Reinforcement Learning from Human Feedback (RLHF) này, tạo sinh văn bản được xem như một vấn đề ra quyết định tuần tự nơi mỗi "hành động" của LLM là một token mới và "trạng thái" tương ứng với prompt. Hầu hết các phương pháp này sử dụng PPO [Schulman et al., 2017] để tinh chỉnh LLM của họ sử dụng hàm phần thưởng được học trên tập dữ liệu của các tương tác con người được thu thập. Với kỹ thuật này, Ouyang et al. [2022] đã quản lý để tạo ra đầu ra được con người liên kết hơn mặc dù có mô hình (InstructGPT) với 100lần ít tham số hơn GPT-3 [Brown et al., 2020]. Trong khi công trình của chúng tôi chia sẻ việc tinh chỉnh dựa trên PPO với RLHF, thiết lập của chúng tôi khác biệt với nó trong nhiều khía cạnh. Đầu tiên, LLM của chúng tôi được grounded chức năng sử dụng phần thưởng có điều kiện nhiệm vụ bên ngoài từ môi trường (tình cờ thưa thớt trong môi trường BabyAI-Text của chúng tôi) và không phải mô hình phần thưởng được học. Thứ hai, thiết lập RLHF không có động lực học môi trường bên ngoài kiểm soát trạng thái tiếp theo cho trạng thái trước đó và hành động (trạng thái tiếp theo trong RLHF chỉ là trạng thái trước đó với token được tạo cuối cùng được nối thêm). So sánh, công trình của chúng tôi phơi bày một vòng lặp bên ngoài được kiểm soát bởi môi trường mà động lực học của nó, cung cấp trạng thái và phần thưởng tiếp theo, không được biết đến LLM (so với RLHF nơi vòng lặp RL là vòng lặp bên trong trong quá trình tạo sinh token). Chúng tôi tin rằng sử dụng RL để tinh chỉnh LLM có thể được lấy từ góc độ rộng hơn trong đó cả framework của chúng tôi và RLHF đều là các ứng dụng cụ thể.

3 GLAM: Grounding LLM với RL trực tuyến
Chúng tôi giới thiệu phương pháp GLAM (cho Grounded LAnguage Models) nơi LLM được sử dụng như policy agent và được grounded chức năng trong môi trường tương tác sử dụng RL trực tuyến, tận dụng các quan sát và phần thưởng được thu thập để cải thiện bản thân hướng tới việc đạt được mục tiêu được công thức bằng ngôn ngữ. Chúng tôi chi tiết phương pháp này trong các đoạn sau và chuyển hướng người đọc tới Hình 1 để có cái nhìn sơ đồ. Chúng tôi đầu tiên chính thức hóa vấn đề RL văn bản mà chúng tôi giải quyết (a). Sau đó, chúng tôi chi tiết cách chúng tôi sử dụng LLM như policy agent để tương tác với BabyAI-Text (b, c). Cuối cùng, chúng tôi giải thích cách tinh chỉnh RL trực tuyến được sử dụng để ground LLM trong BabyAI-Text (d).

4

--- TRANG 5 ---
3.1 Phát biểu vấn đề
Chúng tôi giả định một thiết lập RL văn bản nơi, cho một từ vựng ngôn ngữ V, môi trường của chúng tôi trả về một quan sát o∈ VN và một phần thưởng r∈R theo sau một hành động a∈ A ⊂ VN (nghĩa là hành động là các chuỗi token). Chúng tôi cũng giả định một mô tả nhiệm vụ hoặc mục tiêu g∈ G ⊂ VN có điều kiện phần thưởng. Môi trường như vậy có thể được đóng khung như một Partially Observable Markov Decision Process có tăng cường mục tiêu M= (S,V,A,T,R,G,O, γ) với S không gian trạng thái, A ⊂ VN không gian hành động, G ⊂ VN không gian mục tiêu, T:S × A 7→ S hàm chuyển tiếp, R:S × A × G 7→ R hàm phần thưởng có điều kiện mục tiêu, O:S 7→ VN hàm quan sát ánh xạ một trạng thái tới mô tả văn bản và cuối cùng γ hệ số chiết khấu.

Trong công trình này, chúng tôi mở rộng nền tảng BabyAI [Chevalier-Boisvert et al., 2019] ban đầu được thiết kế cho grounded language learning và đề xuất một extension chỉ văn bản có tên BabyAI-Text. Chúng tôi tận dụng môi trường minigrid được tạo sinh thủ tục bên trong của BabyAI nơi một agent điều hướng và tương tác với đối tượng thông qua 6 lệnh văn bản: turn left, turn right, go forward, pick up, drop và toggle. Chúng tôi cũng tái sử dụng tập hợp các nhiệm vụ được giới thiệu trong BabyAI cũng như mô tả liên quan của chúng cùng với phần thưởng vô hướng thưa thớt. Khác biệt chính của chúng tôi là mô tả văn bản o∈ VN của quan sát một phần của agent được trả về bởi BabyAI-Text thay vì biểu diễn ký hiệu ban đầu được trả về bởi BabyAI (xem Phụ lục A.2). Chúng tôi tận dụng BabyAI-Text trong Phần 4 để đánh giá phương pháp grounding của chúng tôi.

3.2 LLM như policies trong môi trường tương tác
Để sử dụng LLM như policy trong môi trường văn bản tương tác như vậy, chúng tôi thu thập mô tả nhiệm vụ, mô tả văn bản của quan sát hiện tại và tập hợp các hành động có thể trong một prompt được sử dụng để feed LLM. Chúng tôi đã chọn một template prompt đơn lẻ tùy ý và đơn giản (xem Phụ lục C cho ví dụ) và không thực hiện kỹ thuật prompt engineering chuyên sâu nào. Thật vậy, khi chúng tôi tinh chỉnh LLM, chúng tôi mong đợi nó thích ứng với template prompt được chọn. Tuy nhiên, thiết kế cẩn thận hơn về prompts có thể cải thiện kết quả được hiển thị trong Phần 4.

Cho prompt này, bây giờ chúng tôi cần LLM xuất ra phân phối xác suất trên các hành động có thể P(A). Để làm điều này, Huang et al. [2022a], Li et al. [2022], Wang et al. [2022] đã sử dụng LLM để tạo văn bản. Nếu chuỗi ký tự được tạo tương ứng với một trong các hành động có thể (nghĩa là s∈ A), hành động này được chọn bởi agent. Nếu không, một ánh xạ ad-hoc phải được thực hiện để chọn một hành động ai∈ A cho s. Như một phương pháp thay thế, người ta cũng có thể sử dụng thực hành RL tiêu chuẩn hơn bằng cách thêm action heads - một Multi-Layer Perceptron (MLP) với |A| đầu ra - trên đỉnh LLM. Cuối cùng, Ahn et al. [2022] đề xuất sử dụng trực tiếp LLM để tính xác suất (log) của mỗi hành động ai∈ A bằng cách tính xác suất có điều kiện của mỗi token trong hành động ai={w0, ..., w |ai|} cho prompt p:
PLLM(ai|p) =|ai|Y
j=0PLLM(wj|p, w<j) (1)
với PLLM(wj|p, w<j) xác suất được tính bởi LLM của token wj cho prompt p và các token trước đó w<j (xem (c) từ Hình 1). Phương pháp này gặp khó khăn từ việc yêu cầu một forward pass trên LLM cho mỗi hành động để tính xác suất của chuỗi token của nó (đặc biệt so với action heads mới yêu cầu một forward pass duy nhất trên prompt để tính xác suất của tất cả hành động). Tuy nhiên, nó cũng có một số ưu điểm, đặc biệt, 1) không cần ánh xạ ad-hoc tiềm năng như khi văn bản được tạo, 2) chúng tôi chỉ sử dụng các hoạt động được huấn luyện trước từ LLM và tận dụng prior và heads language modeling 3) phương pháp này mạnh mẽ với bất kỳ không gian hành động nào và do đó có thể được sử dụng trên bất kỳ môi trường văn bản nào mà không có thay đổi.

Vì những lý do này, chúng tôi đã chọn phương pháp sau. Chúng tôi đầu tiên sử dụng log probabilities thay vì probabilities chuẩn hóa sử dụng LPLLM(ai|p) =P|ai|
j=0logPLLM(wj|p, w<j) thay thế cho PLLM(ai|p) để tránh nhiều hoạt động chuẩn hóa. Chúng tôi cuối cùng chuẩn hóa tất cả log probabilities để thu được phân phối trên A sử dụng hàm softmax:
P(ai|p) =eLPLLM(ai|p)
P
aj∈AePLLM(aj|p). (2)

5

--- TRANG 6 ---
3.3 Tinh chỉnh PPO
Bây giờ chúng tôi đề xuất tận dụng trải nghiệm được thu thập bởi LLM để thực hiện functional grounding. Chính thức hơn, chúng tôi nhằm học một policy π:O× G 7→ P(A) tối đa hóa tổng chiết khấu kỳ vọng của phần thưởng cho bất kỳ mục tiêu cho trước g∈ G. Chúng tôi sử dụng cho điều này thuật toán PPO [Schulman et al., 2017] vừa học một policy ˆπ:O×G7→P(A) và một hàm giá trị ˆV:O × G 7→ R xấp xỉ giá trị thực V(s, g) =Ea∼ˆπ(O(s),g)
R(s, g, a ) +γV(T(s, a), g)
.

Như đã đề cập trong Phần 3.2, chúng tôi tính xác suất của mỗi hành động ai∈ A sử dụng likelihood được tính bởi LLM là ˆπ(ai|o, g) =P(ai|p).

Cho xấp xỉ giá trị, chúng tôi thêm một MLP với một đầu ra duy nhất cho giá trị trên đỉnh lớp cuối cùng của khối Decoder đầu tiên (nghĩa là tại chỗ của language modeling heads) để tính ˆV(o|g) =ˆV(p) (xem (d) từ Hình 1). Vị trí này được giải thích bởi thực tế rằng chúng tôi sử dụng LLM Encoder-Decoder trong thí nghiệm của chúng tôi nhưng phương pháp của chúng tôi có thể dễ dàng được sử dụng với mô hình Decoder-only bằng cách gắn value head vào khối Decoder mã hóa token cuối cùng của prompt.

3.4 Policies LLM phân tán sử dụng Lamorel
Sử dụng LLM để tính xác suất trên không gian hành động tốn kém tính toán vì nó đòi hỏi tính toán Q|ai|
j=0PLLM(wj|p, w<j) cho mỗi hành động ai={w0, ..., w |ai|}. Khi người ta sử dụng LLM rất lớn (nghĩa là hơn hàng trăm triệu tham số), tính xác suất của một hành động duy nhất đã có nghĩa là thực hiện một forward pass dài trên toàn bộ mạng. Kết quả là, tính xác suất của mỗi hành động có thể ở mọi bước trở nên rất chậm. Xem xét số lượng tương tác thường cần thiết để giải quyết nhiệm vụ trong BabyAI (và theo phần mở rộng BabyAI-Text), thực hiện tinh chỉnh RL trực tuyến của LLM dễ dàng trở nên không thể thực hiện được với một LLM duy nhất được phân phối trên nhiều GPU. Để vượt qua điều này, chúng tôi triển khai NLLM workers mỗi cái xử lý một tập con hành động để score song song (cho phép giảm thời gian quasi-linear với N). Chúng tôi thêm vào suy luận phân tán này khả năng cũng thực hiện huấn luyện phân tán (nghĩa là tính gradient của minibatches song song và thu thập gradients trước khi cập nhật mô hình). Chúng tôi bao bọc tất cả điều này trong một thư viện Python có tên Lamorel được thiết kế cho các nhà thực hành RL háo hức sử dụng LLM. Nó cho phép người ta sử dụng LLM như black-box nhưng cũng thực hiện các phương pháp tiên tiến hơn như thêm heads mới trên đỉnh chúng. Xem Phụ lục E cho chi tiết hơn.

4 Thí nghiệm
Chúng tôi thiết kế một tập hợp thí nghiệm trong BabyAI-Text nhằm cung cấp câu trả lời cho các câu hỏi khoa học được giới thiệu trong Phần 1. Trong các thí nghiệm này, chúng tôi sử dụng Flan-T5 780M [Rae et al., 2021] cho 1) liên kết gần gũi giữa corpus huấn luyện của nó (chứa tài liệu tuân theo hướng dẫn) và môi trường tương tác có điều kiện ngôn ngữ của chúng tôi, và 2) truy cập mã nguồn mở đơn giản của nó thông qua các công cụ Hugging Face3. Chúng tôi áp dụng phương pháp GLAM của chúng tôi vào Flan-T5 (mà chúng tôi đặt tên GFlan-T5 trong thí nghiệm dưới đây cho Grounded Flan-T5) và so sánh nó với ba baselines. Đầu tiên, chúng tôi cũng huấn luyện một Flan-T5 không được huấn luyện trước nơi chúng tôi chỉ tái sử dụng lớp embedding được huấn luyện trước và thêm action heads trên đỉnh nó (xem Hình 10 trong phụ lục). Như đối với GFlan-T5, chúng tôi lan truyền gradient qua toàn bộ đồ thị (bao gồm action heads ở đây). Chúng tôi gọi baseline này NPAE-Flan-T5 (Non-Pretrained with Action heads and Embedding Flan-T5). Chúng tôi cho thấy trong Phụ lục B.4 rằng sử dụng Flan-T5 không được huấn luyện trước trong khi giữ phương pháp scoring thất bại. Chúng tôi cũng cung cấp như một baseline RL cổ điển hơn một agent DRRN [He et al., 2016] khoảng 1M tham số thường được sử dụng cho TextWorlds. Chúng tôi đặc biệt tái sử dụng implementation từ Wang et al. [2022] đã cho kết quả SOTA và vượt trội LLM. Ở mỗi bước, chúng tôi feed 3 agents trên với template prompt sau được điền sử dụng thông tin được trả về bởi BabyAI-Text (xem Phụ lục C cho ví dụ):
•Một header liệt kê các hành động nào có thể truy cập (nhưng không nhất thiết hữu ích) trong môi trường dưới dạng:
Possible action of the agent: <list of actions >
•Mục tiêu của agent: Goal of the agent: <goal>
3https://huggingface.co/docs/transformers/model_doc/flan-t5

6

--- TRANG 7 ---
•3 quan sát trước đó và 2 hành động cuối cùng, được sử dụng như bộ nhớ ngắn hạn cần thiết để hoàn thành các nhiệm vụ BabyAI-Text (so sánh, DRRN sử dụng các lớp recurrent để xử lý yêu cầu bộ nhớ ngắn hạn):
Obs. 0: <description from BabyAI-Text at step t−2>
Action 0: <action chosen by the agent at step t−2>
Obs. 1: <description from BabyAI-Text at step t−1>
Action 1: <action chosen by the agent at step t−1>
Obs. 2: <description from BabyAI-Text at step t >
Action 2: <the next action to be chosen by the agent >

Cuối cùng, vì BabyAI-Text đơn giản cung cấp một ánh xạ thay thế của quan sát, chúng tôi thêm như một chỉ dẫn hiệu suất của agent PPO được sử dụng trong [Chevalier-Boisvert et al., 2019] chạy trên BabyAI thay vì BabyAI-Text (nghĩa là sử dụng quan sát ký hiệu thay vì mô tả văn bản) và đặt tên agent này Symbolic-PPO trong kết quả dưới đây. Trong Phụ lục B.3, chúng tôi cho thấy quan sát ký hiệu được cung cấp bởi BabyAI mã hóa bias làm dễ dàng việc học so với mô tả văn bản. Tuy nhiên, thậm chí với lợi thế này, GFlan-T5 vượt trội Symbolic-PPO trong tất cả thiết lập của chúng tôi.

Chúng tôi đầu tiên nghiên cứu Q1 bằng cách huấn luyện các agent khác nhau trong thiết lập multi-task đánh giá hiệu quả của chúng trong việc học các nhiệm vụ khác nhau. Sau đó chúng tôi giải quyết câu hỏi Q2, Q3 và Q4 sử dụng một tập hợp thí nghiệm tổng quát hóa (Hình 4) trên khả năng zero-shot của các agent được huấn luyện kết quả chủ yếu được lấy cảm hứng từ [Colas et al., 2020] và [Valmeekam et al., 2022]. Chúng tôi báo cáo tỷ lệ thành công trung bình cũng như độ lệch chuẩn. Chúng tôi so sánh kết quả của GFlan-T5, DRRN cũng như Flan-T5 (nghĩa là LLM được sử dụng trong GFlan-T5 nhưng trước khi tinh chỉnh của chúng tôi) để cho thấy phương pháp grounding của chúng tôi tác động đến nó như thế nào. Tất cả kết quả dưới đây được cho với khoảng tin cậy 99% của chúng (chi tiết toán học được cho trong Phụ lục G).

4.1 LLM có thể thích ứng và học nhanh như thế nào để giải quyết nhiệm vụ? (Q1)
Để nghiên cứu câu hỏi Q1, chúng tôi huấn luyện các agent của chúng tôi trong 1.5 triệu bước trong BabyAI-Text nơi mỗi episode là một nhiệm vụ được lấy mẫu ngẫu nhiên từ sau:
•Go to <object >, một nhiệm vụ điều hướng đơn giản yêu cầu khả năng lý luận để chọn kế hoạch đúng cho vị trí đối tượng;
•Pick up <object >, một nhiệm vụ lý luận kết hợp nhiệm vụ điều hướng;
•Put<object A >next to <object B >, yêu cầu đầu tiên đạt <object A >, nhặt nó lên, đạt <object B >và cuối cùng thả <object A >bên cạnh <object B >;
•Pick up <object A >then go to <object B >và Go to <object B >after pick up <object A>, cả hai phục vụ để kiểm tra khả năng lý luận trên chuỗi thời gian;
•Unlock <door>, một nhiệm vụ bao gồm suy luận rằng cần một chìa khóa để mở khóa cửa, tìm chìa khóa đúng (nghĩa là cái có màu như cửa) và cuối cùng sử dụng hành động toggle với chìa khóa trên cửa.

Trong mỗi nhiệm vụ, agent phải điều hướng trong một phòng được tạo sinh thủ tục với 8 distractors (nghĩa là đối tượng vô dụng cho việc hoàn thành nhiệm vụ).

Chúng tôi vẽ mean và standard deviation của tỷ lệ thành công (nghĩa là 1 nếu mục tiêu đã được đạt, 0 nếu không) trên 4 seeds của GFlan-T5, NPAE-Flan-T5, DRRN và Symbolic-PPO trong Hình 2. Ngoài ra, chúng tôi cũng theo dõi sự phát triển của xác suất của mỗi hành động có thể trên một tập hợp 11 prompts đánh giá để đánh giá khả năng của agents giải quyết mỗi nhiệm vụ trong Phụ lục C. Bằng cách vẽ sự phát triển của phân phối trên các hành động có thể trong Hình 18, chúng tôi nắm bắt tốt hơn cách và khi nào các agents học kỹ năng (ví dụ: kỹ năng điều hướng).

Nhìn vào sự phát triển của tỷ lệ thành công trung bình, GFlan-T5 nhanh chóng đạt 0.8 chỉ sau 250.000 bước (và 0.9 sau khoảng 600.000 bước). So sánh, cả DRRN và NPAE-Flan-T5 vẫn dưới 0.2 sau 1.5 triệu bước. Thậm chí khi so với Symbolic-PPO, sử dụng quan sát ký hiệu (dễ xử lý hơn ngôn ngữ như được hiển thị trong Phụ lục 7), GFlan-T5 thể hiện hiệu quả mẫu tốt hơn đáng kể với Symbolic-PPO gần đạt 0.4 sau 1.5 triệu bước. Hình 18 và Bảng 2 nêu bật cách GFlan-T5 tận dụng kiến thức của nó về mối quan hệ giữa các thực thể để học nhiệm vụ điều hướng trong ít hơn một trăm cập nhật.

7

--- TRANG 8 ---
Hình 2: Q1. Hiệu quả mẫu: Sự phát triển trên 4 seeds của tỷ lệ thành công trung bình và độ lệch chuẩn trên tất cả nhiệm vụ Q1. Chi tiết tính toán tỷ lệ thành công trung bình trên các mục tiêu được cho trong Phụ lục B.2

(a) Tác động của kích thước không gian hành động (3, 6 hoặc 9 hành động với luôn luôn chỉ 3 hành động hữu ích).
(b) Tác động của số lượng distractors.

Hình 3: Tác động của kích thước không gian hành động và số lượng distractors trên measure hiệu quả mẫu (Phương trình (3)). Chúng tôi báo cáo kết quả trung bình trên 2 seeds cho huấn luyện trên nhiệm vụ Go To.

Thất bại của NPAE-Flan-T5 cả nêu bật cách GFlan-T5 tận dụng kiến thức được huấn luyện trước của LLM để xử lý các nhiệm vụ được đề xuất và cách phương pháp tinh chỉnh giúp đạt được mục tiêu grounding. Hơn nữa, thực tế rằng GFlan-T5 vượt trội mạnh mẽ Symbolic-PPO và cái sau tốt hơn NPAE chứng minh cách ngôn ngữ có thể được sử dụng như một công cụ để scaffold việc học nếu đã được tiếp thu. Nó cũng giải thích cách có thể phản tác dụng nếu người ta yêu cầu agent vừa học nhiệm vụ và ngôn ngữ cùng một lúc (xem Phụ lục B.3 cho kết quả thêm).

Bây giờ chúng tôi thực hiện phân tích sâu hơn về hiệu quả mẫu này bằng cách nghiên cứu tác động của việc thay đổi không gian hành động và số lượng distractors. Chúng tôi cung cấp cả sự phát triển của tỷ lệ thành công và một measure hiệu quả mẫu SE:
SE=1
TTX
t=0SRt (3)
nơi T là số bước hoặc frames được thấy và SR tỷ lệ thành công tại frame t.

8

--- TRANG 9 ---
4.1.1 Tác động của chiều của không gian hành động
Trong thí nghiệm này, chúng tôi kiểm tra độ nhạy của LLM với kích thước của không gian hành động bằng cách sử dụng 3 không gian hành động khác nhau khi được huấn luyện trên nhiệm vụ Go to <object >:
•Không gian hành động bị hạn chế bao gồm chỉ 3 hành động hữu ích: turn left, turn right, go forward.
•Không gian hành động canonical bao gồm 6 hành động có thể được thực hiện trong môi trường với 3 hành động hữu ích và 3 hành động vô dụng là pick up, drop và toggle (chúng vô dụng ở đây vì agent chỉ điều hướng).
•Không gian hành động augmented bao gồm 9 hành động (3 hữu ích và 6 vô dụng với pick up, drop, toggle, sleep, do nothing và think). Ba hành động cuối cùng đã được chọn sao cho chúng rõ ràng không có ích cho nhiệm vụ Go To <object > và do đó không nên tác động đến agent có kiến thức về thế giới.

Chúng tôi tiến hành kiểm tra trong môi trường với 1 phòng, 8 distractors và báo cáo kết quả trong Hình 3a. Kết quả cho thấy không có tác động trên GFlan-T5, trong khi hiệu suất của các agent khác giảm với không gian hành động lớn hơn. Chúng tôi giả thuyết rằng điều này do khả năng của LLM trong việc loại bỏ các hành động vô dụng nhanh chóng ở đầu của tinh chỉnh.

4.1.2 Tác động của số lượng distractors
Tương tự, chúng tôi mong đợi LLM ít nhạy cảm hơn với các biến đổi trong độ phức tạp nhiệm vụ. Chúng tôi đánh giá điều này bằng cách vẽ sự phát triển của hiệu quả mẫu (Phương trình (3)) cho 4, 8 và 16 distractors. Chúng tôi tiến hành các kiểm tra này trong môi trường với 1 phòng và quan sát mất hiệu suất nhẹ từ GFlan-T5 khi số lượng distractors tăng (Hình 3b). So sánh, Symbolic-PPO suy giảm khi số lượng distractors tăng với tỷ lệ thành công giảm 38% từ 4 đến 16 distractors trong khi tỷ lệ thành công GFlan-T5 chỉ giảm 14%. Chúng tôi giả thuyết rằng LLM quản lý tập trung vào khía cạnh liên quan của môi trường một cách nhanh chóng.

Do đó, GFlan-T5 có vẻ mạnh mẽ với đường cong học tương tự khi người ta tăng kích thước không gian hành động (từ 3 đến 9 hành động với chỉ 3 hữu ích) hoặc số lượng distractors (từ 4 đến 16) Chúng tôi cũng cung cấp trong Phụ lục B một ablation bổ sung phân tích tác động của kích thước LLM B.5. Kết quả nêu bật rằng số lượng tham số có tác động cao trên quá trình học. Thật vậy, chúng tôi quan sát sự khác biệt mạnh về hiệu quả mẫu và hiệu suất tiệm cận giữa LLM nhỏ (80 triệu tham số) và 780 triệu tham số chúng tôi sử dụng ở đây. Chúng tôi cũng vẽ đường cong học đầy đủ cho ablation trên kích thước không gian hành động và số lượng distractors tương ứng trong phụ lục B.6.1 và B.6.2.

4.2 Q2. Tổng quát hóa cho đối tượng mới
Trong phần này, chúng tôi phân tích cách một agent được grounded chức năng có thể tổng quát hóa kỹ năng của nó cho đối tượng mới. Thật vậy, chúng tôi mong đợi các agent của chúng tôi tập trung vào hình học của môi trường (cách đối tượng được định vị và cách định vị của chúng được mô tả), nhưng không phải trên danh tính của chính các đối tượng (ví dụ: Go to <object > nên được đạt ngay cả khi đối tượng chưa được thấy trong quá trình huấn luyện). Chúng tôi kiểm tra nếu thuộc tính này có mặt trong các agent được huấn luyện của chúng tôi bằng cách đo hiệu suất zero-shot của chúng trong hai môi trường. Đầu tiên, một môi trường với danh từ không có trong từ vựng huấn luyện (ví dụ: "tree")4 và thứ hai, một môi trường với đối tượng được phát minh (được làm từ tính từ được phát minh và danh từ được phát minh như faze dax)5 Chúng tôi sử dụng môi trường mà agent đã được tinh chỉnh (nghĩa là không có thay thế từ nào) như môi trường kiểm soát. Kết quả trong Hình 4 (phần Q2) chỉ ra rằng GFlan-T5 không bị ảnh hưởng khi nhiệm vụ chứa danh từ ngoài từ vựng. Hơn nữa, thậm chí nếu tỷ lệ thành công của GFlan-T5 giảm 13% khi nó ở trong môi trường với đối tượng được phát minh, nó vẫn giữ hiệu suất mạnh so với baselines. Những kết quả này hỗ trợ giả thuyết rằng GFlan-T5 đã grounded chức năng các ký hiệu mô tả hình học của môi trường và các hướng dẫn (ví dụ: từ như "in front", hoặc ý nghĩa của "steps" như một measure khoảng cách)6.
4Danh từ ngoài từ vựng được cho trong Phụ lục H.1.
5Đối tượng được phát minh được cho trong Phụ lục H.2.
6Xem Phần A.2 cho chi tiết hơn về hình học.

9

--- TRANG 10 ---
Hình 4: Kiểm tra tổng quát hóa: Chúng tôi huấn luyện tất cả agents trên hỗn hợp 5 nhiệm vụ khác nhau và đánh giá khả năng tổng quát hóa của chúng trên 1000 episode kiểm tra (cũng chứa hỗn hợp của 5 nhiệm vụ này) (a). Chúng tôi so sánh chúng với hai baselines: một agent chọn hành động ngẫu nhiên (Random) và Flan-T5 zero-shot (không có tinh chỉnh nào). Sau đó chúng tôi thực hiện một số nghiên cứu tổng quát hóa để trả lời Q2 và Q3 bằng cách (b) thay thế tên đối tượng bằng tên ngoài từ vựng, (c) thay thế đối tượng và màu sắc bằng từ được phát minh, (d) kiểm tra một tổ hợp mới của nhiệm vụ, (e) thay thế hành động bằng từ đồng nghĩa và (f) dịch toàn bộ môi trường sang tiếng Pháp cho nhiệm vụ Go To. Cho mỗi agent, chúng tôi vẽ tỷ lệ thành công trung bình trên 2 seeds cùng với khoảng tin cậy và delta với hiệu suất trên cùng nhiệm vụ không có thay đổi nào (ngoại trừ (d), trên đó không có kết quả baseline nào có thể được cung cấp vì nhiệm vụ này hoàn toàn mới). Chi tiết tính toán tỷ lệ thành công trung bình trên các mục tiêu được cho trong Phụ lục B.2.

4.3 Q3. Tổng quát hóa cho nhiệm vụ mới
Trong Phần này, chúng tôi thực hiện kiểm tra tổng quát hóa như trong Phần 4.2, nhưng với nhiệm vụ mới chưa thấy. Sử dụng những cái này, chúng tôi xác minh đến mức độ nào một agent có thể tổ hợp và tổng quát hóa trên các ký hiệu mà nó đã grounded trong quá trình tinh chỉnh.

Bảng 1: Kiểm tra tổng quát hóa cho Behavioral Cloning
Môi trường GFlan-T5 BC-GFlan-T5 BC-Bot Random
Q4 Nhiệm vụ Go To không thay đổi 0.82±0.02 0.69±0.08 0.73±0.07 0.30±0.05
Nhiệm vụ Go To với từ được phát minh 0.74±0.004 0.7±0.07 0.63±0.08

Tổ hợp mới của nhiệm vụ đã học: Pick up <object A >then/after pick up <object B >
Trong quá trình tinh chỉnh, agents học làm cả 1) Pick up <object A > và 2) Pick up <object A >then go to <object B > hoặc Go to <object B >after pick up <object A > nhiệm vụ. Chúng tôi kiểm tra trong thí nghiệm này nếu agent có thể tổ hợp ký hiệu grounded để giải quyết nhiệm vụ mới Pick up <object A > <then/after >pick up<object B >. Kết quả trong Hình 4 (phần Q3) gợi ý rằng, trong khi tất cả agents thất bại trong việc giải quyết các nhiệm vụ mới này, GFlan-T5 vượt trội các baselines khác bằng cách đạt tỷ lệ thành công 0.12 so với Flan-T5 (0.07) hoặc Random (0.05). Những kết quả thấp này có thể được giải thích bởi thực tế rằng không có agents nào quản lý thành thạo nhiệm vụ Pick up <object A >then go to <object B > hoặc Go to <object B >after pick up <object A> trong quá trình huấn luyện (xem Phụ lục C). Chi tiết hơn về grounding của "then" và "after" được cho trong Phụ lục D.4.

Nhiệm vụ đã thấy với hành động từ đồng nghĩa Trong nhiệm vụ này, chúng tôi kiểm tra độ mạnh mẽ của các agent của chúng tôi đối với hành động bằng cách thay thế các hành động được sử dụng trong quá trình huấn luyện bằng từ đồng nghĩa. Ví dụ, "go forward" được thay thế bằng "move ahead"7. Chúng tôi mong đợi LLM, đã học ánh xạ từ tới không gian embedding, cũng ground từ đồng nghĩa khi chúng ground từ của môi trường. Trong môi trường này (xem Hình 4 phần Q3), tỷ lệ thành công của GFlan-T5 là 0.12 so với 0.01 cho Flan-T5. Do đó grounding của một số từ (ở đây các hành động) cũng cải thiện grounding của từ đồng nghĩa của chúng. Tuy nhiên, chúng tôi quan sát giảm 87% hiệu suất so với thiết lập gốc, mà chúng tôi cho rằng do over-fitting của từ vựng hành động.

Ngôn ngữ mới Để hiểu agents có thể tổng quát hóa đến đâu, chúng tôi kiểm tra chúng với ngôn ngữ không thấy trong quá trình huấn luyện (tiếng Pháp). Biết rằng Flan-T5 đã được huấn luyện trước với corpus đa ngôn ngữ và có thể dịch câu đơn giản, chúng tôi kiểm tra liệu grounding trong GFlan-T5 cũng đã tác động đến thao tác của nó với ngôn ngữ khác. Tuy nhiên, chúng tôi quan sát rằng thậm chí chỉ cho một nhiệm vụ điều hướng đơn giản (nghĩa là Go To), mô hình thất bại trong việc tổng quát hóa sang ngôn ngữ mới với tỷ lệ thành công (0.02) tệ hơn ngẫu nhiên (0.30). Chúng tôi giả thuyết rằng khi quá nhiều ký hiệu grounded được sửa đổi cùng một lúc, functional grounding thất bại trong việc được chuyển sang hệ thống con ký hiệu mới này. Thí nghiệm bổ sung xác nhận và củng cố kết quả này được trình bày trong phụ lục D.2 và D.3.

4.4 Tác động của việc sử dụng RL so với Behavioral Cloning cho grounding là gì? (Q4)
Cuối cùng, chúng tôi nghiên cứu cách tương tác trực tuyến với môi trường, cho phép học thông qua can thiệp và thử-và-sai, cải thiện grounding so với Behavioral Cloning (BC) thuần túy. Chúng tôi so sánh GFlan-T5 được huấn luyện trên nhiệm vụ Go To trong 400000 bước với hai baselines được huấn luyện với Behavioral Cloning sử dụng 400000 transitions (xem Phụ lục F.2). Cho baseline được gọi BC-GFlan-T5, transitions được thu thập từ GFlan-T5 được tinh chỉnh trên nhiệm vụ Go To. Cho BC-Bot, transitions được thu thập sử dụng bot thủ tục BabyAI đạt tỷ lệ thành công 1.

Trong Bảng 1, chúng tôi đo tỷ lệ thành công của GFlan-T5 và baselines trên hai nhiệm vụ: Go To và Go To với danh từ và tính từ được phát minh. Đầu tiên, người ta có thể thấy rằng GFlan-T5 vượt trội tất cả baselines trong cả hai nhiệm vụ. Thứ hai, vì GFlan-T5 không đạt tỷ lệ thành công 1 trên nhiệm vụ Go To, các trajectory được thu thập của nó cho BC có thể chứa transitions lừa dối so với những cái được thu thập bởi bot. Do đó, chúng tôi thu được kết quả mong đợi rằng BC-Bot vượt trội BC-GFlan-T5. Cuối cùng, chúng tôi mong đợi các agent của chúng tôi không bị ảnh hưởng bởi môi trường nơi danh từ và tính từ được thay thế bằng những cái được phát minh trong các nhiệm vụ điều hướng như vậy. Thí nghiệm cho thấy GFlan-T5 ít bị ảnh hưởng (0.82→0.74) hơn BC-Bot (0.73→0.63). GFlan-T5 cũng hoạt động tốt hơn trong nhiệm vụ từ được phát minh so với BC-GFlan-T5 (tỷ lệ thành công 0.7).

5 Kết luận
Trong bài báo này, chúng tôi đã đề xuất phương pháp GLAM cho functional grounding (nghĩa là căn chỉnh ký hiệu nội bộ với động lực học bên ngoài để agent có thể sử dụng chúng để giải quyết nhiệm vụ trong môi trường) của LLM trong môi trường văn bản tương tác dựa trên RL trực tuyến. Sử dụng môi trường BabyAI-Text mới của chúng tôi, chúng tôi đã thực hiện một số thí nghiệm nghiên cứu 4 câu hỏi khoa học. Chúng tôi đã cho thấy cách GLAM, yêu cầu gần như không có sửa đổi cụ thể môi trường trên LLM, cho phép cải thiện đáng kể hiệu suất để giải quyết nhiệm vụ RL trong môi trường này so với việc sử dụng zero-shot LLM, so với tinh chỉnh có giám sát và so với tinh chỉnh RL của LLM không được huấn luyện trước. Chúng tôi đã cho thấy cách nó tăng cường cả hiệu quả mẫu và khả năng tổng quát hóa trong kiểm tra zero-shot (cả cho đối tượng mới và một số nhiệm vụ mới). Ngoài những kết quả chính này, chúng tôi đã cung cấp ablations sâu cho thấy hiệu ứng của một số tham số (ví dụ: kích thước) trên grounding. Chúng tôi tin rằng phương pháp này có thể hoạt động như một cột mốc hướng tới grounding và sử dụng LLM trong tương tác với thế giới của chúng ta. Tuy nhiên, nghiên cứu này vẫn gặp phải một số hạn chế, đặc biệt là thực tế rằng các thí nghiệm hiện tại bị hạn chế trong môi trường văn bản, và tính không hiệu quả tính toán khi mở rộng quy mô không gian hành động và kích thước của LLM. Tính không hiệu quả tính toán này đã ràng buộc bài báo này sử dụng một môi trường duy nhất và LLM khá nhỏ. Tuy nhiên, cải thiện hiệu quả tính toán (hoặc truy cập vào nhiều tài nguyên tính toán hơn) có thể cho phép tận dụng các Foundation models đa phương thức gần đây [Alayrac et al., 2022]) cho grounding LLM trong môi trường rộng hơn (ví dụ: cho thiết lập robotics [Lu et al., 2021, Ahn et al., 2022]). Song song với điều này, một hướng tương lai sẽ là nghiên cứu cách grounding chức năng LLM trên môi trường cụ thể ảnh hưởng đến khả năng zero-shot của nó nhưng cũng là tính dẻo dai và khả năng tiếp thu kỹ năng mới trong môi trường khác. Hơn nữa, những kết quả này gợi ý rằng sử dụng LLM như policy agent mở ra con đường để thoát khỏi thiết lập Tabula-Rasa RL và tạo ra các agent RL hiệu quả mẫu hơn nhiều.

Cuối cùng, sự phát triển gần đây của các ứng dụng triển khai thực tế sử dụng LLM đã nêu bật các thách thức xã hội và đạo đức khác nhau của việc sử dụng các mô hình như vậy trong kịch bản thực tế. Tương tự như RLHF, công trình của chúng tôi nghiên cứu cách căn chỉnh LLM tốt hơn (nhưng lần này với môi trường mà trong đó nhiệm vụ phải được giải quyết). Trong khi phương pháp của chúng tôi đứng như một khối xây dựng quan trọng đầu tiên cho các công trình tương lai làm cho LLM phù hợp hơn với môi trường của chúng, nó không được thiết kế để sẵn sàng cho triển khai thực tế và do đó chúng tôi không khuyến nghị sử dụng nó trong bối cảnh ứng dụng như vậy.

Lời cảm ơn và Tiết lộ Tài trợ
Các thí nghiệm được trình bày trong bài báo này đã được thực hiện sử dụng tài nguyên HPC của IDRIS dưới phân bổ 2022-[A0131011996] được thực hiện bởi GENCI. Công trình này cũng đã nhận được tài trợ từ Chương trình Framework Horizon Europe của Ủy ban Châu Âu dưới thỏa thuận tài trợ số 101070381 (dự án PILLAR-robots). Chúng tôi cũng muốn cảm ơn Victor Gondat vì sự giúp đỡ tử tế của anh ấy về schemas.

Tài liệu tham khảo
[Tiếp tục với danh sách tài liệu tham khảo dài...]
