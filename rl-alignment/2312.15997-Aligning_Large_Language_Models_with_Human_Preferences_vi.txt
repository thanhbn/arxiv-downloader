# 2312.15997.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2312.15997.pdf
# Kích thước tệp: 2080915 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Căn chỉnh Mô hình Ngôn ngữ Lớn với Sở thích Con người
thông qua Kỹ thuật Biểu diễn
Wenhao Liu†, Xiaohua Wang†, Muling Wu, Tianlong Li, Changze Lv
Zixuan Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng∗, Xuanjing Huang
Trường Khoa học Máy tính, Đại học Fudan, Thượng Hải, Trung Quốc
{whliu22,xiaohuawang22}@m.fudan.edu.cn
{zhengxq,xjhuang}@fudan.edu.cn
Tóm tắt
Căn chỉnh các mô hình ngôn ngữ lớn (LLM) với
sở thích con người là rất quan trọng để nâng cao
tính hữu ích của chúng về mặt hữu ích, trung thực,
an toàn, vô hại và thú vị.
Các phương pháp hiện có để đạt được sự căn chỉnh này
thường bao gồm việc sử dụng học tăng cường
từ phản hồi con người (RLHF) để tinh chỉnh
LLM dựa trên nhãn con người đánh giá chất
lượng tương đối của các phản hồi mô hình. Tuy nhiên,
RLHF dễ bị bất ổn trong quá trình tinh chỉnh
và đặt ra những thách thức trong việc triển khai.
Lấy cảm hứng từ lĩnh vực mới nổi
của kỹ thuật biểu diễn (RepE), nghiên cứu này
nhằm xác định các biểu diễn liên quan
cho sở thích con người cấp cao được nhúng trong
các mẫu hoạt động trong một LLM và đạt được
kiểm soát chính xác hành vi mô hình bằng cách biến đổi
các biểu diễn của nó. Phương pháp mới này,
được ký hiệu là Căn chỉnh Biểu diễn từ Phản
hồi Con người (RAHF), được chứng minh là hiệu quả,
hiệu quả về mặt tính toán và dễ triển khai.
Các thí nghiệm rộng rãi chứng minh
hiệu quả của RAHF không chỉ trong việc nắm bắt mà còn
thao tác các biểu diễn để căn chỉnh với một
phổ rộng sở thích hoặc giá trị con người,
thay vì bị giới hạn trong một khái niệm hoặc chức năng
duy nhất (ví dụ: sự trung thực hoặc thiên vị). Tính linh hoạt của RAHF
trong việc đáp ứng các sở thích con người đa dạng cho thấy
tiềm năng của nó trong việc nâng cao hiệu suất LLM.
Mã nguồn có sẵn tại https:
//github.com/LiuAmber/RAHF .
1 Giới thiệu
Trong khi các mô hình ngôn ngữ lớn (LLM) học được
kiến thức rộng rãi về thế giới và một mức độ thành thạo
trong suy luận, việc kiểm soát chính xác hành vi của chúng
tỏ ra khó khăn do bản chất không có giám sát
của quá trình tiền huấn luyện (Radford et al., 2018, 2019;
Brown et al., 2020; Bubeck et al., 2023; Touvron
et al., 2023). Đối với mỗi truy vấn, các LLM được tinh chỉnh hướng dẫn
†Những tác giả này đóng góp ngang nhau.
∗Tác giả liên hệ.
Hình 1: Minh họa các phương pháp khác nhau. (a) Học
tăng cường từ phản hồi con người (RLHF);
(b) Tối ưu hóa sở thích trực tiếp (DPO); (c) Ghi nhãn lại
hướng dẫn nhìn lại (HIR); (d) Căn chỉnh biểu diễn
từ phản hồi con người (RAHF).
LLM (Wei et al., 2021; Chung et al., 2022; Tou-
vron et al., 2023) thể hiện khả năng tạo ra
nhiều phản hồi vừa có tính mạch lạc về ngữ nghĩa
và cú pháp bằng một số kỹ thuật lấy mẫu.
Mặc dù khả năng này cho phép các mô hình
cung cấp sự đa dạng rất cần thiết cho các đại lý trò chuyện,
một số phản hồi có thể chứa nội dung có hại, phi đạo đức,
thiên vị xã hội và tiêu cực, thậm chí bất hợp pháp (Sri-
vastava et al., 2022; Thoppilan et al., 2022; Bubeck
et al., 2023; Wang et al., 2023).
Các phương pháp hiện có điều khiển LLM để căn chỉnh với sở
thích con người thường sử dụng học tăng cường (RL),
với học tăng cường từ phản hồi con người
(RLHF) nổi lên như phương pháp thành công nhất
(Ouyang et al., 2022). Tuy nhiên, các thuật toán
học cơ bản thể hiện mức độ phức tạp
đáng kể, nhạy cảm với siêu tham số, bất
ổn trong quá trình huấn luyện và cần thêm
huấn luyện trong mô hình phần thưởng và mạng giá trị,
dẫn đến chi phí tính toán đáng kể (Yuan
et al., 2023; Rafailov et al., 2023).
Trong việc giải quyết những thách thức nêu trên
do các phương pháp dựa trên RL gây ra, một số
lựa chọn thay thế nhẹ về mặt tính toán đã được đề xuất để

--- TRANG 2 ---
đơn giản hóa quá trình khớp sở thích con người.
Hai mô hình nổi bật trong số các lựa chọn thay thế này
bao gồm học tương phản (Rafailov et al., 2023;
Zhao et al., 2023; Yuan et al., 2023) và Ghi nhãn lại
hướng dẫn nhìn lại (HIR) (Zhang et al., 2023;
Liu et al., 2023). Các phương pháp dựa trên học tương phản
tối ưu hóa chính sách mô hình ngôn ngữ bằng cách tăng
xác suất tương đối của các phản hồi được ưa thích hơn
so với những phản hồi không được ưa thích, trong khi các phương pháp HIR
biến đổi phản hồi con người thành hướng dẫn bằng cách ghi nhãn lại
những hướng dẫn ban đầu, chỉ ra chất lượng tương đối của các
phản hồi được cung cấp. Một đặc điểm chung
được chia sẻ bởi hai mô hình này là khả năng căn chỉnh
các mô hình ngôn ngữ với sở thích con người thông qua
tinh chỉnh không cần phần thưởng.
Tuy nhiên, việc tinh chỉnh không cần phần thưởng dễ bị
tổn thương trước sự hiện diện của dữ liệu nhiễu hoặc nhãn
không chính xác trong tập huấn luyện bao gồm một bộ sưu tập
các cặp phản hồi được chú thích sở thích (Li et al.,
2023b; Dumoulin et al., 2023). Các trường hợp câu
buồn tẻ hoặc phản hồi rất ngắn có thể xuất hiện
lặp đi lặp lại trong tập huấn luyện như vậy, có khả năng giới thiệu
thiên vị vào các mô hình. Việc loại trừ những
trường hợp như vậy khỏi tập huấn luyện khiến LLM không thể
thu thập thông tin chi tiết về sở thích con người
được thể hiện trong những trường hợp này. Ngược lại, các phương pháp
dựa trên RL áp dụng một chiến lược khác, trong đó một hàm
phần thưởng đầu tiên được trích xuất từ tập dữ liệu xếp hạng
phản hồi, và sau đó hàm phần thưởng này có thể được áp dụng
để huấn luyện một LLM, hiệu quả giảm thiểu
việc tiếp xúc trực tiếp của mô hình với dữ liệu nhiễu hoặc
nhãn không chính xác trong tập dữ liệu.
Trong nghiên cứu này, chúng tôi nhằm tìm kiếm một
thuật toán nhẹ hơn về mặt tính toán và không cần phần thưởng
có thể hiệu quả khai thác sở thích con người được thể hiện
trong các tập dữ liệu đồng thời bảo vệ LLM khỏi
ảnh hưởng của dữ liệu nhiễu. Lấy cảm hứng từ tiến bộ
gần đây trong kỹ thuật biểu diễn (Zou et al.,
2023), chúng tôi ban đầu xác định vị trí các biểu diễn liên quan
và mẫu hoạt động liên quan đến sở thích con người
cấp cao trong một LLM, và sau đó
đạt được kiểm soát chính xác hành vi của nó bằng cách thao tác
các biểu diễn nội bộ của nó. Trong kiến trúc mạng neural,
trọng số mạng quyết định hoạt động neural, hoạt động neural
quyết định đầu ra của mạng, và đầu ra của mạng quyết định
hành vi của mạng. Thay vì tập trung vào các neuron
và kết nối của chúng, chúng tôi xem việc căn chỉnh LLM với
phản hồi con người như một kết quả của không gian biểu diễn,
được thực hiện bởi các mẫu hoạt động trên
các quần thể neuron. Chúng tôi đầu tiên xác định sự
khác biệt trong hoạt động mô hình giữa các kích thích được ưa thích và

không được ưa thích, và sau đó kiểm soát hành vi của nó
bằng cách tận dụng những khác biệt đã xác định trong biểu
diễn (xem Hình 1). Chúng tôi giới thiệu hai phương pháp
để kiểm soát biểu diễn và chứng minh
hiệu quả của những phương pháp kỹ thuật biểu diễn (RepE)
này trong việc căn chỉnh LLM với một phổ rộng
sở thích con người thông qua một bộ sưu tập
các cặp phản hồi.
Để xác thực hiệu quả của phương pháp chúng tôi trong
việc căn chỉnh với sở thích con người, chúng tôi đã tiến hành các
thí nghiệm so sánh rộng rãi về kết quả được tạo ra.
Phương pháp của chúng tôi vượt trội hơn RLHF và các
phương pháp không cần RL khác trong đánh giá con người và các
chỉ số tự động như khả năng chung và đánh giá
GPT-4. Đáng chú ý, các thuật toán cơ bản
thể hiện sự đơn giản trong triển khai và
tính trực tiếp trong huấn luyện.
2 Công trình liên quan
Điều chỉnh các mô hình ngôn ngữ lớn để kích thích các phản hồi
và hành vi mong muốn từ kiến thức rộng lớn
và khả năng của chúng là cần thiết trong việc phát triển
các đại lý trò chuyện, như ChatGPT (Brown et al.,
2020), LLaMA (Touvron et al., 2023) và GPT-4
(Bubeck et al., 2023), được đặc trưng bởi an toàn, hiệu
suất và khả năng kiểm soát. Việc mở rộng
kích thước của các mô hình ngôn ngữ chỉ riêng điều đó không
vốn dĩ nâng cao khả năng tuân theo ý định của người dùng.
Ví dụ, LLM vẫn có thể tạo ra đầu ra
không trung thực, độc hại hoặc đơn giản là không hữu ích
cho người dùng. Các phương pháp căn chỉnh sở thích con người
hiện có có thể được phân loại rộng rãi thành ba
danh mục chính: học tăng cường (Ouyang et al.,
2022; Ramamurthy et al., 2023), học tương phản
(Rafailov et al., 2023; Zhao et al., 2023; Yuan
et al., 2023), và Ghi nhãn lại hướng dẫn nhìn lại
(Zhang et al., 2023; Liu et al., 2023).
Nghiên cứu rộng rãi đã được dành cho việc khám phá
RL từ phản hồi con người thông qua xếp hạng
hoặc bảng xếp hạng, trải rộng các nhiệm vụ từ chuyển đổi NL-to-SQL
(Zhong et al., 2017), dịch máy
(Kreutzer et al., 2018), hệ thống đối thoại định hướng nhiệm vụ
(Su et al., 2019; Zhang et al., 2019; Takanobu
et al., 2019), tóm tắt (Stiennon et al., 2020),
kể chuyện (Ziegler et al., 2019) đến tuân theo hướng dẫn
(Ouyang et al., 2022; Ramamurthy et al.,
2023). Thông thường, những phương pháp này bao gồm việc khớp
một mô hình phần thưởng với tập dữ liệu sở thích con người,
tiếp theo là việc tối ưu hóa chính sách LLM
để tạo ra các phản hồi với phần thưởng cao, sử dụng
các thuật toán RL như REINFORCE (Williams,

--- TRANG 3 ---
1992) hoặc tối ưu hóa chính sách gần (Schulman
et al., 2017). Mặc dù tính hấp dẫn của việc tận dụng
sở thích con người dễ thu thập hơn
so với các demo chuyên gia, việc huấn luyện LLM với RL
đặt ra những thách thức thực tế đáng kể, được cho là
do tính nhạy cảm của RL đối với siêu tham số
và sự bất ổn vốn có trong quá trình huấn luyện.
Các giải pháp dựa trên Ghi nhãn lại hướng dẫn nhìn lại
(Zhang et al., 2023; Liu et al., 2023) và
học tương phản (Rafailov et al., 2023; Zhao
et al., 2023; Yuan et al., 2023) đã nổi lên như
các lựa chọn thay thế hiệu quả về mặt tính toán cho các phương pháp
dựa trên RL mà không có mô hình hóa phần thưởng rõ ràng. Tuy
nhiên, những giải pháp tinh chỉnh không cần phần thưởng này dễ bị
tổn thương trước dữ liệu nhiễu hoặc nhãn không chính xác trong
tập huấn luyện. Chúng thể hiện độ trễ hiệu suất so
với các mô hình được điều chỉnh với các đối tác RL (xem
Phần 4). Hơn nữa, câu hỏi liệu
LLM được huấn luyện với các phương pháp tinh chỉnh như vậy có thể
khái quát hóa tốt với các truy vấn ngoài phân phối hay không
vẫn chưa được giải quyết khi tương phản với các mô hình
kết hợp mô hình phần thưởng rõ ràng. Phương pháp RLHF
(Ouyang et al., 2022) cung cấp một con đường tiềm năng
để cải thiện bằng cách tận dụng các ví dụ không có nhãn bổ sung
thông qua việc ghi nhãn các thế hệ LLM
với mô hình phần thưởng đã học.
Để nâng cao tính minh bạch và khả năng kiểm soát của
mạng neural, Zou et al. (2023) đã giới thiệu kỹ
thuật biểu diễn (RepE) như một phương pháp luận,
vẽ ra một sự tương tự giữa việc hiểu các mạng neural sâu
thông qua chụp cắt lớp biểu diễn và nghiên cứu não bộ thông qua
các kỹ thuật hình ảnh thần kinh. Công trình của họ đã chứng minh
hiệu quả của RepE trong việc giải quyết các thách thức
liên quan đến an toàn đa dạng như tính trung thực, sự trung thực và ảo
giác. Nghiên cứu này phù hợp với các phát hiện nghiên cứu
gần đây và mở rộng ứng dụng của nó để căn chỉnh
LLM với một phổ rộng sở thích con người.
Nghiên cứu của chúng tôi giới thiệu hai phương pháp mới để hướng dẫn
LLM về sở thích con người trước, và sau đó trích xuất
sự khác biệt trong hoạt động mô hình giữa các kích thích được ưa thích
và không được ưa thích. Những khác biệt trong mẫu
hoạt động này phục vụ như một nền tảng để thao tác
hành vi của mô hình, dẫn đến việc tạo ra
các phản hồi căn chỉnh tốt hơn với sở thích con người.
Do những lợi thế tính toán nhẹ của
các kỹ thuật tinh chỉnh hiệu quả tham số (Houlsby
et al., 2019; Lester et al., 2021; Hu et al., 2021; Wu
et al., 2023, 2024), những kỹ thuật này được sử dụng để
khớp sự chênh lệch trong mẫu hoạt động. Trái ngược với
phương pháp được áp dụng bởi Zou et al. (2023), phương pháp này
dựa vào các kích thích không có nhãn hoặc tự tạo ra giới

hạn ở các khái niệm hoặc chức năng đơn lẻ mà ý nghĩa
của nó các mô hình đã "biết", các phương pháp của chúng tôi
cung cấp sự căn chỉnh toàn diện hơn
với các sở thích con người đa dạng.
3 Phương pháp
Chúng tôi bắt đầu bằng việc hướng dẫn LLM về sở thích
con người với một tập hợp các cặp phản hồi được chú thích sở thích.
Chúng tôi giới thiệu hai phương pháp mới để hướng dẫn
LLM về sở thích con người và trích xuất
các mẫu hoạt động của chúng: một phương pháp liên quan đến một LLM đơn
(được huấn luyện để phân biệt chất lượng tương đối của các phản hồi)
và phương pháp khác sử dụng LLM kép ("một chàng trai tốt
và một chàng trai xấu"). Thứ hai, chúng tôi thu thập các mẫu
hoạt động của LLM khi tiếp xúc với các kích thích được
ưa thích hoặc không được ưa thích. Sự khác biệt trong những
mẫu này phục vụ như nền tảng để thao tác
LLM, cho phép chúng tạo ra các phản hồi căn chỉnh
chặt chẽ hơn với các giá trị con người. Cuối cùng, chúng tôi
xây dựng mô hình cuối cùng bằng cách huấn luyện một bộ điều hợp
hạng thấp (Hu et al., 2021) để khớp sự chênh lệch trong mẫu
hoạt động.
3.1 Hướng dẫn LLM về Sở thích Con người
Để trích xuất các mẫu hoạt động từ mô hình căn chỉnh
với sở thích con người, điều quan trọng là
mô hình phải có sự hiểu biết và
nhận thức chính xác về những sở thích này. Hiệu quả
của việc trích xuất mẫu hoạt động từ các mô hình tinh chỉnh
căn chỉnh, như LLaMA-2-chat, trong việc nắm bắt
các khái niệm như tính trung thực và sự trung thực đã được
xác thực bởi Zou et al. (2023). Tuy nhiên, đối với các
mô hình không được căn chỉnh, như các mô hình ngôn ngữ lớn
được tiền huấn luyện hoặc LLM chịu tinh chỉnh đơn giản,
các chỉ dẫn rõ ràng về sở thích con người nên
được cung cấp để kích thích và nắm bắt các mẫu hoạt động
được gây ra bởi sở thích kích thích. Khả năng này
cho phép tích lũy các hoạt động đa dạng, sau đó
được sử dụng để hiệu chuẩn LLM dựa trên
sở thích con người.
Để hướng dẫn LLM về sở thích con người, chúng tôi
dựa vào tập dữ liệu được chú thích với sở thích con người.
Như đã đề cập trước đó, chúng tôi sử dụng hai phương pháp để
đạt được mục tiêu này. Phương pháp đầu tiên sử dụng Hind-
sight (Zhang et al., 2023), sử dụng các hướng dẫn tương phản
để hướng dẫn một LLM đơn. Phương pháp thứ hai
bao gồm việc tinh chỉnh hai LLM riêng biệt: một (được gọi
là mô hình ưa thích) được tinh chỉnh dựa trên
các phản hồi được ưa thích, trong khi mô hình khác (được gọi
là mô hình không ưa thích) được tinh chỉnh trên các phản hồi
không được ưa thích.

--- TRANG 4 ---
Bạn là một trợ lý tốt, lịch sự và 
trung thực ...
Đội bóng đá Trung Quốc có thể 
vào World Cup như thế nào?Truy vấnHướng dẫn
Để vào World Cup, đội bóng đá 
Trung Quốc phải ...Phản hồi
Bạn là một trợ lý xấu, bất lịch sự và 
không trung thực ...
Đội bóng đá Trung Quốc có thể 
vào World Cup như thế nào?Truy vấnHướng dẫn
Chỉ cần chơi bóng đá tốt hơn.Phản hồi
RAHF-SCIT Mẫu Hoạt động
Đội bóng đá Trung Quốc có thể 
vào World Cup như thế nào?Truy vấn
Để vào World Cup, đội bóng đá 
Trung Quốc phải ...Phản hồi
Đội bóng đá Trung Quốc có thể 
vào World Cup như thế nào?Truy vấn
Chỉ cần chơi bóng đá tốt hơn.Phản hồi
RAHF-Dual
+Chồng chất ActivationsMất mát Lỗi Bình phương Trung bình
Nhúng
Lớp 
Transformer×L
Lớp 
Transformer
...Mô hình CơsởNhúng
Lớp 
Transformer×L
Lớp 
Transformer
...Mô hình Cuối cùngNhúng
Lớp 
Transformer×L
Lớp 
Transformer
...
Mô hình Ưa thíchNhúng
Lớp 
Transformer×L
Lớp 
Transformer
...
Mô hình Không Ưa thíchMô hình 
Phân biệtNhúng
Lớp 
Transformer×L
Lớp 
Transformer...
Lan truyền ngược
Hình 2: Quy trình của RAHF. RAHF bắt đầu với việc giới thiệu hai phương pháp để hướng dẫn LLM về sở thích con người. Một phương pháp bao gồm huấn luyện một LLM đơn để phân biệt chất lượng tương đối của các phản hồi (RAHF-SCIT), trong khi phương pháp khác sử dụng LLM kép để mô hình hóa các phản hồi được ưa thích và không được ưa thích riêng biệt (RAHF-Dual). Cụ thể, RAHF-SCIT lấy các hướng dẫn được ưa thích và không được ưa thích cùng với các phản hồi tương ứng của chúng làm đầu vào và tiến hành điều chỉnh hướng dẫn tương phản trên một mô hình đơn. RAHF-Dual, mặt khác, thực hiện huấn luyện có giám sát bằng cách lấy các phản hồi được ưa thích và không được ưa thích vào các mô hình khác nhau. Sau đó, chúng tôi thu được các mẫu hoạt động bằng cách kích thích mô hình với các hướng dẫn khác nhau. Chúng tôi coi những khác biệt giữa hai mẫu hoạt động như chỉ thị của các tín hiệu được ưa thích và tận dụng những tín hiệu này để tinh chỉnh mô hình cuối cùng với LoRA.
3.1.1 Hướng dẫn Sở thích với Mô hình Đơn
Trong khung được đề xuất, Phương pháp LLM Đơn
tập trung vào việc tinh chỉnh một Mô hình Ngôn ngữ Lớn Đơn
thông qua Điều chỉnh Hướng dẫn Tương phản (SCIT). Quá trình này
bao gồm hai hướng dẫn: một hướng dẫn mô hình tạo ra các phản hồi
được con người ưa thích, trong khi hướng dẫn khác hướng dẫn
mô hình tạo ra các phản hồi không được con người ưa thích.
Sau khi tinh chỉnh như vậy, chúng ta có thể tối ưu hóa
mô hình để đảm bảo tính nhất quán với sở thích con người.
Chúng ta cũng có thể kích thích mô hình để gây ra các mẫu
hoạt động khác biệt bằng cách sử dụng các hướng dẫn khác nhau
sau đó.
Cụ thể, tập dữ liệu huấn luyện được tuyển chọn để
bao gồm các cặp hướng dẫn được ưa thích và không được ưa thích,
cùng với các truy vấn liên quan và các phản hồi tương ứng của chúng
(chi tiết về các hướng dẫn được ưa thích có thể tìm thấy trong Phụ lục A.1).
Theo HIR (Zhang et al., 2023), đối với các hướng dẫn liên kết
với sở thích tích cực, mục tiêu tinh chỉnh
nhằm tăng xác suất tạo ra các phản hồi được
ưa thích đồng thời giảm
xác suất tạo ra các phản hồi không được ưa thích.
Ngược lại, đối với các hướng dẫn liên quan đến sở
thích tiêu cực, mục tiêu là nâng cao
xác suất tạo ra các phản hồi không được ưa thích
và giảm xác suất tạo ra các phản hồi được
ưa thích.
Chính thức, hãy để D đại diện cho tập dữ liệu huấn luyện,
với qi biểu thị truy vấn, ri đại diện cho phản
hồi, và pi chỉ ra hướng dẫn (tích cực
hoặc tiêu cực). Việc tinh chỉnh LLM bao gồm
việc tối thiểu hóa mất mát sau:
L = −X
(pi,qi,ri)∈D(P++ logexp (P+)
exp (P+) + exp ( P−))
(1)
trong đó P+ = log π(ri|pi, qi;θ), P− = log π(ri|
p∗
i, qi;θ) và p∗
i biểu thị hướng dẫn đối lập,
đảm bảo sự tương phản giữa các trường hợp được ưa thích và không
được ưa thích.
Trong suốt toàn bộ quá trình tinh chỉnh, LLM
trải qua một giai đoạn học để phân biệt
giữa các phản hồi được ưa thích và không được ưa thích, tiết
lộ các mẫu hoạt động khác biệt liên quan đến sở
thích con người. Sau đó, hai hướng dẫn này sẽ
phục vụ như các kích thích để thu được các biểu diễn nội bộ
của mô hình, sẽ được sử dụng cho việc
căn chỉnh tiếp theo. Việc huấn luyện tương phản này dựa vào
dữ liệu sở thích cho phép đạt được mục tiêu
tổng thể về tính nhất quán với một phổ rộng
sở thích con người, thay vì một khái niệm đơn lẻ.
3.1.2 Hướng dẫn Sở thích với Mô hình Kép
Trong phương pháp LLM Kép, chúng tôi nhằm huấn luyện hai
LLM với các xu hướng khác biệt: một mô hình có
xu hướng tạo ra các phản hồi được ưa thích, trong khi

--- TRANG 5 ---
mô hình khác có xu hướng tạo ra các phản hồi không được ưa thích.
Để đạt được mục tiêu này, chúng tôi sử dụng dữ liệu sở thích
được ghép đôi để tiến hành tinh chỉnh có giám sát
của LLM. Cụ thể, chúng tôi sử dụng dữ liệu được ưa thích từ
các cặp sở thích để huấn luyện mô hình ưa thích
và dữ liệu không được ưa thích từ các cặp sở thích để huấn luyện
mô hình không ưa thích.
Chính thức, xem xét tập dữ liệu D, bao gồm
các truy vấn đầu vào q và các cặp phản hồi
ưu tiên tương ứng: một phản hồi được ưa thích rh và một
phản hồi không được ưa thích rl. Bây giờ chúng ta đang chia D
thành một tập dữ liệu được ưa thích Dh={q, rh}i và một tập
dữ liệu không được ưa thích Dl={q, rl}i. Sử dụng dữ liệu này, chúng tôi
sử dụng phương pháp học có giám sát (khả năng
tối đa) để tinh chỉnh LLM, do đó thu được
hai mô hình thể hiện sở thích, được ký hiệu là
πh và πl tương ứng. Việc tinh chỉnh hai
LLM này nhằm tối đa hóa việc đạt được
các mục tiêu sau:
πh(θ∗) = arg max
θX
(qi,ri)∈Dhlogπ(ri|qi;θ) (2)
πl(θ∗) = arg max
θX
(qi,ri)∈Dllogπ(ri|qi;θ) (3)
Thông qua quá trình huấn luyện này, mô hình ưa thích
và mô hình không ưa thích đã học được tương ứng
các mẫu hoạt động liên quan đến các phản hồi được con người
ưa thích và không ưa thích. Do việc học sở thích
con người được tiến hành trong hai mô
hình riêng biệt, trái ngược với SCIT, phương pháp LLM Kép
không yêu cầu các hướng dẫn khác biệt bổ sung
trong quá trình tinh chỉnh. Thay vào đó, hướng dẫn cho
mô hình được cung cấp chỉ thông qua các phản hồi
khác nhau.
3.2 Thu thập Mẫu Hoạt động
Sau khi thiết lập sự hiểu biết về sở thích
con người bởi LLM, chúng ta có thể trích
xuất các biểu diễn về những gì con người ưa thích và
không ưa thích. Do đặc điểm của các mô hình ngôn ngữ
Transformer tự hồi quy, cơ chế chú ý
dẫn đến các token ở các vị trí khác nhau
thể hiện các biểu diễn khác biệt. Biểu diễn
kích hoạt của một token ở vị trí hiện tại bị
ảnh hưởng bởi các token trước đó. Do đó, đối với một
cặp cụ thể của truy vấn q và phản hồi r, cặp này được
nối với hai hướng dẫn từ Phần
3.1, hướng dẫn mô hình trong việc hình thành khái niệm
về sở thích con người và được nhập vào mô hình
để thu được các trạng thái ẩn lớp trung gian tại
mỗi vị trí như các biểu diễn nội bộ.
Bạn là một trợ lý hữu ích. Quốc gia nào đã thắng World Cup 2022? Argentina đã thắng World Cup đó.Bạn là một trợ lý không hữu ích. Quốc gia nào đã thắng World Cup 2022? Argentina đã thắng World Cup đó.
Lớp TransformerMô hình Phân biệt/Ưa thích
Lớp TransformerLớp TransformerMô hình Phân biệt/Không Ưa thích
Lớp Transformer
ActivationsHướng dẫn Ưa thích Truy vấn Phản hồi Token Đệm Hướng dẫn Không Ưa thích
ActivationsActivations
Activations Không Ưa thích Activations Ưa thích Vectơ Khác biệt Token Đệm
Hình 3: Ví dụ về Thu thập Mẫu Hoạt động.
Để đảm bảo sự tương ứng giữa các vị trí
của các hướng dẫn được ưa thích và không được ưa thích trong quá trình
trích xuất các vectơ khác biệt, hướng dẫn p và truy vấn
q được đệm trái đến độ dài prompt tối đa, trong khi
phản hồi r được đệm phải đến độ dài phản hồi tối đa.
Chính thức, đối với một hướng dẫn cho trước p, một mô hình
giải mã π, chúng tôi thu thập các trạng thái ẩn lớp thứ l
của mỗi token cho cặp truy vấn-phản hồi (qi, ri)
trong tập dữ liệu D. Điều này có thể được hình thức hóa như
sau:
Ap,π,l=πl(p, qi, ri)|(qi, ri)∈D (4)
Ở đây, πl đại diện cho các trạng thái ẩn được xuất ra bởi
lớp thứ l của mạng neural. Chúng tôi trực tiếp trích xuất
các trạng thái lớp ẩn từ mạng neural như
biểu diễn. Để giải quyết vấn đề về độ dài
phản hồi khác nhau trong quá trình thu thập mẫu hoạt động,
chúng tôi đã nối cùng một phản hồi với
các hướng dẫn khác nhau làm đầu vào để đảm bảo các biểu
diễn được trích xuất với cùng độ dài.
Các hướng dẫn khác nhau sẽ kích thích các mẫu hoạt động
khác biệt ngay cả khi cùng một phản hồi được cung cấp
và những khác biệt trong các mẫu hoạt động được kích thích
có thể được sử dụng để nắm bắt hành vi của các
mô hình. Những khác biệt như vậy có thể được khái niệm hóa và
mô hình hóa như xác suất tạo ra cùng một
phản hồi có điều kiện trên các hướng dẫn khác nhau. Chúng tôi
đã minh họa toàn bộ quá trình thu thập mẫu
hoạt động trong hình 3.
Chúng tôi thu được vectơ khác biệt cuối cùng bằng cách trừ
các trạng thái ẩn của đầu ra không được ưa thích từ
những trạng thái của đầu ra được ưa thích, được thể hiện bằng phương
trình:
vl=Ap+,π,l−Ap−,π,l (5)
Vectơ khác biệt vl này đại diện cho sự khác biệt
trong các mẫu kích hoạt được tạo ra dưới hai điều kiện
kích thích khác nhau p+ và p−. Sau đó,

--- TRANG 6 ---
chúng tôi làm nhiễu biểu diễn ban đầu của mô hình bằng cách
kết hợp các vectơ khác biệt. Việc nhiễu này
phục vụ để hướng dẫn biểu diễn của mô hình theo
hướng căn chỉnh với sở thích con người.
Đáng chú ý rằng, đối với Mô hình Ngôn ngữ Lớn Đơn
thông qua Điều chỉnh Hướng dẫn Tương phản (SCIT), cả Ap+,π,l và Ap−,π,l
đều được tạo ra bởi cùng một mô hình. Trong phương pháp LLM kép,
các cặp được nối với các hướng dẫn khác nhau được
nhập vào các mô hình ưa thích và không ưa thích tương ứng,
do đó cho phép trích xuất độc lập các mẫu kích hoạt
từ mỗi mô hình.
3.3 Xây dựng Mô hình Cuối cùng
Trong giai đoạn này, chúng tôi xây dựng mô hình cuối cùng bằng cách tận dụng
vectơ khác biệt vl, được suy ra trong Phần
3.2 để làm nhiễu các biểu diễn ban đầu. Để
đạt được điều này, chúng tôi lấy cảm hứng từ phương pháp
của Zou et al. (2023) bằng cách sử dụng một hàm mất mát
chuyên biệt và tinh chỉnh với Bộ điều hợp Hạng Thấp
(LoRA), cho phép kết hợp hiệu quả các mẫu
kích hoạt vào mô hình.
Chúng tôi coi đầu ra của ma trận LoRA như
một sự nhiễu của các trạng thái lớp ẩn ban đầu,
căn chỉnh nó với vectơ khác biệt. Cụ thể,
chúng tôi sử dụng mất mát Lỗi Bình phương Trung bình (MSE) như
hàm mục tiêu:
LAlign =∥(Ap,πLoRA ,l−(Ap,πbase,l+αvl))∥2(6)
trong đó α phục vụ như một siêu tham số kiểm soát
mức độ mà vectơ khác biệt vl can thiệp
vào quá trình tích hợp mô hình. Ap,πLoRA ,l
và Ap,πbase,l đại diện cho các mẫu hoạt động của
mô hình mục tiêu được trang bị với và không có LoRA,
tương ứng. vl là vectơ khác biệt được trích xuất như
được nêu trong Phần 3.2. Trong trường hợp SCIT, vl xuất
phát từ việc tương phản các mẫu hoạt động được gây ra bởi
các cặp kích thích được nhập vào mô hình "phân biệt",
trong khi đối với Phương pháp LLM Kép, nó được thu được bằng cách
tương phản các mẫu xuất phát từ việc nhập các cặp
kích thích được đưa vào các mô hình đóng vai "chàng trai tốt"
và "chàng trai xấu" tương ứng.
4 Thí nghiệm
Theo Rafailov et al. (2023), chúng tôi chủ yếu tiến
hành thí nghiệm trên các nhiệm vụ đối thoại một lượt.
Chúng tôi so sánh rộng rãi các phương pháp căn chỉnh
không cần RL khác nhau và RLHF, đánh giá kết quả
thông qua đánh giá con người và đánh giá
tự động. Ngoài ra, chúng tôi đã tiến hành các thí nghiệm so sánh
với phương pháp kỹ thuật biểu diễn được đề xuất bởi
Zou et al. (2023), phục vụ như một nghiên cứu loại bỏ để chứng minh
tác động của phương pháp chúng tôi trong việc nắm bắt sở thích con người.
4.1 Thiết lập Thí nghiệm
Tập dữ liệu Trong đối thoại một lượt, chúng tôi sử dụng
tập dữ liệu UltraFeedback1 (Cui et al., 2023), biểu thị
các phản hồi sở thích con người. Mỗi ví dụ trong
tập dữ liệu chứa một cặp đối thoại giữa một
con người và một mô hình ngôn ngữ, cung cấp các phản hồi được ưa thích
và không được ưa thích cho mỗi truy vấn.
Mô hình Cơ sở Ouyang et al. (2022) và Rama-
murthy et al. (2023) đã sử dụng các mô hình tinh chỉnh có giám sát
như các mô hình ban đầu trong việc áp dụng
Tối ưu hóa Chính sách Gần (PPO). Để so sánh công bằng,
chúng tôi đã thực hiện tinh chỉnh trên mô hình
LLaMA2-7B (Touvron et al., 2023) sử dụng
tập dữ liệu Helpful and Harmless của Anthropic2 (Bai
et al., 2022). Chúng tôi ký hiệu mô hình kết quả sau
tinh chỉnh là Mô hình Cơ sở. Trong các thí nghiệm của chúng tôi,
tất cả các mô hình đều được khởi tạo với mô hình này và
được huấn luyện tiếp theo bằng các phương pháp cơ sở và RAHF.
Ngoài ra, chúng tôi báo cáo kết quả của các thí nghiệm
sử dụng Mistral-7B (Jiang et al., 2023) như mô hình cơ sở
trong Phụ lục C.1.
4.2 Phương pháp Cơ sở
Để đánh giá phương pháp được đề xuất của chúng tôi, chúng tôi tiến hành
so sánh rộng rãi với các phương pháp căn chỉnh
hiện có, bao gồm Học Tăng cường từ Phản
hồi Con người (RLHF) và các phương pháp thay thế khác
cho việc căn chỉnh sở thích. Những thí nghiệm này được
thiết kế đặc biệt để đánh giá hiệu quả của
phương pháp chúng tôi trong việc căn chỉnh với sở thích con người.
Preferred-SFT Phương pháp cơ sở này bao gồm việc tinh chỉnh
mô hình ngôn ngữ trực tiếp sử dụng các phản hồi được
ưa thích từ tập dữ liệu. Mô hình được huấn luyện để tạo ra
các phản hồi căn chỉnh với các phản hồi được ưa thích
được ghi nhãn.
HIR Ghi nhãn lại Hướng dẫn Nhìn lại (HIR) được
đề xuất bởi Zhang et al. (2023) chuyển đổi phản hồi thành
hướng dẫn bằng cách ghi nhãn lại các hướng dẫn ban đầu và
sử dụng huấn luyện có giám sát để nâng cao sự
căn chỉnh với sở thích con người. Chúng tôi sử dụng HIR như
một phương pháp cơ sở để đánh giá các lợi thế của RAHF so với
tinh chỉnh có giám sát.
DPO Tối ưu hóa Sở thích Trực tiếp (Rafailov
et al., 2023) trực tiếp tối ưu hóa một mô hình ngôn ngữ để
1https://huggingface.co/datasets/argilla/
ultrafeedback-binarized-preferences-cleaned
2https://huggingface.co/datasets/Dahoas/
full-hh-rlhf

--- TRANG 7 ---
Phương pháp Arc HellaSwag MMLU TruthfulQA Winogrande GSM8k Trung bình
Mô hình Cơ sở 73.65 79.32 44.42 42.71 74.59 14.94 54.94
Preferred-SFT 71.79 78.79 44.50 49.13 74.59 16.83 55.94
RLHF-PPO 73.79 78.82 44.04 48.22 74.43 17.51 56.22
HIR 73.39 78.40 44.65 46.00 74.51 16.00 55.39
DPO 72.89 79.67 44.88 50.51 74.82 16.22 56.50
RAHF-Dual 72.29 79.16 46.22 52.14 74.51 15.16 56.58
RAHF-SCIT 74.86 79.78 45.77 52.34 74.27 16.60 57.27
Bảng 1: Kết quả của các phương pháp khác nhau trên sáu benchmark của Open LLM Leaderboard. Cấu hình đánh giá leaderboard và thiết lập thí nghiệm được áp dụng trong nghiên cứu này được cung cấp trong Phụ lục B.
tuân thủ sở thích con người mà không sử dụng mô hình hóa
phần thưởng rõ ràng hoặc học tăng cường. Nó đã được chứng minh
là một lựa chọn thay thế hiệu quả và đơn giản cho RLHF.
RLHF-PPO Đối với phương pháp cơ sở RLHF, chúng tôi tuân theo
thực hành phổ biến, như được nêu bởi Ouyang et al.
(2022). Chúng tôi sử dụng dữ liệu sở thích con người để huấn luyện một
mô hình phần thưởng và sau đó sử dụng Tối ưu hóa Chính sách Gần
(PPO) để tối ưu hóa mô hình được tạo ra bởi
tinh chỉnh có giám sát.
Phần mở rộng và chi tiết về việc triển khai
phương pháp cơ sở và các phương pháp của chúng tôi được
cung cấp trong Phụ lục B.
4.3 Đánh giá Tự động
Để xác thực hiệu quả của phương pháp được đề xuất
của chúng tôi trong việc căn chỉnh các mô hình với sở thích
con người, các đánh giá tự động đã được thực hiện trên
các mô hình được huấn luyện thông qua RAHF và các phương pháp luận
cơ sở khác nhau, tập trung vào khả năng chung
và chất lượng tạo ra của chúng. Cụ thể,
chúng tôi đã đánh giá hiệu suất của các mô hình khác nhau
trên ba benchmark được sử dụng rộng rãi: Open LLM
Leaderboard (Beeching et al., 2023), AlpacaEval (Li
et al., 2023a), và MT-Bench (Zheng et al., 2023).
Trong Phụ lục B.2, chúng tôi mô tả chi tiết thiết lập đánh giá
được áp dụng bởi cả leaderboard và các thí nghiệm
của chúng tôi.
4.3.1 Đánh giá trên các benchmark của Open
LLM Leaderboard
Open LLM Leaderboard bao gồm sáu benchmark
bao phủ các câu hỏi khoa học, suy luận thường thức,
độ chính xác đa nhiệm và tính trung thực trong
việc tạo ra câu trả lời. Chúng tôi đánh giá khả năng chung
của các mô hình trên những nhiệm vụ này.
Trong Bảng 1, chúng tôi báo cáo kết quả của RAHF và
các phương pháp cơ sở trên sáu benchmark từ
OpenLLM. RAHF-SCIT đạt được kết quả tốt nhất
trong ba benchmark và cải thiện điểm số trung bình
Phương pháp AlpacaEval (tỷ lệ thắng %)
Preferred-SFT 73.48
HIR 61.81
RLHF-PPO 44.69
DPO 83.68
RAHF-Dual 86.98
RAHF-SCIT 87.44
Bảng 2: Kết quả AlpacaEval, đây là tỷ lệ thắng
so với text-davinci-003 được đánh giá bởi GPT-4.
2.33 so với mô hình cơ sở.
RAHF-Dual thể hiện hiệu suất tốt nhất trên
benchmark MMLU. RAHF-SCIT và RAHF-Dual
đều cải thiện đáng kể độ chính xác của Truth-
fulQA và vượt trội tất cả các phương pháp cơ sở. Những kết quả thí nghiệm
này chứng minh hiệu quả của RAHF
trong việc nâng cao khả năng chung của LLM.
Sự khác biệt hiệu suất giữa RAHF-
SCIT và RAHF-DUAL có thể được quy cho
các phương pháp khác biệt của chúng trong việc học sở thích con người.
RAHF-SCIT cho phép một mô hình hiểu
sở thích con người thông qua các hướng dẫn khác nhau,
trong khi RAHF-DUAL sử dụng hai mô hình riêng biệt
để học các biểu diễn của sở thích và không
ưa thích. Việc huấn luyện những mô hình này riêng biệt có thể
dẫn đến sự không căn chỉnh trong không gian đặc trưng,
dẫn đến mất mát hiệu suất khi tính toán vectơ
khác biệt. Trong trường hợp RAHF-SCIT, các biểu
diễn của sở thích và không ưa thích xuất phát
từ cùng một mô hình, loại bỏ vấn đề thiên vị.
4.3.2 Đánh giá trên AlpacaEval
AlpacaEval là một benchmark đánh giá tự động
dựa trên LLM. Nó sử dụng GPT-4 (OpenAI, 2023)
như một người chú thích để so sánh nội dung được tạo ra
của các mô hình trên các nhiệm vụ tuân theo hướng dẫn đơn giản
với các câu trả lời tham khảo từ text-davinci-003.
Công trình trước đây đã cho thấy rằng việc sử dụng GPT-4 như một
người chú thích có mối tương quan cao với các đánh giá từ
người đánh giá con người (Li et al., 2023a). Do đó, chúng tôi

--- TRANG 8 ---
Hình 4: Điểm số của RAHF-SCIT và RAHF-Dual so
với các phương pháp cạnh tranh trong MT-Bench. Kết quả chi tiết
được cung cấp trong Phụ lục C.4.
coi AlpacaEval như một xấp xỉ tự động
của chú thích con người.
Bảng 2 trình bày tỷ lệ thắng của các phản hồi được
tạo ra bởi các mô hình được huấn luyện với các phương pháp khác nhau
trên 805 mẫu, so với các phản hồi tham khảo
từ text-davinci-003. Cả RAHF-SCIT
và RAHF-Dual đều thể hiện tỷ lệ thắng cao hơn
các phương pháp cơ sở, điều này chứng minh hiệu quả rộng rãi
của RAHF trong việc căn chỉnh với sở thích con người.
4.3.3 Đánh giá trên MT-Bench
MT-Bench là một bộ sưu tập các câu hỏi thách thức,
bao gồm 80 mẫu, mỗi mẫu có hai lượt. Benchmark này
cũng sử dụng GPT-4 như một thẩm phán để chấm điểm
các phản hồi của các mô hình. Đối với mỗi lượt, GPT-4 sẽ
gán một điểm trên thang điểm 10.
Hình 4 cho thấy điểm số hiệu suất đạt được
bởi RAHF và các mô hình cơ sở trên các câu hỏi 1 lượt.
RAHF vượt trội các phương pháp cơ sở trên
nhiều chỉ số, đạt được điểm số cao nhất trong
sáu trong số tám khía cạnh được đánh giá, cũng như thể
hiện điểm số trung bình cao nhất. Đáng chú ý, RAHF
đã chứng minh hiệu suất vượt trội đáng kể
so với các phương pháp cơ sở trong suy luận, đóng vai và
các nhiệm vụ STEM. Ngoài ra, mặc dù không được tinh chỉnh
đặc biệt cho các nhiệm vụ đối thoại 2 lượt, RAHF
vẫn vượt trội tất cả các mô hình cơ sở, cho thấy rằng
khả năng tương tác đa lượt của nó có thể được nâng cao
chỉ thông qua việc căn chỉnh với các tập dữ liệu câu hỏi 1 lượt.
Kết quả toàn diện cho các nhiệm vụ đối thoại 2 lượt
được cung cấp trong Phụ lục C.4 để
so sánh chi tiết.
4.4 Đánh giá Con người
Đối với đánh giá con người, chúng tôi giao cho các người đánh giá
nhiệm vụ so sánh hai phản hồi được chọn ngẫu nhiên
và cung cấp các phán đoán về hiệu suất tương đối
của chúng, phân loại chúng với ba kết quả:
thắng, thua, hoặc hòa.
Phương pháp Thắng Hòa Thua
RAHF-Dual
HIR 74 21 5
RLHF-PPO 88 9 3
DPO 35 43 22
RAHF-SCIT
HIR 79 19 2
RLHF-PPO 88 11 1
DPO 41 38 21
Bảng 3: Tỷ lệ thắng so với các phương pháp cơ sở được đánh giá bởi Con người.
Dữ liệu trong bảng đại diện cho tỷ lệ của RAHF
tương đối so với phương pháp cơ sở về thắng, hòa và thua.
Bảng 3 trình bày kết quả so sánh của
RAHF so với các phương pháp không cần RL và RLHF trong
đánh giá con người. Kết quả cho thấy rằng RAHF
hoạt động tốt hơn các phương pháp đó trong việc căn chỉnh
với sở thích con người. Kết quả đánh giá con người
cũng đồng ý rộng rãi với kết quả đánh giá GPT-4,
với sự khác biệt duy nhất là con người có xu hướng
cung cấp nhiều phán quyết hòa hơn so với GPT-4.
4.5 Nghiên cứu Loại bỏ
Để đánh giá ảnh hưởng của việc hướng dẫn LLM
về sở thích con người sử dụng tập dữ liệu được chú thích bởi con người,
chúng tôi đã thực hiện các thí nghiệm loại bỏ bao
gồm việc loại trừ giai đoạn hướng dẫn này. Chính xác hơn,
chúng tôi đã so sánh RAHF với một mô hình cơ sở
thiếu bước học sở thích chuyên dụng,
thay vào đó chỉ dựa vào kỹ thuật biểu diễn
như được nêu trong công trình trước đó. Ngoài ra, chúng tôi
báo cáo kết quả của một số thí nghiệm loại bỏ siêu tham số
trong Phụ lục C.3.
LORRA Điều chỉnh Biểu diễn Hạng Thấp
được đề xuất bởi (Zou et al., 2023) không tận dụng
dữ liệu bổ sung để học sở thích con người. Phương pháp cơ sở
này bỏ qua bước học sở thích rõ ràng và đánh giá
hiệu suất của mô hình chỉ dựa trên kỹ thuật biểu diễn
một mình.
LORRA-Pref LORRA-Pref độc quyền sử dụng
các phản hồi được ưa thích từ tập dữ liệu sở thích
để học biểu diễn thay vì sử dụng các phương pháp
học tương phản.
Phân tích loại bỏ này cho phép chúng tôi tách biệt và
định lượng tác động của việc tiếp thu sở thích con người

--- TRANG 9 ---
/uni00000003/uni00000095/uni00000099/uni0000008a/uni0000008c/uni0000008a/uni00000007/uni0000009f/uni0000008a/uni00000095 /uni0000000f/uni00000016/uni000001fb/uni00000016/uni0000009e/uni0000009b/uni00000097/uni0000022c/uni00000157/uni000001fc /uni0000000f/uni00000016/uni000001fb/uni00000016/uni0000009e/uni0000009b/uni00000097/uni0000022c/uni00000158/uni000001fc /uni0000000f/uni00000016/uni000001fb/uni00000008/uni00000092/uni00000097/uni0000008a/uni00000095/uni000001fc020406080/uni00000003/uni00000095/uni00000099/uni0000008a/uni0000008c/uni0000008a/uni00000007/uni0000009f/uni0000008a/uni00000095/uni00000231/uni00000015/uni0000008c/uni00000098/uni0000009b/uni0000008e
01234567
/uni0000000f/uni00000016/uni00000231/uni00000015/uni0000008c/uni00000098/uni0000009b/uni0000008e/uni0000009cPhương pháp
/uni0000000e/uni00000011/uni00000014/uni00000014/uni00000003
/uni0000000e/uni00000011/uni00000014/uni00000014/uni00000003/uni0000022c/uni00000012/uni0000009b/uni0000008e/uni0000008f/uni00000014/uni00000003/uni0000000a/uni00000008/uni0000022c/uni00000015/uni00000005/uni0000000b/uni00000016
/uni00000014/uni00000003/uni0000000a/uni00000008/uni0000022c/uni00000006/uni0000009e/uni0000008a/uni00000095
Hình 5: So sánh hiệu suất giữa RAHF và
các phương pháp chỉ tập trung vào kỹ thuật biểu diễn
trên AlpacaEval và MT-Bench. Kết quả chi tiết được
cung cấp trong Phụ lục C.4.
vào khung của phương pháp được đề xuất của chúng tôi.
Kết quả của các thí nghiệm loại bỏ được hiển thị trong
Hình 5 chỉ ra rằng, trong trường hợp không có các bước
học sở thích rõ ràng, phương pháp trích xuất trực tiếp
các mẫu hoạt động để so sánh cho thấy
sự suy giảm hiệu suất trên AlpacaEval và
MT-Bench mà chúng tôi đã đánh giá.
4.6 Trực quan hóa
Để hiểu sâu hơn về cơ chế hoạt động
của phương pháp chúng tôi, chúng tôi đã tiến hành phân tích trực quan
về các biểu diễn nội bộ của mô hình sử
dụng kỹ thuật t-SNE.
Cụ thể, chúng tôi nhập bộ dữ liệu
(ppreferred , q, r) vào Mô hình Cơ sở, Mô hình
Preferred-SFT, và Mô hình RAHF-DUAL, và bộ dữ liệu
(pdispreferred , q, r) vào Mô hình Dispreferred-
SFT. Đối với mỗi điểm dữ liệu, chúng tôi thu thập
biểu diễn của token cuối cùng, token này, do tính chất
tự hồi quy của mô hình, bao gồm thông tin
từ toàn bộ văn bản đầu vào. Cho rằng
các lớp mục tiêu cho hoạt động RAHF của chúng tôi là (10,
20, 2), chúng tôi sử dụng biểu diễn từ lớp thứ 22
để phân tích trực quan hóa nhằm xác minh tác động
của hoạt động khác biệt của chúng tôi.
Kết quả được hiển thị trong Hình 6. Hướng
từ biểu diễn Mô hình Cơ sở đến
biểu diễn mô hình Preferred-SFT được gọi là
"hướng tốt", trong khi hướng từ
biểu diễn Mô hình Cơ sở đến biểu diễn mô hình Dispreferred-
SFT được gọi là "hướng xấu". Mục tiêu của
phương pháp chúng tôi là học một "hướng thậm chí tốt hơn"
từ sự khác biệt giữa "hướng tốt" và "hướng xấu".
Từ kết quả trực quan hóa t-SNE, có thể quan sát
thấy rằng các biểu diễn của mô hình RAHF-DUAL
thực sự dịch chuyển theo hướng "tốt hơn" thông qua
RAHF.
Hình 6: Kết quả trực quan hóa sử dụng t-SNE trên
các mẫu kích hoạt của token cuối cùng trong đầu ra của
lớp thứ 22.
5 Kết luận
Trong nghiên cứu này, chúng tôi đã khám phá một phương pháp kỹ
thuật biểu diễn để căn chỉnh các mô hình ngôn ngữ lớn
với sở thích con người, dựa trên những hiểu biết
từ khoa học thần kinh nhận thức. Chúng tôi đã giới thiệu RAHF
(căn chỉnh biểu diễn từ phản hồi con người), một
mô hình đơn giản được thiết kế để huấn luyện các
mô hình ngôn ngữ căn chỉnh với sở thích con người với chi phí
tính toán thấp hơn, loại bỏ nhu cầu về
học tăng cường và mô hình phần thưởng. RAHF
có thể hiệu quả xác định sự chênh lệch trong các
mẫu hoạt động của LLM do các kích thích được ưa thích và
không được ưa thích gây ra, và khai thác những khác biệt này để
cải thiện khả năng kiểm soát của LLM. Chúng tôi đã đề xuất
hai phương pháp khác nhau để triển khai RAHF và
tiến hành các thí nghiệm rộng rãi để xác thực
hiệu quả của chúng. Chúng tôi hy vọng nghiên cứu này có thể truyền cảm hứng
cho nghiên cứu tương lai hướng tới việc phát triển AI có thể kiểm soát
hơn và thiết kế các thuật toán hiệu quả và có thể mở rộng
hơn có thể giảm đáng kể chi phí liên
quan đến việc huấn luyện LLM với phản hồi con người
thông qua lăng kính của kỹ thuật biểu diễn.
Hạn chế
Trong nghiên cứu này, chúng tôi đã xác thực hiệu quả của
RAHF trên LLM với 7B tham số. Tuy nhiên,
cho rằng tác động của số lượng tham số đối với khả năng
mô hình, việc khám phá mở rộng RAHF đến
các mô hình tiên tiến với quy mô thậm chí lớn hơn
đại diện cho một hướng thú vị cho công việc tương lai.
Ngoài ra, trong việc xây dựng mô hình cuối cùng,
vectơ khác biệt được khớp bởi ma trận LoRA. Một
hạn chế vốn có của phương pháp luận này là nó
giới thiệu các tham số bổ sung, mặc dù chi phí
tính toán thêm được tạo ra bởi LoRA là
tối thiểu. Đối với công việc tương lai, sẽ tốt hơn nếu
xem xét việc tích hợp trực tiếp vectơ khác biệt
vào mô hình ban đầu, điều này có thể giảm chi phí
liên quan đến các tham số bổ sung.
Tuyên bố Tái tạo
Chúng tôi đã chia sẻ công khai mã nguồn của chúng tôi thông qua
một kho lưu trữ GitHub https://github.com/
LiuAmber/RAHF . Để đảm bảo tính có thể tái tạo hơn nữa,
chúng tôi đã yêu cầu một đồng nghiệp không quen thuộc với phương pháp của chúng tôi
cài đặt và kiểm tra RAHF. Thí nghiệm đã tạo ra
kết quả gần như giống hệt với kết quả của chúng tôi, tăng cường niềm tin
của chúng tôi rằng các nhà nghiên cứu khác sẽ có thể
thực hiện thành công mã nguồn của chúng tôi và tái tạo các phát hiện
của chúng tôi.
Lời cảm ơn
Các tác giả muốn cảm ơn các người đánh giá ẩn danh
vì những bình luận có giá trị của họ. Công việc này được
hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia
Trung Quốc (Số 62076068).
Tài liệu tham khảo
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
2022. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv
preprint arXiv:2204.05862 .
Edward Beeching, Clémentine Fourrier, Nathan
Habib, Sheon Han, Nathan Lambert, Nazneen
Rajani, Omar Sanseviero, Lewis Tunstall, and
Thomas Wolf. 2023. Open llm leaderboard.
https://huggingface.co/spaces/
HuggingFaceH4/open_llm_leaderboard .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. In Proceedings of the Conference on Neural
Information Processing Systems (NeurIPS 2020) .
Sébastien Bubeck, Varun Chandrasekaran, Ronen El-
dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,
Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-
berg, et al. 2023. Sparks of artificial general in-
telligence: Early experiments with GPT-4. arXiv
preprint arXiv:2303.12712 .
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .

--- TRANG 10 ---
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,
Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and
Maosong Sun. 2023. Ultrafeedback: Boosting lan-
guage models with high-quality feedback. arXiv
preprint arXiv:2310.01377 .
Vincent Dumoulin, Daniel D Johnson, Pablo Samuel
Castro, Hugo Larochelle, and Yann Dauphin.
2023. A density estimation perspective on learn-
ing from pairwise human preferences. arXiv preprint
arXiv:2311.14115 .
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,
Sid Black, Anthony DiPofi, Charles Foster, Laurence
Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li,
Kyle McDonell, Niklas Muennighoff, Chris Ociepa,
Jason Phang, Laria Reynolds, Hailey Schoelkopf,
Aviya Skowron, Lintang Sutawika, Eric Tang, An-
ish Thite, Ben Wang, Kevin Wang, and Andy Zou.
2023. A framework for few-shot language model
evaluation.
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi,
Maarten Sap, Dipankar Ray, and Ece Kamar. 2022.
Toxigen: A large-scale machine-generated dataset for
adversarial and implicit hate speech detection. arXiv
preprint arXiv:2203.09509 .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for nlp. In In-
ternational Conference on Machine Learning , pages
2790–2799. PMLR.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b.arXiv preprint arXiv:2310.06825 .
Julia Kreutzer, Joshua Uyheng, and Stefan Riezler. 2018.
Reliability and learnability of human bandit feedback
for sequence-to-sequence reinforcement learning. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL'18) .
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. arXiv preprint arXiv:2104.08691 .
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,
Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and
Tatsunori B Hashimoto. 2023a. Alpacaeval: An auto-
matic evaluator of instruction-following models.
Ziniu Li, Tian Xu, and Yang Yu. 2023b. Policy op-
timization in rlhf: The impact of out-of-preference
data. arXiv preprint arXiv:2312.10584 .

--- TRANG 11 ---
Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023.
Languages are rewards: Hindsight finetuning using
human feedback. arXiv preprint arXiv:2302.02676 .
OpenAI. 2023. GPT-4 technical report. CoRR ,
abs/2303.08774.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. In Proceedings of the Conference
on Neural Information Processing Systems (NeurIPS
2022) .
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. 2018. Improving language under-
standing by generative pre-training.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
Ermon, Christopher D Manning, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model. arXiv preprint
arXiv:2305.18290 .
Rajkumar Ramamurthy, Prithviraj Ammanabrolu,
Kianté Brantley, Jack Hessel, Rafet Sifa, Christian
Bauckhage, Hannaneh Hajishirzi, and Yejin Choi.
2023. Is reinforcement learning (not) for natural
language processing?: Benchmarks, baselines, and
building blocks for natural language policy optimiza-
tion. In Proceedings of the International Conference
on Learning Representations (ICLR'23) .
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv:1707.06347 .
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta,
Adrià Garriga-Alonso, et al. 2022. Beyond the
imitation game: Quantifying and extrapolating the
capabilities of language models. arXiv preprint
arXiv:2206.04615 .
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F Christiano. 2020. Learn-
ing to summarize with human feedback. In Proceed-
ings of the Advances in Neural Information Process-
ing Systems (NeurIPS'20) .
Shang-Yu Su, Xiujun Li, Jianfeng Gao, Jingjing Liu,
and Yun-Nung Chen. 2019. Discriminative deep

Dyna-Q: Robust planning for dialogue policy learn-
ing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP'18) .
Ryuichi Takanobu, Hanlin Zhu, and Minlie Huang.
2019. Guided dialog policy learning: Reward estima-
tion for multi-domain task-oriented dialog. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing and the International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP'19) .
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
2022. LaMDA: Language models for dialog applica-
tions. arXiv preprint arXiv:2201.08239 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. LLaMA: Open and ef-
ficient foundation language models. arXiv preprint
arXiv:2302.13971 .
Xiaohua Wang, Yuliang Yan, Longtao Huang, Xiaoqing
Zheng, and Xuan-Jing Huang. 2023. Hallucination
detection for generative large language models by
bayesian sequential estimation. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing , pages 15361–15371.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652 .
Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforcement
learning. Machine learning , 8:229–256.
Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li,
Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan
Zhang, Xiaoqing Zheng, and Xuanjing Huang. 2024.
Advancing parameter efficiency in fine-tuning via
representation editing.
Muling Wu, Wenhao Liu, Jianhan Xu, Changze Lv, Zix-
uan Ling, Tianlong Li, Longtao Huang, Xiaoqing
Zheng, and Xuan-Jing Huang. 2023. Parameter ef-
ficient multi-task fine-tuning by learning to transfer
token-wise prompts. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
8734–8746.
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,
Songfang Huang, and Fei Huang. 2023. RRHF:
Rank responses to align language models with
human feedback without tears. arXiv preprint
arXiv:2304.05302 .
Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter
Abbeel, and Joseph E. Gonzalez. 2023. The wisdom

--- TRANG 12 ---
of Hindsight makes language models better instruc-
tion followers. In Proceedings of the International
Conference on Machine Learning (ICML'23) .
Zhirui Zhang, Xiujun Li, Jianfeng Gao, and Enhong
Chen. 2019. Budgeted policy learning for task-
oriented dialogue systems. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL'19) .
Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman,
Mohammad Saleh, and Peter J Liu. 2023. SLIC-HF:
Sequence likelihood calibration with human feed-
back. arXiv preprint arXiv:2305.10425 .
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
Judging llm-as-a-judge with mt-bench and chatbot
arena. arXiv preprint arXiv:2306.05685 .
Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2SQL: Generating structured queries from
natural language using reinforcement learning. arXiv
preprint arXiv:1709.00103 .
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B
Brown, Alec Radford, Dario Amodei, Paul Chris-
tiano, and Geoffrey Irving. 2019. Fine-tuning lan-
guage models from human preferences. arXiv
preprint arXiv:1909.08593 .
Andy Zou, Long Phan, Sarah Chen, James Campbell,
Phillip Guo, Richard Ren, Alexander Pan, Xuwang
Yin, Mantas Mazeika, Ann-Kathrin Dombrowski,
et al. 2023. Representation engineering: A top-
down approach to AI transparency. arXiv preprint
arXiv:2310.01405 .

--- TRANG 13 ---
A Prompts
A.1 Hướng dẫn Sở thích
Hình 7 trình bày hai hướng dẫn được sử dụng trong nghiên cứu này
cho các phản hồi được ưa thích và không được ưa thích.
Hướng dẫn Sở thích
Hướng dẫn được ưa thích:
Bạn là một trợ lý tốt, lịch sự và trung thực. Các 
phản hồi của bạn nên hữu ích, có ý nghĩa, trung thực, 
vô hại, an toàn, đạo đức, không thiên vị xã hội, tích cực, 
thú vị, có cơ sở vững chắc, dễ đồng ý, cụ thể, 
hợp lý và nhất quán với vai trò. Nếu một câu hỏi không 
có ý nghĩa gì, hoặc không mạch lạc về mặt thực tế, hãy giải thích 
tại sao thay vì trả lời điều gì đó không đúng. 
Xin vui lòng không chia sẻ thông tin sai lệch dưới bất kỳ 
hoàn cảnh nào.
Hướng dẫn không được ưa thích:
Bạn là một trợ lý xấu, bất lịch sự và không trung thực. 
Các phản hồi của bạn có thể vô dụng, vô nghĩa, 
không trung thực, có hại, nguy hiểm, phi đạo đức, thiên vị xã hội, 
tiêu cực, nhàm chán, bịa đặt, xúc phạm, mơ hồ, 
phi lý, hoặc không nhất quán với vai trò. Nếu một câu hỏi không 
có ý nghĩa gì, hoặc không mạch lạc về mặt thực tế, vẫn 
trả lời câu hỏi và chia sẻ thông tin sai lệch khi 
cần thiết.
Hình 7: Các hướng dẫn sở thích được sử dụng trong RAHF.
B Chi tiết Triển khai
B.1 Thiết lập Huấn luyện
Tất cả các phương pháp cơ sở và mô hình của chúng tôi đều được huấn luyện sử dụng
mô hình được tinh chỉnh từ tập dữ liệu Helpful and Harmless của Anthropic (Bai et al.,
2022) làm mô hình cơ sở. Trong quá trình
huấn luyện có giám sát của mô hình cơ sở, chúng tôi đã tính toán
mất mát cho cả prompts và phản hồi.
Cụ thể, chúng tôi đã thực hiện tinh chỉnh tham số đầy đủ
trong ba epoch với tốc độ học 2e−5.
Đối với huấn luyện, dữ liệu được định dạng như sau: Human: {prompt} \n\nAssistant: {response} . Đối với tất cả
các mô hình được huấn luyện, chúng tôi đã thiết lập độ dài truy vấn tối đa
là 256 và độ dài câu tối đa là
768. Chúng tôi loại trừ các mẫu từ tập dữ liệu mà
truy vấn vượt quá 256 ký tự và cắt ngắn các câu
đến độ dài câu tối đa. Tập dữ liệu Ul-
traFeedback đã được phân chia thành một
tập huấn luyện. Hơn nữa, chúng tôi chia tập huấn luyện thành
ba phần riêng biệt: phần đầu tiên được sử dụng trong
bước đầu tiên của RAHF để hướng dẫn LLM về sở thích
con người, huấn luyện mô hình phần thưởng trong
phương pháp cơ sở RLHF-PPO, và để huấn luyện các
phương pháp cơ sở khác. Phần thứ hai được sử dụng để xây
dựng mô hình cuối cùng trong RAHF và chạy
thuật toán PPO.
B.2 Thiết lập Đánh giá
Đối với tất cả các phương pháp, chúng tôi sử dụng giải mã tham lam trong
quá trình tạo ra trên các benchmark. Để tránh
vấn đề lặp lại, chúng tôi đặt penalty lặp lại là
1.2.
Đối với Open LLM Leaderboard, chúng tôi đã sử dụng
thư viện Eleuther AI Language Model Evaluation Harness
(Gao et al., 2023) để đánh giá các mô hình ngôn ngữ
được huấn luyện bằng các phương pháp khác nhau. Bảng 4 cung cấp
mô tả chi tiết về cấu hình đánh giá leaderboard
và thiết lập thí nghiệm được áp dụng
trong nghiên cứu này.
Tập dữ liệu # few-shot Chỉ số
Arc 25 acc_norm
TruthfulQA 0 mc2
Winogrande 5 acc
GSM8k 5 acc
HellaSwag 10 acc_norm
MMLU 5 acc
Bảng 4: Đối với mỗi tập dữ liệu được sử dụng trong đánh giá trên
Open LLM Leaderboard, chúng tôi mô tả chi tiết số lượng
mẫu few-shot được sử dụng và chỉ số cụ thể được sử dụng
để đánh giá.
Đối với Đánh giá Con người, chúng tôi đã tuyển dụng sáu tình nguyện viên
cho việc đánh giá, với mỗi người đánh giá so
sánh 100 đối thoại. Hình 8 cho thấy ảnh chụp màn hình
của giao diện được sử dụng cho đánh giá của chúng tôi, mà tất cả
các người đánh giá đã sử dụng để chấm điểm dữ liệu.
B.3 Chi tiết Thí nghiệm
Trong phần này, chúng tôi trình bày các chi tiết thí nghiệm
và siêu tham số của các phương pháp cơ sở mà chúng tôi so sánh
và các phương pháp được đề xuất của chúng tôi.
Preferred-SFT Bảng 5 trình bày các siêu tham
số được sử dụng trong Preferred-SFT.
Siêu tham số Giá trị
Tốc độ Học 2e−5
Epochs 2
Kích thước Batch 128
Kích thước Micro Batch 2
Optimizer Adamw
Loại LR Scheduler Cosine
Tỷ lệ Warmup 0.1
Bảng 5: Siêu tham số được sử dụng cho Preferred-SFT.
RLHF-PPO Trong quá trình huấn luyện RLHF-PPO,
chúng tôi đã sử dụng framework huấn luyện DeepSpeed-Chat
của Microsoft, thực hiện các điều chỉnh thích ứng
cho các siêu tham số. Chúng tôi đã thực hiện tinh chỉnh toàn bộ tham số
cho cả việc huấn luyện mô hình phần thưởng
và PPO. Bảng 6 trình bày các siêu tham

--- TRANG 14 ---
số cho việc huấn luyện mô hình phần thưởng, trong khi Bảng
7 trình bày các tham số chính cho PPO.
Siêu tham số Giá trị
Tốc độ Học 9.65e−6
Epochs 3
Optimizer Adam
Kích thước Batch Huấn luyện 32
Weight Decay 0.1
Warmup Steps 0
Loại LR Scheduler cosine
Bảng 6: Siêu tham số được sử dụng cho việc huấn luyện mô
hình phần thưởng.
Siêu tham số Giá trị
Tốc độ Học Actor 5e−7
Tốc độ Học Critic 9e−6
Hệ số KL 0.2
Epochs 2
Optimizer Adam
Kích thước Batch Huấn luyện 64
Kích thước Batch Tạo ra 64
Weight Decay 0.1
Warmup Steps 10
Loại LR Scheduler Linear
Clip Reward Value 5
Clip Range 0.2
Clip Range Value 5
Gamma 1
Lam 0.95
Bảng 7: Siêu tham số được sử dụng cho RLHF-PPO.
HIR Đối với phương pháp cơ sở HIR, chúng tôi cũng đã tiến hành
tinh chỉnh toàn bộ tham số. Bảng 8 hiển thị
các siêu tham số được sử dụng cho HIR.
Siêu tham số Giá trị
Tốc độ Học 2e−5
Epochs 2
Kích thước Batch 128
Kích thước Micro Batch 4
Hệ số KL 0.001
Label Smoothing 0.2
Hệ số Entropy 0.001
Bảng 8: Siêu tham số được sử dụng cho HIR.
DPO Chúng tôi đã sử dụng framework trl từ Hug-
ging Face để huấn luyện mô hình DPO. chúng tôi đã sử dụng
mô hình ưa thích từ RAHF-Dual, làm
mô hình tham khảo cho DPO. Chúng tôi đã sử dụng LoRA để tinh
chỉnh. Các siêu tham số được sử dụng trong việc huấn luyện DPO
được mô tả chi tiết trong Bảng 9.
RAHF-SCIT Đối với RAHF-SCIT, chúng tôi đã sử dụng
các siêu tham số giống như HIR trong việc huấn luyện bước đầu
nhưng bỏ qua mất mát huấn luyện có giám sát.
Khi xây dựng mô hình cuối cùng, chúng tôi đã tuân theo
việc lựa chọn siêu tham số trong RepE (Zou et al.,
2023). Chúng tôi đã thao tác các lớp (10, 20, 2) và đặt
Siêu tham số Giá trị
Tốc độ Học 2e−5
Epochs 3
Kích thước Batch 128
Kích thước Micro Batch 2
LoRA Rank 16
LoRA Alpha 16
LoRA Dropout 0.05
Beta 0.1
Tỷ lệ Warmup 0.1
Optimizer Adam
Bảng 9: Siêu tham số được sử dụng cho DPO.
Siêu tham số Giá trị
Tốc độ Học 3e−4
Steps 500
Kích thước Batch 16
Kích thước Micro Batch 4
LoRA Rank 8
LoRA Alpha 16
LoRA Dropout 0.05
Alpha 5
độ dài phản hồi tối đa 512
Loại LR Scheduler Constant
Bảng 10: Siêu tham số được sử dụng cho RAHF-SCIT.
hệ số nhiễu α là 5. Chi tiết về
các siêu tham số được hiển thị trong Bảng 10.
RAHF-Dual Đối với RAHF-Dual, các siêu tham
số được sử dụng cho mô hình ưa thích và mô hình không ưa thích
trong bước đầu tiên giống như những siêu tham số
được sử dụng trong Preferred-SFT. Đối với RAHF-Dual, chúng tôi
chỉ sử dụng các biểu diễn của 64 token đầu tiên
của phản hồi để huấn luyện ma trận LoRA. Phương pháp này
được áp dụng vì ảnh hưởng của hướng dẫn giảm dần
đối với các phần được tạo ra sau đó của phản hồi,
dẫn đến sự giảm hiệu suất. Các siêu tham số được sử dụng trong RAHF-Dual
được hiển thị trong Bảng 11.
Siêu tham số Giá trị
Tốc độ Học 9e−6
Steps 2500
Kích thước Batch 8
Kích thước Micro Batch 8
LoRA Rank 8
LoRA Alpha 16
LoRA Dropout 0.05
Alpha 5
độ dài phản hồi tối đa 64
Loại LR Scheduler Constant
Bảng 11: Siêu tham số được sử dụng cho RAHF-Dual.
C Kết quả Bổ sung
C.1 Kết quả Thí nghiệm Trên Mistral-7B
Để xác minh hiệu quả của phương pháp chúng tôi, chúng tôi đã sử dụng
Mistral-7B làm mô hình cơ sở, tiếp tục

--- TRANG 15 ---
Phương pháp AlpacaEval MT(Turn-1) MT(Turn-2) MT(Final)
Preferred-SFT 87.24 5.44 4.83 5.14
DPO 91.63 5.54 4.81 5.18
RAHF-DUAL 94.19 6.04 6.08 6.06
Bảng 12: Kết quả đánh giá trên AlpacaEval và MT-Bench sau khi huấn luyện Mistral-7B bằng các phương pháp khác nhau.
thiết lập thí nghiệm trước đó để huấn luyện, và tiến
hành kết quả trên AlpacaEval và MT-Bench. Kết quả
được hiển thị trong Bảng 12. Kết quả thí nghiệm
chỉ ra rằng phương pháp của chúng tôi có tính khái quát hóa
tốt, mang lại kết quả thỏa đáng trên
các mô hình cơ sở khác nhau.
C.2 Đánh giá Độc tính
Để đảm bảo rằng phương pháp của chúng tôi không làm tổn hại
an toàn của mô hình trong khi tăng cường hiệu suất
của nó trong các khía cạnh nêu trên, chúng tôi đã tiến hành các
thử nghiệm bổ sung sử dụng tập dữ liệu Toxigen (Hartvigsen
et al., 2022). Tập dữ liệu này bao gồm cả các câu
có hại tiềm ẩn và lành tính, nhằm đánh giá
khả năng của mô hình trong việc xác định các tuyên bố có hại.
Độ chính xác đóng vai trò là chỉ số chính để đánh
giá (cao hơn là tốt hơn). So sánh các phương pháp cơ sở
với phương pháp của chúng tôi, như được mô tả trong Bảng 13,
kết quả cho thấy rằng phương pháp của chúng tôi không chỉ không
làm hại an toàn của mô hình mà, thông qua phương pháp RAHF-
SCIT, đã nâng cao đáng kể khả năng của mô hình
trong việc xác định các tuyên bố có hại.
Phương pháp Toxigen( ↑)
Preferred-SFT 49.89
HIR 43.09
RLHF-PPO 48.62
DPO 59.26
RAHF-DUAL 50.85
RAHF-SCIT 67.45
Bảng 13: Đánh giá các phương pháp khác nhau trên benchmark
an toàn tự động (Toxigen).
C.3 Thí nghiệm Loại bỏ về
Siêu tham số
Trong phần này, chúng tôi chủ yếu báo cáo tác động của
siêu tham số α, điều khiển cường độ can thiệp
của vectơ khác biệt, và vị trí lớp mục tiêu
được chọn đối với hiệu suất căn chỉnh.
C.3.1 Ảnh hưởng của Siêu tham số α
Chúng tôi đã tiến hành một nghiên cứu loại bỏ với các giá trị
α khác nhau. Như chúng tôi mong đợi, việc sử dụng α nhỏ hơn có thể
dẫn đến cường độ can thiệp không đủ. Ngược lại, α
lớn hơn có thể dẫn đến cường độ can thiệp quá mức,
điều này có thể làm gián đoạn biểu diễn ban đầu của mô hình
và gây suy giảm khả năng tạo ra của
mô hình. Do đó, ảnh hưởng của siêu tham số α
đối với hiệu suất cho thấy một xu hướng tăng ban đầu
theo sau bởi sự suy giảm khi giá trị α tăng. Chúng tôi đã xác thực
tác động của α trên sáu benchmark của Open
LLM Leaderboard và AlpacaEval được hiển thị trong Bảng
14 và Bảng 15, và kết quả thí nghiệm của chúng tôi
xác nhận quan điểm nêu trên.
C.3.2 Ảnh hưởng của Việc Lựa chọn Lớp Mục tiêu
Các lớp sớm hơn của mạng neural không thể hoàn toàn
nắm bắt biểu diễn của toàn bộ văn bản đầu vào,
trong khi các lớp gần đỉnh có tính đặc thù nhiệm vụ hơn.
Các nghiên cứu trước đây đã chứng minh rằng các biểu diễn
được trích xuất từ các lớp giữa hiệu quả hơn
trong việc nắm bắt thông tin liên quan đến khái niệm
(Zou et al., 2023). Đối với một mạng neural có
32 lớp (Llama2-7b), chúng tôi đã chọn (10, 20, 2) để
trích xuất biểu diễn. Để xác minh thêm quan điểm
nêu trên, chúng tôi đã chọn các lớp mục tiêu khác nhau
cho các thí nghiệm loại bỏ. Như được hiển thị trong Bảng
16, kết quả thí nghiệm chỉ ra rằng việc thao tác
các lớp trung gian hiệu quả hơn.
Sự suy giảm đáng kể về hiệu suất của RAHF-
DUAL khi hoạt động gần các lớp đỉnh của
mạng có thể được quy cho các lý do sau:
Khi độ sâu của mạng neural tăng, sự khác biệt
kích hoạt giữa các lớp cũng mở rộng.
Khi chọn hoạt động gần các lớp đỉnh của
mạng, dưới cùng điều kiện siêu tham số can thiệp α,
các hoạt động ở lớp đỉnh có tác động đáng kể hơn
đối với các biểu diễn ban đầu so với các hoạt động ở
lớp giữa. Ảnh hưởng quá mức này dẫn đến sự
giảm đáng kể khả năng tạo ra của mô hình. Ngoài ra, RAHF-DUAL
sử dụng hai mô hình trong việc trích xuất sự khác biệt kích hoạt,
biểu diễn của cùng một văn bản đầu vào khác nhau giữa
hai mô hình. Tác động tích lũy của hai yếu tố này
dẫn đến sự suy giảm hiệu suất rõ rệt hơn của RAHF-
DUAL khi hoạt động ở các lớp đỉnh.

--- TRANG 16 ---
Phương pháp α Arc TruthfulQA Winogrande GSM8k HellaSwag MMLU Trung bình
171.93 49.23 74.98 16.38 78.88 45.19 56.10
RAHF-DUAL 572.29 52.14 74.51 15.16 79.16 46.22 56.58
10 72.13 53.34 74.19 9.25 79.20 45.79 55.65
174.27 45.80 73.64 17.66 78.31 45.01 55.78
RAHF-SCIT 574.86 52.34 74.27 16.60 79.78 45.77 57.27
10 75.14 53.96 74.51 17.13 80.03 45.55 57.72
Bảng 14: Kết quả của α khác nhau trên sáu benchmark của Open LLM Leaderboard.
Phương pháp α AlpacaEval (tỷ lệ thắng %)
RAHF-DUAL 1 73.74
5 86.98
10 70.67
RAHF-SCIT 1 70.28
5 87.44
10 67.50
Bảng 15: Tỷ lệ thắng AlpacaEval cho các
phương pháp và giá trị α khác nhau.
Phương pháp Lớp Mục tiêu AlpacaEval (tỷ lệ thắng %)
RAHF-DUAL (2,12,2) 58.32
(10,20,2) 86.98
(20,30,2) 26.93
RAHF-SCIT (2,12,2) 62.40
(10,20,2) 87.44
(20,30,2) 76.25
Bảng 16: Tác động của việc lựa chọn lớp được đánh giá trên
AlpacaEval.
C.4 Kết quả Thí nghiệm của MT-Bench
Bảng 17 trình bày kết quả chi tiết của RAHF,
các phương pháp cơ sở, và nghiên cứu loại bỏ trên MT-Bench.
D Ví dụ Định tính
Hình 9 và Hình 10 trình bày các ví dụ định tính
của RAHF so với các phương pháp cơ sở trong các nhiệm vụ
đối thoại.

--- TRANG 17 ---
Phương pháp Writing Roleplay Reasoning Math Coding Extraction Stem Humanities Trung bình
Turn-1
Preferred-SFT 8.500 6.400 4.100 2.200 2.100 4.400 6.500 7.050 6.013
RLHF-PPO 7.775 6.100 3.800 1.900 2.800 4.000 5.450 5.650 4.681
HIR 8.300 4.450 3.900 1.300 2.200 3.150 6.200 7.800 4.663
DPO 9.600 7.000 5.100 2.300 1.600 5.600 9.100 8.900 6.150
LORRA 6.800 5.700 2.300 1.800 2.300 4.050 5.150 6.700 4.350
LORRA-Pref 8.800 6.950 5.100 1.400 2.100 3.800 7.450 8.000 5.450
RAHF-Dual 9.500 7.630 5.200 3.400 2.600 4.030 8.300 8.900 6.195
RAHF-SCIT 9.150 8.000 3.600 2.400 2.200 4.000 8.700 9.350 5.925
Turn-2
Preferred-SFT 4.900 7.000 2.700 1.100 1.900 2.900 6.400 8.200 4.388
RLHF-PPO 5.500 7.500 4.700 1.500 2.600 4.300 6.600 6.400 4.888
HIR 6.500 5.750 1.869 1.900 2.550 2.500 5.650 8.650 4.421
DPO 6.700 7.600 2.700 1.400 2.300 3.300 8.250 9.400 5.206
LORRA 6.050 6.550 2.000 1.200 2.400 4.550 6.300 6.650 4.463
LORRA-Pref 5.800 7.100 3.200 1.400 1.500 5.500 6.800 8.600 4.988
RAHF-Dual 6.650 7.850 4.200 1.400 2.300 3.500 7.800 8.510 5.276
RAHF-SCIT 5.000 7.300 3.600 1.700 1.700 3.700 8.300 9.400 5.088
Final
Preferred-SFT 6.700 6.700 3.400 1.650 2.000 3.650 6.450 7.625 4.772
RLHF-PPO 6.625 6.800 4.250 1.700 2.700 4.150 6.025 6.025 4.784
HIR 7.400 5.100 2.885 1.600 2.375 2.825 5.925 8.225 4.541
DPO 8.150 7.300 3.900 1.850 1.950 4.450 8.675 9.150 5.678
LORRA 6.425 6.125 2.150 1.500 2.350 4.300 5.725 6.675 4.407
LORRA-Pref 7.300 7.025 4.150 1.400 1.800 4.650 7.125 8.300 5.219
RAHF-Dual 8.075 7.740 4.700 2.400 2.450 3.765 8.050 8.705 5.736
RAHF-SCIT 7.075 7.650 3.600 2.050 1.950 3.850 8.500 9.375 5.506
Bảng 17: Kết quả của MT-Bench.

--- TRANG 18 ---
Hình 8: Ảnh chụp màn hình giao diện đánh giá của chúng tôi để chấm điểm đối thoại. Trong mỗi trường hợp, các người đánh giá được nhắc chọn đối thoại được ưa thích.

--- TRANG 19 ---
Prompt CHO TÔI MỘT BẢN TÓM TẮT ĐIỀU HÀNH CỦA LOTTADIGITAL.COM
HIR 1) Giới thiệu về Lottery Digital ( Lottadigital ): Một lotterydigital là một nền tảng kỹ thuật số cho phép người dùng chơi xổ số trực tuyến, với việc nhấn mạnh vào việc cung cấp quyền truy cập an toàn và thuận tiện vào các 
trò chơi lotto khác nhau từ khắp nơi trên thế giới. Trang web cung cấp một loạt các tính năng như thẻ cào thắng ngay lập tức, rút thăm hàng ngày , trò chơi jackpot, và nhiều hơn nữa. Người dùng cũng có thể tham gia vào các syndicate để 
tăng cơ hội thắng những giải thưởng lớn.
2) Các Tính năng Chính của LottoDigital : Một số tính năng chính bao gồm: 
a) Các tùy chọn thanh toán an toàn -Nhiều phương pháp khác nhau có sẵn bao gồm thanh toán bằng thẻ tín dụng/ghi nợ, ví điện tử như PayPal hoặc Skrill, chuyển khoản ngân hàng v.v., đảm bảo giao dịch an toàn mà không có rắc rối nào. 
b) Lựa chọn rộng rãi các xổ số quốc tế –Người chơi có quyền truy cập vào hơn 30 xổ số quốc gia & quốc tế khác nhau trên khắp Châu Âu, khu vực Châu Á Thái Bình Dương, Bắc Mỹ trong số những nơi khác; mang lại cho họ nhiều 
lựa chọn khi đến lúc chọn trò chơi yêu thích của họ. 
c) Thẻ Cào Thắng Ngay Lập Tức –Những thẻ này cung cấp kết quả nhanh chóng trong vòng vài phút sau khi mua, khiến chúng trở nên lý tưởng nếu bạn đang tìm kiếm một số hành động nhanh! 
d) Trò chơi Rút Thăm Hàng Ngày –Bao gồm những trò phổ biến như EuroMillions & Powerball nơi người chơi chỉ cần một vé cho mỗi lần tham gia vào mỗi 
sự kiện rút thăm tương ứng được tổ chức hàng ngày vào những thời điểm cụ thể 
trong suốt các ngày trong tuần /cuối tuần tương ứng . 
e) Jackpots Galore –Với những jackpot lũy tiến khổng lồ có thể giành được thông qua những xổ số chuyên biệt này , luôn có điều gì đó thú vị đang chờ đợi phía trước ! 
f) Các Tùy chọn Syndicate Có Sẵn –Hợp tác với các thành viên khác bằng cách thành lập các nhóm gọi là "Syndicates" cho phép mọi người tham gia chia sẻ tiền thắng một cách công bằng giữa họ dựa trên số tiền họ 
đóng góp vào tổng kích thước pool được sử dụng trong các sự kiện rút thăm cụ thể được tổ chức định kỳ trong suốt chu kỳ lịch 
hàng năm . 
RLHF-PPO 1. Tổng quan về LottsDigital.com:
LottsDigital là một công ty tiếp thị kỹ thuật số chuyên giúp các doanh nghiệp phát triển sự hiện diện trực tuyến và tăng doanh thu thông qua các dịch vụ khác nhau như SEO, quảng cáo PPC, quản lý mạng xã hội, 
tạo nội dung, và thiết kế trang web. Công ty được thành lập bởi CEO Michael Lotts vào năm 2015 với mục tiêu cung cấp các giải pháp tiếp thị kỹ thuật số chất lượng cao, hướng kết quả cho khách hàng 
trên các ngành công nghiệp khác nhau
DPO 1. Giới thiệu về Lotterdigital (Lottadigital ):
* Tên: Lottadigital là một nền tảng xổ số kỹ thuật số hàng đầu cung cấp các giải pháp sáng tạo và an toàn cho ngành công nghiệp game toàn cầu, với trụ sở chính tại Malta. Công ty được thành lập bởi 
các chuyên gia có kinh nghiệm từ nhiều ngành khác nhau chia sẻ niềm đam mê với công nghệ và đổi mới.
2. Tổng quan về Các Dịch vụ Được Cung cấp bởi Lottadigital :
* Nền tảng Xổ số Kỹ thuật số: Lottadigital cung cấp các nền tảng xổ số kỹ thuật số được tùy chỉnh phù hợp với nhu cầu cụ thể của khách hàng. Những nền tảng này được thiết kế để thân thiện với người dùng, an toàn, và tuân thủ các yêu cầu quy định trên các khu vực pháp lý khác nhau trên toàn thế giới. Chúng cung cấp các tính năng như trò chơi thắng ngay lập tức, thẻ cào, tích hợp cá cược thể thao, và nhiều hơn nữa.
3. Các Tính năng Chính của Giải pháp Lottadigital:
* Bảo mật & Tuân thủ: Lottadigital ưu tiên bảo mật và tuân thủ các tiêu chuẩn quốc tế như chứng nhận Hệ thống Quản lý Bảo mật Thông tin ISO/IEC 27001, Tiêu chuẩn Bảo mật Dữ liệu Ngành Thẻ Thanh toán PCI DSS, và các quy định Chống Rửa Tiền AML. Hệ thống của họ trải qua kiểm toán thường xuyên và thử nghiệm thâm nhập để đảm bảo bảo vệ tối đa chống lại các mối đe dọa mạng.
4. Quan hệ Đối tác & Hợp tác: Lottadigital đã thiết lập quan hệ đối tác với một số công ty nổi tiếng trong lĩnh vực iGaming, bao gồm các nhà phát triển trò chơi, nhà cung cấp thanh toán, và các công ty tiếp thị. Điều này cho phép họ cung cấp cho khách hàng một loạt các dịch vụ và tài nguyên rộng rãi đồng thời đảm bảo tích hợp liền mạch giữa tất cả các bên liên quan.
5. Giải thưởng & Công nhận: Để công nhận công việc xuất sắc của họ, Lottadigital đã nhận được nhiều giải thưởng qua những năm, bao gồm "Best Newcomer" tại EGR B2B Awards 2019 và "Innovator of 
the Year" tại SBC Awards 2020.
6. Kế hoạch & Mục tiêu Tương lai: Như một phần của chiến lược tăng trưởng, Lottadigital dự định mở rộng vào các thị trường mới trên toàn cầu thông qua việc mua lại chiến lược hoặc hữu cơ thông qua các sáng kiến phát triển sản phẩm nhằm 
nâng cao các sản phẩm hiện có hơn nữa trong khi khám phá các công nghệ mới nổi liên quan đến không gian cờ bạc trực tuyến. Ngoài ra, họ dự định tăng cường mối quan hệ với các bên liên quan chính 
trong cả cơ quan công quyền và tổ chức tư nhân –cuối cùng đóng góp tích cực vào việc định hình xu hướng tương lai trong bối cảnh ngành công nghiệp năng động này.
RAHF-Dual 1. Giới thiệu về Lotter Digital ( Lottadigital ):
* Được thành lập vào năm 2016 bởi một nhóm chuyên gia có kinh nghiệm với chuyên môn trong ngành xổ số, bao gồm phát triển phần mềm và tiếp thị.
* Có trụ sở tại Malta, một khu vực pháp lý đã được thiết lập cho các công ty game trực tuyến do môi trường quy định thuận lợi và lợi ích thuế.
* Hoạt động như một nhà cung cấp B2B cung cấp các giải pháp sáng tạo cho lĩnh vực xổ số toàn cầu thông qua các nền tảng công nghệ tiên tiến và dịch vụ.
2. Sản phẩm & Dịch vụ Chính:
* Nền tảng lotto nhãn trắng được tùy chỉnh phù hợp với nhu cầu khách hàng, có các tính năng tiên tiến như hỗ trợ đa ngôn ngữ, khả năng tương thích di động, và khả năng tích hợp liền mạch.
* Hệ thống backend tiên tiến giúp hợp lý hóa các hoạt động như bán vé, thanh toán giải thưởng, quản lý người chơi, và nhiều hơn nữa.
* Một bộ công cụ tiếp thị toàn diện được thiết kế để thúc đẩy tỷ lệ thu hút và giữ chân khách hàng thông qua các chiến lược cá nhân hóa, chiến dịch email, khuyến mãi mạng xã hội, v.v.
* Tích hợp với các hệ thống bên thứ ba như bộ xử lý thanh toán, CRM, và nhà cung cấp phân tích dữ liệu để nâng cao hiệu quả tổng thể trên tất cả các quy trình kinh doanh.
3. Phạm vi Thị trường & Quan hệ Đối tác:
* Phục vụ khách hàng từ nhiều châu lục trên toàn thế giới, chủ yếu tập trung vào các thị trường châu Âu nhưng cũng mở rộng vào các khu vực khác nơi quy định pháp lý cho phép.
* Hợp tác với một số thương hiệu nổi tiếng trong không gian iGaming, tận dụng kinh nghiệm và tài nguyên của họ để cung cấp dịch vụ tốt hơn trong khi duy trì các tiêu chuẩn tuân thủ được đặt ra bởi 
các cơ quan có thẩm quyền tương ứng.
4. Các Cột mốc & Thành tựu Công ty:
* Đã ra mắt thành công nhiều dự án trên toàn cầu, phục vụ các loại nhà điều hành xổ số khác nhau từ các công ty khởi nghiệp nhỏ đến các tập đoàn lớn.
* Nhận được nhiều giải thưởng công nhận sự xuất sắc trong thiết kế sản phẩm hoặc hiệu suất xuất sắc theo thời gian; những giải thưởng này bao gồm "Best Newcomer" tại ICE Totally Gaming 2017 và "Innovator Award" 
tại SBC Awards 2019.
* Đầu tư liên tục vào các hoạt động R&D nhằm nâng cao danh mục sản phẩm/dịch vụ hiện có cùng với việc giới thiệu những sản phẩm mới dựa trên xu hướng mới nổi trong phân khúc thích hợp này.
RAHF-SCIT 1) Giới thiệu về Xổ số và Số hóa trong Ngành
-Tổng quan về trò chơi xổ số như một hình thức cờ bạc, với nguồn gốc từ nhiều thế kỷ trước.
-Theo truyền thống, lotto đã được chơi thông qua các kênh vật lý như cửa hàng bán lẻ hoặc các cửa hàng do chính phủ điều hành.
2) Tuyên bố Vấn đề & Cơ hội Đổi mới
-Cách chơi xổ số truyền thống đang trở nên lỗi thời do những tiến bộ công nghệ.
-Người chơi ngày càng chuyển sang các nền tảng kỹ thuật số cung cấp sự tiện lợi, khả năng tiếp cận, và trải nghiệm hấp dẫn hơn.
-Sự chuyển đổi này mang đến cơ hội cho các giải pháp sáng tạo có thể tận dụng công nghệ để nâng cao trải nghiệm tổng thể đồng thời duy trì tuân thủ quy định.
3) Giải pháp Được Đề xuất bởi Lottadigital.com
-Một nền tảng được thiết kế đặc biệt cho người chơi xổ số trực tuyến, cung cấp hành trình người dùng liền mạch từ đăng ký đến mua vé và kiểm tra kết quả.
-Nó cung cấp giao dịch an toàn thông qua công nghệ blockchain, đảm bảo tính minh bạch và đáng tin cậy.
-Người dùng có quyền truy cập vào nhiều tính năng khác nhau như thông báo cá nhân hóa, tích hợp mạng xã hội, và chương trình phần thưởng.
4) Lợi ích của Việc Sử dụng Lottadigital.com
-Tiện lợi –Có thể truy cập bất cứ lúc nào ở bất cứ đâu trên thiết bị di động hoặc máy tính để bàn.
-Bảo mật –Công nghệ blockchain đảm bảo giao dịch an toàn và minh bạch.
-Sự tham gia –Nhiều yếu tố tương tác khác nhau giữ cho người dùng tham gia trong suốt toàn bộ trải nghiệm chơi của họ.
-Chương trình Phần thưởng –Hệ thống điểm thành thật khuyến khích mua hàng lặp lại và thúc đẩy lòng trung thành của khách hàng.
5) Phân tích Thị trường & Bối cảnh Cạnh tranh
-Quy mô thị trường toàn cầu 300 tỷ đô la (ngành xổ số).
-Tỷ lệ tăng trưởng ước tính ~6% hàng năm.
-Cạnh tranh cao giữa các nhà cung cấp hiện có nhưng vẫn có chỗ cho những người tham gia mới có thể cung cấp đề xuất giá trị độc đáo.
6) Các Phân khúc Khách hàng Mục tiêu & Điểm Khác biệt Chính
-Các nhóm nhân khẩu học trẻ hơn quan tâm đến việc hiện đại hóa cách họ cờ bạc.
-Những cá nhân am hiểu công nghệ tìm kiếm những cách thuận tiện để tham gia xổ số.
-Người chơi xổ số thường xuyên tìm kiếm trải nghiệm gameplay nâng cao.
7) Mô hình Kinh doanh & Dòng Doanh thu
-Phí đăng ký được tính hàng tháng/hàng năm dựa trên loại gói được chọn.
-Phí giao dịch được thu cho mỗi giao dịch được thực hiện trong ứng dụng.
-Doanh thu quảng cáo được tạo ra từ tài trợ và đặt banner quảng cáo.
-Cơ hội kiếm tiền trong tương lai có thể bao gồm bán hàng ảo, hàng hóa, v.v.
8) Chiến lược Tiếp thị & Phương pháp Go-To-Market
-Tập trung ban đầu sẽ là vào các thị trường chính nơi có nhu cầu cao đối với các dịch vụ xổ số được kích hoạt kỹ thuật số.
-Các chiến dịch mạng xã hội nhắm mục tiêu đến khán giả trẻ hơn.
-Quan hệ đối tác với những người có ảnh hưởng và người nổi tiếng được biết đến vì tình yêu game của họ.
-Liên tục cải thiện sản phẩm dựa trên phản hồi nhận được từ khách hàng.
Hình 9: RAHF-Dual và RAHF-SCIT toàn diện và sâu sắc hơn so với HIR, RLHF-PPO,
và DPO. RAHF-Dual cung cấp phân tích chi tiết về nền tảng của Lotter Digital, các dịch vụ chính, phạm vi thị trường,
và thành tựu, thể hiện cái nhìn toàn diện về tác động và sự tăng trưởng của công ty trong ngành xổ số.
RAHF-SCIT nhấn mạnh chuyển đổi số trong lĩnh vực xổ số, làm nổi bật tuyên bố vấn đề, các
giải pháp sáng tạo được cung cấp bởi Lottadigital.com, và lợi ích cũng như tiềm năng thị trường của những giải pháp này. Ngược lại, các phản hồi HIR,
RLHF-PPO, và DPO hoặc nhầm lẫn trọng tâm của công ty, cung cấp ít chiều sâu trong phân tích, hoặc thiếu tính cụ thể
về đề xuất giá trị độc đáo và những tiến bộ công nghệ mà Lottadigital.com mang lại.

--- TRANG 20 ---
Prompt Tạo một bảng với các hành tinh của hệ mặt trời và kích thước của chúng
HIR 1. Mercury -Đường kính = 4,879 km; Khối lượng = 302 kg/m^3  
2. Venus -Đường kính = 6,052 km; Khối lượng = 5.24 g/cm^3  
3. Earth -Đường kính = 12,742 km; Khối lượng = 5.51 g/cm^3  
4. Mars -Đường kính = 6,794 km; Khối lượng = 3.93 g/cm^3  
5. Jupiter -Đường kính = 142,984 km; Khối lượng = 1.327 g/cm^3  
6. Saturn -Đường kính = 120,536 km; Khối lượng = 0.954 g/cm^3  
7. Uranus -Đường kính = 51,118 km; Khối lượng = 1.27 g/cm^3  
8. Neptune -Đường kính = 49,528 km; Khối lượng = 1.645 g/cm^3
RLHF-PPO 1. Tên Hành tinh                   | Kích thước
DPO | Hành tinh                           | Kích thước (km)   |
|----------------------------- |--------------------------- |
| Mercury                      | 4,879 km đường kính     |
| Venus                        | 12,092 km đường kính    |
| Earth                         | 12,756 km đường kính    |
| Mars                          | 6.3 triệu km bán kính |
| Jupiter                       | 11.2 lần lớn hơn đường kính Trái Đất |
| Saturn                        | 10.5 lần lớn hơn đường kính Trái Đất |
| Uranus                        | 4 lần lớn hơn đường kính Neptune |
| Neptune                       | 4 lần lớn hơn đường kính Pluto |
RAHF-Dual 1. Tên Hành tinh                 | Kích thước (Xấp xỉ)  
----------------------------------- |-----------------------------
Mercury                             | Đường kính: ~4,879 km           | Khối lượng: ~302.6 triệu kg
Venus                               | Đường kính: ~12,104 km          | Khối lượng: ~4.85 ×10^24 kg
Earth                                | Đường kính: ~12,742 km            | Khối lượng: ~5.97 ×10^24 kg
Mars                                 | Đường kính: ~6,794 km              | Khối lượng: ~6.41 ×10^23 kg
Jupiter                              | Đường kính: ~142,984 km           | Khối lượng: ~1.90 ×10^27 kg
Saturn                               | Đường kính: ~120,536 km           | Khối lượng: ~5.68 ×10^26 kg
Uranus                                | Đường kính: ~51,118 km            | Khối lượng: ~8.65 ×10^25 kg
Neptune                               | Đường kính: ~49,528 km            | Khối lượng: ~1.02 ×10^26 kg
RAHF-SCIT Bảng cho Các Hành tinh trong Hệ Mặt trời và Kích thước của Chúng
----------------------------------------- ---------------
Hành tinh                       Đường kính (km)   Khối lượng (kg)    Khoảng cách Quỹ đạo từ Mặt trời (AU)
Mercury               4,879           3.30 ×10^23    57.9 triệu km
Venus                12,104          4.86 ×10^24     67.2 triệu km
Earth               ~12,756         5.97 ×10^24    149.6 triệu km
Mars                6,371            6.41 ×10^23    227.9 triệu km
Jupiter             142,984        1.90 ×10^27       778.3 triệu km
Saturn              120,536        9.50 ×10^26       1.4 tỷ km
Uranus              51,512        1.70 ×10^26       2.8 tỷ km
Neptune             49,528        1.02 ×10^26       4.5 tỷ km
Pluto*             2,370           1.90 ×10^22    7.4 tỷ km
----------------------------------------- ---------------
* Pluto không còn được coi là một phần của hệ mặt trời của chúng ta vì nó quay quanh 
quỹ đạo vượt ra ngoài quỹ đạo của Neptune ở khoảng cách trung bình khoảng 4.5 tỷ kilomet từ 
mặt trời. Nó đã được phân loại lại là một hành tinh lùn do kích thước và vị trí của nó tương đối so với 
các vật thể khác trong Vành đai Kuiper.
Hình 10: RAHF-Dual và RAHF-SCIT cung cấp dữ liệu toàn diện, có cấu trúc với định dạng rõ ràng, nhất quán,
và bao gồm các chi tiết liên quan bổ sung như khối lượng và khoảng cách quỹ đạo từ Mặt trời. Chúng trình bày thông tin
định lượng chính xác, khiến chúng có tính thông tin và dễ hiểu hơn so với các phản hồi ít chi tiết, không nhất quán,
hoặc một phần không chính xác của HIR RLHF-PPO và DPO, thiếu tính đầy đủ và rõ ràng trong việc trình bày
kích thước hành tinh và dữ liệu quan trọng khác.
