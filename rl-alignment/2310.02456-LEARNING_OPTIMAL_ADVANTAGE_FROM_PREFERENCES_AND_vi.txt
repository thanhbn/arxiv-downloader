# 2310.02456.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2310.02456.pdf
# Kích thước file: 3723826 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
HỌC LỢI THẾ TỐI ƯU TỪ SỞ THÍCH VÀ NHẦM LẪN NÓ VỚI PHẦN THƯỞNG
BẢN THẢO
W. Bradley Knox *1,2, Stephane Hatgis-Kessell1, Sigurdur Orn Adalgeirsson2, Serena Booth3, Anca
Dragan4, Peter Stone1,5, và Scott Niekum6
1Đại học Texas tại Austin
2Google Research
3MIT CSAIL
4UC Berkeley
5Sony AI
6Đại học Massachusetts Amherst
TÓM TẮT
Chúng tôi xem xét các thuật toán học hàm phần thưởng từ sở thích của con người đối với các cặp đoạn quỹ đạo, như được sử dụng trong học tăng cường từ phản hồi của con người (RLHF). Hầu hết các nghiên cứu gần đây giả định rằng sở thích của con người được tạo ra chỉ dựa trên phần thưởng tích lũy trong các đoạn đó, hoặc partial return của chúng. Các nghiên cứu gần đây đặt ra nghi ngờ về tính hợp lệ của giả định này, đề xuất một mô hình sở thích thay thế dựa trên regret. Chúng tôi điều tra các hậu quả của việc giả định sở thích dựa trên partial return khi chúng thực sự phát sinh từ regret. Chúng tôi lập luận rằng hàm được học là một xấp xỉ của hàm lợi thế tối ưu, bA∗r, chứ không phải hàm phần thưởng. Chúng tôi thấy rằng nếu một cạm bẫy cụ thể được giải quyết, giả định sai này không đặc biệt có hại, dẫn đến một hàm phần thưởng được định hình cao. Tuy nhiên, việc sử dụng sai bA∗r này kém mong muốn hơn so với cách tiếp cận thích hợp và đơn giản hơn là tối đa hóa tham lam của bA∗r. Từ góc độ của mô hình sở thích regret, chúng tôi cũng cung cấp một giải thích rõ ràng hơn về việc tinh chỉnh các mô hình ngôn ngữ lớn đương đại với RLHF. Bài báo này tổng thể cung cấp cái nhìn sâu sắc về lý do tại sao việc học dưới mô hình sở thích partial return có xu hướng hoạt động rất tốt trong thực tế, mặc dù nó phù hợp kém với cách con người đưa ra sở thích.

*Liên hệ với:
bradknox@cs.utexas.edu

1 Giới thiệu
Khi học từ sở thích của con người (trong RLHF), mô hình chiếm ưu thế giả định rằng sở thích của con người được xác định chỉ bởi phần thưởng tích lũy của mỗi đoạn, hoặc partial return. Knox et al. [2022] lập luận rằng mô hình sở thích partial return có những khuyết điểm cơ bản được loại bỏ hoặc cải thiện bằng cách thay vào đó giả định rằng sở thích của con người được xác định bởi lợi thế tối ưu của mỗi đoạn, đây là một thước đo độ lệch khỏi việc ra quyết định tối ưu và tương đương với regret âm. Nghiên cứu trước đây này lập luận về tính ưu việt của mô hình sở thích regret (1) bằng trực giác, về cách con người có khả năng đưa ra sở thích (ví dụ, xem Hình 2); (2) bằng lý thuyết, cho thấy sở thích dựa trên regret có tính chất identifiability mong muốn mà sở thích từ partial return thiếu; (3) bằng phân tích mô tả, cho thấy khả năng của một tập dữ liệu sở thích con người cao hơn dưới mô hình sở thích regret so với dưới mô hình sở thích partial return; và (4) bằng phân tích thực nghiệm, cho thấy với cả sở thích con người và tổng hợp, mô hình regret yêu cầu ít nhãn sở thích hơn. Phần 2 của bài báo này cung cấp chi tiết về bối cảnh vấn đề chung và về hai mô hình này.

Trong bài báo này, chúng tôi khám phá các hậu quả của việc sử dụng các thuật toán được thiết kế với giả định rằng sở thích được xác định bởi partial return khi những sở thích này thay vào đó được xác định bởi regret. Chúng tôi chỉ ra trong Phần 3 rằng những thuật toán này học một xấp xỉ của hàm lợi thế tối ưu, A∗r, chứ không phải của hàm phần thưởng, như được giả định trong nhiều nghiên cứu trước đây. Sau đó chúng tôi nghiên cứu các tác động của việc diễn giải sai này. Khi được diễn giải như phần thưởng, lợi thế tối ưu chính xác được định hình cao và bảo toàn tập hợp các chính sách tối ưu, điều này cho phép các thuật toán dựa trên partial-return hoạt động tốt. Tuy nhiên, xấp xỉ được học của hàm lợi thế tối ưu, bA∗r, sẽ có lỗi. Chúng tôi đặc trưng khi nào và như thế nào những lỗi này sẽ ảnh hưởng đến tập hợp các chính sách tối ưu đối với phần thưởng bị nhầm lẫn này, và chúng tôi khám phá một phương pháp để giảm một loại lỗi có hại. Chúng tôi kết luận rằng việc sử dụng sai bA∗r này vẫn cho phép hiệu suất khá tốt dưới một số điều kiện nhất định, mặc dù nó kém mong muốn hơn so với cách tiếp cận thích hợp và đơn giản hơn là tối đa hóa tham lam của bA∗r.

Sau đó chúng tôi chỉ ra trong Phần 4 rằng các thuật toán gần đây được sử dụng để tinh chỉnh các mô hình ngôn ngữ tiên tiến ChatGPT [OpenAI 2022], Sparrow [Glaese et al. 2022], và các mô hình khác [Ziegler et al. 2019, Ouyang et al. 2022, Bai et al. 2022, Touvron et al. 2023] có thể được xem như một trường hợp của việc học một hàm lợi thế tối ưu và vô tình coi nó như một hàm phần thưởng. Trong các thiết lập multi-turn (tức là, tuần tự) như của ChatGPT, Sparrow, và nghiên cứu của Bai et al. [2022], khung hình thay thế này cho phép loại bỏ một giả định có vấn đề của các thuật toán này: rằng một hàm phần thưởng được học cho một nhiệm vụ tuần tự thay vào đó được sử dụng trong một thiết lập bandit, hiệu quả đặt hệ số chiết khấu γ về 0.

2 Kiến thức cơ bản: Mô hình sở thích để học hàm phần thưởng
Một quy trình quyết định Markov (MDP) được chỉ định bởi một tuple (S,A,T,γ,D0,r). S và A là các tập hợp các trạng thái và hành động có thể, tương ứng. T: S×A→p(·|s, a) là một hàm chuyển tiếp; γ là hệ số chiết khấu; và D0 là phân phối của các trạng thái bắt đầu. Trừ khi được nêu khác, chúng tôi giả định các nhiệm vụ không được chiết khấu (γ = 1) và có các trạng thái cuối, sau đó chỉ có thể nhận được phần thưởng 0. r là một hàm phần thưởng, r: S×A×S→R, trong đó rt là một hàm của st, at, và st+1 tại thời điểm t. Một MDP \r là một MDP không có hàm phần thưởng.

Trong suốt bài báo này, r đề cập đến hàm phần thưởng ground-truth cho một MDP nào đó; ˆr đề cập đến một xấp xỉ được học của r; và ˜r đề cập đến bất kỳ hàm phần thưởng nào (bao gồm r hoặc ˆr). Một chính sách (π: S×A→[0,1]) chỉ định xác suất của một hành động cho một trạng thái. Qπ˜r và Vπ˜r đề cập tương ứng đến hàm giá trị trạng thái-hành động và hàm giá trị trạng thái cho một chính sách, π, dưới ˜r, và được định nghĩa như sau.

Vπ˜r(s)≜Eπ[∞Xt=0˜r(st, at, st+1)|s0=s]
Qπ˜r(s, a)≜Eπ[˜r(s, a, s′) +Vπ˜r(s′)]

Một chính sách tối ưu π∗˜r là bất kỳ chính sách nào mà Vπ∗˜r˜r(s)≥Vπ˜r(s) tại mọi trạng thái s cho mọi chính sách π. Chúng tôi viết tắt cho Qπ∗˜r˜r và Vπ∗˜r˜r tương ứng là Q∗˜r và V∗˜r.

Hàm lợi thế tối ưu được định nghĩa là A∗˜r(s, a)≜Q∗˜r(s, a)−V∗˜r(s); điều này đo lường một hành động giảm return kỳ vọng bao nhiều so với việc tuân theo một chính sách tối ưu.

Trong suốt bài báo này, khi các sở thích không được tạo ra bởi con người, hàm phần thưởng ground-truth r được sử dụng để tạo ra sở thích một cách thuật toán. r được ẩn trong quá trình học phần thưởng và được sử dụng để đánh giá hiệu suất của các chính sách tối ưu dưới một ˆr được học.

2.1 Học phần thưởng từ sở thích theo cặp
Một hàm phần thưởng thường được học bằng cách tối thiểu hóa cross-entropy loss—tức là, tối đa hóa likelihood—của các nhãn sở thích con người được quan sát [Christiano et al. 2017, Ibarz et al. 2018, Wang et al. 2022, Bıyık et al. 2021, Sadigh et al. 2017, Lee et al. 2021a,b, Ziegler et al. 2019, Ouyang et al. 2022, Bai et al. 2022, Glaese et al. 2022, OpenAI 2022, Touvron et al. 2023].

Các đoạn Cho σ biểu thị một đoạn bắt đầu tại trạng thái sσ0. Độ dài |σ| của nó là số lượng chuyển tiếp trong đoạn. Một đoạn bao gồm |σ|+ 1 trạng thái và |σ| hành động: (sσ0, aσ0, sσ1, aσ1, ..., sσ|σ|). Trong bối cảnh vấn đề này, các đoạn thiếu bất kỳ thông tin phần thưởng nào. Như ký hiệu tắt, chúng tôi định nghĩa σt≜(sσt, aσt, sσt+1). Một đoạn σ là tối ưu đối với ˜r nếu, đối với mọi i∈ {1, ...,|σ|-1}, A∗˜r(sσi, aσi) = 0. Một đoạn không tối ưu là dưới tối ưu. Cho một ˜r và một đoạn σ nào đó, trong đó ˜rσt≜˜r(sσt, aσt, sσt+1), partial return không chiết khấu của một đoạn σ là P|σ|−1t=0˜rσt, mà chúng tôi ký hiệu tắt là Σσ˜r.

Tập dữ liệu sở thích Mỗi sở thích đối với một cặp đoạn tạo ra một mẫu (σ1, σ2, µ) trong tập dữ liệu sở thích D≻. Vector µ=⟨µ1, µ2⟩ biểu thị sở thích; cụ thể, nếu σ1 được ưa thích hơn σ2, ký hiệu σ1≻σ2, µ=⟨1,0⟩. µ là ⟨0,1⟩ nếu σ1≺σ2 và là ⟨0.5,0.5⟩ cho σ1∼σ2 (không có sở thích). Đối với một mẫu (σ1, σ2, µ), chúng tôi giả định rằng hai đoạn có độ dài bằng nhau (tức là, |σ1|=|σ2|).

2

--- TRANG 3 ---
Hàm loss Khi học một hàm phần thưởng từ một tập dữ liệu sở thích, D≻, các nhãn sở thích thường được giả định là được tạo ra bởi một mô hình sở thích P dựa trên một hàm phần thưởng ground-truth không thể quan sát được r. Chúng tôi học ˆr, một xấp xỉ của r, bằng cách tối thiểu hóa cross-entropy loss:

loss(ˆr, D≻) =
−X(σ1,σ2,µ)∈D≻µ1logP(σ1≻σ2|ˆr) +µ2logP(σ1≺σ2|ˆr)(1)

Nếu σ1≻σ2, likelihood của mẫu là P(σ1≻σ2|ˆr) và loss của nó do đó là −logP(σ1≻σ2|ˆr). Nếu σ1≺σ2, likelihood của nó là 1−P(σ1≻σ2|ˆr). Loss này là underspecified cho đến khi mô hình sở thích P(σ1≻σ2|ˆr) được định nghĩa. Các thuật toán trong bài báo này để học các xấp xỉ của r hoặc A∗r từ sở thích có thể được tóm tắt đơn giản là "tối thiểu hóa Phương trình 1".

Các mô hình sở thích Một mô hình sở thích xác định xác suất của một đoạn quỹ đạo được ưa thích hơn một đoạn khác, P(σ1≻σ2|˜r). Các mô hình sở thích có thể được sử dụng để mô hình hóa sở thích do con người hoặc các hệ thống khác cung cấp, hoặc để tạo ra sở thích tổng hợp.

2.2 Các mô hình sở thích: partial return và regret
Partial return Mô hình sở thích chiếm ưu thế (ví dụ, Christiano et al. [2017]) giả định sở thích của con người được tạo ra bởi một phân phối Boltzmann trên partial return của hai đoạn, được biểu thị ở đây như một hàm logistic:2

PΣr(σ1≻σ2|˜r) =logistic(Σσ1˜r−Σσ2˜r).(2)

Regret Knox et al. [2022] giới thiệu một mô hình sở thích con người thay thế. Mô hình dựa trên regret này giả định rằng sở thích dựa trên các độ lệch của các đoạn khỏi việc ra quyết định tối ưu: regret của mỗi chuyển tiếp trong một đoạn. Chúng tôi trước tiên tập trung vào các đoạn với chuyển tiếp xác định. Đối với một chuyển tiếp đơn (st, at, st+1), regret d(σt|˜r)≜V∗˜r(sσt)−[˜rt+V∗˜r(sσt+1)]. Đối với một đoạn đầy đủ,

regret d(σ|˜r)≜|σ|−1Xt=0regret d(σt|˜r)
=V∗˜r(sσ0)−(Σσ˜r+V∗˜r(sσ|σ|)),(3)

với biểu thức bên phải phát sinh từ việc triệt tiêu các giá trị trạng thái trung gian. Do đó, regret xác định đo lường đoạn giảm return kỳ vọng từ V∗˜r(sσ0) bao nhiều. Một đoạn tối ưu σ∗ luôn có regret 0, và một đoạn dưới tối ưu σ¬∗ luôn có regret dương.

Tuy nhiên, các chuyển tiếp trạng thái ngẫu nhiên có thể dẫn đến regret d(σ∗|ˆr)> regret d(σ¬∗|˜r), mất tính chất trên. Để giữ lại nó, chúng tôi lưu ý rằng hiệu ứng lên return kỳ vọng của tính ngẫu nhiên chuyển tiếp từ một chuyển tiếp (st, at, st+1) là [˜rt+V∗˜r(st+1)]−Q∗˜r(st, at) và thêm biểu thức này một lần cho mỗi chuyển tiếp để có regret (σ), loại bỏ chỉ số d đề cập đến tính xác định. Regret cho một chuyển tiếp đơn trở thành regret (σt|˜r) =[V∗˜r(sσt)−[˜rt+V∗˜r(sσt+1)]]+[[˜rt+V∗˜r(sσt+1)]−Q∗˜r(sσt, aσt)]=V∗˜r(sσt)−Q∗˜r(sσt, aσt) =−A∗˜r(sσt, aσt). Regret cho một đoạn đầy đủ là:

regret (σ|˜r) =|σ|−1Xt=0regret (σt|˜r)
=|σ|−1Xt=0[V∗˜r(sσt)−Q∗˜r(sσt, aσt)]
=|σ|−1Xt=0−A∗˜r(sσt, aσt).(4)

Mô hình sở thích regret là phân phối Boltzmann trên tổng của các lợi thế tối ưu, hoặc regret âm:

Pregret (σ1≻σ2|˜r)
≜logistic(|σ1|−1Xt=0A∗˜r(σ1,t)−|σ2|−1Xt=0A∗˜r(σ2,t))
=logistic(regret (σ2|˜r)−regret (σ1|˜r)).(5)

(Theo ký hiệu, A∗˜r(σt) = A∗˜r(sσt, aσt).) Cuối cùng, nếu hai đoạn có chuyển tiếp xác định, kết thúc ở các trạng thái cuối, và có cùng trạng thái bắt đầu, mô hình regret này giảm về mô hình partial return: Pregret (·|˜r) = PΣr(·|˜r).

Một cách trực quan, mô hình sở thích partial return luôn giả định sở thích dựa trên kết quả trong khi mô hình regret có thể tính đến sở thích dựa trên kết quả (Phương trình 3) và sở thích đối với quyết định (Phương trình 4).

[Hình 2: Hai đoạn trong một nhiệm vụ không chiết khấu với phần thưởng −1 mỗi bước thời gian. Partial return của cả hai đoạn đối với hàm phần thưởng thực là −2. Regret của đoạn bên trái là 4. Đoạn bên phải là tối ưu và do đó có regret là 0. Mô hình sở thích regret có nhiều khả năng ưa thích đoạn bên phải hơn—như chúng tôi nghi ngờ độc giả con người của chúng tôi sẽ làm—trong khi mô hình sở thích partial return có khả năng bằng nhau để ưa thích mỗi đoạn.]

Knox et al. [2022] đã chỉ ra regret vừa có các tính chất lý thuyết mong muốn (tức là, nó có thể identifiable trong khi partial return thì không) và là một mô hình tốt hơn về sở thích con người thực. Vì regret mô hình hóa sở thích con người thực tốt hơn, và vì nhiều nghiên cứu gần đây sử dụng sở thích con người thực nhưng giả định chúng được tạo ra theo partial return, chúng tôi hỏi: hậu quả của việc diễn giải sai hàm lợi thế tối ưu như phần thưởng là gì?

3 Học lợi thế tối ưu từ sở thích và sử dụng nó như phần thưởng
Chúng tôi hỏi: thực sự điều gì được học khi sở thích được giả định phát sinh từ partial return nhưng thực tế đến từ regret (Phương trình 2), và điều đó có ý nghĩa gì? Kết quả của chúng tôi có thể được tái tạo thông qua kho mã của chúng tôi, tại github.com/Stephanehk/Learning-OA-From-Prefs.

3.1 Học hàm lợi thế tối ưu
Để bắt đầu, hãy để chúng tôi thống nhất hai mô hình sở thích từ Phần 2.2 thành một mô hình sở thích chung duy nhất.

Pg(σ1≻σ2|˜r)≜logistic(|σ1|−1Xt=0g(σ1,t)−|σ2|−1Xt=0g(σ2,t))(6)

Trong việc thống nhất trên, thống kê đoạn trong mô hình sở thích được biểu thị như một tổng của một hàm g nào đó trên mỗi chuyển tiếp trong đoạn: P|σ|−1t=0g(σt) =P|σ|−1t=0g(sσt, aσt, sσt+1). Khi sở thích được tạo ra theo partial return, g(σt) = ˜r(sσt, aσt, sσt+1), và hàm phần thưởng ˜r được học thông qua Phương trình 1.

Khi sở thích thay vào đó được tạo ra theo regret, g(σt) =A∗˜r(σt) =A∗˜r(sσt, aσt) và các tham số của hàm lợi thế tối ưu này có thể được học trực tiếp, cũng thông qua Phương trình 1. bA∗r có thể được học và sau đó được hành động một cách tham lam, thông qua argmax abA∗r(s, a), một thuật toán mà chúng tôi gọi là greedy cA∗r (thuật toán dưới cùng của Hình 1). Đáng chú ý, thuật toán này không yêu cầu bước bổ sung của cải tiến chính sách và thay vào đó sử dụng bA∗r trực tiếp. Không có hàm phần thưởng nào được biểu thị hoặc học một cách rõ ràng, mặc dù chúng tôi vẫn giả định rằng sở thích được tạo ra bởi regret dưới một hàm phần thưởng ẩn r.

Phần còn lại của phần này xem xét trước tiên các hậu quả của việc sử dụng A∗r không có lỗi như một hàm phần thưởng: rA∗r=A∗r. Chúng tôi gọi cách tiếp cận sai này là greedy Q∗rA∗r. Sau đó chúng tôi xem xét các hậu quả của việc sử dụng xấp xỉ bA∗r như một hàm phần thưởng, rcA∗r=bA∗r, mà chúng tôi gọi là greedy Q∗rcA∗r. Cuộc điều tra sau đây là một nỗ lực để trả lời tại sao việc học trong khi giả định mô hình sở thích partial return có xu hướng hoạt động rất tốt trong thực tế, mặc dù nó phù hợp kém như một mô hình mô tả sở thích con người.

3.2 Sử dụng A∗r như một hàm phần thưởng
Dưới giả định của sở thích dựa trên regret, việc học một hàm phần thưởng với mô hình sở thích partial return hiệu quả sử dụng một xấp xỉ của A∗r như một hàm phần thưởng, ˆr=bA∗r. Hãy để chúng tôi trước tiên giả định suy luận hoàn hảo của A∗r (tức là, bA∗r=A∗r), và xem xét các hậu quả. Chúng tôi sẽ gọi các phiên bản không xấp xỉ của greedy bA∗r và rcA∗r là greedy A∗r và rA∗r.

Các chính sách tối ưu được bảo toàn. Việc sử dụng A∗r như một hàm phần thưởng bảo toàn tập hợp các chính sách tối ưu. Để chứng minh tuyên bố này, chúng tôi trước tiên chứng minh một định lý tổng quát hơn.

Đối với ˜r, một hàm phần thưởng tùy ý, max aA∗˜r(·, a) = 0 theo định nghĩa. Hãy để tập hợp các chính sách tối ưu đối với ˜r được ký hiệu Π∗˜r.

Định lý 3.1 (Hành động tham lam là tối ưu khi phần thưởng tối đa trong mọi trạng thái là 0.) .
Π∗˜r={π:∀s,∀a[π(a|s)>0⇔a∈argmaxa˜r(s, a)]} nếu max a˜r(·, a) = 0 .

Định lý 3.1 được chứng minh trong Phụ lục A. Bản phác thảo của chứng minh là nếu phần thưởng tối đa trong mọi trạng thái là 0, thì return tốt nhất có thể từ mọi trạng thái là 0. Do đó, V∗˜r(·) = 0, làm cho ∀(s, a)∈S×A, Q∗˜r(s, a) = ˜r(s, a) +γEs′[V∗˜r(s)] = ˜r(s, a).

Bây giờ chúng tôi quay lại trường hợp cụ thể của chúng tôi, được chứng minh trong Phụ lục B.

Hệ quả 3.1 (Tính bất biến chính sách của rA∗r).
Cho rA∗r≜A∗r. Nếu max aA∗r(·, a) = 0, Π∗rA∗r= Π∗r.

Một vấn đề underspecification được giải quyết. Như chúng tôi thảo luận trong Phần 4, khi độ dài đoạn là 1, mô hình sở thích partial return bỏ qua hệ số chiết khấu γ, làm cho sự lựa chọn của nó tùy ý mặc dù nó thường ảnh hưởng đến tập hợp các chính sách tối ưu. Với rA∗r, tuy nhiên, việc thiếu γ trong Hệ quả 3.1 thiết lập γ không ảnh hưởng đến tập hợp các chính sách tối ưu. Để đưa ra trực giác, chúng tôi áp dụng kết quả trung gian trong chứng minh của Định lý 3.1 rằng V∗˜r(·) = 0 cho trường hợp cụ thể của Hệ quả 3.1, chúng tôi thấy rằng V∗rA∗r(·) = 0. Do đó, Q∗rA∗r(s, a) =rA∗r(s, a) +γEs′[0], làm cho γ không có tác động lên Q∗rA∗r(s, a) và do đó lên Π∗r.

Phần thưởng được định hình cao. Trong nghiên cứu tinh bảo của Ng et al. [1999] về reward shaping dựa trên tiềm năng, họ nhấn mạnh ϕ(s) =V∗r(s) như một hàm tiềm năng đặc biệt mong muốn. Thao tác đại số cho thấy rằng MDP kết quả từ ϕ này thực sự sử dụng như một hàm phần thưởng rA∗r≜A∗r. Xem Phụ lục C cho việc suy dẫn. Ng et al. cũng lưu ý rằng nó gây ra V∗rA∗r(·) = 0 và do đó dẫn đến "một hàm giá trị đặc biệt dễ học; ... tất cả những gì còn lại phải làm sẽ là học các Q-value khác không." Chúng tôi đặc trưng cách tiếp cận này là được định hình cao vì thông tin cần thiết để hành động tối ưu nằm trong phần thưởng ngay lập tức của agent.

4

--- TRANG 5 ---
Cải tiến chính sách lãng phí tính toán và lấy mẫu môi trường. Khi sử dụng A∗r như một hàm phần thưởng, không cần cải tiến chính sách: đặt π(s) = argmax a[A∗r(s, a)] cung cấp một chính sách tối ưu.

3.3 Sử dụng bA∗r được học như một hàm phần thưởng
Một lưu ý đối với phân tích trước đó là thuật toán không nhất thiết học A∗r. Thay vào đó nó học xấp xỉ của nó, bA∗r. Chúng tôi điều tra các hiệu ứng của lỗi xấp xỉ của bA∗r. Chúng tôi thấy rằng lỗi này chỉ gây ra sự khác biệt về hiệu suất so với greedy bA∗r khi max abA∗r(s, a)̸= 0 trong ít nhất một trạng thái s, và hậu quả của lỗi đó phụ thuộc vào partial return tối đa của tất cả các vòng lặp—các đoạn bắt đầu và kết thúc ở cùng một trạng thái—trong MDP.

Đối với các kết quả thực nghiệm dưới đây, chúng tôi xây dựng dựa trên thiết lập thực nghiệm của Knox et al. [2022], bao gồm cả việc học và tạo ra các MDP ngẫu nhiên. Các siêu tham số và các thiết lập thực nghiệm khác là giống hệt trừ khi được ghi chú. Tất cả sở thích được tạo ra tổng hợp bởi mô hình sở thích regret.

Nếu giá trị tối đa của bA∗r trong mọi trạng thái là 0, hành vi giống hệt nhau giữa greedy Q∗rcA∗r và greedy bA∗r. Từ Định lý 3.1, điều sau đây tầm thường đúng cho một xấp xỉ được học bA∗r.

Hệ quả 3.2. Cho rcA∗r≜bA∗r. Nếu max abA∗r(·, a) = 0, thì Π∗rcA∗r={π:∀s,∀a[π(a|s)>0⇔a∈argmaxabA∗r(s, a)]}.

Do đó, nếu max abA∗r(·, a) = 0, thì một chính sách từ greedy bA∗r giống hệt với một chính sách tối ưu cho greedy Q∗rcA∗r, giả định các quan hệ ngang bằng được giải quyết giống hệt nhau. Chính sách thực tế từ greedy Q∗rcA∗r cũng sẽ giống hệt trừ khi các hạn chế của thuật toán cải tiến chính sách khiến nó không tìm thấy một chính sách trong Π∗rcA∗r trong thiết lập được định hình cao này với hàm phần thưởng cũng có sẵn, không yêu cầu kinh nghiệm để truy vấn. Tuy nhiên, max abA∗r(·, a) = 0 không được đảm bảo cho một xấp xỉ của A∗r, mà chúng tôi xem xét sau trong phần này.

Chúng tôi tiến hành một kiểm nghiệm thực nghiệm của khẳng định trên bằng cách điều chỉnh bA∗r để có tính chất max abA∗r(·, a) = 0 bằng cách dịch chuyển bA∗r bởi một hằng số phụ thuộc vào trạng thái: đối với tất cả (s, a), rcA∗r-shifted(s, a)≜bA∗r(s, a)−max a′bA∗r(s, a′). Lưu ý rằng argmax arcA∗r-shifted(s, a) = argmax abA∗r(s, a). Trong 90 MDP gridworld nhỏ, chúng tôi quan sát không có sự khác biệt giữa greedy bA∗r và greedy Q∗rcA∗r với rcA∗r-shifted (xem Hình 9). Tuy nhiên, chi phí thường được phát sinh từ hành vi dưới tối ưu và lấy mẫu môi trường trong khi một thuật toán cải tiến chính sách học chính sách xấp xỉ tối ưu này, trừ khi thuật toán cải tiến chính sách sử dụng rcA∗r-shifted có sẵn mà không lấy mẫu môi trường và sử dụng kiến thức rằng giá trị trạng thái là 0 trong mọi trạng thái, điều này cùng nhau cho phép nó đơn giản định nghĩa hành vi tối ưu là argmax aQrcA∗r-shifted(s, a) =argmax arcA∗r-shifted(s, a) = argmax abA∗r(s, a), đó là greedy bA∗r.

[Hình 3: Hiệu suất khi các tập dữ liệu sở thích được tạo ra không nhiễu và có bao gồm các đoạn với chuyển tiếp từ trạng thái hấp thụ. Kết quả trên 30 MDP gridworld được tạo ra ngẫu nhiên với các biểu diễn dạng bảng của bA∗r, trong đó các đoạn có độ dài 3 được chọn bằng cách chọn ngẫu nhiên đồng đều một trạng thái bắt đầu và 3 hành động của nó. Khi các chuyển tiếp từ trạng thái hấp thụ không được bao gồm, bất kỳ đoạn nào kết thúc trước chuyển tiếp cuối cùng của nó đều bị từ chối và sau đó được lấy mẫu lại. Đối với greedy bA∗r (màu đỏ) các kiểm nghiệm Wilcoxon paired signed-rank cho thấy việc bao gồm chuyển tiếp từ trạng thái hấp thụ dẫn đến hiệu suất cao hơn đáng kể cho tất cả kích thước tập huấn luyện trừ nhỏ nhất, 300, với p < 0.0007. Không phát hiện sự khác biệt đáng kể về hiệu suất cho greedy Q∗rcA∗r có hoặc không có chuyển tiếp kết thúc trừ tại 30,000 sở thích với p= 0.04 khiêm tốn hơn. Phụ lục G chứa biểu đồ cho sở thích được tạo ra ngẫu nhiên (Hình 11), có chứa các kết quả tương tự.]

Bao gồm các đoạn với chuyển tiếp từ trạng thái hấp thụ khuyến khích max abA∗r(·,a) = 0. Nếu một nhà thiết kế thuật toán tin tưởng rằng các sở thích trong tập dữ liệu sở thích của họ được tạo ra thông qua mô hình sở thích regret, thì kỹ thuật dịch chuyển thủ công bA∗r ở trên có thể được biện minh và khả thi, tùy thuộc vào kích thước của không gian hành động. Tuy nhiên với sự tin tưởng như vậy, hành động để tối đa hóa tham lam bA∗r là đơn giản và hiệu quả hơn. Hơn nữa, một sự hấp dẫn sẽ xuất hiện từ phân tích của chúng tôi là việc giả định thuật toán rằng sở thích phát sinh từ partial return có thể dẫn đến hiệu suất tốt bất kể sở thích thực sự phản ánh partial return hay regret. Kỹ thuật dịch chuyển thủ công có thể thay đổi tập hợp các chính sách tối ưu khi sở thích được tạo ra bởi mô hình sở thích partial return. Do đó, chúng tôi không khuyến nghị áp dụng sự dịch chuyển trên trong thực tế. Dưới đây chúng tôi mô tả một phương pháp khác mà, mặc dù không hoàn hảo, tránh việc ôm ấp rõ ràng một trong hai mô hình sở thích.

Việc thêm một hằng số vào bA∗r không thay đổi likelihood của một tập dữ liệu sở thích, làm cho giá trị được học của max (s,a)bA∗r(s, a) tùy ý. Do đó, nó cũng làm cho max abA∗r(·, a) underspecified. Nếu các nhiệm vụ có chân trời khác nhau, thì các lựa chọn khác nhau cho giá trị tối đa này có thể xác định các tập hợp chính sách tối ưu khác nhau (ví dụ, bằng cách thay đổi liệu việc kết thúc có mong muốn hay không). Một giải pháp là chuyển đổi các nhiệm vụ chân trời khác nhau thành các nhiệm vụ tiếp tục bằng cách bao gồm các chuyển tiếp vô hạn từ các trạng thái hấp thụ đến chính chúng sau khi kết thúc, trong đó tất cả các chuyển tiếp như vậy nhận được phần thưởng 0. Lưu ý rằng vấn đề này không tồn tại khi hành động trực tiếp từ bA∗r—tức là, π(s) = argmax a[bA∗r(s, a)]—mà việc thêm một hằng số vào đầu ra của bA∗r không thay đổi π. Một số tác giả trước đây đã thừa nhận sự không nhạy cảm này đối với một sự dịch chuyển [Christiano et al. 2017, Lee et al. 2021a, Ouyang et al. 2022, Hejna and

5

--- TRANG 6 ---
Sadigh 2023], và thực hành phổ biến của việc buộc tất cả các nhiệm vụ có một chân trời cố định (ví dụ, như được thực hiện bởi Christiano et al. [2017, tr. 14] và Gleave et al. [2022]) có thể một phần là do hiệu suất kém kết quả khi sử dụng mô hình sở thích partial return trong các nhiệm vụ chân trời khác nhau mà không có chuyển tiếp từ trạng thái hấp thụ.

Hình 4 cho thấy tác động lớn của việc bao gồm chuyển tiếp từ trạng thái hấp thụ khi ˆr=bA∗r. Như mong đợi, greedy bA∗r không bị ảnh hưởng đáng kể bởi việc bao gồm các chuyển tiếp như vậy. Hơn nữa, Hình 4 cho thấy việc bao gồm các chuyển tiếp từ trạng thái hấp thụ này thực sự đẩy max aA∗˜r(·, a) về phía 0, nhiều hơn với kích thước tập huấn luyện lớn hơn (cho một số lượng epoch cố định), mặc dù nó không hoàn toàn thực hiện được việc làm cho max aA∗˜r(·, a) = 0.

[Hình 4: So sánh hiệu ứng lên greedy Q∗rcA∗r của việc bao gồm chuyển tiếp từ trạng thái hấp thụ. Đối với mỗi trạng thái trong 30 MDP, các biểu đồ trên cho thấy các giá trị max abA∗r(s, a). Biểu đồ cho thấy việc bao gồm các chuyển tiếp như vậy di chuyển các giá trị tối đa kết quả gần hơn đến 0. Biểu đồ cho sở thích được tạo ra ngẫu nhiên tương tự và có thể được tìm thấy trong Phụ lục F. Sau khi học với chuyển tiếp hấp thụ, max abA∗r(s, a) trên tất cả trạng thái gần 0 hơn về mặt ngẫu nhiên so với khi học không có chúng. Các kiểm nghiệm Wilcoxon paired signed-rank tại mọi kích thước tập huấn luyện đều cực kỳ đáng kể với p <10−7.]

Thiên vị hướng về kết thúc xác định sự khác biệt hiệu suất. Khi max abA∗r(s, a) có xu hướng gần 0, chúng tôi thấy hiệu suất của greedy Q∗rcA∗r và greedy bA∗r tương tự. Nhưng hiệu suất của chúng đôi khi khác nhau. Chúng ta có thể dự đoán thuật toán nào sẽ hoạt động tốt hơn không? Để giải quyết câu hỏi này, chúng tôi đã thực hiện một phân tích chi tiết với 90 MDP gridworld nhỏ, từ đó giả thuyết sau đây nảy sinh. Logic đằng sau giả thuyết sau đây giả định một nhiệm vụ không chiết khấu, mặc dù các hiệu ứng được giả thuyết nên tồn tại ở dạng giảm khi chiết khấu được tăng lên. Chúng tôi định nghĩa một vòng lặp là một đoạn bắt đầu và kết thúc ở cùng một trạng thái và sau đó tập trung vào partial return tối đa bởi rcA∗r trên tất cả các vòng lặp.

[Bảng 1: Giả thuyết về thuật toán nào hoạt động tốt như nhau hoặc tốt hơn thuật toán kia, cho 2 điều kiện.

Điều kiện | π∗r kết thúc | π∗r không kết thúc
Max loop partial return >0 | greedy Q∗rcA∗r | greedy bA∗r  
Max loop partial return <0 | greedy bA∗r | greedy Q∗rcA∗r]

[Hình 5: Xác nhận giả thuyết rằng partial return tối đa bởi rcA∗r trên tất cả các vòng lặp xác định hướng của sự khác biệt hiệu suất giữa greedy bA∗r và greedy Q∗rcA∗r. 1080 lần chạy được hiển thị, được xây dựng từ tập hợp 90 MDP × {10,100,1000} sở thích trong tập huấn luyện × {1,2} độ dài đoạn × {noiselessly, stochastically} sở thích được tạo ra. Các điểm biểu đồ được tô màu cam khi mọi π∗r kết thúc và màu xanh dương khi mọi π∗r không kết thúc. Phần tô màu xanh dương và cam của biểu đồ biểu thị nơi các giả thuyết của chúng tôi dự đoán các vòng tròn của mỗi màu sẽ ở, nếu y̸= 0. Return được chuẩn hóa trên các MDP trong [−1,1] (chi tiết trong Phụ lục D), và trục x là partial return tối đa bởi rcA∗r trên tất cả các vòng lặp trong MDP. Trong số 75 lần chạy với sự khác biệt hiệu suất (y̸= 0), 73 phù hợp với giả thuyết của chúng tôi. Trong 2 lần chạy còn lại, cả hai thuật toán đạt được hành vi gần tối ưu và do đó có sự khác biệt ít hơn 0.1.]

Tập trung vào các nhiệm vụ với chuyển tiếp xác định,3 việc biện minh cho giả thuyết này dựa trên các thiên vị sau đây được tạo ra bởi partial return tối đa của tất cả các vòng lặp:

• Khi partial return tối đa của tất cả các vòng lặp là dương, bất kỳ π∗rcA∗r nào sẽ không kết thúc vì nó có thể đạt được giá trị vô hạn.

• Khi partial return tối đa của tất cả các vòng lặp là âm, bất kỳ πˆr nào cho rcA∗r sẽ kết thúc, vì nó chỉ có thể đạt được giá trị âm vô hạn mà không kết thúc.

Kết quả được hiển thị trong Hình 5 xác nhận giả thuyết này. Trên 1080 lần chạy học bA∗r trong các thiết lập khác nhau, chúng tôi thấy rằng giả thuyết có tính dự đoán cao về các độ lệch trong hiệu suất.

Nguyên nhân của thước đo dự đoán này, partial return tối đa bởi rcA∗r của tất cả các vòng lặp, chưa được đặc trưng. Do đó, một nhà thiết kế thuật toán vẫn nên cẩn thận về việc nhầm lẫn bA∗r với một hàm phần thưởng và dựa vào thước đo dự đoán này để xác định liệu chính sách kết quả tránh hay tìm kiếm sự kết thúc.

Phần thưởng cũng được định hình cao với lỗi xấp xỉ. Chúng tôi cũng kiểm tra liệu reward shaping tồn tại khi sử dụng A∗r như một hàm phần thưởng cũng có mặt khi sử dụng xấp xỉ của nó, bA∗r. Hình 6 cho thấy việc cải tiến chính sách với thuật toán Q learning [Watkins

3 Đối với các nhiệm vụ ngẫu nhiên, khái niệm vòng lặp này tổng quát hóa thành phân phối trạng thái ổn định với phần thưởng trung bình tối đa, trên tất cả các chính sách.

6

--- TRANG 7 ---
và Dayan 1992] hiệu quả hơn mẫu với rA∗r và với rcA∗r so với với ground truth r, như đã được mong đợi.

[Hình 6: Đường cong học cho Q learning trên hàm phần thưởng ground truth r và trên rcA∗r. Mỗi đường cong biểu thị 100 trường hợp của Q learning, mỗi trường hợp trong một MDP khác nhau. bA∗r được học với 100,000 sở thích dựa trên regret không nhiễu. Thậm chí mà không cung cấp cho agent học quyền truy cập vào rcA∗r đã biết, chúng tôi thấy việc học hiệu quả hơn, cho thấy rằng trong thực tế rcA∗r là một hàm phần thưởng được định hình hữu ích, như là sử dụng A∗r thực như một hàm phần thưởng. Chúng tôi định nghĩa AAC là diện tích trên một đường cong và dưới 1.0. Một AAC nhỏ cho thấy hiệu suất học tốt hơn. Các kiểm nghiệm Wilcoxon paired signed-rank cho thấy Q learning với r (tím) có AAC lớn hơn với rA∗r (đỏ), mà lần lượt có AAC lớn hơn với rcA∗r (cả p <0.00003).]

3.4 Tóm tắt
Khi một người học từ sở thích dựa trên regret sử dụng mô hình sở thích partial return, các hậu quả lý thuyết và thực nghiệm ít có hại hơn so với việc sử dụng sai rõ ràng này gợi ý. Chính sách đã có thể được học với mô hình sở thích regret đúng được bảo toàn nếu bA∗r có tối đa 0 trong mọi trạng thái. Hơn nữa, bA∗r hoạt động như một phần thưởng được định hình cao. Có lẽ phân tích này giải thích tại sao mô hình sở thích partial return—được chỉ ra không mô hình hóa sở thích con người tốt [Knox et al. 2022]—tuy nhiên đã đạt được hiệu suất ấn tượng trên nhiều nhiệm vụ. Điều đó nói rằng, việc nhầm lẫn bA∗r với một hàm phần thưởng có các nhược điểm so với greedy bA∗r, bao gồm độ phức tạp mẫu cao hơn và nhạy cảm với một yếu tố ít được nghiên cứu, partial return tối đa bởi rcA∗r của tất cả các vòng lặp.

4 Tái khung hóa các nghiên cứu liên quan về tinh chỉnh các mô hình tạo sinh
Mô hình sở thích partial return đã được sử dụng trong một số ứng dụng nổi bật: để tinh chỉnh các mô hình ngôn ngữ lớn cho tóm tắt văn bản [Ziegler et al. 2019], để tạo ra InstructGPT và ChatGPT [Ouyang et al. 2022, OpenAI 2022], để tạo ra Sparrow [Glaese et al. 2022], trong nghiên cứu của Bai et al. [2022], và để tinh chỉnh Llama 2 [Touvron et al. 2023]. Việc sử dụng mô hình partial return trong những nghiên cứu này may mắn cho phép một giải thích thay thế về cách tiếp cận của họ: họ đang áp dụng một mô hình sở thích regret và đang học một hàm lợi thế tối ưu, không phải một hàm phần thưởng. Những cách tiếp cận này tạo ra một số giả định:

• Sở thích được tạo ra bởi partial return.
• Trong quá trình cải tiến chính sách, nhiệm vụ tuần tự được coi như một nhiệm vụ bandit tại mỗi bước thời gian. Việc đối xử đó tương đương với việc đặt hệ số chiết khấu γ về 0 trong quá trình cải tiến chính sách.
• Hàm phần thưởng là R→S×A, không lấy trạng thái tiếp theo làm đầu vào.

Những cách tiếp cận này học g như trong Phương trình 6, được diễn giải như một hàm phần thưởng theo mô hình sở thích partial return. Họ cũng giả định γ= 0 trong suốt những gì sẽ là giai đoạn cải tiến chính sách. Do đó, ˜r(s, a) = Q∗˜r(s, a), và đối với bất kỳ trạng thái s nào, π∗˜r(s) = argmax aQ∗˜r(s, a) = argmax a˜r(s, a) = argmax ag(s, a).

Vấn đề với các giả định trên Nhiều mô hình ngôn ngữ được xem xét ở đây được áp dụng trong thiết lập tuần tự của cuộc đối thoại tương tác nhiều lượt, như ChatGPT [OpenAI 2022], Sparrow [Glaese et al. 2022], và nghiên cứu của Bai et al. [2022]. Coi những điều này như các nhiệm vụ bandit (tức là, đặt γ= 0) là một quyết định không được giải thích mâu thuẫn với cách các hàm phần thưởng được sử dụng trong các nhiệm vụ tuần tự, để tích lũy trong suốt nhiệm vụ để đánh giá một quỹ đạo như return.

Hơn nữa, sự lựa chọn của γ là tùy ý trong khung hình ban đầu của các thuật toán của họ. Vì họ cũng giả định |σ|= 1, thì partial return của một đoạn giảm về phần thưởng ngay lập tức mà không chiết khấu: P|σ|−1t=0γt˜r(sσt, aσt) = ˜r(sσ0, aσ0). Do đó, γ kỳ quặc không có tác động lên hàm phần thưởng nào được học từ mô hình sở thích partial return (giả định định nghĩa tiêu chuẩn trong thiết lập này rằng 00= 1). Việc thiếu tác động này là một khía cạnh có vấn đề chung của việc học hàm phần thưởng với các mô hình sở thích partial return, vì thay đổi γ cho một hàm phần thưởng cố định được biết là thường thay đổi tập hợp các chính sách tối ưu. (Nếu không, MDP có thể được giải quyết dễ dàng hơn nhiều bằng cách đặt γ= 0 và tối đa hóa cận thị phần thưởng ngay lập tức.)

Mặc dù hai giả định—rằng sở thích được điều khiển chỉ bởi partial return và rằng γ= 0—thiếu biện minh và xuất hiện có hậu quả đáng kể, kỹ thuật này cực kỳ hiệu quả, tạo ra một số mô hình ngôn ngữ có khả năng nhất tại thời điểm viết.

Tinh chỉnh với sở thích dựa trên regret Hãy để chúng tôi thay vào đó giả định sở thích đến từ mô hình sở thích regret. Như được giải thích trong Phần 3.2, giả định γ= 0 sau đó không có hiệu ứng. Do đó nó có thể được loại bỏ, tránh cả hai giả định đáng lo ngại. Cụ thể, nếu sở thích đến từ mô hình sở thích regret, thì đầu ra g của cùng thuật toán là bA∗r. Do đó, dưới khung hình dựa trên regret này, đối với bất kỳ trạng thái s nào, π∗˜r(s) = argmax aA∗˜r(s, a) = argmax ag(s, a). Do đó, cả thuật toán học và lựa chọn hành động cho một chính sách tham lam trong thiết lập này đều tương đương về chức năng với thuật toán của họ, nhưng diễn giải của chúng thay đổi.

7

--- TRANG 8 ---
[Hình 7: Bài báo này tập trung độc quyền vào vấn đề bên trái, liên quan đến nhiều lượt của con người cung cấp một prompt và agent mô hình ngôn ngữ phản hồi. Vấn đề bên phải là một ràng buộc nhân tạo phổ biến trên lựa chọn hành động để làm cho nó khả thi, sử dụng chính sách để lấy mẫu một phản hồi từng token một, tuần tự; nó không liên quan đến bất kỳ tương tác nào với con người (tức là, môi trường).]

Tóm tắt, giả định rằng việc học từ sở thích tạo ra một hàm lợi thế tối ưu—hậu quả của việc áp dụng mô hình sở thích regret được hỗ trợ thực nghiệm hơn—cung cấp một khung hình nhất quán hơn cho các thuật toán này.

Một nguồn gây nhầm lẫn phổ biến Lựa chọn hành động tham lam bản thân có thể thách thức đối với các không gian hành động lớn. Những mô hình ngôn ngữ này có các không gian hành động lớn, vì việc chọn một phản hồi cho prompt mới nhất của con người liên quan đến việc chọn một chuỗi lớn các token. Sự lựa chọn phản hồi này là một hành động đơn dẫn đến tương tác với môi trường, con người. Như một ví dụ, Ouyang et al. [2022] thay vào đó hạn chế nhân tạo việc lựa chọn một hành động để chính nó là một vấn đề ra quyết định tuần tự, buộc các token được chọn từng cái một, theo thứ tự từ đầu đến cuối văn bản, như Hình 7 minh họa. Họ sử dụng một thuật toán policy gradient, PPO [Schulman et al. 2017], để học một chính sách cho vấn đề phụ này, trong đó agent RL nhận được phần thưởng 0 cho đến khi token cuối cùng được chọn. Tại thời điểm đó, dưới diễn giải của họ, nó nhận được phần thưởng bandit được học từ vấn đề bên trái trong Hình 7. Bài báo này không tập trung vào cách thực hiện lựa chọn hành động tham lam, và chúng tôi không có lập trường về việc có nên coi nó như một vấn đề RL từng token hay không. Tuy nhiên, nếu một người muốn sử dụng cách tiếp cận như vậy đối với lựa chọn hành động tham lam trong khi tìm kiếm π(s) =argmax a[bA∗r(s, a)], thì phần thưởng bandit được thay thế đơn giản bởi lợi thế tối ưu, một lần nữa có thể thực hiện bởi cùng mã, vì cả hai đều đơn giản là các đầu ra của g.

Ý nghĩa đối với tinh chỉnh các mô hình tạo sinh Các mở rộng của nghiên cứu tinh chỉnh được thảo luận có thể tìm cách học một hàm phần thưởng để sử dụng ngoài một thiết lập bandit. Động lực để làm như vậy bao gồm các hàm phần thưởng tổng quát hóa tốt hơn khi động lực chuyển tiếp thay đổi và cho phép mô hình ngôn ngữ cải thiện hành vi của nó dựa trên các kết quả dài hạn được trải nghiệm. Để học một hàm phần thưởng để sử dụng trong một thiết lập vấn đề tuần tự như vậy, việc khung hóa tập dữ liệu sở thích như đã được tạo ra bởi mô hình sở thích regret sẽ cung cấp một thuật toán khác để làm như vậy (trong Phần 2). Nó cũng sẽ tránh tính tùy ý của việc đặt γ >0 và học với mô hình sở thích partial return, mà đầu ra cùng hàm phần thưởng dưới các giả định của những bài báo này bất kể hệ số chiết khấu. Thuật toán dựa trên regret để học một hàm phần thưởng nhất quán hơn nội bộ và xuất hiện phù hợp hơn với sở thích của các stakeholder con người.

Tuy nhiên, nó thực sự đưa ra các thách thức nghiên cứu để học các hàm phần thưởng trong các nhiệm vụ phức tạp như những nhiệm vụ mà các mô hình ngôn ngữ này được tinh chỉnh. Đặc biệt, phương pháp đã biết để học một hàm phần thưởng với mô hình sở thích regret yêu cầu một xấp xỉ có thể vi phân của hàm lợi thế tối ưu cho hàm phần thưởng phát sinh từ các tham số thay đổi tại mỗi lần lặp huấn luyện.

5 Kết luận
Bài báo này điều tra các hậu quả của việc giả định rằng sở thích được tạo ra theo partial return khi chúng thay vào đó phát sinh từ regret. Mô hình sở thích regret cung cấp một tài khoản cải thiện về phương pháp hiệu quả của việc tinh chỉnh LLM từ sở thích (Phần 4). Trong trường hợp chung (Phần 3), chúng tôi thấy rằng giả định sai này không tai hại đối với hiệu suất khi được tính trung bình trên nhiều trường hợp của việc học, điều này giải thích thành công của nhiều thuật toán dựa vào giả định sai này. Tuy nhiên, diễn giải sai này làm mờ ám việc học từ sở thích, làm lẫn lộn trực giác của các thực hành viên về sở thích con người và cách sử dụng hàm được học từ sở thích. Chúng tôi tin rằng mô hình sở thích partial return hiếm khi chính xác cho các đoạn quỹ đạo, tức là, hiếm khi sở thích của con người không bị ảnh hưởng bởi bất kỳ giá trị trạng thái cuối của đoạn, giá trị trạng thái bắt đầu, hoặc may mắn trong các chuyển tiếp. Giả định rằng con người kết hợp tất cả ba đặc điểm đoạn đó, như mô hình sở thích regret làm, dẫn đến một mô hình mô tả tốt hơn, tuy nhiên nó không mô tả phổ quát sở thích con người. Để cải thiện hiệu quả mẫu và sự liên kết của các agent học từ sở thích, nghiên cứu tiếp theo nên tập trung thêm vào các mô hình tiềm năng của sở thích con người và cũng về các phương pháp để ảnh hưởng người dân tuân theo một mô hình sở thích mong muốn. Cuối cùng, sau khi đọc bài báo này, một người có thể bị cám dỗ kết luận rằng an toàn để nhắm mắt, nghiến răng, và đặt niềm tin vào mô hình sở thích partial return. Kết luận này không được hỗ trợ bởi bài báo này, vì thậm chí với việc bổ sung các chuyển tiếp từ trạng thái hấp thụ, thiên vị tùy ý để tìm kiếm hoặc tránh kết thúc thường xuyên được giới thiệu. Ý nghĩa của thiên vị này đặc biệt quan trọng vì RLHF hiện tại là cơ chế bảo vệ chính cho LLM [Casper et al. 2023].

8

--- TRANG 9 ---
Lời cảm ơn
Công việc này đã diễn ra một phần trong phòng thí nghiệm Interactive Agents and Collaborative Technologies (InterACT) tại UC Berkeley, Learning Agents Research Group (LARG) tại UT Austin và Safe, Correct, and Aligned Learning and Robotics Lab (SCALAR) tại The University of Massachusetts Amherst. Nghiên cứu LARG được hỗ trợ một phần bởi NSF (FAIN-2019844, NRT-2125858), ONR (N00014-18-2243), ARO (E2061621), Bosch, Lockheed Martin, và thách thức lớn Good Systems của UT Austin. Peter Stone được bồi thường tài chính với tư cách Giám đốc điều hành của Sony AI America, các điều khoản đã được UT Austin phê duyệt. Nghiên cứu SCALAR được hỗ trợ một phần bởi NSF (IIS-1749204), AFOSR (FA9550-20-1-0077), và ARO (78372-CS, W911NF-19-2-0333). Nghiên cứu InterACT được hỗ trợ một phần bởi ONR YIP và NSF HCC. Serena Booth được hỗ trợ bởi NSF GRFP.

Tài liệu tham khảo
[Danh sách các tài liệu tham khảo từ trang 9-10 - tôi sẽ không dịch chi tiết phần này vì chủ yếu là thông tin xuất bản]

10

--- TRANG 11 ---
A Chứng minh Định lý 3.1
Định lý 3.1 (Hành động tham lam là tối ưu khi phần thưởng tối đa trong mọi trạng thái là 0.)
Π∗˜r={π:∀s,∀a[π(a|s)>0⇔a∈argmaxa˜r(s, a)]} nếu max a˜r(·, a) = 0.

Ý tưởng chính là nếu phần thưởng tối đa trong mọi trạng thái là 0, thì return tốt nhất có thể từ mọi trạng thái là 0. Do đó, V∗˜r(·) = 0, làm cho ∀(s, a)∈S×A, Q∗˜r(s, a) = ˜r(s, a) +γEs′[V∗˜r(s)] = ˜r(s, a).

Chứng minh như sau.
∀(s, a)∈S×A,˜r(s, a)≤0, vậy ∀s∈S, V∗˜r(s)≤0.
∀s∈S,∃a∈A: ˜r(s, a) = 0, vậy ∀s∈S, V∗˜r(s)≥0.
V∗˜r(s)≤0 và V∗˜r(s)≥0 suy ra V∗˜r(s) = 0, vậy ∀s∈S, V∗˜r(s) = 0.
∀(s, a)∈S×A,
Q∗˜r(s, a) = ˜r(s, a) +γEs′[V∗˜r(s′)]
Q∗˜r(s, a) = ˜r(s, a) +γEs′[0]
Q∗˜r(s, a) = ˜r(s, a)
argmax aQ∗˜r(s, a) =argmax a˜r(s, a)(7)

Theo định nghĩa, Π∗˜r={π:∀s, π(s) =argmaxaQ∗˜r(s, a)}.
Vì argmax aQ∗˜r(s, a) =argmax a˜r(s, a), Π∗˜r={π:∀s, π(s) =argmaxa˜r(s, a)}. □

B Chứng minh Hệ quả 3.1
Hệ quả 3.1 (Tính bất biến chính sách của rA∗r)
Cho rA∗r≜A∗r. Nếu max aA∗r(·, a) = 0, Π∗rA∗r= Π∗r.

Vì max aA∗r(·, a) = 0 và rA∗r≜A∗r, max arA∗r(·, a) = 0.
Do đó, theo Định lý 3.1, Π∗rA∗r={π:∀s,∀a[π(a|s)>0⇔a∈argmaxarA∗r(s, a)]}.
Cũng vậy, theo định nghĩa, Π∗r={π:∀s,∀a[π(a|s)>0⇔a∈argmaxaA∗r(s, a)]}.
Do đó,
Π∗rA∗r={π:∀s,∀a[π(a|s)>0⇔a∈argmaxarA∗r(s, a)]}
={π:∀s,∀a[π(a|s)>0⇔a∈argmaxaA∗r(s, a)]}
= Π∗r(8)

C Được sử dụng như phần thưởng, A∗r được định hình cao
Trong Phần 3.2, chúng tôi nêu rằng việc tuân theo lời khuyên dưới đây của Ng et al. [1999] tương đương với việc sử dụng A∗r như phần thưởng. Chúng tôi suy dẫn kết quả này sau khi xem xét lời khuyên của họ.

Trong bài báo của họ về potential-based reward shaping, các tác giả đề xuất một dạng mạnh mẽ của việc đặt Φ(s), đó là Φ(s) = V∗M(s). Ký hiệu của họ bao gồm các MDP M và M′, trong đó M là MDP gốc và M′ là MDP được potential-shaped. Ký hiệu cho hai MDP này ánh xạ đến ký hiệu của chúng tôi ở chỗ hàm phần thưởng của M là r, và chúng tôi cuối cùng suy dẫn rằng hàm phần thưởng của M′ là rA∗r.

11

--- TRANG 12 ---
Hệ quả 2 của Ng et al. bao gồm tuyên bố rằng, dưới một số điều kiện nhất định, đối với bất kỳ trạng thái s và hành động a nào, Q∗rA∗r(s, a) = Q∗r(s, a)−Φ(s).

Q∗rA∗r(s, a) =Q∗r(s, a)−Φ(s)
Q∗rA∗r(s, a) =Q∗r(s, a)−V∗r(s)
Q∗rA∗r(s, a) =A∗r(s, a)
max aQ∗rA∗r(s, a) =max aA∗r(s, a)
max aQ∗rA∗r(s, a) = 0
V∗rA∗r(s, a) = 0(9)

Phương trình 9 trên thiết lập hai điều sẽ được áp dụng trong Phương trình 10 dưới đây, rằng Q∗rA∗r(s, a) =A∗r(s, a) và rằng V∗rA∗r(s, a) = 0.

Q∗rA∗r(s, a) =rA∗r+γEs′[V∗˜r(s′)]
Q∗rA∗r(s, a) =rA∗r+γEs′[0]
Q∗rA∗r(s, a) =rA∗r
A∗r(s, a) =rA∗r(10)
□

D Thiết lập thực nghiệm chi tiết
Ở đây chúng tôi cung cấp chi tiết về các nhiệm vụ gridworld và các thuật toán học được sử dụng trong các thí nghiệm của chúng tôi. Các thuật toán học được mô tả bao gồm cả thuật toán để học từ sở thích và để cải tiến chính sách. Vì phần lớn các chi tiết dưới đây được lặp lại từ Knox et al. [2022], một số mô tả trong phần này được chuyển thể từ bài báo đó với sự cho phép của các tác giả.

D.1 Miền gridworld và tạo MDP
Miền gridworld Mỗi instance hóa của miền gridworld bao gồm một lưới các ô. Trong các phần sau, mỗi instance hóa miền gridworld được gọi thay thế như một MDP được tạo ra ngẫu nhiên.

Một ô có thể chứa tối đa một trong bốn loại đối tượng: đối tượng "hơi tốt", đối tượng "hơi xấu", đối tượng thành công cuối, và đối tượng thất bại cuối. Mỗi đối tượng có một thành phần phần thưởng cụ thể, và một hình phạt thời gian cung cấp một thành phần phần thưởng khác. Phần thưởng nhận được khi vào một ô là tổng của tất cả các thành phần phần thưởng. Trạng thái của agent giao hàng là vị trí của nó. Không gian hành động của agent bao gồm một bước đơn theo một trong bốn hướng chính. Episode có thể kết thúc hoặc tại một trạng thái thành công cuối cho một phần thưởng không âm, hoặc tại một trạng thái thất bại cuối cho một phần thưởng âm. Phần thưởng cho một chuyển tiếp không cuối là tổng của bất kỳ thành phần phần thưởng nào. Quy trình để chọn thành phần phần thưởng của mỗi loại ô được mô tả sau trong phần phụ này.

Các hành động sẽ di chuyển agent ra ngoài chu vi của lưới dẫn đến không có chuyển động và nhận được phần thưởng bao gồm thành phần phần thưởng hình phạt thời gian của ô hiện tại nhưng không có bất kỳ thành phần "hơi tốt" hoặc "hơi xấu" nào. Trong công việc này, phân phối trạng thái bắt đầu luôn ngẫu nhiên đồng đều trên các trạng thái không cuối. Miền này được giới thiệu bởi [Knox et al. 2022].

Chuẩn hóa return trên các MDP và định nghĩa hiệu suất gần tối ưu Để so sánh hiệu suất trên các MDP khác nhau, return trung bình của một chính sách π, Vπr, được chuẩn hóa thành (Vπr−VUr)/V∗r, trong đó V∗r là return tối ưu kỳ vọng và VUr là return kỳ vọng của chính sách ngẫu nhiên đồng đều (cả hai cho phân phối trạng thái bắt đầu ngẫu nhiên đồng đều). Return trung bình chuẩn hóa trên 0 tốt hơn VUr. Các chính sách tối ưu có return trung bình chuẩn hóa là 1, và chúng tôi xem xét trên 0.9 là gần tối ưu.

Ngoài ra, khi vẽ biểu đồ trung bình của những return chuẩn hóa này, chúng tôi làm sàn mỗi return như vậy tại -1, điều này ngăn chặn trung bình bị chi phối bởi các chính sách hiệu suất thấp không bao giờ kết thúc. Những chính sách như vậy có thể có, ví dụ,

12

--- TRANG 13 ---
return trung bình chuẩn hóa -1000 hoặc -10000, mà chúng tôi nhóm lại như một mức độ thất bại tương tự, tuy nhiên mà không có sàn tại -1, hai chính sách thất bại này sẽ có các hiệu ứng rất khác nhau lên trung bình.

[Hình 8: Một nhiệm vụ ví dụ và hàm phần thưởng không xác định từ miền giao hàng gridworld. Các nhiệm vụ từ miền này được sử dụng trong tất cả các thí nghiệm. Trong Phụ lục D.1, các đối tượng được mô tả là "hơi tốt" được hiển thị như đồng xu, các đối tượng được mô tả là "hơi xấu" được hiển thị như các chướng ngại vật màu cam, các đối tượng được mô tả là "trạng thái thất bại cuối" được hiển thị như cừu, và các đối tượng được mô tả là "trạng thái thành công cuối" được hiển thị như các dấu đích đỏ. Bề mặt gạch và đối tượng nhà không được sử dụng trong các thí nghiệm của chúng tôi. Hình ảnh gridworld được in lại với sự cho phép, từ Knox et al. [2022].]

Tạo ra các MDP ngẫu nhiên được sử dụng để tạo Hình 3, 4, và 6 Ở đây chúng tôi mô tả quy trình để tạo ra 100 MDP được sử dụng trong Hình 6, bao gồm 30 MDP được sử dụng trong Hình 3 và 4. Quy trình này cũng được sử dụng bởi [Knox et al. 2022].

Chiều cao cho mỗi MDP được lấy mẫu từ tập hợp {5,6,10}, và chiều rộng được lấy mẫu từ {3,6,10,15}. Tỷ lệ các ô chứa đối tượng thất bại cuối được lấy mẫu từ tập hợp {0,0.1,0.3}. Luôn có chính xác một ô với đối tượng thành công cuối. Tỷ lệ đối tượng "hơi xấu" được chọn từ tập hợp {0,0.1,0.5,0.8}, và tỷ lệ đối tượng "hơi tốt" được chọn từ {0,0.1,0.2}. Mỗi tỷ lệ được lấy mẫu được chuyển đổi thành một số lượng đối tượng (làm tròn xuống thành số nguyên khi cần), và sau đó mỗi loại đối tượng được đặt ngẫu nhiên trong các ô trống cho đến khi các tỷ lệ được thỏa mãn. Một ô có thể có không hoặc một đối tượng trong đó.

Sau đó thành phần phần thưởng ground-truth cho mỗi loại ô hoặc đối tượng trên được lấy mẫu từ các tập hợp sau:
• Đối tượng thành công cuối: {0,1,5,10,50}
• Đối tượng thất bại cuối: {−5,−10,−50}
• Đối tượng hơi xấu: {−2,−5,−10}

Đối tượng hơi tốt luôn có thành phần phần thưởng là 1. Một hình phạt thời gian không đổi -1 cũng luôn được áp dụng.

Tạo ra MDP ngẫu nhiên như được thấy trong hình 5 Đối với tất cả 90 MDP, các tham số sau đây được sử dụng. Chiều cao cho mỗi MDP được lấy mẫu từ tập hợp {3,5}, và chiều rộng được lấy mẫu từ {1,2}. Luôn có chính xác một ô cuối dương được đặt ngẫu nhiên trên một trong bốn góc của bảng. Thành phần phần thưởng ground-truth cho trạng thái cuối dương được lấy mẫu từ {0,1.5,10}. Những 90 MDP này không chứa bất kỳ ô "hơi tốt" hoặc "hơi xấu" nào.

Đối với 30/90 MDP, luôn tối ưu để cuối cùng kết thúc tại một ô thất bại cuối hoặc một ô thành công cuối:
• Đối với mỗi MDP có 50% cơ hội rằng một ô thất bại cuối tồn tại. Nếu nó tồn tại thì nó được đặt ngẫu nhiên trên một trong bốn góc của bảng.
• Thành phần phần thưởng ground-truth cho ô thất bại cuối được lấy mẫu từ {−5,−10}.
• Thành phần phần thưởng thực cho các ô trống luôn là −1.

Đối với 30/90 MDP, luôn tối ưu để cuối cùng kết thúc tại một ô thành công cuối:
• Đối với mỗi MDP luôn có một ô thất bại cuối tồn tại và được đặt ngẫu nhiên trên một trong bốn góc của bảng.
• Thành phần phần thưởng ground-truth cho ô thất bại cuối luôn là −10.
• Thành phần phần thưởng thực cho các ô trống luôn là −1.

Đối với 30/90 MDP, luôn tối ưu để lặp mãi mãi và không bao giờ kết thúc:
• Đối với mỗi MDP luôn có một ô thất bại cuối tồn tại và được đặt ngẫu nhiên trên một trong bốn góc của bảng.
• Thành phần phần thưởng ground-truth cho ô thất bại cuối luôn là −10.
• Thành phần phần thưởng thực cho các ô trống luôn là +1.

Tất cả các tham số để lấy mẫu ngẫu nhiên MDP mà không được thảo luận rõ ràng ở trên giống như đối với Hình 3, 4, và 6.

D.2 Thuật toán học
Nhân đôi tập huấn luyện bằng cách đảo ngược các mẫu sở thích Để cung cấp thêm dữ liệu huấn luyện và tránh học các hiệu ứng thứ tự đoạn, đối với tất cả các tập dữ liệu sở thích chúng tôi nhân đôi mỗi mẫu sở thích, hoán đổi các cặp đoạn tương ứng, và đảo ngược sở thích.

Chiết khấu trong value iteration và Q learning Mặc dù miền gridworld là episodic, một chính sách có thể vô tận tránh các trạng thái cuối. Trong một số MDP, như một tập con của những MDP được sử dụng trong Hình 5, đây là một hành vi tối ưu. Trong các MDP khác đây là kết quả của một chính sách hiệu suất thấp. Để tránh một vòng lặp vô hạn của các cập nhật hàm giá trị, chúng tôi áp dụng một hệ số chiết khấu γ= 0.999 trong value iteration, Q learning, và khi đánh giá return trung bình của các chính sách đối với hàm phần thưởng ground-truth, r. Chúng tôi chọn hệ số chiết khấu cao này để có hiệu ứng không đáng kể lên return của các chính sách hiệu suất cao (vì việc kết thúc tương đối nhanh được yêu cầu cho hiệu suất cao) trong khi vẫn cho phép hội tụ trong một thời gian hợp lý.

Siêu tham số để học bA∗r như được thấy trong Hình 3, 4, 5, và 10 Những siêu tham số này khớp chính xác với những siêu tham số được sử dụng trong [Knox et al. 2022], trừ việc chúng tôi giảm số lượng epoch huấn luyện. Đối với tất cả các thí nghiệm, mỗi thuật toán được chạy một lần với một seed được chọn ngẫu nhiên đơn.

• learning rate: 2
• số lượng seed được sử dụng: 1
• số lượng epoch huấn luyện: 1,000
• optimizer: Adam
–β1= 0.9
–β2= 0.999
–eps= 1e−08

Siêu tham số để học bA∗r như được thấy trong Hình 6 Những siêu tham số này khớp chính xác với những siêu tham số được sử dụng trong [Knox et al. 2022]. Đối với tất cả các thí nghiệm, mỗi thuật toán được chạy một lần với một seed được chọn ngẫu nhiên đơn.

• learning rate: 2
• số lượng seed được sử dụng: 1
• số lượng epoch huấn luyện: 30,000
• optimizer: Adam

14

--- TRANG 15 ---
–β1= 0.9
–β2= 0.999
–eps= 1e−08

Siêu tham số cho Q learning như được thấy trong Hình 6 Những siêu tham số này được điều chỉnh trên 10 hàm bA∗r được học trong đó việc đặt hàm phần thưởng như bA∗r và sử dụng value iteration để suy dẫn một chính sách được biết là cuối cùng dẫn đến hiệu suất tối ưu. Các siêu tham số được điều chỉnh sao cho, đối với mỗi hàm bA∗r trong tập hợp này, Q-learning cũng mang lại một chính sách tối ưu. Đối với tất cả các thí nghiệm, mỗi thuật toán được chạy một lần với một seed được chọn ngẫu nhiên đơn.

• learning rate: 1
• số lượng seed được sử dụng: 1
• số lượng episode huấn luyện: 1,600
• độ dài episode tối đa: 1000 bước
• giá trị Q ban đầu: 0
• quy trình khám phá: ϵ-greedy
–ϵ= 0.4
–decay= 0.99

Thông số kỹ thuật máy tính và thư viện phần mềm được sử dụng Tính toán được sử dụng cho tất cả các thí nghiệm có thông số kỹ thuật sau.
• bộ xử lý: 1x Core™ i9-9980XE (18 nhân, 3.00 GHz) & 1x WS X299 SAGE/10G — ASUS — MOBO;
• GPU: 4x RTX 2080 Ti;
• bộ nhớ: 128 GB.

Pytorch 1.7.1 [Paszke et al. 2019] được sử dụng để triển khai tất cả các mô hình học phần thưởng, và các phân tích thống kê được thực hiện sử dụng Scikit-learn 0.23.2 [Pedregosa et al. 2011].

E Dịch chuyển sao cho giá trị tối đa của cA∗r trong mọi trạng thái là 0

[Hình 9: Trong 90 MDP gridworld nhỏ, khi chúng tôi dịch chuyển rõ ràng bA∗r bởi một hằng số phụ thuộc vào trạng thái sao cho max abA∗r(·, a) = 0, chúng tôi quan sát thực nghiệm không có sự khác biệt giữa greedy bA∗r và greedy Q∗rcA∗r.]

Hệ quả 3.2 khẳng định rằng nếu max abA∗r(·, a) = 0, thì Π∗rcA∗r={π:∀s,∀a[π(a|s)>0⇔a∈argmaxabA∗r(s, a)]}. Hình 9 cho thấy kết quả của việc xác nhận thực nghiệm của chúng tôi về khẳng định này. Cụ thể, hình này cho thấy rằng khi maxabA∗r(·, a) = 0, chúng tôi quan sát không có sự khác biệt trong return trung bình chuẩn hóa giữa greedy bA∗r và greedy Q∗rcA∗r.

F Khuyến khích max abA∗r(·, a) = 0 mà không dịch chuyển thủ công các giá trị được học

Hình 4 trong Phần 3.3 sử dụng sở thích được tạo ra không nhiễu. Hình 10 trình bày một phân tích tương tự cho sở thích được tạo ra ngẫu nhiên. Mô hình tương tự như kết quả trong thiết lập không nhiễu, với thậm chí ít biến thiên hơn cho các tập huấn luyện lớn. Cụ thể, Hình 10 cho thấy việc bao gồm chuyển tiếp từ trạng thái hấp thụ di chuyển các giá trị tối đa kết quả của hàm lợi thế tối ưu xấp xỉ gần hơn đến 0.

15

--- TRANG 16 ---
[Hình 10: So sánh hiệu ứng lên greedy Q∗rcA∗r của việc bao gồm chuyển tiếp từ trạng thái hấp thụ. Đối với mỗi trạng thái trong 30 MDP, các biểu đồ trên cho thấy các giá trị max abA∗r(s, a). Biểu đồ cho thấy việc bao gồm các chuyển tiếp như vậy di chuyển các giá trị tối đa kết quả gần hơn đến 0. Biểu đồ này bổ sung cho Hình 4, có chứa một biểu đồ tương tự cho sở thích không nhiễu. Sau khi học với chuyển tiếp hấp thụ, max abA∗r(s, a) trên tất cả trạng thái gần 0 hơn về mặt ngẫu nhiên so với khi học không có chúng. Các kiểm nghiệm Wilcoxon paired signed-rank tại mọi kích thước tập huấn luyện trên 300 đều cực kỳ đáng kể với p <10−57. Đối với 300 sở thích, p= 0.0002.]

G Điều tra sự khác biệt hiệu suất

[Hình 11: Hiệu suất khi các tập dữ liệu sở thích được tạo ra ngẫu nhiên và có bao gồm các đoạn với chuyển tiếp từ trạng thái hấp thụ, bổ sung cho Hình 3. Kết quả trên 30 MDP gridworld được tạo ra ngẫu nhiên với các biểu diễn dạng bảng của bA∗r, trong đó các đoạn có độ dài 3 được chọn bằng cách chọn ngẫu nhiên đồng đều một trạng thái bắt đầu và 3 hành động của nó. Khi các chuyển tiếp từ trạng thái hấp thụ không được bao gồm, bất kỳ đoạn nào kết thúc trước chuyển tiếp cuối cùng của nó đều bị từ chối và sau đó được lấy mẫu lại. Biểu đồ này bổ sung cho Hình 4, cho thấy một biểu đồ tương tự cho sở thích không nhiễu. Đối với greedy bA∗r (màu đỏ) các kiểm nghiệm Wilcoxon paired signed-rank cho thấy việc bao gồm chuyển tiếp từ trạng thái hấp thụ dẫn đến hiệu suất cao hơn đáng kể cho tất cả kích thước tập huấn luyện trừ nhỏ nhất, 300, với p <0.02 cho 1000, và p <0.0002 cho các cái khác. Không phát hiện sự khác biệt đáng kể về hiệu suất cho greedy Q∗rcA∗r có hoặc không có chuyển tiếp kết thúc.]

Hình 3 trong Phần 3.3 sử dụng sở thích được tạo ra không nhiễu. Như trong phần trên, Hình 11 trình bày một phân tích tương tự cho sở thích được tạo ra ngẫu nhiên. Biểu đồ này cũng cho thấy rằng greedy Q∗rcA∗r được học mà không có chuyển tiếp từ trạng thái hấp thụ hoạt động kém. Chúng tôi cũng lưu ý rằng hiệu suất của ba điều kiện khác tương tự hơn so với Hình 3.

16
