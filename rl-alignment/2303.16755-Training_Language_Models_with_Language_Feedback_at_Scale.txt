# 2303.16755.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2303.16755.pdf
# File size: 1207195 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Training Language Models with Language Feedback at Scale
J´er´emy Scheurer1 2Jon Ander Campos1 3Tomasz Korbak1 2 4Jun Shern Chan1 2Angelica Chen1
Kyunghyun Cho1 5 6Ethan Perez1 2 7
Abstract
Pretrained language models often generate out-
puts that are not in line with human preferences,
such as harmful text or factually incorrect sum-
maries. Recent work approaches the above is-
sues by learning from a simple form of human
feedback: comparisons between pairs of model-
generated outputs. However, comparison feed-
back only conveys limited information about hu-
man preferences. In this paper, we introduce Im-
itation learning from Language Feedback (ILF),
a new approach that utilizes more informative
language feedback. ILF consists of three steps
that are applied iteratively: first, conditioning the
language model on the input, an initial LM out-
put, and feedback to generate refinements. Sec-
ond, selecting the refinement incorporating the
most feedback. Third, finetuning the language
model to maximize the likelihood of the chosen
refinement given the input. We show theoretically
that ILF can be viewed as Bayesian Inference,
similar to Reinforcement Learning from human
feedback. We evaluate ILF’s effectiveness on a
carefully-controlled toy task and a realistic sum-
marization task. Our experiments demonstrate
that large language models accurately incorporate
feedback and that finetuning with ILF scales well
with the dataset size, even outperforming finetun-
ing on human summaries. Learning from both
language and comparison feedback outperforms
learning from each alone, achieving human-level
summarization performance.
1New York University2FAR AI3HiTZ Center, Univer-
sity of the Basque Country UPV/EHU4University of Sus-
sex5Genentech6CIFAR LMB7Anthropic. Correspondence
to: J´er´emy Scheurer <jeremy.scheurer@nyu.edu >, Ethan Perez
<ethan@anthropic.com >.
Copyright 2023 by the author(s).
<B><Query><B><Query><B><Query>a...a lower-
case vowel.
eaAIs  incorporated?feedback
Make it lowercase.“A” is a vowel.Give an example of 
a lowercase vowel.
}Figure 1: To learn from language feedback on a language
model (LM) output, we have an LM generate multiple refine-
ments of the original output based on the feedback. We use
an LM to pick the best refinement and finetune the original
LM to maximize the likelihood of the chosen refinement.
1. Introduction
Language Models (LMs) achieve strong performance across
diverse NLP tasks, from summarization to question answer-
ing and dialog (Radford & Narasimhan, 2018; Radford et al.,
2019; Brown et al., 2020; Rae et al., 2021, inter alia ). One
of their key limitations, however, is that they generate text
that violates human preferences, such as misinformation
(Lin et al., 2021), offensive language (Gehman et al., 2020),
and factually incorrect summaries (Stiennon et al., 2020).
To alleviate such issues, existing methods train LMs to gen-
erate text that scores highly according to human preferences
or a predictive model thereof (Ziegler et al., 2019; Stien-
non et al., 2020; Nakano et al., 2021; Ouyang et al., 2022).
These approaches learn from human feedback regarding
which of two outputs is better. However, each comparison
only conveys limited information about human preferences.
We propose an alternative approach that learns from lan-
guage feedback, an information-rich and natural form ofarXiv:2303.16755v3  [cs.CL]  22 Feb 2024

--- PAGE 2 ---
Training Language Models with Language Feedback at Scale
c x1 c x0 f x1
Algorithm 1 Imitation Learning from Language Feedback
Input: number of iterations K, a sequence of sets of
source documents C= [C1, ...,CK], language model πθ,
refinement language model πψ, reward model R
forkin1...K do
Initialize finetuning dataset Dk={}
fordocument cinCkdo
x0∼πθ(x0|c)
Human provides feedback fon(c, x 0)
{x1
1, . . . , xN
1} ∼πψ(x1|c, x 0, f)
x1=argmaxxn
1R(xi
1|x0, f, c)
Add(c, x 1)toDk
end for
Update πθby supervised finetuning on Dk(as in Eq. 4)
end for
Figure 2: Top Left: The graphical model of the target dis-
tribution pθthat our algorithm approximates. cis a context
andx1is a high-quality LM output. Top Right: Graphical
model of the proposal distribution qfor importance sam-
pling. x0is an initial LM output and fis language feedback
onx0.Bottom: Pseudocode for our learning algorithm.
human feedback. We introduce Imitation learning from
Language Feedback (ILF), a 3-step algorithm for learning
from language feedback (Fig. 1). First, we generate multi-
ple refinements of an LM-generated output, given the input,
initial LM-generated output, and human-written feedback
on the output. Second, we use an instruction-finetuned LM
to choose the refinement that best incorporates the feedback.
Third, we finetune the LM that generated the initial output
on the chosen refinement given the input. In this way, we
finetune an LM using language feedback; with the resulting
model, we may then collect more feedback on its outputs
and learn with the above refine-and-finetune approach. The
algorithm’s pseudocode (Algorithm 1) and the correspond-
ing graphical model are shown in Fig 2. ILF departs from
prior work, which uses reinforcement learning (RL) (Ziegler
et al., 2019; Stiennon et al., 2020, inter alia ) or auxiliary
losses (Stacey et al., 2021) and cannot be straightforwardly
generalized to using free-form language feedback.
We analyze our approach both theoretically and empirically.
We show that ILF can be viewed as Bayesian Inference,
similar to RL with Human Feedback with KL penalties (Ko-
rbak et al., 2022). We then validate our algorithm on a
carefully-controlled synthetic task of removing offensive
words from a sentence with GPT-3-based models (Brown
et al., 2020; Ouyang et al., 2022). We find that only the
largest GPT-3-based models (175B parameters) accuratelyrefine outputs. Using this insight, we use the largest GPT-3
models to test our algorithm on text summarization, follow-
ing Stiennon et al. (2020). Our work extends our earlier
unpublished results (Scheurer et al., 2022), showing that
ILF improves LM-generated summaries monotonically with
the amount of feedback provided, testing up to 5k samples.
In all data regimes, ILF leads to comparable or better re-
sults to finetuning on human-written summaries, suggesting
our approach is a strong alternative to supervised learning
on human demonstrations. We also introduce an approach
for learning from both language and comparison feedback
by choosing the best-of-N samples from an ILF-trained
model using a model trained with comparison feedback.
The hybrid approach outperforms learning from each form
of feedback alone, leading to summaries that human evalu-
ators prefer over high-quality human reference summaries
∼50.8% of the time. Our analysis shows that LM-generated
refinements typically incorporate the feedback, especially
when we use an LM to choose the refinement that best incor-
porates the feedback. In our concurrent paper (Chen et al.,
2023), we show that ILF also achieves strong performance
on code generation. Our results suggest that language feed-
back is a promising avenue for learning human preferences.
2. Methods
We now formulate the problem setting and describe our
approach. We aim to generate improved outputs x1(e.g.,
high-quality summaries), according to human preferences,
given language feedback fon an initial model-generated
output x0, and a context c(e.g., a source document). We
tackle this problem by updating an LM πθbased on evidence
provided by language feedback.
Our goal is to sample a diverse set of high-quality outputs
x1given a context c(e.g., a summary of a document), where
cis drawn from the context distribution p(c). We do so by
fitting an autoregressive LM πθto approximate the ground-
truth distribution p∗
c(x1)which is proportional to the quality
ofx1, measured by the reward function R. Fitting πθcan
be written down as minimizing the expected KL-divergence
from the true distribution p∗
c(x1)toπθover the context
distribution p(c):
min
θEc∼p(c)KL(p∗
c, πθ), (1)
where p∗
c(x1)∝exp(βR(x1|c)).
Minimizing the objective in Eq. 1 equivalent to minimizing
the cross-entropy loss (i.e., supervised learning):
L(θ) =−Ec∼p(c)Lθ(c),
where Lθ(c) =X
x1p∗
c(x1) logπθ(x1|c).

--- PAGE 3 ---
Training Language Models with Language Feedback at Scale
It is intractable to compute this loss exactly for a number of
reasons, including the exponential size of the space of x1
as well as the intractability of computing the normalization
constant of p∗
c(x1). To avoid the first issue, we use Monte
Carlo approximation sampling using a small set of samples
drawn from p∗
c. Directly sampling from p∗
cis however still
intractable. We thus resort to using importance sampling
with a proposal distribution qc(x1)that is simpler to sample:
Lθ(c) =X
x1qc(x1)p∗
c(x1)
qc(x1)logπθ(x1|c) (2)
To minimize the variance, we must design qcto be as close
as possible to p∗
c. We achieve this goal by defining qcto in-
corporate human feedback that directly reflects the unknown
reward function R, in the process of sampling. We do so by
first drawing an initial output x0from a suboptimal LM πθ
given the context c. Second, we ask humans to rate x0and
provide language feedback fon the (c, x 0),pair. Third, a
refinement LM πψgenerates a refined output x1conditioned
on(c, x 0, f). The proposal distribution, corresponding to
this sampling procedure, can be written down as:
qc(x1) =X
f,x0πψ(x1|x0, f)p(f|x0)πθ(x0|c).
Letxi
1, . . . , xN
1beNsummaries sampled from qc(x1).
Then, we can approximate the objective in Eq. 2 as:
Lθ(c)≈NX
i=1p∗
c(xi
1)
qc(xi
1)|{z}
=ωilogπθ(xi
1|c), (3)
where ωiis the importance weight of the i-th sample from
qc. The importance weight ωiis not computable as it is
because we do not have access to qcother than being able to
draw samples from it. We avoid this issue by assuming that
qc(xi
1)is constant, implying that our samples are all equally
good due to the high quality of human feedback. We then
replace R(xi
1|c)in the definition of p∗
cbyR(xi
1|x0, f, c), as
the quality is not dependent on the intermediate summary
and feedback but can be more easily assessed with these
quantities. This allows us to compute the unnormalized p∗
c,
after which we use self-normalization to finally compute the
above loss.
We implement Rby conditioning an instruction-finetuned
LM on a binary question such as Does this new text [x1]in-
corporate the feedback [f]provided on the initial text [x0]?
Answer Yes or No. , where the label yis either ygood(“ Yes”)orybad(“ No”). We use the probability of the positive answer
ygoodasR, i.e.R(x1|x0, f, c) =p(ygood|prompt )
p(ygood|prompt )+p(ybad|prompt ).
Finally, we use an extremely low temperature when com-
puting p∗
c, i.e., β→ ∞ . Due to self-normalization, this is
equivalent to using only the best summary x∗
1per context
csampled from qcfor computing the loss, resulting in the
following, final objective:
L(θ)≈Ec∼p(c)logπθ(x∗
1|c) (4)
Our objective of approximating the ground truth distribu-
tionp∗
c(x1), which is proportional to the reward Rhas clear
connections to maximizing reward in RL. However, in RL,
the goal is to find the best policy that maximizes the re-
ward, whereas our algorithm results in a distribution of high-
quality outputs x1given a document c, which allows us to
draw a diverse set of outputs achieving a high reward. The
broad diversity of high-quality outputs endows downstream
users and systems with more control over which aspects they
prefer and want to avoid. In App. A.1, we further provide an
alternative derivation of ILF that follows variational infer-
ence and shows that ILF can also be understood as Bayesian
Inference. This process involves updating an LM based on
the evidence provided by language feedback. This different
lense highlights the correspondence between ILF and RL
with Human Feedback (Ziegler et al., 2019; Stiennon et al.,
2020, inter alia ), which was previously demonstrated to be
equivalent to Bayesian inference (Korbak et al., 2022).
3. Can Language Models Use Feedback?
For our algorithm to work, LMs must be able to accu-
rately incorporate feedback to generate refinements. Thus,
we first validate the refinement step of our algorithm on
a carefully-controlled synthetic task of removing specific
offensive words from a given sentence. We examine how ef-
fectively various models incorporate feedback to determine
what model to use for refining outputs.
Experimental Setup We instruct an LM to refine an
automatically-generated sentence with ≤10offensive
words by removing ≤3specific words (see Appendix D for
a detailed explanation and examples). In this experiment,
we generate one output per sample with greedy decoding,
i.e., we do not sample with best-of- N. We evaluate how
often the generated refinement exactly matches the target
sentence, which we automatically generate. For our LMs,
we use differently-sized GPT-3 models (Brown et al., 2020)
and text-davinci-001, their instruction-finetuned (Feedback
Made Easy or FeedME) counterparts (Ouyang et al., 2022;
OpenAI, 2022b).1We report all hyperparameters used in
1Via the OpenAI API.

--- PAGE 4 ---
Training Language Models with Language Feedback at Scale
ModelsAda
(-)Babbage
(1B)Curie
(6.7B)Davinci
(175B)
GPT-3 1.2±0.3 1 .7±0.4 8 .2±0.7 38 .5±1.3
FeedME 1.6±0.3 2 .2±0.4 6 .0±0.6 35 .8±1.3
Table 1: On the task of removing offensive words from a
sentence, only large LMs incorporate feedback. We report
the percentage of exact string matches with the target.
Appendix G. We report the mean and standard error for all
results in our work.
Results Table 1 shows the results. We observe that only
the largest GPT-3 and FeedME models (175B parameters)
incorporate feedback in a non-negligible amount of time.
Using this insight, we only use the 175B parameter models
in the rest of our experiments. Specifically, we use FeedME,
because it is an instruction-finetuned model.
4. Summarization from Language Feedback
Having established that large LMs can leverage language
feedback, we now evaluate our algorithm on the real-world
task of text summarization. In §4.1, we introduce a novel
summarization dataset that we use to evaluate our algorithm,
in§4.2, we explore different methods for ranking refine-
ments and in §4.3, we use the best ranking method to learn
from language feedback.
4.1. Summarization with Language Feedback Dataset
We evaluate the effectiveness of ILF on the task of text sum-
marization using the TL;DR dataset (V ¨olske et al., 2017),
which consists of Reddit titles, posts, and their correspond-
ing summaries. Stiennon et al. (2020) adapt this dataset and
show that it is a more realistic task for evaluating summa-
rization models compared to the commonly used CNN/DM
dataset (Hermann et al., 2015). To ensure the quality of our
dataset, we follow the same preprocessing steps as outlined
in Stiennon et al. (2020) and extract a train dataset with
5000 samples, a development dataset with 200 samples,
a validation dataset with 500 samples, and a test dataset
with 698 samples2. We then hire experienced annotators
through Surge AI3to create our language feedback dataset,
which we open source along with our code4. For each
sample, we first generate three summaries for each Reddit
post using the instruction-finetuned model text-davinci-001
(FeedME) (Ouyang et al., 2022; OpenAI, 2022b). Two of
2The train and development datasets are taken from Stiennon
et al. (2020)’s train dataset, and the validation and test set are taken
from their test dataset.
3https://surgehq.ai
4Data: HuggingFace; Code: GithubScoring FunctionWin Rate in % vs.
Random Selection
Task Specific
HeuristicMax Length 65.0±2.7
Zero-ShotEmbedding Similarity 48.3±3.0
InstructRM Prompt 1 55.0±3.0
InstructRM Prompt 2 58.0±2.9
InstructRM Prompt 3 56.5±2.9
InstructRM Prompt 4 55.8±2.8
InstructRM Prompt 5 50.0±3.0
InstructRM Ensemble 56.0 ±3.0
Table 2: We compare various ranking methods for selecting
refinements using a human evaluation. InstructRM Ensem-
ble performs best and is used throughout our paper.
these summaries are used for a binary comparison, in which
annotators indicate their preference. The third summary
serves as the initial output for which we solicit language
feedback. This feedback should address the single most
important shortcoming of the summary and can be related
to coverage (how well the summary covers the important
information in the post), accuracy (the factual accuracy of
the summary), coherence (the coherence of the summary
on its own), or other. We do not impose any restrictions on
how the feedback should be written. In addition to provid-
ing feedback, annotators are also asked to write an ideal
summary that is maximally 48 tokens long. The same
crowd worker annotates all three tasks for a given sample.
Overall the dataset collection and human evaluations cost
40K$. On selected samples of the binary comparison task,
we achieve an author-annotator agreement of 81.0%and
annotator-annotator agreement of 70.0%. The human sum-
maries we collect are of excellent quality, as demonstrated in
a human evaluation, where we compare our human-written
summaries to the ones automatically extracted from Reddit
(V¨olske et al., 2017) (also used as baselines in Stiennon et al.
(2020); Scheurer et al. (2022)). We find that our human-
written summaries are preferred 72.0±3.2%of the time,
making them a much stronger baseline.
4.2. Comparing Refinement Ranking Methods
Generating Refinements We condition FeedME on the
initial summaries of our train dataset (generated with
FeedME) and the human-written feedback and generate
5 refinements x1
1, ..., x5
1using the instructions in App. J.1.
Scoring Refinements with InstructRM We chose a re-
finement with a scoring function Rthat scores refinements
for how effectively they incorporate feedback. For Rwe use
the instruction-finetuned LM FeedME and ask it whether
a refinement is better than the initial summary (see §2 for
more details). We then evaluate the probability that the

--- PAGE 5 ---
Training Language Models with Language Feedback at Scale
refinement incorporates language feedback on the initial
summary and is accordingly a high-quality summary, i.e.,
p(ygood|prompt ). LMs are sensitive to the exact prompt
used (Perez et al., 2021; Lu et al., 2021), so we write 5 dif-
ferent prompts (see App. J.2) and select the refinement with
the highest average p(ygood|prompt )and call this method
InstructRM Ensemble.
Scoring Refinements with Embedding Similarity Pre-
vious work (Scheurer et al., 2022) use a contrastive pre-
trained text-embedding function (Neelakantan et al., 2022)
to embed the feedback fand refinements x1
1, ..., x5
1and se-
lect the refinement with the highest cosine similarity to the
feedback. They use this scoring function because feedback
would often describe what the ideal text should look like.
This method is less general because it assumes that good
refinements are semantically similar to the feedback, which
is not necessarily the case for all tasks or forms of feedback.
Results We now evaluate the above ranking methods
on the development dataset by calculating the fraction of
times the refinement selected by a method is better than a
randomly-selected refinement (“win rate”), according to a
ranking given by human evaluators (see App. E for more
details). The results, shown in Table 2, show that the embed-
ding similarity selection does not outperform random selec-
tion, while most (4/5) InstructRM prompts do. While the em-
bedding similarity worked well in previous work (Scheurer
et al., 2022), it does not perform well on our dataset. We
believe this is because the feedback we collect, written by
many annotators, is much more diverse, while in Scheurer
et al. (2022), the authors wrote the feedback themselves.
InstructRM Ensemble has a win rate of 56.0±3.0%against
random selection, demonstrating that an LM can evaluate
its own output to some extent. Based on these results, we
recommend using the InstructRM Ensemble approach, as it
performs well and is less sensitive to the particular prompt.
Throughout our paper, we use InstructRM Ensemble as
our scoring function to select refinements and refer to our
method of generating and selecting refinements as Refine-
ment with Feedback + Best of N .
4.3. Comparing Feedback Learning Algorithms
In this section, we compare various algorithms for learning
from language feedback, binary feedback, and normal super-
vised finetuning. We present an overview of each method
and then provide the results of our evaluations.
4.3.1. M ETHODS
Finetuning on Refinements (ILF) For this evaluation, we
use a single iteration of ILF to learn from language feedback.ILF + OPT-RM (best-of-64)
OPT-RM best-of-64 FeedME FeedME
ILF: Finetuned on Refinements Finetuned on Initial
Summaries Finetuned on Human Summaries
100 1000 5000
Train Dataset Size1520253035404550Win rate vs. 
 Human Summary (%)
Figure 3: How often human evaluators prefer summaries
from ILF, OPT-RM best-of-64 FeedME, ILF + OPT-RM
(best-of-64), finetuning baselines and FeedME to human
summaries. ILF + OPT-RM (best-of-64) generates sum-
maries of a similar quality to human summaries.
We finetune GPT3-175B (davinci) (Brown et al., 2020)5to
maximize the log-likelihood of the refinement given the
input prompt (consisting of the Reddit title, and post), i.e.,
logp(x1|prompt ), using the refinements generated with Re-
finement with Feedback + Best of N. For all our finetuning
methods we add λlogp(prompt )to the loss (Radford et al.,
2018; OpenAI, 2022a), which maximizes the log-probability
of the prompt. The prompt-loss weight λ∈[0,1]is cho-
sen on our development dataset (see paragraph Finetuning
on Human Summaries ). The selected hyperparameters are
detailed in App. G and the finetuning prompts in App. J.3.
Finetuning on Human Summaries Here we finetune
GPT3-175B on the dataset of human-written summaries
xhuman , with the objective of maximizing the log-probability
of human summaries given the input prompt (consisting
of the Reddit title and post) with the additional loss term,
i.e.logp(xhuman|prompt ) +λlogp(prompt ). To ensure the
best performance of our finetuned models, we conduct thor-
ough hyperparameter tuning on the human-written summary
datasets of various sizes (100, 1K, 5K). The hyperparame-
ters optimized include the number of training epochs, the
prompt loss weight λ, and the learning rate multiplier, as de-
tailed in the OpenAI documentation (OpenAI, 2022a). We
use the perplexity of the predicted summaries on the devel-
opment dataset to select the most effective hyperparameters.
The selected hyperparameters are applied to all datasets, i.e.,
finetuning on refinements, initial summaries, and human-
5FeedME cannot be finetuned via OpenAI’s API.

--- PAGE 6 ---
Training Language Models with Language Feedback at Scale
written summaries, with the same sample size. More details
on hyperparameter tuning can be found in Appendix G.
Finetuning on Initial Summaries We finetune GPT3-
175B on the dataset of initial summaries (generated by
FeedME). The objective is to maximize the log probabil-
ity of the initial summary given the prompt (consisting of
the Reddit title and post) with the additional loss term i.e.
logp(x0|prompt ) +λlogp(prompt ). Details on hyperpa-
rameter tuning can be found in the paragraph Finetuning on
Human Summaries and Appendix G.
Learning from Binary Feedback: Best-of- N We com-
pare ILF against binary feedback as a baseline, the standard
approach for learning from feedback. One way of learning
from binary feedback is to train a reward model and use it
to do best-of- Nsampling. We use best-of-N because it is
often competitive with RL from human feedback (Nakano
et al., 2021), a highly effective but more sophisticated ap-
proach (Stiennon et al., 2020; Ouyang et al., 2022). To train
the RM, we finetune OPT-13B (OPT-RM) (Zhang et al.,
2022) to classify whether a summary x0is high quality or
not. To do so, we use the instruction Is the above an excel-
lent summary of the given text? An excellent summary is
coherent, accurate, concise, and detailed. Answer with Yes
or No. , where the label yis either ygood(“ Yes”) or ybad(“
No”). Given human labels on which of two summaries is
preferred, we label the preferred summary with ygoodand
the other summary with ybad. We then finetune the LM to
maximize logp(y|x0) +λlogp(x0), where λ∈[0,1], cho-
sen using the development dataset, and y∈ {ygood, ybad}.
Using the finetuned LM, we evaluate a given summary by
computing p(ygood|x0)and select the summary with the
higher probability. We find that this approach leads to more
accurate RMs than other RM training methods, such as the
commonly used method from Stiennon et al. (2020); see
Appendix F for comparisons and Appendix J.4 for the used
prompts. We perform Bayesian hyperparameter optimiza-
tion for OPT-RM and sweep over the learning rate, batch
size, and prompt-loss weight λ, using classification accu-
racy on the development dataset as the selection criteria (see
Appendix G for more details).
ILF + Learning from Binary Feedback As a final step,
we combine ILF and learning from binary feedback, by first
finetuning GPT3-175B on the refinements as described in
the paragraph finetuning on refinements (ILF). We then train
the reward model, OPT-RM, and use it to perform best-of-
Nsampling, as outlined in the paragraph on learning from
binary feedback. At test time, we generate 64 summaries
with our finetuned model and rank them based on their prob-
ability of being a high-quality summary, pnorm(ygood|x0),
using OPT-RM. The summary with the highest normalized
probability is then selected.4.3.2. E VALUATION
We evaluate the effectiveness of our learning algorithm, by
comparing it to human written reference summaries, several
finetuning baselines, and OPT-RM on the task of text sum-
marization using 100, 1K, and 5K train samples. Using a
test dataset of 698 samples, we generate a summary for each
method and evaluate them with human evaluators who rank
them based on quality, using a standard ranking scheme that
allows for ties between summaries (see App. G for more
details). Based on the rankings, we calculate the fraction
of times each method’s sampled summary outperforms the
human-written reference summary, referred to as the “win
rate”. We sample summaries up to 48 tokens in length (as
in Stiennon et al. (2020)) using nucleus sampling (Holtz-
man et al., 2019) with p= 0.95and temperature t= 1.0
(see App. G for further details on hyperparameters and post-
processing). We use best-of-64 sampling with summaries
sampled from FeedME for learning from binary feedback.
4.3.3. R ESULTS
Our results, shown in Fig. 3, demonstrate that finetun-
ing on refinements (ILF) outperforms all other finetun-
ing methods6), including sampling from FeedME, with
a win rate against human summaries of 31.3±1.7%
(for finetuning on 5K samples), while the other methods
achieve win rates of 27.3±1.7%(finetuning on initial sum-
maries), 28.9±1.7%(finetuning on human summaries),
and22.5±1.6%(FeedME). It is surprising that ILF out-
performs finetuning on human summarise across all sample
sizes, despite human-written summaries generally being of
higher quality (see Fig. 4, top). Further evaluation (see App.
Fig. 8) shows that the model finetuned on 1K refinements
(ILF) exhibits significantly lower loss when evaluated on
the validation dataset of refinements compared to the model
finetuned on human summaries when evaluated on the val-
idation dataset of human summaries, suggesting that the
model is more adept at approximating the distribution of
refinements. Additionally, when evaluating GPT3-175B on
the summaries of 1K samples from various train datasets, we
observe significantly lower loss on the refinement dataset
than on the dataset of human summaries (see Table. 6).
Overall, these results demonstrate the effectiveness of our
proposed ILF approach in accurately incorporating feedback
and improving model performance, even outperforming fine-
tuning on human summaries.
(Scheurer et al., 2022) found that ILF with 100 feedback
samples outperformed FeedME, while here we find it under-
performs FeedME with 100 feedback samples. Prior work
uses author-written feedback that often conveys what the
refinement should include, while our work includes more
6Finetuning on 100 refinements is tied with finetuning on 100
initial summaries.

--- PAGE 7 ---
Training Language Models with Language Feedback at Scale
50 55 60 65 70 75 80 85
Win Rate vs. 
 Initial Summaries (%) Human
 Summaries Refinement
 (w/o
 Feedback) Refine w/
 Feedback Refine w/
 Feedback +
 Best-of-5
0 20 40 60
% of summaries 
 incorporating most important 
 feedback point Human
 Summaries Refinement
 (w/o
 Feedback) Refine w/
 Feedback Refine w/
 Feedback +
 Best-of-5
Figure 4: Top: Human evaluators prefer summaries from
all refinement methods to the initial summaries (FeedME).
Refine with Feedback + best-of-5 is rated highest. Bottom :
Refine with Feedback + best-of-5 generally does incorporate
the most important feedback point.
varied, crowdsourced feedback. As a result, we observe that
embedding similarity does not properly rank refinements on
our human feedback dataset (Table 2), and we believe the
difference in feedback may be a significant source of differ-
ences in results in this section as well; see Appendix H.5
for more discussion.
Our results demonstrate that using OPT-RM for best-of-64
sampling on FeedME summaries outperforms all finetuning
methods and sampling approaches across all sample sizes.
The improved performance of OPT-RM best-of-64 FeedME
comes at the cost of added inference time for best-of- N
sampling. Combining ILF and learning from binary feed-
back (ILF + OPT-RM (best-of-64)) achieves human-level
summarization performance with a win rate of 50.8±1.9%
using 5K samples for training. This suggests that both meth-
ods independently learn valuable information about human
preferences that can be cumulative when used together. It
should be noted that the result for ILF + OPT-RM (best-of-
64) is obtained through a separate human evaluation with
different comparison summaries (see App. Fig. 9), and
was added to Fig. 3 for reference. In App. H.3, we presentsome initial, promising results for multiple iterations of ILF.
These results suggest that the method is effective, but further
experimentation is necessary to understand it better.
4.4. Does Language Feedback Improve Refinements?
The improvements from ILF suggest that the refinements
used for finetuning are high-quality, so here we investi-
gate whether language feedback is responsible for the high
quality. To do so, we have human evaluators rank Refine-
ment with Feedback + Best of N summaries against sum-
maries from several other methods, similar to §4.2. We
use the human ranking to compute a win rate between each
method and the initial summary. We compare against Refine-
ment with Feedback, which randomly chooses a refinement
∈x1
1, . . . , x5
1. This ablation helps to evaluate the impor-
tance of choosing a refinement with our scoring function R,
i.e., InstructRM Ensemble. We also evaluate Refinement
without Feedback, which instructs the LM to refine the ini-
tial summary but without feedback. This ablation helps to
evaluate the importance of using language feedback. Lastly,
we evaluate Human Summaries and Initial Summaries i.e.,
the initial summary x0generated by FeedME. We evaluate
all methods on the validation dataset.
Results. Fig. 4 (top) shows the win rates of summaries
from various methods against initial summaries. Surpris-
ingly, instructing a model to improve its output without
feedback already leads to a significant improvement (win
rate of 59.4±2.1%over the initial summaries). Refinements
with Feedback achieve an improved win rate of 63.9±2.0%,
showing that language feedback is useful for improving re-
finement quality. Refinement with Feedback + Best of N
achieves an even better win rate of 69.1±1.9%, highlight-
ing that Best-of-N with the InstructRM Ensemble further
improves the refinements. Overall, language feedback is im-
portant for high-quality refinements, especially when using
Best-of-N sampling.
4.5. Do Refinements Incorporate the Feedback?
To determine whether refinements are of higher quality due
to incorporating feedback rather than improving the sum-
mary in other ways, we conduct a study on the validation
dataset in which crowd workers evaluate how often the most
important point of the feedback is incorporated in the refine-
ments produced by various methods. As shown in Fig. 4,
bottom, our method Refinement with Feedback + Best of N
incorporates the most important point in the feedback most
frequently ( 57.4±2.2%often). Refinement with Feedback
incorporates feedback 49.6±2.2%of the time, showing
that Best-of-N sampling improves how often the feedback is
incorporated. For reference, Refinement without Feedback
fixes the most important point in the feedback 30.8±2.1%
of the time, despite the model not receiving the language

--- PAGE 8 ---
Training Language Models with Language Feedback at Scale
feedback. Human Summaries address the most important
point in the feedback 74.0±1.9%of the time when writ-
ing the summary from scratch despite not receiving the
feedback explicitly. Our results suggest that refinements
are high-quality in part because they incorporate the most
important point in the feedback.
4.6. Which Finetuning Dataset Changes Models Most?
Here, we aim to understand how the summaries used for
finetuning influence how much the model changes after fine-
tuning. Gao et al. (2022) find that models optimized with
binary human feedback are more likely to learn undesirable
behaviors when their output distribution deviates more from
the initial, pretrained LM. It is unclear whether these find-
ings apply to models trained with language feedback, but
we take a preliminary step in this direction for understand-
ing language feedback-trained models. In particular, we
measure the (reverse) KL divergence (following Gao et al.,
2022) between an ILF-finetuned model and the pretrained
LM before ILF-training, DKL(finetuned |GPT3-175B ), by
unconditionally sampling from the finetuned model and
evaluating the log-likelihood of the generated text with
GPT3-175B . We also report the forward KL divergence,
DKL(GPT3-175B |finetuned ). For reference, we evaluate
both of the above for models finetuned on the initial sum-
maries and on human summaries.
Results. Finetuning on refinements (ILF) shows the
largest KL divergence (in both directions), followed by fine-
tuning on human summaries, and then followed by finetun-
ing on initial summaries; see App. Table 6 for the exact
numbers. We find it surprising that finetuning on refine-
ments results in higher KL divergences than finetuning on
human summaries; we expected the refinements to be closer
to the model’s initial output distribution, relative to human
summaries, therefore causing the finetuned model to un-
dergo less change. The larger KL divergence with ILF may
be partly responsible for the larger gains in human evalua-
tions observed in Fig. 3.
5. Related Work
Our work builds upon our previous report (Scheurer et al.,
2022), which showed that large LMs can refine outputs with
language feedback. There, we introduce the same three-step
algorithm that ILF builds upon, with the key difference that
here we use an LM, i.e., InstructRM Ensemble, to evalu-
ate whether a refinement incorporates feedback, whereas
in Scheurer et al. (2022) we use a contrastive pre-trained
text-embedding function (Neelakantan et al., 2022). In-
structRM Ensemble is more general than this Embedding
Similarity since it does not assume semantic similarity of
the refinements to the feedback. Another difference is thatILF is an iterative, refine-and-finetune algorithm, which
can be understood as Bayesian Inference corresponding to
RL with Human Feedback. In addition, here we conduct
different and more extensive experiments than in Scheurer
et al. (2022) and use human annotators. In particular, we
show that ILF outperforms finetuning on human summaries
and that combining ILF with learning from binary feedback
achieves roughly human-level summarization performance.
For a more detailed comparison to Scheurer et al. (2022) we
refer to App. H.5.
Subsequent work to ours suggests several ways to improve
upon our approach. Saunders et al. (2022) show that LMs
themselves write high-quality feedback on LM outputs. Bai
et al. (2022) then train a dialog assistant using ILF to learn
from LM-written language feedback, eliminating the cost
and effort of collecting human feedback. Liu et al. (2022);
Schick et al. (2022) train LMs to refine outputs based on
feedback (without finetuning on the refinements), an ap-
proach that improves results when incorporated into ILF, as
shown in subsequent work to ours (Shi et al., 2022).
Other work aims to use language in other ways than we do.
Some work investigates using explanations for gold labeled
outputs toclassification tasks , while our work addresses the
more general text generation setting which classification
tasks can be formulated as (Radford et al., 2019; Raffel
et al., 2020; Brown et al., 2020). Explanations describe why
a labeled output is correct, while feedback describes how to
improve a candidate’s output. Prior work explores ways of
using explanations to train text classification models, with
mixed results (Camburu et al., 2018; Stacey et al., 2021;
Pruthi et al., 2021; Wiegreffe et al., 2021; Hase & Bansal,
2021; Lampinen et al., 2022, inter alia ). A few prior works
also learn from language feedback for the purpose of ranking
candidate outputs rather than generating outputs (Weston,
2016; Li et al., 2016; Hancock et al., 2019; Li et al., 2022;
Xu et al., 2022). Matiana et al. (2021) learn text embeddings
of language feedback, where improvements could benefit
the refinement-scoring step of our algorithm. Language has
also been used for various purposes in RL settings as well,
as discussed in App. B.
Several other works draw connections between Bayesian
Inference and learning algorithms for LMs. Korbak et al.
(2022) show that KL-regularised RL is equivalent to varia-
tional inference: approximating a Bayesian posterior which
specifies how to update a prior LM to conform with evi-
dence provided by a reward function. Dohan et al. (2022)
further argues that the process of generating output through
multiple rounds of interaction between prompted LMs and
other agents (e.g. humans providing language feedback) can
be seen as executing probabilistic programs.

--- PAGE 9 ---
Training Language Models with Language Feedback at Scale
6. Conclusion
In this work, we propose Imitation learning from Language
Feedback (ILF), an iterative algorithm for training LMs to
behave in line with human preferences, by learning from
language feedback. We validate our approach on a carefully-
controlled word-removal task, showing that only large LMs
(175B parameters) accurately incorporate feedback. Using
this insight, we then test our algorithm on the real-world task
of text summarization. Combining ILF and learning from
binary feedback brought a GPT-3 model to roughly human-
level summarization ability. ILF on its own outperformed
finetuning on human summaries, despite human summaries
being of higher quality, suggesting that the model is better
at approximating the distribution of refinements. Our work
opens up many avenues for future work, from improving
algorithms for learning from language to tackling settings
where it is hard to learn from sparse or binary feedback.
7. Acknowledgements
We are grateful to Nat McAleese, Geoffrey Irving, Jeff
Wu, Jan Leike, Cathy Yeh, William Saunders, Jonathan
Ward, Sam Bowman, Daniel Ziegler, Seraphina Nix, Quintin
Pope, Kay Kozaronek, Peter Hase, Asa Cooper Stickland,
Jacob Pfau, David Lindner, Lennart Heim, Nitarshan Ra-
jkumar, Kath Lumpante, Pablo Morena, Edwin Chen, Scott
Heiner, and David Dohan for helpful conversations and
feedback. J ´er´emy Scheurer and Jun Shern Chan thank Open
Philanthropy for funding that enabled this research. Ethan
Perez thanks the National Science Foundation and Open
Philanthropy for fellowship support. Jon Ander Campos
is supported by a doctoral grant from the Spanish MECD.
Angelica Chen and Kyunghyun Cho are supported by the
NYU Center for Data Science National Science Foundation
(Award 1922658). KC was supported by 42dot, Hyundai
Motor Company (under the project Uncertainty in Neural
Sequence Modeling), Samsung Advanced Institute of Tech-
nology (under the project Next Generation Deep Learning:
From Pattern Recognition to AI), and NSF Award 1922658
NRT-HDR: FUTURE Foundations, Translation, and Re-
sponsibility for Data Science We also thank OpenAI for
providing access and credits to their models via the API
Academic Access Program.References
Andreas, J., Klein, D., and Levine, S. Modular multi-
task reinforcement learning with policy sketches. In
Precup, D. and Teh, Y . W. (eds.), Proceedings of
the 34th International Conference on Machine Learn-
ing, volume 70 of Proceedings of Machine Learn-
ing Research , pp. 166–175. PMLR, 06–11 Aug 2017.
URLhttps://proceedings.mlr.press/v70/
andreas17a.html .
Austin, J., Odena, A., Nye, M. I., Bosma, M., Michalewski,
H., Dohan, D., Jiang, E., Cai, C. J., Terry, M., Le, Q. V .,
and Sutton, C. Program synthesis with large language
models. CoRR , abs/2108.07732, 2021. URL https:
//arxiv.org/abs/2108.07732 .
Bai, Y ., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,
Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKin-
non, C., et al. Constitutional ai: Harmlessness from ai
feedback. arXiv preprint arXiv:2212.08073 , 2022.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems , 33:
1877–1901, 2020. URL https://arxiv.org/pdf/
2005.14165.pdf .
Camburu, O.-M., Rockt ¨aschel, T., Lukasiewicz, T., and
Blunsom, P. e-snli: Natural language inference with
natural language explanations. Advances in Neural In-
formation Processing Systems , 31, 2018. URL https:
//arxiv.org/pdf/1812.01193.pdf .
Chen, A., Scheurer, J., Korbak, T., Campos, J. A., Chan,
J. S., Bowman, S. R., Cho, K., and Perez, E. Improv-
ing code generation by training with natural language
feedback. arXiv preprint arXiv:2303.16749 , 2023.
Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D.,
Lopes, R. G., Wu, Y ., Michalewski, H., Saurous, R. A.,
Sohl-Dickstein, J., et al. Language model cascades. arXiv
preprint arXiv:2207.10342 , 2022.
Elgohary, A., Hosseini, S., and Hassan Awadallah, A. Speak
to your parser: Interactive text-to-SQL with natural lan-
guage feedback. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics , pp.
2065–2077, Online, July 2020. Association for Compu-
tational Linguistics. doi: 10.18653/v1/2020.acl-main.
187. URL https://aclanthology.org/2020.
acl-main.187 .
Fidler, S. et al. Teaching machines to describe images
with natural language feedback. Advances in Neural
Information Processing Systems , 30, 2017.

--- PAGE 10 ---
Training Language Models with Language Feedback at Scale
Gao, L., Schulman, J., and Hilton, J. Scaling laws for
reward model overoptimization, 2022. URL https:
//arxiv.org/abs/2210.10760 .
Gehman, S., Gururangan, S., Sap, M., Choi, Y .,
and Smith, N. A. Realtoxicityprompts: Eval-
uating neural toxic degeneration in language
models. arXiv preprint arXiv:2009.11462 , 2020.
URL https://aclanthology.org/2020.
findings-emnlp.301.pdf .
Goyal, P., Niekum, S., and Mooney, R. J. Using Natural Lan-
guage for Reward Shaping in Reinforcement Learning,
2019.
Hancock, B., Bordes, A., Mazare, P.-E., and Weston, J.
Learning from dialogue after deployment: Feed yourself,
chatbot! arXiv preprint arXiv:1901.05415 , 2019.
Hase, P. and Bansal, M. When can models learn from
explanations? a formal framework for understand-
ing the roles of explanation data. arXiv preprint
arXiv:2102.02201 , 2021. URL https://arxiv.
org/pdf/2102.02201.pdf .
Hermann, K. M., Kocisky, T., Grefenstette, E., Espeholt,
L., Kay, W., Suleyman, M., and Blunsom, P. Teaching
machines to read and comprehend. Advances in neural
information processing systems , 28, 2015.
Hilton, J. and Gao, L. Measuring goodhart’s law.
https://openai.com/blog/measuring-goodharts-law/, 2022.
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y .
The curious case of neural text degeneration. arXiv
preprint arXiv:1904.09751 , 2019. URL https://
arxiv.org/pdf/1904.09751.pdf .
Kaplan, R., Sauer, C., and Sosa, A. Beating Atari with
Natural Language Guided Reinforcement Learning, 2017.
Korbak, T., Perez, E., and Buckley, C. L. Rl with kl penalties
is better viewed as bayesian inference. arXiv preprint
arXiv:2205.11275 , 2022.
Lampinen, A. K., Dasgupta, I., Chan, S. C., Matthewson,
K., Tessler, M. H., Creswell, A., McClelland, J. L., Wang,
J. X., and Hill, F. Can language models learn from ex-
planations in context? arXiv preprint arXiv:2204.02329 ,
2022.
Li, J., Miller, A. H., Chopra, S., Ranzato, M., and Weston,
J. Dialogue learning with human-in-the-loop. arXiv
preprint arXiv:1611.09823 , 2016.
Li, Z., Sharma, P., Lu, X. H., Cheung, J. C., and Reddy,
S. Using interactive feedback to improve the accuracy
and explainability of question answering systems post-
deployment. arXiv preprint arXiv:2204.03025 , 2022.Lin, J., Fried, D., Klein, D., and Dragan, A. Infer-
ring rewards from language in context. arXiv preprint
arXiv:2204.02515 , 2022.
Lin, S., Hilton, J., and Evans, O. TruthfulQA: Measuring
How Models Mimic Human Falsehoods, 2021.
Liu, Y ., Deb, B., Teruel, M., Halfaker, A., Radev, D., and
Awadallah, A. H. On improving summarization fac-
tual consistency from natural language feedback. arXiv
preprint arXiv:2212.09968 , 2022.
Lu, Y ., Bartolo, M., Moore, A., Riedel, S., and Stenetorp,
P. Fantastically ordered prompts and where to find them:
Overcoming few-shot prompt order sensitivity. arXiv
preprint arXiv:2104.08786 , 2021.
Luketina, J., Nardelli, N., Farquhar, G., Foerster, J., An-
dreas, J., Grefenstette, E., Whiteson, S., and Rockt ¨aschel,
T. A survey of reinforcement learning informed by nat-
ural language. In Proceedings of the Twenty-Eighth In-
ternational Joint Conference on Artificial Intelligence,
IJCAI-19 , pp. 6309–6317. International Joint Confer-
ences on Artificial Intelligence Organization, 7 2019. doi:
10.24963/ijcai.2019/880. URL https://doi.org/
10.24963/ijcai.2019/880 .
Matiana, S., Smith, J., Teehan, R., Castricato, L., Biderman,
S., Gao, L., and Frazier, S. Cut the carp: Fishing for zero-
shot story evaluation. arXiv preprint arXiv:2110.03111 ,
2021.
Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L.,
Kim, C., Hesse, C., Jain, S., Kosaraju, V ., Saun-
ders, W., et al. WebGPT: Browser-assisted question-
answering with human feedback. arXiv preprint
arXiv:2112.09332 , 2021. URL https://arxiv.
org/pdf/2112.09332.pdf .
Neelakantan, A., Xu, T., Puri, R., Radford, A., Han, J. M.,
Tworek, J., Yuan, Q., Tezak, N., Kim, J. W., Hallacy,
C., Heidecke, J., Shyam, P., Power, B., Nekoul, T. E.,
Sastry, G., Krueger, G., Schnurr, D., Such, F. P., Hsu,
K., Thompson, M., Khan, T., Sherbakov, T., Jang, J.,
Welinder, P., and Weng, L. Text and Code Embeddings
by Contrastive Pre-Training, 2022.
Nguyen, K. X., Misra, D., Schapire, R., Dud ´ık, M., and
Shafto, P. Interactive learning from activity description.
InInternational Conference on Machine Learning , pp.
8096–8108. PMLR, 2021.
OpenAI. Openai finetuning documentation.
https://beta.openai.com/docs/api-reference/fine-
tunes/create, 2022a.
OpenAI. Model index for researchers.
https://beta.openai.com/docs/model-index-for-
researchers, 2022b.

--- PAGE 11 ---
Training Language Models with Language Feedback at Scale
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K.,
Ray, A., et al. Training language models to follow
instructions with human feedback. Preprint , 2022.
URL https://cdn.openai.com/papers/
Training_language_models_to_follow_
instructions_with_human_feedback.pdf .
Perez, E., Kiela, D., and Cho, K. True few-shot learning
with language models. Advances in Neural Information
Processing Systems , 34:11054–11070, 2021.
Pruthi, D., Bansal, R., Dhingra, B., Soares, L. B., Collins,
M., Lipton, Z. C., Neubig, G., and Cohen, W. W. Evalu-
ating Explanations: How much do explanations from the
teacher aid students?, 2021.
Qi, P., Zhang, Y ., Zhang, Y ., Bolton, J., and Manning, C. D.
Stanza: A Python Natural Language Processing Toolkit
for Many Human Languages. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics: System Demonstrations , 2020.
Radford, A. and Narasimhan, K. Improving Lan-
guage Understanding by Generative Pre-Training,
2018. URL https://openai-assets.
s3.amazonaws.com/research-covers/
language-unsupervised/language_
understanding_paper.pdf .
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.,
et al. Improving language understanding by generative
pre-training, 2018.
Radford, A., Wu, J., Child, R., Luan, D., Amodei,
D., and Sutskever, I. Language Models are
Unsupervised Multitask Learners, 2019. URL
https://d4mucfpksywv.cloudfront.net/
better-language-models/language_
models_are_unsupervised_multitask_
learners.pdf .
Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoff-
mann, J., Song, F., Aslanides, J., Henderson, S., Ring,
R., Young, S., et al. Scaling language models: Meth-
ods, analysis & insights from training gopher. arXiv
preprint arXiv:2112.11446 , 2021. URL https://
arxiv.org/pdf/2112.11446.pdf .
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the
Limits of Transfer Learning with a Unified Text-to-Text
Transformer, 2020.
Rupprecht, C., Laina, I., Navab, N., Hager, G. D., and
Tombari, F. Guide me: Interacting with deep networks.
InProceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pp. 8551–8561, 2018.Saunders, W., Yeh, C., Wu, J., Bills, S., Ouyang, L., Ward, J.,
and Leike, J. Self-critiquing models for assisting human
evaluators. arXiv preprint arXiv:2206.05802 , 2022.
Scheurer, J., Campos, J. A., Chan, J. S., Chen, A., Cho,
K., and Perez, E. Training language models with lan-
guage feedback. In The First Workshop on Learning with
Natural Language Supervision at ACL , 2022.
Schick, T., Dwivedi-Yu, J., Jiang, Z., Petroni, F., Lewis,
P., Izacard, G., You, Q., Nalmpantis, C., Grave, E., and
Riedel, S. Peer: A collaborative language model. arXiv
preprint arXiv:2208.11663 , 2022.
Shi, W., Dinan, E., Shuster, K., Weston, J., and Xu, J. When
life gives you lemons, make cherryade: Converting feed-
back from bad responses into good labels. arXiv preprint
arXiv:2210.15893 , 2022.
Stacey, J., Belinkov, Y ., and Rei, M. Supervising Model
Attention with Human Explanations for Robust Natural
Language Inference. arXiv preprint arXiv:2104.08142 ,
2021. URL https://arxiv.org/pdf/2104.
08142.pdf .
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R.,
V oss, C., Radford, A., Amodei, D., and Christiano,
P. F. Learning to summarize with human feedback.
Advances in Neural Information Processing Systems ,
33:3008–3021, 2020. URL https://arxiv.org/
pdf/2009.01325.pdf .
Sumers, T. R., Ho, M. K., Hawkins, R. D., Narasimhan,
K., and Griffiths, T. L. Learning rewards from linguistic
feedback. feedback , 1(2):3, 2021.
Tam, A. C., Rabinowitz, N. C., Lampinen, A. K., Roy,
N. A., Chan, S. C., Strouse, D., Wang, J. X., Banino,
A., and Hill, F. Semantic exploration from language ab-
stractions and pretrained representations. arXiv preprint
arXiv:2204.05080 , 2022.
V¨olske, M., Potthast, M., Syed, S., and Stein, B. TL;DR:
Mining Reddit to learn automatic summarization. In
Proceedings of the Workshop on New Frontiers in
Summarization , pp. 59–63, Copenhagen, Denmark,
September 2017. Association for Computational Lin-
guistics. doi: 10.18653/v1/W17-4508. URL https:
//aclanthology.org/W17-4508 .
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,
Le, Q., and Zhou, D. Chain of thought prompting elic-
its reasoning in large language models. arXiv preprint
arXiv:2201.11903 , 2022.
Weston, J. E. Dialog-based language learning. Advances in
Neural Information Processing Systems , 29, 2016.

--- PAGE 12 ---
Training Language Models with Language Feedback at Scale
Wiegreffe, S., Marasovi ´c, A., and Smith, N. A. Measuring
association between labels and free-text rationales. In
Proceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , pp. 10266–10284,
Online and Punta Cana, Dominican Republic, November
2021. Association for Computational Linguistics. doi:
10.18653/v1/2021.emnlp-main.804. URL https://
aclanthology.org/2021.emnlp-main.804 .
Xu, J., Ung, M., Komeili, M., Arora, K., Boureau, Y .-L.,
and Weston, J. Learning new skills after deployment:
Improving open-domain internet-driven dialogue with
human feedback. arXiv preprint arXiv:2208.03270 , 2022.
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V .,
et al. Opt: Open pre-trained transformer language models.
arXiv preprint arXiv:2205.01068 , 2022.
Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Rad-
ford, A., Amodei, D., Christiano, P., and Irving, G.
Fine-tuning language models from human preferences.
arXiv preprint arXiv:1909.08593 , 2019. URL https:
//arxiv.org/pdf/1909.08593.pdf .

--- PAGE 13 ---
Training Language Models with Language Feedback at Scale
A. Additional derivations
A.1. Imitation Learning from Language Feedback as Bayesian Inference
Language Feedback as Variational Inference Our goal is to produce a high-quality output x1for a context c∼p(c)(e.g.,
a summary of a document). We use an LM πθto generate an output x1, by conditioning on the context c, i.e.,x1∼pθ(x1|c).
We then introduce the predicate I, a random variable such that I= 1 if the output is high quality according to human
preferences. We denote this data-generating process, shown in Fig. 5 left, as:
pθ(c, x 1,I) =p(c)πθ(x1|c)p(I|c, x 1). (5)
We frame our goal as maximizing the marginal log probability of quality across contexts: Ec∼p(c)logp(I= 1|c).For a
particular context c, we approximate logp(I= 1|c)by introducing an importance sampling proposal distribution q(x1|c)
and using the Evidence Lower Bound (ELBo):
logp(I= 1|c) = logX
x1pθ(x1,I= 1|c) (6)
≥X
x1q(x1|c) logpθ(x1,I= 1|c)
q(x1|c)(7)
We maximize the lower bound in Eq. 6, henceforth called F(θ, q), using an Expectation-Maximization (EM) procedure:
alternating between maximizing Fw.r.t. the proposal distribution q(E-step) and w.r.t. πθ(M-step) We call this algorithm
Imitation learning from Language Feedback.
E-step Maximizing F(θ, q)w.r.tqcorresponds to refining the proposal distribution qto assign higher likelihood to
high-quality texts. This is achieved by embedding x1into a data-generating process involving humans, by introducing the
initial output x0, and human feedback f(via sum rule):
q(x1|c) =X
x0,fpθ(x0, f, x 1|I= 1, c) (8)
∝X
x0,fpθ(x0, f, x 1|c)pθ(I= 1|c, x 0, f, x 1) (9)
=X
x0,fpθ(x0|c)p(f|c, x 0)pθ(x1|c, x 0, f)
pθ(I= 1|c, x 0, f, x 1). (10)
Eq. 10 gives rise to the following sampling procedure (see also Fig. 5, right): First, an LM is conditioned on the context
cand generates an initial output x0. Second, a human provides language feedback fon the (c, x 0)pair. Third, the LM
generates a refined text x1conditioned on (c, x 0, f). Finally, a binary variable Iindicates whether x1is a high-quality text,
given an initial output x0, feedback f, and a context c. We model pθ(I= 1|c, x 0, f, x 1)as a Boltzmann distribution:
pθ(I= 1|c, x 0, f, x 1)∝exp(R(c, x 0, f, x 1)/β), (11)
which uses a reward function Rdefined in terms of four variables: c, x 0, f, x 1;βis a temperature hyperparameter. This
Boltzmann distribution makes quality easy to evaluate since it expresses it as a reward function Rof a previous output and
human language feedback.
c x1 I c x0 f x1 I
Figure 5: Left: The graphical model of the target distribution pθthat our algorithm approximates. cis a context and x1is a
high-quality LM output and Iindicates whether the output is high-quality according to human preferences. Right: The
graphical model of the proposal distribution qwe use for importance sampling. x0is an initial LM output and fis language
feedback on x0.

--- PAGE 14 ---
Training Language Models with Language Feedback at Scale
We now argue why the E-step results in a proposal distribution that is better than the original distribution pθ(x1|c), i.e.,
why samples from q(x1|c)tend to be of higher quality than samples from pθ(x1|c). First, we know that x0is already a
reasonably good output (since πθold≈πθ). We can assume that the feedback fis informative and high-quality. Therefore
x1∼pθ(x1|c, x 0, f)is going to be of higher quality than x0∼pθ(x0|c)because it leverages useful information from the
feedback. Furthermore, let us choose Rto assign higher values to refined texts x1that improve upon x0w.r.t to fand
c. Consequently, Eq. 11 assigns a higher likelihood to high-quality outputs x1, allowing us to put additional weight on
high-quality outputs and improving the proposal distribution qfurther.
M-step Maximizing F(θ, q)w.r.t. the policy πθis equivalent to supervised learning (minimizing cross-entropy loss) on a
distribution defined by q. To see that, we drop all the terms from Eq. 7 that do not depend on θ:
argmax
θF(θ, q) = argmax
θEx1∼q(x1|c)logpθ(x1,I= 1|c)
= argmin
θEx1∼q(x1|c)−logπθ(x1|c). (12)
ILF: Imitation learning from Language Feedback In ILF, we alternate between the E-step and M-step, using the
pseudocode in Algorithm 1. In the M-step, we use the model from the previous iteration πθoldas both pθ(x0|c)and
pθ(x1|c, x 0, f). In practice, we implement Rby conditioning an instruction-finetuned LM on a binary question such as
Does this new text incorporate the feedback provided? Answer Yes or No. where the label yis either ygood(“ Yes”) or
ybad(“ No”). We use the probability of the positive answer ygoodgiven the prompt as a reward, i.e. p(ygood|prompt ) =
p(ygood|prompt )
p(ygood|prompt )+p(ybad|prompt ). With these assumptions, qtakes the form:
q(x1|c)∝Ex0∼πθold(x0|c)Ef∼p(f|c,x0)
πθold(x1|c, x 0, f) exp( R(c, x 0, f, x 1)/β).
We take advantage of this proposal distribution and perform the M-step, i.e., argmaxθF(θ, q)on optimized data. Finally,
we approximate sampling from q(x1|c)by best-of- Nsampling. To obtain a sample x1∼q, we sample Nrefinements
{x1
1, . . . , xN
1} ∼πθold(x1|c, x 0, f), and compute
x1=argmaxxi
1expR(c, x 0, f, xi
1).
In summary, we show that ILF can be understood as Bayesian inference. This process involves updating an LM based on
the evidence provided by language feedback. This lens highlights the correspondence between ILF and RL with Human
Feedback (Ziegler et al., 2019; Stiennon et al., 2020, inter alia ), which was previously demonstrated to be equivalent to
Bayesian inference (Korbak et al., 2022).
B. Additional Related Work on Language in RL Settings
Language has been widely used in RL for various purposes (see Luketina et al., 2019, for an overview), such as specifying
tasks (“instruction following” , inter alia ) driving exploration (Tam et al., 2022), inferring reward functions (Lin et al., 2022;
Sumers et al., 2021; Fidler et al., 2017, inter alia ), and training a model via strong supervision (Andreas et al., 2017; Kaplan
et al., 2017), reward shaping (Goyal et al., 2019), or by providing descriptions of trajectories (Nguyen et al., 2021). In
contrast, we use language to correct faulty behavior. Other work uses language feedback at test time to correct mistakes in a
model’s behavior, e.g., image segmentation (Rupprecht et al., 2018) or code generation (Elgohary et al., 2020; Austin et al.,
2021). In contrast, we use feedback to train models, and our approach does not require human intervention at test time.
C. Dataset Collection and Analysis
Annotation process To ensure the high quality of our human annotations, we employ experienced annotators sourced
through the data-labeling company Surge AI. During an onboarding and evaluation process, we calculate author-annotator
agreement on the binary comparison task and manually review the quality of the written feedback and ideal summaries to
ensure their high quality. Then we select 31 qualified annotators for all annotation tasks, though they can choose which tasks
to participate in and for how long. To further ensure the quality of our annotations, we provide detailed instructions, which

--- PAGE 15 ---
Training Language Models with Language Feedback at Scale
we provide to the annotators, and update throughout the process to ensure continuous improvement (these instructions can
be found in Appendix I). To measure the agreement rate between the annotators and the authors, we select a sample of 10
Reddit posts from the training dataset as a gold standard and have 17 annotators label them. When comparing the binary
comparison annotations with our own ones, this results in an author-annotator agreement rate of 81.0%. We also calculate
the average agreement rate between all the possible annotator combinations, yielding an annotator-annotator agreement
of70%. By utilizing these thorough processes and evaluations, we can ensure the accuracy and reliability of our human
annotations.
Dataset Analysis The feedback we collect typically addresses the most critical shortcomings of the summaries. In 92.0%
of our train samples, the annotators’ feedback was complete and addressed all important shortcomings of the summary,
as reported by the annotators. Across our train dataset, we observe that the majority of the feedback pertains to coverage
(77.0%), with smaller percentages relating to accuracy ( 16.0%), coherence ( 5.0%), and other categories ( 2.0%). We also
analyze the length of the various summaries and feedback, measured in the average number of tokens. Our human-written
summaries have an average length of 41.0±0.1tokens, the extracted human summaries from Reddit had an average length
of32.5±0.1tokens, the initial summaries generated by FeedME have an average length of 29.3±0.1tokens, and the
feedback written by annotators on these initial summaries has an average length of 20.4±0.2tokens.
In addition to these analyses, we also measure the time it takes annotators to complete various tasks (i.e., binary comparison,
feedback writing, and ideal summary writing) on our development dataset. We ignore outliers and consider only samples
with annotation times of at least 20 seconds and at most 420 seconds (7 minutes). Annotators take 61.5±5.3seconds on
average on the binary comparison task, 182.5±6.3seconds on the feedback task, and 195.5±6.1seconds on the ideal
summary task. We plot the annotation times on the development dataset for the tasks of annotating binary comparisons,
writing feedback, and writing ideal summaries as histograms in Fig. 6. The annotators are much faster at annotating binary
comparisons than feedback or ideal summaries. Writing feedback takes less time than writing ideal summaries, which is
expected, as critiquing a task is usually easier than solving it. These comprehensive evaluations demonstrate the high quality
and thoroughness of our dataset and annotation processes.
100 200 300 400051015202530Bin countMedian Time: 61.50Binary Comparison Annotations
100 200 300 400
Annotation Time (seconds)024681012Median Time: 182.50Feedback Annotations
50 100 150 200 250 300 350 40002468Median Time: 195.50Human Summary Annotations
Figure 6: Histogram Plot of annotation times (in seconds) of the binary comparison task, the feedback annotation task and
the human summary writing task. The evaluation is conducted on the development dataset. We observe that annotators are
much quicker at the binary comparison task, which is expected. The results also show that writing feedback takes less time
than writing an ideal summary.
D. Targeted Word Removal Details
Below is an example of how we instruct or “prompt” an LM to remove specific, offensive words from a sentence.
“In this text, many toxic and offensive words are used: You are such a jerk, and a nice person, and an idiot. The
ideal text should remove the word jerk, but otherwise be unchanged: You are”
Here, the target completion is “ such a nice person and an idiot. ” More formally, we sample offensive sentences by using k
offensive words from a fixed set of 25 offensive words drawn uniformly at random (without replacement). Each offensive
sentence also includes the words ”nice person” in addition to all the offensive words. For each k∈ {1, . . . , 10}, we sample

--- PAGE 16 ---
Training Language Models with Language Feedback at Scale
50 offensive sentences. The task is then to remove l∈[1,2,3]offensive words from a given sentence with k≥l. Since
we include the words ”nice person” in the offensive sentence, we can remove l=koffensive words and still have a target
sentence that intuitively makes sense.
E. Details about Ranking Procedure
We use a standard ranking scheme where each of Ksummaries is given a rank between 1 and K(inclusive). Sometimes
refinements are exact copies of the initial summaries or are very similar in terms of quality, which is why we allow for
summaries to be tied. When calclating the win rate we assign 0.5wins for tied samples. We assign the rank r′to all
summaries ranked in a tie, where r′=r+(r+n−1)
2,ris the rank of the tied elements, and nis the number of ties at the rank.
For example, we map a ranking of (1,2,2,4,5)→(1,2.5,2.5,4,5)and a ranking of (1,2,3,3,3)→(1,2,4,4,4).
F. Reward Model
Here we describe the various RMs that we evaluate in more detail. We evaluate the final RM that we use, which produces a
language output (e.g., “ Yes” or “ No”) and a standard reward model that produces a scalar output.
Standard RM. Akin to (Stiennon et al., 2020), we remove the last embedding layer of a language model and train it to
output a scalar value. This scalar value predicts which summary, x∈ {x0
0, x1
0}, is better as judged by a human, given a
context c. We use the OPT 13B LM, introduced in (Zhang et al., 2022), as the base model for our RM and finetune it on
the human preference comparisons that we collected. It is worth noting that it is not possible to add linear layers on top of
GPT-3 models provided via the API, which is why we use the OPT model.
Reward Model with Language Output. In addition to the classic RM (Stiennon et al., 2020), we train an RM to output
language tokens instead of a scalar value. To do so, we finetune an LM to classify whether a summary x0is high quality or
not, by training it to predict a label y∈ {ygood, ybad}. We then finetune the LM to maximize λlogp(x0) + log p(y|x0),
where λ∈[0,1], chosen using the development dataset. The complete loss can also be written as:
L(pθ, x, y) =−λ·|x|X
t=1logpθ(xt|x<t)−|y|X
t=1logpθ(yt|x, y<t).
where the subscript tindicates the token index. We evaluate the finetuned LM on a given summary x0by computing
p(ygood|x0). The best RM overall uses the following instruction Is the above an excellent summary of the given text? An
excellent summary is coherent, accurate, concise, and detailed. Answer with Yes or No. , which we refer to as the OPT-RM
(when finetuning OPT-13B) and GPT-3 Binary (when finetuning GPT-3-175B). We also explore finetuning on another
prompt, where we provide both summaries AandBto the LM and instruct it to indicate which summary is preferred, i.e.
Question: Which summary is the better one? An excellent summary is coherent, accurate, concise, and detailed. Answer
with A or B. We then finetune the LM on the label of the preferred summary (according to binary human feedback), i.e.
ony∈ {yA, yB}. We evaluate the finetuned LM on a given summary x0by computing p(yA|x0). We refer to this RM as
Comparison RM. We explore two RMs, namely, OPT-13B Zhang et al. (2022), and GPT-3-175B and refer to Appendix G
for the hyperparameters we use and to Appendix J.4 for the prompt templates).
Results. We evaluate all RMs on our validation dataset, and calculate the accuracy of predicting the preferred summary
out of two, based on human preferences. Table 4 shows the complete results, and here we report on some of the RMs trained
on 5K samples. The OPT model with the standard RM loss achieves an accuracy of 71.8±2.0%on the validation dataset.
The results further show that both of our methods for training OPT with the LM loss outperform the standard RM loss,
with OPT comparison achieving an accuracy of 72.6±1.9%, and OPT-RM an accuracy of 73.4±1.9%. We obtain similar
results with finetuning GPT-3-175B, achieving an accuracy of 71.2±2.0%with the GPT3 Comparison, and an accuracy of
74.2±2.0%with GPT-3 Binary, which outperforms the OPT-RM.
Based on these results, we further evaluate the OPT Binary and GPT-3-175B Binary models on the development dataset that
we use to evaluate the scoring functions in §4.2. We calculate the fraction of times the refinement selected by an RM is better
than a randomly-selected refinement (“win rate”), according to a ranking given by human evaluators (see App. E for more
details). The results can be found in Table 3. OPT-RM achieves a win rate of 63.3±2.7%, and the GPT-3-175B Binary

--- PAGE 17 ---
Training Language Models with Language Feedback at Scale
Scoring Function Win Rate vs Random Selection (in %)
Task Specific Heuristic Max Length 65.0±2.7
Zero-ShotEmbedding Similarity 48.3±3.0
InstructRM Ensemble 56.0 ±3.0
Finetuning on 5K samplesOPT Binary 63.3 ±2.7
GPT-3 Binary 61.8±2.9
Table 3: In a human evaluation, we compare reward models and ranking methods on the development dataset (in the same
way as in Fig 2. Both RMs are trained on 5K samples and outperform the zero-shot methods.
Models # Params Train Data Size Development Accuracy (in %) Validation Accuracy (in %)
LM Loss / Our datasetOPT Comparison 13B 5K 66.5±3.3 72 .6±1.9
OPT RM 1.3B 5K 70.0±3.2 69 .6±2.0
OPT RM 13B 100 54.5±3.5 53 .4±2.2
OPT RM 13B 1K 68.5±3.2 67 .2±2.1
OPT RM 13B 5K 69.5 ±3.2 73.4 ±1.9
GPT-3 Comparison - 5K 68.0 71.2±2.0
GPT-3 Binary - 5K - 74.2±2.0
RM Loss / Our dataset OPT 13B 5K 68.5±3.2 71 .8±2.0
RM Loss / Stiennon et al. (2020) train dataset Stiennon et al. (2020) RM 1.3B 64K 58.0±3.4 63 .8±2.1
LM Loss / Stiennon et al. (2020) train dataset OPT Binary 13B 90K 69.0±3.2 68 .6±2.0
Table 4: In a human evaluation, we evaluate various RMs on the development dataset and validation dataset. We also report
the results of training on the train dataset of Stiennon et al. (2020) and evaluating on our development and validation datasets.
We calculate the accuracy of predicting which of two summaries is preferred by a human.
model achieved a win rate of 61.8±2.9%. In this evaluation, OPT-RM outperforms GPT-3 Binary. When considering the
results from both the validation and development datasets, both OPT-RM and GPT-3-Binary seem to perform similarly.
Given that we have more control over the training process of OPT, the possibility of releasing the model, and the cost
involved in training using OpenAI’s API, we select OPT-RM model as our reward model for comparison with ILF. In
Figure 7, we show the validation accuracy of OPT-RM trained on 100, 1K, and 5K samples on a log-log plot. The figure
shows scaling when increasing the dataset size.
We further evaluate results for finetuning OPT-RM on the dataset of Stiennon et al. (2020), and also evaluating their model
with 1.3B parameters on our dataset. We observe that the binary preference distribution of the training dataset has a
significant impact on the performance of the reward model. For example, OPT-RM trained on 5K samples of our own train
dataset (i.e., our final reward model) achieves an accuracy of 61.9±0.2%on the test set from Stiennon et al. (2020) (not
shown in Table 4). When this same model is trained on 90K samples from the train dataset of Stiennon et al. (2020), it
achieves an accuracy of 69.3±0.2%on their test set (also not shown in Table 4). In contrast, this same model trained on 90K
samples from their train dataset achieves an accuracy of only 68.6±2.0%on our validation dataset, which is significantly
lower than the accuracy of 73.4±1.9%achieved by the model trained on 5K samples of our own train dataset. Similar
patterns can be observed when comparing the OPT Binary model with 1.3B parameters trained on 5K samples of our own
train dataset to the released 1.3B reward model trained by Stiennon et al. (2020) on approx. 64K samples of their own train
dataset. The former model achieves an accuracy of 69.6±2.0%on our validation dataset, while the latter only achieves
an accuracy of 63.8±2.1%(note, though, that the RMs are trained with different loss functions). These results highlight
two important considerations: (1) preference distributions can vary significantly and have a strong effect on what a reward
model learns, and (2) the sample efficiency of a reward model depends heavily on the train and test distributions. If the test
distribution differs from the train distribution, reward models may be very sample inefficient and fail to accurately learn the
true distribution, even when given significantly more samples.

--- PAGE 18 ---
Training Language Models with Language Feedback at Scale
Samples Epochs Prompt Loss Weight Learning Rate
100 1 0 0 .05
1K 1 0 .05∗ 0.02
5K 1 0 .1 0 .2
Table 5: We report the chosen hyperparameters of finetuning on 100, 1K, and 5K samples of H UMAN SUMMARIES .
*This hyperparameter is optimal but used only for finetuning on HUMAN SUMMARIES . For finetuning on REFINEMENTS
and I NITIAL SUMMARIES we inadvertently use the prompt loss weight 0.
G. Hyper Parameters
G.1. Generating Refinements
For the targeted word removal experiments ( §3), we use greedy decoding until 200 tokens or / nis generated. For all
summarization experiments we sample up to 48 tokens (as in Stiennon et al., 2020) with nucleus sampling (Holtzman et al.,
2019) with p= 0.95and temperature t= 1.0. We strip non-alphanumeric characters (e.g., newlines) from the beginning of
sampled summaries. We further remove empty white spaces in the generated summaries and remove all text that comes after
a new line token / n. Due to the maximum token length, sampled summaries sometimes end with incomplete sentences.
Thus, we remove ending sentences that do not end in “.”, “!”, or “?”. The described temperature and post-processing are
applied to all summary generations, i.e., for generating initial summaries, refinements, and test summaries.
G.2. Finetuning on Summaries
We conduct independent hyperparameter optimization sweeps with three dataset sizes of human summaries of 100, 1K and
5K samples, and then use the same hyperparameters for finetuning on refinements (ILF) and finetuning on initial summaries.
We choose to run the hyperparameter sweep on Human summaries since this will not give an unfair advantage to our
algorithm that finetunes on refinements. For the sweep, we utilize the train dataset of human summaries (consisting of 100,
1K, and 5K samples) and evaluate on the development dataset. Unfortunately, the OpenAI API only provides validation
loss and token accuracy for batches of the development dataset, making it impossible to evaluate the model on the full
development dataset during training. As a result, we utilize the model API to evaluate on the full development dataset after
finetuning and calculate the perplexity of the generated summaries as a performance measure.
To determine the optimal hyperparameters, we perform a sweep over a range of values for the following parameters: epochs
{1,2,3,4},prompt loss weight {0,0.01,0.05,0.1}, and learning rates {0.02,0.05,0.1,0.2}. We first sweep over epochs
and select the best value, then perform a sweep using that value for the prompt loss weight, and so on. Our empirical
observations indicate that the number of epochs has the greatest impact on perplexity, with training for more than one epoch
resulting in overfitting. The selected hyperparameters can be found in Table 5.
During the finetuning phase for the REFINEMENTS andINITIAL SUMMARIES datasets with 1K samples each, we made
an error in our hyperparameter selection. Instead of using a prompt loss weight of 0.05, we mistakenly used a value of 0,
when finetuning on human summaries. While this error may have slightly impacted our results, the difference in perplexity
between the two settings is minimal, with a value of 6.68for a prompt loss weight of 0.05and6.71for a prompt loss
weight of 0. Despite this mistake, our method still outperforms finetuning on human summaries for 1K samples, as well as
finetuning on initial summaries using suboptimal hyperparameters.
G.3. Multiple Iterations of ILF
To evaluate multiple iterations of ILF, i.e., multiple iterations of refining-and-finetuning, we finetune GPT-3-175B on a
refinement dataset with 200 and 300 samples. Thus we conduct a hyperparameter optimization on a train dataset of 200 and
300 refinements and evaluate on a development dataset of 200 refinements (instead of human summaries). To determine
the optimal hyperparameters, we perform a sweep over a range of values for the following parameters: epochs {1,2,3,4},
prompt loss weight {0,0.01,0.05,0.1}, and learning rates {0.02,0.05,0.1,0.2}. We first sweep over epochs and select the
best value, then perform a sweep using that value for the prompt loss weight, and so on. For finetuning on 200 refinements
we select the following hyperparameters: epochs = 1,prompt loss weight = 0.05,learning rate multiplier = 0.1. For
finetuning on 300 refinements we select epochs = 1, prompt loss weight = 0, and learning rate multiplier = 0.2.

--- PAGE 19 ---
Training Language Models with Language Feedback at Scale
OPT-RM, LM Loss/Binary
100 1000 5000
Train Dataset Size506070Accuracy in %)
Figure 7: Here we plot the validation accuracy of OPT-RM trained on 100, 1K, and 5K samples on a log-log plot. The figure
shows scaling when increasing the dataset size.
G.4. Finetuning Reward Models
OPT Reward Model. For finetuning the OPT Reward Model, we perform bayesian hyperparameter optimization for
each of the three different types of reward models: Standard ,Comparison andClassification (see section F). We sweep
over the learning rate in the range of [1e−5,1e−6]and the batch size {32,64}for all the models. For the reward models
using the language loss, we also optimize the prompt-loss weight {0.0,0.01,0.05,0.1,0.5,1.0}. We run 10 iterations per
model and evaluate all the sweeps with the 200 development examples. We use a linear learning rate scheduler and a weight
decay of 0.1for all the runs. The optimal batch size is 32 for all the models. The best prompt loss weight is 0.01for both
theComparison andClassification RMs. As for the learning rate, we use 9.3e−6for the Standard RM, 5.8e−6for the
Classification RM and 1e−6for the Comparison RM. In the final finetuning, we select the best RM in the validation split
over 10 epochs.
GPT-3 Reward Model. In order to finetune GPT-3-175B as an RM, we utilize the OpenAI API. We finetune two types of
RMs: the Comparison RM, which learns to predict which of two summaries is superior, and the Classification RM, which
predicts whether a given summary is of high quality or not. For cost considerations, we conduct hyperparameter tuning on a
training dataset of 1K samples (instead of 5K) and evaluate on a development dataset of 200 samples. We use a dataset with
1K samples for cost reasons. We then apply the same hyperparameters when finetuning on 5K samples while implementing
early stopping in terms of epochs. Due to the binary nature of the human preference annotations in the classification reward
model, the effective train dataset size for this model is doubled to 2K samples.
In order to determine the optimal hyperparameters, we perform a sweep over a range of values for the number of epochs
{1,2,3,4}and the prompt loss weights {0,0.001,0.005,0.01,0.05,0.1,0.5}. The OpenAI API provides classification
accuracy (for both the comparison and classification tasks) for the full development dataset after each epoch, allowing us to
select the appropriate number of epochs and prompt loss weight. When finetuning on 5K samples, we utilize early stopping
to prevent overfitting, using 1 epoch and a prompt loss weight of 0 for the comparison model and 4 epochs and a prompt
loss weight of 0.001for the classification model. We use default values for all other hyperparameters, which may vary
depending on the dataset size.
H. Additional Results
H.1. Analyis of Finetuned Models
In Table 6, we evaluate GPT-3-175B on various finetuning datasets used for finetuning: the refinements, the initial summaries,
and the human summaries. We evaluate the log-likelihood of GPT-3-175B on the summaries of 1K samples from the
various train datasets (i.e. initial summaries, refinements, and human summaries). Concretely, we pass the whole prompt to

--- PAGE 20 ---
Training Language Models with Language Feedback at Scale
Finetuned on Human Summaries Finetuned on Refinements Finetuned on Initial Summaries
102103
Train Dataset Size100
9×101
1.1×1001.2×1001.3×1001.4×1001.5×100Negative Log-Likelihood
Figure 8: Evaluation of models finetuned on 5K initial summaries, refinements, and human summaries on 500 samples from
the corresponding validation datasets. For example, the model finetuned on human summaries is evaluated on 500 human
summaries from the validation dataset. The model finetuned on refinements has a significantly lower negative log-likelihood
than the model finetuned on human summaries.
ModelNeg. Log Likelihood of GPT-3-175B
on 1K train samples of respective distributionDKL(GPT-3-175B |finetuned )(in nats) DKL(finetuned |GPT-3-175B )(in nats)
Finetuned on Initial Summaries 1.19±0.01 0 .43±0.11 0 .83±0.08
Finetuned on Refinements 1.37±0.01 0 .60±0.10 1 .10±0.06
Finetuned on Human Summaries 1.61±0.01 0 .12±0.09 0 .55±0.01
OPT-RM best-of-64 FeedME - - 3.17
Table 6: First we evaluate the log-likelihood of GPT-3-175B on the 1K samples of the various data distributions that we
finetune on. Then we empirically calculate the KL-divergence by sampling 2000 texts of length 64 tokens from GPT-3-175B
and evaluating the log-likelihood of the finetuned models on the samples (for the reverse KL we sample from the finetuned
models and evaluate GPT-3-175B on the samples). We report the mean and standard error across 2 runs. For Best of 64 on a
specific reward model, we use the analytical formula KL(N, RM ) = log N−N−1
N(see also (Hilton & Gao, 2022)).
GPT-3-175B, including the Reddit post, but only evaluate the log-likelihood of the completion, i.e. the generated summary.
We also measure the (reverse) KL divergence (following Gao et al., 2022) between an ILF-finetuned model and the pretrained
LM before ILF-training, DKL(finetuned |GPT-3-175B ). We sample unconditionally (i.e. using a beginning of sentence
token) from the finetuned models and evaluate the log-likelihood of the generated text with GPT-3-175B. We also report the
forward KL divergence, DKL(GPT-3-175B |finetuned ). We discuss the results in §4.6.
H.2. Results: ILF + OPT-RM
In this section, we present the full results of our best-performing method ILF + OPT-RM and other additional methods
(see§4.3.1 for a description of ILF + OPT-RM and §4.3.3 for a discussion of the results). We conduct the same evaluation
as described in §4.3.2, i.e. in a human evaluation, annotators rank various test summaries based on quality. We then
calculate the win rate against human written summaries, which we use as an evaluation metric. Importantly, all methods
evaluated here are trained on datasets with 5K samples. Note that the methods compared here are not exactly the same
as the methods compared in Fig. 3. Concretely, the test summaries generated by the methods finetuning on refinements
(ILF), finetuning on human summaries, and OPT-RM best-of-64 FeedME are the same as in Fig. 3, for the test summaries
generated by corresponding methods trained on 5K samples. Here, however, we don’t evaluate FeedME and finetuning on
initial summaries. However, we evaluate ILF + OPT-RM (best-of-64), our best-performing model, which we also added to
Fig. 3 for reference. We also evaluate a new method called Finetuned on Feedback + Refinements , which we describe below.
For finetuning on feedback + refinements, we us a title, post, and summary as input and the model is trained to predict the

--- PAGE 21 ---
Training Language Models with Language Feedback at Scale
corresponding feedback and refinement. Our motivation for this approach is that generating feedback first may improve
the quality of the resulting refinements, similar to the findings of previous work on self-prompting methods Saunders et al.
(2022); Bai et al. (2022) and the Chain of Thought (CoT) prompting technique Wei et al. (2022). CoT has been shown
to improve the performance of models across various tasks Wei et al. (2022) when allowing the model to reason before
answering a question. For finetuning on feedback and refinements, we utilize the initial summaries that were used to gather
human feedback, as well as the refinements generated by our method. We use the loss logp(x1, f|prompt )+λlogp(prompt ),
i.e. we learn to predict the refinement and the feedback. We employ the same hyperparameters as in the finetuning on
refinements algorithm (including the prompt loss weight). During testing, we require initial summaries, from which we
generate feedback and refinements. As initial summaries, we use the test samples generated by FeedME (as evaluated in
Figure 3). To ensure compatibility with the 48-token length restriction of the test summaries, we append the special end
token / n ### to the end of the feedback and refinements during training. At test time, we set the maximum number of tokens
to generate 300, and terminate generation when the stop-word / n ### appears. We then apply the same postprocessing
procedure outlined in Appendix G.1 to shorten the refinements to 48 tokens. We refer to Appendix J.3 for the exact prompt
templates we used.
We present all the results in Fig. 9. We find that finetuning on a set of 5K refinements achieves a win rate of 36.0±1.8%,
while ILF + OPT-RM (best-of-64) has a win rate of 50.8±1.9%, achieving human-level summarization performance (see
§4.3.3 for a more detailed discussion). OPT-rM best-of-64 FeedMe achieves a win rate of 45.1±1.9%, finetuning on a set
of 5K human-generated summaries achieves a win rate of 35.4±1.8%, and finetuning on a combination of 5K feedback and
refinements has a win rate of 26.1±1.7%. It is worth noting that the performance of finetuning on feedback and refinements
is lower than that of finetuning on refinements alone. We attribute this to the increased difficulty of generating both feedback
and refinements and believe that this discrepancy may be due to limitations in our models, dataset size, or hyperparameters.
Previous work has demonstrated the feasibility of training models to generate feedback Saunders et al. (2022); Bai et al.
(2022), so we believe that further optimization and experimentation may improve the performance of this method. We
further want to note that the results for finetuning on 5K refinements, 5K human summaries, and best-of-64 FeedME deviate
from the results in Fig 3. This is because we compare different methods with each other, and human annotations generally
contain some amount of noise (given that different people annotate the same samples).
 ILF:
 Finetuned
 on
 Refinements ILF +
 OPT-RM
 (best-of-64) OPT-RM
 best-of-64
 FeedMe Finetuned
 on 5K Human
 Summaries Finetuned
 on 5K
 Feedback +
 Refinements01020304050Win Rate vs. 
 Human Summaries (%)
Figure 9: How often human evaluators prefer summaries from ILF: Finetuned on Refinements, OPT-RM best-of-64 FeedME,
ILF + OPT-RM (best-of-64), finetuning on human summaries, and finetuning on feedback + refinements (all methods
finetuned on 5K samples). ILF + OPT-RM (best-of-64) generates summaries of a similar quality to human summaries.
Finetuning on feedback + refinements performs worse than finetuning on refinements (ILF).
H.3. Multiple Iterations of ILF
Our experiments suggest that ILF is an effective method for leveraging language feedback in the training of LMs. Here we
explore ILF in its most general form by doing multiple iterations of refining-and-finetuning.
Dataset Improvement. In this experiment, we evaluate the effectiveness of iterative refinement of the dataset distribution
using ILF. To this end, we first finetune GPT-3-175B on 100 refinements from iteration 1 of ILF (i.e. doing one iteration

--- PAGE 22 ---
Training Language Models with Language Feedback at Scale
Initial Model Finetuned ModelFinetuning datasetProduces DatasetILF iteration 1 ILF iteration 2 ILF iteration 3
GPT-3-175B M100
1 D100
1 D100
2
M100
1 M200
1 D100∗
1,
GPT-3-175B M200
scratch, 1 D200
1
GPT-3-175B M300
scratch, 1 D300
1
M100
1 M200
1,2 D100
1 D100
2 D100
3
M200
1,2 M300
1,2,3 D100
1 D100
2 D100
3
GPT-3-175B M200
scratch, 1,2 D100
1+D100
2
GPT-3-175B M300
scratch, 1,2,3 D100
1+D100
2+D100
3
Table 7: Datasets (refinements) over which the models Mare trained, and which they generate. The superscript indicates
the number of samples, whereas the subscript indicates the ILF step. In this figure we do not show FeedME which is used to
generate the refinements given feedback.
* these samples are new samples from the interval [100,200] of D200
1
.
of refining initial summaries, as we did in the main results of our paper, see §4.3.2) and refer to this finetuned model as
M100
1. The notation we use here is that the subscript indicates the iteration of ILF that the refinements were generated in,
and the superscript indicates the number of overall samples the model is finetuned on. We also refer to the dataset of 100
refinements from iteration 1 as D100
1. As a baseline, we finetune M100
1on an additional 100 refinements from ILF iteration
1, resulting in M200
1, i.e., a model trained on 200 refinements from ILF iteration 1. We then compare this baseline to two
iterations of ILF. Specifically, we use M100
1to generate summaries for an additional 100 samples (the same Reddit posts as
for the baseline) and collect human feedback on those summaries. We then use this feedback to generate 5 refinements using
the FeedME7and then select the best refinement using our InstructRM method. We refer to these 100 selected refinements
from the second iteration of ILF as D100
2. Finally, we finetune M100
1onD100
2to obtain the model M200
1,2, which has been
trained on a total of 200 refinements generated in both the first and second iterations of ILF. All finetuning was performed
using the same hyperparameters as described in Appendix G for finetuning on 100 refinements. We refer to Table 7 for an
overview of all models and train datasets.
In this human evaluation, we compare the performance of the summaries generated by the baseline model ( M200
1) with those
generated by two iterations of ILF ( M200
1,2) on our test set. Human evaluators are asked to indicate their preferred summary
for each comparison, and the win rate of M200
1,2against M200
1is calculated and plotted in Fig. 10 (left)8. Our results show
that two iterations of ILF outperform one iteration with a win rate of 53.2±1.9%indicating that applying multiple rounds
of ILF can improve the data distribution. However, we also want to investigate whether multiple rounds of ILF lead to
better models than directly finetuning on the same number of refinements from the first round from scratch. In other words,
while our current baseline consists of further finetuning M100
1on an additional 100 samples, it is also possible to directly
finetune GPT-3-175B on 200 refinements from the first iteration of ILF from scratch, i.e. M200
scratch, 1. We aim to determine
the relative effectiveness of these two approaches in improving model performance on the text summarization task.
Model Improvement. In this experiment, we aim to compare the performance of multiple rounds of ILF to directly
finetuning on a comparable number of refinements from the first iteration of ILF. As a baseline, we finetune GPT-3-175B on
200 and 300 refinements from the first iteration of ILF and conduct hyperparameter tuning as described in the Appendix G.
We then compare these baselines to two and three rounds of ILF. For the two-round ILF model, we use the previously
described M200
1,2. To obtain the three-round ILF model, we use M200
1,2to generate summaries for an additional 100 samples
(on the same Reddit posts as for the baseline), gather human feedback, generate 5 refinements with GPT-3-175B using the
feedback, and select the best refinement using InstructRM, resulting in D100
3. We then finetune M200
1,2onD100
3to obtain the
model M300
1,2,3. It is important to note that while our baselines finetune GPT-3-175B from scratch on 200 and 300 refinements,
the models M200
1,2andM300
1,2,3are obtained by continuously finetuning a model iteratively on additional refinements. This
difference in approach may introduce a discrepancy in the results, as we use different hyperparameters, and the dataset size
7Ideally, one would use the same model M100
1to generate the refinements. However, in our case, this is not possible since we finetuned
GPT-3-175B, which is not an instruction-finetuned model.
8Note, we set the win rate manually to 50% at 100 samples, since the baseline is equivalent to one iteration of ILF.

--- PAGE 23 ---
Training Language Models with Language Feedback at Scale
may affect the learning dynamics. To control for this potential difference, we also finetune GPT-3-175B from scratch on the
refinements generated through various iterations of ILF. Specifically, as an alternative to M200
1,2, we finetune GPT-3-175B
from scratch on a concatenation of 100 refinements from the first round of ILF (i.e., D100
1) and 100 refinements from the
second round of ILF (i.e., D100
2), and refer to the resulting model as M200
scratch 1,2. Similarly, as an alternative to M300
1,2,3, we
finetune GPT-3-175B from scratch on a concatenation of 100 refinements from the first round of ILF ( D100
1), 100 refinements
from the second round of ILF D100
2, and refinements from the third round of ILF (i.e. D100
3), and refer to the resulting model
asM300
scratch 1,2,3. It is worth noting that the refinements from the second and third rounds of ILF (i.e. D100
2andD100
3) are
based on summaries generated using models that were continuously finetuned (i.e. M100
1andM200
1,2). As such, the models
M200
scratch 1,2andM300
scratch 1,2,3are not a direct application of ILF, but rather an approximation of the distribution induced by
ILF. We refer to Table 7 for an overview of all models and train datasets.
Using a human evaluation, we compare the performance of the three methods on the test dataset: the baseline, ILF with
continuous finetuning, and ILF approximated by finetuning from scratch. The results are shown in Fig. 10 (right). With this
more realistic baseline, we find that directly applying ILF does not improve upon the baselines, with win rates of 49.4±1.9%
and50.9±1.9%for 200 and 300 samples, respectively. However, approximating ILF by finetuning from scratch on the
distributions induced by ILF significantly improves upon the baseline for 300 samples, with a win rate of 55.6±1.9%.
The method is slightly worse than the baseline for 200 samples, with a win rate of 48.9±1.9%. We currently hypothesize
that continuous finetuning may lead to catastrophic forgetting, while finetuning from scratch may not have this problem.
This could explain why M300
scratch 1,2,3performs significantly better than M300
1,2,3for 300 samples. Specifically, M200
1,2may
actually generate an improved distribution in the third iteration of ILF. However, when further finetuning M200
1,2on this
improved distribution D100
2, the model may forget what it learned previously. On the other hand, the model M300
scratch 1,2,3
that learns from scratch on the concatenation of all datasets produced by ILF may actually benefit from the improved dataset
distribution because it does not unlearn anything. It is, however, unclear why M200
scratch 1,2does not benefit from the improved
data distribution D100
2. It is also possible that the hyperparameters play a significant role in the final performance of the
various models and that the dataset size has a strong influence on model performance (e.g., finetuning on more samples may
be more stable than finetuning on fewer samples). In future work, we plan to conduct more elaborate experiments to answer
these questions and better understand the effects of the dataset size and number of iterations on ILF. Specifically, we aim to
run multiple iterations of ILF and use M200
scratch 1,2as the model to generate summaries in the third round of ILF (instead of
M200
1,2). This would be a direct implementation of ILF, rather than an approximation of it, as we would be finetuning the
same model with which we are also generating an improved distribution. We also hope to investigate the effect of the dataset
size and number of iterations on ILF. Overall, our results suggest that ILF has the potential to improve the performance of
natural language processing systems by continuously incorporating human feedback into the training of language models,
but further research is needed to fully understand the best ways to leverage this approach.
H.4. Part-of-Speech Distribution for Finetuning Datasets
We evaluate the negative log-likelihood of GPT-3-175B on the three finetuning datasets, i.e. on initial summaries, refinements,
and human summaries. We use the training dataset with 1K samples and calculate the negative log-likelihood over different
Part-of-Speech tags. We use Stanza (Qi et al., 2020) as the PoS tagger for this experiment and then we separate the words
into three groups: function words, content words, and others. The function words are words that have little lexical meaning:
articles, pronouns, adpositions, conjunctions, auxiliary verbs, particles and interjections. On the other hand, content words
are words that contain semantic information: nouns, adjectives, adverbs and lexical verbs. We keep numbers and symbols
under the group others . With this analysis, we want to spot different patterns between model-generated (initial summaries
and refinements) and human-written summaries. Note that a high negative log-likelihood implies a high loss. We present the
results in Fig 11. Since the average loss is higher for human summaries, we normalize all the loss values by transforming
them to have mean 0 and standard deviation 1. Overall, the word distribution is very similar for all three finetuning datasets.
In terms of normalized mean loss, it is interesting how the content words have a bigger influence on the refinements dataset.
We believe that this is related to our results in section 4.3.3, where we obtain the best results when finetuning on refinements.
H.5. Comparison to Results of Scheurer et al. (2022)
Here we relate our results to previous work by Scheurer et al. (2022). In Fig. 2 of Scheurer et al. (2022), they compare
their method of finetuning on refinements against various baselines, such as finetuning on initial summaries, sampling from
FeedME (called InstructGPT), and sampling from GPT-3-175B. They calculate the win rate of all methods against human

--- PAGE 24 ---
Training Language Models with Language Feedback at Scale
ILF (M200
1,2)
100 200
Train Dataset Size505152535455Win rate vs. 
 Finetuning on Refinements
 ILF (continuous finetuning) - ( M200
1,2/M300
1,2,3)
Approximating ILF - ( M200
scratch 1,2/M300
scratch 1,2,3)
100 200 300
Train Dataset Size464850525456Win rate vs. 
 Finetuning on Refinements
Figure 10: Left: Win rate of 2 iterations of ILF against finetuning on the same number of refinements from the first iteration
of ILF. Right : Win rate of 3 iterations of ILF, and approximating 3 iterations of ILF by finetuning from scratch, against
finetuning on the same number of refinements from the first iteration of ILF.
written summaries (V ¨olske et al., 2017) that are automatically extracted from Reddit. As shown in §4.1 and App.C, our
human summaries are preferred 72.3±3.2%to the human summaries of V ¨olske et al. (2017). This implies that the win
rates in Scheurer et al. (2022) are much higher than in our case since we use a much stronger baseline.
We now present three differences between the results found in Scheurer et al. (2022) and the results found in our paper.
Then we will provide various potential reasons that could explain the differences. First, when comparing the results (in
relative terms) in Scheurer et al. (2022) Fig. 2 to our results in Fig. 3 where we finetune on 100 samples, we see differences
in performance. Scheurer et al. (2022) reports that finetuning on refinements outperforms finetuning on initial summaries.
And both methods outperform sampling from FeedME (i.e., InstructGPT). In our experiments finetuning on 100 refinements
achieves a win rate of 19.6±1.5%against human summaries, finetuning on initial summaries a win rate of 19.6±1.5%,
and FeedME a win rate of 20.8±1.5%. Thus both finetuned methods perform equally and are worse than sampling from
FeedME.
Second, we compare the results of refining a summary with feedback. Note that Scheurer et al. (2022) uses an embedding-
based scoring function to select refinements, whereas we use InstructRM. In Scheurer et al. (2022) Fig. 3 (left) REFINE
WITH FEEDBACK + B EST OF Nachieves a win rate of 67.0±3.1%against initial summaries (sampled from FeedME),
REFINE WITH FEEDBACK achieves a win rate of 60.5±3.0%,REFINE WITHOUT FEEDBACK achieves 50.3±2.6%and
Human Summaries have a win rate of 60.8±3.4. In our Fig. 4 (left) Refine with Feedback + Best-of-5 achieves a win rate
of69.1±1.9%, Refine with Feedback achieves a win rate of 63.9±2.0%, Refinement without Feedback achieves a win
rate of 59.4±2.0%and Human Summaries a win rate of 83.2±1.7%. The difference in the human summaries is expected,
given that we use better human summaries. The Refinement without Feedback method achieves higher results in our work
than in Scheurer et al. (2022).
Third, it is also noteworthy that using the embedding similarity as a scoring function worked well in Scheurer et al. (2022),
while it does not work in our setting (see Table 2 and §4.2 for a discussion of the results). We believe this is because the
feedback we collect is written by many annotators and is thus much more diverse, while in Scheurer et al. (2022), the authors
themselves wrote the feedback.
Here we now list various differences in the setup of Scheurer et al. (2022) and our paper, which could all account for the
different results.
1.Scheurer et al. (2022) use an embedding similarity as a scoring function, while we use InstructRM Ensemble. Looking
at Tab. 2 and the corresponding discussion in §4.2, already shows that the methods are very different.
2.The human-written summaries are of much higher quality in our paper than in Scheurer et al. (2022) (see §4.1 and

--- PAGE 25 ---
Training Language Models with Language Feedback at Scale
Distribution0.00.20.40.6
Function Words Content Words OthersInitial Summary Refinement Human SummaryWord distribution
Normalized mean loss-1.0-0.50.00.51.01.5
Function Words Content Words Others Initial Summary Refinement Human SummaryNormalized mean loss
Figure 11: Distribution of tokens of various finetuning datasets with 1K samples in terms of content and function words. We
only evaluate the various completions, i.e., summaries, since the prompts are the same for all distributions.
App. C)
3.In Scheurer et al. (2022), the annotation instructions specifically state that the feedback should mention how to improve
a summary. In our work, we collect much more unrestricted and diverse feedback. This difference is also apparent in
the fact that the embedding similarity does not work well as a scoring function in our setting.
4.In Scheurer et al. (2022), the authors themselves annotated the data, i.e., they wrote the feedback and evaluated the
final summaries. In our case, we use independent evaluators who are trained on this task. Using 31 annotators overall
also gives us a more diverse and less biased estimate of our methods. Also, doing human evaluations is inherently
noisy and will never lead to the exact same results.
5.The evaluation in Scheurer et al. (2022) was done on a different dataset than in this work. Specifically, they used only
100 samples to evaluate their method, while we use a test set of 698 samples.
6.The hyperparameters in Scheurer et al. (2022) used for sampling and finetuning are different from the hyperparameters
used in our work.
7. Overall, we use different prompts than Scheurer et al. (2022) (see App. J.3 and App. J.1)
I. Annotator Instructions
Overall we completed many annotations to create datasets and evaluate our algorithm. The instructions were task-specific
and also continuously updated. In the following, we provide the instructions we used to create our train dataset and the
instructions we provided for evaluating the summary quality (of 6 summaries). We will not share more instructions for
brevity but can provide them upon request.
I.1. Train Dataset Annotation Instructions
Task Overview
You are given a Reddit Post, which you first need to read carefully. You then need to complete 5 subtasks which consist of
comparing two summaries, writing feedback on a summary, classifying the type of feedback, indicating whether there is
additional Feedback, and writing an ideal summary. When doing these tasks, please adhere to the guidelines below.
What makes for a good summary? Roughly speaking, a good summary is a short piece of text that has the essence of the
original text. A good summary tries to accomplish the same purpose and conveys the same information as the original text.
We would like you to consider these different dimensions of summaries:
Essence : Is the summary a good representation of the post? How well does the summary cover the important information in
the post?

--- PAGE 26 ---
Training Language Models with Language Feedback at Scale
Clarity : Is the summary reader-friendly? Does it express ideas clearly?
Accuracy : Does the summary contain the same information as the post?
Purpose : Does the summary serve the same purpose as the original post?
Concise : Is the summary short and to the point?
Style : Is the summary written in the same style as the original post?
Generally speaking, we give higher weight to the dimensions at the top of the list. The evaluation can be complicated though,
since none of the above dimensions are simple yes/no matters, and there aren’t hard and fast rules for trading off different
dimensions. Use your best judgment and common sense to make these trade-offs. In case the subreddit, title, and Reddit
post leave open some ambiguity about what happened, it is important to accurately reflect that in your annotations and not
just interpret the text in a certain way. Always look at all the subreddit, title, and Reddit Post and use all information given
to make your judgments (sometimes the title may contain crucial information that does not appear in the post but should
nevertheless be used).
First, read the Subreddit category, title, and post carefully. A Subreddit is a forum dedicated to a specific topic on the website
Reddit. Take your time with this step and re-read the parts that you might not have understood at first. Below is a detailed
description of the task you will need to complete for each Reddit post.
Below is a detailed description of each task you will need to complete for each Reddit post:
1.Comparison Task : Given a pair of summaries, indicate which is better.
Details : Use the above description of what makes a good summary. It is alright to choose either summary if both
summaries are identical copies of each other or if there is no distinguishing feature that makes one summary superior
to the other. However, if there is a small detail that makes one summary better than the other, that is enough reason to
select that summary.
2.Feedback Task : Write short and simple feedback on the given summary about the single, most important shortcoming
of the summary. The feedback should NOT mention what category (Accuracy, Coverage, Coherence, other) the
feedback belongs to, nor should it assume knowledge about the definitions of “Coverage”, “Accuracy”, or “Coherence”
(see below). Otherwise, the feedback should be as short and simple as possible while still addressing the most important
shortcoming of the summary.
Details : You can write the feedback in one or several sentences, but it should only address the single, most important
shortcoming of the summary and be as short as possible. There are no other restrictions as to how you write the
feedback and what exactly it addresses. If there are no shortcomings in the summary, the feedback can also mention
a positive thing about the summary. Use the description of what makes a good summary to trade off the various
dimensions that make for a good summary. Often the feedback will (but does not have to) address one of the following
axes.
•Coverage : For this axis, answer the question, “how well does the summary cover the important information
in the post?” A summary has good coverage if it mentions the main information from the post that’s important
to understand the situation described in the post. A summary has poor coverage if someone reading only the
summary would miss several important pieces of information about the situation in the post. A summary with
good coverage should also match the purpose of the original post (e.g., to ask for advice).
•Accuracy : For this axis, answer the question, “does the factual information in the summary accurately match
the post?” A summary is accurate if it doesn’t say things that aren’t in the article, doesn’t mix up people, and
is generally not misleading. If the summary says anything at all that is not mentioned in the post or contradicts
something in the post, it is NOT accurate.
•Coherence : For this axis, answer the question, “how coherent is the summary on its own?” A summary is coherent
if, when read by itself, it’s easy to understand and free of English errors. A summary is not coherent if it’s
difficult to understand what the summary is trying to say. Generally, it’s more important that the summary is
understandable than being free of grammar errors.
Additional Rules: The feedback should NOT mention what category (Accuracy, Coverage, Coherence, other) the
feedback belongs to, nor should it assume knowledge about the definitions of “Coverage”, “Accuracy”, “Coherence”,

--- PAGE 27 ---
Training Language Models with Language Feedback at Scale
or “other” (as defined above). Example: One should NOT write ”This is missing in the area of coverage”, or ”This
summary lacks in the category of accuracy, because ...”. The feedback should be understandable to a person who has
never read the definition of ”Coverage”, ”Accuracy”, and ”Coherence”. You are, however, ALLOWED to use those
words if they make sense on their own, e.g., you CAN say, ”This summary does not cover the important parts of the
text because”, or ”This summary is inaccurate as it states ...”, or ”This is not a coherent summary because ...”.
3.Feedback Type Task : If your feedback falls into the categories Accuracy-related, Coherence-related, or Coverage-
related, mark it as such by checking the corresponding checkbox for the (single) category it is related to. If your
feedback is not related to any of these three categories, then check the ”Other” checkbox.
4.More Feedback Task : Answer with Yes if there is additional Feedback about an important shortcoming of the summary
that you would want to mention and No otherwise.
5.Ideal Summary Task : Ideal Summary Task: Write a short summary for the Reddit post that is ideal in your view.
Details : The ideal summary should be ideal in terms of all the criteria mentioned above, i.e., essence, clarity, accuracy,
coverage, purpose, conciseness, coherence, and style. In other words, you should not be able to find an obvious critique
of the ideal summary that you write. It is okay to reuse parts of previous summaries but only if those parts should be a
part of an ideal summary. The ideal summary should maximally be 48 tokens long (otherwise, you can’t submit your
annotation). Tokens are generated by taking your ideal summary and splitting up certain words into individual pieces
(this is necessary to train our AI). The interface will show you how many tokens your ideal summary has already taken
up.
I.2. Summary Quality Evaluation Instructions
Task Overview
You will be given a Subreddit category, a title, and a Reddit Post, which you first need to read carefully. Your task is then to
compare 6 summaries and rank them according to quality.
What makes for a good summary? Roughly speaking, a good summary is a short piece of text that has the essence of the
original text. A good summary tries to accomplish the same purpose and conveys the same information as the original text.
We would like you to consider these different dimensions of summaries:
Essence : Is the summary a good representation of the post? How well does the summary cover the important information in
the post?
Clarity : Is the summary reader-friendly? Does it express ideas clearly?
Accuracy : Does the summary contain the same information as the post?
Purpose : Does the summary serve the same purpose as the original post?
Concise : Is the summary short and to the point?
Style : Is the summary written in the same style as the original post?
Generally speaking, we give higher weight to the dimensions at the top of the list. The evaluation can be complicated though,
since none of the above dimensions are simple yes/no matters, and there aren’t hard and fast rules for trading off different
dimensions. Use your best judgment and common sense to make these trade-offs. In case the subreddit, title, and Reddit
post leave open some ambiguity about what happened, it is important to accurately reflect that in your annotations and not
just interpret the text in a certain way. Always look at all the subreddit, title, and Reddit Post and use all information given
to make your judgments (sometimes the title may contain crucial information that does not appear in the post but should
nevertheless be used).
First, read the Subreddit category, title, and post carefully. A Subreddit is a forum dedicated to a specific topic on the website
Reddit. Take your time with this step and re-read the parts that you might not have understood at first. Below is a detailed
description of the task you will need to complete for each Reddit post.
Comparison Task : Given 6 summaries, indicate which is better by ranking them according to quality. Rank 1 is considered
the highest rank, and Rank 6 is considered the lowest rank. The summary with the best quality should be ranked highest,
i.e., as Rank 1, and the summary with the worst quality should be ranked lowest, i.e. Rank 6. Use the above description of

--- PAGE 28 ---
Training Language Models with Language Feedback at Scale
what makes a good summary. Ties between summaries are allowed, but only if summaries are exact copies of each other or
if there is no distinguishing feature that makes one summary superior to the other. However, if there is a small detail that
makes one summary better than the other, that is enough reason to rank that summary as better than the other summary. We
use Standard Competition ranking (i.e., example rankings of 122456). In standard competition ranking, items that compare
equally receive the same ranking number, and then a gap is left in the ranking numbers. The number of ranking numbers
that are left out in this gap is one less than the number of items that are compared equally. Equivalently, each item’s ranking
number is 1 plus the number of items ranked above it.
J. Prompts
J.1. Summarization Prompts
We report all prompt templates used to generate INTIAL SUMMARIES ,REFINEMENT WITH FEEDBACK , and REFINEMENT
WITHOUT FEEDBACK in Table 8.
Methods Format
INITIAL SUMMARY Write an excellent summary of the given text.
Title:{title }
Text:{text}
TL;DR:
REFINEMENT WITH FEED-
BACKWrite an excellent summary that incorporates the feed-
back on the given summary and is better than the given
summary.
Title:{title }
Text:{text}
Summary: {summary }
Feedback on Summary: {feedback }
Improved TL;DR:
REFINEMENT WITHOUT
FEEDBACKWrite an excellent summary that is better than the given
summary.
Title:{title }
Text:{text}
Summary: {summary }
Improved TL;DR:
Table 8: Prompt templates used for summarization.
J.2. InstructRM Prompts
We instructed one of the authors of this paper (who at the time had not been involved in the research project) to write 5
prompts that would achieve the goal of selecting high-quality summaries, i.e., refinements. The author did not have any
domain knowledge or prior information on what kinds of prompts would work. The instructions provided to the author can

--- PAGE 29 ---
Training Language Models with Language Feedback at Scale
be viewed here. We report all 5 prompt templates in Table 9.
InstructRM Prompts Format
PROMPT 1 Here’s a summary of a Reddit post, feedback on the sum-
mary, and a new summary. You will be asked to determine
whether the new summary incorporates the feedback pro-
vided.
A good summary is a short piece of text that has the
essence of the original text. A good summary tries to
accomplish the same purpose and conveys the same infor-
mation as the original text.
Post title: {title }
Below, there’s the content of the post that was summa-
rized.
Original post: {text}
Original summary: {summary }
A human then provided feedback on the above summary.
Feedback: {feedback }
Based on this feedback, a new summary was written.
New summary: {refinement }
Does this new summary incorporate the feedback pro-
vided? Answer Yes or No.
Answer:
PROMPT 2 Post title: {title }
Original post: {text}
Original summary: {summary }
Feedback: {feedback }
New summary: {refinement }
Question: Does the new summary incorporate the feed-
back provided? Answer Yes or No.
Answer:
PROMPT 3 You will be given a Reddit post title, its content, an origi-
nal summary of that post, and feedback for that summary.
Then, your goal will be to determine whether the new sum-
mary improves upon the original with respect to provided
feedback.

--- PAGE 30 ---
Training Language Models with Language Feedback at Scale
Post title: {title }
Post content: {text}
Original summary: {summary }
Feedback: {feedback }
New summary: {refinement }
Question: Does the new summary incorporate the feed-
back provided? Answer True or False.
Answer:
PROMPT 4 Here’s a summary of a Reddit post, feedback on the sum-
mary, and a new summary. You will be asked to determine
whether the new summary incorporates the feedback pro-
vided.
A good summary is a short piece of text that has the
essence of the original text. A good summary tries to
accomplish the same purpose and conveys the same infor-
mation as the original text. Remember, you will be asked
to determine whether the new summary incorporates the
feedback provided.
Post title: {title }
Below, there’s the content of the post that was summa-
rized.
Original Post: {text}
Remember, you will be asked to determine whether the
new summary incorporates the feedback provided. Here’s
the original summary.
Original summary: {summary }
Remember, you will be asked to determine whether the
new summary incorporates the feedback provided. A
human then provided feedback on the above summary.
Feedback: {feedback }
Based on this feedback, a new summary was written.
New summary: {refinement }
Does this new summary incorporate the feedback pro-
vided? Answer Yes or No.
Answer:

--- PAGE 31 ---
Training Language Models with Language Feedback at Scale
PROMPT 5 Here’s a summary of a Reddit post, feedback on the sum-
mary, and a new summary. You will be asked to determine
whether the new summary incorporates the feedback pro-
vided.
The feedback was:
Feedback: feedback
Here’s the post that was summarized in the first place.
Post title: {title }
Original Post: {text}
Remember, you will be asked to determine whether the
new summary incorporates the feedback provided. Here’s
the original summary.
Original summary: {summary }
Remember, you will be asked to determine whether the
new summary incorporates the feedback provided. A
human then provided feedback on the above summary.
Here’s the feedback again.
Feedback: {feedback }
Based on this feedback, a new summary was written.
New summary: {refinement }
Does this new summary incorporate the feedback pro-
vided? Answer True or False.
Answer:
Table 9: Prompt templates used for InstructRM Ensemble.
J.3. Finetuning Prompts
In Table 10, we report the prompts we use for finetuning on summaries and finetuning on feedback + refinements. The
completion for finetuning on summaries indicates that we can have completions generated from various sources, i.e., either
initial summaries from FeedMe , refinements generated with our method, or ideal human written summaries. For finetuning
feedback + refinements, we first generate the feedback and then the refinement.
Methods Prompt Completion
FINETUNING ON
SUMMARIESWrite an excellent summary of the given text. {summary/refinement/human summary }
Title:{title }
Text:{post}

--- PAGE 32 ---
Training Language Models with Language Feedback at Scale
TL;DR:
FINETUNING ON
FEEDBACK
+ R EFINEMENTSWrite an excellent summary that incorporates the
feedback on the given summary and is better than
the given summary.{feedback }
Improved TL;DR: {refinement }
###
Title:{title }
Text:{post}
Summary: {summary }
Feedback on summary:
Table 10: Prompt templates used for Finetuning on Summaries and Feedback + Refinement.
J.4. Reward Model Prompts
Reward Model Type Prompt Completion
BINARY RM Title:{title } {" Yes"/" No" }
Text:{post}
TL;DR: {summary A/summary B}
Question: Is the above an excellent summary of
the given text? An excellent summary is coherent,
accurate, concise, and detailed. Answer with Yes
or No.
Answer:
COMPARISON RM Title:{title } {" A"/" B" }
Text:{post}
Summary A: {summary A}
Summary B: {summary B}
Question: Which summary is the better one? An
excellent summary is coherent, accurate, concise,
and detailed. Answer with A or B.
Answer:
Table 11: Prompt templates used for training the reward model with the language model loss. Both classification and
comparison prompts are shown.
