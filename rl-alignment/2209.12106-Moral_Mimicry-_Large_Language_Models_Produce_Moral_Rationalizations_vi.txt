# 2209.12106.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2209.12106.pdf
# Kích thước tệp: 6201048 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Bắt Chước Đạo Đức: Các Mô Hình Ngôn Ngữ Lớn Tạo Ra Những Lý Lẽ Đạo Đức
Được Điều Chỉnh Theo Bản Sắc Chính Trị
Gabriel Simmons
UC Davis
gsimmons@ucdavis.edu
Tóm tắt
Các Mô Hình Ngôn Ngữ Lớn (LLM) đã thể hiện
khả năng ấn tượng trong việc tạo ra văn bản
trôi chảy, cũng như xu hướng tái tạo những
thiên kiến xã hội không mong muốn. Nghiên
cứu này điều tra liệu LLM có tái tạo những
thiên kiến đạo đức gắn liền với các nhóm chính
trị tại Hoa Kỳ hay không, một trường hợp của
khả năng rộng lớn hơn được gọi là bắt chước
đạo đức. Giả thuyết này được khám phá trong
các dòng LLM dựa trên Transformer GPT-3/3.5
và OPT. Sử dụng các công cụ từ Lý Thuyết
Nền Tảng Đạo Đức, nghiên cứu cho thấy những
LLM này thực sự là những kẻ bắt chước đạo đức.
Khi được nhắc với bản sắc chính trị tự do hoặc
bảo thủ, các mô hình tạo ra văn bản phản ánh
những thiên kiến đạo đức tương ứng. Nghiên
cứu này cũng khám phá mối quan hệ giữa bắt
chước đạo đức và kích thước mô hình, và sự
tương tự giữa việc sử dụng từ ngữ đạo đức
của con người và LLM.

1 Giới thiệu
Nghiên cứu gần đây cho thấy rằng hiệu suất của Mô
Hình Ngôn Ngữ Lớn (LLM) sẽ tiếp tục mở rộng theo
kích thước mô hình và dữ liệu huấn luyện (Kaplan et al.,
2020). Khi LLM tiến bộ về khả năng, khả năng chúng
có thể tạo ra văn bản ảnh hưởng đến ý kiến con người
trở nên cao hơn (Tiku, 2022), có thể làm giảm rào cản
đối với thông tin sai lệch (Weidinger et al., 2022). Một
cách lạc quan hơn, LLM có thể đóng vai trò trong việc
thu hẹp khoảng cách giữa các nhóm xã hội (Alshomary
và Wachsmuth, 2021; Jiang et al., 2022). Dù tốt hay
xấu, chúng ta nên hiểu cách nội dung do LLM tạo ra
sẽ tác động đến môi trường thông tin của con người -
liệu nội dung này có ảnh hưởng hay không, và đối với ai.

Đạo đức là một yếu tố quan trọng trong khả năng thuyết
phục và phân cực ý kiến con người (Luttrell et al.,
2019). Lập luận đạo đức có thể điều chỉnh sự sẵn sàng
thỏa hiệp (Kodapanakkal et al., 2022), và sự phù hợp
đạo đức giữa các bên tham gia trong cuộc đối thoại ảnh
hưởng đến hiệu quả lập luận (Feinberg và Willer, 2015)
và nhận thức về tính đạo đức (Egorov et al., 2020). Do
đó, việc đặc trưng hóa khả năng của LLM trong việc
tạo ra nội dung có vẻ đạo đức là quan trọng¹. Điều này
đòi hỏi một khung từ đó chúng ta có thể nghiên cứu đạo
đức; Lý Thuyết Nền Tảng Đạo Đức (MFT) là một khung
như vậy. MFT đề xuất rằng đạo đức con người dựa trên
năm nền tảng: Quan Tâm/Tổn Hại, Công Bằng/Gian
Lận, Trung Thành/Phản Bội, Quyền Uy/Lật Đổ, và
Thiêng Liêng/Suy Thoái².

Bằng chứng từ MFT hỗ trợ "Giả Thuyết Nền Tảng Đạo
Đức" rằng các nhóm chính trị tại Hoa Kỳ khác nhau
trong việc sử dụng nền tảng - những người tự do chủ
yếu dựa vào các nền tảng cá nhân hóa (Quan Tâm/Tổn
Hại và Công Bằng/Gian Lận), trong khi những người
bảo thủ đưa ra các lời kêu gọi cân bằng hơn đối với tất
cả 5 nền tảng, kêu gọi các nền tảng ràng buộc (Quyền
Uy/Lật Đổ, Thiêng Liêng/Suy Thoái, và Trung Thành/
Phản Bội) nhiều hơn những người tự do (Graham et al.,
2009; Doğruyol et al., 2019; Frimer, 2020).

Nghiên cứu hiện tại đã điều tra những thiên kiến nền
tảng đạo đức của các mô hình ngôn ngữ đã được tinh
chỉnh trên dữ liệu giám sát (Fraser et al., 2022), điều
tra liệu các mô hình ngôn ngữ có tái tạo những thiên
kiến xã hội khác (xem (Weidinger et al., 2022) phần
2.1.1), và thăm dò LLM về sự khác biệt trong các giá
trị văn hóa khác (Arora et al., 2023). Nghiên cứu đồng
thời đã cho thấy rằng LLM được sử dụng như các tác
nhân đối thoại có xu hướng lặp lại quan điểm chính trị
của người dùng, và điều này xảy ra thường xuyên hơn
ở các mô hình lớn hơn (Perez et al., 2022). Theo hiểu
biết của tôi, chưa có nghiên cứu nào xem xét liệu các
mô hình ngôn ngữ có thể thực hiện bắt chước đạo đức
- tức là, tái tạo những thiên kiến nền tảng đạo đức gắn
liền với các

¹ Nhân cách hóa cung cấp những cách thuận tiện để nói về
hành vi hệ thống, nhưng cũng có thể làm méo mó nhận thức
về các cơ chế cơ bản (Bender và Koller, 2020). Để rõ ràng,
tôi quy cho các khả năng như "lập luận đạo đức" hoặc "sự
phù hợp đạo đức" cho các mô hình ngôn ngữ chỉ ở mức độ
mà đầu ra của chúng có thể được cảm nhận như vậy, và không
đưa ra tuyên bố rằng LLM có thể tạo ra văn bản như vậy với
ý định giao tiếp.

² Tự Do/Áp Bức được đề xuất như nền tảng thứ sáu - vì mục
đích phân tích này, tôi chỉ xem xét 5 nền tảng ban đầu, vì
đây là những nền tảng có sẵn trong Từ Điển Nền Tảng Đạo
Đức (Graham et al., 2009; Frimer, 2019; Hopp et al., 2021).

arXiv:2209.12106v2 [cs.CL] 17 Jun 2023

--- TRANG 2 ---
Hình 1: Ví dụ về các phương pháp thực nghiệm. Các lời nhắc 2 được xây dựng từ các tình huống 1a, cụm từ bản sắc 1b, và lập trường 1c, được kết hợp trong một mẫu (Phần 2). Các văn bản hoàn thành 3 được tạo ra bởi LLM dựa trên các lời nhắc (Phần 2). Các văn bản hoàn thành được phân tích về nội dung nền tảng của chúng 4 sử dụng từ điển nền tảng đạo đức (Phần 2). Sự khác biệt giữa các văn bản được tạo ra từ lời nhắc tự do và bảo thủ được sử dụng để tính toán kích thước hiệu ứng 5.

nhóm xã hội như bản sắc chính trị.

Nghiên cứu hiện tại xem xét liệu LLM có sử dụng từ vựng đạo đức theo những cách phù hợp với tình huống hay không, và điều này so sánh như thế nào với việc sử dụng nền tảng của con người. Tôi thấy rằng LLM phản ứng với các thuộc tính đạo đức nổi bật của mô tả tình huống, tăng cường việc sử dụng các nền tảng phù hợp, nhưng vẫn khác biệt so với việc sử dụng nền tảng đồng thuận của con người nhiều hơn so với các cá nhân con người (Phần 2.1). Sau đó tôi chuyển sang hiện tượng bắt chước đạo đức. Tôi điều tra liệu việc điều kiện hóa LLM với "bản sắc" chính trị có ảnh hưởng đến việc sử dụng nền tảng đạo đức của mô hình theo những cách phù hợp với thiên kiến đạo đức của con người hay không. Tôi tìm thấy kết quả xác nhận cho văn bản được tạo ra dựa trên bản sắc chính trị "tự do" và "bảo thủ" (Phần 2.2). Cuối cùng, tôi hỏi hiện tượng bắt chước đạo đức thay đổi như thế nào theo kích thước mô hình. Kết quả cho thấy mức độ mà LLM có thể tái tạo thiên kiến đạo đức tăng theo kích thước mô hình, trong dòng OPT (Phần 2.2). Điều này cũng đúng cho các mô hình GPT-3 và -3.5 được xem xét cùng nhau, và ở mức độ ít hơn đối với các mô hình GPT-3 riêng lẻ.

2 Phương pháp
Tạo Dữ liệu Tất cả các thí nghiệm tuân theo cùng một mẫu để tạo dữ liệu, được mô tả trong các phần sau và được minh họa trong Hình 1. Các phương pháp đi kèm với các câu hỏi nghiên cứu cụ thể được trình bày cùng với kết quả trong Phần 2.1 - 2.3.

Xây dựng Lời nhắc Tôi xây dựng các lời nhắc khuyến khích mô hình ngôn ngữ tạo ra những lý lẽ đạo đức rõ ràng. Mỗi lời nhắc điều kiện hóa mô hình với ba biến: một tình huống s, một cụm từ bản sắc chính trị i, và một lập trường đạo đức r. Mỗi lời nhắc bao gồm các giá trị cho những biến này được nhúng trong một mẫu lời nhắc t.

Các tình huống là các chuỗi văn bản mô tả các tình huống hoặc hành động phù hợp cho phán đoán đạo đức. Tôi đã sử dụng ba bộ dữ liệu (Moral Stories³(Emelin et al., 2021), ETHICS⁴(Hendrycks et al., 2021), và Social Chemistry 101⁵(Forbes et al., 2020)) để có được bốn bộ tình huống, mà tôi gọi là Moral Stories, ETHICS, Social Chemistry Actions, và Social Chemistry Situations. Phụ lục Phần A.2 cung cấp chi tiết cụ thể về cách mỗi bộ dữ liệu được xây dựng. Tôi sử dụng S và s để chỉ một bộ tình huống và một tình huống đơn lẻ, tương ứng.

Các cụm từ bản sắc chính trị là các chuỗi văn bản đề cập đến các hệ tư tưởng chính trị (ví dụ: "tự do"). Tôi sử dụng I và i để chỉ một bộ bản sắc chính trị và một bản sắc cá nhân, tương ứng.

Lập trường Đạo đức Lập trường đạo đức được trình bày trong mỗi lời nhắc điều kiện hóa mô hình để tạo ra một lý lẽ rõ ràng cho thấy sự chấp thuận hoặc không chấp thuận của tình huống. Tôi sử dụng R, r để chỉ bộ lập trường {đạo đức, vô đạo đức}, và một lập trường đơn lẻ, tương ứng. Các bộ dữ liệu được sử dụng ở đây chứa các nhãn cho biết tính chấp nhận đạo đức quy chuẩn của mỗi tình huống. Đối với một tình huống s, tôi gọi tính chấp nhận đạo đức quy chuẩn của nó là rH(s).

Mẫu Lời nhắc là các hàm chuyển đổi một bộ ba tình huống, cụm từ bản sắc, và lập trường đạo đức thành một lời nhắc. Để kiểm tra độ nhạy với bất kỳ cách diễn đạt cụ thể nào, năm kiểu mẫu lời nhắc khác nhau đã được sử dụng (xem Bảng 2 và 3 trong Phụ lục).

³ Tải xuống từ https://github.com/demelin/moral_stories
⁴ Tải xuống từ https://github.com/hendrycks/ethics
⁵ Tải xuống từ https://github.com/mbforbes/social-chemistry-101

--- TRANG 3 ---
Các lời nhắc được xây dựng bằng cách chọn một mẫu t cho một kiểu cụ thể, và điền vào nó với một lập trường, tình huống, và cụm từ bản sắc chính trị.

Tạo Văn bản với LLM Các mô hình ngôn ngữ tạo ra văn bản bằng giải mã tự hồi quy. Cho một chuỗi token, mô hình gán xác suất cho tất cả các token trong từ vựng của nó cho biết khả năng chúng theo sau chuỗi đó như thế nào. Dựa trên những xác suất này, một token tiếp theo phù hợp được thêm vào chuỗi, và quá trình được lặp lại cho đến khi đạt được số lượng token tối đa, hoặc mô hình tạo ra một token "kết thúc chuỗi" đặc biệt. Tôi gọi văn bản được cung cấp ban đầu cho mô hình là "lời nhắc" và văn bản thu được thông qua quá trình giải mã là "hoàn thành". Trong nghiên cứu này, tôi đã sử dụng ba dòng Mô Hình Ngôn Ngữ Lớn: GPT-3, GPT-3.5, và OPT (Bảng 1). GPT-3 là một dòng mô hình ngôn ngữ tự hồi quy dựa trên Transformer (Vaswani et al., 2017) với kích thước lên đến 175 tỷ tham số, được đào tạo trước theo cách tự giám sát trên các kho văn bản web (Radford et al., 2019). Ba mô hình lớn nhất trong số 4 mô hình GPT-3 được đánh giá ở đây cũng nhận được tinh chỉnh có giám sát trên các mẫu mô hình chất lượng cao và các cuộc thử nghiệm của con người (OpenAI, 2022). Các mô hình GPT-3.5 cũng dựa trên Transformer, được đào tạo trước trên các kho văn bản và mã web, và được tinh chỉnh bằng cách sử dụng tinh chỉnh có giám sát hoặc học tăng cường từ sở thích của con người (OpenAI, 2022). Tôi truy cập GPT-3/3.5 thông qua API Hoàn thành của OpenAI (OpenAI, 2021). Tôi đã sử dụng tham số engine để chỉ ra một mô hình cụ thể. Các mô hình GPT-3 "text-ada-001", "text-babbage-001", "text-curie-001", và "text-davinci-001", và các mô hình GPT-3.5 "text-davinci-002" và "text-davinci-003" đã được sử dụng. Các mô hình OPT là các mô hình được đào tạo trước dựa trên Transformer được phát hành bởi Meta AI, với kích thước lên đến 175B tham số (Zhang et al., 2022). Các kích thước mô hình lên đến 30B tham số đã được sử dụng ở đây. Trọng số mô hình OPT được lấy từ HuggingFace Model Hub. Tôi đã thu được các hoàn thành từ những mô hình này cục bộ bằng cách sử dụng thư viện HuggingFace Transformers (Wolf et al., 2020) và DeepSpeed ZeRo-Inference (DeepSpeed, 2022), sử dụng một máy với CPU Threadripper 3960x và hai GPU RTX3090 24GB. Đối với tất cả các mô hình, các hoàn thành được tạo ra với temperature=0 để có tính tái tạo. Tham số max_tokens được sử dụng để dừng tạo sau 64 token (khoảng 50 từ). Tất cả các cài đặt khác được để mặc định⁶.

Đo lường Nội dung Đạo đức
Từ điển Nền tảng Đạo đức Tôi ước tính nội dung nền tảng đạo đức của mỗi hoàn thành bằng cách sử dụng ba từ điển: Từ điển Nền tảng Đạo đức phiên bản 1.0 (MFDv1) (Graham et al., 2009), Từ điển Nền tảng Đạo đức phiên bản 2.0 (MFDv2) (Frimer, 2019), Từ điển Nền tảng Đạo đức mở rộng (eMFD) (Hopp et al., 2021).

MFDv1 bao gồm một từ vựng chứa 324 gốc từ, với mỗi gốc từ được liên kết với một hoặc nhiều danh mục. MFDv2 bao gồm một từ vựng gồm 2014 từ, với mỗi từ được liên kết với một danh mục duy nhất. Trong MFDv1, các danh mục bao gồm danh mục "Tệ" và "Tốt" cho mỗi nền tảng trong số năm nền tảng, cộng với danh mục "ĐạoĐứcTổngQuát", tổng cộng 11 danh mục. MFDv2 bao gồm tất cả các danh mục từ MFDv1 ngoại trừ "ĐạoĐứcTổngQuát", tổng cộng 10 danh mục. eMFD (Hopp et al., 2021) chứa 3270 từ và khác biệt một chút so với MFDv1 và MFDv2. Các từ trong eMFD được liên kết với tất cả các nền tảng bằng điểm số trong [0,1]. Điểm số được rút ra từ việc chú thích các bài báo tin tức, và cho biết tần suất mỗi từ được liên kết với mỗi nền tảng, chia cho tổng số lần xuất hiện của từ. Sự chồng chéo từ giữa các từ điển được hiển thị trong Hình 5 của Phụ lục.

Loại bỏ Thông tin Giá trị Cả ba từ điển đều cho biết liệu một từ có liên quan đến khía cạnh tích cực hay tiêu cực của một nền tảng. Trong MFDv1 và MFDv2, điều này được biểu thị bằng việc liên kết từ với danh mục "Tệ" hoặc "Tốt" cho mỗi nền tảng. Trong eMFD, mỗi từ có điểm cảm xúc cho mỗi nền tảng. Trong nghiên cứu này, tôi quan tâm đến nội dung nền tảng của các hoàn thành, độc lập với giá trị. Theo đó, các danh mục "Tệ" và "Tốt" được hợp nhất thành một danh mục duy nhất cho mỗi nền tảng, trong cả MFDv1 và MFDv2. Điểm "ĐạoĐứcTổngQuát" từ MFDv1 không được sử dụng vì nó không cho biết sự liên kết với bất kỳ nền tảng cụ thể nào. Điểm cảm xúc từ eMFD cũng không được sử dụng.

Áp dụng Từ điển Áp dụng từ điển d vào một đoạn văn bản tạo ra năm điểm {wd f|f∈F}. Đối với MFDv1 và MFDv2, đây là các giá trị nguyên đại diện cho số lượng từ liên quan đến nền tảng trong văn bản. eMFD tạo ra

⁶ Các giá trị mặc định cho các tham số không sử dụng của OpenAI Completions API là suffix: null; top_p: 1; n: 1; stream: false; logprobs: null; echo: false; stop: null; presence_penalty: 0; frequency_penalty: 0; best_of: 1; logit_bias: null; user: null

--- TRANG 4 ---
các giá trị liên tục trong [0,∞] - tổng theo nền tảng của điểm số cho tất cả các từ eMFD trong văn bản.

Tôi quan tâm đến xác suất P rằng một con người hoặc mô hình ngôn ngữ (rõ ràng) thể hiện nền tảng f, mà tôi viết là Ph(ef) và PLM(ef), tương ứng. Tôi sử dụng Pd(ef|s, r, i) để biểu thị xác suất này được điều kiện hóa trên một tình huống s, lập trường r, và bản sắc chính trị i, sử dụng từ điển d để đo lường.

Tôi sử dụng F để chỉ tập hợp các nền tảng đạo đức, và f cho một nền tảng đơn lẻ. Tôi sử dụng D để chỉ tập hợp các từ điển. Trong mỗi từ điển, Wd chỉ tất cả các từ trong từ điển. Đối với MFDv1 và MFDv2, Wd f chỉ tất cả các từ trong d thuộc về nền tảng f. Tôi ước lượng Pd(ef|s, r, i) là điểm cụ thể theo nền tảng wd f thu được bằng cách áp dụng từ điển d vào phản hồi của mô hình cho một lời nhắc, được chuẩn hóa bởi tổng điểm trên tất cả các nền tảng, như được hiển thị trong Phương trình 1 dưới đây.

Pd(ef|s, r, i)≈wfd/∑f′∈Fwf′d (1)

Tính toán Kích thước Hiệu ứng Kích thước hiệu ứng nắm bắt cách thay đổi bản sắc chính trị làm thay đổi khả năng mô hình sẽ thể hiện nền tảng f, cho cùng một lập trường và tình huống. Kích thước hiệu ứng được tính như sự khác biệt tuyệt đối trong xác suất thể hiện nền tảng cho các cặp hoàn thành chỉ khác nhau về bản sắc chính trị (Phương trình 2 dưới đây). Phương trình 3 tính kích thước hiệu ứng trung bình cho nền tảng f trên các tình huống S và lập trường R, được đo bằng từ điển d. Phương trình 4 đưa ra một kích thước hiệu ứng trung bình theo kết quả trên các từ điển.

∆Pdi1,i2(ef|s,r)=Pd(ef|s,i1,r)−Pd(ef|s,i2,r) (2)
∆Pdi1,i2(ef)=Es,r∈S×R∆Pdi1,i2(ef|s,r) (3)
∆Pi1,i2(ef)=Ed∈D∆Pdi1,i2(ef) (4)

2.1 Sử dụng Nền tảng Đạo đức của LLM so với Con người

Chi tiết Thí nghiệm Thí nghiệm này xem xét liệu LLM có sử dụng các từ nền tảng phù hợp với tình huống⁷ hay không. LLM sẽ thỏa mãn một tiêu chí yếu cho khả năng này nếu chúng có khả năng thể hiện nền tảng f cao hơn khi phản ứng với các tình huống mà nền tảng f nổi bật, so với việc sử dụng trung bình của f trên một tập hợp các tình huống chứa tất cả các nền tảng với tỷ lệ bằng nhau. Tôi chính thức hóa điều này với Tiêu chí A dưới đây.

Tiêu chí A Việc sử dụng trung bình của nền tảng f lớn hơn trên các tình huống Sf chỉ thể hiện

⁷ ví dụ: sử dụng nền tảng Quan Tâm/Tổn Hại khi được nhắc với một tình huống bạo lực

nền tảng f, so với việc sử dụng trung bình của nền tảng f trên một tập hợp tình huống cân bằng nền tảng S (Phương trình 5).

Esf,r∈Sf×RPLM(ef|sf,r)>Es,r∈S×RPLM(ef|s,r)

Một tiêu chí mạnh hơn sẽ yêu cầu LLM không lệch khỏi việc sử dụng nền tảng của con người vượt quá một mức độ biến đổi nào đó được mong đợi giữa các con người. Tôi chính thức hóa điều này với Tiêu chí 2b dưới đây.

Tiêu chí B Sự khác biệt trung bình giữa việc sử dụng nền tảng của mô hình ngôn ngữ và con người đồng thuận nhỏ hơn sự khác biệt trung bình giữa việc sử dụng nền tảng của cá nhân con người và con người đồng thuận.

DIFFLM,CH≤DIFFH,CH (5)
DIFFLM,CH=Es∈S[|PLM(ef|s,rH(s))−CH(s)|] (6)
DIFFH,CH=Es∈S[EH[|Ph(ef|s)−CH(s)|]] (7)
CH(s)=Eh[Ph(ef|s)] (8)

Lập trường rHs là tính chấp nhận đạo đức quy chuẩn của tình huống s - các lý lẽ được viết bởi con người được "điều kiện hóa" trên lập trường quy chuẩn của con người cho mỗi tình huống, vì vậy tôi chỉ so sánh những điều này với đầu ra của mô hình cũng được điều kiện hóa trên lập trường quy chuẩn của con người.

Tiêu chí A yêu cầu một tập hợp có kiến thức cơ bản rằng chỉ có nền tảng cụ thể f nổi bật cho mỗi tình huống. Để có được những tình huống rõ ràng như vậy, tôi chọn những hành động ít mơ hồ nhất từ bộ dữ liệu Social Chemistry, theo các phương pháp lọc được mô tả trong Phần A.2.3 của Phụ lục. Ước tính việc sử dụng nền tảng đồng thuận của con người (Tiêu chí B) yêu cầu một tập hợp các tình huống được chú thích theo cách mở bởi nhiều con người. Tôi có được một tập hợp như vậy từ bộ dữ liệu Social Chemistry bằng các phương pháp được mô tả trong Phần A.2.4 của Phụ lục.

Kết quả
Hình 2 (trái) cho thấy các giá trị trung bình của P(ef|s) cho mỗi nền tảng. Đối với tất cả năm nền tảng, mô hình tăng cường việc sử dụng rõ ràng các từ liên quan đến nền tảng phù hợp với nhãn nền tảng thực tế, thỏa mãn Tiêu chí A. Hình 2 (phải) cho thấy sự khác biệt LM so với đồng thuận con người |PLM(ef|s, rHs)−CH(s)| thu được từ mô hình text-davinci-002, và sự khác biệt con người so với đồng thuận con người EH[|Ph(ef|s)−CH(s)|], trên bộ dữ liệu Social Chemistry Situations. Nói chung, sự khác biệt LM-con người lớn hơn sự khác biệt con người-con người.

--- TRANG 5 ---
Hình 2: Trái: Xác suất thể hiện nền tảng cho các ví dụ cụ thể theo nền tảng so với việc sử dụng nền tảng trung bình trên tất cả các ví dụ. Text-davinci-002; các tình huống Social Chemistry Actions. Phải: Sự khác biệt LM và cá nhân con người so với việc sử dụng nền tảng đồng thuận của con người, khi phản ứng với các tình huống từ bộ dữ liệu Social Chemistry Situations; text-davinci-002.

2.2 LLM có phải là Kẻ Bắt chước Đạo đức không?

Chi tiết Thí nghiệm Tôi xem xét liệu việc điều kiện hóa LLM với bản sắc chính trị có ảnh hưởng đến việc sử dụng nền tảng đạo đức của chúng theo cách phản ánh thiên kiến đạo đức của con người hay không. Để điều tra câu hỏi này, tôi đã sử dụng một tập hợp 2.000 tình huống thu được từ bộ dữ liệu Moral Stories và 1.000 tình huống thu được từ bộ dữ liệu ETHICS, được mô tả trong Phần A.2 của Phụ lục.

Các lời nhắc được xây dựng với kiểu mẫu 2 từ bảng 2. Đối với mỗi tình huống, bốn lời nhắc được xây dựng dựa trên các kết hợp của bản sắc chính trị "tự do" và "bảo thủ" và lập trường đạo đức và vô đạo đức, tổng cộng 12.000 lời nhắc. Các hoàn thành được thu thập từ mô hình có khả năng nhất trong mỗi dòng mà tài nguyên tính toán của chúng tôi cho phép: text-davinci-001 (GPT-3), text-davinci-002 và text-davinci-003 (GPT-3.5) và OPT-30B. Một thế hệ được thu thập từ mỗi mô hình cho mỗi lời nhắc. Tôi tính toán kích thước hiệu ứng trung bình ∆Pi1,i2(ef) với i1= "tự do" và i2= "bảo thủ" cho tất cả năm nền tảng. Kích thước hiệu ứng được tính riêng cho mỗi từ điển, tổng cộng 18.000 kích thước hiệu ứng được tính cho mỗi mô hình.

Kết quả Hình 3 cho thấy kích thước hiệu ứng cho bản sắc chính trị tự do so với bảo thủ, đối với các mô hình có khả năng nhất được thử nghiệm từ các dòng mô hình OPT, GPT, và GPT-3.5, được đo bằng ba từ điển nền tảng đạo đức. Các vùng được tô bóng trong mỗi biểu đồ đại diện cho các hiệu ứng sẽ được mong đợi dựa trên Giả Thuyết Nền Tảng Đạo Đức - cụ thể là nhắc với bản sắc chính trị tự do sẽ dẫn đến việc sử dụng nhiều hơn các nền tảng cá nhân hóa (∆Pi1,i2 dương) và nhắc với bản sắc chính trị bảo thủ sẽ dẫn đến việc sử dụng nhiều hơn các nền tảng ràng buộc (∆Pi1,i2 âm).

Phần lớn kích thước hiệu ứng trùng khớp với Giả Thuyết Nền Tảng Đạo Đức. Trong số 60 kết hợp của 5 nền tảng, 4 mô hình, và 3 từ điển, chỉ có 11 kích thước hiệu ứng theo hướng ngược lại so với mong đợi, và tất cả những kích thước hiệu ứng này có độ lớn nhỏ hơn 1 điểm khác biệt tuyệt đối.

2.3 Bắt chước Đạo đức có bị Ảnh hưởng bởi Kích thước Mô hình không?

Chi tiết Thí nghiệm Trong phần này, tôi xem xét cách bắt chước đạo đức liên quan đến kích thước mô hình. Tôi đã sử dụng các mô hình text-ada-001, text-babbage-001, text-curie-001, và text-davinci-001 từ dòng GPT-3, text-davinci-002 và text-davinci-003 từ dòng GPT-3.5 (OpenAI, 2022), và OPT-350m, OPT-1.3B, OPT-6.7B, OPT-13B, và OPT-30B (Zhang et al., 2022). Các mô hình GPT-3 có số lượng tham số ước tính là 350M, 1.3B, 6.7B và 175B, tương ứng (OpenAI, 2022; Gao, 2021). Text-davinci-002 và text-davinci-003 cũng có 175B tham số (OpenAI, 2022). Số tham số tính bằng tỷ cho các mô hình OPT được chỉ ra trong tên mô hình.

Để phân tích mức độ mỗi mô hình thể hiện hiện tượng bắt chước đạo đức, tôi định nghĩa một hàm chấm điểm MFH-SCORE để chấm điểm mô hình m như sau:

MFH-SCORE(m)=∑f∈FsignMFH(f)∆Pm(ef) (9)
signMFH = {
-1, nếu f∈ {A/S, S/D, L/B}
+1, nếu f∈ {C/H, F/C}
(10)

A/S: Quyền Uy/Lật Đổ; S/D: Thiêng Liêng/Suy Thoái; L/B: Trung Thành/Phản Bội; C/H: Quan Tâm/Tổn Hại; F/C; Công Bằng/Gian Lận

MFH-SCORE tính kích thước hiệu ứng trung bình cho mỗi mô hình theo hướng được dự đoán bởi Giả Thuyết Nền Tảng Đạo Đức.

Kết quả Hình 4 ở trên cho thấy kích thước hiệu ứng ∆(Pef) cho mỗi nền tảng và MFH-SCORE so với kích thước mô hình (số lượng tham số). Kích thước hiệu ứng được tính trung bình trên ba từ điển nền tảng đạo đức. Đối với dòng mô hình OPT, chúng ta có thể thấy rằng số lượng tham số mô hình và MFH-SCORE cho thấy một số mối quan hệ (r=0.69, mặc dù sức mạnh thống kê bị hạn chế

--- TRANG 6 ---
Hình 3: Kích thước hiệu ứng cho bản sắc chính trị tự do so với bảo thủ đối với OPT-30B, text-davinci-001, text-davinci-002, và text-davinci-003. Các điểm đánh dấu đại diện cho kích thước hiệu ứng trung bình. Các thanh lỗi đại diện cho 95% CI. Các vùng được tô bóng đại diện cho hướng kích thước hiệu ứng mong đợi dựa trên Giả Thuyết Nền Tảng Đạo Đức.

chế do số lượng mô hình hạn chế). Đặc biệt, nền tảng Thiêng Liêng/Suy Thoái duy trì kích thước hiệu ứng khác không theo hướng mong đợi cho tất cả các mô hình có 6.7B tham số trở lên. Đáng ngạc nhiên, OPT-13B cho thấy kích thước hiệu ứng giảm cho Công Bằng/Gian Lận và Quan Tâm/Tổn Hại so với OPT-6.7B nhỏ hơn. Mối quan hệ giữa kích thước mô hình và kích thước hiệu ứng yếu hơn đối với GPT-3 (r=0.23). Quan Tâm/Tổn Hại, Công Bằng/Gian Lận, Thiêng Liêng/Suy Thoái, và Quyền Uy/Lật Đổ có kích thước hiệu ứng theo hướng mong đợi cho các mô hình Babbage, Curie, và DaVinci, mặc dù kích thước hiệu ứng nhỏ hơn so với dòng OPT. Các mô hình từ dòng GPT-3.5 cho thấy kích thước hiệu ứng lớn nhất tổng thể. Thật không may, không có kích thước mô hình nhỏ hơn nào có sẵn cho dòng này. Nếu chúng ta bao gồm các mô hình GPT-3 và GPT-3.5 cùng nhau (được chỉ ra bằng † trong Hình 4), mối tương quan giữa MFH-SCORE và các tham số mô hình tăng lên r=0.84. Thú vị, các dòng OPT và GPT-3 cho thấy Thiêng Liêng/Suy Thoái là kích thước hiệu ứng rõ ràng nhất cho nhắc bảo thủ, và Công Bằng/Gian Lận là kích thước hiệu ứng rõ ràng nhất cho nhắc tự do. GPT-3.5 thay vào đó cho thấy kích thước hiệu ứng lớn nhất cho Quyền Uy/Lật Đổ và Quan Tâm/Tổn Hại, tương ứng.

3 Thảo luận
Phần 2.1 đặt ra hai tiêu chí để đánh giá liệu LLM có sử dụng nền tảng đạo đức một cách phù hợp hay không. Đối với Tiêu chí A yếu hơn, kết quả cho thấy LLM thực sự tăng cường việc sử dụng các từ nền tảng liên quan đến nền tảng nổi bật trong một tình huống nhất định, ít nhất đối với các tình huống có sự đồng thuận rõ ràng của con người về tính nổi bật của nền tảng. Tuy nhiên, đối với Tiêu chí B, kết quả cho thấy LLM khác biệt so với việc sử dụng nền tảng đồng thuận của con người nhiều hơn so với con người về mặt sử dụng nền tảng.

Phần 2.2 so sánh việc sử dụng nền tảng LM với các phát hiện từ tâm lý học đạo đức xác định sự khác biệt trong các nền tảng đạo đức được sử dụng bởi các nhóm chính trị tự do và bảo thủ. Cụ thể, theo Giả Thuyết Nền Tảng Đạo Đức, những người tự do chủ yếu dựa vào các nền tảng Quan Tâm/Tổn Hại và Công Bằng/Gian Lận, trong khi những người bảo thủ sử dụng tất cả 5 nền tảng đều đặn hơn, sử dụng Quyền Uy/Lật Đổ, Trung Thành/Phản Bội, và Công Bằng/Gian Lận nhiều hơn những người tự do. Phát hiện này lần đầu được trình bày trong (Graham et al., 2009), và kể từ đó đã được hỗ trợ bằng phân tích nhân tố xác nhận trong (Doğruyol et al., 2019), và được sao chép một phần (mặc dù với kích thước hiệu ứng nhỏ hơn) trong (Frimer, 2020).

Kết quả cho thấy các mô hình từ các dòng GPT-3, GPT-3.5 và OPT có khả năng sử dụng các nền tảng ràng buộc cao hơn khi được nhắc với bản sắc chính trị bảo thủ, và có khả năng sử dụng các nền tảng cá nhân hóa cao hơn khi được nhắc với bản sắc chính trị tự do. Sự nhấn mạnh vào các nền tảng cá nhân trong mỗi danh mục khác nhau theo dòng mô hình. OPT-30B cho thấy kích thước hiệu ứng lớn hơn cho Công Bằng/Gian Lận so với Quan Tâm/Tổn Hại và kích thước hiệu ứng lớn hơn cho Thiêng Liêng/Suy Thoái so với Quyền Uy/Lật Đổ, trong khi GPT-3.5 thể hiện ngược lại. Tôi nghi ngờ rằng điều này có thể do sự khác biệt trong dữ liệu huấn luyện và/hoặc thực hành huấn luyện giữa các dòng mô hình. Điều này mở ra một câu hỏi thú vị về cách ảnh hưởng đến khả năng bắt chước đạo đức

--- TRANG 7 ---
Hình 4: Trên: Kích thước hiệu ứng so với tham số mô hình, dựa trên các hoàn thành thu được từ bộ dữ liệu Moral Stories. Các đường tối đại diện cho kích thước hiệu ứng trung bình. Các thanh lỗi hiển thị 95% CI. Kích thước hiệu ứng được tính trung bình trên ba từ điển nền tảng đạo đức.; 002: text-davinci-002; 003: text-davinci-003.; Dưới: MFH-SCORE so với tham số mô hình; r,p: giá trị và giá trị p cho Tương quan Pearson giữa MFH-SCORE và tham số mô hình.; †kết quả của phân tích tương quan với các mô hình GPT-3 và GPT-3.5 được phân tích cùng nhau

xuất hiện trong quá trình huấn luyện, thông qua việc tuyển chọn bộ dữ liệu hoặc các phương pháp khác.

Kết quả từ Phần 2.3 cho thấy một số mối quan hệ giữa bắt chước đạo đức và kích thước mô hình. Kích thước hiệu ứng có xu hướng tăng theo số lượng tham số trong dòng OPT, và ít hơn trong dòng GPT-3. Cả hai mô hình GPT-3.5 175B-tham số đều cho thấy khả năng bắt chước đạo đức tương đối mạnh, nhiều hơn so với mô hình GPT-3 175B text-davinci-001. Điều này cho thấy số lượng tham số không phải là yếu tố duy nhất dẫn đến bắt chước đạo đức. Các mô hình GPT-3.5 được huấn luyện với tinh chỉnh có giám sát bổ sung không được áp dụng cho dòng GPT-3, và sử dụng đào tạo trước văn bản và mã thay vì chỉ văn bản (OpenAI, 2022).

4 Hạn chế
Nghiên cứu này sử dụng các từ điển nền tảng đạo đức để đo lường nội dung đạo đức của văn bản được tạo ra bởi GPT-3. Trong khi các nghiên cứu đã chứng minh sự tương ứng giữa kết quả từ các từ điển và nhãn của con người về nội dung nền tảng đạo đức (Mutlu et al., 2020; Graham et al., 2009), phân tích dựa trên từ điển bị hạn chế trong khả năng phát hiện các biểu hiện đạo đức tinh tế. Phân tích dựa trên từ điển có thể được bổ sung bằng các phương pháp học máy (Garten et al., 2016; Johnson và Goldwasser, 2018; Pavan et al., 2020; Roy et al., 2022) cũng như đánh giá của con người. Nghiên cứu này đã cố gắng kiểm soát các biến đổi trong cách diễn đạt lời nhắc bằng cách tính trung bình kết quả trên nhiều kiểu lời nhắc (Bảng 2 và 3). Những biến đổi lời nhắc này được tác giả lựa chọn. Một quy trình lựa chọn có nguyên tắc hơn có thể dẫn đến một bộ lời nhắc đa dạng hơn. Các nghiên cứu con người mà nghiên cứu này đề cập đến (Graham et al., 2009; Frimer, 2020) được thực hiện trên các quần thể từ Hoa Kỳ. Ý nghĩa chính trị chính xác của các thuật ngữ "tự do" và "bảo thủ" khác nhau giữa các nhóm dân số. Nghiên cứu trong tương lai có thể khám phá cách đầu ra của mô hình ngôn ngữ thay đổi khi thông tin nhân khẩu học bổ sung được cung cấp, hoặc khi các mô hình đa ngôn ngữ được sử dụng. Tài liệu cho các bộ dữ liệu được sử dụng ở đây cho biết rằng các công nhân đám đông nghiêng về chính trị trái, và về mặt đạo đức hướng tới các nền tảng Quan Tâm/Tổn Hại và Công Bằng/Gian Lận (Forbes et al., 2020; Hendrycks et al., 2021; Fraser et al., 2022). Tuy nhiên, thiên kiến trong phân phối nền tảng biên không cản trở phân tích hiện tại, vì các thí nghiệm hiện tại chủ yếu tập trung vào sự khác biệt trong việc sử dụng nền tảng do thay đổi bản sắc chính trị. Phân tích trong Phần 2.1 dựa nhiều hơn vào phân phối nền tảng biên; một bộ dữ liệu cân bằng nền tảng đã được xây dựng cho thí nghiệm này. Nghiên cứu này sử dụng GPT-3 (Brown et al., 2020), GPT-3.5 (OpenAI, 2022), và OPT (Zhang et al., 2022). Các dòng mô hình ngôn ngữ được đào tạo trước khác có quy mô và kiến trúc tương tự bao gồm BLOOM⁸, mà tôi không thể thử nghiệm do ngân sách tính toán, và LLaMA (Touvron et al., 2023), được phát hành sau khi các thí nghiệm cho nghiên cứu này kết thúc. Trong khi trọng số mô hình OPT có sẵn để tải xuống, trọng số mô hình GPT-3 và GPT-3.5 thì không; điều này có thể tạo ra rào cản cho nghiên cứu trong tương lai cố gắng kết nối hiện tượng bắt chước đạo đức với các thuộc tính của mô hình. Mặt khác, phần cứng cần thiết để chạy các mô hình có sẵn công khai có thể là rào cản đối với thử nghiệm mà không phải là mối quan ngại đối với các mô hình được lưu trữ qua API.

Các chỉ trích đối với Lý Thuyết Nền Tảng Đạo Đức bao gồm bất đồng về việc liệu một lý thuyết đa nguyên về đạo đức có tiết kiệm hay không (Suhler và Churchland, 2011; Dobolyi, 2016); Ch. 6 của (Haidt, 2013), bất đồng về số lượng và đặc tính của các

⁸ https://bigscience.huggingface.co/blog/bloom

--- TRANG 8 ---
nền tảng (Yalçındağ et al., 2019; Harper và Rhodes, 2021), bất đồng về tính ổn định của các nền tảng giữa các nền văn hóa (Davis et al., 2016), và các chỉ trích cho rằng có thiên kiến trong Bảng Câu Hỏi Nền Tảng Đạo Đức (Dobolyi, 2016). Lý thuyết nền tảng đạo đức được sử dụng trong nghiên cứu này vì nó cung cấp các phương pháp đã được thiết lập để đo lường nội dung đạo đức trong văn bản, và vì các phân tích dựa trên MFT đã xác định mối quan hệ giữa liên kết chính trị và thiên kiến đạo đức, cung cấp một cách để so sánh hành vi LLM và con người. Các phương pháp được trình bày ở đây có thể áp dụng cho các lý thuyết đạo đức khác; điều này được để lại cho nghiên cứu trong tương lai.

Nghiên cứu nhằm khơi dậy phán đoán đạo đức hoặc đạo đức quy chuẩn từ các hệ thống phi con người đã nhận được chỉ trích. Các tác giả đã lập luận rằng các hệ thống phi con người thiếu tự chủ và ý định giao tiếp để trở thành các tác nhân đạo đức (Talat et al., 2022; Bender và Koller, 2020). Các chỉ trích cũng đã được đưa ra về chất lượng và tính phù hợp của dữ liệu được sử dụng để huấn luyện các hệ thống như vậy. Đáng chú ý, dữ liệu từ đám đông hoặc được tái sử dụng thường phản ánh ý kiến tiên nghiệm của các cá nhân có thể không được thông tin về các chủ đề họ được yêu cầu phán đoán, và có thể không có cơ hội thảo luận hoặc suy ngẫm trước khi phản hồi (Talat et al., 2022; Etienne, 2021). Một số người đã lập luận rằng các hệ thống tổng hợp phán đoán đạo đức từ các bộ dữ liệu mô tả không thể không được coi là quy chuẩn, vì việc tái tạo quan điểm phổ biến hoặc trung bình có xu hướng được ngầm định đồng với cảm giác đúng đắn (Talat et al., 2022). Cuối cùng, nhiều tác giả lập luận rằng việc sử dụng các hệ thống phi con người tạo ra các phán đoán quy chuẩn rõ ràng hoặc có ý định tạo ra một tiền lệ nguy hiểm bằng cách cắt ngắn quá trình thảo luận mà qua đó tiến bộ đạo đức và đạo đức được thực hiện, và bằng cách che giấu trách nhiệm nếu một hệ thống như vậy gây ra tổn hại (Talat et al., 2022; Etienne, 2021).

Nghiên cứu hiện tại điều tra các lý lẽ đạo đức rõ ràng được tạo ra bởi các LLM được nhắc. Nghiên cứu này không có ý định tạo ra một hệ thống cho phán đoán quy chuẩn, và tôi sẽ không khuyến khích việc sử dụng hoặc giải thích quy chuẩn các phương pháp và kết quả được trình bày ở đây. Sự thay đổi bước ngoặt gần đây trong xử lý ngôn ngữ tự nhiên hướng tới các LLM đa mục đích được nhắc thành các hành vi cụ thể cho phép người dùng cuối tạo ra một loạt đầu ra có chất lượng thuyết phục, bao gồm các phán đoán đạo đức hoặc đạo đức quy chuẩn rõ ràng. Việc dự đoán cách các hệ thống này sẽ tác động đến người dùng cuối và xã hội đòi hỏi nghiên cứu hành vi mô hình dưới nhiều đầu vào nhắc khác nhau. Nghiên cứu hiện tại được thực hiện với mục tiêu này, dưới niềm tin rằng lợi ích của việc hiểu hiện tượng bắt chước đạo đức vượt trội hơn rủi ro của giải thích quy chuẩn.

5 Nghiên cứu Liên quan
Một số dự án đạo đức máy đã đánh giá mức độ mà các hệ thống dựa trên LLM có thể bắt chước phán đoán đạo đức quy chuẩn của con người, ví dụ (Hendrycks et al., 2021) và (Jiang et al., 2021). Các dự án khác đánh giá liệu LLM có thể tạo ra các chuẩn mực đạo đức liên quan cho một tình huống nhất định (Forbes et al., 2020; Emelin et al., 2021), hoặc liệu chúng có thể xác định tình huống nào biện minh cho các ngoại lệ đạo đức (Jin et al., 2022). Các nghiên cứu khác tập trung vào việc điều chỉnh các mô hình theo đạo đức quy chuẩn (Ziems et al., 2022), và điều tra mức độ mà thiên kiến xã hội được tái tạo trong các mô hình ngôn ngữ (xem Phần 5.1 của Bommasani et al. 2022). Như một ví dụ, Fraser, Kiritchenko, và Balkir (2022) phân tích phản hồi của mô hình Delphi (Jiang et al., 2021) đối với Bảng Câu Hỏi Nền Tảng Đạo Đức (Graham et al., 2011), phát hiện rằng phản hồi của nó phản ánh thiên kiến nền tảng đạo đức của các nhóm đã tạo ra mô hình và dữ liệu huấn luyện của nó.

Các hướng nghiên cứu nói trên thường điều tra các mô hình ngôn ngữ không được nhắc với bất kỳ bản sắc cụ thể nào. Khung này ngụ ý mô hình được đào tạo trước bản thân như là trung tâm nơi một bộ thiên kiến gắn kết có thể tồn tại. Nghiên cứu gần đây cho thấy một quan điểm thay thế rằng một mô hình đơn lẻ có thể có khả năng mô phỏng vô số "bản sắc", và những bản sắc rõ ràng này có thể được lựa chọn bằng cách điều kiện hóa mô hình thông qua nhắc (Argyle et al., 2023; Aher et al., 2023). Dựa trên quan điểm sau, nghiên cứu hiện tại nhắc LLM mô phỏng hành vi tương ứng với các bản sắc chính trị đối lập, và đánh giá độ trung thực của những bản sao này đối với thiên kiến nền tảng đạo đức. Mối quan hệ giữa nghiên cứu hiện tại và các nghiên cứu khác có quan điểm "mô phỏng" này được tóm tắt dưới đây.

Arora et. al. thăm dó các giá trị văn hóa bằng cách sử dụng lý thuyết sáu chiều của Hofstede (Hofstede, 2001) và Khảo sát Giá trị Thế giới (Survey, 2022), và sử dụng ngôn ngữ nhắc thay vì token nhắc để điều kiện hóa mô hình với "bản sắc" văn hóa. Alshomary et al. 2021 và Qian et al. 2021 tinh chỉnh các mô hình GPT-2 (1.5B tham số) trên các kho dữ liệu cụ thể theo lĩnh vực, và điều kiện tạo văn bản với lập trường về các vấn đề xã hội. Nghiên cứu hiện tại, ngược lại, điều kiện trên bản sắc chính trị thay vì

--- TRANG 9 ---
lập trường, đánh giá các mô hình lớn hơn mà không có tinh chỉnh cụ thể theo lĩnh vực, và điều tra khả năng LLM bắt chước sở thích đạo đức. Nghiên cứu đồng thời thăm dò các mô hình ngôn ngữ về các hành vi bao gồm xu nịnh, xu hướng phản ánh quan điểm chính trị của người dùng trong môi trường đối thoại (Perez et al., 2022). Perez et. al. thấy rằng xu hướng này tăng theo quy mô trên ~10B tham số. Trong khi xu nịnh mô tả cách văn bản do mô hình tạo ra có vẻ thể hiện quan điểm chính trị, được điều kiện hóa trên quan điểm chính trị của người dùng đối thoại, bắt chước đạo đức mô tả cách văn bản do mô hình tạo ra có vẻ thể hiện tính nổi bật nền tảng đạo đức, được điều kiện hóa trên nhãn bản sắc chính trị. Argyle et. al. đề xuất khái niệm "độ trung thực thuật toán" - khả năng của LLM "mô phỏng chính xác phân phối phản hồi . . . của các nhóm con người" dưới điều kiện thích hợp (Argyle et al., 2023). Bắt chước đạo đức có thể được coi như một trường hợp của độ trung thực thuật toán nơi việc sử dụng nền tảng đạo đức là biến phản hồi quan tâm. Argyle et. al. nghiên cứu các biến phản hồi khác: mô tả đảng phái, mô hình bỏ phiếu, và cấu trúc tương quan trong phản hồi khảo sát.

6 Kết luận
Nghiên cứu này đánh giá liệu LLM có thể tái tạo thiên kiến nền tảng đạo đức gắn liền với các nhóm xã hội hay không, một khả năng được gọi là bắt chước đạo đức. Tôi đo lường việc sử dụng rõ ràng của năm nền tảng đạo đức trong văn bản được tạo ra bởi các mô hình ngôn ngữ được đào tạo trước được điều kiện hóa với bản sắc chính trị. Tôi cho thấy rằng LLM tái tạo thiên kiến nền tảng đạo đức gắn liền với bản sắc chính trị tự do và bảo thủ, sửa đổi việc sử dụng nền tảng đạo đức của chúng theo tình huống, mặc dù không không thể phân biệt với con người, và bắt chước đạo đức có thể liên quan đến kích thước mô hình.

Lời cảm ơn
Tôi muốn cảm ơn các nhà phê bình ẩn danh đã cung cấp những bình luận có giá trị về bài báo này. Tôi cũng muốn cảm ơn các Giáo sư Dipak Ghosal, Jiawei Zhang, và Patrice Koehl, đã cung cấp phản hồi có giá trị về nghiên cứu này, và các đồng nghiệp, bạn bè, và gia đình vì những cuộc thảo luận sâu sắc.

Tài liệu tham khảo
Gati Aher, Rosa I. Arriaga, và Adam Tauman Kalai. 2023. Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies.

Milad Alshomary, Wei-Fan Chen, Timon Gurcke, và Henning Wachsmuth. 2021. Belief-based Generation of Argumentative Claims. Trong Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, trang 224–233, Online. Association for Computational Linguistics.

Milad Alshomary và Henning Wachsmuth. 2021. Toward audience-aware argument generation. Patterns, 2(6):100253.

Lisa P. Argyle, Ethan C. Busby, Nancy Fulda, Joshua R. Gubler, Christopher Rytting, và David Wingate. 2023. Out of One, Many: Using Language Models to Simulate Human Samples. Political Analysis, trang 1–15.

Arnav Arora, Lucie-aimee Kaffee, và Isabelle Augenstein. 2023. Probing pre-trained language models for cross-cultural differences in values. Trong Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP), trang 114–130, Dubrovnik, Croatia. Association for Computational Linguistics.

Emily M. Bender và Alexander Koller. 2020. Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. Trong Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, trang 5185–5198, Online. Association for Computational Linguistics.

Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael

--- TRANG 10 ---
Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, và Percy Liang. 2022. On the Opportunities and Risks of Foundation Models.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, và Dario Amodei. 2020. Language models are few-shot learners. Trong Advances in Neural Information Processing Systems, volume 33, trang 1877–1901. Curran Associates, Inc.

Don E. Davis, Kenneth Rice, Daryl R. Van Tongeren, Joshua N. Hook, Cirleen DeBlaere, Everett L. Worthington Jr., và Elise Choe. 2016. The moral foundations hypothesis does not replicate well in Black samples. Journal of Personality and Social Psychology, 110(4):e23–e30.

DeepSpeed. 2022. ZeRO-Inference: Democratizing massive model inference. https://www.deepspeed.ai/2022/09/09/zero-inference.html.

David Dobolyi. 2016. Critiques | Moral Foundations Theory.

Burak Doğruyol, Sinan Alper, và Onurcan Yilmaz. 2019. The five-factor model of the moral foundations theory is stable across WEIRD and non-WEIRD cultures. Personality and Individual Differences, 151:109547.

Maxim Egorov, Karianne Kalshoven, Armin Pircher Verdorfer, và Claudia Peus. 2020. It's a Match: Moralization and the Effects of Moral Foundations Congruence on Ethical and Unethical Leadership Perception. Journal of Business Ethics, 167(4):707–723.

Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, và Yejin Choi. 2021. Moral Stories: Situated Reasoning about Norms, Intents, Actions, and their Consequences. Trong Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, trang 698–718, Online và Punta Cana, Dominican Republic. Association for Computational Linguistics.

Hubert Etienne. 2021. The dark side of the 'Moral Machine' and the fallacy of computational ethical decision-making for autonomous vehicles. Law, Innovation and Technology, 13(1):85–107.

Matthew Feinberg và Robb Willer. 2015. From Gulf to Bridge: When Do Moral Arguments Facilitate Political Influence? Personality and Social Psychology Bulletin, 41(12):1665–1681.

Maxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, và Yejin Choi. 2020. Social chemistry 101: Learning to reason about social and moral norms. Trong Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), trang 653–670, Online. Association for Computational Linguistics.

Kathleen C. Fraser, Svetlana Kiritchenko, và Esma Balkir. 2022. Does Moral Code have a Moral Code? Probing Delphi's Moral Philosophy. Trong Proceedings of the 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP 2022), trang 26–42, Seattle, U.S.A. Association for Computational Linguistics.

Jeremy Frimer. 2019. Moral Foundations Dictionary 2.0.

Jeremy A. Frimer. 2020. Do liberals and conservatives use different moral languages? Two replications and six extensions of Graham, Haidt, and Nosek's (2009) moral text analysis. Journal of Research in Personality, 84:103906.

Leo Gao. 2021. On the Sizes of OpenAI API Models. https://blog.eleuther.ai/gpt3-model-sizes/.

Justin Garten, Reihane Boghrati, J. Hoover, Kate M. Johnson, và Morteza Dehghani. 2016. Morality Between the Lines: Detecting Moral Sentiment In Text.

Jesse Graham, Jonathan Haidt, và Brian A. Nosek. 2009. Liberals and conservatives rely on different sets of moral foundations. Journal of Personality and Social Psychology, 96(5):1029–1046.

Jesse Graham, Brian A. Nosek, Jonathan Haidt, Ravi Iyer, Spassena Koleva, và Peter H. Ditto. 2011. Mapping the Moral Domain. Journal of personality and social psychology, 101(2):366–385.

Jonathan Haidt. 2013. The Righteous Mind: Why Good People Are Divided by Politics and Religion. Vintage Books.

Craig A. Harper và Darren Rhodes. 2021. Reanalysing the factor structure of the moral foundations questionnaire. The British Journal of Social Psychology, 60(4):1303–1329.

Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, và Jacob Steinhardt. 2021. Aligning AI with shared human values. Trong 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.

Geert Hofstede. 2001. Culture's Recent Consequences: Using Dimension Scores in Theory and Research. International Journal of Cross Cultural Management, 1(1):11–17.

--- TRANG 11 ---
Frederic R. Hopp, Jacob T. Fisher, Devin Cornell, Richard Huskey, và René Weber. 2021. The extended Moral Foundations Dictionary (eMFD): Development and applications of a crowd-sourced approach to extracting moral intuitions from text. Behavior Research Methods, 53(1):232–246.

Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O'Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, và Ves Stoyanov. 2023. OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization.

Hang Jiang, Doug Beeferman, Brandon Roy, và Deb Roy. 2022. CommunityLM: Probing partisan worldviews from language models. Trong Proceedings of the 29th International Conference on Computational Linguistics, trang 6818–6826, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.

Liwei Jiang, Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jenny Liang, Jesse Dodge, Keisuke Sakaguchi, Maxwell Forbes, Jon Borchardt, Saadia Gabriel, Yulia Tsvetkov, Oren Etzioni, Maarten Sap, Regina Rini, và Yejin Choi. 2021. Can Machines Learn Morality? The Delphi Experiment.

Zhijing Jin, Sydney Levine, Fernando Gonzalez Adauto, Ojasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada Mihalcea, Josh Tenenbaum, và Bernhard Schölkopf. 2022. When to make exceptions: Exploring language models as accounts of human moral judgment. Trong NeurIPS.

Kristen Johnson và Dan Goldwasser. 2018. Classification of Moral Foundations in Microblog Political Discourse. Trong Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 720–730, Melbourne, Australia. Association for Computational Linguistics.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, và Dario Amodei. 2020. Scaling Laws for Neural Language Models.

Rabia I. Kodapanakkal, Mark J. Brandt, Christoph Kogler, và Ilja van Beest. 2022. Moral Frames Are Persuasive and Moralize Attitudes; Nonmoral Frames Are Persuasive and De-Moralize Attitudes. Psychological Science, 33(3):433–449.

Andrew Luttrell, Aviva Philipp-Muller, và Richard E. Petty. 2019. Challenging Moral Attitudes With Moral Messages. Psychological Science, 30(8):1136–1150.

Ece Çiğdem Mutlu, Toktam Oghaz, Ege Tütüncüler, và Ivan Garibay. 2020. Do Bots Have Moral Judgement? The Difference Between Bots and Humans in Moral Rhetoric. Trong 2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), trang 222–226.

OpenAI. 2021. OpenAI API. https://openai.com/api/.

OpenAI. 2022. Model Index for Researchers.

Matheus C. Pavan, Vitor G. Dos Santos, Alex G. J. Lan, Joao Martins, Wesley R. Santos, Caio Deutsch, Pablo B. Costa, Fernando C. Hsieh, và Ivandre Paraboni. 2020. Morality Classification in Natural Language Text. IEEE Transactions on Affective Computing, trang 1–1.

Ethan Perez, Sam Ringer, Kamilė Lukošiūtė, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Ben Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemí Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, và Jared Kaplan. 2022. Discovering Language Model Behaviors with Model-Written Evaluations.

Ming Qian, Jaye Laguardia, và Davis Qian. 2021. Morality Beyond the Lines: Detecting Moral Sentiment Using AI-Generated Synthetic Context. Trong Artificial Intelligence in HCI, Lecture Notes in Computer Science, trang 84–94, Cham. Springer International Publishing.

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.

Shamik Roy, Nishanth Sridhar Nakshatri, và Dan Goldwasser. 2022. Towards Few-Shot Identification of Morality Frames using In-Context Learning. Trong Proceedings of the Fifth Workshop on Natural Language Processing and Computational Social Science (NLP+CSS), trang 183–196, Abu Dhabi, UAE. Association for Computational Linguistics.

Christopher Suhler và Pat Churchland. 2011. Can Innate, Modular "Foundations" Explain Morality? Challenges for Haidt's Moral Foundations Theory. Journal of cognitive neuroscience, 23:2103–16; discussion 2117.

World Values Survey. 2022. WVS Database. https://www.worldvaluessurvey.org/wvs.jsp.

Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira Ganesh, Ryan Cotterell, và Adina Williams. 2022. On the Machine Learning of Ethical Judgments from

--- TRANG 12 ---
Natural Language. Trong Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, trang 769–779, Seattle, United States. Association for Computational Linguistics.

Nitasha Tiku. 2022. The Google engineer who thinks the company's AI has come to life. Washington Post.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, và Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, và Illia Polosukhin. 2017. Attention is all you need. Trong Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.

Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, và Iason Gabriel. 2022. Taxonomy of Risks posed by Language Models. Trong 2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT '22, trang 214–229, New York, NY, USA. Association for Computing Machinery.

Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, và Alexander M. Rush. 2020. HuggingFace's Transformers: State-of-the-art Natural Language Processing.

Bilge Yalçındağ, Türker Özkan, Sevim Cesur, Onurcan Yilmaz, Beyza Tepe, Zeynep Ecem Piyale, Ali Furkan Biten, và Diane Sunar. 2019. An Investigation of Moral Foundations Theory in Turkey Using Different Measures. Current Psychology, 38(2):440–457.

Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, và Luke Zettlemoyer. 2022. OPT: Open Pre-trained Transformer Language Models.

Caleb Ziems, Jane Yu, Yi-Chia Wang, Alon Halevy, và Diyi Yang. 2022. The moral integrity corpus: A benchmark for ethical dialogue systems. Trong Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), trang 3755–3773, Dublin, Ireland. Association for Computational Linguistics.

A Phụ lục A: Chi tiết Bổ sung Liên quan đến Phương pháp Thí nghiệm

A.1 Chi tiết Bổ sung Liên quan đến LLM Được Sử dụng trong Nghiên cứu

[Bảng 1 được dịch với thông tin về các mô hình được đánh giá trong nghiên cứu này]

A.2 Chi tiết Bổ sung Liên quan đến Bộ dữ liệu Được Sử dụng trong Nghiên cứu

A.2.1 Chi tiết Tiền xử lý cho Bộ dữ liệu Moral Stories

Mỗi ví dụ trong Moral Stories bao gồm một chuẩn mực đạo đức (một kỳ vọng quy chuẩn về hành vi đạo đức), một tình huống mô tả trạng thái của một số nhân vật, một ý định mô tả điều mà một nhân vật cụ thể muốn, và hai con đường, một con đường đạo đức và một con đường vô đạo đức. Mỗi con đường bao gồm một hành động đạo đức hoặc vô đạo đức (một hành động tuân theo hoặc vi phạm chuẩn mực) và một hậu quả đạo đức hoặc vô đạo đức (một kết quả có thể xảy ra của hành động). Đối với các thí nghiệm hiện tại, tôi xây dựng các tình huống như chuỗi nối của tình huống, ý định, và hành động đạo đức hoặc hành động vô đạo đức của một ví dụ. Chúng tôi không sử dụng hậu quả hoặc chuẩn mực, vì chúng thường bao gồm lý do tại sao hành động đó đạo đức/vô đạo đức, và do đó có thể làm thiên lệch nội dung nền tảng đạo đức của các hoàn thành.

Chúng tôi đã sử dụng 2.000 tình huống được tạo ra từ bộ dữ liệu Moral Stories, bao gồm 1.000 tình huống đạo đức được lấy mẫu ngẫu nhiên và 1.000 tình huống vô đạo đức được lấy mẫu ngẫu nhiên.

--- TRANG 13 ---
A.2.2 Chi tiết Tiền xử lý cho Bộ dữ liệu ETHICS

Bộ dữ liệu ETHICS chứa năm tập con dữ liệu, mỗi tập tương ứng với một khung đạo đức cụ thể (deontology, justice, utilitarianism, commonsense, và virtue), mỗi tập được chia thành phần "train" và "test". Đối với các thí nghiệm hiện tại, tôi sử dụng phần chia "train" của phần "commonsense" của bộ dữ liệu, chứa 13.910 ví dụ về các tình huống được ghép với nhãn nhị phân thực tế về tính chấp nhận đạo đức. Trong số này, 6.661 là ví dụ "ngắn", có độ dài 1-2 câu. Những ví dụ ngắn này được lấy từ các công nhân Amazon Mechanical Turk và bao gồm 3.872 ví dụ đạo đức, và 2.789 ví dụ vô đạo đức. Từ những ví dụ này, tôi chọn ngẫu nhiên 1.000 ví dụ được chia đều theo tính chấp nhận quy chuẩn, dẫn đến 500 tình huống đạo đức và 500 tình huống vô đạo đức.

Phần chia train của phần commonsense của bộ dữ liệu ETHICS cũng chứa 7.249 ví dụ "dài", có độ dài 1-6 đoạn, được lấy từ Reddit. Những ví dụ này không được sử dụng trong thí nghiệm hiện tại, chủ yếu do chi phí tăng khi sử dụng các tình huống dài hơn.

A.2.3 Chi tiết Tiền xử lý cho Bộ dữ liệu Social Chemistry Actions

Bộ dữ liệu Social Chemistry 101 (Forbes et al., 2020) chứa 355.922 chú thích có cấu trúc của 103.692 tình huống, được rút ra từ bốn nguồn (Dear Abby, Reddit AITA, Reddit Confessions, và các câu từ kho dữ liệu ROCStories; xem (Forbes et al., 2020) để biết tài liệu tham khảo). Các tình huống là những mô tả ngắn gọn về các sự kiện trong cuộc sống hàng ngày nơi các chuẩn mực xã hội hoặc đạo đức có thể quyết định hành vi, ví dụ "rút khỏi một dự án nhóm vào phút cuối". Các tình huống được chú thích với Quy Tắc Thực Hành (RoT), là những phán đoán về các hành động xảy ra trong tình huống, chẳng hạn như "Thật tệ khi không thực hiện theo cam kết của bạn". Một số tình huống có thể chứa nhiều hơn một hành động, nhưng tôi xem xét các tình huống được chú thích nhất trí là chỉ có một hành động cho thí nghiệm hiện tại, vì điều này đơn giản hóa việc giải thích các chú thích nền tảng đạo đức. RoT trong bộ dữ liệu được chú thích với "Phân tích RoT". Phân tích RoT phân tích mỗi RoT thành hành động cấu thành của nó (ví dụ: "không thực hiện theo cam kết") và phán đoán ("thật tệ"). Các phán đoán được chuẩn hóa thành năm mức độ chấp thuận/không chấp thuận: rất tệ, tệ, mong đợi/OK, tốt, rất tốt. Tôi loại bỏ các hành động được gắn nhãn "mong đợi/OK", và gộp "rất tệ" và "tệ" lại với nhau, và "rất tốt" và "tốt" lại với nhau để có được các hành động được chú thích với tính chấp nhận đạo đức nhị phân. Các hành động cũng được chú thích với nhãn nền tảng đạo đức (ví dụ trong câu trước được chú thích với các nền tảng Công Bằng/Gian Lận và Trung Thành/Phản Bội). Ngoài ra, mỗi RoT thuộc về một trong các danh mục sau - đạo đức-đạo đức, chuẩn mực-xã hội, lời khuyên, mô tả. Tôi sử dụng RoT thuộc về danh mục "đạo đức-đạo đức", vì đây là danh mục cho biết rằng RoT chứa lý luận đạo đức thay vì lời khuyên hoặc khuyến nghị về nghi thức. Sau khi lọc RoT và tình huống theo danh mục, và chọn các ví dụ có đánh giá nhất trí về nền tảng đạo đức và tính chấp nhận quy chuẩn, tôi có được một bộ dữ liệu gồm 1300 hành động - 130 hành động đạo đức quy chuẩn và 130 hành động vô đạo đức quy chuẩn cho mỗi nền tảng đạo đức trong số năm nền tảng. Những tình huống này được sử dụng trong thí nghiệm liên quan đến Tiêu chí A trong Phần 2.1.

A.2.4 Chi tiết Tiền xử lý cho Bộ dữ liệu Social Chemistry Situations

Tiêu chí B yêu cầu so sánh PH(ef|s) và PLM(ef|s), cho phản hồi văn bản mở của con người và LLM chứa lý luận đạo đức về một số tình huống. Tôi sử dụng các tình huống từ bộ dữ liệu Social Chemistry 101 (Forbes et al., 2020), và sử dụng RoT được viết bởi con người để ước tính PH(ef|s) bằng cách sử dụng các từ điển nền tảng đạo đức. Để ước tính phán đoán đồng thuận của con người CH(s), tôi sử dụng các tình huống được chú thích nhiều lần. Cụ thể, tôi lọc bộ dữ liệu Social Chemistry 101 thành các tình huống có 4 RoT trở lên, và 4 phân tích RoT trở lên cho mỗi RoT. Điều này dẫn đến một tập hợp gồm 170 tình huống. Không giống như bộ dữ liệu Social Chemistry Actions, bộ dữ liệu Social Chemistry Situations này không được cân bằng nền tảng - tôi gặp phải sự đánh đổi giữa số lượng chú thích tối thiểu cho mỗi tình huống và kích thước tập hợp cuối cùng - việc cân bằng bộ dữ liệu về mặt nền tảng sẽ làm giảm kích thước bộ dữ liệu hơn nữa. Bộ tình huống được sử dụng cho thí nghiệm liên quan đến Tiêu chí B trong Phần 2.1.

A.3 Chi tiết Bổ sung Liên quan đến Từ điển Nền tảng Đạo đức

A.4 Chi tiết Bổ sung Liên quan đến Xây dựng Lời nhắc

Các mẫu từ Bảng 2 được sử dụng cho các bộ dữ liệu Moral Stories, ETHICS, và Social Chemistry Situations, nơi các tình huống là những mô tả dài hơn về các sự kiện, có độ dài một câu trở lên. Các mẫu từ Bảng 3 được sử dụng cho bộ dữ liệu Social Chemistry Actions, nơi các tình huống là những mô tả hành động ngắn gọn (các đoạn câu). Điều này được thực hiện để đảm bảo tính ngữ pháp.

[Hình 5 trong Phụ lục: Biểu đồ Venn về sự chồng chéo từ giữa MFDv1, MFDv2 và eMFD]

[Bảng 2: Các kiểu mẫu lời nhắc cho tình huống]

[Bảng 3: Các kiểu mẫu lời nhắc cho hành động]

B Phụ lục B: Kết quả Thí nghiệm Bổ sung

B.1 Kích thước Hiệu ứng so với Bộ dữ liệu

Hình 6 cho thấy kích thước hiệu ứng cho nhắc bản sắc tự do so với bảo thủ, dựa trên các hoàn thành thu được từ 2000 tình huống được tạo ra từ Moral Stories và 1000 tình huống được tạo ra từ ETHICS. Điểm số được phân tách theo từ điển và bộ dữ liệu. Xem Phần 2 để biết các phương pháp được sử dụng để tính toán kích thước hiệu ứng.

Kích thước hiệu ứng và hướng nhất quán trên các bộ dữ liệu cho các nền tảng Quan Tâm/Tổn Hại và Quyền Uy/Lật Đổ.

B.2 Kích thước Hiệu ứng so với Kiểu Mẫu Lời nhắc

Hình 7 cho thấy kết quả thu được từ phân tích các hoàn thành thu được từ năm kiểu lời nhắc khác nhau, như được mô tả trong 2.

Các hiệu ứng của bản sắc chính trị tự do so với bảo thủ đồng nhất theo hướng cho các nền tảng Quan Tâm/Tổn Hại và Quyền Uy/Lật Đổ. Bất kể kiểu lời nhắc hoặc từ điển được sử dụng, các hoàn thành chứa nhiều từ Quan Tâm/Tổn Hại hơn khi sử dụng bản sắc chính trị tự do, và nhiều từ Quyền Uy/Lật Đổ hơn khi sử dụng bản sắc chính trị bảo thủ. Các hiệu ứng gần như đồng nhất theo hướng cho nền tảng Công Bằng/Gian Lận, với bản sắc chính trị tự do dẫn đến việc sử dụng tăng nền tảng này cho mười ba trong số mười lăm kết hợp kiểu lời nhắc và từ điển. Nhắc tự do dẫn đến việc sử dụng giảm nền tảng Công Bằng/Gian Lận cho các kiểu lời nhắc 1 và 2, khi đo bằng MFDv2.

Kết quả cho các nền tảng Thiêng Liêng/Suy Thoái và Trung Thành/Phản Bội đa dạng hơn. Hướng hiệu ứng đồng nhất cho nền tảng Thiêng Liêng/Suy Thoái khi đo bằng MFDv2 - bản sắc chính trị tự do dẫn đến việc sử dụng Thiêng Liêng/Suy Thoái thấp hơn 1-2 điểm phần trăm trên tất cả các kiểu lời nhắc. Các hiệu ứng trên Thiêng Liêng/Suy Thoái ít nhất quán hơn khi đo bằng MFDv1 hoặc eMFD - nhắc tự do dẫn đến việc sử dụng từ Thiêng Liêng/Suy Thoái giảm chỉ cho ba trong số năm kiểu lời nhắc. Đo bằng eMFD, nhắc tự do dẫn đến việc sử dụng từ Thiêng liêng/suy thoái giảm cho bốn trong số năm kiểu lời nhắc.

Hướng hiệu ứng đồng nhất cho Trung Thành/Phản Bội khi đo bằng MFDv1 - nhắc với bản sắc chính trị tự do dẫn đến điểm phần trăm lớn hơn cho Trung Thành cho tất cả các kiểu lời nhắc. Kết quả đa dạng khi đo bằng MFDv1 - nhắc tự do dẫn đến việc sử dụng giảm chỉ cho ba trong số năm kiểu lời nhắc. Khi đo bằng eMFD, nhắc tự do dẫn đến việc sử dụng giảm hoặc bằng nền tảng Trung Thành/Phản Bội trên các kiểu lời nhắc, điều này nhất quán trong từ điển, nhưng ngược lại về hướng hiệu ứng so với MFDv1.

--- TRANG 16 ---
C Phụ lục C: Ví dụ Đầu ra LLM

Hình 8: Ví dụ về các hoàn thành thu được từ bộ dữ liệu Moral Stories, từ các mô hình OpenAI có kích thước tăng dần. Các ví dụ được chọn ngẫu nhiên
