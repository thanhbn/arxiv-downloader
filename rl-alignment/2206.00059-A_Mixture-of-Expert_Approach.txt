# 2206.00059.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2206.00059.pdf
# File size: 1373428 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
A Mixture-of-Expert Approach
to RL-based Dialogue Management
Yinlam Chow Aza Tulepbergenov OÔ¨År Nachum
Moonkyung Ryu Mohammad Ghavamzadeh Craig Boutilier
Google Research
{yinlamchow, atulep, ofirnachum, mkryu, ghavamza, cboutilier}@google.com
Abstract
Despite recent advancements in language models (LMs), their application to di-
alogue management (DM) problems and ability to carry on rich conversations
remain a challenge. We use reinforcement learning (RL) to develop a dialogue
agent that avoids being short-sighted (outputting generic utterances) and maxi-
mizes overall user satisfaction. Most existing RL approaches to DM train the agent
at the word-level, and thus, have to deal with a combinatorially complex action
space even for a medium-size vocabulary. As a result, they struggle to produce a
successful and engaging dialogue even if they are warm-started with a pre-trained
LM. To address this issue, we develop a RL-based DM using a novel mixture of
expert language model (MoE-LM) that consists of (i) a LM capable of learning
diverse semantics for conversation histories, (ii) a number of specialized LMs (or
experts ) capable of generating utterances corresponding to a particular attribute
or personality, and (iii) a RL-based DM that performs dialogue planning with the
utterances generated by the experts. Our MoE approach provides greater Ô¨Çexibility
to generate sensible utterances with different intents and allows RL to focus on
conversational-level DM. We compare it with SOTA baselines on open-domain
dialogues and demonstrate its effectiveness both in terms of the diversity and
sensibility of the generated utterances and the overall DM performance.
1 Introduction
With the tremendous advancements in natural language understanding and generation, increasing
attention has been directed to construct intelligent dialogue agents that can carry out rich and engaging
conversations with human users. Such interactions can be quite open-ended, contain many different
topics, and often involve an underlying task, such as building a rapport, negotiation, information
exchange, and recommendation. Therefore, to satisfy the user, a good dialogue agent should not only
generate natural responses, but also be capable of pursuing the task‚Äôs objectives and adapting to the
user‚Äôs feedback on-the-Ô¨Çy.
A standard solution is to train the dialogue agent using behavioral cloning, where the agent is a
language model (LM) that imitates the utterances in the training set [Ga≈°i ¬¥c et al., 2011, Fatemi et al.,
2016]. By leveraging deep neural networks, e.g., RNNs [Sutskever et al., 2014] and Transform-
ers [Vaswani et al., 2017], a LM encodes the conversation to a low-dimensional dialogue state and
predicts an utterance, but steering such generation for particular purposes remains an open question.
Several works studied ways to Ô¨Åne-tune a LM to generate texts with speciÔ¨Åc contexts [Ziegler et al.,
2019, Ficler and Goldberg, 2017]. Other results learned a single steerable LM that is capable of
Preprint. Under review.arXiv:2206.00059v1  [cs.CL]  31 May 2022

--- PAGE 2 ---
generating utterances for multiple speciÔ¨Åc intents [Gu et al., 2017, Chen et al., 2018, Subramani et al.,
2019, Dathathri et al., 2019]. While these LMs produce Ô¨Çuent and relevant responses, it is unclear
how to control them to systematically pursue goals during multi-turn dialogue conversations.
Another popular approach is to view dialogue management (DM) as a control problem and use
reinforcement learning (RL) to optimize the agent‚Äôs policy (which is often a LM itself). Using
RL for dialogue systems has a long history. Earlier work relies on speciÔ¨Åc, hand-crafted semantic
states [Levin and Pieraccini, 1997, Singh et al., 2002, Walker, 2000] or partially observable belief
states [Williams and Young, 2007, Young et al., 2010], in which the agent chooses the best hand-
crafted dialogue act at each turn, with the goal of either satisfying the user [Shah et al., 2018],
completing the task [Shi and Yu, 2018], or responding to the user‚Äôs query [Serban et al., 2017a].
However, the application of these approaches is limited to problems whose action space can be
captured by hand-crafted representations, and they cannot handle complex conversations. On the
other hand, more recent approaches use deep learning to extract semantic representations from
conversation histories, treat these representations as dialogue belief states, and apply RL to learn
a word-level generative DM agent [Jaques et al., 2019, Li et al., 2016, 2017, Shin et al., 2020].
However, since there are innumerable possibilities of language utterances, and thus, the action
space of the RL problem is extremely large, the agent often performs planning poorly and generates
incomprehensible utterances [Zhao et al., 2019]. Another issue is that RL only optimizes a scalar
reward, while the aforementioned methods often need to optimize for both the quality of the generated
utterance, e.g., ease of answering [Li et al., 2016] and Ô¨Çuency [Li et al., 2017, 2019], and the goal,
e.g., conversation length [Zhou et al., 2020], user‚Äôs sentiment [Hancock et al., 2019], and task
completion [Verma et al., 2022]. Moreover, deÔ¨Åning the reward as weighted combination of these
metrics is not ideal, since the hand-picked weights do not often reÔ¨Çect the underlying success criteria.
To address the above issues related to using RL in dialogue management (DM) systems, we propose
an RL-based DM agent using a novel mixture of expert (MoE) approach. Our MoE approach is based
on a mixture of expert language model (MoE-LM), which consists of three main components: 1)a
LM (a probabilistic encoder and a decoder) capable of learning diverse semantics for conversation
histories, and as a result generating diverse utterances, which we refer to as the primitive LM or LM0,
2)a number of specialized LMs (or experts ),fLMigm
i=1, that each is constructed using the latent space
learned by LM0, but has been trained such that it is capable of generating utterances corresponding to
a certain intent or personality, and 3)an RL-based dialogue manager (DM) that at each turn, given
the latent state shared by the experts fLMigm
i=0and the utterance action(s) they suggest, chooses one
among them for the agent to execute. Our MoE-LM can be seen as a special case of hierarchical
LMs (e.g., Serban et al. 2017a, Zhao et al. 2019, Saleh et al. 2020), but it is different than them
because it learns both the LMs (experts) and the DM. Moreover, the DM in MoE-LM is a policy
conditioned on both the latent state and the actions suggested by the experts, and not just the state
as it is common in hierarchical RL. The primitive LM ( LM0) plays an important role in this model
because it learns diverse semantics for conversation histories and allows the agent to generate a wide
variety of utterances. This diversity is also shared with the specialized LMs (experts) and gives them
Ô¨Çexibility in generating their (more) specialized utterances. Another important feature of MoE-LM
is its modularity that facilitates adding and removing specialized LMs (experts). Moreover, this
hierarchical architecture allows us to solve an RL problem with much smaller state and action spaces,
which is quite important in the quality of the learned policy. Finally, since the candidate utterances
are generated by experts with different intents, instead of combining all agent-user signals into a
single RL reward, our DM agent can focus on optimizing the speciÔ¨Åc goal of the conversation task.
We start the paper with a brief introduction of LMs and the use of Markov decision processes
(MDPs) in modeling dialogue management problems in Section 2. We then describe the overall
architecture of our MoE-LM in Section 3, followed by the detailed implementation of each of its
three main components (described in the above paragraph) in Sections 4 to 6. Finally, in Section 7,
we demonstrate the effectiveness of our MoE-LM in open-domain dialogues, in terms of both its
ability to generate diverse and sensible utterances and its overall DM performance.
2 Preliminaries
Language Models (LMs) In this work, we employ seq2seq LMs to generate the next utterances
in a dialogue. We assume access to a dataset of the form D=f(X(k);Y(k))gjDj
k=1, where each
X=X(k)is aL-turn conversation history X=fXlgL 1
l=0andYis its next utterance. We denote
2

--- PAGE 3 ---
byNX, an upper-bound on the length (number of tokens) of each utterance XlinX.1The role of
a LM is to predict the probability of the next utterance Y, consisting of Ntokens, conditioned on
the conversation history X, i.e.,p 
Y=fyngN
n=1jX
. In the transformer architecture [Wolf et al.,
2019], the LM Ô¨Årst encodes the conversation history Xusing an encoder to a(LNX)-length
sequence of embeddings f(zl;0;:::;zl;NX 1)gL 1
l=0, where each zl;nis a vector in the latent space.
For notational convenience, we concatenate these embeddings into a single embedding z2Z Rd
and denote the overall dimension of the latent space as d. In the RNN architecture [Serban et al.,
2016], the LM‚Äôs encoder directly maps the conversation history Xto a latent state z2Z Rd. In
both architectures, the next utterance bY=fbyngN
n=1is sampled token-by-token from the decoder 	,
i.e.,bY	 
jz
=QN
n=1	 
bynjby0;:::;byn 1;z
, whereby0is a Ô¨Åxed initial (start-of-sentence)
token [Chien and Kuo, 2019], and the latent state is denoted as z= (X).2
Markov Decision Processes (MDPs) have been used to model dialogue management problems
in a variety of settings [Li et al., 2016, Asadi and Williams, 2016, Jaques et al., 2019]. In such
MDPs, denoted by M= (S;A;P;r;s 0;), the state spaceSrepresents the tokenized conversation
history and the initial state s02 S is the initial user‚Äôs query. The action space Ais also the
tokenized language space with each action a2A being the agent‚Äôs next utterance (which is a
Ô¨Åxed-length, NX, sequence of tokens). The transition kernel Pmodels the user‚Äôs response to the
action taken by the agent (bot). Finally, the reward function rmeasures the user‚Äôs satisfaction.
In these MDPs, we can think of the entire LM as a policy that maps conversation histories to
next utterances, and solve them by Ô¨Ånding a policy with maximum expected discounted return,
i.e.,2arg maxJ:=E[P1
k=0trtjP;s0;]. Note that the size of the tokenized state and
action spaces grow exponentially with the size of the vocabulary. This makes it intractable to solve
the MDP even for a medium-size vocabulary. As a result, it would quite desirable to develop a novel
MDP paradigm that is more amendable to RL-based DM systems.
3 Mixture of Experts (MoE) Language Model
We start by explaining how a MoE language model (MoE-LM) can enrich the bot‚Äôs utterances
and improve the overall performance of the DM. While our approach is applicable to any DM
system, we use open-domain dialogue [Sankar et al., 2019] as a running example to show how
MoE-LM-based agents can improve user satisfaction measured by an improvement on a sentiment
or engagement. Intuitively a good DM agent should possess different behaviors (e.g., inquisitive,
explorative, relevant, soothing, empathetic, complimentary, provoking) and swiftly decide which
intent to use to pivot a conversation, build rapport, pique the user‚Äôs interests, improve their mood, etc.
To achieve this goal, we require the LM to have a language representation (primitive discovery) that
captures different semantics, in order to encode different conversations and avoid generating dull and
repetitive responses. We also need a machinery (expert construction) to embed different intents into
sub-models of this LM, so that they can behave accordingly when prompted, and respond efÔ¨Åciently.
Finally, with various candidate utterances available, the DM module of this LM should understand
the current level of user satisfaction and determine which response is the most appropriate. Motivated
by these observations, we construct our MoE-LM in three steps as shown in Figure 1. We give the
main idea behind each step here and leave their detailed descriptions to Sections 4, 5, and 6.
Step 1: Primitive Discovery. We Ô¨Årst employ the dataset D, introduced in Section 2, and learn a
language model LM0= (;G0;	)consisting of a stochastic encoder (i.e., an encoder and a latent
space distributionG0that maps the encoded conversation into a latent distribution), and a decoder 	.
The stochastic encoder (;G0)comprises an encoder that maps tokenized conversation histories X
to a latent spaceZRd, i.e.,z= (X)2Z, which is then used to construct a parameterized d-
dimensional Gaussian distribution G0(z0jz) =N 
0(z);2
0(z)Idd
overRd. The decoder predicts
the next utterance bY0(token-by-token) conditioned on the point z0sampled from the latent distribu-
tion, i.e., 	(bY0jz0)3; z0G 0(jz). We denote by LM0(YjX) :=Ez0G0(jz);z=(X)[	(Yjz0)], the
primitive and learn it using a loss function that in addition to predicting the next utterance accurately,
encourages diversity and generalization in the learned latent space Z(see Eq. 1 and Fig. 2). As we
will explain in Section 4, our loss function is inspired by those in prior work, and more speciÔ¨Åcally by
1If the actual utterance Xlhas fewer tokens than NX, it will be padded by a speciÔ¨Åc token and masked.
2Note that we use Yas the next utterance in the dataset and ^Yas the one predicted by the LM.
3In practice, we can use both latent states as the input to the decoder model 	(bY0jz0;z).
3

--- PAGE 4 ---
<latexit sha1_base64="vXrCpQE7bIqHA3pzUtWhaecwsVQ=">AAACPnicfZDNSsNAFIUn/ht/q0s3wSqISElE1GVRF27EClaFJpSb6W07dDIJMxOxhL6CW30cX8MXcCduXTqpEdSKFwY+zj0zc+8JE86Udt1na2x8YnJqembWnptfWFxaLq1cqTiVFOs05rG8CUEhZwLrmmmON4lEiEKO12HvOO9f36JULBaXup9gEEFHsDajoHPJr3VZc7nsVtxhOaPgFVAmRdWaJWvDb8U0jVBoykGphucmOshAakY5Dmw/VZgA7UEHGwYFRKiCbDjswNk0Sstpx9IcoZ2h+v1GBpFS/Sg0zgh0V/3u5eJfvUaq24dBxkSSahT086N2yh0dO/nmTotJpJr3DQCVzMzq0C5IoNrkY/snaHaReGbePU9Qgo7lduaD7ERwNzC7dfydnP4zMvFlNGSbXL3fKY7C1W7F26/sXeyVq0dFwjNkjayTLeKRA1Ilp6RG6oSSLrknD+TRerJerFfr7dM6ZhV3VsmPst4/AA34rus=</latexit> 
<latexit sha1_base64="tjHbOgrf4ho9DvviQX3BPsA5tPU=">AAACQ3icfZDLSgMxFIYz3q13XboJVkFEyoyIuhR0oQtRwWqlU+RMeqYNJpkhyYhl6FO41cfxIXwGd+JWMFMreMMDgY///EnO+aNUcGN9/8kbGBwaHhkdGy9NTE5Nz8zOzZ+bJNMMqywRia5FYFBwhVXLrcBaqhFkJPAiut4r+hc3qA1P1JntpNiQ0FI85gysky5DCbYdxbR2NVv2K36v6G8I+lAm/Tq5mvOWw2bCMonKMgHG1AM/tY0ctOVMYLcUZgZTYNfQwrpDBRJNI+9N3KUrTmnSONHuKEt76tcbOUhjOjJyzmJC87NXiH/16pmNdxo5V2lmUbGPj+JMUJvQYn3a5BqZFR0HwDR3s1LWBg3MupBK4T66XTQeuXePU9RgE72Wh6BbEm67brdWuF7Qf0auPo2OSi7X4GeKv+F8oxJsVTZPN8u7h/2Ex8giWSKrJCDbZJcckBNSJYxIckfuyYP36D17L97rh3XA699ZIN/Ke3sHH16w7Q==</latexit>X<latexit sha1_base64="mPpLqvfiTr3sYUH3/8vodE3xifA=">AAACPHicfZDLSgMxFIYzXmu969JNsAoiUmakqMuCLnQhXrC10ClyJj3ThmYyQ5IR69A3cKuP43u4dyduXZvWCt7wQODjP3+Sc/4gEVwb131yRkbHxicmc1P56ZnZufmFxaWqjlPFsMJiEataABoFl1gx3AisJQohCgReBp39fv/yGpXmsbww3QQbEbQkDzkDY6XzW3q1UHCL7qDob/CGUCDDOr1adNb8ZszSCKVhArSue25iGhkow5nAXt5PNSbAOtDCukUJEepGNhi1R9et0qRhrOyRhg7UrzcyiLTuRoF1RmDa+mevL/7Vq6cm3GtkXCapQck+PgpTQU1M+3vTJlfIjOhaAKa4nZWyNihgxqaT9w/Q7qLw2L57kqACE6vNzAfViuCmZ3dr+Vt9+s/I5afRUt7m6v1M8TdUt4veTrF0ViqUj4YJ58gKWSUbxCO7pEwOySmpEEZCckfuyYPz6Dw7L87rh3XEGd5ZJt/KeXsHIjqt+w==</latexit>z<latexit sha1_base64="+t0/fCT+bwdn8UYzdjxaOo9NJME=">AAACPHicfZDLSsNAFIYn9V6v1aWbYBVFpCQi6rKgC12IF2wrNKWcTE/SoZNJmJmINfQN3Orj+B7u3Ylb105rBW94YODjP//MnPP7CWdKO86TlRsZHRufmJzKT8/Mzs0vFBarKk4lxQqNeSyvfFDImcCKZprjVSIRIp9jze8c9Pu1a5SKxeJSdxNsRBAKFjAK2kgXt+vNhaJTcgZl/wZ3CEUyrLNmwVr1WjFNIxSaclCq7jqJbmQgNaMce3kvVZgA7UCIdYMCIlSNbDBqz14zSssOYmmO0PZA/Xojg0ipbuQbZwS6rX72+uJfvXqqg/1GxkSSahT046Mg5baO7f7edotJpJp3DQCVzMxq0zZIoNqkk/cO0ewi8cS8e5qgBB3LzcwDGUZw0zO7hd5Wn/4zMvFpNJQ3ubo/U/wN1e2Su1vaOd8plo+HCU+SZbJCNohL9kiZHJEzUiGUBOSO3JMH69F6tl6s1w9rzhreWSLfynp7By8brgI=</latexit>z0
<latexit sha1_base64="PmwWrbsHQqqrqct+03DAPOgnz4Y=">AAACPnicfVDBThsxEPXS0kKAAu2Ri0VAIIRW3pA26Q1BD72gUqkBpGwUzTqTxML2rmxv1XSVX+AKn9Pf6A9wq3rtsd4QJEDAkyw9vXkznnlJJoV1jP0OZl68nH31em6+srC49GZ5ZfXtiU1zw7HFU5maswQsSqGx5YSTeJYZBJVIPE3OD8v66Xc0VqT6mxtl2FEw0KIvOLhS+rnVVd2VKgsbtShq1ikLI7YXNT56wprva6xBo5BNUCVTHHdXg424l/JcoXZcgrXtiGWuU4BxgkscV+LcYgb8HAbY9lSDQtspJsuO6aZXerSfGv+0oxP1bkcBytqRSrxTgRvah7VSfKzWzl2/2SmEznKHmt981M8ldSktL6c9YZA7OfIEuBF+V8qHYIA7n08l/oT+FoNHfu6XDA241OwUMZiBgh9jf9sg3i3Zc0ahb42eVXyut+HRp8lJLYw+hPWv9er+wTThObJG1sk2iUiD7JPP5Ji0CCdDckEuyVXwK7gO/gR/b6wzwbTnHbmH4N9/ZVivHQ==</latexit>z0m
<latexit sha1_base64="O7NQF5PRKNqjJUlhRpMqc7MMXTM=">AAACPnicfVBNTxsxEPVCPyCU8tEjF4uAqCq0skPapDcEPXBBpVIDSNkomnUmiYXXu7K9FWGVv8AVfk7/Rv8At4orR7whSG1VeJKlpzdvxjMvzpS0jrFfwczsi5evXs/NVxbeLL5dWl5ZPbZpbgS2RKpScxqDRSU1tpx0Ck8zg5DECk/is/2yfvIDjZWp/u5GGXYSGGjZlwJcKV1sdXl3ucrCRo3zZp2ykLMd3vjsCWt+rLEG5SGboEqmOOquBBtRLxV5gtoJBda2OctcpwDjpFA4rkS5xQzEGQyw7amGBG2nmCw7ppte6dF+avzTjk7UPzsKSKwdJbF3JuCG9t9aKf6v1s5dv9kppM5yh1o8fNTPFXUpLS+nPWlQODXyBISRflcqhmBAOJ9PJfqC/haDh37u1wwNuNR8KCIwgwTOx/62QbRdsueMUj8aPav4XB/Do0+T41rIP4X1b/Xq7t404TmyRtbJe8JJg+ySA3JEWkSQIbkkV+Q6+BncBL+D2wfrTDDteUf+QnB3D/blruE=</latexit>z01
<latexit sha1_base64="L5Vl11zRP3oJrUNfhcgBJUpG4hc=">AAACRnicfVBdaxNBFL2bVq3xq62PvgyNgogsMzE28a2oUF+kFUxbyIZwd3KTDp2dXWZmi2HJ3/DV/pz+Bf+Eb6WvnU1TqKIeGDice+6de09aaOU85z+jxsrqnbv31u43Hzx89PjJ+sbmgctLK6kvc53boxQdaWWo75XXdFRYwizVdJiefKjrh6dkncrNVz8raJjh1KiJkuiDlCQZ+mOJmu2OxGi9xeNuW4heh/FY8Dei+y4Q3nvb5l0mYr5AC5bYH21Ez5NxLsuMjJcanRsIXvhhhdYrqWneTEpHBcoTnNIgUIMZuWG1WHrOXgRlzCa5Dc94tlBvd1SYOTfL0uCsl3R/1mrxb7VB6Se9YaVMUXoy8vqjSamZz1mdABsrS9LrWSAorQq7MnmMFqUPOTWTjxRusfQ5zN0ryKLP7asqQTvN8Ns83DZNXtfsf0ZlboyBNUOuN+Gxf5ODdiy2486XTmvn/TLhNXgGW/ASBHRhBz7BPvRBQgHf4QecRefRr+giury2NqJlz1P4DQ24ArVEsS4=</latexit>G1
<latexit sha1_base64="6CFy4sEr48HlSd8MTpBCL2k1yXc=">AAACRnicfVBdaxNBFL2bVq3xq62PvgyNgogsszE28a2oUF+kFUxbyIZwd3KTDp2ZXWZmi2HJ3/DV/pz+Bf+Eb6WvnU1TqKIeGDice+6de09WKOk85z+jxsrqnbv31u43Hzx89PjJ+sbmgctLK6gvcpXbowwdKWmo76VXdFRYQp0pOsxOPtT1w1OyTubmq58VNNQ4NXIiBfogpalGfyxQsd2RHq23eNxtJ0mvw3ic8DdJ910gvPe2zbssifkCLVhif7QRPU/HuSg1GS8UOjdIeOGHFVovhaJ5My0dFShOcEqDQA1qcsNqsfScvQjKmE1yG57xbKHe7qhQOzfTWXDWS7o/a7X4t9qg9JPesJKmKD0Zcf3RpFTM56xOgI2lJeHVLBAUVoZdmThGi8KHnJrpRwq3WPoc5u4VZNHn9lWVop1q/DYPt03T1zX7n1GaG2NgzZDrTXjs3+SgHSfbcedLp7XzfpnwGjyDLXgJCXRhBz7BPvRBQAHf4QecRefRr+giury2NqJlz1P4DQ24AiO3sWo=</latexit>Gm
<latexit sha1_base64="rODIjbgl+1AbQU4DQzfN+yZaqbc=">AAACQHicfVBNTxsxEPVS2tL0C8qRi9W0UlVVK28aSLghwYEeUEFqAlI2QrPOJLh47ZU9i4hW+Q9c6c/pv+g/4Ia49oQTgtRWbZ9k6enNm/HMywqtPAnxI1p4sPjw0eOlJ7Wnz56/eLm88qrrbekkdqTV1h1l4FErgx1SpPGocAh5pvEwO92e1g/P0HllzRcaF9jPYWTUUEmgIHXTs4Elf7xcF3GrkSTtJhdxIj4mrc1ARHu9IVo8icUMdTbH/vFK9CYdWFnmaEhq8L6XiIL6FThSUuOklpYeC5CnMMJeoAZy9P1qtu6Evw3KgA+tC88Qn6m/dlSQez/Os+DMgU78n7Wp+Ldar6Rhu18pU5SERt59NCw1J8unt/OBcihJjwMB6VTYlcsTcCApJFRLdzDc4nAvzP1coAOy7n2VghvlcD4Jt43SD1P2P6My98bAaiHX+/D4v0m3EScbcfOgWd/6NE94ia2x1+wdS1iLbbFdts86TLKv7IJdsm/R9+gquo5u7qwL0bxnlf2G6Octxh2wVw==</latexit>...<latexit sha1_base64="rtd53eRvdjXL7aJ/XJ2AToV2vfk=">AAACR3icfVBNbxMxEPWGAm34auHIxWpAQgit7BCacKugBy6IVmraouwqmnUmiVWvd2vPAtEqv4Mr/Tn9CfwKbogjTppKgIAnWXp682Y887LSaE9CfI0a19au37i5vtG8dfvO3XubW/ePfFE5hX1VmMKdZODRaIt90mTwpHQIeWbwODt9vagff0DndWEPaVZimsPE6rFWQEFKk496hFOg+v18KIebLRF321L2OlzEUjyX3ZeBiN6LtuhyGYslWmyF/eFW9CgZFarK0ZIy4P1AipLSGhxpZXDeTCqPJahTmOAgUAs5+rRebj3nj4My4uPChWeJL9VfO2rIvZ/lWXDmQFP/Z20h/q02qGjcS2tty4rQqsuPxpXhVPBFBHykHSoys0BAOR125WoKDhSFoJrJHoZbHL4Nc9+V6IAK97ROwE1y+DQPt02SZwv2P6O2V8bAmiHXq/D4v8lRO5Y7ceeg09p9tUp4nT1k2+wJk6zLdtkbts/6TLEz9pl9YefRRfQt+h79uLQ2olXPA/YbGtFPpeOyLg==</latexit>bY1
<latexit sha1_base64="Uvry7M7d/1+yNOnfr0h+vfaieXU=">AAACR3icfVBNbxMxEPWGAm34auHIxWpAQgitvCE04VZBD1wQrdS0RdlVNOtMEqu2d2vPAtEqv4Mr/Tn9CfwKbogjTppKgIAnWXp682Y88/JSK09CfI0a19au37i5vtG8dfvO3XubW/ePfFE5iX1Z6MKd5OBRK4t9UqTxpHQIJtd4nJ++XtSPP6DzqrCHNCsxMzCxaqwkUJCy9KMa4RSofj8fmuFmS8TddpL0OlzEiXiedF8GInov2qLLk1gs0WIr7A+3okfpqJCVQUtSg/eDRJSU1eBISY3zZlp5LEGewgQHgVow6LN6ufWcPw7KiI8LF54lvlR/7ajBeD8zeXAaoKn/s7YQ/1YbVDTuZbWyZUVo5eVH40pzKvgiAj5SDiXpWSAgnQq7cjkFB5JCUM10D8MtDt+Gue9KdECFe1qn4CYGPs3DbZP02YL9z6jslTGwZsj1Kjz+b3LUjpOduHPQae2+WiW8zh6ybfaEJazLdtkbts/6TLIz9pl9YefRRfQt+h79uLQ2olXPA/YbGtFPFFayag==</latexit>bYm
<latexit sha1_base64="eow8BoOVgK6gRUw64Z0JIyLW5ro=">AAACPnicfZDNSsNAFIUn/ht/q0s3wSqISElE1GVRF27EClaFJpSb6W07dDIJMxOxhL6CW30cX8MXcCduXTqpEdSKFwY+zj0zc+8JE86Udt1na2x8YnJqembWnptfWFxaLq1cqTiVFOs05rG8CUEhZwLrmmmON4lEiEKO12HvOO9f36JULBaXup9gEEFHsDajoHPJrynWXC67FXdYzih4BZRJUbVmydrwWzFNIxSaclCq4bmJDjKQmlGOA9tPFSZAe9DBhkEBEaogGw47cDaN0nLasTRHaGeofr+RQaRUPwqNMwLdVb97ufhXr5Hq9mGQMZGkGgX9/KidckfHTr6502ISqeZ9A0AlM7M6tAsSqDb52P4Jml0knpl3zxOUoGO5nfkgOxHcDcxuHX8np/+MTHwZDdkmV+93iqNwtVvx9it7F3vl6lGR8AxZI+tki3jkgFTJKamROqGkS+7JA3m0nqwX69V6+7SOWcWdVfKjrPcPIkCu9g==</latexit> <latexit sha1_base64="rODIjbgl+1AbQU4DQzfN+yZaqbc=">AAACQHicfVBNTxsxEPVS2tL0C8qRi9W0UlVVK28aSLghwYEeUEFqAlI2QrPOJLh47ZU9i4hW+Q9c6c/pv+g/4Ia49oQTgtRWbZ9k6enNm/HMywqtPAnxI1p4sPjw0eOlJ7Wnz56/eLm88qrrbekkdqTV1h1l4FErgx1SpPGocAh5pvEwO92e1g/P0HllzRcaF9jPYWTUUEmgIHXTs4Elf7xcF3GrkSTtJhdxIj4mrc1ARHu9IVo8icUMdTbH/vFK9CYdWFnmaEhq8L6XiIL6FThSUuOklpYeC5CnMMJeoAZy9P1qtu6Evw3KgA+tC88Qn6m/dlSQez/Os+DMgU78n7Wp+Ldar6Rhu18pU5SERt59NCw1J8unt/OBcihJjwMB6VTYlcsTcCApJFRLdzDc4nAvzP1coAOy7n2VghvlcD4Jt43SD1P2P6My98bAaiHX+/D4v0m3EScbcfOgWd/6NE94ia2x1+wdS1iLbbFdts86TLKv7IJdsm/R9+gquo5u7qwL0bxnlf2G6Octxh2wVw==</latexit>...
<latexit sha1_base64="H/zPQBHbEU+inRtdEtB/PMWdxBg=">AAACRnicfZDPSiNBEMZr4u6q2XX9s0cvzWYFEQkzEtSjuAt6ERWMETIh1HQqSZOenqG7RwxDXmOv+ji+gi/hTbzaEyOoEQsafnz1dXfVF6VSGOv7d15p5svXb7Nz8+XvPxZ+Li4tr5ybJNOc6jyRib6I0JAUiupWWEkXqSaMI0mNaPC36DcuSRuRqDM7TKkVY0+JruBonRSGMdo+R8kO2n57qeJX/XGxaQgmUIFJnbSXvT9hJ+FZTMpyicY0Az+1rRy1FVzSqBxmhlLkA+xR06HCmEwrHw89YmtO6bBuot1Rlo3V1zdyjI0ZxpFzFkOa971C/KjXzGx3t5ULlWaWFH/+qJtJZhNWJMA6QhO3cugAuRZuVsb7qJFbl1M5/EduF01H7t3jlDTaRG/kIepejFcjt1sv3CzoM6NQL0ZHZZdr8D7FaTjfqgbb1dpprbK3P0l4DlbhN6xDADuwB4dwAnXgkMJ/uIYb79a79x68x2dryZvc+QVvqgRPPpyw6w==</latexit>G0<latexit sha1_base64="JjAfhP4R8Kdx4+VQdQPYW4DRyJM=">AAACR3icfZDLSsNAFIYn9V7vunQTrIKIlEREXYq6cCMqWC80oZxMT9vBySTOnKgl9Dnc6uP4CD6FO3HptFbwhgcGPv7zz8w5f5RKYcjznp3CwODQ8MjoWHF8YnJqemZ27swkmeZY4YlM9EUEBqVQWCFBEi9SjRBHEs+jq71u//wGtRGJOqV2imEMTSUaggNZKQxuRR1bQPllp+bVZkpe2euV+xv8PpRYv45rs85SUE94FqMiLsGYqu+lFOagSXCJnWKQGUyBX0ETqxYVxGjCvDd1x122St1tJNoeRW5P/Xojh9iYdhxZZwzUMj97XfGvXjWjxnaYC5VmhIp/fNTIpEuJ243ArQuNnGTbAnAt7Kwub4EGTjaoYrCPdheNh/bdoxQ1UKJX8wB0M4a7jt2tGax16T+jUJ9GS0Wbq/8zxd9wtl72N8sbJxulnd1+wqNsgS2yFeazLbbDDtgxqzDOrtk9e2CPzpPz4rw6bx/WgtO/M8++VcF5By87ses=</latexit>bY0<latexit sha1_base64="KqzGpleD13x0G7Vrlw5EMNlUH1U=">AAACRXicfVBNb9NAEF2Xr5JSaOmxlxUpEqoqyzZWEm4V9NALokikLYqjaryZJKusd63dMRBZ+Rm9ws/hN/AjekNcYZOmEkUtT1rp6c2b2ZmXl0o6iqIfwcqdu/fuP1h92Fh7tP74ycbm02NnKiuwK4wy9jQHh0pq7JIkhaelRShyhSf55M28fvIJrZNGf6Bpif0CRloOpQDyUi/7LAc4Bqo/zs42mlGYJmn8ssWjsJO0X7VST5J2q5MkPA6jBZpsiaOzzWAnGxhRFahJKHCuF0cl9WuwJIXCWSOrHJYgJjDCnqcaCnT9erHzjD/3yoAPjfVPE1+of3fUUDg3LXLvLIDG7t/aXLyp1qto2OnXUpcVoRaXHw0rxcnweQB8IC0KUlNPQFjpd+ViDBYE+Zga2QH6Wyy+9XPflWiBjN2tM7CjAr7M/G2jbG/O/meU+sroWcPnehUev50cJ2HcCtP3aXP/9TLhVbbNnrEXLGZtts8O2RHrMsEMO2df2bfge3AR/Ax+XVpXgmXPFruG4PcfZ8Kymg==</latexit>bY
Step 1Step 2Step 3deterministic transitionstochastic transition<latexit sha1_base64="espj5Jd4DP6QlfdMdutbPiataE4=">AAACPXicfVDLShxBFK32kZjJw9cym8IxECQ03Z1mZtyJunAjUXRUmB7kds2dtrCquqmqFodmPsFt8jl+hx/gTty6Tc04ghHNgYLDuefeuvekheDGBsGNNzU9M/vu/dyH2sdPn7/MLywuHZm81AzbLBe5PknBoOAK25ZbgSeFRpCpwOP0fGtUP75AbXiuDu2gwK6ETPE+Z2CddJDI8nShHvhxFIc/GzTwW1FzvRE7EjUbrSiioR+MUScT7J0ueqtJL2elRGWZAGM6YVDYbgXaciZwWEtKgwWwc8iw46gCiaZbjXcd0m9O6dF+rt1Tlo7V5x0VSGMGMnVOCfbMvKyNxNdqndL2W92Kq6K0qNjjR/1SUJvT0eG0xzUyKwaOANPc7UrZGWhg1sVTS7bR3aJx1839VaAGm+u1KgGdSbgcutuy5MeI/c/I1ZPRsZrL9Sk8+jY5ivyw4cf7cX1jc5LwHPlKVsh3EpIm2SA7ZI+0CSMZuSK/yR/v2rv17rz7R+uUN+lZJv/Ae/gLBquu9Q==</latexit>¬µ
Figure 1: (Left) MoE-LM Architecture. (Right) Sample utterance workÔ¨Çow generated by an MoE-
LM trained with Reddit data. Step 1: encodes conversation history. Step 2: 	Gi,8i, generate
candidate bot utterances. Step 3: selects the bot response by Q-score ranking & post-processing.
the one in OPAL [Ajay et al., 2020], i.e., an unsupervised learning method for discovering primitive
skills in trajectories that are used by some downstream RL tasks.
Step 2: Expert Construction. Given the latent space Z, encoder (;G0), and decoder 	
learned in Step 1, we now learn mlatent distributions fGigm
i=1, each deÔ¨Åned as Gi(z0jz) =
N 
i(z);2
i(z)Idd
. Intuitively, eachGicorresponds to an attribute, e.g., an intent or a personality
(in case of a chatbot) and generates samples in speciÔ¨Åc parts of the latent space Z. This results in
havingmLMs,fLMigm
i=1;LMi= (;Gi;	), each of them corresponds to a specialized version of
the original LM, LM0, and serves as an expert in our MoE-LM. Upon receiving a conversation history
X, each expert LMigenerates a candidate (or more) for the next utterance bYiin certain parts of the
language space that are compatible with its attribute (personality). As we will explain in Section 5,
eachGiis learned using a loss function that encourages its corresponding LM, LMi, to generate
utterances consistent with its attribute (see Eq. 2).
Step 3: Dialogue Manager (DM). Thedialogue manager , denoted by , takes as input the encoded
conversation history z= (X)and the candidate action utterances generated by the experts fbYigm
i=0,
and selects one of them as the action for the bot to execute, i.e., bY(jz;fbYigm
i=0). We will
describe how DM is trained using reinforcement learning (RL) in Section 6.
4 Primitive Discovery in MoE-LM
Motivated by literature in the reinforcement and imitation learning Ô¨Åelds [Ajay et al., 2020], we
propose to learn the primitive LM, LM0, in our MoE-LM by solving the following KL-constrained
optimization problem that aims at capturing diverse semantics:
min
(;G0;	);bEz0(jz;Y);z=(X)
 log 	(Yjz0)
;s.t.bEz=(X)
KL 
(z0jz;Y)jjG0(z0jz)
KL;(1)
wherebEis the empirical expectation over (X;Y)in the datasetD,is a distribution over the latent
space conditioned on the encoded conversation history zand the target utterance Y, andKLis a posi-
tive real-valued threshold. Using (1), we learn LM0= (;G0;	)by maximizing the log-likelihood,
while enforcing consistency between the latent variable z0predicted byG0(jz)and(jz;Y)via
the KL constraint. The distribution (jz;Y)is a GaussianN 
(z;(Y));2
(z;(Y))Idd
in
which is a pre-trained encoder for the target utterance Y, and mean(;)and variance 2
(;)
are trainable models. One reason for using a separate encoder for the target utterance Yis to
avoid overÔ¨Åtting (i.e., to avoid having back-propagation gradient of withYas input).
Connection to V AE-like objectives In practice, we implement the KL constraint in (1)as a penalty
weighted by an appropriately chosen coefÔ¨Åcient. Thus, one may interpret the objective in (1)as a
variation of-V AE [Burgess et al., 2018]. Due to the connection to V AEs, one may draw similarities
4

--- PAGE 5 ---
between our method and existing dialogue approaches such as VHRED [Serban et al., 2017b] and
VHCR [Park et al., 2018]. However, we emphasize that there are key differences, and these may be
explained by Ô¨Årst understanding how the objective in (1)encourages diversity , which is key to good
primitive learning. Namely, it is important that primitive discovery learns an encoder-decoder ;	
which can be modulated by the choice of z; i.e., changing z0while Ô¨Åxing Xshould lead to different
distributions over generated utterances. The objective in (1)encourages this diversity by conditioning
the latent variable z0on both the target utterance Yandz= (X), i.e.,z0(jz;Y). In contrast,
the KL constraint is used to make sure that the stochastic encoder G0(jz)of our primitive LM is not
too varied for different Y, and thus has a limiting effect on diversity. For example, in the extreme
whenKL= 0(or,!1 when used as a regularizer) there will be no specialization of the latent
space for different Y. Although!1 is an extreme case, degenerate behavior can also happen
when= 1, i.e., in the traditional variational loss used by VHRED and VHCR. SpeciÔ¨Åcally, it is
well-known that the traditional V AE loss is an upper bound on the negative log-likelihood of the data
under a stochastic encoder-decoder parameterization. Thus if the data can be modeled by a single
LM then a V AE-optimal decoder 	can simply ignore G0, leading to a degenerated latent space as
observed in previous work [Park et al., 2018]. This is precisely the reason that, in our approach, we
weaken the KL constraint ( KL0or, equivalently, 1). This enables our approach to more
reliably guarantee that a unique z0represents each distinct conversation pair (X;Y), thus capturing
diverse semantic modalities and enabling easier downstream specialization.
In the mathematical results below, we formalize the claim above, namely, that the log-likelihood
objective in (1)leads to a learned ;	that can easily recover any arbitrary desired LM by specializing
the latent spaceG. We begin with a deÔ¨Ånition that characterizes the coverage of an arbitrary LM on
the conditional conversation data distribution PD(YjX).
DeÔ¨Ånition 1. LMD;is a-common LM of data DifED[TV(LMD;(YjX)jjPD(YjX)))]:
Leveraging Theorem 4.1 in Ajay et al. [2020], we now present the theoretical result characterizing
the representational power of our primitive encoder-decoder pair (;	)on dataD.
Lemma 1. Let(;;	)be the solution to (1) with bEz0(jz;Y);z=(X)[ log 	(Yjz0)] =. Then
there exists LM:= (;G;	)such that ED[TV(LMD;(YjX)jjLM(YjX))]+q
1
2(+H);where
G(z0jz) =EYD[(z0jz;Y)], andH=ED[logPD(YjX)]is a constant depending on D.
The result above shows that, as long as LMD;is-common inD, then there exists a specialization of
the latent spaceGthat, when paired with ;	, can approximately recover LMD;. The quality of the
approximation is a function of ‚Äî how well the objective in (1)was optimized ‚Äî and . In practice,
to construct the primitive by replacing GwithG0, i.e., LM0= (;G0;	), becauseG0(z0jz)can be
viewed as an distillation of (z0jz;Y). This theoretical result also motivates the next section, where
we explain our algorithm‚Äôs ‚ÄúStep 2: Expert Construction‚Äù. SpeciÔ¨Åcally, we show how to use the
trained encoder-decoder pair ;	to learn a spectrum of different specialized experts parameterized
by different latent distributions Gi.
5 Expert Construction with Plug-and-play Language Models
To complete the MoE framework one needs to systematically create a gamut of different experts
LMi,8i2f1;:::;mg, with each generating candidate utterances of different intents. By viewing
each expert as a distribution of particular behaviors in conversation data D, we leverage the results
of Section 4 and Lemma 1 and adopt a universal encoder-decoder (;	)among all the experts.
Therefore, each expert iis only parameterized by an arbitrary d-dimensional latent distribution
(e.g., Gaussian), and it samples certain regions of the latent space Z. Following the terminology
from Dathathri et al. [2019], these experts can all be catagorized as plug-and-play language models
(PPLMs) . Creating experts is handy because it only requires learning new latent distributions, while
switching between experts amounts to sampling a different distribution.
Denote by`i(X;Y)2Ra real-valued label that characterizes the intent of expert i2f1;:::;mg,
e.g., determined by an off-the-shelf sentiment classiÔ¨Åer. We train the latent distribution Gi(z)of
expertiby solving the optimization problem
min
GibEz0Gi(jz);z=(X);Y	(jz0)[ `i(X;Y)]: (2)
Unlike the weighted maximum likelihood approach considered in Dathathri et al. [2019], which
assigns weight `ito training samples that correspond to expert i, we propose to learn each expert via
5

--- PAGE 6 ---
reward-maximization and treat`ias a reward signal w.r.t. expert ito be maximized. Interestingly,
this approach is also linked to reinforcement learning (RL), in which both the ‚Äústate‚Äù and ‚Äúaction‚Äù
spaces are the latent space Z, and the ‚Äúpolicy‚Äù is the latent distribution Gi. The main beneÔ¨Åt of our
approach is that it does not require the target utterance Yfrom dataDand is thus less vulnerable to
data-imbalance issues in Don certain intents. Notice from (2)that the reward-maximization problem
is myopic, i.e., the above RL problem has a discounting factor of 0. The main motivation is that,
unlike dialogue management that is a sequential decision-making problem, here we want each expert
to possess particular behaviors, and this can readily be done via greedy maximization; Long-term
dialogue optimization will be handled by the dialogue manager rather than these experts.
For example in the case of a Gaussian Gi, we use the standard REINFORCE [Sutton et al., 1999]
algorithm to learn the model parameters (i;2
i)ofGiaccording to
fi;ig fi;ig+Ez0Gi(jz);Y	(jz0)[`i(X;Y)rfi;iglogPGi(z0jz)]; i2f1;:::;mg;
where>0is the learning rate. To reduce the variance of these estimates, we also adopt the baseline
reduction technique [Greensmith et al., 2004] in policy gradient. This can be simply done by replacing
`i(X;Y)with`i(X;Y) :=`i(X;Y) EY	(j(X))[`i(X;Y)]. To quantify the performance of
expert LMi, following arguments from Lemma 1 and Lemma 4.0.1 in Ajay et al. [2020], we have the
sub-optimality result.
Corollary 1. Denote the i-th reward-maximizing objective as Li(LM) :=bEYLM(jX)[`i(X;Y)].
Suppose an optimal LM for this objective LMi;2arg max LMLi(LM)is-common inD. Moreover,
letG?
ibe in the arg min of(2). Then with expert LMi= (;G?
i;	)and (,H) from Lemma 1, we
havejLi(LMi) Li(LMi;)j2k`ik1(+q
1
2(+H)).
While it may be obvious that optimizing Giw.r.t. (2) encourages expert LMito capture the behaviors
encouraged by `i, this corollary has two further implications: (i) Since the sub-optimality of LMi
compared to the oracle LMi;is bounded by the quantity deÔ¨Åned in Lemma 1, it justiÔ¨Åes using the
primitive (	;), which optimizes , for expert construction; (ii) Sub-optimality further depends on ,
quantifying how well LMi;is represented in the original dataset D.
6 Reinforcement Learning for MoE-LM Dialogue Manager
We now describe the dialogue manager (DM) of our MoE-LM and propose RL algorithms to train it.
As mentioned in Section 3, the DM is a policy that takes the encoded conversation history z= (X)
and them+ 1candidate action utterances generated by the experts fbYigm
i=0,4and stochastically
selects one of them to execute, i.e., bY(jz;fbYigm
i=0). Note that each expert i2f0;:::;mgis
a LM, LMi, that acts as a policy i(jX)and maps each conversation history Xto an utterance bYi.
With this architecture we address the large size of state and action spaces in the original MDP that
grows exponentially with the size of the vocabulary. As described in Section 2, the state and action
spaces of the original MDP are the tokenized conversation history and the tokenized language space,
respectively, while here the DM should choose among m+ 1actions given the latent space Zof
encoded conversations. It is important to note that our MoE-LM is different than other hierarchical
architectures [D et al., 2016] in which the decision at the high-level is to choose a low-level controller
only based on the current state of the system. In MoE-LM, the DM observes both the current state
and the actions suggested by the experts and then chooses one among them.
We learn the DM policy by solving an MDP, which we refer to as MoE-MDP, and denote it by
M= (S;A;P;R;s0;). The state space of MoE-MDP is the product of the learned latent space Z
and the joint action space of the m+ 1experts, i.e., S=ZAm+1. Its action space consists of
them+ 1experts, i.e., A=f0;:::;mg. Its initial state is the encoding of the initial user‚Äôs query
and the utterances suggested by the experts in response to this query. The transition still models
user‚Äôs responses but is now over the joint space of the latent states and experts‚Äô actions. The reward
function is the same as in the original MDP, i.e., r(s;a) =r(X;aj), where s= (z;faigm
i=0)with
aii(jX)andz= (X), and a2f0;:::;mgis the expert selected by the DM. Since MoE-MDP
has Ô¨Ånite number of actions, learning DM is equivalent to solving a Ô¨Ånite-action MDP and Ô¨Ånding a
policyin them-dimensional simplex with maximum expected cumulative discounted reward.
4For simplicity, we assume that each expert generates only a single candidate utterance at each step. It would
be straightforward to extend this to multiple (and even a varying number of) candidate utterances.
6

--- PAGE 7 ---
We use two RL algorithms to solve the MoE-MDP and learn the DM policy . The Ô¨Årst one
isconservative Q-learning (CQL) [Kumar et al., 2020], a popular ofÔ¨Çine RL algorithm. This
is suitable for our setting in which the policy must be learned from the collected conversations
Dwith no further (online) interaction with the real system. CQL is a regularization scheme
that learns a conservative Q-function that lower-bounds the true one. Given the ofÔ¨Çine conver-
sation dataD, we parameterize the Q-function by parameter and learnby minimizing the
Bellman error with behavior regularization: minP
(s;a;r;s+)2D(Ea[Q(s;a)] Q(s;a0)) +
(r+Qtarget(s+;arg maxa+2AQ(s+;a+)) Q(s;a))2, wherea0is action suggested by the primi-
tive LM (which approximates the behavior policy of D)a00,>0is a regularization parameter,
andtargetis the target Q-function parameter. Intuitively, CQL regularization minimizes the differ-
ences inQ-values of our DM and the primitive. According to the CQL algorithm, we can set the
DM as(ajs)/exp(Q(s;a)), which corresponds to the optimal policy of the entropy-regularized
Q-learning [Schulman et al., 2017].
The second RL algorithm we use is model-based RL (MBRL) [Shah et al., 2018, Wei et al., 2018].
Here we Ô¨Årst learn a user utterance model Puser(X+jX;a) :=Ez= user([X;a])[	user(X+jz)]via maxi-
mum likelihood, then generate data DMB, whose next-state bs+encodes the next conversation generated
from roll-outs and the corresponding candidate actions, and Ô¨Ånally solve the Bellman error minimiza-
tion in MoE-MDP: minP
(s;a;r;bs+)2D MB(r+Qtarget(bs+;arg max a+2JQ(bs+;a+)) Q(s;a))2.
The beneÔ¨Åt of MBRL over CQL is two-fold. First, one can easily obtain a high-Ô¨Ådelity user utterance
model [Peng et al., 2020] by simply Ô¨Åne-tuning a large LM (e.g., GPT-3 [Floridi and Chiriatti, 2020]).
Second, with sufÔ¨Åcient dialogue roll-outs that captures many different scenarios, MBRL does not
require behavior regularization and can be less conservative.
7 Experiments
We evaluate our MoE-approach for dialogue management on two benchmark open-domain tasks.
The Ô¨Årst one is Cornell [Danescu-Niculescu-Mizil and Lee, 2011], which consists of conversations
between speakers in different movie lines and has a median conversation length of 3utterances, the
second is the Reddit [Ghandeharioun et al., 2019], which is a casual conversation corpus on various
topics between users of at least 3turns with the median conversation containing 7utterances.
We conduct several experiments to test the efÔ¨Åcacy of different parts in the MoE-LM, namely (i)
the predictive power and diversity of the primitive, (ii) the quality of experts, and (iii) the overall
DM performance. For each metric, we report the mean standard deviation over a evaluation set of
100conversations. We also ran an ablation study on 4transformer-based MoE-LMs, namely MoE-1,
MoE-2, MoE-3- MoE-4, to understand how performance is affected by different model architectures,
language encoders, and latent generators. MoE-1 and MoE-2 use a simpler architecture, while MoE-3
and MoE-4 use the same encoder architecture as BERT [Devlin et al., 2018]. MoE-1 uses much
smaller latent distribution models fGigthan MoE-2; MoE-3 uses the pre-trained BERT encoder,
while MoE-4 trains that from scratch. Details of these models can be found in Appendix ??.
EXP 1: Comparing Primitive Models We compare the quality of latent representations learned
by the 4MoE-LMs (via optimizing Eq. 1) and 2baselines (standard Transformer [Wolf et al.,
2019] and VHRED [Serban et al., 2017b]). To assess their quality, for each test conversation we
generated 25utterances and reported the following 3metrics: (i) Diversity : The 1 Sparsity [Hurley
and Rickard, 2009] of the singular values of the embedded utterances, i.e., Diversity (f^Yig) :=
1 p
d kSVDk1=kSVDk2=p
d 12[0;1], where SVD :=SVD(fSE(^Yig25
i=1), and SEis a
pre-trained sentence encoder (e.g., a USE [Cer et al., 2018]); (ii) Dist-f1;2;3g[Li et al., 2015]:
Ratio of uniquef1;2;3g-gram in the generated utterances; (iii) Perplexity [Bahl et al., 1983]. These
metrics measure both accuracy and semantic diversity.
The results of the above experiments are reported in Table 1 and 10 (Appendix B.1), and sample
utterances generated by these LMs can be found in Appendix B.2. In comparisons with the baselines
(Transformer and VHRED), generally (i) transformer-based LMs out-perform VHRED due to their
attention mechanism that explicitly encodes sequential semantic information, and (ii) the MoE-LMs
achieve way better diversity without sacriÔ¨Åcing much on accuracy (i.e., the perplexity scores are still
quite low). Qualitatively, the sample utterances generated the Transformer are closer to the targets
than that by MoE-2 and MoE-4, likely because Transformer tends to memorize the corpus [Kharitonov
et al., 2021]. Contrarily, MoE-LMs generate utterances that have similar contexts with targets but
paraphrased or similar structures but different contexts, demonstrating their generalizability.
7

--- PAGE 8 ---
Method Diversity Dist-1 Dist-2 Dist-3 Perplexity
MoE-1 0.0780.04 0.29 0.68 0.79 28.6841.32
MoE-2 0.150.02 0.33 0.81 0.92 42.1435.24
MoE-3 0.0930.02 0.31 0.77 0.90 44.2757.73
MoE-4 0.170.01 0.41 0.89 0.94 52.1218.20
Trans. 0.0910.02 0.28 0.71 0.83 20.7710.22
VHRED 0.110.02 0.37 0.68 0.80 88.6123.55
Table 1: Accuracy (Perplexity) and Diversity of
Language Primitive Experts Trained with Reddit.Method User Tot. Sent. User Sent. Trans. GPT-Perplexity
MoE-4 Primitive -0.050.07 0.050.09 81.1743.41
MoE-4 DQN 0.040.10 0.020.16 79.4345.70
MoE-4 CQL 0.090.16 0.080.17 75.4847.13
MoE-4 MBRL 0.120.15 0.100.13 82.7542.72
Trans. RL -0.260.12 -0.110.14 168.8075.35
Trans. KLC 0.030.09 -0.030.05 108.7368.21
Trans. VHRL 0.050.11 0.010.11 135.4755.78
Table 2: Performance (w.r.t. Avg. User Satisfaction in
Conversation) of RL-based DM Trained with Reddit.
(a)Transformer Primitive, TSNE
 (b)MoE-4 Primitive, TSNE
 (c)MoE-4 Sentiment, PCA
 (d)MoE-4 Emotion, PCA
Figure 2: Latent space visualizations. Figures (a) and (b) compares two primitive representations. Figures (c)
and (d) illustrates how experts (deÔ¨Åned by different sentiments and emotions) are represented by latent clusters.
Among different MoE-LMs, MoE-2 and MoE-4 have the best performances, particularly MoE-4 has
better diversity while MoE-2 has lower perplexity. This corroborates with our hypotheses that (i)
jointly training the encoder and decoder with Eq. 1 seems necessary to encourage semantic diversity
(as opposed to using a pre-trained BERT encoder, which maximizes likelihood), (ii) sufÔ¨Åcient
representation power is necessary for G0to match the posterior distribution in order to capture
different semantics in D. In Fig. 2a and 2b, we visualize the latent space of 200conversation data
samples for both Transformer and MoE-4. The latent states of MoE-4 are much more dispersed
across the embedding space, implying that most conversations get encoded uniquely. In contrast,
the latent space of Transformer has many clusters, suggesting it is more prone to generating similar
utterances even with different input conversation and is thus less generalizable.
EXP 2: Quality of Experts We compare the performance of experts learned by the 4MoE-LMs
(where experts are separately trained by optimizing Eq. 2) and 2baselines (WD [Holtzman et al.,
2018] and PPLM [Dathathri et al., 2019]). To study the sub-optimality gap in Corollary 1, we also
include the performance of Transformer-based expert end-to-end LMs that are individually optimized
with REINFORCE [Li et al., 2016], using the expert labels f`igas rewards. We use the following label
functions to deÔ¨Åne the intents of experts: (i) `pos-sent (Y),`neg-sent (Y),`joy(Y),`optimism (Y),`anger(Y),
`sadness (Y)quantify 6different sentiment tones and are constructed by a RoBERTa-based sentiment
detector [Liao et al., 2021] that predicts whether an utterance is of positive or negative sentiment,
and whether it falls into any of the 4more-reÔ¨Åned emotions: {joy, optimisim, sadness, anger}; (ii)
`sent-coh (X;Y)measures empathy , i.e., bot‚Äôs sentiment coherence with user‚Äôs; (iii) `question (Y)outputs
1when a bot question is detected and 0otherwise; (iv) `exp(X;Y)quantiÔ¨Åes exploration , i.e., the
tendency to avoid repetitive contexts. See Appendix A.7 for details.
The results of the above experiments are reported in Table 3 and 12 (Appendix B.1), with sample
utterances reported in Appendix B.3 to B.9. Compared with the baseline LMs, generally the experts
created under the MoE-LM framework, especially under MoE-2 and MoE-4, better capture all
different language intents (where WD and PPL appear to capture negative sentiments and emotions
much more effectively than behaviors), demonstrating the efÔ¨Åcacy of our approach which constructs
specialized experts on a diverse language space via reward maximization (instead of weighted MLE).
Similar to the ablation study in EXP 1, all the experts associated with MoE-2 and MoE-4 are also
among the best ones in capturing language intents. Interestingly, with the Reddit data the experts
in MoE-4 perform the best, while with much less data (Cornell) the best experts are built upon the
simpler MoE-2 architecture. We conjecture this difference is due to over-Ô¨Åtting issues faced by the
larger LMs (MoE-4) when there is insufÔ¨Åcient data for expert Ô¨Åne-tuning. In Fig. 2c and 2d we
visualize the latent space of the sentiment-based experts in MoE-4, each with 200conversation data
samples. Notice that the sentiment experts‚Äô latent distributions are clearly separated (because positive
and negative sentiments have opposite behaviors), while the emotion expert‚Äôs latent distribution have
more gradual separations and even some overlaps (because e.g., joy versus optimism are quite similar,
while joy versus anger are quite different). This validates our MoE-LM represents different behaviors
8

--- PAGE 9 ---
Method Question Exploration Positive Sent. Negative Sent. Sent. Coherence Joy Optimism Anger Sadness
MoE-1 0.750.14 0.490.12 1.020.35 0.400.33 0.510.48 1.120.32 -0.190.55 0.470.56 1.010.67
MoE-2 0.960.18 0.580.20 3.410.56 2.080.71 0.550.44 2.020.65 1.270.41 1.320.69 2.090.99
MoE-3 0.430.27 0.500.33 1.030.32 1.050.29 0.620.41 1.170.42 0.420.51 0.280.32 1.150.27
MoE-4 0.910.25 0.530.40 3.340.77 2.120.64 0.500.36 2.281.59 1.590.99 2.011.32 2.270.45
WD 0.00.0 0.160.29 -0.961.86 1.362.02 0.690.23 -0.840.40 -1.771.36 0.920.35 1.991.5
PPLM 0.210.34 0.560.32 0.460.21 0.550.09 0.640.27 0.390.15 0.410.48 0.690.22 1.000.21
Trans. RL* 0.990.23 0.540.18 3.531.64 1.891.20 0.720.30 2.882.36 1.802.39 1.622.39 2.351.47
Table 3: Quality of each expert PPLM trained on Reddit dataset w.r.t. its trained label.
in separate regions of the latent space and justiÔ¨Åes our structural prior of modeling each expert as a
specialized version of the primitive LM, whose latent distribution focuses on particular regions.
EXP 3: MoE-RL Against DialoGPT Simulated Users We compare the dialogue management
performance of MoE-LM, for which their DMs are trained with different methods (BC, DQN,
CQL, MBRL), with 3baselines (REINFORCE [Li et al., 2016], KL-control [Jaques et al., 2019],
and VHRL [Saleh et al., 2020]). According to the results on expert quality in EXP2, we pick
the MoE-2 and MoE-4 frameworks for the Cornell and Reddit tasks respectively. For systematic
evaluation, we perform the experiment by having these RL agents interact with a DialoGPT [Zhang
et al., 2019] simulated user environment (a large LM capable of conducting natural open-domain
conversations), for a maximum of 5turns. The DM task is to maximize total user satisfaction
in the conversation level, which is measured by both (i) user‚Äôs overall sentiment, and (ii) user‚Äôs
sentiment transition. To construct an immediate reward that serves as a surrogate for user satisfaction,
we setr(X;a;X +) =1`sent(X+) +2(`sent(X+) 1 
1 LPL 1
l=0l`sent(Xl)), where the linear
combination weights (1;2) = (0:75;0:25)correlate with Ghandeharioun et al. [2019], and `sent(X)
is the same RoBerTa-based sentiment labeler as in EXP2, which assigns a score from [ 1;1]that is
proportional to the positive sentiment and inversely proportional to the negative sentiment prediction
probabilities. To ensure the baseline RL DM methods can also possess certain bot-level features, e.g.,
question, positive sentiment, etc., besides the above RL reward for user satisfaction we also optimize
a linear combination of bot-based rewards when training the baseline models, see Appendix B of
Saleh et al. [2020] for more details. Since the DM problem lasts at most 5turns, we use this as the
effective horizon and set = 1 1=5 = 0:8. To evaluate the Ô¨Çuency of our LMs, we also report the
GPT-perplexity, which compute the perplexity score of the Ô¨Ånal utterance w.r.t. a DialoGPT LM.
The results of the above experiments are reported in Table 2 and 11 (Appendix B.1), with sample
utterances reported in Appendix B.10. Our experiments show that MoE-LMs outperform most
baseline methods on DM performance. We attribute this Ô¨Ånding to two factors: (i) MoE-MDP
restricts the action space into a smaller set of candidate utterances generated by experts (whose
qualities are validated in EXP2), the corresponding RL problem then becomes simpler and requires
less data (especially in Cornell) to solve. (ii) Unlike the baseline RL methods, which need to optimize
both bot-and-user signals, the MoE DM agents focus on optimizing the user satisfaction goal and are
therefore more effective. Interestingly, our MoE-LMs also have lower (better) GPT-perplexity scores
than other methods. This may be due to the fact that MoE-LM uses pre-trained encoder and decoder
from the primitive LM, which are optimized for generalization and accuracy, while other RL methods
may distort their language representations to create utterances that maximize reward but become less
natural. Among different MoE-RL strategies, MBRL, which Ô¨Årst learns a user utterance model (the
model uses the same encoder from the primitive LM and learns a separate decoder for user-utterance
prediction) and does Q-learning, performs the best. CQL moderately improves upon the primitive
LM (behavior policy), while DQN appears to be less effective in solving the ofÔ¨Çine RL problem.
8 Concluding Remarks
We developed a mixture-of-expert (MoE) approach for RL-based dialogue management (DM). Our
MoE language model (MoE-LM) comprises of three main components: (i) a LM that can generate
diverse semantics for conversation histories, (ii) a number of specialized LMs (or experts) that
can produce utterances corresponding to a particular attribute or intent, and (iii) a RL-based DM
that performs dialogue planning with the utterances generated by the experts. To understand how
well our MoE approach generates diverse and sensible utterances, and solves DM problems, we
evaluated it using two open-domain dialogue tasks and compared it with SOTA baselines. Our
results showed that MoE-LM (i) improves diversity of text generation, (ii) can generate utterances
with speciÔ¨Åc intents, and (iii) yields better overall performance. We consider our work as a step
9

--- PAGE 10 ---
forward in creating steerable LMs that possess different intents and in training RL-based DMs that
can carry on rich conversations. Future work includes improving the language representation with
information-theoretic approaches, Ô¨Åne-tuning the experts based on the DM objective, extending the
RL agent to track users‚Äô behaviors (via abstract belief states) and plan upon them, and evaluating our
MoE-LM on more realistic problems, such as information retrieval, recommendation, and negotiation.
10

--- PAGE 11 ---
Checklist
1. For all authors...
(a)Do the main claims made in the abstract and introduction accurately reÔ¨Çect the paper‚Äôs
contributions and scope? [Yes]
(b) Did you describe the limitations of your work? [Yes] See the summary (Section 8).
(c)Did you discuss any potential negative societal impacts of your work? [N/A] In this
work we proposed an abstract approach of designing dialogue managers, it is unrelated
to any negative impact applications.
(d)Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results...
(a)Did you state the full set of assumptions of all theoretical results? [Yes] For each result,
Section 4 and Section 5 either state the assumptions directly or refers to corresponding
references.
(b)Did you include complete proofs of all theoretical results? [N/A] All theoretical are
cited with corresponding references.
3. If you ran experiments...
(a)Did you include the code, data, and instructions needed to reproduce the main experi-
mental results (either in the supplemental material or as a URL)? [No] Unfortunately,
due to IP concerns our legal team does not allow code releases at this point.
(b)Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] General details in the beginning of Section 7, and speciÔ¨Åc details
per experiment in Appendix A.
(c)Did you report error bars (e.g., with respect to the random seed after running exper-
iments multiple times)? [Yes] In Talbes 10,1,12, 3, 11, 2, , we provide not only the
mean result but also the standard deviation.
(d)Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] In the beginning of Appendix A.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [N/A]
(b) Did you mention the license of the assets? [N/A]
(c)Did you include any new assets either in the supplemental material or as a URL? [N/A]
(d)Did you discuss whether and how consent was obtained from people whose data you‚Äôre
using/curating? [N/A]
(e)Did you discuss whether the data you are using/curating contains personally identiÔ¨Åable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects...
(a)Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b)Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c)Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]
11

--- PAGE 12 ---
References
A. Ajay, A. Kumar, P. Agrawal, S. Levine, and O. Nachum. Opal: OfÔ¨Çine primitive discovery for
accelerating ofÔ¨Çine reinforcement learning. arXiv preprint arXiv:2010.13611 , 2020.
K. Asadi and J. Williams. Sample-efÔ¨Åcient deep reinforcement learning for dialog control. arXiv
preprint arXiv:1612.06000 , 2016.
L. Bahl, F. Jelinek, and R. Mercer. A maximum likelihood approach to continuous speech recognition.
IEEE transactions on pattern analysis and machine intelligence , (2):179‚Äì190, 1983.
C. Burgess, I. Higgins, A. Pal, L. Matthey, N. Watters, G. Desjardins, and A. Lerchner. Understanding
disentangling in -vae. arXiv preprint arXiv:1804.03599 , 2018.
D. Cer, Y . Yang, S. Kong, N. Hua, N. Limtiaco, R. John, N. Constant, M. Guajardo-Cespedes, S. Yuan,
C. Tar, et al. Universal sentence encoder. arXiv preprint arXiv:1803.11175 , 2018.
Y . Chen, V . Li, K. Cho, and S. Bowman. A stable and effective learning strategy for trainable greedy
decoding. arXiv preprint arXiv:1804.07915 , 2018.
J. Chien and C. Kuo. Markov recurrent neural network language model. In 2019 IEEE Automatic
Speech Recognition and Understanding Workshop (ASRU) , pages 807‚Äì813. IEEE, 2019.
T. Kulkarni D, K. Narasimhan, A. Saeedi, and J. Tenenbaum. Hierarchical deep reinforcement
learning: Integrating temporal abstraction and intrinsic motivation. Advances in neural information
processing systems , 29, 2016.
C. Danescu-Niculescu-Mizil and L. Lee. Chameleons in imagined conversations: A new approach to
understanding coordination of linguistic style in dialogs. arXiv preprint arXiv:1106.3077 , 2011.
S. Dathathri, A. Madotto, J. Lan, J. Hung, E. Frank, P. Molino, J. Yosinski, and R. Liu. Plug
and play language models: A simple approach to controlled text generation. arXiv preprint
arXiv:1912.02164 , 2019.
A. Van den Oord, Y . Li, and O. Vinyals. Representation learning with contrastive predictive coding.
arXiv e-prints , pages arXiv‚Äì1807, 2018.
J. Devlin, M. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers
for language understanding. arXiv preprint arXiv:1810.04805 , 2018.
M. Fatemi, L. Asri, H. Schulz, J. He, and K. Suleman. Policy networks with two-stage training for
dialogue systems. arXiv preprint arXiv:1606.03152 , 2016.
J. Ficler and Y . Goldberg. Controlling linguistic style aspects in neural language generation. arXiv
preprint arXiv:1707.02633 , 2017.
L. Floridi and M. Chiriatti. GPT-3: Its nature, scope, limits, and consequences. Minds and Machines ,
30(4):681‚Äì694, 2020.
M. Ga≈°i ¬¥c, F. Jur Àác√≠Àácek, B. Thomson, K. Yu, and S. Young. On-line policy optimisation of spoken
dialogue systems via live interaction with human subjects. In 2011 IEEE Workshop on Automatic
Speech Recognition & Understanding , pages 312‚Äì317. IEEE, 2011.
A. Ghandeharioun, J. Shen, N. Jaques, C. Ferguson, N. Jones, A. Lapedriza, and R. Picard. Approxi-
mating interactive human evaluation with self-play for open-domain dialog systems. Advances in
Neural Information Processing Systems , 32, 2019.
E. Greensmith, P. Bartlett, and J. Baxter. Variance reduction techniques for gradient estimates in
reinforcement learning. Journal of Machine Learning Research , 5(9), 2004.
J. Gu, K. Cho, and V . Li. Trainable greedy decoding for neural machine translation. arXiv preprint
arXiv:1702.02429 , 2017.
T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic: Off-policy maximum entropy deep
reinforcement learning with a stochastic actor. ICML , 2018.
12

--- PAGE 13 ---
D. Hafner, T. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models. arXiv
preprint arXiv:2010.02193 , 2020.
B. Hancock, A. Bordes, P. Mazare, and J. Weston. Learning from dialogue after deployment: Feed
yourself, chatbot! arXiv preprint arXiv:1901.05415 , 2019.
A. Holtzman, J. Buys, M. Forbes, A. Bosselut, D. Golub, and Y . Choi. Learning to write with
cooperative discriminators. arXiv preprint arXiv:1805.06087 , 2018.
N. Hurley and S. Rickard. Comparing measures of sparsity. IEEE Transactions on Information
Theory , 55(10):4723‚Äì4741, 2009.
N. Jaques, A. Ghandeharioun, J. Shen, C. Ferguson, A. Lapedriza, N. Jones, S. Gu, and R. Pi-
card. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog.
arXiv:1907.00456 , 2019.
E. Kharitonov, M. Baroni, and D. Hupkes. How bpe affects memorization in transformers. arXiv
preprint arXiv:2110.02782 , 2021.
A. Kumar, A. Zhou, G. Tucker, and S. Levine. Conservative q-learning for ofÔ¨Çine reinforcement
learning. Advances in Neural Information Processing Systems , 33:1179‚Äì1191, 2020.
E. Levin and R. Pieraccini. A stochastic model of computer-human interaction for learning dialogue
strategies. In Eurospeech , volume 97, pages 1883‚Äì1886. Citeseer, 1997.
J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan. A diversity-promoting objective function for
neural conversation models. arXiv preprint arXiv:1510.03055 , 2015.
J. Li, W. Monroe, A. Ritter, M. Galley, J. Gao, and D. Jurafsky. Deep reinforcement learning for
dialogue generation. arXiv preprint arXiv:1606.01541 , 2016.
J. Li, W. Monroe, T. Shi, S. Jean, A. Ritter, and D. Jurafsky. Adversarial learning for neural dialogue
generation. arXiv preprint arXiv:1701.06547 , 2017.
Z. Li, J. Kiseleva, and M. De Rijke. Dialogue generation: From imitation learning to inverse rein-
forcement learning. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , volume 33,
pages 6722‚Äì6729, 2019.
W. Liao, B. Zeng, X. Yin, and P. Wei. An improved aspect-category sentiment analysis model for
text sentiment analysis based on roberta. Applied Intelligence , 51(6):3522‚Äì3533, 2021.
Y . Park, J. Cho, and G. Kim. A hierarchical latent structure for variational conversation modeling.
arXiv preprint arXiv:1804.03424 , 2018.
B. Peng, C. Zhu, C. Li, X. Li, J. Li, M. Zeng, and J. Gao. Few-shot natural language generation for
task-oriented dialog. arXiv preprint arXiv:2002.12328 , 2020.
A. Razavi, A. van den Oord, and O. Vinyals. Generating diverse high-resolution images with vq-vae.
2019.
A. Saleh, N. Jaques, A. Ghandeharioun, J. Shen, and R. Picard. Hierarchical reinforcement learning for
open-domain dialog. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , volume 34,
pages 8741‚Äì8748, 2020.
C. Sankar, S. Subramanian, C. Pal, S. Chandar, and Y . Bengio. Do neural dialog systems use the
conversation history effectively? an empirical study. arXiv preprint arXiv:1906.01603 , 2019.
J. Schulman, X. Chen, and P. Abbeel. Equivalence between policy gradients and soft q-learning.
arXiv preprint arXiv:1704.06440 , 2017.
I. Serban, A. Sordoni, Y . Bengio, A. Courville, and J. Pineau. Building end-to-end dialogue systems
using generative hierarchical neural network models. In Proceedings of the AAAI Conference on
ArtiÔ¨Åcial Intelligence , volume 30, 2016.
13

--- PAGE 14 ---
I. Serban, C. Sankar, M. Germain, S. Zhang, Z. Lin, S. Subramanian, T. Kim, M. Pieper, S. Chandar,
N. Ke, et al. A deep reinforcement learning chatbot. arXiv preprint arXiv:1709.02349 , 2017a.
I. Serban, A. Sordoni, R. Lowe, L. Charlin, J. Pineau, A. Courville, and Y . Bengio. A hierarchical
latent variable encoder-decoder model for generating dialogues. In Proceedings of the AAAI
Conference on ArtiÔ¨Åcial Intelligence , volume 31, 2017b.
P. Shah, D. Hakkani-Tur, B. Liu, and G. T√ºr. Bootstrapping a neural conversational agent with
dialogue self-play, crowdsourcing and on-line reinforcement learning. In Proceedings of the 2018
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 3 (Industry Papers) , pages 41‚Äì51, 2018.
W. Shi and Z. Yu. Sentiment adaptive end-to-end dialog systems. arXiv preprint arXiv:1804.10731 ,
2018.
J. Shin, P. Xu, A. Madotto, and P. Fung. Generating empathetic responses by looking ahead the user‚Äôs
sentiment. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) , pages 7989‚Äì7993. IEEE, 2020.
R. Shu, T. Nguyen, Y . Chow, T. Pham, K. Than, M. Ghavamzadeh, S. Ermon, and H. Bui. Predictive
coding for locally-linear control. In International Conference on Machine Learning , pages 8862‚Äì
8871. PMLR, 2020.
S. Singh, D. Litman, M. Kearns, and M. Walker. Optimizing dialogue management with reinforcement
learning: Experiments with the njfun system. Journal of ArtiÔ¨Åcial Intelligence Research , 16:105‚Äì
133, 2002.
N. Subramani, S. Bowman, and K. Cho. Can unconditional language models recover arbitrary
sentences? Advances in Neural Information Processing Systems , 32, 2019.
I. Sutskever, O. Vinyals, and Q. Le. Sequence to sequence learning with neural networks. Advances
in neural information processing systems , 27, 2014.
R. Sutton, D. McAllester, S. Singh, and Y . Mansour. Policy gradient methods for reinforcement
learning with function approximation. Advances in neural information processing systems , 12,
1999.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, L. Kaiser, and I. Polosukhin.
Attention is all you need. Advances in neural information processing systems , 30, 2017.
S. Verma, J. Fu, M. Yang, and S. Levine. Chai: A chatbot ai for task-oriented dialogue with ofÔ¨Çine
reinforcement learning. arXiv preprint arXiv:2204.08426 , 2022.
M. Walker. An application of reinforcement learning to dialogue strategy selection in a spoken
dialogue system for email. Journal of ArtiÔ¨Åcial Intelligence Research , 12:387‚Äì416, 2000.
W. Wei, Q. Le, A. Dai, and J. Li. Airdialogue: An environment for goal-oriented dialogue research.
InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing ,
pages 3844‚Äì3854, 2018.
J. Williams and S. Young. Partially observable markov decision processes for spoken dialog systems.
Computer Speech & Language , 21(2):393‚Äì422, 2007.
T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,
M. Funtowicz, et al. Huggingface‚Äôs transformers: State-of-the-art natural language processing.
arXiv preprint arXiv:1910.03771 , 2019.
M. Yang and O. Nachum. Representation matters: OfÔ¨Çine pretraining for sequential decision making.
InInternational Conference on Machine Learning , pages 11784‚Äì11794. PMLR, 2021.
S. Young, M. Ga≈°i ¬¥c, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, and K. Yu. The hidden
information state model: A practical framework for pomdp-based spoken dialogue management.
Computer Speech & Language , 24(2):150‚Äì174, 2010.
14

--- PAGE 15 ---
Y . Zhang, S. Sun, M. Galley, Y . Chen, C. Brockett, X. Gao, J. Gao, J. Liu, and B. Dolan. Di-
alogpt: Large-scale generative pre-training for conversational response generation. arXiv preprint
arXiv:1911.00536 , 2019.
T. Zhao, K. Xie, and M. Eskenazi. Rethinking action spaces for reinforcement learning in end-to-end
dialog agents with latent variable models. arXiv preprint arXiv:1902.08858 , 2019.
L. Zhou, J. Gao, D. Li, and H. Shum. The design and implementation of xiaoice, an empathetic social
chatbot. Computational Linguistics , 46(1):53‚Äì93, 2020.
D. Ziegler, N. Stiennon, J. Wu, T. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving.
Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019.
15

--- PAGE 16 ---
This section describes more details about our experimental setup to evaluate the algorithms.
A Experimental Details
A.1 KL-Contrastive Constraint
Recall that the main purpose of the KL-constraint is to enforce consistency in the latent variables
predicted by the semantic generator and the posterior. However, since both G0(z0jz)and(z0jz;Y)
are both models to be learned, if the data can be modeled with a single, stationary LM that does not
depend on the latent space, one trivial degenerated solution (that satisÔ¨Åes the KL constraint) is to
have both of these models output very small values, which would impede the generalizability (and
diversity) of the LM embedding, especially to unseen conversations. Utilizing analogous arguments
as in Shu et al. [2020], den Oord et al. [2018] that connects KL distribution matching and mutual
information maximization in representation learning, we tackle this issue by considering the KL-
contrastive constraint, i.e., let F(z;Y) := KLz0 
(z0jz;Y)jjG0(z0jz), we replace the constraint in
the optimization problem in (1) with
bEz=(X);YDh
F(z;Y) +logexp( F(z;Y))R
Y0exp( F(z;Y))i
KL;
where > 0is a trade-off factor in the KL-constrastive constraint. While the Ô¨Årst part of the
constraint limits F(z;Y), the second part enforces F(z;Y)to a much higher value in the positive
samples (w.r.t. ground-truth next utterance) than in the negative samples (w.r.t. other next-utterance
candidates). Therefore, this constraint can prevent the aforementioned degeneration issue and inject
Ô¨Çexibility to control the size of the latent representation space.
A.2 Discretization of andGi
Inspired by various works which use discrete bottlenecks [Razavi et al., 2019, Hafner et al., 2020,
Yang and Nachum, 2021], we employ discretization of andG0to encourage better generalization
to out-of-distribution inputs. SpeciÔ¨Åcally, we parameterize (X) = disc 16(e(X)), where the output
ofeis a continuous-valued vector in Rdand the discretization operator discKworks as follows:
‚Ä¢ Given vector v2Rd, splitvintov1;:::;vd=Kvectors, each of size K.
‚Ä¢For eachvi, sample a one-hot vector v0
iof sizeKbased on a softmax categorical distribution
with logits given by vi.
‚Ä¢ Concatenate the vectors v0
ito yield a multi-hot vector v0of sized.
‚Ä¢When computing the backward pass on v0, use straight-through gradients; i.e.,@v0
i
@vi=
@softmax(vi)
@vi.
We similarly discretize z0, the output ofG0. Namely, we Ô¨Årst parameterize Gias a Gaussian and then
discretize that to form a multinomial distribution that produces multi-hot ( d=K -hot,K= 16 ) vectors.
We employ the same discretization for the experts Giand the prior (z0jz;Y).
To avoid convergence to local minima for , we employ an entropy regularizer on the discrete distri-
bution it is sampled from. Namely, we use an adaptive entropy regularizer in the style of [Haarnoja
et al., 2018] with a Ô¨Åxed target entropy that is a hyper-parameter.
A.3 Model parameters
In this section, the model parameters are described for MoE-1, 2, 3 and 4. All of these models
represent Mixture of Experts and are based on transformer [Vaswani et al., 2017]. Transformer
is an encoder-decoder based model that uses self-attention to capture relationships between the
elements of the sequence. In our implementation, we used multi-head attention with implementa-
tion similar to https://www.tensorflow.org/text/tutorials/transformer#point_wise_
feed_forward_network .
MoE-1 and MoE-2 use the simple transformer architecture, while MoE-3 and MoE-4 use the same en-
coder architecture as BERT [Devlin et al., 2018]. MoE-1 uses much smaller latent distribution models
16

--- PAGE 17 ---
fGigthan MoE-2; MoE-3 uses the pre-trained BERT encoder https://tfhub.dev/tensorflow/
bert_en_uncased_L-12_H-768_A-12/4 , while MoE-4 trains that from scratch.
The transformer model parameters for the simple transformer architecture are summarized in Table
4:
Parameter Value
Number of layers 2
Embedding hidden size 256
FFN inner hidden size 512
Attention heads 8
Key size 256
Value size 256
Dropout 0.1
Table 4: Simple Transformer Architecture
The BERT based transformer model is similar to the architecture from Table 4. The differences for a
pre-trained BERT model are captured in Table 5.
Parameter Value
Embedding hidden size 768
Number of layers 12
Attention heads 12
Table 5: BERT-based Transformer Architecture
The differences for a BERT model we train from scratch are captured in Table 6.
Parameter Value
Embedding hidden size 768
Number of layers 2
Attention heads 8
Table 6: Trainable, Smaller ‚ÄúBERT‚Äù-based Transformer Architecture
Latent distributions fGigare implemented as FFN that model mean and variance of the normal
distribution. The MoE-1, 2, 3, 4 use different values for the hidden size of the neural network.
Additionally, MoE-1 and 3 use target entropy of 0:1, MoE-2 and 4 use target entropy of 1:0. The
common parameters for FFN are captured in Table 7 (note: FFN has a Ô¨Ånal layer without an
activation).
Finally, Table 8 shows individual parameters distinct for each of MoE models.
A.4 Computational resources
Training and evaluation were run on 8 GPU instances with 20GB of RAM and a NVIDIA Tesla P100
graphics card.
A.5 Dataset
We trained our models on Reddit Casual and Cornell Movie conversational datasets. Both datasets
were downloaded from Neural Chat datasets of the MIT Media Lab https://affect.media.mit.
edu/neural_chat/datasets . They contain conversational exchanges between pairs of speakers.
Each batch of training data contains a subset of such conversations. The Reddit Casual is about 3
times bigger than Cornell corpus.
17

--- PAGE 18 ---
fGigFFN parameter Value
Number of layers 1
Activation tanh
Table 7:fGigFFN architecture
MoE model parameter fGigFFN hidden size Embedding hidden size Uses BERT encoder Pre-trained BERT
MoE-1 128 128 No N/A
MoE-2 32 128 No N/A
MoE-3 256 768 Yes Yes
MoE-4 256 768 Yes No
Table 8: MoE parameters
A.6 Expert Label Functions
During EXP2, we deÔ¨Åne several types of language experts whose utterance outputs can constitute to
smooth bot responses when used in dialogue interactions with users.
The Ô¨Årst type of experts we aim to create is sentiment-based , because a Ô¨Çuent expression of emotions
is important for creating a sense of understanding in human conversations. To quantify the emotional
tone of a bot utterance, we use a state-of-the-art (e.g., RoBERTa Sentiment [Liao et al., 2021])
sentiment detector, which outputs 2sets of prediction probabilities ‚Äì (i) whether a bot utterance
is of positive, neutral, or negative sentiment; (ii) whether the bot utterance falls into any of the 4
more-reÔ¨Åned emotions: {joy, optimisim, sadness, anger}.
Neglecting the neutral sentiment output, we deÔ¨Åne 6sentiment labeling functions: `pos-sent (Y),
`neg-sent (Y),`joy(Y),`optimism (Y),`anger(Y),`sadness (Y), which outputs a score that depends on senti-
ment prediction probability of any candidate bot utterance. We can also create a sentiment-empathetic
expert whose utterances will be coherent with the user‚Äôs sentiment.
RoBERTa gives scores for each of the 6 categories listed above, which then are summed weighted by
the coefÔ¨Åcients in Table 9 to produce the sentiment-based score:
Sentiment CoefÔ¨Åcient
pos-sent 0.5
neg-sent -0.5
joy 0.5
optimism 1
sadness -1
anger -0.5
Table 9: Sentiment Based Label CoefÔ¨Åcients
To quantify sentiment coherence between the user and bot, the labeling function `sent-coh (X;Y)calcu-
lates the cosine similarity of user-bot sentiment embeddings (which corresponds to the logit vector
of the Ô¨Årst set of RoBERTa predictions) in the conversation history. Concretely, it‚Äôs implemented
using tf.keras.losses.cosine_similarity ( https://www.tensorflow.org/api_docs/python/tf/
keras/losses/cosine_similarity ).
The second type of experts of interests is engagement-based , whose goal is to encourage user‚Äôs
participation in the conversation. One primary candidate is the question expert, because having a bot
that is appropriately inquisitive demonstrates the system‚Äôs attentiveness to users, and thereby also
increases user‚Äôs responsiveness. To characterize whether a bot utterance is linked to a question, we
deÔ¨Åne the label function `question (Y)which outputs 1when both a question word and a question mark
are present and 0otherwise. Another engagement skill of interests is exploration , i.e., the expert is
able to change the tone or topic of conversations to avoid having a stale conversation. To measure the
amount of exploration, in the label function `exp(X;Y)we utilize a sentence encoder (e.g., USE [Cer
et al., 2018]) to encode both the conversation history and the bot utterance and output the negative
cosine similarity between these two embeddings.
18

--- PAGE 19 ---
A.7 RL Rewards
For EXP3, the reward for RL-based dialogue management is deÔ¨Åned as r(X;a;X +) =1`sent(X+)+
2(`sent(X+) 1 
1 LPL 1
l=0l`sent(Xl)), where the linear combination weights (1;2) =
(0:75;0:25)correlate with Ghandeharioun et al. [2019], and `sent(X)is the same RoBerTa-based
sentiment labeler as in EXP2, which assigns a score from [ 1;1]that is proportional to the positive
sentiment and inversely proportional to the negative sentiment prediction probabilities. Intuitively, 1
assigns weight to sentiment-based score of the next user response X+.2assigns weight to transition
of user sentiment, which we deÔ¨Åne as the difference between sentiment-based score of next user
response and discounted sentiment-based score of the current conversation.
B Additional Results
B.1 Dialogue Management Results on Cornell Movie Corpus
Method Diversity Dist-1 Dist-2 Dist-3 Perplexity
MoE-1 0.0730.02 0.24 0.59 0.67 76.1733.04
MoE-2 0.120.01 0.30 0.65 0.77 45.2440.77
MoE-3 0.110.01 0.28 0.66 0.72 77.9426.55
MoE-4 0.160.01 0.35 0.77 0.84 60.0623.74
Trans. 0.0600.03 0.25 0.55 0.70 69.8616.88
VHRED 0.0950.03 0.27 0.53 0.68 101.8854.78
Table 10: Accuracy (Perplexity) and Diversity of
Language Primitive Experts Trained with Cornell.Method User Tot. Sent. User Sent. Trans. GPT-Perplexity
MoE-2 Primitive -0.140.11 0.100.05 93.1150.41
MoE-2 DQN -0.170.13 -0.080.05 90.0130.76
MoE-2 CQL 0.170.15 0.180.15 88.8357.78
MoE-2 MBRL 0.130.11 0.090.07 85.5940.01
Trans. RL -0.350.19 -0.150.18 199.7564.23
Trans. KLC -0.090.06 -0.130.08 145.0955.71
Trans. VHRL -0.070.13 0.050.07 160.0260.30
Table 11: Quality of DM Trained with Cornell Dataset.
Method Question Exploration Positive Sent. Negative Sent. Sent. Coherence Joy Optimism Anger Sadness
MoE-1 1.190.23 0.330.25 1.240.43 1.300.64 0.790.44 2.040.39 0.021.01 1.920.99 3.771.05
MoE-2 1.570.27 0.360.21 3.622.01 1.801.89 0.680.25 2.670.97 2.261.35 2.801.14 4.321.76
MoE-3 1.140.31 0.290.37 0.350.31 1.890.58 0.750.21 1.880.77 0.290.81 0.650.99 2.991.01
MoE-4 1.840.33 0.260.51 0.171.86 2.201.64 0.800.98 0.521.91 0.580.61 2.720.75 3.771.43
WD 0.861.7 0.210.289 -0.480.44 0.410.46 0.840.19 0.131.48 -1.110.88 1.512.02 1.220.80
PPLM 0.310.45 0.350.36 -0.492.32 0.422.13 0.740.24 -1.002.54 -0.862.41 0.690.05 0.742.27
Trans. RL* 2.200.32 0.440.26 3.581.05 2.981.53 0.820.30 3.061.85 1.641.21 3.222.01 3.981.88
Table 12: Quality of Each Expert PPLM Trained on Cornell Dataset w.r.t. Its Trained Label.
19

--- PAGE 20 ---
B.2 Sample Utterances for EXP 1
B.2.1 Reddit Casual
1.Input Conversation: (...) i mean , i have n‚Äôt changed or done anything drastically different
, but things have been good .
‚Ä¢MoE-1 R1: but my outlook on life is n‚Äôt meh .
‚Ä¢MoE-1 R2: but my outlook on life is subjective .
‚Ä¢MoE-1 R3: but my outlook on life is still meh .
‚Ä¢MoE-1 R4: but my view on life is still meh ?
‚Ä¢MoE-1 R5: but my outlook on life is n‚Äôt meh ?
‚Ä¢MoE-2 R1: i love the good things that i ‚Äôm starting too.
‚Ä¢MoE-2 R2: i do love your life , because mine ‚Äôs nothing .
‚Ä¢MoE-2 R3: no , i ‚Äôm not too busy in life
‚Ä¢MoE-2 R4: good for you , i ‚Äôm working on it .
‚Ä¢MoE-2 R5: but my life is boring and boring .
‚Ä¢MoE-3 R1: but my outlook on life is still sad .
‚Ä¢MoE-3 R2: but my outlook on life is the same .
‚Ä¢MoE-3 R3: but my outlook on life is my same !
‚Ä¢MoE-3 R4: but my outlook on life is not good .
‚Ä¢MoE-3 R5: but my outlook on life is my friend .
‚Ä¢MoE-4 R1: i ‚Äôm in my early 20 ‚Äôs , i do n‚Äôt know what to do
‚Ä¢MoE-4 R2: i ‚Äôm so happy for you .
‚Ä¢MoE-4 R3: for me , it ‚Äôs tough to go through life with pain .
‚Ä¢MoE-4 R4: i used to think that you ‚Äôd enjoy that much more than most people .
‚Ä¢MoE-4 R5: can you describe what happened ?
2.Input Conversation: (...) but my outlook on life is still meh
‚Ä¢MoE-1 R1: no need for drastically change, just take it slow for a break ?
‚Ä¢MoE-1 R2: no need to slow down, just take it a long day ?
‚Ä¢MoE-1 R3: no need to slow down , just take a step too long .
‚Ä¢MoE-1 R4: no need a blanket , just take it off .
‚Ä¢MoE-1 R5: no need for a breath , just take it a bit of time
‚Ä¢MoE-2 R1: being a good person is n‚Äôt a bad thing
‚Ä¢MoE-2 R2: i ‚Äôm in a similar situation for it .
‚Ä¢MoE-2 R3: take a break , try to Ô¨Ånd something else .
‚Ä¢MoE-2 R4: oh yeah , life ‚Äôs pretty good for me .
‚Ä¢MoE-2 R5: like you ‚Äôre in a bad place for a few years ?
‚Ä¢MoE-3 R1: no need for drastic changes , just make it super busy
‚Ä¢MoE-3 R2: no need for drastic changes , just do it extra time
‚Ä¢MoE-3 R3: no need for unnecessary stress , just move the next direction
‚Ä¢MoE-3 R4: no change for unnecessary things , just move on .
‚Ä¢MoE-3 R5: no need for unnecessary consumption , just take with the move
‚Ä¢MoE-4 R1: it seems like a great time to just go outside and play a few songs , i ‚Äôm
always glad to hear that you have a good friend
‚Ä¢MoE-4 R2: i know life ‚Äôs not perfect , it can be difÔ¨Åcult at times !
‚Ä¢MoE-4 R3: end complaining all the time and just consider your life a failure .
20

--- PAGE 21 ---
‚Ä¢MoE-4 R4: you ‚Äôd be surprised at how nice life can go !
‚Ä¢MoE-4 R5: i mean some sort of hobby can deÔ¨Ånitely help out .
3.Input Conversation: (...) no need for drastically difference , just take it slow .
‚Ä¢MoE-1 R1: and it ‚Äôs nice to hear someone else ‚Äôs good , but it ‚Äôs all about my life .
‚Ä¢MoE-1 R2: and it ‚Äôs nice to hear , when you have new seasons of american world is
just a nature in canada .
‚Ä¢MoE-1 R3: and it ‚Äôs nice to hear why you ‚Äôve been doing it , but my friend is still in it
...
‚Ä¢MoE-1 R4: and it ‚Äôs very good to hear why you ‚Äôve been good with my wife , a lot of
things that ‚Äôs happening in the same way .
‚Ä¢MoE-1 R5: and it ‚Äôs nice to hear things have been good , my view of life changes with
time and mood
‚Ä¢MoE-2 R1: i Ô¨Ånd it hard to get out of this world , but there is also a good way to Ô¨Ånd a
positive way of life .,
‚Ä¢MoE-2 R2: you seem to be in the same situation , but it is a good way to do it for a lot
of things in life
‚Ä¢MoE-2 R3: my life is n‚Äôt bad , i ‚Äôm in a very long time and i m pretty happy about it .
‚Ä¢MoE-2 R4: i just wanted to see what ‚Äôs happening in my life and i m not sure what to
do .
‚Ä¢MoE-2 R5: at this point i ‚Äôm pretty sure the most part of my life is pretty normal , but
it s a lot of positive .
‚Ä¢MoE-3 R1: and it ‚Äôs nice to hear that things have been good for you that your view life
is a lot like mine .
‚Ä¢MoE-3 R2: and it ‚Äôs nice to hear that things have been good for you so my perspective
of life is too much and so i think that ‚Äôs a pass
‚Ä¢MoE-3 R3: and it ‚Äôs nice to hear that things have been good for you in my view of life
, like a lot of people are trying to have
‚Ä¢MoE-3 R4: and it ‚Äôs nice to hear that things have been good for you , my view my life
is awful that , but i feel like this is a lot better
‚Ä¢MoE-3 R5: and it ‚Äôs nice to hear that things have been good for you in the future of
your life , so maybe you ‚Äôre looking forward to it
‚Ä¢MoE-4 R1: i ‚Äôve been trying to learn things about myself .
‚Ä¢MoE-4 R2: that ‚Äôs like i would be at this stage of life .
‚Ä¢MoE-4 R3: you will need to enjoy life for yourself .
‚Ä¢MoE-4 R4: for me it ‚Äôs like i do n‚Äôt have the time .
‚Ä¢MoE-4 R5: but like you said , you ‚Äôve been trying to get your life together and learn to
be a normal person to love yourself .
4.Input Conversation: (...) and it ‚Äôs nice to hear that things have been good for you d my
view of life is meh too , it changes depending on time and mood
‚Ä¢MoE-1 R1: and i do n‚Äôt know why i say this , this is not a relevant post , but i can
really say the same things .
‚Ä¢MoE-1 R2: and i do n‚Äôt know why i say this , this is not even relevant , but any things
will be good .
‚Ä¢MoE-1 R3: and i do n‚Äôt know why i say this , this is not even a relevant topic , i do n‚Äôt
remember the same things i said
‚Ä¢MoE-1 R4: and i do n‚Äôt know why i say this , this is n‚Äôt even relevant , but deÔ¨Ånitely i
enjoy the same things
‚Ä¢MoE-1 R5: and i do n‚Äôt know why i say this , this is not a relevant topic , but i think
there ‚Äôs actually a bunch of things to do
21

--- PAGE 22 ---
‚Ä¢MoE-2 R1: i think i ‚Äôm just trying to be a good person , but i m not sure what it is like
in my life .
‚Ä¢MoE-2 R2: for me it ‚Äôs my life i feel like it s the same way of being around the world
and i m pretty sure it s not a lot
‚Ä¢MoE-2 R3: to be honest , i ‚Äôm not sure what the most beautiful things are like in the
world
‚Ä¢MoE-2 R4: i feel like you ‚Äôre going to be a good person , but i ‚Äôm not a very happy
about the world of things you are around
‚Ä¢MoE-2 R5: i just want to be in a way to see my life and i m not really sure how much
of other things i like
‚Ä¢MoE-3 R1: and i do n‚Äôt know why i say this , this is not really relevant , but i do wish
you the best .
‚Ä¢MoE-3 R2: and i do n‚Äôt know why i say this , this is not really relevant , but i can count
the experience
‚Ä¢MoE-3 R3: and i do n‚Äôt know why i say this , this is not even relevant , but now i feel
like it ‚Äôs the kind of boring ...
‚Ä¢MoE-3 R4: and i do n‚Äôt know why i say this , this is not even relevant , but i like
watching ultra match in my room .
‚Ä¢MoE-3 R5: and i do n‚Äôt know why i say this , this is not even relevant , but i do feel
like you ‚Äôre welcome here .
‚Ä¢MoE-4 R1: i hope things get better for you soon
‚Ä¢MoE-4 R2: i think it ‚Äôs Ô¨Åne , i enjoy life and could be a little happier every day
‚Ä¢MoE-4 R3: i like it d be a good time to think about the things that you don t need to
do .
‚Ä¢MoE-4 R4: but let ‚Äôs forget on those things that make you miserable and trying to
focus on the basics .
‚Ä¢MoE-4 R5: i know it ‚Äôs not perfect , but enjoy the things you have .
5.Input Conversation: (...) and i do n‚Äôt know why i say this , this is not even relevant , but
remember rule 32 enjoy the little things
‚Ä¢MoE-1 R1: i ‚Äôm trying to enjoy the little things like walking down the street , or
playing random games .
‚Ä¢MoE-1 R2: i ‚Äôm just trying to enjoy the little things like sports , like small talk , or
random ones , or even give up
‚Ä¢MoE-1 R3: i ‚Äôm just trying to enjoy the little things like walking down the street with
the same interactions or drinking
‚Ä¢MoE-1 R4: i ‚Äôm trying to enjoy the little things like going down the beach with the
same thing , or a nice spot
‚Ä¢MoE-1 R5: i ‚Äôm trying to enjoy the little things like walking down the beach ,
watching the weird movies , listen to random music
‚Ä¢MoE-2 R1: this is a good time , i ‚Äôm just looking for a place to go and i ‚Äôve always
wanted to go to the store
‚Ä¢MoE-2 R2: my favorite thing is that i ‚Äôm just looking for a good thing to do , but i ‚Äôm
always looking forward to the people i ‚Äôve been
‚Ä¢MoE-2 R3: i might be in the same boat , but i ‚Äôm a little scared to know what is a
really good thing to look at the moment
‚Ä¢MoE-2 R4: i ‚Äôve been trying to Ô¨Ånd out what i want to do , but i ‚Äôm not really sure
‚Ä¢MoE-2 R5: i know it ‚Äôs a good thing , but i ‚Äôm always in the same boat as i ‚Äôve been in
a long run
‚Ä¢MoE-3 R1: yeah haha , i ‚Äôm deÔ¨Ånitely trying to enjoy the little things like walking
down the street , or just have a hot cup of tea
22

--- PAGE 23 ---
‚Ä¢MoE-3 R2: yeah haha , i ‚Äôm deÔ¨Ånitely trying to enjoy the little things like walking
down the street or seeing what is in my neighborhood
‚Ä¢MoE-3 R3: yes , i m deÔ¨Ånitely trying to enjoy the little things like walking down stairs
, but i can t Ô¨Ågure out what to see
‚Ä¢MoE-3 R4: yeah dude , i m deÔ¨Ånitely trying to enjoy the little things like walking
down the street with a huge fan of these kids
‚Ä¢MoE-3 R5: yeah man , i ‚Äôm deÔ¨Ånitely trying to enjoy the little things like from
walking up the street or to my little kid ‚Äôs house
‚Ä¢MoE-4 R1: that ‚Äôs what i ‚Äôm really trying to do and enjoy the moment .
‚Ä¢MoE-4 R2: i ‚Äôve always been like that , i ‚Äôve been really chill at least trying to enjoy
my life
‚Ä¢MoE-4 R3: i ‚Äôm trying to get a good life and have a good time , i ‚Äôm not sure what to
do in my free time
‚Ä¢MoE-4 R4: and i ‚Äôve deÔ¨Ånitely learned a lot of things to do
‚Ä¢MoE-4 R5: i d say that it has been given me both the time and pleasure, but i m still
an introvert with few hobbies
B.2.2 Cornell Movie Corpus
1.Input Conversation: (...) excuse me
‚Ä¢MoE-1 R1: i do n‚Äôt know .
‚Ä¢MoE-1 R2: i do n‚Äôt know .
‚Ä¢MoE-1 R3: yes .
‚Ä¢MoE-1 R4: what do you mean ?
‚Ä¢MoE-1 R5: i ‚Äôm sorry .
‚Ä¢MoE-2 R1: i ‚Äôll give you a few minutes .
‚Ä¢MoE-2 R2: i do n‚Äôt want to talk about it .
‚Ä¢MoE-2 R3: may i come in ?
‚Ä¢MoE-2 R4: where is this , mr. kessler ?
‚Ä¢MoE-2 R5: what do you think ?
‚Ä¢MoE-3 R1: what ‚Äôs it ? are you alright ?
‚Ä¢MoE-3 R2: you want me to read ?
‚Ä¢MoE-3 R3: i do n‚Äôt know .
‚Ä¢MoE-3 R4: i ‚Äôll just have to kill you .
‚Ä¢MoE-3 R5: you ca n‚Äôt say it ‚Äôs over .
‚Ä¢MoE-4 R1: well if you hang up , you ‚Äôll die on it .
‚Ä¢MoE-4 R2: superman ? oh , was he here ?
‚Ä¢MoE-4 R3: well , you know , that ‚Äôs a lie , i do n‚Äôt have your money .
‚Ä¢MoE-4 R4: are you the killer ben ?
‚Ä¢MoE-4 R5: there ‚Äôs nothing to be wrong with that .
2.Input Conversation: (...) yes ?
‚Ä¢MoE-1 R1: what do you mean ?
‚Ä¢MoE-1 R2: i ‚Äôm not sure .
‚Ä¢MoE-1 R3: what ‚Äôs the matter ?
‚Ä¢MoE-1 R4: i do n‚Äôt know .
‚Ä¢MoE-1 R5: no .
‚Ä¢MoE-2 R1: what did you say ?
23

--- PAGE 24 ---
‚Ä¢MoE-2 R2: he says that you ‚Äôre not a good man .
‚Ä¢MoE-2 R3: i ‚Äôll tell you what i ‚Äôll do . i ‚Äôll be right back .
‚Ä¢MoE-2 R4: do you want to go to the police ?
‚Ä¢MoE-2 R5: are you still in love with him ?
‚Ä¢MoE-3 R1: sam , i ‚Äôm getting married .
‚Ä¢MoE-3 R2: johnny . it was announced this morning . he ‚Äôs going to play the lead in
the new Ô¨Ålm .
‚Ä¢MoE-3 R3: i ‚Äôm sorry , mister mcmurphy , i count only eight .
‚Ä¢MoE-3 R4: i said i was n‚Äôt looking for him . will you excuse my miss ... ?
‚Ä¢MoE-3 R5: pay ! in and i say !
‚Ä¢MoE-4 R1: i would like to marry you . but i have already promised myself ... to god .
‚Ä¢MoE-4 R2: do you know where he ‚Äôs living ?
‚Ä¢MoE-4 R3: i know you ‚Äôre soft .
‚Ä¢MoE-4 R4: it ‚Äôs susan ‚Äôs cooking , claire . why uncle james lets them stay ?
‚Ä¢MoE-4 R5: i do n‚Äôt want to be any trouble .
3.Input Conversation: (...) can i ask my friend , can he come in ? just for a few minutes ?
‚Ä¢MoE-1 R1: i ‚Äôm not sure .
‚Ä¢MoE-1 R2: i do n‚Äôt know
‚Ä¢MoE-1 R3: he ‚Äôs not a man , is n‚Äôt he ?
‚Ä¢MoE-1 R4: no, i ‚Äôm sorry .
‚Ä¢MoE-1 R5: what do you mean ?
‚Ä¢MoE-2 R1: all right . what are you going to do ?
‚Ä¢MoE-2 R2: no , i ‚Äôve never seen him before .
‚Ä¢MoE-2 R3: i want to be with him .
‚Ä¢MoE-2 R4: this is a very interesting man .
‚Ä¢MoE-2 R5: he ‚Äôs not the greatest . you do n‚Äôt know .
‚Ä¢MoE-3 R1: oh , come on , i ‚Äôm sick and tired of your life .
‚Ä¢MoE-3 R2: no . i still do n‚Äôt have any reason to do that .
‚Ä¢MoE-3 R3: my god , are you kidding , i ‚Äôm a little tired .
‚Ä¢MoE-3 R4: i do n‚Äôt understand . where is he ?
‚Ä¢MoE-3 R5: no . i still do n‚Äôt have any reason let you be together
‚Ä¢MoE-4 R1: oh , of course !
‚Ä¢MoE-4 R2: because he was scary , right ?
‚Ä¢MoE-4 R3: i ‚Äôm sure you ‚Äôre a good friend .
‚Ä¢MoE-4 R4: can i get you a glass of champagne ?
‚Ä¢MoE-4 R5: so i like to make a point . you got a fucking friend that does n‚Äôt work
4.Input Conversation: (...) your friend ?
‚Ä¢MoE-1 R1: i do n‚Äôt know
‚Ä¢MoE-1 R2: you ‚Äôre not going to tell him ?
‚Ä¢MoE-1 R3: well , i ‚Äôm not sure .
‚Ä¢MoE-1 R4: i do n‚Äôt know .
‚Ä¢MoE-1 R5: i ‚Äôm sorry
‚Ä¢MoE-2 R1: i like him . he ‚Äôs a very good man .
24

--- PAGE 25 ---
‚Ä¢MoE-2 R2: maybe i should go home and talk to him .
‚Ä¢MoE-2 R3: i ‚Äôm not going to let him go .
‚Ä¢MoE-2 R4: oh , please .
‚Ä¢MoE-2 R5: if you do n‚Äôt mind , you ‚Äôll have to do it , will you ?
‚Ä¢MoE-3 R1: what ‚Äôs the idea of working with you ? what ‚Äôs the matter ? !
‚Ä¢MoE-3 R2: i ‚Äôll look into her . tell ‚Äôem .
‚Ä¢MoE-3 R3: that ‚Äôs something i have n‚Äôt done the before .
‚Ä¢MoE-3 R4: so , our lawyers are here .
‚Ä¢MoE-3 R5: we ‚Äôve got some plans here .
‚Ä¢MoE-4 R1: my friend , you must be really glad to help . if you do n‚Äôt mind .
‚Ä¢MoE-4 R2: is n‚Äôt there something you can do . how can you help at night ?
‚Ä¢MoE-4 R3: how do you do , mrs. ryan ? i ‚Äôd heard you were beautiful , and so you are
.
‚Ä¢MoE-4 R4: the winning side would have paid you much better .
‚Ä¢MoE-4 R5: now , here ‚Äôs a babe ! I give you a blue ox , you got ta help me !
5.Input Conversation: (...) he ‚Äôs going back to the front this evening . i ca n‚Äôt see him
otherwise .
‚Ä¢MoE-1 R1: i ‚Äôm not going to be here .
‚Ä¢MoE-1 R2: what do you mean ?
‚Ä¢MoE-1 R3: i ‚Äôm not sure . it ‚Äôs not your fault .
‚Ä¢MoE-1 R4: i do n‚Äôt know .
‚Ä¢MoE-1 R5: you ‚Äôre not going to tell him ?
‚Ä¢MoE-2 R1: i think i ‚Äôll tell you what i ‚Äôm gon na do .
‚Ä¢MoE-2 R2: he ‚Äôs on the way , right ?
‚Ä¢MoE-2 R3: that ‚Äôs not true .
‚Ä¢MoE-2 R4: not too late . i ‚Äôm not going to be here .
‚Ä¢MoE-2 R5: what are you talking about ? he ‚Äôs a very sweet man .
‚Ä¢MoE-3 R1: what ? i think it ‚Äôs wonderful .
‚Ä¢MoE-3 R2: sure , i guess !
‚Ä¢MoE-3 R3: oh , for christ ‚Äôs sake .
‚Ä¢MoE-3 R4: they ‚Äôll be all right , you ‚Äôre my guest .
‚Ä¢MoE-3 R5: leon , you are not going to negotiate !
‚Ä¢MoE-4 R1: ok. he asks about you all the time . i want to tell you .
‚Ä¢MoE-4 R2: the way i feel , believe me , it ‚Äôs hard to part and see a guy ‚Äôs picture .
‚Ä¢MoE-4 R3: do n‚Äôt worry . i ‚Äôll take care of it .
‚Ä¢MoE-4 R4: certainly it ‚Äôs important .
‚Ä¢MoE-4 R5: yeah , and what a real hero .
B.3 Sample Utterances for EXP 2: Question Intent
B.3.1 Reddit Casual
1.Sample Conversation: (...) i love how genuine your comments are and you only comment
if it s how you really feel and not for karma like when you wish people a merry christmas
‚Ä¢MoE-1 R1: what was your favorite guys and why did you have a boyfriend ?
‚Ä¢MoE-1 R2: i really love your username , and i am so happy for you .
25

--- PAGE 26 ---
‚Ä¢MoE-1 R3: what was your favorite guys and why did you have a nice day ?
‚Ä¢MoE-2 R1: how did you Ô¨Ånd someone that you like ?
‚Ä¢MoE-2 R2: just wondering if you re interested in reddit ?
‚Ä¢MoE-2 R3: how did you feel about it ?
‚Ä¢MoE-3 R1: people like you really have a special place in my heart .
‚Ä¢MoE-3 R2: people like you really have a special place in my heart to me
‚Ä¢MoE-3 R3: people like you really have a special place in my heart?
‚Ä¢MoE-2 R1: how are you liking it ?
‚Ä¢MoE-2 R2: do you know i just love being together with you ?
‚Ä¢MoE-2 R3: how are you my friend ? ?
2.Sample Conversation: (...) people like you really have a special place in my heart .
‚Ä¢MoE-1 R1: that s nice !
‚Ä¢MoE-1 R2: happy cake day !
‚Ä¢MoE-1 R3: that sounds nice , you have a good time to stay alone ?
‚Ä¢MoE-2 R1: just wondering if you re happy with me ?
‚Ä¢MoE-2 R2: how did you feel about it ?
‚Ä¢MoE-2 R3: how did you get this ?
‚Ä¢MoE-3 R1: hope your christmas was a nice one to celebrate with you
‚Ä¢MoE-3 R2: hope your christmas was pretty good , my friend being celebrate with .
‚Ä¢MoE-3 R3: hope your christmas was nice with my friend who celebrate with you
‚Ä¢MoE-4 R1: omg why !
‚Ä¢MoE-4 R2: how far into your heart ?
‚Ä¢MoE-4 R3: how are you liking this happiness as well ?
3.Sample Conversation: (...) hope your christmas was pretty good , my friend assuming you
celebrate .
‚Ä¢MoE-1 R1: thank you !
‚Ä¢MoE-1 R2: thanks !,
‚Ä¢MoE-1 R3: thank you !
‚Ä¢MoE-2 R1: how did you guys meet this year ?
‚Ä¢MoE-2 R2: how was your day going ?
‚Ä¢MoE-2 R3: how did you celebrate this year ?
‚Ä¢MoE-3 R1: haha thank you !
‚Ä¢MoE-3 R2: thank you !
‚Ä¢MoE-3 R3: thank you .
‚Ä¢MoE-4 R1: how ‚Äôs your day going ?
‚Ä¢MoE-4 R2: how are you liking it so far ?
‚Ä¢MoE-4 R3: i have no idea what it ‚Äôs about ?
26

--- PAGE 27 ---
B.3.2 Cornell Movie Corpus
1.Sample Conversation: that ‚Äôs some face they left hanging on you . you look like ...
‚Ä¢MoE-1 R1: what ? what are you talking about ?
‚Ä¢MoE-1 R2: what ? what are you talking about ?
‚Ä¢MoE-1 R3: what ?
‚Ä¢MoE-2 R1: what ? what ‚Äôs wrong with it ?
‚Ä¢MoE-2 R2: what ? you think they ‚Äôre going to kill me ?
‚Ä¢MoE-2 R3: what is it ?
‚Ä¢MoE-3 R1: well . what ‚Äôs the difference ?
‚Ä¢MoE-3 R2: what ? what ‚Äôs the deal ?
‚Ä¢MoE-3 R3: what ? what ? what ‚Äôs wrong with what happened ?
‚Ä¢MoE-4 R1: what ? there ‚Äôs no pain in my mind ?
‚Ä¢MoE-4 R2: what ? why do you think i want to know ?
‚Ä¢MoE-4 R3: what can i do ? there ‚Äôs no way i can change .
2.Sample Conversation: what hotel ?
‚Ä¢MoE-1 R1: what the fuck are you doing here ?
‚Ä¢MoE-1 R2: what ?
‚Ä¢MoE-1 R3: what are you talking about ?,
‚Ä¢MoE-2 R1: what ‚Äôs the matter ?
‚Ä¢MoE-2 R2: what about it ?
‚Ä¢MoE-2 R3: who ‚Äôre you traveling with?
‚Ä¢MoE-3 R1: whatever . i ‚Äôm going to work here .
‚Ä¢MoE-3 R2: tom . how are you ?
‚Ä¢MoE-3 R3: what ? what about the hotel ?
‚Ä¢MoE-4 R1: do you think i want the plaza ?
‚Ä¢MoE-4 R2: what do you think ? where ‚Äôs my mother ?
‚Ä¢MoE-4 R3: what hotel i should go?
3.Sample Conversation: the holiday hotel .
‚Ä¢MoE-1 R1: what are you talking about ?
‚Ä¢MoE-1 R2: what ?
‚Ä¢MoE-1 R3: what are you doing here ?
‚Ä¢MoE-2 R1: what is your travel plan?
‚Ä¢MoE-2 R2: how can i get there?
‚Ä¢MoE-2 R3: why do you want to be there?
‚Ä¢MoE-3 R1: what ? are you walking in there ?,
‚Ä¢MoE-3 R2: absolutely . what ‚Äôs up with the hotel ?
‚Ä¢MoE-3 R3: what ? bring ‚Äôem in
‚Ä¢MoE-4 R1: what are you ? a cop ?
‚Ä¢MoE-4 R2: what about that ? i work here .
‚Ä¢MoE-4 R3: what ? ... oh you know he ‚Äôs there ?,
27

--- PAGE 28 ---
B.4 Sample Utterances for EXP 2: Positive Sentiment Intent
B.4.1 Reddit Casual
1.Sample Conversation: (...) it s deÔ¨Ånitely not the college experience .
‚Ä¢MoE-1 R1: i feel like i m more of a high school and talking to people .
‚Ä¢MoE-1 R2: i feel like i ‚Äôm not a teacher , and i talk to , i talk to people about it .
‚Ä¢MoE-1 R3: i feel like a lot of people are just as nervous as i talk to , and i really really
like talking about .
‚Ä¢MoE-2 R1: i m so proud of you for being able to keep up with it !
‚Ä¢MoE-2 R2: i hope you re having a great day !
‚Ä¢MoE-2 R3: i have a very good time on social media !
‚Ä¢MoE-3 R1: seems like middle school to end up way more stable than everyone and
don t know who does this
‚Ä¢MoE-3 R2: seems like real good compared to everyone after work and nobody is
around me even the age of 35
‚Ä¢MoE-3 R3: feels like a real awkward going to say though and nobody cares about
each other with me
‚Ä¢MoE-4 R1: if you want to get into college , i ‚Äôm pretty good at it !
‚Ä¢MoE-4 R2: thank you for taking the time to look at it , i ‚Äôm trying to get a few more
opportunities for you now for your semester
‚Ä¢MoE-4 R3: good for you , i like college and have more people in my life , i know
better than high school i ve been
2.Sample Conversation: (...) feels more like high school except way easier to mess up and
nobody really talks to anyone lmao .
‚Ä¢MoE-1 R1: i m on reddit , and i m going to be pretty far from my Ô¨Årst year s college ,
but i d probably have to go to college
‚Ä¢MoE-1 R2: i m not really good at all , but i m a college student , i d be working on a
job , i d be able to go,
‚Ä¢MoE-1 R3: i m pretty sure it s a lot easier to get a job , but i guess it s a lot of work in
my Ô¨Åeld
‚Ä¢MoE-2 R1: i m going to be a senior and i m happy to meet people !
‚Ä¢MoE-2 R2: i hope you re having a great time !
‚Ä¢MoE-2 R3: i get to see my new friends and family !
‚Ä¢MoE-3 R1: i ‚Äôm looking forward to being able to say , but not because i am a great
life all for everyone to see each other once in a while
‚Ä¢MoE-3 R2: i ‚Äôm looking forward to going out , not because i feel so much better
knowing i will still have a great day , and i ‚Äôm happy to see
‚Ä¢MoE-3 R3: i ‚Äôm looking forward to going home , not only because i think i will be a
lot happier with my family and looking at the very young people .
‚Ä¢MoE-4 R1: for sure , i ve been in college for the past few years that i ve had plenty of
people in my life .
‚Ä¢MoE-4 R2: thank you for your input , i appreciate it !
‚Ä¢MoE-4 R3: you ll be proud of yourself and i hope you are able to enjoy those people
on your Ô¨Årst year
3.Sample Conversation: (...) i m looking forward to Ô¨Ånally being out , not only because i
think i ll be a lot happier at a full uni , but also bc i hate home
28

--- PAGE 29 ---
‚Ä¢MoE-1 R1: i Ô¨Ånally got the job that i m going to be a very good dad , and it just sucks .
‚Ä¢MoE-1 R2: i Ô¨Ånally got a new job that i feel like is not a good thing .
‚Ä¢MoE-1 R3: i Ô¨Ånally got the hang of my parents , and i feel very lucky to be very far as
possible .
‚Ä¢MoE-2 R1: now i m a college student and i m very proud of myself !
‚Ä¢MoE-2 R2: i ll try to meet new people !
‚Ä¢MoE-2 R3: i feel like i m a little too proud of myself !
‚Ä¢MoE-3 R1: yeah well and the whole thing will really help me feel a bit shitty right
now is n‚Äôt easy
‚Ä¢MoE-3 R2: yeah well this is the start and really make me feel good a bit better than
what i am .
‚Ä¢MoE-3 R3: yeah well and the goal is really help me feel good since everyone is mostly
different in their lives .
‚Ä¢MoE-4 R1: good advice , i really like it when i have to get a college degree , but i ‚Äôve
also got a decent amount of friends who have not
‚Ä¢MoE-4 R2: thank you for the advice , i really like to keep on going for college and i
‚Äôm sure my friends will go to college
‚Ä¢MoE-4 R3: for sure , i want to get back on track to how much i used to be doing , but i
don t really like the idea of studying
B.4.2 Cornell Movie Corpus
1.Sample Conversation: (...) no i fucking do n‚Äôt ! you ca n‚Äôt make a cup of tea , edward .
‚Ä¢MoE-1 R1: what ?
‚Ä¢MoE-1 R2: no .
‚Ä¢MoE-1 R3: no , i ‚Äôm not .
‚Ä¢MoE-2 R1: yeah , i got it .
‚Ä¢MoE-2 R2: oh , i ‚Äôm so glad you say it .
‚Ä¢MoE-2 R3: yes , it ‚Äôs a very nice idea .
‚Ä¢MoE-3 R1: yes , sir . what ‚Äôs wrong ?
‚Ä¢MoE-3 R2: yes . yes , sir .
‚Ä¢MoE-3 R3: yes . who is this ?
‚Ä¢MoE-4 R1: oh , yes . i ‚Äôm going to miss you .,
‚Ä¢MoE-4 R2: yes . i ‚Äôm sorry , i did n‚Äôt know how to do it .
‚Ä¢MoE-4 R3: yes , yes . i ‚Äôll tell you something about it .
2.Sample Conversation: (...) why not ? the whole of the british empire was built on cups of
tea .
‚Ä¢MoE-1 R1: what ?
‚Ä¢MoE-1 R2: no !
‚Ä¢MoE-1 R3: no , no .
‚Ä¢MoE-2 R1: good . i ‚Äôm glad you say that .
‚Ä¢MoE-2 R2: yeah , i got it .
‚Ä¢MoE-2 R3: yes , you are right about that .
‚Ä¢MoE-3 R1: yes . yes , sir .
29

--- PAGE 30 ---
‚Ä¢MoE-3 R2: yes , of course .
‚Ä¢MoE-3 R3: yes . yes . that ‚Äôs right .
‚Ä¢MoE-4 R1: yes , i ‚Äôll tell you what it is .
‚Ä¢MoE-4 R2: yes , yes . i ‚Äôll tell you something else !
‚Ä¢MoE-4 R3: yes , i ‚Äôm with you .
3.Sample Conversation: (...) if you ‚Äôll be good enough to remove those newspapers i ‚Äôll have
a seat .
‚Ä¢MoE-1 R1: no , i ‚Äôm not .
‚Ä¢MoE-1 R2: what ?
‚Ä¢MoE-1 R3: no !
‚Ä¢MoE-2 R1: yes , i think it ‚Äôs a good idea .
‚Ä¢MoE-2 R2: yes , yes . i ‚Äôll be in a minute .
‚Ä¢MoE-2 R3: good . yes , you ‚Äôre not going to have to do it .
‚Ä¢MoE-3 R1: yes . yes . yes . yes .
‚Ä¢MoE-3 R2: what ‚Äôs the matter with you ?
‚Ä¢MoE-3 R3: yes , ma‚Äôam , i do .
‚Ä¢MoE-4 R1: oh , yes ! i ‚Äôll do it .
‚Ä¢MoE-4 R2: yes , yes . i ‚Äôm sure you must be a very sick man .
‚Ä¢MoE-4 R3: yes , yes . i ‚Äôm sure you do ..
B.5 Sample Utterances for EXP 2: Negative Sentiment Intent
B.5.1 Reddit Casual
1.Sample Conversation: (...) it s deÔ¨Ånitely not the college experience .
‚Ä¢MoE-1 R1: i m not much of a sports person , i Ô¨Ånd that sort of an emotional connection
to others .
‚Ä¢MoE-1 R2: i ‚Äôm more of a high school student and i m wondering if anyone else to
talk to about this .
‚Ä¢MoE-1 R3: i ‚Äôm not a social person , but i ‚Äôm just really curious how to talk to other
people .
‚Ä¢MoE-2 R1: i don t know if i should be going to school right now .
‚Ä¢MoE-2 R2: i don t know how to do it , i m nervous .
‚Ä¢MoE-2 R3: i don t think i m going to be able to do all the things i ve been doing .
‚Ä¢MoE-3 R1: feels like shit not less to lie than excuse myself to just miss class all the
time .
‚Ä¢MoE-3 R2: feels like decent becoming middle school system and not close to him
anymore anyways because nobody ever mentioned .
‚Ä¢MoE-3 R3: feels like middle school only harder to stay up and nobody really talks
about anything except nowadays .
‚Ä¢MoE-4 R1: because i hate making out of my freshman year i was happy about it
‚Ä¢MoE-4 R2: but i hate to admit it , so like this was the Ô¨Årst time i even went to college .
‚Ä¢MoE-4 R3: and people are terrible at Ô¨Åguring out what to do , you could n‚Äôt pay extra
for this year !
30

--- PAGE 31 ---
2.Sample Conversation: (...) feels more like high school except way easier to mess up and
nobody really talks to anyone lmao .
‚Ä¢MoE-1 R1: i m not really good at all , but i guess i would have to go to a university
for a good job .
‚Ä¢MoE-1 R2: so far that i ‚Äôm not happy to have my own experience , but i m also going
to have a job that i d like to have .
‚Ä¢MoE-1 R3: i m on the same boat as you , but i m also a college student for a few years
.
‚Ä¢MoE-2 R1: i m not going to try and start doing anything for the next semester .
‚Ä¢MoE-2 R2: i don t know if it s the same .
‚Ä¢MoE-2 R3: i don t know if it s going to be the same since i m still in college .
‚Ä¢MoE-3 R1: i m looking forward to going back to being alone , but not because i feel
like i m a better Ô¨Åt in with the wait til stressful one
‚Ä¢MoE-3 R2: i ‚Äôm looking forward to going back home , not only because i feel that
there ‚Äôs a lot of time to do something productive after i get there
‚Ä¢MoE-3 R3: i ‚Äôm looking forward to being at school , not only because i think i ‚Äôll be a
bit happier now and will have to pay for 4 months
‚Ä¢MoE-4 R1: and i hate it , i get bored !
‚Ä¢MoE-4 R2: i just switched because i was a senior student and i know absolutely hate
that i had to do
‚Ä¢MoE-4 R3: and i hate talking about it , i ‚Äôll get left out
3.Sample Conversation: (...) i m looking forward to Ô¨Ånally being out , not only because i
think i ll be a lot happier at a full uni , but also bc i hate home
‚Ä¢MoE-1 R1: so i Ô¨Ånally got a new job that i am not a very good person .
‚Ä¢MoE-1 R2: i ‚Äôm not going to be able to Ô¨Ånd a way to feel better about it , but i just
feel better .
‚Ä¢MoE-1 R3: i m not the most comfortable person who wants to be so bad about it or
something that i should be able to be able to do .
‚Ä¢MoE-2 R1: i feel like i m not going to be honest .
‚Ä¢MoE-2 R2: i feel like i m not going to be in the same position .
‚Ä¢MoE-2 R3: i don t know if it s all about going to school .
‚Ä¢MoE-3 R1: yeah well all and the person does really make me feel better for everyone
to be so serious about it .
‚Ä¢MoE-3 R2: yeah well this and the beat all does seem feel a bit better because everyone
is basically just that it is in their ass .
‚Ä¢MoE-3 R3: yeah well and the post really not me feel better a lot better since it is not
being considered so big .
‚Ä¢MoE-4 R1: and i hate waking up , i feel like it all the time .
‚Ä¢MoE-4 R2: and i hate to admit it , i do n‚Äôt know what ‚Äôs going on .
‚Ä¢MoE-4 R3: and i know there are people who can keep up with their homework but i
do n‚Äôt know how to handle them !
B.5.2 Cornell Movie Corpus
1.Sample Conversation: (...) no i fucking do n‚Äôt ! you ca n‚Äôt make a cup of tea , edward .
‚Ä¢MoE-1 R1: i ‚Äôm sorry .
31

--- PAGE 32 ---
‚Ä¢MoE-1 R2: i ‚Äôm not sure .
‚Ä¢MoE-1 R3: i do n‚Äôt know , i do n‚Äôt know .
‚Ä¢MoE-2 R1: what do i have to do ?
‚Ä¢MoE-2 R2: why do n‚Äôt you just leave it be ?
‚Ä¢MoE-2 R3: you do n‚Äôt know what you ‚Äôre talking about .
‚Ä¢MoE-3 R1: i ‚Äôm not interested in what people used to do .
‚Ä¢MoE-3 R2: i ‚Äôm afraid that ‚Äôs not up to me
‚Ä¢MoE-3 R3: yes . what ‚Äôs the problem with it ?
‚Ä¢MoE-4 R1: i do n‚Äôt want to hear your problems .
‚Ä¢MoE-4 R2: i do n‚Äôt care . i have a lot of money in my life .
‚Ä¢MoE-4 R3: i do n‚Äôt want a goddamn cup of tea . i want to go home . i ‚Äôll have to be
here , okay ?
2.Sample Conversation: (...) why not ? the whole of the british empire was built on cups of
tea .
‚Ä¢MoE-1 R1: i do n‚Äôt know .
‚Ä¢MoE-1 R2: i ‚Äôm not sure . i ‚Äôm not going to be here .
‚Ä¢MoE-1 R3: i do n‚Äôt know .
‚Ä¢MoE-2 R1: who do you think you are ?
‚Ä¢MoE-2 R2: who cares ? i do n‚Äôt know what to say .
‚Ä¢MoE-2 R3: do n‚Äôt be a fool . do n‚Äôt you know what i ‚Äôm talking about .
‚Ä¢MoE-3 R1: yes . what ‚Äôs more like this ?
‚Ä¢MoE-3 R2: i do n‚Äôt believe that either .
‚Ä¢MoE-3 R3: that is correct . and how do we decide ?
‚Ä¢MoE-4 R1: i do n‚Äôt believe in it !
‚Ä¢MoE-4 R2: i do n‚Äôt want to hear you anymore . you hear me ?
‚Ä¢MoE-4 R3: i do n‚Äôt want to hear it . i know you ‚Äôre very clever .
3.Sample Conversation: (...) if you ‚Äôll be good enough to remove those newspapers i ‚Äôll have
a seat .
‚Ä¢MoE-1 R1: i do n‚Äôt know .
‚Ä¢MoE-1 R2: i ‚Äôm not sure .
‚Ä¢MoE-1 R3: i do n‚Äôt know .
‚Ä¢MoE-2 R1: i do n‚Äôt know what you ‚Äôre talking about .
‚Ä¢MoE-2 R2: who are you ? can you get out of here ?
‚Ä¢MoE-2 R3: do n‚Äôt be a fool . what do you want me to do ?
‚Ä¢MoE-3 R1: you ‚Äôre welcome to one of these , whatever .
‚Ä¢MoE-3 R2: what difference does it make ?
‚Ä¢MoE-3 R3: i ‚Äôm afraid not . have to ask you a question .
‚Ä¢MoE-4 R1: i do n‚Äôt want to hear it . i do n‚Äôt want to hear any more today .
‚Ä¢MoE-4 R2: look at this ! why are you laughing ?
‚Ä¢MoE-4 R3: i do n‚Äôt care ! i do n‚Äôt know what ‚Äôs good with you !
32

--- PAGE 33 ---
B.6 Sample Utterances for EXP 2: Joy Emotion Intent
B.6.1 Reddit Casual
1.Sample Conversation: (...) besides every anime intro and outro song ever at least from the
series i like , i ‚Äôve been getting a kick out of j rock in general .
‚Ä¢MoE-1 R1: i got a song a few months ago and i can relate .
‚Ä¢MoE-1 R2: i got a song from a song that i can recently Ô¨Ånish .
‚Ä¢MoE-1 R3: i got a song from a song that has been a really rough time lately .
‚Ä¢MoE-2 R1: what s your favourite anime to listen to ?
‚Ä¢MoE-2 R2: i just enjoy the new anime and i ‚Äôm a big fan of it .
‚Ä¢MoE-2 R3: i ‚Äôm always a fan of the anime and i ‚Äôve been listening to it all the lately
‚Ä¢MoE-3 R1: i got very into french music this year which i can relate every year
‚Ä¢MoE-3 R2: i got into love pop music this year , so i can relate with very well .
‚Ä¢MoE-3 R3: i got into love rap recently this year so i can relate with you haha
‚Ä¢MoE-4 R1: haha i ‚Äôm into anime !
‚Ä¢MoE-4 R2: haha , anime manga is considered my Ô¨Årst language .
‚Ä¢MoE-4 R3: sounds like my favourite anime and manga japan , and they probably are
my main thing .
2.Sample Conversation: (...) i got deep into idol music this year so i can relate a bit .
‚Ä¢MoE-1 R1: i ‚Äôve been thinking about the same as a lot of new bands i listen to .
‚Ä¢MoE-1 R2: i just started a new anime where i feel like i ‚Äôm into the same genre .
‚Ä¢MoE-1 R3: i just like the new design and see it on youtube .
‚Ä¢MoE-2 R1: <unk> is my favorite song
‚Ä¢MoE-2 R2: i love the song !
‚Ä¢MoE-2 R3: what do you like about the music ?
‚Ä¢MoE-3 R1: i have really watched deep dark <unk> recently .
‚Ä¢MoE-3 R2: yeah i have heard this song called so long
‚Ä¢MoE-3 R3: i know just your song i played video games sometimes .
‚Ä¢MoE-4 R1: i ‚Äôm also loving the music so far .
‚Ä¢MoE-4 R2: haha , i love anime manga from the 90 ‚Äôs .
‚Ä¢MoE-4 R3: haha yes but for me it ‚Äôs all anime .
3.Sample Conversation: (...) some people have the same username across multiple websites
such as steam or xbox .
‚Ä¢MoE-1 R1: people might even get mad at them , because they are on their own .
‚Ä¢MoE-1 R2: i might be the opposite .
‚Ä¢MoE-1 R3: i never thought about being a dick .
‚Ä¢MoE-2 R1: i guess there ‚Äôs a reason for people to use their username too .
‚Ä¢MoE-2 R2: i just use reddit , and the new design is pretty good .
‚Ä¢MoE-2 R3: i see a lot of people using this as well as the same goes .
‚Ä¢MoE-3 R1: you might also be able to get a hold of their different places with only
their username .
‚Ä¢MoE-3 R2: you might even be able to get a hold of them in different places with only
their username .
33

--- PAGE 34 ---
‚Ä¢MoE-3 R3: you might also be able to get a hold of my different places with the
internet access .
‚Ä¢MoE-4 R1: i ‚Äôm also loving the music so far .
‚Ä¢MoE-4 R2: haha well i guess you are talking about the internet .
‚Ä¢MoE-4 R3: haha good karma happens in a lot of us , but for me i ‚Äôm most on reddit .
B.6.2 Cornell Movie Corpus
1.Sample Conversation: (...) i guess i cut pretty loose in my day too .
‚Ä¢MoE-1 R1: oh , i ‚Äôm Ô¨Åne .
‚Ä¢MoE-1 R2: oh , yeah .
‚Ä¢MoE-1 R3: oh , yeah .
‚Ä¢MoE-2 R1: yes , darling . i ‚Äôm looking for you .
‚Ä¢MoE-2 R2: nice . good .
‚Ä¢MoE-2 R3: okay , we ‚Äôll see you .
‚Ä¢MoE-3 R1: oh yeah . that ‚Äôs nice .
‚Ä¢MoE-3 R2: oh , it ‚Äôs probably a good idea .
‚Ä¢MoE-3 R3: oh , it ‚Äôs worth it .
‚Ä¢MoE-4 R1: it ‚Äôs so beautiful . there ‚Äôs a lot of money in there .
‚Ä¢MoE-4 R2: i ‚Äôm so sure you ‚Äôll Ô¨Ånd out how that d go.
‚Ä¢MoE-4 R3: i said it ‚Äôs good call .
2.Sample Conversation: (...) i could get ben to tap into the baxter ‚Äôs main power to generate
enough voltage
‚Ä¢MoE-1 R1: yes , sir .
‚Ä¢MoE-1 R2: oh , yes , sir .
‚Ä¢MoE-1 R3: oh , yes , sir .
‚Ä¢MoE-2 R1: and i ‚Äôm sure it ‚Äôs a good idea
‚Ä¢MoE-2 R2: yes , ma‚Äôam . i ‚Äôm going to work .
‚Ä¢MoE-2 R3: oh , i ‚Äôm going to have a nice time .
‚Ä¢MoE-3 R1: hey . that ‚Äôs the problem .
‚Ä¢MoE-3 R2: oh , that ‚Äôs a possibility .
‚Ä¢MoE-3 R3: well . hope you enjoy it .
‚Ä¢MoE-4 R1: i do n‚Äôt know . but i ‚Äôm sure it ‚Äôs a very good idea .
‚Ä¢MoE-4 R2: i ‚Äôm glad you got your sense of humor back . think you ‚Äôll keep it ?
‚Ä¢MoE-4 R3: yes , that ‚Äôs exactly what we got to do .
3.Sample Conversation: (...) or that he ca n‚Äôt be bothered to write us a proper letter ? a
dissertation on apples , we do n‚Äôt need !
‚Ä¢MoE-1 R1: no , sir .
‚Ä¢MoE-1 R2: yes , sir .
‚Ä¢MoE-1 R3: yes , sir .
‚Ä¢MoE-2 R1: yes , sir . they ‚Äôre telling me all this .
‚Ä¢MoE-2 R2: he ‚Äôs a bad guy , huh ?
34

--- PAGE 35 ---
‚Ä¢MoE-2 R3: oh , that ‚Äôs a good idea .
‚Ä¢MoE-3 R1: oh . well , that ‚Äôs good .
‚Ä¢MoE-3 R2: oh , that sounds great .
‚Ä¢MoE-3 R3: yes , he ‚Äôs that sweetheart .
‚Ä¢MoE-4 R1: he says he does n‚Äôt feel good too . i thought you might know .
‚Ä¢MoE-4 R2: yes . he was all right .
‚Ä¢MoE-4 R3: jeez , right . he ‚Äôs like a ghost !
B.7 Sample Utterances for EXP 2: Optimism Emotion Intent
B.7.1 Reddit Casual
1.Sample Conversation: (...) besides every anime intro and outro song ever at least from the
series i like , i ‚Äôve been getting a kick out of j rock in general .
‚Ä¢MoE-1 R1: got a few more of my favorite songs !
‚Ä¢MoE-1 R2: have you ever got a song that you can listen to ?
‚Ä¢MoE-1 R3: i ve been listening to a lot of songs lately and i can relate .
‚Ä¢MoE-2 R1: good lord , deÔ¨Ånitely send forever shit to the artist ...
‚Ä¢MoE-2 R2: good for you , the fantastic video !
‚Ä¢MoE-2 R3: good one , but the next one is , better !
‚Ä¢MoE-3 R1: i got deep into idol music this year so i can relate a bit .
‚Ä¢MoE-3 R2: i got big into pop music today so i can relate your success !
‚Ä¢MoE-3 R3: i got deep into music recently so i will love my new year !
‚Ä¢MoE-4 R1: good for you , it s a good anime intro haha
‚Ä¢MoE-4 R2: might have some japanese jazz fusion and art
‚Ä¢MoE-4 R3: good one , the anime and manga really Ô¨Åt me
2.Sample Conversation: (...) i got deep into idol music this year so i can relate a bit .
‚Ä¢MoE-1 R1: what do you like about it ?
‚Ä¢MoE-1 R2: what do you like to see ?
‚Ä¢MoE-1 R3: i ‚Äôm not sure what to watch ?
‚Ä¢MoE-2 R1: good one , the next anime ‚Äôs been awesome !,
‚Ä¢MoE-2 R2: good for you , my friend !
‚Ä¢MoE-2 R3: the best way to start the new year !
‚Ä¢MoE-3 R1: is it your favourite song ever ?
‚Ä¢MoE-3 R2: does it match your taste ?
‚Ä¢MoE-3 R3: is it really your favorite movie ?
‚Ä¢MoE-4 R1: good one , especially for the current age
‚Ä¢MoE-4 R2: thanks for your good song it gets more popular
‚Ä¢MoE-4 R3: good lord , the music is popular !
3.Sample Conversation: (...) some people have the same username across multiple websites
such as steam or xbox .
‚Ä¢MoE-1 R1: it ‚Äôs not even a joke , but i think it is a scam .
35

--- PAGE 36 ---
‚Ä¢MoE-1 R2: why people would use it as a joke ?
‚Ä¢MoE-1 R3: how do you know how to deal with it ?
‚Ä¢MoE-2 R1: what you can see is a new generation of the future.
‚Ä¢MoE-2 R2: be aware of what their user names are and comments are made to post
‚Ä¢MoE-2 R3: it depends on the mods and the subreddit , i see them as a possible
community .
‚Ä¢MoE-3 R1: you might even be able to get a hold of their different places with different
people
‚Ä¢MoE-3 R2: you might even be able to get a hold of their different places with different
people than your own view .
‚Ä¢MoE-3 R3: you may even be able to get a hold of your friend in different places with
their username ?
‚Ä¢MoE-4 R1: alright , so , especially reddit . might have to check out the real difference
, especially with older ones !
‚Ä¢MoE-4 R2: good on you for noticing when you mentioned that though not to be a
problem !
‚Ä¢MoE-4 R3: good for you , despite only being the mom and dad for me , i believe there
are some old people now , still hoping to do that
B.7.2 Cornell Movie Corpus
1.Sample Conversation: (...) i guess i cut pretty loose in my day too .
‚Ä¢MoE-1 R1: oh , i ‚Äôm Ô¨Åne .
‚Ä¢MoE-1 R2: oh , yeah .
‚Ä¢MoE-1 R3: oh , i ‚Äôm sorry .
‚Ä¢MoE-2 R1: okay , take it easy .
‚Ä¢MoE-2 R2: all right . everything ‚Äôs been pretty .
‚Ä¢MoE-2 R3: go ahead . the last time you ‚Äôre right .
‚Ä¢MoE-3 R1: i ‚Äôm talking about ...
‚Ä¢MoE-3 R2: let ‚Äôs see if i ever known .
‚Ä¢MoE-3 R3: you learned from your american friends ?
‚Ä¢MoE-4 R1: i‚Äôm sure it can be Ô¨Åne . it ‚Äôs just you and me , brad .
‚Ä¢MoE-4 R2: it ‚Äôs so cool . really .
‚Ä¢MoE-4 R3: you ‚Äôre right . i ‚Äôm sure it will work . i could feel you in your position .
2.Sample Conversation: (...) i could get ben to tap into the baxter ‚Äôs main power to generate
enough voltage
‚Ä¢MoE-1 R1: yes , sir . but we ‚Äôre not going to do the same thing .
‚Ä¢MoE-1 R2: no , sir .
‚Ä¢MoE-1 R3: yes , sir .
‚Ä¢MoE-2 R1: all right . it ‚Äôs the way . i ‚Äôm going to get it .
‚Ä¢MoE-2 R2: all right . you ‚Äôll do it .
‚Ä¢MoE-2 R3: go ahead . i ‚Äôll be all right .
‚Ä¢MoE-3 R1: it is ! i ‚Äôm stopping !
36

--- PAGE 37 ---
‚Ä¢MoE-3 R2: that bastard ! i ‚Äôll gladly dispose of him in the name of the order , son of
malkovich .
‚Ä¢MoE-3 R3: i think they should stop and start looking right here i think they should
stop and start looking right here .
‚Ä¢MoE-4 R1: i ‚Äôm looking at it . i ‚Äôm going to get it right .
‚Ä¢MoE-4 R2: there ‚Äôs something else , major . i ‚Äôm starting to think what you said has a
ton of beauty in it
‚Ä¢MoE-4 R3: see , that ‚Äôs what we ‚Äôre here for .
3.Sample Conversation: (...) or that he ca n‚Äôt be bothered to write us a proper letter ? a
dissertation on apples , we do n‚Äôt need !
‚Ä¢MoE-1 R1: yes , sir . is that what you mean ?
‚Ä¢MoE-1 R2: yes , sir .
‚Ä¢MoE-1 R3: oh , yes , sir .
‚Ä¢MoE-2 R1: all right . we ‚Äôll be able to Ô¨Ågure out what ‚Äôs wrong with him .
‚Ä¢MoE-2 R2: okay , take it easy .
‚Ä¢MoE-2 R3: all right . we ‚Äôll be able to Ô¨Ågure out what ‚Äôs going on .
‚Ä¢MoE-3 R1: so ? the man got silver , china , rugs
‚Ä¢MoE-3 R2: he ‚Äôs out to the hospital ...
‚Ä¢MoE-3 R3: i see , i was just thinking ... maybe it ‚Äôs time to settle down .
‚Ä¢MoE-4 R1: you do n‚Äôt understand , i ‚Äôve been in love with him !
‚Ä¢MoE-4 R2: he ‚Äôll be back . i ‚Äôll wait for his car .
‚Ä¢MoE-4 R3: i ‚Äôll think of something . i do n‚Äôt think he ‚Äôs a bad guy .
B.8 Sample Utterances for EXP 2: Anger Emotion Intent
B.8.1 Reddit Casual
1.Sample Conversation: (...) besides every anime intro and outro song ever at least from the
series i like , i ‚Äôve been getting a kick out of j rock in general .
‚Ä¢MoE-1 R1: i got ta have this song , but i can relate to this .,
‚Ä¢MoE-1 R2: i got this song a few months ago , and i can relate to a bit .
‚Ä¢MoE-1 R3: i got my thoughts into this , but i can relate to this .
‚Ä¢MoE-2 R1: why are you into anime ?,
‚Ä¢MoE-2 R2: i lose interest in it .
‚Ä¢MoE-2 R3: i ‚Äôm not an expert when it comes to it .
‚Ä¢MoE-3 R1: i got deep into idol music this year so i can relate a bit .
‚Ä¢MoE-3 R2: i got deep into this punk shows recently so i can relate with it .
‚Ä¢MoE-3 R3: i got ta learn rock this pop punk so i can relate with it .
‚Ä¢MoE-4 R1: keeping my speed for reading the anime shit .
‚Ä¢MoE-4 R2: why are you damn talking english only ?
‚Ä¢MoE-4 R3: and this is why i am not such a rich dude .
2.Sample Conversation: (...) i got deep into idol music this year so i can relate a bit .
‚Ä¢MoE-1 R1: oh wow , that ‚Äôs a good one .
37

--- PAGE 38 ---
‚Ä¢MoE-1 R2: i like it , i ‚Äôm not a fan of the music genre , but i ‚Äôve had a lot of music .
‚Ä¢MoE-1 R3: oh wow .
‚Ä¢MoE-2 R1: why are you considering the big ex ?
‚Ä¢MoE-2 R2: i don ‚Äôt watch anime in general at the moment .
‚Ä¢MoE-2 R3: why are you considering anime good ?
‚Ä¢MoE-3 R1: i can totally see that this was all about the songs i did n‚Äôt talk to about .
‚Ä¢MoE-3 R2: i would n‚Äôt miss out on that video .
‚Ä¢MoE-3 R3: i don t miss that so much .
‚Ä¢MoE-4 R1: fuck yeah , i was there for like shit .
‚Ä¢MoE-4 R2: why are you considering doing a lot of anime ?
‚Ä¢MoE-4 R3: why do you think anime is bad ?
3.Sample Conversation: (...) some people have the same username across multiple websites
such as steam or xbox .
‚Ä¢MoE-1 R1: i think it all comes down to if they ‚Äôre serious .
‚Ä¢MoE-1 R2: i think it ‚Äôs just how they are in their mid 20s how they are , they are used
to their own lives .
‚Ä¢MoE-1 R3: it might seem like a lot of people in their late 20s , they have a large
problem with their accent .
‚Ä¢MoE-2 R1: why do people assume they have their own identity ?
‚Ä¢MoE-2 R2: it ‚Äôs just a series of spam jokes and whatnot
‚Ä¢MoE-2 R3: i think people are just dumb to the bottom line of their head .
‚Ä¢MoE-3 R1: you might also be able to Ô¨Ånd a hold of them with different people than
the previous ones .
‚Ä¢MoE-3 R2: you might also be able to get a hold of them in different places with the
username .
‚Ä¢MoE-3 R3: you might even be able to get a hold of them in different places with their
username .
‚Ä¢MoE-4 R1: i dont know who downvoted you , but let people tell you the truth of your
karma . why are you asking on reddit ?
‚Ä¢MoE-4 R2: i dont necessarily agree with them but just became the Ô¨Årst and its just
became a good one .
‚Ä¢MoE-4 R3: you ‚Äôre around and just do n‚Äôt use the internet for any speciÔ¨Åc reason , i
think there are thousands of people watching .
B.8.2 Cornell Movie Corpus
1.Sample Conversation: (...) i guess i cut pretty loose in my day too .
‚Ä¢MoE-1 R1: you do n‚Äôt have to .
‚Ä¢MoE-1 R2: no .
‚Ä¢MoE-1 R3: you do n‚Äôt have to have a good look .
‚Ä¢MoE-2 R1: i think you must be crazy . you ca n‚Äôt tell me what you ‚Äôre doing here !
‚Ä¢MoE-2 R2: you ‚Äôre not the one trying to get away . who owns the gun ?
‚Ä¢MoE-2 R3: you do n‚Äôt pull this cowboy shit , you ‚Äôre gon na get him out ?
‚Ä¢MoE-3 R1: what do you know . i saw you ?
38

--- PAGE 39 ---
‚Ä¢MoE-3 R2: what the hell is that
‚Ä¢MoE-3 R3: oh , you do n‚Äôt know . i ‚Äôm sorry .
‚Ä¢MoE-4 R1: what are you talking about ? you love your job .
‚Ä¢MoE-4 R2: you ‚Äôre like an undercover guy !
‚Ä¢MoE-4 R3: you do n‚Äôt get paid till we close tonight .
2.Sample Conversation: (...) i could get ben to tap into the baxter ‚Äôs main power to generate
enough voltage
‚Ä¢MoE-1 R1: you do n‚Äôt have to do this .
‚Ä¢MoE-1 R2: you do n‚Äôt know what you ‚Äôre talking about .
‚Ä¢MoE-1 R3: you do n‚Äôt know what you ‚Äôre talking about .
‚Ä¢MoE-2 R1: you got to be kidding !
‚Ä¢MoE-2 R2: we ‚Äôre not going to do this .
‚Ä¢MoE-2 R3: we want to frighten you away . so that you ‚Äôll move out
‚Ä¢MoE-3 R1: do n‚Äôt you dare , i ‚Äôll be all that ...
‚Ä¢MoE-3 R2: so you ‚Äôre telling the truth ?
‚Ä¢MoE-3 R3: you think i ‚Äôm right
‚Ä¢MoE-4 R1: you ‚Äôre driving me nuts !
‚Ä¢MoE-4 R2: you ‚Äôre trying to kill me , are n‚Äôt you ?
‚Ä¢MoE-4 R3: you do n‚Äôt get it , pham . the idea was we were ripping them off .
3.Sample Conversation: (...) or that he ca n‚Äôt be bothered to write us a proper letter ? a
dissertation on apples , we do n‚Äôt need !
‚Ä¢MoE-1 R1: do n‚Äôt worry about it
‚Ä¢MoE-1 R2: do n‚Äôt you think he ‚Äôs in the dark .
‚Ä¢MoE-1 R3: do n‚Äôt worry about it .
‚Ä¢MoE-2 R1: i ca n‚Äôt believe you did n‚Äôt see that coming . you do n‚Äôt want to lose this !
‚Ä¢MoE-2 R2: what ? you ‚Äôre not going to kill him .
‚Ä¢MoE-2 R3: now what ? we ‚Äôre going to have to do something
‚Ä¢MoE-3 R1: because you do n‚Äôt know what you are doing .
‚Ä¢MoE-3 R2: oh , you do n‚Äôt know . i guess it ‚Äôs just not true .
‚Ä¢MoE-3 R3: and what the hell is that ?
‚Ä¢MoE-4 R1: he ‚Äôll kill you ! motherfucker ! !
‚Ä¢MoE-4 R2: see , i told you he was gon na kill us all .
‚Ä¢MoE-4 R3: and you do n‚Äôt care what it says ?
B.9 Sample Utterances for EXP 2: Sadness Emotion Intent
B.9.1 Reddit Casual
1.Sample Conversation: (...) besides every anime intro and outro song ever at least from the
series i like , i ‚Äôve been getting a kick out of j rock in general .
‚Ä¢MoE-1 R1: i got this feeling of thinking about this year and i can relate .
‚Ä¢MoE-1 R2: i got this feeling i can relate to this song a bit .
39

--- PAGE 40 ---
‚Ä¢MoE-1 R3: i got this song a lot , and i can relate to it .
‚Ä¢MoE-2 R1: i really miss the music i was listening to it .
‚Ä¢MoE-2 R2: i have n‚Äôt heard of it for a long time .
‚Ä¢MoE-2 R3: i ‚Äôm not a fan of anime at all .
‚Ä¢MoE-3 R1: i got deep into idol music this year so i can relate a bit .
‚Ä¢MoE-3 R2: i got very deep into this show so i can relate with this most .
‚Ä¢MoE-3 R3: i got very into Ô¨Ålm school this year so i can relate with you .
‚Ä¢MoE-4 R1: no i would n‚Äôt have watched anime or video .
‚Ä¢MoE-4 R2: i only watched it in japanese and i was at a time .
‚Ä¢MoE-4 R3: i feel like i m just listening to the same music again .
2.Sample Conversation: (...) i got deep into idol music this year so i can relate a bit .
‚Ä¢MoE-1 R1: i ‚Äôm not the most interested in this movie .
‚Ä¢MoE-1 R2: i ‚Äôve been listening to the whole genre of the past .
‚Ä¢MoE-1 R3: i ‚Äôve been listening to a lot of people from the past few years .
‚Ä¢MoE-2 R1: i really miss the music i ‚Äôve listened to before
‚Ä¢MoE-2 R2: i feel like i ‚Äôve been listening to that music lately .
‚Ä¢MoE-2 R3: i hate reddit and have no idea what it is
‚Ä¢MoE-3 R1: i was also a huge fan of music so i do n‚Äôt listen to them tv .
‚Ä¢MoE-3 R2: i was very confused on my youtube was bc the record is bothering me .
‚Ä¢MoE-3 R3: i ‚Äôm also very into Ô¨Ålm .
‚Ä¢MoE-4 R1: no but i only listen to rap .
‚Ä¢MoE-4 R2: i just watched anime and manga japan was probably the worst .
‚Ä¢MoE-4 R3: i have n‚Äôt heard anyone like anime .
3.Sample Conversation: (...) some people have the same username across multiple websites
such as steam or xbox .
‚Ä¢MoE-1 R1: i think it ‚Äôs because people who use the internet , they do n‚Äôt use them
anymore .
‚Ä¢MoE-1 R2: i think that ‚Äôs the case with the people who have their own knowledge of
their own .
‚Ä¢MoE-1 R3: i think people are just too used to it in their 30s or at their own cult .
‚Ä¢MoE-2 R1: i ‚Äôve seen people get downvoted and it ‚Äôs all weird .
‚Ä¢MoE-2 R2: i even feel like it ‚Äôs true for everyone .
‚Ä¢MoE-2 R3: i ‚Äôve heard people are n‚Äôt on reddit .
‚Ä¢MoE-3 R1: you might also be able to get a hold of their own place without similar
type places .
‚Ä¢MoE-3 R2: you might also be able to get a hold of them within the same price or not .
‚Ä¢MoE-3 R3: you might also be able to get a hold of them within different countries or
whatever they send you .
‚Ä¢MoE-4 R1: i have noticed it on twitter , and facebook . i never have anonymous
enough on twitter
‚Ä¢MoE-4 R2: i have n‚Äôt gotten around to it . i am literally wanting to delete my account .
‚Ä¢MoE-4 R3: i do n‚Äôt know why people are so dumb and useless .
40

--- PAGE 41 ---
B.10 Sample Utterances for EXP 3 (with Full MoE Decision Flow)
B.10.1 Reddit Casual, MoE-4 MBRL
1.Turn 0: (...) when i actually look back at it , it ‚Äôs not bad .
‚Ä¢MoE Primitive 1: oh god those words mean a lot in general . Q-value: 0.15
‚Ä¢MoE Primitive 2: when i get a good amount of things i put in the mood . Q-value:
-0.32
‚Ä¢MoE Question 1: i have n‚Äôt listened to music , but i played a few songs , what ‚Äôs it ?
Q-value: -0.38
‚Ä¢MoE Question 2: how much is it ? Q-value: -0.41
‚Ä¢MoE Exploration 1: and that ‚Äôs just me being the only way i can discover a life .
Q-value: -0.26
‚Ä¢MoE Exploration 2: i ‚Äôm 22 and have n‚Äôt hugged a girl yet forget about kissing .
Q-value: -0.55
‚Ä¢MoE Positive Sentiment 1: thank you for the amazing reply , i deÔ¨Ånitely look at it to
a high day ! Q-value: 0.45
‚Ä¢MoE Positive Sentiment 2: good for you , i really love the genre so things i consider
a best with as my life , it was a very difÔ¨Åcult decision Q-value: 0.05
‚Ä¢MoE Negative Sentiment 1: but on my own , i just do n‚Äôt see the appeal anymore
with how it is . Q-value: 0.05
‚Ä¢MoE Negative Sentiment 2: i hate it when i ‚Äôm slow . Q-value: -0.46
‚Ä¢MoE Sentiment Coherence 1: for me , i have a taste in music so i can not stand a lot
of art or without it i would be so tired Q-value: -0.38
‚Ä¢MoE Sentiment Coherence 2: the reason i love it , i have n‚Äôt been in a while , but i
feel like i was in a way that was when i was younger . Q-value: 0.25
‚Ä¢MoE Joy Emotion 1: haha well i ‚Äôm going through a lot of shit . Q-value: -0.25
‚Ä¢MoE Joy Emotion 2: haha yes i have my own one of my favorite things to do .
Q-value: -0.62
‚Ä¢MoE Optimism Emotion 1: good for you , i like the original music , it ‚Äôs a new skill
in painting , some of it , and just like every other Q-value: -0.14
‚Ä¢MoE Optimism Emotion 2: good on you for doing the right thing , just remember
that the past few months , i ‚Äôve been putting it off , and have no idea about music
Q-value: 0.69
‚Ä¢MoE Anger Emotion 1: why do you think you re an adult ? Q-value: -0.16
‚Ä¢MoE Anger Emotion 2: why are you not asking me a question ? Q-value: -0.29
‚Ä¢MoE Sadness Emotion 1: i do n‚Äôt have much of an effect on my mood , what should i
do !Q-value: 0.60
‚Ä¢MoE Sadness Emotion 2: i have n‚Äôt done that yet but i ‚Äôm going to check out .
Q-value: -0.25
‚Ä¢Final Bot Response: Q-value: good on you for doing the right thing , just remember
that the past few months , i ‚Äôve been putting it off and have no idea about music
Q-value: 0.69
‚Ä¢GPT-based Next-user Response: just love these memory of having that moment of
myself and lots to remember the past few years on life of music .
2.Turn 1: (...) just love these memory of having that moment of myself and lots to remember
the past few years on life of music .
‚Ä¢MoE Primitive 1: it has become an actual meme , and then on a cracked up . Q-value:
0.21
‚Ä¢MoE Primitive 2: i just love music , what genre would it be ? Q-value: 0.42
‚Ä¢MoE Question 1: how much do you enjoy now ? Q-value: -0.15
‚Ä¢MoE Question 2: how do you discover new music ? Q-value: 0.08
41

--- PAGE 42 ---
‚Ä¢MoE Exploration 1: but i was thinking of going to be a one , and maybe <unk> my
favorite shows . Q-value: 0.35
‚Ä¢MoE Exploration 2: and it ‚Äôs a good time to focus on something else , like a good
<unk> ‚Äô , <unk> out of the good things . Q-value: -0.43
‚Ä¢MoE Positive Sentiment 1: thanks for the advice , i really hope things turn to someone
who does n‚Äôt have a life and does n‚Äôt deserve it and as much to be Q-value: -0.51
‚Ä¢MoE Positive Sentiment 2: thank you for the inspiration ! Q-value: 0.13
‚Ä¢MoE Negative Sentiment 1: but i hate when i have to do it all . Q-value: 0.18
‚Ä¢MoE Negative Sentiment 2: i am not one of those and i do n‚Äôt know about to be
honest .Q-value: -0.47
‚Ä¢MoE Sentiment Coherence 1: for the future , i love it and remember that song <unk>
would be a very nice place to achieve something like <unk> or goals . Q-value: -0.57
‚Ä¢MoE Sentiment Coherence 2: for me , i love the feeling of wanting to have a good
christmas tree since i was a kid Q-value: -0.77
‚Ä¢MoE Joy Emotion 1: haha , i am enjoying a lot of it , so i personally even know
nothing about it . Q-value: 0.09
‚Ä¢MoE Joy Emotion 2: haha , this is one of my favorite things to listen in the genre .
Q-value: 0.71
‚Ä¢MoE Optimism Emotion 1: good for you , the best of luck in the future ! Q-value:
0.13
‚Ä¢MoE Optimism Emotion 2: for me , the only thing i really like , is the music i like ,
and the way it gives me time to myself and be Q-value: 0.48
‚Ä¢MoE Anger Emotion 1: and what is an example of things you could do ? Q-value:
-0.11
‚Ä¢MoE Anger Emotion 2: and i do n‚Äôt like music , i ‚Äôve felt so far . Q-value: -0.02
‚Ä¢MoE Sadness Emotion 1: i do n‚Äôt know what to do with my knowledge but i have no
idea what it is . Q-value: -0.03
‚Ä¢MoE Sadness Emotion 2: i have no clue why i do that these days . Q-value: -0.31
‚Ä¢Final Bot Response: haha , this is one of my favorite things to listen in the genre .
Q-value: 0.71
‚Ä¢GPT-based Next-user Response: I wonder how how it ‚Äôd go . but i do enjoy them .
they feel live and have the meaning of life .
B.10.2 Reddit Casual, MoE-4 CQL
1.Turn 0: (...) when i actually look back at it , it ‚Äôs not bad .
‚Ä¢MoE Primitive 1: well i ‚Äôm glad you ‚Äôre feeling better . Q-value: 0.38
‚Ä¢MoE Primitive 2: did you get a song that you ‚Äôre listening to ? Q-value: -0.06
‚Ä¢MoE Question 1: what ‚Äôs it worth ? Q-value: 0.35
‚Ä¢MoE Question 2: how did you get out of it ? Q-value: 0.12
‚Ä¢MoE Exploration 1: lately i ‚Äôve been a lot of good things to do . Q-value: -0.55
‚Ä¢MoE Exploration 2: i love this song , and i will be happy with the music . Q-value:
-0.26
‚Ä¢MoE Positive Sentiment 1: i ‚Äôm glad you ‚Äôre able to Ô¨Ånd a great life ! Q-value: 0.40
‚Ä¢MoE Positive Sentiment 2: i hope you have a great day ! Q-value: 0.39
‚Ä¢MoE Negative Sentiment 1: i do n‚Äôt know if it ‚Äôs worth a listen to it . Q-value: 0.22
‚Ä¢MoE Negative Sentiment 2: i do n‚Äôt know if i ‚Äôm going to be a bit stressful . Q-value:
0.17
‚Ä¢MoE Sentiment Coherence 1: i ‚Äôll be able to do something with my own , but i ‚Äôll be
able to Ô¨Ånd a new genre . Q-value: -0.11
‚Ä¢MoE Sentiment Coherence 2: i might try to Ô¨Ånd some new songs that i ‚Äôm not sure if
i ‚Äôm a bit interested in the music . Q-value: 0.20
42

--- PAGE 43 ---
‚Ä¢MoE Joy Emotion 1: i feel so much better now Q-value: -0.45
‚Ä¢MoE Joy Emotion 2: i want to do that myself . Q-value: -0.22
‚Ä¢MoE Optimism Emotion 1: be proud of yourself and happy for the Ô¨Årst time i ‚Äôve
been able to make a great life . Q-value: 0.29
‚Ä¢MoE Optimism Emotion 2: be proud of yourself and be happy for the Ô¨Årst time i ‚Äôve
been to a great life . Q-value: 0.09
‚Ä¢MoE Anger Emotion 1: it ‚Äôs so much easier to say than done ! Q-value: -0.50
‚Ä¢MoE Anger Emotion 2: i need to start doing something productive , but i ‚Äôm not
doing anything . Q-value: -0.11
‚Ä¢MoE Sadness Emotion 1: i just started a year ago and i ‚Äôm not sure what to do .
Q-value: 0.16
‚Ä¢MoE Sadness Emotion 2: i ‚Äôve been trying to Ô¨Ånd some new songs that are n‚Äôt a thing
.Q-value: 0.12
‚Ä¢Final Bot Response: i ‚Äôm glad you ‚Äôre able to Ô¨Ånd a great life ! Q-value: 0.40
‚Ä¢GPT-based Next-user Response: yes , life feels young and happy again .
2.Turn 1: (...) yes , life feels young and happy again .
‚Ä¢MoE Primitive 1: i feel the same way . Q-value: 0.061
‚Ä¢MoE Primitive 2: i ‚Äôve been through a lot of pain , but i do n‚Äôt know how to do it .
Q-value: -0.045
‚Ä¢MoE Question 1: how do you feel about it ? Q-value: -0.35
‚Ä¢MoE Question 2: i can understand that . how do you cope with this ? Q-value: 0.03
‚Ä¢MoE Exploration 1: but that ‚Äôs how i feel about it and i ‚Äôm not going to be able to
enjoy it .Q-value: 0.15
‚Ä¢MoE Exploration 2: because it ‚Äôs not a good feeling to be able to do something i
Q-value: -0.56
‚Ä¢MoE Positive Sentiment 1: be proud of yourself and be the best part of life , i ‚Äôm
happy that you ‚Äôre happy . Q-value: 0.85
‚Ä¢MoE Positive Sentiment 2: i hope i can Ô¨Ånd a great life too ! Q-value: 0.65
‚Ä¢MoE Negative Sentiment 1: i do n‚Äôt know if it ‚Äôs going to be the problem . Q-value:
-0.22
‚Ä¢MoE Negative Sentiment 2: yeah i feel like i have a hard time . Q-value: 0.18
‚Ä¢MoE Sentiment Coherence 1: i might try to get into the good stuff i ‚Äôm not going to
do .Q-value: 0.05
‚Ä¢MoE Sentiment Coherence 2: i ‚Äôll be able to Ô¨Ånd the better , but i ‚Äôm not sure if i can
.Q-value: -0.19
‚Ä¢MoE Joy Emotion 1: i just enjoy playing music , and i ‚Äôm happy for myself . Q-value:
0.55
‚Ä¢MoE Joy Emotion 2: just in the mood , i ‚Äôm going to enjoy a bit of it . Q-value: -0.33
‚Ä¢MoE Optimism Emotion 1: just in the mood , i ‚Äôm going to enjoy a bit of it . Q-value:
-0.61
‚Ä¢MoE Optimism Emotion 2: i just enjoy playing music , and i ‚Äôm happy for myself .
Q-value: 0.28
‚Ä¢MoE Anger Emotion 1: i just started a new year ago and i ‚Äôm not a huge fan of the
music .Q-value: -0.43
‚Ä¢MoE Anger Emotion 2: why do you have to do this ? Q-value: -0.59
‚Ä¢MoE Sadness Emotion 1: got any hobbies that i feel like i ‚Äôm alone . Q-value: -0.13
‚Ä¢MoE Sadness Emotion 2: do you have any advice or am i just a little curious about
this ?Q-value: -0.08
‚Ä¢Final Bot Response: be proud of yourself and be the best part of life , i ‚Äôm happy
that you ‚Äôre happy .
43

--- PAGE 44 ---
‚Ä¢GPT-based Next-user Response: thank you i ‚Äôd proud of you ‚Äôve been enjoy life too !
B.10.3 Reddit Casual, MoE-4 DQN
1.Turn 0: (...) when i actually look back at it , it ‚Äôs not bad .
‚Ä¢MoE Primitive 1: oh god those words mean a lot in general too . Q-value: 0.03
‚Ä¢MoE Primitive 2: when i m in a really bad mood , i ll have a look about myself .
Q-value: -0.05
‚Ä¢MoE Question 1: how do you know what ‚Äôs good ? Q-value: -0.25
‚Ä¢MoE Question 2: how in the world do you have all the time ? Q-value: -0.35
‚Ä¢MoE Exploration 1: and that ‚Äôs what i think of when i want to have a good idea .
Q-value: 0.52
‚Ä¢MoE Exploration 2: at least i have a couple of hours of listening to the same tunes .
Q-value: 0.28
‚Ä¢MoE Positive Sentiment 1: thanks for the good things , i ‚Äôve found in 2018 , still ,
and if you like it ‚Äôs a little bit of inspiration , my own memories . Q-value: 0.16
‚Ä¢MoE Positive Sentiment 2: thank you for your recommendations , i deÔ¨Ånitely do n‚Äôt
have a fantastic artist , it ‚Äôs absolutely amazing to me , but i really want to learn more
Q-value: -0.4
‚Ä¢MoE Negative Sentiment 1: but on my mind , i ca n‚Äôt even focus on the music .
Q-value: -0.06
‚Ä¢MoE Negative Sentiment 2: but no i just do n‚Äôt know ... Q-value: 0.55
‚Ä¢MoE Sentiment Coherence 1: thanks for the recommendation i really enjoyed it !
Q-value: -0.04
‚Ä¢MoE Sentiment Coherence 2: that ‚Äôs what i ‚Äôve heard about in the Ô¨Årst place . Q-
value: -0.13
‚Ä¢MoE Joy Emotion 1: not bad , i have all my <unk> on the side and <unk> . Q-value:
-0.18
‚Ä¢MoE Joy Emotion 2: good for you , for me , my best friend is <unk> Q-value: -0.27
‚Ä¢MoE Optimism Emotion 1: good on you for the time , but it ‚Äôs actually a bit of
a hiatus from the genre your get to your development aspect , regardless of songs
Q-value: -0.67
‚Ä¢MoE Optimism Emotion 2: you know what i m missing out on . Q-value: 0.24
‚Ä¢MoE Anger Emotion 1: i do n‚Äôt like the term toxic masculinity , but i think this idea
of pride is accurately described . Q-value: -0.36
‚Ä¢MoE Anger Emotion 2: why do you think the weed smell is a good idea ? Q-value:
-0.64
‚Ä¢MoE Sadness Emotion 1: i do n‚Äôt have much of a high expectation in my life .
Q-value: 0.15
‚Ä¢MoE Sadness Emotion 2: i ‚Äôm no expert on turtles and different types of turtle
species , i just saw an interesting documentary on sea turtles . Q-value: -0.04
‚Ä¢Final Bot Response: but no i just do n‚Äôt know ... Q-value: 0.55
‚Ä¢GPT-based Next-user Response: i agree , just make sure what you want to do and
Ô¨Ågure out how to proud
2.Turn 1: (...) i agree , just make sure what you want to do and Ô¨Ågure out how to proud
‚Ä¢MoE Primitive 1: it has become an actual effect on my mood . Q-value: 0.65
‚Ä¢MoE Primitive 2: but i ‚Äôve discovered that by not giving any fucks about what i want .
Q-value: 0.13
‚Ä¢MoE Question 1: how the fuck do you think about it ? Q-value: -0.06
‚Ä¢MoE Question 2: how are you enjoying life , what you used to do ? Q-value: 0.18
‚Ä¢MoE Exploration 1: that s why i have a favourite meme playlist right . Q-value:
-0.24
44

--- PAGE 45 ---
‚Ä¢MoE Exploration 2: for me , i do n‚Äôt know what a genre is hit these days . Q-value:
-0.34
‚Ä¢MoE Positive Sentiment 1: for me , i love the good drawing and trying to keep it up ,
but it s not a big deal , i like a lot of it Q-value: 0.23
‚Ä¢MoE Positive Sentiment 2: good for you , for me , my best friend is music Q-value:
0.12
‚Ä¢MoE Negative Sentiment 1: i hate to admit it , i just look through the whole world .
Q-value: 0.08
‚Ä¢MoE Negative Sentiment 2: i hate to admit it , i do n‚Äôt know what i like to listen
Q-value: 0.28
‚Ä¢MoE Sentiment Coherence 1: taste is all subjective and i have no matter to taste or
texture other things on the genre . Q-value: -0.55
‚Ä¢MoE Sentiment Coherence 2: although i ‚Äôm on youtube , i enjoy old music . Q-value:
0.17
‚Ä¢MoE Joy Emotion 1: good for you , there s really no way to do it , but i m in the
mood for it , in my day Q-value: -0.23
‚Ä¢MoE Joy Emotion 2: good on you for the time , i think it ‚Äôs a little complicated haha
Q-value: 0.45
‚Ä¢MoE Optimism Emotion 1: good one , i have an amazing time , but , the little it can
be so challenging , especially to discover the passion . Q-value: 0.75
‚Ä¢MoE Optimism Emotion 2: this is the best recommendation ever , and at a good old
dance moves are n‚Äôt any other , or at the same time Q-value: 0.28
‚Ä¢MoE Anger Emotion 1: no taste in music , but the more <unk> ‚Äô is my favorite song
ever on things . Q-value: -0.48
‚Ä¢MoE Anger Emotion 2: also , i have n‚Äôt watched an entire year of it . Q-value: -0.08
‚Ä¢MoE Sadness Emotion 1: why do i have to not use it ? Q-value: -0.0
‚Ä¢MoE Sadness Emotion 2: i do n‚Äôt know why i stopped putting on so much . Q-value:
0.15
‚Ä¢Final Bot Response: good one , i have an amazing time , but , the little it can be so
challenging , especially to discover the passion . Q-value: 0.75
‚Ä¢GPT-based Next-user Response: i already have a really vivid of memories of those
conversations , thats what i have heard from .
B.10.4 Cornell Movie Corpus, MoE-2 MBRL
1.Turn 0: (...) excuse me
‚Ä¢MoE Primitive 1: you , huh ? is that you ? Q-value: 0.17
‚Ä¢MoE Primitive 2: i ‚Äôll promise you . but i ‚Äôd not like one of them , do n‚Äôt worry .
Q-value: 0.28
‚Ä¢MoE Question 1: what ? what ? because he was worried about ? Q-value: -0.34
‚Ä¢MoE Question 2: what ? oh , i thought maybe you did too ? Q-value: -0.28
‚Ä¢MoE Exploration 1: he asked you for a few bucks ? i want him along , anyway
Q-value: 0.05
‚Ä¢MoE Exploration 2: i do n‚Äôt know . i ‚Äôve been hearing from you since i left . Q-value:
-0.01
‚Ä¢MoE Positive Sentiment 1: he says you ‚Äôre very nice . he has a gun . Q-value: -0.46
‚Ä¢MoE Positive Sentiment 2: that ‚Äôs right . he said that his name was familiar and the
formal negotiation is handled ‚Äô Q-value: 0.18
‚Ä¢MoE Negative Sentiment 1: i do n‚Äôt know . i packed up my shit and ran away to here
.Q-value: -0.22
‚Ä¢MoE Negative Sentiment 2: no , i was n‚Äôt gon na see you tonight . Q-value: -0.59
45

--- PAGE 46 ---
‚Ä¢MoE Sentiment Coherence 1: i do n‚Äôt know what happened . i ‚Äôve never been there
before .Q-value: 0.05
‚Ä¢MoE Sentiment Coherence 2: i do n‚Äôt know . i was with him and you ‚Äôre all i know
is that these years i thought you were going to do it Q-value: -0.35
‚Ä¢MoE Joy Emotion 1: i ‚Äôm glad you got your memory back , father ? Q-value: 0.33
‚Ä¢MoE Joy Emotion 2: haha , it is all good . Q-value: -0.17
‚Ä¢MoE Optimism Emotion 1: i ‚Äôm so sure you do . and he ‚Äôs got lots of friends left
Q-value: -0.07
‚Ä¢MoE Optimism Emotion 2: i was n‚Äôt saying anything . i was singing . Q-value: 0.16
‚Ä¢MoE Anger Emotion 1: What ? you do n‚Äôt pull that ? Q-value: -0.77
‚Ä¢MoE Anger Emotion 2: you did n‚Äôt work for him , did n‚Äôt you ? Q-value: -0.56
‚Ä¢MoE Sadness Emotion 1: he does n‚Äôt like me ? Q-value: -0.15
‚Ä¢MoE Sadness Emotion 2: i ‚Äôm sorry , i was n‚Äôt feeling that way . Q-value: 0.21
‚Ä¢Final Bot Response: i ‚Äôm glad you got your memory back , father ? Q-value: 0.33
‚Ä¢GPT-based Next-user Response: sure yea . you see i should worry nothing and do
exactly as said
2.Turn 1: (...) sure yea . you see i should worry nothing and do exactly as said
‚Ä¢MoE Primitive 1: i went down to meet the president . Q-value: -0.14
‚Ä¢MoE Primitive 2: he ‚Äôs the one i told him you . Q-value: 0.08
‚Ä¢MoE Question 1: what ? what did you think he wanted to do , then ? Q-value: -0.35
‚Ä¢MoE Question 2: what did you think ? when did you talk about it ? Q-value: -0.05
‚Ä¢MoE Exploration 1: i ‚Äôm a little girl . if you do n‚Äôt get me a girlfriend , maybe Ô¨Åve
bucks .Q-value: -0.20
‚Ä¢MoE Exploration 2: it was like tommy was possessed or something . there was a hate
in his eyes that i could n‚Äôt really call human . Q-value: -0.41
‚Ä¢MoE Positive Sentiment 1: it ‚Äôs so nice to see you again . Q-value: 0.45
‚Ä¢MoE Positive Sentiment 2: it was so cool . you know who ‚Äôs being a jerk . Q-value:
0.05
‚Ä¢MoE Negative Sentiment 1: i do n‚Äôt want to be a fool . you do n‚Äôt want to look ?
Q-value: -0.04
‚Ä¢MoE Negative Sentiment 2: i do n‚Äôt care what you say ! i do n‚Äôt want to Ô¨Åght you .
kinda <unk> . Q-value: -0.46
‚Ä¢MoE Sentiment Coherence 1: no . i think he ‚Äôs in the house . Q-value: -0.43
‚Ä¢MoE Sentiment Coherence 2: i do n‚Äôt know what else you ‚Äôd call it . Q-value: -0.35
‚Ä¢MoE Joy Emotion 1: it ‚Äôs not that bad , it just the way you always wanted it to be , or
did you ?Q-value: 0.33
‚Ä¢MoE Joy Emotion 2: there you go . i like you . Q-value: 0.34
‚Ä¢MoE Optimism Emotion 1: i ‚Äôm sure he ‚Äôs very Ô¨Çattering , birdie , and i ‚Äôm sure there
‚Äôs nothing wrong with that . Q-value: -0.25
‚Ä¢MoE Optimism Emotion 2: he said it ‚Äôs up to us . Q-value: 0.02
‚Ä¢MoE Anger Emotion 1: you do n‚Äôt get paid till you calm down . you ‚Äôre sure you
want to stay ? Q-value: -0.26
‚Ä¢MoE Anger Emotion 2: you say that because you have no idea ? Q-value: -0.31
‚Ä¢MoE Sadness Emotion 1: no , i ‚Äôm surprised what three years ago . i do n‚Äôt know .
Q-value: -0.05
‚Ä¢MoE Sadness Emotion 2: i ‚Äôm sorry , i was n‚Äôt feeling like this , all . Q-value: -0.10
‚Ä¢Final Bot Response: it ‚Äôs so nice to see you again . Q-value: 0.45
‚Ä¢GPT-based Next-user Response: we think about you too , love you ,
46

--- PAGE 47 ---
B.10.5 Cornell Movie Corpus, MoE-2 CQL
1.Turn 0: (...) excuse me
‚Ä¢MoE Primitive 1: you , huh ? is that you ? Q-value: -0.42
‚Ä¢MoE Primitive 2: jesus , not sure . how are you ? Q-value: -0.24
‚Ä¢MoE Question 1: what ? what ? you think he wanted to kill the baby ? Q-value:
-0.091
‚Ä¢MoE Question 2: what ‚Äôs the matter ? Q-value: -0.080
‚Ä¢MoE Exploration 1: i ‚Äôll destroy you , i swear it . Q-value: -0.51
‚Ä¢MoE Exploration 2: he asked you for a few bucks ? do you want him along ? Q-value:
-0.13
‚Ä¢MoE Positive Sentiment 1: sure , i ‚Äôd love to see you again . Q-value: -0.07
‚Ä¢MoE Positive Sentiment 2: it ‚Äôs so beautiful . he was crazy about you . Q-value: 0.37
‚Ä¢MoE Negative Sentiment 1: i do n‚Äôt want to hear it . i know you ‚Äôre talking to him .
Q-value: -0.11
‚Ä¢MoE Negative Sentiment 2: i did n‚Äôt want to get involved . did you ever think to work
with the police station chiefs . Q-value: -0.24
‚Ä¢MoE Sentiment Coherence 1: i do n‚Äôt know what else you ‚Äôd call it . Q-value: -0.26
‚Ä¢MoE Sentiment Coherence 2: i was tired , what is that for ? Q-value: -0.26
‚Ä¢MoE Joy Emotion 1: he ‚Äôs likely to be in love with you . Q-value: -0.24
‚Ä¢MoE Joy Emotion 2: i ‚Äôll be right there . i ‚Äôll have the money , you got that go .
Q-value: 0.25
‚Ä¢MoE Optimism Emotion 1: i ‚Äôm sure you ‚Äôll remember that , in the Ô¨Årst date ?
Q-value: 0.25
‚Ä¢MoE Optimism Emotion 2: i ‚Äôm surprised you do n‚Äôt know . when i was a kid , i live
off the street . Q-value: 0.12
‚Ä¢MoE Anger Emotion 1: you do n‚Äôt sound like it ! Q-value: -0.61
‚Ä¢MoE Anger Emotion 2: you were n‚Äôt attracted to him ? Q-value: -0.11
‚Ä¢MoE Sadness Emotion 1: i ‚Äôm sorry , i do n‚Äôt know how to make it out . Q-value:
-0.17
‚Ä¢MoE Sadness Emotion 2: i ‚Äôm sorry , i do n‚Äôt know what to say . Q-value: 0.05
‚Ä¢Final Bot Response: it ‚Äôs so beautiful . he was crazy about you . Q-value: 0.37
‚Ä¢GPT-based Next-user Response: i ‚Äôm glad to know . but tonight mr. wade may not
come here .
2.Turn 1: (...) i ‚Äôm glad to know . but tonight mr. mel may not come here .
‚Ä¢MoE Primitive 1: certainly , when i get to go up to your room . Q-value: -0.33
‚Ä¢MoE Primitive 2: no . do n‚Äôt even be serious . but i ‚Äôm telling me the truth . Q-value:
-0.21
‚Ä¢MoE Question 1: what do you think he got to lose ? Q-value: -0.06
‚Ä¢MoE Question 2: then what ‚Äôs he talking about ? i ‚Äôm sorry . Q-value: -0.10
‚Ä¢MoE Exploration 1: i just want to hear the details ... Q-value: 0.39
‚Ä¢MoE Exploration 2: i do n‚Äôt want to hear you anymore . you made love ? Q-value:
-0.09
‚Ä¢MoE Positive Sentiment 1: yes . i can live here with you Q-value: 0.25
‚Ä¢MoE Positive Sentiment 2: right , youth . that ‚Äôs why i need you to lead the troops .
Q-value: -0.45
‚Ä¢MoE Negative Sentiment 1: i do n‚Äôt know . he fell for you . Q-value: -0.35
‚Ä¢MoE Negative Sentiment 2: i do n‚Äôt blame him . what happened ? Q-value: 0.01
‚Ä¢MoE Sentiment Coherence 1: but you did n‚Äôt think about it . what ‚Äôs the one said .
Q-value: -0.08
47

--- PAGE 48 ---
‚Ä¢MoE Sentiment Coherence 2: i do n‚Äôt know . i thought i might . but i ‚Äôm sure he ‚Äôs
quite human . Q-value: -0.06
‚Ä¢MoE Joy Emotion 1: it ‚Äôs just you know , i mean . it ‚Äôs been a pleasure . Q-value:
‚Ä¢MoE Joy Emotion 2: i ‚Äôm sure he ‚Äôs the right person . Q-value:
‚Ä¢MoE Optimism Emotion 1: yeah , i know . it ‚Äôs been so long . Q-value: -0.18
‚Ä¢MoE Optimism Emotion 2: i ‚Äôm sure you have n‚Äôt done anything . Q-value: -0.22
‚Ä¢MoE Anger Emotion 1: what ? i do n‚Äôt know what else ? Q-value: 0.13
‚Ä¢MoE Anger Emotion 2: you do n‚Äôt have to prove it ? Q-value:
‚Ä¢MoE Sadness Emotion 1: i was n‚Äôt aware of that . but this is the beauty of it . Q-value:
0.55
‚Ä¢MoE Sadness Emotion 2: clark . i would n‚Äôt want to come in tonight . Q-value:
-0.24
‚Ä¢Final Bot Response: i was n‚Äôt aware of that . but this is the beauty of it . Q-value:
0.55
‚Ä¢GPT-based Next-user Response: i ‚Äôm surprised you do n‚Äôt know . he said some
good thing about you too
B.10.6 Cornell Movie Corpus, MoE-2 DQN
1.Turn 0: (...) excuse me
‚Ä¢MoE Primitive 1: i ‚Äôll promise you . but i do n‚Äôt have any new evidence . Q-value:
-0.19
‚Ä¢MoE Primitive 2: no , it ‚Äôs not the same thing . you do n‚Äôt have the same . Q-value:
-0.26
‚Ä¢MoE Question 1: what ? what do you think happened here ? Q-value: 0.45
‚Ä¢MoE Question 2: what did you think ? i ‚Äôm going to tell you what . it ‚Äôs over , boy .
Q-value: 0.11
‚Ä¢MoE Exploration 1: it ‚Äôs a lot of money for you to do . you know , i mean , you know
, i ‚Äôm not going to do that Q-value: 0.04
‚Ä¢MoE Exploration 2: it ‚Äôs the only way i can do it . i ‚Äôm not going to be a writer .
Q-value: 0.05
‚Ä¢MoE Positive Sentiment 1: yes . it ‚Äôs very nice . Q-value: -0.20
‚Ä¢MoE Positive Sentiment 2: yes , it ‚Äôs true . Q-value: -0.32
‚Ä¢MoE Negative Sentiment 1: i do n‚Äôt want to be here . i ‚Äôm not in love with you .
Q-value: -0.25
‚Ä¢MoE Negative Sentiment 2: do you want to be sure ? i want to know what you want .
Q-value: -0.16
‚Ä¢MoE Sentiment Coherence 1: i do n‚Äôt know what you ‚Äôre talking about . Q-value:
-0.15
‚Ä¢MoE Sentiment Coherence 2: you do n‚Äôt have to be a cop . you ‚Äôre not going to be
there .Q-value: 0.03
‚Ä¢MoE Joy Emotion 1: i ‚Äôm so sure you could have been anything ? Q-value: -0.28
‚Ä¢MoE Joy Emotion 2: it was you , was n‚Äôt it ? i ‚Äôve seen your show . Q-value: 0.11
‚Ä¢MoE Optimism Emotion 1: it ‚Äôs not your fault . Q-value: 0.15
‚Ä¢MoE Optimism Emotion 2: that ‚Äôs all right , you ‚Äôre gon na be Ô¨Åne . Q-value: -0.51
‚Ä¢MoE Anger Emotion 1: you do n‚Äôt pull that kick out your ass , you ‚Äôll try ? Q-value:
-0.26
‚Ä¢MoE Anger Emotion 2: you do n‚Äôt have to do this to me ! Q-value: -0.53
‚Ä¢MoE Sadness Emotion 1: i ‚Äôm sorry , i did n‚Äôt know it . all i know is he did n‚Äôt get to
be here Ô¨Årst . Q-value: 0.23
‚Ä¢MoE Sadness Emotion 2: i ‚Äôm sorry to trouble you , but i have to . Q-value: 0.15
48

--- PAGE 49 ---
‚Ä¢Final Bot Response: what ? what do you think happened here ? Q-value: 0.45
‚Ä¢GPT-based Next-user Response: jesus , not sure . huh ? is that you ?
2.Turn 1: (...) jesus , not sure . huh ? is that you ?
‚Ä¢MoE Primitive 1: we ran up against a platoon of soldiers . there were actually walking
dead .Q-value: -0.05
‚Ä¢MoE Primitive 2: no . it ‚Äôs him , but he did n‚Äôt make the choice . Q-value: 0.87
‚Ä¢MoE Question 1: what ? what would you think ? i ‚Äôm going to be here . i ‚Äôve never
seen this guy . Q-value: 0.75
‚Ä¢MoE Question 2: what did you think ? when did you talk about it ? Q-value: 0.18
‚Ä¢MoE Exploration 1: that ‚Äôs all i ever wanted . a man with the world without a different
girl from one , desert one ‚Äôs a dead , and later . Q-value: -0.51
‚Ä¢MoE Exploration 2: i dunno ... police . Q-value: -0.02
‚Ä¢MoE Positive Sentiment 1: it was like , was n‚Äôt it ? you ‚Äôre far too good . Q-value:
-0.44
‚Ä¢MoE Positive Sentiment 2: it ‚Äôs so nice to see you again . Q-value: -0.56
‚Ä¢MoE Negative Sentiment 1: oh , i do n‚Äôt care . but it ‚Äôs got to be different . Q-value:
-0.11
‚Ä¢MoE Negative Sentiment 2: i do n‚Äôt want to hurt you . he ‚Äôs not trying to get involved
.Q-value: -0.30
‚Ä¢MoE Sentiment Coherence 1: no . i was just going to say , you know . Q-value:
-0.45
‚Ä¢MoE Sentiment Coherence 2: i do n‚Äôt know what else you ‚Äôd call it . Q-value: -0.53
‚Ä¢MoE Joy Emotion 1: i can be sure of that when you want your opinion . Q-value:
-0.07
‚Ä¢MoE Joy Emotion 2: it was so obvious . i mean , all of nicky ‚Äôs half assed mechanics
, they were real signal happy . Q-value: 0.28
‚Ä¢MoE Optimism Emotion 1: i ‚Äôm sure he ‚Äôs very Ô¨Çattering , birdie , and i ‚Äôm sure there
‚Äôs nothing wrong with that . Q-value: -0.15
‚Ä¢MoE Optimism Emotion 2: there you go . that ‚Äôs the truth . Q-value: 0.08
‚Ä¢MoE Anger Emotion 1: you do n‚Äôt like him , do you ? Q-value: -0.07
‚Ä¢MoE Anger Emotion 2: he ‚Äôs sleeping in the back , does n‚Äôt he ? Q-value: -0.31
‚Ä¢MoE Sadness Emotion 1: no i was right behind you , but you ‚Äôre too dumb to turn
around .Q-value: 0.07
‚Ä¢MoE Sadness Emotion 2: i do n‚Äôt know , i ‚Äôve seen him . that ‚Äôs the name of a man i
thought .Q-value: -0.25
‚Ä¢Final Bot Response: no . it ‚Äôs him , but he did n‚Äôt make the choice . Q-value: 0.87
‚Ä¢GPT-based Next-user Response: that left me a surprise . did he say something ?
49
