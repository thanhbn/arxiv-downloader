# 2402.00396.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2402.00396.pdf
# Kích thước tệp: 925174 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Khám phá Hiệu quả cho LLMs
Vikranth Dwaracherla1, Seyed Mohammad Asghari1, Botao Hao1và Benjamin Van Roy1, 2
1Google DeepMind,1,2Đại học Stanford
Chúng tôi trình bày bằng chứng về lợi ích đáng kể từ việc khám phá hiệu quả trong thu thập phản hồi của con người để cải thiện các mô hình ngôn ngữ lớn. Trong các thí nghiệm của chúng tôi, một tác nhân tuần tự tạo ra các truy vấn trong khi khớp một mô hình phần thưởng với phản hồi nhận được. Tác nhân hoạt động tốt nhất của chúng tôi tạo ra các truy vấn sử dụng lấy mẫu Thompson kép, với sự không chắc chắn được biểu diễn bởi một mạng neural nhận thức. Kết quả của chúng tôi chứng minh rằng việc khám phá hiệu quả cho phép đạt được mức hiệu suất cao với ít truy vấn hơn nhiều. Hơn nữa, cả việc ước lượng sự không chắc chắn và việc lựa chọn sơ đồ khám phá đều đóng vai trò quan trọng.

1. Giới thiệu
Các mô hình ngôn ngữ lớn thể hiện khả năng đáng chú ý sau khi học từ khối lượng dữ liệu văn bản khổng lồ Anil et al. (2023); Hoffmann et al. (2022); OpenAI (2023). Tuy nhiên, học tăng cường từ phản hồi của con người (RLHF) cải thiện đáng kể hành vi của chúng thậm chí chỉ sau hàng chục nghìn tương tác Bai et al. (2022); Glaese et al. (2022); Ouyang et al. (2022); Stiennon et al. (2020). Việc sử dụng chatbot tạo ra cơ hội thu thập khối lượng phản hồi của con người ngày càng tăng, với mỗi tương tác thúc đẩy các biểu hiện về sự hài lòng hoặc sở thích OpenAI (2022). Tự nhiên là chúng ta tự hỏi những khả năng mới nào có thể xuất hiện với nguồn dữ liệu đang phát triển này. Sự sáng tạo siêu phàm vẫn là một khả năng hấp dẫn.

Với khối lượng ngày càng tăng, có thể suy ra nhiều hơn từ phản hồi của con người. Điều này cho phép hành vi lệch khỏi mô hình được đào tạo trước. Nhưng cho rằng quá trình này chỉ học từ con người, làm thế nào chúng ta có thể hy vọng sự xuất hiện của sự sáng tạo siêu phàm? Có lẽ một kết quả như vậy là có thể vì việc đánh giá dễ hơn việc tổng hợp nội dung mới. Điều này tương tự như cách, đối với một bài toán NP-complete, mặc dù việc giải quyết khó khăn, việc xác minh một lời giải đề xuất lại dễ dàng.

Ví dụ, giả sử một mô hình được đào tạo trước ngoại suy từ dữ liệu đào tạo của nó để tạo ra số lượng lớn - có thể hàng triệu hoặc hàng tỷ - ý tưởng, một trong số đó rất tài tình. Trong khi con người có thể không nghĩ ra ý tưởng đó, việc học từ đủ phản hồi của con người có thể xác định nó từ số lượng lớn các ý tưởng được tạo ra bởi mô hình. Và, dựa trên sự đổi mới này, việc ngoại suy thêm có thể tiếp tục mở rộng ranh giới của sự sáng tạo. Theo cách này, với đủ phản hồi từ con người, một mô hình nên có khả năng tạo ra nội dung mà con người không thể làm được. Nhưng việc thu thập phản hồi cần thiết sẽ mất vài tháng, vài năm hay vài thập kỷ?

Chúng tôi trình bày trong bài viết này bằng chứng về lợi ích to lớn từ việc khám phá tích cực. Bằng việc khám phá tích cực, chúng tôi có nghĩa là việc điều chỉnh các tương tác để khơi gợi phản hồi hữu ích. Cụ thể, kết quả của chúng tôi chứng minh rằng có thể đạt được mức hiệu suất cao với ít phản hồi hơn nhiều. Sự tăng tốc này có thể cho phép sự sáng tạo siêu phàm sớm hơn nhiều, có thể hàng thập kỷ.

Một thực hành phổ biến trong học tăng cường từ phản hồi của con người (RLHF) là gửi các truy vấn, mỗi truy vấn bao gồm một lời nhắc và một cặp phản hồi khác biệt, đến những người đánh giá con người. Mỗi người đánh giá thể hiện sở thích cho một phản hồi hơn phản hồi kia. Các lời nhắc được lấy từ một tập hợp, trong khi các phản hồi được tạo ra bởi mô hình ngôn ngữ lớn. Khi quá trình này tiến triển, một mô hình phần thưởng được khớp với dữ liệu và điều hướng các phản hồi tiếp theo để phù hợp với phản hồi đã nhận được cho đến nay.

Trong bài viết này, chúng tôi hạn chế chú ý vào loại tương tác nói trên, trong đó mỗi truy vấn bao gồm một lời nhắc và cặp phản hồi khác biệt. Chúng tôi gọi thực hành chuẩn của việc lấy mẫu mỗi

--- TRANG 2 ---
Khám phá Hiệu quả cho LLMs
cặp phản hồi sử dụng mô hình ngôn ngữ là khám phá thụ động. Chúng tôi so sánh hiệu suất của khám phá thụ động với một số thuật toán khám phá tích cực. Một thuật toán là khám phá Boltzmann, có xu hướng chọn các phản hồi với phần thưởng dự đoán cao hơn. Chúng tôi cũng thử hai phương pháp tận dụng ước lượng không chắc chắn được cung cấp bởi một mạng neural nhận thức (ENN). Phương pháp đầu tiên, mà chúng tôi gọi là infomax, chọn một cặp phản hồi với mục đích tối đa hóa thông tin được tiết lộ bởi phản hồi. Điều này thuộc về bộ sưu tập các thuật toán được sử dụng rộng rãi nhằm tối đa hóa việc tăng thông tin (xem, ví dụ, Houthooft et al. (2016); MacKay (1992); Sadigh et al. (2018); Sun et al. (2011)). Phương pháp thứ hai, gọi là lấy mẫu Thompson kép Wu và Liu (2016), lấy mẫu các phản hồi theo xác suất chúng là tối ưu. Các thuật toán khám phá này sẽ được mô tả chính xác hơn trong Phần 4.

Hình 1 so sánh kết quả thực nghiệm được tạo ra bằng các thuật toán khám phá khác nhau. Các thí nghiệm tạo ra những kết quả này được mô tả trong Phần 5. Mỗi điểm được vẽ tương ứng với một mức hiệu suất đạt được. Tọa độ ngang xác định số lượng truy vấn mà double TS cần để đạt được mức hiệu suất đó, trong khi tọa độ dọc xác định số lượng cần thiết bởi một phương án thay thế. Biểu đồ cho khám phá thụ động rõ ràng chứng minh rằng khám phá tích cực sử dụng double TS giảm đáng kể số lượng truy vấn cần thiết để đạt được mức hiệu suất cao.

Khám phá Boltzmann hoạt động tốt nhất trong số các thuật toán chúng tôi thử chỉ sử dụng mô hình phần thưởng ước lượng điểm, không có ước lượng không chắc chắn. Biểu đồ cho Boltzmann chứng minh rằng ước lượng không chắc chắn, như được sử dụng bởi double TS, cho phép cải thiện đáng kể. Cuối cùng, biểu đồ cho infomax cho thấy cách, ngay cả trong số các thuật toán đã được thử nghiệm và kiểm tra tận dụng ước lượng không chắc chắn, việc lựa chọn thuật toán khám phá có thể dẫn đến sự khác biệt hiệu suất lớn.

Hình 1|Các truy vấn cần thiết bởi double TS so với các phương án thay thế để đạt được các mức hiệu suất khác nhau.

Mặc dù theo hiểu biết của chúng tôi, đây là những kết quả đầu tiên chứng minh lợi ích đáng kể từ khám phá tích cực trong việc điều chỉnh các mô hình ngôn ngữ lớn, chúng dựa trên lịch sử công việc lâu dài liên quan đến các thuật toán khám phá Lattimore và Szepesvári (2020). Cụ thể, bài toán của chúng tôi là một thể hiện của contextual dueling bandit Dudík et al. (2015); Saha (2021); Yue et al. (2012) và các thuật toán của chúng tôi dựa trên các sơ đồ tìm kiếm thông tin Hennig và Schuler (2012); Houthooft et al. (2016); MacKay (1992); Russo và Van Roy (2014); Ryzhov et al. (2012); Sadigh et al. (2018); Sun et al. (2011) và lấy mẫu Thompson Russo et al. (2018); Thompson (1933); Wu và Liu (2016). Hơn nữa, nỗ lực của chúng tôi tiếp tục một dòng công việc đã mở rộng các thuật toán khám phá hiệu quả đến các môi trường ngày càng phức tạp sử dụng mạng neural Badia et al. (2020); Bellemare et al. (2016); Burda et al. (2018); Dwaracherla et al. (2020); Lu và Van Roy (2017); Osband et al. (2016, 2019, 2023b); Ostrovski

--- TRANG 3 ---
Khám phá Hiệu quả cho LLMs
et al. (2017); Riquelme et al. (2018); Zhang et al. (2020); Zhou et al. (2020).

2. Quy trình Thí nghiệm
Chúng tôi bắt đầu bằng việc trình bày quy trình thí nghiệm mà chúng tôi sử dụng để nghiên cứu các thuật toán khám phá. Quy trình này dựa trên các công cụ hiện có, bao gồm các tập dữ liệu Anthropic Bai et al. (2022) và các mô hình ngôn ngữ được đào tạo trước Gemini Nano và Gemini Pro Team et al. (2023). Nó sử dụng một trình mô phỏng phản hồi của con người, tạo ra phản hồi với mỗi truy vấn một biểu hiện nhị phân về sở thích giữa các phản hồi. Quy trình bao gồm hai phần: một quy trình học tập và một quy trình đánh giá. Phần trước quản lý giao diện giữa tác nhân và trình mô phỏng phản hồi của con người trong quá trình truy vấn tuần tự và học tập. Phần sau quản lý giao diện giữa mô hình ngôn ngữ được đào tạo trước, mô hình tạo phản hồi mới và trình mô phỏng phản hồi của con người trong quá trình đánh giá hiệu suất tương đối.

Một tác nhân học tuần tự từ phản hồi đến các truy vấn, mỗi truy vấn bao gồm một lời nhắc và hai phản hồi thay thế. Như được minh họa trong Hình 2, mỗi truy vấn được tác nhân tạo ra và trình bày cho một trình mô phỏng sở thích của con người, trình này chỉ ra sở thích nhị phân giữa hai phản hồi. Trong mỗi epoch tương tác, tác nhân truyền một lô 𝐵 truy vấn và nhận được 𝐵 bit phản hồi. Mỗi lời nhắc được lấy mẫu đồng nhất từ tập dữ liệu đào tạo Anthropic Helpfulness Base. Mỗi tác nhân chúng tôi nghiên cứu, khi được trình bày một lời nhắc, tạo ra cặp phản hồi của nó bằng cách đầu tiên tạo ra 𝑁 ứng viên sử dụng mô hình Gemini Nano và sau đó áp dụng một thuật toán khám phá chọn hai từ số 𝑁 này.

Sơ đồ khám phá truy cập một mô hình phần thưởng được đào tạo trên các truy vấn và phản hồi đã quan sát cho đến nay. Mỗi tác nhân chúng tôi xem xét được phân biệt bởi thuật toán khám phá của nó và kiến trúc cũng như thuật toán đào tạo tạo ra mô hình phần thưởng của nó. Trong một số tác nhân chúng tôi xem xét, mô hình phần thưởng có dạng của một mạng neural nhận thức, cung cấp cho thuật toán khám phá quyền truy cập vào ước lượng không chắc chắn ngoài ước lượng điểm của phần thưởng. Mỗi mô hình phần thưởng dựa trên thân của mô hình Gemini Nano. Bằng điều này, chúng tôi có nghĩa là mô hình phần thưởng đầu tiên tính toán embedding lớp cuối của mô hình transformer được đào tạo trước và sau đó áp dụng một đầu perceptron đa lớp (MLP). Chúng tôi mở rộng về kiến trúc và thuật toán đào tạo trong Phần 3.

Hình 2|Quy trình truy vấn tuần tự và học tập.

Để mô phỏng cách con người lựa chọn giữa các phản hồi, chúng tôi sử dụng một mô hình phần thưởng chấm điểm mỗi cặp lời nhắc-phản hồi. Đối với mỗi truy vấn, một sở thích được lấy mẫu theo mô hình lựa chọn Bradley-Terry dựa trên điểm số được gán cho hai cặp lời nhắc-phản hồi. Mô hình phần thưởng được sử dụng bởi trình mô phỏng này được khớp với các tập dữ liệu Anthropic, với một kiến trúc tái sử dụng thân của mô hình ngôn ngữ Gemini Pro. Chi tiết thêm được cung cấp trong Phụ lục A. Lưu ý rằng, vì Gemini Pro lớn hơn nhiều so với Gemini Nano, các lựa chọn được thực hiện bởi một mô hình phức tạp hơn nhiều so với mô hình có sẵn cho tác nhân. Sự khác biệt về quy mô này nhằm phản ánh thực tế rằng con người có thể thể hiện hành vi phức tạp hơn so với hành vi được mô hình hóa bởi tác nhân. Thuật toán 1 cung cấp một bản trình bày súc tích về các tương tác - cụ thể, những gì được truyền (tx) và nhận (rx) đến và từ tác nhân và trình mô phỏng - trong quy trình học tập của chúng tôi.

Thuật toán 1 giao diện học tập
đầu vào: prompt_set, agent, feedback_simulator
siêu tham số: 𝐵,𝑇
1:for𝑡 in 1,...,𝑇 do
2:tx đến agent: 𝐵 prompts
3:rx từ agent: hai phản hồi cho mỗi prompt
4:tx đến simulator: 𝐵 queries
5:rx từ simulator: 𝐵 bits phản hồi
6:tx đến agent: 𝐵 bits phản hồi
7:end for

Hình 3 minh họa quy trình đánh giá hiệu suất tác nhân của chúng tôi. Hiệu suất được đo so với mô hình Gemini Nano. Một chuỗi lời nhắc được lấy mẫu từ tập dữ liệu đánh giá Anthropic Helpfulness Base. Đối với mỗi lời nhắc, hai phản hồi được lấy mẫu. Một bởi Gemini Nano và phản hồi khác bởi một mô hình tạo phản hồi mới sử dụng mô hình phần thưởng đã học. Mô hình mới này hoạt động bằng cách lấy mẫu 𝑁 phản hồi sử dụng Gemini Nano và sau đó chọn phản hồi có điểm số cao nhất theo mô hình phần thưởng của tác nhân. Trình mô phỏng sở thích của con người xuất ra xác suất lựa chọn phản hồi của tác nhân so với phản hồi thay thế được tạo ra bởi Gemini Nano. Các xác suất này được tính trung bình trên các lời nhắc, và giá trị trung bình này được gọi là tỷ lệ thắng của tác nhân, vì nó đại diện cho tỷ lệ thời gian mà phản hồi của tác nhân được ưa thích. Lưu ý rằng tỷ lệ thắng cũng có thể được ước lượng bằng cách tính trung bình các chỉ số nhị phân của lựa chọn mô phỏng, mặc dù một số lượng lớn hơn các truy vấn sẽ được yêu cầu để một ước lượng được tạo ra theo cách này hội tụ. Thuật toán 2 cung cấp một bản trình bày súc tích về các tương tác trong giai đoạn đánh giá.

Hình 3|Quy trình đánh giá hiệu suất.

Thuật toán 2 giao diện đánh giá
đầu vào: prompt_set, model1, model2, feedback_simulator
1:for prompt in prompt_set do
2:tx đến models: prompt
3:rx từ models: một phản hồi cho mỗi mô hình
4:tx đến simulator: query (prompt + 2 phản hồi)
5:rx từ simulator: xác suất ưa thích phản hồi 1
6:end for
return trung bình của các xác suất sở thích

--- TRANG 4 ---
Khám phá Hiệu quả cho LLMs
Lưu ý rằng quy trình thí nghiệm của chúng tôi bỏ qua loại phương pháp policy-gradient thường được sử dụng để tối ưu hóa phần thưởng. Thay vào đó, tác nhân của chúng tôi lấy mẫu 𝑁 phản hồi từ mô hình ngôn ngữ cơ sở (Gemini Nano) và chọn từ số đó phản hồi tối đa hóa phần thưởng. Quy trình best-of-𝑁 này phục vụ để xấp xỉ tối ưu hóa dựa trên policy-gradient, nhưng không có các yêu cầu tính toán cồng kềnh của nó. Quy trình best-of-𝑁 cũng thúc đẩy các phân tích minh bạch hơn, vì nó tránh sự phụ thuộc được hiểu kém vào việc điều chỉnh siêu tham số thường cần thiết để có được kết quả hợp lý từ các phương pháp policy gradient. Một phương pháp policy gradient nguyên mẫu tối thiểu hóa một hàm mất mát cân bằng giữa hai mục tiêu: sự tương tự với mô hình ngôn ngữ cơ sở và sự liên kết với phần thưởng. Một siêu tham số vô hướng nhân với thước đo tương tự, tạo ra sự cân bằng giữa các mục tiêu này. Tham số 𝑁 đóng vai trò tương tự trong phương pháp best-of-𝑁. Khi 𝑁 tăng, việc tối đa hóa trên các phản hồi phù hợp chặt chẽ hơn với tác nhân với phần thưởng. Việc điều chỉnh 𝑁 khuyến khích hành vi tác nhân tương tự hơn với mô hình ngôn ngữ cơ sở.

3. Kiến trúc và Đào tạo Mô hình Phần thưởng
Các mô hình phần thưởng hướng dẫn việc lựa chọn phản hồi trong cả giai đoạn học tập và đánh giá của quy trình thí nghiệm. Chúng tôi xem xét hai loại mô hình phần thưởng, mỗi loại được khớp với dữ liệu sở thích đã quan sát. Loại đầu tiên là một ước lượng điểm gán phần thưởng cho mỗi cặp lời nhắc-phản hồi. Loại thứ hai phụ thuộc thêm vào một chỉ số nhận thức. Việc lấy mẫu một chỉ số nhận thức từ một phân phối tham chiếu gây ra tính ngẫu nhiên trong phần thưởng, mô hình hóa sự không chắc chắn nhận thức về phần thưởng. Trong phần này, chúng tôi mô tả các kiến trúc mạng neural và thuật toán đào tạo được sử dụng trong các thí nghiệm của chúng tôi.

Chúng tôi đào tạo các mô hình phần thưởng, mỗi mô hình nhận đầu vào là embedding lớp cuối của mô hình ngôn ngữ Gemini Nano. Như được minh họa trong Hình 4, một phần thưởng được gán cho một cặp lời nhắc-phản hồi bằng cách đầu tiên truyền nó qua thân mô hình ngôn ngữ và sau đó qua một mô hình phần thưởng.

Hình 4|Các mô hình phần thưởng của chúng tôi nhận đầu vào là embedding lớp cuối của mô hình ngôn ngữ Gemini Nano. Một stop gradient ngăn cản việc cập nhật trọng số thân.

3.1. Ước lượng Điểm
Trong kiến trúc của chúng tôi, một mô hình phần thưởng ước lượng điểm có dạng của một perceptron đa lớp feedforward (MLP). Mô hình phần thưởng này nhận đầu vào là embedding lớp cuối của mô hình ngôn ngữ Gemini Nano, mô hình này nhận đầu vào là một cặp lời nhắc-phản hồi (𝑥,𝑦). Mô hình phần thưởng sau đó xuất ra một phần thưởng vô hướng b𝑟𝜃(𝑥,𝑦). Ở đây, 𝜃 là vector các tham số MLP.

Chúng tôi đào tạo các mô hình phần thưởng trên dữ liệu sở thích. Mỗi điểm dữ liệu bao gồm một truy vấn, bao gồm một lời nhắc và cặp phản hồi, và một chỉ báo nhị phân về sở thích giữa các phản hồi. Cho một tập D của các điểm dữ liệu như vậy, để tính toán các tham số MLP, chúng tôi tối ưu hóa hàm mất mát

Lpoint(𝜃|D)=∑︁
(𝑥,𝑦,𝑦′,𝑐)∈Dce(𝑟𝜃(𝑥,𝑦),𝑟𝜃(𝑥,𝑦′),𝑐)+𝜆∥𝜃∥2
2, (1)

--- TRANG 5 ---
Khám phá Hiệu quả cho LLMs
trong đó 𝜆 là cường độ chính quy hóa, 𝑐 chỉ ra lựa chọn hoặc sở thích, và ce(·,·,·) biểu thị mất mát cross entropy:

ce(𝑅,𝑅′,𝑐)=−(1−𝑐)𝑅−𝑐𝑅′+ln(𝑒𝑅+𝑒��′). (2)

Lưu ý rằng khi phản hồi 𝑦 được ưa thích hơn 𝑦′, chỉ báo sở thích 𝑐 là 0 và ngược lại.

3.2. Mạng Neural Nhận thức
Chúng tôi sử dụng các mạng neural nhận thức (ENN) để mô hình hóa sự không chắc chắn nhận thức về phần thưởng (Osband et al., 2023a). Cho tập dữ liệu D, các tham số ENN được thu được bằng cách tối thiểu hóa hàm mất mát

LENN(𝜃|D)=𝜆∥𝜃−˜𝜃∥2+∫
𝑧∈Z𝑝𝑧(𝑑𝑧)L(𝜃|D,𝑧), (3)

trong đó 𝑝𝑧 là phân phối tham chiếu chỉ số nhận thức, ˜𝜃 là vector tham số ban đầu, và

L(𝜃|D,𝑧)=∑︁
(𝑥,𝑦,𝑦′,𝑐)∈Dce(𝑟𝜃(𝑥,𝑦|𝑧),𝑟𝜃(𝑥,𝑦′|𝑧),𝑐).

Để giải thích các đối tượng này, lưu ý rằng với 𝑧 được lấy mẫu từ 𝑝𝑧, hàm phần thưởng 𝑟˜𝜃(·|𝑧) đại diện cho một mẫu từ phân phối tiên nghiệm trên các hàm phần thưởng. Trong hàm mất mát LENN, việc chính quy hóa về ˜𝜃 phục vụ để duy trì một mức độ đa dạng phù hợp trên các chỉ số nhận thức sau khi đào tạo.

3.3. Đào tạo
Để đào tạo mỗi mô hình phần thưởng, chúng tôi duy trì một bộ đệm replay và áp dụng một thuật toán stochastic gradient descent (SGD) đối với các hàm mất mát được mô tả trong Phần 3.1 và 3.2. Cụ thể, vào cuối mỗi epoch tương tác, trong đó tác nhân truyền 𝐵 truy vấn và nhận 𝐵 bit phản hồi, tác nhân chèn 𝐵 điểm dữ liệu kết quả vào một bộ đệm replay FIFO với dung lượng 𝐶. Sau đó, các bước SGD được áp dụng với các minibatch ngẫu nhiên từ bộ đệm replay, với kích thước bước được điều chỉnh bởi ADAM. Mô hình phần thưởng đã được đào tạo được sử dụng để xác định các truy vấn được công thức trong epoch tiếp theo.

4. Thuật toán Khám phá
Bây giờ chúng tôi mô tả tập hợp các thuật toán khám phá được sử dụng trong nghiên cứu thực nghiệm của chúng tôi.

4.1. Khám phá Thụ động
Các hệ thống RLHF hiện tại thường khám phá một cách thụ động, chọn các cặp phản hồi theo Thuật toán 3. Thuật toán này nhận một lời nhắc 𝑥 và một mô hình ngôn ngữ 𝜋 làm đầu vào. Mô hình ngôn ngữ mã hóa một phân phối 𝜋(·|𝑥) từ đó nó lấy mẫu các phản hồi. Thuật toán trả về hai phản hồi được lấy mẫu bởi mô hình ngôn ngữ.

4.2. Khám phá Tích cực với Ước lượng Điểm
Khi chọn một cặp phản hồi, tác nhân có thể sử dụng một mô hình phần thưởng đã được đào tạo trên phản hồi đến tất cả hoặc một số truy vấn trong quá khứ. Khám phá thụ động từ bỏ cơ hội này. Bây giờ chúng tôi xem xét khám phá Boltzmann, sử dụng một mô hình phần thưởng ước lượng điểm, gán phần thưởng 𝑟(𝑥,𝑦) cho mỗi cặp lời nhắc-phản hồi. Điều này tạo thành một dạng khám phá tích cực: các phản hồi được

--- TRANG 6 ---
Khám phá Hiệu quả cho LLMs
Thuật toán 3 khám phá thụ động
đầu vào: 𝑥,𝜋
1:lấy mẫu phản hồi 𝑦∼𝜋(·|𝑥)
2:lặp lại
3:lấy mẫu phản hồi 𝑦′∼𝜋(·|𝑥)
4:cho đến khi 𝑙𝑦′≠𝑦
trả về 𝑦,𝑦′

được điều chỉnh dựa trên phản hồi trong quá khứ, với mục đích thu thập phản hồi tương lai hữu ích hơn so với khám phá thụ động.

Như được trình bày trong Thuật toán 4, ngoài các đầu vào 𝑥 và 𝜋 được sử dụng cho khám phá thụ động, khám phá Boltzmann yêu cầu một mô hình phần thưởng ước lượng điểm 𝑟. Hơn nữa, có hai siêu tham số: một nhiệt độ 𝜏 và một số lượng tập phản hồi 𝑁. Mô hình ngôn ngữ tạo ra 𝑁 phản hồi, và hai được lấy mẫu từ một phân phối Boltzmann với số mũ 𝑟(𝑥,˜𝑦𝑛)/𝜏 được gán cho mỗi phản hồi thứ 𝑛 là ˜𝑦𝑛.

Thuật toán 4 Boltzmann
đầu vào: 𝑥,𝜋,𝑟
siêu tham số: 𝜏,𝑁
1:lấy mẫu phản hồi ˜𝑦1,..., ˜𝑦𝑁∼𝜋(·|𝑥)
2:xác suất 𝑞𝑛=exp(𝑟(𝑥,˜𝑦𝑛)/𝜏)Í𝑁
𝑛′=1exp(𝑟(𝑥,˜𝑦𝑛′)/𝜏),∀𝑛
3:lấy mẫu không thay thế 𝑖,𝑖′∼𝑞
trả về 𝑦𝑖,𝑦𝑖′

Lưu ý rằng thuật toán này khôi phục khám phá thụ động khi nhiệt độ 𝜏 tăng. Mặt khác, khi 𝜏 biến mất, khám phá Boltzmann có xu hướng chọn các phản hồi tối ưu hoặc gần như vậy. Người ta cũng có thể xem xét một sự tổng quát của thuật toán sử dụng hai nhiệt độ khác nhau 𝜏1 và 𝜏2 để chọn hai phản hồi. Sau đó, ví dụ, khi 𝜏1 biến mất và 𝜏2 tăng, phản hồi đầu tiên trở thành tối ưu trong khi phản hồi thứ hai được lấy mẫu đồng nhất. Trong công việc thực nghiệm của chúng tôi, chúng tôi không thấy việc sử dụng các nhiệt độ riêng biệt cải thiện hiệu suất. Hơn nữa, chúng tôi thấy Thuật toán 4 cung cấp hiệu suất tốt nhất trong số nhiều phương án thay thế nhận cùng đầu vào. Điều này cho thấy rằng khám phá Boltzmann chọn các phản hồi gần như tốt nhất có thể hy vọng dựa trên một mô hình phần thưởng ước lượng điểm.

4.3. Khám phá Tích cực với ENN
Tiếp theo chúng tôi xem xét các thuật toán sử dụng một mô hình phần thưởng ENN, trong đó phần thưởng 𝑟(𝑥,𝑦|𝑧) được gán cho mỗi cặp lời nhắc-phản hồi phụ thuộc thêm vào một chỉ số nhận thức. Như đã thảo luận trong Phần 3.2, ENN được đặc trưng bởi mô hình phần thưởng 𝑟 và một phân phối tham chiếu 𝑝. Đối với 𝑥 và 𝑦 cố định, bằng cách lấy mẫu nhiều chỉ số nhận thức từ 𝑝, sự không chắc chắn phần thưởng có thể được xác định từ phương sai giữa các mẫu này.

Infomax (Thuật toán 5) nhận một mô hình phần thưởng ENN làm đầu vào. Giống như khám phá Boltzmann (Thuật toán 4), infomax bắt đầu với mô hình ngôn ngữ tạo ra 𝑁 phản hồi. Sau đó, 𝑀 chỉ số nhận thức được lấy mẫu từ 𝑝. Đối với mỗi cặp phản hồi và mỗi chỉ số nhận thức, ENN gán một xác suất cho sự kiện mà một người đánh giá con người ngẫu nhiên ưa thích phản hồi đầu tiên hơn phản hồi thứ hai. Infomax đánh giá sự không chắc chắn về xác suất này bằng cách tính toán phương sai mẫu trên 𝑀 chỉ số nhận thức. Sau đó, thuật toán chọn cặp phản hồi để tối đa hóa sự không chắc chắn. Trực quan, điều này có thể được coi là tối đa hóa một thước đo về tính thông tin của phản hồi.

--- TRANG 7 ---
Khám phá Hiệu quả cho LLMs
Thuật toán 5 infomax
đầu vào: 𝑥,𝜋,(𝑟,𝑝)
siêu tham số: 𝑁,𝑀
1:lấy mẫu phản hồi ˜𝑦1,..., ˜𝑦𝑁∼𝜋(·|𝑥)
2:lấy mẫu chỉ số 𝑧1,...,𝑧𝑀∼𝑝
3:phần thưởng 𝑅𝑛,𝑚=𝑟(𝑥,˜𝑦𝑛|𝑧𝑚),∀𝑚,𝑛
4:xác suất sở thích 𝑃𝑛,𝑛′,𝑚=𝑅𝑛,𝑚
(𝑅𝑛,𝑚+𝑅𝑛′,𝑚),∀𝑚,𝑛,𝑛′
5:trung bình 𝜇𝑛,𝑛′=Í
𝑚𝑃𝑛,𝑛′,𝑚
𝑀,∀𝑛,𝑛′
6:phương sai 𝜎2
𝑛,𝑛′=Í
𝑚(𝑃𝑛,𝑛′,𝑚−𝜇𝑛,𝑛′,𝑚)2
𝑀−1,∀𝑛,𝑛′
6:(𝑖,𝑖′)∈arg max𝑛,𝑛′𝜎2
𝑛,𝑛′
trả về 𝑦𝑖,𝑦𝑖′

Một hạn chế có thể có của infomax là thuật toán đầu tư vào việc tìm kiếm thông tin về phần thưởng bất kể thông tin đó có hữu ích để chọn các phản hồi tốt nhất hay không. Ví dụ, infomax có thể đầu tư vào việc tinh chỉnh ước lượng phần thưởng được gán cho một phản hồi đã được xác định dựa trên phản hồi trước đó là một lựa chọn kém. Mặt khác, lấy mẫu Thompson kép Wu và Liu (2016) có xu hướng tập trung nhiều hơn vào các truy vấn hữu ích trong việc xác định các phản hồi tốt nhất. Như chúng ta sẽ thấy trong Phần 5, double TS cải thiện hiệu suất của infomax, cũng như khám phá Boltzmann.

Trực quan, double TS (Thuật toán 6) nhằm chọn hai phản hồi mà mỗi phản hồi có một số cơ hội là tối ưu. Giống như Thuật toán 4 và 5, chúng tôi bắt đầu bằng việc lấy mẫu 𝑁 phản hồi. Sau đó, hai trong số 𝑁 phản hồi này được chọn bằng cách lấy mẫu hai chỉ số nhận thức từ 𝑝 và tối đa hóa trên các phần thưởng được quy định bởi mỗi chỉ số. Trong trường hợp các mẫu giống hệt nhau, phản hồi thứ hai được lấy mẫu lại cho đến khi nó khác biệt. Nếu không có sự khác biệt sau 𝐾 lần lặp, phản hồi thứ hai thay vào đó được lấy mẫu đồng nhất.

Thuật toán 6 lấy mẫu Thompson kép
đầu vào: 𝑥,𝜋,(𝑟,𝑝)
siêu tham số: 𝑁,𝐾
1:lấy mẫu phản hồi ˜𝑦1,..., ˜𝑦𝑁∼𝜋(·|𝑥)
2:lấy mẫu chỉ số 𝑧∼𝑝
3:chọn phản hồi 𝑖∈arg max𝑛𝑟(𝑥,˜𝑦𝑛|𝑧)
4:lặp lại
5:lấy mẫu chỉ số 𝑧′∼𝑝
6:chọn phản hồi 𝑖′∈arg max𝑛𝑟(𝑥,˜𝑦𝑛|𝑧′)
7:sau 𝐾 lần thử, thay vào đó lấy mẫu 𝑖′∼unif(1,...,𝑁)
8:cho đến khi 𝑖′≠𝑖
trả về 𝑦𝑖,𝑦𝑖′

5. Kết quả Thực nghiệm
Trong các thí nghiệm của chúng tôi, vào đầu mỗi epoch tương tác, mỗi tác nhân nhận một lô 𝐵=32 lời nhắc và sau đó, đối với mỗi lời nhắc, tạo ra một cặp phản hồi để tạo thành một truy vấn. 𝐵=32 truy vấn của mỗi tác nhân được gửi đến trình mô phỏng sở thích, tạo ra 𝐵=32 bit phản hồi. Mỗi tác nhân chèn lô 𝐵=32 điểm dữ liệu của nó vào bộ đệm replay của nó. Các bộ đệm replay là bộ đệm first-in-first-out (FIFO), với dung lượng tối đa 𝐶=3200 điểm dữ liệu. Nói cách khác, bộ đệm replay chứa dữ liệu sở thích từ tối đa 100 epoch gần đây nhất. Vào cuối mỗi epoch, mỗi tác nhân

--- TRANG 8 ---
Khám phá Hiệu quả cho LLMs
cập nhật mô hình phần thưởng của nó như đã thảo luận trong Phần 3.

Nhớ lại rằng mỗi thuật toán khám phá chọn mỗi cặp phản hồi từ 𝑁 ứng viên được lấy mẫu bởi Gemini Nano. Trong các thí nghiệm của chúng tôi, chúng tôi đặt 𝑁=100. Hiệu suất được đánh giá theo tỷ lệ thắng so với Gemini Nano trên 2048 lời nhắc đánh giá Anthropic Helpfulness base ngoài mẫu, như được giải thích trong Phần 2. Mỗi phản hồi được chọn trong đánh giá này được chọn để có điểm cao nhất trong số 𝑁=100 ứng viên được lấy mẫu bởi Gemini Nano theo mô hình phần thưởng của tác nhân. Lưu ý rằng chúng tôi sử dụng 𝑁=100 phản hồi trong cả quy trình đào tạo và đánh giá của chúng tôi.

Đối với một ước lượng điểm đơn lẻ, chúng tôi sử dụng một perceptron đa lớp feedforward (MLP) bao gồm hai lớp ẩn, với 128 đơn vị ẩn trong mỗi lớp. Như một kiến trúc ENN, chúng tôi sử dụng một tập hợp 𝑆=10 MLP, gọi mỗi MLP riêng lẻ là một hạt. Mỗi hạt của ensemble bao gồm hai lớp ẩn 128 đơn vị. Phân phối tham chiếu 𝑝𝑧 được định nghĩa là phân phối đồng nhất trên {1,2,...,𝑆}. Khi chọn một chỉ số nhận thức 𝑧 được lấy mẫu từ Unif(1,2,...,𝑆), hạt 𝑧 được sử dụng để tạo ra đầu ra cho chỉ số cụ thể đó 𝑧. Hàm mất mát ENN được trình bày trong Phần 3.2 duy trì sự đa dạng trên các hạt bằng cách chính quy hóa mỗi hạt về các tham số ban đầu.

Đối với sơ đồ khám phá Boltzmann, chúng tôi quét qua một số nhiệt độ và thấy rằng nhiệt độ nhỏ tạo ra kết quả tốt nhất. Một mức hiệu suất tương tự đã đạt được bởi một biến thể của sơ đồ Boltzmann chọn một trong các phản hồi một cách tham lam và phản hồi thứ hai sử dụng Boltzmann. Chi tiết thêm có thể được tìm thấy trong Phụ lục D.

Trong trường hợp infomax, chúng tôi sử dụng 30 chỉ số nhận thức để tính toán trung bình và phương sai. Đối với tác nhân double TS, chúng tôi đặt số lần thử tối đa để tạo ra một phản hồi thứ hai khác biệt là 𝐾=30.

Phụ lục B trình bày chi tiết thêm về quá trình lựa chọn siêu tham số của chúng tôi.

5.1. Đánh giá Thuật toán Khám phá
Hình 5 vẽ tỷ lệ thắng của mỗi tác nhân trên các số epoch tương tác khác nhau. Kết quả, thu được bằng cách tính trung bình trên 5 seed ngẫu nhiên, rõ ràng chứng minh rằng khám phá tích cực tăng tốc học tập và dẫn đến tỷ lệ thắng cao hơn. Đáng chú ý, tác nhân double TS nổi lên như người thực hiện hàng đầu.

Chúng tôi quan sát thấy rằng infomax hoạt động rất tốt trong các epoch đầu nhưng sau đó tụt xa double TS. Sự phân kỳ này có thể do xu hướng của infomax trong việc tìm kiếm thông tin, bất kể thông tin đó có hữu ích trong việc chọn các phản hồi mong muốn hay không.

Mỗi đường cong hiệu suất trong Hình 5 dường như hội tụ, trong khi người ta hy vọng sự cải thiện tiếp tục khi khối lượng tương tác con người tăng lên. Khả năng mô hình phần thưởng - có thể được coi một cách lỏng lẻo như số lượng tham số hiệu quả được học từ phản hồi - kiểm soát mức độ cải thiện. Đối với bất kỳ khả năng nào, người ta sẽ mong đợi sự hội tụ khi số lượng truy vấn tăng lên. Việc tăng khả năng cho phép cải thiện thêm với chi phí tính toán tăng. Điều này liên quan đến khái niệm được giải thích bởi Arumugam và Van Roy (2021) rằng có lợi khi điều chỉnh độ phức tạp của một mục tiêu học tập dựa trên thời gian mà một tác nhân dự kiến khám phá.

5.2. Mở rộng với Khối lượng Phản hồi
Hình 1, được tái tạo từ Phần 1 để thuận tiện, vẽ số lượng truy vấn cần thiết bởi các phương án thay thế để phù hợp với hiệu suất của double TS, mà chúng tôi thấy là hiệu quả nhất trong số các thuật toán khám phá chúng tôi xem xét. Mặc dù các biểu đồ không quyết định, chúng tôi nhận ra rằng chúng lõm. Giả sử chúng ta đo lường lợi thế của khám phá hiệu quả theo tỷ lệ phần trăm giảm dữ liệu cần thiết để đạt được bất kỳ mức hiệu suất nhất định nào. Tính lõm của các biểu đồ trong Hình

--- TRANG 9 ---
Khám phá Hiệu quả cho LLMs
Hình 5|Hiệu suất với các thuật toán khám phá thụ động, Boltzmann, infomax và double TS. Chúng ta có thể thấy rằng khám phá tích cực dẫn đến mức hiệu suất tốt hơn nhiều với cùng lượng dữ liệu. Sơ đồ khám phá double TS dẫn đến mức hiệu suất tốt nhất.

Hình 1|Các truy vấn cần thiết bởi double TS so với các phương án thay thế để đạt được các mức hiệu suất khác nhau.

1 ngụ ý rằng, khi quy mô dữ liệu phản hồi của con người tăng lên, lợi thế được mang lại bởi khám phá hiệu quả cũng tăng theo. Đối với mức hiệu suất đạt được bởi 30.000 truy vấn thụ động, double TS giảm yêu cầu dữ liệu một bậc độ lớn. Một khả năng hấp dẫn là, khi số lượng tương tác tăng lên hàng tỷ, khám phá hiệu quả có thể cung cấp hiệu ứng nhân với mức độ lên đến vài bậc độ lớn. Điều này có tiềm năng tăng tốc hàng thập kỷ việc đạt được sự sáng tạo siêu phàm.

5.3. Chất lượng Ước lượng Không chắc chắn
Khám phá Boltzmann hoạt động tốt nhất trong số các thuật toán chúng tôi thử chọn truy vấn dựa trên một mô hình phần thưởng ước lượng điểm. Sự cải thiện lớn được chứng minh bởi double TS được kích hoạt bởi ước lượng không chắc chắn được cung cấp bởi mô hình phần thưởng ENN của chúng tôi.

Chất lượng của ước lượng không chắc chắn có thể được đánh giá theo mất mát nhật ký âm kép dyadic joint negative-log loss

--- TRANG 10 ---
Khám phá Hiệu quả cho LLMs
Hình 6|Marginal nll                  Hình 7|Dyadic joint nll

(NLL) Osband et al. (2022), sử dụng xác suất sở thích. Hình 6 và 7 vẽ marginal và dyadic joint NLL cho các mô hình phần thưởng ước lượng điểm và ENN của chúng tôi, mỗi mô hình được đào tạo trên 40.000 truy vấn. Các biểu đồ này cho thấy rằng, trong khi cả hai mô hình phần thưởng đều tạo ra marginal NLL tương tự, mô hình phần thưởng ENN cung cấp dyadic joint NLL rất thuận lợi. Điều này phục vụ như một kiểm tra tính hợp lý rằng mô hình phần thưởng ENN của chúng tôi thực sự tạo ra ước lượng không chắc chắn có ý nghĩa.

Chúng tôi cũng sử dụng dyadic joint NLL để hướng dẫn việc lựa chọn siêu tham số cho các mô hình phần thưởng ước lượng điểm và ENN được sử dụng bởi các thuật toán khám phá của chúng tôi. Cụ thể, chúng tôi quét qua các ứng viên cho tỷ lệ học tập, đào tạo tác nhân qua nhiều epoch để xác định tỷ lệ học tập tối thiểu hóa dyadic joint NLL.

5.4. Cuộc đời của một Lời nhắc
Kết quả của chúng tôi cho thấy rằng double TS có xu hướng hội tụ về các phản hồi tốt hơn so với các phương án thay thế. Để hiểu cụ thể hơn về cách điều này xảy ra, chúng ta hãy nghiên cứu sự tiến hóa của phần thưởng mà các mô hình gán cho các phản hồi đối với một lời nhắc cụ thể. Để đơn giản hóa cuộc điều tra này, chúng tôi sẽ chỉ so sánh double TS với khám phá Boltzmann.

Nhớ lại rằng chúng tôi thấy khám phá Boltzmann là người thực hiện hàng đầu trong số các thuật toán dựa quyết định trên một mô hình phần thưởng ước lượng điểm. Mặt khác, double TS sử dụng ước lượng không chắc chắn được cung cấp bởi một mô hình phần thưởng ENN. Chúng tôi sẽ kiểm tra các ước lượng liên quan đến một lời nhắc đơn lẻ và hai phản hồi, được chọn từ tập dữ liệu đánh giá. Phản hồi đầu tiên là phản hồi mà double TS đến, trong khi phản hồi thứ hai là phản hồi mà khám phá Boltzmann đến. Trình mô phỏng phản hồi của con người chỉ ra sở thích cho lời nhắc đầu tiên 57,5% thời gian.

Hình 8 vẽ dự đoán được cung cấp bởi mỗi mô hình phần thưởng về xác suất phản hồi đầu tiên được ưa thích. Đường ngang nét đứt thể hiện xác suất 0,575 mà trình mô phỏng phản hồi thể hiện sở thích cho phản hồi đầu tiên. Các dự đoán tiến hóa khi các mô hình phần thưởng học từ các truy vấn. Sau 40.000 truy vấn, double TS đến một dự đoán lớn hơn một nửa, thể hiện sở thích cho phản hồi đầu tiên. Mặt khác, khám phá Boltzmann thể hiện sở thích cho phản hồi thứ hai với một dự đoán nhỏ hơn một nửa.

Cũng được hiển thị trong hình là khoảng tin cậy hai độ lệch chuẩn dựa trên sự không chắc chắn được thể hiện bởi mô hình phần thưởng ENN. Mặc dù double TS tại một số điểm dự đoán nhỏ hơn một nửa, giới hạn trên của khoảng tin cậy của nó vẫn lớn hơn một nửa. Do đó, nó vẫn không chắc chắn về phản hồi nào tốt hơn. Trong việc giải quyết sự không chắc chắn này, nó phục hồi và đến một dự đoán lớn hơn một nửa. Mặt khác, khám phá Boltzmann không được hướng dẫn bởi ước lượng không chắc chắn và do đó không phục hồi từ dự đoán sai lệch của nó.

--- TRANG 11 ---
Khám phá Hiệu quả cho LLMs
Hình 8|Đối với một lời nhắc cụ thể, đường nét đứt chỉ ra xác suất mà trình mô phỏng thể hiện sở thích cho một phản hồi so với phản hồi khác. Ước lượng không chắc chắn cho phép double TS phục hồi từ một dự đoán không chính xác trong khi khám phá Boltzmann thì không.

6. Nhận xét Kết thúc
Theo hiểu biết của chúng tôi, các kết quả chúng tôi đã trình bày đại diện cho những kết quả đầu tiên chứng minh lợi ích đáng kể của khám phá tích cực trong việc điều chỉnh các mô hình ngôn ngữ lớn. Điều đó nói rằng, vẫn còn nhiều chỗ cho công việc tiếp theo trong lĩnh vực này. Để kết thúc bài viết này, chúng tôi thảo luận một số hướng nghiên cứu quan trọng.

Các thí nghiệm của chúng tôi sử dụng một kiến trúc ENN đặc biệt đơn giản bao gồm một ensemble các MLP. Như được chứng minh trong Osband et al. (2023a), các kiến trúc thay thế tạo ra sự đánh đổi hiệu quả hơn giữa yêu cầu tính toán và chất lượng ước lượng không chắc chắn. Hơn nữa, thay vì thiết kế ENN dựa trên MLP, có thể cải thiện hiệu suất, đặc biệt khi lượng dữ liệu phản hồi của con người tăng lên, bằng cách dựa thiết kế ENN trên kiến trúc transformer.

Một hạn chế khác của kiến trúc mô hình phần thưởng của chúng tôi là mỗi mô hình chỉ là một "đầu" nhận embedding lớp cuối của một LLM làm đầu vào. Hiệu suất có thể được cải thiện bằng cách cũng điều chỉnh thân LLM. Trong khi các lợi thế được mang lại bởi khám phá hiệu quả sẽ mở rộng, việc xác định các kiến trúc và thuật toán hiệu quả nhất để khám phá trong khi điều chỉnh nhiều hơn của LLM vẫn còn cho công việc tương lai.

Cuối cùng, khám phá hiệu quả của đối thoại đa lượt trình bày một hướng thú vị và quan trọng cho nghiên cứu tương lai. Trong bài viết này, chúng tôi xem khám phá như một phương tiện để nhanh chóng xác định một phản hồi được coi là mong muốn một cách cô lập. Trong đối thoại đa lượt, các phản hồi có thể được chọn thay vào đó vì cách chúng định hình các tương tác tiếp theo. Chủ đề khám phá sâu giải quyết cách một tác nhân có thể hiệu quả xác định các phản hồi hiệu quả tạo nên các tương tác tuần tự Osband et al. (2016, 2019). Việc tận dụng các thuật toán khám phá sâu để cải thiện đối thoại vẫn là một thách thức.

Tài liệu Tham khảo
R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey, Z. Chen, E. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hellstern, G. Mishra, E. Moreira, M. Omernick, K. Robinson, S. Ruder, Y. Tay, K. Xiao, Y. Xu, Y. Zhang, G. H. Abrego, J. Ahn, J. Austin, P. Barham, J. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta, Y. Cheng, C. Cherry, C. A. Choquette-Choo, A. Chowdhery, C. Crepy, S. Dave, M. Dehghani, S. Dev, J. Devlin, M. Díaz, N. Du, E. Dyer, V. Feinberg, F. Feng, V. Fienber, M. Freitag, X. Garcia, S. Gehrmann, L. Gonzalez, G. Gur-Ari, S. Hand, H. Hashemi, L. Hou, J. Howland, A. Hu, J. Hui, J. Hurwitz, M. Isard, A. Ittycheriah,

--- TRANG 12 ---
Khám phá Hiệu quả cho LLMs
M. Jagielski, W. Jia, K. Kenealy, M. Krikun, S. Kudugunta, C. Lan, K. Lee, B. Lee, E. Li, M. Li, W. Li, Y. Li, J. Li, H. Lim, H. Lin, Z. Liu, F. Liu, M. Maggioni, A. Mahendru, J. Maynez, V. Misra, M. Moussalem, Z. Nado, J. Nham, E. Ni, A. Nystrom, A. Parrish, M. Pellat, M. Polacek, A. Polozov, R. Pope, S. Qiao, E. Reif, B. Richter, P. Riley, A. C. Ros, A. Roy, B. Saeta, R. Samuel, R. Shelby, A. Slone, D. Smilkov, D. R. So, D. Sohn, S. Tokumine, D. Valter, V. Vasudevan, K. Vodrahalli, X. Wang, P. Wang, Z. Wang, T. Wang, J. Wieting, Y. Wu, K. Xu, Y. Xu, L. Xue, P. Yin, J. Yu, Q. Zhang, S. Zheng, C. Zheng, W. Zhou, D. Zhou, S. Petrov, và Y. Wu. Báo cáo kỹ thuật Palm 2, 2023.

D. Arumugam và B. Van Roy. Quyết định học gì: Một phương pháp tỷ lệ-méo. Trong Proceedings of the 38th International Conference on Machine Learning, trang 373–382, 2021.

A. P. Badia, P. Sprechmann, A. Vitvitskyi, D. Guo, B. Piot, S. Kapturowski, O. Tieleman, M. Arjovsky, A. Pritzel, A. Bolt, et al. Không bao giờ bỏ cuộc: Học các chiến lược khám phá có hướng. arXiv preprint arXiv:2002.06038, 2020.

Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. Đào tạo một trợ lý hữu ích và vô hại với học tăng cường từ phản hồi của con người. arXiv preprint arXiv:2204.05862, 2022.

M. Bellemare, S. Srinivasan, G. Ostrovski, T. Schaul, D. Saxton, và R. Munos. Thống nhất khám phá dựa trên đếm và động lực nội tại. Advances in neural information processing systems, 29, 2016.

R. A. Bradley và M. E. Terry. Phân tích Hạng của Thiết kế Khối Không hoàn chỉnh: I. Phương pháp So sánh Cặp. Biometrika, 39(3/4):324–345, 1952.

Y. Burda, H. Edwards, A. Storkey, và O. Klimov. Khám phá bằng chưng cất mạng ngẫu nhiên. arXiv preprint arXiv:1810.12894, 2018.

M. Dudík, K. Hofmann, R. E. Schapire, A. Slivkins, và M. Zoghi. Contextual dueling bandits. Trong Conference on Learning Theory, trang 563–587. PMLR, 2015.

V. Dwaracherla, X. Lu, M. Ibrahimi, I. Osband, Z. Wen, và B. Van Roy. Hypermodels cho khám phá. Trong International Conference on Learning Representations, 2020.

A. Glaese, N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, et al. Cải thiện sự liên kết của các tác nhân đối thoại thông qua phán đoán của con người có mục tiêu. arXiv preprint arXiv:2209.14375, 2022.

X. Glorot và Y. Bengio. Hiểu khó khăn của việc đào tạo mạng neural feedforward sâu. Trong Proceedings of the thirteenth international conference on artificial intelligence and statistics, trang 249–256. JMLR Workshop and Conference Proceedings, 2010.

P. Hennig và C. J. Schuler. Tìm kiếm entropy cho tối ưu hóa toàn cục hiệu quả thông tin. Journal of Machine Learning Research, 13(6), 2012.

J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. Rae, và L. Sifre. Một phân tích thực nghiệm về đào tạo mô hình ngôn ngữ lớn tối ưu tính toán. Trong S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, và A. Oh, biên tập viên, Advances in Neural Information Processing Systems, tập 35, trang 30016–30030. Curran Associates, Inc., 2022.

R. Houthooft, X. Chen, X. Chen, Y. Duan, J. Schulman, F. De Turck, và P. Abbeel. Vime: Khám phá tối đa hóa thông tin biến phân. Trong D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, và R. Garnett, biên tập viên, Advances in Neural Information Processing Systems, tập 29. Curran Associates, Inc., 2016.

--- TRANG 13 ---
Khám phá Hiệu quả cho LLMs
T. Lattimore và C. Szepesvári. Thuật toán Bandit. Cambridge University Press, 2020.

X. Lu và B. Van Roy. Lấy mẫu Ensemble. Trong Proceedings of the 31st International Conference on Neural Information Processing Systems, trang 3260–3268, 2017.

D. J. MacKay. Các hàm mục tiêu dựa trên thông tin cho việc lựa chọn dữ liệu tích cực. Neural computation, 4(4):590–604, 1992.

OpenAI. ChatGPT: Tối ưu hóa Mô hình Ngôn ngữ cho Đối thoại, 2022. URL https://openai.com/blog/chatgpt/.

OpenAI. Báo cáo Kỹ thuật GPT-4. Báo cáo kỹ thuật, OpenAI, 2023.

I. Osband, C. Blundell, A. Pritzel, và B. Van Roy. Khám phá sâu qua DQN bootstrap. Trong D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, và R. Garnett, biên tập viên, Advances in Neural Information Processing Systems, tập 29. Curran Associates, Inc., 2016.

I. Osband, B. Van Roy, D. J. Russo, và Z. Wen. Khám phá sâu qua các hàm giá trị ngẫu nhiên. Journal of Machine Learning Research, 20(124):1–62, 2019.

I. Osband, Z. Wen, S. M. Asghari, V. Dwaracherla, X. Lu, và B. Van Roy. Đánh giá phân phối dự đoán bậc cao trong học sâu. Trong The 38th Conference on Uncertainty in Artificial Intelligence, 2022.

I. Osband, Z. Wen, M. Asghari, V. Dwaracherla, M. Ibrahimi, X. Lu, và B. Van Roy. Mạng neural nhận thức. Advances in Neural Information Processing Systems, 34, 2023a.

I. Osband, Z. Wen, S. M. Asghari, V. Dwaracherla, M. Ibrahimi, X. Lu, và B. Van Roy. Lấy mẫu Thompson xấp xỉ qua mạng neural nhận thức. Trong R. J. Evans và I. Shpitser, biên tập viên, Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence, tập 216 của Proceedings of Machine Learning Research, trang 1586–1595. PMLR, 31 Jul–04 Aug 2023b.

G. Ostrovski, M. G. Bellemare, A. Oord, và R. Munos. Khám phá dựa trên đếm với các mô hình mật độ neural. Trong International conference on machine learning, trang 2721–2730. PMLR, 2017.

L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, và R. Lowe. Đào tạo các mô hình ngôn ngữ để tuân theo hướng dẫn với phản hồi của con người. Trong S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, và A. Oh, biên tập viên, Advances in Neural Information Processing Systems, tập 35, trang 27730–27744. Curran Associates, Inc., 2022.

C. Riquelme, G. Tucker, và J. Snoek. Đối đầu bandit Bayesian sâu: Một so sánh thực nghiệm của mạng Bayesian sâu cho lấy mẫu Thompson. arXiv preprint arXiv:1802.09127, 2018.

D. Russo và B. Van Roy. Học tối ưu hóa qua lấy mẫu theo hướng thông tin. Advances in Neural Information Processing Systems, 27:1583–1591, 2014.

D. J. Russo, B. Van Roy, A. Kazerouni, I. Osband, và Z. Wen. Một Hướng dẫn về Lấy mẫu Thompson. Foundations and Trends ®in Machine Learning, 11(1):1–96, 2018.

I. O. Ryzhov, W. B. Powell, và P. I. Frazier. Thuật toán gradient kiến thức cho một lớp tổng quát các vấn đề học trực tuyến. Operations Research, 60(1):180–195, 2012.

--- TRANG 14 ---
Khám phá Hiệu quả cho LLMs
D. Sadigh, N. Landolfi, S. S. Sastry, S. A. Seshia, và A. D. Dragan. Lập kế hoạch cho xe hơi phối hợp với người: Tận dụng hiệu ứng trên hành động của con người để lập kế hoạch và thu thập thông tin tích cực về trạng thái nội tại của con người. Autonomous Robots (AURO), 42(7):1405–1426, Oct. 2018. ISSN 1573-7527. doi: 10.1007/s10514-018-9746-1.

A. Saha. Thuật toán tối ưu cho bandit sở thích ngữ cảnh ngẫu nhiên. Advances in Neural Information Processing Systems, 34:30050–30062, 2021.

N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, và P. F. Christiano. Học tóm tắt với phản hồi của con người. Advances in Neural Information Processing Systems, 33:3008–3021, 2020.

Y. Sun, F. Gomez, và J. Schmidhuber. Lập kế hoạch để bị ngạc nhiên: Khám phá Bayesian tối ưu trong môi trường động. Trong J. Schmidhuber, K. R. Thórisson, và M. Looks, biên tập viên, Artificial General Intelligence, trang 41–51, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg.

G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, et al. Gemini: một họ các mô hình đa phương thức có khả năng cao, 2023.

W. R. Thompson. Về khả năng một xác suất chưa biết vượt quá xác suất khác theo quan điểm bằng chứng của hai mẫu. Biometrika, 25(3/4):285–294, 1933.

H. Wu và X. Liu. Lấy mẫu Thompson kép cho dueling bandit. Trong D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, và R. Garnett, biên tập viên, Advances in Neural Information Processing Systems, tập 29. Curran Associates, Inc., 2016.

Y. Yue, J. Broder, R. Kleinberg, và T. Joachims. Vấn đề dueling bandit 𝐾-armed. Journal of Computer and System Sciences, 78(5):1538–1556, 2012.

W. Zhang, D. Zhou, L. Li, và Q. Gu. Lấy mẫu Thompson neural. arXiv preprint arXiv:2010.00827, 2020.

D. Zhou, L. Li, và Q. Gu. Bandit ngữ cảnh neural với khám phá dựa trên ucb. Trong International Conference on Machine Learning, trang 11492–11502. PMLR, 2020.

--- TRANG 15 ---
Khám phá Hiệu quả cho LLMs
A. Trình Mô phỏng Sở thích Con người
Chúng tôi sử dụng một mô hình phần thưởng oracle để xây dựng trình mô phỏng sở thích của chúng tôi. Cho một truy vấn, bao gồm một lời nhắc và hai phản hồi tiềm năng, trình mô phỏng sở thích tạo ra phản hồi nhị phân chỉ ra sở thích giữa hai phản hồi bằng cách đầu tiên tính toán điểm số cho mỗi trong hai phản hồi. Để mô phỏng xác suất sở thích từ các điểm số này, chúng tôi sử dụng mô hình Bradley-Terry Bradley và Terry (1952), một phương pháp đã được thiết lập trong phân tích quyết định. Trong khi chúng tôi sử dụng phản hồi nhị phân được lấy mẫu từ xác suất sở thích trong quy trình đào tạo, chúng tôi trực tiếp sử dụng xác suất sở thích trong quy trình đánh giá. Việc sử dụng trực tiếp xác suất sở thích trong quy trình đánh giá của chúng tôi là để giảm tính ngẫu nhiên trong đánh giá.

Mô hình phần thưởng oracle được xây dựng với kiến trúc hai phần, có thân chịu trách nhiệm tạo ra embedding và một lớp đầu tuyến tính tạo ra ước lượng vô hướng. Thân được khởi tạo thành thân transformer Gemini Pro được đào tạo trước, trong khi đầu tuyến tính sử dụng khởi tạo Xavier Glorot và Bengio (2010). Quá trình đào tạo bao gồm tối ưu hóa cả thân và đầu phần thưởng thông qua hàm mất mát cross-entropy được áp dụng cho các tập dữ liệu Anthropic Helpfulness và Harmlessness Bai et al. (2022).

Mô hình phần thưởng oracle của chúng tôi đạt được độ chính xác 0,755 trên Anthropic Helpfulness và 0,748 trên các tập dữ liệu đánh giá Anthropic Harmlessness. Đáng chú ý, mức hiệu suất này cao hơn hiệu suất của các mô hình lớn nhất được báo cáo trong (Bai et al., 2022).

Trình mô phỏng phản hồi của chúng tôi được thiết kế để nắm bắt sự không chắc chắn phát sinh từ cả sự biến đổi giữa các người đánh giá khác nhau và trong các phản hồi của người đánh giá cá nhân. Tuy nhiên, chúng tôi tin rằng nguồn không chắc chắn chủ yếu phát sinh từ sự khác biệt giữa các người đánh giá. Điều này là vì không có khả năng cùng một người đánh giá, khi được trình bày cùng một truy vấn nhiều lần, sẽ chỉ ra sở thích không nhất quán. Trong trình mô phỏng sở thích của chúng tôi, chúng tôi đã quan sát thấy rằng xác suất trung bình của việc có được cùng một nhãn hai lần cho một truy vấn là khoảng 0,62, với độ lệch chuẩn 0,14 trên các truy vấn khác nhau. Điều này cho thấy rằng nhãn sở thích cho một lô truy vấn có thể được coi là được tạo ra bởi các người đánh giá khác nhau. Điều này có nghĩa là khi được đào tạo trên dữ liệu sở thích được gắn nhãn bằng trình mô phỏng sở thích của chúng tôi, nó tương tự như tối ưu hóa cho sở thích của một nhóm người đánh giá thay vì sở thích của một người đánh giá duy nhất.

Lưu ý rằng, vì Gemini Pro lớn hơn nhiều so với Gemini Nano, các lựa chọn được thực hiện bởi một mô hình phức tạp hơn nhiều so với mô hình có sẵn cho tác nhân. Sự khác biệt về quy mô này nhằm phản ánh thực tế rằng con người có thể thể hiện hành vi phức tạp hơn so với hành vi được mô hình hóa bởi tác nhân.

B. Lựa chọn Siêu tham số cho Thí nghiệm
Chúng tôi điều chỉnh các siêu tham số của tác nhân để tối ưu hóa hiệu suất. Các siêu tham số này bao gồm cường độ chính quy hóa l2, tỷ lệ học tập và số bước stochastic gradient descent (SGD) được thực hiện sau mỗi epoch tương tác. Mỗi bước stochastic gradient descent (SGD) trong quá trình đào tạo của chúng tôi được thực hiện trên một lô dữ liệu được lấy mẫu ngẫu nhiên từ bộ đệm replay của tác nhân.

Chúng tôi quét tỷ lệ học tập qua {1𝑒−6,1𝑒−5,1𝑒−4} cho cả mô hình phần thưởng ước lượng điểm và ENN và chọn tỷ lệ học tập tốt nhất. Chúng tôi thấy rằng tỷ lệ học tập tốt nhất nhất quán trên cả các thí nghiệm joint nll của chúng tôi được mô tả trong Phần 5.3 và các thí nghiệm học tập tích cực của chúng tôi.

Để thích ứng với bản chất tiến hóa của quá trình thu thập dữ liệu, chúng tôi thực hiện một chiến lược phân rã cho cường độ chính quy hóa. Cường độ chính quy hóa, được ký hiệu là 𝜆 trong Phương trình 1 và 3, được điều chỉnh theo khối lượng dữ liệu được tích lũy bởi tác nhân. Cụ thể, đối với mỗi bước stochastic gradient descent (SGD) được thực hiện tại mỗi epoch trên một lô bao gồm 𝐵=32 điểm dữ liệu sở thích từ bộ đệm replay, chúng tôi kết hợp một cường độ chính quy hóa phân rã được cho bởi 𝜆=32×𝜆′
|D|, trong đó D đại diện cho tổng số điểm dữ liệu phản hồi được tích lũy bởi tác nhân, và chúng tôi điều chỉnh 𝜆′ bằng cách quét qua {0.1,1,10,100,1000} cho tất cả các tác nhân.

Chúng tôi cũng quét qua số bước sgd được thực hiện sau mỗi epoch tương tác từ {1,10,100}. Chúng tôi quan sát thấy rằng tác nhân infomax có lợi từ việc chạy nhiều bước sgd hơn trong khi hiệu suất của các tác nhân khác giảm sút vượt quá một điểm do over fitting.

Trong trường hợp các mô hình ENN, chúng tôi cũng điều chỉnh tham số quy mô đầu ra, chịu trách nhiệm điều chỉnh sự đa dạng của các hạt ensemble. Cụ thể, chúng tôi quét qua các giá trị {0.1,1,10} và chọn giá trị dẫn đến hiệu suất tốt nhất cho mỗi tác nhân. Lưu ý rằng chúng tôi sử dụng chính quy hóa trọng số về trọng số ban đầu tương tự như Phần 2.1 trong Dwaracherla et al. (2020).

Hơn nữa, chúng tôi cũng điều chỉnh số đơn vị ẩn cho MLP hai lớp trong mô hình ước lượng điểm bằng cách quét qua {128,256,512,1024,2048} trong bối cảnh các thí nghiệm ước lượng không chắc chắn của chúng tôi được chi tiết trong Phần 5.3. Mặc dù khám phá kỹ lưỡng của chúng tôi, chúng tôi quan sát thấy không có sự nâng cao đáng kể nào trong hiệu suất liên quan đến việc tăng số đơn vị ẩn. Do đó, chúng tôi chọn sử dụng 128 đơn vị ẩn cho MLP trên tất cả các kết quả thực nghiệm của chúng tôi được trình bày trong bài viết này.

C. Hiệu suất trên các kích thước ensemble khác nhau
Hình 9 cho thấy hiệu suất của tác nhân lấy mẫu Thompson kép trên các kích thước ensemble khác nhau. Chúng tôi quan sát thấy rằng hiệu suất cải thiện với việc tăng kích thước ensemble; tuy nhiên, vượt quá một ngưỡng nhất định, các cải thiện trở nên krận biên. Cụ thể, sự cải thiện bình ổn xung quanh kích thước ensemble là 10. Do đó, chúng tôi sử dụng kích thước ensemble là 10 trong các thí nghiệm của chúng tôi.

Hình 9|Hiệu suất trên các kích thước ensemble khác nhau

D. Thuật toán Khám phá với Ước lượng Điểm Đơn lẻ
Trong phần này, chúng tôi đánh giá hiệu suất của thuật toán Boltzmann trên các giá trị nhiệt độ khác nhau. Chúng tôi biến đổi tham số nhiệt độ, được ký hiệu là 𝜏, trong sơ đồ khám phá Boltzmann (tham khảo Thuật toán 4). Phạm vi nhiệt độ được khám phá bao gồm 𝜏∈ {1𝑒−4,1𝑒−2,1𝑒−1,0,1,10,100,1000}. Hiệu suất tương ứng của tác nhân Boltzmann dưới các giá trị 𝜏 khác nhau được minh họa trong Hình 10. Đáng chú ý, chúng tôi quan sát hiệu suất tối ưu cho các tác nhân Boltzmann với nhiệt độ nhỏ hơn. Ngoài ra, các phát hiện của chúng tôi khẳng định kỳ vọng rằng các tác nhân Boltzmann với nhiệt độ rất cao thể hiện hiệu suất tương tự như một tác nhân thụ động.

--- TRANG 16 ---
Khám phá Hiệu quả cho LLMs
Hình 10|Biểu đồ này chứng minh hiệu suất của tác nhân Boltzmann trên các nhiệt độ khác nhau.

Chúng tôi cũng thử nghiệm với một biến thể của sơ đồ Boltzmann được gọi là Greedy-Boltzmann, như được nêu trong Thuật toán 7. Trong phương pháp Greedy-Boltzmann, một phản hồi được chọn một cách tham lam, dựa vào mô hình phần thưởng, trong khi việc lựa chọn phản hồi khác tuân theo sơ đồ khám phá Boltzmann. Về mặt khái niệm, Greedy-Boltzmann có thể được xem như một sự sửa đổi của Boltzmann với hai nhiệt độ, được ký hiệu là 𝜏1 và 𝜏2, trong đó 𝜏1 được cố định ở 0, và 𝜏2 có thể thay đổi.

Thuật toán 7 Greedy-Boltzmann
đầu vào: 𝑥,𝜋,𝑟
siêu tham số: 𝜏,𝑁
1:lấy mẫu phản hồi ˜𝑦1,..., ˜𝑦𝑁∼𝜋(·|𝑥)
2:chọn phản hồi 𝑖∈arg max𝑛𝑟(𝑥,𝑦𝑛)
3:xác suất 𝑞𝑛=exp(𝑟(𝑥,˜𝑦𝑛)/𝜏)Í𝑁
𝑛′=1,𝑛≠𝑖exp(𝑟(𝑥,˜𝑦𝑛′)/𝜏)∀𝑛≠𝑖,𝑞𝑖=0
4:lấy mẫu 𝑖′∼𝑞
trả về 𝑦𝑖,𝑦𝑖′

Hiệu suất của biến thể Greedy-Boltzmann trên các nhiệt độ khác nhau được minh họa trong Hình 11. Đáng chú ý, sau khi tinh chỉnh tham số nhiệt độ, các kết quả cho thấy rằng Greedy-Boltzmann không cải thiện so với hiệu suất đạt được bởi tác nhân Boltzmann tiêu chuẩn, như được trình bày trong Thuật toán 4.

Các tác nhân Boltzmann và Greedy-Boltzmann có thể được khái niệm hóa như xấp xỉ các chiến lược khám phá khác nhau được xác định bởi các nhiệt độ được sử dụng trong Thuật toán 4 và 7. Điều này bao gồm các ví dụ như khám phá "tham lam", bao gồm việc lựa chọn hai phản hồi tốt nhất; khám phá "tham lam-đồng nhất", trong đó phản hồi đầu tiên được chọn một cách tham lam và phản hồi thứ hai được chọn ngẫu nhiên; và khám phá "thụ động", trong đó cả hai phản hồi đều được lấy mẫu ngẫu nhiên. Do đó, khi đánh giá hiệu suất của Boltzmann và Greedy-Boltzmann, chúng tôi thực chất đang đánh giá một phổ rộng các sơ đồ khám phá bắt nguồn từ một mô hình phần thưởng ước lượng điểm.

--- TRANG 17 ---
Khám phá Hiệu quả cho LLMs
Hình 11|Hiệu suất của Greedy-Boltzmann trên các nhiệt độ khác nhau cho Boltzmann. Chúng ta có thể thấy rằng Greedy-Boltzmann tốt nhất và tác nhân Boltzmann tốt nhất hoạt động rất tương tự, và Greedy-Boltzmann không mang lại lợi thế so với Boltzmann.

E. Dyadic NLL
Để đánh giá chất lượng của các dự đoán kết hợp, chúng tôi lấy mẫu các lô truy vấn (𝑥1,...,𝑥𝜏) và đánh giá log-loss đối với các xác suất sở thích tương ứng của chúng (𝑝1,...,𝑝𝜏). Trong không gian đầu vào chiều cao, các truy vấn được lấy mẫu đồng nhất ngẫu nhiên thường gần như độc lập. Do đó, một giá trị 𝜏 lớn đáng kể trở nên cần thiết để phân biệt hiệu quả các dự đoán kết hợp nắm bắt các phụ thuộc giữa các truy vấn. Tuy nhiên, phương pháp này nhanh chóng trở nên không thực tế do sự tăng trưởng lũy thừa trong yêu cầu tính toán khi 𝜏 tăng.

Để giải quyết thách thức này, lấy mẫu dyadic, như được đề xuất trong (Osband et al., 2022), trình bày một phương pháp thực dụng. Lấy mẫu dyadic chọn lọc các truy vấn một cách chiến lược để đảm bảo rằng khả năng phân biệt của các tác nhân hiệu quả có thể đạt được với một 𝜏 có thể quản lý. Phương pháp này bao gồm việc ban đầu lấy mẫu hai truy vấn và sau đó liên tục chọn 𝜏 truy vấn có thay thế từ cặp này. Tiếp theo, log-likelihood âm kết hợp (nll) được tính toán trên các truy vấn được lấy mẫu sử dụng các xác suất sở thích tương ứng của chúng. Quá trình này được lặp lại nhiều lần, và dyadic joint nll trung bình được báo cáo.

Chúng tôi đã sử dụng mã trong thư viện enn (Osband et al., 2023a) để thực hiện dyadic joint nll.
