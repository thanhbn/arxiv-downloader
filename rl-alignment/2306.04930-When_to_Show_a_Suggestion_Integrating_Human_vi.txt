# 2306.04930.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2306.04930.pdf
# Kích thước tệp: 1538706 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Khi nào nên hiển thị gợi ý? Tích hợp phản hồi của con người trong lập trình hỗ trợ AI
Hussein Mozannar1, Gagan Bansal2, Adam Fourney2, và Eric Horvitz2
1Massachusetts Institute of Technology, Cambridge, USA
2Microsoft Research, Redmond, USA
Tóm tắt
Các hệ thống đề xuất mã được hỗ trợ bởi AI, như Copilot và CodeWhisperer, cung cấp gợi ý mã bên trong môi trường của lập trình viên (ví dụ: IDE) với mục đích cải thiện năng suất. Chúng tôi theo đuổi các cơ chế để tận dụng tín hiệu về việc lập trình viên chấp nhận và từ chối gợi ý mã để hướng dẫn các đề xuất. Chúng tôi khai thác dữ liệu được rút ra từ các tương tác với GitHub Copilot, một hệ thống được hàng triệu lập trình viên sử dụng, để phát triển các can thiệp có thể tiết kiệm thời gian cho các lập trình viên. Chúng tôi giới thiệu một khung lý thuyết tiện ích để thúc đẩy các quyết định về gợi ý hiển thị so với giấu đi. Phương pháp này, hiển thị gợi ý có điều kiện từ phản hồi của con người (CDHF), dựa vào một chuỗi các mô hình cung cấp khả năng mã được đề xuất sẽ được chấp nhận. Những khả năng này được sử dụng để ẩn gợi ý một cách có chọn lọc, giảm cả độ trễ và thời gian xác minh của lập trình viên. Sử dụng dữ liệu từ 535 lập trình viên, chúng tôi thực hiện đánh giá hồi cứu của CDHF và cho thấy chúng tôi có thể tránh hiển thị một phần đáng kể các gợi ý sẽ bị từ chối. Chúng tôi tiếp tục chứng minh tầm quan trọng của việc kết hợp trạng thái tiềm ẩn không quan sát được của lập trình viên trong các quyết định về thời điểm hiển thị gợi ý thông qua một nghiên cứu cắt bỏ. Cuối cùng, chúng tôi trình bày cách sử dụng việc chấp nhận gợi ý như một tín hiệu phần thưởng để hướng dẫn hiển thị gợi ý có thể dẫn đến gợi ý chất lượng giảm, chỉ ra một cạm bẫy bất ngờ.

importnumpyasnp
classLogisticRegression :
def__init(self):
self.w= None
self.b= None
|# triển khai phương thức predict
gợi ý prompt
Pr(accept| P) = 0.5…shown accepted
t1 t2A1A2
(S1,P1)
Dữ liệu Telemetry của 
Gợi ý và 
Hành động
Mô hình Chấp nhận Giai đoạn 2
Pr(accept| P ,S) = 0.1
Mô hình Chấp nhận Giai đoạn 1
Ẩn Gợi ý
Huấn luyện mô hình sử dụng 
Phản hồi TelemetryCDHF
lập trình viên
Copilot
Hình 1: Chế độ hoạt động của Copilot bên trong Visual Studio Code và cách CDHF ảnh hưởng đến tương tác bằng cách ẩn một cách chọn lọc các gợi ý nhất định. Dữ liệu được thu thập bởi tương tác được lưu trữ trong telemetry và được sử dụng để huấn luyện CDHF tạo một vòng phản hồi.

--- TRANG 2 ---
1 Giới thiệu
Các hệ thống đề xuất mã được hỗ trợ bởi các mô hình ngôn ngữ thần kinh quy mô lớn, như Github Copilot [Git22] và Amazon CodeWhisperer [Ama22], nhằm mục đích cung cấp cho các lập trình viên gợi ý mã để giúp cải thiện năng suất của họ. Các hệ thống này thường hoạt động bằng cách hiển thị gợi ý dưới dạng văn bản ma - một gợi ý mã màu xám inline bên trong IDE. Các lập trình viên có thể chấp nhận gợi ý, duyệt qua các gợi ý khác nhau, hoặc từ chối gợi ý (xem Hình 1). Các gợi ý mã xuất hiện hoặc khi lập trình viên yêu cầu rõ ràng hoặc khi lập trình viên tạm dừng con trỏ khi viết mã. GitHub báo cáo một nghiên cứu ngẫu nhiên gần đây với 95 người tham gia viết một máy chủ web, nơi họ phát hiện rằng Copilot có thể giảm thời gian hoàn thành nhiệm vụ gấp đôi [Kal22]. Những báo cáo này và các báo cáo khác cho rằng các hệ thống đề xuất mã có thể cải thiện năng suất lập trình viên thúc đẩy nghiên cứu của chúng tôi để theo đuổi cải tiến cho các hệ thống này.

Các hệ thống đề xuất mã được hỗ trợ bởi các mô hình ngôn ngữ lớn (LLM) như GPT được huấn luyện trên các mục tiêu mô hình hóa ngôn ngữ tiêu chuẩn sử dụng dữ liệu Common Crawl [RWC+19], và sau đó được tinh chỉnh trên các kho lưu trữ mã công khai [CTJ+21]. Việc triển khai công khai các mô hình đề xuất mã đã thu hút hàng triệu lập trình viên, tạo ra cơ hội độc đáo để tận dụng dữ liệu của các lập trình viên tương tác với các mô hình. Trong công việc này, chúng tôi nghiên cứu GitHub's Copilot được sử dụng bởi hàng triệu lập trình viên [Git22]. Đối với một nhóm lập trình viên trong tổ chức của chúng tôi đã đồng ý cho phép thu thập dữ liệu sử dụng của họ, chúng tôi đã thu thập dữ liệu telemetry về các gợi ý Copilot, cùng với các prompt liên quan và hành động của lập trình viên để chấp nhận hoặc từ chối các gợi ý. Chúng tôi tận dụng dữ liệu telemetry này để thiết kế các cơ chế và can thiệp có thể cải thiện tương tác giữa lập trình viên và Copilot.

Cụ thể, chúng tôi tìm cách xác định khi nào hiển thị gợi ý mã. Chúng tôi đầu tiên định nghĩa tiện ích mong đợi của việc hiển thị một gợi ý, một giá trị đo lường tác động của việc hiển thị gợi ý lên tổng thời gian để viết một đoạn mã cụ thể. Giá trị này cung cấp một tiêu chí tối ưu cho thời điểm hiển thị gợi ý. Tuy nhiên, việc tính toán tiện ích của gợi ý là khó khăn và hiện tại không khả thi. Thay vào đó, chúng tôi dựa vào kết quả rằng tiện ích gợi ý tăng càng nhiều khi một gợi ý có khả năng được chấp nhận và giảm với độ trễ tăng để tạo gợi ý - hai đại lượng chúng tôi có thể ước tính và kiểm soát một cách đáng tin cậy, tương ứng. Chúng tôi phát triển một quy trình, được đặt tên là hiển thị gợi ý có điều kiện từ phản hồi con người (CDHF) hướng dẫn liệu có nên hiển thị hay ẩn gợi ý. Tại mỗi khoảng dừng trong keystrokes, CDHF quyết định liệu có đáng để tạo gợi ý và liệu lập trình viên có khả năng chấp nhận gợi ý được tạo. CDHF sử dụng một chuỗi các mô hình dự đoán việc chấp nhận gợi ý. Quy trình tối ưu hóa đảm bảo rằng bất kỳ gợi ý nào bị ẩn (hoặc không được tạo) sẽ bị từ chối nếu được hiển thị với xác suất ít nhất p, nơi, ví dụ, p có thể là 0.99.

Sử dụng dữ liệu từ các phiên lập trình của 535 lập trình viên với phản hồi trên 168k gợi ý, chúng tôi thực hiện đánh giá hồi cứu của CDHF. Chúng tôi cho thấy rằng chúng tôi có thể ẩn 25% gợi ý được hiển thị trong khi đảm bảo rằng 95% trong số chúng sẽ bị từ chối. Hơn nữa, chúng tôi tránh tạo 13% các gợi ý này. Kết quả cho thấy CDHF sẽ tăng tỷ lệ chấp nhận 7.2%.

--- TRANG 3 ---
Quy trình cho phép kiểm soát sự đánh đổi cân bằng số lượng gợi ý được hiển thị với tăng độ trễ, được kiểm soát bằng một tham số dừng việc tạo. Chúng tôi lưu ý rằng một phiên bản tối thiểu của CDHF đã được triển khai trong phiên bản mới hơn của GitHub Copilot [Zha23] sau buổi trình bày các phiên bản trước của công việc của chúng tôi cho GitHub. Bài báo của chúng tôi cung cấp lộ trình để xây dựng và triển khai các dạng hiển thị gợi ý tốt hơn.

Ngoài các quyết định về hiển thị đề xuất, chúng tôi kiểm tra tính khả thi của việc sử dụng chấp nhận gợi ý như một tín hiệu phần thưởng để chọn gợi ý nào hiển thị và cho thấy cách các hoàn thành một phần có thể được ưu tiên hơn so với việc tạo các phân đoạn mã hoàn chỉnh. Trong khi chúng tôi nghiên cứu Copilot trong công việc này, chúng tôi tin rằng những hiểu biết của chúng tôi mở rộng đến các mô hình AI khác và các nhiệm vụ không dựa trên mã. Vui lòng tham khảo phiên bản arXiv của công việc này cho phụ lục [MBFH23].

2 Công việc liên quan
Công việc liên quan gần nhất với chúng tôi là quy trình ẩn gợi ý có chọn lọc trong [SDS+22] (ước lượng chất lượng trước khi hoàn thành, QEBC). Khác với công việc này, QEBC [SDS+22] không dựa trên phản hồi con người về chấp nhận gợi ý mà dựa trên việc xây dựng một bộ ước lượng học được về chất lượng hoàn thành mã từ bộ dữ liệu các phân đoạn mã được ghép nối và hoàn thành mô hình. Bộ ước lượng CDHF của chúng tôi sử dụng dữ liệu hành vi lập trình viên thực và dựa trên dữ liệu từ một hệ thống đề xuất mã đang được sử dụng hiện tại (Copilot) thay vì các hệ thống được huấn luyện tùy chỉnh trong [SDS+22]. Các số liệu và bộ dữ liệu khác nhau đã được đề xuất để đánh giá hiệu suất của các mô hình đề xuất mã, nhưng chúng thường đánh giá mức độ tốt của mô hình có thể hoàn thành mã trong môi trường offline mà không có đầu vào của nhà phát triển thay vì đánh giá mức độ tốt của nó hỗ trợ lập trình viên tại chỗ [ZKL+22,LCC+22,EBSB22,DMN+22]. Tích hợp sở thích con người khi huấn luyện các mô hình học máy đã được nghiên cứu lâu trong tài liệu [KS08,MHL+17]. Đặc biệt, học tăng cường từ phản hồi con người (RLHF) đã được sử dụng để cải thiện LLM được sử dụng như chatbot đối thoại [ZSW+19,BJN+22], đặc biệt là ChatGPT [Ope22]. Ngược lại, CDHF sử dụng phản hồi con người được thu thập một cách tự nhiên thông qua telemetry. Mục tiêu là suy luận nhanh để giảm độ trễ và ẩn gợi ý thay vì cập nhật LLM. Công việc liên quan thêm có thể được tìm thấy trong phụ lục. Các công thức lý thuyết của chúng tôi xây dựng trên công việc trước đó về khai thác học máy và tiện ích để hướng dẫn AI so với đóng góp do con người thực hiện trong các môi trường tương tác AI-con người [Hor99], mà chúng tôi áp dụng cho môi trường của chúng tôi.

3 Thiết lập vấn đề
Copilot. Chúng tôi xem xét Copilot, là một công cụ được sử dụng phổ biến và tiêu biểu của các đề xuất mã được hỗ trợ AI được sử dụng bởi hàng triệu lập trình viên [Git22]. Copilot được hỗ trợ bởi một mô hình ngôn ngữ lớn (LLM) để cung cấp gợi ý mã cho lập trình viên trong IDE bất cứ khi nào lập trình viên tạm dừng việc gõ. Hình minh họa Copilot gợi ý mã dưới dạng đoạn inline, đơn màu được hiển thị trong Hình 1. Lập trình viên có thể chọn chấp nhận gợi ý này thông qua phím tắt

--- TRANG 4 ---
…được hiển thị
t0=0được chấp nhận được hiển thị bị từ chối
t1 t2 t3 t4A1 A2 A3 A4
(S1,P1) (S3,P3) (S1,P1) (S3,P3)Hình 2: Sơ đồ telemetry với Copilot như một dòng thời gian. Đối với một phiên mã hóa nhất định, telemetry chứa một chuỗi timestamp và hành động với các prompt và gợi ý liên quan.

(ví dụ: tab).
Lập trình hỗ trợ AI. Chúng tôi cố gắng công thức hóa toán học lập trình với sự giúp đỡ của một mô hình đề xuất mã như Copilot, mà chúng tôi gọi là Lập trình hỗ trợ AI. Lập trình viên muốn hoàn thành một nhiệm vụ T nhất định, ví dụ, để triển khai một bộ phân loại hồi quy logistic. Khi lập trình viên viết mã bắt đầu từ thời điểm 0, Copilot cố gắng cung cấp gợi ý mã tại các thời điểm khác nhau. Tại thời điểm t nhất định, Copilot sử dụng một phần của mã Xt để tạo prompt Pt, được chuyển đến LLM cơ bản. Copilot sau đó tạo gợi ý mã St, được hiển thị cho người dùng tại thời điểm t+τ nơi τ tính đến độ trễ LLM. Khi gợi ý được hiển thị, lập trình viên phải thực hiện một hành động tại thời điểm t'> t+τ, hành động là At'∈ {chấp nhận, từ chối}; hành động từ chối được kích hoạt ngầm bằng cách tiếp tục gõ.

Telemetry. Copilot ghi nhận các khía cạnh của tương tác thông qua telemetry, mà chúng tôi tận dụng trong nghiên cứu của chúng tôi. Chúng tôi tham chiếu đến các vị trí sự kiện được rút ra từ phân chia của thời gian trong một phiên. Cụ thể, bất cứ khi nào một gợi ý được hiển thị, chấp nhận hoặc từ chối, chúng tôi ghi lại một bộ tuple vào cơ sở dữ liệu telemetry, (ti, Ai, Pi, Si), nơi ti đại diện cho timestamp trong phiên của sự kiện thứ i (t0= 0), Ai chi tiết hành động được thực hiện (được bổ sung để bao gồm 'được hiển thị'), và Pi và Si nắm bắt prompt và gợi ý, tương ứng. Hình 2 hiển thị một phần của dòng thời gian được xây dựng từ dữ liệu telemetry được rút ra từ một phiên mã hóa. Dữ liệu telemetry từ mỗi lập trình viên được lưu trữ trong cơ sở dữ liệu D={(ti, Ai, Pi, Si)}n i=1 và đại diện cho đại diện rời rạc hóa của tương tác và cung cấp dữ liệu phản hồi con người mà chúng tôi tận dụng.

Trạng thái lập trình viên. Khi đối mặt với một gợi ý, lập trình viên có đang nhìn và xác minh nó, hay đang tham gia vào các hoạt động khác như suy nghĩ về mã của họ hoặc xem tài liệu? Trạng thái của lập trình viên là quan trọng trong giá trị mong đợi của đề xuất. Tuy nhiên, chúng tôi không thể trả lời câu hỏi này vì telemetry không nắm bắt các hoạt động và suy nghĩ của lập trình viên giữa hai timestamp liên tiếp ti và ti+1, tức là khoảng trống giữa các mũi tên trong Hình 2 mà chúng tôi gọi là trạng thái tiềm ẩn của lập trình viên. Trong một xuất bản trước đó [MBFH22], chúng tôi mô tả một nghiên cứu về 21 người tham gia tập trung vào việc hiểu các chuỗi trạng thái được truy cập trong quá trình viết mã, bao gồm các trạng thái tiềm ẩn. Công việc sử dụng video và phỏng vấn để thu thập thông tin về các trạng thái tiềm ẩn. Chúng tôi đã chỉ ra trong công việc rằng việc bao gồm thông tin về các trạng thái tiềm ẩn có thể tăng đáng kể dự đoán về việc chấp nhận đề xuất, thúc đẩy việc thu thập dữ liệu ngoài những gì được nắm bắt trong telemetry.

Trong công việc này, chúng tôi nỗ lực hiểu tác động của trạng thái tiềm ẩn lập trình viên được ký hiệu là ϕt và ảnh hưởng của nó đến khả năng của chúng tôi tận dụng telemetry (dữ liệu phản hồi con người) để cải thiện các hệ thống đề xuất mã AI.

4 Công thức lý thuyết của Tiện ích Gợi ý
Một câu hỏi thiết kế quan trọng trong tương tác lập trình viên-Copilot là khi nào mô hình nên đưa gợi ý vào IDE? Phiên bản mà chúng tôi Copilot cung cấp gợi ý khi nó phát hiện tạm dừng ngắn trong IDE. Các thiết kế tương tác thay thế sẽ yêu cầu lập trình viên yêu cầu gợi ý bằng phím tắt hoặc kích hoạt hỗn hợp các sáng kiến của con người và máy. Yêu cầu lập trình viên hỏi có thể dẫn đến tương tác không tối ưu vì thành công của nó sẽ phụ thuộc vào lập trình viên có mô hình tinh thần chính xác về khả năng Copilot [SGN+22] có thể yêu cầu tương tác lâu dài với mô hình [BNK+19] hoặc đào tạo [MSS22]. Thứ hai, yêu cầu kêu gọi rõ ràng có thể phá vỡ dòng chảy tự nhiên của lập trình, phá vỡ trạng thái dòng chảy đạt được trong quá trình tập trung chuyên sâu [CL14]. Các thiết kế yêu cầu sáng kiến người dùng cũng như những thiết kế tự động hiển thị nội dung có thể làm gánh nặng người dùng với sự gián đoạn làm giảm hiệu suất tác vụ [BKC01,CCH01]. Chúng tôi lưu ý rằng các chi phí như vậy có thể được suy luận và tính toán chính thức trong các hệ thống dựa trên lý thuyết tiện ích [HA03, HJH99].

Lý tưởng, Copilot nên hiển thị gợi ý khi các gợi ý cung cấp giá trị ròng cho lập trình viên. Ví dụ, xem xét nhiệm vụ hoàn thành một hàm và thời gian cần thiết để hoàn thành nó như một proxy cho tổng nỗ lực. Nếu thời gian mong đợi cần thiết để xác minh và chỉnh sửa gợi ý của Copilot vượt quá thời gian để viết mã bằng chính họ (chi phí phản thực), thì Copilot không nên hiển thị gợi ý của nó. Ngược lại, nếu thời gian mong đợi để viết vượt quá thời gian để xác minh và chỉnh sửa, có thể hữu ích để hiển thị gợi ý. Bây giờ chúng tôi chính thức hóa trực giác này với một công thức dựa trên lý thuyết tiện ích và, trong phần tiếp theo, thảo luận về phương pháp để làm cho nó thực tế.

Mô hình lập trình viên. Tại một thời điểm nhất định trong một phiên, Copilot trích xuất prompt P từ tệp mã X và tạo gợi ý mã S. Nếu gợi ý này được hiển thị, chúng tôi giả định lập trình viên dành thời gian mong đợi E[verification |X, S, ϕ] để xác minh nó và chấp nhận gợi ý với xác suất P(A=accept |X, S, ϕ). Khi một gợi ý được chấp nhận, lập trình viên có thể tiếp tục chỉnh sửa gợi ý với thời gian mong đợi E[editing |X, S, ϕ, A = accept] để đạt được nhiệm vụ của họ. Mặt khác, nếu lập trình viên từ chối gợi ý, họ sẽ phải dành thời gian viết mã để đạt được nhiệm vụ của họ, được ký hiệu bởi E[writing |X, S, A =reject]. Do đó, tổng thời gian phát sinh với hiển thị gợi ý, được ký hiệu là E[S shown |X, S, ϕ], là:
E[S shown |X, ϕ] =E[verification |X, S, ϕ] (1)
+P(A=accept |X, S, ϕ)·E[editing |X, S, ϕ, A = accept]

--- TRANG 5 ---
+P(A=reject|X, S, ϕ)·E[writing |X, S, ϕ, A = reject]
Trong khi chỉnh sửa và viết, Copilot có thể tiếp tục đưa ra nhiều gợi ý hơn; do đó, thời gian chỉnh sửa và thời gian viết nên bao gồm tương tác với các gợi ý tương lai. Bây giờ, mặt khác, nếu gợi ý không được hiển thị, lập trình viên sẽ dành thời gian E[writing |X] viết mã cho nhiệm vụ của họ. Chúng tôi cũng cần tính đến độ trễ, chi phí thời gian τ để tính toán gợi ý khi chúng tôi quyết định tạo gợi ý. Độ trễ chỉ được lập trình viên trải nghiệm nếu trạng thái tiềm ẩn ϕ của họ bao gồm mong đợi gợi ý và chờ đợi nó. Nếu lập trình viên đang mong đợi gợi ý, chúng tôi nên thêm τ vào tổng thời gian khi chúng tôi hiển thị gợi ý; nếu không, lập trình viên tiếp tục viết mã mà không mong đợi gợi ý.

Bây giờ chúng tôi định nghĩa tiện ích gợi ý, một giá trị chỉ ra sự thay đổi trong thời gian mã hóa của lập trình viên do hiển thị gợi ý.
Định nghĩa 1 (Tiện ích Gợi ý). Tác động thời gian δ, được ký hiệu là tiện ích gợi ý, của việc hiển thị S so với không hiển thị được định nghĩa là:
δ=E[writing |X, ϕ]| {z }
S không được hiển thị−E[S shown |X, ϕ]| {z }
S được hiển thị−E[τ|X, ϕ]|{z }
độ trễ(2)
Từ phần trên, gợi ý S tại thời điểm nhất định chỉ nên được hiển thị nếu δ >0 (Phương trình 2), nơi lập trình viên sẽ dành ít thời gian hơn để đạt được nhiệm vụ của họ nếu nó được hiển thị. Một sơ đồ tối ưu để biết khi nào hiển thị gợi ý sẽ là tạo gợi ý thường xuyên nhất có thể, tính toán tiện ích gợi ý δ của chúng, và hiển thị chúng nếu δ >0.

Tính khả thi của việc ước tính δ. Theo Phương trình (1), việc tính toán tiện ích gợi ý yêu cầu tính toán bốn đại lượng: (1) thời gian mong đợi dành để xác minh gợi ý, (2) thời gian mong đợi chỉnh sửa gợi ý, (3) thời gian mong đợi để viết một đoạn mã và (4) xác suất chấp nhận gợi ý. Có thể cố gắng xây dựng bộ ước lượng cho (1), bằng cách dự đoán từ prompt và gợi ý thời gian dành để xác minh gợi ý i sẽ là ti+1−ti sử dụng các bộ ước lượng hồi quy tiêu chuẩn. Thật không may, sử dụng cùng các tính năng và bộ dữ liệu được chi tiết trong phần thử nghiệm của chúng tôi, bộ ước lượng tốt nhất của chúng tôi chỉ có thể đạt R2= 0.13, không tốt hơn nhiều so với ước tính thời gian trung vị ngây thơ. Điều này có thể do phương sai cao và các yếu tố gây nhiễu không quan sát được chi phối thời gian xác minh. Ước tính thời gian chỉnh sửa và xác minh (đại lượng 2 và 3 ở trên) chỉ phức tạp và thách thức hơn. Do đó, chúng tôi hạn chế phương pháp của mình để xem khi nào chúng tôi có thể đánh giá δ chỉ sử dụng bộ ước lượng của chúng tôi cho xác suất chấp nhận (4).

Học quyết định chấp nhận của lập trình viên. Điều kiện đầy đủ cho xác suất lập trình viên chấp nhận gợi ý là P(A=accept |X, S, ϕ). Với telemetry, chúng tôi chỉ có thể tính toán P(A=accept |X, S) nơi trạng thái tiềm ẩn của lập trình viên không thể quan sát được. Sử dụng các phương pháp phân loại được hiệu chỉnh tiêu chuẩn, chúng tôi có thể ước tính xác suất P(A=accept |X, S) bằng cách sử dụng các hành động Ai như nhãn. Chúng tôi chỉ ra rằng một cơ chế đơn giản của việc ngưỡng xác suất ước tính mà lập trình viên chấp nhận gợi ý tương đương dưới các giả định nhất định với việc kiểm tra nếu δ <0:

Mệnh đề 1. Dưới các giả định rằng lập trình viên dành nhiều thời gian hơn để viết mã khi họ từ chối gợi ý so với khi họ chấp nhận gợi ý và chỉnh sửa nó, với mã, gợi ý và trạng thái tiềm ẩn cụ thể (X, S, ϕ), nếu xác suất chấp nhận P(A=accept|X, S, ϕ) gợi ý của lập trình viên dưới P*, được định nghĩa là:
P*=E[verification] + E[latency]
E[writing |A= reject] −E[editing |A= accept](3)
thì gợi ý không nên được hiển thị. Lưu ý rằng P* được định nghĩa như một hàm P*(X, S, ϕ) được đánh giá theo điểm.

Tuyên bố chính thức và chứng minh có sẵn trong phụ lục. Mệnh đề trên cho thấy rằng so sánh xác suất chấp nhận với P* có thể hướng dẫn khi nào hiển thị gợi ý. Chúng tôi cung cấp cái nhìn đồ họa của phân tích trong Hình 3, theo tinh thần của các phân tích liên quan về giao diện tương tác được hướng dẫn bởi tiện ích [Hor99]. Thực tế, nếu chúng tôi so sánh xác suất chấp nhận với một giới hạn dưới không đổi của P*, chúng tôi có thể đảm bảo rằng chúng tôi ẩn gợi ý chỉ khi δ <0.

Ảnh hưởng của trạng thái tiềm ẩn lập trình viên. Như đã đề cập trước đó, trạng thái tiềm ẩn của lập trình viên không có sẵn thông qua telemetry. Do đó, chúng tôi chỉ có thể cung cấp dự đoán P(A=accept |X, S) so với việc xem xét rõ ràng trạng thái tiềm ẩn, P(A=accept |X, S, ϕ). Trong công việc trước đó [MBFH22], chúng tôi thu thập dữ liệu telemetry của 21 lập trình viên thực hiện các nhiệm vụ khác nhau và yêu cầu người tham gia hồi cứu gắn nhãn telemetry với trạng thái tiềm ẩn của họ từ một tập hợp mười hai trạng thái duy nhất (1096 gợi ý). Chúng tôi sử dụng dữ liệu này để xây dựng các mô hình dự đoán có và không có trạng thái tiềm ẩn sử dụng cùng phương pháp trong phần thử nghiệm. Sử dụng chiến lược đánh giá leave-one-out lập trình viên, mô hình không có trạng thái tiềm ẩn đạt độ chính xác 61.9±1.9 trong khi mô hình với trạng thái tiềm ẩn đạt 83.6±2.4, một sự khác biệt có ý nghĩa thống kê theo t-test ghép nối

--- TRANG 6 ---
1.0 0.0
P(A|X,S, ϕ)P*
writeNot ShownTime verify 
edit  verify Hình 3: Mô tả đồ họa của phân tích Mệnh đề 1 khi độ trễ bằng không. Trục y hiển thị tổng thời gian và trục x là xác suất lập trình viên chấp nhận P(A=accept |X, S, ϕ). Tại xác suất P*, hiển thị và không hiển thị gợi ý có chi phí thời gian bằng nhau.

(p= 6.9e−7, t= 7.11); kết quả tương tự xảy ra khi so sánh diện tích dưới đường cong đặc tính hoạt động của máy thu (AUC). Những kết quả này làm nổi bật cơ hội thu thập dữ liệu bên ngoài ngoài telemetry để xây dựng các mô hình dự đoán như vậy và chỉ ra rằng việc chấp nhận có thể không chỉ đơn giản là một thuộc tính của gợi ý và bối cảnh mã.

5 Hiển thị Gợi ý có Điều kiện từ Phản hồi Con người
Trong phần này, chúng tôi mô tả phương pháp CDHF có thể được triển khai sử dụng dữ liệu telemetry để xác định khi nào hiển thị gợi ý, như được minh họa trong Hình 1. Chúng tôi lưu ý từ Phương trình (3) rằng xác suất chấp nhận gợi ý càng cao và độ trễ để tạo gợi ý càng thấp, gợi ý càng có khả năng hữu ích (δ >0). Phương pháp đề xuất của chúng tôi như sau: Mỗi khi lập trình viên tạm dừng gõ, chúng tôi quyết định sử dụng một bộ dự đoán liệu có nên hiển thị gợi ý. Quan trọng, chúng tôi làm điều này sử dụng sơ đồ hai giai đoạn để tránh tạo gợi ý khi chúng tôi biết lập trình viên sẽ từ chối chúng.

Quyết định hiển thị. Gọi m(X, S) là một bộ dự đoán nhị phân biểu thị liệu, tại thời điểm nhất định trong mã X, chúng tôi có nên hiển thị gợi ý S; chúng tôi gọi đây là quyết định hiển thị. Nếu m(X, S) = 1, chúng tôi hiển thị gợi ý; nếu không, chúng tôi không. Cách đơn giản nhất để xây dựng hàm m như vậy là ước tính xác suất lập trình viên chấp nhận gợi ý: P(A=accept )|X, S) và sau đó ngưỡng xác suất để các gợi ý rơi dưới xác suất t bị ẩn. Tuy nhiên, điều này sẽ dẫn chúng tôi đến việc tạo gợi ý bao gồm những gợi ý sẽ không bao giờ được hiển thị, do đó lãng phí tài nguyên tính toán. Chúng tôi đề xuất phân tách hàm m để chúng tôi đầu tiên quyết định chỉ sử dụng mã liệu chúng tôi có thể đưa ra quyết định hiển thị mà không tạo gợi ý S với hàm r(X). Nếu r(X) = 1, chúng tôi đưa ra quyết định hiển thị sử dụng mô hình giai đoạn 1 m1(X) mà không tạo gợi ý, nếu không nếu r(X) = 0 chúng tôi tạo gợi ý S và đưa ra quyết định hiển thị với mô hình giai đoạn 2 m2(X, S) như sau:
m(X, S) =r(X)·m1(X) + (1 −r(X))·m2(X, S) (4)
Công thức này cho phép chúng tôi tránh tạo gợi ý khi chúng tôi có thể đưa ra quyết định hiển thị chính xác trước khi biết gợi ý. Ví dụ, trong bối cảnh nơi lập trình viên đã từ chối 30 gợi ý cuối, họ không chắc sẽ chấp nhận gợi ý tiếp theo.

Mục tiêu và đảm bảo. Mục tiêu của chúng tôi trong việc học các hàm r, m1, m2 là (1) ẩn càng nhiều gợi ý sẽ bị từ chối và (2) tối đa hóa số lượng quyết định hiển thị được đưa ra mà không tạo gợi ý để giảm độ trễ trên hệ thống. Có một sự đánh đổi cố hữu giữa hai mục tiêu này vì việc đưa ra quyết định với quyền truy cập vào các gợi ý sẽ chính xác hơn. Hơn nữa, chúng tôi muốn đảm bảo rằng chúng tôi không ẩn gợi ý sẽ được chấp nhận, vì điều này sẽ hạn chế tính hữu ích của trợ lý mã. Do đó, chúng tôi áp đặt ràng buộc rằng, bất cứ khi nào chúng tôi ẩn gợi ý, có ít nhất xác suất p nó sẽ bị từ chối, một ràng buộc về tỷ lệ âm tính thực (TNR). Chúng tôi dịch các mục tiêu và ràng buộc thành

--- TRANG 7 ---
classLogisticRegression :
def__init__(self, X, y, alpha= 0.01, max_iter =1000):
self.X= X
self.y= y
self.alpha= alpha
self.max_iter = max_iter
self.theta= np.zeros (X.shape[1])
self.costs= []
self.theta_history = [
self.theta
]
self.cost_history = [
self.cost()
]
defcost(self):
return np.sum(self.y* np.log( self.h()))session
prompt
suggestion
Xác suất lập trình viên chấp nhận gợi ý29
1
0.95
3
0
…
1427
4
1
1
1
1
15.7
-8.3
1.3
1.5
…
9,4
-2.3Độ dài tài liệu
Chỉ số sự kiện
5 hành động
trước độ dài
n của dòng
Độ tin cậy LLM
n của token
1 nếu 'def'
…
1 nếu 'returnPrompt
Embedding
(dim=768)29
1
0.95
1
…
0độ dài
n của dòng
n của token
1 nếu 'def'
…
1 nếu 'return-3.1
4.2
1.0
4.5
…
1.2
3.5Suggestion
Embedding
(dim=768)
codeBERTa
Hình 4: Các tính năng được sử dụng để xây dựng mô hình dự đoán hành động trong Thí nghiệm 6, bao gồm từ gợi ý, prompt và phiên.

vấn đề tối ưu hóa sau:
max
r,m1,m2λE[1−m(X, S)] + (1 −λ)E[r(x)] (5)
s.t.P(A=reject|m(X, S) = 0) ≥p (6)
Tham số hóa. Chúng tôi có thể kiểm soát sự đánh đổi giữa hai mục tiêu với một siêu tham số λ∈[0,1]. Tương đương, thay vì kiểm soát sự đánh đổi với λ, chúng tôi có thể đặt ràng buộc trên E[r(x)] := R và đặt λ= 1. Chúng tôi đề xuất một quy trình sau hậu trực quan để giải quyết vấn đề tối ưu hóa (5): Chúng tôi đầu tiên học các bộ ước lượng được hiệu chỉnh về xác suất chấp nhận gợi ý ˆP(A= accept |X, S)(với gợi ý) và ˆP(A=accept |X)(không có gợi ý). Sau đó chúng tôi tham số hóa:
m1(X) =IˆP(A=accept |X)≥t1, m2(X) =IˆP(A=accept |X,S)≥t2
vàr(X) =IH(ˆP(A=accept |X))≤tr(H(.)là entropy Shannon), và tối ưu hóa cùng nhau trên tuple của các ngưỡng t1, t2, tr trên [0,1]3. Đây là một quy trình khá hiệu quả có thể đạt được kết quả tốt. Chúng tôi lưu ý rằng quy trình này tiết kiệm độ trễ gián tiếp bằng cách giảm số lượng cuộc gọi LLM trong phiên và trên các người dùng khác nhau, và chúng tôi vẫn nên cho phép người dùng xem gợi ý với phím tắt đặc biệt để ghi đè quyết định hiển thị. Trong phần tiếp theo, chúng tôi thực hiện đánh giá hồi cứu của CDHF.

--- TRANG 8 ---
6 Thí nghiệm
Mục tiêu chính của chúng tôi với các thí nghiệm là hiểu mức độ tốt của quy trình CDHF có thể đưa ra quyết định hiển thị trong đánh giá hồi cứu. Mã có sẵn2 và chi tiết bổ sung có thể được tìm thấy trong phụ lục.

6.1 Bộ dữ liệu và Kỹ thuật tính năng.
Bộ dữ liệu. Để xây dựng và đánh giá các phương pháp của chúng tôi, chúng tôi trích xuất một số lượng lớn nhật ký telemetry từ người dùng Copilot (chủ yếu là kỹ sư phần mềm và nhà nghiên cứu) tại Microsoft. Các lập trình viên đã cung cấp sự đồng ý cho việc sử dụng dữ liệu của họ, và việc sử dụng của nó đã được phê duyệt bởi hội đồng tư vấn đạo đức của Microsoft. Cụ thể, trong khoảng thời gian hai tuần, chúng tôi đã trích xuất tất cả các sự kiện telemetry cho 535 người dùng lập trình Python. Tổng cộng 4,749 phiên mã hóa, nơi một phiên được định nghĩa là một chuỗi liên tục các hành động người dùng với tối đa 30 phút giữa các sự kiện liên tiếp. Các phiên này từ việc sử dụng thực tế của Copilot cho các nhiệm vụ hàng ngày của các kỹ sư phần mềm và nhà nghiên cứu, dữ liệu được thu thập trước khi bắt đầu công việc của chúng tôi. Trung bình, mỗi người dùng đóng góp chín phiên, với mỗi phiên kéo dài 21 phút (trung vị, 12 phút). Các phiên chứa trung bình 97 sự kiện (hiển thị, chấp nhận và từ chối). Tổng cộng gần 1,675 giờ mã hóa với 168,807 sự kiện được hiển thị và 33,523 sự kiện chấp nhận, mang lại tỷ lệ chấp nhận 21.4% (không có nghĩa là đại diện cho tỷ lệ chấp nhận trung bình của Copilot).

Tính năng mô hình. Bộ dữ liệu telemetry D được mô tả ở trên chứa cho mỗi người dùng một danh sách các sự kiện trong mỗi phiên mã hóa của họ; chúng tôi ký hiệu Di,j là danh sách các sự kiện cho phiên thứ j của người dùng thứ i. Bộ dữ liệu D={Di,j} chứa cho mỗi người dùng i và phiên j, một danh sách các sự kiện xảy ra trong phiên mã hóa tương ứng. Chúng tôi trích xuất chỉ các sự kiện chấp nhận và từ chối, cũng như các tính năng prompt và phiên của các sự kiện được hiển thị tương ứng. Đối với mỗi cặp prompt và gợi ý, chúng tôi trích xuất: id lập trình viên như vectơ one hot, độ dài của tài liệu, năm hành động trước, các tính năng gợi ý (ví dụ: độ dài gợi ý), các tính năng trước của năm gợi ý cuối được hiển thị, độ tin cậy được báo cáo bởi Copilot, một embedding sử dụng codeBERTa [FGT+20] của prompt và gợi ý, sự hiện diện của từ khóa Python (ví dụ: import, def try, v.v.), và đầu ra của Tree-sitter Parser [Kal23]. Cuối cùng, chúng tôi trích xuất các tính năng của prompt, bao gồm embedding, tính năng văn bản và đầu ra parser của nó. Hình 4 tóm tắt kỹ thuật tính năng. Điều quan trọng cần lưu ý là các tính năng không rò rỉ bất kỳ thông tin nào về các sự kiện tương lai và có thể được tính toán ngay khi gợi ý được tạo bởi Copilot. Đối với mô hình giai đoạn đầu (m1) trong CDHF, các tính năng gợi ý được bỏ qua trong khi chúng tôi bao gồm tất cả các tính năng cho mô hình giai đoạn thứ hai (m2). Kỹ thuật tính năng này kết hợp các hành động và gợi ý trước mà lập trình viên đã thấy và cho phép chúng tôi sử dụng các thuật toán ML thường xuyên thay vì các phương pháp chuỗi thời gian.

2https://github.com/microsoft/coderec_programming_states

--- TRANG 9 ---
6.2 Đánh giá mô hình
Trước khi chúng tôi đánh giá CDHF, chúng tôi thực hiện đánh giá mô hình chấp nhận lập trình viên m2(X, S). Chúng tôi chia bộ dữ liệu telemetry theo tỷ lệ 70:10:20 cho đào tạo, xác thực và kiểm tra tương ứng. Quan trọng, chúng tôi thực hiện phân chia này theo hai cách: (1) bằng cách chia ngẫu nhiên trên các lập trình viên để không có lập trình viên nào được chia sẻ trên ba phân chia và, (2) bằng cách chia ngẫu nhiên trên các phiên để người dùng trong đào tạo cũng có thể được thấy trong kiểm tra để cho phép cá nhân hóa.

Kết quả. Chúng tôi đánh giá các mô hình học máy tiêu chuẩn khác nhau trên nhiệm vụ này và phát hiện rằng mô hình hoạt động tốt nhất là eXtreme Gradient Boosting (XGB) [CHB+15]. Khi chúng tôi chia trên người dùng, XGB có thể đạt độ chính xác 81.1% (95% CI 80.7-81.6) và, quan trọng hơn, 0.780 (95% CI 0.775-0.786) AUC. Trong phụ lục, chúng tôi hiển thị số liệu cho các mô hình khác nhau được đánh giá, bao gồm mạng sâu [MBFH23]. Kết quả chỉ ra rằng mô hình có thể phân biệt giữa các gợi ý có khả năng được chấp nhận so với những gợi ý có khả năng bị từ chối. Mô hình cũng được hiệu chỉnh tốt: lỗi hiệu chỉnh mong đợi là 0.10 [NCH15].

Chúng tôi lưu ý sự gia tăng đáng kể trong AUC khi chúng tôi cho phép cá nhân hóa: bao gồm ID lập trình viên như một tính năng và chia trên các phiên, điều này dẫn đến AUC 0.795 (95% CI 0.789-0.801), một sự gia tăng đáng kể (cơ sở của mô hình m2). Khi chúng tôi loại bỏ các tính năng gợi ý khỏi mô hình, mô hình kết quả (cơ sở của mô hình m1) đạt AUC 0.631 (95% CI 0.624-0.638). Thời gian để tính toán các tính năng cần thiết cho các mô hình và thực hiện suy luận trên một điểm dữ liệu duy nhất có thể mất 10ms với GPU và ít hơn 1ms trên CPU khi bỏ qua embedding, ngoài độ trễ gửi và nhận thông tin giữa máy chủ và máy khách. Trong phụ lục, chúng tôi hiển thị kết quả cho cắt bỏ khác nhau của các tính năng mô hình, biểu đồ độ phức tạp mẫu và biểu đồ tầm quan trọng tính năng.

6.3 Đánh giá hồi cứu của CDHF
Chúng tôi huấn luyện các mô hình m1 và m2 sử dụng tập đào tạo theo phần con trước. Chúng tôi đặt các ngưỡng t1, t2, tr trên tập xác thực cho CDHF và đánh giá trên tập kiểm tra.

Kết quả. Trong Hình 5, chúng tôi thay đổi tỷ lệ TNR mong muốn (độ chính xác khi gợi ý bị ẩn) và vẽ biểu đồ số lượng gợi ý chúng tôi có thể ẩn từ những gợi ý được hiển thị trước đó trong khi đảm bảo tỷ lệ TNR mong muốn. Chúng tôi hiển thị hành vi của phương pháp CDHF với các giá trị λ khác nhau, hoặc, tương đương, với các ràng buộc khác nhau về tần suất mô hình m1 (giai đoạn đầu) được sử dụng: R:=E[r(x)]. Để minh họa những gì CDHF có thể thực hiện, chúng tôi có thể ẩn 25.3% gợi ý được hiển thị trong khi đảm bảo rằng 94.7% trong số chúng sẽ bị từ chối và tránh tạo 12.9% gợi ý. Nếu chúng tôi không có mối quan tâm về độ trễ, chúng tôi có thể ẩn 52.9% gợi ý trong khi đảm bảo rằng 91.3% trong số chúng sẽ bị từ chối. Hình 5 hiển thị cách chúng tôi có thể đạt được các đánh đổi khác nhau bằng cách chọn một điểm hoạt động trên bất kỳ đường cong nhất định nào. CDHF có thể thỏa mãn ràng buộc FNR trên tập kiểm tra với vi phạm tối đa 0.3% tức là đảm bảo 95% FNR trên tập xác thực tương đương với 94.7-95.3% trên tập kiểm tra.

Tăng tỷ lệ chấp nhận phản thực. Trên tập kiểm tra, tỷ lệ chấp nhận gợi ý là 22.5%. Hồi cứu, nếu chúng tôi đã sử dụng CDHF để ẩn 52.9% gợi ý, chúng tôi có thể

--- TRANG 10 ---
78 80 82 84 86 88 90 92 94 96 98 100
Độ chính xác khi ẩn gợi ý (TNR)0102030405060708090100Gợi ý bị ẩn (%)
CDHF với R=0.0 (chỉ m2)
CDHF với R=0.1
CDHF với R=0.25
CDHF với R=0.5
CDHF với R=0.75
CDHF với R=0.9
CDHF với R=1.0 (chỉ m1)Hình 5: Đánh giá CDHF để ẩn gợi ý có chọn lọc. Đối với ràng buộc nhất định trên FNR (độ chính xác khi gợi ý bị ẩn) trên trục x, chúng tôi hiển thị trên trục y phần trăm tổng số gợi ý chúng tôi có thể ẩn trong khi đảm bảo FNR mong muốn. Chúng tôi vẽ các đường cong này trong khi thay đổi tần suất quyết định được đưa ra tạo gợi ý (R:= E[r(x)], khi R=0, chúng tôi tạo gợi ý sau đó quyết định ẩn hay không, khi R=1, chúng tôi quyết định ẩn mà không biết gợi ý).

tính toán tỷ lệ chấp nhận phản thực. Tỷ lệ chấp nhận phản thực có thể được tính toán như:
S được chấp nhận ·(1−% ẩn ·(1−TNR ))
S được hiển thị ·% không ẩn=22.5(1−0.529·0.087)
0.471= 45.6%, đây là tăng 23.1 điểm, một giá trị chúng tôi mong đợi là ước tính quá cao.

Thảo luận và hạn chế. Đánh giá hồi cứu cho thấy CDHF có triển vọng trong việc giảm thời gian nhà phát triển dành để xác minh gợi ý hoặc chờ gợi ý. Chúng tôi lưu ý rằng đánh giá của chúng tôi là hồi cứu. Mặc dù GitHub đã chỉ ra rằng các bộ lọc gợi ý có điều kiện tương tự như CDHF tăng tỷ lệ chấp nhận gợi ý, một nghiên cứu người dùng được yêu cầu để xác minh liệu phương pháp có làm cho lập trình viên năng suất hơn. Như luật Goodhart nói, một khi một số liệu trở thành mục tiêu, nó không còn là một biện pháp tốt; tỷ lệ chấp nhận không ngoại lệ. Hơn nữa, nếu CDHF không được huấn luyện với dữ liệu đủ nắm bắt các trường hợp sử dụng của lập trình viên, nó có thể làm cho trải nghiệm lập trình tồi tệ hơn bằng cách ẩn gợi ý hữu ích. Hơn nữa, một gợi ý bị từ chối vẫn có thể hữu ích, mà chúng tôi không tính đến ở đây. Cuối cùng, vấn đề tối ưu hóa trong (5) có thể tiếp nhận các quy trình lấy cảm hứng từ học để hoãn [MS20] có thể vượt trội so với quy trình sau hậu được đề xuất.

7 Gợi ý nào nên hiển thị?
Chúng tôi tập trung trong nghiên cứu này vào vấn đề khi nào hiển thị gợi ý. Chúng tôi không giải quyết câu hỏi gợi ý nào hiển thị trong số một tập hợp ứng viên. Với quyền truy cập vào dữ liệu telemetry, bao gồm gợi ý được bối cảnh hóa với tín hiệu chấp nhận và từ chối, có thể diễn giải việc chấp nhận như hành động ưa thích gợi ý hơn không có gợi ý. Hợp lý khi khai thác dữ liệu telemetry như tập dữ liệu ưa thích và xây dựng mô hình phần thưởng về sở thích của lập trình viên, điều này sẽ tương đương với việc ước tính xác suất chấp nhận của lập trình viên ˆP(A=accept |X, S). Do đó, một quy trình hợp lý là lấy một tập hợp ứng viên gợi ý S và hiển thị gợi ý tối đa hóa xác suất chấp nhận trên tập hợp; điều này về cơ bản là phương pháp best-of-n cơ bản trong RLHF [RSM+23].

Thiên vị tiềm năng đối với gợi ý ngắn. Chúng tôi giả thuyết rằng sơ đồ xếp hạng như vậy sẽ không hiệu quả và có thể dẫn đến gợi ý kém có độ dài ngắn. Lý luận của chúng tôi như sau: giả sử LLM có thể tạo gợi ý nhiều dòng S cho truy vấn người dùng gần đúng với những gì người dùng mong muốn. Để tối đa hóa xác suất người dùng chấp nhận gợi ý, sẽ có lợi khi chia gợi ý S theo từng dòng và hiển thị cho người dùng từng bước. Lý luận là dòng đầu tiên của S có khả năng đúng hơn toàn bộ S đều đúng, do đó có khả năng được chấp nhận hơn.

Thí nghiệm. Để kiểm tra giả thuyết này, chúng tôi thực hiện thí nghiệm sau: Chúng tôi học một mô hình m chấp nhận gợi ý chỉ cho prompt và embedding gợi ý mà không có tính năng phiên trên dữ liệu telemetry từ phần trước. Sau đó chúng tôi tận dụng bộ dữ liệu HumanEval [CTJ+21], bao gồm 164 vấn đề Python, mỗi vấn đề có docstring liên quan và giải pháp thân hàm sự thật cơ bản. Các giải pháp có ít nhất hai dòng và bảy dòng mã trung vị. Với mô hình m và mỗi vấn đề, chúng tôi để prompt là sự kết hợp của docstring và k dòng đầu tiên của giải pháp và để tập hợp ứng viên gợi ý S như sau: Với giải pháp S được biểu diễn như một mảng token có độ dài N, chúng tôi để S={S[:i]}N i=1. Ví dụ, nếu giải pháp S là "return np.mean(x)", thì S={"return" ,"return np.mean(x)" }.

Kết quả. Chúng tôi thay đổi tham số k trong tập hợp {0,1,2,3} để prompt từ docstring bao gồm các dòng của giải pháp. Khi k= 0, độ dài được chuẩn hóa của gợi ý được đánh giá cao nhất, theo mô hình trên 164 vấn đề, gần như đồng nhất trên [0,1], một kiểm tra Kolmogorov–Smirnov so sánh với phân phối đồng nhất có p-value 0.53 (KS=0.06). Tối ưu, chúng tôi muốn độ dài được chuẩn hóa tập trung xung quanh 1 để bao gồm giải pháp đầy đủ. Tuy nhiên, khi k >0, có nghĩa là prompt bao gồm các dòng mã, chúng tôi thấy rằng đối với hơn 60 trong số 164 vấn đề, gợi ý được chấm điểm cao nhất nằm trong [0,0.2], và, đối với ít nhất 40 vấn đề, đó là token đầu tiên. Biểu đồ hiển thị hiện tượng này trong Hình 11 cho k= 1. Điều này cung cấp một số bằng chứng rằng tối ưu hóa cho việc chấp nhận có thể thiên vị đối với gợi ý ngắn hơn vì gợi ý được xếp hạng cao nhất thường ở trong vài dòng đầu tiên của giải pháp.

Hạn chế. Tuy nhiên, có những hạn chế quan trọng trong thí nghiệm của chúng tôi. Đầu tiên, mô hình m chỉ được huấn luyện trên gợi ý Copilot. Do đó, thiên vị đối với gợi ý ngắn có thể do một phần Copilot có khả năng hiển thị gợi ý ngắn. Tối đa hóa việc chấp nhận sẽ không làm giảm thiên vị như vậy. Thứ hai, trong khi việc sử dụng embedding của gợi ý cho mô hình phần thưởng dẫn đến dự đoán chính xác về việc chấp nhận (AUC=0.701), chúng có thể bị thiên vị theo một số cách so với liên minh về tinh chỉnh mô hình ngôn ngữ.

--- TRANG 11 ---
0.0 0.2 0.4 0.6 0.8 1.0
Phần trăm chỉ số tối đa0510152025303540Số lượng(a) Biểu đồ vị trí gợi ý tối đa
0 20 40 60 80
Phân vị của độ dài0.540.560.580.600.620.640.66Điểm trung bình được chuẩn hóa
 Đường xu hướng bậc hai (b) Xác suất chấp nhận theo độ dài
Hình 6: Biểu đồ cho thí nghiệm xếp hạng gợi ý theo xác suất chấp nhận. Biểu đồ (a) hiển thị trong thùng phân vị độ dài nào gợi ý tối đa hóa xác suất chấp nhận nằm và Biểu đồ (b) hiển thị điểm chấp nhận bằng cách tăng độ dài của gợi ý. Những biểu đồ này dành cho k= 1(docstring + dòng đầu tiên của giải pháp)

8 Kết luận
Chúng tôi đề xuất một chiến lược để quyết định khi nào hiển thị gợi ý mã trong lập trình hỗ trợ AI để cải thiện hiệu quả thời gian. Chiến lược này dựa trên công thức lý thuyết tiện ích và sử dụng quy trình hai giai đoạn sử dụng mô hình dự đoán chấp nhận gợi ý. Đánh giá hồi cứu cho thấy chúng tôi có thể giảm số lượng gợi ý và do đó thời gian của lập trình viên mà không hy sinh tính hữu ích của Copilot. Tuy nhiên, một nghiên cứu tiến cứu đánh giá tác động của Copilot có và không có CDHF có thể giúp đánh giá kết luận và là cơ sở cho công việc tương lai. Hơn nữa, công việc tương lai sẽ cố gắng ước tính trực tiếp Mệnh đề 1 tận dụng các phương pháp được cải thiện. Chúng tôi không tin rằng CDHF có thể mang lại hậu quả tiêu cực ngoài những gì Copilot mang lại cho lập trình viên vì nó hoạt động như một cơ chế lọc cho các gợi ý không hữu ích. Hơn nữa, chúng tôi tin rằng phương pháp CDHF có thể được sử dụng trong một loạt rộng các nhiệm vụ hợp tác AI-con người streaming như viết hỗ trợ. Công việc tương lai sẽ kết hợp trạng thái tiềm ẩn của lập trình viên vào các mô hình dự đoán, điều tra cách xếp hạng gợi ý sử dụng các mô hình từ CDHF, và xác nhận hiệu quả của CDHF trong nghiên cứu người dùng.

Lời cảm ơn
Hussein Mozannar một phần thực hiện công việc này trong thời gian thực tập tại Microsoft Research (MSR). Chúng tôi ghi nhận phản hồi có giá trị từ các đồng nghiệp trên MSR và GitHub bao gồm Saleema Amershi, Victor Dibia, Forough Poursabzi, Andrew Rice, Eirini Kalliamvakou, và Edward Aftandilian.

--- TRANG 12 ---
Tài liệu tham khảo
[Ama22] Amazon. Ml-powered coding companion – amazon codewhisperer, 2022.
[BCC+23]Umang Bhatt, Valerie Chen, Katherine M Collins, Parameswaran Kamalaruban, Emma Kallina, Adrian Weller, và Ameet Talwalkar. Learning personalized decision support policies. arXiv preprint arXiv:2304.06701, 2023.
[BJN+22]Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.
[BJP22] Shraddha Barke, Michael B James, và Nadia Polikarpova. Grounded copilot: How programmers interact with code-generating models. arXiv preprint arXiv:2206.15000, 2022.
[BKC01] Brian P Bailey, Joseph A Konstan, và John V Carlis. The effects of interruptions on task performance, annoyance, and anxiety in the user interface. In Interact, volume 1, pages 593–601, 2001.
[BMR+20]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
[BNK+19]Gagan Bansal, Besmira Nushi, Ece Kamar, Walter S Lasecki, Daniel S Weld, và Eric Horvitz. Beyond accuracy: The role of mental models in human-ai team performance. InProceedings of HCOMP, volume 7, pages 2–11, 2019.
[CCH01] Edward Cutrell, Mary Czerwinski, và Eric Horvitz. Notification, disruption, and memory: Effects of messaging interruptions on memory and performance. In IFIP TC13 International Conference on Human-Computer Interaction, 2001.
[CG16] Tianqi Chen và Carlos Guestrin. XGBoost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, pages 785–794, New York, NY, USA, 2016. ACM.
[CHB+15]Tianqi Chen, Tong He, Michael Benesty, Vadim Khotilovich, Yuan Tang, Hyunsu Cho, Kailong Chen, et al. Xgboost: extreme gradient boosting. R package version 0.4-2, 1(4):1–4, 2015.
[CL14] Mihaly Csikszentmihalyi và Reed Larson. Flow and the foundations of positive psychology, volume 10. Springer, 2014.

--- TRANG 13 ---
[CLB+17]Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, và Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.
[CTJ+21]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.
[DK18] John J Dudley và Per Ola Kristensson. A review of user interface design for interactive machine learning. ACM Transactions on Interactive Intelligent Systems (TiiS), 8(2):1–37, 2018.
[DMN+22]Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse Khomh, Michel C Desmarais, Zhen Ming, et al. Github copilot ai pair programmer: Asset or liability? arXiv preprint arXiv:2206.15331, 2022.
[EBSB22] Mikhail Evtikhiev, Egor Bogomolov, Yaroslav Sokolov, và Timofey Bryksin. Out of the bleu: how should we assess quality of the code generation models? arXiv preprint arXiv:2208.03133, 2022.
[FGT+20]Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural languages. arXiv preprint arXiv:2002.08155, 2020.
[Git22] Github. Github copilot - your ai pair programmer, 2022.
[HA03] Eric Horvitz và Johnson Apacible. Learning and reasoning about interruption. In Proceedings of the 5th International Conference on Multimodal Interfaces, ICMI '03, page 20–27, 2003.
[HBK+21]Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021.
[HJH99] Eric Horvitz, Andy Jacobs, và David Hovel. Attention-sensitive alerting. In Proceedings of UAI, page 305–313, 1999.
[Hor99] Eric Horvitz. Principles of mixed-initiative user interfaces. In Proceedings of CHI, pages 159–166, 1999.
[Kal22] Eirini Kalliamvakou. Research: Quantifying github copilot's impact on developer productivity and happiness, Sep 2022.
[Kal23] Eirini Kalliamvakou. Tree-sitter parser generator tool, Jan 2023.

--- TRANG 14 ---
[KS08] W Bradley Knox và Peter Stone. Tamer: Training an agent manually via evaluative reinforcement. In 2008 7th IEEE international conference on development and learning, pages 292–297. IEEE, 2008.
[LCC+22]Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond,TomEccles,JamesKeeling,FelixGimeno,AgustinDalLago,etal. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814, 2022.
[MBFH22] Hussein Mozannar, Gagan Bansal, Adam Fourney, và Eric Horvitz. Reading between the lines: Modeling user behavior and costs in ai-assisted programming. arXiv preprint arXiv:2210.14306, 2022.
[MBFH23] Hussein Mozannar, Gagan Bansal, Adam Fourney, và Eric Horvitz. When to show a suggestion? integrating human feedback in ai-assisted programming. arXiv preprint arXiv:2306.04930, 2023.
[MHL+17]James MacGlashan, Mark K Ho, Robert Loftin, Bei Peng, Guan Wang, David L Roberts, Matthew E Taylor, và Michael L Littman. Interactive learning from policy-dependent human feedback. In ICML, pages 2285–2294. PMLR, 2017.
[MPZ18] David Madras, Toni Pitassi, và Richard Zemel. Predict responsibly: improving fairness and accuracy by learning to defer. Advances in Neural Information Processing Systems, 31, 2018.
[MS20] Hussein Mozannar và David Sontag. Consistent estimators for learning to defer to an expert. In ICML, pages 7076–7087. PMLR, 2020.
[MSS22] Hussein Mozannar, Arvind Satyanarayan, và David Sontag. Teaching humans when to defer to a classifier via exemplars. In AAAI, volume 36, pages 5323–5331, 2022.
[NCH15] Mahdi Pakdaman Naeini, Gregory Cooper, và Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In Proceedings of AAAI, 2015.
[ODR21] Nastaran Okati, Abir De, và Manuel Rodriguez. Differentiable learning under triage. Advances in Neural Information Processing Systems, 34:9140–9151, 2021.
[Ope22] OpenAI. Chatgpt: Optimizing language models for dialogue, 2022.
[PGM+19]Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, NataliaGimelshein, LucaAntiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, và Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pages 8024–8035. Curran Associates, Inc., 2019.

--- TRANG 15 ---
[PVG+11]F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, và E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.
[RSM+23]Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, và Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.
[RWC+19]Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.
[SDS+22]Zhensu Sun, Xiaoning Du, Fu Song, Shangwen Wang, Mingze Ni, và Li Li. Learning to prevent profitless neural code completion. arXiv preprint arXiv:2209.05948, 2022.
[SGN+22]Advait Sarkar, Andrew D Gordon, Carina Negreanu, Christian Poelitz, Sruti Srinivasa Ragavan, và Ben Zorn. What is it like to program with artificial intelligence? arXiv preprint arXiv:2208.06213, 2022.
[SK09] Xiaoyuan Su và Taghi M Khoshgoftaar. A survey of collaborative filtering techniques. Advances in artificial intelligence, 2009, 2009.
[TN22] Maxim Tabachnyk Tabachnyk và Stoyan Nikolov. Ml-enhanced code completion improves developer productivity, Jul 2022.
[VZG22] Priyan Vaithilingam, Tianyi Zhang, và Elena L Glassman. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In CHI Conference on Human Factors in Computing Systems Extended Abstracts, pages 1–7, 2022.
[WMH+21]Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross, Fernando Martinez, Mayank Agarwal, và Kartik Talamadupula. Perfection not required? human-ai partnerships in code translation. In 26th International Conference on Intelligent User Interfaces, pages 402–412, 2021.
[Zha23] Shuyin Zhao. GitHub Copilot now has a better AI model and new capabilities. https://github.blog/2023-02-14-github-copilot-now-has-a-better-ai- model-and-new-capabilities/, February 2023.
[ZKL+22]Albert Ziegler, Eirini Kalliamvakou, X Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, và Edward Aftandilian. Productivity assessment of neural code completion. In Proceedings of SIGPLAN, pages 21–29, 2022.
[ZSW+19]Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, và Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019.

--- TRANG 16 ---
A Công việc liên quan mở rộng
Lập trình hỗ trợ AI. Các mô hình ngôn ngữ lớn (LLM) như GPT-3 [BMR+20], đã được sử dụng rộng rãi trong xử lý ngôn ngữ tự nhiên. Một ví dụ về điều này là Codex [CTJ+21], một mô hình GPT được huấn luyện trên 54 triệu kho lưu trữ GitHub, chứng minh hiệu quả của LLM trong việc giải quyết các nhiệm vụ lập trình khác nhau. Ví dụ, Codex được thử nghiệm trên bộ dữ liệu HumanEval gồm 164 vấn đề lập trình, nơi nó được yêu cầu viết thân hàm từ docstring và đạt độ chính xác 37.7% với một lần tạo duy nhất [CTJ+21]. Các số liệu và bộ dữ liệu khác nhau đã được đề xuất để đánh giá hiệu suất của các mô hình đề xuất mã, nhưng chúng thường đánh giá mức độ tốt của mô hình có thể hoàn thành mã trong môi trường offline mà không có đầu vào của nhà phát triển, thay vì đánh giá mức độ tốt của nó hỗ trợ lập trình viên tại chỗ [HBK+21,LCC+22,EBSB22,DMN+22]. Các nhà nghiên cứu đã phát hiện rằng các nhà phát triển không cần một mô hình đề xuất hoàn hảo để nó hữu ích. Weisz et al. đã thực hiện phỏng vấn với các nhà phát triển và phát hiện rằng họ không yêu cầu một mô hình đề xuất hoàn hảo để mô hình hữu ích [WMH+21], trong khi Ziegler et al. khảo sát hơn 2,000 người dùng Copilot và phát hiện rằng họ cảm thấy năng suất hơn khi sử dụng Copilot [ZKL+22]. Một nghiên cứu của Google phát hiện rằng một mô hình Copilot nội bộ đã giảm 6% 'thời gian lặp mã hóa' [TN22]. Mặt khác, một nghiên cứu về 24 người tham gia của Vaithilingam et al. không cho thấy cải thiện đáng kể trong thời gian hoàn thành nhiệm vụ, tuy nhiên người tham gia đã tuyên bố sự ưa thích rõ ràng cho Copilot [VZG22]. [BJP22] cho thấy rằng tương tác với Copilot rơi vào hai danh mục rộng: lập trình viên hoặc ở 'chế độ tăng tốc' hoặc ở 'chế độ khám phá'. Một chiến lược tương tự để ẩn gợi ý mã có chọn lọc đã được đề xuất trong [SDS+22] (Ước lượng Chất lượng Trước khi Hoàn thành, QEBC). Tuy nhiên, QEBC [SDS+22] không dựa trên phản hồi con người về chấp nhận gợi ý, mà dựa trên việc xây dựng bộ ước lượng học được về chất lượng hoàn thành mã từ bộ dữ liệu các phân đoạn mã được ghép nối và hoàn thành mô hình. Ngược lại, bộ ước lượng CDHF của chúng tôi sử dụng dữ liệu hành vi lập trình viên thực và dựa trên dữ liệu từ hệ thống đề xuất mã đang được sử dụng hiện tại (Copilot) thay vì các hệ thống được huấn luyện tùy chỉnh trong [SDS+22].

Phản hồi con người. Tích hợp sở thích con người khi huấn luyện các mô hình dựa trên học máy đã được nghiên cứu lâu trong tài liệu [KS08,MHL+17]. Đặc biệt, Học Tăng cường từ Phản hồi Con người là một phương pháp nơi nhà thiết kế đầu tiên thu thập sở thích con người rõ ràng về các hành động được sử dụng để cải thiện mô hình sử dụng RL [CLB+17]. Gần đây hơn, phương pháp này đã được sử dụng để cải thiện LLM cho các nhiệm vụ khác nhau [ZSW+19,BJN+22] đặc biệt bao gồm ChatGPT [Ope22] và bao gồm ba bước: thu thập sở thích con người về các tùy chọn, huấn luyện mô hình phần thưởng về sở thích con người, sử dụng mô hình phần thưởng để cập nhật LLM sử dụng RL. Ngược lại, phương pháp CDHF của chúng tôi sử dụng phản hồi con người ngầm thông qua telemetry được thu thập dễ dàng hơn không phản ánh đầy đủ sở thích thực sự và tránh cập nhật LLM. Lọc cộng tác sử dụng dữ liệu sở thích con người để xếp hạng lại nội dung [SK09], mặc dù tránh sự phức tạp của việc tạo và xếp hạng nội dung được yêu cầu ở đây. Các công thức lý thuyết của chúng tôi trong Phần 4 xây dựng trên công việc của giao diện người dùng tương tác [Hor99,DK18] và tổng quát hóa chúng cho môi trường của chúng tôi. Công việc về hoãn thuật toán [MPZ18,MS20,ODR21] điều tra một câu hỏi tương tự về liệu con người hay AI nên thực hiện nhiệm vụ và dựa trên ước tính lỗi (thay vì ước tính chi phí thời gian ở đây) của con người so với AI, cũng như công việc về chính sách hỗ trợ quyết định cá nhân hóa [BCC+23] điều tra liệu hỗ trợ có nên được hiển thị nhưng không phải khi nào.

--- TRANG 17 ---
B Dẫn xuất P*
Mệnh đề 1. Dưới các giả định của mô hình lập trình viên, đặc biệt là lập trình viên dành nhiều thời gian hơn để viết mã khi họ từ chối gợi ý so với khi họ chấp nhận gợi ý và chỉnh sửa nó. Với mã, gợi ý và trạng thái tiềm ẩn cụ thể (X, S, ϕ), nếu xác suất chấp nhận P(A=accept|X, S, ϕ) gợi ý của lập trình viên dưới P* được định nghĩa là:
P*=E[verification] + E[τ|X, ϕ]
E[writing |A= reject] −E[editing |A= accept]
thì gợi ý không nên được hiển thị. Lưu ý rằng P* được định nghĩa như một hàm P*(X, S, ϕ) được đánh giá theo điểm.

Chứng minh. Bắt đầu từ phương trình δ= 0, giả sử lập trình viên chỉ có hai hành động chấp nhận hoặc từ chối. Điều này được hỗ trợ bởi phân tích mẫu telemetry của chúng tôi nơi chúng tôi nhận thấy rằng ít hơn 1% gợi ý được duyệt. Đầu tiên chúng tôi có:
−(E[verification |X, S, ϕ] +P(A=accept |X, S, ϕ)·E[editing |X, S, ϕ, A = accept]
+P(A=reject|X, S, ϕ)·E[writing |X, S, ϕ, A = reject]) + E[writing |X, ϕ]−E[τ|X, ϕ] = 0
Bây giờ chúng tôi thay thế P(A=accept |X, S, ϕ) bằng P*(X, S, ϕ) và di chuyển các thuật ngữ:
P*(X, S, ϕ)·(−E[editing |X, S, ϕ, A = accept] + E[writing |X, S, ϕ, A = reject])
=E[verification |X, S, ϕ] +E[writing |X, S, ϕ, A = reject] −E[writing |X, ϕ] +E[τ|X, ϕ]
Giả sử rằng E[writing |X, S, ϕ, A =reject]−E[editing |X, S, ϕ, A =accept] >0, có nghĩa là khi gợi ý được chấp nhận, thời gian chỉnh sửa cho chúng ít hơn thời gian để viết gợi ý. Sau đó chúng tôi có thể tách P*(X, S, ϕ) sang LHS với tương đương đầy đủ (bình đẳng vẫn giữ mà không có giả định này):
P*(X, S, ϕ) =E[verification |X, S, ϕ] +E[writing |X, S, ϕ, A = reject] −E[writing |X, ϕ] +E[τ|X, ϕ]
E[writing |X, S, ϕ, A = reject] −E[editing |X, S, ϕ, A = accept]
Lưu ý rằng P*(X, S, ϕ) không nhất thiết là xác suất hợp lệ trong [0,1]. Nếu chúng tôi giả sử rằng nếu lập trình viên từ chối gợi ý, họ không hưởng lợi gì từ nó khi viết mã sau đó có nghĩa là E[writing |X, S, ϕ, A = reject] = E[writing |X, ϕ], chúng tôi đến:
P*(X, S, ϕ) =E[verification |X, S, ϕ] +E[τ|X, ϕ]
E[writing |X, S, ϕ, A = reject] −E[editing |X, S, ϕ, A = accept]

--- TRANG 18 ---
Giả sử độ trễ bằng không có nghĩa là τ= 0, thì rõ ràng P*(X, S, ϕ) = 0 nếu và chỉ nếu thời gian xác minh không đáng kể: E[verification |X, S, ϕ] = 0, và theo giả định đó là P*(X, S, ϕ)≥0 vì mẫu số dương. Hàm ý là nếu P(A=accept |X, S, ϕ)≤P*(X, S, ϕ), chúng tôi không nên hiển thị gợi ý tuân theo trực tiếp từ phương trình δ≤0, và tương tự nếu P(A=accept |X, S, ϕ)≥P*(X, S, ϕ) chúng tôi hiển thị gợi ý.

Để tái phát biểu, dẫn xuất này đã đưa ra các giả định sau:
•Lập trình viên chỉ có hai hành động chấp nhận hoặc từ chối.
•E[writing |X, S, ϕ, A =reject]−E[editing |X, S, ϕ, A =accept] >0, có nghĩa là khi gợi ý được chấp nhận, thời gian chỉnh sửa cho chúng ít hơn thời gian để viết gợi ý.
•Nếu chúng tôi giả sử rằng nếu lập trình viên từ chối gợi ý, họ không hưởng lợi gì từ nó khi viết mã sau đó có nghĩa là E[writing |X, S, ϕ, A = reject] = E[writing |X, ϕ].

C Đánh giá và Phân tích Mô hình
Tất cả thí nghiệm được chạy với Python 3.8 trên máy với GPU A100 đơn. Khoảng tin cậy trong thân được thu được bằng bootstrapping với 1000 mẫu bootstrap cho độ chính xác và AUC.

Trong bảng 1 chúng tôi đánh giá hiệu suất của các mô hình khác nhau để dự đoán việc chấp nhận gợi ý. Chúng tôi sử dụng Scikit-learn [PVG+11] để huấn luyện các mô hình Random Forest và hồi quy logistic, thư viện XGBoost để huấn luyện các mô hình XGB [CG16] và PyTorch để huấn luyện mạng thần kinh được kết nối đầy đủ [PGM+19]. FCNN là mạng 3 lớp với 50 đơn vị ẩn được huấn luyện sử dụng Adam với tỷ lệ học 1e−2. Chúng tôi đã sử dụng tập xác thực để chọn số đơn vị ẩn cũng như chọn mô hình tốt nhất sau khi huấn luyện cho 100 epochs. Chúng tôi không sử dụng bất kỳ điều chỉnh siêu tham số nào cho các mô hình XGBoost và sử dụng các tham số tiêu chuẩn.

Bảng 1: So sánh các bộ phân loại khác nhau trên tập kiểm tra dựa trên AU-ROC, độ chính xác và macro f1 cho mô hình đầy đủ để dự đoán việc chấp nhận gợi ý lập trình viên. FCNN đề cập đến mạng thần kinh được kết nối đầy đủ.
Phương pháp AU-ROC Độ chính xác (%) Macro F1 (%)
XGBoost 0.780 81.1 64
Hồi quy Logistic 0.726 79.5 58
Random Forest 0.756 80.9 61
FCNN 0.741 80.2 59
Baseline 0.5 78.9 44

Trong hình 7 chúng tôi trình bày rằng mô hình giai đoạn 2 m2 được hiệu chỉnh tốt.

--- TRANG 19 ---
0.0 0.2 0.4 0.6 0.8 1.0
Xác suất dự đoán trung bình trong mỗi bin0.00.20.40.60.81.0Tỷ lệ dương tính
XGBoost, ECE=0.10
Lý tưởng được hiệu chỉnhHình 7: Đường cong hiệu chỉnh cho mô hình XGBoost giai đoạn 2.

Độ phức tạp mẫu. Để hiểu chúng tôi có thể cần bao nhiêu mẫu để huấn luyện các mô hình CDHF cá nhân hóa, chúng tôi thực hiện phân tích độ phức tạp mẫu trên mô hình dự đoán chấp nhận giai đoạn 2 nơi chúng tôi huấn luyện với một phần ngẫu nhiên của bộ dữ liệu huấn luyện trong Hình 8. Với 1% bộ dữ liệu huấn luyện tương đương với 1688 mẫu huấn luyện, hiệu suất đạt 0.69 AU-ROC. Với 25% dữ liệu huấn luyện, mô hình có thể đạt 0.75 AU-ROC.

0.0 0.2 0.4 0.6 0.8 1.0
Phần kích thước dữ liệu huấn luyện0.640.660.680.700.720.740.760.78AUC
Hình 8: Phân tích độ phức tạp mẫu của mô hình XGBoost giai đoạn 2 khi được huấn luyện trên một phần của dữ liệu huấn luyện và vẽ biểu đồ AU-ROC trên tập kiểm tra đầy đủ.

Các yếu tố ảnh hưởng đến hành động lập trình. Phân tích mô hình XGBoost giai đoạn 2, tiết lộ các yếu tố tương quan với quyết định của lập trình viên. Chúng tôi kiểm tra trọng số tầm quan trọng tính năng (Hình 9). Cụ thể, chúng tôi báo cáo số đếm F-score tính năng, nắm bắt tần suất mỗi tính năng được chia trong bất kỳ cây nào trong mô hình XGBoost. Hai tính năng quan trọng nhất là độ tin cậy của gợi ý từ mô hình gợi ý cốt lõi của Copilot, và độ dài của gợi ý. Cùng nhau, hai tính năng này mang lại một mô hình đạt AUROC 0.71. Các tính năng quan trọng khác bao gồm bối cảnh của sự kiện hiện tại trong phiên mã hóa như liệu gợi ý cuối có được chấp nhận hay không. Chúng tôi cũng thấy rằng các yếu tố văn bản của gợi ý quan trọng. Ví dụ, tính năng chỉ ra liệu gợi ý hiện tại có bao gồm ký tự '#' (được sử dụng để chỉ ra bình luận trong Python) đã được chia tổng cộng 8 lần. Những tính năng này không nên được diễn giải theo cách nguyên nhân mà là tương quan với hành vi của lập trình viên.

0 10 20 30 40 50 60 70Gợi ý bao gồm '#'Sự kiện cuối là ChấpnhậnSố sự kiện được hiển thị trong 10 cáiĐộ dài PromptChỉ số hiện tạiĐộ dài gợi ýCodeRec Confidence
Tầm quan trọng tính năng (điểm F)
# bình luận
0.913
1.
2.
3. (hiện tại)
1.
2. Được chấp nhận
3. (hiện tại)
Hình 9: Tầm quan trọng tính năng cho bảy tính năng độc đáo được đánh giá cao nhất của mô hình để dự đoán khả năng chấp nhận gợi ý. Tầm quan trọng tính năng theo điểm F đếm tần suất một tính năng được chia trên trong tập hợp cây.

Phân tích gợi ý. Mô hình được học trên mẫu lớn telemetry cũng có thể được áp dụng cho telemetry được thu thập trong nghiên cứu người dùng 21 người tham gia của [MBFH22]. Khi chúng tôi đánh giá mô hình trên 1029 sự kiện được chấp nhận và từ chối của nghiên cứu người dùng, chúng tôi thu được AU-ROC 0.73. Kiểm tra những kết quả này thêm, chúng tôi thấy rằng có ít nhất hai cụm được xác định cho các gợi ý dự đoán có khả năng bị từ chối nhất: (1) gợi ý ký tự đơn không chữ cái như (,),[,:,;, và (2) hoàn thành giữa từ như 'agonal()' (hoàn thành của 'diagonal()'), 'lass LogisticRegression:' (hoàn thành của 'class Logistic Regression:'). Chúng tôi giả thuyết rằng đối với cụm (1) các gợi ý quá nhỏ để được chú ý. Đối với cụm (2), chúng tôi giả thuyết rằng lập trình viên đã đang trong hành động gõ, vì vậy các gợi ý có thể là sự phân tâm (tức là, chi phí gián đoạn nhiều hơn được tiết kiệm bằng cách loại bỏ hành động vật lý gõ một số keystrokes đã được xác định).

D Gợi ý nào nên hiển thị: Biểu đồ
Theo phần cuối của bài báo, chúng tôi hiển thị hai biểu đồ cho mỗi k∈ {0,1,2,3}: 1) biểu đồ hiển thị trong phân vị độ dài nào về giải pháp sự thật cơ bản gợi ý với xác suất chấp nhận cao nhất (theo mô hình) nằm và 2) xác suất chấp nhận được chuẩn hóa theo độ dài của gợi ý (đối với mỗi ví dụ, chúng tôi chuẩn hóa xác suất chấp nhận thô bằng xác suất chấp nhận tối đa trên tất cả độ dài cho ví dụ nhất định - chúng tôi quan sát cùng xu hướng mà không chuẩn hóa).

--- TRANG 20 ---
0.0 0.2 0.4 0.6 0.8 1.0
Phần trăm chỉ số tối đa024681012Số lượng(a) Biểu đồ vị trí gợi ý tối đa
0 20 40 60 80
Phân vị của độ dài0.4500.4750.5000.5250.5500.5750.6000.625Điểm trung bình được chuẩn hóa
 Đường xu hướng bậc hai (b) Xác suất chấp nhận theo độ dài
Hình 10: Biểu đồ cho thí nghiệm xếp hạng gợi ý theo xác suất chấp nhận. Biểu đồ (a) hiển thị trong thùng phân vị độ dài nào gợi ý tối đa hóa nằm và Biểu đồ (b) hiển thị điểm chấp nhận bằng cách tăng độ dài của gợi ý. Những biểu đồ này dành cho k= 0 (chỉ docstring)

0.0 0.2 0.4 0.6 0.8 1.0
Phần trăm chỉ số tối đa0510152025303540Số lượng
(a) Biểu đồ vị trí gợi ý tối đa
0 20 40 60 80
Phân vị của độ dài0.540.560.580.600.620.640.66Điểm trung bình được chuẩn hóa
 Đường xu hướng bậc hai (b) Xác suất chấp nhận theo độ dài
Hình 11: Biểu đồ cho thí nghiệm xếp hạng gợi ý theo xác suất chấp nhận. Biểu đồ (a) hiển thị trong thùng phân vị độ dài nào gợi ý tối đa hóa nằm và Biểu đồ (b) hiển thị điểm chấp nhận bằng cách tăng độ dài của gợi ý. Những biểu đồ này dành cho k= 1 (docstring + dòng đầu tiên của giải pháp)

--- TRANG 21 ---
0.0 0.2 0.4 0.6 0.8 1.0
Phần trăm chỉ số tối đa01020304050Số lượng(a) Biểu đồ vị trí gợi ý tối đa
0 20 40 60 80
Phân vị của độ dài0.580.600.620.640.660.680.700.72Điểm trung bình được chuẩn hóa
 Đường xu hướng bậc hai (b) Xác suất chấp nhận theo độ dài
Hình 12: Biểu đồ cho thí nghiệm xếp hạng gợi ý theo xác suất chấp nhận. Biểu đồ (a) hiển thị trong thùng phân vị độ dài nào gợi ý tối đa hóa nằm và Biểu đồ (b) hiển thị điểm chấp nhận bằng cách tăng độ dài của gợi ý. Những biểu đồ này dành cho k= 2 (docstring + hai dòng đầu tiên của giải pháp)

0.0 0.2 0.4 0.6 0.8 1.0
Phần trăm chỉ số tối đa0102030405060Số lượng
(a) Biểu đồ vị trí gợi ý tối đa
0 20 40 60 80
Phân vị của độ dài0.580.600.620.640.660.680.700.72Điểm trung bình được chuẩn hóa
 Đường xu hướng bậc hai (b) Xác suất chấp nhận theo độ dài
Hình 13: Biểu đồ cho thí nghiệm xếp hạng gợi ý theo xác suất chấp nhận. Biểu đồ (a) hiển thị trong thùng phân vị độ dài nào gợi ý tối đa hóa nằm và Biểu đồ (b) hiển thị điểm chấp nhận bằng cách tăng độ dài của gợi ý. Những biểu đồ này dành cho k= 3 (docstring + ba dòng đầu tiên của giải pháp)
