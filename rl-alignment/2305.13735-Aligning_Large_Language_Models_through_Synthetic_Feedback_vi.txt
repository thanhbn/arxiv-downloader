# 2305.13735.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2305.13735.pdf
# Kích thước file: 1471036 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Căn chỉnh Mô hình Ngôn ngữ Lớn thông qua Phản hồi Tổng hợp
Sungdong Kim1,2,3Sanghwan Bae1Jamin Shin1,2
Soyoung Kang1Donghyun Kwak1Kang Min Yoo1,2,4Minjoon Seo3
NA VER Cloud1NA VER AI Lab2KAIST AI3SNU AI Center4
{sungdong.kim, sanghwan.bae, jamin.shin}@navercorp.com
{soyoung.kang, donghyun.kwak, kangmin.yoo}@navercorp.com
minjoon@kaist.ac.kr

Tóm tắt
Việc căn chỉnh các mô hình ngôn ngữ lớn (LLM) với giá trị con người đã trở nên ngày càng quan trọng vì nó cho phép điều khiển tinh vi các LLM. Tuy nhiên, điều này đòi hỏi các minh chứng và phản hồi của con người đáng kể hoặc chưng cất từ các LLM độc quyền như ChatGPT. Trong công trình này, chúng tôi đề xuất một khung học tập căn chỉnh mới với phản hồi tổng hợp không phụ thuộc vào các chú thích con người rộng rãi và các LLM độc quyền. Đầu tiên, chúng tôi thực hiện mô hình hóa phần thưởng (RM) với phản hồi tổng hợp bằng cách đối chiếu các phản hồi từ các LLM vanilla với các kích thước và prompt khác nhau. Sau đó, chúng tôi sử dụng RM để mô phỏng các minh chứng chất lượng cao để huấn luyện một chính sách có giám sát và tối ưu hóa thêm mô hình với học tăng cường. Mô hình kết quả của chúng tôi, Mô hình Ngôn ngữ Căn chỉnh với Bộ dữ liệu Huấn luyện Tổng hợp (ALMoST), vượt trội hơn các mô hình mã nguồn mở gần đây, được huấn luyện trên đầu ra của InstructGPT hoặc các minh chứng được chú thích bởi con người, trong các benchmark căn chỉnh. Trong đánh giá con người, mô hình của chúng tôi được ưa thích hơn Alpaca và Dolly-v2, lần lượt là 55.0% và 58.5% thời gian. Các phân tích thêm chứng minh hiệu quả và tầm quan trọng của phản hồi tổng hợp trong khung của chúng tôi1.

1 Giới thiệu
Học tập căn chỉnh đã là một sơ đồ học tập thiết yếu để căn chỉnh hành vi của các mô hình ngôn ngữ lớn (LLM) với các giá trị con người như an toàn và chân thực trong khi tuân theo ý định của người dùng một cách chính xác (Ouyang et al., 2022). Các LLM vanilla - những mô hình chưa được căn chỉnh - có thể hiểu sai ý định người dùng hoặc tạo ra các phản hồi không an toàn và không chính xác. Các giá trị con người mong muốn như tính hữu ích, vô hại, hoặc trung thực có thể được định nghĩa, và các minh chứng con người với những giá trị này sau đó được sử dụng cho việc học tập căn chỉnh (Askell et al., 2021; Bai et al., 2022a).

1Mã nguồn có sẵn tại github.com/naver-ai/almost.

Hình 1: Một quy trình mô hình hóa phần thưởng thông qua phản hồi tổng hợp. Chúng tôi giả định rằng phản hồi từ LLM lớn hơn với nhiều và tốt hơn các minh chứng có thể cho phản hồi tốt hơn tổng thể. Chúng tôi huấn luyện một mô hình phần thưởng với các so sánh tổng hợp được tạo ra dựa trên giả định này.

Thông thường, học tập căn chỉnh bao gồm ba giai đoạn: tinh chỉnh có giám sát (SFT), mô hình hóa phần thưởng (RM), và học tăng cường từ phản hồi con người (RLHF) (Ouyang et al., 2022; Bai et al., 2022a).

Tuy nhiên, công thức huấn luyện ba giai đoạn đòi hỏi nỗ lực con người đáng kể, đặc biệt là trong hai giai đoạn đầu. Cụ thể hơn, cả hai giai đoạn huấn luyện SFT và RM đều phải được cung cấp với một lượng lớn các minh chứng con người chất lượng cao và bộ dữ liệu xếp hạng để có được các mô hình để tạo điều kiện cho RLHF. Ví dụ, Ouyang et al. (2022) chuẩn bị và sử dụng 13k minh chứng con người và 33k so sánh.

Mặt khác, Self-Instruct (Wang et al., 2022) cố gắng tạo ra các bộ dữ liệu hướng dẫn tự tạo tổng hợp bằng cách sử dụng học trong ngữ cảnh với một vài minh chứng hạt giống. Trong khi đó, việc phát hành LLaMA (Touvron et al., 2023) mang lại nhiều LLM căn chỉnh mã nguồn mở được huấn luyện trên đầu ra của các LLM độc quyền hoặc hướng dẫn được chú thích bởi con người. Tuy nhiên, nó vẫn phụ thuộc nặng vào các API LLM độc quyền như InstructGPT và ChatGPT (Ouyang et al., 2022; OpenAI, 2023; Taori et al., 2023; Chiang et al., 2023) hoặc các chú thích con người chuyên sâu (DataBricks, 2023; Köpf et al., 2023).

Trong bài báo này, chúng tôi giới thiệu một khung mới cho học tập căn chỉnh chỉ đòi hỏi lao động con người tối thiểu và không phụ thuộc vào các LLM độc quyền. Không giống như quy trình học tập căn chỉnh thông thường thu thập minh chứng đầu tiên (Ouyang et al., 2022), chúng tôi đầu tiên phát triển một mô hình phần thưởng (RM) trên một bộ dữ liệu so sánh tổng hợp được xây dựng bằng cách đối chiếu đầu ra từ các LLM vanilla trong các cấu hình khác nhau, như được hiển thị trong Hình 1. Các quy tắc tạo ra dữ liệu xếp hạng tổng hợp này bắt nguồn từ giả thuyết của chúng tôi rằng các phản hồi được tạo ra bởi các mô hình lớn hơn, được prompt tối ưu vượt trội hơn những phản hồi được tạo ra bởi các mô hình nhỏ hơn, được prompt không đầy đủ, như được báo cáo bởi công trình trước đây (Askell et al., 2021). Sau đó, chúng tôi giới thiệu Tự chơi Hướng dẫn bởi Mô hình Phần thưởng (RMSP) để mô phỏng các minh chứng chất lượng cao với lấy mẫu từ chối sử dụng RM (Ouyang et al., 2022). Chúng tôi huấn luyện LLaMA-7B (Touvron et al., 2023) trên các minh chứng tổng hợp (SFT) và tối ưu hóa thêm mô hình với phần thưởng từ RM tổng hợp, cụ thể là, Học Tăng cường từ Phản hồi Tổng hợp (RLSF).

Mô hình Ngôn ngữ Căn chỉnh với Bộ dữ liệu Huấn luyện Tổng hợp (ALMoST) của chúng tôi vượt trội hơn Alpaca (Taori et al., 2023) - được chưng cất từ InstructGPT (Ouyang et al., 2022) - và Dolly-v2 (DataBricks, 2023) và OpenAssistant (Köpf et al., 2023) được huấn luyện trên các minh chứng được chú thích bởi con người trong các benchmark liên quan đến căn chỉnh (Askell et al., 2021; Lin et al., 2021; Chiang et al., 2023). Đáng chú ý, mô hình của chúng tôi được ưa thích hơn các mô hình mã nguồn mở gần đây, Alpaca và Dolly-v2, 55-58% thời gian (tỷ lệ thắng) trong đánh giá con người mà không có chưng cất từ các LLM độc quyền cũng như không có các chú thích con người chuyên sâu. Chúng tôi suy đoán hiệu suất mạnh mẽ của mô hình chúng tôi là do các chỉ báo thực nghiệm của hành vi được căn chỉnh tốt đã được kết hợp hiệu quả vào một mô hình nền tảng mạnh mẽ thông qua phản hồi tổng hợp, cho phép khả năng tự căn chỉnh vốn có được khai thác và thay thế một phần nhu cầu về phản hồi con người.

Các đóng góp chính của chúng tôi gồm ba phần:
• Chúng tôi đề xuất một khung học tập căn chỉnh mới bằng cách giới thiệu phản hồi tổng hợp. Nó tự động xây dựng các so sánh và minh chứng chất lượng cao mà không dựa vào phản hồi con người và các LLM độc quyền.
• Mô hình kết quả của chúng tôi, ALMoST, thể hiện hành vi được căn chỉnh tốt với các giá trị con người trong các benchmark căn chỉnh. Trong nghiên cứu con người, ALMoST được ưa thích hơn Alpaca và Dolly-v2, thể hiện tỷ lệ thắng 55-58%.
• Phân tích về RM thêm chứng minh hiệu quả của phản hồi tổng hợp và làm nổi bật tầm quan trọng của việc tiêm các tiên nghiệm thực nghiệm, ví dụ, phương pháp lọc được đề xuất của chúng tôi và thiết kế prompt trung thực.

2 Phương pháp
Trong phần này, chúng tôi sẽ mô tả các quy trình chi tiết của khung của chúng tôi như được mô tả trong Hình 2.

2.1 Bước 1: Mô hình hóa Phần thưởng với Phản hồi Tổng hợp

Baseline được Prompt Vì chúng tôi không có các baseline được căn chỉnh sẵn có cho các so sánh, chúng tôi sử dụng prompt HHH (Hữu ích, Vô hại, và Trung thực) được thiết kế bởi Askell et al. (2021). Nó chứa 14 cuộc hội thoại được viết bởi con người để hướng dẫn căn chỉnh LLM2. Chúng tôi sử dụng các mô hình LLaMA được prompt HHH để tạo ra các so sánh tổng hợp (Touvron et al., 2023).

Tạo ra So sánh Tổng hợp Thay vì thu thập phản hồi con người, chúng tôi tạo ra các so sánh tổng hợp dựa trên các giả định ngây thơ theo quan sát thực nghiệm. Askell et al. (2021) chứng minh rằng mô hình lớn hơn hoạt động tốt hơn một chút so với mô hình nhỏ hơn, và mô hình với prompt dài hơn tốt hơn mô hình với prompt ngắn hơn về mặt sở thích con người. Tóm lại, chúng tôi giả định chất lượng của phản hồi tuân theo quy tắc ngón tay cái:
• Mô hình lớn hơn > Mô hình nhỏ hơn
• Nhiều few-shot hơn > Ít few-shot hơn
• Minh chứng tốt hơn > Minh chứng tệ hơn

2gist.github.com/jareddk/2509330...

--- TRANG 3 ---

Hình 2: Tổng quan về khung được đề xuất của chúng tôi cho học tập căn chỉnh của LLM. Bước 1. Chúng tôi đầu tiên thực hiện mô hình hóa phần thưởng với một bộ dữ liệu so sánh được tạo ra tổng hợp (phản hồi tổng hợp). Bước 2. Bộ dữ liệu minh chứng được tạo ra bằng mô phỏng với sự hướng dẫn của mô hình phần thưởng và huấn luyện chính sách có giám sát với các minh chứng tổng hợp. Bước 3. Chúng tôi tối ưu hóa thêm mô hình chống lại mô hình phần thưởng với học tăng cường.

Đối với cùng một input x, chúng tôi đầu tiên lấy mẫu các phản hồi Y={y1, y2, ..., y|Y|} từ các mô hình với các cấu hình khác nhau. Sau đó, chúng tôi áp dụng quy tắc để chọn phản hồi tốt hơn trong số các phản hồi được tạo ra. Cụ thể hơn, chúng tôi bao gồm các LLM {7,13,30}B với {1,3,5} shot của các minh chứng HHH cho việc so sánh. Như được minh họa trong Hình 1, nếu chúng tôi lấy mẫu phản hồi từ (1) 30B với 5 shot, (2) 13B với 3 shot, và (3) 7B với 1 shot, việc xếp hạng trở thành y1 > y2 > y3 theo quy tắc ngón tay cái của chúng tôi. Sau đó, chúng tôi có thể có một tập hợp các so sánh nhị phân, ví dụ, {(y1, y2),(y2, y3),(y1, y3)}. Chúng tôi ký hiệu cái trước là phản hồi 'được chọn' (y1) và cái sau là phản hồi 'bị từ chối' (y2) trong một cặp so sánh (y1, y2).

Xác thực Hậu kỳ Các giả định của chúng tôi thường sai vì bản chất ngẫu nhiên của việc tạo ra dựa trên prompt. Những nhiễu này trong bộ dữ liệu làm cho mô hình hóa phần thưởng không ổn định và phân kỳ cuối cùng. Do đó, chúng tôi đưa ra phương pháp xác thực hậu kỳ để lọc ra những nhiễu như vậy.

Đầu tiên, chúng tôi thiết kế Bộ lọc Heuristic (HF) dựa trên kiến thức tiên nghiệm. Nó loại bỏ các phản hồi xấu chứa hoặc bắt đầu với các từ khóa như "Tôi không biết" hoặc "well". Ngoài ra, chúng tôi thấy một cách thực nghiệm rằng phản hồi tốt hơn thường có độ dài dài hơn phản hồi tệ hơn. Đặc biệt nếu phản hồi ngắn, nó thường có xu hướng là trường hợp thất bại tạo ra xác suất. Tuy nhiên, việc huấn luyện RM chỉ trên các so sánh với phản hồi được chọn dài hơn sẽ làm cho mô hình kết quả bị thiên vị theo độ dài. Do đó, chúng tôi áp dụng HF để lấy các cặp so sánh mà phản hồi được chọn dài hơn phản hồi bị từ chối hoặc M−S/2, trong đó M là trung bình, và S là độ lệch chuẩn của độ dài của Y ở mức ký tự. Ràng buộc độ dài này giảm xác suất rằng việc tạo ra ngắn sẽ là thất bại tạo ra ngẫu nhiên bằng cách kiểm tra xem độ dài của mỗi phản hồi có nằm trong khoảng tin cậy hay không. Hơn nữa, nó không rơi vào thiên vị độ dài. Vui lòng xem Phụ lục B để biết các ví dụ chi tiết. Chúng tôi sẽ chứng minh lợi ích trong Phần 4.2.

Thứ hai, chúng tôi tận dụng RM Nguyên trạng để lọc dữ liệu thêm. Cụ thể, chúng tôi huấn luyện một RM khác với bộ dữ liệu QA cộng đồng như StackExchange (Askell et al., 2021; Beeching et al., 2023). Nghiên cứu sơ bộ của chúng tôi không tìm thấy lợi ích của việc tiền huấn luyện quy mô lớn cho RM được thảo luận trong Askell et al. (2021). Do đó, chúng tôi lấy mẫu 20k cặp từ bộ dữ liệu StackExchange được tiền xử lý cho việc huấn luyện của chúng tôi3. Chúng tôi giữ các so sánh tổng hợp kết quả chỉ khi RM Nguyên trạng đồng ý với quyết định.

Mô hình hóa Phần thưởng Cuối cùng, chúng tôi huấn luyện mô hình phần thưởng dựa trên các so sánh tổng hợp được mô tả ở trên. Chúng tôi tuân theo mục tiêu mô hình hóa sở thích được xếp hạng từ các công trình trước đây (Askell et al., 2021; Ouyang et al., 2022). Mục tiêu là làm cho mô hình phần thưởng rtheta gán một giá trị vô hướng cho chất lượng tổng thể của một phản hồi yj cho một truy vấn đã cho x so với phản hồi baseline đối tác yk. Hàm mất mát được định nghĩa như sau:

J(theta) = −E(x,yj,yk)∼D log(sigma(rtheta(x, yj)−rtheta(x, yk)))

trong đó D là tập huấn luyện của các so sánh tổng hợp và rtheta(x, y) là đầu ra vô hướng của mô hình phần thưởng chỉ ra chất lượng phản hồi tổng thể y cho input x của nó.

Chi tiết Thực hiện Chúng tôi bắt đầu bằng việc tạo ra các truy vấn ban đầu, tức là, một tập hợp đa dạng các input x. Cụ thể, chúng tôi áp dụng công thức của Self-Instruct (Wang et al., 2022) để tạo ra 10k truy vấn ban đầu dựa trên học trong ngữ cảnh few-shot. Chi tiết hơn về việc tạo truy vấn có trong Phụ lục C.

Để tạo phản hồi, chúng tôi bao gồm năm mô hình được prompt với các cấu hình dưới đây.
• A. LLaMA-30B-Faithful-3shot
• B. LLaMA-30B-HHH-5shot
• C. LLaMA-13B-HHH-3shot
• D. LLaMA-7B-HHH-3shot
• E. LLaMA-7B-HHH-1shot

Đối với mỗi truy vấn, chúng tôi tạo ra năm phản hồi từ các mô hình và lấy xếp hạng, yA > yB > yC > yD > yE, phản ánh quy tắc ngón tay cái. Faithful chỉ ra các prompt được thiết kế thủ công của chúng tôi bao gồm ba cuộc hội thoại phản hồi trung thực và dài hơn trong khi xem xét định dạng phản hồi, và HHH chỉ ra các prompt được viết bởi Askell et al. (2021). Các ví dụ chi tiết có trong Phụ lục A. Cuối cùng, chúng tôi tạo ra 13k so sánh tổng hợp nhị phân sau xác thực hậu kỳ (HF và RM Nguyên trạng) và huấn luyện một mô hình phần thưởng với các so sánh tổng hợp.

2.2 Bước 2: Tinh chỉnh Có giám sát

Trong bước thứ hai, chúng tôi đề xuất Tự chơi Hướng dẫn bởi Mô hình Phần thưởng (RMSP) để mô phỏng các minh chứng chất lượng cao, tức là, các cuộc hội thoại giữa người dùng và trợ lý AI. Các minh chứng được mô phỏng được sử dụng để tinh chỉnh có giám sát cho mô hình chính sách được căn chỉnh ban đầu (SFT).

Tự chơi Mô phỏng cơ bản được kích hoạt bởi việc luân phiên giữa các mô hình vai trò người dùng và trợ lý với các prompt tương ứng, tức là, tự chơi. Chúng tôi tiếp tục sử dụng cùng baseline được prompt, LLaMA-30B-Faithful-3shot, cho vai trò trợ lý. Ngoài ra, chúng tôi đã thực hiện các điều chỉnh nhỏ đối với prompt HHH gốc (Askell et al., 2021) để phù hợp hơn với vai trò người dùng, LLaMA-30B-User-3shot4. Bắt đầu từ các truy vấn ban đầu được tạo ra trong giai đoạn đầu tiên, LLaMA-30B-Faithful-3shot tạo ra phản hồi cho các truy vấn. Sau đó, LLaMA-30B-User-3shot tiếp theo phản hồi của trợ lý. Việc luân phiên được tiếp tục cho đến lượt tối đa T.

Tự chơi Hướng dẫn bởi RM (RMSP) Để đảm bảo phản hồi được căn chỉnh hơn từ trợ lý, chúng tôi đề xuất bao gồm RM tổng hợp, được huấn luyện trong giai đoạn đầu tiên, vào vòng lặp, cụ thể là Tự chơi Hướng dẫn bởi Mô hình Phần thưởng (RMSP). Trong thiết lập này, mô hình trợ lý, LLaMA-30B-Faithful-3shot, đầu tiên lấy mẫu N phản hồi cho một ngữ cảnh hội thoại đã cho. Sau đó, RM cho điểm N phản hồi, và phản hồi được chấm điểm tốt nhất được chọn làm phản hồi cuối cùng cho mô phỏng, tức là, RM thực hiện lấy mẫu từ chối (lấy mẫu tốt nhất trong N) (Ouyang et al., 2022; Scheurer et al., 2023). Giống như Tự chơi, việc luân phiên với LLaMA-30B-User-3shot được tiếp tục cho đến lượt tối đa. Vui lòng xem Hình 10 để biết các ví dụ.

Chi tiết Thực hiện Chúng tôi tạo ra khoảng 20k minh chứng chất lượng cao sử dụng RMSP. Chúng tôi đặt lượt tối đa là 2 cho đơn giản, tập trung vào tình huống một lượt. Số lần lấy mẫu từ chối N được đặt là 4 xem xét ràng buộc tài nguyên5. Sau đó, chúng tôi huấn luyện LLaMA-7B trên các minh chứng được tạo ra, tức là, tinh chỉnh chính sách có giám sát (SFT). Chi tiết huấn luyện thêm có trong Phụ lục D.

2.3 Bước 3: Học Tăng cường từ Phản hồi Tổng hợp (RLSF)

Trong giai đoạn cuối, chúng tôi thực hiện học tăng cường từ phản hồi tổng hợp (RLSF) để căn chỉnh thêm mô hình SFT sử dụng tín hiệu phần thưởng từ RM tổng hợp. Theo các công trình trước đây (Ouyang et al., 2022; Bai et al., 2022a), chúng tôi sử dụng Tối ưu hóa Chính sách Gần kề (PPO) (Schulman et al., 2017). Trong giai đoạn này, một chính sách piϕ tự hồi quy tạo ra một phản hồi y cho một prompt x. Sau đó, một điểm số phần thưởng rtheta(x, y) được xác định bởi mô hình phần thưởng rtheta. Mục tiêu huấn luyện là tối đa hóa phần thưởng kỳ vọng.

Ex∼D,y∼piϕ(·|x)[rtheta(x, y)]

Stiennon et al. (2020) đề xuất rằng thêm một số hạng phạt KL ước tính giữa chính sách ban đầu rho và chính sách piϕ vào rtheta(x, y) có thể tăng cường hiệu suất. Điều chỉnh này dẫn đến mục tiêu cuối cùng như sau:

Ex∼D,y∼piϕ(·|x)[rtheta(x, y)−lambda log piϕ(y|x)/rho(y|x)],

trong đó lambda là hệ số KL.

Chi tiết Thực hiện Chúng tôi khởi tạo chính sách rho với LLaMA-7B được tinh chỉnh SFT từ Bước 2. Ngoài ra, các prompt cho huấn luyện PPO được biên dịch bằng cách trích xuất chỉ các input (truy vấn ban đầu) từ bộ dữ liệu minh chứng được tạo ra bởi RMSP được mô tả trong Phần 2.2. Chi tiết thêm cho huấn luyện PPO có trong Phụ lục D.

3 Đánh giá Căn chỉnh của ALMoST

Chúng tôi xác thực mô hình kết quả của chúng tôi, Mô hình Ngôn ngữ Căn chỉnh với Bộ dữ liệu Huấn luyện Tổng hợp (ALMoST), trong ba benchmark căn chỉnh, đánh giá HHH Tĩnh (Askell et al., 2021), TruthfulQA (Lin et al., 2021), và Câu hỏi Vicuna (Chiang et al., 2023).

3.1 Bộ dữ liệu

Căn chỉnh HHH Tĩnh và TruthfulQA Askell et al. (2021) giới thiệu benchmark căn chỉnh HHH Tĩnh để đo lường mức độ các mô hình được căn chỉnh tốt với các giá trị con người6. Tương tự như bộ dữ liệu so sánh, mô hình nên chọn một phản hồi phù hợp hơn cho input giữa hai tùy chọn dựa trên các giá trị con người. Bộ dữ liệu bao gồm ba danh mục giá trị con người, hữu ích, vô hại, và trung thực, và chứa một danh mục misc (khác). Chúng tôi bao gồm bộ dữ liệu để có được mối quan hệ căng thẳng giữa các giá trị con người từ đánh giá, mặc dù toàn bộ bộ dữ liệu chỉ có 221. Lin et al. (2021) đề xuất TruthfulQA để đo lường cách LLM tạo ra câu trả lời chân thực cho một câu hỏi đã cho7. Nó đặc biệt chứa 817 câu hỏi đối kháng để gợi ra câu trả lời mô phỏng sai lầm từ các LLM. Để đơn giản, chúng tôi đánh giá các mô hình với thiết lập trắc nghiệm (MC1) thay vì thiết lập tạo sinh. Lưu ý rằng tất cả đánh giá đều dựa trên zero-shot, có nghĩa là chúng tôi không tinh chỉnh bộ dữ liệu mục tiêu. Vui lòng xem Phụ lục I để biết chi tiết thêm về prompt đánh giá.

Câu hỏi Vicuna Chúng tôi kiểm tra các mô hình của chúng tôi sử dụng câu hỏi đánh giá Vicuna (Chiang et al., 2023). Nó bao gồm 80 câu hỏi về các chủ đề khác nhau bao gồm QA tổng quát, viết, lý luận, v.v., để xác định sở thích người dùng8. Đầu tiên, hai mô hình khác nhau tạo

6github.com/google/BIG-bench
7github.com/sylinrl/TruthfulQA
8github.com/lm-sys/FastChat

--- TRANG 6 ---

Hình 3: (a) Đánh giá con người và (b) kết quả đánh giá GPT-4 trong các mô hình 7B trên Câu hỏi Vicuna (Chiang et al., 2023). Nó hiển thị tỷ lệ phần trăm thắng, hòa, và thua của ALMoST-PPO so với các mô hình khác.

ra một câu trả lời cho mỗi câu hỏi giống nhau. Sau đó, chúng tôi tiến hành các kiểm tra A/B của con người để chọn một câu trả lời được ưa thích giữa các câu trả lời từ hai mô hình. Cụ thể, chúng tôi tuyển dụng ba công nhân cho mỗi kiểm tra. Chúng tôi yêu cầu các công nhân chọn câu trả lời hữu ích, vô hại, và trung thực hơn với sự xem xét cẩn thận về nội dung trong các câu trả lời (Askell et al., 2021). Vui lòng xem Phụ lục E.1 để biết chi tiết thêm về đánh giá con người. Ngoài ra, chúng tôi tiến hành đánh giá tự động sử dụng GPT-4. Trong kiểm tra, GPT-4 đánh giá hai câu trả lời bằng cách đưa ra điểm số vô hướng 1-10 cho câu trả lời tương ứng và cung cấp giải thích phù hợp cho phán đoán (OpenAI, 2023). Mặc dù nó không phải là một đánh giá nghiêm ngặt, chúng tôi có thể so sánh chất lượng phản hồi tổng thể của các mô hình với chi phí hợp lý. Xem xét thiên vị vị trí của đánh giá GPT-49, chúng tôi đánh giá cùng một trường hợp hai lần bằng cách đảo ngược thứ tự của hai câu trả lời.

3.2 Baseline

Chúng tôi bao gồm các mô hình mã nguồn mở gần đây để so sánh hành vi căn chỉnh của các LLM theo mô hình nền tảng và bộ dữ liệu huấn luyện của chúng. Alpaca là mô hình tuân theo hướng dẫn mã nguồn mở đầu tiên dựa trên LLaMA (Touvron et al., 2023; Taori et al., 2023). Nó được huấn luyện trên bộ dữ liệu hướng dẫn tổng hợp 52k được tạo ra bởi LLM độc quyền, InstructGPT (Ouyang et al., 2022). Tương tự, Vicuna được huấn luyện trên bộ dữ liệu ShareGPT 70k, là nhật ký trò chuyện được chia sẻ giữa người dùng với ChatGPT, một trong những mô hình được căn chỉnh mạnh mẽ (OpenAI, 2023; Chiang et al., 2023). Dolly-v2 là một mô hình mã nguồn mở khác được huấn luyện trên bộ dữ liệu hướng dẫn được chú thích bởi con người 15k (DataBricks, 2023). Nó dựa trên Pythia, một LLM mã nguồn mở khác (Biderman et al., 2023). OpenAssistant (Oasst) là một dự án mã nguồn mở để xây dựng LLM căn chỉnh dựa trên những người tham gia của cộng đồng web (Köpf et al., 2023). Nó cũng phát hành một mô hình Oasst (SFT) được huấn luyện trên bộ dữ liệu được chú thích bởi con người10.

3.3 Kết quả Đánh giá

Căn chỉnh HHH Tĩnh và TruthfulQA Các mô hình của chúng tôi vượt trội hơn Alpaca, Dolly-v2, và OpenAssistant mà không có bất kỳ chưng cất nào từ các LLM độc quyền hoặc các chú thích con người chuyên sâu, như được hiển thị trong Bảng 1. Đối với tất cả các kích thước, các ALMoST của chúng tôi hiển thị độ chính xác tốt hơn một cách nhất quán trong tất cả các phần HHH và TruthfulQA ngoại trừ Vicuna được huấn luyện trên đầu ra của ChatGPT (Chiang et al., 2023; OpenAI, 2023). Tuy nhiên, RM của chúng tôi hiển thị hiệu suất xuất sắc trong việc chọn phản hồi phù hợp theo các giá trị con người, thậm chí đánh bại Vicuna-7B. Đây là quan sát nhất quán với Askell et al. (2021). Hơn nữa, đáng chú ý là ALMoST-PPO của chúng tôi đạt được độ chính xác cao nhất trong phần Hữu ích của đánh giá HHH, thậm chí bao gồm các mô hình 13 tỷ. Khi so sánh các mô hình SFT và được huấn luyện PPO của chúng tôi, mô hình PPO cải thiện tính hữu ích, vô hại, và chân thực trong khi hy sinh tính trung thực. Tính trung thực và chân thực trông tương tự, nhưng chúng hơi khác nhau. Tính trung thực liên quan đến việc thể hiện sự không chắc chắn, trong khi chân thực chủ yếu đo lường mức độ mạnh mẽ của mô hình chống lại sự sai lầm đối kháng.

Đánh giá Con người trên Câu hỏi Vicuna Chúng tôi xác nhận thế mạnh của các mô hình trong sở thích con người thực tế trong đánh giá con người11. Trong Hình 3a, chúng tôi thấy rằng con người cũng đánh giá mô hình ALMoST của chúng tôi thuận lợi hơn Dolly-v2 và Alpaca (DataBricks, 2023; Taori et al., 2023), hiển thị tỷ lệ thắng 58.8% và 55.0%, tương ứng. Ngoài ra, ALMoST-PPO cải thiện sở thích của ALMoST-SFT với tỷ lệ thắng cao hơn (37.5%), trong khi chúng cũng hiển thị tỷ lệ hòa cao nhất (36.3%). Nó chỉ ra hiệu quả của huấn luyện RLSF của chúng tôi. Hơn nữa, mô hình của chúng tôi được đánh giá là cạnh tranh với Vicuna, hiển thị tỷ lệ thắng 25% và tỷ lệ hòa 25%, thậm chí không có sự phụ thuộc vào mô hình độc quyền mạnh mẽ, ChatGPT. Tuy nhiên, vẫn còn khoảng trống đáng kể để cải thiện giữa ALMoST của chúng tôi và Vicuna. Chúng tôi cũng bao gồm các ví dụ định tính của đánh giá trong Phụ lục H.

Đánh giá GPT-4 trên Câu hỏi Vicuna Trong Hình 3b, chúng tôi có thể quan sát xu hướng tương tự của đánh giá GPT-4 với đánh giá con người. ALMoST-PPO nhất quán hiển thị tỷ lệ thắng cao hơn so với Dolly-v2, Alpaca, và ALMoST-SFT. Tuy nhiên, chúng tôi thấy rằng GPT-4 không có khả năng đưa ra cùng điểm số cho các câu trả lời hiển thị tỷ lệ hòa thường thấp hơn so với đánh giá con người. Hơn nữa, GPT-4 đánh giá phản hồi của Vicuna thuận lợi hơn so với con người. Tuy nhiên, chúng tôi có thể có được khoảng cách tổng thể giữa các mô hình với chi phí hợp lý từ đánh giá GPT-4. Khi chúng tôi mở rộng đánh giá cho các mô hình có kích thước khác nhau, mô hình 7B của chúng tôi vượt trội hơn các baseline 12-13B, bao gồm Alpaca-13B, Oasst-12B, và Dolly-v2-12B trong đánh giá như được hiển thị trong Hình 8.

4 Phân tích

4.1 Thăm dò Giả thuyết Chính

Chúng tôi tiếp tục kiểm tra các giả định của chúng tôi cho phản hồi tổng hợp. Cụ thể, chúng tôi muốn biết mỗi giả định, (1) kích thước mô hình, (2) số lượng minh chứng, và (3) chất lượng minh chứng, đóng góp như thế nào vào chất lượng cuối cùng của các phản hồi được lấy mẫu. Để làm điều này, chúng tôi tiến hành đánh giá GPT-4 trên Câu hỏi Vicuna để so sánh các bộ tạo phản hồi được prompt của chúng tôi được sử dụng cho việc tạo ra phản hồi tổng hợp trong Phần 2.1. Chúng tôi sử dụng Alpaca-7B (Taori et al., 2023) làm mô hình baseline cho các so sánh theo cặp. Kết quả được hiển thị trong Bảng 2.

Đầu tiên, như mong đợi, chúng tôi có thể thấy rằng kích thước mô hình đóng góp đáng kể vào chất lượng phản hồi cho cả hai loại prompt (HHH và Faithful). Tỷ lệ thắng so với Alpaca-7B tăng đơn điệu khi chúng tôi tăng kích thước mô hình. Khoảng cách giữa 13B và 30B đặc biệt lớn. Thứ hai, khi so sánh các mô hình LLaMA-7B-HHH-{1,3,5}shot, số lượng minh chứng cũng cải thiện tỷ lệ thắng, nhưng cải thiện bởi yếu tố này tương đối nhỏ. Cuối cùng, chúng tôi thấy rằng chất lượng minh chứng là yếu tố quan trọng nhất. Đáng ngạc nhiên, mô hình nhỏ hơn với prompt được thiết kế tốt (LLaMA-7B-Faithful-3shot) vượt trội hơn mô hình lớn hơn với prompt bình thường (LLaMA-30B-HHH-5shot). Thông qua đánh giá nội tại này, chúng tôi có thể thấy bộ dữ liệu phản hồi tổng hợp của chúng tôi hiệu quả bao phủ các phản hồi có chất lượng khác nhau.

4.2 Đánh giá RM

Chúng tôi tiếp tục đánh giá RM của chúng tôi trên một bộ dữ liệu so sánh khác, HH-RLHF (Bai et al., 2022a) để xác thực các so sánh được tạo ra tổng hợp của chúng tôi (Phản hồi Tổng hợp). HH-RLHF chứa các phần khác nhau theo giai đoạn phát triển, ví dụ, tập cơ sở để xây dựng chính sách ban đầu hoặc tập trực tuyến được thu thập với hệ thống được triển khai. Chúng tôi tập trung vào phần 'Helpful-base', giả định chúng tôi không có hệ thống có thể triển khai.

Mô hình hóa Phần thưởng Trong Bảng 3, chúng tôi thấy RM của chúng tôi được huấn luyện với Phản hồi Tổng hợp đạt được 90% hiệu suất của giới hạn trên được huấn luyện trên toàn bộ bộ dữ liệu huấn luyện. Ngoài ra, nó đạt được cùng độ chính xác với kết quả được tinh chỉnh trên tập con một lượt (Helpful-base*). Vui lòng lưu ý HH-RLHF bao gồm ngữ cảnh nhiều lượt, trong khi bộ dữ liệu tổng hợp của chúng tôi tập trung vào các tình huống một lượt.

Hiệu ứng của Xác thực Hậu kỳ Chúng tôi tiến hành hai loại xác thực hậu kỳ để giảm nhiễu trong việc tạo ra so sánh tổng hợp được mô tả trong Phần 2.1. Bảng 4 hiển thị rằng mỗi phương pháp lọc đóng góp vào chất lượng mô hình phần thưởng cuối cùng. Đáng chú ý, chúng tôi thấy bộ lọc heuristic (HF) xem xét phân phối độ dài đóng một vai trò quan trọng trong việc tạo ra dữ liệu tổng hợp. Khi chúng tôi loại trừ HF, hiệu suất của RM giảm khoảng 10 điểm phần trăm. Hơn nữa, HF ngăn chặn RM rơi vào thiên vị độ dài được thảo luận trong Phần 2.1. RM, được huấn luyện trên bộ dữ liệu với HF, vượt trội hơn baseline dài luôn chọn phản hồi dài hơn làm phản hồi tốt hơn.

RMSP so với Tự chơi Chúng tôi kiểm tra lợi ích của Tự chơi Hướng dẫn bởi RM (RMSP) so với đối tác không có hướng dẫn RM, tức là, Tự chơi. Cụ thể, chúng tôi so sánh hai chính sách có giám sát (SFT) được huấn luyện trên các minh chứng được tạo ra bởi RMSP hoặc Tự chơi. Trong Bảng 5, chúng tôi thấy rằng mô hình SFT được huấn luyện với RMSP vượt trội hơn mô hình với Tự chơi trong các benchmark khác nhau. Trong đánh giá GPT-4 so với Alpaca-7B, chỉ mô hình với RMSP hiển thị tỷ lệ thắng cao hơn 50%. Hơn nữa, chúng tôi xác nhận tầm quan trọng của việc thiết kế prompt tốt. Nếu chúng tôi sử dụng prompt HHH thay vì Faithful cho mô phỏng, hiệu suất cho căn chỉnh giảm đáng kể. Chúng tôi bao gồm các ví dụ định tính để so sánh các phương pháp trong Bảng 10.

5 Công trình Liên quan

Căn chỉnh LLM với giá trị Con người Điều kiện các mô hình ngôn ngữ theo các giá trị con người (Askell et al., 2021; Korbak et al., 2023; Liu et al., 2023) đã được tìm thấy là cải thiện khả năng của các mô hình trong việc tạo ra văn bản được căn chỉnh với con người. Việc kết hợp các mô hình phần thưởng (Askell et al., 2021; Liu et al., 2022; Scheurer et al., 2023; Yuan et al., 2023) để nói cách văn bản được tạo ra phản ánh tốt các giá trị con người đã cho phép huấn luyện các mô hình ngôn ngữ được căn chỉnh tốt hơn và phục vụ như một thành phần quan trọng cho một phương pháp hiệu quả khác - học tăng cường từ phản hồi con người (RLHF). RLHF đã được điều tra rộng rãi trong những ngày gần đây để căn chỉnh LLM với các giá trị con người (Christiano et al., 2017; Ziegler et al., 2020; Ouyang et al., 2022; Bai et al., 2022a; Stiennon et al., 2022; Glaese et al., 2022). Gần đây, Zhou et al. (2023) tuyên bố Giả thuyết Căn chỉnh Bề mặt rằng hầu hết các khả năng của LLM được học trong giai đoạn tiền huấn luyện, và tinh chỉnh trên một vài bộ dữ liệu được tuyển chọn có thể gợi ra các hành vi được căn chỉnh tốt từ các mô hình.

Chưng cất từ LLM độc quyền Các mô hình mã nguồn mở gần đây như Alpaca tuân theo công thức của Self-Instruct (Wang et al., 2022) để giảm gánh nặng thu thập các minh chứng con người (Taori et al., 2023; Peng et al., 2023). Tuy nhiên, nó tạo ra các bộ dữ liệu hướng dẫn tổng hợp sử dụng các LLM độc quyền, ví dụ, InstructGPT hoặc ChatGPT (Ouyang et al., 2022; OpenAI, 2023), khác với Self-Instruct, sử dụng một LLM vanilla, GPT-3 (Brown et al., 2020). Tương tự, Peng et al. (2023) cố gắng chưng cất đầu ra GPT-4 cho căn chỉnh. Vicuna là một mô hình mã nguồn mở khác được huấn luyện trên bộ dữ liệu ShareGPT 70k, là đầu ra ChatGPT được chia sẻ công khai bởi người dùng (Chiang et al., 2023). Mặt khác, Gudibande et al. (2023) chỉ ra những hạn chế của chưng cất để huấn luyện các LLM được căn chỉnh. Cụ thể, họ hiển thị rằng việc mở rộng số lượng bộ dữ liệu tổng hợp không cải thiện các nhiệm vụ liên quan đến kiến thức và cũng như sở thích con người, trong khi việc mở rộng kích thước mô hình đóng góp vào kết quả. Từ các thí nghiệm, họ cảnh báo rằng việc sử dụng bộ dữ liệu tổng hợp chưng cất phong cách của giáo viên, không phải kiến thức.

Học Tự Căn chỉnh Askell et al. (2021) giới thiệu chưng cất ngữ cảnh để có được một chính sách ban đầu với các minh chứng few-shot được thiết kế thủ công bởi các tác giả. Một mô hình học sinh, không được prompt, được chưng cất từ một mô hình giáo viên được prompt với các minh chứng few-shot. Self-Instruct là cách tiếp cận căn chỉnh LLM với các bộ dữ liệu hướng dẫn tự tạo (Wang et al., 2022). Để đạt được điều này, Wang et al. (2022) thiết kế thủ công 175 nhiệm vụ hạt giống và tiến hành bộ dữ liệu hướng dẫn tự động thông qua học trong ngữ cảnh và lọc. Chúng tôi phát triển các phương pháp bằng cách bao gồm mô hình hóa phần thưởng với phản hồi tổng hợp. Dromedary là một công trình đồng thời có động cơ tương tự như chúng tôi, tức là, học căn chỉnh với nỗ lực con người tối thiểu (Sun et al., 2023). Họ thiết kế một vài "nguyên tắc" được viết bởi con người cho LLM để tuân theo, và các LLM tạo ra phản hồi được căn chỉnh với sự hướng dẫn của các nguyên tắc thông qua học trong ngữ cảnh, tương tự như Constitutional AI (Bai et al., 2022b). Cụ thể, nó đòi hỏi khoảng 200 chú thích con người, 195 prompt hạt giống, 16 nguyên tắc, và 5 mẫu cho căn chỉnh, trong khi khung của chúng tôi chỉ đòi hỏi 18 chú thích con người, 10 prompt hạt giống cho khai thác truy vấn, và 8 minh chứng.

6 Kết luận

Trong công trình này, chúng tôi đề xuất một khung mới để căn chỉnh LLM với các giá trị con người bằng cách giới thiệu phản hồi tổng hợp. Chúng tôi xác định phản hồi tốt hơn từ các LLM vanilla với các kích thước và prompt khác nhau, dựa vào kiến thức tiên nghiệm thực nghiệm. Chúng tôi đầu tiên huấn luyện một mô hình phần thưởng với các so sánh được tạo ra tổng hợp. Sau đó, chúng tôi tạo ra một bộ dữ liệu tổng hợp khác để huấn luyện các chính sách được căn chỉnh sử dụng mô hình phần thưởng. Kết quả thí nghiệm chứng minh hiệu quả của khung của chúng tôi hiển thị hiệu suất xuất sắc trong các benchmark căn chỉnh. Chúng tôi tin rằng hiệu suất mạnh mẽ của mô hình chúng tôi được bắt nguồn từ việc kết hợp hiệu quả các chỉ báo thực nghiệm của hành vi được căn chỉnh tốt thông qua phản hồi tổng hợp. Hơn nữa, phương pháp của chúng tôi có hiệu quả về chi phí trong việc nó không đòi hỏi các minh chứng con người rộng rãi và không phụ thuộc vào các LLM độc quyền.

Hạn chế

Mặc dù chúng tôi hiển thị khung của chúng tôi hoạt động tốt trên nhiều benchmark liên quan đến căn chỉnh (Askell et al., 2021; Lin et al., 2021; Chiang et al., 2023), các đánh giá của chúng tôi thiếu sót trong việc xác định các khía cạnh khác của các mô hình được căn chỉnh kết quả. Ví dụ, Gudibande et al. (2023) giải thích những hạn chế của các bộ dữ liệu mô phỏng tổng hợp từ các LLM được căn chỉnh trước bằng cách bao gồm các benchmark liên quan đến kiến thức như MMLU, HumanEval, và Natural Questions (Hendrycks et al., 2020; Chen et al., 2021; Kwiatkowski et al., 2019). Askell et al. (2021); Bai et al. (2022a) cũng giải thích hiện tượng 'thuế căn chỉnh' trong đó các mô hình được căn chỉnh kết quả hy sinh các khả năng khác của chúng hiển thị hiệu suất suy giảm trên các nhiệm vụ NLP khác. Thực tế, chúng tôi quan sát kết quả tương tự khi chúng tôi kiểm tra các mô hình của chúng tôi trên các nhiệm vụ MMLU và LAMBADA zero-shot (Hendrycks et al., 2020; Paperno et al., 2016) để xác định thuế căn chỉnh, như được hiển thị trong Phụ lục F. Mô hình PPO của chúng tôi hiển thị hiệu suất suy giảm cho cả hai bộ dữ liệu, ngụ ý sự hiện diện của thuế căn chỉnh. Mặc dù có báo cáo rằng các mô hình dưới 10B thường gặp phải thuế căn chỉnh và việc mở rộng các tham số giảm thiểu sự đánh đổi (Bai et al., 2022a), cách tiếp cận của chúng tôi có thể bị hạn chế trong việc nó chủ yếu tập trung vào việc căn chỉnh LLM với các giá trị mục tiêu, ví dụ, tính hữu ích. Chúng tôi để lại một đánh giá toàn diện hơn về khung của chúng tôi và giảm thiểu thuế căn chỉnh cho công việc tương lai.

Lời cảm ơn

Công trình này được hỗ trợ một phần bởi Trung tâm AI Siêu sáng tạo KAIST-NAVER và tài trợ từ Viện Kế hoạch và Đánh giá Công nghệ Thông tin & Truyền thông (IITP) được tài trợ bởi chính phủ Hàn Quốc (MSIT) (Số 2019-0-00075, Chương trình Trường Đại học Trí tuệ Nhân tạo (KAIST), 20%). Các tác giả muốn cảm ơn các thành viên của KAIST LKLab và NAVER Cloud cho những nhận xét xây dựng của họ.
