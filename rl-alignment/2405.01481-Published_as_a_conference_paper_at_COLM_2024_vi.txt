# 2405.01481.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2405.01481.pdf
# Kích thước tệp: 688990 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024
NeMo-Aligner: Bộ công cụ có thể mở rộng để căn chỉnh mô hình hiệu quả

Gerald Shen, Zhilin Wang, Olivier Delalleau, Jiaqi Zeng, Yi Dong,
Daniel Egert, Shengyang Sun, Jimmy Zhang, Sahil Jain, Ali Taghibakhshi
Markel Sanz Ausin, Ashwath Aithal, Oleksii Kuchaiev
NVIDIA
{geshen, zhilinw }@nvidia.com

Tóm tắt
Việc căn chỉnh các Mô hình Ngôn ngữ Lớn (LLM) với các giá trị và sở thích của con người là điều cần thiết để làm cho chúng hữu ích và an toàn. Tuy nhiên, việc xây dựng các công cụ hiệu quả để thực hiện căn chỉnh có thể là thách thức, đặc biệt đối với những LLM lớn nhất và có năng lực nhất thường chứa hàng chục hoặc hàng trăm tỷ tham số. Chúng tôi tạo ra NeMo-Aligner, một bộ công cụ để căn chỉnh mô hình có thể mở rộng hiệu quả lên đến một nghìn GPU để huấn luyện những LLM mã nguồn mở lớn nhất như Nemotron 4 340B và Llama 3.1 405B. NeMo-Aligner đi kèm với các triển khai được tối ưu hóa cao và có thể mở rộng cho các mô hình chính của căn chỉnh mô hình như: Học tăng cường từ phản hồi của con người (RLHF), Tối ưu hóa sở thích trực tiếp (DPO), SteerLM, và Tinh chỉnh tự chơi (SPIN). Ngoài ra, bộ công cụ của chúng tôi hỗ trợ chạy hầu hết các kỹ thuật căn chỉnh trong cài đặt Tinh chỉnh hiệu quả tham số (PEFT). NeMo-Aligner được thiết kế để có thể mở rộng, cho phép hỗ trợ các kỹ thuật căn chỉnh khác với nỗ lực tối thiểu. Nó được mã nguồn mở với Giấy phép Apache 2.0 và chúng tôi mời gọi đóng góp của cộng đồng tại https://github.com/NVIDIA/NeMo-Aligner .

1 Giới thiệu
Việc tiền huấn luyện các mô hình ngôn ngữ lớn trên lượng lớn văn bản không nhãn đã cho thấy những khả năng đầy hứa hẹn (Brown et al., 2020; Zhang et al., 2022). Mặc dù các mô hình tiền huấn luyện không giám sát như vậy đã đạt được kết quả ấn tượng, việc căn chỉnh các mô hình để tuân theo hướng dẫn của người dùng sau đó là một bước quan trọng để khai thác khả năng của LLM cho các trường hợp sử dụng thực tế (Sanh et al., 2022; Wei et al., 2022). Các nỗ lực dựa trên Tinh chỉnh có giám sát (Conover et al., 2023; Köpf et al., 2023; Taori et al., 2023) đã được chứng minh là kém hiệu quả hơn so với các kỹ thuật cũng sử dụng phản hồi để điều chỉnh mô hình hướng tới các phản hồi hữu ích hơn và tránh xa các phản hồi ít hữu ích hơn (Bai et al., 2022a; Ouyang et al., 2022; Touvron et al., 2023; Dong et al., 2023).

Mặc dù có lợi ích của việc huấn luyện mô hình sử dụng phản hồi, các quy trình này được biết đến là khó thực hiện đúng cách (Lambert & Calandra, 2023; Zheng et al., 2023b), ngăn cản việc áp dụng rộng rãi và hiệu quả bên ngoài một số tổ chức có nguồn lực tốt được lựa chọn. Ví dụ, biến thể Tối ưu hóa chính sách gần đúng (PPO) phổ biến của phương pháp Học tăng cường từ phản hồi của con người (RLHF) (Ouyang et al., 2022) yêu cầu chạy một quy trình phức tạp với bốn mô hình ngôn ngữ lớn tương tác theo cách phức tạp trong quá trình huấn luyện. Các thuật toán căn chỉnh như vậy đưa ra những thách thức hệ thống mới cho việc huấn luyện hiệu quả đòi hỏi phải suy nghĩ lại các khía cạnh khác nhau của ngăn xếp phần mềm bao gồm khả năng mở rộng mô hình, phối hợp giữa các mô hình, và tạo văn bản trong vòng lặp huấn luyện.

Có những công cụ mã nguồn mở hiện có cho căn chỉnh mô hình, đáng chú ý nhất là HuggingFace TRL (von Werra et al., 2020), CarperAI trlX (Havrilla et al., 2023) và Microsoft DeepSpeed-Chat (Yao et al., 2023). Những công cụ này cung cấp một điểm khởi đầu tuyệt vời về khả năng sử dụng và tập tính năng. Tuy nhiên, với NeMo-Aligner chúng tôi nhằm cải thiện đáng kể hiệu suất và khả năng mở rộng lên hơn một nghìn GPU, đặc biệt hữu ích cho việc căn chỉnh những mô hình lớn nhất và có năng lực nhất

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024
như Nemotron 4 340B (Nvidia et al., 2024), Llama 3.1 405B (Dubey et al., 2024) và hơn thế nữa.

NeMo-Aligner giải quyết các thách thức về khả năng mở rộng bằng cách (I) xây dựng dựa trên Megatron-LM (Shoeybi et al., 2020) với huấn luyện song song 3D (dữ liệu, tensor, và pipeline), (II) có phương pháp phân tán cho huấn luyện Tối ưu hóa chính sách gần đúng (PPO) trong RLHF và (III) tích hợp các tối ưu hóa suy luận PPO dựa trên TensorRT-LLM (NVIDIA, 2023b) trong giai đoạn rollout. Kết hợp lại, những tối ưu hóa này cho phép người dùng huấn luyện hiệu quả những mô hình lớn nhất trên hơn một nghìn GPU giảm thời gian lặp lại nghiên cứu.

NeMo-Aligner tối ưu hóa các kỹ thuật căn chỉnh phổ biến bao gồm Tinh chỉnh có giám sát (SFT), RLHF dựa trên PPO (Ouyang et al., 2022), Tối ưu hóa sở thích trực tiếp (Rafailov et al., 2023), SteerLM (Dong et al., 2023) và Tinh chỉnh tự chơi (Chen et al., 2024). Chúng tôi tóm tắt ngắn gọn nền tảng cho những kỹ thuật này trong Phần 2, tiếp theo là một khám phá chuyên sâu về huấn luyện với từng kỹ thuật trong các Phần 3, 4, 5, và 6. Cuối cùng, chúng tôi trình bày thiết kế có thể mở rộng của NeMo-Aligner trong Phần 7.

2 Nền tảng căn chỉnh mô hình

[Hình 1: Công thức huấn luyện cho RLHF dựa trên Ouyang et al. (2022). Bước 1: Dữ liệu gợi ý-phản hồi được chú thích được sử dụng để thực hiện Tinh chỉnh có giám sát trên mô hình tiền huấn luyện (cơ sở). Bước 2: Mô hình SFT kết quả được huấn luyện với Dữ liệu sở thích để tạo ra một Mô hình phần thưởng. Bước 3: Mô hình SFT được sử dụng để khởi tạo Mạng chính sách, và Mô hình phần thưởng được sử dụng để khởi tạo Mạng giá trị - cùng với các gợi ý đầu vào, cả bốn mô hình được sử dụng để huấn luyện một Mô hình chính sách. Mô hình SFT cũng được sử dụng để tính toán hình phạt phân kỳ KL trong Bước 3 (không được minh họa).]

2.1 Tinh chỉnh có giám sát
Cho một mô hình tiền huấn luyện (còn được gọi là mô hình "cơ sở"), tinh chỉnh có giám sát (SFT) cập nhật các tham số của mô hình cơ sở trên các gợi ý với phản hồi mong đợi, trong đó các phản hồi mong đợi có thể đến từ chú thích của chuyên gia con người (Köpf et al., 2023) hoặc các mô hình ngôn ngữ khác (Ding et al., 2023). Mô hình được huấn luyện để bắt chước các phản hồi mong đợi cho các gợi ý sử dụng hàm mất mát entropy chéo ở cấp độ token. SFT là một bước tiên quyết quan trọng trong Học tăng cường từ phản hồi của con người (Ouyang et al., 2022) và Tối ưu hóa sở thích trực tiếp (Rafailov et al., 2023) bởi vì không có nó, mô hình cơ sở rất khó có thể tạo ra các phản hồi tuân theo hướng dẫn của người dùng. Bước này đôi khi cũng được gọi là nhân bản hành vi vì mô hình được mong đợi bắt chước phản hồi của con người hoặc một mô hình khác.

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

2.2 Học tăng cường từ phản hồi của con người
Học tăng cường từ phản hồi của con người (RLHF) được giới thiệu bởi Christiano et al. (2017) như một cách để tránh các hàm phần thưởng được định nghĩa thủ công trong Học tăng cường. Thay vào đó, một mô hình phần thưởng được huấn luyện từ một tập dữ liệu về sở thích của con người bao gồm các cặp quỹ đạo "được chọn" và "bị từ chối". Hàm mất mát của mô hình phần thưởng, được phái sinh từ mô hình Bradley-Terry (Bradley & Terry, 1952), cố gắng tối đa hóa khả năng rằng rchosen>rrejected (tức là, các phần thưởng dự đoán phù hợp với sở thích của con người). Khi mô hình phần thưởng được huấn luyện, nó có thể được sử dụng để tính toán phần thưởng cho thuật toán RL. Hai phương pháp phổ biến nhất được sử dụng trong RLHF là REINFORCE (Williams, 1992) và Tối ưu hóa chính sách gần đúng (PPO) (Schulman et al., 2017). Trong NeMo-Aligner chúng tôi tập trung vào PPO, cụ thể như được mô tả bởi Ouyang et al. (2022).

RLHF đã được chứng minh mang lại lợi ích đáng kể cho căn chỉnh mô hình (Ouyang et al., 2022; Bai et al., 2022a; Touvron et al., 2023) với công thức huấn luyện điển hình như sau, cũng được minh họa trong Hình 1:

1. Từ một mô hình cơ sở tiền huấn luyện, huấn luyện một mô hình SFT ban đầu như được mô tả trong Phần 2.1.

2. Từ mô hình SFT, huấn luyện một mô hình phần thưởng sử dụng một tập dữ liệu về sở thích của con người được tạo thành từ các cặp phản hồi "được chọn" và "bị từ chối" cho một tập các gợi ý, theo Christiano et al. (2017). Thông thường, chúng tôi khởi tạo một đầu mô hình phần thưởng tuyến tính trên đầu mô hình SFT trước khi huấn luyện.

3. Từ mô hình SFT, huấn luyện một chính sách với thuật toán Tối ưu hóa chính sách gần đúng trực tuyến (PPO, Schulman et al., 2017), với phần thưởng được cung cấp bởi mô hình phần thưởng đã huấn luyện. Các gợi ý đầu vào có thể không nhất thiết giống với những gợi ý được sử dụng cho huấn luyện mô hình phần thưởng. Một thuật ngữ điều chỉnh dựa trên phân kỳ KL w.r.t. mô hình SFT giúp ngăn chính sách đi quá xa khỏi điểm khởi đầu và khai thác "điểm mù" của mô hình phần thưởng (Stiennon et al., 2020; Ouyang et al., 2022). Bộ phê bình PPO thường được khởi tạo từ mô hình phần thưởng.

2.3 Tối ưu hóa sở thích trực tiếp
Tối ưu hóa sở thích trực tiếp (Rafailov et al., 2023) là một thuật toán ngoại tuyến, off-policy sử dụng dữ liệu sở thích để trực tiếp huấn luyện một chính sách tối ưu mà không cần một mô hình phần thưởng rõ ràng. Thay vì sử dụng một mô hình phần thưởng, một chính sách tham chiếu được sử dụng để ngầm định phái sinh phần thưởng giữa một cặp được chọn và bị từ chối thông qua mô hình Bradley-Terry. Điều này được thực hiện thông qua sự khác biệt trong xác suất log giữa các phản hồi được chọn và bị từ chối, được tính toán cho các chính sách tối ưu và tham chiếu. Sự khác biệt này được chia tỷ lệ và sau đó được biến đổi bởi hàm sigmoid để phái sinh hàm mất mát. Chính sách tham chiếu được đóng băng trong quá trình huấn luyện và đại diện cho chính sách được sử dụng để tạo ra các phản hồi được chọn/bị từ chối. Nếu chính sách tham chiếu được sử dụng để tạo ra dữ liệu sở thích không có sẵn, nó có thể được ước tính bằng tinh chỉnh có giám sát trên các gợi ý và phản hồi ưa thích của dữ liệu sở thích.

2.4 SteerLM
SteerLM (Dong et al., 2023) là một thuật toán căn chỉnh mô hình dựa trên tinh chỉnh có giám sát tránh sử dụng các phương pháp RL phức tạp, tương tự như DPO. SteerLM bao gồm ba bước. Bước đầu tiên là huấn luyện một Mô hình dự đoán thuộc tính học cách dự đoán các giá trị (từ 0 đến 4 trong đó cao hơn là nhiều hơn) cho các khía cạnh ngữ nghĩa khác nhau của một phản hồi làm cho phản hồi hữu ích và an toàn, chẳng hạn như tính đúng đắn và độc tính của nó (Köpf et al., 2023; Wang et al., 2023). Tiếp theo, Mô hình dự đoán thuộc tính có thể được sử dụng để chú thích các thuộc tính khác nhau góp phần vào tính hữu ích và an toàn trong một loạt các tập dữ liệu gợi ý-phản hồi. Cuối cùng, những tập dữ liệu được chú thích này có thể được sử dụng để thực hiện Tinh chỉnh có giám sát theo điều kiện thuộc tính nơi mô hình học cách tạo ra phản hồi dựa trên gợi ý cũng như các thuộc tính được chú thích được định dạng thành một chuỗi, chẳng hạn như helpfulness:4,correctness:4,toxicity:0. Bước này dạy mô hình phân biệt giữa các phản hồi hữu ích/an toàn hơn và những phản hồi ít hữu ích hơn, theo cách tinh tế cho từng khía cạnh ngữ nghĩa. Tại thời điểm suy luận, gợi ý có thể được nối thêm với các giá trị thuộc tính tối ưu, như trên, để tạo ra phản hồi hữu ích nhất.

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

2.5 Tinh chỉnh tự chơi
Tinh chỉnh tự chơi (SPIN) (Chen et al., 2024) là một thuật toán dựa trên tự chơi, trong đó một mô hình mạnh được phát triển từ một mô hình yếu hơn bằng cách chơi đối với các phiên bản trước đó của chính nó. Bắt đầu từ một tập dữ liệu SFT của các cặp gợi ý/phản hồi, các phản hồi mới được tạo ra từ các lần lặp trước đó của mô hình. Chính sách của nó sau đó được cải thiện bằng cách phân biệt giữa những phản hồi tự tạo này và các phản hồi SFT được tạo bởi con người thực tế. Điều này được thực hiện thông qua một hàm mất mát sở thích giống hệt với hàm được sử dụng bởi DPO (Phần 2.3). Khi huấn luyện SPIN bắt đầu lần đầu, chúng tôi sử dụng một bản sao của chính sách ban đầu làm chính sách tham chiếu trong hàm mất mát DPO. "Trò chơi" tự chơi sau đó được chơi trong một số lần lặp trong đó chúng tôi huấn luyện chính sách như trong DPO trong khi giữ chính sách tham chiếu đóng băng, và ở cuối mỗi lần lặp chúng tôi cập nhật trọng số của chính sách tham chiếu với những trọng số từ chính sách đã huấn luyện. Trong mỗi lần lặp, chúng tôi lặp qua tập dữ liệu huấn luyện SFT của chúng tôi và sử dụng chính sách tham chiếu để tạo ra phản hồi cho mỗi gợi ý, xây dựng một bộ ba sở thích giữa phản hồi "được chọn" SFT thực tế của con người và phản hồi "bị từ chối" được tạo ra. Khi chúng tôi có những bộ ba sở thích này cho toàn bộ epoch, chúng tôi cập nhật trọng số mô hình thông qua hàm mất mát DPO từ những bộ ba cặp sở thích "(được chọn, bị từ chối)" này. Do đó mô hình ngầm định học cách ưa thích các phản hồi SFT thực tế hơn những phản hồi được tạo ra bởi lần lặp trước đó của chính nó, tạo thành cơ chế tự chơi.

3 Huấn luyện RLHF (PPO)
NeMo-Aligner được thiết kế để hỗ trợ nhiều kỹ thuật căn chỉnh một cách hiệu quả ở quy mô cực lớn. Nó thực hiện điều này bằng cách xây dựng dựa trên Megatron-LM (Shoeybi et al., 2020) và NeMo (Kuchaiev et al., 2019) để bao gồm các tính năng như kernel tối ưu hóa từ Transformer Engine (NVIDIA, 2022), bộ tối ưu hóa adam phân tán hợp nhất và hỗ trợ song song 3D. NeMo-Aligner hỗ trợ toàn bộ quy trình RLHF như được giới thiệu bởi Ouyang et al. (2022) và được mô tả trong Phần 2.2. Quy trình huấn luyện được chia thành ba giai đoạn riêng biệt như được minh họa trong Hình 1: Tinh chỉnh có giám sát, Huấn luyện mô hình phần thưởng, và Tối ưu hóa chính sách gần đúng. Những thách thức với hiệu quả quy trình chủ yếu đến từ giai đoạn Tối ưu hóa chính sách gần đúng, và phần này mô tả phương pháp của chúng tôi để giải quyết những thách thức này, như được tóm tắt trong Hình 2.

[Hình 2: Tối ưu hóa cho huấn luyện RLHF. Tối ưu hóa cho huấn luyện và suy luận PPO được chi tiết trong các Phần 3.1 và 3.2 tương ứng.]

3.1 Phương pháp phân tán cho huấn luyện PPO
Giai đoạn PPO yêu cầu chạy huấn luyện và/hoặc suy luận trên bốn mô hình khác nhau, như được minh họa trong Hình 3:

1. PPO Actor (huấn luyện và suy luận, được khởi tạo từ mô hình SFT): Mô hình chúng tôi muốn tinh chỉnh với PPO.
2. Chính sách tham chiếu (chỉ suy luận, được đặt thành mô hình SFT): Mô hình để tính toán hình phạt KL.
3. PPO Critic (huấn luyện và suy luận, được khởi tạo từ mô hình phần thưởng): Được sử dụng trong PPO để tính toán ước lượng giá trị.
4. Mô hình phần thưởng (chỉ suy luận): Cung cấp phần thưởng RL trên dữ liệu rollout được tạo ra.

Tất cả những mô hình này có thể cực kỳ lớn (ví dụ Llama 3.1 405B), vì vậy NeMo-Aligner áp dụng một phương pháp phân tán cho huấn luyện PPO. Chúng tôi cho phép người dùng thiết lập các máy chủ và máy khách PyTriton (NVIDIA, 2022) để giao tiếp qua các mô hình khác nhau trong PPO. Những máy chủ PyTriton này giúp có thể chạy các mô hình trên các cụm tính toán khác nhau, loại bỏ yêu cầu phải có cả critic và actor trên cùng một phân bổ tính toán. Một cách ngây thơ, bốn máy chủ khác nhau (tức là một cho mỗi mô hình) sẽ được khởi chạy. Tuy nhiên, chúng tôi lưu ý rằng chính sách tham chiếu và PPO actor là cùng một mô hình nhưng với các trọng số khác nhau. Do đó, chúng tôi kết hợp chúng thành một công việc và tải trọng số của chính sách tham chiếu lên CPU, hoán đổi chúng với trọng số của actor cho bước suy luận chính sách tham chiếu. Chúng tôi triển khai cùng một chiến lược cho mô hình phần thưởng và critic. Tất cả giao tiếp được thực hiện không đồng bộ, cho phép suy luận/huấn luyện critic được đường ống hóa với suy luận/huấn luyện chính sách.

Chúng tôi mở rộng kích thước phân bổ tính toán sao cho [suy luận mô hình phần thưởng + suy luận critic] ≈ [lấy mẫu actor + suy luận chính sách tham chiếu] và [huấn luyện critic] ≤ [huấn luyện actor + khởi tạo suy luận actor]. Điều này đảm bảo rằng đường ống có thể sử dụng khả năng tính toán có sẵn một cách hiệu quả nhất.

[Hình 3: Kiến trúc hệ thống PPO của NeMo-Aligner. PPO Actor là một máy khách PyTriton (NVIDIA, 2022) gửi các yêu cầu không đồng bộ đến máy chủ (PPO critic và mô hình phần thưởng) để có được phần thưởng và giá trị của các rollout được tạo ra, và để gửi dữ liệu huấn luyện cho critic.]

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

3.2 Tối ưu hóa cho rollout PPO
Việc tạo ra phản hồi trong bước rollout chiếm ưu thế thời gian từ đầu đến cuối của huấn luyện PPO. Giai đoạn tạo ra của actor bao gồm nhiều lần truyền xuôi, với một token được tạo ra mỗi lần truyền xuôi. Do đó, các kernel giai đoạn tạo ra nói chung bị ràng buộc bởi độ trễ khởi chạy và băng thông bộ nhớ, có nghĩa là việc tái sử dụng trực tiếp triển khai truyền xuôi được tối ưu hóa tính toán của giai đoạn huấn luyện dẫn đến hiệu suất rất kém.

Để giải quyết những nút thắt cổ chai này, chúng tôi triển khai giai đoạn tạo ra bằng TensorRT-LLM (NVIDIA, 2023b), một khung triển khai LLM hiệu suất cao. TensorRT-LLM tích hợp các kernel được tối ưu hóa suy luận và hợp nhất kernel tự động vào một runtime dựa trên TensorRT để đạt được hiệu suất tốt hơn. Khi bắt đầu RLHF, mô hình được chuyển đến TensorRT-LLM biên dịch mô hình thành một engine TensorRT; TensorRT-LLM tải engine vào runtime của nó và thực hiện tạo ra. Engine giữ một bản sao của trọng số mô hình cùng với KV-cache runtime và các kích hoạt. Do chi phí tuần tự hóa engine, chúng tôi giữ engine trong bộ nhớ trong quá trình huấn luyện. Kết quả là, chúng tôi giảm áp lực bộ nhớ đỉnh bằng cách tính toán lại các kích hoạt giai đoạn huấn luyện trong lượt truyền ngược. Hơn nữa, vì tạo ra có yêu cầu bộ nhớ thấp hơn so với huấn luyện, chúng tôi tái phân đoạn mô hình để chỉ sử dụng song song tensor trong suy luận nếu bộ nhớ cho phép, loại bỏ overhead từ giao tiếp giữa các nút khi chạy với song song pipeline.

Ở các bước huấn luyện tiếp theo, engine phải được đồng bộ với trọng số tham số đã cập nhật từ giai đoạn huấn luyện. Engine được cập nhật tại chỗ sử dụng TensorRT Refitter (NVIDIA, 2023c). Chúng tôi tránh biên dịch lại engine vốn sẽ gây ra overhead lớn vì việc tạo ra không thể bắt đầu cho đến khi trọng số được cập nhật.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Đối với các kích thước bài toán lớn, có thể có sự khác biệt về thời gian tạo ra giữa worker song song dữ liệu nhanh nhất và chậm nhất trong quá trình tạo ra do sự khác biệt trong độ dài phản hồi. Để giảm thiểu điều này, chúng tôi cho phép người dùng thiết lập một nhóm worker để cân bằng tải động giữa các worker song song dữ liệu để cung cấp cho các worker có thế hệ ngắn hơn tương ứng nhiều công việc hơn.

3.3 Chi tiết huấn luyện mô hình và chất lượng
Như một minh chứng cho huấn luyện RLHF quy mô lớn thực tế với NeMo-Aligner, chúng tôi huấn luyện mô hình Llama3 (Meta AI, 2024) 70B sử dụng PPO như được quy định trong Wang et al. (2024). Mô hình PPO được huấn luyện với kích thước batch toàn cục rollout là 128, kích thước batch toàn cục huấn luyện là 128, tốc độ học tập không đổi là 1e-7, hình phạt KL là 0.003, và sử dụng HelpSteer2 làm nguồn gợi ý. Theo Wang et al. (2024); Meng et al. (2024), chúng tôi sử dụng MT-Bench (Zheng et al., 2023a) với giám khảo GPT-4-Turbo để đánh giá hiệu suất của Mô hình RLHF đã huấn luyện. Mô hình kết quả đạt được hiệu suất 8.13 trên MT-Bench, là một cải thiện so với checkpoint SFT mạnh khởi đầu với MT-Bench là 7.96. Mô hình phần thưởng và mô hình được huấn luyện RLHF được phát hành công khai tại https://huggingface.co/nvidia/Llama3-70B-SteerLM-RM và https://huggingface.co/nvidia/Llama3-70B-PPO-Chat tương ứng.

3.4 Khả năng mở rộng

[THIS IS TABLE: Bảng về thời gian mỗi bước và tăng tốc tương đối cho các cấu hình node khác nhau]

Để chứng minh hiệu quả mở rộng của NeMo-Aligner, chúng tôi lặp lại các thiết lập huấn luyện giống hệt từ Phần 3.3, với 8 node actor + 4 node critic và 16 node actor + 8 node critic. Như được hiển thị trong Bảng 1, thời gian tổng thể mỗi bước giảm tương ứng, đạt được tăng tốc 1.80x giữa 8+4 node và 16+8 node. Tăng tốc trong thời gian tổng thể mỗi bước được đóng góp bởi tăng tốc trong cả giai đoạn Train và Rollout, chứng minh tối ưu hóa hiệu quả mà NeMo-Aligner đã thực hiện cho cả hai giai đoạn.

Việc mở rộng giai đoạn Train là dưới tuyến tính do số lượng micro-batch trên mỗi rank song song dữ liệu giảm khi số lượng node tăng. Vì tất cả các giai đoạn pipeline phải hoàn thành trước khi bộ tối ưu hóa được gọi trong các mô hình song song pipeline, chúng tôi gặp phải overhead để lấp đầy và làm khô pipeline độc lập với số lượng micro-batch (Shoeybi et al., 2020). Do đó, giảm số lượng micro-batch trên mỗi rank song song dữ liệu tăng tỷ lệ của

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

bước huấn luyện được dành cho việc lấp đầy và làm khô pipeline, nơi việc sử dụng GPU kém. Một vấn đề tương tự rõ ràng trong giai đoạn tính toán xác suất log với việc mở rộng 1.43x.

[Bảng 2 với các cấu hình node và thời gian khác nhau]

Khả năng mở rộng của hệ thống cũng cần được xem xét trong bối cảnh của các yêu cầu bài toán. Thiết lập huấn luyện trong Phần 3.3 có Llama 3 Actor 70B, Llama 3 Critic 70B cũng như kích thước batch toàn cục rollout là 128. Thiết lập như vậy giới hạn việc chứng minh hiệu quả của việc mở rộng hệ thống của chúng tôi vượt quá 16 + 8 node vì không có đủ công việc để chia sẻ một cách có ý nghĩa qua nhiều worker song song dữ liệu hơn. Do đó, chúng tôi sửa đổi thiết lập một chút để sử dụng kích thước batch toàn cục rollout là 1024 trong Bảng 2 để đo lường hiệu suất hệ thống khi các yêu cầu cao hơn. Bảng 2 cho thấy rằng các yêu cầu tăng lên của công việc huấn luyện cho phép nó mở rộng một cách có ý nghĩa đến 64 + 32 node (với tổng cộng 768 GPU H100) cho các giai đoạn khác nhau trong PPO.

3.5 Điều gì góp phần vào hiệu suất hệ thống?

[Bảng 4 về nghiên cứu loại bỏ]

Để hiểu rõ hơn tầm quan trọng của từng khía cạnh trong thiết kế hệ thống PPO của NeMo-Aligner, chúng tôi tiến hành các nghiên cứu loại bỏ bằng cách loại bỏ một khía cạnh tại một thời điểm và đo lường thời gian tổng thể mỗi bước như được hiển thị trong Bảng 4. Chúng tôi thấy rằng Tích hợp TensorRT-LLM là thành phần quan trọng nhất cho hiệu suất hệ thống cao, không có nó PPO sẽ mất gần bảy lần thời gian cho mỗi bước. Tiếp theo là tái phân đoạn mô hình của chúng tôi để chỉ sử dụng song song tensor trong suy luận (3.87x), sử dụng TensorRT Refit để tránh biên dịch lại engine TensorRT-LLM (3.15x), việc sử dụng yêu cầu không đồng bộ giữa các mô hình actor và critic (1.54x). Chúng tôi không quan sát thấy tăng tốc có ý nghĩa trong việc sử dụng nhóm worker để cân bằng công việc giữa các rank song song dữ liệu vì kích thước bài toán nhỏ (với kích thước batch toàn cục rollout là 128) và do đó sự mất cân bằng worker ít hơn overhead chúng tôi gặp phải khi thực hiện việc cân bằng. Tuy nhiên, chúng tôi mong đợi tính năng này giúp ích cho các kích thước bài toán lớn hơn.

3.6 Huấn luyện LLM mã nguồn mở lớn nhất

[Bảng 5 về Llama 3.1 405B]

NeMo-Aligner hỗ trợ căn chỉnh các LLM mã nguồn mở lớn nhất tính đến tháng 7 năm 2024, như Nemotron 4 340B (Nvidia et al., 2024) và Llama 3.1 405B (Dubey et al., 2024). Trong Bảng 5, chúng tôi thực hiện PPO trên Llama 3.1 405B sử dụng 1008 GPU H100, sử dụng một cấu hình dựa trên Phần 3.3. Chúng tôi sử dụng Llama 3.1 405B Instruct làm actor và chúng tôi huấn luyện một Mô hình phần thưởng trên đầu Llama 3.1 405B Instruct sử dụng cùng dữ liệu và siêu tham số như Nemotron 4 340B Reward (Wang et al., 2024). So với mô hình 70B, mô hình 405B chậm hơn đáng kể để huấn luyện trong PPO, chủ yếu bị nghẽn cổ chai bởi giai đoạn tạo ra phản hồi chậm.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

Điều này là do việc tạo ra mô hình 405B không thể phù hợp với một node duy nhất và yêu cầu tạo ra song song pipeline mà chúng tôi dự định tối ưu hóa thêm trong công việc tương lai. Ngoài ra, trong vài bước đầu của PPO, mô hình 405B tạo ra phản hồi dài hơn (trung bình 916 token) so với mô hình 70B với trung bình 351 token.

4 Huấn luyện DPO
Chúng tôi tuân theo công thức huấn luyện Zephyr-7B-Beta (Tunstall et al., 2023), một mô hình được huấn luyện với SFT và DPO. Tóm tắt, SFT đầu tiên được thực hiện trên Mistral-7B (Jiang et al., 2023) sử dụng tập dữ liệu Ultrachat (Ding et al., 2023). Mô hình sau đó được huấn luyện thêm với DPO sử dụng tập dữ liệu Ultrafeedback (Cui et al., 2023). Đối với SFT, chúng tôi sử dụng tốc độ học tập không đổi là 2e-5, kích thước batch toàn cục là 512, và huấn luyện mô hình trong 3 epoch. Đối với huấn luyện DPO, chúng tôi sử dụng hệ số điều chỉnh KL là 3e-4, kích thước batch toàn cục là 512 và lịch trình tốc độ học tập cosine với LR đỉnh là 1e-7, LR tối thiểu là 1e-8, 50 bước khởi động, và tối đa 300 bước. Chúng tôi có được điểm số MT-Bench tốt hơn một chút so với những gì được báo cáo bởi Tunstall et al. (2023) cho cả mô hình cuối cùng (7.60 so với 7.34) và mô hình ban đầu chỉ SFT (6.77 so với 6.64).

5 Huấn luyện SteerLM với LoRA
Thích ứng thứ hạng thấp (Hu et al., 2021) cho phép tinh chỉnh các mô hình ngôn ngữ lớn theo cách hiệu quả và tiết kiệm chi phí hơn. Được hỗ trợ cho các kỹ thuật căn chỉnh khác nhau trong NeMo-Aligner, LoRA được áp dụng cho huấn luyện SteerLM theo công thức huấn luyện của Wang et al. (2023) sử dụng mô hình Llama 2 70B cũng như các tập dữ liệu HelpSteer (Wang et al., 2023) và Open Assistant (Köpf et al., 2023). Cụ thể, chúng tôi áp dụng LoRA cho tất cả các lớp attention, với rank là 32. Chúng tôi sử dụng kích thước batch toàn cục là 128, tốc độ học tập không đổi là 1e-5 sau 10 bước khởi động với bộ tối ưu hóa AdamW, và huấn luyện trong 3 epoch. Như được hiển thị trong Bảng 6, việc áp dụng LoRA cho huấn luyện SteerLM với BF16 có thể giảm số lượng GPU 80GB tối thiểu cần thiết từ 32 xuống 8. Với cùng số lượng GPU, LoRA đạt được tăng tốc 5× so với tinh chỉnh tham số đầy đủ, trong khi duy trì hiệu suất mô hình tương đương: MT-Bench 7.43 so với 7.54, nằm trong mức nhiễu cho benchmark này (Jiang et al., 2023).

[Bảng 6: So sánh tham số đầy đủ và LoRA SteerLM]

Khi chúng tôi tăng số lượng GPU được sử dụng cho huấn luyện LoRA, thông lượng tương đối (đo bằng mẫu mỗi giây) cải thiện gần như tỷ lệ thuận, như được hiển thị trong Hình 4. Điều này cho thấy rằng NeMo-Aligner có thể phân phối và song song hóa hiệu quả khối lượng công việc qua một số lượng lớn GPU với overhead tối thiểu và lợi nhuận giảm dần.

6 Huấn luyện SPIN
Chúng tôi tái tạo mô hình SFT Zephyr-7B-Beta (Tunstall et al., 2023) thông qua SPIN thay vì SFT như được hình thành bởi Chen et al. (2024). Chúng tôi bắt đầu với mô hình cơ sở Mistral-7B (Jiang et al., 2023) và thực hiện huấn luyện SPIN theo Chen et al. (2024). Tuy nhiên, chúng tôi có một vài khác biệt so với phương pháp của họ, ở chỗ chúng tôi không tiêm các thế hệ từ lần lặp trước vào lần lặp hiện tại (điều này sẽ tăng gấp đôi kích thước tập dữ liệu mỗi epoch), và chúng tôi chỉ huấn luyện cho một lần lặp duy nhất, với 1 epoch mỗi lần lặp. Ngoài ra, chúng tôi sử dụng một tập con ngẫu nhiên chỉ 50k mẫu từ Ultrachat200k (Ding et al., 2023) thay vì toàn bộ

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại COLM 2024

tập dữ liệu, và sử dụng AdamW thay vì RMSProp. Tốc độ học tập của chúng tôi là 5e-7 với tổng cộng 400 bước, 40 bước khởi động, và LR này sau đó được giảm xuống 1e-7 cho 100 bước cuối cùng sử dụng ủ cosine. Kích thước batch toàn cục là 64, weight decay là 0.0, và hệ số điều chỉnh KL là 0.1, theo Chen et al. (2024). Sử dụng phương pháp này, chúng tôi đạt được điểm số MT-Bench là 7.04 vượt quá 6.64 của Zephyr-7B-Beta sử dụng SFT (Tunstall et al., 2023), cũng như 6.78 của mô hình SPIN 3 lần lặp (Chen et al., 2024).

[Hình 4: Thông lượng tương đối của LoRA được áp dụng cho huấn luyện SteerLM khi số lượng GPU tăng.]

7 Tính mở rộng của framework
Chúng tôi thiết kế NeMo-Aligner với tính mở rộng trong tâm trí, cho phép người dùng dễ dàng sửa đổi các thuật toán bất chấp sự phức tạp của huấn luyện phân tán. Chúng tôi thực hiện điều này bằng cách sử dụng trừu tượng hóa trainer, khuyến khích tái sử dụng các phương pháp trainer hiện có qua các bước và phương pháp khác nhau. Tính mở rộng của NeMo-Aligner cho phép các biến thể của DPO được tích hợp với những thay đổi mã tối thiểu, bao gồm Tối ưu hóa sở thích nhận dạng (Azar et al., 2023), DPO bảo thủ (Mitchell, 2023), và Tối ưu hóa Kahneman-Tversky (Ethayarajh et al., 2023). Hơn nữa, các kỹ thuật căn chỉnh mô hình khác như AI hiến pháp (Bai et al., 2022b), Lấy mẫu từ chối (Touvron et al., 2023), và Mô hình ngôn ngữ tự thưởng (Yuan et al., 2024) cũng đang được tích hợp vào NeMo-Aligner, được tạo điều kiện bởi thiết kế framework.

8 Kết luận
Các kỹ thuật căn chỉnh mô hình hiện đại, đặc biệt là những kỹ thuật dựa trên Học tăng cường, đặt ra những thách thức tối ưu hóa phức tạp về triển khai hệ thống. Chúng tôi tạo ra và mã nguồn mở NeMo-Aligner để cho phép các nhà nghiên cứu và thực hành AI thử nghiệm hiệu quả với căn chỉnh LLM bằng cách sử dụng tất cả tính toán có sẵn theo cách có thể mở rộng. Framework của chúng tôi mở rộng tốt một cách nhất quán khi huấn luyện các mô hình lớn với nhiều tính toán hơn. Vì đây là phiên bản đầu tiên của chúng tôi, chúng tôi mong đợi việc mở rộng này chỉ cải thiện với các phiên bản tương lai. Ngoài ra, chúng tôi hỗ trợ SFT, PPO, DPO, SteerLM theo cách hiệu quả tham số sử dụng LoRA cho các cài đặt tính toán hạn chế. Như một codebase mã nguồn mở được cấp phép Apache 2.0, NeMo-Aligner có thể làm cho nghiên cứu căn chỉnh hiệu quả và dễ tiếp cận hơn.

Lời cảm ơn
Chúng tôi muốn cảm ơn nhiều nhóm tại NVIDIA đã đóng góp cho việc kích hoạt NeMo-Aligner, đặc biệt là các nhóm NeMo, TRT-LLM và TensorRT.

--- TRANG 10-16 ---
[Phần tài liệu tham khảo tiếp theo...]
