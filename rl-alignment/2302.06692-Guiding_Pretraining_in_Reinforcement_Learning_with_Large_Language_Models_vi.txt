- Tấn công bộ xương

Bạn thấy nước, cỏ, bò, và kim cương. Bạn đang nhắm mục tiêu cỏ. Bạn có trong kho đồ thực vật. Bạn làm gì?
- Uống nước
- Chặt cỏ
- Tấn công bò
- Đặt thực vật

Tổng cộng, các hành động có trong gợi ý chiếm:
• 6 / 10 (60%) các hành động tốt mà tác nhân ELLM nhận được.
• 6 / 21 (28.6%) tất cả hành động được thưởng mà tác nhân nhận được.
• 7 / 15 (50%) tất cả hành động tốt được đề xuất.
• 7 / 51 (13.7%) tất cả hành động được đề xuất.

Trong công việc tương lai, sẽ thú vị khi khám phá cách hiệu suất thay đổi với ít hành động hơn được bao gồm trong gợi ý. Như một thí nghiệm sơ bộ, chúng tôi đã thấy rằng hiệu suất tiền huấn luyện được duy trì nếu bạn cung cấp một gợi ý chỉ với một ví dụ về danh sách các mục tiêu hợp lệ. Danh sách chỉ chứa hai mục tiêu. Thay vào đó, chúng tôi sử dụng hướng dẫn mở rộng hơn để nói với tác nhân các đề xuất tốt trông như thế nào. Xem gợi ý bên dưới và so sánh tiền huấn luyện trong Hình 11. Gợi ý mới này đi kèm với sự giảm tỷ lệ đề xuất "Tốt" (được hiển thị trong Bảng 4, cho thấy rằng độ chính xác đề xuất không hoàn toàn tương quan với thành công.

Gợi ý mới: Hành động hợp lệ: ngủ, ăn, tấn công, chặt, uống, đặt, làm, khai thác
Bạn là một người chơi đang chơi trò chơi giống Minecraft. Đề xuất những hành động tốt nhất mà người chơi có thể thực hiện theo các hướng dẫn sau.
1. Đưa ra đề xuất dựa trên những gì bạn thấy và các vật phẩm trong kho đồ của bạn.
2. Mỗi cảnh độc lập. Chỉ đưa ra đề xuất dựa trên các đối tượng có thể nhìn thấy, trạng thái, và kho đồ trong cảnh hiện tại.
3. Mỗi đề xuất nên là một hành động hợp lệ đơn, hoặc một cụm từ bao gồm một hành động và một đối tượng. (ví dụ: "Ăn thực vật").
4. Không đưa ra các đề xuất không thể hoặc không mong muốn, chẳng hạn như "Ăn bộ xương".
5. Chỉ đưa ra các đề xuất hợp lý cho cảnh hiện tại (ví dụ chỉ "Ăn thực vật" nếu một thực vật có thể nhìn thấy).
6. Bạn có thể đề xuất nhiều hành động với cùng một đối tượng, nhưng không lặp lại các mục danh sách.
7. Sử dụng kiến thức của bạn về Minecraft để đưa ra đề xuất.
8. Ưu tiên các hành động liên quan đến đối tượng mà bạn đang đối mặt hoặc mà tác nhân chưa đạt được trước đây.
9. Mỗi cảnh sẽ bao gồm số lượng đề xuất tối thiểu và tối đa. Giữ trong phạm vi này.

Cảnh mới: Bạn thấy thực vật, bò, và bộ xương. Bạn đang đối mặt với bộ xương. Bạn làm gì (bao gồm 1-2 đề xuất)?
- Ăn thực vật
- Tấn công bộ xương

Cảnh mới: Bạn thấy [CHÈN MÔ TẢ CẢNH HIỆN TẠI.] Bạn làm gì (bao gồm 2-7 đề xuất)?

--- TRANG 15 ---
Hướng dẫn Tiền huấn luyện trong Học tăng cường với Mô hình Ngôn ngữ Lớn

[THIS IS TABLE: Table 4 showing fractions of suggested and rewarded goals]
Bảng 4: Tỷ lệ các mục tiêu được đề xuất và được thưởng tốt, được tạo với gợi ý hai ví dụ đã sửa đổi.

Được đề xuất | Được thưởng
Không nhạy cảm ngữ cảnh | 21.0% | 0.8%
Không nhạy cảm thường thức | 20.5% | 54.8%
Tốt | 34.1% | 44.4%
Không thể | 24.5% | 0%

[THIS IS FIGURE: Graph showing comparison between performance of prompts containing 7 vs 2 example goals over time steps]
Hình 11: So sánh giữa hiệu suất của gợi ý chứa 7 mục tiêu ví dụ (được sử dụng trong toàn bộ bài báo) và gợi ý đã sửa đổi chỉ bao gồm 2 ví dụ.

E. Không gian Hành động Crafter

Chúng tôi mở rộng không gian hành động của Crafter để tăng độ khó khám phá và nghiên cứu xem ELLM có thể học tránh các hành động vô nghĩa hoặc không khả thi hay không. Không gian hành động đầy đủ bao gồm chỉ các động từ (cho các hành động không tác động lên bất cứ thứ gì, như ngủ) hoặc các kết hợp động từ + danh từ như sau:

• Động từ: không làm gì (không có danh từ), di chuyển trái (không có danh từ), di chuyển phải (không có danh từ), di chuyển lên (không có danh từ), di chuyển xuống (không có danh từ), ngủ (không có danh từ), khai thác, ăn, tấn công, chặt, uống, đặt, làm

• Danh từ: zombie, bộ xương, bò, cây, đá, than, sắt, kim cương, nước, cỏ, bàn chế tạo, lò, thực vật, rìu gỗ, rìu đá, rìu sắt, kiếm gỗ, kiếm đá, kiếm sắt

Ví dụ, một hành động có thể là uống nước hoặc uống cỏ.

F. Nhiệm vụ Housekeep

Điểm chuẩn Housekeep gốc có một tập hợp lớn các cảnh và tập khác nhau của hộ gia đình với các đối tượng và thùng chứa khác nhau có thể được khởi tạo. Việc đặt đối tượng-thùng chứa đúng sự thật cơ bản được xác định bằng cách thu thập từ đám đông con người. Tuy nhiên, vì trọng tâm của chúng tôi là tiền huấn luyện RL, chúng tôi không sử dụng các phương pháp ánh xạ và lập kế hoạch từ điểm chuẩn gốc. Để định phạm vi vấn đề cho RL, chúng tôi tập trung vào 4 nhiệm vụ đầu tiên với 5 đối tượng bị đặt sai chỗ khác nhau mỗi nhiệm vụ.

[THIS IS TABLE: Table 5 showing misplaced objects per task]
Bảng 5: Các đối tượng mỗi nhiệm vụ

Các đối tượng bị đặt sai chỗ
Nhiệm vụ 1 | bạc hà, đèn, đèn lồng, phi lê cá trích, bình hoa
Nhiệm vụ 2 | đèn, nước có ga, cây, giá nến, chai mù tạt
Nhiệm vụ 3 | lon pepsi, máy sưởi điện, mũ bảo hiểm, bóng golf, bánh kẹo trái cây
Nhiệm vụ 4 | sô cô la, ramekin, chảo, máy xé giấy, dao

--- TRANG 16 ---
Hướng dẫn Tiền huấn luyện trong Học tăng cường với Mô hình Ngôn ngữ Lớn

[THIS IS TABLE: DQN Hyperparameters table showing parameters for Crafter and Housekeep]
Tên | Giá trị (Crafter) | Giá trị (Housekeep)
Frame Stack | 4 | 4
γ | .99 | .99
Seed Frames | 5000 | 5000
n-step | 3 | 3
batch size | 64 | 256
lr | 6.25e-5 | 1e-4
target update τ | 1.0 | 1.0
ε-min | 0.01 | 0.1
update frequency | 4 | 4

Bảng 6: Siêu tham số DQN

G. Gợi ý Housekeep

Bạn là một robot trong một ngôi nhà. Bạn có khả năng nhặt các đối tượng và đặt chúng vào các vị trí mới.
Đối với mỗi ví dụ, hãy nói xem vật phẩm có nên được lưu trữ trong/trên thùng chứa hay không.

Bạn có nên lưu trữ một thìa bẩn trong/trên ghế: Không.
Bạn có nên lưu trữ một bát trộn trong/trên máy rửa bát: Có.
Bạn có nên lưu trữ một tất sạch trong/trên ngăn kéo: Có.

H. Chi tiết Thuật toán

Chúng tôi sử dụng DQN (Mnih et al., 2013), với học Q kép (Van Hasselt et al., 2016), mạng đối kháng (Wang et al., 2016), và học đa bước (Sutton et al., 1998).

Đối với cả hai môi trường, các chính sách nhận đầu vào là hình ảnh 84 ×84 được mã hóa bằng CNN Atari Nature tiêu chuẩn (Mnih et al., 2015). Hình ảnh sau đó được truyền qua một lớp tuyến tính để xuất ra một vector 512 chiều. Nếu chính sách có điều kiện văn bản, chúng tôi tính toán nhúng ngôn ngữ của chú thích trạng thái sử dụng mô hình SBERT paraphrase-MiniLM-L3-v2 (Reimers & Gurevych, 2019), và nếu chính sách có điều kiện mục tiêu, chúng tôi tương tự tính toán nhúng ngôn ngữ của các mục tiêu g1:k sử dụng paraphrase-MiniLM-L3-v2. Chúng tôi mã hóa tất cả mục tiêu như một chuỗi văn bản đơn vì chúng tôi không thấy cải thiện nào từ việc mã hóa chúng riêng lẻ và tổng hoặc nối các nhúng. Các nhúng hình ảnh và văn bản sau đó được nối với nhau trước khi được truyền đến các Q-network. Mỗi luồng giá trị và lợi thế của hàm Q được tham số hóa như các MLP 3 lớp, với các chiều ẩn 512 và phi tuyến ReLU.

Trong môi trường Crafter, chúng tôi quét qua các siêu tham số sau cho các điều kiện Oracle và Scratch (không tiền huấn luyện): tốc độ học, lịch trình suy giảm khám phá, và tần suất cập nhật mạng. Sau đó chúng tôi áp dụng các siêu tham số này cho tất cả các điều kiện, sau khi xác nhận rằng các siêu tham số thành công một cách rộng rãi trong mỗi trường hợp.

Đối với tiền huấn luyện Housekeep, chúng tôi quét lr ∈[1e−3,1e−4,1e−5], ε-min∈[0.1,0.01], và batch size ∈[64,256].

I. Chi tiết Bộ chú thích Mã hóa cứng

Crafter Bộ chú thích trạng thái dựa trên mẫu được hiển thị trong Hình 3 (trái). Điều này bao gồm ba thành phần: quan sát, các vật phẩm, và trạng thái tác nhân.

• Quan sát: Chúng tôi lấy biểu diễn ngữ nghĩa cơ bản của hình ảnh hiện tại từ trình mô phỏng. Về cơ bản điều này ánh xạ mỗi ô lưới có thể nhìn thấy thành một mô tả văn bản (ví dụ mỗi đồ họa cây được ánh xạ thành "cây"). Sau đó chúng tôi lấy tập hợp các mô tả này (tức là không tính đến số lượng của mỗi đối tượng) và điền vào ô "quan sát" của mẫu.

• Vật phẩm: Chúng tôi chuyển đổi mỗi vật phẩm kho đồ thành bộ mô tả văn bản tương ứng, và sử dụng tập hợp mô tả này để điền vào ô "vật phẩm" của mẫu.

• Trạng thái sức khỏe: Chúng tôi kiểm tra xem có trạng thái sức khỏe nào dưới mức tối đa hay không, và nếu có, chuyển đổi từng cái thành mô tả ngôn ngữ tương ứng (ví dụ nếu trạng thái đói < 9, chúng tôi nói tác nhân "đói").

Bộ chú thích chuyển đổi sử dụng các nhãn hành động. Mỗi hành động ánh xạ trực tiếp đến một cặp động từ + danh từ được định nghĩa trước (ví dụ "ăn bò").

Housekeep Bộ chú thích trạng thái dựa trên mẫu được hiển thị trong Hình 3 (phải). Chúng tôi sử dụng cảm biến ngữ nghĩa của trình mô phỏng để có danh sách tất cả các đối tượng, thùng chứa có thể nhìn thấy, và đối tượng hiện đang được cầm. Bộ chú thích chuyển đổi cũng dựa trên cảm biến ngữ nghĩa của trình mô phỏng, cho biết các đối tượng có thể nhìn thấy hiện đang ở trong thùng chứa nào.

J. Bộ chú thích Crafter Đã học

Bộ chú thích được huấn luyện với thuật toán ClipCap đã sửa đổi một chút (Mokady et al., 2021a) trên tập dữ liệu các quỹ đạo được tạo bởi một chính sách được huấn luyện sử dụng triển khai PPO từ Stanić et al. (2022). Các quan sát thị giác ở bước thời gian t và t+1 được nhúng với mô hình CLIP ViT-B-32 được tiền huấn luyện và đông lạnh (Radford et al., 2021) và được nối với nhau cùng với sự khác biệt trong các nhúng ngữ nghĩa giữa hai trạng thái tương ứng. Các nhúng ngữ nghĩa bao gồm kho đồ và một nhúng đa nóng của tập hợp các đối tượng có trong tầm nhìn cục bộ của tác nhân. Biểu diễn nối này của chuyển đổi sau đó được ánh xạ thông qua một hàm ánh xạ đã học thành một chuỗi 10 token. Cuối cùng, chúng tôi sử dụng 10 token này như một tiền tố và theo đuổi giải mã sử dụng GPT-2 được tiền huấn luyện và đông lạnh để tạo chú thích (Radford et al., 2019). Chúng tôi huấn luyện ánh xạ từ biểu diễn chuyển đổi đến token GPT trên tập dữ liệu 847 nhãn con người và 900 nhãn tổng hợp được thu thập bằng cách lấy mẫu từ một tập hợp giữa 3 và 8 chú thích khác nhau cho mỗi loại chuyển đổi riêng biệt. Thay vì "chặt cây" và "tấn công zombie" theo chương trình, các chú thích được gán nhãn bao gồm các câu hoàn chỉnh: "Bạn đã thu thập một cây con từ mặt đất", "Bạn đã chế tạo một thanh kiếm từ gỗ", hoặc "Bạn chỉ nhìn chằm chằm ra biển". Do sự đa dạng ngôn ngữ bổ sung này, chúng tôi so sánh chú thích với mục tiêu với ngưỡng tương đồng cosine thấp hơn là .5.

Các bộ chú thích không hoàn hảo có thể gây ra vấn đề học tập theo hai cách khác nhau: (1) chúng có thể tạo ra chú thích hoàn toàn sai và (2) chúng có thể tạo ra chú thích hợp lệ vẫn dẫn đến tính toán phần thưởng sai. Nếu chú thích khác biệt về mặt ngôn ngữ quá nhiều so với thành tựu mà nó chú thích, phần thưởng dựa trên độ tương đồng có thể không thể nhận ra nó (phần thưởng âm tính giả). Cùng sự biến đổi ngôn ngữ này có thể khiến hàm thưởng phát hiện việc đạt được một thành tựu khác không được đạt được (phần thưởng dương tính giả). Hình 12 đo lường tất cả các vấn đề này cùng một lúc. Đối với mỗi hàng, nó trả lời: xác suất mà hàm thưởng sẽ phát hiện phần thưởng dương cho mỗi thành tựu cột là bao nhiêu khi thành tựu thực sự là nhãn hàng? Tỷ lệ âm tính giả trung bình là 11% (1 - các giá trị đường chéo), với tỷ lệ âm tính giả cao hơn nhiều đối với chặt cỏ (100%). Thực sự, chú thích con người đề cập đến kết quả của hành động đó thay vì bản thân hành động (thu thập cây con); mà phần thưởng dựa trên độ tương đồng không thể nắm bắt được. Tỷ lệ dương tính giả (tất cả giá trị không đường chéo) đáng kể ở đây: tác nhân có thể được thưởng cho một số thành tựu mà nó không mở khóa. Điều này thường xảy ra khi các thành tựu chia sẻ từ (ví dụ gỗ, đá, thu thập). Điều này cho thấy khó khăn của độ tương đồng ngữ nghĩa trong việc phân biệt giữa các thành tựu liên quan đến những từ này.

K. Phân tích LLM Crafter

Bảng 2 cho thấy rằng các hành động mà tác nhân được thưởng bị thống trị bởi các hành động tốt (66.5%) và hành động xấu (32.4%). Điều này hợp lý; các hành động không thể không bao giờ có thể đạt được. Hầu hết không nhạy cảm ngữ cảnh không thể đạt được (ví dụ "uống nước" được đề xuất khi không có nước). Chúng tôi coi một hành động là "thành công" bằng cách kiểm tra xem tác nhân có cố gắng một hành động cụ thể trước đúng đối tượng hay không, vì vậy tác nhân thỉnh thoảng được thưởng khi nó thực hiện một hành động không nhạy cảm ngữ cảnh ở vị trí vật lý phù hợp nhưng không có các điều kiện tiên quyết cần thiết (ví dụ khai thác đá mà không có rìu hoặc không ở bên đá).

Bảng 7 đưa ra ví dụ về các đề xuất LLM trong Crafter.

[THIS IS TABLE: Table 7 showing LLM suggestion types and examples]
Loại đề xuất | Ví dụ
Tốt | chặt cây, tấn công bộ xương, đặt thực vật
Không nhạy cảm ngữ cảnh | làm bàn chế tạo (không có gỗ), khai thác đá (không có rìu hoặc không ở bên đá)
Không nhạy cảm thường thức | khai thác cỏ, làm kim cương, tấn công thực vật
Không thể | làm đường, làm gỗ, đặt dung nham

Bảng 7: Độ chính xác phân loại của LLM cho mỗi nhiệm vụ Housekeep (cột trái là dương tính thật, cột phải là âm tính thật).

--- TRANG 18 ---
Hướng dẫn Tiền huấn luyện trong Học tăng cường với Mô hình Ngôn ngữ Lớn

[THIS IS FIGURE: A reward confusion matrix showing achievement detection probabilities. The matrix is labeled "Hình 12" and shows various activities like "do nothing", "chop tree", "make crafting table" etc. on both axes, with probability values represented in grayscale.]

Hình 12: Ma trận nhầm lẫn phần thưởng. Mỗi hàng đưa ra xác suất rằng bất kỳ thành tựu cột nào được phát hiện khi thành tựu hàng thực sự được mở khóa. Ví dụ, trong hàng 2, khi tác nhân chặt cây, với xác suất cao tác nhân sẽ được thưởng cho các hành động "chặt cây" và "chặt cỏ". Được kiểm tra trên các quỹ đạo thu thập từ một chính sách PPO chuyên gia, mỗi hàng ước tính xác suất sử dụng từ 27 đến 100 điểm dữ liệu (27 cho khai thác sắt, thành tựu hiếm nhất). Các hàng không tổng bằng một, vì một thành tựu cho trước, tùy thuộc vào chú thích cụ thể của nó, có thể kích hoạt một số phần thưởng.

L. Ablation Bonus Mới lạ

Chúng tôi ablate tầm quan trọng của thiên vị mới lạ của ELLM trong Hình 13 bằng cách cho phép tác nhân được thưởng lặp đi lặp lại vì đạt được cùng một mục tiêu. Chúng ta thấy rằng không có bonus mới lạ, tác nhân chỉ học lặp lại một tập hợp nhỏ các mục tiêu dễ dàng và thất bại trong việc khám phá đa dạng.

M. Phân tích các Phương pháp Huấn luyện Xuôi dòng

Chúng tôi khám phá hai phương pháp sử dụng các chính sách khám phá: tinh chỉnh, nơi các trọng số của chính sách khám phá được tinh chỉnh và phương pháp khám phá có hướng dẫn, nơi một chính sách mới được huấn luyện từ đầu và chính sách được tiền huấn luyện được sử dụng cho khám phá ε-greedy.

Chúng tôi thấy rằng trong Housekeep cả hai phương pháp đều hiệu quả đối với ELLM (Hình 7a và Hình 7b). Tuy nhiên, trong Crafter chúng tôi thấy rằng phương pháp tinh chỉnh hoạt động kém trên tất cả các phương pháp (ELLM, đường cơ sở, và oracle). Thường xuyên, chúng tôi quan sát thấy rằng sớm trong tinh chỉnh, tác nhân sẽ không học tất cả các hành vi hữu ích trước đây của nó, bao gồm cả việc di chuyển xung quanh môi trường để tương tác với các đối tượng. Chúng tôi giả thuyết rằng điều này do sự không khớp trong mật độ và cường độ phần thưởng giữa tiền huấn luyện và tinh chỉnh. Khi tác nhân tinh chỉnh thấy rằng nó đạt được lợi nhuận thấp hơn nhiều so với lợi nhuận dự kiến khi thực hiện các hành động điển hình của nó, nó giảm trọng số khả năng thực hiện những hành động đó và không học các kỹ năng trước đây của nó. Chúng tôi thấy rằng việc giảm tốc độ học, đông lạnh các lớp đầu của mạng, điều chỉnh thủ công phần thưởng tinh chỉnh ở cùng quy mô với phần thưởng tiền huấn luyện, và giảm tốc độ khám phá ban đầu một phần giảm thiểu vấn đề này. Tuy nhiên, những điều này cũng giảm hiệu quả mẫu và/hoặc hiệu suất ở hội tụ của chính sách được tinh chỉnh so với đường cơ sở huấn luyện từ đầu. Trong Hình 14), trên tất cả các phương pháp, phương pháp này kém tin cậy hơn so với phương pháp khám phá có hướng dẫn (Hình 5).

Những phát hiện này phù hợp với các phát hiện Housekeep của chúng tôi. Trong môi trường đó, nhiệm vụ tiền huấn luyện ELLM (đạt được việc đặt đối tượng được đề xuất bởi một LLM) và nhiệm vụ tinh chỉnh (đạt được việc đặt đối tượng được đề xuất bởi con người) đủ tương tự chúng tôi chỉ thấy sự giảm nhỏ về hiệu suất khi tinh chỉnh bắt đầu. Tuy nhiên, các đường cơ sở RND và APT có sự không khớp tiền huấn luyện-tinh chỉnh lớn hơn, và chúng tôi quan sát thấy những phương pháp đó hoạt động tương đối tốt hơn với phương pháp khám phá có hướng dẫn.

--- TRANG 19 ---
Hướng dẫn Tiền huấn luyện trong Học tăng cường với Mô hình Ngôn ngữ Lớn

[THIS IS FIGURE: Two sets of graphs labeled (a) and (b):
(a) Shows Crafter pretraining runs with different methods plotted over environment steps
(b) Shows Housekeep pretraining runs with multiple panels showing success rates over environment steps]

Hình 13

[THIS IS FIGURE: A set of 8 graphs showing success rates across training for different downstream tasks in the Crafter environment, including "Place Crafting Table", "Attack Cow", "Make Wood Sword", "Mine Stone", "Deforestation", "Plant Row", "Gardening", and "Crafter (Game Reward)"]

Hình 14: Tỷ lệ thành công qua việc huấn luyện cho từng nhiệm vụ xuôi dòng trong môi trường Crafter. Mỗi lần chạy tinh chỉnh tác nhân được tiền huấn luyện sử dụng tốc độ học thấp hơn so với được sử dụng trong quá trình tiền huấn luyện (2e−5). Các biểu đồ hiển thị trung bình ±độ lệch chuẩn cho 5 hạt giống

N. Các Đường cơ sở Bổ sung

Chúng tôi cũng bao gồm các thí nghiệm với NovelD (Zhang et al., 2021) trong Hình 15, một phương pháp khám phá tiên tiến sử dụng ước tính sự mới lạ của trạng thái để thưởng cho tác nhân vì di chuyển đến các trạng thái mới lạ hơn. Trong quá trình tiền huấn luyện, chúng tôi thấy nó hoạt động tương tự như các phương pháp động lực nội tại không có tiên nghiệm khác.

O. Mã và Tính toán

Tất cả mã sẽ được phát hành sớm, được cấp phép dưới giấy phép MIT (với Crafter, Housekeep được cấp phép dưới các giấy phép tương ứng của chúng).

Để truy cập LLM, chúng tôi sử dụng API của OpenAI. Các thí nghiệm ban đầu với các mô hình GPT-3 nhỏ hơn dẫn đến hiệu suất suy giảm, do đó chọn Codex và Davinci cho các thí nghiệm của chúng tôi. Codex miễn phí sử dụng và Davinci có giá $0.02/1000 token. Chúng tôi thấy việc cache rất hữu ích trong việc giảm số lượng truy vấn được thực hiện đến API. Mỗi truy vấn API mất .02 giây, vì vậy không có cache một lần chạy huấn luyện 5 triệu bước sẽ dành 27 giờ truy vấn API (và xa hơn nữa khi chúng ta đạt đến giới hạn tốc độ OpenAI) và tốn hàng nghìn đô la. Vì chúng tôi cache mạnh mẽ và tái sử dụng cache qua các lần chạy, đến cuối thí nghiệm của chúng tôi, hầu như không thực hiện truy vấn API nào mỗi lần chạy.

Chúng tôi sử dụng NVIDIA TITAN Xps và NVIDIA GeForce RTX 2080 Tis, với 2-3 hạt giống mỗi GPU và chạy khoảng

--- TRANG 20 ---
Hướng dẫn Tiền huấn luyện trong Học tăng cường với Mô hình Ngôn ngữ Lớn

[THIS IS FIGURE: Two sets of graphs labeled (a) and (b):
(a) Shows "Crafter pretraining curve" with lines for different methods including Oracle, ELLM, RND, APT, Novelty, Uniform, and NovelD plotted against Environment Steps
(b) Shows "Housekeep pretraining curves" with multiple panels showing different task performance over Environment Steps]

Hình 15: Các đường cong tiền huấn luyện bổ sung bao gồm đường cơ sở NovelD.

100k bước/giờ. Qua tất cả các ablation, điều này tương đương với khoảng 100 GPU cho tiền huấn luyện.

P. Tác động Xã hội

Mặc dù các tiên nghiệm LLM đã được chứng minh thể hiện khả năng thường thức ấn tượng, cũng được biết rằng những mô hình như vậy rất dễ bị thiên vị xã hội có hại và rập khuôn (Bender et al., 2021; Abid et al., 2021; Nadeem et al., 2020). Khi sử dụng những mô hình như vậy làm hàm thưởng cho RL, như trong ELLM, cần thiết phải hiểu đầy đủ và giảm thiểu bất kỳ hành vi tiêu cực có thể nào có thể được học do những thiên vị như vậy. Trong khi chúng tôi tập trung vào các môi trường và nhiệm vụ mô phỏng trong công việc này, chúng tôi nhấn mạnh rằng nghiên cứu cẩn thận hơn là cần thiết nếu một hệ thống như vậy được triển khai để học mở hơn trong thế giới thực. Các biện pháp giảm thiểu tiềm năng với ELLM cụ thể có thể là: lọc tích cực các tạo sinh LLM cho nội dung có hại trước khi sử dụng chúng như các mục tiêu được đề xuất, gợi ý LM với hướng dẫn về loại gợi ý nào nên xuất ra, và/hoặc chỉ sử dụng biến thể ELLM dạng đóng với không gian mục tiêu được ràng buộc cẩn thận hơn.
