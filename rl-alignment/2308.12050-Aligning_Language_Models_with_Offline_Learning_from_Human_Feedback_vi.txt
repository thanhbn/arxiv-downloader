# 2308.12050.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2308.12050.pdf
# Kích thước tệp: 797713 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Căn chỉnh Mô hình Ngôn ngữ với Học tập Ngoại tuyến từ Phản hồi Con người
Jian Hu
NVIDIA
jianh@nvidia.comLi Tao∗
NVIDIA
lit@nvidia.comJune Yang
NVIDIA
juney@nvidia.comChandler Zhou
NVIDIA
chandlerz@nvidia
Tóm tắt
Học hỏi từ sở thích của con người là rất quan trọng
để các mô hình ngôn ngữ (LM) có thể phục vụ hiệu quả
nhu cầu con người và các giá trị xã hội. Nghiên cứu
trước đây đã đạt được tiến bộ đáng kể bằng cách tận
dụng phản hồi của con người để tuân theo hướng dẫn.
Tuy nhiên, các phương pháp này chủ yếu dựa vào
kỹ thuật học trực tuyến như Tối ưu hóa Chính sách
Gần kề (PPO), được chứng minh là không ổn định
và khó điều chỉnh cho các mô hình ngôn ngữ. Hơn nữa,
PPO yêu cầu triển khai hệ thống phân tán phức tạp,
cản trở hiệu quả của việc huấn luyện phân tán quy mô lớn.
Trong nghiên cứu này, chúng tôi đề xuất khung học tập
ngoại tuyến từ phản hồi con người để căn chỉnh LM mà
không tương tác với môi trường. Cụ thể, chúng tôi khám phá
căn chỉnh lọc (FA), hồi quy có trọng số phần thưởng
(RWR), và căn chỉnh có điều kiện (CA) để căn chỉnh
các mô hình ngôn ngữ với sở thích con người. Bằng cách
sử dụng hàm mất mát tương tự như tinh chỉnh có giám sát,
các phương pháp của chúng tôi đảm bảo việc huấn luyện
mô hình ổn định hơn PPO với hệ thống học máy đơn giản
(MLSys) và ít tài nguyên tính toán hơn nhiều (khoảng 9%).
Kết quả thực nghiệm cho thấy căn chỉnh có điều kiện
vượt trội hơn các phương pháp căn chỉnh ngoại tuyến khác
và có thể so sánh được với PPO.

1 Giới thiệu
Gần đây, những tiến bộ trong các mô hình ngôn ngữ (LM)
đã cách mạng hóa xử lý ngôn ngữ tự nhiên, mở khóa
những khả năng chưa từng có trong việc tạo sinh văn bản.
Mặc dù các mô hình này rất mạnh mẽ, đôi khi chúng
tạo ra các đầu ra khác với mong muốn của con người,
như việc hữu ích, không gây phản cảm (Bai
et al., 2022), trung thực và công bằng. Một thách thức
quan trọng là làm cho các mô hình ngôn ngữ phù hợp với
các giá trị và sở thích của con người. Điều này có nghĩa là
điều chỉnh chúng sao cho các văn bản được tạo ra phù hợp
với ý định của một người hoặc được coi là hữu ích, trung thực
và vô hại. Các nhà nghiên cứu đang tìm cách để các mô hình
∗Đóng góp bằng nhauNgôn ngữ tạo ra các văn bản tuân theo đạo đức và
chuẩn mực xã hội của con người (Ouyang et al., 2022).
Một trong những yếu tố quan trọng nhất để đạt được
điều này là việc sử dụng phản hồi của con người, và
phương pháp nổi tiếng nhất cho phép các mô hình ngôn ngữ
căn chỉnh với ý định con người là Học tăng cường với
Phản hồi Con người (RLHF). RLHF sử dụng các thuật toán
học trực tuyến (Ouyang et al., 2022), chẳng hạn như
Tối ưu hóa Chính sách Gần kề (PPO) (Schulman
et al., 2017), kết hợp với các mô hình phần thưởng được
huấn luyện trên phản hồi con người để tinh chỉnh các mô hình ngôn ngữ.
Tuy nhiên, PPO gặp phải những thách thức về tính ổn định
huấn luyện và khó khăn trong việc điều chỉnh siêu tham số,
chẳng hạn như hạt giống, phạt phân kỳ Kullback-Leibler (KL),
tốc độ học, kích thước lô và các thủ thuật triển khai khác
(Islam et al., 2017; Henderson et al., 2018; Huang et al., 2022; Hu et al., 2021).
Các siêu tham số không phù hợp có thể khiến chính sách PPO
sụp đổ. Hơn nữa, việc triển khai thuật toán PPO cho mô hình
ngôn ngữ trong môi trường phân tán quy mô lớn là phức tạp,
đòi hỏi giao tiếp và phối hợp giữa nhiều mô-đun (NVIDIA, 2023),
chẳng hạn như các mô hình diễn viên, phê bình, chính sách
khởi tạo và phần thưởng được hiển thị trong Hình 1.
Điều này thêm độ phức tạp có thể cản trở hiệu quả
của hệ thống huấn luyện quy mô lớn. Ngoài ra, các mô-đun
diễn viên và phê bình được tham gia vào cả các nhiệm vụ
huấn luyện và suy luận để tạo mẫu, điều này làm tăng thêm
khó khăn trong việc tối ưu hóa hệ thống.

Trong công trình này, chúng tôi đề xuất một khung
căn chỉnh ngoại tuyến mà không tương tác với môi trường
và ba thuật toán căn chỉnh ngoại tuyến: căn chỉnh lọc (FA),
hồi quy có trọng số phần thưởng (RWR) (Peters and Schaal, 2007),
và căn chỉnh có điều kiện (CA). Đối với căn chỉnh lọc,
điểm phần thưởng được sử dụng để lọc mẫu, để chỉ những
mẫu chất lượng cao được sử dụng trong huấn luyện căn chỉnh.
Đối với RWR, điểm phần thưởng sẽ được sử dụng để điều chỉnh
mất mát, trong đó các mẫu có phần thưởng cao có tác động
nhiều hơn đến việc cập nhật tham số. ĐốiarXiv:2308.12050v2  [cs.CL]  10 Dec 2023

--- TRANG 2 ---
căn chỉnh có điều kiện, nó giới thiệu một chính sách dựa
trên phương pháp entropy chéo làm cho việc huấn luyện
ổn định và hiệu quả hơn. Đối với việc triển khai cụ thể,
chúng tôi đầu tiên huấn luyện một mô hình phần thưởng
chất lượng cao sử dụng các tập dữ liệu sở thích con người.
Mô hình phần thưởng sau đó được sử dụng để gán nhãn
các mẫu tinh chỉnh với phần thưởng. Cuối cùng, chúng tôi
sử dụng phần thưởng để tinh chỉnh mô hình ngôn ngữ
với các phương pháp đã đề cập ở trên.

Bằng cách sử dụng hàm mất mát tương tự như tinh chỉnh
có giám sát, các phương pháp căn chỉnh ngoại tuyến có thể
huấn luyện với hệ thống học máy đơn giản (MLSys) và
ít tài nguyên tính toán hơn nhiều so với PPO, phù hợp
cho các thí nghiệm huấn luyện căn chỉnh nhanh. Kết quả
thực nghiệm của chúng tôi cho thấy căn chỉnh có điều kiện
hoạt động tốt hơn các phương pháp căn chỉnh ngoại tuyến
khác và có thể so sánh được với PPO. Tóm lại, khung
căn chỉnh ngoại tuyến của chúng tôi cho phép căn chỉnh
hiệu quả và ổn định hơn các mô hình ngôn ngữ với sở thích
con người mà không cần hệ thống RL phân tán phức tạp.

2 Các Công trình Liên quan
2.1 Căn chỉnh Mô hình Ngôn ngữ với Phản hồi Con người

ChatGPT (OpenAI, 2022; Ouyang et al., 2022)
huấn luyện một mô hình ngôn ngữ lớn (LLM) dựa trên
mô hình Transformer Tạo sinh Tiền huấn luyện (GPT)-3.5
(Brown et al., 2020) theo 3 bước: tinh chỉnh có giám sát,
huấn luyện mô hình phần thưởng và huấn luyện PPO.

Tinh chỉnh Có giám sát (SFT) Các nhà nghiên cứu
tinh chỉnh GPT-3 trên các minh họa của con người từ
những người gán nhãn của họ sử dụng mất mát học có giám sát
trong Eq. 1.
loss(ϕ) =−X
ilogpϕ(xi|p, x<i),(1)
trong đó xi là token thứ i trong chuỗi, và p
là các hướng dẫn và lời nhắc của con người.

Huấn luyện Mô hình Phần thưởng (RM) Bắt đầu từ
mô hình SFT với lớp không nhúng cuối cùng được loại bỏ,
các nhà nghiên cứu huấn luyện một mô hình để nhận vào
một lời nhắc và phản hồi và đưa ra một phần thưởng vô hướng.
Cụ thể, hàm mất mát cho mô hình phần thưởng là,
loss(θ) =
−E(x,yw,yl)∼D[log (σ(rθ(x, yw)−rθ(x, yl)))],
(2)trong đó rθ(x, y) là đầu ra vô hướng của mô hình
phần thưởng với tham số θ cho lời nhắc x và phản hồi
y, yw là phản hồi được ưa thích trong cặp yw
và yl, và D là tập dữ liệu so sánh của con người.

Huấn luyện PPO Các nhà nghiên cứu tinh chỉnh mô hình
SFT trên môi trường bandit của họ sử dụng PPO. Trong
môi trường này, một lời nhắc khách hàng ngẫu nhiên được
trình bày và một phản hồi được mong đợi. Môi trường
sau đó tạo ra một phần thưởng được xác định bởi mô hình
phần thưởng và kết thúc tập, cho cặp lời nhắc-phản hồi.
Ngoài ra, một phạt KL theo token từ mô hình SFT được
thêm vào mỗi token để giảm thiểu việc tối ưu hóa quá mức
của mô hình phần thưởng. Hàm giá trị được khởi tạo từ RM.

Môi trường bandit cho phép tối ưu hóa trực tiếp mô hình
SFT cho phần thưởng từ mô hình phần thưởng được tiền
huấn luyện trong khi điều chỉnh để không phân kỳ quá xa
khỏi SFT ban đầu thông qua phạt KL. Khởi tạo hàm giá trị
từ RM cung cấp một điểm khởi đầu ổn định cho tinh chỉnh RL.
Hàm mất mát của PPO-ptx được hiển thị trong Eq. 3.

objective (ϕ) =
E(x,y)∼DπRL
ϕ
rθ(x, y)−βlog
πRL
ϕ(y|x)/πSFT(y|x)
+γEx∼Dpretrain
log
πRL
ϕ(x)
,
(3)

trong đó πRL là chính sách học tăng cường đã học,
πSFT là mô hình tinh chỉnh có giám sát, và β là hệ số
phần thưởng KL kiểm soát độ mạnh của phạt KL.
Dpretrain là phân phối tiền huấn luyện và γ là hệ số
kiểm soát độ mạnh của mất mát tiền huấn luyện nhằm
sửa chữa sự suy thoái hiệu suất trên các tập dữ liệu
NLP công cộng. Khi γ được đặt thành 0, chúng ta gọi
thuật toán này là PPO.

2.2 Hệ thống Huấn luyện PPO Phân tán cho Các Mô hình Lớn

Có một số khung huấn luyện RLHF mã nguồn mở
như ColossalChat1 và DeepSpeed-Chat (Yao et al., 2023),
được phát triển dựa trên ZeRO (Rajbhandari et al., 2020).
Tuy nhiên, các khung này không phù hợp để huấn luyện
các mô hình cực kỳ lớn, chẳng hạn như các mô hình 70B2.
Điều này là do, với quy mô lớn của cả bốn mô hình
1https://chat.colossalai.org/
2Mặc dù DeepSpeedChat đã chứng minh việc huấn luyện
các mô hình 60B, kích thước mô hình phê bình và phần thưởng
của họ chỉ là 350 triệu tham số.

--- TRANG 3 ---
Hình 1: NVIDIA NeMo Aligner (NVIDIA, 2023) cho
các mô hình ngôn ngữ trên 22 nút - 22B cho các mô hình
diễn viên và chính sách và 8B cho các mô hình phần thưởng
và giá trị. Mỗi nút là một máy tính DGXA100 bao gồm
8 Đơn vị Xử lý Đồ họa NVIDIA A100 (GPU) được kết nối
với NVLink (Li et al., 2019). Các nút màu xám chỉ được
sử dụng để suy luận trong quá trình tạo mẫu PPO, trong khi
các nút màu xanh lá cây xử lý cả việc tạo mẫu và huấn luyện.
Các mô-đun này giao tiếp với nhau bằng Giao thức Truyền
Siêu văn bản (HTTP).

trong RLHF, sẽ là thách thức để tải chúng vào bộ nhớ
GPU chia sẻ. Đối với các mô hình lớn, hệ thống huấn luyện
PPO thường được thiết kế với nhiều mô-đun có thể giao tiếp
để giải quyết vấn đề là một nút duy nhất không thể tải
bốn mô hình lớn. Chẳng hạn như NVIDIA NeMo Aligner
(NVIDIA, 2023) được phát triển trên NeMo Megatron
(Shoeybi et al., 2019) bao gồm bốn mô-đun - diễn viên,
phê bình, chính sách khởi tạo và các mô hình phần thưởng.

Như được hiển thị trong Hình 1, bốn mô-đun này cần
giao tiếp với nhau trong quá trình huấn luyện. Và trong
quá trình huấn luyện PPO, mô-đun diễn viên tạo mẫu
dựa trên lời nhắc khách hàng và phân phối chúng đến
các mô-đun phần thưởng, chính sách khởi tạo và phê bình.
Giai đoạn này chúng ta gọi là giai đoạn tạo sinh thường
liên quan đến việc tối ưu hóa hiệu suất suy luận, bao gồm
bộ nhớ đệm khóa-giá trị (KV) (kipply, 2022), Flash-Attention
(Dao et al., 2022), chuyển đổi bố cục tensor (Vanholder, 2016)
cho động cơ suy luận, v.v., điều này làm tăng độ phức tạp
của toàn bộ hệ thống. Sau đó nó thu thập các phần trả về,
xác suất log ban đầu và giá trị từ các mô-đun này để
lắp ráp các mẫu huấn luyện PPO đầy đủ trong bộ đệm rollout.
Cuối cùng, các mẫu PPO được gửi đến các mô-đun diễn viên
và phê bình để tính toán mất mát PPO cho huấn luyện.

Trong khi thiết kế kiến trúc này cho phép các mô hình lớn
được huấn luyện, nó cũng gây ra thách thức trong việc cải thiện
hiệu quả sử dụng GPU và hiệu quả huấn luyện tổng thể do
độ phức tạp của hệ thống. Tối ưu hóa các tương tác để giảm
chi phí giao tiếp, chẳng hạn như sử dụng giao tiếp không đồng bộ
giữa mô-đun Diễn viên và các mô-đun khác, và tối đa hóa hiệu quả
một nút, là rất quan trọng để nâng cao hiệu suất tổng thể.
Tuy nhiên, độ phức tạp này ảnh hưởng đến cả hiệu quả
của huấn luyện mô hình và tính dễ sử dụng của khung huấn luyện.

3 Học tập Ngoại tuyến từ Phản hồi Con người

Trong phần này, chúng tôi đề xuất một khung căn chỉnh
ngoại tuyến - tức là mô hình ngôn ngữ được tinh chỉnh
trên các mẫu được tạo trước cho việc căn chỉnh ý định
con người, mà không có tương tác môi trường trong học trực tuyến.
Quá trình huấn luyện của căn chỉnh ngoại tuyến có thể
được mô tả trong các bước sau và được minh họa trong Hình 2.

3.1 Các Bước Căn chỉnh Ngoại tuyến

1. Tinh chỉnh Có giám sát Tương tự như ChatGPT, chúng tôi
đầu tiên tinh chỉnh mô hình tiền huấn luyện trên dữ liệu
hướng dẫn được gán nhãn bởi con người sử dụng học có giám sát.
Điều này cho phép mô hình, mà chúng tôi gọi là SFT, học
định dạng và ý định của các hướng dẫn con người.

2. Huấn luyện Mô hình Phần thưởng Chúng tôi cũng huấn luyện
một mô hình sở thích con người riêng biệt trên các hàm mất mát
có sẵn như mất mát nhị phân hoặc mất mát xếp hạng (Eq. 2).
Mô hình sở thích này học cách dự đoán phần thưởng.

3. Gán nhãn Mẫu Huấn luyện Chúng tôi sử dụng mô hình
phần thưởng để gán nhãn các mẫu hướng dẫn được thu thập,
bao gồm cả các mẫu được gán nhãn bởi con người và các mẫu
theo cặp từ huấn luyện mô hình phần thưởng.

4. Tinh chỉnh Căn chỉnh Cuối cùng, chúng tôi tinh chỉnh
mô hình SFT sử dụng các thuật toán học ngoại tuyến trên
các mẫu huấn luyện được gán nhãn phần thưởng để căn chỉnh
mô hình với sở thích con người. Các hướng dẫn và hàm mất mát
phụ thuộc vào phương pháp căn chỉnh ngoại tuyến nào chúng ta
sử dụng (tức là FA, RWR hoặc CA).

Chúng tôi cũng đề xuất ba thuật toán căn chỉnh ngoại tuyến:
1) căn chỉnh lọc, 2) hồi quy có trọng số phần thưởng
(Peters and Schaal, 2007), và 3) căn chỉnh có điều kiện.
Quá trình huấn luyện của các thuật toán này tuân theo
các bước được mô tả trước đó, nhưng mỗi thuật toán sử dụng
các hàm mất mát và phương pháp tiền xử lý dữ liệu khác nhau.
Cụ thể, FA sử dụng mất mát log-likelihood trong khi lọc ra
các mẫu có phần thưởng thấp. RWR cân bằng các mẫu theo
điểm phần thưởng trong một hồi quy. CA ánh xạ các token
trước đó và lời nhắc phần thưởng đến token tiếp theo.

Đối với tất cả các phương pháp, chúng tôi chuẩn hóa
phần thưởng bằng cách trừ trung bình và chia chúng cho
độ lệch chuẩn, vì các giá trị phần thưởng được dự đoán

--- TRANG 4 ---
Mẫu
SFT
Mẫu
RMMô hình Tiền huấn luyện
Mô hình SFTMô hình SFT
Mô hình Phần thưởng (RM)
Tinh chỉnh RM với mất mát SFT
Tinh chỉnh với mất mát RM
Tập dữ liệu
Hỗn hợpDự đoán phần thưởng và tiền xử lý mẫu
Mô hình SFTTinh chỉnh với FA, RWR hoặc CA
Mô hình Được Căn chỉnhTrộn dữ liệuHình 2: Quy trình làm việc của khung căn chỉnh ngoại tuyến.

−20 0 20 400.000.010.020.030.040.050.06điểm gốc
−5.0 −2.5 0.0 2.5 5.00.00.10.20.30.4điểm chuẩn hóa

Hình 3: Phân phối phần thưởng (điểm). Điểm phần thưởng
thô trực tiếp từ mô hình phần thưởng, có phạm vi rộng
do mục tiêu huấn luyện của nó. Để sử dụng tốt hơn điểm
phần thưởng trong căn chỉnh ngoại tuyến, chúng tôi chuẩn hóa
phần thưởng để có tâm bằng không.

bởi mô hình phần thưởng là liên tục mà không có phạm vi
được xác định trước, được hiển thị trong Hình 3. Eq. 4,
Eq. 5 và Eq. 6 hiển thị hàm mất mát của chúng, trong đó
θ là tham số của LM, π là chính sách, p là lời nhắc của
mẫu, x là phản hồi, R(p, x) là phần thưởng được gán nhãn
bởi mô hình phần thưởng. Bằng cách sử dụng hàm mất mát
tương tự như tinh chỉnh có giám sát, các phương pháp của
chúng tôi đảm bảo huấn luyện mô hình ổn định hơn PPO
với triển khai đơn giản và ít tài nguyên tính toán hơn.

3.2 Căn chỉnh Lọc

Chìa khóa cho căn chỉnh lọc là chỉ sử dụng dữ liệu
chất lượng cao. Một mô hình phần thưởng tốt có thể
xác định các phản hồi tốt hơn với điểm phần thưởng cao hơn.
Do đó, chúng tôi sử dụng mô hình phần thưởng để kiểm soát
mẫu nào có thể được sử dụng trong quá trình lan truyền ngược
trong huấn luyện, và hàm mất mát được định nghĩa là

LFilter(x) =−logπθ(x|p),nếuR(p, x)> t
0,ngược lại
(4)

trong đó t là ngưỡng để lọc các mẫu phần thưởng thấp.

3.3 Hồi quy Có trọng số Phần thưởng

Thay vì chỉ sử dụng các mẫu chất lượng cao, phương pháp
RWR sẽ áp dụng các trọng số mất mát khác nhau cho
các mẫu khác nhau theo điểm phần thưởng. Hàm mất mát
được định nghĩa là

LRWR(x) =
−|x|X
i=1logπθ(xi|x<i, p) exp ( R(p, x)/β)(5)

trong đó β là một siêu tham số để kiểm soát phần thưởng
ảnh hưởng đến mất mát như thế nào. xi là token thứ i
trong phản hồi.

3.4 Căn chỉnh Có điều kiện

Đối với phương pháp căn chỉnh có điều kiện, điểm phần thưởng
sẽ được đặt như một phần của lời nhắc chẳng hạn như
⟨reward ⟩a.b
trong đó a.b chỉ ra giá trị phần thưởng, cho phép
mô hình ngôn ngữ hiểu ý nghĩa của giá trị phần thưởng.
Do khả năng của các mô hình ngôn ngữ hiện tại, LM có thể
tự động nhận biết và mô hình hóa phản hồi với điểm phần thưởng.
Vì một lời nhắc bổ sung được thêm vào trong quá trình huấn luyện,

--- TRANG 5 ---
trong giai đoạn suy luận, cùng một mẫu lời nhắc nên được sử dụng.
Hàm mất mát cho CA trở thành

LCA(x) =−X
ilogpθ(xi|R∗(p, x), x<i, p)
(6)

trong đó hàm R∗(p, x) đề cập đến lời nhắc phần thưởng
của p và x. xi là token thứ i trong phản hồi.

4 Thiết lập Đánh giá
4.1 Kiến trúc Mô hình

Chúng tôi sử dụng mô hình GPT với 8 tỷ (B) tham số
làm mô hình cơ sở. Mô hình này có các đặc điểm kiến trúc
sau và được tiền huấn luyện trên 1,1 nghìn tỷ token đa ngôn ngữ.
Dưới đây là một số thuộc tính của mô hình:

•Mô hình sử dụng hàm kích hoạt SwiGLU (Shazeer, 2020)
•Nhúng vị trí xoay (RoPE) (Su et al., 2021)
• Độ dài chuỗi tối đa là 4.096.
• Không có dropout (Srivastava et al., 2014).
• Không có thuật ngữ bias trong tất cả các lớp tuyến tính.
• Lớp nhúng và đầu ra không liên kết.

4.2 Đường cơ sở và Đánh giá

Chúng tôi sử dụng mô hình PPO và mô hình SFT làm
đường cơ sở. Chúng tôi huấn luyện ba mô hình căn chỉnh
ngoại tuyến với căn chỉnh lọc, RWR và CA tương ứng.
Vậy, chúng tôi có tổng cộng 5 mô hình trong danh sách mô hình.
Tất cả các mô hình có 8B tham số, bao gồm diễn viên
và mô hình phần thưởng trong PPO. Chúng tôi đánh giá
hiệu suất của các mô hình thông qua đánh giá con người
và đánh giá GPT-4 (Chiang et al., 2023; Zheng et al., 2023)
sử dụng lời nhắc từ LMSys ChatBot Arena3. Tập dữ liệu
đánh giá này có 160 lời nhắc chất lượng cao bao gồm
các vấn đề chung, kiến thức, thông thức, fermi, phản thực tế,
chơi vai, khoa học, lập trình, toán học, viết, lý luận,
trích xuất, nhân văn, trích xuất và nhân văn.

3https://chat.lmsys.org/?arena4.3 Tập dữ liệu Tinh chỉnh Có giám sát

Đối với dữ liệu SFT, nó chứa các hướng dẫn và phản hồi.
Trong quá trình huấn luyện, chỉ mất mát của phản hồi
được tính toán. Tập dữ liệu SFT của chúng tôi là một hỗn hợp
của một số tập dữ liệu công cộng và tập dữ liệu được chú thích
thủ công, tổng cộng khoảng 100K mẫu, bao gồm

•Open Assistant (Köpf et al., 2023)4.
•Flan v2 (Wei et al., 2021)5.
•Databricks Dolly (Conover et al., 2023)6.
•Self-Instruct, chúng tôi đã tạo một số dữ liệu
thông qua OpenAI gpt-3.5-turbo qua công nghệ
Self-Instruct (Wang et al., 2022).

Và các mẫu dữ liệu được xây dựng với định dạng sau,
User:{instruction }Assistant: {response }

4.4 Tập dữ liệu Sở thích Con người

Đối với tập dữ liệu Sở thích Con người, chúng tôi sử dụng
cùng định dạng như ChatGPT, tức là so sánh từng phần
của con người, bao gồm lời nhắc, phản hồi tốt và phản hồi xấu.
Chúng tôi sử dụng hỗn hợp hai tập dữ liệu sở thích con người
công cộng sau để huấn luyện mô hình RM của chúng tôi:

•Tập dữ liệu Anthropic Helpful (Bai et al., 2022)7
là một bộ sưu tập quy mô lớn được cộng đồng thu thập
về phản hồi của con người về các phản hồi của trợ lý AI.
Nó chứa hơn 100.000 sở thích con người trên một loạt
các chủ đề hội thoại hàng ngày đa dạng.

•Open Assistant cũng đã thu thập một tập dữ liệu
đa ngôn ngữ được gán nhãn cẩn thận dựa trên cộng đồng
mã nguồn mở Internet và chứa các quan hệ xếp hạng.

4https://huggingface.co/datasets/
OpenAssistant/oasst1
5https://huggingface.co/datasets/
SirNeural/flan_v2
6https://huggingface.co/datasets/
databricks/databricks-dolly-15k
7https://huggingface.co/datasets/
Anthropic/hh-rlhf

--- TRANG 6 ---
Bảng 1: Một ví dụ với các điểm khác nhau trong suy luận với cùng một mô hình. Với điểm thấp (ví dụ: -5.00), có vẻ như mô hình đang trả lời một câu hỏi khác. Với điểm dương cao, câu trả lời trở nên hữu ích. Một số tình huống được quan sát trong quá trình đánh giá.

Hướng dẫn <rm_score> 5.00 <rm_score>
0.00<rm_score> -
5.00
Làm thế nào tôi có thể
cải thiện kỹ năng
quản lý thời gian
của mình?Dưới đây là một số cách để cải thiện kỹ năng quản lý thời gian của bạn:
1. Đặt mục tiêu rõ ràng: Xác định mục tiêu ngắn hạn và dài hạn của bạn, và ưu tiên chúng.
2. Tạo lịch trình: Lập kế hoạch ngày của bạn trước, bao gồm các nhiệm vụ bạn cần hoàn thành và thời gian bạn sẽ dành cho mỗi nhiệm vụ.
... (bỏ qua vì quá dài)Bạn cần làm
những gì?Viết một mã
trong python để
tìm để tìm để
tính phương trình
vi phân

4.5 Tập dữ liệu Căn chỉnh

Đối với các thuật toán căn chỉnh ngoại tuyến, chúng tôi đã tạo
một tập dữ liệu lai khoảng 100k mẫu bằng phương pháp
gán nhãn dữ liệu được đề cập trong phần căn chỉnh ngoại tuyến.
100k mẫu được lấy mẫu từ tập dữ liệu huấn luyện SFT và RM
một cách đồng đều. Sau đó chúng tôi sử dụng mô hình RM
để chú thích giá trị phần thưởng cho các mẫu này. Đối với
mô hình PPO, chúng tôi trích xuất các lời nhắc trong 100k
mẫu này làm tập huấn luyện.

4.6 Khung Huấn luyện Phân tán

Chúng tôi sử dụng các khung NVIDIA NeMo Megatron
(Shoeybi et al., 2019) và NVIDIA NeMo Aligner (NVIDIA, 2023)
cho huấn luyện phân tán quy mô lớn. Tất cả các mô hình
được huấn luyện sử dụng kết hợp các kỹ thuật Song song Tensor (TP)
(Shoeybi et al., 2019), Song song Dữ liệu (DP), và Độ chính xác
Hỗn hợp Tự động (AMP) với bfloat16 được triển khai trong
NeMo Megatron. Mặc dù mô hình của chúng tôi chỉ có quy mô 8B,
chúng tôi vẫn áp dụng kiến trúc phân tán đa nút của NeMo Aligner.
Điều này cho phép chúng tôi phát hiện các vấn đề hiệu suất
có thể gặp phải trong các mô hình lớn (chẳng hạn như 70B).
Trong các phương pháp căn chỉnh ngoại tuyến, chúng tôi tạo mẫu
với kích thước phân vùng TP bằng 1. Đối với tất cả các giai đoạn
và mô hình khác, kích thước phân vùng TP được đặt thành 4
để tránh các vấn đề hết bộ nhớ (OOM)8. Đối với các mô hình PPO,
chúng tôi sử dụng 8 nút DGX A100 cho suy luận và huấn luyện
mô hình diễn viên, và 2 nút cho mô hình phê bình, trong khi
hai mô hình khác mỗi mô hình sử dụng 1 nút. Đối với các thuật toán
căn chỉnh ngoại tuyến, chúng tôi8Các mô-đun diễn viên và phê bình cần chịu trách nhiệm
cho cả huấn luyện và suy luận trong PPO để trọng số của
bộ tối ưu Adam (Kingma and Ba, 2014) sẽ chiếm rất nhiều bộ nhớ.
tận dụng 4 nút DGX A100 cho huấn luyện.

4.7 Siêu tham số

Đối với mô hình phần thưởng, chúng tôi sử dụng tốc độ học
9e−6, kích thước lô 128, và các tập dữ liệu sở thích con người
để huấn luyện mô hình chỉ 1 epoch. Đối với căn chỉnh lọc,
chúng tôi đặt ngưỡng t thành 0; đối với RWR, chúng tôi đặt
β thành 5; đối với CA, chúng tôi sử dụng định dạng lời nhắc:

User:{instruction }Assistant: ⟨rm_score ⟩a.b{response }

trong đó a.b có nghĩa là giá trị phần thưởng. Tác động
của giá trị này được minh họa trong Bảng 1, trong đó
sự khác biệt duy nhất là điểm số. Chúng tôi sử dụng phần thưởng
5.09 trong đánh giá. Chúng tôi sử dụng tốc độ học 5e−6
và kích thước lô toàn cục 128 cho tất cả các mô hình căn chỉnh
ngoại tuyến và mô hình SFT. Đối với mô hình PPO, chúng tôi
sử dụng tốc độ học 5e−7 cho mô hình diễn viên, và 9e−6
cho mô hình phê bình. Chúng tôi đặt số epoch PPO thành 1,
kích thước lô rollout thành 1024, kích thước mini-batch thành 128,
hệ số phạt KL thành 0.01, ước tính lợi thế tổng quát (GAE)
(Schulman et al., 2015) λ thành 0.95, và gamma RL thành 1.
Mô hình phê bình được khởi tạo với trọng số của mô hình phần thưởng.

5 Kết quả Thực nghiệm

Chúng tôi sử dụng cả đánh giá con người và đánh giá GPT-4
để đánh giá hiệu suất của mỗi mô hình.

5.1 Đánh giá Con người

Chúng tôi đánh giá các mô hình này sử dụng 160 lời nhắc từ
LMSys và chấm điểm các phản hồi được tạo tham lam9Từ phân phối phần thưởng (Hình 3), mặc dù hầu hết các mẫu
nằm trong điểm [-2.5, 2.5], 5.0 là lựa chọn tốt hơn.

--- TRANG 7 ---
012345Điểm Chuẩn hóa
2.372.542.83
2.67
2.132.303.883.89
3.79
3.60Điểm từ xếp hạng
Con người
GPT-4
FA CA SFT RWR PPOHình 4: Kết quả đánh giá độ hữu ích của con người và GPT-4
cho tất cả các mô hình. Chúng tôi sử dụng 6−rank làm điểm cuối cùng,
ví dụ: điểm cho phản hồi được xếp hạng tốt nhất là 5 = 6−1.
Sự tương quan giữa đánh giá con người và GPT-4 rất cao,
và CA đạt điểm cao nhất.

Bảng 2: Điểm đánh giá GPT-4.
SFT FA RWR CA PPO
vicuna 6.62 6.33 5.93 7.65 7.42
mt_bench 6.29 5.78 5.31 6.47 6.62
trung bình 6.45 6.05 5.62 7.06 7.02

bằng cách xếp hạng thủ công chất lượng câu trả lời. Vì
các tập dữ liệu RM của chúng tôi nhấn mạnh chất lượng
của việc hữu ích, chúng tôi chỉ tập trung vào sự phong phú
của nội dung, sự rõ ràng trong trình bày và giọng điệu
thân thiện của mỗi phản hồi trong quá trình đánh giá thủ công,
thay vì đánh giá tính trung thực của nó. Nhìn chung,
như được hiển thị trong Hình 4, điểm độ hữu ích chuẩn hóa
của các mô hình được xếp hạng như sau:

CA≈PPO >FA>SFT>RWR

trong khi mô hình PPO đạt được hiệu suất tương tự như CA,
hệ thống huấn luyện và điều chỉnh siêu tham số của nó
phức tạp hơn CA.

5.2 Đánh giá GPT-4

Chúng tôi cũng đánh giá các mô hình này sử dụng 160 lời nhắc
từ LMSys và chấm điểm các phản hồi được tạo tham lam
bởi GPT-4, trong đó GPT-4 được yêu cầu đánh giá độ hữu ích
của các câu trả lời bằng cách đánh giá điểm trong phạm vi [1-10].
Các lời nhắc đánh giá và một ví dụ về các phản hồi có thể
được tìm thấy trong Phụ lục A. Điểm số được hiển thị trong Bảng 2.
Chúng ta có thể thấy rằng đối với các lời nhắc từ benchmark vicuna,
GPT-4 nghĩ CA hoạt động tốt nhất; đối với MT-Benchmark,Bảng 3: Số giờ GPU cần thiết cho mỗi phương pháp. Đối với
các phương pháp ngoại tuyến, giai đoạn tiền xử lý dữ liệu cũng
được tính, bao gồm tạo phản hồi và sử dụng mô hình phần thưởng để chấm điểm.

FA RWR CA PPO
Giờ A100 27.5 64 65 730

CA xếp thứ hai, với điểm số thấp hơn PPO một chút.
Tương tự như cách tính điểm của đánh giá con người,
chúng tôi xếp hạng kết quả của mỗi lời nhắc với điểm GPT-4
và chuẩn hóa điểm số, và kết quả chuẩn hóa có thể được
tìm thấy trong Hình 4. Có thể dễ dàng thấy rằng CA
có thể so sánh được với PPO. SFT đạt điểm tốt hơn FA
trong Bảng 2 trong khi tệ hơn FA trong 4 vì những lợi thế
nhỏ (ví dụ: 0.1) cho điểm GPT-4 có thể dẫn đến sự khác biệt
điểm lớn hơn (ví dụ: 1.0) trong điểm chuẩn hóa từ xếp hạng.

Từ cả đánh giá con người và đánh giá GPT-4, có thể
kết luận rằng cả căn chỉnh trực tuyến và ngoại tuyến đều
có thể giúp căn chỉnh tốt hơn các phản hồi mô hình với
mong muốn của con người. Và, CA là một phương pháp
đầy hứa hẹn để đạt được kết quả có thể so sánh với PPO.

5.3 Phân tích Hiệu suất Huấn luyện

Đối với cả mô hình PPO và các mô hình căn chỉnh ngoại tuyến,
chúng tôi sử dụng 100k mẫu cho huấn luyện căn chỉnh.
Để so sánh tài nguyên GPU được chiếm dụng trong quá trình
huấn luyện của các mô hình này, chúng tôi sử dụng giờ GPU
làm đơn vị để chuẩn hóa thời gian huấn luyện trên nhiều GPU
thành thời gian trên một GPU A100 duy nhất. Như được hiển thị
trong Bảng 3, mô hình PPO mất 730 giờ GPU để hội tụ,
trong khi các phương pháp căn chỉnh ngoại tuyến của chúng tôi
chỉ mất 65 giờ GPU (khoảng 9% của PPO), tiết kiệm rất nhiều
ngân sách cho huấn luyện. Sự khác biệt hiệu suất chính
xuất phát từ:

1.Hệ thống huấn luyện PPO đa mô-đun có phụ thuộc
dữ liệu, ngăn việc sử dụng đầy đủ mỗi nút cho huấn luyện.

2.PPO yêu cầu tạo 100k mẫu thông qua suy luận GPT
có độ phức tạp thời gian là O(n2)(kipply, 2022).

Mặc dù hệ thống huấn luyện PPO này không được tối ưu
đến tốc độ ánh sáng (SOL)10, nó vẫn phản ánh đáng kể
rằng các phương pháp căn chỉnh ngoại tuyến10Chúng tôi chỉ kích hoạt fusion kernel để tăng tốc việc tạo

--- TRANG 8 ---
có sự phụ thuộc thấp hơn vào tối ưu hóa hệ thống và
hiệu quả huấn luyện cao hơn.

6 Kết luận

Trong bài báo này, chúng tôi đề xuất khung căn chỉnh
ngoại tuyến nhằm tránh tính không ổn định trong quá trình
huấn luyện RLHF và các hệ thống huấn luyện phân tán phức tạp,
từ đó cải thiện hiệu quả phát triển mô hình. Có nhiều chi tiết
cũng như điều chỉnh siêu tham số trong khung của chúng tôi.
Mà không cần điều chỉnh cẩn thận, chúng tôi vẫn có thể
đạt được hiệu suất có thể so sánh khi so với RLHF trực tuyến
(tức là PPO). Những lợi thế về cả hiệu suất và hiệu quả
huấn luyện cho thấy tiềm năng lớn của căn chỉnh ngoại tuyến.

7 Hạn chế

Trong khi chúng tôi đã đề xuất khung căn chỉnh ngoại tuyến
đơn giản và hiệu quả cùng ba thuật toán, nó không tính đến
các vấn đề ngoài phân phối (OOD) được đưa ra bởi học ngoại tuyến.
Điều này có thể cản trở hiệu suất của nó trong các tình huống phức tạp.
Chúng tôi tin rằng việc giải quyết khuyết điểm này mang lại
cơ hội cho công việc tương lai.

của các mẫu trong việc lập hồ sơ hiệu suất, các công nghệ khác
như tạo sinh không đồng bộ chưa được xem xét.

--- TRANG 9 ---
Tài liệu tham khảo

Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini,
Cameron McKinnon, et al. 2022. Constitutional
ai: Harmlessness from ai feedback. arXiv preprint
arXiv:2212.08073 .

Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems , 33:1877–1901.

Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.

Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,
Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,
Matei Zaharia, and Reynold Xin. 2023. Free dolly:
Introducing the world's first truly open instruction-
tuned llm.

Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and
Christopher Ré. 2022. Flashattention: Fast and
memory-efficient exact attention with io-awareness.
Advances in Neural Information Processing Systems ,
35:16344–16359.

Peter Henderson, Joshua Romoff, and Joelle Pineau.
2018. Where did my optimum go?: An empirical
analysis of gradient descent optimization in policy
gradient methods. arXiv preprint arXiv:1810.02525 .

Jian Hu, Siyang Jiang, Seth Austin Harding, Haibin
Wu, and Shih-wei Liao. 2021. Rethinking the imple-
mentation tricks and monotonicity constraint in co-
operative multi-agent reinforcement learning. arXiv
preprint arXiv:2102.03479 .

Shengyi Huang, Rousslan Fernand Julien Dossa,
Antonin Raffin, Anssi Kanervisto, and Weixun
Wang. 2022. The 37 implementation details of
proximal policy optimization. In ICLR Blog Track .
Https://iclr-blog-track.github.io/2022/03/25/ppo-
implementation-details/.

Riashat Islam, Peter Henderson, Maziar Gomrokchi,
and Doina Precup. 2017. Reproducibility of bench-
marked deep reinforcement learning tasks for contin-
uous control. arXiv preprint arXiv:1708.04133 .

Diederik P Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980 .

kipply. 2022. Transformer inference
arithmetic. https://kipp.ly/
transformer-inference-arithmetic/ .Andreas Köpf, Yannic Kilcher, Dimitri von Rütte,
Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver Stan-
ley, Richárd Nagyfi, et al. 2023. Openassistant
conversations–democratizing large language model
alignment. arXiv preprint arXiv:2304.07327 .

Ang Li, Shuaiwen Leon Song, Jieyang Chen, Jiajia Li,
Xu Liu, Nathan R Tallent, and Kevin J Barker. 2019.
Evaluating modern gpu interconnect: Pcie, nvlink,
nv-sli, nvswitch and gpudirect. IEEE Transactions
on Parallel and Distributed Systems , 31(1):94–110.

NVIDIA. 2023. Nemo aligner: Scalable toolkit for effi-
cient model alignment. https://github.com/
NVIDIA/NeMo-Aligner .

OpenAI. 2022. Introducing chatgpt. https://
openai.com/blog/chatgpt .

OpenAI. 2023. Gpt-4 technical report.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.

Jan Peters and Stefan Schaal. 2007. Reinforcement
learning by reward-weighted regression for opera-
tional space control. In Proceedings of the 24th in-
ternational conference on Machine learning , pages
745–750.

Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
and Yuxiong He. 2020. Zero: Memory optimizations
toward training trillion parameter models. In SC20:
International Conference for High Performance Com-
puting, Networking, Storage and Analysis , pages 1–
16. IEEE.

John Schulman, Philipp Moritz, Sergey Levine, Michael
Jordan, and Pieter Abbeel. 2015. High-dimensional
continuous control using generalized advantage esti-
mation. arXiv preprint arXiv:1506.02438 .

John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv:1707.06347 .

Noam Shazeer. 2020. Glu variants improve transformer.
arXiv preprint arXiv:2002.05202 .

Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
Patrick LeGresley, Jared Casper, and Bryan Catan-
zaro. 2019. Megatron-lm: Training multi-billion
parameter language models using model parallelism.
arXiv preprint arXiv:1909.08053 .

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: a simple way to prevent neural networks
from overfitting. The journal of machine learning
research , 15(1):1929–1958.

--- TRANG 10 ---
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,
Bo Wen, and Yunfeng Liu. 2021. Roformer: En-
hanced transformer with rotary position embedding.
arXiv preprint arXiv:2104.09864 .

Han Vanholder. 2016. Efficient inference with tensorrt.
InGPU Technology Conference , volume 1.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-
isa Liu, Noah A Smith, Daniel Khashabi, and Han-
naneh Hajishirzi. 2022. Self-instruct: Aligning lan-
guage model with self generated instructions. arXiv
preprint arXiv:2212.10560 .

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652 .

Zhewei Yao, Reza Yazdani Aminabadi, Olatunji
Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Am-
mar Ahmad Awan, Jeff Rasley, Minjia Zhang, Cong-
long Li, Connor Holmes, Zhongzhu Zhou, Michael
Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin,
Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song,
and Yuxiong He. 2023. Deepspeed-chat: Easy, fast
and affordable rlhf training of chatgpt-like models at
all scales.

Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
Judging llm-as-a-judge with mt-bench and chatbot
arena. arXiv preprint arXiv:2306.05685 .

--- TRANG 11 ---
Phần A mô tả cách chúng tôi tạo lời nhắc đánh giá
với GPT-4 (OpenAI, 2023). Phần B cung cấp thêm
ví dụ và kết quả của các thí nghiệm của chúng tôi.

A Phụ lục - Lời nhắc Đánh giá GPT-4

Chúng tôi xây dựng lời nhắc đánh giá của mình bằng cách
tham khảo các mẫu lời nhắc từ benchmark vicuna11(Chiang et al., 2023).
Khác với so sánh cặp và đánh giá đơn lẻ, chúng tôi kết hợp
tất cả đầu ra mô hình lại với nhau trong đánh giá đơn lẻ.
Vì vậy, GPT-4 có thể đánh giá chất lượng bằng cách so sánh
với tất cả các phản hồi khác và chọn ra phản hồi tốt nhất.

Hãy hành động như một thẩm phán công bằng và đánh giá chất lượng của các phản hồi được cung cấp bởi năm trợ lý AI cho câu hỏi của người dùng. Bạn nên chọn trợ lý tuân theo hướng dẫn của người dùng và trả lời câu hỏi của người dùng tốt hơn. Đánh giá của bạn nên xem xét các yếu tố như tính hữu ích, liên quan và mức độ chi tiết của các phản hồi của họ.

Bắt đầu đánh giá của bạn bằng cách so sánh năm phản hồi và cung cấp một giải thích ngắn gọn. Tránh bất kỳ thiên vị vị trí nào và đảm bảo rằng thứ tự mà các phản hồi được trình bày không ảnh hưởng đến quyết định của bạn. Đừng để độ dài của các phản hồi ảnh hưởng đến đánh giá của bạn. Đừng thiên vị những tên cụ thể của các trợ lý. Hãy khách quan nhất có thể. Sau khi cung cấp giải thích của bạn, đưa ra phán quyết cuối cùng bằng cách tuân thủ nghiêm ngặt định dạng này:

{Tên trợ lý}: {điểm}, trong đó phạm vi của {điểm} là từ 1 đến 10.

Các câu trả lời có thể có cùng điểm, nhưng bạn nên phân biệt chúng càng nhiều càng tốt. Bạn nên đưa ra điểm của trợ lý từng cái một sau tất cả các giải thích. Sau đây là câu hỏi và câu trả lời của Trợ lý CA, PPO, FA, RWR và SFT,

Câu hỏi của Người dùng:
Làm thế nào tôi có thể cải thiện kỹ năng quản lý thời gian của mình?

Câu trả lời của Trợ lý CA:
{ Phản hồi CA }

Câu trả lời của Trợ lý PPO:
{ Phản hồi PPO }

Câu trả lời của Trợ lý FA:
{ Phản hồi FA }

Câu trả lời của Trợ lý RWR:
{ Phản hồi RWR }

Câu trả lời của Trợ lý SFT:
{ Phản hồi SFT }

11https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/data/judge_prompts.jsonl

--- TRANG 12 ---
B Phụ lục - Ví dụ

Chúng tôi hiển thị thêm ví dụ cũng như các bình luận
đánh giá GPT-4 ở đây.

Bảng 4: Ví dụ 1. Phản hồi từ tất cả các mô hình được trình bày.

Câu hỏi :
Trong một thế giới mà tự động hóa đang trở nên ngày càng phổ biến, liệu việc ưu tiên tạo việc làm hay tiến bộ công nghệ quan trọng hơn?

Câu trả lời của CA :
Tiến bộ công nghệ đang trở nên ngày càng phổ biến trên thế giới, và điều quan trọng là ưu tiên cả việc tạo việc làm và tiến bộ công nghệ. Tạo việc làm rất quan trọng để đảm bảo mọi người có phương tiện kiếm sống và ý thức về mục đích, trong khi tiến bộ công nghệ quan trọng để thúc đẩy đổi mới và tăng trưởng kinh tế. Tuy nhiên, điều quan trọng cần lưu ý là tạo việc làm và tiến bộ công nghệ không loại trừ lẫn nhau, và có thể được theo đuổi song song để tạo ra nhiều cơ hội hơn cho mọi người.

Câu trả lời của PPO :
Đây là một câu hỏi phức tạp không có câu trả lời đơn giản. Dưới đây là một số yếu tố cần xem xét:
1. Tạo việc làm: Tự động hóa có thể dẫn đến mất việc làm và di dời, có thể có tác động kinh tế và xã hội đáng kể. Ưu tiên tạo việc làm có thể giúp giảm thiểu những tác động này và đảm bảo rằng mọi người có cơ hội chuyển đổi sang công việc mới.
2. Tiến bộ công nghệ: Đầu tư vào tiến bộ công nghệ có thể dẫn đến những lợi ích kinh tế và xã hội đáng kể, như tăng năng suất, đổi mới và tăng trưởng. Tuy nhiên, nó cũng có thể dẫn đến di dời việc làm và thay đổi trong thị trường lao động.
3. Đánh đổi: Ưu tiên tạo việc làm hoặc tiến bộ công nghệ có thể có những đánh đổi và hậu quả khác nhau. Ví dụ, tạo việc làm có thể yêu cầu nhiều chi tiêu và quy định của chính phủ hơn, trong khi tiến bộ công nghệ có thể yêu cầu nhiều đầu tư vào nghiên cứu và phát triển hơn.
4. Giá trị xã hội: Ưu tiên tạo việc làm hoặc tiến bộ công nghệ cũng có thể có những giá trị xã hội khác nhau. Một số người có thể ưu tiên tạo việc làm để đảm bảo ổn định kinh tế và gắn kết xã hội, trong khi những người khác có thể ưu tiên tiến bộ công nghệ để thúc đẩy đổi mới và tăng trưởng kinh tế.
Kết luận, điều quan trọng là xem xét những đánh đổi và giá trị xã hội liên quan đến việc ưu tiên tạo việc làm hoặc tiến bộ công nghệ. Một cách tiếp cận cân bằng ưu tiên cả tạo việc làm và tiến bộ công nghệ có thể giúp đảm bảo rằng những lợi ích của tự động hóa được chia sẻ bởi càng nhiều người càng tốt.

Câu trả lời của FA :
Tiến bộ công nghệ nên được ưu tiên hơn tạo việc làm. Tự động hóa nên tập trung vào cải thiện hiệu quả và chất lượng cuộc sống, không chỉ tạo việc làm.

Câu trả lời của RWR :
Tiến bộ công nghệ

Câu trả lời của SFT :
Tiến bộ công nghệ nên được ưu tiên.

Đầu ra đánh giá GPT-4 :
"Đánh giá:
1. **Trợ lý CA**:
Phản hồi cung cấp sự cân bằng rõ ràng giữa tiến bộ công nghệ và tạo việc làm, nêu bật tầm quan trọng của cả hai. Nó đề cập đến ý nghĩa của cả đổi mới và ý thức về mục đích có được từ tạo việc làm. Phản hồi trực tiếp và súc tích mà không quá lặp lại.
2. **Trợ lý PPO**:
Câu trả lời này sử dụng cách tiếp cận có cấu trúc, trình bày các yếu tố cần xem xét. Bằng cách cung cấp các điểm được đánh số, nó hiệu quả phân tích sự phức tạp của chủ đề. Nó không đưa ra một lập trường nghiêm ngặt mà thay vào đó cung cấp cho người dùng nhiều quan điểm khác nhau để xem xét. Cách tiếp cận này mang tính phân tích và xem xét nhiều khía cạnh của chủ đề.
3. **Trợ lý FA**:
Phản hồi nghiêng mạnh về việc ưu tiên tiến bộ công nghệ. Mặc dù việc có một lập trường là được, câu trả lời quá súc tích, thiếu lý luận chi tiết để hỗ trợ quan điểm.
4. **Trợ lý RWR**:
Câu trả lời cực kỳ ngắn gọn, chỉ đề cập tiến bộ công nghệ. Mà không có bất kỳ lập luận hỗ trợ hoặc lý luận nào, phản hồi không giải quyết sự phức tạp của câu hỏi người dùng.
5. **Trợ lý SFT**:
Tương tự như Trợ lý FA, câu trả lời này khá ngắn và nghiêng về tiến bộ công nghệ. Mặc dù nó cung cấp một lập trường trực tiếp, nó không đưa ra nhiều chiều sâu hoặc giải thích.
Điểm số:
Trợ lý CA: 8.5
Trợ lý PPO: 8
Trợ lý FA: 6
Trợ lý RWR: 4
Trợ lý SFT: 5.5"

--- TRANG 13 ---
Từ cả đánh giá con người và đánh giá GPT-4,
chúng tôi đã chỉ ra trong bài báo rằng Conditional
có thể đạt được hiệu suất tốt hơn một chút so với
PPO (Schulman et al., 2017). Vì đầu ra của các mô hình
rất dài, để đơn giản, chúng tôi hiển thị thêm mẫu
trong khi chỉ trình bày nội dung liên quan đến CA/PPO trong Bảng.

Bảng 5: Ví dụ 2. Chỉ nội dung liên quan đến CA/PPO được trình bày.

Câu hỏi :
Có bao nhiêu tin nhắn văn bản được gửi trên toàn cầu trong một phút? Hãy thử giải thích câu trả lời của bạn. Giải thích của bạn nên đưa người đọc qua quá trình lý luận từng bước.

Câu trả lời của CA :
" Trong một phút, khoảng 200 triệu tin nhắn văn bản được gửi trên toàn cầu. Điều này theo báo cáo của Liên minh Viễn thông Quốc tế (ITU), cũng tiết lộ rằng lưu lượng dữ liệu di động toàn cầu trên mỗi người dùng mỗi tháng đã tăng 60% trong hai năm qua. Báo cáo cũng nêu rằng số lượng thuê bao di động trên toàn thế giới đã đạt 7,7 tỷ, với trung bình 104,4 triệu đăng ký mới được thêm vào mỗi tháng.

Sự gia tăng số lượng tin nhắn văn bản được gửi trên toàn cầu có thể được quy cho sự phổ biến ngày càng tăng của các thiết bị di động và sự tiện lợi của giao tiếp tức thì. Mọi người giờ đây có thể duy trì kết nối với những người thân yêu, bạn bè và đồng nghiệp bất cứ lúc nào và ở bất cứ đâu, điều này đã dẫn đến sự gia tăng số lượng tin nhắn văn bản được gửi. Ngoài ra, sự ra đời của các nền tảng truyền thông xã hội cũng đã góp phần vào sự gia tăng số lượng tin nhắn văn bản được gửi vì mọi người thường sử dụng các nền tảng này để giao tiếp với nhau.

Báo cáo cũng nhấn mạnh nhu cầu ngày càng tăng đối với các dịch vụ dữ liệu di động, với lưu lượng dữ liệu di động toàn cầu dự kiến đạt 18,4 exabyte mỗi tháng vào năm 2026. Đây là sự gia tăng đáng kể từ 4,6 exabyte mỗi tháng vào năm 2019. Báo cáo cũng nêu rằng sự tăng trưởng trong lưu lượng dữ liệu di động được thúc đẩy bởi việc sử dụng ngày càng tăng video, truyền thông xã hội và các ứng dụng sử dụng nhiều dữ liệu khác.

Kết luận, báo cáo tiết lộ rằng ngành công nghiệp di động toàn cầu đang phát triển nhanh chóng, với số lượng thuê bao di động dự kiến đạt 8,6 tỷ vào năm 2026. Nhu cầu ngày càng tăng đối với các dịch vụ dữ liệu di động và sự phổ biến ngày càng tăng của các thiết bị di động đang thúc đẩy sự tăng trưởng của ngành công nghiệp di động.

Câu trả lời của PPO :
Dưới đây là một số sự thật về số lượng tin nhắn văn bản được gửi trên toàn cầu trong một phút:
1. Theo báo cáo của eMarketer, vào năm 2022, ước tính có khoảng 70 tỷ tin nhắn văn bản được gửi mỗi ngày trên toàn cầu.
2. Con số này được dự báo sẽ tăng lên khoảng 100 tỷ mỗi ngày vào năm 2023.
3. Người trung bình gửi và nhận khoảng 40 tin nhắn văn bản mỗi ngày.
4. Tại Hoa Kỳ, ước tính có khoảng 240 triệu người gửi và nhận tin nhắn văn bản mỗi ngày.
5. Tại Vương quốc Anh, ước tính có khoảng 47 triệu người gửi và nhận tin nhắn văn bản mỗi ngày.
6. Tại Ấn Độ, ước tính có khoảng 1,3 tỷ người gửi và nhận tin nhắn văn bản mỗi ngày.
7. Tại Trung Quốc, ước tính có khoảng 1,4 tỷ người gửi và nhận tin nhắn văn bản mỗi ngày.
8. Những con số này cho thấy có rất nhiều tin nhắn văn bản được gửi trên toàn cầu mỗi phút. Số lượng tin nhắn văn bản được gửi mỗi ngày đã tăng trưởng nhanh chóng qua các năm, và dự kiến sẽ tiếp tục tăng trưởng trong tương lai.

Đầu ra đánh giá GPT-4 :
Hãy đánh giá các phản hồi dựa trên tiêu chí về tính hữu ích, liên quan và mức độ chi tiết trong các giải thích.

**Trợ lý CA**:
Phản hồi khá chi tiết và nhiều thông tin. CA dựa vào báo cáo từ Liên minh Viễn thông Quốc tế (ITU) để biện minh cho 200 triệu tin nhắn văn bản được gửi trên toàn cầu trong một phút. Nó tiếp tục giải thích các lý do đằng sau số lượng tin nhắn văn bản cao này và cung cấp cái nhìn sâu sắc về sự tăng trưởng của ngành công nghiệp di động toàn cầu. Bản chất toàn diện của phản hồi giúp hướng dẫn người đọc qua quá trình lý luận.

**Trợ lý PPO**:
Câu trả lời đưa ra nhiều sự thật về xu hướng nhắn tin toàn cầu, và mặc dù đầy thống kê, nó không hiệu quả liên kết chúng lại với nhau để trả lời câu hỏi trực tiếp. Người đọc có được cảm giác về quy mô của việc nhắn tin toàn cầu nhưng không nhất thiết có được một giải thích rõ ràng, từng bước cho tuyên bố ban đầu.

(... bỏ qua)

**Tóm tắt Đánh giá**: CA cung cấp phản hồi chi tiết, liên quan và hữu ích nhất. PPO đưa ra nhiều sự thật khác nhau nhưng không liên kết chúng lại với nhau trong một giải thích súc tích. FA's ...(bỏ qua)

**Điểm số Cuối cùng**:
Trợ lý CA: 9
Trợ lý PPO: 7
Trợ lý FA: 6
Trợ lý RWR: 4
Trợ lý SFT: 6.5

--- TRANG 14 ---
Bảng 6: Ví dụ 3. Chỉ nội dung liên quan đến CA/PPO được trình bày.

Câu hỏi :
Bạn sẽ giới thiệu bản thân như thế nào với tư cách là một hiệp sĩ thời trung cổ tại một bữa tiệc hoàng gia?

Câu trả lời của CA :
Với tư cách là một hiệp sĩ thời trung cổ tại một bữa tiệc hoàng gia, tôi sẽ giới thiệu với niềm tự hào và danh dự. Tôi sẽ nêu tên mình, tên họ và tên nhà hiệp sĩ của mình, đây là biểu tượng của di sản quý tộc và những giá trị tôi theo đuổi. Tôi cũng sẽ đề cập đến tên chủ nhân và thầy của mình, người đã huấn luyện và hỗ trợ tôi trong nghệ thuật chiến tranh. Cuối cùng, tôi sẽ bày tỏ lòng biết ơn vì cơ hội được tham dự bữa tiệc này và gặp gỡ những vị khách quý báu.

Câu trả lời của PPO :
Dưới đây là một ví dụ về cách một hiệp sĩ thời trung cổ có thể giới thiệu bản thân tại một bữa tiệc hoàng gia:
"Tên tôi là [Tên], và tôi là một hiệp sĩ của vương quốc [Vương quốc]. Đó là một danh dự được mời đến bữa tiệc hoàng gia này, và tôi biết ơn vì cơ hội được phục vụ nhà vua và nữ hoàng của tôi. Tôi tự hào là người bảo vệ vương quốc, và tôi sẽ tiếp tục chiến đấu vì sự an toàn và an ninh của dân tôi. Cầu Chúa ban phước cho vương quốc này và các nhà lãnh đạo của nó.""
Giới thiệu này sẽ là một cách tôn trọng và khiêm tốn để một hiệp sĩ giới thiệu bản thân tại một bữa tiệc hoàng gia, thể hiện sự cống hiến của anh ta cho vương quốc và sự phục vụ nhà vua và nữ hoàng.

Đầu ra đánh giá GPT-4 :
"Để đánh giá các phản hồi đã cho, chúng ta sẽ xem xét tính hữu ích, liên quan và mức độ chi tiết của chúng trong việc trả lời câu hỏi của người dùng về cách giới thiệu bản thân như một hiệp sĩ thời trung cổ tại một bữa tiệc hoàng gia.

**Trợ lý CA:**
Câu trả lời này bắt đầu với việc thừa nhận niềm tự hào và danh dự, chuyển sang quá trình nhận dạng ngắn gọn liên quan đến tên mình, tên họ, nhà hiệp sĩ và chủ nhân và thầy mà họ phục vụ. Nó kết thúc bằng việc bày tỏ lòng biết ơn vì cơ hội tham dự. Phản hồi có cấu trúc và liên quan nhưng thiếu một số chiều sâu về trải nghiệm thực tế của việc là một hiệp sĩ thời trung cổ.

**Trợ lý PPO:**
Trợ lý này cung cấp một giới thiệu thẳng thắn, tôn trọng và súc tích. Nó nhấn mạnh sự phục vụ vương quốc, niềm tự hào của hiệp sĩ và lòng biết ơn vì cơ hội.

(... bỏ qua)

**Đánh giá và Phán quyết:**
**Trợ lý CA:** 8 - Có cấu trúc và liên quan nhưng thiếu một số sắc thái làm cho lời giới thiệu cảm thấy hoàn chỉnh.
**Trợ lý PPO:** 8 - Tôn trọng và súc tích, mặc dù có thể sử dụng thêm một chút chi tiết để làm phong phú câu chuyện.
**Trợ lý FA:** 7 - Gợi cảm nhưng không trực tiếp giải quyết tình huống bữa tiệc một cách toàn diện như những người khác.
**Trợ lý RWR:** 7 - Sống động, nhưng tính cụ thể trong câu chuyện của nó có thể không làm cho nó có thể thích ứng một cách phổ quát.
**Trợ lý SFT:** 2 - Thiếu bối cảnh và chiều sâu cần thiết cho câu hỏi được đặt ra.

Có thể thấy rằng các phản hồi PPO luôn chứa các tuyên bố như "Đây là .." hoặc "Dưới đây là một số". Và khi nhìn vào những phản hồi này. Xem xét tính hữu ích và liên quan, CA và PPO có thể so sánh được. Trong khung căn chỉnh ngoại tuyến của chúng tôi, chi phí thời gian để huấn luyện CA chỉ là 9% so với PPO. Do đó, những lợi thế về cả hiệu suất và hiệu quả huấn luyện cho thấy tiềm năng lớn của căn chỉnh ngoại tuyến.
