# 2310.08491.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2310.08491.pdf
# File size: 35824528 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
PROMETHEUS : INDUCING FINE-GRAINED
EVALUATION CAPABILITY IN LANGUAGE MODELS
Seungone Kim1,2∗†Jamin Shin2,3∗†Yejin Cho1∗†Joel Jang4Shayne Longpre5
Hwaran Lee2,3Sangdoo Yun2,3Seongjin Shin3Sungdong Kim1,2,3
James Thorne1Minjoon Seo1†
1KAIST AI2NA VER AI Lab3NA VER Cloud4University of Washington5MIT
{seungone, yejin cho, minjoon }@kaist.ac.kr jamin.shin@outlook.com
ABSTRACT
Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-
4) as an evaluator for long-form responses has become the de facto standard.
However, for practitioners with large-scale evaluation tasks and custom criteria
in consideration (e.g., child-readability), using proprietary LLMs as an evalua-
tor is unreliable due to the closed-source nature, uncontrolled versioning, and
prohibitive costs. In this work, we propose P ROMETHEUS , a fully open-source
LLM that is on par with GPT-4’s evaluation capabilities when the appropri-
ate reference materials (reference answer, score rubric) are accompanied. We
first construct the F EEDBACK COLLECTION , a new dataset that consists of 1K
fine-grained score rubrics, 20K instructions, and 100K responses and language
feedback generated by GPT-4. Using the F EEDBACK COLLECTION , we train
PROMETHEUS , a 13B evaluator LLM that can assess any given long-form text
based on customized score rubric provided by the user. Experimental results show
that P ROMETHEUS scores a Pearson correlation of 0.897 with human evaluators
when evaluating with 45 customized score rubrics, which is on par with GPT-4
(0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring cor-
relation with GPT-4 with 1222 customized score rubrics across four benchmarks
(MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends,
bolstering P ROMETHEUS ’s capability as an evaluator LLM. Lastly, P ROMETHEUS
achieves the highest accuracy on two human preference benchmarks (HHH Align-
ment & MT Bench Human Judgment) compared to open-sourced reward models
explicitly trained on human preference datasets, highlighting its potential as an
universal reward model. We open-source our code, dataset, and model1.
1 I NTRODUCTION
Evaluating the quality of machine-generated text has been a long-standing challenge in Natural
Language Processing (NLP) and remains especially essential in the era of Large Language Models
(LLMs) to understand their properties and behaviors (Liang et al., 2022; Chang et al., 2023; Zhong
et al., 2023; Chia et al., 2023; Holtzman et al., 2023). Human evaluation has consistently been the
predominant method, for its inherent reliability and capacity to assess nuanced and subjective di-
mensions in texts. In many situations, humans can naturally discern the most important factors of
assessment, such as brevity, creativity, tone, and cultural sensitivities. On the other hand, conven-
tional automated evaluation metrics (e.g., BLEU, ROUGE) cannot capture the depth and granularity
of human evaluation (Papineni et al., 2002; Lin, 2004b; Zhang et al., 2019; Krishna et al., 2021).
Applying LLMs (e.g. GPT-4) as an evaluator has received substantial attention due to its potential
parity with human evaluation (Chiang & yi Lee, 2023; Dubois et al., 2023; Li et al., 2023; Liu et al.,
∗denotes equal contribution. Work was done while Seungone was interning at NA VER AI Lab.
†Corresponding authors
1https://kaistai.github.io/prometheus/
1arXiv:2310.08491v2  [cs.CL]  9 Mar 2024

--- PAGE 2 ---
LLM UserLLM De v eloperFine-gr ained Ev aluation
wit h User -Defined Scor e Rubrics
Scor e of a
specific domain or taskScor e based on
Helpfulness/HarmlessnessScor e based on t he
Cust omiz ed Scor e Crit eriaOur Pr oposed Appr oachVicuna Bench,
MT Bench,
AlpacaFarm,
...Vicuna Bench,
MT Bench,
AlpacaFarm,
...+MMLU ,
Big Bench,
MA TH,
HumanEv al,
...Con v entional 
LLM Ev aluationCoarse-Gr ained
Ev aluation
GPT -4
Ev aluationA ccur acy ,
EM,  R ougePr omet heus
Ev aluation
Only
diagnoses about a
specific
domain or taskOnly
diagnoses about t h e
gener al pr ef er ence  
of t he publicClose-sour ce N atur e

Uncontr olled  
V ersionin g

Pr ohibitiv e Cost sF ully Open-sour c e

R epr oducibl e
Ev aluatio n

I ne x pensiv e Cost s
GPT -4
Ev aluation
W hich LLM is t he
most humor ous one 
out t her e ?I s t he LLM I’ m 
de v eloping inspiring 
w hile being culturall y
sensitiv e ?  <I nstruction Set >Scor e Rubric :  D oes t he 
r esponse demonstrat e 
cultural sensitivit y
 Scor e 1:  ~~~
      .. 
 Scor e 5:  ~~~

R ef er ence Answ er :
...Figure 1: Compared to conventional, coarse-grained LLM evaluation, we propose a fine-grained
approach that takes user-defined score rubrics as input.
2023; Peng et al., 2023; Zheng et al., 2023; Ye et al., 2023b; Min et al., 2023). Initial investigations
and observations indicate that, when aptly prompted, LLMs can emulate the fineness of human
evaluations. However, while the merits of using proprietary LLMs as an evaluation tool are evident,
there exist some critical disadvantages:
1.Closed-source Nature : The proprietary nature of LLMs brings transparency concerns as
internal workings are not disclosed to the broader academic community. Such a lack of
transparency hinders collective academic efforts to refine or enhance its evaluation capa-
bilities. Furthermore, this places fair evaluation, a core tenet in academia, under control of
for-profit entity and raises concerns about neutrality and autonomy.
2.Uncontrolled Versioning : Proprietary models undergo version updates that are often be-
yond the users’ purview or control (Pozzobon et al., 2023). This introduces a reproducibil-
ity challenge. As reproducibility is a cornerstone of scientific inquiry, any inconsistency
stemming from version changes can undermine the robustness of research findings that
depend on specific versions of the model, especially in the context of evaluation.
3.Prohibitive Costs : Financial constraints associated with LLM APIs are not trivial. For
example, evaluating four LLMs variants across four sizes (ranging from 7B to 65B) using
GPT-4 on 1000 evaluation instances can cost over $2000. Such scaling costs can be pro-
hibitive, especially for academic institutions or researchers operating on limited budgets.
2

--- PAGE 3 ---
Despite these limitations, proprietary LLMs such as GPT-4 are able to evaluate scores based on
customized score rubrics. Specifically, current resources are confined to generic , single-dimensional
evaluation metrics that are either too domain/task-specific (e.g. EM, Rouge) or coarse-grained (e.g.
helpfulness/harmlessness (Dubois et al., 2023; Chiang et al., 2023; Liu et al., 2023) as shown in left-
side of Figure 1. For instance, AlpacaFarm’s (Dubois et al., 2023) prompt gives a single definition
of preference, asking the model to choose the model response that is generally preferred. However,
response preferences are subject to variation based on specific applications and values. In real-world
scenarios, users may be interested in customized rubric such as “Which LLM generates responses
that are playful and humorous” or “Which LLM answers with particularly care for cultural sensitiv-
ities?” Yet, in our initial experiments, we observe that even the largest open-source LLM (70B) is
insufficient to evaluate based on a customized score rubric compared to proprietary LLMs.
To this end, we propose P ROMETHEUS , a 13B LM that aims to induce fine-grained evaluation ca-
pability of GPT-4, while being open-source, reproducible, and inexpensive. We first create the
FEEDBACK COLLECTION , a new dataset that is crafted to encapsulate diverse and fine-grained user
assessment score rubric that represent realistic user demands (example shown in Figure 2). We de-
sign the F EEDBACK COLLECTION with the aforementioned consideration in mind, encompassing
thousands of unique preference criteria encoded by a user-injected score rubric. Unlike prior feed-
back datasets (Ye et al., 2023a; Wang et al., 2023a), it uses custom , not generic preference score
rubric, to train models to flexibly generalize to practical and diverse evaluation preferences. Also, to
best of our knowledge, we are first to explore the importance of including various reference materials
– particularly the ‘Reference Answers’ – to effectively induce fine-grained evaluation capability.
We use the F EEDBACK COLLECTION to fine-tune Llama-2-Chat-13B in creating P ROMETHEUS .
On 45 customized score rubrics sampled across three test sets (MT Bench, Vicuna Bench, Feed-
back Bench), P ROMETHEUS obtains a Pearson correlation of 0.897 with human evaluators, which
is similar with GPT-4 (0.882), and has a significant gap with GPT-3.5-Turbo (0.392). Unex-
pectely, when asking human evaluators to choose a feedback with better quality in a pairwise set-
ting, P ROMETHEUS was preferred over GPT-4 in 58.67% of the time, while greatly outperformed
GPT-3.5-Turbo with a 79.57% win rate. Also, when measuring the Pearson correlation with GPT-4
evaluation across 1222 customized score rubrics across 4 test sets (MT Bench, Vicuna Bench, Feed-
back Bench, Flask Eval), P ROMETHEUS showed higher correlation compared to GPT-3.5-Turbo and
Llama-2-Chat 70B. Lastly, when testing on 2 unseen human preference datasets (MT Bench Human
Judgments, HHH Alignment), P ROMETHEUS outperforms two state-of-the-art reward models and
GPT-3.5-Turbo, highlighting its potential as an universal reward model.
Our contributions are summarized as follows:
• We introduce the F EEDBACK COLLECTION dataset specifically designed to train an eval-
uator LM. Compared to previous feedback datasets, it includes customized scoring rubrics
and reference answers in addition to the instructions, responses, and feedback.
• We train P ROMETHEUS , the first open-source LLM specialized for fine-grained evalua-
tion that can generalize to diverse, real-world scoring rubrics beyond a single-dimensional
preference such as helpfulness and harmlessness.
• We conduct extensive experiments showing that by appending reference materials (refer-
ence answers, fine-grained score rubrics) and fine-tuning on feedback, we can induce eval-
uation capability into language models. P ROMETHEUS shows high correlation with human
evaluation, GPT-4 evaluation in absolute scoring settings, and also shows high accuracy in
ranking scoring settings.
2 R ELATED WORK
Reference-based text evaluation Previously, model-free scores that evaluate machine-generated
text based on a golden candidate reference such as BLEU (Papineni et al., 2002) and ROUGE (Lin,
2004a) scores were used as the dominant approach. However, Krishna et al. (2021) reported limi-
tations in reference-based metrics, such as ROUGE, observing that they are not reliable for evalua-
tion. In recent years, model-based approaches have been widely adopted such as BERTScore (Zhang
et al., 2019), BLEURT (Sellam et al., 2020), and BARTScore (Yuan et al., 2021) which are able to
capture semantic information rather than only evaluating on lexical components.
3

--- PAGE 4 ---
Pr omet heus
Does t he r esponse consider r egulations
and compliance r equir ement s?


[Scor e 1] The r esponse disr egar d 
r egulations and compliance r equir ement s.

[Scor e 2] The r esponse sho ws minimal 
awar eness of r egulat or y compliance, 
making se v eral err ors.

[Scor e 3] The r esponse r ecogniz es cer tain 
r egulat or y r equir ement s but lacks dept h.

[Scor e 4] The r esponse most ly considers
r egulat or y compliance, 
wit h only minor o v ersight s.

[Scor e 5] The r esponse per f ect ly 
account s f or and int egrat es 
r egulat or y compliance r equir ement s.

Tit le: Online Banking Ser vice Fin T ech
Star tup Business Strat egy

Ex ecutiv e Summar y:
Our star tup, “DigitalBank” , aims t o
r e v olutioniz e t he traditional banking
industr y b y pr o viding an inno v ativ e ...Cust omiz ed Scor e Rubric
Establishing a Fin T ech star tup t hat off ers
online banking ser vices r equir es a
compr ehensiv e strat egy t o tackle 
v arious challenges inher ent t o t he industr y 
and capitaliz e on mark et oppor tunities.
..
Mark et R esear ch: ..
R egulat or y and Compliance: ..
Pr oduct De v elopment: ..
Ser vice Diff er entiation: ..
Go-t o-Mark et Strat egy: ..
Gr o wt h and Expansion: ..
Sustainability and T rust: ...

These it ems ar e impor tant aspect s t o
consider when star ting a Fint ech star tup
t hat off ers online banking ser vices.R ef er ence Answ erR esponse t o Ev aluat e
De v elop me a business strat egy f or
cr eating a Fin T ech star tup 
t hat off ers online banking ser vices.
Instruction
The r esponse pr o vides 
a compr ehensiv e o v er view .
...
Ov erall, it includes a detaile d
plan of ho w t o star t a Fin T ech
star tup b y pr esenting a n
imaginar y star tup “DigitalBank” .
...
H o w e v er , it lacks a deeper div e
int o r egulat or y compliance.
...F eedback
Scor e
3 out of 5Figure 2: The individual components of the F EEDBACK COLLECTION . By adding the appropriate
reference materials (Score Rubric and Reference Answer) and training on GPT-4’s feedback, we
show that we could obtain a strong open-source evaluator LM.
LLM-based text evaluation Recent work has used GPT-4 or a fine-tuned critique LLM as an
evaluator along a single dimension of “preference” (Chiang & yi Lee, 2023; Li et al., 2023; Dubois
et al., 2023; Zheng et al., 2023; Liu et al., 2023). For instance, AlpacaFarm (Dubois et al., 2023) asks
the model to select “which response is better based on your judgment and based on your own pref-
erence” Another example is recent work that showed ChatGPT can outperform crowd-workers for
text-annotation tasks (Gilardi et al., 2023; Chiang & yi Lee, 2023). Wang et al. (2023b) introduced
PandaLM, a fine-tuned LLM to evaluate the generated text and explain its reliability on various
preference datasets. Similarly, Ye et al. (2023a) and Wang et al. (2023a) create critique LLMs.
However, the correct preference is often subjective and depends on applications, cultures, and ob-
jectives, where degrees of brevity, formality, honesty, creativity, and political tone, among many
other potentially desirable traits that may vary (Chiang & yi Lee, 2023). While GPT-4 is unreliable
due to its close-source nature, uncontrolled versioning, and prohibitive costs, it was the only option
explored for fine-grained and customized evaluation based on the score rubric (Ye et al., 2023b). On
the contrary, we train, to best of our knowledge, the first evaluator sensitive to thousands of unique
preference criteria, and show it significantly outperforms uni-dimensional preference evaluators in
a number of realistic settings. Most importantly, compared to previous work, we strongly argue
the importance of appending reference materials (score rubric and reference answer) in addition to
fine-tuning on the feedback in order to effectively induce fine-grained evaluation capability.
3 T HEFEEDBACK COLLECTION DATASET
While previous work has demonstrated that fine-tuning on feedback is effective for improving LMs
to function as a critique (Ye et al., 2023a; Wang et al., 2023a), the datasets used in previous work
are not directly applicable for improving LMs to function as a fine-grained evaluator . We thus
introduce the F EEDBACK COLLECTION , a new dataset for the sole purpose of fine-tuning an open-
sourced evaluator LLM. Our 4 main considerations during dataset construction are: (1) including as
many reference materials (i.e. reference answer, and scoring rubric) as possible, (2) maintaining a
uniform length among the reference answers for each score (1 to 5) to prevent undesired length bias,
(3) maintaining a uniform score distribution to prevent undesired decision bias, and (4) limiting the
scope of our instructions and responses to realistic situations where a user is interacting with a LLM.
4

--- PAGE 5 ---
Table 1: Information about our training dataset F EEDBACK COLLECTION . Note that there are 20
instructions accompanied for each score rubric, leading to a total number of 20K. Also, there is a
score 1-5 response and feedback for each instruction, leading to a total number of 100K.
Evaluation Mode Data # Score Rubrics # Instructions & Reference Answer # Responses & Feedback
Absolute Evaluation FEEDBACK COLLECTION1K (Fine-grained &
Customized)Total 20K
(20 for each score rubric)Total 100K(5 for each instruction;
20K for each score within 1-5)
Taking these into consideration, we construct each instance within the F EEDBACK COLLECTION
to encompass four components for the input (instruction, response to evaluate, customized score
rubric, reference answer) and two components in the output (feedback, score). An example of an
instance is shown in Figure 2 and the number of each component is shown in Table 1.
The four components for the input are as follows:
1.Instruction : An instruction that a user would prompt to an arbitrary LLM.
2.Response to Evaluate : A response to the instruction that the evaluator LM has to evaluate.
3.Customized Score Rubric : A specification of novel criteria decided by the user. The eval-
uator should focus on this aspect during evaluation. The rubric consists of (1) a description
of the criteria and (2) a description of each scoring decision (1 to 5).
4.Reference Answer : A reference answer that would receive a score of 5. Instead of re-
quiring the evaluator LM to solve the instruction, it enables the evaluator to use the mutual
information between the reference answer and the response to make a scoring decision.
The two components for the output are as follows:
1.Feedback : A rationale of why the provided response would receive a particular score. This
is analogous to Chain-of-Thoughts (CoT), making the evaluation process interpretable.
2.Score : An integer score for the provided response that ranges from 1 to 5.
Each instance has an accompanying scoring rubric and reference answer upon the instruction in
order to include as much reference material as possible. Also, we include an equal number of 20K
instances for each score in the range of 1 to 5, preventing undesired decision bias while training the
evaluator LLM. A detailed analysis of the F EEDBACK COLLECTION dataset is in Appendix A.
3.1 D ATASET CONSTRUCTION PROCESS
We construct a large-scale F EEDBACK COLLECTION dataset by prompting GPT-4. Specifically,
the collection process consists of (1) the curation of 50 initial seed rubrics, (2) the expansion of
1K new score rubrics through GPT-4, (3) the augmentation of realistic instructions, and (4) the
augmentation of the remaining components in the training instances (i.e. responses including the
reference answers, feedback, and scores). Figure 3 shows the overall augmentation process.
Step 1: Creation of the Seed Rubrics We begin with the creation of a foundational seed dataset
of scoring rubrics. Each author curates a detailed and fine-grained scoring rubric that each personnel
considers pivotal in evaluating outputs from LLMs. This results in an initial batch of 50 seed rubrics.
Step 2: Augmenting the Seed Rubrics with GPT-4 Using GPT-4 and our initial seed rubrics, we
expand the score rubrics from the initial 50 to a more robust and diverse set of 1000 score rubrics.
Specifically, by sampling 4 random score rubrics from the initial seed, we use them as demonstra-
tions for in-context learning (ICL), and prompt GPT-4 to brainstorm a new novel score rubric. Also,
we prompt GPT-4 to paraphrase the newly generated rubrics in order to ensure P ROMETHEUS could
generalize to the similar score rubric that uses different words. We iterate the brainstorming →
paraphrasing process for 10 rounds. The detailed prompt used for this procedure is in Appendix G.
Step 3: Crafting Novel Instructions related to the Score Rubrics With a comprehensive dataset
of 1000 rubrics at our disposal, the subsequent challenge was to craft pertinent training instances.
5

--- PAGE 6 ---
Adaptive CommunicationAmbiguity NavigationGlobal Cultural 
AwarenessEmotional Intelligence 
in CommunicationSpecialized Language MasteryCultural Sensitivity & RespectInnovative Problem-
SolvingError & Misinterpretation 
ManagementIndustry & Technical
 Language ProficiencySafe & Engaging Content Delivery
Tone & Style Modulation
Contextual Language Adaptation
Emotionally Attuned Responses
Proactive 
Clarification Seeking
Ambiguous Input Clarification
Unclear Query Handling
Sensitivity to Cultural 
Differences
Respect for Cultural 
Diversity
Understanding of Global 
Traditions
Emotionally Considerate
Communication
Recognition & Acknowledgment 
of Emotions
Empathetic & Supportive Responses
Precise Technical 
Language Use
Technical Vocabulary Interpretation
Sector-Specific Terminology
Nuanced Cultural Understanding
Avoidance of Cultural Stereotypes
Societal Manners & Eti
quette
Con
flict Resolution Strategies
Creative Solution Crafting
Novel Idea Generation
Error Identification & Correction
Misinterpretation Mitigation
Effective Misconception Handling
Accurate Industry 
Terminology Use
Specialized Language 
F luency
Technical Term Mastery
Child Safety Promotion
Clear & Concise Information Provision
Engagement Enhancement StrategiesEmotionally Attuned Responses
Technical Ter m
MasteryNovel Idea 
Generation...21
34Seed 
Scor e Rubrics
F eedback Collection Instance AugmentationF eedback Collection
Scor e Rubrics (1K)
Is t he answ er writt en 
pr of essionally and f ormally ,  
so t hat I could 
send it t o m y boss?

[Scor e 1] The answ er lacks
an y sense of pr of essionalism 
and is inf ormal.

...

[Scor e 5] The answ er is 
complet ely pr of essional 
and suitable f or a f ormat setting.I am an entr y le v el emplo y ee at
a multinational corporation and
I ha v e been ask ed t o writ e a
r epor t on t he curr ent tr ends
in our industr y .

I am unsur e ho w t o structur e
t he r epor t and le v el of f ormality
r equir ed. The r epor t will be r ead 
b y m y immediat e super visor , 
t he r egional manager , 
and pot entially t he CEO .

Can y ou giv e me a sample of
ho w t he r epor t 
should be writt en?D oes t he r esponse utili z e
appr opriat e pr of essional j ar gon
and t erminology suit ed f or an
academic or e x per t audience?

[Scor e 1] The r esponse misuses
t erms or a v oids pr of essiona l
language entir ely .
...

[Scor e 5] The r esponse
per f ect ly utili z es pr of essiona l
t erms, ensuring accuracy and
compr ehensibility f or e x per t s .
Is t he answ er e x plained lik e
a f ormal pr oof?

[Scor e 1] The answ er lacks an y
structur e r esembling a
f ormal pr oof .

...

[Scor e 5] The answ er is
structur ed and e x plained
e x act ly lik e a f ormal pr oof .
Cust omi z ed Scor e RubricInstructionR ef er ence Answ er (Scor e 5 ) &
R esponses t o Ev aluat eH er e ’ s a plan y ou can f ollo w .

F irst, do r esear ch on all t he
lat est stuff in y our industr y .
Then, cr eat e an out line t o
k eep y our t hought s or gani z ed .
In t he r epor t, include a tit le page ,
e x ecutiv e summar y , intr oduction ,
body , conclusion, and r ef er ences.

K eep y our language f ormal, but
don ’ t be t oo stiff . U se headings ,
subheadings, and add char t s o r
graphs. H er e ’ s an e x ample :
...The r esponse pr o vides a helpfu l
guide t o appr oaching t he task ,
but it could be mor e pr of essiona l
in t one and phrasing.

Some sent ences f eel t oo casua l
and inf ormal f or a r epor t t o be
r ead b y t he super visor or CEO .

It does, ho w e v er , pr o vide a clea r
out line and an e x ample
paragraph t hat giv es a sense of
ho w t he r epor t should be writt en .
So, t he o v erall scor e is 3 .F eedback
Scor eFigure 3: Overview of the augmentation process of the F EEDBACK COLLECTION . The keywords
included within the score rubrics of the F EEDBACK COLLECTION is also displayed.
For example, a score rubric asking “Is it formal enough to send to my boss” is not related to a
math problem. Considering the need for a set of instructions closely related to the score rubrics, we
prompt GPT-4 to generate 20K unique instructions that are highly relevant to the given score rubric.
Step 4: Crafting Training Instances Lastly, we sequentially generate a response to evaluate and
corresponding feedback by prompting GPT-4 to generate each component that will get a score of i
(1≤i≤5). This leads to 20 instructions for each score rubric, and 5 responses & feedback for each
instruction. To eliminate the effect of decision bias when fine-tuning our evaluator LM, we generate
an equal number of 20K responses for each score. Note that for the response with a score of 5, we
generate two distinctive responses so we could use one of them as an input (reference answer).
3.2 F INE-TUNING AN EVALUATOR LM
Using the F EEDBACK COLLECTION dataset, we fine-tune Llama-2-Chat (7B & 13B) and obtain
PROMETHEUS to induce fine-grained evaluation capability. Similar to Chain-of-Thought Fine-
tuning (Ho et al., 2022; Kim et al., 2023a), we fine-tune to sequentially generate the feedback and
then the score. We highlight that it is important to include a phrase such as ‘ [RESULT] ’ in be-
tween the feedback and the score to prevent degeneration during inference. We include the details
of fine-tuning, inference, and ablation experiments (reference materials, base model) in Appendix C.
6

--- PAGE 7 ---
Table 2: Information about the datasets we use to test evaulator LMs. Note that F EEDBACK BENCH
is a dataset that is crafted with the exact same procedure as the F EEDBACK COLLECTION as ex-
plained in Section 3.1. We include additional analysis of F EEDBACK BENCH in Appendix B. Sim-
ulated GPT-4 †denotes GPT-4 prompted to write a score of i(1≤i≤5) during augmentation.
Evaluation Mode Evaluation Data Source / Types of Score Rubric Response LMs
Absolute EvaluationFEEDBACK BENCH (Seen Rubric) 1K Machine Generated
Simluated GPT-4 †
FEEDBACK BENCH (Unseen Rubric) 50 Hand Crafted
Vicuna Bench
80 Hand CraftedWizardLM, Vicuna,
Llama2-Chat, ChatGPTMT Bench
Flask EvalLogical Thinking (3), Background Knowledge (2)
Problem Handling (4), User Alignment (3)Alpaca, Vicuna, Bard, ChatGPT
Ranking EvaluationMT Bench Human Judgments Human Preference (Helpfulness)Alpaca, Llama, Vicuna, ChatGPT,
Claude-v1, GPT-4
HHH AlignmentHelpfulness, Harmlessness,
Honesty, OtherHuman Annotation
4 E XPERIMENTAL SETTING : EVALUATING AN EVALUATOR LM
In this section, we explain our experiment setting, including the list of experiments, metrics, and
baselines that we use to evaluate fine-grained evaluation capabilities of an evaluator LLM. Compared
to measuring the instruction-following capability of a LLM, it is not straightforward to directly
measure the capability to evaluate. Therefore, we use human evaluation and GPT-4 evaluation as
a standard and measure how similarly our evaluator model and baselines could closely simulate
them. We mainly employ two types of evaluation methods: Absolute Grading andRanking Grading .
Detailed information on the datasets used for the experiment is included in Table 2.
4.1 L IST OF EXPERIMENTS AND METRICS
Absolute Grading We first test in an Absolute Grading setting, where the evaluator LM should
generate a feedback and score within the range of 1 to 5 given an instruction, a response to evaluate,
and reference materials (as shown in Figure 2). Absolute Grading is challenging compared to Rank-
ing Grading since the evaluator LM does not have access to an opponent to compare with and it is
required to provide a score solely based on its internal decision. Yet, it is more practical for users
since it relieves the need to prepare an opponent to compare with during evaluation.
We mainly conduct three experiments in this setting: (1) measuring the correlation with human
evaluators (Section 5.1), (2) comparing the quality of the feedback using human evaluation (Sec-
tion 5.1), and (3) measuring the correlation with GPT-4 evaluation (Section 5.2). For the experiments
that measure the correlation, we use 3 different correlation metrics: Pearson ,Kdendall-Tau , and
Spearman . For measuring the quality of the generated feedback, we conduct a pairwise compari-
sonbetween the feedback generated by P ROMETHEUS , GPT-3.5-Turbo, and GPT-4, asking human
evaluators to choose which has better quality and why they thought so. Specifically, we recruited 9
crowdsource workers and split them into three groups: P ROMETHEUS vs GPT-4, P ROMETHEUS vs
ChatGPT, and GPT-4 vs ChatGPT. The annotators are asked to answer the following three questions:
1. What score would you give to the response based on the given score rubric?
2. Among the two Feedback, which is better for critiquing the given response?
3. Why did you reject that particular feedback?
We use the following four benchmarks to measure the correlation with human evaluation and GPT-4
evaluation. Note that F EEDBACK BENCH is a dataset generated with the same procedure as the
FEEDBACK COLLECTION , and is divided into two subsets (Seen Rubric and Unseen Rubric).
•FEEDBACK BENCH : The Seen Rubric subset shares the same 1K score rubrics with the
FEEDBACK COLLECTION across 1K instructions (1 per score rubric). The Unseen Rubric
subset also consists of 1K new instructions but with 50 new score rubrics that are generated
the same way as the training set. Details are included in Appendix B.
7

--- PAGE 8 ---
•Vicuna Bench : We adapt the 80 test prompt set from Vicuna (Chiang et al., 2023) and
hand-craft customized score rubrics for each test prompt. In order to obtain reference
answers, we concatenate the hand-crafted score rubric and instruction to prompt GPT-4.
•MT Bench : We adapt the 80 test prompt set from MT Bench (Zheng et al., 2023), a multi-
turn instruction dataset. We hand-craft customized score rubrics and generate a reference
answer using GPT-4 for each test prompt as well. Note that we only use the last turn of this
dataset for evaluation, providing the previous dialogue as input to the evaluator LM.
•FLASK Eval : We adapt the 200 test prompt set from FLASK (Ye et al., 2023b), a fine-
grained evaluation dataset that includes multiple conventional NLP datasets and instruction
datasets. We use the 12 score rubrics (that are relatively coarse-grained compared to the 1K
score rubrics used in the F EEDBACK COLLECTION ) such as Logical Thinking, Background
Knowledge, Problem Handling, and User Alignment.
Ranking Grading To test if an evaluator LM trained only on Absolute Grading could be utilized
as a universal reward model based on anycriteria, we use existing human preference benchmarks and
useaccuracy as our metric (Section 5.3). Specifically, we check whether the evaluator LM could
give a higher score to the response that is preferred by human evaluators. The biggest challenge
of employing an evaluator LM trained in an Absolute Grading setting and testing it on Ranking
Grading was that it could give the same score for both candidates. Therefore, we use a temperature
of 1.0 when evaluating each candidate independently and iterate until there is a winner. Hence, it’s
noteworthy that the settings are not exactly fair compared to other ranking models. This setting is
NOT designed to claim SOTA position in these benchmarks, but is conducted only for the purpose
of checking whether an evaluator LM trained in an Absolute Grading setting could also generalize
in a Ranking Grading setting according to general human preference. Also, in this setting, we do
not provide a reference answer to check whether P ROMETHEUS could function as a reward model.
We use the following two benchmarks to measure the accuracy with human preference datasets:
•MT Bench Human Judgement : This data is another version of the aforementioned MT
Bench (Zheng et al., 2023). Note that it includes a tie option as well and does not require
iterative inference to obtain a clear winner. We use Human Preference as our criteria.
•HHH Alignment : Introduced by Anthropic (Askell et al., 2021), this dataset (221 pairs) is
one of the most widely chosen reward-model test-beds that measures preference accuracy in
Helpfulness, Harmlessness, Honesty, and in General (Other) among two response choices.
4.2 B ASELINES
The following list shows the baselines we used for comparison in the experiments:
• L LAMA 2-C HAT-{7,13,70 }B (Touvron et al., 2023): The base model of P ROMETHEUS
when fine-tuning on the F EEDBACK COLLECTION . Also, it is considered as the best option
among the open-source LLMs, which we use as an evaluator in this work.
• LL AMA -2-C HAT-13B + C OARSE : To analyze the effectiveness of training on thousands
of fine-grained score rubrics, we train a comparing model only using 12 coarse-grained
score rubrics from Ye et al. (2023b). Detailed information on this model is in Appendix D.
• GPT-3.5- TURBO -0613: Proprietary LLM that offers a cheaper price when employed as
an evaluator LLM. While it is relatively inexpensive compared to GPT-4, it still has the
issue of uncontrolled versioning and close-source nature.
• GPT-4- {0314,0613, R ECENT }: One of the most powerful proprietary LLM that is con-
sidered the main option when using LLMs as evaluators. Despite its reliability as an eval-
uator LM due to its superior performance, it has several issues of prohibitive costs, uncon-
trolled versioning, and close-source nature.
• STANFORD NLP R EWARD MODEL2: One of the state-of-the-art reward model directly
trained on multiple human preference datasets in a ranking grading setting.
• ALMOST R EWARD MODEL (Kim et al., 2023b): Another state-of-the-art reward model
trained on synthetic preference datasets in a ranking grading setting.
2https://huggingface.co/stanfordnlp/SteamSHP-flan-t5-xl
8

--- PAGE 9 ---
P earson Corr elation Betw een LLM Ev aluat ors and Human Ev aluat ors Scor es
P earson Corr elation (higher is bett er)Ov er allF eedback Collection 
(Unseen)MT BenchVicuna BenchGPT - 3 . 5
turboGPT -4Pr omet heus
13BFigure 4: The Pearson correlation between scores from human annotators and the score from GPT-
3.5-Turbo, Prometheus, and GPT-4 on 45 customized score rubrics from the Feedback Bench, Vi-
cuna Bench, and MT Bench. P ROMETHEUS shows a high correlation with human evaluators.
Pr omet heus
vs 
ChatGPTPr omet heus
vs
GPT -4GPT -4
vs
ChatGPT
Left WinsRight WinsBot h ar e GoodBot h ar e Bad
Figure 5: Pairwise comparison of the quality of the feedback generated by GPT-4, P ROMETHEUS
and GPT-3.5-Turbo. Annotators are asked to choose which feedback is better at assessing the given
response. P ROMETHEUS shows a win-rate of 58.62% over GPT-4 and 79.57% over GPT-3.5-Turbo.
5 E XPERIMENTAL RESULTS
5.1 C ANPROMETHEUS CLOSELY SIMULATE HUMAN EVALUATION ?
Correlation with Human Scoring We first compare the correlation between human annotators
and our baselines using 45 instances each with an unique customized score rubric, namely the
FEEDBACK BENCH (Unseen Score Rubric subset), MT Bench (Zheng et al., 2023), and Vicuna
Bench (Chiang et al., 2023). The results are shown in Figure 4, showing that P ROMETHEUS is
on par with GPT-4 across all the three evaluation datasets, where P ROMETHEUS obtains a 0.897
Pearson correlation, GPT-4 obtains 0.882, and GPT-3.5-Turbo obtains 0.392.
Pairwise Comparison of the Feedback with Human Evaluation To validate the effect of
whether P ROMETHEUS generates helpful/meaningful feedback in addition to its scoring decision,
we ask human annotators to choose a better feedback. The results are shown in Figure 5, showing
that P ROMETHEUS is preferred over GPT-4 58.62% of the times, and over GPT-3.5-Turbo 79.57%
of the times. This shows that P ROMETHEUS ’s feedback is also meaningful and helpful.
9

--- PAGE 10 ---
Figure 6: The reason why GPT-4’s or Prometheus’s feedback was not chosen over the other.
PROMETHEUS generates less abstract and general feedback, but tends to write overly critical ones.
Analysis of Why Prometheus’s Feedback was Preferred In addition to a pairwise comparison of
the feedback quality, we also conduct an analysis asking human annotators to choose why they pre-
ferred one feedback over the other by choosing at least one of the comprehensive 6 options (“rejected
feedback is not consistent with its score” / “too general and abstract” / “overly optimistic” / “not rel-
evant to the response” / “overly critical” / “unrelated to the score rubric”). In Figure 6, we show
the percentage of why each evaluator LLM (GPT-4 and P ROMETHEUS ) was rejected. It shows that
while GPT-4 was mainly not chosen due to providing general or abstract feedback, P ROMETHEUS
was mainly not chosen because it was too critical about the given response. Based on this result, we
conclude that whereas GPT-4 tends to be more neutral and abstract, P ROMETHEUS shows a clear
trend of expressing its opinion of whether the given response is good or not. We conjecture this is an
effect of directly fine-tuning P ROMETHEUS to ONLY perform fine-grained evaluation, essentially
converting it to an evaluator rather than a generator. We include (1) additional results of analyz-
ing “P ROMETHEUS vsGPT-3.5-Turbo” and “GPT-4 vsGPT-3.5-Turbo” in Appendix E and (2) a
detailed explanation of the experimental setting of human evaluation in Appendix J.
5.2 C ANPROMETHEUS CLOSELY SIMULATE GPT-4 E VALUATION ?
Correlation with GPT-4 Scoring We compare the correlation between GPT-4 evaluation and our
baselines using 1222 score rubrics across 2360 instances from the F EEDBACK BENCH (Seen and
Unseen Score Rubric Subset), Vicuna Bench (Chiang et al., 2023), MT Bench (Zheng et al., 2023),
and Flask Eval (Ye et al., 2023b) in an absolute grading scheme. Note that for the F EEDBACK
BENCH , we measure the correlation with the scores augmented from GPT-4-0613, and for the
other 3 datasets, we measure the correlation with the scores acquired by inferencing GPT-4-0613.
The results on these benchmarks are shown across Table 3 and Table 4.
In Table 3, the performance of L LAMA -2-C HAT 13B degrades over the 7B model and slightly
improves when scaled up to 70B size, indicating that naively increasing the size of a model does
not necessarily improve an LLM’s evaluation capabilities. On the other hand, P ROMETHEUS 13B
shows a +0.420 and +0.397 improvement over its base model L LAMA 2-C HAT 13B in terms of
Pearson correlation on the seen and unseen rubric set, respectively. Moreover, it even outperforms
LLAMA 2-C HAT 70B, GPT-3.5-T URBO -0613, and different versions of GPT-4. We conjecture
the high performance of P ROMETHEUS is mainly because the instructions and responses within the
10

--- PAGE 11 ---
Table 3: Pearson, Kendall-Tau, Spearman correlation with data generated by GPT-4-0613. All scores were
sampled across 3 inferences. The best comparable statistics are bolded and second best underlined .
Evaluator LMFEEDBACK COLLECTION TEST SET (GENERATED BY GPT-4-0613)
SEEN CUSTOMIZED RUBRICS UNSEEN CUSTOMIZED RUBRIC
Pearson Kendall-Tau Spearman Pearson Kendall-Tau Spearman
LLAMA 2-C HAT 7B 0.485 0.422 0.478 0.463 0.402 0.465
LLAMA 2-C HAT 13B 0.441 0.387 0.452 0.450 0.379 0.431
LLAMA 2-C HAT 70B 0.572 0.491 0.564 0.558 0.477 0.549
LLAMA 2-C HAT 13B + C OARSE . 0.482 0.406 0.475 0.454 0.361 0.427
PROMETHEUS 7B 0.860 0.781 0.863 0.847 0.767 0.849
PROMETHEUS 13B 0.861 0.776 0.858 0.860 0.771 0.858
GPT-3.5-T URBO -0613 0.636 0.536 0.617 0.563 0.453 0.521
GPT-4-0314 0.754 0.671 0.762 0.753 0.673 0.761
GPT-4-0613 0.742 0.659 0.747 0.743 0.660 0.747
GPT-4 (R ECENT ) 0.745 0.659 0.748 0.733 0.641 0.728
Table 4: Pearson, Kendall-Tau, Spearman correlation with scores sampled from GPT-4-0613 across 3 infer-
ences. Note that GPT-4-0613 was sampled 6 times in total to measure self-consistency. The best comparable
statistics are bolded and second best underlined among baselines. We include GPT-4 as reference to show it
self-consistency when inferenced multiple times.
Evaluator LMVICUNA BENCH MT B ENCH FLASK EVAL
Pearson Kendall-Tau Spearman Pearson Kendall-Tau Spearman Pearson Kendall-Tau Spearman
LLAMA 2-C HAT 7B 0.175 0.143 0.176 0.132 0.113 0.143 0.271 0.180 0.235
LLAMA 2-C HAT 13B 0.211 0.203 0.253 -0.020 -0.029 -0.038 0.265 0.182 0.235
LLAMA 2-C HAT 70B 0.376 0.318 0.391 0.226 0.175 0.224 0.336 0.267 0.346
LLAMA 2-C HAT 13B + C OARSE . 0.307 0.196 0.245 0.417 0.328 0.420 0.517 0.349 0.451
PROMETHEUS -7B 0.457 0.365 0.457 0.293 0.216 0.295 0.367 0.285 0.371
PROMETHEUS -13B 0.466 0.346 0.429 0.473 0.341 0.451 0.467 0.345 0.455
GPT-3.5-T URBO -0613 0.270 0.187 0.232 0.275 0.202 0.267 0.422 0.299 0.371
GPT-4-0314 0.833 0.679 0.775 0.857 0.713 0.849 0.785 0.621 0.747
GPT-4-0613 0.925 0.783 0.864 0.952 0.834 0.927 0.835 0.672 0.798
GPT-4 (RECENT) 0.932 0.801 0.877 0.944 0.812 0.914 0.832 0.667 0.794
test set might share a similar distribution with the train set we used (simulating a scenario where
a user is interacting with a LLM) even if the score rubric holds unseen. Also, training on feed-
back derived from coarse-grained score rubrics (denoted as L LAMA 2-C HAT 13B + C OARSE ) only
slightly improves performance, indicating the importance of training on a wide range of score rubric
is important to handle customized rubrics that different LLM user or researcher would desire.
In Table 4, the trends of L LAMA 2-C HAT among different sizes hold similar; simply increasing size
does not greatly improve the LLM’s evaluation capabilities. On these benchmarks, P ROMETHEUS
shows a +0.255, +0.493, and +0.202 improvement over its base model L LAMA 2-C HAT-13B in
terms of Pearson correlation on the Vicuna Bench, MT Bench, and Flask Eval dataset, respectively.
While P ROMETHEUS outperforms L LAMA 2-C HAT 70B and GPT-3.5-T URBO -0613, it lacks be-
hind GPT-4. We conjecture that this might be because the instructions from the F EEDBACK COL-
LECTION and these evaluation datasets have different characteristics; the F EEDBACK COLLECTION
are relatively long and detailed (e.g., I’m a city planner ... I’m looking for a novel and progressive
solution to handle traffic congestion and air problems derived from population increase), while the
datasets used for evaluation hold short (e.g., Can you explain about quantum mechanics?).
On the other hand, it is important to note that on the Flask Eval dataset, L LAMA 2-C HAT 13B +
COARSE (specifically trained with the Flask Eval dataset) outperforms P ROMETHEUS . This indi-
cates that training directly on the evaluation dataset might be the best option to acquire a task-specific
evaluator LLM, and we further discuss this in Section 6.4.
11

--- PAGE 12 ---
Table 5: Human Agreement accuracy among ranking datasets. The best comparable statistics are bolded .
Evaluator LMHHH A LIGNMENT MT B ENCH HUMAN JUDG .
Help. Harm. Hon. Other Total Avg. Human Preference
RANDOM 50.00 50.00 50.00 50.00 50.00 34.26
STANFORD NLP R EWARD MODEL 69.49 60.34 52.46 51.16 58.82 44.79
ALMOST R EWARD MODEL 74.58 67.24 78.69 86.05 76.02 49.90
LLAMA 2-C HAT 7B 66.10 81.03 70.49 74.42 72.85 51.78
LLAMA 2-C HAT 13B 74.58 87.93 55.74 79.07 73.76 52.34
LLAMA 2-C HAT 70B 66.10 89.66 67.21 74.42 74.21 53.67
LLAMA 2-C HAT 13B + C OARSE . 68.74 68.97 65.57 67.44 67.42 46.89
GPT-3.5-T URBO -0613 76.27 87.93 67.21 86.05 78.73 57.12
PROMETHEUS 7B 69.49 84.48 78.69 90.70 80.09 55.14
PROMETHEUS 13B 81.36 82.76 75.41 76.74 79.19 57.72
GPT-4-0613 91.53 93.10 85.25 83.72 88.69 63.87
5.3 C ANPROMETHEUS FUNCTION AS A REWARD MODEL ?
We conduct experiments on 2 human preference datasets: HHH Alignment (Askell et al., 2021) and
MT Bench Human Judgment (Zheng et al., 2023) that use a ranking grading scheme. In Table 5,
results show that prompting L LAMA -2-C HAT surprisingly obtains reasonable performance, which
we conjecture might be the effect of using a base model that is trained with Reinforcement Learn-
ing from Human Feedback (RLHF). When training on feedback derived from coarse-grained score
rubrics (denoted as L LAMA 2-C HAT 13B + C OARSE ), it only hurts performance. On the other hand,
PROMETHEUS 13B shows a +5.43% and +5.38% margin over its base model L LAMA 2-C HAT-13B
on the HHH Alignment and MT Bench Human Judgement dataset, respectively. These results are
surprising because they indicate that training on an absolute grading scheme could also improve
performance on a ranking grading scheme even without directly training on ranking evaluation in-
stances. Moreover, it shows the possibilities of using a generative LLM (P ROMETHEUS ) as a reward
model for RLHF (Kim et al., 2023b). We leave the exploration of this research to future work.
6 D ISCUSSIONS AND ANALYSIS
6.1 W HY IS IT IMPORTANT TO INCLUDE REFERENCE MATERIALS ?
Evaluating a given response without any reference material is a very challenging task (i.e., Directly
asking to decide a score only when an instruction and response are given), since the evaluation LM
should be able to (1) know what the important aspects tailored with the instruction is, (2) inter-
nally estimate what the answer of the instruction might be, and (3) assess the quality of responses
based on the information derived from the previous two steps. Our intuition is that by incorporating
each component within the reference material, the evaluator LM could solely focus on assessing
the quality of the response instead of determining the important aspects or solving the instruction.
Specifically, we analyze the role of each component as follows:
•Score Rubric : Giving information of the the pivotal aspects essential for addressing the
given instruction. Without the score rubric, the evaluator LM should inherently know what
details should be considered from the given instruction.
•Reference Answer : Decomposing the process of estimating a reference answer and evalu-
ating it at the same time into two steps. Since the reference answer is given as an additional
input, the evaluator LM could only focus on evaluating the given response. This enables
to bypass a natural proposition that if an evaluator LM doesn’t have the ability to solve the
problem, it’s likely that it cannot evaluate different responses effectively as well.
As shown in Table 6, we conduct an ablation experiment by excluding each reference material and
also training only on the score rubric without generating a feedback. Additionally, we also ablate the
effect of using different model variants (Llama-2, Vicuna, Code-Llama) instead of Llama-2-Chat.
12

--- PAGE 13 ---
Table 6: Pearson, Kendall-Tau, Spearman correlation with data generated by GPT-4-0613 (Feedback Collec-
tion Test set) and scores sampled from GPT-4-0613 across 3 inferences (Vicuna Bench).
Evaluator LMFEEDBACK COLLECTION TEST SET VICUNA BENCH
Seen Score Rubric Unseen Score Rubric -
Pearson Pearson Pearson
PROMETHEUS 7B 0.860 0.847 0.457
Training Ablation
W/OSCORE RUBRIC 0.837 0.745 0.355
W/OFEEDBACK DISTILLATION 0.668 0.673 0.413
W/OREFERENCE ANSWER 0.642 0.626 0.349
Model Ablation
LLAMA -2 7B B ASELINE 0.839 0.818 0.404
VICUNA -V1.5 7B B ASELINE 0.860 0.829 0.430
CODE-LLAMA 7B B ASELINE 0.823 0.761 0.470
Training Ablation The results indicate that each component contributes orthogonally to
PROMETHEUS ’s superior evaluation performance. Especially, excluding the reference answer shows
the most significant amount of performance degradation, supporting our claim that including a ref-
erence answer relieves the need for the evaluator LM to internally solve the instruction and only
focus on assessing the response. Also, while excluding the score rubric on the F EEDBACK BENCH
does not harm performance a lot, the performance drops a lot when evaluating on Vicuna Bench. As
in our hypothesis, we conjecture that in order to generalize on other datasets, the role of providing
what aspect to evaluate holds relatively crucial.
Model Ablation To test the effect using L LAMA 2-C HAT, a model that has been instruction-tuned
with both supervised fine-tuning and RLHF, we ablate by using different models as a starting point.
Results show that different model choices do not harm performance significantly, yet a model trained
with both supervised fine-tuning and RLHF shows the best performance, possibly due to additional
training to follow instructions. However, we find that using Code-Llama has some benefits when
evaluating on code domain, and we discuss the effect on Section 6.5.
6.2 N ARROWING PERFORMANCE GAP TO GPT-4 E VALUATION
The observed outcomes, in which P ROMETHEUS consistently surpasses GPT-4 based on human
evaluations encompassing both scores and quality of feedback, as well as correlations in the F EED-
BACK BENCH , are indeed noteworthy. We firmly posit that these findings are not merely serendipi-
tous and offer the following justifications:
• Regarding results on F EEDBACK BENCH , our model is directly fine-tuned on this data, so
it’s natural to beat GPT-4 on a similar distribution test set if it is well-trained. In addition,
for GPT-4, we compare the outputs of inferencing on the instructions and augmenting
new instances, causing the self-consistency to be lower.
• Regarding score correlation for human evaluation, our model shows similar or slightly
higher trends. First, our human evaluation set-up excluded all coding or math-related ques-
tions, which is where it is non-trivial to beat GPT-4 yet. Secondly, there’s always the margin
of human error that needs to be accounted for. Nonetheless, we highlight that we are the
first work to argue that an open-source evaluator LM could closely reach GPT-4 evaluation
only when the appropriate reference materials are accompanied .
• As shown in Figure 6, P ROMETHEUS tends to be critical compared to GPT-4. We conjec-
ture this is because since it is specialized for evaluation, it acquires the characteristics of
seeking for improvement when assessing responses.
13

--- PAGE 14 ---
6.3 Q UALITATIVE EXAMPLES OF FEEDBACK GENERATED BY PROMETHEUS
We present five qualitative examples to compare the feedback generated by P ROMETHEUS and
GPT-4 in Appendix I. Specifically, Figure 16 shows an example where human annotators labeled
that GPT-4 generate an abstract/general feedback not suitable for criticizing the response. Figure 17
shows an example where human annotators labeled that P ROMETHEUS generate overly critical feed-
back. Figure 18 shows an example of human annotators labeled as a tie. In general, P ROMETHEUS
generates a detailed feedback criticizing which component within the response is wrong and seek
improvement. This qualitatively shows that P ROMETHEUS could function as an evaluator LM.
Moreover, we present an example of evaluating python code responses using P ROMETHEUS , GPT-
4, and Code-Llama in Figure 19. We discuss the effect of using a base model specialized on code
domain for code evaluation in Section 6.5.
6.4 A P RACTITIONER ’SGUIDE FOR DIRECTLY USING PROMETHEUS EVALUATION
Preparing an Evaluation Dataset As shown in the previous sections, P ROMETHEUS functions as
a good evaluator LM not only on the F EEDBACK BENCH (a dataset that has a similar distribution
with the dataset it was trained on), but also on other widely used evaluation datasets such as the Vi-
cuna Bench, MT Bench, and Flask Eval. As shown in Figure 1, users should prepare the instruction
dataset they wish to evaluate their target LLM on. This could either be a widely used instruction
dataset or a custom evaluation users might have.
Deciding a Score Rubric to Evaluate on The next step is to choose the score rubric users would
want to test their target LLM on. This could be confined to generic metrics such as helpfulness/harm-
lessness, but P ROMETHEUS also supports fine-grained score rubrics such as “Child-Safety”, “Cre-
ativity” or even “Is the response formal enough to send to my boss”.
Preparing Reference Answers While evaluating without any reference is also possible, as shown
in Table 6, P ROMETHEUS shows superior performance when the reference answer is provided.
Therefore, users should prepare the reference answer they might consider most appropriate based
on the instructions and score rubrics they would want to test on. While this might require additional
cost to prepare, there is a clear trade-off in order to improve the precision or accuracy of the overall
evaluation process, hence it holds crucial.
Generating Responses using the Target LLM The last step is to prepare responses acquired
from the target LLM that users might want to evaluate. By providing the reference materials (score
rubrics, reference answers) along with the instruction and responses to evaluate on, P ROMETHEUS
generates a feedback and a score. Users could use the score to determine how their target LLM is
performing based on customized criteria and also refer to the feedback to analyze and check the
properties and behaviors of their target LLM. For instance, P ROMETHEUS could be used as a good
alternative for GPT-4 evaluation while training a new LLM. Specifically, the field has not yet come
up with a formalized procedure to decide the details of instruction-tuning or RLHF while developing
a new LLM. This includes deciding how many training instances to use, how to systematically decide
the training hyperparameters, and quantitatively analyzing the behaviors of LLMs across multiple
versions. Most importantly, users might not want to send the outputs generated by their LLMs to
OpenAI API calls. In this regard, P ROMETHEUS provides an appealing solution of having control
over the whole evaluation process, also supporting customized score rubrics.
6.5 A P RACTITIONER ’SGUIDE FOR TRAINING A NEWEVALUATION MODEL
Users might also want to train their customized evaluator LM as P ROMETHEUS for different use
cases. As shown in Table 4, training directly on the Flask dataset (denoted as L LAMA 2-C HAT
13B + C OARSE ) shows a higher correlation with GPT-4 on the Flask Eval dataset compared to
PROMETHEUS that is trained on the F EEDBACK COLLECTION . This implies that directly training
on a target feedback dataset holds the best performance when evaluating on it. Yet, this requires
going through the process of preparing a new feedback dataset (described in Section 3.1). This
implies that there is a trade-off between obtaining a strong evaluator LM on a target task and paying
14

--- PAGE 15 ---
the initial cost to prepare a new feedback dataset. In this subsection, we provide some guidelines for
how users could also train their evaluator LM using feedback datasets.
Preparing a Feedback Dataset to train on As described in Section 3, some important consider-
ations to prepare a new feedback dataset are: (1) including as many reference materials as possible,
(2) maintaining a uniform length among the reference answers for each score (1 to 5) to prevent un-
desired length bias, (3) maintaining a uniform score distribution to prevent undesired decision bias.
While we did not explore the effect of including other possible reference materials such as a “Score
1 Reference Answer” or “Background Knowledge” due to limited context length, future work could
also explore this aspect. The main intuition is that providing more reference materials could enable
the evaluator LM to solely focus on evaluation instead of solving the instruction.
Choosing a Base Model to Train an Evaluator LM As shown in Figure 19, we find that training
on C ODE-LLAMA provides more detailed feedback and a reasonable score decision when evaluating
responses on code domains (7 instances included within the Vicuna Bench dataset). This indicates
that choosing a different base model based on the domain to evaluate might be crucial when de-
signing an evaluator LM. We also leave the exploration of training an evaluator LM specialized on
different domains (e.g., code and math) as future work.
7 C ONCLUSION
In this paper, we discuss the possibility of obtaining an open-source LM that is specialized for fine-
grained evaluation. While text evaluation is an inherently difficult task that requires multi-faceted
considerations, we show that by incorporating the appropriate reference material, we can effectively
induce evaluation capability into an LM. We propose a new dataset called the F EEDBACK COL-
LECTION that encompasses thousands of customized score rubrics and train an open-source evalu-
ator model, P ROMETHEUS . Surprisingly, when comparing the correlation with human evaluators,
PROMETHEUS obtains a Pearson correlation on par with GPT-4, while the quality of the feedback
was preferred over GPT-4 58.62% of the time. When comparing Pearson correlation with GPT-4,
PROMETHEUS shows the highest correlation even outperforming GPT-3.5-Turbo. Lastly, we show
that P ROMETHEUS shows superior performance on human preference datasets, indicating its possi-
bility as an universal reward model. We hope that our work could stimulate future work on using
open-source LLMs as evaluators instead of solely relying on proprietary LLMs.
ACKNOWLEDGMENTS
This work was partly supported by KAIST-NA VER Hypercreative AI Center and Institute of In-
formation & communications Technology Planning & Evaluation (IITP) grant funded by the Korea
government (MSIT) (No.2022-0-00264, Comprehensive Video Understanding and Generation with
Knowledge-based Deep Logic Neural Network, 40%; No.2021-0-02068, Artificial Intelligence In-
novation Hub, 20%). We thank Minkyeong Moon, Geonwoo Kim, Minkyeong Cho, Yerim Kim,
Sora Lee, Seunghwan Lim, Jinheon Lee, Minji Kim, and Hyorin Lee for helping with the human
evaluation experiments. We thank Se June Joo, Dongkeun Yoon, Doyoung Kim, Seonghyeon Ye,
Gichang Lee, and Yehbin Lee for helpful feedback and discussions.
15

--- PAGE 16 ---
REFERENCES
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,
Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Her-
nandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack
Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a labora-
tory for alignment, 2021.
Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan
Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. arXiv
preprint arXiv:2307.03109 , 2023.
Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. Instructeval: Towards holistic
evaluation of instruction-tuned large language models. arXiv preprint arXiv:2306.04757 , 2023.
Cheng-Han Chiang and Hung yi Lee. Can large language models be an alternative to human evalu-
ations?, 2023.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An
open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:
//lmsys.org/blog/2023-03-30-vicuna/ .
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos
Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for
methods that learn from human feedback. arXiv preprint arXiv:2305.14387 , 2023.
Fabrizio Gilardi, Meysam Alizadeh, and Ma ¨el Kubli. Chatgpt outperforms crowd-workers for text-
annotation tasks. arXiv preprint arXiv:2303.15056 , 2023.
Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers.
arXiv preprint arXiv:2212.10071 , 2022.
Ari Holtzman, Peter West, and Luke Zettlemoyer. Generative models as a complex systems science:
How can we make sense of large language model behavior? arXiv preprint arXiv:2308.00189 ,
2023.
Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning
language models with (almost) no human labor. arXiv preprint arXiv:2212.09689 , 2022.
Seungone Kim, Se June Joo, Doyoung Kim, Joel Jang, Seonghyeon Ye, Jamin Shin, and Minjoon
Seo. The cot collection: Improving zero-shot and few-shot learning of language models via
chain-of-thought fine-tuning. arXiv preprint arXiv:2305.14045 , 2023a.
Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Min Yoo,
and Minjoon Seo. Aligning large language models through synthetic feedback. arXiv preprint
arXiv:2305.13735 , 2023b.
Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. Hurdles to progress in long-form question answering.
arXiv preprint arXiv:2103.06332 , 2021.
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following
models. https://github.com/tatsu-lab/alpaca_eval , 2023.
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian
Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language
models. arXiv preprint arXiv:2211.09110 , 2022.
Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization
Branches Out , pp. 74–81, Barcelona, Spain, July 2004a. Association for Computational Linguis-
tics. URL https://aclanthology.org/W04-1013 .
16

--- PAGE 17 ---
Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization
branches out , pp. 74–81, 2004b.
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg
evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634 , 2023.
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,
Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual
precision in long form text generation. arXiv preprint arXiv:2305.14251 , 2023.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association
for Computational Linguistics , pp. 311–318, 2002.
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning
with gpt-4. arXiv preprint arXiv:2304.03277 , 2023.
Luiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara Hooker. On the challenges of using black-
box apis for toxicity evaluation in research, 2023.
Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. Bleurt: Learning robust metrics for text
generation. In Annual Meeting of the Association for Computational Linguistics , 2020. URL
https://api.semanticscholar.org/CorpusID:215548699 .
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,
Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh
Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,
2023.
Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O’Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu,
Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Shepherd: A
critic for language model generation. arXiv preprint arXiv:2308.04592 , 2023a.
Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang,
Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark for llm
instruction tuning optimization. arXiv preprint arXiv:2306.05087 , 2023b.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and
Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.
arXiv preprint arXiv:2212.10560 , 2022.
Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo.
Selfee: Iterative self-revising llm empowered by self-feedback generation. Blog post, May 2023a.
URL https://kaistai.github.io/SelFee/ .
Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo,
James Thorne, Juho Kim, and Minjoon Seo. Flask: Fine-grained language model evaluation
based on alignment skill sets. arXiv preprint arXiv:2307.10928 , 2023b.
Weizhe Yuan, Graham Neubig, and Pengfei Liu. BARTScore: Evaluating generated text as text
generation. In A. Beygelzimer, Y . Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in
Neural Information Processing Systems , 2021. URL https://openreview.net/forum?
id=5Ya8PbvpZ9 .
17

--- PAGE 18 ---
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluat-
ing text generation with bert. arXiv preprint arXiv:1904.09675 , 2019.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and
chatbot arena. arXiv preprint arXiv:2306.05685 , 2023.
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,
Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation
models. arXiv preprint arXiv:2304.06364 , 2023.
18

--- PAGE 19 ---
Figure 7: Rouge-L score distribution among two randomly sampled score rubrics from the F EED-
BACK COLLECTION . A left-skewed distribution with low values shows the score rubrics are diverse.
Spearman Corr elation: 0 . 58
Figure 8: Box and whisker plot for average sentiment per score description. A linearly increasing
trend is crucial for the evaluator LM to decide a score in an Absolute Scoring setting.
A A NALYSIS OF THE FEEDBACK COLLECTION DATASET
In this section, we provide a comprehensive analysis of the characteristics of the F EEDBACK COL-
LECTION dataset. To ensure the quality, we answer each question one by one, emphasizing our main
considerations during the creation of the dataset.
Are the Score Criteria Diverse Enough? Following previous work (Wang et al., 2022; Honovich
et al., 2022), we plot the rouge-L distribution between two instances among our whole set of 1K
score rubrics. Specifically, we use the score criteria (description of the criteria) and measure the
rouge-L value between the two score criteria. Figure 7 shows the overall distribution plot. The
results indicate that each criteria does not overlap with one another, ensuring that we include many
novel score rubrics in our training set.
Are the Score Descriptions Well Formulated? Another component in the score rubric is a de-
scription of each score (i.e., A comprehensive reason why a score of i(1≤i≤5should be given).
In an Absolute Scoring setting, it is important to evaluate the given response based on the score
descriptions instead of giving a score of 1 for all responses that lack a minor detail or giving a score
19

--- PAGE 20 ---
F eedback Collection T r aining SetFigure 9: Box and whisker plot plotting the length distribution of responses on each score range.
We check whether there is a length bias (i.e., a higher score given for longer responses).
Table 7: Distinct N-gram measured on each component of the training instance. A higher diversity
ratio indicates that each component tends to be more diverse.
Distinct N-gram Bigram Diversity Ratio Trigram Diversity Ratio
Instruction 0.43 0.79
Reference 0.43 0.82
Score Rubric 0.60 0.81
Responses 0.32 0.77
Feedback 0.26 0.66
of 5 for all responses that seem to be good on the surface. Due to these reasons, the role of the score
descriptions hold crucial, where the main role is to show a monotonically increasing tendency of
sentiment, not dramatically. Figure 8 shows that the F EEDBACK COLLECTION holds a smoothly in-
creasing sentiment tendency for each score description. This ensures the quality of the score rubric,
confirming that it plays a role in deciding the score.
Is there a length bias among the Responses? Previous work has demonstrated that when LMs
are used as evaluators, they tend to give higher scores to longer responses (Li et al., 2023; Dubois
et al., 2023; Zheng et al., 2023). In order to minimize this effect during fine-tuning P ROMETHEUS ,
one of our main consideration was to maintain a length distribution equal among the score range of 1
to 5. As shown in Figure 9, most of the responses within the F EEDBACK COLLECTION maintained
a similar length among different scores (near 200 tokens). We also include a comprehensive analysis
of whether P ROMETHEUS possessed any length bias during evaluation in Appendix F.
Are the Instructions, Responses, and Feedback Diverse as Well? In addition to the analysis of
the score rubric and responses, we also analyze whether the instructions, responses, and feedback
within the F EEDBACK COLLECTION are diverse enough. For this purpose, we examine the bigram
and trigram ratios. The results are shown in Table 7, indicating a variety in how terms are expressed,
and our findings suggest a moderate level of diversity. While there is some term repetition, the
dataset also showcases a notable range of expressions.
20

--- PAGE 21 ---
Figure 10: Rouge-L score distribution among a randomly sampled score rubric from the F EEDBACK
COLLECTION and a score rubric from the F EEDBACK BENCH . A left-skewed distribution with low
values shows that they do not overlap with each other, hence meaning that a UNSEEN score rubric
assumption is satisfied.
Table 8: Hyperparameters used for fine-tuning P ROMETHEUS .
Model Base Model Batch size LR LR Scheduler Optimizer Max Length (Input & Output)
PROMETHEUS -7B Llama-2-Chat-7B 28 1e-5 StepLR AdamW 4096
PROMETHEUS -13B Llama-2-Chat-13B 20 1e-5 StepLR AdamW 4096
B A NALYSIS OF THE FEEDBACK BENCH EVALUATION DATASET
In this section, we provide a analysis of whether the F EEDBACK BENCH consists of unseen score
rubrics against the score rubrics from the F EEDBACK COLLECTION .
Does the testset maintain Unseen Score Rubrics? One of the main considerations of our exper-
iments in Section 5.2 using the F EEDBACK BENCH was testing whether P ROMETHEUS could gen-
eralize to unseen customized score rubrics. For this purpose, we built an unseen customized rubric
subset. We plot the rouge-L distribution between a random score rubric within the F EEDBACK COL-
LECTION and a random score rubric within the F EEDBACK BENCH . As shown in Figure 10, there
is a low overlap among the train and test sets, confirming that the F EEDBACK BENCH is valid to be
claimed as an unseen test set to measure the evaluation capability of evaluator LMs.
C F INE-TUNING AND INFERENCE DETAILS OF PROMETHEUS
We use 8xA100 (80GB) GPUs to train our models with PyTorch Fully-Sharded Data Parallel (FSDP)
option. The code we used for training and inference is the official Llama2 fine-tuning code released
by Meta AI3. The hyper-parameters we used are the basic settings in the fine-tuning code except
for the training batch size which was set according to the model size: for 7B models we used 28
and for 13B models we used 20 to fully leverage GPU memory. Note that in the official Llama2
fine-tuning code, the loss is only calculated on the feedback and score decision, not the instruction.
We empirically find that not masking out the instruction leads to poor performance while evaluating
responses. The detailed hyper-parameters are shown in Table 8.
For inference, we use the hyper-parameters as shown in Table 9. When inferencing with the naive
Llama-2-Chat model (not trained on the F EEDBACK COLLECTION ) it was extremely difficult to
3https://github.com/facebookresearch/llama-recipes
21

--- PAGE 22 ---
Table 9: Hyperparameters used for inferencing P ROMETHEUS , GPT-3.5-Turbo, and GPT-4. Verbal-
izer denotes accepting outputs such as ”[Score 5]” or ”Score: 4 out of 5” whereas the exact format
is ”[Result] 5” (format is mentioned concretely within the instruction given to the evaluator LM).
Even after applying a verbalizer, Llama-2-Chat is not able to generate a score decision that could
easily be parsed, highlighting the benefits of fine-tuning it on feedback data.
Params Model Temperature Top-p Repetition Penalty Max Output Length Verbalizer
7B Llama-2-Chat-7B 1.0 0.9 1.03 256 Yes
13B Llama-2-Chat-13B 1.0 0.9 1.03 256 Yes
70B Llama-2-Chat-70B 1.0 0.9 1.03 256 Yes
7B P ROMETHEUS -7B 1.0 0.9 1.03 256 No
13B P ROMETHEUS -13B 1.0 0.9 1.03 256 No
- GPT-3.5-Turbo 1.0 0.9 - 256 No
- GPT-4 1.0 0.9 - 256 No
steer the model to generate a final score in the form to be easily parsed (e.g., “[RESULT] 3”). While
in-context learning (ICL) could solve this issue, most of our instances contained a maximum of
3072 tokens, so we could not utilize demonstrations during inference. Therefore, we empirically
found patterns such as “[SCORE 5]” or “Score: 4 out of 5” and applied verbalizer to map those
outputs to a final score decision. This highlights the benefits of directly training to generate in a
structured format as P ROMETHEUS . On the other hand, we also find that proprietary LLMs such as
GPT-3.5-Turbo and GPT-4 excel at generating structured outputs when the prompt is adeptly given.
Also, note that we found that if we set the temperature to 0.0, evaluator LMs are not able to generate
meaningful feedback compared to using a temperature of 1.0.
D T RAINING A EVALUATOR LM ONCOARSE -GRAINED SCORE RUBRICS
For the purpose of exploring the benefits of training on thousands of fine-grained and customized
score rubrics, we employ a baseline of only training on relatively coarse-grained score rubrics.
Since the F EEDBACK COLLECTION ’s instructions are closely tied with the score rubrics during its
creation process, we could not directly use it and only change the score rubrics into coarse-grained
ones.
So, we used the Flask dataset (Ye et al., 2023b) and split it into training data and evaluation data.
The evaluation data is denoted as Flask Eval throughout the paper. Specifically, the Flask dataset
consists of 1.7K instructions acquired across conventional NLP datasets and instruction datasets.
Also, there exists 76.5K responses acquired across 15 response LMs. Each instance has a score
rubric among 12 options (Logical Robustness, Logical Correctness, Logical Efficiency, Factuality,
Commonsense Understanding, Harmlessness, Readability, Comprehension, Insightfulness, Com-
pleteness, Metacognition, Conciseness). While these 12 score rubrics are more fine-grained and di-
verse compared to previous works only using helpfulness and harmlessness, they are coarse-grained
compared to the thousands of score rubrics included within the F EEDBACK COLLECTION , so we
denote as coarse-grained in this work.
Among the 1.5K instructions & 67.5K responses as training data, we found that the score distribution
is extremely skewed towards the score of 5. We distributed the instances so that the number of
instances within the score range of 1 to 5 remains equal, which leads to 30K training instances. We
trained the Llama-2-Chat model on the Flask train set, which led to one of our baselines denoted as
LLAMA -2-C HAT + C OARSE in Table 3, Table 4, Table 5, Table 6.
E P AIRWISE COMPARISON OF THE QUALITY OF THE FEEDBACK
In this section, we further explain the experimental setting and present additional results & analysis
for the experiment of comparing the quality of the generated feedback (Section 5.1).
In addition to Figure 6, the reason why each annotator rejected the feedback from either
PROMETHEUS , GPT-3.5-Turbo, GPT-4 is shown in Figure 11 and Figure 12.
22

--- PAGE 23 ---
Figure 11: The reason why GPT-3.5-Turbo’s or Prometheus’s feedback was not chosen over the
other. P ROMETHEUS generates less abstract and general feedback, but tends to write overly critical
ones.
Figure 12: The reason why GPT-4’s or GPT-3.5-Turbo’s feedback was not chosen over the other.
GPT-4 generates less abstract and general feedback, but tends to write overly critical ones.
The results further support our claim that P ROMETHEUS tends to be critical over GPT-4 and GPT-
3.5-Turbo. Interestingly, GPT-4 was considered to be more critical compared to GPT-3.5-Turbo and
the gap was even wider when comparing GPT-3.5-Turbo and P ROMETHEUS . This indicates that
PROMETHEUS can serve as a critical judge when evaluating responses generated by LLMs, but it
could also be biased towards not being optimistic generally. The degree of being critical could be
useful or a limitation based on different use cases. For instance, we conjecture that it could be helpful
when analyzing the limitations of LLMs or providing feedback as supervision to further improve a
target LLM (e.g., RLHF), yet we leave this exploration to future work.
23

--- PAGE 24 ---
Figure 13: Box and whisker plot describing a relationship between a given response and its corre-
sponding score. We check if the response lengths correlate with its scores.
Figure 14: Box and whisker plot describing a relationship between a given response and its corre-
sponding score. We check if the response lengths correlate with its scores.
Figure 15: Box and whisker plot describing a relationship between a given response and its corre-
sponding score. We check if the response lengths correlate with its scores.
24

--- PAGE 25 ---
F I S THERE A LENGTH BIAS DURING EVALUATION ?
One of the limitations of employing an LLM as an evaluator LM is that it could be vulnerable to
various biases. In this work, we train/test on an Absolute Scoring evaluation setting, hence there
exists no position bias. Yet, it is crucial to analyze whether P ROMETHEUS showed any bias towards
favoring longer responses. Hence, we conduct a comprehensive analysis in this section.
As shown in Figure 13, Figure 14, and Figure 15, both GPT-4 and P ROMETHEUS and GPT-4 shows
a similar trend of not favoring longer responses (i.e., similar length distribution among different
scores). However, as mentioned in Zheng et al. (2023), LLM evaluators might favor more verbose
responses, yet the responses from our test instances (F EEDBACK BENCH and Vicuna Bench) did not
include any adversarial examples to test this phenomenon. More extensive research on whether the
length bias is also transferred to fine-tuned evaluator LMs should be explored in future work.
G P ROMPT FOR FEEDBACK COLLECTION CREATION
In this section, we provide the extensive list of prompts used to create the F EEDBACK COLLECTION .
Note that in the prompt of generating a response and a feedback, we use the sentence length of
the reference answer and append it to “ {SENT NUM }” within the prompt. This was crucial to
make the length even across different scores as shown in Figure 9. Also, note that for the 1K
score rubrics, 20K instructions & reference answers, and 100K responses & feedback within the
FEEDBACK COLLECTION , each prompt was sequentially used. In early experiments, we found that
generating every component all at once leads to very poor generation quality (i.e., similar responses
& feedback across different score ranges). Yet, we found that grouping (1) the instruction and
reference answer generation and (2) the response and feedback generation had a positive synergy,
leading to better generation quality and less amount of cost. Also, to the best of our knowledge, we
are first to explore acquiring negative and neutral responses (Score 1 ∼4 responses) through GPT-4
augmentation. We hope future work could also explore applying this strategy to different use cases.
Prompt for Brainstorming New Score Rubrics
We are brainstorming criteria with which to grade a language model on its responses in
diverse situations.
A ‘criteria‘ is some useful, real-world objective, and associated rubric for scores 1-5, that
tests a capability.
Here you will see 4 examples of ‘criteria‘, and their scoring rubrics, formatted as
JSON.
Criteria 1:
{JSON LIST 1 }
Criteria 2:
{JSON LIST 2 }
Criteria 3:
{JSON LIST 3 }
Criteria 4:
{JSON LIST 4 }
Please brainstorm a new criteria and scoring rubrics.
Be creative and create new but useful criteria that people in different settings or industries
might find practical.
Please format the output as same as the above examples with no extra or surrounding text.
Write [END] after you are done.
New Criteria:
25

--- PAGE 26 ---
Prompt for Paraphrasing as a New Score Rubric
Please paraphrase the sentences inside the dictionary below.
Each paraphrase should not change the meaning or substance of the original sentence, be
naturally written, but sufficiently diverse from one another.
Diversity can come from differences in diction, phrasing, sentence structure, formality,
detail, and/or other stylistic changes.
The dictionary:
{CRITERIA }
Respond with only dictionary (same format as the given dictionary) with no extra or
surrounding text.
Write [END] after you are done.
Dictionary with Paraphrased Sentences:
26

--- PAGE 27 ---
Prompt for Generating an Instruction and Reference Answer
Your job is to generate a new novel problem and a response that is related to the given score
rubric.
The score rubric:
{CRITERIA }
* Problem
- The problem should inherently be related to the score criteria and score rubric given above.
Specifically, the score criteria should be the core attributes required to solve the problem.
- The problem itself should not be too generic or easy to solve.
- If the score rubric is related to logical abilities, generate problems that require math or
coding abilities.
- Try to make the person who might solve the problem not notice the existence of the score
rubric by not explicitly mentioning it, and also provide additional inputs and options if
needed.
- Assume a situation where a user is interacting with an AI model. The user would try to
ask in a first-person point of view, but not using terms like ”I”, ”A User” or ”You” in the
first sentence.
- Do not give a role to the AI, assume that the user is asking a question from his point of
view.
- Do not include any phrase related to AI model in the problem.
* Response
- The response should be a response that would get a score of 5 from the score rubric.
- The response should be as detailed as possible unless the score rubric is related to
conciseness or brevity. It should consist of multiple paragraphs, a list of items, or a
step-by-step reasoning process.
- The response should look like how a well-prompted GPT-4 would normally answer your
problem.
* Format
- DO NOT WRITE ANY GREETING MESSAGES, just write the problem and response
only.
- In front of the problem, append the phrase ”Problem:” and in front of the response, append
the phrase ”Response:”.
- Write in the order of ”Problem” - ”Response”, where the two items are separated by the
phrase ”[NEXT]”.
- Write [END] after you are done.
Data Generation:
27

--- PAGE 28 ---
Prompt for Generating Responses and Feedback
Your job is to generate a response that would get a score of {SCORE }and corresponding
feedback based on the given score rubric. For reference, a reference response that would get
a score of 5 is also given.
Instruction:
{INSTRUCTION }
The score rubric:
{CRITERIA }
Reference response (Score 5):
{REFERENCE }
* Response
- The quality of the score {SCORE }response should be determined based on the score
rubric, not by its length.
- The score {SCORE }response should have the same length as the reference response,
composed of {SENT NUM }sentences.
- Do not explicitly state the keywords of the score rubric inside the response.
* Feedback
- The score {SCORE }feedback should each be an explanation of why the response would
get a score of {SCORE }. It should be written based on the generated response and score
rubric.
- The score {SCORE }feedback shouldn’t just copy and paste the score rubric, but it should
also give very detailed feedback on the content of the corresponding response.
- The score {SCORE }feedback should include the phrase ”So the overall score is
{SCORE }” in the last sentence.
* Format
- DO NOT WRITE ANY GREETING MESSAGES, just write the problem and response
only.
- In front of the response, append the phrase ”Response:” and in front of the feedback,
append the phrase ”Feedback:”.
- Write in the order of ”Response” - ”Feedback”, where the two items are separated by the
phrase ”[NEXT]”.
- Write [END] after you are done.
Data Generation:
28

--- PAGE 29 ---
H P ROMPT USED FOR PROMETHEUS
In this section, we provide the prompt used for training/inferencing P ROMETHEUS . Note that after
applying the prompt template shown below, we also apply Llama-2’s basic conversation prompt
template in order to minimize the discrepancy between the training process of Llama-2 and training
on the F EEDBACK COLLECTION .
Prompt for Prometheus
###Task Description:
An instruction (might include an Input inside it), a response to evaluate, a reference answer
that gets a score of 5, and a score rubric representing an evaluation criterion is given.
1. Write a detailed feedback that assesses the quality of the response strictly based on the
given score rubric, not evaluating in general.
2. After writing a feedback, write a score that is an integer between 1 and 5. You should
refer to the score rubric.
3. The output format should look as follows: ¨Feedback: (write a feedback for criteria)
[RESULT] (an integer number between 1 and 5)¨
4. Please do not generate any other opening, closing, and explanations.
###The instruction to evaluate:
{instruction }
###Response to evaluate:
{response }
###Reference Answer (Score 5):
{reference answer }
###Score Rubrics:
[{criteria description }]
Score 1: {score1 description }
Score 2: {score2 description }
Score 3: {score3 description }
Score 4: {score4 description }
Score 5: {score5 description }
###Feedback:
29

--- PAGE 30 ---
I Q UALITATIVE EXAMPLES OF GENERATED FEEDBACK
Figure 16, Figure 17, Figure 18, Figure 19 shows a qualitative example of feedback generated by
either GPT-4, P ROMETHEUS (13B), and Code-Llama trained on the F EEDBACK COLLECTION .
J E XPERIMENTAL DETAILS FOR HUMAN EVALUATION
The user interface used for human evaluation is shown in Figure 20. In order to acquire a score
decision for the response, a decision of which feedback is better, and an annotation of why they
made a decision to choose one of the feedback, we constructed the user interface in sequential order.
Each annotator was informed not to solve the questions below before solving the ones above them.
The test instances were randomly selected among F EEDBACK BENCH (Customized Unseen Rubric
set), Vicuna Bench, and MT Bench except for math and coding instances. The experiments were
conducted on LabelStudio, hosted on Huggingface Spaces. We asked each annotator to spend at
least 3 hours annotating 45 instances each and paid $ 50. The 9 annotators were divided into 3
groups: (1) P ROMETHEUS vsGPT-4, (2) P ROMETHEUS vsGPT-3.5-Turbo, and (3) GPT-4 vsGPT-
3.5-Turbo. The order of the feedback was randomly decided in order to prevent order bias during
annotation.
30

--- PAGE 31 ---
Pr omet heus's F eedbackR esponse fr om ChatGPT
GPT -4's F eedback
Her e is a dialogue betw een an assistant and an user .
Generat e a human pr ef erable r esponse b y t he assistant.

 User: One morning aft er sunrise, Sur esh was standing f acing a pole. The shado w of t he pole f ell 
e xact ly t o his right. Can y ou t ell me t he dir ection t o war ds which t he shado w was pointing - east, sout h, 
w est, or nor t h? Explain y our r easoning st eps.  
 Assistant:Based on t he giv en inf ormation, Sur esh was standing f acing a pole, and t he shado w of t he 
pole f ell e xact ly t o his right. T o det ermine t he dir ection t o war ds which t he shado w was pointing, w e 
need t o consider t he mo v ement of t he sun during t he da y . \n \n The sun rises in t he east and set s in t he 
w est. In t he morning, when Sur esh was standing f acing t he pole, t he sun w ould ha v e been in t he east ern 
sky . Since t he shado w of t he pole f ell e xact ly t o Sur esh's right, it means t hat t he shado w was pointing 
t o war ds t he w est.  
Ther ef or e, t he dir ection t o war ds which t he shado w was pointing is w est.  
 User: T o which dir ection was Sur esh f acing? Ho w do y ou solv e t his?  
 Assistant:

Apologies f or t he confusion.  
T o det ermine t he dir ection Sur esh was f acing, w e need t o consider t he position of t he shado w in r elation t o 
him. If t he shado w f ell e xact ly t o his right, it means t hat Sur esh was f acing t o war ds t he w est.  
The shado w is f ormed when an object blocks t he light fr om t he sun. In t his case, since t he shado w is t o 
Sur esh's right, it indicat es t hat t he sun is in fr ont of him, casting t he shado w t o war ds his back.  
Ther ef or e, Sur esh was f acing t o war ds t he east.Does t he model demonstr at e an ability t o logically pr ocess and int erpr et giv en 
inf ormation,  apply r ele v ant kno wledge,  and pr esent an insightful conclusion?
Pr omet heus's F eedback
The model int erpr et s t he inf ormation fr om 
t he user accurat ely and applies t he 
concept of t he sun's position and it s eff ect 
on t he f ormation of shado ws. Ho w e v er , it 
f ails t o r each t he corr ect conclusion t hat 
Sur esh is f acing t he sout h. Inst ead, it 
incorr ect ly concludes t hat Sur esh is f acing 
t he east. The r esponse demonstrat es 
par tial understanding of t he pr oblem but 
lacks complet e logical coher ence in t he 
solution. 
The final scor e is 3 out of 5 .
The r esponse pr o vided b y t he model is 
inaccurat e and doesn't demonstrat e a clear 
understanding of t he giv en inf ormation and task. 
Accor ding t o t he user's quer y , Sur esh was 
standing f acing a pole, and t he shado w of t he 
pole f ell e xact ly t o his right. This implies t hat 
Sur esh was f acing t o war ds t he w est, not t he 
east as t he model suggest s. F ur t hermor e, t he 
model f ailed t o pr operly e xplain t he r easoning 
behind it s conclusion, leading t o an illogical 
conclusion t hat contradict s t he initial inf ormation 
pr o vided. The model didn't corr ect ly apply t he 
kno wledge of t he mo v ement of t he sun during 
t he da y , nor did it tak e int o account t he position 
of t he shado w in r elation t o Sur esh. 
The final scor e is 1 out of 5 .Scor e Rubric:InstructionFigure 16: An example of comparing the feedback generated by P ROMETHEUS and GPT-4. GPT-4
was labeled to generate an abstract feedback.
31

--- PAGE 32 ---
GPT -4's F eedbackInstructionWh y might someone pr ef er t o shop at a small, locally-o wned business 
inst ead of a lar ge chain st or e, e v en if t he prices ar e higher?Scor e Rubric
Does t he R esponse r epr esent a balanced vie w of t he f act ors?
R esponse fr om Vicuna 13BTher e ar e se v eral r easons wh y someone might pr ef er t o shop at a small, locally-o wned business inst ead of a lar ge 
chain st or e, e v en if t he prices ar e higher:  
1 . Suppor ting t he local econom y: By shopping at a small, locally-o wned business, y ou ar e suppor ting t he local 
econom y and helping t o k eep mone y wit hin t he community . 
2. P ersonaliz ed ser vice: Small, locally-o wned businesses oft en off er personaliz ed ser vice and att ention t hat ma y not 
be a v ailable at a lar ge chain st or e. The staff ma y be mor e kno wledgeable about t he pr oduct s t he y sell and ma y be 
mor e willing t o help y ou find what y ou'r e looking f or . 
3 . Unique pr oduct s: Small, locally-o wned businesses oft en carr y unique pr oduct s t hat y ou ma y not be able t o find at 
a lar ge chain st or e. This can be especially true f or businesses t hat specializ e in niche pr oduct s or t hat off er cust om-
made it ems. 
4 . Community in v olv ement: Small, locally-o wned businesses ar e oft en mor e in v olv ed in t he local community t han 
lar ge chain st or es. The y ma y sponsor local e v ent s, suppor t local charities, and be mor e connect ed t o t he community 
as a whole. 
5 . Bett er cust omer ser vice: Small, locally-o wned businesses oft en ha v e a mor e personal r elationship wit h t heir 
cust omers and ma y be mor e willing t o go abo v e and be y ond t o ensur e cust omer satisf action. 
6 . En vir onmental and et hical considerations: Some people pr ef er t o shop at small, locally-o wned businesses because 
t he y belie v e t hat t hese businesses ar e mor e lik ely t o ha v e en vir onmentally and socially r esponsible practices.  
Ov erall, shopping at a small, locally-o wned business can be a wa y t o suppor t t he local community , get personaliz ed 
ser vice, and find unique pr oduct s t hat y ou ma y not be able t o find at a lar ge chain st or e.  
The r esponse has done an e x cellent job of 
pr esenting a balanced view of t he f act ors t hat 
might mak e someone pr ef er shopping at a small, 
locally-o wned business inst ead of a lar ge chain 
st or e, e v en if t he prices ar e higher . Six diff er ent 
r easons w er e giv en, each of t hem e xplained 
t hor oughly and in dept h. No one r eason was 
giv en mor e emphasis t han t he ot hers, and each 
e xplanation was f air and unbiased. Ho w e v er , it's 
w or t h noting t hat t he r esponse f ailed t o consider 
f act ors t hat might f a v or lar ge chain st or es, as 
t he r ef er ence answ er did.

The final scor e is 4 out of 5 .The r esponse does a good job of pr esenting t he f act ors 
of wh y someone might pr ef er t o shop at a small, locally-
o wned business, including suppor ting t he local econom y , 
personaliz ed ser vice, unique pr oduct s, community 
in v olv ement, bett er cust omer ser vice, and en vir onmental 
and et hical considerations. Ho w e v er , t he r esponse could 
impr o v e in pr esenting all f act ors wit h equal rigor . 

F or e xample, t he r esponse does not delv e deeply enough 
int o t he specifics of ho w shopping at a small, locally-
o wned business could suppor t t he local econom y , nor 
does it e xplain ho w t hese businesses might be mor e 
in v olv ed in t he community compar ed t o lar ge chain 
st or es. The r esponse could also fur t her discuss ho w 
t hese f act ors might not be as impor tant t o all consumers, 
similar t o ho w t he r ef er ence r esponse ackno wledges t he 
impor tance of f act ors lik e price, con v enience, and 
pr oduct range in lar ge chain st or es. 

The final scor e is 4 out of 5 .Pr omet heus's F eedbackFigure 17: An example of comparing the feedback generated by P ROMETHEUS and GPT-4.
PROMETHEUS was labeled to generate an overly critical feedback.
32

--- PAGE 33 ---
R esponse fr om Simulat ed GPT -4
GPT -4's F eedbackInstruction
Contextual Language Adaptation
Y our e xplanation of phot osynt hesis is decent 
and uses analogies t o mak e some aspect s 
mor e r elatable t o a non-e xper t, such as a 
middle school student. Ho w e v er , t her e ar e 
still ar eas t hat could be challenging f or t hem. 
The use of t he t erms 'biochemical r eaction' , 
'chlor oph yll' , 'chlor oplast s' , and t he chemical 
f ormulas f or wat er , carbon dio xide, and 
glucose could confuse a non-e xper t, making 
it har der f or t hem t o grasp t he o v erall 
concept. Inst ead of diving straight int o t he 
comple x aspect s of phot osynt hesis, y ou 
might ha v e star t ed wit h a simpler and mor e 
r elatable analogy . Additionally , pr o viding a 
st ep-b y-st ep description of t he pr ocess 
w ould ha v e made y our e xplanation mor e 
digestible. 

So t he o v erall scor e is 3 .
Pr omet heus's F eedbackThe e xplanation giv en in t his r esponse is 
generally understandable, but t her e ar e 
se v eral t erms and concept s t hat might be 
difficult f or a middle school student t o grasp. 
F or instance, t erms lik e "chlor oph yll, " 
"chlor oplast s" , "phot osynt hesis" , "solar 
ener gy" , "glucose" , and "o xy gen" ar e used 
wit hout sufficient cont e xtual e xp`lanations. 
While t he main ideas of phot osynt hesis ar e 
con v e y ed, t he language used could pot entially 
be confusing f or a non-e xper t. Additionally , 
t he concept of plant s using sunlight, wat er , 
and carbon dio xide t o pr oduce glucose and 
o xy gen is described but lacks concr et e, 
r elatable analogies t hat w ould ha v e helped 
simplify t he e xplanation. The e xplanation 
could be impr o v ed b y using simpler language 
and incorporating mor e accessible e xamples 
or metaphors t o help illustrat e t he pr ocess. 

So t he o v erall scor e is 3 .
Explain t he pr ocess of phot osynt hesis in plant s in a wa y t hat a middle school student 
could understand.Phot osynt hesis, essentially , is a biochemical r eaction in which plant s use sunlight (solar ener gy), 
wat er (H2O), and carbon dio xide (CO2) t o pr oduce glucose (C6H12O6) and o xy gen (O2). 

It's t he k e y met hod t hat plant s use f or t heir nutritional r equir ement s. T o begin wit h, plant s captur e 
solar ener gy wit h t he chlor oph yll in t heir lea v es. 

Then, wit h t he absorbed solar ener gy , t he plant's chlor oplast s tak e wat er fr om t he gr ound and CO2 
fr om t he atmospher e t o per f orm t he comple x phot osynt hesis pr ocess. 

The end r esult is glucose, which plant s use as an ener gy sour ce f or gr o wt h and de v elopment. 
Additionally , plant s emit o xy gen, which is beneficial f or us as it's necessar y f or our r espiration. 

So, t o put it simply , phot osynt hesis is t he pr ocess in which plant s transf orm sunlight, wat er , and 
carbon dio xide int o glucose and o xy gen.Scor e Rubric:Can t he model's r esponse be underst ood b y a non-e xper t in t he subject?Figure 18: An example of comparing the feedback generated by P ROMETHEUS and GPT-4. Both
evaluator LM generated a good feedback, and hence was labeled as a tie.
33

--- PAGE 34 ---
R esponse fr om Wizar d LM 13BInstructionDe v elop a C++ pr ogram t hat r eads a t e xt file line b y line and count s t he number 
of occurr ences of a specific w or d in t he file.Her e is a C++ pr ogram t hat r eads a t e xt file line b y line and count s t he number of occurr ences of a specific w or d in t he 
file: 
``` 
#include <iostr eam> 
#include <f str eam> 
#include <string>  
using namespace st d; 
int main() { 
    // File name 
    string filename = \ "e xample.txt \ ";  
    // If t he file does not e xist, cr eat e it 
    if (!e xist s(filename)) { 
        of str eam out(filename); 
        out << \ "No such file.\ " << endl; 
    }  
    // Open t he file in r ead mode 
    if str eam in(filename, ios::in); 
    // Count t he number of occurr ences of t he w or d 
    int count = 0; 
    string w or d = \ "y our_w or d _her e \ ";  
     
    // R eplace wit h t he w or d y ou want t o count 
    while (in >> w or d) { 
        if (w or d == w or d) { 
            count ++; 
        } 
    } 
    // Print t he r esult 
    cout << \ "The w or d \ " << w or d << \ " appears \ " << count << \ " times in t he file.\ " << endl;  
    // Close t he file 
    in.close();  
    r eturn 0; 
} 
``` 
Her e's ho w t he pr ogram w orks: 
1 . The first f ew lines include t he necessar y header files and define t he `main()` function. 
2. The `filename` v ariable is defined t o st or e t he name of t he file t o be r ead. 
3 . The pr ogram checks if t he file e xist s and cr eat es it if it does not. 
4 . The file is opened in r ead mode using t he `if str eam` class. 
5 . The pr ogram ent ers a loop t hat r eads each line of t he file. 
6 . Inside t he loop, t he pr ogram checks if t he curr ent line contains t he w or d t o be count ed. If it does, t he `count` v ariable 
is incr ement ed. 
7 . Aft er t he loop e xit s, t he pr ogram print s t he r esult t o t he console. 
8 . Finally , t he file is closed using t he `close()` met hod of t he `if str eam` object.  
T o use t his pr ogram, simply r eplace t he `filename` v ariable wit h t he name of t he file y ou want t o r ead, and run t he 
pr ogram. The output will be t he number of times t he specified w or d appears in t he file.Scor e Rubric
Ho w w ell is t he implementation in t erms of it s corr ectness,  efficiency ,  documentation,  and 
err or handling?34

--- PAGE 35 ---
Pr omet heus's F eedback
The implementation pr o vided is not fully corr ect. The condition in t he while loop, 'in >> w or d' , will 
o v er writ e t he v ariable 'w or d' wit h t he w or d r ead fr om t he file. This mak es t he f ollo wing 'if' 
stat ement t o alwa ys e v aluat e t o true. Ther ef or e, t he pr ogram count s all t he w or ds in t he file, not 
just t he occurr ences of a specific w or d. Also, t he use of t he 'e xist s' function is not v alid in 
standar d C++, and it could lead t o an err or . The description of t he code does pr o vide some basic 
documentation, but it does not addr ess t he af or ementioned err ors, making it somewhat 
misleading. Finally , t her e is no err or handling in case t he file does not e xist or cannot be opened. 
Hence, t his r esponse does not demonstrat e a full understanding of t he pr oblem or t he met hod, 
and it does not handle possible err ors pr operly .

The final scor e is 3 out of 5 .GPT -4's F eedback
The pr o vided C++ pr ogram corr ect ly implement s a solution t o r ead a t e xt file line b y line and 
count t he number of occurr ences of a specific w or d. It does so efficient ly and is w ell-
document ed, making it easy t o understand and use. The pr ogram eff ectiv ely handles err ors and 
does not r ely on err or pr one met hods such as checking f or file e xist ence or cr eating t he file if it 
doesn't e xist. The code is w ell-or ganiz ed and w ell-structur ed, making it easy t o f ollo w and 
maintain. So t he o v erall scor e is 5 .Code-Llama (7B) Fine-tuned on t he F eedback Collection
F eedback: \n \n The pr o vided r esponse meet s some of t he crit eria in t he scor e rubric, but t her e ar e 
ar eas f or impr o v ement. Her e ar e some specific f eedback point s:

* Corr ectness: The r esponse corr ect ly implement s t he task of counting t he number of 
occurr ences of a specific w or d in a file. Ho w e v er , t he code could benefit fr om additional err or 
handling and handling of edge cases, such as dealing wit h files t hat do not e xist or containing 
in v alid data. (Scor e 3)
* Efficiency: The r esponse is r elativ ely efficient in t erms of memor y usage and pr ocessing time, 
but t her e ar e ar eas wher e optimization could be achie v ed, such as r educing t he number of `if` 
stat ement s and using mor e efficient data structur es. (Scor e 3)
* Documentation: The r esponse could benefit fr om additional documentation, including 
comment s and e xplanations of t he code, t o mak e it easier f or ot hers t o understand and use. 
(Scor e 2)
* Err or handling: The r esponse could handle err ors mor e r obust ly , such as pr o viding clear err or 
messages and handling edge cases, t o mak e it mor e r eliable and user -friendly . (Scor e 3)

Ov erall, t he r esponse scor es a 3 out of 5 in t erms of t he e v aluation crit eria.Figure 19: An example of comparing the feedback generated by P ROMETHEUS , GPT-4, and Code-
Llama trained on the F EEDBACK COLLECTION . Compared to P ROMETHEUS , using a base model
specialized on the code domain also helps to criticize and evaluate responses within the code domain.
35

--- PAGE 36 ---
36

--- PAGE 37 ---
Figure 20: The annotation user interface for labeling the human scores, pairwise comparison of the
two feedback and gathering labels of why one feedback was preferred over the other one.
37
