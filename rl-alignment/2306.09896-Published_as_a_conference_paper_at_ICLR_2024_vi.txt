# 2306.09896.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2306.09896.pdf
# Kích thước tệp: 3023070 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
LIỆU TỰ SỬA CHỮA CÓ PHẢI LÀ VIÊN ĐẠN BẠC CHO
VIỆC TẠO MÃ ?
Theo X. Olausson1,†,Jeevana Priya Inala2,Chenglong Wang2,
Jianfeng Gao2,Armando Solar-Lezama1
1MIT CSAIL2Microsoft Research
TÓM TẮT
Các mô hình ngôn ngữ lớn đã cho thấy năng lực đáng chú ý trong việc tạo mã, nhưng
vẫn gặp khó khăn trong việc thực hiện các tác vụ phức tạp. Tự sửa chữa—trong đó mô hình
gỡ lỗi và sửa chữa mã của chính nó—gần đây đã trở thành một cách phổ biến để nâng cao
hiệu suất trong những bối cảnh này. Tuy nhiên, mặc dù ngày càng phổ biến, các nghiên cứu
hiện có về tự sửa chữa còn hạn chế về phạm vi; trong nhiều bối cảnh, hiệu quả của nó vẫn
chưa được hiểu rõ. Trong bài báo này, chúng tôi phân tích khả năng thực hiện tự sửa chữa
của Code Llama, GPT-3.5 và GPT-4 trên các bài toán từ HumanEval và APPS. Chúng tôi
phát hiện rằng khi chi phí thực hiện sửa chữa được tính vào, lợi ích về hiệu suất thường
khiêm tốn, thay đổi nhiều giữa các tập con của dữ liệu, và đôi khi hoàn toàn không có mặt.
Chúng tôi đưa ra giả thuyết rằng điều này là do tự sửa chữa bị nghẽn cổ chai bởi khả năng
cung cấp phản hồi về mã của chính mô hình; sử dụng một mô hình mạnh hơn để tăng cường
nhân tạo chất lượng phản hồi, chúng tôi quan sát được những cải thiện hiệu suất đáng kể hơn.
Tương tự, một nghiên cứu quy mô nhỏ trong đó chúng tôi cung cấp cho GPT-4 phản hồi từ
các tham gia viên con người cho thấy rằng ngay cả đối với các mô hình mạnh nhất, tự sửa
chữa vẫn thua kém xa so với những gì có thể đạt được với khả năng gỡ lỗi ở mức con người.

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) đã chứng minh khả năng tạo ra các đoạn mã từ các đặc tả
ngôn ngữ tự nhiên, nhưng vẫn gặp khó khăn với các thử thách lập trình phức tạp như những
thử thách trong các cuộc thi và phỏng vấn kỹ thuật phần mềm chuyên nghiệp. Các nghiên cứu
gần đây đã tìm cách cải thiện hiệu suất bằng cách tận dụng tự sửa chữa (Gupta et al., 2020;
Le et al., 2022; Chen et al., 2023b; Zhang et al., 2023), trong đó mô hình tự xem xét và sửa
chữa các lỗi trong mã của chính nó. Hình 1 cho thấy một quy trình làm việc điển hình. Đầu
tiên, một chương trình được lấy mẫu từ một mô hình tạo mã; chương trình này sau đó được
chạy trên một bộ các bài kiểm tra đơn vị được cung cấp như một phần của đặc tả; nếu chương
trình thất bại ở bất kỳ bài kiểm tra nào, thì thông báo lỗi và chương trình bị lỗi được đưa
cho một mô hình tạo phản hồi, mô hình này xuất ra một giải thích ngắn gọn về lý do tại sao
mã thất bại; cuối cùng, phản hồi được chuyển cho một mô hình sửa chữa, mô hình này tạo ra
một phiên bản đã sửa của chương trình.1 Trên bề mặt, đây là một ý tưởng rất hấp dẫn. Nó
cho phép hệ thống vượt qua các lỗi do các mẫu không may mắn trong quá trình giải mã; dễ
dàng tích hợp phản hồi trong giai đoạn sửa chữa từ các hệ thống ký hiệu như trình biên dịch,
công cụ phân tích tĩnh và engine thực thi; và bắt chước cách thử và sai mà các kỹ sư phần
mềm con người viết mã.

Tuy nhiên, quan trọng là phải nhớ rằng tự sửa chữa đòi hỏi nhiều lần gọi mô hình hơn, do đó
tăng chi phí tính toán. Đặc biệt, việc tự sửa chữa có phải là một chiến lược thắng lợi hay không
cuối cùng phụ thuộc vào việc bạn có—với một ngân sách tính toán tương đương—có cơ hội
thành công lớn hơn nếu bạn chỉ đơn giản rút thêm các mẫu mã i.i.d. từ mô hình và kiểm tra
chúng với bộ các bài kiểm tra đơn vị được cung cấp như một phần của tác vụ. Quan trọng,
trong một bối cảnh lập trình cạnh tranh, hiệu quả của tự sửa chữa không chỉ phụ thuộc vào
khả năng tạo mã của mô hình, điều đã được nghiên cứu rộng rãi trong tài liệu, mà còn phụ
thuộc vào khả năng xác định cách mã (được tạo bởi chính mô hình) sai so với đặc tả tác vụ.
Theo như chúng tôi biết, không có nghiên cứu nào trước đây đã nghiên cứu tác động của giai
đoạn này một cách chi tiết.

1Trong thực tế, việc tạo phản hồi và sản xuất mã đã sửa có thể được thực hiện thông qua một tương tác
duy nhất với mô hình; như chúng ta sẽ thấy, vẫn có thể hữu ích khi xem chúng như các bước riêng biệt về
mặt khái niệm.
†Liên hệ với theoxo@csail.mit.edu. Công việc được thực hiện một phần khi T.X.O. làm việc tại Microsoft
Research. Mã và dữ liệu có sẵn tại github.com/theoxo/self-repair.
1arXiv:2306.09896v5 [cs.CL] 2 Feb 2024

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
assert f(x3) == y3
Người dùng(1)
Mô hình Mãassert f(x1) == y1
assert f(x2) == y2
𐄂✓
✓
Thực thiassert f(x3) == y3assert f(x1) == y1
assert f(x2) == y2✓
✓
✓(2) (3)
Mô hình Phản hồi(4)
Mô hình Mã(5)

Cho một chuỗi s đại diện cho thứ trong tuần
hôm nay. s là một trong SUN,
MON, TUE, WED, THU, FRI, hoặc SAT.
Sau bao nhiều ngày là Chủ nhật tiếp theo
(ngày mai hoặc sau đó)?
# BÀI KIỂM TRA ĐƠN VỊ
# (CÓ THỂ THỰC THI)
assertf('MON') == 6
assertf('WED') == 4
assertf('SUN') == 7(1)deff(s):
return(7 - ['SUN', ... , 'FRI', 'SAT'].index(s)) % 7(2)

Với đầu vào 'SUN', chương trình trả về 0, nhưng đầu ra mong đợi là 7. (3)
Mã không tính đến trường hợp đầu vào là 'SUN' và đầu ra phải
là 7. Điều này có thể được sửa bằng cách loại bỏ phép toán modulo.(4)
deff(s):
return(7 - ['SUN', ... , 'FRI', 'SAT'].index(s)) # % 7(5)

Hình 1: Tự sửa chữa với các mô hình mã và phản hồi riêng biệt. Đầu tiên, người dùng đưa ra một đặc tả dưới
dạng văn bản và một bộ các bài kiểm tra đơn vị (1). Sau đó, một mô hình mã (màu xanh) tạo ra một chương trình (2). Chương
trình được kiểm tra với các bài kiểm tra đơn vị bằng engine thực thi ký hiệu, và một thông báo lỗi
được trả về (3). Để cung cấp nhiều tín hiệu hơn cho mô hình mã, phản hồi văn bản về lý do tại sao điều này
xảy ra được cung cấp bởi một mô hình phản hồi (màu vàng; 4). Cuối cùng, phản hồi này được sử dụng bởi mô hình mã để sửa chữa chương trình (5).

Đóng góp: Trong bài báo này, chúng tôi điều tra hiệu quả của các kỹ thuật tự sửa chữa được áp dụng cho
CodeLlama-13b-instruct (Rozière et al., 2023), GPT-3.5 (Ouyang et al., 2022; OpenAI, 2022),
và GPT-4 (OpenAI, 2023) cho các tác vụ lập trình Python độc lập. Chúng tôi tập trung vào việc đánh giá
khả năng của các mô hình trong việc suy ngẫm, cung cấp phản hồi và gỡ lỗi mã. Chúng tôi quan sát rằng:

•Tự sửa chữa không phải là viên đạn bạc: khi chi phí sửa chữa được tính vào, chúng tôi tìm thấy một số
trường hợp trong đó tỷ lệ vượt qua cao hơn hoặc bằng với việc lấy mẫu i.i.d. (không sửa chữa),
đặc biệt là khi ngân sách nhỏ. Chúng tôi suy đoán rằng điều này là do tỷ lệ tạo chương trình và
sửa chữa có xu hướng cùng nhau, và nhiều yếu tố tinh tế ảnh hưởng đến cái nào sẽ thắng thế với
cái kia cho một tác vụ nhất định (xem Phụ lục C).

•Tự sửa chữa có nhiều khả năng có lợi hơn khi nhiều ngân sách lấy mẫu được dành cho việc tạo ra
một tập hợp đa dạng các chương trình ban đầu hơn là thực hiện sửa chữa rộng rãi. Ví dụ, đối với GPT-4
trên APPS, rút 10 mẫu trước và sau đó 1 ứng viên sửa chữa mỗi cái (tổng cộng 20 mẫu)
dẫn đến tỷ lệ vượt qua cao hơn 1,05× so với pass@20 từ cùng mô hình không sửa chữa; rút 2
mẫu trước và sau đó rút 10 ứng viên sửa chữa mỗi cái (tổng cộng 22 mẫu) dẫn đến một
tỷ lệ vượt qua thấp hơn so với baseline pass@22 (0,97×).

•Tăng cường chất lượng phản hồi một cách nhân tạo cải thiện đáng kể hiệu quả của tự sửa chữa. Chúng
tôi thay thế phản hồi của Code Llama bằng phản hồi được sản xuất bởi GPT-3.5 hoặc GPT-4, và phản hồi của GPT-3.5
bằng của GPT-4; trong mọi trường hợp, cấu hình được tăng cường đánh bại cả
baseline i.i.d. tương ứng và cấu hình tự sửa chữa tương ứng ở tất cả ngân sách. Hơn nữa, thay thế
giải thích của chính GPT-4 bằng giải thích của một lập trình viên con người cải thiện sửa chữa đáng kể, tăng
phần trăm các chương trình được sửa chữa vượt qua các bài kiểm tra lên 1,58× (từ 33,3% lên 52,6%).

2 CÔNG TRÌNH LIÊN QUAN
Tổng hợp chương trình với các mô hình ngôn ngữ lớn. Việc sử dụng các mô hình ngôn ngữ lớn cho tổng hợp chương trình
đã được nghiên cứu rộng rãi trong tài liệu (Li et al., 2022; Austin et al., 2021; Chen et al.,
2021; Le et al., 2022; Fried et al., 2023; Nijkamp et al., 2023; Chowdhery et al., 2022; Touvron et al.,
2023; Li et al., 2023). Tài liệu này chủ yếu tập trung vào việc đánh giá các mô hình về
độ chính xác thô hoặc thông số pass@k (Kulal et al., 2019; Chen et al., 2021), thường tận dụng
các kỹ thuật lọc dựa trên thực thi (Li et al., 2022; Shi et al., 2022) hoặc xếp hạng (Chen et al., 2021;
Inala et al., 2022; Zhang et al., 2022) để giảm số lượng mẫu được xem xét cho
câu trả lời cuối cùng. Công trình của chúng tôi khác với một số công trình trong tài liệu này ở chỗ chúng tôi giả định có quyền truy cập vào
bộ sưu tập đầy đủ các ví dụ đầu vào-đầu ra, như thường được thực hiện trong tổng hợp quy nạp (Kitzelmann, 2010;

--- TRANG 3 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Polozov & Gulwani, 2015; Gulwani et al., 2017; Chen et al., 2019a; Ellis et al., 2021). Đặc biệt,
không giống như một số công trình trước đây (Li et al., 2022; Shi et al., 2022), chúng tôi không phân biệt giữa các bài kiểm tra công khai
được sử dụng để lọc và các bài kiểm tra riêng tư được sử dụng để xác định tính đúng đắn, vì phương pháp của chúng tôi không
liên quan đến việc lọc các đầu ra.

Sửa chữa mã. Sửa chữa mã thống kê và dựa trên học máy có lịch sử phong phú trong cả cộng đồng ngôn ngữ lập trình
và học máy, mặc dù nó chủ yếu được áp dụng cho mã được viết bởi con người trong bối cảnh kỹ thuật phần mềm (Long & Rinard, 2016; Bader et al., 2019;
Le Goues et al., 2021; Yasunaga & Liang, 2021; Chen et al., 2019b; Mesbah et al., 2019; Wang
et al., 2018). Gần đây hơn, việc sử dụng sửa chữa như một bước hậu xử lý để cải thiện mã được
tự động tổng hợp đã được sử dụng trong việc tổng hợp cả ngôn ngữ cụ thể miền
(Gupta et al., 2020) và mã mục đích chung (Le et al., 2022; Yasunaga & Liang, 2021; 2020). Đóng góp của chúng tôi
khác với hầu hết công trình trước đây trong tài liệu này ở việc sử dụng phản hồi văn bản để sửa chữa,
điều này có thể nhờ vào sự gia tăng đã đề cập ở trên trong việc sử dụng LLM cho tổng hợp chương trình.

Công trình đương đại về tự sửa chữa LLM. Có nhiều công trình đương đại tìm cách tự sửa chữa
với LLM, cả trong việc tạo mã và ngoài. Chúng tôi bây giờ làm nổi bật một số công trình này đặc biệt
gần với chúng tôi; xem Pan et al. (2023) để có một khảo sát hoàn chỉnh hơn về công trình gần đây trong lĩnh vực này
đang phát triển nhanh chóng. Zhang et al. (2023) khám phá tự sửa chữa không có phản hồi ngôn ngữ tự nhiên
trên APPS (Hendrycks et al., 2021) sử dụng cả các mô hình được tinh chỉnh và tự sửa chữa dựa trên prompt với
Codex (Chen et al., 2021), InCoder (Fried et al., 2023), và CodeGen (Nijkamp et al., 2023). Đáng chú ý,
khung của họ không xem xét chi phí liên quan đến phản hồi và sửa chữa, điều này trình bày một
quan điểm khác nhau đáng kể. Tương tự, Chen et al. (2023b) đánh giá khả năng tự sửa chữa của Codex
trên nhiều tác vụ khác nhau, trong một khung tương tự như cái chúng tôi nghiên cứu trong công trình này.
Tuy nhiên, nghiên cứu của họ khác với chúng tôi về các mô hình được xem xét và, quan trọng hơn,
mục tiêu nghiên cứu, vì chúng tôi đặc biệt nhằm điều tra tầm quan trọng của giai đoạn phản hồi văn bản.
Ngoài việc tạo mã, tự sửa chữa đã được sử dụng cho nhiều mục đích khác nhau, bao gồm
giảm thiểu ảo giác và cải thiện nền tảng thực tế trong trợ lý tìm kiếm (Peng et al., 2023) cũng như
tối ưu hóa mã và cải thiện khả năng đọc (Madaan et al., 2023). Cuối cùng, chúng tôi xem
công trình của mình, trong đó chúng tôi điều tra tầm quan trọng của giai đoạn phản hồi văn bản đặc biệt, là
bổ sung cho nghiên cứu đương đại tìm cách đánh giá tự sửa chữa trong bối cảnh rộng hơn; chúng tôi
mong muốn thấy những tác động của kết quả chúng tôi sẽ là gì trong các lĩnh vực khác này.

3 PHƯƠNG PHÁP
3.1 TỔNG QUAN TỰ SỬA CHỮA
Như được hiển thị trong Hình 1, chúng tôi mô hình hóa tự sửa chữa như bao gồm bốn giai đoạn: tạo mã, thực thi mã, tạo phản hồi, và sửa chữa mã. Chúng tôi bây giờ định nghĩa chính thức các giai đoạn khác nhau này.

Tạo mã. Được đưa ra một đặc tả ψ, một mô hình lập trình MP đầu tiên tạo ra np mẫu
i.i.d., mà chúng tôi ký hiệu
{pi}np
i=1i.i.d.∼MP(ψ)

Thực thi mã. Các np mẫu mã này sau đó được thực thi trên một testbed.2 Nếu bất kỳ mẫu p nào vượt qua
tất cả các bài kiểm tra—mà chúng tôi ký hiệu p|=ψ—chúng tôi dừng lại, vì một chương trình thỏa mãn đã được tìm thấy.
Nếu không, chúng tôi thu thập các thông báo lỗi {ei}i được trả về bởi môi trường thực thi. Các thông báo lỗi này
chứa thông tin lỗi biên dịch/runtime hoặc một ví dụ đầu vào mà đầu ra của chương trình khác với đầu ra mong đợi. Một ví dụ được hiển thị trong Hình 1 (thành phần 3).

Tạo phản hồi. Thông báo lỗi từ môi trường thực thi thường rất cấp cao,
cung cấp ít tín hiệu cho việc sửa chữa. Do đó, như một bước trung gian, chúng tôi sử dụng một mô hình phản hồi để
tạo ra một giải thích chi tiết hơn về những gì đã sai; Hình 1 (thành phần 4) cho thấy một ví dụ.
Chính thức, trong giai đoạn này, chúng tôi tạo ra nf chuỗi phản hồi, {fij}j, cho mỗi chương trình sai, pi, như
sau:
{fij}nf
j=1i.i.d.∼MF(ψ;pi;ei)

Có một bước tạo phản hồi rõ ràng cho phép chúng tôi loại bỏ thành phần này để chúng tôi có thể nghiên cứu
tầm quan trọng của nó một cách riêng lẻ.

2Chúng tôi giả định có quyền truy cập vào tập hợp đầy đủ các bài kiểm tra dưới dạng có thể thực thi; xem Phần 5 để thảo luận ngắn gọn về
tính hợp lệ của giả định này trong các lĩnh vực kỹ thuật phần mềm.

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
𝛙
...
... ...
... ... ... ...
Tạo MãPhản hồi
Sửa chữa

Hình 2: Một cây sửa chữa bắt đầu với một đặc tả ψ (nút gốc), sau đó phát triển thành các chương trình ban đầu
{pi}, phản hồi {fij}, và sửa chữa {rijk}.

Sửa chữa mã. Trong bước cuối cùng, cho mỗi chương trình ban đầu pi và phản hồi fij, nr ứng viên chương trình được sửa chữa
được lấy mẫu từ MP3:
{rijk}nr
k=1i.i.d.∼MP(ψ;pi;ei;fij)

Cây sửa chữa. Chúng tôi gọi cây văn bản và chương trình xen kẽ được tạo ra bởi quy trình này—bắt nguồn từ
đặc tả ψ, sau đó phân nhánh thành các chương trình ban đầu pi, mỗi chương trình phân nhánh thành phản hồi fij
và sau đó sửa chữa rijk—là một cây sửa chữa, T (Hình 2).

Lấy mẫu chung phản hồi và sửa chữa. Khung tổng quát được trình bày ở trên không yêu cầu
mô hình lập trình và mô hình phản hồi phải giống nhau, do đó cho phép sử dụng các mô hình chuyên biệt
trong hệ thống. Khi MP=MF, chúng tôi tạo chung cả phản hồi và chương trình được sửa chữa
trong một mẫu duy nhất từ mô hình; xem Phụ lục G để xem chi tiết cách prompt khác
giữa cài đặt này và cài đặt trước đó. Chính thức, chúng tôi ký hiệu điều này là
{(fij, rij)}nfr
j=1i.i.d.∼MP(ψ;pi;ei)

3.2 PASS@K CHO TỰ SỬA CHỮA
Trong tổng hợp chương trình không có tự sửa chữa, hiệu suất thường được đo bằng pass@k (Chen et al.,
2021; Kulal et al., 2019)—xác suất rằng ít nhất một trong k mẫu chương trình i.i.d. từ mô hình
thỏa mãn một đặc tả nhất định. Trong tự sửa chữa, các mẫu chương trình được rút từ mô hình cả trong
giai đoạn mẫu ban đầu và trong giai đoạn sửa chữa; do đó, chúng ta cần áp dụng pass@k để tính đến
số lượng mẫu từ cả hai giai đoạn.

Trong phần chính của công trình này, chúng tôi xem các cây sửa chữa T như chính chúng tạo thành các mẫu độc lập
từ một mô hình chung T∼M= (MP◦MF◦MP) và định nghĩa số chương trình trong cây
là |programs(T)|≜np+npnfr (hoặc |programs(T)|≜np+npnfnr); sau đó chúng tôi so sánh với một
baseline với k=|programs(T)| mẫu i.i.d. Chúng tôi tin rằng điều này sẽ làm cho các phát hiện của chúng tôi phù hợp nhất
với các nhà thực hành, những người có khả năng triển khai các agent tự sửa chữa với lấy mẫu theo lô. Phụ lục A
lặp lại các thí nghiệm của chúng tôi với hai chiến lược đánh giá thay thế, trong đó chúng tôi thay đổi chiến lược tìm kiếm
và đo chi phí lấy mẫu bằng tổng số token được lấy mẫu từ mô hình để tính đến độ dài khác nhau
của các mẫu phản hồi và chương trình. Quan trọng, mặc dù chi tiết khác nhau, các xu hướng tổng thể mà chúng tôi quan sát vẫn giống nhau.

Việc tạo ra một lượng lớn cây sửa chữa độc lập cho mỗi cài đặt của các siêu tham số
nhanh chóng trở nên không khả thi về mặt tính toán, vì vậy chúng tôi vẽ các ước tính bootstrap của tỷ lệ pass trong
các thí nghiệm của chúng tôi. Chúng tôi đầu tiên tạo ra một cây sửa chữa rất lớn duy nhất cho mỗi đặc tả tác vụ, với:
Np≥np mẫu chương trình ban đầu; Nf≥nf chuỗi phản hồi cho mỗi chương trình sai; và Nr≥nr
ứng viên sửa chữa cho mỗi chuỗi phản hồi. Được đưa ra một cài đặt của (np, nf, nr), chúng tôi sau đó lấy mẫu con (với
thay thế) Nt cây con sửa chữa khác nhau từ tập dữ liệu đông lạnh này và tính trung bình trên các lần chạy. Chúng tôi
sử dụng Np= 50 cho tất cả các thí nghiệm, và xem xét np≤25 cho các phương pháp tự sửa chữa và np≤50
cho phương pháp baseline, không sửa chữa. Tương tự, cho các chuỗi phản hồi, chúng tôi sử dụng Nf= 25 và

3Chúng tôi sử dụng cùng một mô hình cho cả việc tạo mã ban đầu và sửa chữa mã, vì đây là các tác vụ
tương tự về cơ bản.

--- TRANG 5 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
nf≤10 (ngoại trừ Phần 4.2, trong đó chúng tôi chỉ xem xét nf= 1 và do đó thỏa hiệp với Nf= 10
thay thế). Đối với các ứng viên sửa chữa, vì chúng tôi thực hiện lấy mẫu chung phản hồi và sửa chữa trong hầu hết
các thí nghiệm của chúng tôi, chúng tôi đặt Nr=nr= 1. Cuối cùng, chúng tôi sử dụng Nt= 1000 cho tất cả các cài đặt. Ước tính tỷ lệ pass
theo cách này giảm đáng kể chi phí tính toán của các thí nghiệm của chúng tôi, vì chúng tôi có thể tái sử dụng
cùng một tập dữ liệu ban đầu để tính toán các ước tính cho tất cả các lựa chọn khác nhau của np, nf, và nr.

4 THÍ NGHIỆM
Trong phần này, chúng tôi thực hiện các thí nghiệm để trả lời các câu hỏi nghiên cứu sau: (a) Trong bối cảnh
các câu đố lập trình Python, liệu tự sửa chữa có tốt hơn lấy mẫu i.i.d. không sửa chữa cho
các mô hình chúng tôi xem xét không? Nếu có, dưới các siêu tham số nào thì tự sửa chữa hiệu quả nhất? (b) Liệu
một mô hình phản hồi mạnh hơn có tăng cường hiệu suất sửa chữa của mô hình không? (c) Liệu việc giữ một con người trong
vòng lặp để cung cấp phản hồi có mở khóa hiệu suất sửa chữa tốt hơn ngay cả cho mô hình mạnh nhất?

Chúng tôi đánh giá các giả thuyết này cho hai mô hình được phục vụ API—GPT-3.5 (Ouyang et al., 2022; OpenAI,
2022) và GPT-44 (OpenAI, 2023)—cũng như CodeLlama-13b-instruct5 (Rozière et al., 2023), một
mô hình với trọng số có thể truy cập công khai có thể được chạy cục bộ trên phần cứng cấp người tiêu dùng. Chúng tôi
xem xét các thử thách lập trình Python từ cả APPS (Hendrycks et al., 2021) và HumanEval
(Chen et al., 2021); cho mỗi tập dữ liệu chúng tôi hạn chế sự chú ý của mình vào một mô hình với hiệu suất baseline
mạnh hơn (GPT-3.5 trên HumanEval, GPT-4 trên APPS) và một mô hình với hiệu suất baseline
yếu hơn (Code LLama trên HumanEval, GPT-3.5 trên APPS). Trên APPS, để giữ cho các thí nghiệm của chúng tôi
khả thi, chúng tôi đánh giá trên một tập hợp được chọn ngẫu nhiên gồm 300 tác vụ.6 Chúng tôi triển khai tự sửa chữa
sử dụng nối chuỗi mẫu với prompting một lần; các prompt của chúng tôi được đưa ra trong Phụ lục G.
Dựa trên các thí nghiệm sơ bộ, chúng tôi đặt nhiệt độ giải mã thành 0.8 cho tất cả các mô hình. Khi thích hợp, chúng tôi so sánh với một baseline không sửa chữa. Baseline này, được hiển thị bằng một đường đen
trong các biểu đồ, đơn giản là lấy mẫu i.i.d. từ mô hình tương ứng (ví dụ, GPT-4 khi chúng tôi khám phá
liệu GPT-4 có khả năng tự sửa chữa không).

4.1 TỰ SỬA CHỮA KHÔNG PHẢI LÀ VIÊN ĐẠN BẠC, NHƯNG CẢI THIỆN VỚI CÁC MẪU BAN ĐẦU ĐA DẠNG
Trong tiểu mục này, chúng tôi xem xét thiết lập trong đó MP=MF, tức là một thiết lập tự sửa chữa thực sự trong đó
một mô hình duy nhất được sử dụng cho cả việc tạo mã/sửa chữa và tạo phản hồi. Để đánh giá xem
tự sửa chữa có dẫn đến hiệu suất tốt hơn so với phương pháp baseline dựa trên lấy mẫu i.i.d., không sửa chữa hay không, chúng tôi
thay đổi np và nfr—tức là số lượng mẫu cơ sở ban đầu i.i.d. và các mẫu phản hồi, sửa chữa chung
được rút từ MP—trong phạm vi (np, nfr)∈ {1,2,5,10,25} × { 1,3,5,10}.7

Hình 4 cho thấy kết quả cho Code LLama và GPT-3.5 trên HumanEval, trong khi Hình 3 cho thấy
kết quả cho GPT-3.5 và GPT-4 trên tập dữ liệu APPS thách thức hơn. (Chúng tôi cũng chạy GPT-4 trên
HumanEval và CodeLlama trên APPS, nhưng hoãn các kết quả này đến Phụ lục B để ngắn gọn.) Trong các
biểu đồ con bên trái, màu sắc của mỗi chấm cho biết số lượng mẫu ban đầu (np), trong khi hình dạng của nó
cho biết số lượng mẫu phản hồi-sửa chữa (nfr). Trong các biểu đồ bên phải, chúng tôi hiển thị một bản đồ nhiệt
với hai siêu tham số dọc theo các trục, trong đó giá trị trong mỗi ô cho biết tỷ lệ pass trung bình với tự sửa chữa
được chuẩn hóa bởi tỷ lệ pass trung bình của phương pháp baseline, không sửa chữa khi được đưa ra
cùng ngân sách. Khi tỷ lệ pass trung bình chuẩn hóa là 1, điều này có nghĩa là tự sửa chữa đạt được
cùng tỷ lệ pass như phương pháp baseline ở ngân sách mẫu đó; một giá trị cao hơn (≥1)
có nghĩa là tự sửa chữa hoạt động tốt hơn baseline.

Trên APPS, chúng tôi quan sát những cải thiện nhỏ cho GPT-3.5 chỉ cho các giá trị lớn nhất của np. GPT-4, mặt khác, cho thấy những cải thiện đáng kể hơn, đánh bại baseline lên đến 8%. Khi chúng tôi
chia nhỏ các bài toán theo mức độ khó (xem các hình trong Phụ lục C), chúng tôi thấy rằng lợi ích lớn hơn
trên các bài toán khó hơn: GPT-3.5 thấy lên đến 34% cải thiện hiệu suất so với baseline trên
các bài toán cấp độ cạnh tranh, chẳng hạn. Trong khi đó, trên HumanEval chúng tôi quan sát những cải thiện hiệu suất

4Chúng tôi sử dụng các endpoint đông lạnh gpt-3.5-turbo-0301 và gpt-4-0314.
5https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf
6Các tác vụ này được lấy mẫu theo tỷ lệ phù hợp với tần suất của các mức độ khó khác nhau trong
tập kiểm tra APPS rộng hơn: 180 câu hỏi cấp độ phỏng vấn, 60 câu hỏi cấp độ cạnh tranh, và 60 câu hỏi cấp độ giới thiệu. Tất cả các tác vụ được liệt kê trong Phụ lục H.
7Nhớ lại rằng khi MP=MF, chúng tôi lấy mẫu chung cho nfr cặp chuỗi phản hồi và chương trình sửa chữa
thay vì lấy mẫu chúng lần lượt (Phần 3.1).

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
0 10 20 30 40 50
Số chương trình được lấy mẫu0.00.20.40.60.81.0Tỷ lệ pass trung bìnhnp=1
np=2
np=5
np=10
np=25nfr=1
nfr=3
nfr=5
nfr=10
1 2 5 10 25
Chương trình ban đầu (np)10
5
3
1Phản hồi-sửa chữa (nfr)0.83 0.89 O.O.B. O.O.B. O.O.B.
0.85 0.90 0.96 O.O.B. O.O.B.
0.87 0.92 0.96 1.01 O.O.B.
0.91 0.95 0.98 1.00 1.04
(a) GPT-3.5.

0 10 20 30 40 50
Số chương trình được lấy mẫu0.00.20.40.60.81.0Tỷ lệ pass trung bìnhnp=1
np=2
np=5
np=10
np=25nfr=1
nfr=3
nfr=5
nfr=10
1 2 5 10 25
Chương trình ban đầu (np)10
5
3
1Phản hồi-sửa chữa (nfr)0.90 0.97 O.O.B. O.O.B. O.O.B.
0.92 0.97 1.04 O.O.B. O.O.B.
0.94 0.98 1.03 1.06 O.O.B.
0.99 1.00 1.03 1.05 1.08
(b) GPT-4.

Hình 3: Kết quả tự sửa chữa GPT-3.5 và GPT-4 trên APPS. Trái: Tỷ lệ pass trung bình so với số
mẫu được tạo. Đường đen là lấy mẫu i.i.d. không sửa chữa từ cùng mô hình. Lưu ý rằng
các thanh lỗi thường nhỏ hơn các điểm đánh dấu. Phải: Tỷ lệ pass trung bình chuẩn hóa so với
baseline ở ngân sách tương đương. Các ô mà số mẫu vượt quá 50 được đánh dấu O.O.B.
(ngoài giới hạn).

tương tự như GPT-4 trên APPS cho Code Llama (lên đến 10% cải thiện so với baseline),
trong khi lợi ích cho GPT-3.5 bị hạn chế vì nó tiếp cận trần (lên đến 3%).

Từ những quan sát này, rõ ràng là tự sửa chữa không phải lúc nào cũng là chiến lược tốt nhất khi so sánh với một
baseline không sửa chữa với cùng ngân sách mẫu, đặc biệt là cho các ngân sách nhỏ hơn. Hơn nữa, khó dự đoán khi nào
tự sửa chữa sẽ hiệu quả. Trong một phân tích về tỷ lệ thành công sửa chữa (Phụ lục C),
chúng tôi thấy rằng các mô hình mạnh hơn có tỷ lệ thành công sửa chữa cao hơn trên các tác vụ dễ hơn—nhưng đồng thời,
cơ hội nhận được một chương trình đúng bằng cách lấy mẫu lại cũng tăng khi tác vụ càng dễ. Do đó,
chúng ta thấy rằng tỷ lệ thành công tạo chương trình và sửa chữa có xu hướng cùng nhau, và nhiều yếu tố tinh tế chưa biết
ảnh hưởng đến cái nào sẽ thắng thế với cái kia trong bất kỳ lĩnh vực nhất định nào.

Mặc dù hiệu quả tổng thể của tự sửa chữa không rõ ràng, chúng tôi quan sát một xu hướng rõ ràng liên quan đến
mối quan hệ giữa các siêu tham số. Được đưa ra một số lượng phản hồi-sửa chữa cố định (nfr),
tăng số lượng chương trình ban đầu (np) (tức là di chuyển sang phải dọc theo trục x trên các bản đồ nhiệt)
một cách nhất quán dẫn đến lợi ích hiệu suất tương đối cho tất cả các mô hình. Mặt khác, cố định np và
tăng nfr (tức là di chuyển lên dọc theo trục y trên các bản đồ nhiệt) dường như không đáng giá chi phí bổ sung
phát sinh, đưa ra lợi ích nhỏ ở ngân sách cao hơn và thường thậm chí giảm hiệu suất ở ngân sách thấp hơn. Điều này cho thấy rằng, với một ngân sách cố định, yếu tố quan trọng nhất
xác định liệu tự sửa chữa có dẫn đến một chương trình đúng hay không là sự đa dạng của các mẫu cơ sở
được tạo ra trước, thay vì sự đa dạng của các sửa chữa được lấy mẫu. Có nhiều mẫu ban đầu hơn tăng khả năng có ít nhất một chương trình gần với chương trình lý tưởng và, do đó, có thể được sửa chữa thành công.

Vì nfr= 1 dường như là lựa chọn tổng thể tốt nhất cho siêu tham số nfr, tiếp theo chúng tôi cô lập
tác động của số lượng chương trình ban đầu, np, bằng cách khám phá một tập hợp các giá trị có thể dày đặc hơn

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
0 10 20 30 40 50
Số chương trình được lấy mẫu0.00.20.40.60.81.0Tỷ lệ pass trung bìnhnp=1
np=2
np=5
np=10
np=25nfr=1
nfr=3
nfr=5
nfr=10
1 2 5 10 25
Chương trình ban đầu (np)10
5
3
1Phản hồi-sửa chữa (nfr)0.94 1.00 O.O.B. O.O.B. O.O.B.
0.94 1.00 1.06 O.O.B. O.O.B.
0.94 0.99 1.06 1.09 O.O.B.
0.95 1.00 1.04 1.07 1.09
(a) CodeLlama-13b-instruct.

0 10 20 30 40 50
Số chương trình được lấy mẫu0.00.20.40.60.81.0Tỷ lệ pass trung bìnhnp=1
np=2
np=5
np=10
np=25nfr=1
nfr=3
nfr=5
nfr=10
1 2 5 10 25
Chương trình ban đầu (np)10
5
3
1Phản hồi-sửa chữa (nfr)0.97 1.00 O.O.B. O.O.B. O.O.B.
0.98 1.00 1.02 O.O.B. O.O.B.
0.98 1.00 1.02 1.02 O.O.B.
0.99 1.00 1.02 1.02 1.03
(b) GPT-3.5.

Hình 4: Kết quả tự sửa chữa CodeLlama-13b-instruct và GPT-3.5 trên HumanEval. Trái: Tỷ lệ pass trung bình
so với số mẫu được tạo. Đường đen là lấy mẫu i.i.d. không sửa chữa từ cùng mô hình. Lưu ý rằng
các thanh lỗi thường nhỏ hơn các điểm đánh dấu. Phải: Tỷ lệ pass trung bình chuẩn hóa so với
baseline ở ngân sách tương đương. Các ô mà số mẫu vượt quá 50 được đánh dấu O.O.B. (ngoài giới hạn).

10 20 30 40 50
Số chương trình được lấy mẫu0.20.40.60.81.0Tỷ lệ pass trung bình
(a) HumanEval.

10 20 30 40 50
Số chương trình được lấy mẫu0.20.40.60.81.0Tỷ lệ pass trung bình
 (b) APPS.

MP= Code Llama (không sửa chữa)
MP= GPT-3.5 (không sửa chữa)
MP= GPT-4 (không sửa chữa)
MP=MF= Code Llama
MP=MF= GPT-3.5
MP=MF= GPT-4
MP= Code Llama; MF= GPT-3.5
MP= Code Llama; MF= GPT-4
MP= GPT-3.5; MF= GPT-4

Hình 5: Kết quả khi nfr (hoặc nf và nr) = 1. Vùng tô bóng hiển thị ±1 độ lệch chuẩn.

ues:(np, nfr)∈ {1,2, ....,24,25} × { 1}. Các biểu đồ được hiển thị trong Hình 5 cho MP=MF∈
{CodeLlama ,GPT-3.5 ,GPT-4} và các phương pháp baseline, không sửa chữa.8 9 Chúng tôi quan sát lợi ích hiệu suất
cho cả Code Llama và GPT-3.5 trên HumanEval. Trên APPS, chỉ GPT-4 hưởng lợi đáng kể
từ tự sửa chữa, trong khi cả Code Llama và GPT-3.5 chủ yếu thua kém hoặc bằng các baseline của họ,
có thể thấy một số lợi ích rất nhỏ ở ngân sách cao. Trong tất cả các trường hợp, lợi ích hiệu suất ở ngân sách nhỏ hơn
rất nhỏ hoặc không tồn tại, nhưng tăng lên phần nào khi ngân sách tăng.

8Vì GPT-3.5 đã gần đạt trần trên HumanEval, chúng tôi bỏ qua GPT-4 khỏi hình này để giảm bớt sự lộn xộn.
9Lưu ý rằng vì nfr được cố định, trong các biểu đồ này, có mối tương quan trực tiếp giữa np và k: k=np+np.

--- TRANG 8 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 1: Tỷ lệ thành công sửa chữa với giải thích của GPT-4 so với những giải thích của các tham gia viên con người của chúng tôi.

Độ khó | Giới thiệu | Phỏng vấn | Cạnh tranh | Tổng thể
GPT-4 Phản hồi | 42.64% | 19.33% | 3.67% | 33.30%
Phản hồi Con người | 62.21% | 45.67% | 14.67% | 52.60%

4.2 TĂNG CƯỜNG PHẢN HỒI MỞ KHÓA LỢI ÍCH HIỆU SUẤT TỪ SỬA CHỮA
Tiếp theo, chúng tôi tiến hành một thí nghiệm trong đó chúng tôi đánh giá tác động của việc sử dụng một mô hình riêng biệt, mạnh hơn
để tạo phản hồi; điều này là để kiểm tra giả thuyết rằng tự sửa chữa bị cản trở bởi sự không thể của mô hình
trong việc tự xem xét và gỡ lỗi mã của chính nó. Do đó chúng tôi đặt MP là một mô hình yếu hơn (Code Llama trên
HumanEval, Code Llama hoặc GPT-3.5 trên APPS) và MF là một mô hình mạnh hơn (GPT-3.5 hoặc GPT-4
cho Code Llama trên HumanEval; GPT-3.5 cho Code Llama và GPT-4 cho GPT-3.5 trên APPS). Sau đó chúng tôi
thay đổi các siêu tham số như (np, nf, nr)∈ {1, ....,25} × { 1} × { 1}, tương tự như thí nghiệm trước đó.10 11

Kết quả cho thí nghiệm này cũng được hiển thị trong Hình 5 (Code Llama kết hợp với GPT-3.5 màu vàng;
Code Llama với GPT-4 màu xanh sáng; GPT-3.5 với GPT-4 màu xanh sáng). Chúng tôi quan sát một
xu hướng nhất quán: trên APPS, cả Code Llama và GPT-3.5 bây giờ đánh bại cả baseline của họ (xanh đậm, xám) và các chế độ tự sửa chữa tương ứng của họ (tím, đỏ). Trên HumanEval, hiệu suất
mà Code Llama đạt được tăng thêm với sức mạnh của mô hình phản hồi; lưu ý đặc biệt
hiệu suất mà Code Llama đạt được khi được đưa ra phản hồi từ GPT-4 (đường xanh sáng). Điều này
cho thấy rằng giai đoạn phản hồi văn bản tự nó có tầm quan trọng then chốt, và việc cải thiện nó làm giảm
nghẽn cổ chai trong tự sửa chữa.

4.3 PHẢN HỒI CON NGƯỜI CẢI THIỆN ĐÁNG KỂ TỶ LỆ THÀNH CÔNG SỬA CHỮA CỦA GPT-4
Đối với thí nghiệm cuối cùng của chúng tôi, chúng tôi xem xét tác động của việc sử dụng phản hồi của một lập trình viên con người chuyên gia
khi thực hiện sửa chữa với các mô hình rất mạnh như GPT-4. Mục tiêu của nghiên cứu này không phải là thực hiện một
so sánh trực tiếp giữa phương pháp có con người trong vòng lặp so với tự sửa chữa, vì phương pháp có con người trong vòng lặp
áp đặt gánh nặng nhận thức nhiều hơn, điều mà chúng tôi không nghiên cứu. Thay vào đó, mục tiêu của chúng tôi là tiếp tục
điều tra cách thức và lý do chất lượng phản hồi ảnh hưởng đến hiệu suất downstream trong tự sửa chữa.

Phương pháp thu thập dữ liệu. Chúng tôi tuyển dụng 16 tham gia viên và thu thập tổng cộng 2 phần phản hồi được viết bởi con người
cho mỗi trong số 40 chương trình thất bại được lấy mẫu từ GPT-4. Mỗi chương trình được hiển thị cho
hai tham gia viên khác nhau, để giảm phương sai do mức độ kỹ năng và phong cách viết của tham gia viên.
Các tham gia viên được yêu cầu dành khoảng một giờ cho nghiên cứu tổng thể, và được bồi thường
bằng thẻ quà tặng $15. Nghiên cứu này được phê duyệt bởi Hội đồng Đánh giá Thể chế (IRB) của chúng tôi và được thực hiện
hoàn toàn thông qua một cuộc khảo sát trực tuyến. Xem Phụ lục D để biết thêm chi tiết về phương pháp thu thập dữ liệu, bao gồm một bản sao hoàn chỉnh của các hướng dẫn mà chúng tôi cung cấp cho các tham gia viên.

Phân tích định lượng. Sau khi có được hai phần phản hồi được viết bởi con người cho mỗi chương trình,
chúng tôi lấy mẫu 25 ứng viên sửa chữa cho mỗi cặp (phản hồi, chương trình) từ GPT-4. Chúng tôi điều kiện hóa
đặc tả, chương trình ban đầu và chuỗi phản hồi; ngoài phản hồi được thu thập
từ các tham gia viên của chúng tôi, chúng tôi cũng thử hai chuỗi phản hồi của chính GPT-4 cho mỗi chương trình. Cuối cùng, chúng tôi
thực thi tất cả các ứng viên sửa chữa này với testbed, và ghi nhận tần suất chúng vượt qua.

Kết quả được tóm tắt trong Bảng 1, với phân tích chi tiết từng tác vụ trong Phụ lục E. Chúng tôi
lưu ý rằng tỷ lệ thành công tổng thể tăng 1.58× khi chúng tôi thay thế phản hồi của chính GPT-4
bằng phản hồi của các tham gia viên con người. Có lẽ không ngạc nhiên, sự khác biệt tương đối tăng khi các
bài toán trở nên khó hơn, cho thấy rằng khả năng sản xuất phản hồi chính xác và hữu ích của GPT-4 thua kém
xa hơn so với các tham gia viên con người của chúng tôi khi tác vụ (và mã) trở nên phức tạp hơn.

Phân tích định tính. Chúng tôi xem qua thủ công tất cả phản hồi của GPT-4 và các tham gia viên và
ghi nhận xem phản hồi có: (a) có vẻ, nhìn qua, đúng, hoặc nếu nó rõ ràng
không chính xác; (b) rõ ràng đề xuất một thay đổi nhỏ đối với mã (ví dụ "thay đổi điều kiện trên dòng
10Lưu ý rằng vì chúng tôi hiện đang hoạt động trong một thiết lập trong đó các giai đoạn phản hồi và sửa chữa phải được tách biệt,
chúng tôi có ba siêu tham số—np, nf, nr—thay vì hai—np, nfr (Phần 3.1).
11Để giảm chi phí, chúng tôi sử dụng Nf= 10 thay vì Nf= 25 cho thí nghiệm này (xem Phần 3.2).

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2024
X"); (c) rõ ràng đề xuất một thay đổi lớn đối với mã (ví dụ "đặt bài toán như min-cut thay vì
shortest-path"); (d) chứa các khối mã giả hoặc Python (điều mà phản hồi của GPT-4 không bao giờ làm,
theo thiết kế thí nghiệm của chúng tôi); hoặc (e) bày tỏ sự không chắc chắn (sử dụng các cụm từ như "không chắc chắn", "có vẻ như",
v.v.).12 Ví dụ về mỗi danh mục được hiển thị trong Phụ lục F. Chúng tôi thấy rằng

•Hầu như tất cả phản hồi do con người đóng góp xen kẽ ngôn ngữ tự nhiên với các biểu thức toán học/mã đơn lẻ thỉnh thoảng; chỉ 2/80 phản hồi bao gồm mã giả hoặc Python rõ ràng.

• Phản hồi của GPT-4 có nhiều khả năng không chính xác hơn (32/80 so với 7/80 cho phản hồi con người).

•GPT-4 có nhiều khả năng đề xuất rõ ràng các thay đổi nhỏ (54/80 so với 42/80 cho GPT-4 và các
tham gia viên, tương ứng; 28/48 so với 38/73 nếu chúng tôi lọc ra các đề xuất rõ ràng không đúng),
trong khi các tham gia viên con người cho thấy xu hướng hơi lớn hơn để đề xuất các thay đổi cấp cao (23/80 so với
18/80 cho GPT-4; 21/73 so với 13/48 khi có vẻ đúng).

• Các tham gia viên con người của chúng tôi đôi khi bày tỏ sự không chắc chắn (7/80); GPT-4 không bao giờ làm vậy (0/80).

Phân tích sâu hơn này cho thấy rằng kết quả trong Bảng 1 không phải do các artifacts như các tham gia viên của chúng tôi
cung cấp các khối mã rõ ràng mà mô hình chỉ đơn giản sao chép. Thay vào đó, sự khác biệt về hiệu suất
dường như được gây ra bởi sự kết hợp của phản hồi chính xác hơn, khả năng lớn hơn để đề xuất các thay đổi cấp cao, quy mô lớn đối với mã khi cần thiết, và khả năng của các tham gia viên của chúng tôi trong việc bày tỏ sự
không chắc chắn của họ (thay vì tự tin đưa ra phản hồi có khả năng không chính xác).

5 HẠN CHẾ
Đầu tiên, để giảm chi phí tính toán, chúng tôi điền trước và sau đó lấy mẫu con từ một cây sửa chữa lớn duy nhất để bootstrap một số lượng lớn cây sửa chữa cho mỗi cài đặt của các siêu tham số (Phần 3.2).
Điều này có nguy cơ đưa các artifacts thống kê vào phân tích của chúng tôi. Để giảm thiểu rủi ro này, chúng tôi giới hạn np và
nfr xa dưới Np và Nfr, tương ứng, trong các thí nghiệm tự sửa chữa của chúng tôi. Hơn nữa, chúng tôi lưu ý rằng
độ lệch chuẩn rất nhỏ trong các thí nghiệm của chúng tôi cho tất cả các giá trị của np và nfr (xem các biểu đồ phân tán
trong Hình 3, 4), mang lại sự tin tương tăng trong kết quả của chúng tôi.

Thứ hai, các thí nghiệm của chúng tôi tập trung vào các tác vụ lập trình Python độc lập với các bài kiểm tra đơn vị có thể thực thi. Điều này khá khác với các tác vụ phát triển phần mềm thực tế, nơi các đặc tả thường
không đầy đủ, có các phụ thuộc ngữ cảnh dài, và các bài kiểm tra không có khả năng có sẵn cho
mỗi đoạn mã riêng lẻ. Công việc tương lai sẽ được yêu cầu để xem vai trò mà tự sửa chữa có thể đóng ở đó: ví dụ, liệu nó có thể giải quyết sự mơ hồ trong đặc tả, hoặc nếu các kỹ thuật tổng hợp bài kiểm tra đơn vị tự động (Li et al., 2022; Chen et al., 2023a) có thể được tận dụng cùng với các thực hành kỹ thuật đã được thiết lập như Phát triển Hướng Kiểm tra (Astels, 2003) để vượt qua việc thiếu các bài kiểm tra chất lượng cao.

Cuối cùng, nghiên cứu của chúng tôi về dữ liệu con người không theo dõi thời gian mà các tham gia viên mất để gỡ lỗi các
chương trình. Kết quả là, chúng tôi chỉ có thể đánh giá chất lượng của phản hồi (và tác động này có
đối với sửa chữa). Nghiên cứu thêm tại giao điểm của Tương tác Con người-Máy tính, AI, và tổng hợp chương trình
là cần thiết để khám phá khi nào và cách thức can thiệp của con người nên được tận dụng, cũng như cách thức
các trợ lý lập trình nên được thiết kế để tạo điều kiện cho phong cách tương tác này.

6 KẾT LUẬN
Chúng tôi đã điều tra tự sửa chữa cho việc tạo mã, nhìn đặc biệt vào CodeLlama-13b-instruct,
GPT-3.5 và GPT-4 trên các bài toán từ HumanEval và APPS. Trong một loạt các thí nghiệm, chúng tôi
quan sát rằng (1) khi chi phí thực hiện sửa chữa được tính vào, lợi ích hiệu suất từ
tự sửa chữa thường khiêm tốn, thay đổi không chỉ giữa mà còn trong các tập dữ liệu, và dựa vào việc đạt được
sự đa dạng đủ trong các chương trình ban đầu. Hơn nữa, bằng cách thay thế giai đoạn phản hồi, chúng tôi thấy rằng
(2) thay thế phản hồi của một mô hình yếu hơn bằng phản hồi của một mô hình mạnh hơn cải thiện đáng kể
hiệu suất. Cuối cùng, chúng tôi thực hiện một thí nghiệm với các tham gia viên con người, trong đó chúng tôi thấy rằng
(3) thay thế phản hồi tự tạo của GPT-4 bằng phản hồi được cung cấp bởi một lập trình viên có kinh nghiệm
tăng số lượng chương trình được sửa chữa vượt qua tất cả các bài kiểm tra đơn vị lên 1.58×. Kết quả của chúng tôi cho thấy
rằng tự sửa chữa không phải là viên đạn bạc cho việc tạo mã, và các mô hình hiện tại bị cản trở bởi
sự không thể sản xuất phản hồi chính xác và hữu ích một cách đáng tin cậy về lý do tại sao mã sai.
