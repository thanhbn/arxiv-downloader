# 2305.13804.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2305.13804.pdf
# Kích thước tệp: 1474861 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
OER: Phát lại Trải nghiệm Ngoại tuyến cho Học tăng cường Ngoại tuyến Liên tục
Sibo Gaiab;*, Donglin Wangb;**và Li Heb
aTrường Đại học Fudan, Thượng Hải, Trung Quốc
bTrường Đại học Westlake, Chiết Giang, Trung Quốc
Tóm tắt. Khả năng liên tục học các kỹ năng mới thông qua một chuỗi các tập dữ liệu ngoại tuyến được thu thập trước là mong muốn cho một tác nhân. Tuy nhiên, việc liên tục học một chuỗi các nhiệm vụ ngoại tuyến có thể dẫn đến vấn đề quên thảm họa dưới các tình huống có nguồn lực hạn chế. Trong bài báo này, chúng tôi xây dựng một thiết lập mới, học tăng cường ngoại tuyến liên tục (CORL), trong đó một tác nhân học một chuỗi các nhiệm vụ học tăng cường ngoại tuyến và theo đuổi hiệu suất tốt trên tất cả các nhiệm vụ đã học với một bộ đệm phát lại nhỏ mà không khám phá bất kỳ môi trường nào của tất cả các nhiệm vụ tuần tự. Để học nhất quán trên tất cả các nhiệm vụ tuần tự, một tác nhân cần thu được kiến thức mới và đồng thời bảo tồn kiến thức cũ theo cách ngoại tuyến. Để đạt được điều này, chúng tôi đã giới thiệu các thuật toán học liên tục và thông qua thí nghiệm phát hiện ra phát lại trải nghiệm (ER) là thuật toán phù hợp nhất cho bài toán CORL. Tuy nhiên, chúng tôi quan sát thấy rằng việc đưa ER vào CORL gặp phải một vấn đề dịch chuyển phân phối mới: sự không khớp giữa các trải nghiệm trong bộ đệm phát lại và quỹ đạo từ chính sách đã học. Để giải quyết vấn đề này, chúng tôi đề xuất một sơ đồ lựa chọn trải nghiệm dựa trên mô hình (MBES) mới để xây dựng bộ đệm phát lại, trong đó một mô hình chuyển tiếp được học để xấp xỉ phân phối trạng thái. Mô hình này được sử dụng để thu hẹp độ lệch phân phối giữa bộ đệm phát lại và mô hình đã học bằng cách lọc dữ liệu từ dữ liệu ngoại tuyến gần nhất với mô hình đã học để lưu trữ. Hơn nữa, để nâng cao khả năng học các nhiệm vụ mới, chúng tôi cải tiến phương pháp phát lại trải nghiệm với kiến trúc cloning hành vi kép (DBC) mới để tránh nhiễu loạn của mất mát behavior-cloning đối với quá trình Q-learning. Nhìn chung, chúng tôi gọi thuật toán của mình là phát lại trải nghiệm ngoại tuyến (OER). Các thí nghiệm rộng rãi chứng minh rằng phương pháp OER của chúng tôi vượt trội hơn các baseline SOTA trong các môi trường Mujoco được sử dụng rộng rãi.

1 Giới thiệu
Tương tự như con người, một tác nhân trí tuệ đa mục đích được kỳ vọng sẽ học các nhiệm vụ mới một cách liên tục. Các nhiệm vụ tuần tự như vậy có thể là nhiệm vụ trực tuyến được học thông qua khám phá hoặc nhiệm vụ ngoại tuyến được học thông qua các tập dữ liệu ngoại tuyến, trong đó nhiệm vụ sau cũng quan trọng tương đương nhưng chưa nhận được đủ sự chú ý cho đến nay. Học các nhiệm vụ tuần tự trong thiết lập ngoại tuyến có thể cải thiện đáng kể hiệu quả học và tránh quá trình khám phá nguy hiểm trong thực tế. Hơn nữa, học các nhiệm vụ tuần tự trực tuyến không phải lúc nào cũng khả thi do các ràng buộc về thời gian và không gian của môi trường và bản thân tác nhân. Do đó, việc nghiên cứu học tăng cường ngoại tuyến trong bối cảnh liên tục là khá quan trọng và có giá trị cho trí tuệ đa mục đích. Nếu có đủ nguồn lực tính toán, việc thực hiện mục tiêu như vậy là dễ dàng. Tuy nhiên, đối với một tác nhân có nguồn lực hạn chế, các phương pháp học liên tục là không thể thiếu để xử lý các tập dữ liệu ngoại tuyến như vậy. Do đó, chúng tôi đề xuất một thiết lập mới có tên là học tăng cường ngoại tuyến liên tục (CORL) trong bài báo này, tích hợp RL ngoại tuyến và học liên tục.

RL ngoại tuyến học từ các tập dữ liệu được thu thập trước thay vì tương tác trực tiếp với môi trường [11]. Bằng cách tận dụng các tập dữ liệu được thu thập trước, các phương pháp RL ngoại tuyến tránh các tương tác tốn kém và do đó nâng cao hiệu quả học và an toàn. Tuy nhiên, các phương pháp RL ngoại tuyến gặp phải vấn đề đánh giá quá cao của dữ liệu ngoài phân phối (OOD), trong đó dữ liệu chưa thấy bị đánh giá sai là có giá trị cao. Hiện tượng này xuất phát từ sự dịch chuyển phân phối giữa chính sách hành vi và chính sách học. Nhiều phương pháp khác nhau đã được đề xuất để giải quyết vấn đề đánh giá quá cao [11, 21]. Trong bài báo này, chúng tôi tập trung vào việc xử lý một chuỗi các tập dữ liệu ngoại tuyến, điều này đòi hỏi các kỹ thuật học liên tục.

Thách thức chính của học liên tục là làm thế nào để giảm thiểu vấn đề quên thảm họa [31] về các nhiệm vụ trước khi học các nhiệm vụ mới. Có ba loại phương pháp học liên tục, bao gồm các phương pháp dựa trên chính quy hóa [19, 53], các phương pháp mô-đun [8, 30], và các phương pháp dựa trên rehearsal [27, 4]. Phát lại trải nghiệm (ER) là một phương pháp dựa trên rehearsal được sử dụng rộng rãi [42], luân phiên giữa việc học một nhiệm vụ mới và phát lại các mẫu của các nhiệm vụ trước. Trong bài báo này, chúng tôi xem xét việc sử dụng ER làm cơ sở cho vấn đề của riêng chúng tôi. Chúng tôi sẽ chỉ ra rằng ER là thuật toán phù hợp nhất cho bài toán CORL trong phần thí nghiệm sau.

Tuy nhiên, vì ER được thiết kế cho RL liên tục trực tuyến, việc áp dụng trực tiếp ER trong CORL của chúng tôi mang lại hiệu suất kém do hai loại dịch chuyển phân phối. Loại đầu tiên là sự dịch chuyển phân phối giữa chính sách hành vi và chính sách học, và loại thứ hai là sự dịch chuyển phân phối giữa bộ đệm phát lại được chọn và chính sách học tương ứng. Các phương pháp hiện tại tập trung vào việc giải quyết vấn đề dịch chuyển đầu tiên, và không có công trình liên quan nào xem xét vấn đề thứ hai, chỉ xuất hiện trong thiết lập CORL của chúng tôi. Do đó, việc đơn giản tích hợp ER với Offline RL không thể giảm thiểu quên thảm họa. Để giải quyết vấn đề mới trên, chúng tôi đề xuất một phương pháp lựa chọn trải nghiệm dựa trên mô hình (MBES) mới để điền vào bộ đệm phát lại. Ý tưởng chính là tận dụng mô hình động để tìm kiếm và thêm các tập phim có giá trị nhất trong tập dữ liệu ngoại tuyến vào bộ đệm phát lại.

Sau khi có một bộ đệm phát lại tốt cho các nhiệm vụ trước, chính sách đã học tương ứng với một nhiệm vụ mới cần clone các nhiệm vụ trước. Cloning hành vi (BC) như một phương pháp ER trực tuyến được sử dụng rộng rãi [54], không tương thích với kiến trúc actor-critic trong RL ngoại tuyến. Mặc dù chúng ta có thể điều chỉnh cẩn thận trọng số của mất mát BC, việc điều chỉnh siêu tham số là một nhiệm vụ phiền toái nói chung và khó khăn trong thiết lập ngoại tuyến. Do đó, việc tích hợp ER trực tuyến với RL ngoại tuyến thường dẫn đến một chính sách không hội tụ. Lý do là mô hình actor-critic khó huấn luyện [39], và số hạng rehearsal trong hàm mất mát có tác động tiêu cực đến quá trình học. Để phát lại trải nghiệm hiệu quả, chúng tôi đề xuất kiến trúc cloning hành vi kép (DBC) thay thế để giải quyết xung đột tối ưu hóa, trong đó một chính sách tối ưu hóa hiệu suất của nhiệm vụ mới bằng cách sử dụng kiến trúc actor-critic, và chính sách thứ hai tối ưu hóa từ góc độ liên tục cho cả nhiệm vụ mới và đã học.

Tóm lại, bài báo này xem xét việc điều tra một thiết lập CORL mới. Một MBES mới được đề xuất để chọn các trải nghiệm có giá trị và khắc phục sự không khớp giữa các trải nghiệm trong bộ đệm phát lại và quỹ đạo từ chính sách đã học. Sau đó, kiến trúc DBC được đề xuất để xử lý vấn đề xung đột tối ưu hóa. Bằng cách lấy MBES và DBC làm hai ý tưởng chính cho thiết lập CORL, chúng tôi đặt tên sơ đồ tổng thể của mình là phát lại trải nghiệm ngoại tuyến (OER). Các đóng góp chính của bài báo này có thể được tóm tắt như sau:
• Chúng tôi trình bày một thiết lập CORL mới và sau đó đề xuất sơ đồ OER mới cho thiết lập CORL.
• Chúng tôi đề xuất phương pháp lựa chọn MBES mới cho bộ đệm phát lại ngoại tuyến bằng cách sử dụng mô hình động để giảm sự dịch chuyển phân phối giữa trải nghiệm từ bộ đệm phát lại và chính sách đã học.
• Mặt khác, chúng tôi đề xuất kiến trúc DBC mới để ngăn quá trình học khỏi sụp đổ bằng cách tách Q-learning trên nhiệm vụ hiện tại và các quá trình BC trên tất cả các nhiệm vụ trước.
• Chúng tôi xác minh thực nghiệm hiệu suất của các mô-đun khác nhau và đánh giá phương pháp của chúng tôi trên các nhiệm vụ điều khiển liên tục. Phương pháp OER của chúng tôi vượt trội hơn tất cả các baseline SOTA cho tất cả trường hợp.

2 Các Công trình Liên quan
Học tăng cường Ngoại tuyến RL ngoại tuyến học từ một tập dữ liệu ngoại tuyến đã thu thập và gặp phải vấn đề của dữ liệu ngoài phân phối (OOD). Một số công trình trước đề xuất ràng buộc chính sách đã học hướng về chính sách hành vi bằng cách thêm KL-divergence [38, 36, 45, 55], MSE [6], hoặc chính quy hóa của việc lựa chọn hành động [21]. Một số bài báo cho rằng nếu dữ liệu thu thập là dưới tối ưu, các phương pháp này thường không hoạt động tốt [29]. Nhưng các công trình khác chỉ ra rằng việc thêm một số hạng học có giám sát vào mục tiêu cải thiện chính sách [10] cũng sẽ nhận được hiệu suất cao bằng cách giảm khám phá. Một cách hiệu quả khác là học một Q-function bảo thủ [22, 28, 20], gán giá trị Q thấp cho các trạng thái OOD và sau đó trích xuất chính sách tham lam tương ứng. Hơn nữa, các công trình khác đề xuất sử dụng mô hình ensemble để ước tính giá trị Q [1] hoặc xem xét importance sampling [38]. Các phương pháp như vậy trước đây chưa xem xét thiết lập của các nhiệm vụ tuần tự và việc dịch chuyển đơn giản chúng vào thiết lập CORL là không hiệu quả, trong khi bài báo này tập trung vào các nhiệm vụ tuần tự và nhằm giải quyết vấn đề quên thảm họa trong quá trình học một chuỗi các nhiệm vụ RL ngoại tuyến.

Mặt khác, các công trình gần đây [5, 24] đề xuất huấn luyện một mô hình động để dự đoán giá trị của các mẫu OOD theo cách học có giám sát. Các phương pháp RL ngoại tuyến dựa trên mô hình như vậy cung cấp tiềm năng lớn để giải quyết vấn đề OOD, mặc dù mô hình chuyển tiếp khó có thể chính xác nghiêm ngặt. Thuật toán mô hình được cho là giảm thiểu vấn đề OOD mà RL ngoại tuyến gặp phải và do đó cải thiện tính bền vững của tác nhân ngoại tuyến. Các phương pháp RL ngoại tuyến dựa trên mô hình có hai danh mục chính: một tập trung vào việc đo lường độ không chắc chắn của mô hình động đã học [52, 17], và danh mục khác xem xét ước tính bi quan [51]. Khác với hầu hết các công trình này sử dụng mô hình động để tạo ra các mẫu OOD khi huấn luyện tác nhân, trong bài báo này, chúng tôi sử dụng mô hình động để tìm kiếm các tập phim có giá trị nhất trong tập dữ liệu ngoại tuyến cho phương pháp ER.

Học tăng cường Liên tục Các phương pháp ngoại tuyến có thể xem xét kịch bản đơn nhiệm vụ hoặc đa nhiệm vụ [50, 25, 26]. Ngược lại, học liên tục cố gắng học các nhiệm vụ mới sau khi đã học các nhiệm vụ cũ và có được kết quả tốt nhất có thể trên tất cả các nhiệm vụ. Nói chung, các phương pháp học liên tục có thể được phân loại thành ba danh mục [37]: các phương pháp dựa trên chính quy hóa [19, 53] thêm một số hạng chính quy hóa để ngăn các tham số khỏi xa giá trị đã học từ các nhiệm vụ trước; các phương pháp mô-đun [8, 30] xem xét các tham số cố định một phần cho một nhiệm vụ chuyên dụng; và các phương pháp dựa trên rehearsal [27, 4] huấn luyện một tác nhân bằng cách hợp nhất dữ liệu của các nhiệm vụ đã học trước với dữ liệu của nhiệm vụ hiện tại. Cả ba loại phương pháp học liên tục đều đã được áp dụng cho các nhiệm vụ RL [15, 48, 32, 23]. Cụ thể, công trình của chúng tôi dựa trên phương pháp rehearsal trong thiết lập RL [42, 14]. Do đó, chúng tôi sẽ chi tiết các công trình liên quan đến danh mục này sau.

Có hai câu hỏi thiết yếu cần trả lời trong học liên tục dựa trên rehearsal. Đầu tiên là làm thế nào để chọn các mẫu từ toàn bộ tập dữ liệu để lưu trữ trong bộ đệm phát lại với kích thước hạn chế [49]. Các mẫu đại diện nhất [41, 43] hoặc các mẫu dễ quên [3] thường được chọn trong bộ đệm phát lại trong khi lựa chọn ngẫu nhiên cũng đã được sử dụng trong một số công trình [40, 2]. Tuy nhiên, các thuật toán này được thiết kế cho phân loại hình ảnh và không áp dụng được cho RL. [14] tập trung vào việc lấy mẫu câu hỏi bộ đệm phát lại trong thiết lập RL trực tuyến. Thứ hai là làm thế nào để tận dụng các mẫu phát lại đã lưu [49, 3]. Trong RL, hai phương pháp được sử dụng phổ biến nhất là BC và bộ nhớ hoàn hảo [46] trong RL liên tục, trong đó BC hiệu quả hơn trong việc giảm thiểu quên thảm họa. Hiện tại, tất cả các phương pháp này được thiết kế cho thiết lập RL trực tuyến. Khác với các công trình trước, chúng tôi xem xét thiết lập RL ngoại tuyến trong bài báo này, trong đó quên thảm họa và đánh giá quá cao phải được khắc phục đồng thời.

Theo hiểu biết tốt nhất của chúng tôi, đây là công trình đầu tiên giải quyết các vấn đề RL ngoại tuyến trong thiết lập học liên tục.

3 Xây dựng Vấn đề và Kiến thức Cơ bản
Học tăng cường Ngoại tuyến Liên tục Trong bài báo này, chúng tôi điều tra CORL, học một chuỗi các nhiệm vụ RL T=(T₁,···,T_N). Mỗi nhiệm vụ T_n được mô tả như một Quá trình Quyết định Markov (MDP) được biểu diễn bằng một bộ {S,A,P_n,ρ₀,_n,r_n,γ}, trong đó S là không gian trạng thái, A là không gian hành động, P_n:S×A×S→[0,1] là xác suất chuyển tiếp, ρ₀,_n:S là phân phối của trạng thái ban đầu, r_n:S×A→[-R_max,R_max] là hàm phần thưởng, và γ∈[0,1) là hệ số chiết khấu. Chúng tôi giả định rằng các nhiệm vụ tuần tự có P_n, ρ₀,_n và r_n khác nhau, nhưng chia sẻ cùng S, A, và γ để đơn giản. Lợi tức được định nghĩa là tổng phần thưởng tương lai được chiết khấu R_t,n=Σᵢ₌ₜᴴ γ⁽ⁱ⁻ᵗ⁾r_n(s_i,a_i), trong đó H là chân trời.

Chúng tôi định nghĩa một Q-function tham số Q(s,a) và một chính sách tham số π(a|s). Các phương pháp Q-learning huấn luyện một Q-function bằng cách áp dụng lặp lại toán tử Bellman B*Q(s,a)=r(s,a)+γE_{s'~P(s'|s,a)}(max_{a'}Q(s',a')). Chúng tôi cũng huấn luyện một mô hình chuyển tiếp cho mỗi nhiệm vụ P̂_n(s'|s,a) bằng cách sử dụng ước lượng hợp lý cực đại min_{P̂_n} E_{(s,a,s')~D}[log P̂(s'|s,a)]. Chúng tôi sử dụng kiến trúc đa đầu cho mạng chính sách π để tránh vấn đề cùng-trạng thái-khác-nhiệm vụ [16]. Cụ thể, mạng chính sách bao gồm một bộ trích xuất đặc trưng θ_z cho tất cả các nhiệm vụ và nhiều đầu θ_n, n∈[1,N], trong đó một đầu cho mỗi nhiệm vụ. π_n được định nghĩa để biểu diễn mạng với các tham số kết hợp [θ_z,θ_n] và h_n được định nghĩa để biểu diễn đầu với các tham số θ_n. Mục tiêu của chúng tôi là huấn luyện các nhiệm vụ tuần tự tất cả qua [T₁,···,T_{N-1}] một cách tuần tự và có được hiệu suất trung bình cao và mức quên thấp của tất cả các nhiệm vụ đã học mà không truy cập dữ liệu từ các nhiệm vụ trước ngoại trừ một bộ đệm nhỏ.

Trong thiết lập RL trực tuyến, các trải nghiệm e=(s,a,s',r) có thể được thu thập thông qua tương tác môi trường. Tuy nhiên, trong thiết lập RL ngoại tuyến, chính sách π_n(a|s) chỉ có thể được học từ một tập dữ liệu tĩnh D_n={e_i^n}, e_i^n=(s_i^n,a_i^n,s'_i^n,r_i^n), được giả định được thu thập bởi một chính sách hành vi không xác định π_β^n(a|s).

Phát lại Trải nghiệm ER [42] là phương pháp học liên tục dựa trên rehearsal được sử dụng rộng rãi nhất. Về nhiệm vụ T_n, mục tiêu của ER là duy trì hiệu suất tốt trên các nhiệm vụ trước [T₁,···,T_{n-1}], bằng cách sử dụng các bộ đệm phát lại tương ứng [B₁,···,B_{n-1}], được gọi là bộ nhớ hoạt động. Hơn nữa, hai mất mát cloning hành vi bổ sung, bao gồm mất mát cloning actor và mất mát cloning critic, thường được sử dụng cho các nhiệm vụ trước như sau:

L_{actor_cloning} := Σ_{s,a∈B} ||π_n(s) - a||₂²,                    (1)
L_{critic_cloning} := Σ_{s,a,Q_{replay}∈B} (Q_n(a,s) - Q_{replay})²,  (2)

trong đó B là bộ đệm phát lại và Q_{replay} có nghĩa là giá trị Q được lưu từ các nhiệm vụ trước. Hai mất mát này được gọi là BC [46].

Công trình tiếp theo [46] cho thấy rằng đối với kiến trúc soft actor-critic [13], mất mát phát lại được thêm vào mạng actor (Phương trình 1) hoạt động tốt, nhưng mất mát được thêm vào mạng critic (Phương trình 2) có hiệu quả kém. Do đó, chúng tôi chỉ xem xét mất mát cloning actor (Phương trình 1) trong công trình của chúng tôi.

Tuy nhiên, việc đơn giản tích hợp ER với RL ngoại tuyến dẫn đến giảm hiệu suất đáng kể cho các vấn đề CORL. Để giải quyết vấn đề này, chúng tôi đề xuất phương pháp OER mới, bao gồm hai thành phần thiết yếu như sau.

4 Phát lại Trải nghiệm Ngoại tuyến (OER)
Trong phần này, đầu tiên chúng tôi trình bày chi tiết cách chọn các trải nghiệm có giá trị và xây dựng bộ đệm phát lại. Sau đó, chúng tôi mô tả kiến trúc DBC mới như mô hình phát lại của chúng tôi. Cuối cùng, chúng tôi tóm tắt thuật toán của chúng tôi và cung cấp Mã giả.

4.1 Sơ đồ Lựa chọn Trải nghiệm Ngoại tuyến
Vấn đề Dịch chuyển Phân phối Mới: Đối mặt với các nhiệm vụ tuần tự, một tác nhân học nhiệm vụ T_n theo thứ tự sau khi học T_{n-1}, và nên chuẩn bị cho việc học tiếp theo. Về nhiệm vụ T_n, cách chọn dữ liệu có giá trị một phần từ tập dữ liệu ngoại tuyến D_n để xây dựng bộ đệm phát lại B_n có kích thước hạn chế là rất quan trọng. Trong thiết lập RL trực tuyến, các phương pháp dựa trên rehearsal thường sử dụng hàm xếp hạng R(s_i,a_i)=r_i+γmax_{a'}Q(s'_i,a')-Q(s_i,a_i) để đánh giá giá trị của quỹ đạo. Ở đây, R(s_i,a_i) với (s_i,a_i)∈D đánh giá quỹ đạo phát lại về độ chính xác giá trị Q hoặc tổng phần thưởng tích lũy [14], trong đó các quỹ đạo được thu thập bởi chính sách tối ưu π*_n tương tác với môi trường. Ngược lại, trong thiết lập RL ngoại tuyến, nếu chúng ta xem xét phương pháp tương tự trên, việc lựa chọn dữ liệu chỉ có thể được thực hiện trong tập dữ liệu ngoại tuyến D_n tương ứng với chính sách hành vi π_β^n. Do đó, tồn tại sự dịch chuyển phân phối giữa π_β^n và π*_n, điều này không thể tránh khỏi ảnh hưởng đến hiệu suất của rehearsal. Trong sơ đồ lựa chọn của chúng tôi, chúng tôi cố gắng xác định và lọc ra những quỹ đạo ngoại tuyến trong D_n không có khả năng được tạo ra bởi π*_n. Để làm rõ điểm này, chúng tôi đưa ra một minh họa trong Hình 1, trong đó những quỹ đạo gần với quỹ đạo tối ưu ngoại tuyến được chọn và lưu trữ trong bộ đệm phát lại B_n. Phân phối của các quỹ đạo này về cả S và A khác với tập dữ liệu ngoại tuyến D_n.

Lựa chọn Trải nghiệm Dựa trên Mô hình (MBES): Để giải quyết vấn đề dịch chuyển phân phối mới trên, chúng tôi đề xuất sơ đồ MBES mới. Như được hiển thị trong Hình 2, dựa trên các quỹ đạo ngoại tuyến trong D_n được thu thập từ π_β^n, MBES nhằm tạo ra một quỹ đạo mới tương ứng với π*_n. Cụ thể, MBES xem xét cả chính sách tối ưu đã học và mô hình động cụ thể cho nhiệm vụ P̂_n, trong đó mô hình động P̂_n được thu được thông qua học có giám sát bằng cách sử dụng D_n. Bắt đầu từ trạng thái thứ t s_t, chúng tôi lặp lại lấy mẫu một hành động bởi π*_n và dự đoán trạng thái tiếp theo s'_t=P̂_n(s_t,π*_n(s_t)).

Tuy nhiên, việc lặp lại lấy mẫu hành động trên các trạng thái được dự đoán gây ra sự tích lũy lỗi kết hợp đáng kể sau vài bước, điều này cản trở các quỹ đạo được chọn khỏi việc được xem xét lại sau này. Để loại bỏ lỗi kết hợp, sau khi học nhiệm vụ n, bắt đầu từ trạng thái thứ t s_t, thay vì trực tiếp lấy đầu ra mô hình làm trạng thái tiếp theo, một trạng thái trong D_n giống nhất với dự đoán mô hình s'_t được chọn làm trạng thái tiếp theo s_{t+1} cho việc khai thác giả. Ở đây, chúng tôi sử dụng metric L₂ để đo độ tương tự như sau:

s_{t+1} = argmin_{s∈D_n} dist(s,s'_t)
s'_t ~ P̂_n(s_t,π*_n(s_t)),                                        (3)

trong đó dist có nghĩa là khoảng cách giữa s và s'_t. Theo các phương pháp RL ngoại tuyến dựa trên mô hình [5, 24], chúng tôi tiếp tục giới thiệu phương sai của P̂_n(s_t,π*_n(s_t)) để xác định liệu kết quả của mô hình động có đáng tin cậy hay không, trong đó chúng tôi sử dụng Phương trình 3 để lựa chọn chỉ khi phương sai của P̂_n thấp hơn ngưỡng; nếu không, giữ π_β^n(s_t) thay thế. Trong các thí nghiệm của chúng tôi, chúng tôi cụ thể sử dụng 2σ_{P̂_n}(s_t,a_t) làm ngưỡng, có tác động tối thiểu.

Để khởi động, chúng tôi lấy mẫu s₀ từ ρ₀,_n, và sau đó tiến hành lặp lại. Cuối cùng, chúng tôi lưu quỹ đạo được tạo trong B_n.

4.2 Cloning Hành vi Kép (DBC)
Các phương pháp dựa trên rehearsal trước sử dụng mạng chính sách đa đầu π và mạng Q Q_n để học nhiệm vụ T_n, như được hiển thị trong Hình 3 (a). Trong quá trình học, mạng chính sách π sao chép dữ liệu trải nghiệm được lưu trữ trong các bộ đệm phát lại B₁ đến B_{n-1} cho tất cả các nhiệm vụ trước T₁ đến T_{n-1}. Tuy nhiên, kiến trúc như vậy gặp phải sự giảm hiệu suất rõ ràng khi số lượng nhiệm vụ tăng, điều này cho thấy rằng chính sách π được huấn luyện thông qua BC không có khả năng thành thạo cả kiến thức trước từ các nhiệm vụ cũ và kiến thức mới từ nhiệm vụ mới.

Sự giảm hiệu suất này là do sự không nhất quán hiện tại giữa hai mục tiêu sau: một mặt, chính sách đa đầu π được tối ưu hóa cho tất cả các nhiệm vụ hiện tại và trước T₁ đến T_n để dự đoán hành động; mặt khác, chính sách π cũng được sử dụng để cập nhật mạng Q hiện tại Q_n. Cụ thể hơn, chúng tôi bắt đầu phân tích này từ Q-learning [34, 35] với phương trình Bellman của Q(s,a)=r(s,a)+γ*max_{a'}Q(s',a'). Trong không gian hành động liên tục, vì toán tử cực đại trên không thể được thực hiện, một mạng actor tham số μ(s) thường được sử dụng thay thế để thực hiện hành động tối ưu tương ứng với giá trị Q cực đại. μ và π có thể được xem xét tương đương trong thiết lập đơn nhiệm vụ. Tuy nhiên, đối với học liên tục, π bị ràng buộc phải clone tất cả các nhiệm vụ trước từ T₁ đến T_{n-1} trong khi μ chỉ phụ thuộc vào nhiệm vụ hiện tại T_n. Do đó, khó khăn cho chính sách π thực hiện hành động tương ứng với phần thưởng tương lai tối đa cho s' trong nhiệm vụ T_n.

Dựa trên phân tích như vậy, chúng tôi đề xuất sơ đồ DBC mới để giải quyết sự không nhất quán được đề cập ở trên, và sơ đồ kiến trúc được đề xuất của chúng tôi được đưa ra trong Hình 3(b). So với kiến trúc mạng đa đầu hiện tại trong Hình 3(a) [42], chúng tôi đề xuất một mạng chính sách bổ sung μ_n để học ánh xạ trạng thái-hành động tối ưu cho T_n. Cụ thể, khi học nhiệm vụ T_n, chúng tôi đầu tiên thu được μ_n và Q_n bằng cách sử dụng thuật toán RL ngoại tuyến. Sau đó, trong giai đoạn rehearsal, chính sách học liên tục π được yêu cầu clone các trải nghiệm trước từ B₁ đến B_{n-1} và đồng thời gần với μ_n. Do đó, mất mát tương ứng L_π có thể được viết như sau:

L_π = E_{(s,a,s')~D_n}[(π_n(s,a) - μ_n(s,a))²] 
     + λ_r(1/n)Σ_{j=1}^{n-1} E_{(s,a)~B_j}[||π_j(s) - a||²],        (4)

trong đó λ_r là hệ số cho hạng BC. Đáng chú ý là chính sách học liên tục π cố gắng clone hành vi của cả μ_n và các trải nghiệm trước từ B₁ đến B_{n-1} đồng thời. Do đó, chúng tôi đặt tên sơ đồ của mình là DBC.

4.3 Tóm tắt Thuật toán
Khi xem xét một nhiệm vụ mới T_n, chúng tôi đầu tiên sử dụng DBC để học hai mạng chính sách π, μ_n, và mô hình động P̂_n cho đến khi hội tụ. Sau đó, chính sách đã học π và mô hình động P̂_n được sử dụng trong MBES để chọn dữ liệu có giá trị cho B_n. Để tóm tắt, quá trình tổng thể của OER, bao gồm DBC và MBES, được đưa ra trong Thuật toán 1.

5 Chi tiết Triển khai
Chúng tôi mô hình hóa Q-function và mạng chính sách như một perceptron đa lớp (MLP) với hai lớp ẩn của 128 neuron, mỗi lớp với phi tuyến ReLU dựa trên [44]. Chúng tôi sử dụng mô hình động ensemble chứa năm mô hình riêng lẻ, cũng được mô hình hóa như một MLP, giống như Q-function và mạng chính sách. Bất kỳ phương pháp RL ngoại tuyến nào đều tương thích với kiến trúc của chúng tôi. Chúng tôi chọn TD3+BC [10] làm thuật toán cơ sở vì cấu trúc đơn giản của nó để chứng minh rằng thuật toán của chúng tôi không phụ thuộc vào một thuật toán ngoại tuyến cụ thể. Hơn nữa, chúng tôi sử dụng Adam [18] với tốc độ học 0.001 để cập nhật cả Q-function và mô hình động và 0.003 để cập nhật cả mạng chính sách π và μ_n. Sau đó, cho mỗi nhiệm vụ, chúng tôi huấn luyện 30,000 bước và chuyển sang bước tiếp theo. Chúng tôi thấy rằng việc khởi tạo μ_n với π_{n-1} và học π và μ_n đồng thời hoạt động tốt từ kinh nghiệm. Ngoài ra, việc học π và μ_n cùng nhau sẽ giảm phạm vi gradient và tránh thay đổi nhảy để đảm bảo học ổn định và giảm thiểu quên thảm họa. Kết quả được tính toán thông qua năm mô phỏng lặp lại với số hạt giống khác nhau.

Thuật toán 1 Phương pháp OER được đề xuất của chúng tôi
Yêu cầu: Số lượng nhiệm vụ N; khởi tạo chính sách π.
1: for Nhiệm vụ T_n trong [1,···,N] do
2:    Lấy tập dữ liệu ngoại tuyến D_n; Khởi tạo bộ đệm phát lại B_n=∅; Khởi tạo đầu mới h_n cho π; Khởi tạo μ_n, Q_n và P̂_n.
3:    while Chưa Hội tụ do
4:        Cập nhật μ_n và Q_n thông qua phương pháp học ngoại tuyến.
5:        Cập nhật π thông qua Phương trình 4 để clone μ_n và B₀ đến B_{n-1}.
6:        Cập nhật mô hình động P̂_n.
7:    end while
8:    Lấy mẫu trạng thái ban đầu s₀ từ ρ₀,_n, và s_t←s₀.
9:    while B_n chưa đầy do
10:       if s_t không phải trạng thái kết thúc then
11:           Chọn s_{t+1} từ D_n bởi π_n và P̂_n thông qua Phương trình 3.
12:       else
13:           Lấy mẫu s_{t+1} từ ρ₀,_n.
14:       end if
15:       Thêm s_{t+1} vào B_n, và s_t←s_{t+1}.
16:   end while
17: end for
Đảm bảo: π.

6 Thí nghiệm
Các thí nghiệm rộng rãi được tiến hành để chứng minh hiệu quả của sơ đồ được đề xuất và kiểm tra xem chúng ta có thể giữ cả tính ổn định và tính dẻo đại đồng thời khi học các nhiệm vụ RL ngoại tuyến tuần tự. Chúng tôi đánh giá hiệu suất của MBES và DBC riêng biệt để kiểm tra hiệu suất của từng phương pháp.

6.1 Baseline và Tập dữ liệu
Baseline Một mặt, để đánh giá MBES, chúng tôi xem xét sáu phương pháp lựa chọn bộ đệm phát lại, trong đó bốn từ [14] được đưa ra như sau.
• Surprise [14]: lưu trữ quỹ đạo với lỗi TD trung bình cực đại: min_{τ∈D_i} E_{s,a,s_{t+1}∈τ} ||B*Q(s,a) - Q(s,a)||₂².
• Reward [14]: lưu trữ quỹ đạo với phần thưởng trung bình cực đại: max_{τ∈D_i} R_{t,n}.
• Random [14]: chọn ngẫu nhiên các mẫu trong tập dữ liệu.
• Coverage [14]: lưu trữ những mẫu để tối đa hóa độ bao phủ của không gian trạng thái, hoặc đảm bảo tính thưa thớt của các trạng thái được chọn: min_{s∈D_i} |N_i|; N_i={s' s.t. dist(s'-s)<d}.

Xem xét rằng các baseline này được thiết kế cho RL trực tuyến và không đủ công bằng để chỉ sử dụng chúng làm baseline, chúng tôi đã thiết kế hai thuật toán để so sánh có thể áp dụng cho RL ngoại tuyến, dựa trên ý tưởng của [14].
• Match: chọn các mẫu trong tập dữ liệu ngoại tuyến nhất quán nhất với chính sách đã học. Quỹ đạo được chọn theo cách này gần nhất với chính sách đã học trong không gian hành động, nhưng có thể không khớp trong không gian trạng thái: min_{τ∈D_i} E_{s,a∈τ} ||a - π*_i(s)||₂².
• Model: Cho rằng chúng tôi đã sử dụng phương pháp Dựa trên Mô hình để lọc dữ liệu của chúng tôi, chúng tôi cũng sử dụng nó như một tiêu chí cho việc liệu các quỹ đạo có khớp với xác suất chuyển tiếp: min_{τ∈D_i} E_{s,a∈τ} ||P̂_i(s,a) - P_i(s,a)||₂². Metric này được sử dụng để chứng minh rằng việc giới thiệu phương pháp Dựa trên Mô hình một mình không cải thiện hiệu suất của thuật toán CORL.

Mặt khác, để đánh giá DBC, chúng tôi xem xét năm phương pháp học liên tục được sử dụng rộng rãi, trong đó ba phương pháp cần sử dụng bộ đệm phát lại.
• BC [42]: một phương pháp liên tục dựa trên rehearsal cơ bản thêm một số hạng clone hành vi trong hàm mất mát của mạng chính sách.
• Gradient episodic memory (GEM) [27]: một phương pháp sử dụng bộ nhớ episodic của gradient tham số để hạn chế cập nhật chính sách.
• Averaged gradient episodic memory (AGEM) [4]: một phương pháp dựa trên GEM chỉ sử dụng một batch gradient để hạn chế cập nhật chính sách.

Ngoài ra, hai phương pháp dựa trên chính quy hóa sau không cần rehearsal nên chúng độc lập với các phương pháp lựa chọn trải nghiệm.
• Elastic weight consolidation (EWC) [19]: ràng buộc các thay đổi đối với các tham số quan trọng thông qua ma trận thông tin Fisher.
• Synaptic intelligence (SI) [53]: ràng buộc các thay đổi sau mỗi bước tối ưu hóa.

Chúng tôi cũng hiển thị hiệu suất trên đa nhiệm vụ làm tham khảo. Thiết lập học đa nhiệm vụ không gặp phải vấn đề quên thảm họa và có thể được xem là vượt trội.

Tập dữ liệu Tuần tự Ngoại tuyến Chúng tôi xem xét ba bộ nhiệm vụ từ thư viện meta-RL ngoại tuyến điều khiển liên tục được sử dụng rộng rãi như trong [33]:
• Ant-2D Direction (Ant-Dir): huấn luyện một con kiến mô phỏng với 8 khớp nối để chạy theo hướng 2D;
• Walker-2D Params (Walker-Par): huấn luyện một tác nhân mô phỏng để di chuyển về phía trước, trong đó các nhiệm vụ khác nhau có các tham số khác nhau. Cụ thể, các nhiệm vụ khác nhau yêu cầu tác nhân di chuyển với tốc độ khác nhau;
• Half-Cheetah Velocity (Cheetah-Vel): huấn luyện một con báo để chạy với vận tốc ngẫu nhiên.

Đối với mỗi bộ nhiệm vụ, chúng tôi lấy mẫu ngẫu nhiên năm nhiệm vụ để tạo thành các nhiệm vụ tuần tự T₁ đến T₅.

Để xem xét chất lượng dữ liệu khác nhau như [9], chúng tôi huấn luyện một soft actor-critic để thu thập hai benchmark [12] cho mỗi nhiệm vụ T_n, n=1,···,5: 1) Medium (M) với quỹ đạo từ chính sách chất lượng trung bình, và 2) Medium-Random (M-R) bao gồm quỹ đạo từ cả chính sách chất lượng trung bình và quỹ đạo được lấy mẫu ngẫu nhiên.

Metric Theo [7], chúng tôi áp dụng hiệu suất trung bình (PER) và chuyển tiếp ngược (BWT) làm metric đánh giá,

PER = (1/N)Σ_{n=1}^N a_{N,n}, BWT = (1/(N-1))Σ_{n=1}^{N-1} (a_{n,n} - a_{N,n}),     (5)

trong đó a_{i,j} có nghĩa là phần thưởng tích lũy cuối cùng của nhiệm vụ j sau khi học nhiệm vụ i. Đối với PER, cao hơn là tốt hơn; đối với BWT, thấp hơn là tốt hơn. Hai metric này hiển thị hiệu suất của việc học các nhiệm vụ mới trong khi giảm thiểu vấn đề quên thảm họa.

6.2 Kết quả Tổng thể
Đánh giá MBES: Đầu tiên, phương pháp OER của chúng tôi được so sánh với mười một baseline trên hai loại chất lượng M-R và M. Vì OER của chúng tôi bao gồm MBES và DBC, sáu phương pháp lựa chọn trải nghiệm được thêm với DBC để so sánh công bằng. Kết quả tổng thể được báo cáo trong Bảng 1, về PER và BWT trên ba nhiệm vụ tuần tự. Từ Bảng 1, chúng tôi rút ra các kết luận sau: 1) Phương pháp OER của chúng tôi vượt trội hơn tất cả baseline cho tất cả trường hợp, cho thấy hiệu quả của sơ đồ MBES của chúng tôi; 2) Phương pháp OER của chúng tôi thể hiện ưu thế lớn hơn trên tập dữ liệu M-R so với tập dữ liệu M, cho thấy rằng tập dữ liệu M-R có sự dịch chuyển phân phối lớn hơn tập dữ liệu M, và MBES giải quyết sự dịch chuyển như vậy; 3) Random+DBC hoạt động tốt hơn năm baseline khác vì năm sơ đồ lựa chọn trải nghiệm này được dành riêng cho các tình huống trực tuyến chứ không phải ngoại tuyến. Hơn nữa, Hình ?? hiển thị quá trình học của Ant-Dir trên năm nhiệm vụ tuần tự. Từ Hình 4(a) - 4(d), Hình 4(g) - 4(h) và Tài liệu Bổ sung, chúng ta có thể quan sát thấy rằng so với baseline, OER của chúng tôi thể hiện ít sự giảm hiệu suất hơn với sự gia tăng của các nhiệm vụ, cho thấy rằng OER có thể giải quyết tốt hơn việc quên thảm họa.

Đánh giá DBC: Thứ hai, phương pháp OER của chúng tôi được so sánh với năm baseline. Tương tự, các phương pháp học liên tục được thêm với MBES để so sánh công bằng, và hiệu suất tổng thể được báo cáo trong Bảng 2 và Tài liệu Bổ sung. Từ Bảng 2 và Tài liệu Bổ sung, chúng tôi rút ra các kết luận sau: 1) Phương pháp OER của chúng tôi vượt trội hơn tất cả baseline, cho thấy hiệu quả của sơ đồ DBC của chúng tôi; 2) Bốn phương pháp học liên tục không hoạt động tốt do vấn đề quên; 3) MBES+BC hoạt động tệ nhất do sự không nhất quán của hai chính sách π và μ_n trong Phần 4.2. Từ Hình 4(e) - 4(h) và Tài liệu Bổ sung, chúng ta có thể quan sát thấy rằng OER của chúng tôi có thể học các nhiệm vụ mới và nhớ các nhiệm vụ cũ tốt; các phương pháp học liên tục khác chỉ có thể học các nhiệm vụ mới nhưng quên các nhiệm vụ cũ, trong khi BC thậm chí không thể học các nhiệm vụ mới.

6.3 Phân tích Tham số
Kích thước của Bộ đệm B_n Trong học liên tục dựa trên rehearsal, kích thước của bộ đệm phát lại là một yếu tố then chốt. Trong thiết lập CORL của chúng tôi, kích thước của bộ đệm B_n được chọn là 1K cho tất cả n, bằng cách xem xét cả không gian lưu trữ và các vấn đề quên. Với sự gia tăng kích thước bộ đệm, chúng ta cần nhiều không gian lưu trữ hơn nhưng có ít vấn đề quên hơn. Để định lượng phân tích như vậy, chúng tôi xem xét kích thước bộ đệm 10K cho OER và hai baseline, và kết quả được liệt kê trong Bảng 3. Từ Bảng 3, chúng ta có thể quan sát thấy rằng 1) Với sự gia tăng kích thước bộ đệm, OER và hai baseline đạt được hiệu suất tốt hơn như mong đợi. 2) Phương pháp DBC của chúng tôi vẫn tốt hơn nhiều so với BC, cho thấy rằng việc giải quyết sự không nhất quán là đáng kể; 3) Với không gian lưu trữ lớn hơn, baseline Random hoạt động tương tự như MBES, vì trong trường hợp này vấn đề quên trở nên nhỏ hơn nhiều và việc lựa chọn trải nghiệm trở nên không quan trọng.

Hệ số Phát lại λ_r Một yếu tố quan trọng khác là hệ số λ_r trong Phương trình 4, trong đó λ_r được sử dụng để cân bằng hạng BC chống quên và hạng ràng buộc chính sách mới. Trong thiết lập CORL của chúng tôi, chúng tôi chọn λ_r là 1, cũng là lựa chọn chung trong các phương pháp dựa trên ER [42, 14], và hiệu suất tốt đã được đạt được, như đã đề cập ở trên. Chúng tôi phân tích các giá trị khác nhau của λ_r và hiển thị hiệu suất tương ứng của OER và baseline trong Bảng 4, trong đó λ_r được chọn là 0.3, 1 và 3, tương ứng. Từ Bảng 4, chúng ta có thể quan sát thấy rằng với λ_r lớn hơn, vấn đề quên giảm dần, nhưng nó trở nên lỏng lẻo hơn rằng chính sách học π clone μ_n trong Phương trình 4, và ngược lại. Kết quả là, chúng tôi đạt được hiệu suất tốt nhất khi λ_r=1. Đây là lý do tại sao chúng tôi sử dụng λ_r=1 cho tất cả các thí nghiệm trong bài báo này.

7 Kết luận
Trong công trình này, chúng tôi xây dựng thiết lập CORL mới và trình bày phương pháp OER mới, chủ yếu bao gồm hai thành phần chính: MBES và DBC. Chúng tôi chỉ ra một vấn đề độ lệch phân phối mới và bất ổn định huấn luyện độc đáo với thiết lập CORL mới. Cụ thể, để giải quyết vấn đề dịch chuyển phân phối mới trong CORL, chúng tôi đề xuất sơ đồ MBES mới để chọn các trải nghiệm có giá trị từ tập dữ liệu ngoại tuyến để xây dựng bộ đệm phát lại. Hơn nữa, để giải quyết vấn đề không nhất quán giữa việc học nhiệm vụ mới và clone các nhiệm vụ cũ, chúng tôi đề xuất sơ đồ DBC mới. Thí nghiệm và phân tích cho thấy rằng OER vượt trội hơn các baseline SOTA trên các nhiệm vụ điều khiển liên tục khác nhau.
