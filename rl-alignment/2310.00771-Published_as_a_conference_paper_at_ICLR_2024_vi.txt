# Huấn luyện trước với dữ liệu tổng hợp giúp ích cho học tăng cường ngoại tuyến

Gần đây, đã được chứng minh rằng đối với học tăng cường sâu ngoại tuyến (DRL), việc huấn luyện trước Decision Transformer với một kho ngữ liệu ngôn ngữ lớn có thể cải thiện hiệu suất hạ nguồn (Reid et al., 2022). Một câu hỏi tự nhiên được đặt ra là liệu mức độ cải thiện hiệu suất này chỉ có thể đạt được với việc huấn luyện trước ngôn ngữ, hay có thể đạt được với các sơ đồ huấn luyện trước đơn giản hơn không liên quan đến ngôn ngữ. Trong bài báo này, trước tiên chúng tôi chỉ ra rằng ngôn ngữ không phải là yếu tố thiết yếu để cải thiện hiệu suất, và thực sự việc huấn luyện trước với dữ liệu IID tổng hợp trong một số ít các bước cập nhật có thể đạt được mức cải thiện hiệu suất tương đương với việc huấn luyện trước với một kho ngữ liệu ngôn ngữ lớn; hơn nữa, việc huấn luyện trước với dữ liệu được tạo ra bởi một chuỗi Markov một bước có thể cải thiện hiệu suất hơn nữa. Được truyền cảm hứng từ những kết quả thực nghiệm này, sau đó chúng tôi xem xét việc huấn luyện trước Conservative Q-Learning (CQL), một thuật toán DRL ngoại tuyến phổ biến, sử dụng backbone Multi-Layer Perceptron (MLP) và thường dựa trên Q-learning. Đáng ngạc nhiên, việc huấn luyện trước với dữ liệu tổng hợp đơn giản trong một số ít các bước cập nhật cũng có thể cải thiện CQL, mang lại sự cải thiện hiệu suất nhất quán trên các bộ dữ liệu D4RL Gym locomotion. Kết quả của bài báo này không chỉ minh họa tầm quan trọng của việc huấn luyện trước cho DRL ngoại tuyến mà còn chỉ ra rằng dữ liệu huấn luyện trước có thể là tổng hợp và được tạo ra với các cơ chế đơn giản đáng kinh ngạc.

## GIỚI THIỆU

Việc huấn luyện trước có thể mang lại những cải thiện đáng kể về hiệu suất và tính mạnh mẽ cho các tác vụ hạ nguồn, cả trong Xử lý Ngôn ngữ Tự nhiên (NLP) và Thị giác Máy tính (CV), điều này đã được biết đến rộng rãi. Gần đây, trong lĩnh vực Học Tăng cường Sâu (DRL), nghiên cứu về huấn luyện trước cũng đang trở nên ngày càng phổ biến. Một bước quan trọng theo hướng huấn luyện trước các mô hình DRL là bài báo gần đây của Reid et al. (2022), đã chỉ ra rằng đối với Decision Transformer (DT) (Chen et al., 2021), việc huấn luyện trước với kho ngữ liệu Wikipedia có thể cải thiện đáng kể hiệu suất của tác vụ RL ngoại tuyến hạ nguồn. Reid et al. (2022) cũng chỉ ra rằng việc huấn luyện trước để dự đoán các chuỗi pixel làm giảm hiệu suất. Các tác giả tuyên bố rằng kết quả của họ chỉ ra "một tương lai có thể dự đoán được khi mọi người nên sử dụng một mô hình ngôn ngữ được huấn luyện trước cho RL ngoại tuyến". Trong một bài báo gần đây hơn, Takagi (2022) khám phá sâu hơn tại sao việc huấn luyện trước với kho ngữ liệu ngôn ngữ có thể cải thiện DT. Tuy nhiên, vẫn chưa rõ liệu dữ liệu ngôn ngữ có đặc biệt trong việc mang lại lợi ích như vậy hay không, hoặc liệu các phương pháp huấn luyện trước naif hơn có thể đạt được hiệu quả tương tự. Hiểu được câu hỏi quan trọng này có thể giúp chúng ta phát triển các sơ đồ huấn luyện trước tốt hơn cho các thuật toán DRL có hiệu suất cao hơn, mạnh mẽ hơn và hiệu quả hơn.

Đầu tiên chúng tôi khám phá việc huấn luyện trước Decision Transformer (DT) với dữ liệu tổng hợp được tạo ra từ một phương pháp đơn giản và có vẻ naif. Cụ thể, chúng tôi tạo ra một Chuỗi Markov trạng thái hữu hạn với một số lượng nhỏ các trạng thái (100 trạng thái theo mặc định). Ma trận chuyển đổi của chuỗi Markov được lấy ngẫu nhiên và không liên quan đến môi trường hoặc các bộ dữ liệu ngoại tuyến. Sử dụng MC một bước, chúng tôi tạo ra một chuỗi các trạng thái MC tổng hợp. Trong quá trình huấn luyện trước, chúng tôi coi mỗi trạng thái MC trong chuỗi như một token, đưa chuỗi vào transformer, và sử dụng dự đoán trạng thái tiếp theo (token) tự hồi quy, như thường được thực hiện trong các LLM dựa trên transformer (Brown et al., 2020). Chúng tôi huấn luyện trước với dữ liệu tổng hợp trong một số lượng tương đối nhỏ các bước cập nhật so với số lượng bước cập nhật huấn luyện trước ngôn ngữ trong (Reid et al., 2022). Sau khi huấn luyện trước với dữ liệu tổng hợp, sau đó chúng tôi tinh chỉnh với một bộ dữ liệu ngoại tuyến cụ thể sử dụng thuật toán DT DRL ngoại tuyến. Đáng ngạc nhiên, phương pháp đơn giản này vượt trội đáng kể so với DT tiêu chuẩn (tức là không có huấn luyện trước) và cũng vượt trội so với việc huấn luyện trước với kho ngữ liệu Wiki lớn. Ngoài ra, chúng tôi chỉ ra rằng ngay cả việc huấn luyện trước với dữ liệu Độc lập và Phân phối Đồng nhất (IID) vẫn có thể đạt được hiệu suất tương đương với việc huấn luyện trước Wiki.

Được truyền cảm hứng từ những kết quả này, sau đó chúng tôi xem xét việc huấn luyện trước Conservative Q-Learning (CQL) (Kumar et al., 2020) sử dụng backbone Multi-Layer Perceptron (MLP). Ở đây, chúng tôi tạo ra ngẫu nhiên một chính sách và xác suất chuyển đổi, từ đó chúng tôi tạo ra một chuỗi các cặp trạng thái-hành động Quá trình Quyết định Markov (MDP). Sau đó chúng tôi đưa các cặp trạng thái-hành động vào các MLP Q-network và huấn luyện trước chúng bằng cách dự đoán trạng thái tiếp theo. Sau đó, chúng tôi tinh chỉnh chúng với một bộ dữ liệu ngoại tuyến cụ thể sử dụng CQL. Đáng ngạc nhiên, việc huấn luyện trước với dữ liệu IID và MDP đều có thể tăng cường CQL.

Các thí nghiệm và ablation rộng rãi của chúng tôi cho thấy rằng việc huấn luyện trước các mô hình DRL ngoại tuyến với các bộ dữ liệu tổng hợp đơn giản có thể cải thiện đáng kể hiệu suất so với những mô hình không có huấn luyện trước, cả cho backbone dựa trên transformer và MLP, với chi phí tính toán thấp. Kết quả cũng cho thấy rằng các bộ dữ liệu ngôn ngữ lớn không cần thiết để có được sự tăng cường hiệu suất, điều này làm sáng tỏ loại chiến lược huấn luyện trước nào quan trọng để cải thiện hiệu suất RL và ủng hộ việc tăng cường sử dụng huấn luyện trước với dữ liệu tổng hợp để có được sự tăng cường hiệu suất dễ dàng và nhất quán.

## CÔNG VIỆC LIÊN QUAN

Nhiều ứng dụng thực tế của RL hạn chế các agent học từ một bộ dữ liệu ngoại tuyến đã được thu thập sẵn, mà không có tương tác thêm với môi trường (Fujimoto et al., 2019; Levine et al., 2020). Các bài báo DRL ngoại tuyến ban đầu thường sử dụng kiến trúc Multi-Layer Perceptron (MLP) (Fujimoto et al., 2019; Chen et al., 2020; Kumar et al., 2020; Kostrikov et al., 2021). Gần đây hơn, đã có sự quan tâm đáng kể đến các kiến trúc dựa trên transformer cho DRL ngoại tuyến, bao gồm DT (Chen et al., 2021), Trajectory Transformer (Janner et al., 2021) và các mô hình khác (Furuta et al., 2021; Li et al., 2023). Trong bài báo này, chúng tôi nghiên cứu cả các phương pháp dựa trên transformer và các phương pháp dựa trên Q-learning truyền thống hơn để hiểu cách các sơ đồ huấn luyện trước khác nhau có thể ảnh hưởng đến hiệu suất của chúng.

Việc huấn luyện trước có thể mang lại những cải thiện đáng kể về hiệu suất và tính mạnh mẽ cho các tác vụ hạ nguồn, cả trong Xử lý Ngôn ngữ Tự nhiên (NLP) (Devlin et al., 2018; Radford et al., 2018; Brown et al., 2020) và Thị giác Máy tính (CV) (Donahue et al., 2014; Huh et al., 2016; Kornblith et al., 2019), điều này đã được biết đến rộng rãi. Trong DRL ngoại tuyến, huấn luyện trước đang trở thành một chủ đề nghiên cứu ngày càng phổ biến. Một bước quan trọng theo hướng huấn luyện trước các mô hình DRL ngoại tuyến là Reid et al. (2022), chỉ ra rằng đối với DT, việc huấn luyện trước trên Wikipedia có thể cải thiện đáng kể hiệu suất của tác vụ RL hạ nguồn. Takagi (2022) khám phá sâu hơn tại sao việc huấn luyện trước như vậy cải thiện DT. Được truyền cảm hứng từ những phát hiện gần đây này, chúng tôi hướng tới một sự hiểu biết toàn diện hơn về huấn luyện trước trong DRL.

Cũng có các công việc huấn luyện trước trên dữ liệu hình ảnh chung hoặc sử dụng chính dữ liệu DRL ngoại tuyến để học các biểu diễn và sau đó sử dụng chúng để học các tác vụ DRL ngoại tuyến hoặc trực tuyến (Yang & Nachum, 2021; Zhan et al., 2021; Wang et al., 2022; Shah & Kumar, 2021; Hansen et al., 2022; Nair et al., 2022; Parisi et al., 2022; Radosavovic et al., 2023; Karamcheti et al., 2023). Xie et al. (2023) chỉ ra rằng huấn luyện trước không giám sát có điều kiện tương lai dẫn đến hiệu suất vượt trội trong cài đặt ngoại tuyến-trực tuyến. Khác với những công việc này, chúng tôi tập trung vào việc hiểu liệu huấn luyện trước ngôn ngữ có đặc biệt trong việc mang lại sự tăng cường hiệu suất và điều tra liệu huấn luyện trước tổng hợp có thể giúp ích cho DRL.

Huấn luyện trước với dữ liệu tổng hợp đã được chỉ ra là có lợi cho một loạt các tác vụ NLP hạ nguồn (Papadimitriou & Jurafsky, 2020; Krishna et al., 2021; Ri & Tsuruoka, 2022; Wu et al., 2022; Chiang & Lee, 2022), các tác vụ CV (Kataoka et al., 2020; Anderson & Farrell, 2022), và các tác vụ lý luận toán học (Wu et al., 2021). Cũng có các công việc nghiên cứu tác động của các thuộc tính khác nhau của dữ liệu NLP tổng hợp Ri & Tsuruoka (2022); Chiang & Lee (2022); He et al. (2022b). Đặc biệt, chúng tôi cung cấp kết quả cho thấy các sơ đồ dữ liệu tổng hợp Identity và Case-Mapping từ He et al. (2022b) cũng có thể cải thiện hiệu suất RL ngoại tuyến trong Phụ lục F. Trong khi những công việc này tập trung vào các ứng dụng CV và NLP, chúng tôi nghiên cứu tác động của huấn luyện trước từ dữ liệu tổng hợp với khoảng cách miền lớn trong DRL. Theo hiểu biết tốt nhất của chúng tôi, đây là bài báo đầu tiên chỉ ra rằng huấn luyện trước trên dữ liệu tổng hợp đơn giản có thể là một phương pháp hiệu quả đáng ngạc nhiên để cải thiện hiệu suất DRL ngoại tuyến cho cả các phương pháp dựa trên transformer và dựa trên Q-learning.

## HUẤN LUYỆN TRƯỚC DECISION TRANSFORMER VỚI DỮ LIỆU TỔNG HỢP

### TỔNG QUAN VỀ DECISION TRANSFORMER

Chen et al. (2021) đã giới thiệu Decision Transformer (DT), một thuật toán dựa trên transformer cho RL ngoại tuyến. Một bộ dữ liệu ngoại tuyến bao gồm các quỹ đạo s₁, a₁, r₁, ..., sₙ, aₙ, trong đó sₙ, aₙ, và rₙ là trạng thái, hành động, và phần thưởng tại bước thời gian n. DT mô hình hóa các quỹ đạo bằng cách biểu diễn chúng như

σ = (R̂₁, s₁, a₁, ..., R̂ₙ, sₙ, aₙ),

trong đó R̂ₙ = Σᵢ₌ₙᴺ rᵢ là return-to-go tại bước thời gian n. Chuỗi σ được mô hình hóa với một transformer theo cách tự hồi quy tương tự như mô hình hóa ngôn ngữ tự hồi quy ngoại trừ việc R̂ₙ, sₙ, aₙ tại cùng bước thời gian n trước tiên được chiếu thành các embedding riêng biệt trong khi nhận cùng một embedding vị trí. Trong Chen et al. (2021), mô hình được tối ưu hóa để dự đoán mỗi hành động aₙ từ (R̂₁, s₁, a₁, ..., R̂ₙ₋₁, sₙ₋₁, aₙ₋₁, R̂ₙ, sₙ). Sau khi mô hình được huấn luyện với các quỹ đạo ngoại tuyến, tại thời điểm kiểm tra, hành động tại bước thời gian t được chọn bằng cách đưa vào transformer đã được huấn luyện quỹ đạo kiểm tra (R̂₁, s₁, a₁, ..., R̂ₜ, sₜ), trong đó R̂ₜ bây giờ là một ước tính của return-to-go tối ưu.

Trong bài báo DT gốc (Chen et al., 2021), không có huấn luyện trước, tức là huấn luyện bắt đầu với các trọng số ban đầu ngẫu nhiên. Reid et al. (2022) xem xét việc huấn luyện trước transformer bằng kho ngữ liệu Wikipedia trước, sau đó tinh chỉnh với thuật toán DT để tạo ra một chính sách cho tác vụ RL ngoại tuyến hạ nguồn.

### TẠO DỮ LIỆU CHUỖI MARKOV TỔNG HỢP

Chúng tôi khám phá việc huấn luyện trước DT với dữ liệu tổng hợp được tạo ra từ một Chuỗi Markov. Đối với dữ liệu tổng hợp, chúng tôi đơn giản tạo ra một chuỗi các trạng thái (token) sử dụng một Chuỗi Markov trạng thái hữu hạn với một số lượng nhỏ các trạng thái. Các xác suất chuyển đổi của chuỗi Markov được lấy ngẫu nhiên (như được mô tả bên dưới) và không liên quan đến môi trường hoặc bộ dữ liệu ngoại tuyến. Sau khi tạo ra dữ liệu chuỗi tổng hợp, trong quá trình huấn luyện trước, chúng tôi đưa chuỗi vào transformer và sử dụng dự đoán trạng thái tiếp theo (token), như thường được thực hiện trong các LLM dựa trên transformer (Brown et al., 2020). Sau khi huấn luyện trước với dữ liệu tổng hợp, sau đó chúng tôi tinh chỉnh với bộ dữ liệu ngoại tuyến mục tiêu sử dụng DT.

Chúng tôi tạo ra các xác suất chuyển đổi MC như sau. Gọi S = 1, 2, ..., M biểu thị không gian trạng thái hữu hạn của MC, với M = 100 là giá trị mặc định. Đối với mỗi trạng thái trong S, chúng tôi rút M giá trị độc lập và phân phối đều, và sau đó tạo ra một phân phối trên không gian trạng thái S bằng cách áp dụng softmax cho vector M giá trị. Theo cách này, chúng tôi tạo ra M phân phối xác suất, một cho mỗi trạng thái, trong đó mỗi phân phối là trên S. Sử dụng các xác suất chuyển đổi cố định này, chúng tôi tạo ra chuỗi huấn luyện trước x₀, x₁, ..., xₜ như sau: chúng tôi lấy mẫu ngẫu nhiên từ S để có được trạng thái ban đầu x₀ trong chuỗi; sau khi có được xₜ, chúng tôi tạo ra xₜ₊₁ sử dụng các xác suất chuyển đổi MC.

Trong quá trình huấn luyện trước, chúng tôi huấn luyện với dự đoán trạng thái tiếp theo tự hồi quy (Brown et al., 2020):

L(x₀, x₁, ..., xₜ; θ) = -log Pθ(x₀, x₁, ..., xₜ) = -Σᵢ₌₁ᵀ log Pθ(xᵢ|x₀, x₁, ..., xᵢ₋₁).

Vì các trạng thái là rời rạc và tương tự như các token trong các tác vụ mô hình hóa ngôn ngữ, các embedding cho các trạng thái được học trong quá trình huấn luyện trước như thường được thực hiện trong tài liệu NLP.

### KẾT QUẢ CHO HUẤN LUYỆN TRƯỚC DT VỚI DỮ LIỆU TỔNG HỢP

Đầu tiên chúng tôi so sánh hiệu suất của baseline DT (DT), DT với huấn luyện trước Wikipedia (DT+Wiki), và DT với huấn luyện trước trên dữ liệu tổng hợp được tạo ra từ MC 1 bước với 100 trạng thái (DT+Synthetic). Chúng tôi xem xét cùng ba môi trường MuJoCo và bộ dữ liệu D4RL (Fu et al., 2020) được xem xét trong Reid et al. (2022) cộng với môi trường Ant có chiều cao, tổng cộng 12 bộ dữ liệu. Để so sánh công bằng, chúng tôi sử dụng mã của các tác giả từ Reid et al. (2022) khi chạy các thí nghiệm hạ nguồn cho DT, và chúng tôi giữ các siêu tham số giống hệt với những siêu tham số được sử dụng trong (Chen et al., 2021; Reid et al., 2022) bất cứ khi nào có thể (Chi tiết trong Phụ lục A.1). Đối với mỗi bộ dữ liệu, chúng tôi tinh chỉnh trong 100.000 bước cập nhật. Đối với DT+Wiki, chúng tôi thực hiện 80K bước cập nhật trong quá trình huấn luyện trước theo các tác giả. Đối với DT+Synthetic, tuy nhiên, chúng tôi phát hiện rằng chúng tôi có thể đạt được hiệu suất tốt với ít bước cập nhật huấn luyện trước hơn nhiều, cụ thể là 20K bước cập nhật. Sau mỗi 5K bước cập nhật trong quá trình tinh chỉnh, chúng tôi chạy 10 quỹ đạo đánh giá và ghi lại hiệu suất kiểm tra được chuẩn hóa. Chúng tôi báo cáo cả hiệu suất cuối cùng và, trong Phụ lục B, hiệu suất tốt nhất. Đối với hiệu suất tốt nhất, chúng tôi sử dụng hiệu suất kiểm tra tốt nhất được thấy trong toàn bộ giai đoạn tinh chỉnh. Hiệu suất tốt nhất cũng được sử dụng trong (Chen et al., 2021; Reid et al., 2022). Trong thực tế, để xác định khi nào hiệu suất tốt nhất xảy ra (và do đó các tham số mô hình tốt nhất), cần có tương tác với môi trường, điều này không phù hợp với công thức bài toán ngoại tuyến. Hiệu suất cuối cùng có thể là một chỉ số tốt hơn vì nó không giả định rằng chúng ta có thể tương tác với môi trường. Đối với hiệu suất cuối cùng, chúng tôi tính trung bình trên bốn tập đánh giá cuối cùng (sau 85K, 90K, 95K, và 100K bước cập nhật cho DT). Khi so sánh các thuật toán, hai biện pháp (cuối cùng và tốt nhất) dẫn đến các kết luận định tính rất tương tự. Đối với tất cả các biến thể DT, chúng tôi báo cáo giá trị trung bình và độ lệch chuẩn của hiệu suất trên 20 seed ngẫu nhiên.

Bảng 1 cho thấy hiệu suất cuối cùng cho DT, DT được huấn luyện trước với dữ liệu Wiki, và DT được huấn luyện trước với dữ liệu MC tổng hợp. Chúng ta thấy rằng, đối với mọi bộ dữ liệu, huấn luyện trước tổng hợp hoạt động tốt như hoặc tốt hơn baseline DT, và mang lại sự cải thiện trung bình tổng thể gần 10%. Hơn nữa, huấn luyện trước tổng hợp cũng vượt trội hơn huấn luyện trước Wiki 5% khi tính trung bình trên tất cả các bộ dữ liệu, và điều này được thực hiện với ít bước cập nhật huấn luyện trước hơn đáng kể. So với DT+Wiki, DT+Synthetic hiệu quả tính toán hơn nhiều, chỉ sử dụng 3% tính toán trong quá trình huấn luyện trước và 67% trong quá trình tinh chỉnh (Chi tiết trong Phụ lục A). Hình 1 cho thấy điểm số được chuẩn hóa và mất mát huấn luyện cho DT, DT với huấn luyện trước Wiki, và DT với huấn luyện trước MC. Các đường cong được tổng hợp trên tất cả 12 bộ dữ liệu, mỗi bộ với 20 seed khác nhau (đường cong theo môi trường trong Phụ lục B.2). Để tính đến các bước cập nhật huấn luyện trước, chúng tôi cũng dịch chuyển đường cong cho DT+Synthetic sang phải 20K bước cập nhật. Lưu ý rằng trong thực tế, huấn luyện trước chỉ cần được thực hiện một lần, nhưng việc dịch chuyển ở đây giúp chỉ ra rằng ngay cả với bất lợi này, DT+Synthetic vẫn nhanh chóng vượt trội hơn hai biến thể kia.

Dữ liệu tổng hợp của chúng tôi sử dụng không gian trạng thái nhỏ (từ vựng) và không mang thông tin ngữ cảnh hoặc ngữ nghĩa dài hạn. Từ Bảng 1 và Hình 1 chúng ta có thể kết luận rằng những cải thiện hiệu suất có được bằng cách huấn luyện trước với kho ngữ liệu Wikipedia không phải do các thuộc tính đặc biệt của ngôn ngữ, chẳng hạn như từ vựng lớn hoặc thông tin ngữ cảnh và ngữ nghĩa dài hạn phong phú trong bộ dữ liệu, như được phỏng đoán trong Reid et al. (2022) và Takagi (2022). Trong phần con tiếp theo, chúng tôi nghiên cứu cách các thuộc tính khác nhau của dữ liệu tổng hợp ảnh hưởng đến hiệu suất RL hạ nguồn.

### ABLATION CHO HUẤN LUYỆN TRƯỚC DT VỚI DỮ LIỆU TỔNG HỢP

Trong các kết quả trên, chúng tôi đã sử dụng MC một bước để tạo ra dữ liệu tổng hợp. Trong ngôn ngữ tự nhiên, các phụ thuộc token không chỉ đơn giản là phụ thuộc một bước. Bây giờ chúng tôi xem xét liệu việc tăng các phụ thuộc trạng thái vượt quá một bước có thể cải thiện hiệu suất hạ nguồn hay không. Cụ thể, chúng tôi xem xét việc sử dụng Chuỗi Markov nhiều bước để tạo ra dữ liệu tổng hợp x₀, x₁, ..., xₜ. Trong chuỗi Markov n-bước, xₜ phụ thuộc vào xₜ₋₁, xₜ₋₂, ..., xₜ₋ₙ. Đối với MC n-bước, chúng tôi xây dựng ngẫu nhiên các xác suất chuyển đổi n-bước cố định, từ đó chúng tôi tạo ra dữ liệu tổng hợp. Bảng 2 cho thấy hiệu suất cuối cùng được tính trung bình trên bốn giai đoạn đánh giá cuối cùng cho baseline DT và cho huấn luyện trước MC với số lượng bước MC khác nhau. Chúng ta thấy rằng dữ liệu tổng hợp với các giá trị bước khác nhau đều mang lại hiệu suất tốt hơn baseline DT; tuy nhiên, việc tăng lượng phụ thuộc quá khứ trong dữ liệu tổng hợp MC không cải thiện hiệu suất so với MC một bước.

Bây giờ chúng tôi điều tra liệu việc tăng kích thước không gian trạng thái MC (tương tự như việc tăng kích thước từ vựng trong NLP) có cải thiện hiệu suất hay không. Bảng 3 cho thấy hiệu suất cuối cùng cho baseline DT và DT được huấn luyện trước với dữ liệu MC với các kích thước không gian trạng thái khác nhau. Kết quả cho thấy tất cả các kích thước không gian trạng thái đều cải thiện hiệu suất so với baseline, với 100 và 1000 mang lại kết quả tốt nhất.

Bây giờ chúng tôi xem xét cách thay đổi tham số nhiệt độ trong công thức softmax ảnh hưởng đến kết quả. (Nhiệt độ mặc định là 1.0.) Nhiệt độ thấp hơn dẫn đến chuyển đổi trạng thái xác định hơn, trong khi nhiệt độ cao hơn dẫn đến chuyển đổi trạng thái đồng nhất hơn. Bảng 4 cho thấy hiệu suất cuối cùng cho DT với huấn luyện trước MC với các giá trị nhiệt độ khác nhau. Kết quả cho thấy tất cả các nhiệt độ đều mang lại cải thiện hiệu suất, với nhiệt độ 1 là tốt nhất. Trong bảng này, chúng tôi cũng xem xét việc tạo ra dữ liệu tổng hợp với các trạng thái Độc lập và Phân phối Đồng nhất (IID) với phân phối đều trên không gian trạng thái có kích thước 100. Đáng ngạc nhiên, ngay cả sơ đồ này cũng hoạt động tốt hơn đáng kể so với cả baseline và huấn luyện trước Wiki. Điều này cung cấp bằng chứng thêm rằng các phụ thuộc token phức tạp trong kho ngữ liệu Wiki không có khả năng là nguyên nhân của sự tăng cường hiệu suất.

Bảng 5 cho thấy hiệu suất cuối cùng cho DT với huấn luyện trước MC với số lượng bước cập nhật huấn luyện trước khác nhau. Kết quả của chúng tôi cho thấy rằng chỉ với 1k bước cập nhật, huấn luyện trước MC đã đạt được hiệu suất của huấn luyện trước DT+Wiki. Sử dụng ít nhất là 20k bước cập nhật (một phần tư của DT+Wiki), phương pháp của chúng tôi đã có được hiệu suất tốt hơn đáng kể.

Những kết quả ablation này cho thấy rằng huấn luyện trước tổng hợp mạnh mẽ trên các cài đặt khác nhau của dữ liệu tổng hợp, bao gồm mức độ phụ thuộc quá khứ, kích thước không gian trạng thái MC, mức độ ngẫu nhiên trong các chuyển đổi, và số lượng bước cập nhật huấn luyện trước.

## HUẤN LUYỆN TRƯỚC CQL VỚI DỮ LIỆU TỔNG HỢP

Cho rằng huấn luyện trước với dữ liệu tổng hợp có thể tăng đáng kể hiệu suất của DT, bây giờ chúng tôi nghiên cứu liệu dữ liệu tổng hợp cũng có thể giúp ích cho các thuật toán DRL ngoại tuyến dựa trên MLP khác hay không. Cụ thể, chúng tôi xem xét CQL, một thuật toán DRL ngoại tuyến phổ biến cho các bộ dữ liệu được xem xét trong bài báo này. Đối với mục tiêu huấn luyện trước, chúng tôi sử dụng dự đoán động lực học thuận, vì nó đã được chỉ ra là hữu ích trong các phương pháp dựa trên mô hình (Janner et al., 2019) và tài liệu mất mát phụ (He et al., 2022a). Vì dự đoán động lực học thuận sẽ yêu cầu cả trạng thái và hành động làm đầu vào, chúng tôi tạo ra một loại dữ liệu tổng hợp mới, mà chúng tôi gọi là dữ liệu Quá trình Quyết định Markov (MDP) tổng hợp. Khác với MC tổng hợp, khi tạo ra dữ liệu MDP tổng hợp, chúng tôi cũng xem xét các hành động.

### TẠO DỮ LIỆU MDP TỔNG HỢP

Để tạo ra dữ liệu MDP tổng hợp, trước tiên chúng tôi định nghĩa một không gian trạng thái rời rạc S, một không gian hành động rời rạc A, một phân phối chính sách ngẫu nhiên π, và một phân phối chuyển đổi ngẫu nhiên p. Tương tự như cách chúng tôi tạo ra MC cho decision transformer, các phân phối chính sách và chuyển đổi được thu được bằng cách áp dụng hàm softmax trên các vector giá trị ngẫu nhiên, và hình dạng của các phân phối được điều khiển bởi một số hạng nhiệt độ τ. Đối với mỗi quỹ đạo trong dữ liệu được tạo ra, chúng tôi bắt đầu bằng cách chọn một trạng thái từ không gian trạng thái, và sau đó cho mỗi bước tiếp theo trong quỹ đạo, chúng tôi lấy mẫu một hành động từ phân phối chính sách và sau đó lấy mẫu một trạng thái từ phân phối chuyển đổi. Vì CQL sử dụng mạng MLP và các chiều trạng thái và hành động khác nhau cho mỗi tác vụ MuJoCo, trong quá trình huấn luyện trước chúng tôi ánh xạ mỗi trạng thái MDP rời rạc và hành động MDP thành một vector có cùng chiều với tác vụ RL MuJoCo. Đối với mỗi vector trạng thái và hành động, các mục được chọn ngẫu nhiên từ một phân phối đều giữa -1 và 1, và sau đó cố định.

Chúng tôi huấn luyện trước MLP với mục tiêu động lực học thuận, tức là chúng tôi dự đoán trạng thái tiếp theo ŝ' và giảm thiểu mất mát MSE (ŝ' - s')², trong đó s' là trạng thái tiếp theo thực tế trong quỹ đạo. Sau khi huấn luyện trước, sau đó chúng tôi tinh chỉnh MLP với một bộ dữ liệu cụ thể sử dụng thuật toán CQL.

### KẾT QUẢ CHO CQL VỚI HUẤN LUYỆN TRƯỚC DỮ LIỆU TỔNG HỢP

Trong các kết quả thực nghiệm được trình bày ở đây, đối với baseline CQL, chúng tôi huấn luyện trong 1 triệu bước cập nhật. Đối với CQL với huấn luyện trước MDP tổng hợp (CQL+MDP), chúng tôi huấn luyện trước chỉ trong 100K bước cập nhật và sau đó huấn luyện (tức là tinh chỉnh) trong 1 triệu bước cập nhật. Theo mặc định, chúng tôi đặt kích thước không gian trạng thái và hành động là 100 và sử dụng nhiệt độ τ = 1 cho cả phân phối chính sách và chuyển đổi. Tất cả kết quả đều trên 20 seed ngẫu nhiên. Chúng tôi không điều chỉnh bất kỳ siêu tham số nào cho CQL hoặc CQL với huấn luyện trước tổng hợp mà trực tiếp áp dụng những siêu tham số mặc định trong codebase được khuyến nghị bởi các tác giả CQL. Chi tiết thêm về các thí nghiệm CQL có thể được tìm thấy trong Phụ lục A.2.

Bảng 6 so sánh hiệu suất của CQL với CQL+MDP. Bảng bao gồm một loạt các kích thước không gian trạng thái/hành động MDP và cho thấy rằng huấn luyện trước tổng hợp mang lại sự tăng cường hiệu suất đáng kể và nhất quán. Với 1.000 trạng thái/hành động, huấn luyện trước tổng hợp mang lại 10% cải thiện trung bình trên tất cả các bộ dữ liệu và lên đến 84% và 49% cho hai bộ dữ liệu medium-expert.

Bảng 7 cho thấy hiệu suất cuối cùng cho CQL+MDP với các giá trị nhiệt độ khác nhau sử dụng kích thước không gian trạng thái/hành động mặc định. Kết quả cho thấy rằng nhiệt độ quá nhỏ hoặc quá lớn có thể giảm sự tăng cường hiệu suất, trong khi nhiệt độ mặc định (τ = 1) mang lại hiệu suất cuối cùng tốt khi tính trung bình trên tất cả các bộ dữ liệu. Bảng 7 cũng cho thấy kết quả với dữ liệu tổng hợp IID phân phối đều, tương đương với việc sử dụng nhiệt độ lớn vô hạn. Đáng ngạc nhiên, dữ liệu IID hoạt động gần như tốt bằng dữ liệu MDP tổng hợp, cho thấy tính mạnh mẽ của huấn luyện trước tổng hợp bất kể phụ thuộc trạng thái. Chúng tôi cung cấp giải thích lý thuyết một phần cho hành vi này trong phần con tiếp theo.

Bảng 8 cho thấy hiệu suất cuối cùng cho CQL+MDP với số lượng bước cập nhật huấn luyện trước khác nhau. Ngay cả chỉ với 1K bước cập nhật, huấn luyện trước MDP tổng hợp vẫn vượt trội hơn baseline. Sự tăng cường hiệu suất tốt nhất đạt được với nhiều bước cập nhật huấn luyện trước hơn là 100K và 500K.

Hình 2 cho thấy điểm số được chuẩn hóa và mất mát huấn luyện (mất mát Q cộng mất mát bảo thủ CQL) được tính trung bình trên tất cả các bộ dữ liệu trong quá trình tinh chỉnh. Tương tự như Hình 1, các thí nghiệm tổng hợp của chúng tôi (CQL+MDP và CQL+IID) đã được dịch chuyển 100k bước cập nhật. Cả hai sơ đồ huấn luyện trước đều bắt đầu vượt trội hơn baseline CQL vào khoảng 400K bước cập nhật và duy trì lợi thế hiệu suất đáng kể từ đó trở đi. Ngoài ra, việc thực hiện một bước cập nhật huấn luyện trước khá nhanh vì mục tiêu động lực học thuận chỉ liên quan đến việc tính toán mất mát MSE của việc dự đoán trạng thái tiếp theo và lan truyền ngược của Q-network. 100K bước huấn luyện trước có thể được thực hiện trong 5 phút trên một GPU duy nhất.

Tóm lại, những kết quả này cho thấy rằng đối với một loạt các cài đặt dữ liệu MDP, huấn luyện trước với dữ liệu tổng hợp mang lại sự cải thiện hiệu suất nhất quán so với baseline CQL. Do hạn chế về không gian, một số thí nghiệm và phân tích bổ sung được trình bày trong Phụ lục E, F, G, H, I, J, K.

### PHÂN TÍCH MỤC TIÊU TỐI ƯU HÓA

Để có được một số hiểu biết về tại sao dữ liệu tổng hợp IID hoạt động gần như tốt bằng dữ liệu MDP, bây giờ chúng tôi xem xét kỹ hơn hàm mất mát huấn luyện trước. Gọi fθ(s, a) là một MLP nhận đầu vào là một cặp trạng thái-hành động (s, a) và đầu ra là một vector trạng thái s'. Gọi σ = (s₀, a₀, s₁, a₁, ..., sₜ₋₁, aₜ₋₁, sₜ) biểu thị dữ liệu huấn luyện trước, trong đó các trạng thái và hành động đến từ các không gian trạng thái và hành động hữu hạn S và A. Đối với dữ liệu huấn luyện trước σ cho trước, chúng tôi tối ưu hóa θ để giảm thiểu mục tiêu động lực học thuận:

J(θ) = Σᵢ₌₀ᵀ⁻¹ ||fθ(sᵢ, aᵢ) - sᵢ₊₁||²

Gọi Δ(s, a) là tập hợp các trạng thái s' trực tiếp theo sau (s, a) trong bộ dữ liệu huấn luyện trước σ. Nếu s' trực tiếp theo sau (s, a) nhiều lần, chúng tôi liệt kê s' lặp lại trong Δ(s, a) cho mỗi lần xuất hiện. Sau đó chúng tôi có thể viết lại J(θ) như

J(θ) = Σ(s,a)∈S×A Σs'∈Δ(s,a) ||fθ(s, a) - s'||²

Bây giờ giả sử rằng MLP rất biểu cảm để chúng ta có thể chọn θ sao cho fθ(s, a) có thể nhận bất kỳ vector mong muốn x nào. Đối với (s, a) cố định, gọi

x*(s, a) = arg min_x Σs'∈Δ(s,a) ||x - s'||²

Lưu ý rằng x*(s, a) đơn giản là trọng tâm cho dữ liệu trong Δ(s, a). Do đó

min_θ J(θ) = Σ(s,a)∈S×A Σs'∈Δ(s,a) ||x*(s, a) - s'||²

Nói cách khác, mục tiêu động lực học thuận tương đương với việc tìm trọng tâm của Δ(s, a) cho mỗi (s, a) ∈ S × A. Đối với mỗi (s, a) chúng ta muốn MLP dự đoán trọng tâm trong Δ(s, a), tức là chúng ta muốn fθ(s, a) = x*(s, a). Quan sát này đúng bất kể dữ liệu huấn luyện trước σ được tạo ra như thế nào, ví dụ, bởi một MDP hoặc nếu mỗi cặp (s, a) là IID.

Bây giờ chúng ta hãy so sánh các phương pháp dữ liệu huấn luyện trước MDP và IID. Đối với hai phương pháp, các giá trị trọng tâm sẽ khác nhau. Cụ thể, đối với trường hợp IID, các trọng tâm sẽ gần nhau và thu gọn thành một điểm duy nhất cho chuỗi σ có độ dài vô hạn. Đối với trường hợp MDP, các trọng tâm sẽ cách xa nhau hơn và sẽ khác biệt cho mỗi cặp (s, a) trong trường hợp giới hạn. Từ kết quả trong Bảng 7, hiệu suất sau tinh chỉnh phần lớn không nhạy cảm với khoảng cách giữa các trọng tâm khác nhau.

## KẾT LUẬN

Trong bài báo này, chúng tôi đã xem xét DRL ngoại tuyến và nghiên cứu tác động của một số sơ đồ huấn luyện trước với dữ liệu tổng hợp. Những đóng góp của bài báo này như sau:

1. Chúng tôi đề xuất một sơ đồ huấn luyện trước tổng hợp đơn giản nhưng hiệu quả cho DT. Dữ liệu được tạo ra từ Chuỗi Markov một bước với không gian trạng thái nhỏ mang lại hiệu suất tốt hơn so với huấn luyện trước với kho ngữ liệu Wiki, có từ vựng lớn hơn nhiều và chứa các phụ thuộc token phức tạp hơn nhiều. Phát hiện mới này thách thức quan điểm trước đây rằng huấn luyện trước ngôn ngữ có thể mang lại lợi ích độc đáo cho DRL.

2. Chúng tôi chỉ ra rằng huấn luyện trước tổng hợp của CQL với backbone MLP cũng có thể dẫn đến cải thiện hiệu suất đáng kể. Đây là bài báo đầu tiên chỉ ra rằng huấn luyện trước trên dữ liệu tổng hợp đơn giản là một phương pháp hiệu quả đáng ngạc nhiên để cải thiện hiệu suất DRL ngoại tuyến cho cả các thuật toán dựa trên transformer và dựa trên Q-learning.

3. Chúng tôi cung cấp các ablation cho thấy tính mạnh mẽ đáng ngạc nhiên của huấn luyện trước tổng hợp trên phụ thuộc quá khứ, kích thước không gian trạng thái/hành động, và độ nhọn của các phân phối chuyển đổi và chính sách, mang lại sự tăng cường hiệu suất nhất quán trên các cài đặt tạo dữ liệu khác nhau.

4. Hơn nữa, chúng tôi chỉ ra rằng phương pháp được đề xuất hiệu quả và dễ sử dụng. Đối với DT, huấn luyện trước dữ liệu tổng hợp đạt được hiệu suất vượt trội với ít hơn 4× bước cập nhật huấn luyện trước, chỉ mất 3% thời gian tính toán ở huấn luyện trước và 67% ở tinh chỉnh so với DT+Wiki. Đối với CQL, dữ liệu được tạo ra có chiều trạng thái và hành động nhất quán với tác vụ RL hạ nguồn, làm cho nó dễ sử dụng với MLPs, và huấn luyện trước chỉ mất 5 phút.

5. Cuối cùng, chúng tôi cung cấp những hiểu biết lý thuyết về tại sao dữ liệu IID vẫn có thể đạt được hiệu suất tốt. Chúng tôi chỉ ra rằng mục tiêu động lực học thuận tương đương với việc tìm các trọng tâm trạng thái cơ bản của bộ dữ liệu tổng hợp, và CQL phần lớn không nhạy cảm với phân phối của chúng.

Những phát hiện mới trong bài báo này mở ra một số hướng nghiên cứu tương lai thú vị. Một là hiểu sâu hơn tại sao huấn luyện trước trên dữ liệu hoàn toàn không liên quan đến tác vụ RL có thể cải thiện hiệu suất. Ở đây, không có khả năng sự cải thiện đến từ việc chuyển giao tích cực các đặc trưng, vì vậy chúng tôi nghi ngờ rằng huấn luyện trước như vậy có thể đã giúp làm cho quá trình tối ưu hóa mượt mà hơn trong quá trình tinh chỉnh. Các hướng thú vị khác bao gồm khám phá các sơ đồ tạo dữ liệu tổng hợp khác nhau và điều tra mức độ mà dữ liệu tổng hợp có thể hữu ích.
