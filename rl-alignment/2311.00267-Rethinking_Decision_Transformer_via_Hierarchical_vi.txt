# 2311.00267.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2311.00267.pdf
# Kích thước tệp: 2518121 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Suy nghĩ lại về Decision Transformer thông qua Học tăng cường phân cấp
Yi Ma1,∗Chenjun Xiao2,∗Hebin Liang1Jianye Hao1,2
1Học viện Trí tuệ và Tính toán, Đại học Thiên Tân2Huawei, Phòng thí nghiệm Noah's Ark,
{mayi, lianghebin, jianye.hao}@tju.edu.cn, chenjun@ualberta.ca
Tóm tắt
Decision Transformer (DT) là một thuật toán đột phá tận dụng những tiến bộ gần đây của kiến trúc transformer trong học tăng cường (RL). Tuy nhiên, một hạn chế đáng chú ý của DT là sự phụ thuộc vào việc nhớ lại các quỹ đạo từ tập dữ liệu, mất đi khả năng liền mạch nối kết các quỹ đạo không tối ưu với nhau. Trong công trình này, chúng tôi giới thiệu một khung mô hình hóa chuỗi tổng quát để nghiên cứu việc đưa ra quyết định tuần tự thông qua lăng kính của Học tăng cường phân cấp. Tại thời điểm đưa ra quyết định, một chính sách cấp cao trước tiên đề xuất một prompt lý tưởng cho trạng thái hiện tại, một chính sách cấp thấp sau đó tạo ra một hành động được điều kiện hóa theo prompt đã cho. Chúng tôi chỉ ra rằng DT xuất hiện như một trường hợp đặc biệt của khung này với các lựa chọn nhất định về chính sách cấp cao và cấp thấp, và thảo luận về khả năng thất bại tiềm năng của những lựa chọn này. Được truyền cảm hứng từ những quan sát này, chúng tôi nghiên cứu cách tối ưu hóa đồng thời các chính sách cấp cao và cấp thấp để kích hoạt khả năng nối kết, điều này dẫn đến việc phát triển các thuật toán RL offline mới. Kết quả thực nghiệm của chúng tôi cho thấy rõ ràng rằng các thuật toán được đề xuất vượt trội đáng kể so với DT trên một số benchmark điều khiển và điều hướng. Chúng tôi hy vọng những đóng góp của mình có thể truyền cảm hứng cho việc tích hợp các kiến trúc transformer trong lĩnh vực RL.

1 Giới thiệu
Một trong những đặc điểm đáng chú ý nhất được quan sát trong các mô hình chuỗi lớn, đặc biệt là các mô hình Transformer, là khả năng học trong ngữ cảnh [Radford et al., 2019, Brown et al., 2020, Ramesh et al., 2021, Gao et al., 2020, Akyürek et al., 2022, Garg et al., 2022, Laskin et al., 2022, Lee et al., 2023]. Với một prompt thích hợp, một transformer đã được huấn luyện trước có thể học các nhiệm vụ mới mà không cần giám sát rõ ràng và cập nhật tham số bổ sung. Decision Transformer (DT) là một phương pháp đột phá cố gắng khám phá ý tưởng này cho việc đưa ra quyết định tuần tự [Chen et al., 2021]. Không giống như các thuật toán học tăng cường (RL) truyền thống, mà học một hàm giá trị bằng bootstrapping hoặc tính toán gradient chính sách, DT trực tiếp học một mô hình sinh tự hồi quy từ dữ liệu quỹ đạo sử dụng một causal transformer [Vaswani et al., 2017, Radford et al., 2019]. Cách tiếp cận này cho phép tận dụng các kiến trúc transformer hiện có được sử dụng rộng rãi trong các nhiệm vụ ngôn ngữ và thị giác dễ dàng mở rộng, và hưởng lợi từ một khối lượng nghiên cứu đáng kể tập trung vào việc huấn luyện ổn định transformer [Radford et al., 2019, Brown et al., 2020, Fedus et al., 2022, Chowdhery et al., 2022].

DT được huấn luyện trên dữ liệu quỹ đạo, (𝑅0,𝑠0,𝑎0,…,𝑅𝑇,𝑠𝑇,𝑎𝑇), trong đó 𝑅𝑡 là return-to-go, tổng của các phần thưởng tương lai dọc theo quỹ đạo bắt đầu từ bước thời gian 𝑡. Điều này có thể được xem như việc học một mô hình dự đoán hành động nào nên thực hiện tại một trạng thái cho trước để đạt được nhiều return như vậy. Theo đó, chúng ta có thể xem prompt return-to-go như một công tắc, hướng dẫn mô hình trong việc đưa ra quyết định tại thời điểm kiểm tra. Nếu một mô hình như vậy có thể được học hiệu quả và tổng quát hóa tốt ngay cả cho return-to-go ngoài phân phối, thì việc mong đợi DT có thể tạo ra một chính sách tốt hơn bằng cách prompting một return cao hơn là hợp lý. Thật không may, điều này dường như đòi hỏi một mức độ khả năng tổng quát hóa thường quá cao trong các bài toán đưa ra quyết định tuần tự thực tế. Trên thực tế, thách thức chính mà DT đối mặt là làm thế nào để cải thiện tính mạnh mẽ của nó đối với phân phối dữ liệu cơ bản, đặc biệt khi học từ các quỹ đạo được thu thập bởi các chính sách không gần với tối ưu. Các nghiên cứu gần đây đã chỉ ra rằng đối với các bài toán yêu cầu khả năng nối kết, tức khả năng tích hợp các quỹ đạo không tối ưu từ dữ liệu, DT không thể cung cấp lợi thế đáng kể so với behavior cloning [Fujimoto and Gu, 2021, Emmons et al., 2021, Kostrikov et al., 2022, Yamagata et al., 2023, Badrinath et al., 2023, Xiao et al., 2023a]. Điều này càng khẳng định rằng một prompt return-to-go ngây thơ không đủ tốt để giải quyết các bài toán đưa ra quyết định tuần tự phức tạp.

∗Đóng góp ngang nhau.
1arXiv:2311.00267v1 [cs.LG] 1 Nov 2023

--- TRANG 2 ---
1Causal Transformer
𝒑𝒕−𝟏𝒔𝒕−𝟏𝒂𝒕−𝟏𝒑𝒕𝒂𝒕
𝒔𝒕𝒂𝒕 𝒑𝒕−𝟐𝒔𝒕−𝟐𝒂𝒕−𝟐
𝜋ℎ𝜋ℎ𝜋ℎ𝒂𝒕−𝟏 𝒂𝒕−𝟐
... ... Hình 1: Kiến trúc ADT. Chính sách cấp cao tạo ra các prompt thông báo cho chính sách cấp thấp để đưa ra quyết định. Chúng tôi nối prompt với states thay vì coi chúng như các token riêng biệt. Các embedding của token được đưa vào một causal transformer dự đoán actions một cách tự hồi quy.

Tiến bộ gần đây trong các mô hình ngôn ngữ lớn cho thấy rằng các prompt được điều chỉnh cẩn thận, dù được viết bởi con người hay tự khám phá bởi mô hình, làm tăng đáng kể hiệu suất của các mô hình transformer [Lester et al., 2021, Singhal et al., 2022, Zhang et al., 2022, Wei et al., 2022, Wang et al., 2022, Yao et al., 2023, Liu et al., 2023]. Đặc biệt, người ta đã quan sát thấy rằng khả năng thực hiện lý luận phức tạp xuất hiện một cách tự nhiên trong các mô hình ngôn ngữ lớn đủ lớn khi chúng được trình bày với một vài minh chứng chain of thought như mẫu trong các prompt [Wei et al., 2022, Wang et al., 2022, Yao et al., 2023]. Được thúc đẩy bởi tầm quan trọng của những công trình này trong các mô hình ngôn ngữ, một câu hỏi nảy sinh: Đối với RL, liệu có khả thi không khi học để tự động điều chỉnh prompt, sao cho một mô hình quyết định tuần tự dựa trên transformer có thể học các chính sách điều khiển tối ưu từ dữ liệu offline? Bài báo này cố gắng giải quyết vấn đề này. Những đóng góp chính của chúng tôi là:

• Chúng tôi trình bày một khung tổng quát để nghiên cứu việc đưa ra quyết định thông qua mô hình hóa tuần tự bằng cách kết nối nó với Học tăng cường phân cấp [Nachum et al., 2018]: một chính sách cấp cao trước tiên đề xuất một prompt cho trạng thái hiện tại, một chính sách cấp thấp sau đó tạo ra một hành động được điều kiện hóa theo prompt đã cho. Chúng tôi cho thấy DT có thể được khôi phục như một trường hợp đặc biệt của khung này.

• Chúng tôi điều tra khi nào và tại sao DT thất bại trong việc nối kết các quỹ đạo không tối ưu. Để khắc phục nhược điểm này của DT, chúng tôi điều tra cách tối ưu hóa đồng thời các chính sách cấp cao và cấp thấp để kích hoạt khả năng nối kết. Điều này dẫn đến việc phát triển hai thuật toán mới cho RL offline. Khung tối ưu hóa chính sách kết hợp là đóng góp chính của chúng tôi so với các nghiên cứu trước đây về cải thiện các mô hình quyết định dựa trên transformer [Yamagata et al., 2023, Wu et al., 2023, Badrinath et al., 2023].

• Chúng tôi cung cấp kết quả thí nghiệm trên một số benchmark RL offline, bao gồm điều khiển vận động, điều hướng và robot, để chứng minh hiệu quả của các thuật toán được đề xuất. Ngoài ra, chúng tôi tiến hành các nghiên cứu ablation kỹ lưỡng về các thành phần chính của thuật toán để có được hiểu biết sâu sắc hơn về đóng góp của chúng. Thông qua các nghiên cứu ablation này, chúng tôi đánh giá tác động của các thiết kế thuật toán cụ thể đến hiệu suất tổng thể.

2 Kiến thức nền

2.1 Học tăng cường Offline
Chúng tôi xem xét Quá trình Quyết định Markov (MDP) được xác định bởi 𝑀={,,𝑃,𝑟,𝛾}[Puterman, 2014], trong đó  và  đại diện cho không gian trạng thái và hành động. Hệ số chiết khấu được cho bởi 𝛾∈[0,1), 𝑟∶×→ℝ biểu thị hàm phần thưởng, 𝑃∶×→Δ() định nghĩa dynamics chuyển đổi1. Đặt 𝜏=(𝑠0,𝑎0,𝑟0,…,𝑠𝑇,𝑎𝑇,𝑟𝑇) là một quỹ đạo. Return của nó

1Chúng tôi sử dụng Δ() để biểu thị tập hợp các phân phối xác suất trên  cho một tập hữu hạn .
2

--- TRANG 3 ---
được định nghĩa như tổng có chiết khấu của các phần thưởng dọc theo quỹ đạo: 𝑅=∑𝑇
𝑡=0𝛾𝑡𝑟𝑡. Cho một chính sách 𝜋∶→Δ(), chúng ta sử dụng 𝔼𝜋 để biểu thị kỳ vọng dưới phân phối được tạo ra bởi sự kết nối giữa 𝜋 và môi trường. Hàm giá trị xác định tổng phần thưởng có chiết khấu tương lai thu được bằng cách tuân theo chính sách 𝜋,

𝑉𝜋(𝑠)=𝔼𝜋
[∞
∑
𝑡=0𝛾𝑡𝑟(𝑠𝑡,𝑎𝑡)|||𝑠0=𝑠], (1)

Tồn tại một chính sách tối ưu 𝜋∗ tối đa hóa giá trị cho tất cả các trạng thái 𝑠∈.

Trong công trình này, chúng tôi xem xét việc học một chính sách điều khiển tối ưu từ tập dữ liệu offline đã thu thập trước đó, ={𝜏𝑖}𝑛−1
𝑖=0, bao gồm 𝑛 quỹ đạo. Mỗi quỹ đạo được tạo ra bởi quy trình sau: một trạng thái ban đầu 𝑠0∼𝜇0 được lấy mẫu từ phân phối trạng thái ban đầu 𝜇0; cho bước thời gian 𝑡≥0, 𝑎𝑡∼𝜋, 𝑠𝑡+1∼𝑃(⋅|𝑠𝑡,𝑎𝑡), 𝑟𝑡=𝑟(𝑠𝑡,𝑎𝑡), quá trình này lặp lại cho đến khi đạt được bước thời gian tối đa của môi trường. Ở đây 𝜋 là một chính sách hành vi chưa biết. Trong RL offline, thuật toán học chỉ có thể lấy mẫu từ  mà không thu thập dữ liệu mới từ môi trường [Levine et al., 2020].

2.2 Decision Transformer
Decision Transformer (DT) là một ví dụ phi thường nối cầu giữa mô hình hóa chuỗi với việc đưa ra quyết định [Chen et al., 2021]. Nó cho thấy rằng một mô hình đưa ra quyết định tuần tự có thể được tạo ra thông qua việc chỉnh sửa tối thiểu kiến trúc transformer [Vaswani et al., 2017, Radford et al., 2019]. Nó xem xét biểu diễn quỹ đạo sau đây cho phép huấn luyện và tạo sinh tự hồi quy:

𝜏=(̂𝑅0,𝑠0,𝑎0,̂𝑅1,𝑠1,𝑎1,…,̂𝑅𝑇,𝑠𝑇,𝑎𝑇). (2)

Ở đây ̂𝑅𝑡=∑𝑇
𝑖=𝑡𝑟𝑖 là returns-to-go bắt đầu từ bước thời gian 𝑡. Chúng ta ký hiệu 𝜋DT(𝑎𝑡|𝑠𝑡,̂𝑅𝑡,𝜏𝑡) là chính sách DT, trong đó 𝜏𝑡=(𝑠0,𝑎0,̂𝑅0,…,𝑠𝑡−1𝑎𝑡−1,̂𝑅𝑡−1)2 là quỹ đạo con trước bước thời gian 𝑡. Như được chỉ ra và xác minh bởi Lee et al. [2023], 𝜏𝑡 có thể được xem như một đầu vào ngữ cảnh của một chính sách, tận dụng đầy đủ khả năng học trong ngữ cảnh của mô hình transformer để tổng quát hóa tốt hơn [Akyürek et al., 2022, Garg et al., 2022, Laskin et al., 2022].

DT gán một returns-to-go mong muốn 𝑅0, cùng với một trạng thái ban đầu 𝑠0 được sử dụng như đầu vào khởi tạo của mô hình. Sau khi thực hiện hành động được tạo ra, DT giảm return mong muốn bằng phần thưởng đạt được và tiếp tục quá trình này cho đến khi episode kết thúc. Chen et al. [2021] lập luận rằng mô hình dự đoán có điều kiện có thể thực hiện tối ưu hóa chính sách mà không sử dụng lập trình động. Tuy nhiên, các công trình gần đây quan sát thấy rằng DT thường cho thấy hiệu suất kém hơn so với các thuật toán RL offline dựa trên lập trình động khi tập dữ liệu offline bao gồm các quỹ đạo không tối ưu [Fujimoto and Gu, 2021, Emmons et al., 2021, Kostrikov et al., 2022].

3 Autotuned Decision Transformer
Trong phần này, chúng tôi trình bày Autotuned Decision Transformer (ADT), một mô hình quyết định dựa trên transformer mới có thể nối kết các quỹ đạo không tối ưu từ tập dữ liệu offline. Thuật toán của chúng tôi được phát triển dựa trên một khung quyết định phân cấp tổng quát trong đó DT xuất hiện một cách tự nhiên như một trường hợp đặc biệt. Trong khung này, chúng tôi thảo luận về cách ADT khắc phục một số hạn chế của DT bằng cách tự động điều chỉnh prompt cho việc đưa ra quyết định.

3.1 Các quan sát chính
Thuật toán của chúng tôi được phát triển bằng cách xem xét một khung tổng quát kết nối các mô hình quyết định dựa trên transformer với học tăng cường phân cấp (HRL) [Nachum et al., 2018]. Cụ thể, chúng tôi sử dụng biểu diễn phân cấp sau của chính sách

𝜋(𝑎|𝑠)=∫𝜋ℎ(𝑝|𝑠)⋅𝜋𝑙(𝑎|𝑠,𝑝)𝑑𝑝, (3)

2Chúng tôi định nghĩa 𝜏0 là chuỗi rỗng để hoàn chỉnh.
3

--- TRANG 4 ---
trong đó  là một tập hợp các prompt. Để đưa ra quyết định, chính sách cấp cao 𝜋ℎ trước tiên tạo ra một prompt 𝑝∈, được hướng dẫn bởi đó chính sách cấp thấp 𝜋𝑙 trả về một hành động được điều kiện hóa theo 𝑝. DT phù hợp một cách tự nhiên với khung quyết định phân cấp này. Xem xét cơ chế prompting giá trị sau. Tại trạng thái 𝑠∈, chính sách cấp cao tạo ra một prompt giá trị thực 𝑅∈ℝ, đại diện cho "Tôi muốn thu được 𝑅 returns bắt đầu từ 𝑠.". Được thông báo bởi prompt này, chính sách cấp thấp phản hồi một hành động 𝑎∈, "Được, nếu bạn muốn thu được returns 𝑅, bạn nên thực hiện hành động 𝑎 ngay bây giờ.". Đây chính xác là những gì DT làm. Nó áp dụng một chính sách cấp cao giả mạo ban đầu chọn một prompt return-to-go mục tiêu và sau đó giảm dần nó dọc theo quỹ đạo. Chính sách cấp thấp DT, 𝜋DT(⋅|𝑠,𝑅,𝜏), học dự đoán hành động nào nên thực hiện tại trạng thái 𝑠 để đạt được returns 𝑅 cho trước ngữ cảnh 𝜏.

Để hiểu rõ hơn về sự thất bại của DT với dữ liệu không tối ưu, chúng tôi xem xét lại ví dụ minh họa được hiển thị trong Hình 2 của Chen et al. [2021]. Tập dữ liệu bao gồm các quỹ đạo bước ngẫu nhiên và return-to-go theo từng trạng thái liên quan của chúng. Giả sử rằng chính sách DT 𝜋DT hoàn toàn ghi nhớ tất cả thông tin quỹ đạo có trong tập dữ liệu. Prompt return-to-go thực tế hoạt động như một công tắc để hướng dẫn mô hình đưa ra quyết định. Đặt (𝑠) là tập hợp các quỹ đạo bắt đầu từ 𝑠 được lưu trữ trong tập dữ liệu, và 𝑅(𝜏) là return của một quỹ đạo 𝜏. Cho 𝑅′∈{𝑅(𝜏),𝜏∈(𝑠)}, 𝜋DT có thể đưa ra một hành động dẫn tới 𝜏. Do đó, cho một oracle return 𝑅∗(𝑠)=max 𝜏∈(𝑠)𝑅(𝜏), dự kiến là DT có thể tuân theo quỹ đạo tối ưu có trong tập dữ liệu theo công tắc.

Có một số vấn đề. Thứ nhất, oracle return 𝑅∗ không được biết. Prompt return-to-go ban đầu của DT được chọn bằng tay và có thể không nhất quán với cái được quan sát trong tập dữ liệu. Điều này đòi hỏi mô hình phải tổng quát hóa tốt cho return-to-go và quyết định chưa thấy. Thứ hai, ngay cả khi 𝑅∗ được biết cho tất cả các trạng thái, việc ghi nhớ thông tin quỹ đạo vẫn không đủ để có được khả năng nối kết vì 𝑅∗ chỉ phục vụ như một cận dưới cho return đạt được tối đa. Để hiểu điều này, hãy xem xét một ví dụ với hai quỹ đạo 𝑎→𝑏→𝑐, và 𝑑→𝑏→𝑒. Giả sử rằng 𝑒 dẫn đến return là 10, trong khi 𝑐 dẫn đến return là 0. Trong trường hợp này, sử dụng 10 làm prompt return-to-go tại trạng thái 𝑏, DT sẽ có thể chuyển sang quỹ đạo mong muốn. Tuy nhiên, thông tin "nghiêng về 𝑐 có thể đạt được return là 10" không được truyền tới 𝑎 trong quá trình huấn luyện, vì quỹ đạo 𝑎→𝑏→𝑒 không tồn tại trong dữ liệu. Nếu dữ liệu offline chứa một quỹ đạo khác bắt đầu từ 𝑎 và dẫn đến return trung bình (ví dụ 1), DT có thể chuyển sang quỹ đạo đó tại 𝑎 sử dụng 10 làm prompt return-to-go, bỏ lỡ một con đường hứa hẹn hơn. Do đó, việc dự đoán chỉ được điều kiện hóa theo return-to-go không đủ cho tối ưu hóa chính sách. Vẫn cần một số hình thức lan truyền thông tin ngược.

3.2 Thuật toán
ADT tối ưu hóa đồng thời các chính sách phân cấp để khắc phục các hạn chế của DT đã thảo luận ở trên. Một minh họa kiến trúc ADT được cung cấp trong Hình 1. Tương tự DT, ADT áp dụng một mô hình transformer cho chính sách cấp thấp. Thay vì (2), nó xem xét biểu diễn quỹ đạo sau,

𝜏=(𝑝0,𝑠0,𝑎0,𝑝1,𝑠1,𝑎1,…,𝑝𝑇,𝑠𝑇,𝑎𝑇). (4)

Ở đây 𝑝𝑖 là prompt được tạo ra bởi chính sách cấp cao 𝑝𝑖∼𝜋ℎ(⋅|𝑠𝑖), thay thế prompt return-to-go được sử dụng bởi DT. Tức là, đối với mỗi quỹ đạo trong tập dữ liệu offline, chúng tôi gán nhãn lại bằng cách thêm một prompt được tạo ra bởi các chính sách cấp cao cho mỗi chuyển đổi. Được trang bị khung quyết định phân cấp tổng quát này, chúng tôi đề xuất hai thuật toán áp dụng chiến lược tạo prompt cấp cao khác nhau trong khi chia sẻ một khung tối ưu hóa chính sách cấp thấp thống nhất. Chúng tôi học một chính sách cấp cao 𝜋𝜔≈𝜋ℎ với tham số 𝜙, và một chính sách cấp thấp 𝜋𝜃≈𝜋𝑙 với tham số 𝜃.

3.2.1 Value-prompted Autotuned Decision Transformer
Thuật toán đầu tiên của chúng tôi, Value-promped Autotuned Decision Transformer (V-ADT), sử dụng các giá trị vô hướng làm prompt. Nhưng không giống DT, nó áp dụng một thiết kế có nguyên tắc hơn của các prompt giá trị thay vì return-to-go. V-ADT nhằm trả lời hai câu hỏi: giá trị đạt được tối đa bắt đầu từ trạng thái 𝑠 là gì, và hành động nào nên thực hiện để đạt được giá trị như vậy? Để trả lời những câu hỏi này, chúng tôi xem tập dữ liệu offline như một MDP thực nghiệm, 𝑀={,,𝑃,𝑟,𝛾}, trong đó ⊆ là tập hợp các trạng thái quan sát được trong dữ liệu, 𝑃 là chuyển đổi, đây là một ước tính thực nghiệm của chuyển đổi gốc 𝑃 [Fujimoto et al., 2019]. Giá trị tối ưu của MDP thực nghiệm này là

𝑉∗
(𝑠)= max
𝑎∶𝜋(𝑎|𝑠)>0𝑟(𝑠,𝑎)+𝛾𝔼𝑠′∼𝑃(⋅|𝑠,𝑎)[𝑉∗
(𝑠′)]. (5)
4

--- TRANG 5 ---
Đặt 𝑄∗
(𝑠,𝑎) là giá trị trạng thái-hành động tương ứng. 𝑉∗
 được biết đến như giá trị tối ưu trong mẫu trong RL offline [Fujimoto et al., 2018, Kostrikov et al., 2022, Xiao et al., 2023b]. Tính toán giá trị này yêu cầu thực hiện lập trình động mà không truy vấn các hành động ngoài phân phối. Chúng tôi áp dụng Implicit Q-learning (IQL) để học 𝑉𝜙≈𝑉∗
 và 𝑄𝜓≈𝑄∗
 với tham số 𝜙,𝜓 [Kostrikov et al., 2022]. Chi tiết của IQL được trình bày trong Phụ lục. Bây giờ chúng tôi mô tả cách V-ADT tối ưu hóa đồng thời các chính sách cấp cao và thấp để tạo điều kiện nối kết.

Chính sách cấp cao V-ADT xem xét =ℝ và áp dụng một chính sách xác định 𝜋𝜔∶→ℝ, dự đoán giá trị tối ưu trong mẫu 𝜋𝜔≈𝑉∗
. Vì chúng ta đã có một giá trị tối ưu trong mẫu xấp xỉ 𝑉𝜙, chúng ta đặt 𝜋𝜔=𝑉𝜙. Chính sách cấp cao này mang lại hai lợi thế chính. Thứ nhất, cách tiếp cận này hiệu quả tạo điều kiện lan truyền thông tin ngược về các trạng thái sớm hơn trên một quỹ đạo, giải quyết một hạn chế chính của DT. Điều này đạt được bằng cách sử dụng 𝑉∗
 làm prompt giá trị, đảm bảo rằng chúng ta có kiến thức chính xác về return đạt được tối đa cho bất kỳ trạng thái nào. Việc dự đoán được điều kiện hóa theo 𝑅∗(𝑠) không đủ cho tối ưu hóa chính sách, vì 𝑅∗(𝑠)=max 𝜏∈(𝑠)𝑅(𝜏) chỉ đưa ra cận dưới cho 𝑉∗
(𝑠) và do đó sẽ là hướng dẫn yếu hơn (xem Phần 3.1 để thảo luận chi tiết). Thứ hai, định nghĩa của 𝑉∗
 tập trung độc quyền vào giá trị tối ưu được rút ra từ dữ liệu quan sát và do đó tránh các return ngoài phân phối. Điều này ngăn chính sách cấp thấp đưa ra quyết định được điều kiện hóa theo các prompt đòi hỏi ngoại suy.

Chính sách cấp thấp Việc huấn luyện trực tiếp mô hình để dự đoán quỹ đạo, như được thực hiện trong DT, không phù hợp với cách tiếp cận của chúng tôi. Điều này là do hành động 𝑎𝑡 quan sát trong dữ liệu có thể không nhất thiết tương ứng với hành động tại trạng thái 𝑠𝑡 dẫn đến return 𝑉∗
(𝑠𝑡). Tuy nhiên, xác suất chọn 𝑎𝑡 nên tỷ lệ thuận với giá trị của hành động này. Do đó, chúng tôi sử dụng advantage-weighted regression để học chính sách cấp thấp [Peng et al., 2019, Kostrikov et al., 2022, Xiao et al., 2023b]: cho dữ liệu quỹ đạo (4), mục tiêu được định nghĩa là

(𝜃)=−𝑇
∑
𝑡=0exp(𝑄𝜓(𝑠𝑡,𝑎𝑡)−𝑉𝜙(𝑠𝑡)
𝛼 )log𝜋𝜃(𝑎𝑡|𝑠𝑡,𝜋𝜔(𝑠𝑡)), (6)

trong đó 𝛼 >0 là một siêu tham số. Chính sách cấp thấp lấy đầu ra của chính sách cấp cao làm đầu vào. Điều này đảm bảo không có sự khác biệt giữa prompt giá trị huấn luyện và kiểm tra được sử dụng bởi các chính sách. Chúng tôi lưu ý rằng sự khác biệt duy nhất của điều này so với mục tiêu maximum log-likelihood tiêu chuẩn cho mô hình hóa chuỗi là áp dụng trọng số cho mỗi chuyển đổi. Người ta có thể dễ dàng triển khai điều này với dữ liệu quỹ đạo cho một transformer. Trong thực tế, chúng tôi cũng quan sát thấy rằng lược đồ tokenization khi xử lý dữ liệu quỹ đạo ảnh hưởng đến hiệu suất của ADT. Thay vì coi prompt 𝑝𝑡 như một token duy nhất như trong DT, chúng tôi thấy có lợi khi nối 𝑝𝑡 và 𝑠𝑡 với nhau và tokenize vector đã nối. Chúng tôi cung cấp một nghiên cứu ablation về điều này trong Phần 5.2.3. Điều này hoàn tất mô tả V-ADT.

3.2.2 Goal-prompted Autotuned Decision Transformer
Trong HRL, chính sách cấp cao thường xem xét một không gian hành động tiềm ẩn. Các lựa chọn điển hình của hành động tiềm ẩn bao gồm sub-goal [Nachum et al., 2018, Park et al., 2023], skills [Ajay et al., 2020, Jiang et al., 2022], và options [Sutton et al., 1999, Bacon et al., 2017, Klissarov and Machado, 2023]. Chúng tôi xem xét bài toán đạt mục tiêu như một ví dụ và sử dụng sub-goal như hành động tiềm ẩn, dẫn đến thuật toán thứ hai, Goal-promped Autotuned Decision Transformer (G-ADT). Đặt  là không gian mục tiêu3. Hàm phần thưởng có điều kiện mục tiêu 𝑟(𝑠,𝑎,𝑔) cung cấp phần thưởng của việc thực hiện hành động 𝑎 tại trạng thái 𝑠 để đạt được mục tiêu 𝑔∈. Đặt 𝑉(𝑠,𝑔) là hàm giá trị phổ quát được định nghĩa bởi các phần thưởng có điều kiện mục tiêu [Nachum et al., 2018, Schaul et al., 2015]. Tương tự, chúng tôi định nghĩa 𝑉∗
(𝑠,𝑔) và 𝑄∗
(𝑠,𝑎,𝑔) là hàm giá trị phổ quát tối ưu trong mẫu. Chúng tôi cũng huấn luyện 𝑉𝜙≈𝑉∗
 và 𝑄𝜓≈𝑄∗
 để xấp xỉ các hàm giá trị phổ quát. Bây giờ chúng tôi mô tả cách G-ADT tối ưu hóa đồng thời các chính sách.

Chính sách cấp cao G-ADT xem xét = và sử dụng một chính sách cấp cao 𝜋𝜔∶→. Để tìm đường đi ngắn hơn, chính sách cấp cao 𝜋𝜔 tạo ra một chuỗi sub-goal 𝑔𝑡=𝜋𝜔(𝑠𝑡) hướng dẫn người học từng bước về phía mục tiêu cuối cùng. Chúng tôi sử dụng một sub-goal nằm trong 𝑘-bước xa hơn từ trạng thái hiện tại, trong đó 𝑘 là một siêu tham số của thuật toán được điều chỉnh cho mỗi miền [Badrinath et al., 2023, Park et al., 2023]. Cụ thể, cho dữ liệu quỹ đạo (4), chính sách cấp cao học bước nhảy k-bước tối ưu sử dụng thuật toán Hierarchical Implicit Q-learning (HIQL) được đề xuất gần đây [Park et al., 2023]:

(𝜙)=−𝑇
∑
𝑡=0exp(∑𝑘−1
𝑡′=𝑡𝛾𝑡′−𝑡𝑟(𝑠𝑡′,𝑎𝑡′,𝑔)+𝛾𝑘𝑉𝜙(𝑠𝑡+𝑘,𝑔)−𝑉𝜙(𝑠𝑡,𝑔)
𝛼 )log𝜋𝜔(𝑠𝑡+𝑘|𝑠𝑡,𝑔).

Chính sách cấp thấp Chính sách cấp thấp trong G-ADT học để đạt được sub-goal được tạo ra bởi chính sách cấp cao. G-ADT chia sẻ cùng mục tiêu chính sách cấp thấp như V-ADT. Cho dữ liệu quỹ đạo (4), nó xem xét như sau

(𝜃)=−𝑇
∑
𝑡=0exp(𝑄𝜓(𝑠𝑡,𝑎𝑡,𝜋𝜔(𝑠𝑡))−𝑉𝜙(𝑠𝑡,𝜋𝜔(𝑠𝑡))
𝛼 )log𝜋𝜃(𝑎𝑡|𝑠𝑡,𝜋𝜔(𝑠𝑡)),

Lưu ý rằng điều này hoàn toàn giống như (6) ngoại trừ việc các advantage được tính bởi các hàm giá trị phổ quát. G-ADT cũng áp dụng cùng phương pháp tokenization như V-ADT bằng cách nối 𝜋𝜔(𝑠𝑡) và 𝑠𝑡 với nhau trước. Điều này kết thúc mô tả thuật toán G-ADT.

4 Thảo luận

Loại Prompt Reed et al. [2022] đã đi sâu vào khả năng mở rộng tiềm năng của các mô hình quyết định dựa trên transformer thông qua prompting. Họ cho thấy rằng một causal transformer, được huấn luyện trên các tập dữ liệu offline đa nhiệm vụ, thể hiện khả năng thích ứng đáng chú ý với các nhiệm vụ mới thông qua fine-tuning. Khả năng thích ứng đạt được bằng cách cung cấp một prompt chuỗi như đầu vào của mô hình transformer, thường được biểu diễn như một quỹ đạo minh chứng chuyên gia. Không giống các prompt quỹ đạo chuyên gia như vậy, prompt của chúng tôi có thể được xem như một hành động tiềm ẩn được tạo ra bởi chính sách cấp cao, phục vụ như hướng dẫn cho chính sách cấp thấp để thông báo quá trình đưa ra quyết định của nó.

So sánh với các cải tiến DT khác Một số công trình gần đây đã được đề xuất để khắc phục các hạn chế của DT. Yamagata et al. [2023] gán nhãn lại dữ liệu quỹ đạo bằng cách thay thế return-to-go với các giá trị được học bởi các thuật toán RL offline. Badrinath et al. [2023] đề xuất sử dụng sub-goal làm prompt, hướng dẫn chính sách DT tìm đường đi ngắn hơn trong các bài toán điều hướng. Wu et al. [2023] học returns đạt được tối đa, 𝑅∗(𝑠)=max 𝜏∈(𝑠)𝑅(𝜏), để tăng cường khả năng nối kết của DT tại thời điểm quyết định. Liu and Abbeel [2023] cấu trúc dữ liệu quỹ đạo bằng cách gán nhãn lại return mục tiêu cho mỗi quỹ đạo như tổng phần thưởng tối đa trong một chuỗi quỹ đạo. Phát hiện của họ cho thấy rằng cách tiếp cận này cho phép một mô hình quyết định dựa trên transformer tự cải thiện trong cả thời gian huấn luyện và kiểm tra. So với những nỗ lực trước đây này, ADT giới thiệu một khung có nguyên tắc của tối ưu hóa chính sách phân cấp. Các nghiên cứu thực tế của chúng tôi cho thấy rằng tối ưu hóa đồng thời các chính sách cấp cao và thấp là chìa khóa để tăng cường hiệu suất của các mô hình quyết định dựa trên transformer.

5 Thí nghiệm
Chúng tôi điều tra ba câu hỏi chính trong các thí nghiệm của mình. Thứ nhất, ADT hoạt động tốt như thế nào trên các nhiệm vụ RL offline so với các phương pháp dựa trên DT trước đây? Thứ hai, việc tự động điều chỉnh prompt cho mô hình quyết định dựa trên transformer có cần thiết không? Thứ ba, ảnh hưởng của các chi tiết triển khai khác nhau trong ADT đến hiệu suất tổng thể như thế nào? Chúng tôi giới thiệu độc giả đến Phụ lục A để biết chi tiết bổ sung và các thí nghiệm bổ sung.

Các bài toán Benchmark Chúng tôi tận dụng các tập dữ liệu qua một số miền bao gồm Gym-Mujoco, AntMaze, và FrankaKitchen từ benchmark RL offline D4RL [Fu et al., 2020]. Đối với Mujoco, các tập dữ liệu offline được tạo ra sử dụng ba chính sách hành vi riêng biệt: '-medium', '-medium-play', và '-medium-expert', và trải rộng qua ba nhiệm vụ cụ thể: 'halfcheetah', 'hopper', và 'walker2d'. Mục tiêu chính trong nhiệm vụ điều hướng tầm xa AntMaze là hướng dẫn một robot Ant 8-DoF từ vị trí bắt đầu đến một vị trí mục tiêu được định trước. Chúng tôi sử dụng sáu tập dữ liệu bao gồm '-umaze', '-umaze-diverse', '-medium-play', 'medium-diverse', '-large-play', và '-large-diverse'. Miền Kitchen tập trung vào việc hoàn thành bốn nhiệm vụ phụ riêng biệt sử dụng robot Franka 9-DoF. Chúng tôi sử dụng ba tập dữ liệu thu giữ một loạt các hành vi: '-complete', '-partial', và '-mixed' cho miền này.

6

--- TRANG 7 ---
Bảng 1: Hiệu suất của V-ADT trên tất cả các tập dữ liệu. Các phương pháp bên phải đường thẳng đứng là các phương pháp dựa trên transformer, điểm số cao nhất trong số chúng được tô đậm.

Environment TD3+BC CQL IQL DT QLDT V-ADT
halfcheetah-medium-v2 48.3 ±0.3 44.0±5.4 47.4±0.2 42.4±0.2 42.3±0.4 48.7±0.2
hopper-medium-v2 59.3 ±4.2 58.5±2.1 66.2±5.7 63.5±5.2 66.5±6.3 60.6±2.8
walker2d-medium-v2 83.7 ±2.1 72.5±0.8 78.3±8.7 69.2±4.9 67.1±3.2 80.9±3.5
halfcheetah-medium-replay-v2 44.6 ±0.5 45.5±0.5 44.2±1.2 35.4±1.6 35.6±0.5 42.8±0.2
hopper-medium-replay-v2 60.9 ±18.8 95.0 ±6.4 94.7±8.6 43.3±23.9 52.1±20.3 83.5±9.5
walker2d-medium-replay-v2 81.8 ±5.5 77.2±5.5 73.8±7.1 58.9±7.1 58.2±5.1 86.3±1.4
halfcheetah-medium-expert-v2 90.7 ±4.3 91.6±2.8 86.7±5.3 84.9±1.6 79.0±7.2 91.7±1.5
hopper-medium-expert-v2 98.0 ±9.4 105.4 ±6.8 91.5±14.3 100.6±8.3 94.2±8.2 101.6±5.4
walker2d-medium-expert-v2 110.1 ±0.5 108.8±0.7 109.6±1.0 89.6±38.4 101.7 ±3.4 112.1±0.4
mujoco-avg 75.3 ±4.9 77.6±3.4 76.9±5.8 65.3±10.1 66.3 ±6.1 78.7±2.8
antmaze-umaze-v2 78.6 74.0 87.5 ±2.6 53.6±7.3 67.2±2.3 88.2±2.5
antmaze-umaze-diverse-v2 71.4 84.0 62.2 ±13.8 42.2±5.4 62.1±1.6 58.6±4.3
antmaze-medium-play-v2 10.6 61.2 71.2 ±7.3 0.0±0.0 0.0±0.0 62.2±2.5
antmaze-medium-diverse-v2 3.0 53.7 70.0 ±10.9 0.0±0.0 0.0±0.0 52.6±1.4
antmaze-large-play-v2 0.2 15.8 39.6 ±5.8 0.0±0.0 0.0±0.0 16.6±2.9
antmaze-large-diverse-v2 0.0 14.9 47.5 ±9.5 0.0±0.0 0.0±0.0 36.4±3.6
antmaze-avg 27.3 50.6 63.0 ±8.3 16.0±2.1 21.6±0.7 52.4±2.9
kitchen-complete-v0 25.0 ±8.8 43.8 62.5 46.5±3.0 38.8±15.8 55.1±1.4
kitchen-partial-v0 38.3 ±3.1 49.8 46.3 31.4±19.5 36.9±10.7 46.0±1.6
kitchen-mixed-v0 45.1 ±9.5 51.0 51.0 25.8±5.0 17.7±9.5 46.8±6.3
kitchen-avg 36.1 ±7.1 48.2 53.3 34.6±9.2 30.5±12.0 49.3±3.1
average 52.7 63.7 68.3 43.8±7.3 45.4±5.3 65.0±2.9

Bảng 2: Hiệu suất của G-ADT trên tất cả các tập dữ liệu. Các phương pháp bên phải đường thẳng đứng là các phương pháp dựa trên transformer, điểm số cao nhất trong số chúng được tô đậm.

Environment RvS-R/G HIQL WT G-ADT
antmaze-umaze-v2 65.4 ±4.9 83.9±5.3 64.9±6.1 83.8±2.3
antmaze-umaze-diverse-v2 60.9 ±2.5 87.6±4.8 71.5±7.6 83.0±3.1
antmaze-medium-play-v2 58.1 ±12.7 89.9 ±3.5 62.8±5.8 82.0±1.7
antmaze-medium-diverse-v2 67.3 ±8.0 87.0±8.4 66.7±3.9 83.4±1.9
antmaze-large-play-v2 32.4 ±10.5 87.3 ±3.7 72.5±2.8 71.0±1.3
antmaze-large-diverse-v2 36.9 ±4.8 81.2±6.6 72.0±3.4 65.4±4.9
antmaze-avg 53.5 ±7.2 86.2±5.4 68.4±4.9 78.1±2.5
kitchen-complete-v0 50.2 ±3.6 43.8±19.5 49.2±4.6 51.4±1.7
kitchen-partial-v0 51.4 ±2.6 65.0±9.2 63.8±3.5 64.2±5.1
kitchen-mixed-v0 60.3 ±9.4 67.7±6.8 70.9±2.1 69.2±3.3
kitchen-avg 54.0 ±5.2 58.8±11.8 61.3±3.4 61.6±3.4
average 53.7 ±6.5 77.1±7.5 66.0±4.4 72.6±2.8

Các thuật toán Baseline Chúng tôi so sánh hiệu suất của ADT với một số baseline đại diện bao gồm (1) các phương pháp RL offline: TD3+BC [Fujimoto and Gu, 2021], CQL [Kumar et al., 2020] và IQL [Kostrikov et al., 2022]; (2) các phương pháp có điều kiện giá trị: Decision Transformer (DT) [Chen et al., 2021] và Q-Learning Decision Transformer (QLDT) [Yamagata et al., 2023]; (3) các phương pháp có điều kiện mục tiêu: HIQL [Park et al., 2023], RvS [Emmons et al., 2021] và Waypoint Transformer (WT) [Badrinath et al., 2023]. Tất cả kết quả baseline ngoại trừ QLDT được lấy từ [Badrinath et al., 2023] và [Park et al., 2023] hoặc bằng cách chạy các mã của kho CORL [Tarasov et al., 2022]. Đối với HIQL, chúng tôi trình bày hiệu suất của HIQL với biểu diễn mục tiêu trong Kitchen và không có biểu diễn mục tiêu trong AntMaze, theo triển khai của chúng tôi trong ADT, để đảm bảo so sánh công bằng. QLDT và transformer-based actor của ADT được triển khai dựa trên mã DT trong CORL, với kiến trúc tương tự. Chi tiết được đưa ra trong Phụ lục. Các critic và các chính sách để tạo prompt được sử dụng trong ADT được triển khai lại trong PyTorch theo các mã chính thức của IQL và HIQL. Trong tất cả các thí nghiệm được tiến hành, năm seeds ngẫu nhiên riêng biệt được sử dụng. Kết quả được mô tả với khoảng tin cậy 95%, được biểu diễn bằng các vùng tô màu trong các hình và thể hiện như độ lệch chuẩn trong các bảng. Kết quả báo cáo của ADT trong các bảng được thu được bằng cách đánh giá các mô hình cuối cùng.

5.1 Kết quả chính
Bảng 1 và 2 trình bày hiệu suất của hai biến thể của ADT được đánh giá trên các tập dữ liệu offline. ADT vượt trội đáng kể so với các thuật toán đưa ra quyết định dựa trên transformer trước đây. So với DT và QLDT, hai thuật toán dựa trên transformer để đưa ra quyết định, V-ADT thể hiện sự vượt trội đáng kể đặc biệt trên AntMaze và Kitchen yêu cầu khả năng nối kết để thành công. Trong khi đó, Bảng 2 cho thấy rằng G-ADT vượt trội đáng kể so với WT, một thuật toán sử dụng sub-goal làm prompt cho một chính sách transformer. Chúng tôi lưu ý rằng ADT có hiệu suất so sánh với các phương pháp RL offline tiên tiến. Ví dụ, V-ADT vượt trội tất cả các baseline RL offline trong các bài toán Mujoco. Trong AntMaze và Kitchen, V-ADT khớp với hiệu suất của IQL, và vượt trội đáng kể so với TD3+BC và CQL. Bảng 2 kết thúc với những phát hiện tương tự cho G-ADT.

5.2 Các nghiên cứu Ablation

5.2.1 Hiệu quả của Prompting
[Tiếp tục với phần còn lại...]
