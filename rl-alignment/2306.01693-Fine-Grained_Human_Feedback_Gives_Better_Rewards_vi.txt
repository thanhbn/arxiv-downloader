# 2306.01693.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2306.01693.pdf
# Kích thước tệp: 2451829 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Phản hồi chi tiết từ con người mang lại phần thưởng tốt hơn
cho huấn luyện mô hình ngôn ngữ
Zeqiu Wu1∗Yushi Hu1∗Weijia Shi1Nouha Dziri2Alane Suhr3
Prithviraj Ammanabrolu45Noah A. Smith12Mari Ostendorf1Hannaneh Hajishirzi12
1University of Washington2Allen Institute for Artificial Intelligence
3University of California, Berkeley4University of California, San Diego5MosaicML
Tóm tắt
Các mô hình ngôn ngữ (LM) thường thể hiện các hành vi tạo văn bản không mong muốn, bao
gồm tạo ra các đầu ra sai, độc hại hoặc không liên quan. Học tăng cường từ phản hồi của con người
(RLHF)—nơi các đánh giá ưu tiên của con người về đầu ra LM được chuyển đổi thành tín hiệu
học tập—gần đây đã cho thấy tiềm năng trong việc giải quyết những vấn đề này. Tuy nhiên, phản
hồi toàn diện như vậy truyền đạt thông tin hạn chế về các đầu ra văn bản dài; nó không chỉ ra
những khía cạnh nào của đầu ra ảnh hưởng đến sở thích của người dùng; ví dụ: phần nào chứa
loại lỗi gì. Trong bài báo này, chúng tôi sử dụng phản hồi chi tiết từ con người (ví dụ: câu nào
sai, câu phụ nào không liên quan) như một tín hiệu huấn luyện rõ ràng. Chúng tôi giới thiệu
FINE-GRAINED RLHF, một framework cho phép huấn luyện và học từ các hàm phần thưởng
chi tiết theo hai khía cạnh: (1) mật độ, cung cấp phần thưởng sau mỗi đoạn (ví dụ: một câu)
được tạo ra; và (2) kết hợp nhiều mô hình phần thưởng liên quan đến các loại phản hồi khác nhau
(ví dụ: sai sự thật, không liên quan và thiếu thông tin). Chúng tôi tiến hành thí nghiệm về khử
độc tố và trả lời câu hỏi dạng dài để minh họa cách học với các hàm phần thưởng như vậy dẫn đến
hiệu suất được cải thiện, được hỗ trợ bởi cả đánh giá tự động và con người. Ngoài ra, chúng tôi
cho thấy rằng các hành vi LM có thể được tùy chỉnh bằng cách sử dụng các kết hợp khác nhau
của các mô hình phần thưởng chi tiết. Chúng tôi phát hành tất cả dữ liệu, phản hồi từ con người
được thu thập và mã tại https://FineGrainedRLHF.github.io .

1 Giới thiệu
AI hiện đại được xây dựng trên các mô hình ngôn ngữ được tiền huấn luyện sau đó được huấn luyện
thông qua tương tác với con người [29,28,9], với sự kết hợp của học có giám sát và học tăng cường.
Việc kết hợp phản hồi của con người vào quá trình huấn luyện mô hình ngôn ngữ (LM) đã được chứng
minh là hiệu quả để giảm các đầu ra tạo sinh sai, độc hại và các đầu ra không mong muốn khác của mô
hình [29,3,2,33,10]. Nhiều nghiên cứu này áp dụng học tăng cường từ phản hồi của con người (RLHF)
[46], một framework chuyển đổi phản hồi của con người thành tín hiệu huấn luyện LM hiệu quả để đạt
được những mục tiêu này. Cụ thể, con người được trình bày hai hoặc nhiều đầu ra và được yêu cầu
chọn một hoặc xếp hạng chúng, và tín hiệu này sau đó được sử dụng để huấn luyện một mô hình phần
thưởng, tính toán một phần thưởng vô hướng duy nhất cho mỗi chuỗi được tạo bởi LM. LM sau đó
được huấn luyện với RL để tối ưu hóa phần thưởng mà nó nhận được (từ mô hình phần thưởng).

Phần thưởng như vậy cung cấp tín hiệu huấn luyện tương đối thưa thớt, đặc biệt đối với các nhiệm
vụ yêu cầu tạo văn bản dạng dài—làm cho RLHF trong các lĩnh vực như vậy không đáng tin cậy [33].
Hơn nữa, nghiên cứu trước đây [8,18,11,45,25,43] về đánh giá tự động của văn bản được tạo cho thấy
rằng việc so sánh một cách đáng tin cậy chất lượng tổng thể của hai hoặc nhiều đầu ra mô hình khi
các đầu ra chứa hỗn hợp các hành vi không mong muốn đa dạng có thể là thách thức đối với người chú
thích. Họ chứng minh cách phân loại và định vị lỗi mô hình (tức là đánh giá chi tiết) cung cấp cái
nhìn sâu sắc rõ ràng về phần nào của đầu ra mô hình có loại vấn đề gì. Do đó, chúng tôi đặt câu hỏi:
làm thế nào chúng ta có thể cải thiện phần thưởng cho huấn luyện LM thông qua RLHF bằng cách sử
dụng phản hồi chi tiết hơn từ con người?

∗Đóng góp ngang nhau. Liên hệ: <Zeqiu Wu: zeqiuwu1@uw.edu>, <Yushi Hu: yushihu@uw.edu>
Hội nghị thứ 37 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2023).arXiv:2306.01693v2 [cs.CL] 30 Oct 2023

--- TRANG 2 ---
Liên quan: + 0.3  Sự thật: - 0.5Liên quan: + 0.3 Sự thật: + 0.5 Hoàn chỉnh thông tin: + 0.3
Đầu ra LM:Khí quyển Trái Đất là một lớp khí được giữ lại bởi trọng lực của Trái Đất…
Khí quyển thường được biết đến như không khí. Các khí hàng đầu theo thể tích mà không khí khô …
Bước 1: Thu thập phản hồi từ con người và huấn luyện các mô hình phần thưởng
Bước 2: Tinh chỉnh LM chính sách chống lại các mô hình phần thưởng bằng RLLM đầu ra: Khí quyển Trái Đất là một lớp khí được giữ lại bởi trọng lực của Trái Đất. Khí phổ biến nhất, theo thể tích không khí khô, là nitơ. Thứ hai là oxy. Thứ ba là carbon dioxide.
Không liên quan / Dư thừa
Không thể xác minh / Không đúng sự thật
Thiếu sótThứ ba là Argon.
Mô hình phần thưởng tính liên quan3 loại khí phổ biến nhất trong khí quyển trái đất là gì?Lời nhắc:3 loại khí phổ biến nhất trong khí quyển trái đất là gì?Lời nhắc:
Lời nhắc được lấy mẫu: Nước có sôi nhanh hơn ở độ cao lớn không?Nước mất nhiều thời gian hơn để sôi ở độ cao lớn. Lý do là nước sôi ở nhiệt độ thấp hơn ở độ cao lớn hơn.
PPOCập nhật chính sách với phần thưởngPhần thưởng ưu tiên: - 0.35Cập nhật chính sách với phần thưởng
PPO(a) RLHF dựa trên ưu tiên(b) Của chúng tôi: Fine-Grained RLHF
Phản hồi của con người
Mô hình phần thưởng tính sự thật
Mô hình phần thưởng hoàn chỉnh thông tin
Mô hình phần thưởng ưu tiênPhản hồi chi tiết từ con người
Nước mất nhiều thời gian hơn để sôi ở độ cao lớn. Lý do là nước sôi ở nhiệt độ thấp hơn ở độ cao lớn hơn.
Lời nhắc được lấy mẫu: Nước có sôi nhanh hơn ở độ cao lớn không?Không khí bao quanh hành tinh Trái Đất chứa nhiều loại khí khác nhau. Nitơ…Khí quyển Trái Đất là lớp khí, thường được gọi là không khí…   >         =         >

Hình 1: So sánh (a) RL với ưu tiên của con người và (b) FINE-GRAINED RLHF của chúng tôi về
QA dạng dài. Khác với (a), thu thập ưu tiên của con người về chất lượng tổng thể của đầu ra LM,
chúng tôi yêu cầu người chú thích đánh dấu phần nào của đầu ra chứa loại lỗi gì. Chúng tôi huấn
luyện một mô hình phần thưởng chi tiết cho mỗi loại lỗi và tối ưu hóa LM chống lại các mô hình
phần thưởng này. Trong ví dụ này, chúng tôi cung cấp phần thưởng liên quan và phần thưởng sự
thật sau mỗi câu được tạo ra. Cũng có phần thưởng hoàn chỉnh thông tin toàn diện sau khi toàn
bộ văn bản được tạo ra.

Trong bài báo này, chúng tôi đề xuất rằng con người đưa ra phản hồi chi tiết cho đầu ra LM, liên kết
các danh mục hành vi không mong muốn (ví dụ: tạo sinh sai hoặc không liên quan) và một khoảng
văn bản tại một mật độ (ví dụ: cấp độ câu hoặc câu phụ). Để cho phép LM học từ phản hồi chi tiết
như vậy, chúng tôi giới thiệu framework FINE-GRAINED RLHF. Như được hiển thị trong Hình 1,
trước tiên chúng tôi sử dụng phản hồi từ con người được thu thập để huấn luyện các mô hình phần
thưởng chi tiết sao cho mỗi mô hình tập trung vào một danh mục và cung cấp phần thưởng tại mật
độ liên quan đến danh mục đó. Sau đó chúng tôi tích hợp các mô hình phần thưởng này vào Proximal
Policy Optimization (PPO) [37], một thuật toán RL thường được sử dụng để huấn luyện LM với
phản hồi từ con người dựa trên ưu tiên (§2).

Chúng tôi tiến hành thí nghiệm trên hai nhiệm vụ tạo ngôn ngữ—khử độc tố [12] (§3) và trả lời câu
hỏi dạng dài (QA) [39] (§4). Đối với khử độc tố, độc tính là danh mục lỗi duy nhất và chúng tôi
khám phá việc học với phần thưởng dày đặc. Chúng tôi áp dụng PERSPECTIVE [1], một mô hình
phát hiện độc tính ngôn ngữ được sử dụng rộng rãi được huấn luyện trên hàng triệu chú thích của
con người, làm mô hình phần thưởng của chúng tôi. Chúng tôi sử dụng nó để tính toán phần thưởng
chi tiết sau khi tạo ra mỗi câu. Kết quả thí nghiệm của chúng tôi cho thấy hiệu quả và hiệu quả dữ
liệu của việc huấn luyện mô hình với phần thưởng dày đặc so với phần thưởng toàn diện cấp độ chuỗi,
được hỗ trợ bởi kết quả đánh giá tự động.

Với các thí nghiệm về QA dạng dài, chúng tôi nhằm mục đích kiểm tra việc huấn luyện mô hình với
phần thưởng chi tiết tại hai chiều độ chi tiết (mật độ và danh mục lỗi), mà chúng tôi xây dựng một
bộ dữ liệu QA dạng dài, QA-FEEDBACK, cùng với phản hồi từ con người được thu thập của chúng
tôi. Chúng tôi phát triển cẩn thận một quy trình để thu thập phản hồi chi tiết từ con người về ba danh
mục lỗi ở các mức độ mật độ khác nhau: i) không liên quan, lặp lại hoặc không nhất quán (câu phụ),
ii) sự thật không chính xác hoặc không thể xác minh (câu), và iii) thông tin không đầy đủ (toàn bộ
chuỗi; xem Hình 1). Kết quả thí nghiệm của chúng tôi cho thấy kết quả được cải thiện trong mỗi
danh mục lỗi bằng cách học với phản hồi chi tiết như vậy, được hỗ trợ bởi cả kết quả đánh giá tự
động và con người. Trong một kịch bản với nhiều mô hình phần thưởng đại diện cho các loại lỗi
khác nhau, chúng tôi cũng cho thấy FINE-GRAINED RLHF cho phép chúng tôi kết hợp các mô hình
phần thưởng với các trọng số khác nhau và do đó kiểm soát quá trình huấn luyện mô hình hướng tới
một sự kết hợp tùy chỉnh của các hành vi mong muốn.

--- TRANG 3 ---
2 FINE-GRAINED RLHF

Chúng tôi giới thiệu FINE-GRAINED RLHF, một framework cho phép chúng tôi huấn luyện các hàm
phần thưởng chi tiết cho đầu ra tạo sinh trên các loại phản hồi khác nhau. Trước tiên chúng tôi định
nghĩa môi trường RL và thuật toán học tập. Sau đó chúng tôi định nghĩa các mô hình phần thưởng
chi tiết và mô tả cách kết hợp (các) mô hình phần thưởng chi tiết vào một thuật toán RL, trái ngược
với các nghiên cứu RLHF trước đây chỉ xem xét một phần thưởng duy nhất.

Môi trường: tạo ngôn ngữ như một MDP. Chúng tôi tập trung vào các nhiệm vụ tạo ngôn ngữ. Đối
với mỗi nhiệm vụ, chúng tôi được cung cấp một tập hợp các lời nhắc đầu vào nhiệm vụ D={xn}N n=1.
Chúng tôi theo [33] để định nghĩa tạo ngôn ngữ như một Quá trình Quyết định Markov (MDP)
⟨S,A,R, P, γ, T max⟩ với từ vựng hữu hạn V. Mỗi tập MDP bắt đầu với một lời nhắc được lấy mẫu
x= (x1, x2, . . . , x l) với xi∈ V, và kết thúc khi bước thời gian hiện tại vượt quá Tmax hoặc một
token kết thúc chuỗi được tạo ra. S là không gian trạng thái và s0= (x1, x2, . . . , x l)∈ S là trạng
thái ban đầu. Một hành động trong môi trường at∈ A là một token được tạo ra (bởi mô hình LM
chính sách Pθ) tại thời điểm t từ V (a0 là token bắt đầu chuỗi). Hàm chuyển đổi P:S × A → ∆S
nối thêm at vào cuối trạng thái st= (x1, x2, . . . , x l, a0, a1, . . . , a t−1). Quá trình này tiếp tục cho
đến khi bước thời gian kết thúc T≤Tmax được đạt tới, tạo ra một chuỗi được tạo y= (a1, . . . , a T).
Một hàm phần thưởng R:S × A → R, xuất phát từ (các) mô hình phần thưởng trong FINE-GRAINED
RLHF, cung cấp phần thưởng dày đặc trước và khi T được đạt tới. Pθ có thể được khởi tạo với một
mô hình ngôn ngữ được tiền huấn luyện, và đôi khi cũng với tinh chỉnh có giám sát trên các minh
chứng cụ thể cho nhiệm vụ. Hàm phần thưởng được định nghĩa sau.

Thuật toán học tập: tối ưu hóa chính sách gần đúng (PPO). PPO [37] là một thuật toán RL actor-critic
được sử dụng rộng rãi trong công việc RLHF trước đây để tối ưu hóa mô hình chính sách chống lại
một mô hình phần thưởng của phản hồi từ con người. Nó sử dụng một mô hình giá trị Vψ(st) để ước
tính giá trị của trạng thái st, và tối ưu hóa mô hình chính sách với một mục tiêu huấn luyện thay
thế được cắt của PPO. Lợi thế At tại bước thời gian t được ước tính bởi một hàm ước tính lợi thế
tổng quát [36]: At=∑T t′=t(γλ)t′−t(rt′+ γVψ(st′+1)−Vψ(st′)), với γ là một siêu tham số và λ
là hệ số chiết khấu cho phần thưởng. rt là phần thưởng được gán cho at, trong trường hợp của chúng
tôi được thu thập bằng cách sử dụng một hoặc nhiều mô hình phần thưởng đã học. Mô hình giá trị
Vψ(st) được tối ưu hóa với một mất mát lỗi bình phương kỳ vọng với mục tiêu giá trị là Vtarg(st) =
∑T−1 t′=t γt′−trt′+γT−tVψold(sT), trong đó Vψold là mô hình giá trị trễ. Cuối cùng, PPO được huấn
luyện để tối ưu hóa cả mô hình chính sách (Pθ) và giá trị (Vψ) với các mục tiêu tương ứng của chúng.
Không có mô hình phần thưởng nào được tối ưu hóa trong quá trình huấn luyện PPO. Xem Phụ lục
B để biết thêm chi tiết.

Các mô hình phần thưởng chi tiết. Công việc RLHF trước đây áp dụng một mô hình phần thưởng
toàn diện Rϕ ánh xạ lời nhắc đầu vào x và đầu ra được tạo y thành một phần thưởng vô hướng duy
nhất đại diện cho chất lượng tổng thể của nó (Hình 1(a)). Phần thưởng vô hướng duy nhất này chỉ
được gán cho token cuối cùng trong chuỗi được tạo, aT. Chính thức, rt=Rϕ(x, y) nếu t=T và 0
trong trường hợp khác.

Ngược lại, chúng tôi xem xét một hàm phần thưởng được dẫn xuất từ một hoặc nhiều mô hình phần
thưởng chi tiết mà (1) cung cấp phần thưởng một cách dày đặc (tức là cho các chuỗi phụ của đầu
ra được tạo), và (2) tính toán phần thưởng trên các danh mục riêng biệt của các hành vi không mong
muốn (ví dụ: tạo sinh sai hoặc lặp lại), trong đó mỗi danh mục được liên kết với một mô hình phần
thưởng riêng lẻ.

Đối với một mô hình phần thưởng chi tiết Rϕk cung cấp phản hồi về danh mục lỗi Ck, trước tiên
chúng tôi phân đoạn y thành Lk đoạn (yk 1, yk 2, . . . , yk Lk) tương ứng với mật độ (ví dụ: cấp độ
câu) của Rϕk, trong đó mỗi đoạn yk j kết thúc tại bước thời gian Tk j. Rϕk xuất ra một phần thưởng
Rϕk(x, y, j ) cho mỗi đoạn yk j được cung cấp x và y làm đầu vào, được gán cho token cuối cùng
trong yk j. Ngoài ra, để đảm bảo tính trôi chảy của đầu ra được tạo, chúng tôi theo [41] để thêm một
phạt phân kỳ KL xấp xỉ cho mỗi token at với trọng số β, không được lan truyền ngược trong quá
trình huấn luyện. Chính thức, giả sử rằng chúng tôi có K mô hình phần thưởng chi tiết đại diện cho
các danh mục lỗi khác nhau, chúng tôi sẽ có một hàm phần thưởng kết hợp cho mỗi token at như:

rt=∑K k=1 ∑Lk j=1 1(t=Tk j)wkRϕk(x, y, j ) −βlog Pθ(at|st) Pθinit(at|st) (1)

trong đó wk∈R là trọng số được gán cho mô hình phần thưởng Rϕk. Sau đó chúng tôi theo thuật toán
huấn luyện PPO tương tự để tối ưu hóa mô hình chính sách. Chúng tôi thảo luận về cách chúng tôi
định nghĩa và huấn luyện các mô hình phần thưởng chi tiết cho nhiệm vụ khử độc tố và QA dạng
dài trong các thí nghiệm của chúng tôi tương ứng ở §3 và §4.

--- TRANG 4 ---
3 Nhiệm vụ 1: Khử độc tố

Nhiệm vụ khử độc tố nhằm mục đích giảm độc tính trong tạo sinh mô hình y khi được cung cấp một
lời nhắc x. Độc tính là hành vi không mong muốn duy nhất trong nhiệm vụ này, và chúng tôi nhằm
mục đích khám phá việc học với phần thưởng dày đặc so với phần thưởng toàn diện duy nhất. Chúng
tôi tiến hành thí nghiệm trên REALTOXICITY PROMPTS, một bộ dữ liệu gồm 100K lời nhắc cấp
độ câu được thu thập từ web được biết là dễ gây ra các tạo sinh có vấn đề trong GPT-2 [31]. Sử dụng
phần thưởng chi tiết dày đặc cấp độ câu, chúng tôi chứng minh rằng phần thưởng chi tiết của chúng
tôi thể hiện hiệu quả mẫu lớn hơn so với phần thưởng toàn diện, đạt được độc tính thấp hơn với ít
bước huấn luyện hơn trong khi duy trì tính trôi chảy tốt hơn (§3.1).

Phần thưởng toàn diện cho (không) độc tính. Chúng tôi sử dụng PERSPECTIVE API [1] làm mô hình
phần thưởng của chúng tôi, được sử dụng rộng rãi để phát hiện độc tính ngôn ngữ và được huấn
luyện với hàng triệu ví dụ thu thập từ nhiều nền tảng trực tuyến và được chú thích bởi người chú
thích cho độc tính. Điều đó có nghĩa là chúng tôi sử dụng một mô hình phần thưởng off-policy không
được huấn luyện trên đầu ra từ Pθinit. API xuất ra một điểm số từ 0 (không độc hại) đến 1 (độc hại).
Với toàn bộ đầu ra mô hình y, phần thưởng toàn diện cho RL là 1−PERSPECTIVE(y).

Phần thưởng cấp độ câu (chi tiết) cho (không) độc tính. Để tính toán phần thưởng chi tiết, chúng
tôi truy vấn API sau khi mô hình tạo ra mỗi câu thay vì tạo ra toàn bộ chuỗi. Đối với mỗi câu được
tạo yj, chúng tôi gán PERSPECTIVE([y1, . . . , yj−1]) - PERSPECTIVE([y1, . . . , yj]) làm phần
thưởng câu (tức là độc tính thay đổi bao nhiêu từ việc tạo ra yj). Vì chỉ có một danh mục lỗi, chúng
tôi bỏ qua chỉ số trên danh mục, sử dụng yj để biểu thị đoạn thứ j (ví dụ: câu) trong y.

3.1 Thí nghiệm

Chi tiết triển khai. Chúng tôi theo công việc trước đây [17,21] và sử dụng mô hình GPT-2 large làm
mô hình chính sách ban đầu Pθinit. Trong cả giai đoạn khám phá trong huấn luyện RL và suy luận,
chúng tôi sử dụng nucleus sampling decoding với p= 0.9 và temperature = 1.0. Giới hạn độ dài
tạo sinh được đặt thành 48. Mô hình giá trị được sử dụng trong quá trình huấn luyện RL được khởi
tạo với GPT-2-base do hạn chế bộ nhớ GPU. Chúng tôi báo cáo các tham số huấn luyện RL trong
Phụ lục B. Tất cả điểm số được tính trung bình trên 3 lần chạy độc lập.

Các hệ thống so sánh và đánh giá. Chúng tôi báo cáo hiệu suất của FINE-GRAINED RLHF, RLHF
với phần thưởng toàn diện (Hol. RLHF), và các phương pháp tạo sinh được kiểm soát hiện đại GeDi
[17] và DEXPERTS [21]. Chúng tôi theo công việc trước đây [17,21] để báo cáo điểm độc tính được
tính toán trên mỗi chuỗi tạo sinh đầy đủ từ PERPLEXITY API, cũng như các chỉ số thường được
sử dụng khác cho REALTOXICITY PROMPTS, bao gồm đa dạng n-gram và độ phức tạp GPT-2 XL
(PPL) như một proxy cho tính trôi chảy. Độ phức tạp càng thấp, văn bản được tạo càng trôi chảy.
Điểm độc tính được báo cáo là điểm số tối đa trong số 4 đầu ra mô hình được lấy mẫu, được tính
trung bình trên tất cả lời nhắc đầu vào thử nghiệm. Các chỉ số khác được báo cáo là điểm số trung
bình của cùng 4 mẫu.

[Bảng 1 hiển thị kết quả trên tập thử nghiệm REALTOXICITY PROMPTS với các chỉ số Độc tính, Tính trôi chảy và Đa dạng]

Kết quả chính. Bảng 1 cho thấy kết quả thí nghiệm trên tập thử nghiệm REALTOXICITY PROMPTS.
FINE-GRAINED RLHF với phần thưởng chi tiết cấp độ câu đạt được độc tính thấp nhất và độ phức
tạp thấp nhất trong tất cả các phương pháp, trong khi duy trì mức độ đa dạng tương tự.

Phân tích hiệu quả mẫu. Hình 2 cho thấy độc tính tối đa và độ phức tạp trung bình trên tập phát
triển trong quá trình huấn luyện. FINE-GRAINED RLHF có độc tính giảm nhanh hơn nhiều trong
khi giữ độ phức tạp ở mức thấp. Điều này cho thấy rằng việc học từ phần thưởng chi tiết dày đặc
hơn có hiệu quả mẫu hơn so với phần thưởng toàn diện. Một lời giải thích là phần thưởng chi tiết
định vị nơi có nội dung độc hại, đây là tín hiệu huấn luyện mạnh hơn so với phần thưởng vô hướng
cho toàn bộ văn bản. Chi phí là chúng tôi phải truy vấn mô hình phần thưởng nhiều lần hơn cho
mỗi ví dụ.

--- TRANG 5 ---
4 Nhiệm vụ 2: Trả lời câu hỏi dạng dài (QA)

QA dạng dài yêu cầu một LM tạo ra một phản hồi văn bản cho một câu hỏi với một câu trả lời và
giải thích toàn diện. Để kiểm tra việc học với phần thưởng chi tiết tại hai chiều độ chi tiết (danh mục
lỗi và mật độ), chúng tôi thu thập QA-FEEDBACK (§4.1), một bộ dữ liệu QA dạng dài được chú
thích với phản hồi từ con người về các phản hồi được tạo bởi LM. Chúng tôi định nghĩa ba danh
mục lỗi ở các mức độ mật độ khác nhau và huấn luyện một mô hình phần thưởng cho mỗi danh mục
(§4.2). Chúng tôi mô tả thiết lập thí nghiệm trong §4.3. Cả đánh giá của con người và tự động đều
cho thấy rằng FINE-GRAINED RLHF vượt trội hơn RLHF dựa trên ưu tiên và các mô hình tinh chỉnh
có giám sát trên tất cả các danh mục lỗi (§4.4). Sau đó chúng tôi cho thấy rằng việc điều chỉnh trọng
số của các mô hình phần thưởng chi tiết trong quá trình huấn luyện RL dẫn đến các hành vi riêng
biệt trong tạo sinh LM, cho phép chúng tôi tùy chỉnh LM cho người dùng có nhu cầu khác nhau
(§4.5). Cuối cùng, chúng tôi tiến hành phân tích sâu về các mô hình phần thưởng chi tiết, tiết lộ
rằng chúng cạnh tranh với nhau, và cung cấp phân tích về tác động của chúng đến mô hình chính
sách kết quả.

4.1 QA-FEEDBACK: QA dạng dài với phản hồi từ con người

QA-FEEDBACK dựa trên ASQA [39], một bộ dữ liệu tập trung vào trả lời các câu hỏi factoid mơ hồ
[26] trong một thiết lập miền mở. Chúng tôi sử dụng các ngữ cảnh kiến thức oracle được cung cấp
của họ để tái xây dựng nhiệm vụ thành một thiết lập hiểu đọc: với đầu vào x chứa một câu hỏi q và
một tập hợp các đoạn kiến thức P={p1, . . . , p|P|}, tạo ra một phản hồi dạng dài y. Trung bình, có
khoảng 65 từ trong mỗi phản hồi vàng. Vì ASQA không phát hành tập thử nghiệm, chúng tôi tạo
ra phân chia dữ liệu huấn luyện/phát triển/thử nghiệm riêng từ các tập huấn luyện và phát triển
gốc. Chúng tôi đặt tên cho dữ liệu mới được xây dựng, cùng với phản hồi từ con người được thu
thập (thảo luận tiếp theo), là QA-FEEDBACK. Tổng thể, chúng tôi có 3.853 ví dụ huấn luyện, 500
ví dụ phát triển và 948 ví dụ thử nghiệm (chi tiết trong Phụ lục C).

Chính sách ban đầu và phản hồi chi tiết từ con người. Trước khi thu thập phản hồi từ con người,
chúng tôi theo [33] để khởi tạo mô hình chính sách với tinh chỉnh có giám sát trên một tập nhỏ các
ví dụ. Cụ thể, chúng tôi sử dụng 1K ví dụ huấn luyện để tinh chỉnh có giám sát T5-large (đường
cơ sở gốc cho ASQA) [32] để có được Pθinit. Chúng tôi đặt tên cho mô hình chính sách ban đầu
này là SFT. Sau đó chúng tôi lấy mẫu đầu ra từ SFT cho các ví dụ huấn luyện và phát triển còn lại
và thu thập phản hồi chi tiết từ con người trong ba danh mục lỗi— C1: không liên quan, lặp lại hoặc
không nhất quán; C2: sự thật không chính xác hoặc không thể xác minh dựa trên các đoạn kiến thức;
và C3: thông tin không đầy đủ. Các trường hợp phản hồi được thu thập sau đó được sử dụng làm
ví dụ huấn luyện và phát triển để huấn luyện các mô hình phần thưởng. Đối với mỗi lời nhắc nhiệm
vụ x, chúng tôi chỉ thu thập phản hồi chi tiết cho một đầu ra mô hình. Việc thu thập dữ liệu của
chúng tôi có sự chấp thuận của IRB và được coi là miễn trừ.

Chúng tôi hướng dẫn người làm việc xác định bất kỳ lỗi nào trong mỗi đầu ra mô hình y= (a1, . . . , aT),
đánh dấu khoảng văn bản liên quan đến mỗi loại lỗi được xác định. Chính thức, chúng tôi định nghĩa
tập hợp phản hồi được chú thích bởi người dùng cho lời nhắc nhiệm vụ x và đầu ra mô hình y là
F={fi} trong đó mỗi fi=⟨ci, bi, ei⟩ đại diện cho khoảng được xác định bởi người dùng (abi, . . . , aei)
của danh mục lỗi Cci, trong đó ci∈ {1,2,3}. Quan trọng, chúng tôi áp đặt ba hạn chế trong chú
thích: (1) các khoảng lỗi của danh mục C1 hoặc C2 không được trùng lắp với nhau; (2) chỉ các
khoảng không có lỗi C1 mới cần được đánh giá có chứa lỗi C2 hay không; (3) C3 chỉ có thể áp
dụng cho toàn bộ chuỗi đầu ra. Ngoài ra, chúng tôi yêu cầu người làm việc đánh dấu các câu đoạn
chứa thông tin bị thiếu nếu một lỗi C3 được chú thích. Chúng tôi cũng yêu cầu người làm việc viết
lại y thành một phiên bản được sửa y′ giải quyết tất cả phản hồi được chú thích F. Chi tiết về giao
diện thu thập phản hồi, hướng dẫn và kiểm soát chất lượng có trong Phụ lục C.

Để phân tích sự đồng thuận giữa con người, một tập con gồm 300 ví dụ nhận được chú thích từ hai
người làm việc riêng biệt. Chúng tôi quan sát rằng trong khi sự đồng thuận chính xác trong ranh
giới khoảng lỗi là thấp, người làm việc đạt được sự đồng thuận khá cao về việc một câu phụ có chứa
C1 hay không và một câu có chứa C2 hay không.² Do đó, chúng tôi quyết định có mật độ cho loại
lỗi C1, C2 và C3 là câu phụ, câu và toàn bộ chuỗi. Chúng tôi cung cấp thêm phân tích dữ liệu bao
gồm sự đồng thuận của con người trong Phụ lục C.

Phản hồi từ con người dựa trên ưu tiên. Để so sánh, chúng tôi theo [29] để thu thập riêng biệt các
ưu tiên cặp đôi từ con người từ cùng nhóm người làm việc. Chúng tôi lấy mẫu 4 đầu ra mô hình
cho mỗi lời nhắc x, tạo ra 6 cặp đầu ra mô hình. Chúng tôi yêu cầu người làm việc chỉ ra ưu tiên
cặp đôi (hòa được phép) dựa trên tất cả lỗi họ có thể tìm thấy trong mỗi đầu ra mô hình. Họ không
được yêu cầu chú thích rõ ràng những lỗi này.

Chi tiết chú thích. Trung bình, cả hai nhiệm vụ chú thích phản hồi chi tiết và ưu tiên cho một câu
hỏi mất khoảng 6 phút để một người làm việc hoàn thành. Ngược lại, [39] báo cáo rằng họ dành
khoảng 15 phút để gắn nhãn một phản hồi do con người viết cho mỗi câu hỏi, tốn thời gian hơn
nhiều so với chú thích phản hồi của chúng tôi. Trung bình, chúng tôi trả $1,65 cho mỗi ví dụ cho
cả hai nhiệm vụ, dẫn đến $16,50 tiền lương hàng giờ cho người làm việc của chúng tôi. Chúng tôi
bao gồm chi tiết về cấu trúc lương trong Phụ lục C. Chúng tôi quan sát rằng người chú thích có
thể đạt được sự đồng thuận cao hơn trong mỗi khía cạnh của phản hồi chi tiết so với so sánh cặp
đôi vì các định nghĩa phản hồi cụ thể hơn.

² Chúng tôi sử dụng spaCy [15] để phân đoạn đầu ra mô hình được tạo thành các câu. Sau đó chúng tôi chia
các câu thành các câu phụ bằng dấu phẩy hoặc dấu chấm phẩy.

--- TRANG 6 ---
4.2 Các mô hình phần thưởng chi tiết

Chúng tôi huấn luyện ba mô hình phần thưởng riêng biệt Rϕ1, Rϕ2 và Rϕ3 cho các danh mục lỗi
C1, C2 và C3 tương ứng với mật độ câu phụ, câu và toàn bộ chuỗi. Vì các mô hình phần thưởng
cung cấp điểm số phần thưởng vô hướng và không thực hiện tạo sinh, chúng tôi sử dụng Longformer-base
chỉ mã hóa [4] làm mô hình xương sống để xử lý các chuỗi đầu vào dài (chi tiết thêm về mỗi mô
hình phần thưởng có trong Phụ lục D).

C1: Không liên quan, lặp lại hoặc không nhất quán. Rϕ1 nhằm mục đích dự đoán xem mỗi câu phụ
trong y có chứa lỗi loại C1 hay không. Chúng tôi ký hiệu y= (y1 1, . . . , y1 L1), trong đó y1 j là
đoạn thứ j tại mật độ của Rϕ1 (tức là câu phụ), với tổng cộng L1 đoạn. Chúng tôi thêm một lớp
phân loại cấp độ token 2 lớp (một lớp feed-forward duy nhất) trên đỉnh bộ mã hóa Longformer. Đầu
vào mô hình có định dạng "question: q answer: [sep] y1 1[sep] y1 2. . . ", và chúng tôi lấy đầu ra
phân loại tại mỗi token [sep] để chỉ ra liệu y1 j tiếp theo có chứa lỗi C1 hay không. Chúng tôi không
thêm các đoạn vào đầu vào mô hình vì, theo trực giác, việc phát hiện lỗi C1 không phụ thuộc vào
chúng. Để huấn luyện Rϕ1, chúng tôi áp dụng mất mát phân loại cấp độ token cho mỗi token [sep]
trước y1 j, trong đó nhãn vàng gj của nó là "có lỗi" nếu có fi∈ F có (abi, . . . , aei) trùng lắp với
y1 j và ci= 1, và "không có lỗi" trong trường hợp khác. Khi Rϕ1 cung cấp phần thưởng trong quá
trình huấn luyện RL như trong Phương trình 1, chúng tôi đọc phần thưởng Rϕ1(x, y, j ) cho mọi
y1 j được cung cấp x và y. Chúng tôi định nghĩa Rϕ1(x, y, j ) = +1 nếu Rϕ1 dự đoán "không có
lỗi" cho y1 j và −1 trong trường hợp khác.

C2: Sự thật không chính xác hoặc không thể xác minh. Rϕ2 được phát triển để phát hiện lỗi C2 ở
cấp độ câu theo cách tương tự. Đầu vào mô hình có định dạng "question: q context: p1p2. . .answer:
[sep] y2 1[sep] y2 2. . . ", trong đó p's biểu thị các đoạn nền và y2 j đại diện cho câu thứ j. Chúng
tôi huấn luyện Rϕ2 tương tự như Rϕ1, với một ngoại lệ: vì chúng tôi hướng dẫn người làm việc
không chú thích lỗi C2 cho khoảng đã được gắn nhãn là chứa lỗi C1, chúng tôi không tính toán mất
mát trên các câu được gắn nhãn là chứa C1 nhưng không có C2 trong quá trình huấn luyện Rϕ2.

C3: Thông tin không đầy đủ. Rϕ3 được huấn luyện để đo lường tính đầy đủ thông tin của y, ở cấp
độ toàn bộ chuỗi. Được động lực bởi [19], Rϕ3 dự đoán một phần thưởng vô hướng duy nhất và
được huấn luyện với mất mát so sánh cặp đôi [29]:

Lr(ϕ) =−E(x,¯yp,¯yl)∼Dp[log σ(Rϕ3(x,¯yp)−Rϕ3(x,¯yl))] (2)

trong đó Rϕ3(x, y) là đầu ra vô hướng của mô hình phần thưởng cho đầu vào x và đầu ra y; ¯yp
và ¯yl được lấy mẫu từ cùng đầu vào x, và ¯yp có ít thông tin bị thiếu hơn so với ¯yl; Dp chứa các
so sánh cặp đôi được bootstrap từ phản hồi từ con người về lỗi C3 (xem chi tiết trong Phụ lục D).

Mô hình phần thưởng dựa trên ưu tiên. Mô hình phần thưởng dựa trên ưu tiên được huấn luyện
theo cách tương tự như Rϕ3, với ¯yp đại diện cho phản hồi được con người ưa thích so với ¯yl trong
hàm mất mát Phương trình 2. Nó xuất ra một điểm số vô hướng cho x và y được cung cấp đại diện
cho chất lượng phản hồi tổng thể.

4.3 Thiết lập thí nghiệm

Các hệ thống so sánh. Chúng tôi so sánh phương pháp được đề xuất của chúng tôi, FINE-GRAINED
RLHF với mô hình chính sách T5 ban đầu được huấn luyện với 1K ví dụ (SFT) và RLHF với phần
thưởng toàn diện dựa trên ưu tiên (Preference RLHF). Các mô hình phần thưởng được sử dụng trong
thí nghiệm RLHF được huấn luyện trên 2,8K ví dụ với phản hồi được chú thích (nhưng không có
phản hồi vàng từ con người). Để phân tích, chúng tôi cũng sử dụng các phản hồi vàng từ con người
của tất cả ví dụ huấn luyện để tinh chỉnh một mô hình T5 hoàn toàn có giám sát (SFT-Full). Lưu ý
rằng SFT-Full yêu cầu chi phí chú thích cao hơn nhiều vì mất nhiều thời gian hơn (15 phút cho mỗi
ví dụ [39]) để người chú thích soạn thảo các phản hồi dạng dài.

Chi tiết triển khai. Mô hình chính sách của chúng tôi dựa trên T5-large [32] và được tinh chỉnh có
giám sát trên 1K ví dụ huấn luyện, như được giải thích trong §4. Trong quá trình khám phá RL,
chúng tôi sử dụng top-k (k= 20) sampling decoding với temperature = 0.7, được đặt dựa trên công
việc RLHF trước đây [33]. Mô hình giá trị được sử dụng trong quá trình huấn luyện RL được khởi
tạo với T5-base do hạn chế bộ nhớ GPU. Trọng số mô hình phần thưởng chúng tôi sử dụng trong
FINE-GRAINED RLHF là w1= 0.3, w2= 0.5, w3= 0.3, trừ khi được chỉ định khác. Mặc dù chúng
tôi sử dụng ba mô hình phần thưởng trong quá trình huấn luyện RL, chúng tôi chỉ quan sát chi phí
tương đối bổ sung rất nhỏ (khoảng 1% thời gian huấn luyện) so với preference RLHF. Trong quá
trình suy luận, chúng tôi sử dụng greedy decoding để tạo ra các phản hồi. Chúng tôi báo cáo thêm
chi tiết bao gồm các tham số huấn luyện RL trong Phụ lục B. Tất cả điểm số được báo cáo là trung
bình của 3 lần chạy độc lập.

Đánh giá. Chúng tôi tiến hành cả đánh giá của con người và tự động. Đánh giá của con người được
chạy trên 200 ví dụ được lấy mẫu ngẫu nhiên từ tập thử nghiệm QA-FEEDBACK để so sánh Fine-Grained
RLHF với tất cả các đường cơ sở. Mỗi đầu ra mô hình được lấy mẫu từ kết quả suy luận của 3 lần
chạy huấn luyện. Chúng tôi sử dụng cùng giao thức thu thập phản hồi để có cùng nhóm người làm
việc chú thích các khoảng trong mỗi đầu ra mô hình có chứa (1) lỗi không liên quan, lặp lại hoặc
không nhất quán (rel.) và (2) sự thật không chính xác hoặc không thể xác minh (fact.). Họ cũng
được yêu cầu so sánh tính đầy đủ thông tin (comp.) cho mỗi cặp đầu ra. Để báo cáo điểm số đánh
giá cho các khoảng lỗi rel. và fact., trước tiên chúng tôi ánh xạ chúng đến mật độ loại lỗi tương ứng
(câu phụ và câu). Sau đó chúng tôi báo cáo tỷ lệ lỗi cho mỗi loại lỗi, được đo là tỷ lệ phần trăm
câu phụ chứa loại lỗi này. Vì các khoảng có lỗi rel. không được kiểm tra lỗi fact. (thảo luận trong
§4.1), chúng tôi loại trừ các câu phụ chỉ có lỗi rel. khi báo cáo tỷ lệ lỗi của lỗi fact. Đối với đánh
giá tự động, chúng tôi báo cáo RougeLSum [20] như được sử dụng cho dữ liệu ASQA gốc, cũng như
điểm số từ mỗi mô hình phần thưởng chi tiết (Rϕ1, Rϕ2 và Rϕ3). Cụ thể, chúng tôi báo cáo tỷ lệ
phần trăm của tất cả câu phụ (hoặc câu) trong tập thử nghiệm được dự đoán là "không có lỗi" bởi
Rϕ1 (hoặc Rϕ2). Đối với Rϕ3, chúng tôi báo cáo điểm số đầu ra trung bình cho tất cả ví dụ thử nghiệm.

4.4 Kết quả chính

Hình 3 cho thấy kết quả đánh giá của con người cho các loại lỗi rel. và fact. Bảng 2 cho thấy kết
quả so sánh cặp đôi của con người cho tính đầy đủ thông tin (comp.).

[Hình 3 và Bảng 2 hiển thị kết quả đánh giá]

FINE-GRAINED RLHF vượt trội hơn SFT và Preference RLHF trên tất cả các loại lỗi. Hình 3 và
Bảng 2 cho thấy rằng FINE-GRAINED RLHF của chúng tôi dẫn đến tạo sinh chính xác hơn nhiều
về mặt sự thật và chứa thông tin đầy đủ hơn, so với tất cả các hệ thống khác. Nó tạo ra ít lỗi không
liên quan, lặp lại và không nhất quán hơn, so với SFT và Preference RLHF. Trong khi đó, Preference
RLHF, mặc dù giảm đáng kể lỗi sự thật so với mô hình chính sách ban đầu SFT, tạo ra thậm chí
nhiều lỗi không liên quan, lặp lại và không nhất quán hơn so với SFT. FINE-GRAINED RLHF vượt
trội hơn Preference RLHF có thể do tín hiệu huấn luyện cụ thể và địa phương hóa hơn. Ngoài ra,
chúng tôi yêu cầu người chú thích so sánh chất lượng tạo sinh tổng thể của FINE-GRAINED RLHF
và preference RLHF. Mặc dù Preference RLHF được huấn luyện trực tiếp với phản hồi ưu tiên như
vậy, FINE-GRAINED RLHF được đánh giá tốt hơn Preference RLHF trong 30,5% tất cả ví dụ và
tệ hơn trong 24,5% ví dụ. Người chú thích chỉ ra hòa trong 45% trường hợp còn lại. Đáng ngạc
nhiên, FINE-GRAINED RLHF vượt trội hơn SFT-Full với tạo sinh sự thật và đầy đủ hơn, mặc dù
chi phí chú thích thấp hơn nhiều.

RLHF đặc biệt hiệu quả trong việc giảm lỗi sự thật. Hình 3 cho thấy rằng cả FINE-GRAINED RLHF
và Preference RLHF đều hiệu quả trong việc giảm lỗi sự thật trong tạo sinh mô hình. Trong khi đó,
chúng tôi thấy ít hoặc không có cải thiện trong việc giảm lỗi không liên quan, lặp lại hoặc không
nhất quán. Chúng tôi cung cấp phân tích sâu hơn cho quan sát này trong §4.5.

Bảng 3 cho thấy điểm số tự động trên tập thử nghiệm QA-FEEDBACK, cho thấy xu hướng tương tự
như đánh giá của con người về so sánh hệ thống, trong khi tất cả bốn hệ thống đạt được điểm số
Rouge tương tự.

--- TRANG 7 ---
[Bảng 3 và 4 hiển thị kết quả đánh giá tự động và cấu hình trọng số khác nhau]

4.5 Tùy chỉnh LM với FINE-GRAINED RLHF

Vì chúng tôi sử dụng nhiều mô hình phần thưởng trong FINE-GRAINED RLHF, việc điều chỉnh trọng
số của chúng (xem Phương trình 1) trong quá trình RL có thể dẫn đến các hành vi LM khác nhau.
Ví dụ, thêm trọng số nhiều hơn cho một mô hình phần thưởng liên quan đến một loại hành vi mong
muốn cụ thể (ví dụ: tính đầy đủ thông tin) có thể dẫn tạo sinh hướng tới loại hành vi đó hơn so với
những loại khác (ví dụ: tính liên quan thông tin). Tính linh hoạt này có thể phù hợp với người dùng
có nhu cầu đa dạng. Do đó, trong phần này, chúng tôi khám phá khả năng tùy chỉnh hành vi LM của
FINE-GRAINED RLHF.

Tùy chỉnh LM. Như trong Bảng 4, chúng tôi khám phá ba cấu hình trọng số mô hình phần thưởng
(w1, w2 và w3 cho Rϕ1, Rϕ2 và Rϕ3) và đặt tên chúng là 'ngắn', 'trung bình' và 'dài' theo độ
dài tạo sinh trung bình của LM. Để đơn giản, chúng tôi cố định w2= 0.5 và w3= 0.3, và sử dụng
0.4, 0.3 và 0.2 cho w1, dẫn đến đầu ra tạo sinh 'ngắn', 'trung bình' và 'dài' tương ứng. Chúng tôi
kiểm tra thủ công 30 ví dụ ngẫu nhiên và quan sát rằng (1) 'ngắn' tạo ra nội dung liên quan hơn,
nhưng ít sự thật và đầy đủ hơn; (2) 'dài', ngược lại, cho tạo sinh sự thật và đầy đủ nhất. Điều này
phản ánh rằng LM đang tham chiếu một lượng lớn nội dung từ các đoạn; (3) Cấu hình 'trung bình'
cân bằng ba phần thưởng và có điểm Rouge cao nhất. 24/30 ví dụ tuân theo quy tắc trên. Phân tích
định tính và ví dụ về tùy chỉnh LM có trong Phụ lục A.

Đánh đổi giữa các loại lỗi. Chúng tôi quan sát rằng w1 cao hơn dẫn đến phần thưởng rel. lớn hơn,
phần thưởng fact. và comp. nhỏ hơn, và đầu ra được tạo ngắn hơn. Một cách giải thích là Rϕ1 phạt
các khoảng văn bản không liên quan đến câu hỏi. Như vậy, nó khuyến khích trả lời câu hỏi trực tiếp
và phạt việc tham chiếu các đoạn và tạo ra thông tin phụ trợ. Điều này làm giảm độ dài tạo sinh mô
hình và tính đầy đủ thông tin, và gây ra nhiều lỗi sự thật hơn.

4.6 Phân tích

[Hình 4 và Bảng 5 hiển thị động lực phần thưởng và kết quả ablation]

Các mô hình phần thưởng đang cạnh tranh với nhau. Trong phần trước, chúng tôi thấy có sự đánh
đổi giữa các loại lỗi. Để tìm hiểu sâu hơn về hiện tượng này, chúng tôi khám phá động lực của mỗi
mô hình phần thưởng trong quá trình huấn luyện. Hình 4 cho thấy phần thưởng của mỗi mô hình
phần thưởng trên tập phát triển trong quá trình huấn luyện. Tất cả phần thưởng được chuẩn hóa z
để hiển thị. Chúng tôi thấy rằng phần thưởng fact. tăng liên tục. Phần thưởng rel. tăng nhanh trong
250 bước đầu tiên và sau đó bắt đầu giảm, trong khi phần thưởng comp. thể hiện xu hướng ngược
lại, giảm lúc đầu và sau đó bắt đầu tăng. Như đã thảo luận trước đó, một cách giải thích là tính liên
quan (độ chính xác) và tính đầy đủ thông tin (độ nhạy) có thể là các mục tiêu đối kháng, do đó các
phần thưởng đang cạnh tranh. Ba phần thưởng đạt đến điểm cân bằng trong các bước sau.

Ablation: LM có học từ tất cả các mô hình phần thưởng không? Điều gì sẽ xảy ra nếu chúng tôi
loại bỏ một mô hình phần thưởng? Bảng 5 khám phá hành vi LM chính sách khi một trong ba mô
hình phần thưởng bị loại bỏ trong quá trình huấn luyện. Ví dụ định tính có trong Phụ lục A. Đầu
tiên, chúng tôi quan sát rằng phần thưởng tương ứng giảm đáng kể khi mô hình bị loại bỏ. Khi mô
hình phần thưởng rel. (Rϕ1) bị loại bỏ, đầu ra trở nên cực kỳ dài và phần thưởng comp. cực kỳ cao.
Chúng tôi quan sát đầu ra và thấy mô hình đang sao chép rất nhiều nội dung từ các đoạn. Khi mô
hình phần thưởng fact. (Rϕ2) bị loại bỏ, phần thưởng rel. trở nên cao nhất. Chúng tôi quan sát rằng
LM có xu hướng trả lời câu hỏi trực tiếp và không tham chiếu các đoạn, gây ra nhiều ảo giác. Khi
mô hình phần thưởng comp. (Rϕ3) bị loại bỏ, đầu ra ngắn gọn và sự thật nhưng không cung cấp
tất cả thông tin liên quan cho câu hỏi. Do đó, nó có tính đầy đủ thông tin thấp hơn và điểm Rouge
thấp hơn so với LM được huấn luyện với tất cả các mô hình phần thưởng.

Hiệu suất mô hình phần thưởng. Chúng tôi báo cáo và phân tích hiệu suất của mỗi mô hình phần
thưởng trong việc dự đoán danh mục lỗi tương ứng. Mô hình phần thưởng rel. Rϕ1 có độ chính xác
phân loại nhị phân là 69,6 và điểm F1 (cho lớp "có lỗi") là 68,5 trên các câu phụ được tạo bởi mô
hình từ tập phát triển. Chúng tôi lấy mẫu 20 câu phụ mà Rϕ1 dự đoán ngược lại so với nhãn của
con người, và quan sát rằng tất cả chúng đều 1) chứa thông tin phụ trợ liên quan và được đánh
dấu là "không có lỗi" bởi con người, hoặc 2) được đánh dấu là không liên quan bởi con người nhưng
cung cấp thông tin nền liên quan chặt chẽ đến câu hỏi. Nói cách khác, Rϕ1 chủ yếu gặp khó khăn
trong việc dự đoán tính liên quan của thông tin phụ trợ, và nó hiếm khi thất bại trong việc dự đoán
một câu trả lời trực tiếp là "không có lỗi".

Mô hình phần thưởng fact. Rϕ2 có độ chính xác 77,8 và điểm F1 là 67,5. Chúng tôi lấy mẫu 20
câu mà Rϕ2 dự đoán sai và quan sát rằng các lỗi thường xảy ra khi câu được tạo ra có tính trừu
tượng cao thay vì sao chép trực tiếp thông tin từ đoạn. Chúng tôi cũng quan sát rằng hơn 80% lỗi
sự thật được gắn nhãn bởi con người xảy ra khi mô hình tạo ra một câu trả lời trực tiếp (không phải
thông tin phụ trợ) chứa thông tin ảo giác hoặc một thực thể ngẫu nhiên từ một đoạn. Chúng tôi nhận
thấy rằng Rϕ2 bắt được hơn 80% những lỗi như vậy một cách chính xác.

Mô hình phần thưởng comp. Rϕ3 có độ chính xác 70,9 trong so sánh cặp đôi. Ngược lại, mô hình
phần thưởng dựa trên ưu tiên chỉ đạt độ chính xác 57,2. Điều này giúp xác nhận trực giác của chúng
tôi rằng việc đánh giá đầu ra tạo sinh dạng dài một cách toàn diện có thể mơ hồ và chủ quan hơn
so với đánh giá đầu ra với tập trung vào một loại hành vi không mong muốn cụ thể.

So sánh với phản hồi ChatGPT. Chúng tôi thí nghiệm với việc trả lời các câu hỏi bằng ChatGPT.
Để làm quen ChatGPT với phong cách nhiệm vụ LFQA của chúng tôi, chúng tôi nhắc nó với hướng
dẫn nhiệm vụ và một ví dụ QA ngẫu nhiên duy nhất (do hạn chế độ dài). ChatGPT đạt điểm RougeLSum
là 40,92 trên tập thử nghiệm, thấp hơn nhiều so với các mô hình của chúng tôi. Chúng tôi không sử
dụng các mô hình phần thưởng được huấn luyện của chúng tôi để đánh giá đầu ra ChatGPT vì các
mô hình phần thưởng được huấn luyện trên T5-large có thể không tổng quát hóa tốt cho ChatGPT.
Thay vào đó, chúng tôi kiểm tra thủ công các phản hồi ChatGPT, và quan sát rằng chúng chủ yếu
ngắn gọn và sự thật, nhưng thiếu thông tin phụ trợ cần thiết để làm rõ các câu hỏi mơ hồ. Ví dụ
định tính có trong Phụ lục A. Điều này cho thấy khó khăn cho ChatGPT trong việc học các hành vi
mong muốn của người dùng thông qua nhắc nhở đơn giản.

--- TRANG 8 ---
5 Công trình liên quan

Học tăng cường từ phản hồi của con người (RLHF). RLHF [46,42,29] nhằm mục đích tối ưu hóa
mô hình ngôn ngữ chính sách để tạo ra nội dung được con người mong muốn. Framework này đã
được khám phá để cải thiện hiệu suất mô hình trên nhiều nhiệm vụ xử lý ngôn ngữ tự nhiên như tóm
tắt văn bản [40], tuân theo hướng dẫn [29], trả lời câu hỏi [24,27] và giảm tính có hại [3,2,22,10].
Hầu hết các nghiên cứu này thu thập ưu tiên của con người về các cặp đầu ra mô hình trên một hoặc
một tập hợp các thuộc tính mong muốn, để huấn luyện một mô hình phần thưởng gán điểm toàn
diện cho một đầu ra tạo sinh trong quá trình huấn luyện RL. [13] huấn luyện các mô hình phần
thưởng riêng biệt gán điểm cho các thuộc tính mong muốn khác nhau, nhưng vẫn sử dụng một phần
thưởng duy nhất kết hợp điểm từ tất cả các mô hình phần thưởng. Ngược lại, chúng tôi khám phá
RLHF với các mô hình phần thưởng chi tiết được huấn luyện trên phản hồi từ con người trong đó
mỗi mô hình phần thưởng cung cấp phần thưởng dày đặc sau mỗi đoạn văn bản nhỏ cho một loại
hành vi mong muốn cụ thể. [30] khám phá việc sử dụng phần thưởng trung gian để cải thiện hiệu
suất LM trên các nhiệm vụ suy luận.

Học từ phản hồi của con người trong NLP. Cũng tồn tại công việc khám phá các phương pháp
không RL để học từ phản hồi của con người. [44] huấn luyện một mô hình phần thưởng dự đoán
một điểm duy nhất cho mỗi đầu ra mô hình và chọn các mẫu có điểm phần thưởng cao nhất để tinh
chỉnh có giám sát. [38,14,42] huấn luyện một mô hình hội thoại để dự đoán cả phản hồi và điểm
hài lòng nhị phân của người dùng để cải thiện tạo sinh phản hồi. Bên cạnh phản hồi số từ con người
như vậy, phản hồi ngôn ngữ tự nhiên (NL) từ con người cũng đã được khám phá. [23,6] thu thập
và lưu trữ phản hồi NL từ con người trong một bộ nhớ phản hồi để mô hình truy xuất và sau đó
thực hiện nhiệm vụ cuối với điều kiện trên phản hồi được truy xuất. [5,35,34] sử dụng một mô hình
tinh chỉnh để tinh chỉnh đầu ra mô hình với điều kiện trên phản hồi NL từ con người và sau đó sử
dụng một mô hình phần thưởng để chọn đầu ra được tinh chỉnh tốt nhất để tinh chỉnh có giám sát.
Các phương pháp sử dụng mô hình phần thưởng để hướng dẫn tạo sinh LM hướng tới các hành vi
mong muốn tại thời điểm suy luận [21,7] có thể bổ sung cho công việc của chúng tôi nhằm mục đích
cải thiện LM trong quá trình huấn luyện. [16] cũng khám phá việc kết hợp phản hồi từ con người
vào tiền huấn luyện LM.

6 Thảo luận

Chi phí chú thích. Điều quan trọng cần lưu ý là phản hồi chi tiết từ con người được sử dụng để
huấn luyện các mô hình phần thưởng chi tiết của chúng tôi không phát sinh chi phí lớn hơn so với
ưu tiên toàn diện từ con người. Như được nêu trong §4.2, các quan sát của chúng tôi tiết lộ rằng
người chú thích cần một lượng thời gian đáng kể để so sánh hai đầu ra văn bản dài. Đối với nhiệm
vụ QA dạng dài, cả phản hồi chi tiết và phản hồi dựa trên ưu tiên đều mất khoảng 6 phút cho mỗi
mẫu đối với một người chú thích.

6.1 Tác động rộng hơn

Chúng tôi đề xuất framework FINE-GRAINED RLHF có thể kết hợp nhiều mô hình phần thưởng để
cung cấp phần thưởng dày đặc cho huấn luyện RL, dẫn đến đầu ra LM được tối ưu hóa hướng tới
các phần thưởng như vậy. Framework của chúng tôi có thể được áp dụng cho bất kỳ nhiệm vụ tạo
văn bản nào, do đó nâng cao hiệu suất LM bằng cách cung cấp hướng dẫn tinh tế hơn so với phản
hồi toàn diện. Lợi thế chính của framework FINE-GRAINED RLHF có hai mặt:

Tính linh hoạt. Framework của chúng tôi mở rộng đáng kể tính linh hoạt của các mô hình phần
thưởng cho RLHF. Ví dụ, công việc tương lai liên quan đến kiểm tra sự thật, phân loại cảm xúc,
phát hiện độc tính, trong số những thứ khác, đều có thể được kết hợp trong framework này. LM có
thể được huấn luyện chống lại tất cả các mô hình phần thưởng này thông qua FINE-GRAINED RLHF.

Khả năng kiểm soát. Có nhiều mô hình phần thưởng đại diện cho các loại phản hồi khác nhau cho
phép người dùng cuối có quyền kiểm soát lớn hơn đối với huấn luyện RL (ví dụ: thông qua các
kết hợp khác nhau của trọng số mô hình phần thưởng; xem chi tiết trong §4.5). Điều này dẫn đến
các hành vi mô hình tùy chỉnh, một lợi ích đặc biệt có giá trị cho các ứng dụng như công cụ giáo
dục nơi việc cá nhân hóa mô hình là quan trọng.

6.2 Hạn chế và công việc tương lai

Một hạn chế chính của framework chúng tôi đến từ chi phí tính toán bổ sung để có được phần thưởng
chi tiết, so với RLHF với phần thưởng toàn diện. Ví dụ, trong nhiệm vụ khử độc tố, chúng tôi cần
thực hiện nhiều cuộc gọi PERSPECTIVE API cho mỗi đầu ra mô hình tùy thuộc vào số câu được
tạo ra, trong khi RLHF với phần thưởng toàn diện chỉ yêu cầu một cuộc gọi. Trong nhiệm vụ QA
dạng dài, chúng tôi cần tính toán phần thưởng dày đặc từ nhiều mô hình phần thưởng, mất nhiều
thời gian tính toán và bộ nhớ GPU hơn so với một mô hình phần thưởng duy nhất.

Một hạn chế khác là các nhiệm vụ khác nhau có thể có các định nghĩa khác nhau về phản hồi chi
tiết về các loại phản hồi và mức độ mật độ của mỗi loại. Do đó, việc định nghĩa phản hồi phù hợp
cho một nhiệm vụ và huấn luyện các mô hình phần thưởng tương ứng yêu cầu nỗ lực thủ công không
tầm thường.

Cuối cùng, trong công việc này, chúng tôi kiểm soát cẩn thận chất lượng phản hồi được chú thích,
sau đó được sử dụng để huấn luyện các mô hình phần thưởng cho RL. Trong thực tế, khi một mô
hình được triển khai được phát hành cho công chúng, người dùng cuối không phải lúc nào cũng đưa
ra phản hồi sạch. Do đó, làm thế nào để có được tín hiệu học tập hiệu quả từ phản hồi từ con người
nhiễu trong thực tế vẫn cần điều tra thêm.

Một số câu hỏi thú vị khác để khám phá trong tương lai bao gồm: 1) Chúng ta có thể có được phản
hồi chi tiết từ LM như GPT-4 thay vì con người để cải thiện hiệu suất mô hình và giảm chi phí chú
thích không? 2) Các phương pháp không RL khác của việc sử dụng phản hồi từ con người như tạo
sinh được kiểm soát trong thời gian suy luận có thể bổ sung cho FINE-GRAINED RLHF như thế
nào? 3) Kích thước mô hình phần thưởng và giá trị chi tiết sẽ ảnh hưởng đến hiệu suất mô hình
chính sách trong quá trình huấn luyện RL như thế nào?

7 Kết luận

Trong công việc này, chúng tôi giới thiệu FINE-GRAINED RLHF, một framework cho phép LM học
từ nhiều mô hình phần thưởng chi tiết được huấn luyện từ phản hồi của con người, trong đó mỗi
mô hình phần thưởng phát hiện một danh mục lỗi cụ thể và cung cấp phần thưởng dày đặc. Chúng
tôi tiến hành phân tích thí nghiệm trên hai nhiệm vụ tạo văn bản để minh họa lợi ích hiệu suất của
FINE-GRAINED RLHF so với RLHF trên phần thưởng toàn diện, được hỗ trợ bởi cả đánh giá tự động
và con người. Hơn nữa, chúng tôi cho thấy rằng một LM có thể được tùy chỉnh cho nhu cầu cụ thể
bằng cách sử dụng các kết hợp khác nhau của các mô hình phần thưởng chi tiết.

--- TRANG 9-14 ---
[Nội dung từ trang 9-14 bao gồm Lời cảm ơn, Tài liệu tham khảo và Phụ lục với các ví dụ định tính, chi tiết thuật toán và triển khai, chi tiết thu thập dữ liệu và chú thích, và chi tiết huấn luyện mô hình phần thưởng]
