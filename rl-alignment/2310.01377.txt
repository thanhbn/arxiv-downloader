# 2310.01377.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2310.01377.pdf
# File size: 728488 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
Ganqu Cui* 1Lifan Yuan* 1 2Ning Ding1Guanming Yao1 3Bingxiang He1Wei Zhu4Yuan Ni4
Guotong Xie4Ruobing Xie5Yankai Lin6Zhiyuan Liu1Maosong Sun1 7
Abstract
Learning from human feedback has become a
pivot technique in aligning large language mod-
els (LLMs) with human preferences. However,
acquiring vast and premium human feedback is
bottlenecked by time, labor, and human capabil-
ity, resulting in small sizes or limited topics of
current datasets. This further hinders feedback
learning as well as alignment research within the
open-source community. To address this issue,
we explore how to go beyond human feedback
and collect high-quality AI feedback automati-
cally for a scalable alternative. Specifically, we
identify scale and diversity as the key factors
for feedback data to take effect. Accordingly, we
first broaden instructions and responses in both
amount and breadth to encompass a wider range
of user-assistant interactions. Then, we meticu-
lously apply a series of techniques to mitigate an-
notation biases for more reliable AI feedback. We
finally present ULTRA FEEDBACK , a large-scale,
high-quality, and diversified AI feedback dataset,
which contains over 1 million GPT-4 feedback
for 250k user-assistant conversations from vari-
ous aspects. Built upon ULTRA FEEDBACK , we
align a LLaMA-based model by best-of- nsam-
pling and reinforcement learning, demonstrating
its exceptional performance on chat benchmarks.
Our work validates the effectiveness of scaled AI
feedback data in constructing strong open-source
chat language models, serving as a solid founda-
tion for future feedback learning research.
*Equal contribution1NLP Group, DCST, IAI, BNRIST,
Tsinghua University2University of Illinois Urbana-Champaign
3ModelBest.Inc4PingAn Technology5Tencent6Renmin
University of China7Jiangsu Collaborative Innovation
Center for Language Ability. Correspondence to: Ganqu
Cui <cgq22@mails.tsinghua.edu.cn >, Lifan Yuan <li-
fan4@illinois.edu >, Wei Zhu <zhuwei972@pingan.com.cn >,
Zhiyuan Liu <liuzy@tsinghua.edu.cn >, Maosong Sun
<sms@tsinghua.edu.cn >.
Proceedings of the 41stInternational Conference on Machine
Learning , Vienna, Austria. PMLR 235, 2024. Copyright 2024 by
the author(s).1. Introduction
Large language models (LLMs) (OpenAI, 2022; 2023) have
demonstrated proficiency in generating fluent text as well as
solving various language-oriented tasks. Trained on massive
corpora through likelihood maximization techniques, these
LLMs have equipped the ability to execute diverse tasks in
response to user directives (Ouyang et al., 2022; Wei et al.,
2022a; Sanh et al., 2022). Unfortunately, relying solely on
imitation learning during training leads to well-known issues
- LLMs may generate convincing but incorrect or unsafe
content that deviates from human preferences (Stiennon
et al., 2020; Perez et al., 2022). To further align LLMs with
human preferences, learning from human feedback (Ouyang
et al., 2022; Askell et al., 2021; Bai et al., 2022a; Touvron
et al., 2023b) has been introduced and widely adopted by
leading corporations. Over a period, feedback learning has
been widely applied to closed-source models but scarcely
used in open-source models.
Many factors hinder the implementation of feedback learn-
ing in the open-source community, but the first and primary
issue is data. Preference data, which rates and compares
different responses given the same prompt, is central to feed-
back learning. When scaled sufficiently, preference data re-
flects the intrinsic values of the annotators. Such annotators
are often assumed, by default, to be human beings who can
provide the most flexible and accurate supervision signals,
yet the data they generate is severely bounded by factors like
financial resources, time, and knowledge. As a result, exist-
ing preference datasets are either small in scale (Wu et al.,
2023) or limited on specific tasks (Stiennon et al., 2020;
Nakano et al., 2021). To this end, more efficient and prin-
cipled methods to scale preference data are on the horizon.
This study aims to scale feedback data in an efficient man-
ner. Specifically, we explore AI feedback (Bai et al., 2022b;
Lee et al., 2023), which substitutes human annotators with
advanced LLMs. Compared with human feedback, AI feed-
back is more scalable , which means (1) it is easier to collect
and expand with lower cost; (2) its quality improves as
the LLM annotators become more capable. In previous re-
search, it is shown that advanced AI systems are capable of
conducting chatbot evaluations (Dubois et al., 2023; Zheng
et al., 2023a), giving textual critiques (Ye et al., 2023; Wang
1arXiv:2310.01377v2  [cs.CL]  16 Jul 2024

--- PAGE 2 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
et al., 2023c), or assisting human annotators (Saunders et al.,
2022). However, open-source LLMs have not yet benefited
from AI feedback through the lens of feedback learning.
This paper establishes a comprehensive AI feedback collec-
tion pipeline. Besides scalability, we prioritize diversity
of both instructions and responses for holistic language
model alignment. Particularly, we compile a diverse array
of over 60,000instructions and 17models from multiple
sources to produce comparative conversations in broad top-
ics and quality. Then, we adopt a bunch of techniques
to alleviate annotation biases and improve feedback qual-
ity to the greatest extent. These include (1) decomposing
annotation documents into four different aspects, namely
instruction-following, truthfulness, honesty, and helpfulness,
to reduce ambiguity; (2) providing objective grading criteria
and reference responses for score calibration; (3) asking
GPT-4 for detailed textual critique before scores as chain-of-
thought (Wei et al., 2022b) rationales. Comprehending all
above, we finally build ULTRA FEEDBACK , a million-scale
AI feedback dataset for aligning open-source LLMs.
We comprehensively validate the advantage of AI feed-
back in boosting open-source models with ULTRA FEED-
BACK . By fine-tuning a LLaMA2-13B model (Touvron
et al., 2023b), we build a state-of-the-art reward model
UltraRM, which significantly outperforms existing open-
source reward models. Based on UltraRM, we enhance a
powerful open-source model UltraLM (Ding et al., 2023;
Touvron et al., 2023a) with best-of- nsampling and PPO.
Experiments show that both strategies improve the model
dramatically. Moreover, we fine-tune a critique model that
could criticize and judge model responses. We also con-
duct a detailed analysis of the consistency and inconsistency
between AI and human feedback.
To summarize, our contributions are three-fold: (1) To the
best of our knowledge, we for the first time demonstrate the
beneficial effect of scaled AI feedback on open-source chat
LLMs. (2) We establish a systematic and sizable pipeline
to collect high-quality and diversified AI feedback. (3) We
release a suite of resources for feedback learning research,
including a dataset, reward model, and critique model.
2. U LTRA FEEDBACK
2.1. Overview
Inspired by the data engineering principles in supervised
fine-tuning (Ding et al., 2023; Chiang et al., 2023; Xu et al.,
2023), we identify scalability and diversity as pivot factors
of the overall generalizability of preference data. We argue
that existing preference data suffer from satisfying either one
of the two factors. To be specific, human feedback collection
usually relies on human annotators to compare a pair of
completions (Stiennon et al., 2020; Nakano et al., 2021;
Ouyang et al., 2022; Bai et al., 2022a). Thus, the data is hardto scale up due to time and budget constraints, especially
for open-source researchers. On the other hand, existing AI
feedback approaches (Bai et al., 2022b; Lee et al., 2023)
reduce human involvement and enjoy scalability via capable
LLMs, but they are limited to specific domains (Bai et al.,
2022b; Lee et al., 2023) or forms (Ye et al., 2023) and hence
lack the necessary diversity to boost LM performance under
broader contexts.
To this end, we take into account scalability and diversity
in all three stages of the preference data collection process:
collecting instructions, sampling completions, and annotat-
ing comparison pairs. The overview of the data collection
pipeline is shown in Figure 1. Firstly, we collect a large-
scale and diversified instruction set to enhance LLMs’ ca-
pabilities from four aspects: (1) Follow Instructions : LLMs
should respond to humans without deviating from the re-
quirements. (2) Helpful and Informative : LLMs should
provide useful and correct answers to address the given
problems. (3) Truthful : LLMs’ output should be grounded
in the instructions and real-world knowledge, and avoid in-
troducing any self-contradiction. (4) Honesty : LLMs should
know what they (don’t) know and express uncertainty to-
wards the given problem. For the second stage, to avoid
the sameness of comparison responses, we build a pool
of distinct models at different capability levels to sample
completions. Finally, to overcome the issues concerning
scalability (Nakano et al., 2021; Stiennon et al., 2020) and
quality (Ethayarajh et al., 2022), we seek scalable AI feed-
back from GPT-4, and explore several techniques to improve
the reliability. Next, we will introduce our data construction
pipeline in detail.
2.2. Instruction Collection
We select instructions that target four distinct but all-
important abilities of language models, namely instruction-
following, truthfulness, honesty, and helpfulness. Specif-
ically, we include all instructions from TruthfulQA (Lin
et al., 2022) and FalseQA (Hu et al., 2023) training set for
truthfulness. For instruction-following and helpfulness, we
randomly sample 10k instructions from Evol-Instruct (Xu
et al., 2023) and UltraChat (Ding et al., 2023) respectively,
and sample 20k from ShareGPT (Chiang et al., 2023). We
finally include FLAN (Longpre et al., 2023) to improve
LLMs’ helpfulness in various NLP tasks due to the task
diversity within FLAN. We adopt a stratified sampling strat-
egy following (Mukherjee et al., 2023), randomly picking 3k
instructions from the “CoT” subset and sampling 10 instruc-
tions per task for the other three subsets, while excluding
those with overly long instructions. In particular, honesty
will be assessed by TruthfulQA and FLAN as they both
contain reference answers, based on which it is easier for
the annotator to judge if the uncertainty expressed in LLMs’
responses calibrates with the accuracy. We then conduct
2

--- PAGE 3 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
Instruction Pool
Model Pool
MPT
ChatGPTLLaMA
Bard
…
Evol-Instruct
…Why is the problem always DNS?
Because it is a core 
component of the internet…GPT -4 Preference Annotation
Instruction -following
>
A
The statement is a 
humorous exaggeration…B
I'd like to clarify that 
the concept of…C
The phrase is a common 
saying among some IT…DHonesty
A B C D
D B A C= >
> = >Truthfulness=D A C B> >
Helpfulness
=D B A C> >Text A is near alignment 
with the task goal…Text B is correct and 
confident…
Text C is mostly truthful, 
but it contains…Text D is correct and 
provides a basic…
Comparison Data
Figure 1. ULTRA FEEDBACK construction process. We sample instructions and models from large pools to guarantee diversity, then query
GPT-4 with detailed illustrations for fine-grained and high-quality annotations in both textual and numerical formats.
a data contamination detection (see Appendix B). Finally,
we obtain 63,967instructions of various types from the six
publicly available high-quality datasets.
2.3. Completion Sampling
To guarantee that the collected responses are dissimilar and
well-distributed, we include different models to generate
completions for each instruction. To alleviate the potential
spurious correlation between text styles and response quality
within the dataset, we introduce intervention by selecting not
only different series of models at different levels, but also
models with different model sizes, architectures, and train-
ing data within the same model series. This strategy enables
one type of text style to present responses of different qual-
ity levels, namely the response of one series of models may
be better or worse than another depending on model sizes,
thus avoiding the establishment of spurious correlations.
Specifically, we set up a pool of 17 models: (1) For commer-
cial models, we choose GPT-4, gpt-3.5-turbo (Chat-
GPT), and Bard1; (2) For LLaMA-series, we choose
UltraLM-13B/65B (Ding et al., 2023), WizardLM-7B-
v1.1/13B-v1.2/70B-v1.1 (Xu et al., 2023), Vicuna-33B-v1.3
(Chiang et al., 2023), LLaMA2-7B/13B/70B-Chat (Touvron
et al., 2023b), and Alpaca-7B (Taori et al., 2023); (3) For
Non-LLaMA series, we choose MPT-30B-Chat (MosaicML,
2023), Falcon-40B-Instruct (Almazrouei et al., 2023), Star-
Chat (Tunstall et al., 2023), and Pythia-12B (Biderman et al.,
2023). We randomly sample four different models from the
pool to complete each instruction.
To further improve diversity in model responses, we elicit
distinct model behaviors by adding different principles be-
fore completing each instruction. Following Sun et al.
1https://bard.google.com/(2023) and Mukherjee et al. (2023), we first hand-craft one
principle for each aspect and then automize the procedure by
invoking GPT-4 to curate another ten based on the human-
written example. According to dataset characteristics, each
data source is assigned with different principle prompts. We
randomly sample a corresponding principle for each com-
pletion and add it to the system prompt to induce model
behaviors. The principles can be found in Appendix G.1,
and the effects of different principles are plotted in Figure 6.
2.4. AI Feedback Annotation
After generating 255,864model completions based on the
63,967instructions, we employ GPT-4 to provide two types
of feedback for each completion: (1) scalar scores that
indicate the fine-grained quality regarding multiple aspects,
and (2) textual critique that gives detailed guidance on how
to improve the completion. These lead to over 1 million
feedback data in total.
Preference Annotation. Regarding the potential subjec-
tivity and randomness of GPT-4 annotation, we apply four
techniques to improve the annotation quality: (1) Decompo-
sition . To reduce ambiguity and the difficulty of annotation,
we decompose the overall quality assessment into four fine-
grained assessments, namely instruction-following, truth-
fulness, honesty, and helpfulness. (2) Standard . For each
aspect, we provide GPT-4 with detailed documentation of
scores from 1 to 5 for reference, thus avoiding variable and
subjective standards. See Appendix G.2 for an example. (3)
Reference . To prevent inconsistency ratings across differ-
ent runs, we wrap one instruction and all its completions
into the prompt and ask GPT-4 to score four completions
simultaneously to reduce randomness. (4) Rationale . Be-
sides scoring each response, GPT-4 is required to generate a
rationale on how the response should be scored according
to the documentation. Combining all the techniques, we
3

--- PAGE 4 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
Table 1. Statistics of existing preference and critique datasets. The average length refers to the number of tokens.
Dataset # ConvsPrompt
LengthResponse
LengthCritique
LengthFine-
Grained?Feedback
Format# Pairs # Critique Annotator
Preference Dataset
OASST1 35,905 167.6 221.1 - % Scalar 17,966 - Human
OpenAI WebGPT 38,925 50.9 188.2 - % Scalar 19,578 - Human
Anthropic Helpful 118,263 185.7 94.6 - % Ranking 118,263 - Human
OpenAI Summ. 60,674 326.4 36.6 - ! Scalar 92,858 - Human
QA Feedback 11,378 155.8 107.9 - ! Scalar 17,118 - Human
Critique Dataset
SelFee 178,331 100.3 243.9 89.4 ! Text - 316,026 AI
Shepherd 1,316 95.3 97.6 67.2 ! Text - 1,317 Human
ULTRA FEEDBACK 255,864 185.1 305.3 143.1 ! Scalar & Text 340,025 255,864 AI
finally have four fine-grained scalar scores and rationales
for each response.
Critique Generation. Besides scalar reward, we also seek
textual critique from GPT-4. We prompt GPT-4 to act as a tu-
tor and provide detailed suggestions specified for each com-
pletion to help models improve rather than propose answers
directly. Different from the above comparison-oriented an-
notations, critique prompts are generated separately from an
overall perspective for each completion. The prompts can
be found in Appendix G.2.
2.5. Dataset Statistics
We compare ULTRA FEEDBACK with current open-source
datasets in Table 1. ULTRA FEEDBACK stands out to be
the largest one among all preference and critique datasets,
which is at least twice as large as other datasets. Also, its
completions and critiques are the longest. Moreover, we
highlight that ULTRA FEEDBACK isthe only dataset that
provides both scalar preferences and textual feedback ,
enabling it to serve as a preference and critique dataset
simultaneously. Overall, ULTRA FEEDBACK outperforms
previous datasets in both scale and diversity, and we also
validate its high quality by experiment in Section 3.
2.6. U LTRA FEEDBACK -Powered Models
Based on ULTRA FEEDBACK , we develop UltraRM, an ad-
vanced open-source reward model that provides preferences
for AI responses given user instructions. Additionally, we
train a critique model UltraCM from the textual feedback
inULTRA FEEDBACK . UltraCM could interact with human
and AI assistants more flexibly in text.
UltraRM. For reward modeling, we train UltraRM based
on LLaMA2-13B (Touvron et al., 2023b). Specifically,
we train three versions of UltraRM. We mix several
open-source datasets with ULTRA FEEDBACK to trainUltraRM. The open-source datasets include Stanford SHP
(Ethayarajh et al., 2022), OpenAI Summarization (Stiennon
et al., 2020), and Anthropic Helpful (Bai et al., 2022a). To
validate the quality of UltraFeedback, we also train one
model with merely the fine-grained scores of this dataset,
i.e. averaging the preference scores in each aspect to get
a final reward score. Further, to compare the effectiveness
of the fine-grained scores and overall scores, we replace the
fine-grained scores in UltraRM with the assessment ratings
in critique generation, while remaining the open-source
datasets. The details for dataset processing can be found in
Appendix E.1. We keep the training strategy, including loss
objective and training hyperparameters, exactly the same
as Touvron et al. (2023b).
UltraCM. We also train a critique model stemming from
ULTRA FEEDBACK to boost future research in learning from
textual feedback (Wang et al., 2023d). UltraCM has the
same initialization as UltraRM but is trained solely on UL-
TRAFEEDBACK critique data, i.e. 255,864textual feedback
in total. Given a response, we fine-tune the model to give
a corresponding critique that judges the response, figures
out flaws, and provides suggestions for improvement.
3. Experiments
To validate the effect of AI feedback, we first evaluate Ul-
traRM on human preference benchmarks in Section 3.1.
Next, we test UltraRM in enhancing chat language models
with two strategies, namely best-of- nsampling (Section 3.2)
and reinforcement learning (Section 3.3). Finally, we evalu-
ate the feedback quality of UltraCM in Appendix E.3.
3.1. Reward Modeling
Setup. To evaluate how UltraRM aligns with human prefer-
ence, we conduct experiments on four human annotated
preference datasets, OpenAI WebGPT (Nakano et al., 2021),
OpenAI Summarization (Stiennon et al., 2020), Anthropic
HH-RLHF (Bai et al., 2022a), and Standford SHP. On
4

--- PAGE 5 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
Table 2. Reward modeling accuracy (%) results. We compare our UltraRM with baseline open-source reward models. LLaMA2 results
are taken from (Touvron et al., 2023b). The highest results are in bold and the second highest scores are underlined .
Model Backbone Model Open?Anthropic
HelpfulOpenAI
WebGPTOpenAI
Summ.Stanford
SHPAvg.
Moss LLaMA-7B ✓ 61.3 58.1 59.0 54.6 58.3
Ziya LLaMA-7B ✓ 61.4 61.8 60.3 57.0 60.1
OASST DeBERTa-v3-large ✓ 67.6 - 71.8 53.9 -
SteamSHP FLAN-T5-XL ✓ 55.4 62.6 48.4 51.6 54.5
LLaMA2 Helpfulness LLaMA2-70B ✗ 72.0 - 75.5 80.0 -
UltraRM LLaMA2-13B ✓ 71.0 65.2 74.0 73.7 71.0
w/ Only U LTRA FEEDBACK LLaMA2-13B ✓ 66.7 65.1 66.8 68.4 66.8
w/ Overall Score LLaMA2-13B ✓ 71.0 62.0 73.0 73.6 69.9
each dataset, we calculate the rewards of two responses
for one prompt and predict which one is more preferred.
We compare our UltraRM-UF, UltraRM-Overall, and Ul-
traRM with open-source baselines, including Moss (Zheng
et al., 2023b), Ziya (IDEA-CCNL, 2021), OASST2, and
SteamSHP (Ethayarajh et al., 2022). We also report the
results in LLaMA2 (Touvron et al., 2023b), although their
reward models are not released.
Results. The preference prediction accuracy results are
reported in Table 2. As we can see, the UltraRM series
outperforms baseline reward models except for the closed
LLaMA2 reward model (much larger) by a large margin,
indicating that UltraRM series are the best open-source
reward models . Notably, our reward model can still
surpass all other baselines even without mixing open-source
datasets. These results reveal that, ULTRA FEEDBACK
is highly consistent with human preference, and its high
quality as well as diversity enable strong out-of-distribution
generalization. On average, the model trained with only
ULTRA FEEDBACK outperforms open-source baseline
models by over 6.3percent in accuracy, while mixing
open-source datasets with overall scores and fine-grained
scores of ULTRA FEEDBACK achieves 3.1and4.2percent
more improvement respectively.
We highlight that the OpenAI WebGPT dataset has no train-
ing and test splits, and neither most baselines nor we train
reward models on this dataset3, making it a fair benchmark
to evaluate the generalization ability of reward models. Ob-
viously, UltraRM series are significantly better, reaching
2.6%absolute points improvement over baselines. Another
intriguing finding is that adding open-source datasets has a
minor effect on the WebGPT dataset, which again proves
the transferability advantage of ULTRA FEEDBACK . On
another benchmark Stanford SHP, UltraRM also achieves
2https://huggingface.co/OpenAssistant/
reward-model-deberta-v3-large-v2
3The OASST and LLaMA2 Helpfulness reward model used
WebGPT dataset for training. To prevent data leakage, we do not
report their performance on WebGPT.remarkable performance.
A noteworthy finding is that, despite exhibiting comparably
on the other three datasets, the reward model trained with
overall scores discernably lags behind the other two vari-
ants on WebGPT. There can be two potential explanations
for this observation. First, fine-grained annotation, which
scores model outputs from different aspects respectively,
provides a more precise assessment for each completion
than aggregating evaluation into an overall number. Second,
in the overall quality annotation process, each sample is sent
to GPT-4 separately whereas, in fine-grained rating, all four
completions are scored at the same time, which may provide
GPT-4 with cross-references and prevent it from applying
inconsistent standards, reducing the impact of randomness.
These superiorities demonstrate the high quality of our fine-
grained preference data, and we advocate future work to
adopt the fine-grained annotation schema and rate multiple
completions at one time.
3.2. Best-of- nExperiments
/uni00000014 /uni00000015 /uni00000017 /uni0000001b /uni00000014/uni00000019
/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000003/uni00000036/uni00000044/uni00000050/uni00000053/uni0000004f/uni00000048/uni00000056/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000011/uni00000014/uni00000018/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000016/uni00000013/uni00000013/uni00000011/uni00000016/uni00000018/uni00000013/uni00000011/uni00000017/uni00000013/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047
/uni0000001a/uni00000019/uni0000001a/uni0000001b/uni0000001b/uni00000013/uni0000001b/uni00000015/uni0000001b/uni00000017/uni0000001b/uni00000019/uni0000001b/uni0000001b/uni0000001c/uni00000013/uni0000001c/uni00000015
/uni0000003a/uni0000004c/uni00000051/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048
/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047
/uni0000003a/uni0000004c/uni00000051/uni00000003/uni00000035/uni00000044/uni00000057/uni00000048
Figure 2. Win rate against text-davinci-003 on AlpacaEval
benchmark. We sample nresponses and choose the one with the
highest reward.
Setup. To verify that UltraRM could serve as a good indica-
tor of response quality, we conduct best-of- nexperiments.
5

--- PAGE 6 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
Table 3. Head-to-head comparison results on three public benchmarks. The baseline is text-davinci-003 in AlpacaEval and
gpt-3.5-turbo in Evol-Instruct and UltraChat. The judge is GPT-4. The highest win rates are in bold .
Model SizeAlpacaEval
Win (%)Evol-Instruct
Win / Tie / Lose (%)UltraChat
Win / Tie / Lose (%)Average
Win (%)
ChatGPT - 89.4 - - -
LLaMA2
Vicuna-13B-v1.5 13B - 33.0 / 23.9 / 43.1 34.5 / 38.2 / 27.3 -
LLaMA2-13B-Chat 13B 81.1 44.1 / 11.9 / 44.0 53.5 / 21.3 / 25.2 59.5
WizardLM-13B-v1.2 13B 89.2 55.5 / 17.4 / 27.1 59.7 / 25.5 / 14.8 68.1
OpenChat-13B-v3.2super 13B 89.5 55.5 / 11.0 / 33.5 58.7 / 26.7 / 14.5 67.9
LLaMA2-70B-Chat 70B 92.7 56.4 / 13.8 / 29.8 54.0 / 28.6 / 17.4 67.7
LLaMA
UltraLM-13B 13B 80.7 39.9 / 14.7 / 45.4 38.2 / 34.8 / 27.0 52.9
Vicuna-13B-v1.3 13B 82.1 36.7 / 17.4 / 45.9 41.3 / 33.2 / 25.5 53.4
WizardLM-13B-v1.1 13B 86.3 54.1 / 14.7 / 31.2 56.1 / 26.0 / 17.9 65.5
Vicuna-33B-v1.3 33B 89.0 50.0 / 17.0 / 33.0 57.7 / 25.7 / 16.6 65.6
UltraLM-13B-PPO 13B 86.3 57.8 / 10.1 / 32.1 64.9 / 15.6 / 19.5 69.7
On the AlpacaEval benchmark, we randomly sample 16
completions from the original UltraLM-13B and calculate
their corresponding rewards. We then select the best-of-
{1,2,4,8,16}completions as the final response. The sam-
pling parameters are set to temperature = 1and top- p= 1.
Results. We present results in Figure 2. Apparently, we
can see the win rate on AlpacaEval increases proportionally
with rewards. This validates that our UltraRM gives rigorous
rewards that reflect the overall response quality. Notably, the
best-of- nsampling strategy is surprisingly effective. The
initial UltraLM-13B model achieves a 76.53% win rate for
a single sampling, and a simple best-of- 2sample increases
the win rate to 84.64%. With more samples, we can get
even more high-quality responses, and the final best-of- 16
win rate hits 91.54%. The best-of- nsampling is universally
applicable across models and tasks, which enhances models
without training. Please refer to Appendix F.2 for cases.
3.3. PPO Experiments
Setup. Given the state-of-the-art UltraRM, we aim to
push the upper bound of open-source chat language models
with RLAIF. Specifically, we perform PPO over UltraLM-
13B (Ding et al., 2023) to get its PPO version, UltraLM-
13B-PPO. We tune UltraLM for 80 iterations on the UL-
TRAFEEDBACK prompts. In each iteration, we collect 512
samples and update the policy model with a mini-batch size
of 64. The learning rate is fixed at 1e-6.
Baselines. We compare UltraLM-13B-PPO with lead-
ing open-source models and proprietary models, including
LLaMA2-Chat (Touvron et al., 2023b), Vicuna (Chiang
et al., 2023), WizardLM (Xu et al., 2023), OpenChat (Wang
et al., 2023a), and ChatGPT (OpenAI, 2022).Benchmarks. We conduct experiments on three public
benchmarks, namely AlpacaEval (Li et al., 2023), Evol-
Instruct (Xu et al., 2023), and UltraChat (Ding et al., 2023).
On each benchmark, we ask GPT-4 to judge which response
is better given the same instruction. AlpacaEval adopts
text-davinci-003 as the competitor model, while we
compete with gpt-3.5-turbo on Evol-Instruct and Ul-
traChat. To avoid position bias, we randomly switch the
comparing responses. For all models, we use the same
decoding parameter with temperature = 0.7and top- p= 1.
Results. We report experiment results in Table 3. We take
the official results on the AlpacaEval leaderboard for base-
line models and conduct evaluations by ourselves for other
results. Overall, our UltraLM-13B-PPO achieves the high-
est average win rate on the three benchmarks, outperform-
ing all other open-source models. Among LLaMA-based
models, UltraLM-13B-PPO overtakes other models by at
least 3.6percent on average. Even when compared with
the much larger LLaMA2-70B-Chat model, our model still
holds the advantage, illustrating the huge benefit of RLAIF
alignment. Our model also reaches the highest win rate on
two of the benchmarks, Evol-Instruct and UltraChat, against
the more powerful gpt-3.5-turbo . It is worth noting
that, compared with the original UltraLM-13B, the PPO
process benefits the model greatly, leading to a 16.8percent
enhancement. We provide cases in Appendix F.3.
4. Agreement with Human Preferences
The inclusivity of human preferences is known to be hard
to capture (Dubois et al., 2023). Heavily relying on AI feed-
back, it is essential to measure and monitor the agreement
between AI and human preferences. In this section, we con-
duct experiments to see (1) to what extent AI annotations
6

--- PAGE 7 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
are consistent with human preferences (Section 4.1) and (2)
how reliable AI evaluations are (Section 4.2).
4.1. Annotation Consistency
In Section 3.1, we show that the reward models trained
onULTRA FEEDBACK could predict human preference
accurately. To further analyze to what extent AI feedback
could capture human preference, we randomly sample
400comparison pair from ULTRA FEEDBACK , AlpacaEval,
Evol-Instruct and UltraChat test sets (100 each) and
ask 3 independent annotators to compare those pairs
(win/tie/lose). The annotators are undergraduate and
graduate students. We present the agreement ratio between
GPT-4 and annotators, as well as annotators themselves
in Table 4. On average, GPT-4 judge exhibits 59.7%
agreement rate with human labelers, which matches
previous human evaluation on MT-Bench (Zheng et al.,
2023a). We also observe similar agreement rates among
annotators. Notably, the agreement between GPT-4 and the
majority votes of three annotators raises to 68.6%, meaning
that GPT-4 better reflects the collective human preferences.
Table 4. Agreement between judges on 400 samples from ULTRA -
FEEDBACK , AlpacaEval, Evol-Instruct, and UltraChat test sets .
A-1, A-2, A-3 are three human judges. “Majority” stands for the
agreement between each judge and other three judges’s majority
votes. We include tie votes and the random agreement is 33%.
Judge A-1 A-2 A-3 Average Majority
GPT-4 59.2% 60.8% 59.1% 59.7% 68.6%
A-1 - 58.1% 54.7% 57.3% 60.3%
A-2 58.1% - 55.4% 58.1% 63.3%
A-3 54.7% 55.4% - 56.4% 62.0%
4.2. Reliability of AI Evaluation
We first supplement another AI evaluation using Claude-
3 Sonnet (Anthropic, 2024) to investigate the agreement
among different series of AI models. The prompts are the
same as the GPT-4 evaluation. Then, we compare both of
our AI evaluation results with human annotations to examine
if AI evaluations reliably correlate with humans. Particu-
larly, we use the majority votes of the three annotators and
filter out samples with all different votes.
We present GPT-4, Claude-3, and human evaluation results
on the remaining 266 pairs in Table 5. Overall, Claude-3
shares the same trend as GPT-4 and further increases our
models’ win rates. Human evaluations are mostly consistent
with GPT-4, giving a 64.3% against 67.3% average winning
rate. We notice that human labelers tend to assign more ties
than GPT-4, leading to slightly lower winning rates.
For fine-grained analysis, we categorize the evaluated sam-
ples into reasoning, writing, and QA tasks. The categorical
comparison results are presented in Figure 3. It is shownthat human evaluations are mostly consistent with GPT-4,
where they both prefer our models on writing and QA tasks.
On reasoning tasks including coding, math, and logic, hu-
man and GPT-4 judgments diverge on ties and losses, where
GPT-4 gives fewer ties but more losses. To delve into the dis-
crepancy deeper, we ask another expert labeler to determine
the ground truth answer for each question. In this sense, a
model wins when it gives the correct answer while the other
does not, and vice versa. The two models tie when they both
successfully or unsuccessfully answer the question. The
final win/tie/lose rate comes at 42.1%/26.3%/31.6%and
closely matches human evaluations. The GPT-4 judge, in
this case, potentially underestimated our model’s reasoning
performance and still has space to improve.
Table 5. Human evaluation results. We use majority votes from
three human judges and compare GPT-4, Claude-3, and human
evaluations on the same 266 samples.
JudgeAlpacaEval Evol-Instruct UltraChat Avg.
Win (%) Win / Tie / Lose (%) Win (%)
GPT-4 83.9 57.1 / 8.8 / 34.1 61.0 / 17.1 / 21.9 67.3
Claude-3 95.1 59.6 / 1.4 / 39.0 73.5 / 6.8 / 19.7 76.1
Human 78.5 68.1 / 17.6 / 14.3 46.3 / 19.5 / 34.1 64.3
Win Tie
Reasoning (57)Lose Win Tie
Writing (61)Lose Win Tie
QA (148)Lose020406080Percentage (%)Comparison of Human and GPT4 Judgements (in %)
Human
GPT4
Figure 3. Catrgorical comparison of human and GPT-4 judgments.
Human judgments are majority votes from three annotators. Sam-
ple numbers of each category are in parentness.
5. Analysis
In this section, we further analyze how ULTRA FEED-
BACK enhances language models on different subjects (Sec-
tion 5.1) and tasks (Section 5.2).
5.1. Question Type Breakdown
Figure 4 reports the UltraLM-13B-PPO and UltraLM-
13B scores on different question types versus
gpt-3.5-turbo on the Evol-Instruct test set. We
observe that UltraLM-13B-PPO overtakes ChatGPT on
22/29 subjects, especially on writing-related tasks such as
academic writing. Our model is also well-aligned with
human values, getting higher scores on toxicity, ethics,
and TruthfulQA. On some difficult subjects like roleplay,
reasoning, and counterfactual, our model is still on par
with ChatGPT, indicating the strong advanced model
capability. Compared with the original UltraLM-13B,
PPO boosts the model in multiple aspects, including
7

--- PAGE 8 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
/uni00000037/uni00000052/uni0000005b/uni0000004c/uni00000046/uni0000004c/uni00000057/uni0000005c
/uni00000024/uni00000046/uni00000044/uni00000047/uni00000048/uni00000050/uni0000004c/uni00000046/uni00000003/uni0000003a/uni00000055/uni0000004c/uni00000057/uni0000004c/uni00000051/uni0000004a/uni00000028/uni00000051/uni00000057/uni00000048/uni00000055/uni00000057/uni00000044/uni0000004c/uni00000051/uni00000050/uni00000048/uni00000051/uni00000057/uni0000002f/uni0000004c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni00000058/uni00000055/uni00000048/uni00000030/uni00000058/uni00000056/uni0000004c/uni00000046/uni00000025/uni0000004c/uni00000052/uni0000004f/uni00000052/uni0000004a/uni0000005c/uni00000036/uni00000053/uni00000052/uni00000055/uni00000057/uni00000024/uni00000055/uni00000057
/uni00000037/uni00000048/uni00000046/uni0000004b/uni00000051/uni00000052/uni0000004f/uni00000052/uni0000004a/uni0000005c/uni00000026/uni0000004b/uni00000048/uni00000050/uni0000004c/uni00000056/uni00000057/uni00000055/uni0000005c/uni00000028/uni00000057/uni0000004b/uni0000004c/uni00000046/uni00000056
/uni00000033/uni0000004b/uni0000004c/uni0000004f/uni00000052/uni00000056/uni00000052/uni00000053/uni0000004b/uni0000005c/uni0000003a/uni00000055/uni0000004c/uni00000057/uni00000057/uni0000004c/uni00000051/uni0000004a
/uni00000026/uni00000052/uni00000058/uni00000051/uni00000057/uni00000048/uni00000055/uni00000049/uni00000044/uni00000046/uni00000057/uni00000058/uni00000044/uni0000004f/uni0000002f/uni00000044/uni0000005a
/uni00000026/uni00000052/uni00000050/uni00000050/uni00000052/uni00000051/uni00000010/uni00000036/uni00000048/uni00000051/uni00000056/uni00000048/uni00000028/uni00000046/uni00000052/uni00000051/uni00000052/uni00000050/uni0000005c/uni00000033/uni0000004b/uni0000005c/uni00000056/uni0000004c/uni00000046/uni00000056
/uni00000037/uni00000055/uni00000058/uni00000057/uni0000004b/uni00000049/uni00000058/uni0000004f/uni00000034/uni00000024/uni0000002b/uni0000004c/uni00000056/uni00000057/uni00000052/uni00000055/uni0000005c/uni00000030/uni00000048/uni00000047/uni0000004c/uni00000046/uni0000004c/uni00000051/uni00000048 /uni00000035/uni00000052/uni0000004f/uni00000048/uni00000053/uni0000004f/uni00000044/uni0000005c
/uni00000026/uni00000052/uni00000050/uni00000053/uni00000058/uni00000057/uni00000048/uni00000055/uni00000003/uni00000036/uni00000046/uni0000004c/uni00000048/uni00000051/uni00000046/uni00000048/uni00000035/uni00000048/uni00000044/uni00000056/uni00000052/uni00000051/uni0000004c/uni00000051/uni0000004a /uni00000030/uni00000058/uni0000004f/uni00000057/uni0000004c/uni0000004f/uni0000004c/uni00000051/uni0000004a/uni00000058/uni00000044/uni0000004f
/uni00000026/uni00000052/uni00000050/uni00000053/uni0000004f/uni00000048/uni0000005b/uni00000003/uni00000029/uni00000052/uni00000055/uni00000050/uni00000044/uni00000057 /uni00000026/uni00000052/uni00000047/uni00000048/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000030/uni00000044/uni00000057/uni0000004b
/uni00000026/uni00000052/uni00000047/uni00000048/uni00000003/uni00000027/uni00000048/uni00000045/uni00000058/uni0000004a/uni00000013/uni00000008/uni00000015/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008/uni0000001b/uni00000013/uni00000008/uni00000014/uni00000013/uni00000013/uni00000008/uni00000014/uni00000015/uni00000013/uni00000008/uni00000014/uni00000017/uni00000013/uni00000008/uni00000038/uni0000004f/uni00000057/uni00000055/uni00000044/uni0000002f/uni00000030/uni00000010/uni00000033/uni00000033/uni00000032/uni00000003/uni00000012/uni00000003/uni00000026/uni0000004b/uni00000044/uni00000057/uni0000002a/uni00000033/uni00000037/uni00000014/uni00000017/uni00000016/uni00000011/uni0000001b/uni00000008
/uni00000014/uni00000015/uni00000015/uni00000011/uni00000013/uni00000008 /uni00000014/uni00000015/uni00000014/uni00000011/uni00000014/uni00000008/uni00000014/uni00000014/uni0000001b/uni00000011/uni0000001c/uni00000008 /uni00000014/uni00000014/uni0000001a/uni00000011/uni0000001c/uni00000008 /uni00000014/uni00000014/uni0000001a/uni00000011/uni0000001b/uni00000008 /uni00000014/uni00000014/uni00000019/uni00000011/uni00000015/uni00000008 /uni00000014/uni00000014/uni00000019/uni00000011/uni00000013/uni00000008 /uni00000014/uni00000014/uni00000017/uni00000011/uni00000016/uni00000008 /uni00000014/uni00000014/uni00000015/uni00000011/uni00000018/uni00000008 /uni00000014/uni00000014/uni00000015/uni00000011/uni00000015/uni00000008 /uni00000014/uni00000014/uni00000014/uni00000011/uni00000014/uni00000008/uni00000014/uni00000013/uni0000001a/uni00000011/uni00000018/uni00000008 /uni00000014/uni00000013/uni0000001a/uni00000011/uni00000013/uni00000008 /uni00000014/uni00000013/uni00000018/uni00000011/uni0000001c/uni00000008 /uni00000014/uni00000013/uni00000018/uni00000011/uni0000001a/uni00000008 /uni00000014/uni00000013/uni00000017/uni00000011/uni0000001a/uni00000008 /uni00000014/uni00000013/uni00000017/uni00000011/uni00000017/uni00000008 /uni00000014/uni00000013/uni00000016/uni00000011/uni00000019/uni00000008 /uni00000014/uni00000013/uni00000015/uni00000011/uni0000001c/uni00000008 /uni00000014/uni00000013/uni00000015/uni00000011/uni00000016/uni00000008 /uni00000014/uni00000013/uni00000015/uni00000011/uni00000013/uni00000008/uni0000001c/uni00000019/uni00000011/uni0000001c/uni00000008/uni0000001c/uni00000017/uni00000011/uni0000001b/uni00000008 /uni0000001c/uni00000017/uni00000011/uni00000016/uni00000008/uni0000001c/uni00000015/uni00000011/uni00000014/uni00000008
/uni0000001b/uni00000016/uni00000011/uni00000015/uni00000008/uni0000001a/uni0000001c/uni00000011/uni00000015/uni00000008/uni0000001a/uni00000017/uni00000011/uni0000001a/uni00000008/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni0000001d/uni00000003/uni00000014/uni00000013/uni00000013/uni00000011/uni00000016/uni00000008/uni00000038/uni0000004f/uni00000057/uni00000055/uni00000044/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000016/uni00000025/uni00000010/uni00000033/uni00000033/uni00000032
/uni00000038/uni0000004f/uni00000057/uni00000055/uni00000044/uni0000002f/uni00000030/uni00000010/uni00000014/uni00000016/uni00000025
Figure 4. Comparison results between UltraLM-13B-PPO, UltraLM-13B, and gpt-3.5-turbo on Evol-Instruct test set, where
gpt-3.5-turbo scores are 100% .
Table 6. Exact match scores (%) for UltraLM-13B and UltraLM-13B-PPO on model capability benchmarks.
Model BoolQ HellaSwag RACE-h RACE-m MultiRC TriviaQA NQ PIQA OBQA ARC-E ARC-C Avg.
UltraLM-13B 85.0 59.8 66.1 73.5 83.2 50.8 19.4 73.5 57.0 76.1 51.5 63.3
UltraLM-13B-PPO 83.5 62.6 66.8 74.2 83.7 52.5 22.1 74.9 57.0 76.1 53.9 64.3
professional knowledge (economy, chemistry, music,
literature) and reasoning ability (reasoning, complex format,
code generation, math). Meanwhile, our model falls
behind gpt-3.5-turbo on math and code-related tasks,
which might be attributed to the limitation of base model
ability and the lack of relevant data in ULTRA FEEDBACK .
Table 9 in Appendix E.5 provides additional results on the
UltraChat test set and reaches the same conclusion. We
leave this as our future work.
5.2. Does RLAIF Benefit Model Capability?
To test whether RLAIF impacts base model capability, we
conduct experiments on nine commonly used benchmarks
including question answering and multiple-choice questions
(See Appendix E.4 for details). We compare UltraLM-13B
before and after PPO. The results in Table 6 demonstrate
marginal improvements over these benchmarks with about
1 absolute point. We note that this is in line with established
conclusions (OpenAI, 2023) regarding RLHF, which state
that RLHF could produce more preferable responses, but
has a minor effect on model capability.
6. Related Work
Feedback Learning for LLMs. Incorporating human
feedback with imitation learning or reinforcement learn-
ing (Schulman et al., 2017; Rafailov et al., 2023) has been
the mainstream approach to align LLMs with human prefer-
ences in leading cooperations (Stiennon et al., 2020; Ouyang
et al., 2022; Bai et al., 2022a; Glaese et al., 2022; OpenAI,
2022; 2023; Touvron et al., 2023a). However, human feed-
back relies on human capabilities, which makes it hard
to scale up and apply to superhuman tasks. Accordingly,
some researchers proposed scalable oversight , which aims
to supervise potent AI models by models themselves (Irv-
ing et al., 2018; Leike et al., 2018; Christiano et al., 2018).
Empirically for LLMs, Bai et al. (2022b) first presentedConstitutional AI to let LLMs refine their responses given a
set of regulations. Lee et al. (2023) and Burns et al. (2023)
further validated that learning from AI feedback could sur-
pass human feedback on some specific tasks. More broadly,
our work verified that scaled AI feedback could enhance the
general ability of open-source chat models.
Data for LLM Alignment. The importance of data scala-
bility and quality has been widely recognized in the litera-
ture on instruction tuning (also known as SFT). Early works
collected various NLP tasks or real user conversations to
conduct instruction tuning and observed that LLMs could
generalize well across different tasks (Wei et al., 2022a;
Sanh et al., 2022; Wang et al., 2022; Ouyang et al., 2022).
After the release of ChatGPT, most recent research on SFT
emphasized the importance of data construction and reached
conclusions that scalability, diversity, as well as quality, are
vital for the final performance (Ding et al., 2023; Taori et al.,
2023; Chiang et al., 2023; Xu et al., 2023). However, when
it goes to the feedback learning stage, the importance of data
engineering has not been well illustrated. Among current
preference datasets, some of them focus on specific tasks
(e.g. summarization (Stiennon et al., 2020), search-based
question answering (Nakano et al., 2021), safety-oriented
scenarios (Ji et al., 2023), and math problems (Lightman
et al., 2023)), thus cannot boost general chat models. Some
datasets are small in scale (Wu et al., 2023; Wang et al.,
2023c) or provide only community votes as coarse-grained
preferences (Ethayarajh et al., 2022; Askell et al., 2021).
Therefore, a large general-purpose preference dataset with
diverse instructions and fine-grained annotations in the open-
source community is urgently in need, which motivates us
to construct U LTRA FEEDBACK .
7. Conclusion
In this paper, we proposed to enhance open-source LLMs
with scaled AI feedback. Through meticulous designation,
we constructed ULTRA FEEDBACK , a large-scale and di-
8

--- PAGE 9 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
verse AI feedback dataset. With the data, we embarked on a
thorough exploration of AI feedback’s multifaceted utilities,
including modeling human preferences, improving chat lan-
guage models, and training critique models. Our analysis
further delved deep into human agreement and model ca-
pability evaluations, revealing some nuanced insights. We
believe that AI feedback would become a scalable and reli-
able source for future AI oversight. We hope our work could
serve as an early exploration and data support in this area,
facilitating researchers in the open-source community. In fu-
ture work, we will continue exploring diverse, high-quality,
and scalable preference data construction, expanding AI
feedback in multi-turn dialogues, complex reasoning, cod-
ing, and safety scenarios.
Impact Statement
Aligning AI systems, especially advanced LLMs, is impor-
tant for the safety and trustworthiness in their applications.
We manage to enhance open LLMs with scaled AI feedback
which is an underexplored research direction. With high
efficiency and low cost, leveraging AI feedback could sig-
nificantly reduce the consumation of human labors, leading
to more scalable alignment. We should also raise attention
to the limitations of AI feedback, LLMs could be biased
towards certain features, such as answer positions (Zheng
et al., 2023a), response lengths, and certain styles. In this
way, such biases might lead to inaccurate or unfair annota-
tions and evaluations. By overcoming these biases, more
precise and helpful AI feedback can be obtained. In terms of
ULTRA FEEDBACK , we could expect it to improve a consid-
erable amount of open-source LLMs and narrow their gaps
with close-sourced models. We did not add safety-oriented
conversations intentionally, so there could still be toxicity
and unethical behaviors in the aligned models if prompted
adversarially. We believe our paradigm is still useful for
enhancing model safety, and are extensively working on
it. Alongside data, we also release a series of models for
feedback learning research. The reward model and critique
model can be directly used to align LLMs for more pref-
ered behaviors. On the other hand, although our models are
potent in solving tasks and giving feedback, they may also
generate hallucinations and falsehoods. The risk of misuse
is a severe threat to open LLMs, which calls for appropriate
regulation and supervision.
Acknowledgement
This work is supported by the National Key R&D Program
of China (No.2022ZD0116312), National Natural Science
Foundation of China (No. 62236004).References
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Al-
shamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mer-
ouane Debbah, Etienne Goffinet, Daniel Heslow, Julien
Launay, Quentin Malartic, Badreddine Noune, Baptiste
Pannier, and Guilherme Penedo. Falcon-40B: an open
large language model with state-of-the-art performance.
2023.
Anthropic. Introducing the next generation of claude. 2024.
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep
Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph,
Ben Mann, Nova DasSarma, et al. A general language
assistant as a laboratory for alignment. arXiv preprint
arXiv:2112.00861 , 2021.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
Bosma, Henryk Michalewski, David Dohan, Ellen Jiang,
Carrie Cai, Michael Terry, Quoc Le, et al. Program
synthesis with large language models. arXiv preprint
arXiv:2108.07732 , 2021.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell,
Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,
Deep Ganguli, Tom Henighan, et al. Training a help-
ful and harmless assistant with reinforcement learning
from human feedback. arXiv preprint arXiv:2204.05862 ,
2022a.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda
Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna
Goldie, Azalia Mirhoseini, Cameron McKinnon, et al.
Constitutional ai: Harmlessness from ai feedback. arXiv
preprint arXiv:2212.08073 , 2022b.
Stella Biderman, Hailey Schoelkopf, Quentin Gregory
Anthony, Herbie Bradley, Kyle O’Brien, Eric Hal-
lahan, Mohammad Aflah Khan, Shivanshu Purohit,
USVSN Sai Prashanth, Edward Raff, Aviya Skowron,
Lintang Sutawika, and Oskar van der Wal. Pythia: A
suite for analyzing large language models across train-
ing and scaling. In Andreas Krause, Emma Brunskill,
Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and
Jonathan Scarlett (eds.), International Conference on
Machine Learning, ICML 2023, 23-29 July 2023, Hon-
olulu, Hawaii, USA , Proceedings of Machine Learning
Research, 2023.
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,
et al. Piqa: Reasoning about physical commonsense in
natural language. In Proceedings of the AAAI conference
on artificial intelligence , 2020.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
9

--- PAGE 10 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-
guage models are few-shot learners. Advances in neural
information processing systems , 33:1877–1901, 2020.
Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner,
Bowen Baker, Leo Gao, Leopold Aschenbrenner,
Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan
Leike, Ilya Sutskever, Jeff Wu, and OpenAI. Weak-
to-strong generalization: Eliciting strong capabili-
ties with weak supervision. ArXiv , abs/2312.09390,
2023. URL https://api.semanticscholar.
org/CorpusID:266312608 .
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Hen-
rique Ponde de Oliveira Pinto, Jared Kaplan, Harri Ed-
wards, Yuri Burda, Nicholas Joseph, Greg Brockman,
et al. Evaluating large language models trained on code.
arXiv preprint arXiv:2107.03374 , 2021.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-
hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.
Xing. Vicuna: An open-source chatbot impressing gpt-4
with 90%* chatgpt quality, March 2023. URL https:
//lmsys.org/blog/2023-03-30-vicuna/ .
Paul Francis Christiano, Buck Shlegeris, and Dario Amodei.
Supervising strong learners by amplifying weak experts.
ArXiv , abs/1810.08575, 2018. URL https://api.
semanticscholar.org/CorpusID:53041432 .
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom
Kwiatkowski, Michael Collins, and Kristina Toutanova.
Boolq: Exploring the surprising difficulty of natural
yes/no questions. arXiv preprint arXiv:1905.10044 , 2019.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,
Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. Think you have solved question answering?
try arc, the ai2 reasoning challenge. arXiv preprint
arXiv:1803.05457 , 2018.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark
Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert,
Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al.
Training verifiers to solve math word problems. arXiv
preprint arXiv:2110.14168 , 2021.
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng,
Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen
Zhou. Enhancing chat language models by scaling
high-quality instructional conversations. arXiv preprint
arXiv:2305.14233 , 2023.
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,
Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, PercyLiang, and Tatsunori B Hashimoto. Alpacafarm: A sim-
ulation framework for methods that learn from human
feedback. arXiv preprint arXiv:2305.14387 , 2023.
Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta.
Understanding dataset difficulty with V-usable informa-
tion. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song,
Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.),
Proceedings of the 39th International Conference on Ma-
chine Learning , volume 162 of Proceedings of Machine
Learning Research , pp. 5988–6008. PMLR, 17–23 Jul
2022.
Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, An-
thony DiPofi, Charles Foster, Laurence Golding, Jeffrey
Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang,
Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin
Wang, and Andy Zou. A framework for few-shot lan-
guage model evaluation, September 2021. URL https:
//doi.org/10.5281/zenodo.5371628 .
Amelia Glaese, Nat McAleese, Maja Trkebacz, John
Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,
Laura Weidinger, Martin Chadwick, Phoebe Thacker,
et al. Improving alignment of dialogue agents via targeted
human judgements. arXiv preprint arXiv:2209.14375 ,
2022.
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
Arora, Steven Basart, Eric Tang, Dawn Song, and Ja-
cob Steinhardt. Measuring mathematical problem solving
with the math dataset. arXiv preprint arXiv:2103.03874 ,
2021.
Shengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng,
Zhiyuan Liu, and Maosong Sun. Won’t get fooled again:
Answering questions with false premises. In Anna Rogers,
Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Pro-
ceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers),
ACL 2023, Toronto, Canada, July 9-14, 2023 , 2023.
IDEA-CCNL. Fengshenbang-lm. https://github.
com/IDEA-CCNL/Fengshenbang-LM , 2021.
Geoffrey Irving, Paul Francis Christiano, and Dario
Amodei. Ai safety via debate. ArXiv , abs/1805.00899,
2018. URL https://api.semanticscholar.
org/CorpusID:22050710 .
Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang,
Ce Bian, Ruiyang Sun, Yizhou Wang, and Yaodong
Yang. Beavertails: Towards improved safety alignment
of llm via a human-preference dataset. arXiv preprint
arXiv:2307.04657 , 2023.
10

--- PAGE 11 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettle-
moyer. Triviaqa: A large scale distantly supervised chal-
lenge dataset for reading comprehension. arXiv preprint
arXiv:1705.03551 , 2017.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,
Shyam Upadhyay, and Dan Roth. Looking beyond the
surface: A challenge set for reading comprehension over
multiple sentences. In Proceedings of the 2018 Confer-
ence of the North American Chapter of the Association
for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long Papers) , pp. 252–262, 2018.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
Michael Collins, Ankur Parikh, Chris Alberti, Danielle
Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al.
Natural questions: a benchmark for question answering
research. Transactions of the Association for Computa-
tional Linguistics , 7:453–466, 2019.
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,
and Eduard Hovy. Race: Large-scale reading com-
prehension dataset from examinations. arXiv preprint
arXiv:1704.04683 , 2017.
Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu,
Thomas Mesnard, Colton Bishop, Victor Carbune, and
Abhinav Rastogi. Rlaif: Scaling reinforcement learning
from human feedback with ai feedback. arXiv preprint
arXiv:2309.00267 , 2023.
Jan Leike, David Krueger, Tom Everitt, Miljan Mar-
tic, Vishal Maini, and Shane Legg. Scalable agent
alignment via reward modeling: a research direction.
ArXiv , abs/1811.07871, 2018. URL https://api.
semanticscholar.org/CorpusID:53745764 .
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,
Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tat-
sunori B. Hashimoto. Alpacaeval: An automatic evaluator
of instruction-following models. https://github.
com/tatsu-lab/alpaca_eval , 2023.
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Ed-
wards, Bowen Baker, Teddy Lee, Jan Leike, John Schul-
man, Ilya Sutskever, and Karl Cobbe. Let’s verify step by
step. arXiv preprint arXiv:2305.20050 , 2023.
Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa:
Measuring how models mimic human falsehoods. In
Smaranda Muresan, Preslav Nakov, and Aline Villavi-
cencio (eds.), Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume
1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27,
2022 , 2022.Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V . Le,
Barret Zoph, Jason Wei, and Adam Roberts. The flan
collection: Designing data and methods for effective in-
struction tuning. CoRR , abs/2301.13688, 2023.
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sab-
harwal. Can a suit of armor conduct electricity? a new
dataset for open book question answering. arXiv preprint
arXiv:1809.02789 , 2018.
MosaicML. Introducing mpt-30b: Raising the bar for open-
source foundation models, 2023. URL www.mosaicml.
com/blog/mpt-30b . Accessed: 2023-06-22.
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sa-
haj Agarwal, Hamid Palangi, and Ahmed Hassan Awadal-
lah. Orca: Progressive learning from complex explanation
traces of GPT-4. CoRR , abs/2306.02707, 2023.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse, Shan-
tanu Jain, Vineet Kosaraju, William Saunders, et al. We-
bgpt: Browser-assisted question-answering with human
feedback. arXiv preprint arXiv:2112.09332 , 2021.
OpenAI. Chatgpt: Optimizing language models for dia-
logue, 2022.
OpenAI. Gpt-4 technical report, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sand-
hini Agarwal, Katarina Slama, Alex Ray, et al. Train-
ing language models to follow instructions with human
feedback. Advances in Neural Information Processing
Systems , 35:27730–27744, 2022.
Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Mered-
ith Ringel Morris, Percy Liang, and Michael S Bernstein.
Generative agents: Interactive simulacra of human behav-
ior.arXiv preprint arXiv:2304.03442 , 2023.
Ethan Perez, Saffron Huang, H. Francis Song, Trevor
Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat
McAleese, and Geoffrey Irving. Red teaming language
models with language models. In Proceedings of the 2022
Conference on Empirical Methods in Natural Language
Processing, EMNLP 2022, Abu Dhabi, United Arab Emi-
rates, December 7-11, 2022 , pp. 3419–3448. Association
for Computational Linguistics, 2022. doi: 10.18653/v1/
2022.emnlp-main.225. URL https://doi.org/10.
18653/v1/2022.emnlp-main.225 .
Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng
Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Commu-
nicative agents for software development. arXiv preprint
arXiv:2307.07924 , 2023.
11

--- PAGE 12 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning
Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun
Xiao, Chi Han, et al. Tool learning with foundation
models. arXiv preprint arXiv:2304.08354 , 2023.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Er-
mon, Christopher D Manning, and Chelsea Finn. Direct
preference optimization: Your language model is secretly
a reward model. arXiv preprint arXiv:2305.18290 , 2023.
Victor Sanh, Albert Webson, Colin Raffel, Stephen H.
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaf-
fin, Arnaud Stiegler, Arun Raja, Manan Dey, M Sai-
ful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma
Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chh-
ablani, Nihal V . Nayak, Debajyoti Datta, Jonathan Chang,
Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng
Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden,
Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht
Sharma, Andrea Santilli, Thibault F ´evry, Jason Alan
Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo
Gao, Thomas Wolf, and Alexander M. Rush. Multitask
prompted training enables zero-shot task generalization.
InThe Tenth International Conference on Learning Repre-
sentations, ICLR 2022, Virtual Event, April 25-29, 2022 ,
2022. URL https://openreview.net/forum?
id=9Vrb9D0WI4 .
William Saunders, Catherine Yeh, Jeff Wu, Steven
Bills, Long Ouyang, Jonathan Ward, and Jan Leike.
Self-critiquing models for assisting human evaluators.
CoRR , abs/2206.05802, 2022. doi: 10.48550/ARXIV .
2206.05802. URL https://doi.org/10.48550/
arXiv.2206.05802 .
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-
ford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347 , 2017.
Noah Shinn, Federico Cassano, Beck Labash, Ashwin
Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflex-
ion: Language agents with verbal reinforcement learning,
2023.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler,
Ryan Lowe, Chelsea V oss, Alec Radford, Dario Amodei,
and Paul F Christiano. Learning to summarize with hu-
man feedback. Advances in Neural Information Process-
ing Systems , 33:3008–3021, 2020.
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang,
Zhenfang Chen, David D. Cox, Yiming Yang, and
Chuang Gan. Principle-driven self-alignment of language
models from scratch with minimal human supervision.
ArXiv , abs/2305.03047, 2023.Alon Talmor, Jonathan Herzig, Nicholas Lourie, and
Jonathan Berant. Commonsenseqa: A question answer-
ing challenge targeting commonsense knowledge. arXiv
preprint arXiv:1811.00937 , 2018.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tat-
sunori B Hashimoto. Stanford alpaca: An instruction-
following llama model, 2023.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
Llama: Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971 , 2023a.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288 , 2023b.
Lewis Tunstall, Nathan Lambert, Nazneen Rajani, Edward
Beeching, Teven Le Scao, Leandro von Werra, Sheon
Han, Philipp Schmid, and Alexander Rush. Creating a
coding assistant with starcoder. Hugging Face Blog , 2023.
https://huggingface.co/blog/starchat.
Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen
Song, and Yang Liu. Openchat: Advancing open-source
language models with mixed-quality data. arXiv preprint
arXiv:2309.11235 , 2023a.
Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu,
Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhi-
fang Sui. Large language models are not fair evaluators,
2023b.
Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O’Brien,
Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva,
Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Ce-
likyilmaz. Shepherd: A critic for language model genera-
tion. arXiv preprint arXiv:2308.04592 , 2023c.
Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen,
Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating
llms in multi-turn interaction with tools and language
feedback. arXiv preprint arXiv:2309.10691 , 2023d.
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi,
Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Ar-
jun Ashok, Arut Selvan Dhanasekaran, Anjana Arunku-
mar, David Stap, Eshaan Pathak, Giannis Karamanolakis,
Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob
Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar
Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar,
Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza,
12

--- PAGE 13 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Sa-
van Doshi, Shailaja Keyur Sampat, Siddhartha Mishra,
Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong
Shen. Super-naturalinstructions: Generalization via
declarative instructions on 1600+ NLP tasks. In Proceed-
ings of the 2022 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2022, Abu Dhabi,
United Arab Emirates, December 7-11, 2022 , pp. 5085–
5109, 2022. URL https://doi.org/10.18653/
v1/2022.emnlp-main.340 .
Jason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai,
and Quoc V . Le. Finetuned language models are zero-shot
learners. In The Tenth International Conference on Learn-
ing Representations, ICLR 2022, Virtual Event, April
25-29, 2022 , 2022a. URL https://openreview.
net/forum?id=gEZrGCozdqR .
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.
Chain-of-thought prompting elicits reasoning in large
language models. Advances in Neural Information Pro-
cessing Systems , 35:24824–24837, 2022b.
Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr,
Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf,
and Hannaneh Hajishirzi. Fine-grained human feedback
gives better rewards for language model training. arXiv
preprint arXiv:2306.01693 , 2023.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao,
Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wiz-
ardlm: Empowering large language models to follow
complex instructions. arXiv preprint arXiv:2304.12244 ,
2023.
Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhi-
wei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan
Chen, Jianguo Zhang, Devansh Arpit, et al. Retroformer:
Retrospective large language agents with policy gradient
optimization. arXiv preprint arXiv:2308.02151 , 2023.
Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim,
Hyeonbin Hwang, and Minjoon Seo. Selfee: Iterative
self-revising llm empowered by self-feedback genera-
tion. Blog post, May 2023. URL https://kaistai.
github.io/SelFee/ .
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
and Yejin Choi. Hellaswag: Can a machine really finish
your sentence? arXiv preprint arXiv:1905.07830 , 2019.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuo-
han Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E.
Gonzalez, and Ion Stoica. Judging llm-as-a-judgewith mt-bench and chatbot arena. arXiv preprint
arXiv:2306.05685 , 2023a.
Rui Zheng, Shihan Dou, Songyang Gao, Wei Shen, Bing-
hai Wang, Yan Liu, Senjie Jin, Qin Liu, Limao Xiong,
Lu Chen, et al. Secrets of rlhf in large language models
part i: Ppo. arXiv preprint arXiv:2307.04964 , 2023b.
13

--- PAGE 14 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
A. Limitations
In constructing ULTRA FEEDBACK , we made an assumption that powerful LLMs like GPT-4 are capable of imitating human
annotators and fair evaluators. Although more and more works accepted this assumption and demonstrated high agreement
between human and LLM feedbacks (Dubois et al., 2023; Lee et al., 2023; Bai et al., 2022b), LLMs still cannot model
human preference precisely under all situations. How to efficiently and accurately collect preference data and conduct
rigorous evaluation are still challenging. We leave this as future work for further investigation. Another limitation is that
ULTRA FEEDBACK only provides single-turn dialogues to improve the utility of LLMs due to time and budget restrictions.
We will also expand U LTRA FEEDBACK to cover more tasks and scenarios.
B. Data Contamination
To avoid data contamination which could result in unfair even wrong evaluations, we did careful decontamination for
ULTRA FEEDBACK . Following GPT-3 (Brown et al., 2020) and evaluation-harness (Gao et al., 2021), we search for 13-gram
matches between AlpacaEval, Evol-Instruct, and UltraChat test set. We found in total 48 contamination samples and filtered
out them. However, we did not conduct a thorough examination of contamination over other evaluation datasets because of
the huge amount of datasets. Therefore, we suggest researchers decontaminate ULTRA FEEDBACK with their evaluation
datasets before using it.
C. UltraFeedback Statistics
We summarize the scores for each model over different aspects in Figure 5. Overall, the rankings are consistent with model
capabilities. For example, the GPT series is the best in all aspects, and larger models are generally better than smaller ones.
The distinction among different aspects also exists. For instance, the LLaMA2-Chat models received higher scores on
honesty, since they are aligned with human values with RLHF (Touvron et al., 2023b).
We also showcase how different principles stimulate diverse model behaviors. We average the score of each aspect when
applying different principles to models, and plot them in Figure 6.
D. Training Details
D.1. UltraRM
We construct each comparison pair as a binary selection, with one completion being chosen and the other rejected. We
optimize the reward model to select preferred completion by minimizing the binary ranking loss:
Lranking =−log (σ(rθ(x, yc)−rθ(x, yr)−m(r))) (1)
where θrepresents the reward model, rθ(x, yc)is its scalar reward prediction towards the chosen text, rθ(x, yr)is that
towards the rejected text, and m(r)is the absolute difference between the annotated reward of two texts. We set the
m(r) = 0 for datasets with only preference rankings and normalize the margins to (0,1]to avoid training instability due to
a mismatch among the score scales of the datasets. Following Touvron et al. (2023b), we train the 13B reward model for one
epoch with the batch size being 512 pairs (i.e., 1024 completions) and the learning rate being 1e-5. We adopt the cosine
learning rate decay strategy with a warm-up ratio of 3% and a final learning rate of 1e-6.
D.2. UltraCM
We train LLaMA2-13B for two epochs with a batch size of 256 and a learning rate of 2e-5. We adopt the same learning rate
scheduler as reward modeling.
E. Experiment Details
E.1. Dataset Details for UltraRM Training
We mix ULTRA FEEDBACK with other open-source preference datasets for reward modeling. Stanford SHP is a community-
based preference dataset collected from 18 different topics, adopting a strict filtering strategy to ensure text quality and
reliability of preferences. We follow the guidelines in the official repository to further filter the dataset, only retaining
14

--- PAGE 15 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
2345Preference Score4.69 4.67
4.314.22 4.184.09 4.06 4.04 4.013.913.783.683.55
3.28
3.022.95
2.62Honesty
2345Preference Score4.76 4.76
4.38 4.354.22 4.184.12 4.094.033.963.883.78
3.62
3.45 3.42
2.99
2.83Truthfulness
2345Preference Score4.02 4.0
3.84 3.8
3.64 3.63 3.62 3.583.49
3.33 3.283.23.07
2.9
2.542.4
2.24Helpfulness
2345Preference Score4.53 4.51
4.134.063.97 3.92 3.91
3.763.62 3.59 3.56
3.39
3.16
3.012.89
2.56
2.27Instruction Following
Models
GPT-4
GPT-3.5 Turbo
WizardLM-70B
BARD
Vicuna-33BMPT-30B-Chat
WizardLM-13B
LLaMA-2-70B-Chat
UltraLM-65BLLaMA-2-13B-Chat
UltraLM-13B
WizardLM-7B
LLaMA-2-7B-ChatStarChat
Alpaca-7B
Pythia-12B
Falcon-40B-Instruction
Figure 5. Average scores for each model over the four aspects.
15

--- PAGE 16 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
gpt-4
gpt-3.5-turbowizardlm-70bbard
llama-2-70b-chatvicuna-33b
wizardlm-13bmpt-30b-chat
llama-2-13b-chatultralm-65bultralm-13b
llama-2-7b-chatwizardlm-7bstarchat
falcon-40b-instructalpaca-7bpythia-12b2.53.03.54.04.55.0Honesty ScoreEffects of Different Principles on Honesty Scores
Helpfulness
Truthfulness
Honesty
Verbalized Calibration
gpt-4
gpt-3.5-turbobard
wizardlm-70bmpt-30b-chatvicuna-33b
wizardlm-13b
llama-2-70b-chatultralm-65b
llama-2-13b-chatultralm-13bwizardlm-7b
llama-2-7b-chatalpaca-7bstarchat
falcon-40b-instructpythia-12b3.03.54.04.55.0Truthfulness ScoreEffects of Different Principles on Truthfulness Scores
Helpfulness
Truthfulness
Honesty
Verbalized Calibration
gpt-4
gpt-3.5-turbowizardlm-70bbard
wizardlm-13bvicuna-33b
llama-2-70b-chatllama-2-13b-chatmpt-30b-chatultralm-13bultralm-65b
llama-2-7b-chatwizardlm-7bstarchatalpaca-7bpythia-12b
falcon-40b-instruct2.02.53.03.54.0Helpfulness ScoreEffects of Different Principles on Helpfulness Scores
Helpfulness
Truthfulness
Honesty
Verbalized Calibration
gpt-4
gpt-3.5-turbowizardlm-70bbard
vicuna-33b
wizardlm-13bmpt-30b-chat
llama-2-70b-chatultralm-65b
llama-2-13b-chatultralm-13bwizardlm-7b
llama-2-7b-chatstarchatalpaca-7bpythia-12b
falcon-40b-instruct2.02.53.03.54.04.5Instruction_following ScoreEffects of Different Principles on Instruction following Scores
Helpfulness
Truthfulness
Honesty
Verbalized Calibration
Figure 6. Different principles stimulate diverse model behaviors.
preferences with a score ratio greater than 2 and using at most 5 comparison pairs for each post via random sampling.
OpenAI Summarize consists of human-written completions and human-annotated preferences, with the instructions being
much longer than ULTRA FEEDBACK . Hence, we include the high-quality dataset to enhance the subsequent reward model
for long-text scenarios. We adopt the same comparison pair filtering method to avoid the reward model overfitting certain
instructions. Anthropic Helpful is another human-annotated dataset. We incorporate all its samples into our training dataset
to supplement multiturn dialogs data. For ULTRA FEEDBACK , we directly adopt the overall score obtained in critique
annotation as the preference score for UltraRM-Overall, while for fine-grained versions, we average the scores of all aspects
for each sample as the final preference score. Finally, the training dataset for our reward model contains a total of 749,702
comparison pairs, with 340,025from U LTRA FEEDBACK ,198,556from Stanford SHP, 92,858from OpenAI Summarize,
and118,263from Anthropic Helpful.
E.2. Additional Reward Modeling Experiments
We observed that the SteamSHP model is different from other reward models in the input format, for it accepts two responses
simultaneously and outputs which one is better (text-to-text format). During the experiment, we found that there is a position
bias issue for this approach, where the reward model tends to prefer the first responses. To eliminate this issue, we average
the scores from two runs exchanging response orders to get the final scores. We report the detailed results in Table 7.
E.3. Critique Modeling
Setup. To assess the ability of UltraCM to provide reliable critique, we employ GPT-4 to score the quality of critique
based on detailed documentation. we follow (Wang et al., 2023c) to randomly sample 50 instructions from PIQA (Bisk
et al., 2020), OpenBookQA (OBQA) (Mihaylov et al., 2018), CommonsenseQA (Talmor et al., 2018), AlpacaFarm (Dubois
et al., 2023), and FairEval (Wang et al., 2023b). We also supplement HumanEval (Chen et al., 2021), MBPP (Austin et al.,
16

--- PAGE 17 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
Table 7. Reward modeling results for SteamSHP with different sample orders.
DatasetAnthropic
HelpfulOpenAI
WebGPTOpenAI
Summ.Stanford
SHP
Chosen first 72.0 72.4 52.8 71.8
Rejected first 38.8 52.9 44.0 31.4
Avg. 55.4 62.6 48.4 51.6
Table 8. Feedback quality of each model on different datasets rated by GPT-4. The best performance on each dataset is marked in bold ,
and the second has been underlined .
Model PIQA OBQACommon-
senseQAAlpaca-
FarmFair-
EvalHuman-
EvalMBPP MATH GSM8K Avg.
gpt-3.5-turbo 6.08 6.12 6.04 6.44 6.32 6.14 6.48 5.98 5.94 6.17
LLaMA2-13B-Chat 5.92 5.04 5.66 5.26 5.74 4.64 4.82 3.88 4.30 5.03
Vicuna-13B-v1.5 5.66 5.58 5.42 5.58 5.82 4.86 5.20 4.56 4.84 5.28
WizardLM-13B-v1.2 5.90 5.52 5.82 5.66 5.88 5.28 5.34 4.30 4.90 5.40
Shepherd-13B 3.48 3.64 3.48 3.04 3.30 3.08 3.20 3.10 2.76 3.23
SelFee-13B 6.00 5.32 5.74 5.88 5.94 4.84 5.12 4.46 5.40 5.41
UltraCM-13B 6.00 6.12 6.02 5.98 6.18 5.74 5.56 5.84 5.88 5.92
2021), MATH (Hendrycks et al., 2021), and GSM8K (Cobbe et al., 2021) to evaluate critique quality on coding and math
tasks. We then generate model completions for the instructions in the same way as Section 2.2. We adopt two categories
of models for comparison. First, we compare with four general-purpose models, gpt-3.5-turbo , LLaMA2-13B-Chat,
Vicuna-13B-v1.5, and WizardLM-13B-v1.2. Then, we adopt two specifically trained critique models, SelFee4and Shepherd
(Wang et al., 2023c)5. We apply the baseline models and UltraCM to provide feedback on model completions respectively.
Finally, we rate the quality of the critique from 1 to 7 using GPT-4, 1 being the worst and 7 being the best. The prompt is
adapted from (Wang et al., 2023c).
Results. The scores of feedback quality are presented in Table 8. Overall, the performances of UltraCM almost approach
gpt-3.5-turbo and dramatically surpass other models of both categories. To be specific, UltraCM achieves comparable
performance with gpt-3.5-turbo on commonsense reasoning and mathematics reasoning. However, on AlpacaFarm
and code datasets, UltraCM still exhibits deficiencies. Compared with two critique models, we find that (the community-
trained) Shepherd almost always fails to provide high-quality feedback. SelFee achieves the highest average scores after
gpt-3.5-turbo and UltraCM, but it dramatically falls short on HumanEval and MATH. We highlight the comparison
between UltraCM and the other three general-purpose models. All four models are trained from LLaMA2-13B, but UltraCM
is the only one trained to provide textual critique rather than enhancing knowledge or reasoning capability. However,
the feedback of UltraCM consistently gains higher scores than other models across all tasks and datasets, indicating that
criticizing is a learnable task and employing an expert critic is more effective than an expert for downstream tasks in
providing feedback. With more powerful backbone models, we believe ULTRA FEEDBACK will greatly benefit autonomous
agents (Park et al., 2023; Qin et al., 2023; Qian et al., 2023) and feedback learning (Yao et al., 2023; Shinn et al., 2023)
research.
E.4. Capability Experiments
We use nine datasets in Section 5.2 to test the model capability. For world knowledge, we adopt NaturalQues-
tions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). For commonsense reasoning, we use PIQA (Bisk et al.,
2020), HellaSwag (Zellers et al., 2019), OpenBookQA (Mihaylov et al., 2018), and ARC (Clark et al., 2018). For reading
comprehension, we use BoolQ (Clark et al., 2019), RACE (Lai et al., 2017) and MultiRC (Khashabi et al., 2018).
For evaluation, we simply ask models to answer the questions directly with answers (e.g. with options A, B, C, D or Yes/No).
4https://huggingface.co/kaist-ai/selfee-13b
5Note that Wang et al. (2023c) did not open source their model weights, so we use the model from the community that has been trained
on their data: https://huggingface.co/reciprocate/shepherd-13b
17

--- PAGE 18 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
Table 9. Relative scores (%) versus gpt-3.5-turbo across different question types on UltraChat evaluation set.
ModelVicuna Commonsense World Knowledge Professional Knowledge AbilityWriting OverallSet Easy Moderate Easy Difficult Physics Biology Math Reasoning
UltraLM-13B 95.6 113.7 106.8 111.7 103.3 102.1 105.1 89.7 71.0 98.6 98.8
Vicuna-13B-v1.3 93.2 113.4 106.4 109.6 107.1 106.0 108.9 84.7 79.0 98.4 98.8
Vicuna-13B-v1.5 95.7 115.8 106.6 104.9 105.0 100.1 101.2 94.8 73.2 99.1 99.0
LLaMA2-13B-Chat 97.1 114.6 108.5 109.3 107.7 105.9 108.0 91.3 75.0 98.6 100.2
Vicuna-33B-v1.3 98.5 113.4 114.0 105.1 109.0 109.9 112.8 84.4 86.7 103.0 102.4
WizardLM13B-v1.1 100.7 113.9 112.1 106.9 113.0 108.1 110.7 89.9 76.8 102.6 102.6
LLaMA2-70B-Chat 100.5 116.5 106.7 111.5 109.0 106.6 109.4 99.0 77.6 103.6 103.2
OpenChat-13B-v3.2super 98.6 121.2 112.6 116.1 110.1 106.0 110.0 89.3 82.9 104.7 103.9
WizardLM13B-v1.2 102.5 122.0 110.3 114.3 111.7 108.6 109.0 96.3 79.7 103.8 104.9
UltraLM-13B-PPO 97.7 123.5 113.6 131.1 118.4 113.2 120.2 93.0 78.8 101.7 105.7
We then match the output with the ground truth and calculate the exact match scores.
E.5. Question Type Breakdown
Table 9 reports the type-specific performance of our model and baselines compared with gpt-3.5-turbo . As is shown,
our UltraLM-13B-PPO gets the highest average score, especially excels on the commonsense, world knowledge as well
as professional knowledge questions. In the meantime, our model does not show advantages in math and reasoning tasks,
which is consistent with the above results.
F. Case Study
F.1. UltraFeedback
We present a case sampled from ShareGPT in Table 10. The four assistants sampled to complete this instruction are
Falcon-40B-Instruct, GPT-4, LLaMA2-70B-Chat, and UltraLM-13B. We also present GPT-4 annotations on the aspect of
instruction following. GPT-4 accurately points out the lack of details in Falcon-40B-Instruct and GPT-4 responses.
F.2. Best-of- nSampling
We present a case comparing best-of- 1, best-of- 2, and best-of- 16sampling in Table 11. The best-of- 1sample is incorrect at
all, so it received a low reward of −0.73. To compare, the best-of- 2sample is partially correct, as it only addressed the
@ symbol in decorating staticmethod . The example it provided is also problematic. The best-of- 16sample, however,
correctly illustrated the usage of the @ symbol in Python with a valid example. Thus it received a much higher reward
from the reward model. Another worth-noting point is that the best-of- 2sample is longer than the best-of- 16sample, which
verified that our UltraRM does not depend on length shortcut to evaluate the response quality.
F.3. UltraLM-13B-PPO
We compare UltraLM-13B before and after PPO from an AlpacaEval case in Table 12. As we can see, the original
UltraLM-13B made mistakes in describing Lady Gaga’s characters in American Horror Story. The UltraLM-13B-PPO, on
the other hand, correctly answered the question.
F.4. UltraCM-13B
To comprehensively evaluate and analyze the critiques our model generates, we conducted case study in Table 13. On
GSM8K benchmark, we find that UltraCM-13B could give meaningful critiques on incorrect answers, pointing out flaws
and provide suggestions. However, the suggestions are sometimes unnecessarily long, which indicates the imperfection of
the model. We will continue to improve the critique quality in future work.
18

--- PAGE 19 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
Table 10. An example of U LTRA FEEDBACK comparing generations from 4 models from the aspect of instruction-following.
User: what’s the general consensus best time to take vitamin D supplements.
ResponseAnnotation
(Instruction-Following)
Falcon-40B-Instruct : A good rule of thumb is to take vitamin D supplements in the
morning or afternoon so that they can be properly absorbed by the body. However, it is best to
consult with a healthcare professional to determine a specific dosing regimen for your individual
needs.Rating : 3
Rationale : The text partially complies
with the instruction, providing a general
rule of thumb for taking vitamin D sup-
plements. However, it lacks detail and
does not address the various factors that
may influence the best time to take the
supplements.
GPT-4 : The general consensus on the best time to take vitamin D supplements varies, as
individual needs and absorption rates can differ. However, many experts recommend taking
vitamin D supplements with a meal containing healthy fats for better absorption.
Some studies suggest that taking vitamin D supplements in the morning may be preferable to
nighttime due to its potential effects on sleep. Keep in mind that it is crucial to follow the dosage
recommended by a healthcare professional, as individual needs may vary. As always, it’s best to
consult with a healthcare provider for personalized advice on supplement timing and dosage.Rating : 4
Rationale : The text almost fully aligns
with the instruction, providing a general
consensus on the best time to take vi-
tamin D supplements and mentioning
individual factors. However, it could
provide more detail on the various fac-
tors that may influence the best time to
take the supplements.
LLaMA2-70B-Chat : Hello! I’m happy to help you with your question.
The general consensus among health professionals is that the best time to take vitamin D
supplements depends on various factors, including your personal schedule, sun exposure, and
health status. Here are some general guidelines to consider:
1. Morning: Taking vitamin D supplements in the morning can help you start your day with
adequate levels of this essential nutrient. This may be particularly helpful if you have a busy
schedule and may not have time to take them later in the day.
2. After sun exposure: Vitamin D is produced in the skin when it’s exposed to sunlight. Taking a
vitamin D supplement after spending time outdoors can help boost your levels and ensure you’re
getting enough.
3. With a meal: Vitamin D is a fat-soluble vitamin, which means it’s best absorbed when taken
with a meal that contains healthy fats. Having a meal with a source of fat, such as avocado, nuts,
or seeds, can enhance the absorption of vitamin D.
4. Consistency: The most important thing is to establish a consistent routine and take your
vitamin D supplements at the same time every day. This will help maintain a stable level of
vitamin D in your body.
It’s important to note that excessive intake of vitamin D can be harmful, so it’s essential to
consult with a healthcare professional to determine the appropriate dosage for your individual
needs. They can help you identify the best time to take vitamin D supplements based on your
specific circumstances.
Remember, it’s always a good idea to consult with a healthcare professional before starting
any new supplement regimen. They can assess your individual needs and provide personalized
recommendations for your health and well-being.Rating : 5
Rationale : The text comprehensively
addresses the task goal by providing a
general consensus on the best time to
take vitamin D supplements, consider-
ing various factors. It also acknowl-
edges the importance of consulting with
a healthcare professional for personal-
ized advice.
UltraLM-13B : According to current research and guidelines, the general consensus is that the
best time to take vitamin D supplements is either in the morning or early afternoon, as our bodies
produce more vitamin D when exposed to sunlight during this time. However, it’s important
to note that the optimal time to take vitamin D supplements can vary depending on individual
factors such as sun exposure, geographical location, and overall health.
For people living in areas with limited sunlight or those who spend most of their time indoors,
taking vitamin D supplements with a meal can be an effective way to ensure adequate intake.
Some research also suggests that taking larger doses of vitamin D, such as 2,000-5,000 IU
(international units), as a single dose a week or every few weeks, may be more effective for
increasing vitamin D levels and maintaining them over time.
Keep in mind that the optimal time to take vitamin D supplements may also be influenced by
medications or supplements you are currently taking, so it’s best to consult with a healthcare
professional for personalized advice based on your individual needs and circumstances.Rating : 5
Rationale : The text fully aligns with
the instruction, providing a general con-
sensus on the best time to take vitamin
D supplements and considering individ-
ual factors. It also emphasizes the im-
portance of consulting with a healthcare
professional for personalized advice.
19

--- PAGE 20 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
Table 11. Comparative cases of best-of- 1, best-of- 2, and best-of- 16sampling.
User: what does the @ in python do
Best-of- 1: In Python, the @ symbol is used as a way to reference a class within its own namespace. This is often referred to as a class
object. When you create an instance of a class, you are creating a new object that is stored in the namespace of the class. Using the @
symbol before a class declaration allows you to bypass. Reward: −0.73
Best-of- 2: The @ symbol in Python is used in front of a class to indicate that the class is decorated with the @staticmethod decorator.
This decorator is sandwiched between the class definition and the name of the class. A static method in Python is a method that can be
called directly on a class, like a regular method, but does not have access to the instance data (self). It usually takes no parameters and
does not modify the object’s state directly. Instead, it operates on the class itself. As a result, if you define a method as staticmethod ,
it will become a static method and you can call it on an instance of the class without having to create an instance of the class first. For
example, a Rectangle class could define the perimeter method as a static method:
@staticmethod
def perimeter(width, height):
return 2*width +2 *height
You can then call this perimeter method on a Rectangle instance as follows:
rect =Rectangle(10, 5)
print (rect.perimeter()) # Output: 40
In this example, the Rectangle class defines a perimeter method, which is static by nature. The @staticmethod decorator
simply serves to inform the Python interpreter that the method you are defining is indeed intended to be a static method. Moreover, you
could also use this decorator with classmethod orlamdba function syntax. The ‘@staticmethod‘ decorator is by default aligned with
classmethod , but the other formats are used for cases where you need more control over the function’s self context, or more specific
function memory. In summary, the @ symbol is a Python decorator that converts a method into a static method, which is able to be called
on an instance of the class without requiring the creation of an actual instance. This functionality makes your code cleaner, more readable,
and easier to maintain. Please note that Python has several other symbols and decorators that can modify the behavior of your code or
enrich the functionality of your objects, such as @staticmethod ,@classmethod ,@property , and @newmethod . However,
these will generally require some level of understanding and configuration before they can be effectively adapted for your applications.
Remember that these decorators should not be confused with the general syntax to create a static method. If you don’t see the @ symbol
in front of a class method, then it is by default considered as a method. Adding static to the method implies decorating the method with
thestaticmethod decorator. To keep the code interesting, decorators are an elegant and expressive way to integrate various functional
programming concepts into Python, improving encapsulation and abstraction across scripts and classes. Reward: −0.10
Best-of- 16: In Python, the @ symbol is used to define a decorator, which is a way to add functionality to or modify the behavior of
functions or objects (not class methods). A decorator is essentially a decoration around the original function, executing some arbitrary
code or modifying the function’s properties before the function is executed. When a function is correctly decorated using the @ symbol, a
decorator function is applied to it. The decorator function can then modify the behavior of the original function, by, for example, adding
arguments, modifying attributes of the function, or executing additional code before or after the function is executed. Here’s an example
of how to use a simple decorator to add some additional functionality to a function:
from functools import wraps
def login_required(func):
@wraps(func)
def wrapper(username, *args, **kwargs):
ifusername !="secret":
raise Exception ("Please enter the correct username and password.")
return func(username, *args, **kwargs)
return wrapper
@login_required
def secret_page(username):
print (f"Hello, {username }! You are now on the secret page.")
In this example, we are defining a decorator login required to handle the authentication of our user. It will raise an exception if the
provided ‘username‘ argument is incorrect. And then we use this decorator to wrapper the interesting function secret page with this
restriction. This approach allows you to modify the behavior of a function at runtime, making it a very flexible and powerful tool for
Python development. Reward: 0.42
20

--- PAGE 21 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
Table 12. Comparative cases of UltraLM-13B and UltraLM-13B-PPO.
User: who does lady gaga play in american horror story
UltraLM-13B : The question is asking for the character that Lady Gaga has played in the television series American Horror Story. Lady
Gaga has appeared in multiple seasons of the show and has played different characters in each one. The following are the characters Lady
Gaga has played in the series:
1. Season 1: American Horror Story (2011) - Gaga played the role of The Countess or Elizabeth Johnson, a wealthy and sophisticated
owner of a Los Angeles apartment building who resides with her ventriloquist dummy.
2. Season 6: American Horror Story: Hotel (2016) - In this season, Lady Gaga portrayed German singer-songwriter Elizabeth along with
her twin brother Dennis. They have gained fame in the 1970s with their horror rock collaboration, The Countess.
Gaga’s portrayal in American Horror Story has added a unique dimension to the series, showcasing her acting talent, and has become a
fan-favorite character.
UltraLM-13B-PPO : Lady Gaga played the role of The Countess in American Horror Story: Hotel. The Countess is a mysterious and
enigmatic character who runs the Hotel Cortez, a luxurious and decadent hotel that serves as a front for a series of gruesome murders.
The Countess is a seductive and charismatic figure who preys on her victims, using her beauty and charm to lure them into her web.
Throughout the season, viewers are taken on a twisted and terrifying journey as they uncover the dark secrets of the Hotel Cortez and the
sinister forces that lurk within its walls. Lady Gaga’s performance in American Horror Story: Hotel was widely praised for its depth,
complexity, and sheer intensity, cementing her status as one of the most talented and versatile actresses of her generation.
Table 13. Case of UltraCM-13B on GSM8K.
User: A hotel in the Philippines charges 1000 pesos for a 12-hour stay or 1600 pesos for a 24-hour stay. After 12 hours, visitors have the
option to add 70 pesos for every additional hour. Cameron arrives at 5 pm at the hotel and wants to leave at 10 am the next morning. He
decides to go with the option of adding on 70 pesos for every hour after the 12-hour mark instead of paying for 24 hours. How much
money would Cameron save? Answer: 250
Vicuna-33B : First, let’s determine the cost of Cameron’s stay if he were to stay for 12 hours. Since he arrives at 5 pm, his stay would
actually be 13 hours (from 5 pm to 6 am). The cost for a 12-hour stay is 1000 pesos.
Next, let’s calculate the cost of a 24-hour stay. The cost for a 24-hour stay is 1600 pesos.
Now, we need to calculate the cost of Cameron’s stay if he were to stay for 13 hours and pay the additional 70 pesos for every hour after
the 12-hour mark. For the first 12 hours, the cost is 1000 pesos. For the additional 1-hour period (from 12 hours to 13 hours), the cost is
70 pesos. Therefore, the total cost for Cameron’s 13-hour stay would be 1000 pesos + 70 pesos = 1070 pesos.
Now, we can calculate the amount Cameron would save by choosing the 13-hour option over the 24-hour option. The savings would be
the difference between the two costs:
Savings = 1600 pesos (24-hour cost) - 1070 pesos (13-hour cost)
avings = 530 pesos
So, Cameron would save 530 pesos by choosing the option of adding 70 pesos for every additional hour after the 12-hour mark instead of
paying for 24 hours.
UltraCM-13B Critique : Your calculation of the hotel stay cost is incorrect. Cameron’s stay from 5 pm to 10 am is 17 hours, not 13 hours.
You should calculate the cost for a 17-hour stay instead. Also, you didn’t need to calculate the savings by staying for 13 hours. Just focus
on the correct calculation for the 17-hour stay. The rest of your answer is detailed and understandable, but the numbers are wrong due to
the incorrect calculation of the hotel stay hours. Please revise this.
For improvement, always double-check your calculations to avoid such mistakes. Make sure to understand the problem clearly before you
start solving it. Focus on what the question is asking you to find and ignore unnecessary details that might confuse you. As an AI, accuracy
is key. A small mistake can lead to a wrong conclusion and misinformation, which is not our goal as AI assistants. Keep practicing and
focus on your accuracy, try to avoid rush decisions and take your time to understand the questions fully before answering them.
Overall Score: 6
21

--- PAGE 22 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
G. Prompt
G.1. Principle
In model completion, we sample one system prompt to align model behavior from different aspects. The human-written
examples for each aspect are as follows:
Human-written Principle Prompts
**Honesty:**
The assistant should be honest about whether it knows the answer and express its uncertainty explicitly. Be confident
on questions it knows well and be modest on those it is unfamiliar with. Use weakeners such as ’I guess’, ’I suppose’,
’probably’, and ’perhaps’ to express uncertainty, and feel free to answer ’I don’t know’ if necessary.
—
**Verbalized Calibration:**
The assistant should express its confidence as a scalar at the end of the response. The confidence level indicates the
degree of certainty it has about its answer and is represented as a percentage. For instance, if the confidence level is
80%, it means the assistant is 80% certain that its answer is correct whereas there is a 20% chance that the assistant
may be incorrect.
The format is as follows: [Question] [Answer] Confidence: [The assistant’s confidence level, numerical numbers
only, e.g. 80%] Here, tags like [Question] and [Answer] are placeholders and should be omitted in the response.
—
**Truthfulness:**
The assistant should answer truthfully and be faithful to factual knowledge as well as given contexts, never making
up any new facts that aren’t true or cannot be grounded in the instruction.
—
**Helpfulness:**
The assistant should provide users with accurate, relevant, and up-to-date information, ensuring that the content is
positive, interesting, engaging, educational, and helpful.
G.2. Annotation
We first showcase the template that prompts GPT-4 to annotate the quality of four given completions from the aspect of
instruction following. Then, we present the template to annotate critique feedback.
Annotation Template for Instruction Following
**Instruction Following Assessment**
Evaluate alignment between output and intent. Assess understanding of task goals and restrictions.
**Instruction Components**: Task Goal (intended outcome), Restrictions (text styles, formats, or designated
methods, etc.).
**Scoring**: Rate outputs 1 to 5:
1. **Irrelevant**: No alignment.
2. **Partial Focus**: Addresses one aspect poorly.
3. **Partial Compliance**:
- (1) Meets goals or restrictions, neglecting others.
- (2) Acknowledges both but slight deviations.
4. **Almost There**: Near alignment, minor deviations.
5. **Comprehensive Compliance**: Fully aligns, meets all requirements.
22

--- PAGE 23 ---
ULTRA FEEDBACK : Boosting Language Models with Scaled AI Feedback
Annotation Template for Critique Feedback
Given my answer to an instruction, your role is to provide specific and constructive feedback for me. You should
find the best way for me to learn from your feedback and improve my performance.
You should consider multiple aspects of my answer, including helpfulness, truthfulness, honesty, and to what extent
the answer follows instructions.
—
### Instruction
{instruction }
### Answer
completion
—
Please act as a teacher and provide specific and constructive feedback. Besides describing the weaknesses of the
answer, you should also provide specific suggestions to guide me toward understanding how to improve. Please
note, however, that your suggestions should help me better complete the instructions, but you should not introduce
new requirements that are not mentioned in the instructions. Your feedback should focus on enhancing my ability to
think critically and respond accurately. However, never explicitly provide the reference answer, nor do polite phrases
be required. Only respond with concise feedback in chat style. Finally, score the overall quality of the answer from 1
to 10, where 1 is the worst and 10 is the best.
Format
### Feedback
[Your feedback]
Overall Score: [1-10]
—
### Feedback
23
