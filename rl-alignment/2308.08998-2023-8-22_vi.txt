# 2308.08998.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2308.08998.pdf
# Kích thước tệp: 1404121 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
2023-8-22
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ
Caglar Gulcehre*†,1, Tom Le Paine*†,1, Srivatsan Srinivasan*†,1, Ksenia Konyushkova†,1, Lotte Weerts†,1
Abhishek Sharma†,1, Aditya Siddhant†,1, Alex Ahern1, Miaosen Wang1, Chenjie Gu1,
Wolfgang Macherey2, Arnaud Doucet1, Orhan Firat†,1, Nando de Freitas1
*Đóng góp bằng nhau,†Cộng tác viên cốt lõi
1Google DeepMind,2Google Research
Học tăng cường từ phản hồi của con người (RLHF) có thể cải thiện chất lượng đầu ra của mô hình ngôn ngữ lớn
(LLM) bằng cách căn chỉnh chúng với sở thích của con người. Chúng tôi đề xuất một thuật toán đơn giản để căn chỉnh
LLM với sở thích của con người được lấy cảm hứng từ học tăng cường theo lô tăng trưởng (RL), mà chúng tôi gọi là
Tự Huấn Luyện Tăng Cường (ReST). Cho một chính sách LLM ban đầu, ReST tạo ra một tập dữ liệu bằng cách sinh
mẫu từ chính sách, sau đó được sử dụng để cải thiện chính sách LLM sử dụng các thuật toán RL ngoại tuyến.
ReST hiệu quả hơn các phương pháp RLHF trực tuyến điển hình vì tập dữ liệu huấn luyện được tạo ra ngoại tuyến,
cho phép tái sử dụng dữ liệu. Mặc dù ReST là một phương pháp tổng quát áp dụng cho tất cả các thiết lập học sinh,
chúng tôi tập trung vào ứng dụng của nó cho dịch máy. Kết quả của chúng tôi cho thấy ReST có thể cải thiện đáng kể
chất lượng dịch thuật, được đo bằng các chỉ số tự động và đánh giá của con người trên các chuẩn mực dịch máy một cách
hiệu quả về tính toán và mẫu.
Từ khóa: RL ngoại tuyến, học tăng cường, RL từ phản hồi của con người, ngôn ngữ, xử lý ngôn ngữ tự nhiên, dịch máy

1. Giới thiệu
Hình 1|Phương pháp ReST. Trong bước Grow,
một chính sách tạo ra một tập dữ liệu. Ở bước
Improve, tập dữ liệu được lọc được sử dụng
để tinh chỉnh chính sách. Cả hai bước đều
được lặp lại, bước Improve được lặp lại
thường xuyên hơn để khấu hao chi phí
tạo tập dữ liệu.

Các mô hình ngôn ngữ lớn (LLM) đã thể hiện khả năng ấn tượng trong việc tạo ra văn bản chất lượng cao và giải quyết
nhiều tác vụ ngôn ngữ (Brown et al., 2020; Bubeck
et al., 2023; Rae et al., 2021). Những mô hình này được huấn luyện để
tối đa hóa khả năng xảy ra của token tiếp theo một cách tự hồi quy
sử dụng lượng lớn văn bản và tính toán (Hoffmann
et al., 2022; Srivastava et al., 2022). Tuy nhiên, Perez et al.
(2022) đã chỉ ra rằng việc tạo ra văn bản với khả năng xảy ra cao
không nhất thiết phải phù hợp tốt với sở thích của con người
trên các tác vụ khác nhau. Không có sự căn chỉnh thích hợp, các mô hình ngôn ngữ
cũng có thể đưa ra nội dung không an toàn với hậu quả
có hại. Hơn nữa, việc căn chỉnh LLM giúp cải thiện
các tác vụ downstream khác (Ouyang et al., 2022b). Học
tăng cường từ phản hồi của con người (RLHF) nhằm
giải quyết vấn đề căn chỉnh bằng cách sử dụng sở thích
của con người (Glaese et al., 2022; Stiennon et al., 2020; Wu
et al., 2021). Thông thường, phản hồi của con người được sử dụng để học một
mô hình phần thưởng, sau đó được sử dụng để tinh chỉnh LLM với
một mục tiêu học tăng cường (RL).

Tác giả liên hệ: ca9lar@gmail.com
©2023 DeepMind. Tất cả quyền được bảo lưu arXiv:2308.08998v2 [cs.CL] 21 Aug 2023

--- TRANG 2 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

Các phương pháp RLHF thường dựa vào các phương pháp RL trực tuyến như PPO (Schulman et al., 2017) và A2C
(Mnih et al., 2016). Huấn luyện trực tuyến yêu cầu lấy mẫu từ chính sách được cập nhật và chấm điểm các
mẫu với mô hình phần thưởng nhiều lần trong quá trình huấn luyện. Chi phí tính toán để xử lý
luồng liên tục các mẫu mới trở thành một hạn chế của các phương pháp trực tuyến, đặc biệt khi kích thước
của mạng chính sách và phần thưởng tăng lên. Hơn nữa, những phương pháp này dễ bị "hack" phần thưởng
(Skalse et al., 2022), và các nghiên cứu trước (Glaese et al., 2022) đã khám phá regularization mô hình để giảm thiểu
vấn đề này. Thay vào đó, các phương pháp RL ngoại tuyến học từ một tập dữ liệu cố định các ví dụ và do đó chúng
hiệu quả hơn về mặt tính toán và ít dễ bị hack phần thưởng hơn. Tuy nhiên, chất lượng của chính sách
được học ngoại tuyến chắc chắn phụ thuộc vào các thuộc tính của tập dữ liệu ngoại tuyến (Fu et al., 2020; Gulcehre
et al., 2021). Kết quả là, các tập dữ liệu được tuyển chọn cẩn thận trở nên rất quan trọng cho sự thành công của RL ngoại tuyến.
Nếu không, việc cải thiện hiệu suất so với học có giám sát có thể bị hạn chế (Kumar et al., 2021).
Đồng thời với công trình của chúng tôi, (Rafailov et al., 2023) đã đề xuất một phương pháp gọi là DPO (Tối Ưu Hóa Sở Thích Trực Tiếp)
có thể sử dụng dữ liệu ngoại tuyến để căn chỉnh một LM với sở thích của con người.

Chúng tôi đóng khung vấn đề căn chỉnh của một mô hình ngôn ngữ như một vấn đề RL lô tăng trưởng (Lange
et al., 2012). Cụ thể, phương pháp Tự Huấn Luyện Tăng Cường (ReST) của chúng tôi bao gồm hai vòng lặp: trong vòng lặp bên trong
(Improve), chúng tôi cải thiện chính sách trên một tập dữ liệu cố định và trong vòng lặp bên ngoài (Grow), chúng tôi tăng trưởng
tập dữ liệu bằng cách lấy mẫu từ chính sách mới nhất (xem Hình 1). Trong công trình này chúng tôi xem xét mô hình ngôn ngữ
có điều kiện, sau đó các bước của ReST như sau:

1.Grow(G): Chính sách mô hình ngôn ngữ (ban đầu, một chính sách có giám sát) được sử dụng để tạo ra nhiều
dự đoán đầu ra cho mỗi ngữ cảnh để bổ sung tập dữ liệu huấn luyện.

2.Improve (I): Chúng tôi xếp hạng và lọc tập dữ liệu bổ sung với một hàm chấm điểm. Chúng tôi sử dụng một
mô hình phần thưởng đã học được huấn luyện trên sở thích của con người làm hàm chấm điểm trong các thí nghiệm của chúng tôi.
Sau đó, mô hình ngôn ngữ được tinh chỉnh trên tập dữ liệu được lọc với một mục tiêu RL ngoại tuyến. Bước này
có thể được lặp lại với ngưỡng lọc tăng dần. Chính sách cuối cùng sau đó được sử dụng trong
bước Grow tiếp theo.

ReST là một phương pháp tổng quát cho phép các hàm mất mát RL ngoại tuyến khác nhau được sử dụng trong vòng lặp bên trong
khi thực hiện các bước Improve. Để áp dụng nó vào thực tế, người ta chỉ cần khả năng: i) lấy mẫu
từ một mô hình một cách hiệu quả, ii) chấm điểm các mẫu của mô hình. ReST cung cấp một số lợi thế so với các phương pháp RLHF
điển hình với RL trực tuyến hoặc ngoại tuyến:
•Gánh nặng tính toán được giảm đáng kể so với RL trực tuyến nhờ vào
đầu ra của bước Grow được khai thác qua nhiều bước Improve.
•Chất lượng của chính sách không bị hạn chế bởi chất lượng của tập dữ liệu gốc (như trong RL ngoại tuyến)
vì dữ liệu huấn luyện mới được lấy mẫu từ một chính sách được cải thiện trong bước Grow.
•Dễ dàng kiểm tra chất lượng dữ liệu và có thể chẩn đoán các vấn đề căn chỉnh, ví dụ, hack phần thưởng,
vì các bước Grow và Improve được tách biệt.
•Phương pháp này đơn giản, ổn định và chỉ có một số lượng nhỏ siêu tham số để điều chỉnh.

Chúng tôi giải thích chi tiết về phương pháp ReST được đề xuất trong Phần 3. Sau đó, chúng tôi trình bày
kết quả thí nghiệm của chúng tôi trên các chuẩn mực dịch máy trong Phần 4. Dịch máy là một
bài toán học sequence-to-sequence (Sutskever et al., 2014), thường được công thức hóa như
mô hình ngôn ngữ có điều kiện nơi ngữ cảnh để điều kiện là một câu bằng ngôn ngữ nước ngoài
(nguồn). Chúng tôi chọn dịch máy vì i) đây là một ứng dụng có tác động với các baseline mạnh
và một quy trình đánh giá được xác định rõ ràng, ii) một số phương pháp chấm điểm và đánh giá đáng tin cậy hiện có
sẵn để sử dụng làm mô hình phần thưởng (Freitag et al., 2022). Trong các thí nghiệm của chúng tôi, chúng tôi so sánh
một số thuật toán RL ngoại tuyến trên các chuẩn mực IWSLT 2014 (Cettolo et al., 2014) và WMT 2020
(Koehn et al., 2020) cũng như các chuẩn mực nội bộ cạnh tranh hơn, có độ tin cậy cao trên Web Domain.
Trong các thí nghiệm của chúng tôi ReST cải thiện đáng kể điểm số mô hình phần thưởng trên các tập kiểm tra và xác thực.
Hơn nữa, theo người đánh giá con người, ReST tạo ra các bản dịch chất lượng cao hơn so với
baseline học có giám sát.

--- TRANG 3 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

2. Kiến thức cơ bản

Một mô hình ngôn ngữ có điều kiện tạo ra một chuỗi đầu ra 𝒚=(𝑦1,𝑦2,....𝑦𝑇) cho một ngữ cảnh (hoặc
đầu vào nguồn) 𝒙=(𝑥1,𝑥2,...𝑥𝐿), trong đó các token 𝑥𝑙,𝑦𝑡 thuộc về một từ vựng được chọn. Một chính sách
tạo ngôn ngữ 𝜋 trong một mô hình tự hồi quy được đặc trưng bởi một phân phối xác suất có điều kiện
được tham số hóa bởi 𝜃 như

𝜋𝜃(𝒚|𝒙)=𝑇Ö
𝑡=1𝜋𝜃(𝑦𝑡|𝒚1:𝑡−1,𝒙),

với quy ước 𝒚1:0=∅ và 𝒚1:𝑡−1=(𝑦1,𝑦2,....𝑦𝑡−1).

Gọi 𝑝(𝒙,𝒚)=𝑝(𝒙)𝑝(𝒚|𝒙) biểu thị phân phối dữ liệu. Một tập dữ liệu D cho trước bao gồm các mẫu từ
phân phối này:
D={(𝒙𝑖,𝒚𝑖)|𝑁
𝑖=1 sao cho 𝒙𝑖∼𝑝(𝒙),𝒚𝑖∼𝑝(𝒚|𝒙=𝒙𝑖)}.

Cho tập dữ liệu này, chính sách có giám sát được huấn luyện bằng cách tối thiểu hóa hàm mất mát log likelihood âm (NLL):

LNLL(𝜃)=−𝔼(𝒙,𝒚)∼D"𝑇∑︁
𝑡=1log𝜋𝜃(𝑦𝑡|𝒚1:𝑡−1,𝒙)#. (1)

Chúng tôi gọi mô hình được huấn luyện với hàm mất mát NLL là behavioral cloning (BC) (Pomerleau, 1989)
theo cách gọi tên trong tài liệu RL.

3. Tự Huấn Luyện Tăng Cường (ReST)

Chúng tôi trình bày ReST, một thuật toán RLHF căn chỉnh đầu ra của mô hình ngôn ngữ với sở thích của con người.
Sở thích của con người về các chuỗi được mô hình hóa sử dụng một hàm phần thưởng đã học (xem Phụ lục
A.4). Trong quá trình Markov decision process cơ bản cho mô hình ngôn ngữ có điều kiện, các trạng thái là
các chuỗi một phần, và các hành động là các token được tạo (xem Phụ lục A.1).

Thuật toán ReST tách biệt việc tăng trưởng tập dữ liệu và cải thiện chính sách của một pipeline RL điển hình
thành các giai đoạn ngoại tuyến riêng biệt (Hình 1 và 2). Chúng tôi bắt đầu bằng việc huấn luyện một mô hình ban đầu 𝜋𝜃(𝒚|𝒙) để ánh xạ
các chuỗi đầu vào 𝒙 đến các chuỗi đầu ra 𝒚 trên một tập dữ liệu cho trước của các cặp chuỗi D sử dụng hàm mất mát NLL
từ Phương trình (1). Tiếp theo, bước Grow tạo ra một tập dữ liệu mới D𝑔, bổ sung tập dữ liệu huấn luyện ban đầu
với các mẫu từ mô hình:

D𝑔={(𝒙𝑖,𝒚𝑖)|𝑁𝑔
𝑖=1 sao cho 𝒙𝑖∼D,𝒚𝑖∼𝜋𝜃(𝒚|𝒙𝑖)}∪D.

Ở đây, các đầu vào điều kiện được lấy mẫu lại từ tập dữ liệu gốc 𝒙𝑖∼D, như trong self-training,
nhưng trong các tình huống khi người ta có quyền truy cập vào 𝑝(𝒙) họ có thể lấy mẫu trực tiếp từ nó, tức là 𝒙𝑖∼𝑝(𝒙).
Ví dụ, xem xét một mô hình tạo ra hình ảnh từ mô tả văn bản, trong trường hợp này,
phân phối của các đầu vào văn bản có thể được lấy mẫu từ một mô hình ngôn ngữ 𝑝(𝒙).

Sau đó, các bước Improve sử dụng D𝑔 để tinh chỉnh chính sách 𝜋𝜃. Lưu ý rằng chúng tôi giữ lại tập dữ liệu gốc
trong hỗn hợp huấn luyện để đảm bảo rằng các chính sách không phân kỳ. Dưới đây, chúng tôi mô tả các bước Grow
và Improve chi tiết hơn.

--- TRANG 4 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

Grow
Hình 2|Thuật toán ReST. Trên:
Ở các bước Improve I=1,I=2,I=3, tập dữ liệu
từ chính sách ban đầu được lọc
với các ngưỡng 𝜏1< 𝜏2< 𝜏3 và một chuỗi
các chính sách 𝜋𝜃1,𝜋𝜃2,𝜋𝜃3 được tinh chỉnh.
Dưới: Nếu chúng ta lấy mẫu
từ những chính sách đó (màu xám), chất lượng
của các mẫu sẽ tăng lên. Trong thực tế,
chỉ có chính sách cuối cùng 𝜋𝜃3 được sử dụng để tạo
tập dữ liệu D𝑔 tiếp theo.

Bước Grow tương ứng với bước hành động hoặc tạo dữ liệu
trong RL. Chúng tôi tạo một tập dữ liệu quỹ đạo bổ sung
D𝑔 bằng cách lấy mẫu nhiều chuỗi đầu ra từ chính sách hiện tại
𝜋𝜃, tức là 𝒚∼𝜋𝜃(𝒚|𝒙) cho 𝒙∼D. Tập dữ liệu mới
của các chuỗi sau đó được chấm điểm với một hàm phần thưởng 𝑅(𝒙,𝒚).
Các điểm dữ liệu với phần thưởng trên một điểm ngưỡng được
sử dụng để cập nhật chính sách (xem tiếp theo). Một khi chính sách được
cải thiện, một tập dữ liệu mới với các mẫu chất lượng tốt hơn có thể
được tạo ra một lần nữa (Hình 2, dưới).

Improve
Ở bước Improve (khai thác hoặc cải thiện chính sách
trong thuật ngữ RL), mục tiêu là sử dụng tập dữ liệu mới D𝑔
để tinh chỉnh chính sách 𝜋𝜃. Chúng tôi bắt đầu bằng việc định nghĩa một hàm lọc
chỉ bao gồm các mẫu với phần thưởng cao hơn
một ngưỡng nhất định 𝜏:

𝐹(𝒙,𝒚;𝜏)= 1𝑅(𝒙,𝒚)>𝜏.

Hãy để chúng tôi lưu ý rằng hàm lọc dựa trên ngưỡng có thể dẫn
đến học các hành vi không tối ưu ưa chuộng các kết quả
với phương sai cao trong các môi trường với động lực
ngẫu nhiên (Brandfonbrener et al., 2022). Tuy nhiên, trong công trình này
chúng tôi công thức hóa các tác vụ mô hình ngôn ngữ và dịch thuật
như các vấn đề RL xác định (Phụ lục A.1.)

Tiếp theo, chúng tôi tinh chỉnh chính sách tốt nhất hiện tại thường
được huấn luyện với hàm mất mát học có giám sát LNLL
từ phương trình 1 hoặc một hàm mất mát RL ngoại tuyến L(𝒙,𝒚;𝜃) trên dữ liệu được lọc
như V-MPO (Song et al., 2020) hoặc offline actor-critic (Mathieu et al., 2021). Tóm lại, chúng tôi sử dụng hàm mất mát có trọng số phần thưởng 𝐽 sau:

𝐽(𝜃)=𝔼(𝒙,𝒚)∼D𝑔[𝐹(𝒙,𝒚;𝜏)L( 𝒙,𝒚;𝜃)]. (2)

Các phương pháp học mô phỏng tiêu chuẩn, như BC (Pomerleau (1989), phương trình 1) và các phương pháp RL một bước như Behavior Value Estimation (BVE) (Gulcehre et al., 2021) thực hiện một bước
Improve trên tập dữ liệu cố định D. Ngược lại, phiên bản cơ bản của ReST bổ sung bao gồm một
bước Grow cho phép mô hình thu thập nhiều chuỗi đầu ra mới (bản dịch tiềm năng) cho
các ngữ cảnh 𝒙 từ tập dữ liệu gốc (câu nguồn để dịch).

Khi lặp qua các bước Improve, chúng tôi tăng các ngưỡng lọc: 𝜏1<···< 𝜏𝑁−1< 𝜏𝑁
(Hình 2). Việc lọc này với ngưỡng tăng dần dẫn đến các tập con dữ liệu có chất lượng tăng dần nhưng
có kích thước giảm dần. Vì LLM overfit với các tập dữ liệu nhỏ một cách nhanh chóng, chúng tôi tinh chỉnh mỗi chính sách mới từ
chính sách trước đó với tốc độ học thấp hơn. Việc tinh chỉnh liên tiếp các chính sách {𝜋𝜃𝑘}𝑘≥1 trên các tập con dữ liệu
chất lượng cao hơn đảm bảo cải thiện chính sách với một tập dữ liệu cố định D𝑔. Nếu chúng ta lấy mẫu từ
các chính sách{𝜋𝜃𝑘}𝑘≥1, phần thưởng trung bình của các mẫu được tạo sẽ tăng (được hiển thị bằng màu xám trong
Hình 2). Vì việc lấy mẫu từ một chính sách trong bước Grow tốn kém về mặt tính toán, sau mỗi bước như vậy
chúng tôi thực hiện nhiều bước Improve. Do đó, chi phí của một lần tạo tập dữ liệu duy nhất được khấu hao
qua nhiều bước Improve. Thuật toán 1 phác thảo thuật toán ReST đầy đủ với nhiều bước tăng trưởng tập dữ liệu
và cải thiện chính sách.

--- TRANG 5 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

Thuật toán 1: Thuật toán ReST. ReST là một thuật toán RL lô tăng trưởng. Cho một chính sách
ban đầu có chất lượng hợp lý (ví dụ, được huấn luyện trước sử dụng BC) áp dụng lặp đi lặp lại các bước Grow và
Improve để cập nhật chính sách. Ở đây 𝐹 là một hàm lọc, và L là một hàm mất mát.

Đầu vào: D: Tập dữ liệu, D𝑒𝑣𝑎𝑙: Tập dữ liệu đánh giá, L(𝒙,𝒚;𝜃): mất mát, 𝑅(𝒙,𝒚): mô hình phần thưởng, 𝐺:
số bước grow, 𝐼: số bước improve, 𝑁: số mẫu trên mỗi ngữ cảnh

Huấn luyện 𝜋𝜃 trên D sử dụng mất mát L.
for 𝑔=1 to 𝐺 do
// Grow
Tạo tập dữ liệu D𝑔 bằng cách lấy mẫu: D𝑔={(𝒙𝑖,𝒚𝑖)|𝑁𝑔
𝑖=1 s.t. 𝒙𝑖∼D,𝒚𝑖∼𝜋𝜃(𝒚|𝒙𝑖)}∪D.
Chú thích D𝑔 với mô hình phần thưởng 𝑅(𝒙,𝒚).
for 𝑖=1 to 𝐼 do
// Improve
Chọn ngưỡng s.t. 𝜏1>𝑉𝜋𝜃 cho 𝑉𝜋𝜃=𝔼D𝑔[𝑅(𝒙,𝒚)] và 𝜏𝑖+1> 𝜏𝑖.
while phần thưởng cải thiện trên D𝑒𝑣𝑎𝑙 do
Tối ưu hóa 𝜃 trên mục tiêu: 𝐽(𝜃)=𝔼(𝒙,𝒚)∼D𝑔[𝐹(𝒙,𝒚;𝜏𝑖)L( 𝒙,𝒚;𝜃)]
end
end
end
Đầu ra: Chính sách 𝜋𝜃

Giải thích xác suất của bước Improve. Hãy xem xét lựa chọn cụ thể L=LNLL,
với 𝜃′ là các tham số của mô hình từ bước Grow cuối cùng, 𝜆 là tỷ lệ dữ liệu được lấy mẫu
từ mô hình này trong D𝑔 và một bước tăng trưởng duy nhất. Biểu thức cho gradient trong trường hợp này có
dạng sau:

∇𝐽(𝜃)=−𝔼𝒙∼D
𝜆𝔼𝒚∼𝜋𝜃′(𝒚|𝒙)[𝐹(𝒙,𝒚;𝜏)∇log𝜋𝜃(𝒚|𝒙)]+(1−𝜆)𝔼𝒚∼𝑝(𝒚|𝒙)[𝐹(𝒙,𝒚;𝜏)∇log𝜋𝜃(𝒚|𝒙)]
. (3)

Số hạng đầu tiên ở vế phải của (3) tương tự như một số hạng policy gradient trực tuyến ở đầu quá trình huấn luyện
khi 𝜃≈𝜃′ với 𝐹(𝒙,𝒚;𝜏) thay thế hàm giá trị state-action 𝑄𝜋(𝒙,𝒚), khi bắt đầu ở trạng thái
𝒙 và thực hiện các hành động tuần tự 𝒚, tức là tạo dữ liệu tổng hợp 𝒚 sử dụng chính sách 𝜋𝜃 trong ngữ cảnh của chúng ta.
Đối với số hạng thứ hai ở vế phải của (3), chúng tôi xem xét dữ liệu gốc D, nhưng chúng tôi vẫn đảm bảo rằng nó
vượt qua ngưỡng 𝐹(𝒙,𝒚;𝜏). Một cách trực quan, mọi người chọn D để huấn luyện theo một số
tiêu chí có thể không xác định. Trong công trình này, chúng tôi làm cho tiêu chí 𝐹(𝒙,𝒚;𝜏) rõ ràng. Do đó số hạng cuối cùng là một
dạng policy gradients ngoại tuyến ngăn 𝜋𝜃(𝒚|𝒙) di chuyển quá xa khỏi 𝑝(𝒚|𝒙) có thể
dẫn đến sự sụp đổ mô hình (Shumailov et al., 2023). Cuối cùng, lưu ý sự tương đồng của phương pháp này với
các kỹ thuật self-training (Clark et al., 2003; Scudder, 1965; Xie et al., 2020). Chúng tôi cung cấp một
giải thích dân số (tức là khi 𝑁,𝑁𝑔→∞) của ReST trong Phụ lục A.9.

Trong phần tiếp theo, chúng tôi khám phá cách việc lựa chọn hàm mất mát, hàm lọc và ngưỡng, và
dữ liệu tổng hợp được tạo bởi chính sách ngôn ngữ thông qua lấy mẫu (dữ liệu khám phá) ảnh hưởng thực nghiệm đến
hiệu suất của các chính sách kết quả 𝜋𝜃.

4. Thí nghiệm và phân tích

Chúng tôi chọn dịch máy làm bàn thử nghiệm cho ReST vì đây là một ứng dụng có tác động của mô hình ngôn ngữ có điều kiện
nơi các mô hình phần thưởng đã được thiết lập có sẵn, ví dụ, Metric X (Freitag et al., 2022), BLEURT (Sellam et al., 2020) và COMET (Rei et al., 2020). Chúng tôi đã chạy thí nghiệm trên
hai chuẩn mực phổ biến: IWSLT 2014 (Cettolo et al., 2014), và WMT 2020 (Koehn et al., 2020),

--- TRANG 6 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

Hình 3|ReST với nhiều bước Improve. Điểm số mô hình phần thưởng trung bình trên các tập xác thực IWSLT 2014 De-En,
WMT 2020 Zh-En, và Web Domain En-Zh. Trên mỗi tập dữ liệu, chúng tôi báo cáo kết quả với BC
(𝐺=0, 𝐼=0) và ReST với một bước Grow duy nhất và nhiều bước Improve với ngưỡng phần thưởng
tăng dần. Mỗi bước Improve tăng điểm số mô hình phần thưởng trong cả ba tập dữ liệu xác thực. Chúng tôi
thấy rằng số bước Improve phù hợp là một siêu tham số phụ thuộc vào tập dữ liệu.

cũng như một tập dữ liệu chuẩn mực nội bộ mà chúng tôi gọi là Web Domain (một phiên bản của tập dữ liệu này đã
được sử dụng trước đó bởi Ghorbani et al. (2021)). Những tập dữ liệu này chứa một tập hợp các câu trong ngôn ngữ nguồn
và bản dịch "tham chiếu" của con người tương ứng. Chúng tôi đã chọn một cặp ngôn ngữ khác nhau cho mỗi tập dữ liệu
để kiểm tra tính tổng quát của kết quả. Chúng tôi giữ các tập xác thực và kiểm tra riêng biệt
với các câu nguồn chưa thấy cho mục đích đánh giá.

Chúng tôi sử dụng Metric X trong các thí nghiệm của mình, một mô hình phần thưởng không tham chiếu tiên tiến (Freitag et al.,
2022) mà, cho một văn bản nguồn và một bản dịch được đề xuất, đưa ra một điểm số số. Chúng tôi báo cáo
kết quả theo điểm phần thưởng trung bình trên các mẫu được tạo bởi một chính sách trên tập xác thực¹. Để biết
chi tiết về các tập dữ liệu và mô hình, chúng tôi tham chiếu đến Phụ lục A.3. Ngoài ra, Bảng 2 chỉ ra kích thước của
các tập dữ liệu bằng cách báo cáo số mẫu trên mỗi câu nguồn được tạo ra ở mỗi bước Grow.

Cách đặt tên. Chúng tôi đặt tên các biến thể của ReST theo loại hàm mất mát, số bước Grow, và số
bước Improve, ví dụ GOLD G=1 I=2. Với quy ước này, BC G=0 I=0 đề cập đến học
có giám sát tiêu chuẩn, được huấn luyện chỉ trên tập dữ liệu gốc D và không thực hiện bước Grow hay
Improve. Khi loại hàm mất mát không được chỉ định, hàm mất mát BC được sử dụng, tức là mô hình được huấn luyện với
học có giám sát tự hồi quy với hàm mất mát NLL như điển hình trong việc huấn luyện mô hình ngôn ngữ. Trong tất cả
các biểu đồ, chúng tôi tô màu học có giám sát bằng màu xám và các biến thể ReST bằng các sắc thái màu tím.

Baseline. Chúng tôi báo cáo kết quả với một số phương pháp RL ngoại tuyến khác nhau, bao gồm Offline Actor
Critic (OAC) (Mathieu et al., 2021), Behavior VMPO (BVMPO), Generation by Off-policy Learning from
Demonstrations (GOLD) (Pang and He, 2021), và BC (Pomerleau, 1989)².

Liệu nhiều bước Improve trong ReST có tăng điểm số mô hình phần thưởng không? Chúng tôi đánh giá ReST trên
ba tập dữ liệu khác nhau bằng cách cố định hàm mất mát thành BC và tăng số bước Improve.
Phạm vi phần thưởng cho huấn luyện được chuẩn hóa giữa 0 và 1³. Đối với các thí nghiệm của chúng tôi, chúng tôi

¹Hiệu suất trên tập kiểm tra theo cùng xu hướng (xem Phụ lục A.4). Chúng tôi cũng thí nghiệm với điểm BLEURT và BLEU,
và ReST cũng cải thiện những điểm số đó. Chúng tôi nhận thấy rằng thuật toán PPO trực tuyến có thể học cách khai thác
các điểm yếu và thiên lệch của hai chỉ số này một cách nhanh chóng, có thể khiến mô hình tạo ra các mẫu tối đa hóa phần thưởng
nhưng làm xấu đi chất lượng đầu ra của mô hình.
²Chi tiết về baseline của chúng tôi trong Phụ lục A.8 và các thí nghiệm với hàm mất mát bổ sung trong Phụ lục A.2
³Lưu ý rằng các biểu đồ hiển thị phần thưởng giữa 0 và 100.

--- TRANG 7 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

Hình 4|ReST với hai bước Grow. Bước Grow thứ hai với các bước Improve tiếp theo
cải thiện hiệu suất 5.3 điểm trên IWSLT 2014 De-En và 0.8 điểm trên tác vụ Web Domain En-Zh
so với bước Grow đầu tiên.

chọn các ngưỡng lọc 𝜏𝑖 từ một chuỗi các giá trị tăng dần [0.0,0.7,0.8,0.9,0.95,0.99]⁴.
Trường hợp 𝜏0=0.0 tương ứng với việc sử dụng toàn bộ tập dữ liệu. Chúng tôi đã thực hiện năm bước Improve trên IWSLT
2014, bốn trên WMT-2020, và hai trên Web Domain. Trong Hình 3 chúng tôi vẽ biểu đồ phần thưởng trung bình của
các biến thể ReST khác nhau. Chúng tôi thấy rằng mỗi bước Improve tiếp theo cải thiện hiệu suất của
mô hình dịch thuật một cách đáng kể trên cả ba tập dữ liệu.

Hình 5|WMT 2020 zh-en (test): BC (màu
xám, 𝐺=0𝐼=0) và ReST được huấn luyện với
các hàm mất mát RL ngoại tuyến khác nhau. ReST được huấn luyện
với một bước Grow và Improve ngoại trừ 𝐺=
1𝐼=0, được huấn luyện trên toàn bộ tập dữ liệu
được tạo sau bước Grow đầu tiên mà không có
bất kỳ Improve nào (tất cả màu tím). Tất cả biến thể của
ReST vượt trội hơn baseline BC ban đầu, với
hàm mất mát BC dẫn đến hiệu suất tốt nhất.

Liệu các bước Grow bổ sung có cải thiện điểm số mô hình phần thưởng không? Chúng tôi đã thực hiện một bước Grow thứ hai với các
bước Improve liên tiếp để đo lường tác động của bước
Grow thêm đối với hiệu suất. Trong Hình 4, một
phương pháp với bước Grow bổ sung đạt được cải thiện
thêm trên các tập dữ liệu IWSLT 2014 và Web Domain.
Chúng tôi nhận thấy cải thiện 5.3 điểm giữa cuối
bước Grow đầu tiên và thứ hai.

Liệu ReST có cải thiện so với huấn luyện có giám sát không?
Để trả lời câu hỏi này, trong Hình 5 chúng tôi vẽ biểu đồ
phần thưởng trung bình đạt được bởi mô hình học có giám sát
cũng như một số biến thể của ReST với các hàm mất mát khác nhau
và số bước Grow và Improve. Các
biến thể khác nhau của ReST (màu tím) vượt trội đáng kể
so với học có giám sát (màu xám) ngay cả sau chỉ bước
grow đầu tiên. Quan sát này nhất quán trên
các tập dữ liệu và cặp ngôn ngữ khác nhau mà chúng tôi đã kiểm tra.

Hàm mất mát nào là tốt nhất cho một bước ReST duy nhất? Hình 5 mô tả các biến thể ReST với các
hàm mất mát RL ngoại tuyến L(𝒙,𝒚;𝜃) khác nhau. Chúng tôi thấy rằng hàm mất mát BC vượt trội hơn các hàm mất mát khác. Lưu ý rằng thông thường
thuật toán BC không phụ thuộc vào phần thưởng, nhưng trong ReST, phần thưởng được tính đến thông qua
giai đoạn lọc phần thưởng cho 𝐼≥1 (với 𝜏1=0.8 cho WMT 2020.) Kết quả với nhiều bước Grow và
Improve được hiển thị trong Hình 4 (xem thêm Phụ lục A.6).

⁴Nếu chúng tôi sử dụng ít hơn 6 bước, chúng tôi bỏ qua các ngưỡng nhỏ hơn ban đầu. Chi tiết được đưa ra trong Phụ lục A.3. Chúng tôi đảm bảo
rằng trong tất cả các tập dữ liệu của chúng tôi 𝜏1>0.7≥𝑉𝜋𝜃 bằng cách đo lường thực nghiệm 𝑉𝜋𝜃 trên tập dữ liệu.

--- TRANG 8 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

Thuật toán | Phần thưởng Trung bình | Mẫu Khác biệt
BC(G=0, I=0) | 70.9 | 16 000 000
ReST(G=1, I=0) | 71.9 | 16 000 000
ReST(G=1, I=4) | 77.8 | 16 000 000
ReST(G=2, I=3) | 83.1 | 32 000 000
Online RL | 71.6 | 24 000 000

Bảng 1|Online RL cho IWSLT 2014: Online RL hoạt động tốt như ReST(G=1, I=0) và ReST
(G=1, I=4) tốt hơn đáng kể.

Liệu ReST có thể được cải thiện thêm với lấy mẫu Best-of-N tại thời điểm suy luận không? Kỹ thuật lấy mẫu Best-of-N
tại thời điểm suy luận tạo ra 𝑁 mẫu sau đó được xếp hạng bởi mô hình phần thưởng. Sau đó,
ứng viên được xếp hạng cao nhất được chọn (Gao et al., 2022). Chúng tôi hiển thị kết quả với lấy mẫu Best-of-N
trên BC(G=0 I=0) và các biến thể ReST trong Hình 6. Hiệu suất của ReST cải thiện cả
với 𝑁 và với số bước Improve. Biến thể ReST tốt nhất với 𝑁 < 10 khớp với
hiệu suất của mô hình BC với 𝑁=200. Mặc dù RL được biết là hạn chế sự đa dạng của các mẫu,
thí nghiệm này cho thấy ReST vẫn có thể hưởng lợi từ lấy mẫu Best-of-N. Sau ba bước Improve
với 𝑁=200, ReST đạt phần thưởng cao nhất có thể là 1, vượt trội hơn các bản dịch "tham chiếu"
trong D.

Hình 6|Lấy mẫu Best-of-N tại thời điểm suy
luận. Tất cả biến thể của ReST hưởng lợi
nhiều từ lấy mẫu Best-of-N như các mô hình
có giám sát.

ReST so sánh như thế nào với Online RL? Chúng tôi so sánh
ReST với PPO (Schulman et al., 2017), một thuật toán RL trực tuyến
được sử dụng rộng rãi cho RLHF (Glaese et al., 2022;
Ouyang et al., 2022a). Đối với các thí nghiệm RL trực tuyến của chúng tôi,
chúng tôi sử dụng thiết lập của Donato et al. (2022) nơi PPO
có quyền truy cập vào lượng dữ liệu huấn luyện tương tự như ReST
với 1 bước Grow. Kết quả được tóm tắt trong Bảng
1. Online RL hoạt động tốt như ReST với một Grow và
không có bước Improve nào tương đương với BC trên tập dữ liệu
D𝑔. Với cùng lượng dữ liệu huấn luyện, ReST
với nhiều bước Improve đạt phần thưởng cao hơn đáng kể.
Hơn nữa, chúng tôi nhận thấy rằng điểm BLEU
cho chính sách RL trực tuyến trên tập xác thực giảm gần
8 điểm (điểm BLEU của ReST không thay đổi) cho thấy
hành vi hack phần thưởng tiềm năng. Khả năng của ReST
cải thiện điểm số mô hình phần thưởng mà không làm xấu
hiệu suất trên các chỉ số khác cho thấy rằng
"thuế căn chỉnh" mà nó trả thấp hơn so với các phương pháp RL trực tuyến.

Liệu ReST có cải thiện sở thích của con người không? Chúng tôi đánh giá các mô hình ReST bằng người đánh giá con người để
điều tra xem ReST có thể vượt trội hơn BC trong đánh giá con người hay không. Chúng tôi hiển thị một câu trong
ngôn ngữ nguồn và hai bản dịch được tạo: một bởi mô hình BC (G=0 I=0) và một bởi một
biến thể ReST. Người đánh giá con người chấm điểm mỗi bản dịch trên thang điểm từ 0 đến 6, và chúng tôi đo
sự khác biệt giữa điểm số trung bình của phương pháp ReST và của BC mà chúng tôi gọi là "Human eval
diff". Trong Hình 7 (phải), chúng tôi thấy rằng tất cả biến thể của ReST vượt trội hơn baseline BC một cách đáng kể.
Tuy nhiên, nếu chúng tôi so sánh việc tăng điểm số con người với việc tăng phần thưởng đã học (Hình 7, trái),
các xếp hạng không khớp. Chúng tôi giả thuyết rằng sự khác biệt là do thực tế rằng các mô hình phần thưởng

--- TRANG 9 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

không thể tổng quát hóa tốt trên dữ liệu OOD vì các mô hình phần thưởng đã học chỉ là một proxy không hoàn hảo
của sở thích con người. Đặc biệt, chúng tôi thấy rằng các mô hình phần thưởng tổng quát hóa tệ hơn khi chính sách của chúng ta
di chuyển ra khỏi mô hình hành vi có thể xảy ra khi số bước Grow và Improve tăng lên tại thời điểm
ReST có thể bắt đầu overfit với mô hình phần thưởng. Do đó, trong phân tích của chúng tôi, chúng tôi
tập trung vào việc đánh giá các mô hình dựa trên mức độ họ căn chỉnh với tín hiệu phần thưởng và chúng tôi coi
việc tổng quát hóa mô hình phần thưởng như một vấn đề độc lập có thể được giảm thiểu bằng cách, ví dụ, tinh chỉnh
mô hình phần thưởng giữa các bước Grow liên tiếp trên dữ liệu được chú thích bởi con người từ chính sách
gần đây nhất. ReST với hàm mất mát B-VMPO sử dụng một hàm giá trị đã học và số hạng regularization KL
để ngăn chặn over-fitting và do đó đạt điểm sở thích con người cao.

Hình 7|So sánh hiệu suất dựa trên phần thưởng đã học và đánh giá con người. Tất cả
biến thể ReST vượt trội hơn BC về xếp hạng con người, nhưng việc xếp hạng các phương pháp dựa trên
điểm số mô hình phần thưởng và điểm số con người là khác nhau.

5. Các công trình liên quan

Hình 8|ReST vs các phương pháp thay thế: ReST là
phương pháp duy nhất có thể tận dụng dữ liệu khám phá
và phần thưởng, nhưng cũng hiệu quả về mặt tính toán.

Gần đây đã có một số lượng lớn các công trình về
các thuật toán căn chỉnh tự cải thiện cho mô hình
ngôn ngữ. Trong Hình 8, chúng tôi cũng so sánh
ReST với các phương pháp khác nhau: học có giám sát,
self-training, RL trực tuyến và ngoại tuyến.
Chúng tôi kết luận từ so sánh này rằng ReST là
phương pháp duy nhất hiệu quả về tính toán, nhưng
cũng có thể tận dụng dữ liệu khám phá và phần thưởng.
Tiếp theo, chúng tôi mô tả một số công trình cụ thể
liên quan đến ReST.

Self-training. Self-training là một phương pháp học bán giám sát đã được thiết lập sử dụng dữ liệu
không được gán nhãn để cải thiện một mô hình (Scudder, 1965). Kể từ khi được giới thiệu, self-training đã được
áp dụng thành công cho nhiều tác vụ, bao gồm phân loại và nhận dạng hình ảnh (Xie et al., 2020),
gấp nếp protein (Jumper et al., 2021), cũng như một số tác vụ ngôn ngữ (He et al., 2019; Sun et al.,
2021; Yarowsky, 1995; Zhang and Zong, 2016). He et al. (2019) đã chứng minh thực nghiệm rằng
self-training có nhiễu cải thiện hiệu suất của các mô hình dịch thuật. Bước Improve của ReST giống
self-training. Sự khác biệt chính của ReST so với self-training là bước Grow của ReST tạo ra
dữ liệu khám phá tổng hợp để huấn luyện với RL.

--- TRANG 10 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

Expert Iteration (EI). Anthony (2021) đề xuất một khung RL là một dạng phương pháp policy iteration
sử dụng cơ chế lập kế hoạch. EI phân tách rõ ràng vấn đề RL thành hai phần: lập kế hoạch và tổng quát hóa.
Tương tự như ReST, EI sử dụng chính sách để tạo dữ liệu và khai thác nó để học một chính sách với RL. Không giống EI,
ReST không yêu cầu bất kỳ cơ chế lập kế hoạch nào và nó sử dụng các bước Improve lặp đi lặp lại cho phép nó
tận dụng dữ liệu thu thập được hiệu quả hơn.

Lý luận với mô hình ngôn ngữ. Phương pháp EI đã truyền cảm hứng cho một số phương pháp liên quan (Uesato
et al., 2022; Zelikman et al., 2022). Zelikman et al. (2022) đề xuất một kỹ thuật gọi là STAR
lặp đi lặp lại tận dụng một số lượng nhỏ lý lẽ để tinh chỉnh mô hình. Sau đó, họ lấy mẫu lý lẽ
với các câu trả lời từ mô hình và lọc các câu trả lời được tạo theo tính đúng đắn của chúng. Uesato et al.
(2022) đề xuất một phương pháp tương tự học giải quyết các bài toán toán học sử dụng mô hình phần thưởng đã học.
Gần đây, Jung et al. (2023) đề xuất một phương pháp gọi là "Impossible Distillation" tương tự như của chúng tôi,
tạo ra tập dữ liệu cho các mô hình ngôn ngữ lớn bằng cách lấy mẫu từ một mô hình không tối ưu và lọc
các ví dụ chất lượng thấp với cơ chế lọc. Phương pháp này tương ứng với ReST với một bước Grow và Improve duy nhất.
Trái ngược với những phương pháp này, ReST có thể được huấn luyện với bất kỳ hàm mất mát RL ngoại tuyến nào,
có hoặc không có lập kế hoạch và các cơ chế lọc khác nhau. Nó cũng có thể xử lý điểm phần thưởng có giá trị liên tục.
Hơn nữa, ReST có thể thực hiện cải thiện chính sách lặp đi lặp lại trên một tập dữ liệu cố định với
các bước Improve.

Iterated Learning (IL). IL là quá trình mà một agent học hành vi của mình bằng cách tiếp xúc với
hành vi của agent khác, bản thân nó đã học theo cách tương tự (Kirby et al., 2014). Gần đây, phương pháp này
được áp dụng cho học ngôn ngữ tương tác sử dụng deep learning (Lu et al., 2020a,b). IL
khác với ReST vì nó hoạt động trong môi trường đa agent và không sử dụng RL.

Self Imitation Learning (SIL). SIL học một chính sách cho một thuật toán actor-critic off-policy nơi
chính sách cố gắng tái tạo hành vi tốt được thể hiện bởi agent (Oh et al., 2018). SIL
đạt được điều này bằng cách lọc ra các quỹ đạo không thành công từ replay buffer và huấn luyện agent
chỉ trên các quỹ đạo có phần thưởng cao. Theo nghĩa đó, ReST có thể được coi là có liên quan chặt chẽ
đến các phương pháp dựa trên SIL. Sự khác biệt chính là ReST không phụ thuộc vào thuật toán RL cơ bản
được sử dụng để huấn luyện chính sách và, không giống SIL, không cần thiết một hàm giá trị để lọc ra
các quỹ đạo không thành công. Ngoài ra, ReST được áp dụng cho các thiết lập AI sinh, không yêu cầu
tương tác với môi trường theo cách trực tuyến.

Reward ranked Fine-Tuning (RAFT). Đồng thời với công trình của chúng tôi, Dong et al. (2023) đề xuất RAFT.
RAFT có thể được hiểu như một trường hợp cụ thể của ReST chỉ sử dụng một bước Improve cho mỗi
bước Grow, và dựa vào một ngưỡng lọc là một phân vị cố định của phân phối thực nghiệm
của phần thưởng của các mẫu hiện tại. Các tác giả báo cáo cải thiện so với BC và PPO trên
nhiều tác vụ mô hình ngôn ngữ và tạo hình ảnh. Các thí nghiệm của chúng tôi với ReST cho thấy
rằng nhiều bước Improve với ngưỡng lọc tăng dần cho một bước Grow dẫn đến
cải thiện hiệu suất thêm.

6. Thảo luận

Trong bài báo này, chúng tôi đề xuất một thuật toán gọi là ReST đơn giản, có tối thiểu siêu tham số
để điều chỉnh, và linh hoạt để làm việc với nhiều thiết kế của các bước Grow và Improve. Chúng tôi nghiên cứu
hiệu suất của ReST trong dịch máy vì các mô hình phần thưởng mạnh mẽ và đã được thiết lập có sẵn cho

--- TRANG 11 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

tác vụ này. Chúng tôi thí nghiệm với các hàm mất mát RL ngoại tuyến khác nhau trong vòng lặp ReST, nhưng thấy BC hoạt động
tốt nhất để cải thiện điểm số mô hình phần thưởng. Nhiều bước huấn luyện NLL với các ngưỡng lọc
tăng dần trong bước Improve dẫn đến cải thiện liên tục trong phần thưởng của mô hình trên tập holdout.
Tuy nhiên, cải thiện trong điểm số mô hình phần thưởng không nhất thiết phản ánh sở thích của con người vì mô hình phần thưởng
chỉ là một proxy cho sở thích của con người. Kết quả chỉ ra rằng một bước Grow là lựa chọn tốt nhất khi xem xét
điểm đánh giá con người, mặc dù phần thưởng tiếp tục tăng với nhiều bước Grow hơn. Để khắc phục hạn chế này,
các mô hình phần thưởng có thể được tinh chỉnh trên tập con D𝑔 được chú thích với sở thích của con người tương tự như Bai et al. (2022) và
Glaese et al. (2022), mà chúng tôi để lại như công việc tương lai. Hãy để chúng tôi lưu ý rằng nguy cơ overfitting với
mô hình phần thưởng tăng lên với các lần lặp lại của các bước Grow; do đó chúng tôi tin rằng việc giải quyết
vấn đề này là cần thiết, đặc biệt trong các trường hợp cần nhiều bước Grow để huấn luyện mô hình.

Như chúng ta đã thấy, hàm mất mát BC đơn giản vẫn vượt trội hơn nhiều hàm mất mát RL ngoại tuyến về việc căn chỉnh
mô hình với điểm số mô hình phần thưởng. Tuy nhiên, chúng tôi thấy rằng BC có thể overfit với mô hình phần thưởng
vì vậy chúng tôi giải thích điều này bằng thực tế rằng việc học hàm giá trị trong RL là thách thức do phần thưởng thưa thớt,
các vấn đề gán tín dụng, độ nhạy cảm với siêu tham số, và khám phá hạn chế trong bước Grow.
ReST có thể hưởng lợi từ các chiến lược khám phá RL tốt hơn ở bước Grow, như MCTS (Leblond
et al., 2021). Khả năng khai thác dữ liệu được tạo trong bước Grow có thể dẫn đến khám phá rộng hơn
của không gian state-action và tổng quát hóa tốt hơn. Ngoài ra, tính chất xác định của môi trường
không cho phép thu được lợi ích lớn so với BC cho các hàm mất mát RL ngoại tuyến.

Để kết luận, ReST là một phương pháp tổng quát và hiệu quả. Nó có thể được áp dụng khi 1) một mô hình phần thưởng mạnh mẽ
của sở thích con người có sẵn và 2) chúng ta có thể tạo ra các mẫu từ mô hình ở quy mô lớn. Do đó, nó có thể
được áp dụng cho nhiều tác vụ trong lĩnh vực ngôn ngữ, như tóm tắt, đối thoại theo lượt, và các mô hình
âm thanh và video sinh khác. Với một số hướng khám phá và ứng dụng trong tương lai, chúng tôi tin rằng
ReST là một phương pháp RL lô tăng trưởng hữu ích cho RLHF.

Lời cảm ơn. Chúng tôi muốn cảm ơn các thành viên từ các nhóm dịch máy tại
Google và Google DeepMind cho đầu vào của họ vào dự án trong các giai đoạn brainstorming và
để thiết lập codebase mà dự án này được phát triển dựa trên (Yu et al., 2020). Chúng tôi muốn
cảm ơn Matt Hoffman, Bobak Shahriari, Taylan Cemgil và Chris Dyer cho các cuộc thảo luận về dự án này.
Chúng tôi biết ơn phản hồi được cung cấp bởi Bilal Piot cho bản thảo sớm của bài báo này. Chúng tôi
cũng muốn cảm ơn những người chịu trách nhiệm cho các framework khác nhau mà chúng tôi đã sử dụng trong
dự án như hệ sinh thái DeepMind JAX (Babuschkin et al., 2020) và Launchpad (Yang et al.,
2021).

Tài liệu tham khảo

A. Abdolmaleki, S. Huang, G. Vezzani, B. Shahriari, J. T. Springenberg, S. Mishra, D. Tirumala,
A. Byravan, K. Bousmalis, A. György, et al. Về tối ưu hóa chính sách đa mục tiêu như một công cụ cho
học tăng cường. arXiv preprint arXiv:2106.08199, 2021.

R. Agarwal, M. Schwarzer, P. S. Castro, A. Courville, và M. G. Bellemare. Vượt ra ngoài tabula rasa:
Tái sinh học tăng cường. arXiv preprint arXiv:2206.01626, 2022.

T. W. Anthony. Expert iteration. Luận án Tiến sĩ, UCL (University College London), 2021.

I. Babuschkin, K. Baumli, A. Bell, S. Bhupatiraju, J. Bruce, P. Buchlovsky, D. Budden, T. Cai, A. Clark,
I. Danihelka, C. Fantacci, J. Godwin, C. Jones, R. Hemsley, T. Hennigan, M. Hessel, S. Hou,

--- TRANG 12 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

S. Kapturowski, T. Keck, I. Kemaev, M. King, M. Kunesch, L. Martens, H. Merzic, V. Mikulik,
T. Norman, J. Quan, G. Papamakarios, R. Ring, F. Ruiz, A. Sanchez, R. Schneider, E. Sezener,
S. Spencer, S. Srinivasan, L. Wang, W. Stokowiec, và F. Viola. Hệ sinh thái DeepMind JAX,
2020. URL http://github.com/deepmind.

Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan,
et al. Huấn luyện một trợ lý hữu ích và vô hại với học tăng cường từ phản hồi của con người.
arXiv preprint arXiv:2204.05862, 2022.

R. Bellman. Một quá trình quyết định Markovian. Journal of Mathematics and Mechanics, 6(5):679–684,
1957.

D. Brandfonbrener, A. Bietti, J. Buckman, R. Laroche, và J. Bruna. Khi nào học có giám sát có điều kiện trả về
hoạt động cho học tăng cường ngoại tuyến? Advances in Neural Information
Processing Systems, 35:1542–1553, 2022.

T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Mô hình ngôn ngữ là những người học few-shot. Advances in Neural Information
Processing Systems, 2020.

S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li,
S. Lundberg, et al. Tia lửa của trí tuệ nhân tạo tổng quát: Thí nghiệm sớm với GPT-4. arXiv
preprint arXiv:2303.12712, 2023.

M. Cettolo, J. Niehues, S. Stüker, L. Bentivogli, và M. Federico. Báo cáo về chiến dịch đánh giá iwslt lần thứ 11.
Trong Proceedings of the 11th International Workshop on Spoken Language Translation:
Evaluation Campaign, 2014.

L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, và I. Mordatch.
Decision transformer: Học tăng cường thông qua mô hình chuỗi. Trong Advances in Neural
Information Processing Systems, 2021.

S. Clark, J. R. Curran, và M. Osborne. Bootstrapping pos-taggers sử dụng dữ liệu không được gán nhãn. Trong Proceedings
của hội nghị thứ bảy về Natural language learning at HLT-NAACL 2003, trang 49–55, 2003.

D. Donato, L. Yu, W. Ling, và C. Dyer. Mad cho học tăng cường mạnh mẽ trong dịch máy.
arXiv preprint arXiv:2207.08583, 2022.

H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, và T. Zhang. Raft: Tinh chỉnh được xếp hạng phần thưởng
cho việc căn chỉnh mô hình nền tảng sinh. arXiv preprint arXiv:2304.06767, 2023.

L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley,
I. Dunning, S. Legg, và K. Kavukcuoglu. Impala: Deep-rl phân tán có thể mở rộng với kiến trúc
actor-learner có trọng số tầm quan trọng. Trong International Conference on Machine Learning, 2018.

M. Freitag, R. Rei, N. Mathur, C.-k. Lo, C. Stewart, G. Foster, A. Lavie, và O. Bojar. Kết quả của
tác vụ chia sẻ chỉ số wmt21: Đánh giá chỉ số với đánh giá con người dựa trên chuyên gia trên lĩnh vực ted và
tin tức. Trong Proceedings of the Sixth Conference on Machine Translation, trang 733–774, 2021.

M. Freitag, R. Rei, N. Mathur, C.-k. Lo, C. Stewart, E. Avramidis, T. Kocmi, G. Foster, A. Lavie, và
A. F. T. Martins. Kết quả của tác vụ chia sẻ chỉ số WMT22: Ngừng sử dụng BLEU – chỉ số neural tốt hơn và mạnh mẽ hơn.
Trong Proceedings of the Seventh Conference on Machine Translation (WMT), trang 46–68, Abu Dhabi, United Arab Emirates (Hybrid), Dec. 2022. Association for Computational
Linguistics. URL https://aclanthology.org/2022.wmt-1.2.

--- TRANG 13 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

J. Fu, A. Kumar, O. Nachum, G. Tucker, và S. Levine. D4RL: Tập dữ liệu cho học tăng cường
dữ liệu sâu. arXiv preprint arXiv:2004.07219, 2020.

L. Gao, J. Schulman, và J. Hilton. Quy luật mở rộng cho tối ưu hóa quá mức mô hình phần thưởng. arXiv preprint
arXiv:2210.10760, 2022.

B. Ghorbani, O. Firat, M. Freitag, A. Bapna, M. Krikun, X. Garcia, C. Chelba, và C. Cherry. Quy luật mở rộng
cho dịch máy neural. arXiv preprint arXiv:2109.07740, 2021.

A. Glaese, N. McAleese, M. Trębacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger,
M. Chadwick, P. Thacker, et al. Cải thiện việc căn chỉnh của các agent đối thoại thông qua phán xét
có mục tiêu của con người. arXiv preprint arXiv:2209.14375, 2022.

C. Gulcehre, S. G. Colmenarejo, Z. Wang, J. Sygnowski, T. Paine, K. Zolna, Y. Chen, M. Hoffman, R. Pascanu, và N. de Freitas. Ước lượng giá trị hành vi được điều chỉnh. arXiv preprint arXiv:2103.09575,
2021.

J. He, J. Gu, J. Shen, và M. Ranzato. Xem xét lại self-training cho việc tạo chuỗi neural. arXiv
preprint arXiv:1909.13788, 2019.

J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas, L. A.
Hendricks, J. Welbl, A. Clark, et al. Một phân tích thực nghiệm về huấn luyện mô hình ngôn ngữ lớn
tối ưu tính toán. Trong Advances in Neural Information Processing Systems, 2022.

J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates,
A. Žídek, A. Potapenko, et al. Dự đoán cấu trúc protein có độ chính xác cao với alphafold. Nature,
596(7873):583–589, 2021.

J. Jung, P. West, L. Jiang, F. Brahman, X. Lu, J. Fisher, T. Sorensen, và Y. Choi. Chưng cất không thể:
từ mô hình chất lượng thấp đến tập dữ liệu & mô hình chất lượng cao cho tóm tắt và diễn đạt lại. arXiv
preprint arXiv:2305.16635, 2023.

S. Kirby, T. Griffiths, và K. Smith. Học lặp đi lặp lại và sự tiến hóa của ngôn ngữ. Current Opinion
in Neurobiology, 28:108–114, 2014.

P. Koehn, V. Chaudhary, A. El-Kishky, N. Goyal, P.-J. Chen, và F. Guzmán. Phát hiện của tác vụ chia sẻ wmt 2020
về lọc và căn chỉnh corpus song song. Trong Proceedings of the Fifth Conference on
Machine Translation, trang 726–742, 2020.

T. Kudo. Regularization subword: Cải thiện mô hình dịch mạng neural với nhiều ứng viên
subword. Trong Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics,
trang 66–75, 2018.

A. Kumar, J. Hong, A. Singh, và S. Levine. Tôi có nên chạy học tăng cường ngoại tuyến hay behavioral
cloning không? Trong International Conference on Learning Representations, 2021.

S. Lange, T. Gabel, và M. Riedmiller. Học tăng cường theo lô. Trong M. Wiering và M. van
Otterlo, editors, Reinforcement Learning: State-of-the-Art, trang 45–73. Springer Berlin Heidelberg,
2012.

R. Leblond, J.-B. Alayrac, L. Sifre, M. Pislar, L. Jean-Baptiste, I. Antonoglou, K. Simonyan, và
O. Vinyals. Giải mã dịch máy vượt ra ngoài tìm kiếm beam. Trong Proceedings of the Conference on
Empirical Methods in Natural Language Processing, 2021.

--- TRANG 14 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno,
A. Dal Lago, et al. Tạo mã cấp độ cạnh tranh với alphacode. Science, 378(6624):1092–
1097, 2022.

Y. Lu, S. Singhal, F. Strub, A. Courville, và O. Pietquin. Chống lại sự trôi dạt ngôn ngữ với học lặp đi lặp lại
có hạt giống. Trong International Conference on Machine Learning, 2020a.

Y. Lu, S. Singhal, F. Strub, O. Pietquin, và A. Courville. Học lặp đi lặp lại có hạt giống có giám sát
cho học ngôn ngữ tương tác. arXiv preprint arXiv:2010.02975, 2020b.

M. Mathieu, S. Ozair, S. Srinivasan, C. Gulcehre, S. Zhang, R. Jiang, T. Le Paine, K. Zolna, R. Powell,
J. Schrittwieser, et al. Starcraft ii unplugged: Học tăng cường ngoại tuyến quy mô lớn. Trong Deep
RL Workshop NeurIPS 2021, 2021.

V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Harley, T. P. Lillicrap, D. Silver, và K. Kavukcuoglu.
Phương pháp bất đồng bộ cho học tăng cường sâu. Trong International Conference on Learning
Representations, 2016.

J. Oh, Y. Guo, S. Singh, và H. Lee. Self-imitation learning. Trong International Conference on Machine
Learning, trang 3878–3887. PMLR, 2018.

L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,
A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano,
J. Leike, và R. Lowe. Huấn luyện mô hình ngôn ngữ để tuân theo hướng dẫn với phản hồi của con người,
2022a.

L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,
A. Ray, et al. Huấn luyện mô hình ngôn ngữ để tuân theo hướng dẫn với phản hồi của con người. arXiv preprint
arXiv:2203.02155, 2022b.

R. Y. Pang và H. He. Tạo văn bản bằng cách học từ các cuộc trình diễn. Trong International Conference
on Learning Representations, 2021.

E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, và G. Irving. Red
teaming mô hình ngôn ngữ với mô hình ngôn ngữ. arXiv preprint arXiv:2202.03286, 2022.

D. A. Pomerleau. ALVINN: Một xe tự hành trên đất liền trong một mạng neural. Trong Advances in Neural
Information Processing Systems, trang 305–313, 1989.

J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Henderson, R. Ring,
S. Young, et al. Mở rộng mô hình ngôn ngữ: Phương pháp, phân tích & hiểu biết từ việc huấn luyện gopher. arXiv
preprint arXiv:2112.11446, 2021.

R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, và C. Finn. Tối ưu hóa sở thích trực tiếp:
Mô hình ngôn ngữ của bạn bí mật là một mô hình phần thưởng. arXiv preprint arXiv:2305.18290,
2023.

R. Rei, C. Stewart, A. C. Farinha, và A. Lavie. COMET: Một khung neural cho đánh giá MT. Trong
Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2020.

J. Schulman, F. Wolski, P. Dhariwal, A. Radford, và O. Klimov. Thuật toán tối ưu hóa chính sách gần đúng.
arXiv preprint arXiv:1707.06347, 2017.

H. Scudder. Xác suất lỗi của một số máy nhận dạng mẫu thích ứng. IEEE Transactions on
Information Theory, 11(3):363–371, 1965.

--- TRANG 15 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

T. Sellam, D. Das, và A. Parikh. BLEURT: Học các chỉ số mạnh mẽ cho việc tạo văn bản. Trong Proceedings
of the 58th Annual Meeting of the Association for Computational Linguistics, 2020.

I. Shumailov, Z. Shumaylov, Y. Zhao, Y. Gal, N. Papernot, và R. Anderson. Lời nguyền của đệ quy:
Huấn luyện trên dữ liệu được tạo khiến mô hình quên. arXiv preprint arxiv:2305.17493, 2023.

J. Skalse, N. H. R. Howe, D. Krasheninnikov, và D. Krueger. Định nghĩa và đặc trưng reward
hacking. Trong Advances in Neural Information Processing Systems, 2022.

H. F. Song, A. Abdolmaleki, J. T. Springenberg, A. Clark, H. Soyer, J. W. Rae, S. Noury, A. Ahuja, S. Liu,
D. Tirumala, N. Heess, D. Belov, M. Riedmiller, và M. M. Botvinick. V-mpo: Tối ưu hóa chính sách
maximum a posteriori trên chính sách cho điều khiển rời rạc và liên tục. Trong International Conference of
Learning Representations, 2020.

A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta,
A. Garriga-Alonso, et al. Vượt ra ngoài trò chơi mô phỏng: Định lượng và ngoại suy khả năng
của mô hình ngôn ngữ. arXiv preprint arXiv:2206.04615, 2022.

N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, và P. F. Christiano.
Học cách tóm tắt với phản hồi của con người. Advances in Neural Information Processing Systems,
2020.

H. Sun, R. Wang, K. Chen, M. Utiyama, E. Sumita, và T. Zhao. Self-training cho dịch máy neural
không giám sát trong các tình huống dữ liệu huấn luyện không cân bằng. Trong Proceedings of the Conference
of the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, 2021.

I. Sutskever, O. Vinyals, và Q. V. Le. Học sequence to sequence với mạng neural. Trong
Advances in Neural Information Processing Systems, 2014.

J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, và I. Higgins. Giải quyết bài toán từ toán học với phản hồi dựa trên quá trình và kết quả. arXiv preprint
arXiv:2211.14275, 2022.

A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, và I. Polosukhin.
Attention is all you need. Trong Advances in Neural Information Processing Systems, 2017.

J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, và P. Christiano. Tóm tắt sách một cách
đệ quy với phản hồi của con người. arXiv preprint arXiv:2109.10862, 2021.

Q. Xie, M.-T. Luong, E. Hovy, và Q. V. Le. Self-training với noisy student cải thiện phân loại imagenet.
Trong Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
trang 10687–10698, 2020.

F. Yang, G. Barth-Maron, P. Stańczyk, M. Hoffman, S. Liu, M. Kroiss, A. Pope, và A. Rrustemi.
Launchpad: Một mô hình lập trình cho nghiên cứu máy học phân tán. arXiv preprint
arXiv:2106.04516, 2021.

D. Yarowsky. Loại bỏ sự nhập nhằng nghĩa từ không giám sát cạnh tranh với các phương pháp có giám sát. Trong 33rd Annual
Meeting of the Association for Computational Linguistics, 1995.

L. Yu, L. Sartran, P.-S. Huang, W. Stokoweic, D. Donato, S. Srinivasan, A. Andreev, W. Ling, S. Mokra,
A. D. Lago, Y. Doron, S. Young, P. Blunsom, và C. Dyer. Hệ thống dịch tài liệu tiếng Trung–tiếng Anh DeepMind
tại wmt2020. Trong Proceedings of the Fifth Conference on Machine Translation, 2020.

--- TRANG 16 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

E. Zelikman, Y. Wu, J. Mu, và N. Goodman. Star: Bootstrapping reasoning với lý luận. Trong
Advances in Neural Information Processing Systems, 2022.

J. Zhang và C. Zong. Khai thác dữ liệu đơn ngữ phía nguồn trong dịch máy neural. Trong
Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2016.

A. Phụ lục

A.1. RLHF cho mô hình ngôn ngữ có điều kiện như MDP

Chúng ta có thể công thức hóa mô hình ngôn ngữ có điều kiện như một vấn đề sequence to sequence. Mục tiêu là
ánh xạ một chuỗi nguồn 𝒙=(𝑥1,𝑥2,...𝑥𝐿) thành một chuỗi đích 𝒚=(𝑦1,𝑦2,....𝑦𝑇), tức là học một
ánh xạ từ 𝒙 đến 𝒚. Dịch máy là một ví dụ cổ điển của vấn đề sequence to sequence
(Sutskever et al., 2014).

Mô hình có thể được mô tả như một Markov Decision Process (MDP) (Bellman, 1957). Một MDP, Mdef=
(S,A,𝑇,𝑟,𝑑), bao gồm các tập hữu hạn của trạng thái S và hành động A, một phân phối chuyển đổi 𝑇(𝑠′|𝑠,𝑎),𝑠,𝑠′∈
S,𝑎∈A, một hàm phần thưởng 𝑟:S×A→ ℝ, và một phân phối trạng thái ban đầu 𝑑:S→[0,1]. Trong
thiết lập ngoại tuyến, agent học từ một tập dữ liệu D chứa các chuỗi (𝑠𝑛,𝑎𝑛,𝑟𝑛+1). Tập dữ liệu
D thường được giả định được tạo bởi một chính sách hành vi không xác định 𝜇 được đặc trưng bởi một phân phối
trên các hành động có điều kiện trên trạng thái: 𝜇(𝑎|𝑠). Các yếu tố của MDP có ý nghĩa sau trong
mô hình ngôn ngữ có điều kiện:

•Trạng thái (𝑠): Trạng thái 𝑠𝑛 bao gồm chuỗi đầu vào và đầu ra được tạo một phần đến
token thứ 𝑛: 𝑠𝑛=[𝒙,𝒚1:𝑛−1].

•Hành động (𝑎): Các hành động là các token 𝑎𝑛=𝑦𝑛 được tạo bởi chính sách (𝜋).

•Phần thưởng (r): Phần thưởng có thể được cho hoặc học. Trong các thí nghiệm của chúng tôi, chúng tôi huấn luyện một mạng neural sâu
trên sở thích của con người để gán một điểm số cho toàn bộ chuỗi được tạo. Phần thưởng được
tạo ra sau token end-of-sequence (EOS), và nó bằng không ở tất cả các bước khác.

•Toán tử chuyển đổi (T): Toán tử chuyển đổi định nghĩa động lực của một môi trường:
𝑇(𝑠′|𝑎,𝑠):=𝑇(𝑦𝑛|𝑠𝑛=[𝒙,𝒚1:𝑛−1]). Trong thiết lập của chúng tôi, đây là một toán tử xác định nối
token mới được tạo (hành động) vào chuỗi hiện tại (trạng thái).

Công thức RLHF này của mô hình ngôn ngữ có điều kiện cũng có thể được xem như một vấn đề
contextual bandit với không gian hành động rất lớn.

A.2. Kết quả âm tính với RL ngoại tuyến

Ngoài các thí nghiệm từ Phần 4, chúng tôi cũng chạy thí nghiệm với Q-learning với các phương pháp RL một bước
kiểu BVE và các phương pháp có điều kiện phần thưởng, như decision transformers (Chen
et al., 2021). Tuy nhiên, chúng tôi không thể có được cải thiện đáng chú ý với chúng so với
baseline có giám sát. Các phương pháp dựa trên Q-function để học một chính sách và hàm giá trị hoạt động tệ hơn
học có giám sát khi được sử dụng như một chính sách ngay cả sau khi huấn luyện từ khởi tạo từ một
checkpoint có giám sát. Điều này khớp với các quan sát trước đó bởi Mathieu et al. (2021) người đã chỉ ra rằng Q-learning ngoại tuyến
gặp khó khăn trong không gian hành động lớn (kích thước từ vựng 32 000 token) và các phương pháp dựa trên state-value
được ưa thích hơn. Hơn nữa, như Brandfonbrener et al. (2022) đã chỉ ra, các phương pháp có điều kiện trả về
có xu hướng học các chính sách không tối ưu trên các tác vụ với phần thưởng liên tục thưa thớt.

--- TRANG 17 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

A.3. Chi tiết Dữ liệu và Mô hình

Trong tất cả các thí nghiệm của chúng tôi, kiến trúc chính sách cơ sở là một sửa đổi nhỏ của kiến trúc Transformer tiêu chuẩn
(Vaswani et al., 2017). Chúng tôi sử dụng từ vựng 32 000 token và độ dài chuỗi tối đa
128 tại thời điểm giải mã. Chúng tôi sử dụng công cụ sentencepiece (Kudo, 2018) để xây dựng các tokenizer.

Bước Grow. Trong bước Grow, chúng tôi lấy mẫu từ checkpoint mới nhất của chính sách với softmax có điều
sử dụng nhiệt độ 0.8 theo quy trình được đề xuất bởi Li et al. (2022) để tạo
tập dữ liệu. Hơn nữa, trong phân tích của chúng tôi, chúng tôi thấy rằng nhiệt độ 0.8 thường bao phủ một phạm vi rộng
phần thưởng trong tập dữ liệu.

Ngưỡng trong bước Improve. Nếu chúng ta đặt ngưỡng 𝜏 thành một giá trị sao cho 𝜏≥𝑉𝜋0, và học một chính sách
𝜋1 trên dữ liệu được lọc với ngưỡng này, giá trị trung bình được cập nhật 𝑉𝜋1 của chính sách này thỏa mãn 𝑉𝜋1>𝑉𝜋0.
Sau đó, ngưỡng lặp đi lặp lại đảm bảo cải thiện chính sách trong bước Improve. Trong các thí nghiệm của chúng tôi, chúng tôi
thực hiện một số bước cải thiện cho đến khi đạt ngưỡng tối đa. Ở mỗi bước, chúng tôi huấn luyện
mô hình trong 500 000 bước SGD và chọn checkpoint với điểm số mô hình phần thưởng tốt nhất.

IWSLT 2014 De-En. Chúng tôi sử dụng các tập train, validation và test từ tập dữ liệu IWSLT 2014 De-En (Cettolo
et al., 2014) bao gồm các câu nguồn bằng tiếng Đức với bản dịch con người (tham chiếu) bằng
tiếng Anh. Trong mỗi bước Grow, chúng tôi tạo ra 100 bản dịch ứng viên cho mỗi câu nguồn trong
tập huấn luyện, hiệu quả mang lại cho chúng tôi |D𝑔|=16 000 000. Đối với tập dữ liệu này, chúng tôi sử dụng một phiên bản nhỏ của
kiến trúc Transformer encoder-decoder (Vaswani et al., 2017) với các lớp MLP feedforward có
kích thước 512, chiều feedforward 1024, 4 attention head và 6 lớp encoder và decoder.

WMT 2020 Zh-En. Chúng tôi sử dụng các cặp source-reference bằng tiếng Trung và tiếng Anh từ công trình của Koehn
et al. (2020) cho các tập training, validation và test của chúng tôi. Chi tiết chính xác về các tập dữ liệu và tiền xử lý
có thể được tìm thấy trong Yu et al. (2020). Trong mỗi bước Grow, chúng tôi tạo ra 25 ứng viên cho mỗi câu nguồn
trong tập huấn luyện, hiệu quả mang lại cho chúng tôi |D𝑔|=890 000 000. Chúng tôi chọn tạo ra 25
ứng viên trên mỗi nguồn vì tập dữ liệu WMT lớn hơn đáng kể (~100 lần) so với tập dữ liệu IWSLT.
Chúng tôi sử dụng một kiến trúc mô phỏng kiến trúc Transformer-base encoder-decoder (Vaswani et al., 2017)
với chiều mô hình 512, chiều feedforward 2048, 8 attention head và 6
lớp encoder và decoder.

Web Domain En-Zh. Cuối cùng, chúng tôi sử dụng tập dữ liệu nội bộ cho dịch từ tiếng Anh sang tiếng Trung với
các tập training, fine-tuning và test tùy chỉnh. Corpus huấn luyện này là tập dữ liệu lớn nhất của chúng tôi⁵, vì vậy chúng tôi sử dụng một
phiên bản sửa đổi của kiến trúc Transformer-big encoder-decoder (Vaswani et al., 2017) với
chiều mô hình 1024, chiều feedforward 8192, 16 attention head và 6
lớp encoder và decoder.

Trong Bảng 2, chúng tôi liệt kê tất cả các tập dữ liệu với kích thước của chúng. Trong tất cả các thí nghiệm, trừ khi được nêu khác,
chúng tôi báo cáo điểm số phần thưởng trung bình trên tập validation.

⁵Vì lý do tính toán, chúng tôi chạy ReST trên corpus fine-tuning với một mô hình được khởi động từ một mô hình có giám sát
được huấn luyện trên toàn bộ corpus huấn luyện khổng lồ.

--- TRANG 18 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

Tập dữ liệu | |D𝑔| | # Mẫu Eval | # Ứng viên
per source
IWSLT 2014 de-en | 16 000 000 | 7466 | 100
WMT 2020 zh-en | 890 000 000 | 2000 | 25
Web Domain Finetuning en-zh | 3 000 000 | 6000 | 1000

Bảng 2|Chi tiết của các tập dữ liệu và kích thước của chúng được sử dụng trong các thí nghiệm ReST.

A.4. Mô hình phần thưởng

Chúng tôi đã sử dụng các mô hình phần thưởng đã học gán một điểm số cho toàn bộ bản dịch. Chúng tôi xem xét hai
loại mô hình phần thưởng cho dịch thuật (Freitag et al., 2022): i) Mô hình phần thưởng không tham chiếu ước tính
mức độ tốt của một bản dịch chỉ dựa trên câu nguồn và ứng viên, ii) Mô hình phần thưởng có tham chiếu
bổ sung sử dụng bản dịch tham chiếu của con người để quyết định mức độ tốt của một bản dịch ứng viên.
Các mô hình phần thưởng không tham chiếu linh hoạt hơn vì chúng không yêu cầu bản dịch tham chiếu.
Hơn nữa, theo thiết kế, các mô hình có tham chiếu gán điểm số cao hơn cho các câu tương tự như tham chiếu,
và điều này có thể xảy ra ngay cả khi tham chiếu có lỗi hoặc không đầy đủ.
Cuối cùng, các mô hình không tham chiếu mở ra khả năng đánh giá và khám phá các bản dịch ứng viên
vượt qua chất lượng của tham chiếu. Do đó, trong các thí nghiệm của chúng tôi, chúng tôi chọn làm việc với các mô hình phần thưởng không tham chiếu.
Mặt khác, vì các mô hình phần thưởng không tham chiếu không được grounded trong một
bản dịch tham chiếu của con người, nó dễ bị tổn thương hơn trước sự dịch chuyển phân phối trong các ứng viên được tạo
bởi mô hình và reward hacking. Trong thực tế, chúng tôi tính toán trước và lưu trữ 𝑅(𝒙,𝒚) cho D𝑔 được tạo.

Trong quá trình phát triển mô hình, chúng tôi dựa vào phương pháp "unit tests" cho các mô hình phần thưởng
nơi chúng tôi kiểm tra chúng trong nhiều thiết lập được tạo thủ công, như trên các hoán vị và lặp lại khác nhau
của câu. Các unit-test đảm bảo chất lượng cao của phần thưởng được tạo cho ReST. Mặc dù chúng tôi đã sử dụng
mô hình phần thưởng không tham chiếu mạnh mẽ nhất có sẵn, chúng tôi thấy rằng nó vẫn
không hoàn hảo, và đôi khi cho thấy dấu hiệu của ảo tưởng. Ví dụ, phần thưởng của một bản dịch
thỉnh thoảng tăng lên khi chúng tôi lặp lại câu, không phụ thuộc vào chất lượng ban đầu của
bản dịch.

A.5. Sự căn chỉnh giữa điểm đánh giá con người và điểm số mô hình phần thưởng

Trong Hình 9 và 10, chúng tôi hiển thị phân phối sở thích con người và điểm số mô hình phần thưởng cho
ReST với BC và G=1 I=4. Ở đây, chúng tôi phát hiện ra rằng mô hình phần thưởng của chúng tôi có phương sai rất cao trên
các mẫu với điểm sở thích con người thấp từ ReST(BC,G=1 I=4) và baseline học có giám sát của chúng tôi
(BC,G=0 I=0). Giả thuyết của chúng tôi cho điều này là tập dữ liệu huấn luyện cho mô hình phần thưởng
bị thống trị bởi các bản dịch chất lượng cao⁶. Do đó mô hình overfitted với các mẫu có
điểm số cao, và nó có phương sai cao trong dự đoán của mình trên các mẫu có chất lượng kém hơn. Khoảng cách này có thể
được giải quyết với việc huấn luyện lại tăng dần của mô hình phần thưởng.

A.6. Kết quả trên các tập test

Hình 11 và 12 trình bày kết quả trên các tập test trong các tập dữ liệu IWSLT 2014 De-En và WMT 2020 Zh-En.
Tương tự như tập validation, ReST vượt trội hơn các baseline RL ngoại tuyến dựa trên giá trị khác. Ngoài ra,
việc tăng số lượng cả bước Grow và Improve giúp tăng thêm phần thưởng.

⁶Dữ liệu đến từ tập dữ liệu tác vụ chia sẻ chỉ số WMT 2021 (Freitag et al., 2021), xem xét đánh giá con người
của các mô hình dịch tốt

--- TRANG 19 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

Hình 9|[IWSLT 2014 De-En]: phân phối sở thích con người và điểm số mô hình phần thưởng cho ReST
(BC, I=4, G=1) trong đánh giá side by side với mô hình có giám sát. Điểm sở thích con người
nhỏ hơn hoặc bằng 3 có phương sai cao hơn đáng kể trong phần thưởng so với điểm sở thích con người
trên 3. Các biểu đồ bên trái là cho các mẫu được tạo từ baseline có giám sát và bên phải là cho
các mẫu được tạo với ReST.

Hình 10|[WMT 2020 Zh-En]: phân phối sở thích con người và điểm số mô hình phần thưởng cho ReST
(BC, I=4, G=1) trong đánh giá side by side với mô hình có giám sát. Điểm sở thích con người
thấp hơn hoặc bằng 3 có phương sai về điểm số mô hình phần thưởng. Mô hình phần thưởng có ít
chắc chắn hơn về điểm số của các ứng viên có điểm số thấp hơn.

--- TRANG 20 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

Hình 11|[IWSLT 2014 De-En]: Kết quả ReST trên tập test. Tất cả các biến thể của ReST (màu tím) vượt trội hơn
baseline học có giám sát (màu xám). Khi chúng ta tăng số bước Grow và Improve, phần thưởng
của mô hình trên tập test tiếp tục tăng.

Hình 12|[WMT 2020 Zh-En]: Kết quả ReST trên tập test. Chúng ta thấy rằng ReST rõ ràng vượt trội hơn các
baseline (phải) và phần thưởng của mô hình trên tập test tăng với số bước Improve.

A.7. Thí nghiệm bổ sung

Phần này báo cáo kết quả của một số ablation trong ReST giải thích một số lựa chọn thiết kế
mà chúng tôi đã thực hiện.

Hàm mất mát GOLD so với BC trong bước Improve. Trong Hình 13, chúng tôi so sánh GOLD với hàm mất mát BC
với số bước ReST Improve tăng dần trên tập dữ liệu WMT 2020 Zh-En. Hiệu suất của
cả BC và GOLD đều cải thiện với số bước Improve, nhưng BC hoạt động tốt hơn GOLD
trong mọi thiết lập.

So sánh hàm mất mát trên IWSLT 2014 De-En. Trong Hình 14, chúng tôi so sánh hàm mất mát BVMPO, BC và GOLD
trong bước Improve trên IWSLT 2014. Kết quả nhất quán với tập dữ liệu WMT: tóm lại, ReST với
hàm mất mát BC và nhiều bước Improve vượt trội hơn các phương pháp khác.

Chọn ngưỡng dựa trên phân vị của điểm số mô hình phần thưởng. Sử dụng một ngưỡng duy nhất cho tất cả
các cặp source-candidate có thể dẫn đến tình huống không có dữ liệu huấn luyện cho một số câu nguồn (khó hơn) nhất định

--- TRANG 21 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

Hình 13|[WMT 2020 Zh-En]: Hàm mất mát GOLD vs
BC trong cải thiện multi-step. Hiệu suất của tất cả các phương pháp cải thiện với nhiều
bước hơn, nhưng hàm mất mát BC luôn hoạt động tốt hơn
các hàm mất mát RL ngoại tuyến khác mà chúng tôi đã thử.

Hình 14|[IWSLT 2014 De-En]: Hàm mất mát BVMPO,
BC, GOLD được sử dụng trong ReST. Hàm mất mát BC với
ba bước Improve mang lại kết quả tốt nhất.

Hình 15|[IWSLT 2014 De-En] phân vị: Lọc dữ liệu dựa trên phân vị của điểm phần thưởng
cho mỗi ứng viên nguồn. ReST hoạt động tốt hơn với phân vị tăng dần và hiệu suất
bão hòa sau 𝑝=90.

hoặc ngữ cảnh. Thay vào đó, chúng ta có thể lọc các câu ứng viên dựa trên phân vị của
điểm số được cho của nguồn của chúng. Cách lọc dựa trên phân vị này hiệu quả dẫn đến việc thay đổi
ngưỡng cho mỗi câu nguồn. Kết quả được trình bày trong Hình 15. Khi phân vị lọc
tăng, hiệu suất tăng, nhưng nó bắt đầu bão hòa, và lợi ích trở nên nhỏ hơn
sau 𝑝=90. Kết quả nhìn chung tương tự như những kết quả với lọc dựa trên ngưỡng toàn cục.

Chọn ngưỡng dựa trên nội suy tuyến tính giữa điểm số trung bình và tối đa của ứng viên. Một cách thay thế để chỉ định bộ lọc dựa trên một câu nguồn là tính toán
bộ lọc dựa trên điểm phần thưởng tối đa (𝑉max(𝑠𝑜𝑢𝑟𝑐𝑒)) và điểm ứng viên trung bình (𝑉mean(𝑠𝑜𝑢𝑟𝑐𝑒))
cho một câu nguồn đã cho. Sau đó, chúng ta chọn ngưỡng cho mỗi câu nguồn như 𝜏𝛾(𝑠𝑜𝑢𝑟𝑐𝑒)=
𝛾𝑉max(𝑠𝑜𝑢𝑟𝑐𝑒)+(1−𝛾)𝑉mean(𝑠𝑜𝑢𝑟𝑐𝑒). Ở mỗi bước Improve, chúng ta tăng tham số 𝛾. Chúng tôi báo cáo
kết quả đối với các giá trị 𝛾 khác nhau trong Hình 16. Tóm lại, việc tính toán ngưỡng bằng cách
nội suy điểm tối đa và trung bình cho một ứng viên đã cho mang lại kết quả tương tự như cách tính toán ngưỡng dựa trên phân vị
trên mỗi nguồn. Ngoài ra, chúng ta có thể thấy rằng lịch trình của ngưỡng

--- TRANG 22 ---
Tự Huấn Luyện Tăng Cường (ReST) cho Mô Hình Ngôn Ngữ

Hình 16|[IWSLT 2014 De-En] thí nghiệm nội suy: Ngưỡng trong bước Improve được chọn
cho mỗi câu ứng viên như 𝛾𝑉max(𝑠𝑜𝑢𝑟𝑐𝑒)+(1−𝛾)𝑉mean(𝑠𝑜𝑢𝑟𝑐𝑒). Bên trái, thí nghiệm với
I=1 cung cấp cải thiện và tăng 𝛾, điều này cũng giúp ích đáng kể. Bên phải, chúng tôi trình bày
kết quả với 𝛾 bắt đầu với 0.5 thay vì 0.5. Như có thể thấy từ hình này, số
bước Improve và cách chọn ngưỡng ảnh hưởng lớn đến hiệu suất cuối cùng của mô hình.

và số bước Improve có tác động lớn đến hiệu suất cuối cùng của mô hình.

A.8. Thuật toán Baseline

Phần này cung cấp chi tiết về các hàm mất mát baseline được sử dụng trong Hình 5.

A.8.1. BVMPO

Phương pháp BVMPO tương tự như DIME được đề xuất bởi Abdolmaleki et al. (2021). Sự khác biệt chính
là chúng tôi sử dụng hàm state-value thay vì Q function với v-trace (Espeholt et al., 2018), tương tự
như V-MPO (Song et al., 2020). Chúng tôi huấn luyện các mạng neural riêng biệt cho chính sách và hàm giá trị. Chính sách
được huấn luyện trước với BC và mạng giá trị được huấn luyện trước với BVE để ước tính giá trị hành vi
𝑉𝛽. BVMPO sử dụng chính sách behavioral cloning 𝜋𝛽(𝑎|𝑠) như một prior hành vi:

L𝐵𝑉𝑀𝑃𝑂 =L𝑉𝑀𝑃𝑂+𝜆KL(𝜋𝛽|𝜋). (4)

Trái ngược với DIME, BVMPO bắt đầu với hệ số cố định 𝜆=1 và anneal nó đến 1𝑒−5 trong
quá trình huấn luyện. Chúng tôi thấy rằng bắt đầu với 𝜆 lớn và anneal nó ổn định quá trình huấn luyện mà không có
sự giảm hiệu suất đáng chú ý. Tương tự, việc anneal hệ số regularization hành vi cũng
được chỉ ra là hiệu quả trong Q-learning ngoại tuyến (Agarwal et al., 2022).

A.8.2. GOLD

Hàm mất mát GOLD được giới thiệu bởi Pang và He (2021). Khi sử dụng nó trong bước Improve, chúng tôi bắt đầu với
một chính sách BC. Trong tất cả các thí nghiệm GOLD, chúng tôi sử dụng 𝑘=5 bước để unroll mô hình MLE vào tương lai cho
các tính toán phần thưởng. Việc tăng 𝑘 không cải thiện hiệu suất.

A.8.3. Offline Actor Critic (OAC)

Offline actor critic là một phương pháp actor critic được đề xuất bởi Mathieu et al. (2021) nơi hàm state-value
được huấn luyện để ước tính giá trị hành vi trong tập dữ liệu (Gulcehre et al., 2021). Không giống
Mathieu et al. (2021) chúng tôi không sử dụng V-trace vì nó không cải thiện so với hàm lợi thế vanilla
trong các thí nghiệm ban đầu của chúng tôi.

A.8.4. Hàm Giá trị

Chúng tôi huấn luyện các hàm state-value trên các tập dữ liệu IWSLT 2014 De-En và WMT 2020 Zh-En sau bước grow
ban đầu. Các hàm giá trị ước tính giá trị hành vi của chính sách trên tập dữ liệu huấn luyện. Các
hàm giá trị dự đoán tổng phần thưởng được chiết khấu cho mỗi từ cho đến cuối câu. Chúng tôi
sử dụng kiến trúc encoder-decoder dựa trên transformer cho các hàm giá trị. Encoder nhận
câu nguồn làm đầu vào, và decoder dự đoán Monte Carlo discounted returns có điều kiện
trên nguồn và lịch sử của các chuỗi token được tạo trong đích.

Trên các tập huấn luyện của IWSLT và WMT, các hàm giá trị của chúng tôi đạt được tương quan Kendall-tau là
92.4% và tương quan Spearman trên 99% đối với điểm số mô hình phần thưởng trong tập dữ liệu.
Vì chúng tôi chỉ thực hiện RL ngoại tuyến, các hàm giá trị không cần tổng quát hóa vượt ra ngoài tập dữ liệu huấn luyện.

A.9. ReST: Giải thích dân số

Ở đây, chúng tôi cung cấp một giải thích dân số của thuật toán ReST. Trong thiết lập này, bước huấn luyện BC
ban đầu có thể được coi như việc xác định 𝜃 bằng cách tối thiểu hóa

KL(𝑝(𝒙,𝒚)||𝜋𝜃(𝒙,𝒚))=∑︁
𝒙,𝒚𝑝(𝒙,𝒚)log𝑝(𝒙,𝒚)
𝜋𝜃(𝒙,𝒚)

=∑︁
𝒙,𝒚𝑝(𝒙,𝒚)log𝑝(𝒚|𝒙)
𝜋𝜃(𝒚|𝒙)

=𝔼𝒙∼𝑝(𝒙)[KL(𝑝(𝒚|𝒙)||𝜋𝜃(𝒚|𝒙))],

trong đó KL là độ lệch Kullback–Leibler, 𝑝(𝒙,𝒚)=𝑝(𝒙)𝑝(𝒚|𝒙) là phân phối dữ liệu (huấn luyện)
và 𝜋𝜃(𝒙,𝒚)=𝑝(𝒙)𝜋𝜃(𝒚|𝒙). Hãy gọi bộ tối thiểu hóa 𝜃0.

Giả sử bây giờ rằng người ta có tham số 𝜃𝑘 ở cuối các bước Improve của bước Grow thứ 𝑘.
Sau đó ở bước Grow thứ (𝑘+1) tiếp theo, chúng ta xem xét các bước Improve sử dụng một chuỗi ngưỡng tăng dần
(𝜏𝑖)𝐼
𝑖=1 được sử dụng để lọc phân phối dữ liệu có sẵn (hỗn hợp của phân phối dữ liệu gốc
và phân phối dữ liệu tổng hợp) để có được một phân phối mới

𝜋𝜏𝑖
𝜃𝑘(𝒙,𝒚)∝{(1−𝜆)𝑝(𝒙,𝒚)+𝜆𝜋𝜃𝑘(𝒙,𝒚)}𝕀(𝑅(𝒙,𝒚)> 𝜏𝑖).

Sau đó chúng ta có được tham số tiếp theo 𝜃 bằng cách tối thiểu hóa KL

KL(𝜋𝜏𝑖
𝜃𝑘(𝒙,𝒚)||𝜋𝜃(𝒙,𝒚))

và tiếp tục tăng ngưỡng cho đến khi phần thưởng 𝔼𝜋𝜃(𝒙,𝒚)[𝑅(𝒙,𝒚)] ngừng tăng. Điều này mang lại
𝜃𝑘+1.
