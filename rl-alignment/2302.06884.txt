# 2302.06884.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2302.06884.pdf
# File size: 1045953 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Conservative State Value Estimation for Offline
Reinforcement Learning
Liting Chen∗
McGill University
Montreal, Canada
98chenliting@gmail.comJie Yan †
Step.ai
Beijing, China
dasistyanjie@gmail.com
Zhengdao Shao∗
University of Sci. and Tech. of China
Hefei, China
zhengdaoshao@mail.ustc.edu.cnLu Wang
Microsoft
Beijing, China
wlu@microsoft.comQingwei Lin
Microsoft
Beijing, China
qlin@microsoft.com
Saravan Rajmohan
Microsoft 365
Seattle, USA
saravar@microsoft.comThomas Moscibroda
Microsoft
Redmond, USA
moscitho@microsoft.comDongmei Zhang
Microsoft
Beijing, China
dongmeiz@microsoft.com
Abstract
Offline reinforcement learning faces a significant challenge of value over-estimation
due to the distributional drift between the dataset and the current learned policy,
leading to learning failure in practice. The common approach is to incorporate a
penalty term to reward or value estimation in the Bellman iterations. Meanwhile,
to avoid extrapolation on out-of-distribution (OOD) states and actions, existing
methods focus on conservative Q-function estimation. In this paper, we propose
Conservative State Value Estimation (CSVE), a new approach that learns con-
servative V-function via directly imposing penalty on OOD states. Compared to
prior work, CSVE allows more effective state value estimation with conservative
guarantees and further better policy optimization.
Further, we apply CSVE and develop a practical actor-critic algorithm in which
the critic does the conservative value estimation by additionally sampling and
penalizing the states around the dataset, and the actor applies advantage weighted
updates extended with state exploration to improve the policy. We evaluate in
classic continual control tasks of D4RL, showing that our method performs better
than the conservative Q-function learning methods and is strongly competitive
among recent SOTA methods.
1 Introduction
Reinforcement Learning (RL) learns to act by interacting with the environment and has shown great
success in various tasks. However, in many real-world situations, it is impossible to learn from scratch
online as exploration is often risky and unsafe. Instead, offline RL([ 1,2]) avoids this problem by
learning the policy solely from historical data. Yet, simply applying standard online RL techniques
to static datasets can lead to overestimated values and incorrect policy decisions when faced with
unfamiliar or out-of-distribution (OOD) scenarios.
∗Work done during the internship at Microsoft. †Work done during full-time employment at Microsoft.
37th Conference on Neural Information Processing Systems (NeurIPS 2023).arXiv:2302.06884v2  [cs.LG]  2 Dec 2023

--- PAGE 2 ---
Recently, the principle of conservative value estimation has been introduced to tackle challenges
in offline RL[ 3–5]. Prior methods, e.g., CQL(Conservative Q-Learning [ 4]), avoid the value over-
estimation problem by systematically underestimating the Q values of OOD actions on the states in
the dataset. In practice, it is often too pessimistic and thus leads to overly conservative algorithms.
COMBO [ 6] leverages a learned dynamic model to augment data in an interpolation way. This
process helps derive a Q function that’s less conservative than CQL, potentially leading to more
optimal policies.
In this paper, we propose CSVE (Conservative State Value Estimation), a novel offline RL approach.
Unlike the above methods that estimate conservative values by penalizing the Q-function for OOD
actions, CSVE directly penalizes the V-function for OOD states. We theoretically demonstrate that
CSVE provides tighter bounds on in-distribution state values in expectation than CQL, and same
bounds as COMBO but under more general discounted state distributions, which potentially enhances
policy optimization in the data support. Our main contributions include:
•The conservative state value estimation with related theoretical analysis. We prove that it
lower bounds the real state values in expectation over any state distribution that is used
to sample OOD states and is up-bounded by the real state values in expectation over the
marginal state distribution of the dataset plus a constant term depending on sampling errors.
Compared to prior work, it enhances policy optimization with conservative value guarantees.
•A practical actor-critic algorithm implemented CSVE. The critic undertakes conservative
state value estimation, while the actor uses advantage-weighted regression(AWR) and
explores states with conservative value guarantee to improve policy. In particular, we use
a dynamics model to sample OOD states that are directly reachable from the dataset, for
efficient value penalizing and policy exploring.
•Experimental evaluation on continuous control tasks of Gym [ 7] and Adroit [ 8] in D4RL [ 9]
benchmarks, showing that CSVE performs better than prior methods based on conservative
Q-value estimation, and is strongly competitive among main SOTA algorithms.
2 Preliminaries
Offline Reinforcement Learning. Consider the Markov Decision Process M:= (S,A, P, r, ρ, γ ),
which comprises the state space S, the action space A, the transition model P:S × A → ∆(S), the
reward function r:S × A → R, the initial state distribution ρand the discount factor γ∈(0,1]. A
stochastic policy π:S → A selects an action probabilistically based on the current state. A transition
is the tuple (st, at, rt, st+1)where at∼π(·|st),st+1∼P(·|st, at), andrt=r(st, at). It’s assumed
that the reward values adhere to |r(s, a)| ≤Rmax,∀s, a. A trajectory under πis the random sequence
τ= (s0, a0, r0, s1, a1, r1, . . . , s T)which consists of continuous transitions starting from s0∼ρ.
Standard RL is to learn a policy π∈Πthat maximize the expected cumulative future rewards, repre-
sented as Jπ(M) =EM,π[P∞
t=0γtrt], through active interaction with the environment M. At any
timet, for the policy π, the value function of state is defined as Vπ(s) :=EM,π[P∞
k=0γt+krt+k|st=
s], and the Q value function is Qπ(s, a) :=EM,π[P∞
k=0γt+krt+k|st=s, at=a]. The Bellman
operator is a function projection: BπQ(s, a) := r(s, a) +γEs′∼P(·|s,a),a′∼π(·|s′)[Q(s′, a′)], or
BπV(s) :=Ea∼π(·|s)[r(s, a) +γEs′∼P(·|s,a)[V(s′)]], resulting initerative value updates. Bellman
consistency implies that Vπ(s) =BπVπ(s),∀sandQπ(s) =BπQπ(s, a),∀s, a. When employing
function approximation in practice, the empirical Bellman operator ˆBπis used, wherein the afore-
mentioned expectations are estimated with data. Offline RL aims to learn the policy πfrom a static
dataset D={(s, a, r, s′)}made up of transitions collected by any behavior policy, with the objective
of performing well in the online setting. Note that, unlike standard online RL, offline RL does not
interact with the environment during the learning process.
Conservative Value Estimation. One main challenge in offline RL arises from the over-estimation
of values due to extrapolation in unseen states and actions. Such overestimation can lead to the
deterioration of the learned policy. To address this issue, conservatism or pessimism is employed in
value estimation. For instance, CQL learns a conservative Q-value function by penalizing the value
2

--- PAGE 3 ---
of unseen actions:
ˆQk+1←arg min
Q1
2Es,a,s′∼D[(Q(s, a)−ˆβπˆQk(s, a))2]+α(Es∼D
a∼µ(·|s)[Q(s, a)]−Es∼D
a∼ˆπβ(·|s)[Q(s, a)])
(1)
where ˆπβandπare the behaviour policy and learnt policy separately, µis an arbitrary policy different
from ˆπβ, and αrepresents the factor for balancing conservatism.
Constrained Policy Optimization. To address the issues of distribution shift between the learning
policy and the behavior policy, one approach is to constrain the learning policy close to the behavior
policy [ 10–13,1]. As an example, AdvantageWeighted Regression(AWR)[ 14,12] employs an
implicit KL divergence to regulate the distance between policies:
πk+1←arg max
πE
s,a∼Dlogπ(a|s)
Z(s)exp1
λAπk(s, a)
Here, Aπkis the advantage of policy πk, and Zserves as the normalization constant for s.
Model-based Offline RL. In RL, the model is an approximation of the MDP M. Such a model
is denoted as ˆM:= (S,A,ˆP,ˆr, ρ, γ ), with ˆPandˆrbeing approximation of Pandrrespectively.
Within offline RL, the model is commomly used to augment data [ 15,6] or act as a surrogate of
the real environment during interaction [ 16].However, such practices can inadvertently introduce
bootstrapped errors over extended horizons[ 17]. In this paper, we restrict the use of the model to
one-step sampling on the next states that are approximately reachable from the dataset.
3 Conservative State Value Estimation
In the offline setting, the value overestimation is a major problem resulting in failure of learning a
reasonable policy [ 13,1]. In contrast to prior works[ 4,6] that get conservative value estimation via
penalizing Q function for OOD state-action pairs, we directly penalize V function for OOD states.
Our approach provides several novel theoretic results that allow better trade-off of conservative value
estimation and policy improvement. All proofs of our theorems can be found in Appendix A.
3.1 Conservative Off-policy Evaluation
We aim to conservatively estimate the value of a target policy using a dataset to avoid overestimation
of OOD states. To achieve this, we penalize V-values evaluated on states that are more likely to be
OOD and increase the V-values on states that are in the distribution of the dataset. This adjustment is
made iteratively::
ˆVk+1←arg min
V1
2Es∼du(s)[(ˆBπˆVk(s)−V(s))2] +α(Es′∼d(s)V(s′)−Es∼du(s)V(s)) (2)
where du(s)is the discounted state distribution of D, d(s)is any state distribution, and ˆBπis
the empirical Bellman operator (see appendix for more details). Considering the setting without
function approximation, by setting the derivative of Eq. 2 as zero, we can derive the V function using
approximate dynamic programming at iteration k::
ˆVk+1(s) =ˆBπˆVk(s)−α[d(s)
du(s)−1],∀s, k. (3)
Denote the function projection on ˆVkin Eq. 3 as Tπ. We have Lemma 3.1, which ensures that ˆVk
converges to a unique fixed point.
Lemma 3.1. For any dwithsupp d⊆supp du,Tπis aγ-contraction in L∞norm.
Theorem 3.2. For any dwithsupp d⊆supp du(d̸=du), with a sufficiently large α(i.e.,α≥
Es∼d(s)Ea∼π(a|s)Cr,t,δRmax
(1−γ)√
|D(s,a)|/Es∼d(s)[d(s)
du(s)−1])), the expected value of the estimation ˆVπ(s)
under d(s)is the lower bound of the true value, that is: Es∼d(s)[ˆVπ(s)]≤Es∼d(s)[Vπ(s)].
3

--- PAGE 4 ---
ˆVπ(s) = lim k→∞ˆVk(s)is the converged value estimation with the dataset D, andCr,t,δRmax
(1−γ)√
|D(s,a)|
is related to sampling error that arises when using the empirical operator instead of the Bellman
operator. If the counts of each state-action pair is greater than zero, |D(s, a)|denotes a vector of size
|S||A| containing counts for each state-action pair. If the counts of this state action pair is zero, the
corresponding1√
|D(s,a)|is a large yet finite value. We assume that with probability ≥1−δ, the
sampling error is less thanCr,t,δRmax
(1−γ)√
|D(s,a)|, while Cr,t,δis a constant (See appendix for more details.)
Note that if the sampling error can be disregarded, α >0can ensure the lower bound results.
Theorem 3.3. The expected value of the estimation, ˆVπ(s), under the state distribution of the original
dataset is the lower bound of the true value plus the term of irreducible sampling error. Formally:
Es∼du(s)[ˆVπ(s)]≤Es∼du(s)[Vπ(s)] +Es∼du(s)(I−γPπ)−1Ea∼π(a|s)Cr,t,δRmax
(1−γ)√
|D(s,a)|.
where Pπrefers to the transition matrix coupled with policy π(see Appendix for details).
Now we show that, during iterations, the gap between the estimated V-function values of in-
distribution states and OOD states is higher compared to the true V-functions.
Theorem 3.4. For any iteration k, given a sufficiently large α, our method amplifies the difference in
expected V-values between the selected state distribution and the dataset state distribution. This can
be represented as: Es∼du(s)[ˆVk(s)]−Es∼d(s)[ˆVk(s)]>Es∼du(s)[Vk(s)]−Es∼d(s)[Vk(s)].
Our approach, which penalizes the V-function for OOD states, promotes a more conservative estimate
of a target policy’s value in offline reinforcement learning. Consequently, our policy extraction
ensures actions align with the dataset’s distribution.
To apply our approach effectively in offline RL algorithms, the preceding theorems serve as guiding
principles. Here are four key insights for practical use of Eq. 2:
Remark 1. According to Eq. 2, if d=du, the penalty for OOD states diminishes. This means that
the policy will likely avoid states with limited data support, preventing it from exploring unseen
actions in such states. While AWAC [ 12]employs this configuration, our findings indicate that by
selecting a d, our method surpasses AWAC’s performance.
Remark 2. Theorem 3.3 suggests that under du, the marginal state distribution of data, the expectation
estimated value of Vπis either lower than its true value or exceed it, but within a certain limit. This
understanding drives our adoption of the advantage-weighted policy update, as illustrated in Eq. 9.
Remark 3. As per Theorem 3.2, the expected estimated value of a policy under d, which represents
the discounted state distribution of any policy, must be a lower bound of its true value. Grounded
in this theorem, our policy enhancement strategy merges an advantage-weighted update with an
additional exploration bonus, showcased in Eq. 10.
Remark 4. Theorem 3.4 states Es∼d(s)[Vk(s)]−Es∼d(s)[ˆVk(s)]>Es∼du(s)[Vk(s)]−
Es∼du(s)[ˆVk(s)]. In simpler terms, the underestimation of value is more pronounced under d.
With the proper choice of d, we can confidently formulate a newer and potentially superior policy
using ˆVk. Our algorithm chooses the distribution of model predictive next-states as d, i.e., s′∼dis
implemented by s∼D, a∼π(·|s), s′∼ˆP(·|s, a), which effectively builds a soft ’river’ with low
values encircling the dataset.
Comparison with prior work: CQL (Eq.1), which penalizes Q-function of OOD actions,
guarantees the lower bounds on state-wise value estimation: ˆVπ(s) = Eπ(a|s)(ˆQπ(s, a))≤
Eπ(a|s)(Qπ(s, a)) = Vπ(s)for all s∈D. COMBO, which penalizes the Q-function for OOD
states and actions of interpolation of history data and model-based roll-outs, guarantees the lower
bound of state value expectation: Es∼µ0[ˆVπ(s)]≤Es∼µ0[Vπ(s)]where µ0is the initial state dis-
tribution (Remark 1, section A.2 of COMBO [ 6]); which is a special case of our result in Theorem
3.2 when d=µ0. Both CSVE and COMBO intend to enhance performance by transitioning from
individual state values to expected state values. However, CSVE offers the same lower bounds but
under a more general state distribution. Note that µ0depends on the environment or the dynamic
model during offline training. CSVE’s flexibility, represented by d, ensures conservative guarantees
across any discounted state distribution of the learned policy, emphasizing a preference for penalizing
Vover the Q-function.
4

--- PAGE 5 ---
3.2 Safe Policy Improvement Guarantees
Now we show that our method has the safe policy improvement guarantees against the data-implied
behaviour policy. We first show that our method optimizes a penalized RL empirical objective:
Theorem 3.5. LetˆVπbe the fixed point of Eq. 3, then π∗(a|s) = arg maxπˆVπ(s)is equivalently
obtained by solving:
π∗←arg max
πJ(π,ˆM)−α
1−γE
s∼dπ
ˆM[d(s)
du(s)−1]. (4)
Building upon Theorem 3.5, we show that our method provides a ζ-safe policy improvement over πβ.
Theorem 3.6. Letπ∗(a|s)be the policy obtained in Eq. 4. Then, it is a ζ-safe policy improvement
overˆπβin the actual MDP M, i.e., J(π∗, M)≥J(ˆπβ, M)−ζwith high probability 1- δ, where ζis
given by:
ζ=2(Cr,δ
1−γ+γRmaxCT,δ
(1−γ)2)E
s∼dπ
ˆM(s)"
cs
E
a∼π(a|s)[π(a|s)
πβ(a|s)]#
−(J(π∗,ˆM)−J(ˆπβ,ˆM))| {z }
≥α1
1−γEs∼dπ
ˆM(s)[d(s)
du(s)−1]where c=p
|A|/p
|D(s)|.
4 Methodology
In this section, we propose a practical actor-critic algorithm that employs CSVE for value estimation
and extends Advantage Weighted Regression[ 18] with out-of-sample state exploration for policy
improvement. In particular, we adopt a dynamics model to sample OOD states during conservative
value estimation and exploration during policy improvement. The implementation details are in
Appendix B. Besides, we discuss the general technical choices of applying CSVE into algorithms.
4.1 Conservative Value Estimation
Given a dataset Dacquired by the behavior policy πβ, our objective is to estimate the value function
Vπfor a target policy π. As stated in section 3, to prevent the value overestimation, we learn a
conservative value function ˆVπthat lower bounds the real values of πby adding a penalty for OOD
states within the Bellman projection sequence. Our method involves iterative updates of Equations 5 -
7, where ˆQkis the target network of ˆQk.
ˆVk+1←arg min
VLπ
V(V;ˆQk) (5)
=Es∼Dh
(Ea∼π(·|s)[ˆQk(s, a)]−V(s))2i
+α 
Es∼D,a∼π(·|s)
s′∼ˆP(s,a)[V(s′)]−Es∼D[V(s)]!
ˆQk+1←arg min
QLπ
Q(Q;ˆVk+1) = E
s,a,s′∼D
r(s, a) +γˆVk+1(s′)−Q(s, a)2
(6)
ˆQk+1←(1−ω)ˆQk+ωˆQk+1(7)
The RHS of Eq. 5 is an approximation of Eq. 2, with the first term representing the standard TD
error. In this term, the target state value is estimated by taking the expectation of ˆQkovera∼π,
and the second term penalizes the value of OOD states. In Eq. 6, the RHS is TD errors estimated on
transitions in the dataset D. Note that the target term is the sum of the reward r(s, a)and the next
step state’s value ˆVk+1(s′). In Eq. 7, the target Q values are updated with a soft interpolation factor
ω∈(0,1).ˆQkchanges slower than ˆQk, which makes the TD error estimation in Eq. 5 more stable.
Constrained Policy. Note that in RHS of Eq. 5, we use a∼π(·|s)in expectation. To safely
estimate the target value of V(s)byEa∼π(·|s)[ˆQ(s, a)], we almost always requires supp( π(·|s))⊂
5

--- PAGE 6 ---
supp( πβ(·|s)). We achieve this by the advantage weighted policy update , which forces π(·|s)to
have significant probability mass on actions taken by πβin data, as detailed in section 3.2.
Model-based OOD State Sampling. In Eq. 5, we implement the state sampling process s′∼din
Eq. 2 as a flow of {s∼D;a∼π(a|s), s′∼ˆP(s′|s, a)}, that is the distribution of the predictive
next-states from Dby following π. This approach proves beneficial in practice. On the one hand,
this method is more efficient as it samples only the states that are approximately reachable from
Dby one step, rather than sampling the entire state space. On the other hand, we only need the
model to do one-step prediction such that it introduces no bootstrapped errors from long horizons.
Following previous work [ 17,15,6], We use an ensemble of deep neural networks, represented
aspθ1, . . . , pθB, to implement the probabilistic dynamics model. Each neural network produces a
Gaussian distribution over the next state and reward: Pi
θ(st+1, r|st, at) =N(ui
θ(st, at), σi
θ(st, at)).
Adaptive Penalty Factor α.The pessimism level is controlled by the parameter α≥0. In practice,
we set αadaptive during training as follows, which is similar to that in CQL([4])
max
α≥0[α(Es′∼d[Vψ(s′)]−Es∼D[Vψ(s)]−τ)], (8)
where τis a budget parameter. If the expected difference in V-values is less than τ,αwill decrease.
Otherwise, αwill increase, penalizing the OOD state values more aggressively.
4.2 Advantage Weighted Policy Update
After learning the conservative ˆVk+1andˆQk+1(orˆVπandˆQπwhen the values have converged),
we improve the policy by the following advantage weighted update [12].
π←arg min
π′Lπ(π′) =−E
s,a∼Dh
logπ′(a|s) exp
βˆAk+1(s, a)i
(9)
where ˆAk+1(s, a) = ˆQk+1(s, a)−ˆVk+1(s). Eq.9 updates the policy πby applying a weighted
maximum likelihood method. This is computed by re-weighting state-action samples in Dusing
the estimated advantage ˆAk+1. It avoids explicit estimation of the behavior policy, and its resulting
sampling errors, which is an important issue in offline RL [12, 4].
Implicit policy constraints. We adopt the advantage-weighted policy update which imposes an
implicit KL divergence constraint between πandπβ. This policy constraint is necessary to guarantee
that the next state s′in Eq. 5 can be safely generated through policy π. As derived in [ 12] (Appendix
A), Eq. 9 is a parametric solution of the following problem (where ϵdepends on β):
max
π′Ea∼π′(·|s)[ˆAk+1(s, a)]
s.t.DKL(π′(·|s)||πβ(·|s))≤ϵ,Z
aπ′(a|s)da= 1.
Note that DKL(π′||πβ)is a reserved KL divergence with respect to π′, which is mode-seeking [ 19].
When treated as Lagrangian it forces π′to allocate its probability mass to the maximum likelihood
supports of πβ, re-weighted by the estimated advantage. In other words, for the space of Awhere
πβ(·|s)has no samples, π′(·|s)has almost zero probability mass too.
Model-based Exploration on Near States. As suggested by remarks in Section 3.1, in practice,
allowing the policy to explore the predicted next states transition (s∼D)following a∼π′(·|s))
leads to better test performance. With this kind of exploration, the policy is updated as follows:
π←arg min
π′Lπ(π′)−λEs∼D,a∼π′(s)
s′∼ˆP(s,a)h
r(s, a) +γˆVk+1(s′)i
.(10)
The second term is an approximation to Es∼dπ(s)[Vπ(s)]. The optimization of this term involves
calculating the gradient through the learned dynamics model. This is achieved by employing analytic
gradients through the learned dynamics to maximize the value estimates. It is important to note that the
value estimates rely on the reward and value predictions, which are dependent on the imagined states
and actions. As all these steps are implemented using neural networks, the gradient is analytically
computed using stochastic back-propagation, a concept inspired by Dreamer[ 20]. We adjust the value
ofλ, a hyper-parameter, to balance between optimistic policy optimization (in maximizing V) and
the constrained policy update (as indicated by the first term).
6

--- PAGE 7 ---
Table 1: Performance comparison on Gym control tasks v2. The results of CSVE are over ten seeds
and we reimplement AWAC using d3rlpy. Results of IQL, TD3-BC, and PBRL are from their original
papers ( Table 1 in [ 21], Table C.3 in [ 22], and Table 1 in [ 10] respectively). Results of COMBO and
CQL are from the reproduction results in [ 23] (Table 1) and [ 10] respectively, since their original
results were reported on v0 datasets.
AWAC CQL CQL-AWR COMBO IQL TD3-BC PBRL CSVERandomHalfCheetah 13.7 17.5±1.5 16 .9±1.5 38.8 18.2 11.0±1.1 13 .1±1.2 26 .8±1.5
Hopper 8.7 7.9±0.4 8 .7±0.5 17.9 16.3 8.5±0.6 31.6±0.3 26 .1±7.6
Walker2D 2.2 5.1±1.3 0 .0±1.6 7.0 5.5 1.6±1.7 8.8±6.3 6 .2±0.8MediumHalfCheetah 50.0 47.0±0.5 50 .9±0.6 54.2 47.4 48.3±0.3 58.2±1.5 48 .4±0.3
Hopper 97.5 53.0±28.5 25 .7±37.4 94.9 66.3 59.3±4.2 81 .6±14.596.7±5.7
Walker2D 89.1 73 .3±17.7 62 .4±24.4 75.5 78.3 83.7±2.1 90.3±1.2 83 .2±1.0Medium
ReplayHalfCheetah 44.9 45.5±0.7 40 .0±0.4 55.1 44.2 44.6±0.5 49 .5±0.8 54.5±0.6
Hopper 99.4 88.7±12.9 91 .0±13.0 73.1 94.7 60.9±18.8100.7±0.4 91 .7±0.2
Walker2D 80.0 83.3±2.7 66 .7±12.1 56.0 73.9 81.8±5.5 86.2±3.4 78 .0±1.5Medium
ExpertHalfCheetah 62.8 75.6±25.7 73 .4±2.0 90.0 86.7 90.7±4.3 93.1±0.2 93.1±0.3
Hopper 87.2 105.6±12.9 102 .2±7.7 111.1 91.5 98.0±9.4111.2±0.7 94 .1±3.0
Walker2D 109.8 107.9±1.6 98 .0±21.7 96.1 109.6 110.1±0.5109.8±0.2109.0±0.1ExpertHalfCheetah 20.0 96.3±1.3 87 .3±8.1 - 94.6 96.7±1.1 96.2±2.393.8±0.1
Hopper 111.6 96 .5±28.0110.0±2.5 - 109.0 107.8±7110.4±0.3111.3±0.6
Walker2D 110.6 108.5±0.5 75 .1±60.7 - 109.4110.2±0.3108.8±0.2108.5±0.1
Average 65.8 67.4 60.6 64.1 69.7 67.5 76.7 74.8
4.3 Discussion on implementation choices
Now we examine the technical considerations for implementing CSVE in a practical algorithm.
Constraints on Policy Extraction. It is important to note that the state value function alone does not
suffice to directly derive a policy. There are two methods for extracting a policy from CSVE. The first
method is model-based planning, i.e., π←arg maxπEs∼d,a∼π(·|s)[ˆr(s, a) +γEs′∼ˆP(s,a)[V(s′)],
which involves finding the policy that maximizes the expected future return. However, this method
heavily depends on the accuracy of a model and is difficult to implement in practice. As an alternative,
we suggest the second method, which learns a Q value or advantage function from the V value
function and experience data, and then extracts the policy. Note that CSVE provides no guarantees
for conservative estimation on OOD actions, which can cause normal policy extraction methods
such as SAC to fail. To address this issue, we adopt policy constraint techniques. On the one hand,
during the value estimation in Eq.5, all current states are sampled from the dataset, while the policy
is constrained to be close to the behavior policy (ensured via Eq.9). On the other hand, during the
policy learning in Eq.10, we use AWR [ 18] as the primary policy extraction method (first term of
Eq.10), which implicitly imposes policy constraints and the additional action exploration (second
term of Eq.10) is strictly applied to states in the dataset. This exploration provides a bonus to actions
that: (1) themselves and their model-predictive next-states are both close to the dataset (ensured by
the dynamics model), and (2) their values are favorable even with conservatism.
Taking Advantage of CSVE. As outlined in Section 3.1, CSVE allows for a more relaxed lower
bound on conservative value estimation compared to conservative Q values, providing greater potential
for improving the policy. To take advantage of this, the algorithm should enable exploration of out-
of-sample but in-distribution states, as described in Section 3. In this paper, we use a deep ensemble
dynamics model to support this speculative state exploration, as shown in Eq. 10. The reasoning
behind this is as follows: for an in-data state sand any action a∼π(·|s), if the next state s′is in-data
or close to the data support, its value is reasonably estimated, and if not, its value has been penalized
according to Eq.5. Additionally, the deep ensemble dynamics model captures epistemic uncertainty
well, which can effectively cancel out the impact of rare samples of s′. By utilizing CSVE, our
algorithm can employ the speculative interpolation to further improve the policy. In contrast, CQL
and AWAC do not have this capability for such enhanced policy optimization.
7

--- PAGE 8 ---
Table 2: Performance comparison on Adroit tasks. The results of CSVE are over ten seeds. Results
of IQL are from Table 3 in [21] and results of other algorithms are from Table 4 in [10].
AWAC BC BEAR UWAC CQL CQL-AWR IQL PBRL CSVEHumanPen 18.7 34.4 -1.0 10.1±3.2 37.5 8.4±7.1 71 .5 35 .4±3.3 106.2±5.0
Hammer -1.8 1.5 0.3 1.2±0.7 4.4 0.3±0.0 1.4 0.4±0.3 3 .5±2.6
Door -1.8 0.5 −0.3 0 .4±0.2 9.9 3.5±1.8 4.3 0.1±0.0 2 .8±2.4
Relocate -0.1 0.0 -0.3 0.0±0.0 0.2 0.1±0.0 0.1 0.0±0.0 0 .1±0.0ClonedPen 27.2 56.9 26.5 23.0±6.9 39.2 29.3±7.1 37 .3 74.9±9.8 54 .5±5.4
Hammer -1.8 0.8 0.3 0.4±0.0 2.1 0.31±0.06 2.1 0.8±0.5 0 .5±0.2
Door -2.1 -0.1 −0.1 0 .0±0.0 0.4 −0.2±0.1 1.6 4.6±4.8 1 .2±1.0
Relocate -0.4 -0.1 -0.3 −0.3±0.2 -0.1 −0.3±0.0 0.0 −0.1±0.0−0.3±0.0ExpertPen 60.9 85.1 105.9 98.2±9.1 107.0 47.1±6.8 117.2 135.7±3.4144.0±9.4
Hammer 31.0 125.6 127.3 107.7±21.7 86.7 0.2±0.0 124.1127.5±0.2126.5±0.3
Door 98.1 34.9 103.4 104.7±0.4 101.5 85.0±15.9105.2 95 .7±12.2104.2±0.8
Relocate 49.0 101.3 98.6 105.5±3.2 95.0 7.2±12.5105.9 84 .5±12.2102.9±0.9
Average 23.1 36.7 38.4 37.6 40.3 15.1 47.6 46.6 53.8
5 Experiments
This section evaluates the effectiveness of our proposed CSVE algorithm for conservative value
estimation in offline RL. In addition, we aim to compare the performance of CSVE with state-of-the-
art (SOTA) algorithms. To achieve this, we conduct experimental evaluations on a variety of classic
continuous control tasks of Gym[7] and Adroit[8] in the D4RL[9] benchmark.
Our compared baselines include: (1) CQL[ 4] and its variants, CQL-AWR (Appendix D.2) which uses
AWR with extra in-sample exploration as policy extractor, COMBO[ 6] which extends CQL with
model-based rollouts; (2) AWR variants, including AWAC[ 12] which is a special case of our algorithm
with no value penalization (i.e., d=duin Eq. 2) and exploration on OOD states, IQL[ 21] which
adopts expectile-based conservative value estimation; (3) PBRL[ 10], a strong algorithm in offline
RL, but is quite costly on computation since it uses the ensemble of hundreds of sub-models; (4)
other SOTA algorithms with public performance results or high-quality open source implementations,
including TD3-BC[ 22], UWAC[ 24] and BEAR[ 25]). Comparing with CQL variants allows us to
investigate the advantages of conservative estimation on state values over Q values. By comparing
with AWR variants, we distinguish the performance contribution of CSVE from the AWR policy
extraction used in our implementation.
5.1 Overall Performance
Evaluation on the Gym Control Tasks. Our method, CSVE, was trained for 1 million steps and
evaluated. The results are shown in Table 1.Compared to CQL, CSVE outperforms it in 11 out of
15 tasks, with similar performance on the remaining tasks. Additionally, CSVE shows a consistent
advantage on datasets that were generated by following random or sub-optimal policies (random and
medium). The CQL-AWR method showed slight improvement in some cases, but still underperforms
compared to CSVE. When compared to COMBO, CSVE performs better in 7 out of 12 tasks and
similarly or slightly worse on the remaining tasks, which highlights the effectiveness of our method’s
better bounds on V . Our method has a clear adcantage in extracting the best policy on medium and
medium-expert tasks. Overall, our results provide empirical evidence that using conservative value
estimation on states, rather than Q, leads to improved performance in offline RL. CSVE outperforms
AWAC in 9 out of 15 tasks, demonstrating the effectiveness of our approach in exploring beyond
the behavior policy. Additionally, our method excels in extracting the optimal policy on data with
mixed policies (medium-expert) where AWAC falls short. In comparison to IQL, our method achieves
higher scores in 7 out of 9 tasks and maintains comparable performance in the remaining tasks.
Furthermore, despite having a significantly lower model capacity and computation cost, CSVE
outperforms TD3-BC and is on par with PBRL. These results highlight the effectiveness of our
conservative value estimation approach.
Evaluation on the Adroit Tasks. In Table 2, we report the final evaluation results after training
0.1 million steps. As shown, our method outperforms IQL in 8 out of 12 tasks, and is competitive
with other algorithms on expert datasets. Additionally, we note that CSVE is the only method that
8

--- PAGE 9 ---
can learn an effective policy on the human dataset for the Pen task, while maintaining medium
performance on the cloned dataset. Overall, our results empirically support the effectiveness of our
proposed tighter conservative value estimation in improving offline RL performance.
5.2 Ablation Study
Effect of Exploration on Near States. We analyze the impact of varying the factor λin Eq. 10,
which controls the intensity on such exploration. We investigated λvalues of {0.0,0.1,0.5,1.0}in
the medium tasks, fixing β= 0.1. The results are plotted in Fig. 1. As shown in the upper figures, λ
has an obvious effect on policy performance and variances during training. With increasing λfrom
0, the converged performance gets better in general. However, when the value of λbecomes too
large (e.g., λ= 3 for hopper and walker2d), the performance may degrade or even collapse. We
further investigated the Lπloss as depicted in the bottom figures of Eq. 9, finding that larger λvalues
negatively impact Lπ; however, once Lπconverges to a reasonable low value, larger λvalues lead to
performance improvement.
0 200 400 600 800 1000
grandient steps(thousands)01020304050score
halfcheetah-medium
0.0
0.1
0.5
1.0
3.0
0 200 400 600 800 1000
grandient steps(thousands)020406080100
hopper-medium
0.0
0.1
0.5
1.0
3.0
0 200 400 600 800 1000
grandient steps(thousands)020406080
walker2d-medium
0.0
0.1
0.5
1.0
3.0
0 200 400 600 800 1000
grandient steps(thousands)050100150200250300350loss
halfcheetah-medium
0.0
0.1
0.5
1.0
3.0(*1e3)
0 200 400 600 800 1000
grandient steps(thousands)0200400600800
hopper-medium
0.0
0.1
0.5
1.0
3.0(*1e3)
0 200 400 600 800 1000
grandient steps(thousands)60
40
20
02040
walker2d-medium
0.0
0.1
0.5
1.0
3.0
Figure 1: Effect of λto performance scores (upper figures) and Lπlosses (bottom figures) in Eq. 9
on medium tasks.
Effect of In-sample Policy Optimization. We examined the impact of varying the factor βin Eq. 9
on the balance between behavior cloning and in-sample policy optimization. We tested different β
values on mujoco medium datasets, as shown in Fig.2. The results indicate that βhas a significant
effect on the policy performance during training. Based on our findings, a value of β= 3.0was
found to be suitable for medium datasets. Additionally, in our implementation, we use β= 3.0for
random and medium tasks, and β= 0.1for medium-replay, medium-expert, and expert datasets.
More details can be found in the ablation study in the appendix.
6 Related work
The main idea behind offline RL algorithms is to incorporate conservatism or regularization into the
online RL algorithms. Here, we briefly review prior work and compare it to our approach.
Conservative Value Estimation: Prior offline RL algorithms regularize the learning policy to
be close to the data or to an explicitly estimated behavior policy. and penalize the exploration
9

--- PAGE 10 ---
0 200 400 600 800 1000
grandient steps(thousands)01020304050score
0.1
3.0
10.0Halfcheetah-medi um-v2
0 200 400 600 800 1000
grandient steps(thousands)020406080
0.1
3.0
10.0Hopper-medium-v2
0 200 400 600 800 1000
grandient steps(thousands)1020304050607080
0.1
3.0
10.0Walker2d-medium-v2Figure 2: Effect of βto performance scores on medium tasks.
ofthe OOD region, via distribution correction estimation [ 26,27], policy constraints with support
matching [ 11] and distributional matching [ 1,25], applying policy divergence based penalty on
Q-functions [ 28,29] or uncertainty-based penalty [ 30] on Q-functions and conservative Q-function
estimation [ 4]. Besides, model-based algorithms [ 15] directly estimate dynamics uncertainty and
translate it into reward penalty. Different from this prior work that imposes conservatism on state-
action pairs or actions, ours directly does such conservative estimation on states and requires no
explicit uncertainty quantification.
In-Sample Algorithms: AWR [ 18] updates policy constrained on strictly in-sample states and
actions, to avoid extrapolation on out-of-support points. IQL[ 21] uses expectile-based regression
to do value estimation and AWR for its policy updates. AWAC[ 12], whose actor is AWR, is an
actor-critic algorithm to accelerate online RL with offline data. The major drawback of AWR method
when used for offline RL is that the in-sample policy learning limits the final performance.
Model-Based Algorithms: Model-based offline RL learns the dynamics model from the static dataset
and uses it to quantify uncertainty [ 15], data augmentation [ 6] with roll-outs, or planning [ 16,31].
Such methods typically rely on wide data coverage when planning and data augmentation with
roll-outs, and low model estimation error when estimating uncertainty, which is difficult to satisfy in
reality and leads to policy instability. Instead, we use the model to sample the next-step states only
reachable from data, which has no such strict requirements on data coverage or model bias.
Theoretical Results: Our theoretical results are derived from conservative Q-value estimation (CQL)
and safe policy improvement [ 32]. Compared to offline policy evaluation[ 33], which aims to provide
a better estimation of the value function, we focus on providing a better lower bound. Additionally,
hen the dataset is augmented with model-based roll-outs, COMBO [ 6] provides a more conservative
yet tighter value estimation than CQL. CSVE achives the same lower bounds as COMBO but under
more general state distributions.
7 Conclusions
In this paper, we propose CSVE, a new approach for offline RL based on conservative value estimation
on states. We demonstrated how its theoretical results can lead to more effective algorithms. In
particular, we develop a practical actor-critic algorithm, in which the critic achieves conservative
state value estimation by incorporating the penalty of the model predictive next-states into Bellman
iterations, and the actor does the advantage-weighted policy updates enhanced via model-based state
exploration. Experimental evaluation shows that our method performs better than alternative methods
based on conservative Q-function estimation and is competitive among the SOTA methods, thereby
validating our theoretical analysis. Moving forward, we aim to delve deeper into designing more
powerful algorithms grounded in conservative state value estimation.
10

--- PAGE 11 ---
References
[1]Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning
without exploration. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of
the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine
Learning Research , pages 2052–2062. PMLR, 09–15 Jun 2019.
[2]Sascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In
Reinforcement learning , pages 45–73. Springer, 2012.
[3]Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, and Yuejie Chi. Pessimistic q-learning for offline
reinforcement learning: Towards optimal sample complexity. In Kamalika Chaudhuri, Stefanie
Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the
39th International Conference on Machine Learning , volume 162 of Proceedings of Machine
Learning Research , pages 19967–20025. PMLR, 17–23 Jul 2022.
[4]Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for
offline reinforcement learning. Advances in Neural Information Processing Systems , 33:1179–
1191, 2020.
[5]Jacob Buckman, Carles Gelada, and Marc G Bellemare. The importance of pessimism in
fixed-dataset policy optimization. In International Conference on Learning Representations ,
2020.
[6]Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea
Finn. Combo: Conservative offline model-based policy optimization. Advances in neural
information processing systems , 34:28954–28967, 2021.
[7]Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,
and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540 , 2016.
[8]Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel
Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement
learning and demonstrations. arXiv preprint arXiv:1709.10087 , 2017.
[9]Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for
deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219 , 2020.
[10] Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhi-Hong Deng, Animesh Garg, Peng Liu, and
Zhaoran Wang. Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning.
InInternational Conference on Learning Representations , 2021.
[11] Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement
learning. arXiv preprint arXiv:1911.11361 , 2019.
[12] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online
reinforcement learning with offline datasets, 2020.
[13] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning:
Tutorial, review, and perspectives on open problems, 2020.
[14] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177 , 2019.
[15] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural
Information Processing Systems , 33:14129–14142, 2020.
[16] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel:
Model-based offline reinforcement learning. Advances in neural information processing systems ,
33:21810–21823, 2020.
[17] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model:
Model-based policy optimization. Advances in Neural Information Processing Systems , 32,
2019.
11

--- PAGE 12 ---
[18] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. CoRR , abs/1910.00177, 2019.
[19] Jonathon Shlens. Notes on kullback-leibler divergence and likelihood. CoRR , abs/1404.2000,
2014.
[20] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control:
Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603 , 2019.
[21] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit
q-learning. In International Conference on Learning Representations , 2021.
[22] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.
Advances in neural information processing systems , 34:20132–20145, 2021.
[23] Marc Rigter, Bruno Lacerda, and Nick Hawes. Rambo-rl: Robust adversarial model-based
offline reinforcement learning. Advances in Neural Information Processing Systems , 2022.
[24] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov,
and Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. arXiv
preprint arXiv:2105.08140 , 2021.
[25] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-
policy q-learning via bootstrapping error reduction. Advances in Neural Information Processing
Systems , 32, 2019.
[26] Bo Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesv ´ari, and Dale Schuurmans.
Coindice: Off-policy confidence interval estimation. Advances in neural information processing
systems , 33:9398–9411, 2020.
[27] Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation
via the regularized lagrangian. Advances in Neural Information Processing Systems , 33:6551–
6561, 2020.
[28] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement
learning with fisher divergence critic regularization. In International Conference on Machine
Learning , pages 5774–5783. PMLR, 2021.
[29] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E
Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized
regression. Advances in Neural Information Processing Systems , 33:7768–7778, 2020.
[30] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective
on offline reinforcement learning. In International Conference on Machine Learning , pages
104–114. PMLR, 2020.
[31] Xiong-Hui Chen, Yang Yu, Qingyang Li, Fan-Ming Luo, Zhiwei Qin, Wenjie Shang, and
Jieping Ye. Offline model-based adaptable policy learning. Advances in Neural Information
Processing Systems , 34:8432–8443, 2021.
[32] Romain Laroche, Paul Trichelair, and Remi Tachet Des Combes. Safe policy improvement with
baseline bootstrapping. In International Conference on Machine Learning , pages 3652–3661.
PMLR, 2019.
[33] Carles Gelada and Marc G Bellemare. Off-policy deep reinforcement learning by bootstrapping
the covariate shift. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 33,
pages 3647–3655, 2019.
[34] Takuma Seno and Michita Imai. d3rlpy: An offline deep reinforcement learning library. arXiv
preprint arXiv:2111.03788 , 2021.
[35] Yuwei Fu, Di Wu, and Benoit Boulet. A closer look at offline rl agents. In Advances in Neural
Information Processing Systems , 2022.
12

--- PAGE 13 ---
A Proofs
We first redefine notation for clarity and then provide the proofs of the results from the main paper.
Notation . Let k∈Ndenotes an iteration of policy evaluation(in Section 3.2). Vkdenotes the
true, tabular (or functional) V-function iterate in the MDP. ˆVkdenotes the approximate tabular (or
functional) V-function iterate.
The empirical Bellman operator is defined as:
(ˆBπˆVk)(s) =Ea∼π(a|s)ˆr(s, a) +γX
s′Ea∼π(a|s)ˆP(s′|s, a)[ˆVk(s′)] (11)
where ˆr(s, a)is the empirical average reward derived from the dataset when performing action aat
states. The true Bellman operator is given by:
(BπVk)(s) =Ea∼π(a|s)r(s, a) +γX
s′Ea∼π(a|s)P(s′|s, a)[Vk(s′)] (12)
Now we first prove that the iteration in Eq.2 has a fixed point. Assume the state value function is
lower bounded, i.e., V(s)≥C∀s∈S, then Eq.2 can always be solved with Eq.3. Thus, we only
need to investigate the iteration in Eq.3.
Defining this iteration as a function operator TπonVand supposing that supp d⊆supp du, it’s
evident that the operator Tπdisplays a γ-contraction within L∞norm where γis the discounting
factor.
Proof of Lemma 3.1: LetVandV′be any two state value functions with the same support, i.e.,
suppV=suppV′.
|(TπV− TπV′)(s)|=(ˆBπV(s)−α[d(s)
du(s)−1])−(ˆBπV′(s)−α[d(s)
du(s)−1])
=ˆBπV(s)−ˆBπV′(s)
=|(Ea∼π(a|s)ˆr(s, a) +γEa∼π(a|s)X
s′ˆP(s′|s, a)V(s′))
−(Ea∼π(a|s)ˆr(s, a) +γEa∼π(a|s)X
s′ˆP(s′|s, a)V′(s′))|
=γEa∼π(a|s)X
s′ˆP(s′|s, a)[V(s′)−V′(s′)]
||TπV− TπV′||∞= max
s|(TπV− TπV′)(s)|
= max
sγEa∼π(a|s)X
s′ˆP(s′|s, a)[V(s′)−V′(s′)]
≤γEa∼π(a|s)X
s′ˆP(s′|s, a) max
s′′|V(s′′)−V′(s′′)|
=γmax
s′′|V(s′′)−V′(s′′)|
=γ||(V−V′)||∞
We provide a bound on the difference between the empirical Bellman operator and the true Bellman
operator. Following previous work [ 4], we make the following assumptions. Let Pπbe the transition
matrix associated with the policy, specifically, PπV(s) =Ea′∼π(a′|s′),s′∼P(s′|s,a′)[V(s′)]
13

--- PAGE 14 ---
Assumption A.1. ∀s, a∈ M , the relationship below hold with at least a (1−δ)(δ∈(0,1))
probability,
|r−r(s, a)| ≤Cr,δp
|D(s, a)|,||ˆP(s′|s, a)−P(s′|s, a)||1≤CP,δp
|D(s, a)|(13)
Given this assumption, the absolute difference between the empirical Bellman operator and the true
one can be deduced as follows:
|(ˆBπ)ˆVk−(Bπ)ˆVk)|=Ea∼π(a|s)|r−r(s, a) +γX
s′Ea′∼π(a′|s′)(ˆP(s′|s, a)−P(s′|s, a))[ˆVk(s′)]|
(14)
≤Ea∼π(a|s)|r−r(s, a)|+γ|X
s′Ea′∼π(a′|s′)(ˆP(s′|s, a′)−P(s′|s, a′))[ˆVk(s′)]|
(15)
≤Ea∼π(a|s)Cr,δ+γCP,δ2Rmax/(1−γ)p
|D(s, a)|(16)
The error in estimation due to sampling can therefore be bounded by a constant, dependent on Cr,δ
andCt,δ. We define this constant as Cr,T,δ.
Thus we obtain:
∀V, s∈D,|ˆBπV(s)− BπV(s)| ≤Ea∼π(a|s)Cr,t,δ
(1−γ)p
|D(s, a)|(17)
Next we provide an important lemma.
Lemma A.2. (Interpolation Lemma) For any f∈[0,1], and any given distribution ρ(s), letdfbe
an f-interpolation of ρand D, i.e., df(s) :=fd(s) + (1 −f)ρ(s), letv(ρ, f) :=Es∼ρ(s)[ρ(s)−d(s)
df(s)],
thenv(ρ, f)satisfies v(ρ, f)≥0.
The proof can be found in [6]. By setting fas 1, we have Es∼ρ(s)[ρ(s)−d(s)
d(s)]>0.
Proof of Theorem 3.2: The V function of approximate dynamic programming in iteration kcan be
obtained as:
ˆVk+1(s) =ˆBπˆVk(s)−α[d(s)
du(s)−1]∀s, k (18)
The fixed point:
ˆVπ(s) =ˆBπˆVπ(s)−α[d(s)
du(s)−1]≤ BπˆVπ(s) +Ea∼π(a|s)Cr,t,δRmax
(1−γ)p
|D(s, a)|−α[d(s)
du(s)−1]
(19)
Thus we obtain:
ˆVπ(s)≤Vπ(s) + (I−γPπ)−1Ea∼π(a|s)Cr,t,δRmax
(1−γ)p
|D(s, a)|−α(I−γPπ)−1[d(s)
du(s)−1](20)
, where Pπis the transition matrix coupled with the policy πand PπV(s) =
Ea′∼π(a′|s′)s′∼P(s′|s,a′)[V(s′)].
Then the expectation of Vπ(s)under distribution d(s)satisfies:
Es∼d(s)ˆVπ(s)≤Es∼d(s)(Vπ(s)) +Es∼d(s)(I−γPπ)−1Ea∼π(a|s)Cr,t,δRmax
(1−γ)p
|D(s, a)|
−α Es∼d(s)(I−γPπ)−1[d(s)
du(s)−1])
| {z }
>0(21)
14

--- PAGE 15 ---
When α≥Es∼d(s)Ea∼π(a|s)Cr,t,δRmax
(1−γ)√
|D(s,a)|
Es∼d(s)[d(s)
du(s)−1]),Es∼d(s)ˆVπ(s)≤Es∼d(s)(Vπ(s)).
Proof of Theorem 3.3: The expectation of Vπ(s)under distribution d(s)satisfies:
Es∼du(s)ˆVπ(s)≤Es∼du(s)(Vπ(s)) +Es∼du(s)(I−γPπ)−1Ea∼π(a|s)Cr,t,δRmax
(1−γ)p
|D(s, a)|
−αEs∼du(s)(I−γPπ)−1[d(s)
du(s)−1])(22)
Noticed that the last term:
X
s∼du(s)(df(s)
du(s)−1) =X
sdu(s)(df(s)
du(s)−1) =X
sdf(s)−X
sdu(s) = 0 (23)
We obtain that:
Es∼du(s)ˆVπ(s)≤Es∼du(s)(Vπ(s)) +Es∼du(s)(I−γPπ)−1Ea∼π(a|s)Cr,t,δRmax
(1−γ)p
|D(s, a)|(24)
Proof of Theorem 3.4: Recall that the expression of the V-function iterate is given by:
ˆVk+1(s) =BπkˆVk(s)−α[d(s)
du(s)−1]∀s, k (25)
Now the expectation of Vπ(s)under distribution du(s)is given by:
Es∼du(s)ˆVk+1(s) =Es∼du(s)
BπkˆVk(s)−α[d(s)
du(s)−1]
=Es∼du(s)BπkˆVk(s) (26)
The expectation of Vπ(s)under distribution d(s)is given by:
Es∼d(s)ˆVk+1(s) =Es∼d(s)BπkˆVk(s)−α[d(s)
du(s)−1] =Es∼d(s)BπkˆVk(s)−αEs∼d(s)[d(s)
du(s)−1]
(27)
Thus we can show that:
Es∼du(s)ˆVk+1(s)−Es∼d(s)ˆVk+1(s) =Es∼du(s)BπkˆVk(s)−Es∼d(s)BπkˆVk(s) +αEs∼d(s)[d(s)
du(s)−1]
=Es∼du(s)Vk+1(s)−Es∼d(s)Vk+1(s)−Es∼d(s)[Bπk(ˆVk−Vk)]
+Es∼du(s)[Bπk(ˆVk−Vk)] +αEs∼d(s)[d(s)
du(s)−1]
(28)
By choosing α:
α >Es∼d(s)[Bπk(ˆVk−Vk)]−Es∼du(s)[Bπk(ˆVk−Vk)]
Es∼d(s)[d(s)
du(s)−1](29)
We have Es∼du(s)ˆVk+1(s)−Es∼d(s)ˆVk+1(s)> E s∼du(s)Vk+1(s)−Es∼d(s)Vk+1(s)hold.
Proof of Theorem 3.5: ˆVis obtained by solving the recursive Bellman fixed point equation in the
empirical MDP, with an altered reward, r(s, a)−α[d(s)
du(s)−1], hence the optimal policy π∗(a|s)
obtained by optimizing the value under Eq. 3.5.
Proof of Theorem 3.6: The proof of this statement is divided into two parts. We first relates the
return of π∗in the empirical MDP ˆMwith the return of πβ, we can get:
J(π∗,ˆM)−α1
1−γEs∼dπ∗
ˆM(s)[d(s)
du(s)−1]≥J(πβ,ˆM)−0 =J(πβ,ˆM) (30)
15

--- PAGE 16 ---
Algorithm 1 CSVE based Offline RL Algorithm
Input: dataD={(s, a, r, s′)}
Parametered Models: Qθ,Vψ,πϕ,Qθ,Mν
Hyperparameters: α, λ, learning rates ηθ, ηψ, ηϕ, ω
▷Train the transition model with the static dataset D
Mν←train(D).
▷Train the conservative value estimation and policy functions
Initialize function parameters θ0, ψ0, ϕ0,θ0=θ0
forstepk= 1toNdo
ψk←ψk−1−ηψ∇ψLπ
V(Vψ;ˆQθk)
θk←θk−1−ηθ∇θLπ
Q(Qθ;ˆVψk)
ϕk←ϕk−1−ηϕ∇ϕL+
π(πϕ)
θk←ωθk−1+ (1−ω)θk
end for
The next step is to bound the difference between J(πβ,ˆM)andJ(πβ, M)and the difference between
J(π∗,ˆM)andJ(π∗, M). We quote a useful lemma from [4] (Lemma D.4.1):
Lemma A.3. For any MDP M, an empirical MDP ˆMgenerated by sampling actions according to
the behavior policy πβand a given policy π,
|J(π,ˆM)−J(π, M)| ≤(Cr,δ
1−γ+γRmaxCT,δ
(1−γ)2)Es∼dπ∗
ˆM(s)[p
|A|p
|D(s)|s
Ea∼π(a|s)(π(a|s)
πβ(a|s))](31)
Setting πin the above lemma as πβ, we get:
|J(πβ,ˆM)−J(πβ, M)| ≤(Cr,δ
1−γ+γRmaxCT,δ
(1−γ)2)Es∼dπ∗
ˆM(s)[p
|A|p
|D(s)|s
Ea∼π∗(a|s)(π∗(a|s)
πβ(a|s))]
(32)
given thatq
Ea∼π∗(a|s)[π∗(a|s)
πβ(a|s)]is a pointwise upper bound ofq
Ea∼πβ(a|s)[πβ(a|s)
πβ(a|s)]([4]). Thus we
get,
J(π∗,ˆM)≥J(πβ,ˆM)−2(Cr,δ
1−γ+γRmaxCT,δ
(1−γ)2)Es∼dπ∗
ˆM(s)[p
|A|p
|D(s)|s
Ea∼π∗(a|s)(π∗(a|s)
πβ(a|s))]
+α1
1−γEs∼dπ
ˆM(s)[d(s)
du(s)−1]
(33)
which completes the proof.
Here, the second term represents the sampling error, which arises due to the discrepancy between ˆM
andM. The third term signifies the enhancement in policy performance attributed to our algorithm
inˆM. It’s worth noting that when the first term is minimized, smaller values of αcan achieve
improvements over the behavior policy.
B CSVE Algorithm and Implementation Details
In Section 4, we deteiled the complete formula descriptions of the CSVE practical offline RL
algorithm. Here we consolidate those details and present the full deep offline reinforcement learning
algorithm as illustrated in Alg. 1. In particular, the dynamic model, value functions, and policy are
parameterized with deep neural networks and optimized via stochastic gradient descent methods.
We implement our method based on an offline deep reinforcement learning library d3rlpy [34]. The
code is available at: https://github.com/2023AnnonymousAuthor/csve .
16

--- PAGE 17 ---
Table 3: Hyper-parameters of CSVE evaluation
Hyper-parameters Value and description
B 5, number of ensembles in dynamics model
α 10, to control the penalty of OOD states
τ 10, budget parameter in Eq. 8
βIn Gym domain, 3 for random and medium tasks, 0.1 for the other tasks;
In Adroit domain, 30 for human and cloned tasks, 0.01 for expert tasks
γ 0.99, discount factor.
H 1 million for Mujoco while 0.1 million for Adroit tasks.
w 0.005, target network smoothing coefficient.
lr of actor 3e-4, policy learning rate
lr of critic 1e-4, critic learning rate
C Additional Ablation Study
Effect of model errors. Compared to traditional model-based offline RL algorithms, CSVE exhibits
greater resilience to model biases. To access this resilience quantitatively, we measured the perfor-
mance impact of model biases using the average L2 error in transition prediction as an indicator. As
shown in Fig. 3, the influence of model bias on RL performance is CSVE is marginal. Specifically, in
the halfcheetah task, there is no observable impact of model errors on scores, model errors show no
discernible impact on scores. For the hopper and walker2d tasks, only a minor decline in scores is
observed as the errors escalate.
6 7 8 9
errors020406080100120score(normalized)
halfcheetah-medium-v2
���������������������������
0.10 0.12 0.14 0.16
errors020406080100120
hopper-medium-v2
6 8 10 12
errors020406080100120
walker2d-medium-v2
Figure 3: Effect of the model biases to performance scores. The correlation coefficient is −0.32,
−0.34, and−0.29respectively.
D Experimental Details and Complementary Results
D.1 Hyper-parameters of CSVE evaluation in experiments
Table 3 provides a detailed breakdown of the hyper-parameters used for evaluating CSVE in our
experiments.
D.2 Details of Baseline CQL-A WR
To facilitate a direct comparison between the effects of conservative state value estimation and
Q-value estimation, we formulated a baseline method named CQL-AWR as detailed below:
ˆQk+1←arg min
Qα(Es∼D,a∼π(a|s)[Q(s, a)]−Es∼D,a∼ˆπβ(a|s)[Q(s, a)]) +1
2Es,a,s′∼D[(Q(s, a)−ˆβπˆQk(s, a))2]
π←arg min
π′Lπ(π′) =−Es,a∼Dh
logπ′(a|s) exp
βˆAk+1(s, a)i
−λEs∼D,a∼π′(s)h
ˆQk+1(s, a)i
where ˆAk+1(s, a) =ˆQk+1(s, a)−Ea∼π[ˆQk+1(s, a)].
17

--- PAGE 18 ---
0 200 400 600 800 1000
grandient steps(thousands)01020304050score
halfcheetah-medium-replay
0.0
0.5
1.0
3.0
0 200 400 600 800 1000
grandient steps(thousands)020406080100score
hopper-medium-replay
0.0
0.5
1.0
3.0
0 200 400 600 800 1000
grandient steps(thousands)020406080score
walker2d-medium-replay
0.0
0.5
1.0
3.0
0 200 400 600 800 1000
grandient steps(thousands)2468loss
halfcheetah-medium-replay
0.0
0.5
1.0
3.0
0 200 400 600 800 1000
grandient steps(thousands)020406080loss
hopper-medium-replay
0.0
0.5
1.0
3.0
0 200 400 600 800 1000
grandient steps(thousands)1234567loss
walker2d-medium-replay
0.0
0.5
1.0
3.0Figure 4: Effect of λto Score (upper figures) and Lπloss in Eq. 9 (bottom figures)
In CQL-AWR, the critic adopts a standard CQL equation, while the policy improvement part uses an
AWR extension combined with novel action exploration as denoted by the conservative Q function.
When juxtaposed with our CSVE implementation, the policy segment of CQL-AWR mirrors ours,
with the primary distinction being that its exploration is rooted in a Q-based and model-free approach.
D.3 Reproduction of COMBO
In Table 1 of our main paper, we used the results of COMBO as presented in the literature [ 23]. Here
we detail additional attempts to reproduce the results and compare the performance of CSVE with
COMBO.
Official Code. We initially aimed to rerun the experiment using the official COMBO code provided
by the authors. The code is implemented in Tensoflow 1.x and relies on software versions from
2018. Despite our best efforts to recreate the computational environment, we encountered challenges
in reproducing the reported results. For instance, Fig. 5 illustrates the asymptotic performance
on medium datasets up to 1000 epochs, where the scores have been normalized based on SAC
performance metrics. Notably, for both the hopper and walker2d tasks, the performance scores
exhibited significant variability. The average scores over the last 10 epochs for halfcheetah, hopper,
and walker2d were 71.7, 65.3, and -0.26, respectively. Furthermore, we observed that even when
using the D4RL v0 dataset, COMBO demonstrated similar performance patterns when recommended
hyper-parameters were applied.
0 200 400 600 800 1000010203040506070Scorehalfcheetah_v2: Score of Return Average
0 200 400 600 800 1000010203040506070Scorehopper_v2: Score of Return Average
0 200 400 600 800 10002
0246810Scorewalker2d_v2: Score of Return Average
Figure 5: Return of official COMBO implementation on D4RL mujoco v2 tasks, fixing seed=0.
18

--- PAGE 19 ---
JAX-based optimized implementation Code [ 35].We also tested one recent re-implementation
available in RIQL. This version is regarded as the most highly-tuned implementation to date. The
results of our tests can be found in Fig.6. For the random and expert datasets, we applied the same
hyper-parameters as those used for the medium and medium-expert datasets, respectively. For all
other datasets, we adhered to the default hyper-parameters provided by the authors [ 35]. Despite
these efforts, when we compared our outcomes with the original authors’ results (as shown in Table
10 and Fig.7 of [ 35]), our reproduced results consistently exhibited both lower performance scores
and greater variability.
0 200 400 600 800 1000
Time Step (1e3)051015202530halfcheetah-random-v2
0 200 400 600 800 1000
Time Step (1e3)5
051015202530hopper-random-v2
0 200 400 600 800 1000
Time Step (1e3)0246810121416walker2d-random-v2
0 200 400 600 800 1000
Time Step (1e3)0102030405060halfcheetah-medium-v2
0 200 400 600 800 1000
Time Step (1e3)020406080100hopper-medium-v2
0 200 400 600 800 1000
Time Step (1e3)020406080walker2d-medium-v2
0 200 400 600 800 1000
Time Step (1e3)1020304050halfcheetah-medium-replay-v2
0 200 400 600 800 1000
Time Step (1e3)20406080100hopper-medium-replay-v2
0 200 400 600 800 1000
Time Step (1e3)020406080walker2d-medium-replay-v2
0 200 400 600 800 1000
Time Step (1e3)020406080halfcheetah-medium-expert-v2
0 200 400 600 800 1000
Time Step (1e3)20406080100hopper-medium-expert-v2
0 200 400 600 800 1000
Time Step (1e3)20
0204060walker2d-medium-expert-v2
0 200 400 600 800 1000
Time Step (1e3)010203040halfcheetah-expert-v2
0 200 400 600 800 1000
Time Step (1e3)020406080100120hopper-expert-v2
0 200 400 600 800 1000
Time Step (1e3)1.0
0.5
0.00.51.01.52.02.5walker2d-expert-v2
Figure 6: Return of an optimized COMBO implementation[ 35] on D4RL mujoco v2 tasks. The data
are obtained by running with 5 seeds for each task, and the dynamics model has 7 ensembles.
D.4 Effect of Exploration on Near Dataset Distributions
As discussed in Section 3.1 and 4.2, the appropriate selection of exploration on the distribution ( d)
beyond data ( du) should help policy improvement. The factor λin Eq. 10 controls the trade-off on
such ’bonus’ exploration and complying with the data-implied behavior policy.
19

--- PAGE 20 ---
In section 5.2, we examined the effect of λon the medium datasets of mujoco tasks. Now let us
further take the medium-replay type of datasets for more analysis of its effect. In the experiments,
with fixed β= 0.1, we investigate λvalues of {0.0,0.5,1.0,3.0}. As shown in the upper figures in
Fig. 4, λshows an obvious effect on policy performance and variances during training. In general,
there is a value under which increasing λleads to performance improvement, while above which
further increasing λhurts performance. For example, with λ= 3.0in hopper-medium-replay task
and walker2d-medium-replay task, the performance gets worse than with smaller λvalues. The value
ofλis task-specific, and we find that its effect is highly related to the loss in Eq. 9 which can be
observed by comparing the bottom and upper figures in Fig. 4. Thus, in practice, we can choose
proper λaccording to the above loss without online interaction.
D.5 Conservative State Value Estimation by Perturbing Data State with Noise
In this section, we investigate a model-free method for sampling OOD states and compare its results
with the model-based method adopted in section 4.
The model-free method samples OOD states by randomly adding Gaussian noise to the sampled
states from the data. Specifically, we replace the Eq.5 with the following Eq. 34, and other parts are
consistent with the previous technology.
ˆVk+1←arg min
VLπ
V(V;ˆQk) =α 
Es∼D,s′=s+N(0,σ2)[V(s′)]−Es∼D[V(s)]
+Es∼Dh
(Ea∼π(·|s)[ˆQk(s, a)]−V(s))2i
.(34)
The experimental results on the Mujoco control tasks are summarized in Table 4. As shown, with
different noise levels ( σ2), the model-free CSVE perform worse than our original model-based
CSVE implementation; and for some problems, the model-free method shows very large variances
across seeds. Intuitively, if the noise level covers the reasonable state distribution around data, its
performance is good; otherwise, it misbehaves. Unfortunately, it is hard to find a noise level that is
consistent for different tasks or even the same tasks with different seeds.
Table 4: Performance comparison on Gym control tasks. The results of different noise levels are over
three seeds.
CQL CSVE σ2=0.05 σ2=0.1 σ2=0.15RandomHalfCheetah 17.5±1.5 26 .7±2.0 20 .8±0.4 20 .4±1.3 18 .6±1.1
Hopper 7.9±0.4 27 .0±8.5 4 .5±3.1 14 .2±15.3 6 .7±5.4
Walker2D 5.1±1.3 6 .1±0.8 3 .9±3.8 7 .5±6.9 1 .7±3.5MediumHalfCheetah 47.0±0.5 48 .6±0.0 48 .2±0.2 47 .5±0.0 46 .0±0.9
Hopper 53.0±28.5 99 .4±5.3 36 .9±32.6 46 .1±2.1 18 .4±30.6
Walker2D 73.3±17.7 82 .5±1.5 81 .5±1.0 75 .5±1.9 78 .6±2,9Medium
ReplayHalfCheetah 45.5±0.7 54 .8±0.8 44 .8±0.4 44 .1±0.5 43 .8±0.4
Hopper 88.7±12.9 91 .7±0.3 85 .5±3.0 78 .3±4.3 70 .2±12.0
Walker2D 81.8±2.7 78 .5±1.8 78 .7±3.3 76 .8±1.3 66 .8±4.0Medium
ExpertHalfCheetah 75.6±25.7 93 .1±0.3 87 .5±6.0 89 .7±6.6 93 .8±1.6
Hopper 105.6±12.9 95 .2±3.8 63 .2±54.4 99 .0±11.0 37 .6±63.9
Walker2D 107.9±1.6 109 .0±0.1 108 .4±1.9 109 .5±1.3 110 .4±0.6ExpertHalfCheetah 96.3±1.3 93 .8±0.1 59 .0±28.6 67 .5±21.9 75 .3±27.3
Hopper 96.5±28.0 111 .2±0.6 67 .3±57.7 109 .2±2.4 109 .4±2.1
Walker2D 108.5±0.5 108 .5±0.0 109 .7±1.1 108 .9±1.6 108 .6±0.3
20
