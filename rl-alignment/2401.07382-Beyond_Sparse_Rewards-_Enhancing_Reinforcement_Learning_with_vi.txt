# 2401.07382.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2401.07382.pdf
# Kích thước tệp: 1017166 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Vượt qua phần thưởng thưa thớt: Nâng cao học tăng cường với phê bình mô hình ngôn ngữ trong sinh văn bản
Meng Cao1,3,†,‡, Lei Shu4, Lei Yu2, Yun Zhu4, Nevan Wichers4, Yinxiao Liu4, Lei Meng4,‡
1Trường Khoa học Máy tính, Đại học McGill
2Khoa Khoa học Máy tính, Đại học Toronto
3Mila - Viện Trí tuệ nhân tạo Québec
4Google Research

Tóm tắt
Học tăng cường (RL) có thể căn chỉnh các mô hình ngôn ngữ với tín hiệu phần thưởng không khả vi, chẳng hạn như sở thích của con người. Tuy nhiên, một thách thức lớn phát sinh từ tính thưa thớt của các tín hiệu phần thưởng này - thông thường, chỉ có một phần thưởng duy nhất cho toàn bộ đầu ra. Tính thưa thớt của phần thưởng này có thể dẫn đến việc học không hiệu quả và không ổn định. Để giải quyết thách thức này, bài báo của chúng tôi giới thiệu một khung mới tận dụng khả năng phê bình của các Mô hình Ngôn ngữ Lớn (LLM) để tạo ra phần thưởng bước trung gian trong quá trình huấn luyện RL. Phương pháp của chúng tôi bao gồm việc kết hợp một mô hình chính sách với một mô hình ngôn ngữ phê bình, có trách nhiệm cung cấp phản hồi toàn diện cho từng phần của đầu ra. Phản hồi này sau đó được chuyển đổi thành phần thưởng cấp token hoặc cấp đoạn có thể được sử dụng để hướng dẫn quá trình huấn luyện RL. Chúng tôi điều tra phương pháp này trong hai thiết lập khác nhau: một là mô hình chính sách nhỏ hơn được ghép nối với một mô hình phê bình mạnh hơn, và một thiết lập khác là một mô hình ngôn ngữ duy nhất đảm nhận cả hai vai trò. Chúng tôi đánh giá phương pháp của mình trên ba nhiệm vụ sinh văn bản: kiểm soát cảm xúc, khử độc mô hình ngôn ngữ và tóm tắt. Kết quả thực nghiệm cho thấy việc kết hợp phần thưởng nội tại nhân tạo cải thiện đáng kể cả hiệu quả mẫu và hiệu suất tổng thể của mô hình chính sách, được hỗ trợ bởi cả đánh giá tự động và con người.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) đã có sự tiến bộ nhanh chóng trong những năm gần đây, thể hiện khả năng đáng chú ý trong việc hiểu và sinh ngôn ngữ tự nhiên (Brown et al., 2020b; Touvron et al., 2023; OpenAI, 2023; Biderman et al., 2023; Jiang et al., 2023; Shu et al., 2023). Trong khi đó, học tăng cường đã nổi lên như một công cụ bổ sung để tinh chỉnh thêm khả năng của LM. RL cho phép tối ưu hóa LM hướng tới bất kỳ tín hiệu phần thưởng không khả vi nào. Ví dụ, các kỹ thuật như học tăng cường từ phản hồi con người (RLHF) (Ziegler et al., 2019; Stiennon et al., 2020) đã được sử dụng để điều hướng các mô hình ngôn ngữ để căn chỉnh tốt hơn với sở thích của con người.

Tuy nhiên, các tín hiệu phần thưởng nhận được từ môi trường thường thưa thớt, một bottleneck cơ bản hạn chế hiệu quả học tập (Andrychowicz et al., 2017; Sukhbaatar et al., 2018). Thông thường, trong các nhiệm vụ sinh văn bản, một phần thưởng vô hướng duy nhất được nhận sau khi một câu hoặc đoạn văn đã được sinh ra hoàn toàn. Tín hiệu phần thưởng đơn lẻ này gây ra vấn đề gán nhận tín dụng thời gian, khiến mô hình khó học được token nào chịu trách nhiệm cho phần thưởng nhận được. Các nỗ lực trước đây để vượt qua tính thưa thớt của phần thưởng trong RL đã bao gồm định hình phần thưởng (Ng et al., 1999; Devidze et al., 2022; Goyal et al., 2019), khám phá thúc đẩy bởi tò mò (Bellemare et al., 2016; Pathak et al., 2017; Ostrovski et al., 2017), và RL phân cấp (Nachum et al., 2018; Zhang et al., 2021). Tuy nhiên, những phương pháp này hoặc yêu cầu các đặc trưng thủ công hoặc không dịch chuyển trực tiếp vào lĩnh vực sinh văn bản. Một giải pháp trực tiếp là tinh chỉnh mô hình phần thưởng toàn diện của môi trường với một mô hình cung cấp phần thưởng dày đặc. Lightman et al. (2023) và Wu et al. (2023) đã khám phá việc sử dụng các chú thích viên con người để cung cấp phản hồi chi tiết tại mỗi bước trung gian của việc sinh của mô hình. Những chú thích này sau đó có thể được sử dụng để huấn luyện một mô hình phần thưởng tinh vi. Tuy nhiên, phương pháp này phát sinh chi phí cao, và các mô hình phần thưởng kết quả có xu hướng đặc thù cao cho nhiệm vụ, hạn chế khả năng ứng dụng của chúng trên các nhiệm vụ khác nhau.

Trong bối cảnh những hạn chế này, chúng tôi giới thiệu RELC (Rewards from Language model Critique), một khung mới tận dụng khả năng phê bình của LLM (Madaan et al., 2023; Saunders et al.; Luo et al., 2023) để cung cấp tín hiệu phần thưởng nhân tạo cho các bước trung gian trong quá trình huấn luyện RL. Như được minh họa trong Hình 1, chúng tôi định nghĩa rõ ràng một tác nhân RL như sự tích hợp của 1) một mô hình chính sách chịu trách nhiệm sinh đầu ra, và 2) một mô hình phê bình được giao nhiệm vụ đánh giá chất lượng đầu ra được tạo ra bởi mô hình chính sách. LM phê bình, được thông báo bởi câu hỏi, đầu ra của mô hình chính sách và tín hiệu phần thưởng duy nhất được cung cấp bởi môi trường, sinh ra đánh giá bằng lời của từng đoạn đầu ra của mô hình chính sách. Những đánh giá này sau đó được chuyển đổi thành tín hiệu phần thưởng. Chúng tôi gán nhãn các phần thưởng được sinh ra bởi mô hình phê bình là "phần thưởng nội tại" để phân biệt chúng với các tín hiệu phần thưởng được cung cấp bởi môi trường. Mô hình phê bình có thể được tích hợp liền mạch vào các thuật toán RL như PPO (Schulman et al., 2017), không yêu cầu hoặc rất ít sửa đổi các thuật toán đó.

Đánh giá của chúng tôi về phương pháp được đề xuất được thực hiện trong hai thiết lập riêng biệt: một sử dụng mô hình chính sách nhỏ hơn (GPT-2 Large) kết hợp với mô hình phê bình tiên tiến hơn (GPT-3.5), và thiết lập "tự phê bình" thách thức hơn, nơi một mô hình duy nhất (Llama 2) đảm nhận cả hai vai trò. Chúng tôi đánh giá hiệu quả của phương pháp thông qua ba nhiệm vụ sinh văn bản: kiểm soát cảm xúc, khử độc LM và tóm tắt văn bản trừu tượng. Kết quả thực nghiệm cho thấy việc sử dụng phần thưởng nội tại được sinh bởi LLM cải thiện đáng kể hiệu quả mẫu trên tất cả các nhiệm vụ, với phương pháp của chúng tôi vượt trội hơn các phương pháp baseline đã được thiết lập theo cả đánh giá tự động và con người. Mặc dù chi phí suy luận bổ sung phát sinh do kết hợp mô hình phê bình, phương pháp của chúng tôi được chứng minh là hiệu quả tính toán hơn, đạt được hiệu suất vượt trội so với baseline trong cùng ngân sách tính toán.

2 Công trình liên quan
RL cho Sinh văn bản. Các phương pháp RL đã được sử dụng trong nhiều nhiệm vụ sinh văn bản khác nhau bao gồm tóm tắt văn bản (Ryang and Abekawa, 2012; Pang and He, 2021; Dong et al., 2018; Cao et al., 2022a), dịch máy (Norouzi et al., 2016; Ranzato et al., 2016; He et al., 2016; Bahdanau et al., 2017), hệ thống đối thoại (Fatemi et al., 2016; Li et al., 2016; Dhingra et al., 2017; Jaques et al., 2019) và trả lời câu hỏi (Buck et al., 2018; Xiong et al., 2018; Nakano et al., 2021). Các nghiên cứu gần đây đã tập trung vào việc kết hợp RL với các mô hình ngôn ngữ được huấn luyện trước như GPT-3 (Brown et al., 2020a) để sinh văn bản (Ouyang et al., 2022; Bai et al., 2022; Nakano et al., 2021; Stiennon et al., 2020) được căn chỉnh tốt hơn với sở thích con người như là thực tế, liên quan và hữu ích.

Định hình phần thưởng và phần thưởng nội tại. Ng et al. (1999) đã đặt nền móng cho định hình phần thưởng dựa trên tiềm năng trong RL, chứng minh rằng việc định hình như vậy có thể giảm hiệu quả thời gian huấn luyện mà không thay đổi chính sách tối ưu. Bellemare et al. (2016); Ostrovski et al. (2017); Tang et al. (2017) đã sử dụng phần thưởng dựa trên pseudo-count để khuyến khích khám phá trong các môi trường có phần thưởng thưa thớt. Zheng et al. (2018) đề xuất một phương pháp trong đó một mô hình phần thưởng nội tại tham số hóa được học trong quá trình huấn luyện để sinh tín hiệu phần thưởng dày đặc. Tuy nhiên, phương pháp này gặp một số khó khăn tối ưu hóa do sự cần thiết tính toán gradient bậc hai. Wu et al. (2023); Lightman et al. (2023) sử dụng các chú thích viên con người để cung cấp tín hiệu phần thưởng chi tiết cấp đoạn, chứng minh rằng những phần thưởng tinh vi này mang lại hiệu suất tốt hơn so với phần thưởng toàn diện.

LLM cho Thiết kế phần thưởng. Lee et al. (2023) đã sử dụng một LLM có sẵn để tạo nhãn sở thích bằng cách so sánh các cặp phản hồi ứng viên. Những nhãn này sau đó được sử dụng để huấn luyện một mô hình phần thưởng toàn diện. Tương tự, Kwon et al. (2023) điều tra việc sử dụng GPT-3 như một thay thế cho hàm phần thưởng thực tế trong huấn luyện RL. Phương pháp của họ vượt trội hơn mô hình phần thưởng được huấn luyện thông qua học có giám sát, nhưng nó không đạt được hiệu quả của hàm phần thưởng thực. Du et al. (2023); Klissarov et al. (2023) sử dụng LLM để sinh tín hiệu phần thưởng để khuyến khích khám phá của một tác nhân chơi game hoặc robot. Ma et al. (2023) đã sử dụng GPT-4 để sinh mã cho một hàm phần thưởng.

3 Phương pháp
Ý tưởng cơ bản đằng sau phương pháp của chúng tôi là tận dụng LLM để sinh tín hiệu phần thưởng nội tại dày đặc rin và cung cấp nó cho một tác nhân RL, sẽ tối ưu hóa sự kết hợp của phần thưởng nội tại và ngoại tại. Trong phần này, đầu tiên chúng tôi thiết lập quá trình quyết định Markov (MDP) cho sinh văn bản. Sau đó, chúng tôi thảo luận về phương pháp RL dựa trên gradient chính sách được sử dụng rộng rãi cho các nhiệm vụ sinh văn bản. Cuối cùng, chúng tôi chi tiết quá trình kết hợp phần thưởng nội tại được sinh bởi LLM vào huấn luyện RL.

3.1 RL cho Sinh văn bản
Hãy xem xét quy trình sinh ngôn ngữ như một MDP (Puterman, 1994), được định nghĩa bởi tuple (S,A,P,R, gamma). Ở đây, S đại diện cho tập hợp tất cả các trạng thái có thể, A là tập hợp các hành động, P: S × A × S 7→ [0,1] là hàm chuyển đổi trạng thái, R: S × A × S 7→ R là hàm phần thưởng gán một giá trị số cho mỗi chuyển đổi (s, a, s′), và gamma ∈ [0,1] là hệ số chiết khấu. Trong bối cảnh sinh văn bản, chúng tôi hoạt động dưới giả định của một thiết lập RL tập đoạn, hành động rời rạc. Prompt đầu vào s0 ∈ S đặt trạng thái bắt đầu. Tại mỗi bước giải mã t, trạng thái st ∈ S bao gồm prompt và sự nối của các token đã được sinh trước đó. Chọn một hành động bao gồm việc chọn một token từ từ vựng, dẫn đến một trạng thái mới st+1, được tạo ra bằng cách nối token đã chọn vào câu một phần hiện đang được sinh. Chính sách của tác nhân πθ(a|s), là một mô hình ngôn ngữ được tham số hóa bởi θ, xác định xác suất chọn mỗi hành động tại một trạng thái cho trước. Mục tiêu của tác nhân là tối đa hóa phần thưởng tích lũy có chiết khấu trong suốt quỹ đạo: J(θ) = Eτ∼πθ[∑T t=0 γt rt].

3.2 RL dựa trên Gradient chính sách & PPO
Các phương pháp gradient chính sách, thường được áp dụng trong sinh văn bản, trực tiếp tham số hóa mô hình chính sách để tối ưu hóa các tham số θ với mục tiêu tối đa hóa J(θ). Gradient ∇θJ(θ) tỉ lệ với kỳ vọng của tích của gradient của log chính sách và return Gt (Sutton et al., 1999):

∇θJ(θ) = Eπθ[∇θ log πθ(at|st)Gt] (1)

trong đó return được định nghĩa là Gt = ∑T i=t γi−t ri.
Một return cao dẫn đến việc tăng cường tất cả các hành động bằng cách tăng xác suất chọn của chúng. Để giảm phương sai, một chiến lược được áp dụng rộng rãi bao gồm việc thay thế return thô Gt trong Phương trình 1 bằng một hàm ước lượng lợi thế tổng quát (Schulman et al., 2016):

Ât = ∑T t′=t (γλ)t′−t (rt′ + γV(st′+1) − V(st′))

trong đó λ là một siêu tham số và V(st′) là hàm giá trị đại diện cho return kỳ vọng tại trạng thái st′. Một số biến thể của phương pháp gradient chính sách cơ bản đã được đề xuất để cải thiện tính ổn định huấn luyện. Một biến thể được sử dụng rộng rãi, đặc biệt trong bối cảnh sinh văn bản, là Tối ưu hóa Chính sách Gần đúng (PPO) (Schulman et al., 2017). PPO giới thiệu các cơ chế để ổn định quá trình huấn luyện bằng cách giới hạn các cập nhật cho chính sách tại mỗi bước, hiệu quả ngăn chặn các cập nhật lớn có tính phá hoại có thể khiến chính sách hoạt động tệ hơn. Trong công trình này, chúng tôi sử dụng hàm mục tiêu surrogate bị cắt của PPO được biểu thị là:

L(θ) = Êt[min(rt(θ)Ât, clip(rt(θ), 1−ε, 1+ε)Ât)]

trong đó rt(θ) = πθ(at|st)/πθold(at|st) là xác suất thực hiện hành động at tại trạng thái st trong chính sách hiện tại chia cho chính sách trước đó.

3.3 Học với phần thưởng nội tại được sinh bởi LLM
Các khung RL hiện tại cho sinh văn bản, như RLHF (Ziegler et al., 2019; Stiennon et al., 2020), môi trường nhận toàn bộ văn bản được sinh làm đầu vào và trả về một điểm vô hướng. Do đó, việc học thường phụ thuộc vào một tín hiệu phần thưởng thưa thớt chỉ có thể truy cập được khi sinh ra một câu hoàn chỉnh. Chúng tôi gọi tín hiệu phần thưởng này là phần thưởng ngoại tại rex và chúng tôi có rex t<T = 0. Phương pháp của chúng tôi khác biệt với các phương pháp hiện có bằng cách phân biệt giữa phần thưởng ngoại tại từ môi trường và một phần thưởng nội tại bổ sung rin được sinh bởi LLM. Như được hiển thị trong Hình 1, trong tác nhân, khung của chúng tôi kết hợp một mô hình ngôn ngữ phê bình bổ sung bên cạnh mô hình chính sách. Nhiệm vụ của mô hình phê bình là xác định các token hoặc đoạn trong đầu ra của chính sách góp phần trực tiếp vào việc nhận phần thưởng từ môi trường. Mô hình phê bình được cung cấp mô tả nhiệm vụ D, một tập hợp các ví dụ few-shot E, trạng thái hiện tại s như được xác định bởi đầu ra của mô hình chính sách, và tùy chọn, phần thưởng rex nhận được từ môi trường. Đối với token tại bước t, nếu nó là một phần của đoạn đã xác định, chúng tôi gán một giá trị khác không cho phần thưởng nội tại rin t. Phần thưởng cuối cùng được định nghĩa là tổng có trọng số của phần thưởng ngoại tại và nội tại: r(s, a) = α1rex(s, a) + α2rin(s, a) trong đó α1 và α2 là các siêu tham số kiểm soát trọng số của phần thưởng. Lưu ý rằng phần thưởng ngoại tại chỉ khác không tại bước thời gian cuối cùng, cụ thể là khi t = T. LM chính sách được tối ưu hóa để tối đa hóa phần thưởng kết hợp: J(θ)RELC = Eτ∼πθ[∑T t=0 γt(α1rex + α2rin)] trong đó mô hình chính sách được tham số hóa bởi θ. LM phê bình được đóng băng trong quá trình huấn luyện. Trong công trình này, chúng tôi đã sử dụng thuật toán PPO để huấn luyện tác nhân. Tuy nhiên, đáng chú ý rằng khung của chúng tôi linh hoạt và cũng có thể được tích hợp với các thuật toán học tăng cường khác, như Advantage Actor-Critic (A2C) (Mnih et al., 2016). Một minh họa về cách phần thưởng được tính toán trong nhiệm vụ kiểm soát cảm xúc được cung cấp trong Hình 2. Trong các phần tiếp theo, tham chiếu đến PPO cụ thể biểu thị việc sử dụng PPO chỉ với phần thưởng ngoại tại.

Lựa chọn LLM và Thiết kế Prompt Trong công trình này, chúng tôi sử dụng hai LLM làm phê bình: gpt-3.5-turbo và 7B Llama2 (Touvron et al., 2023). Prompt đầu vào được cấu trúc thành ba phần. Đầu tiên, chúng tôi định nghĩa nhiệm vụ trong prompt, nêu rõ các loại phản hồi đúng hoặc lỗi mà mô hình phê bình nên xác định. Ví dụ, trong nhiệm vụ khử độc, chúng tôi chỉ định rõ ràng điều gì cấu thành ngôn ngữ độc hại. Tiếp theo, chúng tôi bao gồm một tập hợp các ví dụ few-shot được tuyển chọn (3-shot trừ khi được ghi chú khác). Những ví dụ này được chọn tỉ mỉ để bao gồm một phổ rộng các phản hồi mẫu và lỗi điển hình được tạo ra bởi mô hình chính sách. Cuối cùng, chúng tôi cung cấp cho mô hình phê bình câu hỏi hiện tại, đầu ra từ mô hình chính sách, và tùy chọn, phần thưởng ngoại tại từ môi trường. Phần thưởng ngoại tại này được kết hợp để căn chỉnh tốt hơn đánh giá của phê bình với các kết quả mong muốn. Điều quan trọng cần lưu ý là mục tiêu chính của chúng tôi là tối ưu hóa tác nhân hướng tới phần thưởng ngoại tại, vì đây là các chỉ số cuối cùng của hiệu suất. Mặt khác, phần thưởng nội tại chỉ được sử dụng để cung cấp phản hồi ngay lập tức và nâng cao quá trình học. Do đó, chúng tôi muốn đảm bảo rằng phản hồi của mô hình phê bình và phần thưởng ngoại tại được căn chỉnh tốt. Các chi tiết cụ thể của prompt được sử dụng được chi tiết trong Phụ lục.

--- TRANG 5 ---
4 Thực nghiệm
Trong phần này, chúng tôi chứng minh rằng phương pháp của chúng tôi vượt trội hơn baseline PPO trong ba nhiệm vụ sinh văn bản: kiểm soát cảm xúc, khử độc LM và tóm tắt văn bản.

4.1 Kiểm soát cảm xúc
Trong nhiệm vụ kiểm soát cảm xúc, mục tiêu là hướng dẫn LM tạo ra các phản hồi có cảm xúc tích cực, bắt đầu từ các prompt trung tính hoặc tiêu cực.

4.1.1 Thiết lập thực nghiệm
Chúng tôi xem xét hai thiết lập: 1) một LM chính sách nhỏ (GPT-2 large) được ghép nối với một LM phê bình mạnh (gpt-3.5-turbo); 2) mô hình chính sách và mô hình phê bình giống nhau, chúng tôi sử dụng Llama 2 (Touvron et al., 2023) làm khởi tạo. Chúng tôi truy cập mô hình gpt-3.5-turbo thông qua API của OpenAI. Để huấn luyện, chúng tôi sử dụng bộ dữ liệu IMDB chứa 25K đánh giá phim (Maas et al., 2011). Chúng tôi ngẫu nhiên trích xuất 4 đến 10 token đầu tiên từ mỗi đánh giá làm prompt đầu vào. Mô hình chính sách được huấn luyện trên tập huấn luyện trong một epoch. Chúng tôi đặt α1 = 1 và α2 = 0.2. Theo thiết lập thực nghiệm của (Liu et al., 2021; Lu et al., 2022), chúng tôi sử dụng bộ dữ liệu OpenWebText (OWT) Corpus (Gokaslan and Cohen, 2019) làm tập test. Liu et al. (2021) đã tuyển chọn ba tập test riêng biệt từ OWT: trung tính (5K prompt), tích cực (2.5K prompt) và tiêu cực (2.5K prompt). Những tập này được tạo ra dựa trên khả năng của prompt dẫn đến các tiếp tục tích cực hoặc tiêu cực. Đối với mô hình phần thưởng, chúng tôi sử dụng một bộ phân loại BERT được chưng cất được huấn luyện trên bộ dữ liệu IMDB‡. Chi tiết về prompt, ví dụ few-shot và các siêu tham số bổ sung có thể được tìm thấy trong Phụ lục A.

Baseline và chỉ số đánh giá Chúng tôi so sánh phương pháp của mình với bảy phương pháp baseline bao gồm PPLM (Dathathri et al., 2020), CTRL (Keskar et al., 2019), DAPT (Gururangan et al., 2020), GeDi (Krause et al., 2021), DEXPERTS (Liu et al., 2021), RECT (Cao et al., 2023) và PPO. Để đánh giá cảm xúc, chúng tôi áp dụng phương pháp của Liu et al. (2021); Lu et al. (2022) và tính tỷ lệ phần trăm trung bình của các tiếp tục tích cực/tiêu cực từ 25 đầu ra được sinh bằng bộ phân loại phân tích cảm xúc của Hugging-Face được tinh chỉnh trên SST-2. Hơn nữa, chúng tôi phân tích tính trôi chảy và đa dạng để đo lường cách mỗi phương pháp ảnh hưởng đến chất lượng văn bản tổng thể. Chúng tôi sử dụng perplexity GPT-2 XL (PPL) như một proxy cho tính trôi chảy. Đối với đa dạng, chúng tôi tính số lượng bigram duy nhất được chuẩn hóa.

4.1.2 Kết quả
Hình 3 trình bày các đường cong học cho cả phương pháp của chúng tôi và baseline. Từ hình, chúng ta có thể thấy rằng RELC có hiệu quả mẫu tốt hơn so với baseline trong cả hai thiết lập. Bảng 1 hiển thị kết quả đánh giá trên tập test OWT Corpus. Như được hiển thị trong bảng, phương pháp của chúng tôi vượt trội hơn tất cả các baseline về việc điều hướng hướng tới cảm xúc tích cực. Bên cạnh đó, so với baseline, mô hình của chúng tôi có tác động ít nhất đến tính trôi chảy của các câu được sinh.

4.2 Khử độc LM
Trong thực nghiệm này, chúng tôi tập trung vào nhiệm vụ khử độc LM. Chúng tôi chứng minh rằng việc tích hợp phần thưởng nội tại được sinh bởi LLM vào huấn luyện RL có thể cải thiện cả hiệu quả mẫu và hiệu suất khử độc cuối cùng.

4.2.1 Thiết lập thực nghiệm
Trong các thực nghiệm khử độc của chúng tôi, chúng tôi sử dụng benchmark REALTOXICITY PROMPTS (RTP) (Gehman et al., 2020) để huấn luyện và đánh giá. RTP chứa 100K tiền tố câu được viết bởi con người (tức là prompt) được lấy từ văn bản web tiếng Anh. Để đánh giá độc tính, chúng tôi sử dụng Perspective API‡, một công cụ được sử dụng rộng rãi trong công trình trước đây để đánh giá độc tính tự động. API cung cấp điểm từ 0 đến 1, với 1 chỉ ra độc tính cao và 0 biểu thị nội dung không độc hại. Theo thiết lập thực nghiệm của công trình trước, chúng tôi sử dụng 85K prompt này làm tập huấn luyện. Đánh giá của chúng tôi được thực hiện trên 10K prompt test không độc hại được sử dụng bởi Liu et al. (2021); Lu et al. (2022). Chúng tôi sử dụng 1−PERSPECTIVE(y) làm tín hiệu phần thưởng. Chúng tôi đặt α1 là 1 và α2 là 0.5. Thông tin thêm về prompt, ví dụ few-shot và siêu tham số có thể được tìm thấy trong Phụ lục B.

Baseline và chỉ số đánh giá. Chúng tôi thực hiện phân tích so sánh phương pháp của mình với bảy phương pháp baseline. Trong số này, sáu phương pháp giống với những phương pháp được thảo luận trong Phần 4.1. Ngoài ra, chúng tôi thêm một phương pháp baseline khác là RECT (Cao et al., 2023). Chúng tôi báo cáo hai chỉ số: trung bình điểm độc tính tối đa trên 25 lần sinh và xác suất thực nghiệm của một tiếp tục độc hại xuất hiện ít nhất một lần trên 25 lần sinh.

4.2.2 Kết quả
Như được hiển thị trong Hình 4, việc kết hợp phần thưởng nội tại cải thiện đáng kể hiệu quả mẫu. Một phát hiện thú vị khác là việc chỉ sử dụng phần thưởng nội tại cũng vượt trội hơn baseline phần thưởng ngoại tại. Bảng 2 hiển thị kết quả đánh giá trên tập test. Như được hiển thị trong bảng, phương pháp của chúng tôi giảm đáng kể tỷ lệ sinh độc hại so với tất cả các phương pháp baseline. Hơn nữa, phương pháp của chúng tôi có tác động tối thiểu đến tính trôi chảy, được đo bằng perplexity, đồng thời cũng duy trì mức độ đa dạng tương tự. Trong Bảng 3, chúng tôi so sánh phương pháp của mình với Fine-Grained RLHF (Wu et al., 2023) truy vấn API sử dụng các câu được sinh một phần để nhận phần thưởng tinh vi. Như được hiển thị trong Bảng 3, phương pháp của chúng tôi vượt trội hơn Fine-Grained RLHF về cả hiệu suất khử độc và chất lượng văn bản. Chúng tôi cũng trực tiếp prompt Llama 2 với hướng dẫn khử độc và ví dụ few-shot. Như được hiển thị trong Bảng 4, phương pháp của chúng tôi vượt trội hơn phương pháp dựa trên prompting.

4.3 Tóm tắt
Trong phần này, chúng tôi chứng minh cách phương pháp của chúng tôi hiệu quả cải thiện khả năng của mô hình ngôn ngữ sinh ra các bản tóm tắt được căn chỉnh tốt hơn với sở thích con người.

4.3.1 Thiết lập thực nghiệm
Chúng tôi sử dụng bộ dữ liệu Reddit TL;DR (Völske et al., 2017) cho thực nghiệm tóm tắt. Bộ dữ liệu chứa khoảng 3 triệu bài đăng được thu thập từ reddit.com, trải dài trên một loạt các chủ đề. Chúng tôi sử dụng phiên bản lọc của bộ dữ liệu gốc như được cung cấp bởi Stiennon et al. (2020), bao gồm khoảng 116K mẫu huấn luyện, 6K mẫu validation và test. Chúng tôi tinh chỉnh mô hình GPT2-large thông qua học có giám sát trên toàn bộ tập huấn luyện trong 9.000 bước, sử dụng batch size 64. Mô hình này phục vụ như khởi tạo cho mô hình chính sách. Để huấn luyện RL, chúng tôi tinh chỉnh mô hình chính sách trên 30K mẫu huấn luyện trong một epoch. Theo Stiennon et al. (2020), mô hình phần thưởng của chúng tôi là một mô hình ngôn ngữ 6B được tinh chỉnh trên bộ dữ liệu so sánh tóm tắt theo cặp được chú thích bởi con người 92K. Mô hình phần thưởng đạt độ chính xác 75.94% trên tập validation. Chúng tôi đặt α1 = 1.0 và α2 = 0.1. Chúng tôi sử dụng gpt-3.5-turbo để sinh phần thưởng nội tại trong thiết lập 3-shot. Chi tiết về prompt, ví dụ few-shot và các siêu tham số được áp dụng trong thực nghiệm tóm tắt được cung cấp trong Phụ lục C.

Baseline và chỉ số đánh giá. Chúng tôi so sánh phương pháp của mình với hai phương pháp baseline: baseline tinh chỉnh có giám sát (SFT) và baseline PPO. Để đánh giá chất lượng tóm tắt, chúng tôi sử dụng cả điểm ROUGE và điểm sở thích được tính bằng mô hình phần thưởng. Đáng nói là điểm ROUGE thường không đáng tin cậy và không nắm bắt được sở thích con người. Như được hiển thị trong Stiennon et al. (2020), điểm sở thích luôn vượt trội hơn điểm ROUGE với sự đồng ý tốt hơn với các chú thích viên con người về chất lượng tóm tắt.

4.3.2 Kết quả
Hình 5 hiển thị hiệu suất của tác nhân được đánh giá trên tập test TL;DR mỗi 100 bước huấn luyện. Như được chứng minh trong hình, phương pháp của chúng tôi vượt trội hơn baseline PPO về cả điểm sở thích và hiệu quả mẫu. Bảng 5 tiếp tục củng cố những phát hiện này, cho thấy việc kết hợp phần thưởng nội tại đạt được điểm sở thích cao hơn đáng kể so với baseline PPO.

Đánh giá con người. Chúng tôi thực hiện đánh giá con người trên 200 mẫu được chọn ngẫu nhiên từ tập test TL;DR. Chúng tôi thuê năm người đánh giá được chứng nhận IELTS để đánh giá chất lượng của các bản tóm tắt được sinh. Để chuẩn bị cho việc chú thích thực tế, một nghiên cứu thí điểm sơ bộ được thực hiện với một tập riêng biệt gồm 20 mẫu. Chúng tôi tập trung vào ba khía cạnh chất lượng chính: coverage, coherence và factuality. Kết quả, như được minh họa trong Hình 6, chứng minh rằng phương pháp của chúng tôi vượt trội hơn cả baseline PPO và học có giám sát trong tất cả các chiều chất lượng, với lợi thế đáng chú ý trong factuality. Hướng dẫn chú thích chi tiết và đánh giá thỏa thuận giữa các chú thích viên được cung cấp trong Phụ lục D.

5 Phân tích
5.1 Phần thưởng nội tại ngẫu nhiên
Để hiểu rõ hơn về đóng góp của phần thưởng nội tại được sinh bởi LLM cho tác nhân, chúng tôi thực hiện một thực nghiệm ablation trong đó phần thưởng nội tại được gán cho token trên cơ sở ngẫu nhiên. Chúng tôi sử dụng phương pháp trung bình động để ước lượng tỷ lệ token nhận phần thưởng nội tại từ LM phê bình thực, được ký hiệu là Pt = α * (# token phần thưởng nội tại) / (# token seq.) + (1−α) * Pt−1. Sau đó, phần thưởng nội tại được gán ngẫu nhiên cho mỗi token dựa trên Pt. Tất cả các siêu tham số bổ sung vẫn nhất quán với những tham số được mô tả trong Phần 4.2. Đường cong học từ nghiên cứu ablation này được trình bày trong Hình 7. Kết quả, như được minh họa, chỉ ra rằng việc tích hợp phần thưởng nội tại ngẫu nhiên không cải thiện quá trình học. Phát hiện này hỗ trợ kết luận rằng hiệu quả của phương pháp chúng tôi chủ yếu được quy cho việc gán nhận tín dụng chính xác bởi LLM phê bình.

5.2 Hiệu quả tính toán
Trong Phần 4, chúng tôi chứng minh rằng phương pháp của chúng tôi hiệu quả mẫu hơn so với baseline. Do chi phí tính toán bổ sung được LLM phê bình giới thiệu trong phương pháp của chúng tôi, phân tích này tìm cách đánh giá liệu phương pháp của chúng tôi có duy trì lợi thế so với baseline với lượng tính toán tương đương hay không. Chúng tôi báo cáo số lượng phép toán dấu phẩy động (FLOP) được sử dụng trong huấn luyện mô hình. Phân tích này được thực hiện trong bối cảnh thực nghiệm khử độc sử dụng Llama 2. Như được minh họa trong Hình 3, chúng tôi vẽ biểu đồ hiệu suất của mô hình so với số FLOP, cho thấy rõ ràng rằng phương pháp của chúng tôi đạt được hiệu suất tốt hơn baseline dưới cùng lượng tính toán.

6 Kết luận
Trong công trình này, chúng tôi giới thiệu một khung mới tích hợp LM phê bình để sinh tín hiệu phần thưởng nội tại dày đặc nhằm giảm bớt tính thưa thớt phần thưởng và vấn đề gán nhận tín dụng trong huấn luyện mô hình ngôn ngữ. Mô hình phê bình đánh giá các đoạn đầu ra của mô hình chính sách và tạo ra phần thưởng cấp token hoặc cấp đoạn. Những phần thưởng nội tại này được kết hợp với phần thưởng ngoại tại trong huấn luyện RL. Được đánh giá trên các nhiệm vụ kiểm soát cảm xúc, khử độc và tóm tắt, phương pháp của chúng tôi không chỉ cải thiện đáng kể hiệu quả mẫu của thuật toán PPO mà còn vượt trội hơn các phương pháp baseline sử dụng đánh giá tự động và con người.

7 Hạn chế
Khung của chúng tôi phụ thuộc vào mô hình phê bình để cung cấp phản hồi sâu sắc, điều này đòi hỏi mô hình phê bình không thể quá nhỏ. Yêu cầu này có thể hạn chế khả năng ứng dụng của phương pháp được đề xuất trong các thiết lập có tài nguyên tính toán hạn chế. Mặc dù việc truy cập LLM phê bình thông qua API là khả thi, thời gian huấn luyện có thể kéo dài do độ trễ liên quan đến API. Trong nghiên cứu của chúng tôi, chúng tôi coi mô hình phê bình là cố định trong quá trình huấn luyện. Tuy nhiên, khi mô hình chính sách cải thiện, việc đánh giá đầu ra của mô hình chính sách trở nên ngày càng thách thức. Do đó, sẽ có lợi khi cũng tinh chỉnh mô hình phê bình để nâng cao khả năng phê bình của nó. Chúng tôi dự định khám phá sự tinh chỉnh này trong công trình tương lai.

Tài liệu tham khảo
[Danh sách tài liệu tham khảo tiếp tục với định dạng tương tự...]
