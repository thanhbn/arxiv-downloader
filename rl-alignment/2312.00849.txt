# 2312.00849.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2312.00849.pdf
# File size: 1878472 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from
Fine-grained Correctional Human Feedback
Tianyu Yu1Yuan Yao2*Haoye Zhang1Taiwen He1Yifeng Han1
Ganqu Cui1Jinyi Hu1Zhiyuan Liu1∗Hai-Tao Zheng1∗Maosong Sun1Tat-Seng Chua2
1Tsinghua University2National University of Singapore
yiranytianyu@gmail.com yaoyuanthu@gmail.com
https://rlhf-v.github.io
Abstract
Multimodal Large Language Models (MLLMs) have re-
cently demonstrated impressive capabilities in multimodal
understanding, reasoning, and interaction. However, ex-
isting MLLMs prevalently suffer from serious hallucination
problems, generating text that is not factually grounded in
associated images. The problem makes existing MLLMs un-
trustworthy and thus impractical in real-world (especially
high-stakes) applications. To address the challenge, we
present RLHF-V , which enhances MLLM trustworthiness
via behavior alignment from fine-grained correctional hu-
man feedback. Specifically, RLHF-V collects human pref-
erence in the form of segment-level corrections on halluci-
nations, and performs dense direct preference optimization
over the human feedback. Comprehensive experiments on
five benchmarks in both automatic and human evaluation
show that, RLHF-V can enable substantially more trustwor-
thy MLLM behaviors with promising data and computation
efficiency. Remarkably, using 1.4k annotated data sam-
ples, RLHF-V significantly reduces the hallucination rate
of the base MLLM by 34.8%, outperforming the concur-
rent LLaVA-RLHF trained on 10k annotated data. The final
model achieves state-of-the-art performance in trustwor-
thiness among open-source MLLMs, and shows better ro-
bustness than GPT-4V in preventing hallucinations aroused
from over-generalization.
1. Introduction
The recent success of Multimodal Large Language Models
(MLLMs) marks a significant milestone in AI research [4, 6,
14, 16, 27, 35, 37, 54, 63]. By connecting visual signals and
Large Language Models (LLMs), MLLMs show unprece-
*Corresponding authorsdented capabilities in multimodal understanding, reasoning,
and interaction [36, 37, 57]. The models are typically pre-
trained on large-scale image-text data to learn the founda-
tional multimodal knowledge and capabilities [4, 6, 16, 27].
To steer the model behavior, most MLLMs are further fine-
tuned with instruction tuning (also known as supervised
fine-tuning), which supervises models to clone the behav-
ior from demonstration data, enabling MLLMs to engage
users in various open-world tasks [6, 14, 33, 35, 60].
However, current MLLM behaviors are not well aligned
with human preferences. A glaring issue is their tendency
to produce hallucinations — responses that are not factu-
ally grounded in the associated images [29, 33, 37, 48].
This typically includes descriptions of non-existing visual
contents and errors in descriptions. As shown in Figure 1,
current MLLMs can hallucinate about objects, attributes,
numbers, positions, actions, etc. Quantitatively, our hu-
man evaluation shows that the problem is prevalent among
state-of-the-art MLLMs, where even the most advanced
GPT-4V [37] contains obvious hallucinations in 45.9% re-
sponses. The problem makes existing MLLMs untrust-
worthy and thus impractical in real-world (especially high-
stakes) applications, such as guiding visually impaired in-
dividuals [37] or autonomous driving systems [55].
We argue that the problem arises from the lack of posi-
tive/negative human feedback in instruction-tuned models,
making it challenging to learn the precise behavior bound-
aries to exclude hallucination. To address the problem, we
propose RLHF-V , a novel framework that aligns MLLM be-
havior by learning from human feedback. A straightforward
way is to employ the traditional Reinforcement Learning
from Human Feedback (RLHF) method in state-of-the-art
LLMs [38, 51], which involves human annotators ranking
model responses, and utilizing a reward model to guide the
policy LLM learning. However, this approach is fraught
with two key challenges: (1) Annotation ambiguity . Help-arXiv:2312.00849v2  [cs.CL]  8 Mar 2024

--- PAGE 2 ---
Prompt: Please describe the image in detail.MLLM outputsFine-grained Correctional Human Feedback
Final MLLM
ABA*Dense Direct Preference Optimization (DDPO)B*InputsA*
(1) Human Feedback Collection(2) Human Preference LearningTheimagefeaturesalargeclocktowerwithaclockfaceoneachofitssides … Theclocksonthetowerdisplaythetimeas11:50.Thetowerisadornedwithorange and pink flags,addingafestivetouch…Theimageshowsaclocktower … Theclockreadsapproximately11:20 …Therearealsosomeflagsflyingin the top left corner of the image …Thereareseveralpeoplescatteredthroughoutthescene…Theimageshowsaclocktower … Theclockreadsapproximately15:26 …Therearealsosomeflagsflyingin the top right corner of the image …Therearetrees in the background behind the buildings…Theimagefeaturesalargeclocktowerwithaclockfaceonits front side … Theclockonthetowerdisplaysthetimeas15:26. There are red, green, yellow and blue flags,addingafestivetouch…A
···＜
···
Figure 1. The RLHF-V framework for MLLM behavior alignment from human feedback. (1) Given the input image and prompt, we obtain
outputs from MLLMs and collect human feedback in the form of fine-grained segment-level corrections onhallucinations . (2) During
human preference learning, we perform dense direct preference optimization over the fine-grained correctional human feedback.
ful and engaging responses about rich image content are
typically long and complex, making it usually non-obvious
to decide which response is preferable. As shown in Fig-
ure 1 (responses A and B), annotators usually face dilem-
mas when presenting responses with respective advantages
and flaws. Besides, even if labeled with a clear prefer-
ence, the optimal response remains unknown (e.g., the exact
time of the clock). (2) Learning efficiency . The coarse-
grained ranking feedback makes it difficult to accurately
allocate credit to the desirable behaviors. Considering the
linguistic complexity and variance of responses, the desir-
able behavior often requires a large amount of labeled data
to learn [13, 39, 48]. Moreover, misallocation of credit to
the non-robust bias correlated with the data usually leads to
reward hacking and behavior degeneration problems [7, 51].
RLHF-V addresses these challenges by introducing two
key innovations: (1) At the data level, we propose to col-
lect human feedback in the form of fine-grained segment-
level corrections. As shown in Figure 1, we ask human
annotators to directly correct the hallucinated segments
from model responses, providing a clear, dense, and fine-
grained human preference, as well as optimal responses.
This strategy also avoids linguistic variance and non-robust
bias, ensuring that the feedback is accurately allocated to
the desirable behaviors, thereby enhancing learning effi-
ciency and preventing reward hacking problems. (2) At
the method level, we propose dense direct preference op-
timization (DDPO), a new variant of DPO [42] that ad-
dresses the traditional RLHF objective in an equivalent sim-
ple and efficient supervised fashion. DDPO directly op-
timizes the policy model against dense and fine-grained
segment-level preference, where the hallucinated segments
receive stronger feedback to be factually grounded.
Comprehensive experiments on five benchmarks show
that, RLHF-V can substantially enhance the trustworthiness
of MLLMs with promising data and computation efficiency.Using 1.4k preference data, RLHF-V significantly reduces
the object hallucination rate of the base MLLM by 34.8%,
surpassing the concurrent LLaV A-RLHF [48] trained on
10k preference data. We also show that RLHF-V achieves
better robustness than the strong GPT-4V [37] in preventing
hallucinations aroused from over-generalization.
The contribution of this work can be summarized as
threefold: (1) We present RLHF-V , a novel framework that
aligns MLLM behavior through fine-grained correctional
human feedback. (2) We collect high-quality human pref-
erence data to provide human-aligned learning signals for
MLLMs. (3) We conduct comprehensive experiments to
demonstrate the effectiveness of the proposed framework,
achieving state-of-the-art performance in trustworthiness
among open-source MLLMs. All the code, data, and model
weights are open-sourced at https://github.com/
RLHF-V/RLHF-V .
2. Human Preference Collection
The goal of human preference data is to distinguish human-
preferred high-quality responses from inferior ones, provid-
ing human-aligned learning signals to steer the MLLM be-
haviors. We first provide an analysis of underlying factors
of human preference data, based on which we motivate the
human preference collection procedure of RLHF-V .
Human Preference Data: Underlying Factors and
Challenges. Given the input x(including the image and
the prompt), denote the difference between a preferred out-
putywand an inferior output ylasY. The difference Ycan
be essentially decomposed into three factors:
Y=Yp+Ys+Yn, (1)
where Ypis the truly preferred behavior such as being trust-
worthy and helpful, Ysdenotes the shallow non-robust bias
correlated with the data but unrelated to human judgment

--- PAGE 3 ---
(e.g., ywcontains more usage of specific words), and Yn
is the random noise factor denoting the linguistic variance
of natural language (e.g., different ways of expressing the
same meaning). Ypis the factor we want to learn from the
difference Y, while fitting to Yscan lead to reward hack-
ing problems and thus should be avoided. The linguistic
variance Yndoes not bias the preference learning but makes
the learning more difficult, demanding more labeled data
to learn to the preferred factor Yp, and thus should also be
avoided if possible.
The common RLHF practices in LLMs collect human
preference Yin the form of ranking labels, indicating the
overall relative quality of responses [38, 39, 51]. According
to the above analysis, the practice faces several key chal-
lenges: (1) Annotation ambiguity. It can be non-obvious to
annotate which response is superior using an overall rank-
ing label due to the fine-grained nature of Yp, especially for
complex responses. As shown in Figure 1, annotators usu-
ally cannot agree on assigning an overall ranking to differ-
ent responses with respective advantages and flaws. We ob-
serve the issue leads to unsatisfactory annotation quality of
existing RLHF data. Moreover, even if labeled with a clear
preference, the optimal responses for the questions typically
remain unknown. (2) Learning efficiency. During reinforce-
ment learning, it can be challenging and data-demanding to
precisely allocate the sparse and coarse-grained credit from
Ythrough the linguistic variance Ynto the preferred behav-
iorYp. Misallocation to the non-robust bias factor Yswill
lead models to collapse to exploit trivial rewards [7, 51].
Fine-grained Correctional Human Preference Collec-
tion. To address the challenges, we propose to collect fine-
grained human preferences in the form of segment-level
corrections. As shown in Figure 1, given a flawed output yl
from MLLMs, we ask human annotators to directly correct
the hallucinated segments, resulting in a factually optimal
output yw. The annotation simultaneously yields a segment-
level incremental preference pair ( yw,yl). The simple pro-
cedure effectively addresses the challenges: (1) The an-
notation of incremental correction in segments is clearer
and more operable for human labelers. (2) The dense and
fine-grained feedback is directly allocated to the preferred
behavior Yp, excluding the linguistic variance Ynand the
non-robust bias Ys, therefore improving learning efficiency
and preventing reward hacking problems. In experiments,
we find that the procedure greatly improves the annotation
quality and data efficiency, enabling our model to surpass
concurrent models trained on an order of magnitude more
labeled preference data (see Section 4.3).
In practice, we obtain a total of 1.4k prompts as in-
put from existing instruction tuning dataset [60] and im-
age description prompts generated by GPT-4, and get the
responses from Muffin [60] for human annotation. The re-
sponses after annotation contain 64.4 words and 2.65 cor-rected segments on average. We observe that the correc-
tions are diverse in hallucination types, including objects
(41.2%), positions (20.3%), numbers (16.5%), attributes
(10.0%), actions (5.3%) and miscellaneous types (6.8%).
3. Method
We introduce the RLHF-V approach that learns the fine-
grained correctional human feedback by dense direct pref-
erence optimization. In addition, we also mitigate existing
sources of hallucination in MLLM training by addressing
the vision-language mismatch problem.
3.1. Dense Direct Preference Optimization
To leverage the dense and fine-grained human feedback, we
present DDPO, a new variant of direct preference optimiza-
tion [42] for directly optimizing the MLLM policy against
dense human preference. The prevalent RLHF approaches
involve fitting a reward model on the preference data, and
then training the critique, policy and value models to maxi-
mize the reward without deviating too far from the reference
model [13, 39, 51]. This procedure requires training multi-
ple LLMs with extensive sampling and training, which suf-
fers from complex procedures and high computation cost.
Direct Preference Optimization (DPO) [42] solves this
reinforcement learning objective in a simpler equivalent
supervised fashion. Here we briefly introduce the DPO
method, and refer readers to the original paper for more
details. The key observation of DPO is that the reward
function r(x, y)can be analytically expressed by its opti-
mal policy model π∗(y|x)and reference model πref(y|x),
and therefore we can directly optimize the policy model un-
der proper forms on the preference data. Specifically, the
reward model r(x, y)can be represented as:
r(x, y) =βlogπ∗(y|x)
πref(y|x)+βlogZ(x), (2)
where βis a constant and Z(x)is the partition function.
Based on this observation, the policy model can be directly
optimized on the human feedback data:
L=−E(x,yw,yl)
logσ(r(x, yw)−r(x, yl))
=−E(x,yw,yl)
logσ(βlogπ∗(yw|x)
πref(yw|x)−βlogπ∗(yl|x)
πref(yl|x))
,(3)
where the reference model πref(y|x)is usually implemented
by an instruction-tuned base model we want to improve, and
is kept fixed during DPO training. Only the policy model
π∗(y|x)is updated. We note that DPO is more simple, ef-
ficient and stable in aligning MLLM behaviors compared
with traditional RLHF approaches.
Leveraging dense and fine-grained segment-level feed-
back essentially requires the model to evaluate the reward
of segment-level actions. However, DPO is designed for

--- PAGE 4 ---
learning preference in the form of overall response ranking
labels. Specifically, the action score of DPO is given by the
likelihood of the holistic response in practice, where differ-
ent segments are equally treated:
logπ(y|x) =X
yi∈ylogp(yi|x, y<i), (4)
where yiis the i-th token of the response y. We argue that
compared with unchanged segments yu, corrected segments
ycmore directly reveal human judgment in hallucination,
and thus should contribute more to the overall action eval-
uation. Therefore, we propose to score the response as a
weighted aggregation of the fine-grained segments:1
logπ(y|x) =1
NP
yi∈yulogp(yi|x, y<i) +γP
yi∈yclogp(yi|x, y<i)
,(5)
where γ > 1is a weighting hyperprameter, and larger
γmeans more contribution from the corrected segments.
N=|yu|+γ|yc|is a normalizing factor, preventing longer
responses from getting higher scores. In this way, corrected
segments are highlighted to receive stronger human prefer-
ence feedback to be factually grounded. In experiments, we
find that DDPO can better exploit the fine-grained human
feedback, leading to more trustworthy responses.
3.2. Mitigating Hallucination from VL Mismatch
DDPO reduces hallucination by learning from human feed-
back. From another cause-and-effect view, we examine the
mainstream MLLM training paradigm, and identify sources
of hallucinations in training MLLMs. Based on the obser-
vations, we motivate a more trustworthy training recipe.
In general, current MLLMs learn multimodal capabili-
ties in a supervised learning paradigm, where the model out-
puts are supervised against the ground-truth text associated
with the image. In such a paradigm, hallucinations can be
introduced by mismatches between images and text data. In
practice, the mismatch can come from: (1) low-quality text
in pre-training and instruction tuning data, and (2) careless
image augmentation during training. We specify the issues
and solutions in the following.
Addressing Low-quality Text Influence. Current pre-
training data of MLLMs are automatically crawled from the
Web [9, 10, 44], which inevitably suffers from severe noise
in the text even after extensive post-processing. Supervising
MLLMs against such data is essentially teaching them to
hallucinate (e.g., describing elements not present in the im-
age, or producing inconsistent descriptions with the image).
Similarly, most existing visual instruction tuning datasets
are generated by ChatGPT/GPT-4 according to intermediate
text annotations [33, 35, 60], which inevitably introduces
1For denotation simplicity, without confusion we also use yuandycto
denote the set of tokens in unchanged and corrected segments respectively.hallucination into instruction data. While it can be difficult
to repair existing pre-training and instruction-tuning data,
we find that the influence can be countered by simply post-
training MLLMs on high-quality visual question-answering
datasets. Intuitively, human-labeled datasets can provide
accurate learning signals to calibrate model behaviors from
hallucinations, and also enhance instruction-following ca-
pabilities. In our experiments, we find that simply fine-
tuning the model on VQAv2 [18] can significantly reduce
the hallucination rate (see Section 4.3).
Mitigating Untrustworthy Image Augmentation. The
vision-language mismatch can also come from the image
domain. Data augmentation is widely adopted to improve
the data diversity and model robustness in various multi-
modal models [14, 27, 41, 53, 60]. However, we note that
such augmentation must be performed with care in training
MLLMs. The key problem is that some image augmen-
tation operations can significantly change the semantics of
images, which may make the augmented image inconsis-
tent with the associated text. For example, during augmen-
tation, random cropping can make the objects mentioned in
the text absent from the image. This can make the model
describe non-existing objects, with wrong numbers, and in
wrong positions. In our model training, we exclude image
cropping in data augmentation, which improves the trust-
worthiness of MLLMs (see Section 4.3).
4. Experiments
In this section, we empirically investigate the effectiveness
of RLHF-V in aligning MLLM behaviors. In addition to
evaluating the trustworthiness and helpfulness of conversa-
tion, we also analyze the data efficiency and scalability as
well as the robustness. We refer readers to the appendix for
more details on benchmarks, baselines and results.
4.1. Experimental Settings
We first introduce the experimental settings, including eval-
uation, baselines, and implementation details.
Evaluation. We evaluate the models from two perspec-
tives, including trustworthiness reflecting the hallucination
degree, and helpfulness reflecting the general interaction
quality. Similar to [48], we find binary classification eval-
uation (i.e., answering yes/no) [17, 29] cannot well reflect
the MLLM behaviors in open-ended long-form interactions.
We thus adopt benchmarks that directly evaluate the long-
form responses, which are more closely related to the prac-
tical usage scenarios of MLLMs. For trustworthiness, we
perform evaluation on three benchmarks:
(1)Object HalBench [43] is a widely adopted bench-
mark for assessing object hallucination in detailed image
descriptions. It compares the objects in the model output
with object labels exhaustively annotated for COCO im-

--- PAGE 5 ---
ModelObject HalBench ↓ MHumanEval ↓ MMHal-Bench LLaV A Bench VQAv2
Resp. Mention Object Position Number All Info. Resp. ↓Conv. Detail Comp. testdev
LLaV A [35] 63.0 29.5 46.6 21.2 19.9 80.8 31.9 70.8 85.4 74.3 96.3 -
Muffin [60] 50.5 24.5 33.6 16.4 26.0 74.7 33.4 68.8 89.3 79.7 97.7 -
LRV [33] 32.3 22.3 43.2 11.6 19.2 82.9 22.2 78.1 61.7 47.3 55.0 -
LLaV A-RLHF [48] 38.1 18.9 37.7 17.8 18.5 72.6 39.9 65.6 93.8 74.3 111.4 -
InstructBLIP [14] 25.9 14.3 30.8 15.1 17.1 63.7 29.5 64.4 83.2 67.6 90.6 -
Qwen-VL-Chat [6] 43.8 20.0 34.9 16.4 15.8 61.0 38.5 52.1 81.9 77.1 92.3 79.5
LLaV A 1.5 [34] 46.3 22.6 30.8 17.8 17.1 61.0 39.2 52.1 81.6 75.5 95.2 80.0
RLHF-V 12.2 7.5 21.9 7.5 14.4 55.5 40.0 52.1 93.1 75.3 91.6 80.0
GPT-4V [37] 13.6 7.3 22.6 12.3 11.0 45.9 47.6 31.3 96.0 102.5 106.7 77.2*
Table 1. Main experimental results on hallucination. We report hallucination rates in different granularities, including response-level
(Resp.) and mention-level (Mention), and response-level hallucination rates in different types. We also show scores on informativeness
(Info.), multimodal conversation (Conv.), detailed description (Detail), and complex reasoning (Comp.). * denotes zero-shot results on
VQAv2.2The best and second best open-source results are shown in bold and underlined respectively.
ages [31] to detect object hallucination. To improve the
evaluation stability, we augment the benchmark with 8 di-
verse prompts for detailed image descriptions. We report
the response-level hallucination rate (i.e., the percentage of
responses that have hallucinations), as well as the mention-
level hallucination rate (i.e., the percentage of hallucinated
object mentions among all object mentions).
(2)MMHal-Bench [48] evaluates hallucinations and re-
sponse informativeness. It employs GPT-4 to compare
model output with human response and several object labels
to decide the scores. In experiments, we find that GPT-4
cannot reliably detect hallucinations due to the incomplete-
ness of MMHal-Bench text annotations. We therefore only
report the informativeness score from GPT-4, and assess
response-level hallucination rate by human evaluation.
(3)MHumanEval. The above evaluations are either
limited to common object hallucination or dominated by
short-form question answering (i.e., questions that can be
sufficiently answered by a few words). To provide a more
reliable and comprehensive evaluation over diverse halluci-
nation types, we present MHumanEval benchmark, which
covers both long-form image descriptions, and short-form
questions. The benchmark contains 146 samples collected
from Object HalBench (50) and MMHal-Bench (96). Given
model responses, we ask human annotators to label the hal-
lucinated segments and hallucination types of the segments,
including objects, positions, numbers and others. We report
the response-level hallucination rate on these types.
For helpfulness, we adopt two benchmarks: (1) LLaV A
Bench [35] is a widely adopted benchmark for assessing
multimodal conversation, detailed description and complex
reasoning capabilities. It scores model output against ref-
erence response via GPT-4. (2) VQAv2 [18] is a popular
dataset for short-form visual question answering.
2Due to limited instruction-following capability, most MLLMs need toBaselines. We compare our model with state-of-the-
art baselines. (1) General baselines. We adopt Qwen-
VL-Chat [6], LLaV A [35], LLaV A 1.5 [34], Muffin [60],
and InstructBLIP [14] as representative general baselines.
These models are mostly pre-trained on large-scale multi-
modal data, and fine-tuned on high-quality instruction data,
achieving strong performance across various multimodal
tasks. (2) Baselines tailored for hallucination problems.
LRV [33] is fine-tuned on 400k instruction data generated
by GPT-4, and mitigates hallucination by limiting the re-
sponse length. The concurrent LLaV A-RLHF [48] employs
the strong 13B Vicuna v1.5 [62] (fine-tuned from LLaMA-
2 [51]) as LLM backbone. It trains the reward model on
10k human-labeled preference data, and performs proximal
policy optimization [45] on 72k factually augmented data.
(3)Commercial Baseline. We also include GPT-4V [37]
as a strong reference, evaluating the gap between the open-
source models and state-of-the-art commercial models.
Implementation Details. We implement the RLHF-V
framework based on Muffin [60]. The model uses BEiT-
3 [53] as the visual module, and 13B Vicuna v1.0 [12] (fine-
tuned from LLaMA [50]) as the LLM backbone. The hyper-
parameter βis 0.5, and the weighting coefficient γis 5. We
train the model with DDPO for 7 epochs, with image resolu-
tion 448, learning rate 5e-7 and batch size 32. The training
of RLHF-V is computationally efficient, which takes less
than 1 hour on 8 A100 GPUs in total.
4.2. Main Results
The main experimental results are reported in Table 1, from
which we observe that: (1) RLHF-V achieves state-of-
the-art performance in trustworthiness among open-source
models, outperforming strong general models and models
be specifically fine-tuned to produce short-form VQA answers, and there-
fore cannot achieve reasonable zero-shot performance on VQAv2.

--- PAGE 6 ---
ModelLiving Room Kitchen Bathroom Street
∆book, person, bed bottle, bowl, cup toilet, sink, bottle person, car, motorcycle
chair, couch, remote person, chair, knife toothbrush, person, cup traffic light, handbag, truck
Ha Hs ∆ Ha Hs ∆ Ha Hs ∆ Ha Hs ∆
LLaV A-1.5 [34] 25.2 41.8 +16.6 18.9 23.9 +5.0 22.4 30.4 +8.0 20.6 28.0 +7.4 +9.2
LLaV A-RLHF [48] 23.7 34.5 +10.8 13.1 17.4 +4.3 18.2 19.5 +1.4 18.3 22.7 +4.4 +5.2
QWEN-VL [6] 24.5 34.5 +10.0 16.4 20.8 +4.4 21.6 17.5 -4.1 22.5 32.0 +9.5 +5.0
RLHF-V 5.5 8.0 +2.5 3.8 5.9 +2.1 4.1 4.0 -0.1 2.3 4.6 +2.3 +1.7
GPT-4V [37] 8.2 19.4 +11.2 4.6 5.7 +1.1 5.9 13.3 +7.5 4.2 4.6 +0.4 +5.0
Table 2. Experimental results of hallucination from over-generalization on Object HalBench. For each scene, we report the hallucination
rate of the top 10 frequent objects on average on the full benchmark (H a) and under the scene (H s). Top 6 frequent objects are listed for
each scene for brevity. ∆: hallucination rate difference, ∆: average difference across the scenes.
0200 800 1400 220056606468Hallucination Rate
LLaVA-1.5LLaVA-RLHF Data
Our Data
0200 800 1400 2200
Data Scale110130150170Hallucination Count
LLaVA-1.5
Figure 2. Hallucination rate and number on MHumanEval (all
types) with respect to the amount of preference data. We report
the results of different models trained on different RLHF data.
tailored for hallucination. The framework significantly re-
duces the hallucination rate of the base model Muffin by
75.8% relative points for common objects on Object Hal-
Bench, and by 34.8% for overall objects on MHumanEval.
The improvement is consistent in different granularities
including response-level and mention-level hallucinations,
and different hallucination types including objects, posi-
tions, and numbers. The reduction is more significant on
the more challenging long-form answers on Object Hal-
Bench and MHumanEval. The results show that RLHF-V
can effectively learn from fine-grained correctional human
feedback to enable more trustworthy MLLM behaviors. (2)
RLHF-V achieves promising performance in response help-fulness, where the results on MMHalBench, LLaV A Bench
and VQAv2 are strong and comparable to the base model.
This shows that RLHF-V can enhance the trustworthiness
of MLLMs without sacrificing their helpfulness.
4.3. Analysis
In this section, we conduct analyses on the framework con-
sidering the following research questions: (1) How does
RLHF-V’s performance scale with feedback data amount?
(2) What is the advantage of fine-grained correctional pref-
erence data over traditional overall ranking data? (3) Can
RLHF-V’s data and method be adopted to enhance the trust-
worthiness of other MLLMs? (4) How does human feed-
back alleviate hallucinations intuitively?
Scaling feedback data leads to promising results. We
report the hallucination rate and numbers of hallucinated
segments on MHumanEval under different amounts of feed-
back data in Figure 2. We observe that the hallucination
rate and number of RLHF-V show a significant and rapid
decrease as the data amount grows. This shows that fine-
grained correctional human feedback provides effective and
efficient learning signals for MLLM behavior alignment.
Based on this tendency, we expect better performance can
be achieved with an increasing amount of feedback data.
We leave this for future work.
Fine-grained correctional human feedback enables
better learning efficiency. To quantify the advantage of
fine-grained correctional human feedback, we replace our
data with the 2.2k human preference data on hallucination
from LLaV A-RLHF, which gives overall ranking labels fol-
lowing common RLHF practices. From the experimental
results in Figure 2, we observe that model equipped with our
data shows a more significant and rapid reduction in halluci-
nation rate and number. Notably, using only 200 preference
data, our model achieves comparable hallucination rate to
the model that uses an order of magnitude more labeled data
from LLaV A-RLHF. The superior data efficiency is due to
(1) better data quality since label ambiguity is minimized,

--- PAGE 7 ---
ModelMHumanEval ↓ MHB↓VQAv2
Obj. Pos. Num. All Resp. testdev
Muffin [60] 33.6 16.4 26.0 74.7 68.8 -
RLHF-V 21.9 7.5 14.4 55.5 52.1 80.0
w/ vanilla DPO 21.9 11.6 11.6 57.5 54.2 80.0
w/ IT-VQA only 34.3 17.1 17.1 65.1 58.3 80.0
w/ untrust aug. 18.5 13.7 14.4 59.6 54.2 77.1
Table 3. Ablation results on different components. MHB:
MMHal-Bench, IT-VQA: instruction tuning on VQAv2, untrust
aug.: untrustworthy data augmentation.
and (2) more direct feedback on hallucinated segments, ex-
cluding non-robust bias and linguistic variance.
RLHF-V generalizes to enhance other MLLMs. To
investigate the generalization capability of the framework,
we adopt RLHF-V’s data and approach to align the be-
havior of LLaV A [35], a representative and widely used
MLLM. Experimental results show that RLHF-V effec-
tively reduces the hallucination count of LLaV A by 13.8
relative points, as well as the hallucination rate by 5.9
relative points. We also apply RLHF-V to stronger base
models and build the OmniLMM-12B [3] which achieves
new SoTA results on multiple hallucination benchmarks.
For example, OmniLMM-12B exhibits only 4.5% mention-
level hallucination on the Object HalBench. Moreover,
OmniLMM-12B also shows leading performance among
comparable-sized models on multiple benchmarks (1637 on
MME-Perception [17], 71.1 on SeedBench-I [25]). The re-
sults demonstrate that RLHF-V is applicable across differ-
ent MLLMs to improve trustworthiness.
RLHF-V reduces hallucination from correlation and
over-generalization. LLMs possess rich world knowledge
and strong generalization capabilities. Without proper posi-
tive/negative human feedback, MLLMs can over-generalize
to produce highly correlated and plausible concepts, which
leads to hallucinations. For example, a prevalent hallucina-
tion case observed across different MLLMs is claiming the
presence of person as long as they see an image of street .
To quantify the problem, we select a set of representative
scenes {living room ,kitchen ,bathroom ,street}. For each
scene, we identify the corresponding images in COCO by
lexically matching the captions with the scene name. Then
we obtain the top 10 frequent objects in the scene from the
COCO object annotations. We compare the response-level
hallucination rate for these objects (1) on average across
all test samples, and (2) on samples under the target scene.
Models prone to over-generalization will expect a signifi-
cant increase in the hallucination rate ( ∆).
From the experimental results in Table 2, we observe
that: (1) All models including GPT-4V show a substan-
tial increase in the hallucination rate, which demonstrates
the over-generalization hypothesis. (2) RLHF-V exhibitsthe smallest change in the hallucination rate, which is
even more robust than GPT-4V . The reason for the robust-
ness is that RLHF-V provides crucial positive/negative fine-
grained correctional human feedback for MLLMs, which
helps to learn clear behavior boundaries between reason-
able generalizations and over-generalizations. (3) RLHF-
V achieves the lowest hallucination rates for these com-
mon objects both on average and especially under common
scenes. This makes RLHF-V preferable in practical real-
world applications.
Ablation Study. To investigate the contribution of each
component, we perform an ablation study. From the experi-
mental results in Table 3, we can observe that: (1) Learning
human feedback with vanilla DPO leads to performance de-
grades, showing the advantage of DDPO in exploiting the
fine-grained human preference. (2) Fine-tuning on VQAv2
leads to a significant reduction in hallucination rates com-
pared with the base model. This reveals the value of tra-
ditional human-annotated datasets from a new perspective
of hallucination mitigation. (3) Including untrustworthy
data augmentation (i.e., image cropping) in training hurts
the performance on both hallucination and VQAv2. This
shows that careless data augmentation can be a double-
edged sword in training MLLMs.
Case Study. To provide an intuitive understanding and
comparison of different models, we provide qualitative re-
sults in Figure 3. We show cases in two representative sce-
narios: (1) Short-form QA (i.e., questions that can be suf-
ficiently answered in a few words). Our model typically
maintains a good balance between helpfulness, engage-
ment and clarity. In comparison, LLaV A-RLHF is usually
far more engaging, introducing lengthy extensions however
that can be less reasonable or relevant. (2) Long-form QA
(i.e., questions that require long text to answer). We ob-
serve that MLLMs are significantly more prone to halluci-
nations in long-form QA, since it typically requires more
comprehensive capabilities from multiple perspectives. For
example, InstructBLIP and LLaV A-RLHF can confidently
describe non-existing objects in a large proportion of their
responses, whereas RLHF-V introduces significantly fewer
hallucinations while delivering a comparable amount of ef-
fective information. We refer readers to the appendix for
more qualitative results.
5. Related Work
Multimodal Large Language Models. Recent trends in
multimodal learning have witnessed the success of build-
ing MLLMs by connecting visual encoders with pow-
erful LLMs [11, 20, 26, 58, 61]. The current MLLM
training paradigm typically involves two stages: (1) Pre-
training. Models are pretrained on large-scale image-text
pairs [6, 14, 27, 54, 60] or interleaved data [4, 5, 20] to

--- PAGE 8 ---
At what time is this photo taken?
The scene appears to be taken during the daytime, as the sunlight can be seen streaming in through the large window in the kitchen. The bright lighting creates a welcoming and inviting atmosphere, making the kitchen feel more spacious and vibrant.The photo is taken during the daytime.
RLHF-V(This work)
InstructBLIPThe photo is taken during the daytime, as indicated by the presence of natural light and the absence of any artificial lighting. The sunlight streaming in through the windows and the overall brightness of the room suggest that the photo is taken during daylight hours. This could be either in the morning or afternoon, depending on the specific time of day and the position of the sun relative to the windows.The presence of various objects in the room, such as bottles, cups, bowls, and a potted plant, further supports the idea that the photo is taken during the daytime. These objects are likely to be in use or placed in their respective locations during the daytime, as people tend to engage in more activities and prepare meals during the day.In conclusion, the photo is taken during the daytime, as evidenced by the natural light,the absence of artificial lighting, and the presence of various objects in the room that are typically used during daylight hours.LLaVA-RLHF
Short-formQA
What is this photo about? Please answer in detail.
The image depicts a small black dog sitting on a wooden floor in a living room, watching a television screen. The television appears to bemountedon the wall, and it's showing an image of another dog.The image depicts a black dog sitting on the floor in front of a large TV screen. The TV screen is displaying an image of a dog,which appears to be the same breed as the dog in the room. The dog seems to be engrossed in the TV program and is focused on the screen. There are several other dogs visible in the room, but they are not as close to the TV as the main subject. One dog is located near the left side of the TV, while another is positioned closer to the center of the room. A third dog can be seen towards the right side of the image. In addition to the dogs, there are twochairs in the room, one located near the left side of the TV and the other near the center of the room.
RLHF-V(This work)
InstructBLIP
The image features a black dog sitting on a hardwood floor in a room, watching a television. The TV is located on the left side of the room, and the dog is positioned near the center of the scene.There are also two people in the room, one on the left side and the other on the right side, possibly watching the television along with the dog. Additionally, there are two remotes visible in the room, one near the center and the other on the right side.A chair can be seen in the room, placed on the right side, and a bowl is located on the left side of the room, possibly for the dog's food or water.LLaVA-RLHF
Long-formQAFigure 3. Qualitative results of different models on short-form QA and long-form QA. Correct answers ,unreasonable extensions and
hallucinations are highlighted in color respectively.
learn the semantic mapping between visual and text sig-
nals. (2) Instruction Tuning. To enable the model with
instruction-following capability, MLLMs are further fine-
tuned on visual instruction data, including collections of
existing human-annotated datasets [14, 28, 34], and gen-
erated data from ChatGPT/GPT-4 [28, 33, 35, 60]. Despite
the success, current MLLMs suffer from serious hallucina-
tion problems [29, 32, 33, 48]. Notably, even after extensive
efforts, GPT-4V has still been found to be prone to halluci-
nations, making basic factual errors confidently [37]. The
problem undermines practical applications of MLLMs es-
pecially in high-stakes scenarios, which has recently drawn
increasing attention from the community.
Behavior Alignment for LLMs. Aligning language
agent behaviors with human preference has emerged as a
promising research direction [22, 24]. Pivotal approaches
in LLMs include instruction tuning (or supervised fine-
tuning) and RLHF [39, 47]. While supervised fine-tuning
is suitable for basic behavior alignment [15, 49], due to the
mismatch between likelihood maximization objective and
human preference, it may introduce or amplify hallucina-
tion [38, 39]. Therefore, RLHF is widely accepted for fur-
ther behavior and preference alignment [8, 13, 38], whereproximal policy optimization (PPO) [45] is recognized as
the major technique. Later adaptations attempt to stabilize
the optimization process [42] and enclose more fine-grained
signals [30, 56]. However, RLHF has rarely been explored
in MLLMs to align model behaviors with humans.
Reducing Hallucination for MLLMs. Some prelimi-
nary efforts have been made to alleviate hallucination prob-
lems in MLLMs. LRV [33] generates instruction data with
negative responses, and mitigates hallucination by limit-
ing the response length. However, limiting the response
length does not essentially address the problem, and also
undermines the response helpfulness. VIGC [52] itera-
tively refines the instruction data for better instruction tun-
ing. Woodpecker [59] proposes to post-edit hallucinations
by merging the output of MLLMs and a more accurate ex-
pert VQA model using GPT-3.5. The post-editing proce-
dure involves external tools and LLMs much larger than the
target MLLM online in multiple stages, which leads to high
inference costs and delays. Gunjal et al. [19] distinguishes
the inaccurate parts in responses via human annotation, and
internally discourages the hallucinated parts by direct pref-
erence optimization. However, the positive behaviors for
hallucinated parts are unknown, making the human feed-

--- PAGE 9 ---
back not complete enough to learn the behavior boundary.
The concurrent LLaV A-RLHF [48] employs the traditional
RLHF approach [39] on MLLMs, and augments the reward
model with rich additional text descriptions. It is there-
fore similarly challenged with label ambiguity, learning ef-
ficiency, and complex training. In comparison, RLHF-V
presents the first fine-grained correctional human feedback
learning framework for behavior alignment, and systemat-
ically addresses different hallucination sources in training
MLLMs, achieving strong performance in trustworthiness.
6. Conclusion
Hallucination is a critical problem preventing practical ap-
plications of MLLMs in real-world scenarios. In this work,
we present RLHF-V , a novel framework that enhances the
trustworthiness of MLLMs by behavior alignment from
fine-grained correctional human feedback. Comprehensive
experimental results show that our model achieves state-of-
the-art performance in trustworthiness especially in chal-
lenging long-form responses while maintaining strong help-
fulness. In this work, we collect correctional feedback from
human annotators. In future, with the progress of more
trustworthy and capable MLLMs, we will explore collect-
ing accurate preferences from MLLMs, which can facilitate
large-scale preference learning for stronger behavior align-
ment. Besides, we note that the framework of RLHF-V
can potentially also help reduce the hallucinations in LLMs,
which we will explore in future.
Contributions
The authors’ contributions can be outlined as follows:
• In initializing the project, Yuan Yao and Tianyu Yu de-
sign the framework to collect correctional human feed-
back. Tianyu Yu devise the DDPO algorithm. Zhiyuan
Liu, Hai-Tao Zheng, Maosong Sun and Tat-Seng Chua
offer invaluable guidance in project design.
• In data collection, Taiwen He, Haoye Zhang, Tianyu Yu
and Yuan Yao take charge of the annotation process to
ensure the data quality.
• In model training and evaluation, Tianyu Yu implements
the training framework. Tianyu Yu, Haoye Zhang and
Yuan Yao design the evaluation framework. Tianyu Yu
and Haoye Zhang implement the evaluation codebase.
• In paper writing, Yuan Yao and Tianyu Yu write the pa-
per. Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui,
Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun and Tat-Seng
Chua offer suggestions to polish the writing.
• For public usability, Tianyu Yu, Yifeng Han, Jinyi Hu and
Yuan Yao promote the open-source project.
• Throughout the project, Zhiyuan Liu, Hai-Tao Zheng,
Maosong Sun and Tat-Seng Chua provide invaluable
guidance and advice.References
[1] GPT-4 vision document. https : / / platform .
openai . com / docs / guides / vision /
calculating-costs . Accessed: 2023-11-20. 12
[2] John schulman - reinforcement learning from human feed-
back: Progress and challenges. https : / / www .
youtube.com/watch?v=hhiLw5Q_UFg . Accessed:
2023-11-20. 12
[3] Large multi-modal models for strong performance and effi-
cient deployment. https://github.com/OpenBMB/
OmniLMM . Accessed: 2024-03-05. 7
[4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,
Katherine Millican, Malcolm Reynolds, et al. Flamingo: a
visual language model for few-shot learning. NeurIPS , 35:
23716–23736, 2022. 1, 7
[5] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf
Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,
Samir Gadre, Shiori Sagawa, et al. OpenFlamingo: An open-
source framework for training large autoregressive vision-
language models. arXiv preprint arXiv:2308.01390 , 2023.
7
[6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren
Zhou. Qwen-VL: A frontier large vision-language model
with versatile abilities. arXiv preprint arXiv:2308.12966 ,
2023. 1, 5, 6, 7, 13
[7] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell,
Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,
Deep Ganguli, Tom Henighan, et al. Training a helpful and
harmless assistant with reinforcement learning from human
feedback. arXiv preprint arXiv:2204.05862 , 2022. 2, 3
[8] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda
Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna
Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Con-
stitutional AI: Harmlessness from AI feedback. arXiv
preprint arXiv:2212.08073 , 2022. 8
[9] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun
Lee, Woonhyuk Baek, and Saehoon Kim. COYO-700M:
Image-text pair dataset, 2022. 4
[10] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
Soricut. Conceptual 12M: Pushing web-scale image-text
pre-training to recognize long-tail visual concepts. In CVPR ,
pages 3558–3568, 2021. 4
[11] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,
Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam
Grycner, Basil Mustafa, Lucas Beyer, et al. PaLI: A jointly-
scaled multilingual language-image model. arXiv preprint
arXiv:2209.06794 , 2022. 7
[12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-
hao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-
hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.
Xing. Vicuna: An open-source chatbot impressing GPT-4
with 90%* ChatGPT quality, 2023. 5
[13] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei
Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun.

--- PAGE 10 ---
Ultrafeedback: Boosting language models with high-quality
feedback. arXiv preprint arXiv:2310.01377 , 2023. 2, 3, 8
[14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
Fung, and Steven Hoi. InstructBLIP: Towards general-
purpose vision-language models with instruction tuning,
2023. 1, 4, 5, 7, 8, 13
[15] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi
Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and
Bowen Zhou. Enhancing chat language models by scal-
ing high-quality instructional conversations. arXiv preprint
arXiv:2305.14233 , 2023. 8
[16] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,
Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,
Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-
E: An embodied multimodal language model. arXiv preprint
arXiv:2303.03378 , 2023. 1
[17] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,
Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang,
Xiawu Zheng, et al. MME: A comprehensive evaluation
benchmark for multimodal large language models. arXiv
preprint arXiv:2306.13394 , 2023. 4, 7
[18] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Ba-
tra, and Devi Parikh. Making the v in vqa matter: Elevating
the role of image understanding in visual question answer-
ing. In CVPR , pages 6904–6913, 2017. 4, 5, 13, 15
[19] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and
preventing hallucinations in large vision language models.
arXiv preprint arXiv:2308.06394 , 2023. 8, 13
[20] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,
Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,
Owais Khan Mohammed, Qiang Liu, et al. Language is
not all you need: Aligning perception with language mod-
els.arXiv preprint arXiv:2302.14045 , 2023. 7
[21] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade
Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,
Vaishaal Shankar, Hongseok Namkoong, John Miller, Han-
naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-
clip, 2021. If you use this software, please cite it as below.
13
[22] Zachary Kenton, Tom Everitt, Laura Weidinger, Iason
Gabriel, Vladimir Mikulik, and Geoffrey Irving. Alignment
of language agents. arXiv preprint arXiv:2103.14659 , 2021.
8
[23] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-
jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan
Popov, Matteo Malloci, Alexander Kolesnikov, et al. The
open images dataset v4: Unified image classification, object
detection, and visual relationship detection at scale. Interna-
tional Journal of Computer Vision , 128(7):1956–1981, 2020.
13
[24] Jan Leike, David Krueger, Tom Everitt, Miljan Martic,
Vishal Maini, and Shane Legg. Scalable agent alignment
via reward modeling: a research direction. arXiv preprint
arXiv:1811.07871 , 2018. 8
[25] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-
iao Ge, and Ying Shan. Seed-bench: Benchmarking mul-timodal llms with generative comprehension. arXiv preprint
arXiv:2307.16125 , 2023. 7
[26] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,
Jingkang Yang, and Ziwei Liu. Otter: A multi-modal
model with in-context instruction tuning. arXiv preprint
arXiv:2305.03726 , 2023. 7
[27] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2: Bootstrapping language-image pre-training with
frozen image encoders and large language models. arXiv
preprint arXiv:2301.12597 , 2023. 1, 4, 7
[28] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang,
Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu
Sun, et al. M3IT: A large-scale dataset towards multi-
modal multilingual instruction tuning. arXiv preprint
arXiv:2306.04387 , 2023. 8
[29] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin
Zhao, and Ji-Rong Wen. Evaluating object hallucina-
tion in large vision-language models. arXiv preprint
arXiv:2305.10355 , 2023. 1, 4, 8, 13
[30] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Ed-
wards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman,
Ilya Sutskever, and Karl Cobbe. Let’s verify step by step.
arXiv preprint arXiv:2305.20050 , 2023. 8
[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence
Zitnick. Microsoft COCO: Common objects in context. In
ECCV , pages 740–755. Springer, 2014. 5, 13
[32] Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser
Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusion-
Bench: You see what you think? Or you think what you
see? An image-context reasoning benchmark challenging for
GPT-4V(ision), LLaV A-1.5, and other multi-modality mod-
els.arXiv preprint arXiv:2310.14566 , 2023. 8
[33] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser
Yacoob, and Lijuan Wang. Aligning large multi-modal
model with robust instruction tuning. arXiv preprint
arXiv:2306.14565 , 2023. 1, 4, 5, 8, 13
[34] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
Improved baselines with visual instruction tuning. arXiv
preprint arXiv:2310.03744 , 2023. 5, 6, 8, 13
[35] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
Visual instruction tuning. arXiv preprint arXiv:2304.08485 ,
2023. 1, 4, 5, 7, 8, 13, 15
[36] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li,
Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel
Galley, and Jianfeng Gao. MathVista: Evaluating mathemat-
ical reasoning of foundation models in visual contexts. arXiv
preprint arXiv:2310.02255 , 2023. 1
[37] OpenAI. GPT-4V(ision) system card. 2023. 1, 2, 5, 6, 8, 12
[38] OpenAI. GPT-4 technical report, 2023. 1, 3, 8
[39] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al. Training lan-
guage models to follow instructions with human feedback.
NeurIPS , 35:27730–27744, 2022. 2, 3, 8, 9
[40] Bryan A Plummer, Liwei Wang, Chris M Cervantes,
Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-

--- PAGE 11 ---
nik. Flickr30k entities: Collecting region-to-phrase corre-
spondences for richer image-to-sentence models. In Pro-
ceedings of ICCV , 2015. 13
[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML , pages 8748–8763. PMLR, 2021. 4
[42] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Er-
mon, Christopher D Manning, and Chelsea Finn. Direct
preference optimization: Your language model is secretly a
reward model. arXiv preprint arXiv:2305.18290 , 2023. 2, 3,
8
[43] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor
Darrell, and Kate Saenko. Object hallucination in image cap-
tioning. In EMNLP , pages 4035–4045, 2018. 4, 13
[44] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. LAION-5B: An open large-scale dataset for train-
ing next generation image-text models. NeurIPS , 35:25278–
25294, 2022. 4
[45] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad-
ford, and Oleg Klimov. Proximal policy optimization algo-
rithms. arXiv preprint arXiv:1707.06347 , 2017. 5, 8, 13
[46] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,
Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: A
benchmark for visual question answering using world knowl-
edge. In Proceedings of ECCV , 2022. 13
[47] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler,
Ryan Lowe, Chelsea V oss, Alec Radford, Dario Amodei, and
Paul F Christiano. Learning to summarize with human feed-
back. NeurIPS , 33:3008–3021, 2020. 8
[48] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu,
Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui,
Yu-Xiong Wang, Yiming Yang, et al. Aligning large mul-
timodal models with factually augmented RLHF. arXiv
preprint arXiv:2309.14525 , 2023. 1, 2, 4, 5, 6, 8, 9, 13
[49] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B
Hashimoto. Stanford Alpaca: An instruction-following
LLaMA model, 2023. 8
[50] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth ´ee Lacroix, Baptiste
Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
LLaMA: Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971 , 2023. 5
[51] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
LLaMA 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288 , 2023. 1, 2, 3, 5
[52] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping Zhong,
Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi Wang,
et al. VIGC: Visual instruction generation and correction.
arXiv preprint arXiv:2308.12714 , 2023. 8[53] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhil-
iang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mo-
hammed, Saksham Singhal, Subhojit Som, et al. Image as
a foreign language: BEiT pretraining for vision and vision-
language tasks. In CVPR , pages 19175–19186, 2023. 4, 5
[54] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji
Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan
Song, et al. CogVLM: Visual expert for pretrained language
models. arXiv preprint arXiv:2311.03079 , 2023. 1, 7
[55] Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng
Wang, Pinlong Cai, Xin Li, Tao Ma, Yingxuan Li, Linran
Xu, Dengke Shang, et al. On the road with GPT-4V (ision):
Early explorations of visual-language model on autonomous
driving. arXiv preprint arXiv:2311.05332 , 2023. 1
[56] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr,
Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf,
and Hannaneh Hajishirzi. Fine-grained human feedback
gives better rewards for language model training. arXiv
preprint arXiv:2306.01693 , 2023. 8
[57] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,
Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn
of LMMs: Preliminary explorations with GPT-4V(ision).
arXiv preprint arXiv:2309.17421 , 9, 2023. 1
[58] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
Yaya Shi, et al. mPLUG-Owl: Modularization empowers
large language models with multimodality. arXiv preprint
arXiv:2304.14178 , 2023. 7
[59] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang,
Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong
Chen. Woodpecker: Hallucination correction for multimodal
large language models. arXiv preprint arXiv:2310.16045 ,
2023. 8
[60] Tianyu Yu, Jinyi Hu, Yuan Yao, Haoye Zhang, Yue Zhao,
Chongyi Wang, Shan Wang, Yinxv Pan, Jiao Xue, Dahai
Li, et al. Reformulating vision-language foundation models
and datasets towards universal multimodal assistants. arXiv
preprint arXiv:2310.00653 , 2023. 1, 3, 4, 5, 7, 8, 13
[61] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Qiao.
LLaMA-Adapter: Efficient fine-tuning of language models
with zero-init attention. arXiv preprint arXiv:2303.16199 ,
2023. 7
[62] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gon-
zalez, and Ion Stoica. Judging LLM-as-a-judge with MT-
Bench and Chatbot Arena, 2023. 5, 13
[63] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. MiniGPT-4: Enhancing vision-language
understanding with advanced large language models. arXiv
preprint arXiv:2304.10592 , 2023. 1

--- PAGE 12 ---
A. Zoom-in Study regarding GPT-4V
We perform a zoom-in study of RLHF-V concerning GPT-
4V to provide a better understanding of their behaviors.
A.1. Hallucination Patterns
We conduct a comparative analysis of the responses gener-
ated by RLHF-V and GPT-4V , and have the following key
observations:
(1) Compared with RLHF-V , GPT-4V tends to describe
more details in the images and elaborate more on the inter-
relations among them. Quantitatively, we utilize ChatGPT
to extract all the object mentions in the responses of GPT-
4V , and find that the average number per response is 2.1
times larger than RLHF-V . We mainly attribute this to the
higher resolution (7.6 times larger than RLHF-V) [1] and
the more powerful LLM backbone [37].
(2) GPT-4V’s hallucinations are more concentrated in
some responses. In HumanEval, the hallucination rates of
GPT-4V on Object andPosition are comparable to RLHF-
V . However, in the comprehensive ALL metric, the halluci-
nation rate is 17.3% lower than RLHF-V . To better under-
stand the reasons behind this phenomenon, we conduct a
thorough analysis of the evaluation results. We observe that
different types of hallucinations in GPT-4V are often con-
centrated in a small subset of responses, while contributing
to hallucination rates across multiple subcategories. Quan-
titatively, we sort the responses of each model by the hal-
lucination count in descending order, and plot the curve
of hallucination count ratio vs hallucination response ratio.
From the results in Figure 4, we can see that the top 45.6%
hallucinated responses of GPT-4V contribute to 75% hallu-
cination counts. In comparison, the top 64.6% hallucinated
responses of RLHF-V contribute to 75% hallucinations. We
refer readers to Section B for more qualitative results.
A.2. Distillation against GPT-4V
Upon observing GPT-4V’s superior fine-grained image per-
ception and text generation capabilities, an intuitive ques-
tion is, will it be beneficial to distill GPT-4V capabilities
through visual instruction tuning? To this end, we collect
1.2k visual instruction data about long-form image descrip-
tions from GPT-4V . We then use the response generated
by GPT-4V to fine-tune our model. We observe that the
average number of object mentions in the model response
significantly increases by 1.8 times compared with the ori-
gin model. However, this can be a double-edged sword:
as shown in Table 4, the hallucination rate significantly in-
creases as well.
The results are consistent with the hypothesis of [2]:
“If we supervise the model against instruction data that far
exceeds its own foundational capabilities, we are essen-
tially teaching the model to hallucinate.” Specifically, our
0 20 40 60 80 100
Hall. Response Ratio (%)020406080100Hall. Count Ratio (%)RLHF-V
GPT-4VFigure 4. Distribution of hallucination segments over different
responses. GPT-4V hallucinations are more concentrated on a
smaller subset of the responses. Hall.: Hallucination.
model learns to produce more details and the interrelation-
ship among them through distillation against GPT-4V , while
the fundamental capabilities of the model are not enough
for this demand. As a result, the hallucination problem is
remarkably exacerbated. The results show that visual in-
struction data (or distillation target) is not the stronger the
better, but rather should match the foundational capability
of the model.
B. Qualitative Results
We provide more qualitative results in this section to facil-
itate a more intuitive understanding and comparison of dif-
ferent models. Based on the qualitative results, we have the
following observations:
(1) RLHF-V typically exhibits less hallucination in both
short-form QA and long-form QA scenarios, compared with
open-source models such as LLaV A-RLHF and Instruct-
BLIP, as shown in Figure 5, 6, 7, and 8.
(2) GPT-4V is more descriptive regarding details in im-
ages as shown in Figure 6, 8, 9 and 10. For example, in Fig-
ure 9, GPT-4V mentions black dots across each tilewhile
RLHF-V does not describe these details.
(3) RLHF-V is more resistant to the over-generalization
problem as shown in Figure 9 and Figure 10. In Figure 9,
GPT-4V falsely mentions objects which are highly related
to the scene while not shown in the image such as exhaust ,
hood , and bottle .
C. Implementation Details
We provide more implementation details in this section for
better reproducibility. Benefiting from the high efficiency
of training, we make all parameters trainable during the
training process, which costs merely less than 1 hour on
8 A100 GPUs in total. We empirically find that adopting a
longer warm-up (10% training steps) can make the training
more stable and consequently apply this setting for all ex-

--- PAGE 13 ---
ModelHumanEval ↓ MHB↓
Obj. Pos. Num. All Resp.
Muffin [60] 33.6 16.4 26.0 74.7 68.8
RLHF-V 21.9 7.5 14.4 55.5 52.1
w/ GPT-4V distil. 45.2 10.3 20.6 75.3 62.5
Table 4. Experimental results of distillation against GPT-4V .
MHB: MMHal-Bench, GPT-4V distil.: instruction-tune the model
using responses generated by GPT-4V .
periments in this paper. As for data collection, besides the
prompts obtained from [60], we also use image description
prompts generated by GPT-4 during the annotation process
which are listed in Table 5.
D. Evaluation Details
We introduce more evaluation details, including baseline
models and evaluation benchmarks.
D.1. Baselines
We compare with a series of state-of-the-art baselines:
•LLaV A : LLaV A [35] constructs 150K multimodal in-
structions based on the COCO dataset by asking GPT-4
to generate multi-turn dialogues for each image.
•Muffin : Muffin [60] propose to reformulate pre-trained
vision-language models as the bridge toward large lan-
guage models. The model is firstly pre-trained on 180M
image-text pairs and then fine-tuned on their proposed
UniMM-Chat instruction dataset consisting of 1.1M mul-
timodal instructions.
•LRV : LRV [33] is fine-tuned on 400K instruction data
generated by GPT-4, and mitigates hallucination by lim-
iting the response length.
•LLaV A-RLHF : The concurrent LLaV A-RLHF employs
the strong 13B Vicuna 1.5 [62] (fine-tuned from LLaMA-
2) as LLM backbone. It first trains the model with
122K instructions from VQAv2 [18], A-OKVQA [46]
and Flickr30k [40] to improve the foundational capabil-
ities of the model. It then trains the reward model on
10K human-labeled preference data, and performs proxi-
mal policy optimization [45] on 72K factually augmented
data.
•InstructBLIP : InstructBLIP [14] constructs a multi-
modal instruction tuning dataset based on 26 public
datasets by apply pre-defined templates to directly formu-
late these datasets into a unified format. They also devise
a novel instruction-aware Q-Former and train the model
on the proposed dataset.
•Qwen-VL-Chat : Qwen-VL-Chat [6] utilizes a large
ViT with 1.9B parameters initialized from OpenCLIP’s
bigG [21] as image encoder. It is pre-trained on 1.4Bimage-text pairs and fine-tuned on more than 50M high-
quality multimodal instructions.
•LLaV A 1.5 : LLaV A 1.5 [34] also employs the strong
13B Vicuna 1.5 [62] (fine-tuned from LLaMA-2) as LLM
backbone. It is pre-trained on 558K selected image-
text pairs and fine-tuned on 665K multimodal instructions
with elaborately designed training strategies.
D.2. Benchmarks
We introduce additional details about the benchmarks we
used for evaluation.
•Object HalBench : Object HalBench [43] is a widely
adopted benchmark for assessing object hallucination in
detailed image descriptions. To improve the evalua-
tion stability, we augment the benchmark with 8 diverse
prompts for detailed image descriptions during evalua-
tion, where 4 instructions are adopted from [19] and the
other 4 instructions are generated by GPT-4. We con-
firm that there is no overlap between the evaluation in-
structions and the training instructions. Detailed instruc-
tions are listed in Table 6. Following [29], we ran-
domly sample 300 images from the validation set of
COCO [31] to form the evaluation image set. Regard-
ing metrics, the response-level hallucination rate is the
number of responses with object hallucinations divided
by the number of responses that introduce COCO objects,
while the mention-level hallucination rate is the number
of falsely mentioned COCO objects in the generated re-
sponses divided by the total number of mentioned COCO
objects. During evaluation, we first generate descriptions
on images from the benchmark and then leverage Chat-
GPT to extract the mentioned objects in these responses
which are further used to calculate the final scores follow-
ing. Unlike [43] which detects object mentions by exact-
match, we find ChatGPT can perform the extraction with
both better precision and recall and consequently apply
this setting during evaluation. The full prompt we used to
conduct such extraction is shown in Table 7.
•MMHal-Bench : MMHal-Bench [48] evaluates halluci-
nations and response informativeness. It consists of 96
images from the validation and test sets of OpenIm-
ages [23]. Each image in this benchmark is annotated
with a brand new question and the image-question pairs
cover 12 common object meta-categories from COCO.
•HumanEval : The above evaluations are either limited
to common object hallucination or dominated by short-
form question answering (i.e., questions that can be suf-
ficiently answered by a few words). To provide a more
reliable and comprehensive evaluation over diverse hallu-
cination types, we present HumanEval benchmark, which
covers both long-form image descriptions, and short-form
questions. The benchmark contains 146 samples col-
lected from Object HalBench (50) and MMHal-Bench

--- PAGE 14 ---
• Identify and describe each object in the image in detail.
• Describe the key features of the image in great detail.
• What are the main elements in this image? Describe them thoroughly.
• Explain what’s happening in the image with as much detail as possible.
• Detail the image’s components with particular focus on each entity.
• Provide an intricate description of every entity in the image.
• What are the main objects or subjects in the image? Please describe them in detail.
• What is the setting or environment in which the image takes place?
• How do the elements in the image relate to each other in terms of positioning or composition?
• Explain the elements of the image with thorough attention to detail.
• Explain the image’s various components in depth.
• What are the key features you observe in the image?
• Can you point out the details that make this image unique?
• Itemize the elements you identify in the image and describe them thoroughly.
• Convey the specifics of the image with meticulous attention to detail.
• Tell me what catches your eye in the image, and describe those elements in depth.
Table 5. The list of instructions for detailed image description used in training.
• Provide a thorough description of the given image.
• What is this photo about? Please answer in great detail.
• Provide a thorough description of the given picture.
• Explain the narrative or story that the image seems to convey, detailing each part that contributes to it.
• Please provide a detailed description of the image. Describe the visual elements, colors, shapes, textures, and any objects or
people present along with the overall mood or atmosphere portrayed in the image.
• Please provide a detailed description of the image, including its visual elements, such as colors, shapes, textures, objects, and
people.
• Provide an intricate description of the image, capturing its visual elements, including colors, shapes, textures, objects, and any
people present.
• Compose a detailed account of the image, encompassing its visual characteristics, like colors, shapes, textures, objects, and
any human subjects, by paying careful attention to the specifics.
Table 6. The list of instructions for Object HalBench.
You are an expert in image objects extraction according to a question answer pair. We asked an examiner to answer a question
about a picture.
[Start of Question]
<image >{question }
[End of Question]
[Start of Examiner’s Answer]
{answer }
[End of Examiner’s Answer]
Assume that the answer is correct, please identify all visible objects that are directly shown in the image. Please following the
instructions in below:
1. You should only mention objects that are explicitly mentioned in the examiner’s answer.
2. You should only extract the object names without the attributes of the objects.
3. You should not include the properties of the object, like the color, material, etc. as part of the object name in your result.
4. Make your answer precise. Present the results in a JSON list format: [“object 1”, ..., “object n”].
5. You should return an empty JSON list () if no visible objects can be found.
Table 7. The prompt we used to extract object mentions from image captions with ChatGPT.

--- PAGE 15 ---
(96). Given model responses, we ask human annotators to
label the hallucinated segments and hallucination types of
the segments, including objects, positions, numbers and
others. We report the response-level hallucination rate on
these types.
•LLaV A Bench : LLaV A Bench [35] is a widely adopted
benchmark for assessing multimodal conversation, de-
tailed description and complex reasoning capabilities. It
consists of 30 image-question pairs for the aforemen-
tioned three capabilities respectively and scores model
output against reference response via GPT-4.
•VQAv2 : VQAv2 [18] is a dataset for short-form visual
question answering. The test-dev set of VQAv2 con-
sists of 107K image-question pairs which covers a diverse
range of capabilities.

--- PAGE 16 ---
In this picture, the man is giving a thumbs up gesture. The thumbs up sign is a universally recognized symbol of approval or agreement, often used to indicate satisfaction with something. It seems like he may be reacting positively to something that is happening around him.The man in the image is sitting at a table, talking on his cell phone.The man in the image is talking on his cell phone while sitting in a crowded room. He is surrounded by other people, some of whom are also sitting and talking on their phones. The scene suggests that the man is engaged in a conversation, possibly discussing something important or casually chatting with someone on the other end of the line.The presence of multiple people using their cell phones in the same room indicates that the image may have been taken in a public setting, such as a restaurant, a waiting area, or a social event. It is common for people to use their phones in such settings, as it allows them to stay connected with friends, family, or colleagues, or to attend to work-related matters.The man's conversation could be a personal one, or it could be related to a professional matter, such as discussing a project or coordinating plans with a colleague. It is also possible that he is using his phone to access information, such as checking emails, browsing the internet, or using social media.In any case, the image captures a moment in time where the man is actively engaged in a conversation on his cell phone, surrounded by other people who are also using their devices to communicate or attend to their needs. This scene reflects the ubiquity of cell phones in modern life, and the way they have become an integral part of our daily routines and social interactions.The man in the image appears to be scratching or touching his head with one hand while giving a thumbs-up gesture with the other hand.Short-formQAWhat is the man doing in the image?
RLHF-V (This work)
InstructBLIPLLaVA-RLHF
GPT4-V
Figure 5. Qualitative results of different models. Correct answers andhallucinations are highlighted in color respectively.

--- PAGE 17 ---
In this image, we see a small gray elephant standing alone in a dirt field. The elephant appears to be the focal point of the photo, with its impressive size and unique color. In the background, there is a building that complements the natural surroundings very well.The image depicts an elephant standing in a sandy area near a wall. The elephant is the main focus of the scene, occupying most of the space in the frame. There are several other objects scattered around the area, including a few chairs placed near the elephant.  In addition to the elephant and chairs, there are two tires visible in the scene. One tire is positioned closer to the left side of the image, while the other is located on the right side. Both tires appear to be partially buried in the sand, possibly indicating that they have been placed there for the elephant to interact with. Overall, the scene provides a glimpse into the life of an elephant in a controlled environment.The image features a large elephant standing in a sandy area, surrounded by rocks and a building. The elephant is positioned near the center of the scene, with its trunk hanging down. In the background, there are two tires lying on the ground, possibly indicating that the area is used for various activities or training exercises. Additionally, there is a bird perched on a rock in the scene, adding a touch of nature to the environment.The image depicts an elephant in an enclosure, likely at a zoo or a sanctuary. The environment has a concrete wall, a door, and some elements like rocks, a blue tarp, and a tire, possibly used as play or enrichment items for the elephant. The scene suggests that this is a space where the elephant is housed, and the items around might be used to keep it engaged or stimulated.Long-formQAWhat do you think is going on in this snapshot?
RLHF-V (This work)
InstructBLIP
LLaVA-RLHF
GPT4-V
Figure 6. Qualitative results of different models. Hallucinations are highlighted in color.

--- PAGE 18 ---
The objects in the image hold significance in ensuring city traffic flows smoothly as they help regulate and control the movement of vehicles and pedestrians. The traffic lights, for instance, provide visual cues to drivers and pedestrians, indicating when it is safe to proceed, stop, or yield. By doing so, they help reduce traffic congestion, prevent accidents, and improve overall safety.Additionally, the street markings, such as the crosswalk and bike lane, provide clear guidance to drivers, cyclists, and pedestrians by outlining designated areas for each group. These markings contribute to a more organized and safer traffic flow, as they indicate the expected behavior of different users in sharing the road or navigating the intersection.Long-formQAWhat might be the significance or role of the objects you described in ensuring city traffic flows smoothly?
RLHF-V (This work)
The objects and features described in the image play a crucial role in ensuring that city traffic flows smoothly and safely. Here are some of the key aspects they contribute to:1. Traffic lights: The presence of traffic lights at the intersection helps regulate the flow of traffic by controlling the right of way for vehicles and pedestrians. They ensure that traffic moves in an orderly manner, reducing the risk of accidents and congestion. By following the traffic light signals, drivers can safely navigate the intersection and avoid collisions with other vehicles or pedestrians.2. Parking meters: Parking meters play a vital role in managing on-street parking in busy urban areas. They help maintain a steady flow of traffic by preventing vehicles from occupying parking spaces for extended periods, which can lead to congestion and reduced accessibility for other drivers. Parking meters also generate revenue for the city, which can be used to fund public services and infrastructure projects.3. Signage: The presence of various signs on the street, such as no parking signs, helps communicate important information to drivers and pedestrians. These signs provide guidance on parking restrictions, speed limits, and other traffic rules, ensuring that everyone on the road is aware of their responsibilities and the potential consequences of violating traffic regulations.4. Bike lanes: The presence of bike lanes in the image indicates that the city is promoting alternative modes of transportation, such as cycling. Bike lanes provide a dedicated space for cyclists, reducing the risk of accidents between bikes and cars. They also encourage more people to choose cycling as a means of transportation, which can help reduce traffic congestion and improve air quality in the city.5. Pedestrian crossings: The presence of pedestrian crossings in the image highlights the importance of ensuring the safety of pedestrians in busy urban areas. These crossings provide designated areas for pedestrians to cross the street safely, reducing the risk of accidents and promoting a more walkable city environment.In summary, the objects and features described in the image, such as traffic lights, parking meters, signage, bike lanes, and pedestrian crossings, play a significant role in ensuring that city traffic flows smoothly and safely. They help regulate traffic flow, manage parking, communicate important information to drivers and pedestrians, promote alternative modes of transportation, and prioritize the safety of pedestrians. By implementing these measures, cities can create a more efficient and sustainable urban environment for all residents.LLaVA-RLHF
Figure 7. Qualitative results of different models. Unreasonable reasoning and extensions and scene related hallucinations are high-
lighted in color respectively.

--- PAGE 19 ---
In the image, there are several objects that play a significant role in ensuring city traffic flows smoothly. These objects include streetlights, traffic lights, cars, bicycles, and pedestrians. Streetlights provide illumination for drivers and pedestrians during nighttime or low-light conditions, improving visibility and safety on the road. Traffic lights regulate the flow of traffic at intersections, ensuring that vehicles, bicycles, and pedestrians follow the rules of the road and avoid collisions. Cars and bicycles are essential components of the transportation system, contributing to the movement of people and goods within the city. Pedestrians also play a crucial role in ensuring the smooth flow of traffic by crossing streets safely and adhering to traffic rules. Overall, these objects work together to create a safe and efficient transportation system in the city.Long-formQAWhat might be the significance or role of the objects you described in ensuring city traffic flows smoothly?
InstructBLIP
The objects in the image are various traffic control devices used in urban settings to manage and regulate the flow of traffic, ensuring safety and order on the roads. Here's their significance and role:1. Traffic Lights: They control the flow of traffic at intersections, directing vehicles when to stop and go. This prevents collisions and ensures an orderly movement of traffic.2. Street Signs: Various signs in the image, like "No Right Turn", inform drivers of local traffic laws and regulations they must follow, such as which turns are prohibited at an intersection.3. Bicycle Lane Markings: The green painted bike lane signifies a dedicated space for cyclists, which helps in reducing conflicts between motor vehicles and bicycles.4. Pedestrian Signals: Pedestrian traffic lights, although not clearly visible in this image, usually accompany traffic lights to indicate when it is safe for pedestrians to cross the street.5. Road Markings: The various lines and symbols painted on the road surface guide drivers on how to use the lanes properly, where to stop, and the direction of permissible traffic movement.6. Parking Signs: These signs inform drivers of the areas where parking is either allowed or prohibited during certain times, which contributes to the reduction of roadside congestion.Each of these devices plays a crucial role in maintaining a systematic and smooth flow of different modes of transportation within the city streets, minimizing accidents, and ensuring the safety of all road users.GPT4-V
Figure 8. Qualitative results of different models (continued figure). Unreasonable reasoning and extensions are highlighted in color.

--- PAGE 20 ---
The image depicts a small kitchen with white walls and wood cabinets. There is a center island in the middle of the kitchen, providing additional counter space. The kitchen is equipped with standard appliances, such as a stove, located on the right side, and a sink on the left.Certainly! The image showcases an interior view of a kitchen. Here are the details:Cabinetry: The kitchen is equipped with a combination of white overhead and base cabinets, some of which have a horizontal black line running along their middle. The design appears modern, with a sleek finish.Countertop: The countertop appears to be made of a light-colored, possibly marble or granite material. The main countertop extends along one side of the kitchen, while an additional rounded extension provides a potential breakfast or serving area.Flooring: The floor is checkered with white tiles, and numerous black dots are scattered across each tile, creating a distinct pattern.Appliances and Fixtures: On the right, there's a silver, freestanding gas range and oven. Just above it is an exhaust or range hood. A stainless-steel sink is set into the countertop on the left side of the image, beneath which there's a dish drying rack attached to the wall.Lighting: The ceiling is equipped with multiple recessed lights, ensuring the kitchen is well-lit.Other Details: A green trash bin is placed beside the gas range. Various other minor items, like a bottle and some containers, can be seen on the countertop. There's also a metallic vertical pole, possibly a utility or storage pole, standing between the range and the rounded counter extension.Walls and Door: The kitchen walls are painted in a muted beige or light brown color. To the far right, there's a wooden door, possibly leading to a pantry or another room.Ambiance: The kitchen exudes a clean, functional, and organized atmosphere, with a blend of neutral tones and contemporary design elements.Overall, the kitchen appears to be well-maintained and offers a mix of practicality and style.Long-formQAProvide a thorough description of the given picture.
RLHF-V (This work)GPT4-V
Figure 9. Qualitative results of different models. Hallucinations andscene related hallucinations are highlighted in color respectively.

--- PAGE 21 ---
The living room is clean and empty of people, with the sun shining in through the windows. It features a white rug and black furniture that contrasts with the light from the windows. There is a doorway leading to a balcony that showcases the railing. The living room contains a couch, a table, and two chairs.The image showcases an elegantly designed living room.Starting with the overall atmosphere, the room exudes a warm and inviting ambiance. Natural sunlight floods in from large windows on the left, emphasizing the spaciousness of the room and creating a luminous glow.As for the visual elements:Colors: The room primarily features neutral tones—creams, browns, and blacks. These muted colors contribute to the room's calming and sophisticated feel. The wooden floor has a polished honey-brown hue which complements the lighter walls.Furniture: A variety of seating options are available. There are two dark leather chairs on the left side of the room, facing the window. In the center, a black leather couch with decorative cushions sits opposite a glass coffee table with metallic legs. On the far right, there's another seating area with two plush chairs flanking a fireplace.Decorative Elements: Above the fireplace is a white mantel adorned with decorative items. On the left wall, a sleek table holds decorative items like a lamp and a vase with flowers. Above this table, a large mirror is hung, reflecting the room's light and enhancing its spaciousness. On the adjacent wall, three framed artworks are neatly aligned, adding an artistic touch to the room.Textures: The room presents a blend of textures. The plushness of the rug contrasts with the smooth leather of the furniture, the glass surface of the coffee table, and the sleek wooden floor.Shapes: There are various shapes at play. The rectangular and square forms of the artworks, the mirror, and the windows are balanced by the rounded shapes of the seating and the coffee table.Plants: A large potted plant sits by the window, adding a touch of nature and a refreshing green to the neutral palette.The room is a blend of modern and classic elements, providing a sense of comfort while maintaining a refined aesthetic.Long-formQA
Please provide a detailed description of the image. Describe the visual elements, colors, shapes, textures, and any objects or people present along with the overall mood or atmosphere portrayed in the image.
RLHF-V (This work)GPT4-V
Figure 10. Qualitative results of different models. Hallucinations andscene related hallucinations are highlighted in color respectively.
