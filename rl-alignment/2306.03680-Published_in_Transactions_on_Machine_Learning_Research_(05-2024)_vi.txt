# 2306.03680.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2306.03680.pdf
# Kích thước tệp: 5681878 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
Chính sách Đánh giá Ràng buộc Nhẹ
cho Học tăng cường Ngoại tuyến
Linjie Xu linjie.xu@qmul.ac.uk
Queen Mary University of London
Zhengyao Jiang z.jiang@cs.ucl.ac.uk
University College London
Jinyu Wang, Lei Song và Jiang Bian {wang.jinyu, lei.song, jiang.bian}@microsoft.com
Microsoft Research Asia
Được đánh giá trên OpenReview: https: // openreview. net/ forum? id= imAROs79Pb
Tóm tắt
Các phương pháp học tăng cường ngoại tuyến (RL) áp dụng ràng buộc lên chính sách để tuân thủ chặt chẽ với chính sách hành vi, do đó ổn định hóa việc học giá trị và giảm thiểu việc lựa chọn các hành động ngoài phân phối (OOD) trong thời gian kiểm tra. Các phương pháp thông thường áp dụng ràng buộc giống hệt nhau cho cả việc học giá trị và suy luận thời gian kiểm tra. Tuy nhiên, các phát hiện của chúng tôi cho thấy rằng các ràng buộc phù hợp cho ước lượng giá trị thực tế có thể quá hạn chế cho việc lựa chọn hành động trong thời gian kiểm tra. Để giải quyết vấn đề này, chúng tôi đề xuất một Chính sách Đánh giá Ràng buộc Nhẹ (MCEP) cho suy luận thời gian kiểm tra với một chính sách mục tiêu bị ràng buộc nhiều hơn cho ước lượng giá trị. Vì chính sách mục tiêu đã được áp dụng trong nhiều phương pháp trước đây, MCEP có thể được tích hợp liền mạch với chúng như một plugin. Chúng tôi cụ thể hóa MCEP dựa trên các thuật toán TD3BC (Fujimoto & Gu, 2021), AWAC (Nair et al., 2020) và DQL (Wang et al., 2023). Các kết quả thực nghiệm trên D4RL MuJoCo locomotion, humanoid chiều cao và một tập 16 nhiệm vụ thao tác robot cho thấy rằng MCEP mang lại cải thiện hiệu suất đáng kể trên các phương pháp RL ngoại tuyến cổ điển và có thể cải thiện thêm các phương pháp SOTA. Mã nguồn được mở tại https://github.com/egg-west/MCEP.git .

1 Giới thiệu
Học tăng cường ngoại tuyến (RL) trích xuất một chính sách từ dữ liệu được thu thập trước bởi các chính sách chưa biết. Thiết lập này không yêu cầu tương tác với môi trường do đó rất phù hợp cho các nhiệm vụ mà việc tương tác tốn kém hoặc rủi ro. Gần đây, nó đã được áp dụng cho Xử lý Ngôn ngữ Tự nhiên (Snell et al., 2022; Sodhi et al., 2023), thương mại điện tử (Degirmenci & Jones, 2022) và robot thực tế (Kalashnikov et al., 2021; Rafailov et al., 2021; Kumar et al., 2022; Shah et al., 2022; Bhateja et al., 2023) v.v. So với thiết lập trực tuyến tiêu chuẩn nơi chính sách được cải thiện thông qua thử và sai, việc học với một tập dữ liệu ngoại tuyến tĩnh đặt ra những thách thức mới. Một thách thức là sự dịch chuyển phân phối giữa dữ liệu huấn luyện và dữ liệu gặp phải trong quá trình triển khai. Để đạt được hiệu suất đánh giá ổn định dưới sự dịch chuyển phân phối, chính sách được kỳ vọng sẽ ở gần chính sách hành vi. Một thách thức khác là "lỗi ngoại suy" (Fujimoto et al., 2019; Kumar et al., 2019) cho thấy lỗi ước lượng giá trị trên các cặp trạng thái-hành động chưa thấy hoặc các hành động Ngoài Phân phối (OOD). Tệ hơn, lỗi này có thể được khuếch đại với bootstrapping và gây bất ổn trong huấn luyện, điều này cũng được biết đến như deadly-triad (Van Hasselt et al., 2018). Đa số các phương pháp không có mô hình giải quyết những thách thức này bằng cách ràng buộc chính sách tuân thủ chặt chẽ với chính sách hành vi (Wu et al., 2019; Kumar et al., 2019; Fujimoto & Gu, 2021; Wang et al., 2023) hoặc chính quy hóa Q thành ước lượng bi quan cho các hành động OOD (Kumar et al., 2020; Lyu et al., 2022). Trong công trình này, chúng tôi tập trung vào các phương pháp ràng buộc chính sách.

--- TRANG 2 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
Các phương pháp ràng buộc chính sách giảm thiểu sự khác biệt giữa phân phối chính sách và phân phối hành vi. Trong khi đó, cường độ của ràng buộc chính sách đưa ra một sự đánh đổi giữa việc ổn định ước lượng giá trị và đạt được hiệu suất suy luận tốt hơn. Mặc dù nhiều ràng buộc chính sách khác nhau đã được phát triển để giải quyết sự đánh đổi này, nó vẫn là một vấn đề chung cho chúng rằng một chính sách bị ràng buộc quá mức cho phép ước lượng giá trị ổn định nhưng làm giảm hiệu suất đánh giá (Kumar et al., 2019; Singh et al., 2022; Yu et al., 2023). Đặc biệt, việc huấn luyện giá trị không ổn định có thể đặt ra vấn đề gradient bùng nổ. Dưới giới hạn này, các cường độ ràng buộc hợp lệ có thể không hỗ trợ việc khai thác tốt nhất của hàm giá trị đã học. Nói cách khác, hàm giá trị đã học có thể ngụ ý một giải pháp tốt hơn mà chính sách bị hạn chế quá mức không thể học được (Xem Hình 2 và chi tiết hơn trong Phần 5.1). Để tiết lộ điều này, chúng tôi điều tra các phạm vi cường độ của việc học giá trị ổn định và của hiệu suất suy luận tốt hơn. Tuy nhiên, việc điều tra câu hỏi sau bị cản trở bởi sự đánh đổi hiện tại, vì nó yêu cầu điều chỉnh ràng buộc mà không ảnh hưởng đến việc học giá trị. Để tiến hành điều tra này, chúng tôi vượt qua sự đánh đổi và tìm kiếm giải pháp thông qua hàm giá trị đã học.

Ý tưởng của phương pháp chúng tôi được lấy cảm hứng từ (Czarnecki et al., 2019), đã làm sáng tỏ tiềm năng của việc chưng cất một chính sách học sinh cải thiện hơn giáo viên bằng cách sử dụng hàm giá trị đã học của giáo viên. Do đó, chúng tôi đề xuất suy ra một chính sách đánh giá bổ sung từ hàm giá trị. Chính sách đánh giá không tham gia vào bước đánh giá chính sách do đó việc điều chỉnh ràng buộc của nó không ảnh hưởng đến việc học giá trị. Actor từ actor-critic bây giờ được gọi là chính sách mục tiêu vì nó chỉ được sử dụng để ổn định ước lượng giá trị. Với sự trợ giúp của chính sách đánh giá, chúng tôi điều tra thực nghiệm các cường độ ràng buộc cho 1) ổn định việc học giá trị và 2) hiệu suất đánh giá tốt hơn. Kết quả cho thấy rằng một ràng buộc nhẹ hơn cải thiện hiệu suất đánh giá nhưng có thể vượt ra ngoài không gian ràng buộc của ước lượng giá trị ổn định. Phát hiện này cho thấy rằng hiệu suất đánh giá tối ưu có thể không được tìm thấy dưới sự đánh đổi, đặc biệt khi việc học giá trị ổn định là ưu tiên. Do đó, chúng tôi đề xuất tách biệt các vấn đề của việc học giá trị và hiệu suất đánh giá, thay vì giải quyết chúng bằng sự đánh đổi. Kết quả là, chúng tôi đề xuất một phương pháp mới sử dụng Chính sách Đánh giá Ràng buộc Nhẹ (MCEP) được suy ra từ hàm giá trị để tránh giải quyết sự đánh đổi nói trên và đạt được hiệu suất đánh giá tốt hơn. Vì chính sách mục tiêu thường được sử dụng trong các phương pháp trước đây, MCEP của chúng tôi có thể được tích hợp với chúng một cách liền mạch.

Các đóng góp của công trình này được kết luận như sau:
• Một hiểu biết mới về các phương pháp ràng buộc chính sách, cho thấy rằng hàm giá trị đã học không được khai thác tốt dưới sự đánh đổi giữa việc học giá trị và hiệu suất chính sách. Với cái nhìn sâu sắc này, chúng tôi đề xuất tách biệt các vấn đề của việc học giá trị và hiệu suất chính sách thay vì giải quyết sự đánh đổi.
• Chúng tôi đề xuất sử dụng một chính sách đánh giá ràng buộc nhẹ bổ sung với một chính sách mục tiêu bị ràng buộc nhiều hơn, để đạt được hiệu suất chính sách tốt hơn và việc học giá trị ổn định đồng thời.
• Đánh giá hiệu suất trên D4RL MuJoCo locomotion, humanoid chiều cao và một tập 16 nhiệm vụ thao tác robot cho thấy rằng MCEP đạt được cải thiện hiệu suất đáng kể cho các phương pháp ràng buộc chính sách. Hơn nữa, so sánh với các phương pháp lựa chọn hành động thời gian suy luận và 3 nhóm nghiên cứu loại bỏ xác minh tính quan trọng của phương pháp đề xuất.

2 Công trình liên quan
phương pháp ràng buộc chính sách (hoặc phương pháp chính sách điều chỉnh hành vi) (Wu et al., 2019; Kumar et al., 2019; Siegel et al., 2020; Fujimoto & Gu, 2021) buộc phân phối chính sách ở gần phân phối hành vi. Các phép đo khác biệt khác nhau như phân kỳ KL (Jaques et al., 2019; Wu et al., 2019), phân kỳ KL ngược (Cai et al., 2022) và Maximum Mean Discrepancy (Kumar et al., 2019) được áp dụng trong các phương pháp trước đây. (Fujimoto & Gu, 2021) đơn giản thêm một thuật ngữ behavior-cloning (BC) vào phương pháp RL trực tuyến Twin Delayed DDPG (TD3) (Fujimoto et al., 2018) và đạt được hiệu suất cạnh tranh trong thiết lập ngoại tuyến. Trong khi các phương pháp nói trên tính toán phân kỳ từ dữ liệu, (Wu et al., 2022) ước lượng mật độ của phân phối hành vi bằng cách sử dụng VAE, và do đó phân kỳ có thể được tính toán trực tiếp. Ngoài ràng buộc chính sách tường minh, ràng buộc ngầm được đạt được bằng các phương pháp khác nhau. Ví dụ (Zhou et al.,

--- TRANG 3 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
Hình 1: Trái: sơ đồ mô tả quỹ đạo chính sách cho chính sách mục tiêu ˜π và MCEP πe. Phải: các bước đánh giá chính sách để cập nhật Q˜π và các bước cải thiện chính sách để cập nhật ˜π và πe.

2021) đảm bảo các hành động đầu ra ở trong hỗ trợ của phân phối dữ liệu bằng cách sử dụng một bộ giải mã CVAE (Conditional VAE) được huấn luyện trước ánh xạ các hành động tiềm ẩn đến phân phối hành vi. Trong tất cả các phương pháp trước đây, các ràng buộc được áp dụng cho chính sách học được truy vấn trong quá trình đánh giá chính sách (học giá trị) và được đánh giá trong môi trường trong quá trình triển khai. Phương pháp của chúng tôi không dựa vào chính sách học này cho việc triển khai, thay vào đó, nó được sử dụng như một chính sách mục tiêu chỉ cho việc học giá trị.

Ràng buộc chính sách, được biết đến là hiệu quả trong việc giảm lỗi ngoại suy, có thể làm giảm hiệu suất chính sách khi nó quá hạn chế. (Kumar et al., 2019) tiết lộ một sự đánh đổi giữa việc giảm lỗi trong ước lượng Q và giảm độ lệch dưới mức tối ưu làm giảm chính sách đánh giá. Một ràng buộc được thiết kế để tạo ra một không gian chính sách đảm bảo chính sách kết quả nằm dưới sự hỗ trợ của phân phối hành vi để giảm thiểu lỗi bootstrapping. (Singh et al., 2022) thảo luận về sự không hiệu quả của ràng buộc chính sách trên tập dữ liệu heteroskedastic nơi hành vi thay đổi qua không gian trạng thái theo cách rất không đồng đều, vì ràng buộc là trạng thái-agnostic. Một phương pháp tái cân bằng được đề xuất để đạt được một ràng buộc phân phối nhận thức trạng thái để vượt qua vấn đề này. Thay vì nghiên cứu sự đánh đổi nổi tiếng này, chúng tôi đề xuất tách biệt vấn đề của việc học giá trị và hiệu suất chính sách và đưa ra một giải pháp vượt qua sự đánh đổi bằng cách sử dụng một chính sách đánh giá bổ sung.

Có những phương pháp trích xuất một chính sách đánh giá từ một ước lượng Q đã học. One-step RL (Brandfonbrener et al., 2021; Li et al., 2023) đầu tiên ước lượng chính sách hành vi và ước lượng Q của nó, sau đó được sử dụng để trích xuất chính sách đánh giá. Mặc dù đơn giản, one-step RL được phát hiện hoạt động kém trong các vấn đề dài hạn do thiếu lập trình động lặp (Kostrikov et al., 2022). (Kostrikov et al., 2022) đề xuất Implicity Q learning (IQL) tránh truy vấn các hành động OOD bằng cách học một expectile trên của phân phối giá trị trạng thái. Không có chính sách mục tiêu tường minh nào được mô hình hóa trong quá trình học Q của họ. Với ước lượng Q đã học, một chính sách đánh giá được trích xuất bằng cách sử dụng hồi quy có trọng số lợi thế (Wang et al., 2018; Peng et al., 2019). Phương pháp của chúng tôi có dạng tương tự việc trích xuất một chính sách đánh giá từ một ước lượng Q đã học. Tuy nhiên, one-step RL nhằm tránh dịch chuyển phân phối và khai thác lỗi lặp trong quá trình lập trình động lặp. IQL tránh khai thác lỗi bằng cách loại bỏ các truy vấn hành động OOD và từ bỏ cải thiện chính sách (tức là chính sách không được huấn luyện chống lại ước lượng Q). Công trình của chúng tôi thay vào đó cố gắng giải quyết vấn đề khai thác lỗi và hiệu suất đánh giá bằng cách sử dụng các chính sách có cường độ ràng buộc khác nhau.

3 Nền tảng
Chúng tôi mô hình hóa môi trường như một Quá trình Quyết định Markov (MDP) ⟨S,A,R,T,p0(s),γ,⟩, trong đó S là không gian trạng thái, A là không gian hành động, R là hàm phần thưởng, T(s′|s,a) là xác suất chuyển đổi, p0(s) là phân phối trạng thái ban đầu và γ là hệ số chiết khấu. Trong thiết lập ngoại tuyến, một tập dữ liệu tĩnh Dβ={(s,a,r,s′)} được thu thập trước bởi một chính sách hành vi πβ. Mục tiêu là học một chính sách πϕ(s) với tập dữ liệu D tối đa hóa phần thưởng tích lũy có chiết khấu trong MDP:

--- TRANG 4 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
ϕ∗= arg max
ϕEs0∼p0(·),at∼πϕ(st),st+1∼T(·|st,at)[∞/summationdisplay
t=0γtR(st,at)] (1)
Tiếp theo, chúng tôi giới thiệu phương pháp ràng buộc chính sách tổng quát, trong đó chính sách πϕ và một ước lượng Q off-policy Qθ được cập nhật bằng cách lần lượt thực hiện các bước cải thiện chính sách và các bước đánh giá chính sách. Bước đánh giá chính sách giảm thiểu lỗi Bellman:
LQ(θ) =Est,at∼D,at+1∼πϕ(st+1)/bracketleftbig/parenleftbig
Qθ(st,at)−(r+γQθ′(st,at+1))/parenrightbig2/bracketrightbig
. (2)
trong đó θ′ là tham số cho một mạng Q mục tiêu được cập nhật trễ. Giá trị Q cho trạng thái tiếp theo được tính toán với các hành động at+1 từ chính sách học được cập nhật thông qua bước cải thiện chính sách:
Lπ(ϕ) =Es∼D,a∼πϕ(s)[−Qθ(s,a) +wC(πβ,πϕ)], (3)
trong đó C là một ràng buộc đo lường sự khác biệt giữa phân phối chính sách πϕ và phân phối hành vi πβ. w∈(0,∞] là một hệ số trọng số. Các loại ràng buộc khác nhau đã được sử dụng như Maximum Mean Discrepancy (MMD), phân kỳ KL, và phân kỳ KL ngược.

4 Phương pháp
Trong phần này, đầu tiên chúng tôi giới thiệu thuật toán tổng quát có thể được tích hợp với bất kỳ phương pháp ràng buộc chính sách nào. Tiếp theo, chúng tôi giới thiệu ba ví dụ dựa trên các phương pháp RL ngoại tuyến TD3BC, AWAC và DQL. Với một chính sách đánh giá ràng buộc nhẹ, chúng tôi đặt tên cho ba trường hợp này là TD3BC-MCEP, AWAC-MCEP và DQL-MCEP.

4.1 RL ngoại tuyến với chính sách đánh giá ràng buộc nhẹ
Phương pháp đề xuất được thiết kế để vượt qua sự đánh đổi giữa việc học giá trị ổn định và một chính sách đánh giá hiệu quả. Trong các phương pháp chính sách ràng buộc trước đây, một ràng buộc chính sách hạn chế được áp dụng để có được việc học giá trị ổn định. Chúng tôi giữ lại lợi ích này nhưng sử dụng chính sách này (actor) ˜πψ như một chính sách mục tiêu chỉ để có được việc học giá trị ổn định. Để đạt được hiệu suất đánh giá tốt hơn, chúng tôi giới thiệu một MCEP πe
ϕ được cập nhật bằng cách thực hiện các bước cải thiện chính sách với hàm giá trị Q˜πψ. Khác với ˜πψ, πe
ϕ không tham gia vào quy trình đánh giá chính sách. Do đó, một ràng buộc chính sách nhẹ có thể được áp dụng, giúp πe
ϕ đi xa hơn khỏi phân phối hành vi mà không ảnh hưởng đến tính ổn định của việc học giá trị. Chúng tôi minh họa các không gian chính sách và quỹ đạo chính sách cho ˜πψ và πe
ϕ trong sơ đồ bên trái của Hình 1, trong đó πe
ϕ được cập nhật trong không gian chính sách rộng hơn sử dụng Q˜πψ.

Thuật toán 1 Huấn luyện MCEP
1:Siêu tham số:
2:LRαQ,α˜π,απE, EMAη,˜w và we
3:Khởi tạo: θ,θ′,ψ, và ϕ
4:for i=1, 2, ..., N do
5:Bi∼D
6:θ←θ−αQ∇LQ(θ,Bi)(Phương trình 2)
7:θ′←(1−η)θ′+ηθ
8:ψ←ψ−α˜π∇L˜π(ψ; ˜w,Bi)(Phương trình 3)
9:ϕ←ϕ−απE∇Lπe(ϕ;we,Bi)(Phương trình 3)

Thuật toán tổng thể được hiển thị như mã giả (Alg. 1). Ở mỗi bước, Q˜πψ, ˜πψ và πe
ϕ được cập nhật lặp đi lặp lại. Một bước đánh giá chính sách cập nhật Q˜πψ bằng cách giảm thiểu lỗi TD (dòng 6), tức là độ lệch giữa Q xấp xỉ và giá trị mục tiêu của nó. Tiếp theo, một bước cải thiện chính sách cập nhật ˜πψ (dòng 8). Hai bước này tạo thành thuật toán actor-critic. Sau đó, πe
ϕ được trích xuất từ Q˜πψ, bằng cách thực hiện một bước cải thiện chính sách với một ràng buộc chính sách có khả năng nhẹ hơn ràng buộc cho ˜πψ (dòng 9). Nhiều phương pháp có thể được thực hiện để có được một ràng buộc chính sách nhẹ hơn. Ví dụ, điều chỉnh giảm hệ số trọng số we cho thuật ngữ ràng buộc chính sách hoặc thay thế phép đo ràng buộc bằng

--- TRANG 5 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
một cái ít hạn chế hơn. Lưu ý rằng ràng buộc cho πe
ϕ là cần thiết (thuật ngữ ràng buộc không nên bị bỏ) vì Q˜πψ có lỗi xấp xỉ lớn cho các cặp trạng thái-hành động xa khỏi phân phối dữ liệu.

Vì chính sách đánh giá πe
ϕ không tham gia vào các cập nhật actor-critic, người ta có thể muốn cập nhật πe
ϕ sau khi Q˜πψ hội tụ. Một thí nghiệm để so sánh các tùy chọn thiết kế này có thể được tìm thấy trong Phần A.6 của Phụ lục. Thuật toán 1 cập nhật đồng thời hai chính sách và các cập nhật này (dòng 8 và 9) có thể được song song hóa để đạt được thời gian huấn luyện bổ sung ít dựa trên thuật toán cơ sở.

4.2 Ba ví dụ: TD3BC-MCEP, AWAC-MCEP và DQL-MCEP
TD3BC với MCEP TD3BC thực hiện một sửa đổi tối giản trên thuật toán RL trực tuyến TD3. Để giữ cho chính sách đã học ở gần phân phối hành vi, một thuật ngữ behavior-cloning được thêm vào mục tiêu cải thiện chính sách. TD3 học một chính sách xác định do đó behavior cloning được đạt được bằng cách trực tiếp hồi quy các hành động dữ liệu. Đối với TD3BC-MCEP, chính sách mục tiêu ˜πψ có cùng mục tiêu cải thiện chính sách như TD3BC:
L˜π(ψ) =E(s,a)∼D[−˜λQθ(s,˜πψ(s)) +/parenleftbig
a−˜πψ(s)/parenrightbig2], (4)
trong đó ˜λ=˜α
1
N/summationtext
si,ai|Qθ(si,ai)| là một normalizer cho giá trị Q với một siêu tham số ˜α. Qθ được cập nhật với bước đánh giá chính sách tương tự như Eq. 2 sử dụng ˜πψ. MCEP πe
ϕ được cập nhật bằng các bước cải thiện chính sách với Q˜π tham gia. Hàm mục tiêu cải thiện chính sách cho πe
ϕ tương tự như Eq. 4 nhưng với giá trị αe cao hơn cho normalizer Q-value λe. Mục tiêu cuối cùng cho πe
ϕ là
Lπe(ϕ) =E(s,a)∼D[−λeQ(s,πe
ϕ(s)) +/parenleftbig
a−πe
ϕ(s)/parenrightbig2]. (5)

AWAC với MCEP AWAC (Nair et al., 2020) là một phương pháp behavior cloning có trọng số lợi thế. Vì chính sách mục tiêu bắt chước các hành động từ phân phối hành vi, nó ở gần phân phối hành vi trong quá trình học. Trong AWAC-MCEP, đánh giá chính sách tuân theo Eq. 2 với chính sách mục tiêu ˜πψ cập nhật với mục tiêu sau:
L˜π(ψ) =E(s,a)∼D/bracketleftbigg
−exp/parenleftbigg1
˜λA(s,a)/parenrightbigg
log ˜πψ(a|s)/bracketrightbigg
, (6)
trong đó lợi thế A(s,a) =Qθ(s,a)−Qθ(s,˜πψ(s)). Hàm mục tiêu này giải quyết một maximum likelihood có trọng số lợi thế. Lưu ý rằng gradient sẽ không được truyền qua thuật ngữ lợi thế. Vì mục tiêu này không có thuật ngữ cải thiện chính sách, chúng tôi sử dụng cải thiện chính sách ban đầu với phân kỳ KL như ràng buộc chính sách và xây dựng mục tiêu cải thiện chính sách sau:
Lπe(ϕ) =Es,a∼D,ˆa∼πe(·|s)[−A(s,ˆa) +λeDKL/parenleftbig
πβ(·|s)||πe
ϕ(·|s)/parenrightbig
] (7)
=Es,a∼D,ˆa∼πe(·|s)[−A(s,ˆa)−λelogπe
ϕ(a|s)], (8)
trong đó hệ số trọng số λe là một siêu tham số. Mặc dù Eq. 6 được suy ra bằng cách giải quyết Eq. 8 trong một không gian chính sách tham số, vấn đề ban đầu (Eq. 8) ít hạn chế hơn ngay cả với ˜λ=λe vì gradient lan truyền ngược qua thuật ngữ −A(s,πe(s)). Sự khác biệt này có nghĩa là ngay cả với λe>˜λ, ràng buộc chính sách cho πe vẫn có thể thoải mái hơn ràng buộc chính sách cho ˜π.

DQL với MCEP Diffusion Q-Learning (Wang et al., 2023) là một trong những phương pháp RL ngoại tuyến SOTA áp dụng một mô hình khuếch tán có điều kiện biểu đạt cao như chính sách để xử lý phân phối hành vi đa phương thức. Bước cải thiện chính sách của nó là
L˜π(ψ) =Es∼D,a∼˜π[−˜λQ(s,a) +C(πβ,˜π)], (9)
trong đó C(πβ,˜π) là một thuật ngữ behavior cloning và ˜λ là normalizer Q, tương tự như TD3BC. Bước cải thiện chính sách cho chính sách đánh giá có cùng cách thức như chính sách mục tiêu, ngoại trừ việc sử dụng cường độ ràng buộc khác.
LπE(ϕ) =Es∼D,a∼πE[−λEQ(s,a) +C(πβ,πE)]. (10)

--- TRANG 6 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
(a) Mê cung đồ chơi MDP
 (b)V∗,π∗
(c)V˜π,˜π
 (d)V˜π,arg maxQ˜π
Hình 2: Đánh giá phương pháp ràng buộc chính sách trên một mê cung đồ chơi MDP 2a. Trong các hình khác, màu của một ô lưới biểu thị giá trị trạng thái và mũi tên chỉ ra các hành động từ chính sách tương ứng. 2b hiển thị hàm giá trị tối ưu và một chính sách tối ưu. 2c hiển thị một chính sách ràng buộc được huấn luyện từ dữ liệu ngoại tuyến nói trên, với hàm giá trị của nó được tính bằng Vπ=EaQ(s,π(a|s)). Chính sách không hoạt động tốt trong vùng giá trị trạng thái thấp nhưng hàm giá trị của nó gần với hàm giá trị tối ưu. 2d cho thấy một chính sách tối ưu được khôi phục bằng cách suy ra chính sách tham lam từ ước lượng Q off-policy (critic).

5 Thí nghiệm
Trong phần này, chúng tôi trình bày kết quả thí nghiệm nhằm trả lời các câu hỏi nghiên cứu sau. RQ1. Hàm giá trị đã học có ngụ ý các giải pháp tốt hơn chính sách ràng buộc không? RQ2. Giải pháp được ngụ ý bởi hàm giá trị có thể đạt được dưới các phương pháp ràng buộc chính sách hiện tại không? RQ3. MCEP có thể cải thiện hiệu suất đáng kể như thế nào? RQ4. MCEP hoạt động như thế nào so với các phương pháp lựa chọn hành động khác cũng sử dụng hàm giá trị? Ngoài ra, chúng tôi áp dụng 2 nhóm nghiên cứu loại bỏ để xác minh lợi ích của một chính sách đánh giá bổ sung và các ràng buộc nhẹ hơn.

Môi trường D4RL (Fu et al., 2020) là một benchmark RL ngoại tuyến bao gồm nhiều tập nhiệm vụ. Các thí nghiệm của chúng tôi chọn 3 phiên bản của bộ dữ liệu MuJoCo locomotion (-v2): dữ liệu được thu thập bằng cách triển khai một chính sách hiệu suất trung bình (medium), buffer replay trong quá trình huấn luyện một chính sách hiệu suất trung bình (medium-replay), một hỗn hợp 50%−50% của dữ liệu trung bình và minh chứng chuyên gia (medium-expert). Để điều tra các nhiệm vụ chiều cao thách thức hơn, chúng tôi bổ sung thu thập 3 bộ dữ liệu cho các nhiệm vụ Humanoid-v2 theo cùng phương pháp thu thập của D4RL: humanoid-medium-v2, humanoid-medium-replay-v2, humanoid-medium-expert-v2. Nhiệm vụ humanoid-v2 có không gian quan sát 376 chiều và không gian hành động 17 chiều. Nhiệm vụ này không được sử dụng rộng rãi trong nghiên cứu RL ngoại tuyến. (Wang et al., 2020; Bhargava et al., 2023) xem xét nhiệm vụ này nhưng dữ liệu của chúng tôi độc lập với họ. So với (Bhargava et al., 2023), chúng tôi không xem xét dữ liệu chuyên gia thuần túy nhưng bao gồm medium-replay để nghiên cứu buffer replay. Thống kê của bộ dữ liệu humanoid được liệt kê trong Bảng 5. Cuối cùng, chúng tôi xem xét một tập 16 nhiệm vụ Thao tác Robot phức tạp từ (Hussing et al., 2023). Chiến lược thu thập dữ liệu của họ tương tự như các nhiệm vụ locomotion, do đó chúng được đặt tên là manipulation-medium và manipulation-medium-replay trong công trình này.

Giao thức Đánh giá Vì huấn luyện RL ngoại tuyến không phụ thuộc vào môi trường, tất cả các kết quả được báo cáo (ngoại trừ việc trực quan hóa quá trình huấn luyện) được tạo ra bằng cách đánh giá chính sách đã học trên môi trường nơi dữ liệu được thu thập. Để trực quan hóa quá trình huấn luyện, chúng tôi lưu các checkpoint của chính sách từ các bước huấn luyện khác nhau và đánh giá chúng trong môi trường nơi dữ liệu được thu thập.

5.1 Một ví dụ minh họa
Để điều tra ràng buộc chính sách dưới một bộ dữ liệu rất dưới mức tối ưu, chúng tôi thiết lập một mê cung đồ chơi MDP tương tự như cái được sử dụng trong (Kostrikov et al., 2022). Môi trường được mô tả trong Hình 2a, trong đó ô lưới màu vàng dưới bên trái là điểm bắt đầu và ô lưới màu xanh lá cây trên bên trái là trạng thái cuối cùng cho phần thưởng 10. Các ô lưới khác không cho phần thưởng. Màu xanh đậm chỉ ra các khu vực không thể đi được. Không gian hành động được định nghĩa là 4 chuyển động theo hướng (mũi tên) và ở lại nơi agent đang ở (vòng tròn đầy). Có 25% xác suất một hành động ngẫu nhiên được thực hiện thay vì hành động từ agent. Đối với bộ dữ liệu, 99 quỹ đạo được thu thập bởi một agent ngẫu nhiên đồng đều và 1 quỹ đạo được thu thập bởi một chính sách chuyên gia. Hình 2b hiển thị hàm giá trị tối ưu (màu sắc) và một trong các chính sách tối ưu.

--- TRANG 7 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
Hình 4: Quá trình huấn luyện (với lỗi chuẩn) của TD3BC và AWAC. Trái: TD3BC trên hopper-medium-v2. Giữa: TD3BC trên walker2d-medium-replay-v2. Phải: AWAC trên hopper-medium-replay-v2.

Hình 5: Giá trị α trong TD3BC cho ước lượng giá trị và suy luận thời gian kiểm tra trong các nhiệm vụ MuJoCo locomotion.

Chúng tôi huấn luyện một chính sách ràng buộc sử dụng Eq.2 và Eq.8 theo cách actor-critic, trong đó actor bị ràng buộc bởi một phân kỳ KL với hệ số trọng số là 1. Hình 2c hiển thị hàm giá trị và chính sách. Chúng tôi quan sát thấy rằng hàm giá trị đã học gần với hàm tối ưu trong Hình 2b. Tuy nhiên, chính sách không thực hiện các hành động tối ưu ở các vùng dưới bên trái nơi giá trị trạng thái tương đối thấp. Vì mục tiêu cải thiện chính sách cho thấy một sự đánh đổi giữa Q và phân kỳ KL, ở các vùng giá trị Q thấp, phân kỳ KL chiếm phần lớn cho mục tiêu học, điều này làm cho chính sách ở gần hơn với các chuyển đổi ở các vùng giá trị thấp. Tuy nhiên, chúng tôi phát hiện rằng hàm giá trị tương ứng chỉ ra một chính sách tối ưu. Trong Hình 2d, chúng tôi khôi phục một chính sách tham lam cơ bản của hàm giá trị đã học (Czarnecki et al., 2019) cho thấy một chính sách tối ưu. Kết luận, ràng buộc có thể làm giảm hiệu suất đánh giá mặc dù hàm giá trị đã học có thể chỉ ra một chính sách tốt hơn.

5.2 Các ràng buộc nhẹ hơn có khả năng cải thiện hiệu suất nhưng gây ra việc học không ổn định
Thí nghiệm mê cung cho thấy rằng một ràng buộc hạn chế có thể làm hại hiệu suất chính sách, điều này thúc đẩy chúng tôi triển khai các ràng buộc nhẹ hơn có khả năng sử dụng tốt hơn hàm giá trị đã học. Chúng tôi điều tra câu hỏi này trong các nhiệm vụ MuJoCo locomotion. Đầu tiên, chúng tôi nới lỏng ràng buộc chính sách trên TD3BC và AWAC bằng cách thiết lập các giá trị siêu tham số khác nhau kiểm soát cường độ của ràng buộc chính sách. Đối với TD3BC, chúng tôi đặt α={1,4,10} (theo thứ tự giảm dần của cường độ ràng buộc). Đối với AWAC, chúng tôi đặt λ={1.0,0.5,0.3,0.1} (theo thứ tự giảm dần của cường độ ràng buộc). Cuối cùng, chúng tôi trực quan hóa hiệu suất đánh giá và các ước lượng Q đã học.

Trong Hình 4, hai cột bên trái cho thấy việc huấn luyện TD3BC trong hopper-medium-v2 và walker2d-medium-replay-v2. Trong cả hai miền, chúng tôi phát hiện rằng việc sử dụng ràng buộc nhẹ hơn bằng cách điều chỉnh α từ 1 đến 4 cải thiện hiệu suất đánh giá, điều này thúc đẩy chúng tôi kỳ vọng hiệu suất tốt hơn với α= 10. Nhìn từ lợi nhuận chuẩn hóa của α= 10, chúng tôi quan sát hiệu suất cao hơn. Tuy nhiên, việc huấn luyện không ổn định vì sự thay đổi đáng kể về cường độ của các ước lượng Q (lưu ý thang log được sử dụng ở hàng đầu). Thí nghiệm này cho thấy sự đánh đổi giữa ước lượng Q ổn định và hiệu suất đánh giá. Cột ngoài cùng bên phải cho thấy việc huấn luyện AWAC trong hopper-medium-replay-v2, chúng tôi quan sát hiệu suất đánh giá cao hơn bằng cách nới lỏng ràng buộc (λ>1). Mặc dù ước lượng Q giữ ổn định trong quá trình huấn luyện ở tất cả giá trị λ, λ cao hơn vẫn dẫn đến hiệu suất chính sách không ổn định (hàng dưới) và gây ra sự sụp đổ hiệu suất với λ= 0.1.

Kết luận về tất cả các ví dụ này, một ràng buộc nhẹ hơn có thể cải thiện hiệu suất nhưng có thể gây ra ước lượng Q không ổn định hoặc hiệu suất chính sách không ổn định.

--- TRANG 8 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
Tên Nhiệm vụ BC CQL IQL EQL TD3BC AWAC DQL
100% 10% gốc MCEP gốc MCEP gốc MCEP
halfcheetah-m 42.4 43.1 44.0 47.4 46.5 48.7 ±0.255.5±0.4 45.1±046.9±0 49.8±0.253.2±0.2
hopper-m 54.1 56.9 58.5 65 67 56.1 ±1.291.8±0.9 58.9±1.998.1±0.6 81.7±6.695.5±2.2
walker2d-m 71 73.3 72.5 80.4 81.8 85.2 ±0.988.8±0.5 79.6±1.5 81.4 ±1.6 85.5 ±0.8 75.3 ±3.6
halfcheetah-m-r 37.8 39.9 45.5 43.2 43.1 44.8 ±0.350.6±0.2 43.3±0.144.9±0.1 47±0.2 47.8±0.1
hopper-m-r 22.5 72 95.0 74.2 87.9 55.2 ±10.8100.9±0.4 64.8±6.2101.1±0.2 100.6±0.2 100.9 ±0.3
walker2d-m-r 14.4 56.6 77.2 62.7 71.4 50.9 ±16.186.3±3.2 84.1±0.6 83.4 ±0.8 93.6 ±2.5 92.6±2.1
halfcheetah-m-e 62.3 93.5 91.6 91.2 89.4 87.1 ±1.4 71.5 ±3.7 77.6 ±2.6 69.5 ±3.8 95.7 ±0.4 93.4±0.8
hopper-m-e 52.5 108.9 105.4 110.2 97.3 91.7 ±10.5 80.1 ±12.7 52.4 ±8.784.3±16.4 102.1±3.0107.7±1.5
walker2d-m-e 107 111.1 108.8 111.1 109.8 110.4 ±0.5111.7±0.3 109.5±0.2 110.1 ±0.2 109.5 ±0.1 109.7 ±0.0
Trung bình 51.5 72.8 77.6 76.1 77.1 70.0 81.9 68.3 79.9 85 86.2

Bảng 1: Lợi nhuận tập phim chuẩn hóa trên benchmark D4RL. Kết quả (ngoại trừ CQL) là trung bình và lỗi chuẩn từ bước cuối cùng của 5 lần chạy sử dụng các seed ngẫu nhiên khác nhau. Hiệu suất cao hơn so với baseline tương ứng được in đậm và hiệu suất tốt nhất theo nhiệm vụ được gạch dưới.

5.3 Chính sách Đánh giá cho phép ràng buộc nhẹ hơn dưới việc học ổn định
Trong phần này, chúng tôi nghiên cứu có hệ thống các cường độ ràng buộc về tính ổn định học và hiệu suất chính sách. Đối với phương pháp ràng buộc chính sách như TD3BC, chỉ các cường độ ràng buộc không gây ra ước lượng giá trị không ổn định mới hợp lệ. Để tiết lộ phạm vi cường độ hợp lệ, chúng tôi điều chỉnh α cho TD3BC trong S={2.5,5,10,20,30,40,50,60,70,80,90,100}. Đối với mỗi α(αe), chúng tôi triển khai 5 lần huấn luyện với các seed ngẫu nhiên khác nhau. Trong Hình 5, chúng tôi trực quan hóa vùng "ước lượng Q an toàn" được tiết lộ, nơi cường độ ràng buộc cho phép ước lượng Q ổn định cho tất cả các seed. Cạnh của vùng màu xanh cho thấy giá trị α thấp nhất gây ra bùng nổ giá trị Q. Chúng tôi phát hiện rằng trong 4 trong 9 môi trường, việc học không ổn định không xuất hiện với tất cả cường độ ràng buộc được xem xét. Tuy nhiên, trong 5 môi trường còn lại, cường độ hợp lệ tương đối hẹp.

Tiếp theo, chúng tôi quan tâm đến cường độ ràng buộc cho hiệu suất chính sách. Chúng tôi điều tra nó với sự trợ giúp của chính sách đánh giá. Chúng tôi điều chỉnh αe cho chính sách đánh giá (TD3BC-EP) trong S, với ˜α= 2.5 cố định. Vùng màu cam trong Hình 5 cho thấy phạm vi αe nơi chính sách đánh giá đã học vượt trội hơn chính sách mục tiêu. Cạnh của nó (đường màu cam) cho thấy các giá trị αe thấp nhất nơi hiệu suất chính sách đánh giá tệ hơn chính sách mục tiêu.

Lưu ý rằng α cân nhắc thuật ngữ Q và do đó α lớn hơn chỉ ra ràng buộc ít hạn chế hơn. Quan sát từ vùng màu cam, chúng tôi phát hiện rằng trong 7 trong 9 nhiệm vụ (7 trục nơi phạm vi màu cam không bằng không), chính sách đánh giá đạt được hiệu suất tốt hơn chính sách mục tiêu (˜α= 2.5). Trong 5 nhiệm vụ (5 trục nơi phạm vi màu cam lớn hơn phạm vi màu xanh), chính sách đánh giá cho phép ràng buộc chính sách nhẹ hơn gây ra ước lượng q không an toàn trong TD3BC. Kết luận, chính sách đánh giá cho phép ràng buộc chính sách nhẹ hơn cho hiệu suất tốt hơn tiềm năng và không ảnh hưởng đến ước lượng Q.

5.4 Đánh giá hiệu suất trên các nhiệm vụ MuJoCo locomotion và Thao tác Robot
D4RL MuJoCo Locomotion Chúng tôi so sánh phương pháp đề xuất với behavior cloning, các baseline RL ngoại tuyến cổ điển AWAC, TD3BC, CQL và IQL, cùng với các phương pháp RL ngoại tuyến SOTA Extreme Q-Learning (EQL) (Garg et al., 2023) và DQL (Wang et al., 2023). Theo (Fujimoto & Gu, 2021), mỗi phương pháp sử dụng siêu tham số tương tự cho tất cả bộ dữ liệu. Danh sách đầy đủ các siêu tham số có thể được tìm thấy trong Phần A.1. Như được hiển thị trong Bảng 1, chúng tôi quan sát thấy rằng MCEP vượt trội đáng kể so với thuật toán cơ sở tương ứng của chúng (được gắn nhãn "gốc"). TD3BC-MCEP đạt được tiến bộ đáng kể trên tất cả bộ dữ liệu medium và medium-replay. Mặc dù tiến bộ vượt trội, chúng tôi quan sát sự suy giảm hiệu suất trên các bộ dữ liệu medium-expert cho thấy một ràng buộc quá thoải mái cho chính sách đánh giá. Tuy nhiên, TD3BC-MCEP đạt được hiệu suất trung bình tốt hơn nhiều so với thuật toán gốc. Chúng tôi cũng cung cấp so sánh hiệu suất giữa TD3BC và TD3BC-MCEP với siêu tham số của chúng được điều chỉnh theo từng nhiệm vụ (Phần A.4), nơi chúng tôi phát hiện rằng TD3BC-MCEP vượt trội hơn TD3BC trong 7 trong 9 nhiệm vụ. Trong AWAC-MCEP, chúng tôi quan sát cải thiện hiệu suất nhất quán so với thuật toán gốc trên hầu hết các nhiệm vụ và hiệu suất trung bình vượt trội hơn thuật toán gốc đáng kể. Ngoài ra, các chính sách đánh giá từ cả TD3BC-MCEP và AWAC-MCEP vượt trội hơn CQL, IQL, và EQL trong khi các chính sách mục tiêu có hiệu suất tương đối tầm thường. Trên phương pháp SOTA, DQL, chúng tôi phát hiện rằng MCEP đạt được cải thiện hiệu suất tiếp theo mặc dù cải thiện không lớn như trên các phương pháp thông thường. Sự khác biệt này có thể do một phương pháp lựa chọn hành động thời gian suy luận mà DQL sử dụng. tức là sử dụng hàm giá trị đã học để lọc ra một hành động có giá trị xấp xỉ cao từ phân phối chính sách, điều này ngầm nới lỏng ràng buộc. Chúng tôi so sánh MCEP với các phương pháp lựa chọn hành động thời gian suy luận trong Phần 5.5.

Hình 6: Lợi nhuận với lỗi chuẩn trong quá trình huấn luyện trên 3 nhiệm vụ humanoid.

Humanoid Một trong những thách thức chính cho RL ngoại tuyến là sự dịch chuyển phân phối. Trong các môi trường chiều cao, thách thức này được tăng cường vì dữ liệu thu thập tương đối hạn chế hơn. Để đánh giá phương pháp đề xuất về khả năng xử lý các môi trường này, chúng tôi thu thập 3 bộ dữ liệu từ nhiệm vụ MoJoCo Humanoid. Theo cách đặt tên của D4RL, chúng tôi đặt tên cho các bộ dữ liệu này là medium, medium-replay và medium-expert. Chi tiết thu thập dữ liệu và thống kê bộ dữ liệu có thể được tìm thấy trong Phần A.2.

Chúng tôi so sánh TD3BC-MCEP với BC, TD3BC, CRR Wang et al. (2020), IQL và chính sách hành vi. Như thấy trong Hình 6, TD3BC-MCEP đạt được lợi nhuận cao nhất trong medium và medium-expert. Cả hai bộ dữ liệu này đều được thu thập bằng cách triển khai chính sách trực tuyến đã học. Trong medium-replay, nơi bộ dữ liệu là buffer replay của việc huấn luyện trực tuyến, TD3BC-MCEP cũng đạt được hiệu suất vượt trội và cho thấy tốc độ hội tụ nhanh hơn so với IQL. Dựa trên kết quả, chúng tôi kết luận rằng MCEP cải thiện đáng kể hiệu suất trên thuật toán gốc cho các môi trường chiều cao.

Hình 7: Đánh giá (với lỗi chuẩn) trên 16 nhiệm vụ Thao tác Robot.

Thao tác Robot Các nhiệm vụ thao tác robot được công nhận như các nhiệm vụ phức tạp cho RL ngoại tuyến. Chúng tôi lấy 16 nhiệm vụ trên robot IIWA của KUKA từ composition suite (Hussing et al., 2023). Các nhiệm vụ này bao gồm 4 nhiệm vụ cơ bản pickplace, push, shelf, trashcan và 4 đối tượng mục tiêu box, dumbbell, hollow-box, plate. Chúng tôi xem xét các bộ dữ liệu Medium và Medium-Replay. Trên các nhiệm vụ này, chúng tôi so sánh TD3BC-MCEP với BC, CRR, IQL và TD3BC. Tương tự như thiết lập locomotion, chúng tôi xem xét siêu tham số tương tự cho tất cả 16 nhiệm vụ. Chi tiết siêu tham số và kết quả đầy đủ có thể được tìm thấy trong Phần A.3. Kết quả tổng thể được trình bày trong Hình 7, nơi chúng tôi quan sát thấy rằng trong khi TD3BC thất bại trong việc cạnh tranh với các phương pháp khác trong Medium và thất bại trong việc vượt trội hơn IQL trong Medium-Replay, MCEP đạt được tỷ lệ thắng cao nhất và vượt trội hơn tất cả các baseline trong cả hai bộ dữ liệu. Kết quả cho thấy rằng MCEP có thể cải thiện hiệu suất từ thuật toán cơ sở ngay cả trong các miền phức tạp.

5.5 So sánh với các phương pháp lựa chọn hành động Thời gian suy luận
Các phương pháp lựa chọn hành động thời gian suy luận (Wang et al., 2020) cung cấp một phương pháp lựa chọn hành động tức thì bằng cách xem xét các đầu ra hàm giá trị. Vì MCEP cũng sử dụng hàm giá trị đã học để tạo ra chính sách đánh giá, người ta có thể muốn biết sự khác biệt hiệu suất giữa MCEP và các phương pháp lựa chọn hành động thời gian suy luận. Chúng tôi xem xét hai loại phương pháp lựa chọn hành động. Argmax: chọn hành động với giá trị q ước lượng cao nhất. Softmax: lấy mẫu hành động với xác suất tỷ lệ với giá trị q ước lượng của chúng. Chúng tôi so sánh TD3BC và TD3BC-MCEP trên các nhiệm vụ humanoid. Để tạo ra các mẫu hành động, chúng tôi thêm nhiễu Gaussian vào các đầu ra chính sách. Chúng tôi xem xét độ lệch chuẩn [0.01,0.02,0.05,0.1] và xem xét kích thước mẫu [20,50,100]. Kết quả tốt nhất được trực quan hóa trong Hình 8.

--- TRANG 9 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
Hình 8: So sánh với các phương pháp lựa chọn hành động Thời gian suy luận.

Từ Hình 8, chúng tôi quan sát thấy rằng các phương pháp lựa chọn hành động này có thể cải thiện hiệu suất trên cả TD3BC và TD3BC-MCEP. Bằng cách so sánh TD3BC với lựa chọn hành động và TD3BC-MCEP mà không có lựa chọn hành động, chúng tôi lưu ý rằng với Argmax, TD3BC vượt trội hơn TD3BC-MCEP trên bộ dữ liệu Medium. Tuy nhiên, Softmax không đạt được hiệu suất cùng mức. Trong các bộ dữ liệu khác, ngay cả với lựa chọn hành động, TD3BC vẫn thất bại trong việc cạnh tranh với TD3BC-MCEP. Kết luận, MCEP mang lại cải thiện hiệu suất đáng kể hơn so với các phương pháp lựa chọn hành động thời gian suy luận. Kết quả đầy đủ có thể được tìm thấy trong Phụ lục A.5.

5.6 Nghiên cứu Loại bỏ
Trong phần này, chúng tôi thiết kế 2 nhóm nghiên cứu loại bỏ để điều tra hiệu ứng của chính sách đánh giá bổ sung và cường độ ràng buộc của nó. Kết quả được báo cáo được tính trung bình trên 5 seed ngẫu nhiên.

Hình 9: Trái: TD3BC với α= 2.5, α= 10 và TD3BC-MCEP với ˜α= 2.5, αe= 10. Phải: AWAC với λ= 1.0, λ= 0.5 và AWAC-MCEP với ˜λ= 1.0 và λe= 0.5. Các lỗi chuẩn cũng được vẽ.

Hiệu suất của chính sách đánh giá bổ sung. Bây giờ, chúng tôi điều tra hiệu suất của chính sách đánh giá πe được giới thiệu. Đối với TD3BC, chúng tôi đặt tham số α={2.5,10.0}. Một α lớn chỉ ra ràng buộc nhẹ hơn. Sau đó, chúng tôi huấn luyện TD3BC-MCEP với ˜α= 2.5 và αe= 10.0. Đối với AWAC, chúng tôi huấn luyện AWAC với λ={1.0,0.5} và AWAC-MCEP với ˜λ= 1.0 và λe= 0.5.

Kết quả được hiển thị trong Hình 9. Điểm số cho các bộ dữ liệu khác nhau được nhóm cho mỗi miền. Bằng cách so sánh TD3BC của các giá trị α khác nhau, chúng tôi phát hiện một ràng buộc nhẹ hơn (α= 10.0) mang lại cải thiện hiệu suất trong các nhiệm vụ hopper nhưng làm giảm hiệu suất trong các nhiệm vụ walker2d. Sự suy giảm có thể do ước lượng giá trị không ổn định (xem thí nghiệm tại phần 5.2). Cuối cùng, chính sách đánh giá (αE= 10.0) với chính sách mục tiêu ˜α= 2.5 đạt được hiệu suất tốt nhất trong cả ba nhiệm vụ. Trong AWAC, giá trị λ thấp hơn mang lại cải thiện chính sách trong các nhiệm vụ hopper nhưng làm giảm hiệu suất trong các nhiệm vụ half-cheetah và walker2d. Cuối cùng, một chính sách đánh giá đạt được hiệu suất tốt nhất trong tất cả các nhiệm vụ.

Kết luận, chúng tôi quan sát cải thiện hiệu suất nhất quán được mang lại bởi một MCEP bổ sung vượt qua sự đánh đổi được mang lại bởi ràng buộc.

Cường độ ràng buộc của chính sách đánh giá. Chúng tôi thiết lập hai nhóm thí nghiệm loại bỏ để điều tra hiệu suất chính sách đánh giá dưới các cường độ ràng buộc khác nhau. Đối với TD3BC-MCEP, chúng tôi điều chỉnh cường độ ràng buộc bằng cách đặt siêu tham số normalizer Q α. Chính sách mục tiêu được cố định ở ˜α= 2.5. Chúng tôi chọn ba cường độ cho chính sách đánh giá αe={1.0,2.5,10.0} để tạo ra các ràng buộc hạn chế hơn, tương tự, và nhẹ hơn, tương ứng. Đối với AWAC-MCEP, chính sách mục tiêu có ˜λ= 1.0. Tuy nhiên, không đơn giản để tạo ra một ràng buộc tương tự cho chính sách đánh giá vì nó có mục tiêu cải thiện chính sách khác nhau. Chúng tôi đặt λe={0.6,1.0,1.4} để cho thấy hiệu suất thay đổi như thế nào với các cường độ ràng buộc khác nhau.

--- TRANG 10 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
Hình 10: Trái: TD3BC-EP với α= 1.0, α= 2.5 và α= 10.0. Phải: AWAC-EP với λ= 1.4, λ= 1.0 và λ= 0.6.

Các cải thiện hiệu suất so với chính sách mục tiêu được hiển thị trong Hình 10. Đối với TD3BC-MCEP, một ràng buộc hạn chế hơn (αe= 1.0) cho việc đánh giá gây ra sự sụt giảm hiệu suất đáng kể. Với ràng buộc tương tự (˜α=αe= 2.5), hiệu suất được cải thiện nhẹ trong hai miền. Khi chính sách đánh giá có ràng buộc nhẹ hơn (αe= 10), các cải thiện hiệu suất đáng kể được quan sát trong cả 3 miền. Cột bên phải trình bày kết quả của AWAC-MCEP. Nói chung, hiệu suất trong các nhiệm vụ hopper tiếp tục tăng với các ràng buộc nhẹ hơn (λ nhỏ hơn) trong khi các nhiệm vụ half-cheetah và walker2d cho thấy hiệu suất được tăng cường từ λ= 1.4 đến λ= 1 và hiệu suất tương tự giữa λ= 1 và λ= 0.6. Đáng chú ý rằng chính sách đánh giá nhất quán vượt trội hơn chính sách mục tiêu trong các miền halfcheetah và hopper. Trên nhiệm vụ walker2d, một ràng buộc mạnh (λ= 1.4) gây ra sự suy giảm hiệu suất.

Kết luận, đối với cả hai thuật toán, chúng tôi quan sát thấy rằng trên chính sách đánh giá, một ràng buộc nhẹ hơn đạt được hiệu suất cao hơn chính sách mục tiêu trong khi ràng buộc hạn chế có thể làm hại hiệu suất.

Giá trị Q ước lượng cho các chính sách đánh giá đã học Để so sánh hiệu suất của các chính sách trên mục tiêu học (tối đa hóa giá trị Q), chúng tôi trực quan hóa sự khác biệt Q giữa hành động chính sách và hành động dữ liệu Q(s,π(s))−Q(s,a) trong dữ liệu huấn luyện (Hình 14, 15 trong Phần A.7). Chúng tôi phát hiện rằng cả chính sách mục tiêu và MCEP đều có ước lượng Q lớn hơn so với các hành động hành vi. Ngoài ra, MCEP nói chung có giá trị Q cao hơn chính sách mục tiêu, cho thấy rằng MCEP có thể di chuyển xa hơn về phía giá trị Q lớn.

6 Kết luận
Công trình này tập trung vào các phương pháp ràng buộc chính sách nơi ràng buộc giải quyết sự đánh đổi giữa ước lượng giá trị và hiệu suất đánh giá. Chúng tôi đầu tiên điều tra các phạm vi cường độ ràng buộc cho ước lượng giá trị ổn định và cho hiệu suất đánh giá. Các phát hiện của chúng tôi cho thấy rằng hàm giá trị đã học không được khai thác tốt dưới sự đánh đổi này. Sau đó chúng tôi đề xuất tách biệt các vấn đề của việc học giá trị và hiệu suất chính sách, và đưa ra một phương pháp chính sách đánh giá ràng buộc nhẹ đơn giản và tổng quát. Phương pháp mới vượt qua sự đánh đổi nói trên do đó đạt được việc học giá trị ổn định và hiệu suất chính sách đồng thời. Các kết quả thực nghiệm trên các nhiệm vụ locomotion, humanoid và thao tác robot cho thấy rằng MCEP có thể đạt được cải thiện hiệu suất đáng kể.

Hạn chế. Mặc dù MCEP có thể đạt được hiệu suất tốt hơn, chính sách đánh giá đòi hỏi nỗ lực bổ sung trong việc điều chỉnh cường độ ràng buộc của nó. Chúng tôi đề xuất bắt đầu từ cường độ của chính sách mục tiêu và thử các ràng buộc nhẹ hơn. Lưu ý rằng một ràng buộc hiệu quả vẫn không thể thiếu để tránh quá nhiều hành động OOD có nguy cơ ứng dụng thực tế. Hơn nữa, hiệu suất của MCEP phụ thuộc vào ước lượng giá trị ổn định. Việc học giá trị không ổn định có thể làm sụp đổ cả chính sách mục tiêu và chính sách đánh giá. Trong khi chính sách mục tiêu có thể khôi phục hiệu suất của nó bằng cải thiện chính sách lặp và đánh giá chính sách (actor-critic), chúng tôi quan sát thấy rằng chính sách đánh giá có thể thất bại trong việc khôi phục hiệu suất của nó từ sự sụp đổ. Do đó, một chính sách mục tiêu ràng buộc hạn chế ổn định hóa việc học giá trị là cần thiết cho phương pháp đề xuất.

--- TRANG 11 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
Tài liệu tham khảo
Prajjwal Bhargava, Rohan Chitnis, Alborz Geramifard, Shagun Sodhani, và Amy Zhang. Sequence mod-
eling is a robust contender for offline reinforcement learning. arXiv preprint arXiv:2305.14550 , 2023.
Chethan Bhateja, Derek Guo, Dibya Ghosh, Anikait Singh, Manan Tomar, Quan Vuong, Yevgen Chebotar,
Sergey Levine, và Aviral Kumar. Robotic offline rl from internet videos via value-function pre-training.
arXiv preprint arXiv:2309.13041 , 2023.
David Brandfonbrener, Will Whitney, Rajesh Ranganath, và Joan Bruna. Offline rl without off-policy
evaluation. Advances in neural information processing systems , 34:4933–4946, 2021.
Y. Cai, C. Zhang, L. Zhao, W. Shen, X. Zhang, L. Song, J. Bian, T. Qin, và T. Liu. Td3 with reverse kl
regularizer for offline reinforcement learning from mixed datasets. In 2022 IEEE International Conference
on Data Mining (ICDM) , pp. 21–30, Los Alamitos, CA, USA, dec 2022. IEEE Computer Society. doi:
10.1109/ICDM54844.2022.00012. URL https://doi.ieeecomputersociety.org/10.1109/ICDM54844.
2022.00012 .
Wojciech M Czarnecki, Razvan Pascanu, Simon Osindero, Siddhant Jayakumar, Grzegorz Swirszcz, và Max
Jaderberg. Distilling policy distillation. In The 22nd international conference on artificial intelligence and
statistics , pp. 1331–1340. PMLR, 2019.
Soysal Degirmenci và Chris Jones. Benchmarking offline reinforcement learning algorithms for e-commerce
order fraud evaluation. In 3rd Offline RL Workshop: Offline RL as a "Launchpad" , 2022. URL https:
//openreview.net/forum?id=plpbiLbnG7 .
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, và Sergey Levine. D4rl: Datasets for deep data-
driven reinforcement learning. arXiv preprint arXiv:2004.07219 , 2020.
Scott Fujimoto và Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. Advances
in neural information processing systems , 34:20132–20145, 2021.
Scott Fujimoto, Herke Hoof, và David Meger. Addressing function approximation error in actor-critic
methods. In International conference on machine learning , pp. 1587–1596. PMLR, 2018.
Scott Fujimoto, David Meger, và Doina Precup. Off-policy deep reinforcement learning without exploration.
In International conference on machine learning , pp. 2052–2062. PMLR, 2019.
Divyansh Garg, Joey Hejna, Matthieu Geist, và Stefano Ermon. Extreme q-learning: Maxent RL without
entropy. In The Eleventh International Conference on Learning Representations , 2023. URL https:
//openreview.net/forum?id=SJ0Lde3tRL .
Matthew W. Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Nikola Momchev, Danila
Sinopalnikov, Piotr Stańczyk, Sabela Ramos, Anton Raichuk, Damien Vincent, Léonard Hussenot, Robert
Dadashi, Gabriel Dulac-Arnold, Manu Orsini, Alexis Jacq, Johan Ferret, Nino Vieillard, Seyed Kam-
yar Seyed Ghasemipour, Sertan Girgin, Olivier Pietquin, Feryal Behbahani, Tamara Norman, Abbas
Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Abe Friesen, Ruba Haroun,
Alex Novikov, Sergio Gómez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Srivatsan
Srinivasan, Andrew Cowie, Ziyu Wang, Bilal Piot, và Nando de Freitas. Acme: A research frame-
work for distributed reinforcement learning. arXiv preprint arXiv:2006.00979 , 2020. URL https:
//arxiv.org/abs/2006.00979 .
Marcel Hussing, Jorge A Mendez, Anisha Singrodia, Cassandra Kent, và Eric Eaton. Robotic manipulation
datasets for offline compositional reinforcement learning. arXiv preprint arXiv:2307.07091 , 2023.
Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones,
Shixiang Gu, và Rosalind Picard. Way off-policy batch deep reinforcement learning of implicit human
preferences in dialog. arXiv preprint arXiv:1907.00456 , 2019.

--- TRANG 12 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski, Chelsea Finn,
Sergey Levine, và Karol Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at
scale.arXiv preprint arXiv:2104.08212 , 2021.
Ilya Kostrikov. JAXRL: Implementations of Reinforcement Learning algorithms in JAX, 10 2022. URL
https://github.com/ikostrikov/jaxrl2 . v2.
Ilya Kostrikov, Ashvin Nair, và Sergey Levine. Offline reinforcement learning with implicit q-learning. In
International Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=
68n2s9ZJWF8 .
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, và Sergey Levine. Stabilizing off-policy q-learning
via bootstrapping error reduction. Advances in Neural Information Processing Systems , 32, 2019.
Aviral Kumar, Aurick Zhou, George Tucker, và Sergey Levine. Conservative q-learning for offline reinforce-
ment learning. Advances in Neural Information Processing Systems , 33:1179–1191, 2020.
Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, và Sergey Levine. A workflow for offline model-
free robotic reinforcement learning. In Conference on Robot Learning , pp. 417–428. PMLR, 2022.
Jiachen Li, Edwin Zhang, Ming Yin, Qinxun Bai, Yu-Xiang Wang, và William Yang Wang. Offline re-
inforcement learning with closed-form policy improvement operators. In International Conference on
Machine Learning , pp. 20485–20528. PMLR, 2023.
Jiafei Lyu, Xiaoteng Ma, Xiu Li, và Zongqing Lu. Mildly conservative q-learning for offline reinforcement
learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun Cho (eds.), Advances in
Neural Information Processing Systems , 2022. URL https://openreview.net/forum?id=VYYf6S67pQc .
Ashvin Nair, Abhishek Gupta, Murtaza Dalal, và Sergey Levine. Awac: Accelerating online reinforcement
learning with offline datasets. arXiv preprint arXiv:2006.09359 , 2020.
Xue Bin Peng, Aviral Kumar, Grace Zhang, và Sergey Levine. Advantage-weighted regression: Simple and
scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177 , 2019.
Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, và Chelsea Finn. Offline reinforcement learning from
images with latent space models. In Learning for Dynamics and Control , pp. 1154–1168. PMLR, 2021.
Dhruv Shah, Arjun Bhorkar, Hrishit Leen, Ilya Kostrikov, Nicholas Rhinehart, và Sergey Levine. Offline
reinforcement learning for visual navigation. In 6th Annual Conference on Robot Learning , 2022. URL
https://openreview.net/forum?id=uhIfIEIiWm_ .
Noah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas
Lampe, Roland Hafner, Nicolas Heess, và Martin Riedmiller. Keep doing what worked: Behavior mod-
elling priors for offline reinforcement learning. In International Conference on Learning Representations ,
2020. URL https://openreview.net/forum?id=rke7geHtwH .
Anikait Singh, Aviral Kumar, Quan Vuong, Yevgen Chebotar, và Sergey Levine. Offline rl with realistic
datasets: Heteroskedasticity and support constraints. arXiv preprint arXiv:2211.01052 , 2022.
Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, và Sergey Levine. Offline rl for natural language
generation with implicit language q learning. arXiv preprint arXiv:2206.11871 , 2022.
Paloma Sodhi, Felix Wu, Ethan R Elenberg, Kilian Q Weinberger, và Ryan McDonald. On the effectiveness
of offline rl for dialogue response generation. In International Conference on Machine Learning , pp. 32088–
32104. PMLR, 2023.
Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, và Joseph Modayil.
Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648 , 2018.

--- TRANG 13 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
Qing Wang, Jiechao Xiong, Lei Han, Peng Sun, Han Liu, và Tong Zhang. Exponentially weighted imita-
tion learning for batched historical data. In Proceedings of the 32nd International Conference on Neural
Information Processing Systems , pp. 6291–6300, 2018.
Zhendong Wang, Jonathan J Hunt, và Mingyuan Zhou. Diffusion policies as an expressive policy class
for offline reinforcement learning. In The Eleventh International Conference on Learning Representations ,
2023. URL https://openreview.net/forum?id=AHvFDPi-FA .
Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E Reed, Bobak
Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, et al. Critic regularized regression. Advances in
Neural Information Processing Systems , 33:7768–7778, 2020.
Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, và Mingsheng Long. Supported policy optimization
for offline reinforcement learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, và Kyunghyun
Cho (eds.), Advances in Neural Information Processing Systems , 2022. URL https://openreview.net/
forum?id=KCXQ5HoM-fy .
Yifan Wu, George Tucker, và Ofir Nachum. Behavior regularized offline reinforcement learning. arXiv
preprint arXiv:1911.11361 , 2019.
Lantao Yu, Tianhe Yu, Jiaming Song, Willie Neiswanger, và Stefano Ermon. Offline imitation learning with
suboptimal demonstrations via relaxed distribution matching. arXiv preprint arXiv:2303.02569 , 2023.
Wenxuan Zhou, Sujay Bajracharya, và David Held. Plas: Latent action space for offline reinforcement
learning. In Conference on Robot Learning , pp. 1719–1735. PMLR, 2021.

--- TRANG 14 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
A Phụ lục
A.1 Chi tiết triển khai và siêu tham số cho đánh giá locomotion
Đối với CQL, chúng tôi báo cáo kết quả từ bài báo IQL (Kostrikov et al., 2022) để hiển thị kết quả CQL trên các nhiệm vụ "-v2". Đối với IQL, chúng tôi sử dụng triển khai chính thức (Kostrikov, 2022) để đạt được hiệu suất tương tự như những gì được báo cáo trong bài báo của họ. Các triển khai của chúng tôi về TD3BC, TD3BC-MCEP, AWAC, và AWAC-MCEP dựa trên framework (Kostrikov, 2022). Trong tất cả các phương pháp được triển khai lại/triển khai, clipped double Q-learning (Fujimoto et al., 2018) được sử dụng. Trong TD3BC và TD3BC-MCEP, chúng tôi giữ chuẩn hóa trạng thái được đề xuất trong (Fujimoto & Gu, 2021) nhưng các thuật toán khác không sử dụng nó. Đối với EQL và DQL, chúng tôi sử dụng triển khai chính thức của họ và DQL-MCEP cũng được xây dựng dựa trên codebase được phát hành Wang et al., 2023.

Các phương pháp baseline (TD3BC, AWAC và DQL) sử dụng siêu tham số được khuyến nghị bởi các bài báo của họ. TD3BC sử dụng α= 2.5 cho normalizer giá trị Q của nó, AWAC sử dụng 1.0 cho normalizer giá trị lợi thế và DQL sử dụng α= 1.0. Trong TD3BC-MCEP, chính sách mục tiêu sử dụng ˜α= 2.5 và MCEP sử dụng αe= 10. Trong AWAC-MCEP, chính sách mục tiêu có ˜λ= 1.0 và MCEP có λe= 0.6. Trong DQL-MCEP, ˜α= 1.0 cho chính sách mục tiêu và αe= 2.5 cho chính sách đánh giá. Danh sách đầy đủ các siêu tham số được sử dụng trong thí nghiệm có thể được tìm thấy trong Bảng 2.

BC IQL AWAC AWAC-MCEP TD3BC TD3BC-MCEP
actor LR 1e-3 3e-4 3e-5 3e-5 3e-4 3e-4
actor^e LR - 3e-5 - 3e-4
critic LR - 3e-4
VLR - 3e-4 -
actor/critic network (256, 256)
discount factor 0.99
soft update τ - 0.005
dropout 0.1 -
Policy TanhNormal Deterministic
MuJoCo Locomotion
τ cho IQL - 0.7 -
λ/˜λ - 1/λ= 3 1.0 -
λe- 0.6 -
α/˜α - 2.5
αe- 10.0

Bảng 2: Các siêu tham số cho các nhiệm vụ MuJoCo locomotion.

Nhiệm vụ # quỹ đạo # mẫu Trung bình Lợi nhuận
Humanoid Medium 2488 1M 1972.8
Humanoid Medium Replay 3008 0.502M 830.2
Humanoid Medium Expert 3357 1.99M 2920.5

Bảng 3: Thống kê bộ dữ liệu cho dữ liệu humanoid ngoại tuyến.

A.2 Thu thập dữ liệu và điều chỉnh siêu tham số cho các nhiệm vụ humanoid
Siêu tham số. Trong thí nghiệm này, chúng tôi chọn Top-10 Behavior cloning, TD3BC và IQL làm baseline của chúng tôi. Đối với Top-10 Behavior cloning, chỉ 10% dữ liệu có lợi nhuận cao nhất được chọn để học. Đối với TD3BC, chúng tôi tìm kiếm siêu tham số α={0.1,0.5,1.0,2.0,3.0,4.0,5.0}. Đối với IQL, chúng tôi tìm kiếm siêu tham số expectile τ={0.6,0.7,0.8,0.9} và siêu tham số trích xuất chính sách λ={0.1,1.0,2.0,3.0}. Đối với CRR, chúng tôi điều chỉnh hệ số lợi thế β={0.1,0.6,0.8,1.0,1.2,5.0}. Đối với TD3BC-MCEP, chúng tôi tìm kiếm ˜α={0.1,0.5,1.0,2.0,3.0} và αE={3.0,4.0,5.0,10.0}. Các siêu tham số được chọn cuối cùng được liệt kê trong

--- TRANG 15 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
Bảng 4. Đối với CRR, chúng tôi triển khai phiên bản CRR exp dựa trên (Hoffman et al., 2020). Phiên bản này được xem xét vì nó vượt trội hơn các baseline khác trong (Wang et al., 2020) trong các môi trường phức tạp như humanoid. Chúng tôi cũng áp dụng Critic Weighted Policy cũng như một phiên bản argmax của nó (CRR-argmax). Các tùy chọn thiết kế này dẫn đến các biến thể CRR, CRR-CWP và CRR-Argmax. Trong Hình 6, chúng tôi báo cáo biến thể CRR hiệu suất tốt nhất cho mỗi nhiệm vụ. Trong tất cả các biến thể của nó, CRR-Argmax cho thấy hiệu suất tốt hơn trong cả medium và medium-replay trong khi CRR hoạt động tốt nhất trong nhiệm vụ medium-expert.

Thu thập Dữ liệu Humanoide. Trong Bảng 5, chúng tôi cung cấp thống kê của dữ liệu được thu thập.

BC IQL TD3BC TD3BC-MCEP
actor LR 1e-3 3e-4 3e-4 3e-4
actor^e LR - - 3e-4
critic LR - 3e-4
VLR - 3e-4 -
actor/critic network (256, 256)
discount factor 0.99
soft update τ - 0.005
dropout 0.1 -
Policy TanhNormal Deterministic
Humanoid-medium-v2
τ cho IQL - 0.6 -
λ/˜λ - 1 -
α/˜α - 1 0.5
αe- 3
Humanoid-medium-replay-v2
τ cho IQL - 0.6 -
λ/˜λ - 0.1 -
α/˜α - 0.5 1.0
αe- 10
Humanoid-medium-expert-v2
τ cho IQL - 0.6 -
λ/˜λ - 0.1 -
α/˜α - 2 0.5
αe- 3

Bảng 4: Các siêu tham số cho nhiệm vụ Humanoid.

A.3 Kết quả đầy đủ cho các thí nghiệm thao tác robot
Chúng tôi xem xét siêu tham số tương tự cho tất cả các nhiệm vụ. Để có được so sánh công bằng, chúng tôi tìm kiếm rộng rãi các siêu tham số cho tất cả baseline. CRR được điều chỉnh trong λ= [0.4,0.6,0.8,1.0,1.2] (1.0 được chọn) và sử dụng critic weighted policy (Wang et al., 2020). Đối với TD3BC, chúng tôi điều chỉnh λ= [1.0,2.0,3.0,4.0] và chọn 2.0. IQL sử dụng τ= 0.7, λ= 3.0 như được khuyến nghị trong bài báo bộ dữ liệu (Hussing et al., 2023). Đối với TD3BC-MCEP, chúng tôi sử dụng ˜α= 2, αE= [4.0,6.0,8.0,10.0] (8.0 được chọn).

A.4 So sánh với siêu tham số cụ thể theo nhiệm vụ trên các nhiệm vụ locomotion
Để điều tra cường độ ràng buộc chính sách tối ưu cụ thể theo nhiệm vụ, chúng tôi tìm kiếm siêu tham số này cho TD3BC và TD3BC-MCEP (với ˜α= 2.5) trong mỗi nhiệm vụ của tập locomotion. Các giá trị tối ưu của chúng và cải thiện hiệu suất tương ứng được trực quan hóa trong Hình 11. Như chúng tôi quan sát, trong 7 trong 9 nhiệm vụ, các chính sách tối ưu được tìm thấy bởi TD3-MCEP vượt trội hơn các chính sách tối ưu được tìm thấy bởi TD3BC. Trong tất cả các nhiệm vụ medium, mặc dù cường độ ràng buộc tối ưu giống nhau cho TD3BC và TD3BC-MCEP, TD3BC-MCEP vượt trội hơn TD3BC. Điều này được hưởng lợi từ việc nới lỏng ràng buộc của chính sách đánh giá không ảnh hưởng đến ước lượng giá trị. Tuy nhiên, đối với TD3BC, ràng buộc nhẹ hơn có thể gây ra ước lượng giá trị không ổn định

--- TRANG 16 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
Bộ dữ liệu BC CRR IQL TD3BC -MCEP
Medium
Box-PickPlace 10.8(1.3) 92.8(0.5) 93.8(2.7) 89.8(2.9) 100(0)
Box-Push 74.6(1.9) 39.2(4.2) 91.8(1.1) 93.8(1.6) 99.8(0.2)
Box-Shelf 91.8(1.2) 91.6(0.7)) 98.6(0.9) 93.2(2.8) 99.2(0.7)
Box-Trashcan 8.6(2.3) 24(2.9) 0(0) 1.2(1.1) 0(0)
Dumbbell-PickPlace 38.6(2.7) 52.2(1.2) 86.8(2.0) 63.2(3.0) 70.4(9.6)
Dumbbell-Push 55.2(3.7) 20.6(1.2) 66.6(2.4) 54.0(8.6) 58.0(10.4)
Dumbbell-Shelf 40.8(4.9) 50.2(1.5) 0.6(0.4) 21.0(4.3) 44.6(11.2)
Dumbbell-Trashcan 5.2(0.7) 62(1.8) 87.1(3.8) 28.0(10.2) 68.2(16.1)
Hollowbox-PickPlace 42.4(3.5) 85.8(2.0) 95.2(2.4) 82.6(11.1) 92.2(3.6)
Hollowbox-Push 0(0) 55.2(4.8) 69.4(7.5) 49.2(4.8) 98.2(1.0)
Hollowbox-Shelf 72.2(1.8) 94(0.7) 98.2(1.4) 95.4(1.7) 98.4(0.6)
Hollowbox-Trashcan 0(0) 29.2(2.1) 0(0) 0(0) 0(0)
Plate-PickPlace 0(0.2) 60.8(3.4) 1(0.3) 2.2(0.4) 0.4(0.2)
Plate-Push 0(0) 12.6(1.1) 0(0) 0(0) 25.0(11.9)
Plate-Shelf 24.2(7.4) 67(2.0) 99(0.4) 60.4(19.6) 99.8(0.2)
Plate-Trashcan 0.2(0.2) 74(1.1) 0.4(0.2) 0.8(0.2) 0(0)
Trung bình 29.0 56.9 55.5 45.9 59.6
Medium-Replay
Box-PickPlace 0(0) 41.4(2.9) 50.8(11.5) 23.0(12.1) 0(0)
Box-Push 0(0) 2.8(0.8) 0(0) 60.8(5.1) 41.6(10.3)
Box-Shelf 0(0) 3.0(1.4) 16.6(4.6) 6.4(2.1) 49.8(14.6)
Box-Trashcan 0(0) 1.4(0.5) 92.3(2.6) 0(0) 76.2(6.9)
Dumbbell-PickPlace 0(0) 25.6(4.6) 34.1(2.7) 8.2(6.9) 0(0)
Dumbbell-Push 4.1(2.1) 1.4(0) 3.2(1.6) 24.2(5.0) 55.8(6.0)
Dumbbell-Shelf 9.8(2.3) 2.6(0.8) 11.6(6.1) 25.4(7.8) 12.0(10.1)
Dumbbell-Trashcan 5.0(2.3) 17.0(4.3) 65.0(10.2) 29.0(5.6) 95.2(0.5)
Hollowbox-PickPlace 0(0) 5.6(3.3) 0(0) 6.6(5.9) 0.4(0.4)
Hollowbox-Push 0(0) 18.2(3.0) 30.0(3.7) 23.0(12.2) 3.0(2.7)
Hollowbox-Shelf 0(0) 1.8(0.9) 61.4(4.7) 32.0(6.5) 58.4(12.6)
Hollowbox-Trashcan 0(0) 4.4(0.8) 4.8(4.6) 0(0) 0(0)
Plate-PickPlace 0(0) 31.4(2.2) 29.4(17.8) 2.2(2.0) 0(0)
Plate-Push 0(0) 0(0) 0(0) 0(0) 10.6(9.5)
Plate-Shelf 0(0) 0.8(0.4) 0(0) 0(0) 19.4(17.4)
Plate-Trashcan 0.8(0.4) 32.8(1.1) 1.0(0.3) 0.4(0.2) 0(0)
Trung bình 1.2 11.9 25.0 15.0 26.4

Bảng 5: Tỷ lệ thắng với lỗi chuẩn cho các nhiệm vụ Thao tác Robot. Tất cả kết quả được tính trung bình trong 5 seed ngẫu nhiên. Tỷ lệ thắng của MCEP vượt trội hơn thuật toán cơ sở được in đậm.

trong quá trình huấn luyện. Trong tất cả các nhiệm vụ medium-replay, chúng tôi phát hiện các ràng buộc tối ưu cho TD3BC-MCEP nhẹ hơn TD3BC, điều này xác minh yêu cầu của các ràng buộc nhẹ hơn 5.2.

A.5 Điều tra các phương pháp khác cho lựa chọn hành động thời gian suy luận
MCEP nhằm cải thiện hiệu suất thời gian suy luận mà không tăng lỗi ước lượng Bellman. Các công trình trước đây cũng đề xuất sử dụng các phương pháp lựa chọn hành động thời gian suy luận tức thì. Ví dụ, (Wang et al., 2020) đề xuất Critic Weighted Policy (CWP), trong đó critic được sử dụng để xây dựng một phân phối phân loại cho lựa chọn hành động thời gian suy luận. Một phương pháp đơn giản khác là chọn hành động có giá trị Q lớn nhất, được gọi là Argmax. Trong phần này, chúng tôi so sánh hiệu suất của TD3BC và TD3BC-MCEP dưới các phương pháp lựa chọn hành động thời gian kiểm tra khác nhau trong các nhiệm vụ Humanoid chiều cao. Kết quả được trình bày trong Bảng 6 và 7. Cả phương pháp Argmax và CWP đều chọn một hành động từ một tập hành động. Chúng tôi tạo ra tập hành động này bằng cách thêm nhiễu Gaussian vào các đầu ra của chính sách xác định. std là thang nhiễu và N là kích thước của tập hành động này. Từ kết quả, chúng tôi quan sát thấy rằng

--- TRANG 17 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
Hình 11: Trái: Giá trị αE tối ưu cho chính sách đánh giá của TD3BC-MCEP, với α= 2.5 cố định cho chính sách mục tiêu. Giá trị ˜α tối ưu cho TD3BC. Vùng đỏ chỉ ra các giá trị α cho TD3BC gây ra bùng nổ Q-value (trong một hoặc nhiều lần huấn luyện của một huấn luyện 5-seed). Phải: Sự khác biệt hiệu suất (với lỗi chuẩn) giữa chính sách đánh giá của TD3BC-MCEP và actor của TD3BC, sử dụng các giá trị αE(α) được hiển thị trong hình bên trái.

CWP và Argmax giúp cải thiện hiệu suất của cả TD3BC và TD3BC-MCEP. Đáng chú ý rằng, trong nhiệm vụ medium, phương pháp Argmax cải thiện TD3BC lên cùng mức với TD3BC-MCEP. Nhưng trong các nhiệm vụ meidum-replay và medium-expert, hiệu suất được cải thiện vẫn tệ hơn TD3BC-MCEP ban đầu (mà không sử dụng lựa chọn hành động). Trên TD3BC-MCEP, việc áp dụng Argmax và CWP cải thiện thêm hiệu suất chính sách.

Kết luận, hiệu suất thời gian suy luận có thể được cải thiện bằng cách sử dụng các phương pháp lựa chọn hành động thời gian suy luận nhưng MCEP cho thấy cải thiện chính sách đáng kể hơn và không cho thấy xung đột với các phương pháp lựa chọn hành động này.

Bảng 6: TD3BC với lựa chọn hành động thời gian suy luận. Chính sách gốc có lợi nhuận 2483.9, 965.4 và 3898.2 cho medium, medium-replay và medium-expert, tương ứng. Lỗi chuẩn cũng được báo cáo.

Trò chơi Argmax CWP
N\std 0.01 0.02 0.05 0.1 0.01 0.02 0.05 0.1
medium20 2462.4(186.4) 2944.5(371.3) 3098.9(149.2) 3511.1 (244.7) 2441.4(217.4) 2689.8(323.2) 2755.4(303.9) 3113.2(416.5)
50 2564.2(180.1) 2836.0(399.7) 2956.6(128.2) 3156.3(272.4) 2159.8(239.8) 2588.6(181.7) 2462.1(180.5) 2839.6(165.6)
100 2857.0(96.7) 2369.8(250.4) 3122.0(228.7) 3266.8(314.4) 2607.6(140.9) 2665.8(335.5) 2722.5(305.6) 2584.1(216.7)
medium-replay20 895.3(44.1) 1042.3(133.3) 1136.7(135.8) 1524.1(125.3) 973.9(75.7) 931.9(61.9) 932.2(42.4) 1242.7(116.5)
50 994.6(60.3) 976.7(48.9) 1160.2(72.8) 1664.9(248.8) 974.7(66.5) 1030.3(96.5) 1002.1(87.7) 1171.1(124.6)
100 971.7(69.5) 1049.0(55.6) 1232.2(144.2) 1574.7(179.9) 874.2(41.9) 1023.5(85.2) 973.0(74.8) 1232.9(117.2)
medium-expert20 3861.7(345.8) 4068.4(175.7) 4131.0(299.7) 4585.3(206.6) 4181.1(255.3) 4478.3(174.2) 3904.0(166.1) 3636.7(253.4)
50 4460.6(135.7) 4012.2(318.7) 4612.9 (127.7) 4603.0(137.5) 3987.0(288.1) 4068.9(206.0) 3995.1(323.6) 4214.7 (157.0)
100 4130.8(301.2) 4141.7(248.4) 4158.3(302.8) 4421.4(130.4) 4145.3(262.0) 3634.6(389.4) 3933.9(249.4) 3788.3(246.1)

Bảng 7: TD3BC-MCEP với lựa chọn hành động thời gian suy luận. Chính sách gốc có lợi nhuận 2962.8, 4115.6 và 4829.2 cho medium, medium-replay và medium-expert, tương ứng. Lỗi chuẩn cũng được báo cáo.

Trò chơi Argmax CWP
N\std 0.01 0.02 0.05 0.1 0.01 0.02 0.05 0.1
medium20 2368.2(221.2) 2871.4(334.1) 2924.1(211.8) 3392.8(319.4) 2670.5(262.8) 2710.0(96.8) 3146.5(216.6) 2987.9(244.0)
50 2822.1(194.0) 3046.3(287.2) 3283.9(298.3) 3861.7 (225.2) 2612.9(142.0) 2787.1(154.9) 2718.9(344.9) 2841.7(248.7)
100 3405.1(326.9) 2808.2(191.8) 3264.5(310.4) 3751.3(490.7) 3003.3 (312.1) 2896.8(192.2) 2748.9(163.1) 2727.2(284.9)
medium-replay204277.3 (708.5) 4071.5(703.4) 4092.7(694.1) 4253.4(659.7) 4033.1(689.8) 4200.0(652.1) 4254.4(703.6) 4167.0(632.7)
50 4225.5(691.8) 4159.7(681.4) 4028.4(621.6) 4210.8(710.2) 4135.5(651.7) 4219.1(680.6) 4375.7 (760.6) 4230.3(629.4)
100 4190.3(691.4) 3966.4(639.9) 4138.4(694.1) 4270.1(695.0) 4328.5(733.3) 4266.9(638.9) 4275.7(671.3) 4142.5(598.7)
medium-expert20 4752.8(232.4) 4956.7(92.6) 4880.3(229.6) 4887.1(238.3) 4736.4(266.5) 4710.8(226.7) 4748.7(177.5) 4942.1(163.0)
50 4930.7(157.6) 5018.2 (73.6) 4614.2(150.0) 4899.9(158.7) 5053.4 (70.6) 5001.4(85.0) 4808.8(183.4) 4670.6(204.2)
100 4616.8(217.9) 4800.9(145.4) 4700.9(179.7) 4648.0(288.5) 4588.1(237.3) 4770.6(86.3) 4934.3(176.6) 4855.3(146.9)

--- TRANG 18 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
A.6 Tùy chọn thiết kế về cách chính sách đánh giá cập nhật
Vì chính sách đánh giá không tham gia vào cập nhật lặp của actor-critic, người ta có thể muốn cập nhật chính sách đánh giá sau khi actor-critic hội tụ, được gọi là cập nhật sau đó. Mặc dù đây là một tùy chọn thiết kế hợp lệ, phương pháp của chúng tôi cập nhật đồng thời chính sách mục tiêu và chính sách đánh giá (cập nhật đồng thời). Theo cách này, các cập nhật của chúng có thể được song song hóa và không cần thêm thời gian dựa trên việc huấn luyện actor-critic. Việc song song hóa này có thể giảm đáng kể thời gian huấn luyện cho các phương pháp cập nhật chính sách chậm (ví dụ DQL). Hình 12 và 13 trình bày sự hội tụ cho hai tùy chọn thiết kế này. Từ kết quả, chúng tôi quan sát sự hội tụ nhanh hơn của cập nhật sau đó trong một số nhiệm vụ. Tuy nhiên, cũng có nhiều nhiệm vụ mà phương pháp cập nhật sau đó hội tụ sau một triệu bước.

Hình 12: Lợi nhuận tập phim với lỗi chuẩn của cập nhật đồng thời và cập nhật sau đó cho đánh giá cho TD3BC-MCEP. Hàng đầu: halfcheetah. Hàng thứ hai hopper. Hàng thứ ba: walker2d.

Hình 13: Lợi nhuận tập phim với lỗi chuẩn của cập nhật đồng thời và cập nhật sau đó cho đánh giá cho AWAC-MCEP. Hàng đầu: halfcheetah. Hàng thứ hai hopper. Hàng thứ ba: walker2d.

--- TRANG 19 ---
Xuất bản trong Transactions on Machine Learning Research (05/2024)
A.7 Kết quả đầy đủ cho giá trị Q ước lượng của các chính sách đánh giá đã học
Hình 14 và Hình 15 cho thấy trực quan hóa các giá trị Q ước lượng được đạt bởi chính sách mục tiêu và chính sách đánh giá.

(a) medium-expert
 (b) medium
 (c) medium-replay
 (d) random
Hình 14: TD3BC-MCEP. Hàng đầu: halfcheetah. Hàng thứ hai hopper. Hàng thứ ba: walker2d.

(a) medium-expert
 (b) medium
 (c) medium-replay
 (d) random
Hình 15: AWAC-MCEP. Hàng đầu: halfcheetah. Hàng thứ hai: hopper. Hàng thứ ba: walker2d.
