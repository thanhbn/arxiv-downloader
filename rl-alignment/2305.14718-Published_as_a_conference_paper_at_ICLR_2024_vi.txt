# 2305.14718.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2305.14718.pdf
# Kích thước file: 3585130 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
LEFTOVER LUNCH: HỌC TĂNG CƯỜNG NGOẠI TUYẾN DỰA TRÊN LỢI THẾ CHO CÁC MÔ HÌNH NGÔN NGỮ
Ashutosh Baheti♢,♣,∗, Ximing Lu♡,♣,, Faeze Brahman♣, Ronan Le Bras♣,
Maarten Sap♠,♣, Mark Riedl♢
♢Viện Công nghệ Georgia,♠Đại học Carnegie Mellon,
♡Đại học Washington,♣Viện Allen về Trí tuệ Nhân tạo
∗abaheti95@gatech.edu

TÓM TẮT
Học Tăng cường với Phản hồi của Con người (RLHF) là phương pháp nổi bật nhất để căn chỉnh Mô hình Ngôn ngữ (LM). Tuy nhiên, RLHF là một quá trình không ổn định và tốn nhiều dữ liệu, liên tục yêu cầu dữ liệu mới chất lượng cao được tạo bởi LM để tinh chỉnh. Chúng tôi giới thiệu Advantage-Leftover Lunch RL (A-LOL), một lớp thuật toán gradient chính sách ngoại tuyến mới cho phép huấn luyện RL trên bất kỳ dữ liệu có sẵn nào. Bằng cách giả định toàn bộ chuỗi đầu ra LM là một hành động duy nhất, A-LOL cho phép kết hợp các bộ phân loại cấp chuỗi hoặc các hàm tính điểm do con người thiết kế làm phần thưởng. Tiếp theo, bằng cách sử dụng ước tính giá trị của LM, A-LOL chỉ huấn luyện trên các điểm dữ liệu có lợi thế tích cực (thừa), làm cho nó có khả năng chống chịu với nhiễu. Nhìn chung, A-LOL là một công thức huấn luyện LM dễ triển khai, hiệu quả về mẫu và ổn định. Chúng tôi chứng minh hiệu quả của A-LOL và các biến thể của nó với một tập hợp bốn nhiệm vụ tạo ngôn ngữ khác nhau. Chúng tôi so sánh với cả RL trực tuyến (PPO) và các đường cơ sở RL ngoại tuyến dựa trên ưu tiên gần đây (DPO, PRO) và dựa trên phần thưởng (GOLD). Trên điểm chuẩn RLHF thường được sử dụng, Helpful and Harmless Assistant (HHA), các LM được huấn luyện với các phương pháp A-LOL đạt được sự đa dạng cao nhất đồng thời cũng được đánh giá là an toàn và hữu ích hơn so với các đường cơ sở theo con người. Ngoài ra, trong ba nhiệm vụ còn lại, A-LOL có thể tối ưu hóa nhiều hàm phần thưởng riêng biệt ngay cả khi sử dụng dữ liệu huấn luyện nhiễu hoặc không tối ưu.

1 GIỚI THIỆU
Các Mô hình Ngôn ngữ (LM) lớn được tiền huấn luyện (Radford et al., 2019; Brown et al., 2020) và/hoặc điều chỉnh theo hướng dẫn (Wei et al., 2022a; Chung et al., 2022; Wei et al., 2022b) cho thấy những cải thiện to lớn về chất lượng và an toàn khi được tinh chỉnh với Học Tăng cường với Phản hồi của Con người (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Touvron et al., 2023b). Tuy nhiên, phương pháp RLHF phổ biến nhất, Proximal Policy Optimization (PPO) (Schulman et al., 2017), nhạy cảm với các siêu tham số và gặp phải sự không ổn định trong huấn luyện (Yuan et al., 2023; Casper et al., 2023). Quan trọng hơn, PPO định kỳ yêu cầu các lô dữ liệu mới được tạo bởi LM cho mỗi bước huấn luyện dẫn đến chi phí tính toán bổ sung và nguy cơ sụp đổ chế độ (Song et al., 2023; Shumailov et al., 2023; Go et al., 2023). Với những hạn chế này, chúng tôi đặt câu hỏi: Liệu chúng ta có thể thực hiện học tập có phần thưởng, tương tự như PPO, trong khi chỉ sử dụng dữ liệu ngôn ngữ có sẵn trong quá trình huấn luyện không?

Chúng tôi đề xuất Advantage-Leftover Lunch RL (A-LOL), một tập hợp các thuật toán học tập hiệu quả về mẫu và ổn định sử dụng Gradient Chính sách Ngoại tuyến (Degris et al., 2012; Weng, 2018) để tối ưu hóa LM hướng tới bất kỳ phần thưởng mong muốn nào chỉ sử dụng dữ liệu ngôn ngữ được thu thập trước. Đáng chú ý trong A-LOL, chúng tôi giả định toàn bộ chuỗi đầu ra là một bước hành động duy nhất, điều này cho phép nó tính toán lợi thế dữ liệu huấn luyện và lọc các thể hiện không thuận lợi. Lợi thế là ước tính giá trị của LM tham chiếu được trừ khỏi phần thưởng, điều này xác định lợi ích của mỗi thể hiện huấn luyện đối với quá trình học tập. Tiếp theo, loại bỏ các điểm dữ liệu có lợi thế âm cải thiện hiệu quả học tập của A-LOL và làm cho nó bền vững với dữ liệu nhiễu.

A-LOL rất dễ triển khai trên mất mát entropy chéo tiêu chuẩn bằng cách sử dụng hai cải tiến chính: (1) lợi thế cấp chuỗi và (2) trọng số quan trọng (tỷ lệ xác suất của LM mục tiêu và LM tham chiếu ban đầu). Như được minh họa trong Bảng 1, phương pháp của chúng tôi chỉ yêu cầu phần thưởng cấp chuỗi với đầu ra duy nhất cho mỗi điểm dữ liệu, trái ngược với các phương pháp RL ngoại tuyến dựa trên ưu tiên gần đây (Rafailov et al., 2023; Song et al., 2024) yêu cầu so sánh cặp được gắn nhãn bởi con người. Quan trọng là, A-LOL

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 1: Thuộc tính của các thuật toán RL ngoại tuyến và trực tuyến hiện có so sánh với A-LOL và các biến thể của nó.

| Thuật toán | Cần Dữ liệu Ưu tiên Con người? | Biểu diễn Hành động | LM Tham chiếu | Phần thưởng | Lợi thế |
|------------|--------------------------------|-------------------|---------------|-------------|---------|
| NLL (negative log-likelihood) | Không | N/A | ✗ | ✗ | ✗ |
| **RL ngoại tuyến dựa trên ưu tiên** |
| DPO Rafailov et al. (2023) | Có | Chuỗi | ✓ | ✗ | ✗ |
| DPO (ref. free) Rafailov et al. (2023) | Có | Chuỗi | ✗ | ✗ | ✗ |
| PRO Song et al. (2024) | Có | Chuỗi | ✗ | ✓ | ✗ |
| **RL ngoại tuyến dựa trên phần thưởng và lợi thế** |
| wBC Wang et al. (2020) | Không | Tok./Seq. | ✗ | ✓ | ✗ |
| GOLD Pang & He (2021) | Không | Token | ✗ | ✓ | ✗ |
| A-LOL (của chúng tôi) | Không | Chuỗi | ✓ | ✓ | ✓ |
| ▷R-LOL (biến thể) | Không | Chuỗi | ✓ | ✓ | ✗ |
| ▷A-LOL (ref. free) (biến thể) | Không | Chuỗi | ✗ | ✓ | ✓ |
| ▷A-LOL seq. (biến thể) | Không | Chuỗi | ✓ | ✓ | ✓ |
| ▷A-LOL KL (biến thể) | Không | Chuỗi | ✓ | ✓ | ✓ |
| **Học Tăng cường Trực tuyến với Phản hồi Con người** |
| PPO Schulman et al. (2017) | Không | Tok./Seq. | ✓ | ✓ | ✓ |

và các biến thể của nó chia sẻ nhiều điểm tương đồng nhất với PPO, trong khi đơn giản hóa đáng kể việc huấn luyện và cũng cho phép học ngoại tuyến.

Thông qua một loạt bốn nhiệm vụ tạo ngôn ngữ khác nhau, mỗi nhiệm vụ sử dụng một hoặc nhiều bộ phân loại để tính toán phần thưởng, chúng tôi cho thấy rằng A-LOL liên tục vượt trội so với các đường cơ sở trong khi sử dụng lượng dữ liệu huấn luyện ít nhất. Chúng tôi đầu tiên thử nghiệm với nhiệm vụ điểm chuẩn RLHF, Helpful and Harmless Assistant (HHA) (Bai et al., 2022a; Ganguli et al., 2022) (§4), nơi có cả dữ liệu ưu tiên được gắn nhãn bởi con người và mô hình phần thưởng. Chúng tôi so sánh có hệ thống tất cả các thuật toán RL ngoại tuyến sử dụng cùng kiến trúc mô hình cơ sở 7B và cho thấy xu hướng ổn định huấn luyện qua nhiều hạt giống ngẫu nhiên. Chúng tôi thấy rằng các biến thể A-LOL đạt được phần thưởng trung bình tương đương với DPO trong khi cung cấp học tập ổn định hơn, phương sai thấp hơn và đa dạng phản hồi cao hơn so với mọi đường cơ sở khác. Trong đánh giá định tính hơn, con người đánh giá các mô hình A-LOL là hữu ích và an toàn nhất. Trong một thử nghiệm phần thưởng đơn khác với nhiệm vụ Commonsense Reasoning (West et al., 2022) (Phụ lục §C.1), A-LOL một lần nữa cho thấy cải thiện chất lượng cao nhất trong số các đường cơ sở.

Chúng tôi cũng chứng minh tính linh hoạt của A-LOL để sử dụng nhiều phần thưởng trong huấn luyện RL, điều này trái ngược với các phương pháp dựa trên ưu tiên chỉ có thể hỗ trợ ưu tiên một chiều. Cụ thể, chúng tôi thử nghiệm với hai nhiệm vụ đối thoại đa phần thưởng, tạo phản hồi Reddit (§5), và Đối thoại dựa trên kiến thức Trung thực (Dinan et al., 2019) (Phụ lục §C.2). Trong cả hai nhiệm vụ, A-LOL có thể tối ưu hóa đồng thời bốn hoặc nhiều hàm phần thưởng khác nhau cải thiện tính trôi chảy, an toàn, đa dạng và các thuộc tính định tính khác của LM, ngay cả khi có dữ liệu huấn luyện nhiễu. Những phát hiện của chúng tôi chứng minh rằng A-LOL là một phương pháp RL ngoại tuyến mạnh mẽ, ổn định, hiệu quả về mẫu cho việc học mô hình ngôn ngữ có thể dễ dàng thay thế bằng mất mát entropy chéo trong các nhiệm vụ mà phần thưởng giá trị thực có sẵn. Chúng tôi phát hành mã nguồn tại https://github.com/abaheti95/LoL-RL.

2 ADVANTAGE-LEFTOVER LUNCH RL

Trước khi giới thiệu phương pháp chính của chúng tôi, chúng tôi đầu tiên giải thích ngắn gọn cách chúng tôi đóng khung các nhiệm vụ tạo ngôn ngữ như một trò chơi RL với giả định hành động đơn (§2.1). Sau đó chúng tôi dẫn xuất mục tiêu học tập chính của A-LOL sử dụng gradient chính sách ngoại tuyến (§2.2). Để định vị A-LOL tốt hơn trong bối cảnh, chúng tôi cũng thảo luận mối quan hệ của nó với mất mát negative log-likelihood, weighted Behavior Cloning (Wang et al., 2020) (§2.3) và một thuật toán gradient chính sách ngoại tuyến khác GOLD (Pang & He, 2021) (§2.4).

2.1 CÁC NHIỆM VỤ NGÔN NGỮ NHƯ RL VỚI CÁC EPISODE HÀNH ĐỘNG ĐỢN

Chúng tôi xem xét việc tạo ngôn ngữ như một nhiệm vụ chuỗi-tới-chuỗi chứa các tập huấn luyện Dtr và xác thực Dv với các cặp chuỗi đầu vào x và đầu ra y. Trái ngược với các phương pháp RL trước đây xem xét mỗi token trong y như một hành động riêng biệt (Pang & He, 2021; Kim et al., 2022; Snell et al., 2023), chúng tôi xem xét toàn bộ y như một hành động duy nhất từ tác nhân LM, sau đó tác nhân nhận được phần thưởng cấp chuỗi R(x,y,⋆) cụ thể theo nhiệm vụ và episode kết thúc. Giả định hành động đơn cho phép kết hợp bất kỳ bộ phân loại thuộc tính cụ thể được tiền huấn luyện hoặc hàm tính điểm do con người thiết kế nào làm phần thưởng trong quá trình tinh chỉnh ngoại tuyến. Khi có nhiều hàm tính điểm, chúng tôi đặt phần thưởng là tổng của tất cả các hàm riêng lẻ.

2.2 GRADIENT CHÍNH SÁCH NGOẠI TUYẾN ĐẾN ADVANTAGE LOL RL

Để dẫn xuất phương trình học tập chính của chúng tôi, chúng tôi bắt đầu với mục tiêu gradient chính sách off-policy (Degris et al., 2012; Weng, 2018). Gọi πref là chính sách tham chiếu LM được huấn luyện trên Dtr với mất mát negative likelihood tiêu chuẩn (NLL) và πθ là chính sách mục tiêu chúng ta muốn tối ưu hóa, ban đầu giống hệt với πref. Cả πref và πθ đều nhận chuỗi đầu vào x (trạng thái) và tạo ra chuỗi đầu ra y (hành động). Sử dụng giả định episode hành động đơn, chúng ta có thể viết phân phối dừng của chính sách tham chiếu là dπref(x) = P(x|πref) = P(x), trong đó x thuộc về tập hợp tất cả các chuỗi đầu vào trong Dtr. Sau đó chúng ta có thể tối ưu hóa chính sách mục tiêu πθ trên phân phối dừng này dπref với mục tiêu sau:

J(θ) = max θ ∑x∈X dπref(x) ∑y∈Y R(x,y,⋆)πθ(y|x)     (1)

trong đó Y là tập hợp tất cả các đầu ra. Lấy đạo hàm của phương trình trên theo θ cho:

∇θJ(θ) = ∇θEx∼dπref[∑y∈Y R(x,y,⋆)πθ(y|x)] = Ex∼dπref[∑y∈Y R(x,y,⋆)∇θπθ(y|x)]     (2)

Sau đó chúng tôi nhân và chia cho πθ(y|x) và πref(y|x) và đơn giản hóa thêm phương trình như sau:

∇θJ(θ) = Ex∼dπref,y∼πref[R(x,y,⋆)|{z}phần thưởng × πθ(y|x)/πref(y|x)|{z}trọng số quan trọng × ∇θlnπθ(y|x)|{z}NLL]     (3)

Ở đây, trọng số quan trọng là tỷ lệ xác suất cấp chuỗi của y giữa πθ và πref, dẫn đến một yếu tố vô hướng duy nhất. Quan sát rằng các đầu vào của Dtr có trong dπref. Ngoài ra, các đầu ra thật trong Dtr là các đầu ra mà πref được huấn luyện để bắt chước. Sử dụng những quan sát này, chúng tôi xấp xỉ kỳ vọng trong phương trình trước và thu được mục tiêu Reward LOL RL (với dấu âm để hiển thị tối thiểu hóa):

∇θJR-LOL(θ) = -EDtr[R(x,y,⋆) · r(θ,ref) · ∇θlnπθ(y|x)]     (4)

trong đó r(θ,ref) = πθ(y|x)/πref(y|x) là cách viết tắt cho trọng số quan trọng.

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Hình 1: Minh họa Advantage-Leftover Lunch RL trong thực tế. Chúng tôi đầu tiên tinh chỉnh có giám sát chính sách tham chiếu (πref) trên dữ liệu huấn luyện như một tiền đề cho huấn luyện A-LOL. Sau đó, một mô hình phần thưởng bên ngoài được sử dụng để huấn luyện lớp ước tính giá trị (Vπref) trên πref bị đóng băng. Tiếp theo, sử dụng các giá trị chính sách tham chiếu trên Dtr, chúng ta có thể tìm các thể hiện có lợi thế tích cực. A-LOL sau đó nhân lợi thế tích cực và trọng số quan trọng với negative log likelihood để huấn luyện LM mục tiêu (πθ). Đánh giá trên Dtest cho thấy LM được huấn luyện với A-LOL đạt được phần thưởng trung bình cao hơn và phân phối tốt hơn so với chính sách tham chiếu.

Để tăng hiệu quả học tập, chúng ta có thể thay thế R(x,y,⋆) trong phương trình 4 bằng lợi thế, được định nghĩa là Aπθ(x,y,R) = R(x,y,⋆) - Vπθ(x), tức là ước tính phần thưởng kỳ vọng của chính sách cho đầu vào được trừ khỏi phần thưởng thực tế của dữ liệu huấn luyện (Schulman et al., 2016). Tuy nhiên, duy trì ước tính giá trị mới nhất của πθ tốn kém về chi phí, vì nó được cập nhật liên tục trong quá trình huấn luyện. Do đó, chúng tôi hoán đổi phần thưởng trong phương trình 4 với lợi thế của chính sách tham chiếu bị đóng băng, Aπref(x,y,R) = R(x,y,⋆) - Vπref(x). Chúng tôi gọi đây là mục tiêu Advantage LOL RL.

∇θJA-LOL(θ) = -EDtr[Aπref(x,y,R) · r(θ,ref) · ∇θlnπθ(y|x)]     (5)

Để tính toán ước tính giá trị của πref, chúng tôi khởi tạo một mạng nhỏ gồm multi-head attention (Vaswani et al., 2017) và một MLP một lớp trên các tham số bị đóng băng của πref. Mô-đun ước tính giá trị này nhận biểu diễn lớp ẩn cuối cùng của πref(x) và dự đoán phần thưởng tương lai kỳ vọng Vπref(x). Chúng tôi huấn luyện ước tính giá trị này một cách rẻ tiền trên các phần thưởng đạt được bởi πref trên tập xác thực (Dv) với mất mát lỗi bình phương trung bình. Sau đó chúng tôi tính toán Aπref(x,y,R) cho tất cả các thể hiện trong Dtr. Hình 1 minh họa một ví dụ về cách A-LOL cải thiện phân phối phần thưởng kiểm tra bằng cách sử dụng ước tính giá trị của chính sách tham chiếu. Tiếp theo, chúng tôi mô tả một số biến thể khác của thuật toán A-LOL.

**Các biến thể với Trọng số Quan trọng thay thế** Khai thác tính linh hoạt được cung cấp bởi trọng số quan trọng trong A-LOL, chúng tôi thử nghiệm với ba lựa chọn thay thế. Đầu tiên, chúng tôi tạo A-LOL(ref. free) bằng cách đặt trọng số quan trọng bằng 1. Trong biến thể thứ hai, chúng tôi chuyển đổi trọng số quan trọng toàn chuỗi trong A-LOL (phương trình 5) thành trọng số quan trọng theo token. Cụ thể, chúng tôi đề xuất một trọng số quan trọng xấp xỉ được nhân với log-likelihood sử dụng quy tắc chuỗi xác suất như sau: πθ(y|x)/πref(y|x) ∇θlnπθ(y|x) ≈ ∏|y|i=1[πθ(yi|x,y<i)/πref(yi|x,y<i) ∇θlnπθ(yi|x,y<i)], trong đó yi là token thứ i trong y và y<i là các token đứng trước. Chúng tôi đặt tên biến thể này là A-LOL sequence. Cuối cùng, lấy cảm hứng từ các ablation của PPO (Schulman et al., 2017), chúng tôi thử nghiệm với việc thay thế trọng số quan trọng bằng penalty KL có trọng số để có được A-LOL KL:

∇θJA-LOL KL(θ) = -EDtr[Aπref(x,y,R) · ∇θlnπθ(y|x) - β · ∇θlnπθ(y|x)/πref(y|x)]     (6)

Chúng tôi đề xuất hai sửa đổi nữa trong huấn luyện A-LOL để cải thiện tính ổn định và hiệu quả của nó.

**Cắt Trọng số Quan trọng** Việc sử dụng trực tiếp mục tiêu A-LOL (Phương trình 5) trong huấn luyện không ổn định vì các giá trị mất mát có thể dao động mạnh tùy thuộc vào trọng số quan trọng r(θ,ref). Để giảm thiểu vấn đề này, chúng tôi cắt trọng số quan trọng là clip(r(θ,ref), 1-ε, 1+ε) (Schulman et al., 2017). Toán tử clip này ngăn cản những thay đổi lớn khỏi chính sách tham chiếu. Trong A-LOL sequence, chúng tôi áp dụng toán tử clip riêng biệt cho trọng số quan trọng của mỗi token trong đầu ra.

**Lấy mẫu Ưu tiên Phần thưởng/Lợi thế** Trong tất cả các thử nghiệm, chúng tôi thấy rằng một lượng không tầm thường các điểm dữ liệu trong Dtr có được lợi thế âm (Aπref < 0). Chúng tôi loại bỏ những điểm dữ liệu này vì chúng có thể không giúp ích trong việc tổng quát hóa vượt ra ngoài πref. Để tăng cường hiệu quả huấn luyện của A-LOL hơn nữa, chúng tôi sử dụng lấy mẫu có trọng số dựa trên lợi thế tích cực của các thể hiện huấn luyện (tương tự như Welleck et al., 2022). Chúng tôi trình bày mã giả đầy đủ cho A-LOL trong Thuật toán 1. Đối với các phương pháp RL ngoại tuyến dựa trên phần thưởng, chúng tôi tương tự sử dụng lấy mẫu ưu tiên dựa trên phần thưởng trong tất cả các thử nghiệm.

Nhìn chung, A-LOL và các biến thể của nó hiệu quả và dễ triển khai trên negative log-likelihood tiêu chuẩn vì nó chỉ liên quan đến việc nhân hai yếu tố: lợi thế/phần thưởng và trọng số quan trọng. Hơn nữa, lấy mẫu ưu tiên lợi thế tích cực làm cho việc huấn luyện A-LOL rất hiệu quả, đôi khi đạt gần đến đỉnh tổng quát hóa chỉ với 30% bước bổ sung (xem Hình 2).

2.3 MỐI QUAN HỆ VỚI NLL VÀ WEIGHTED BEHAVIOR CLONING

Chúng tôi rút ra các kết nối giữa Reward LOL RL và các phương pháp học tập khác. Nếu chúng ta đặt cả R(x,y,⋆) = 1 và r(θ,ref) = 1 trong phương trình 4, nó giảm xuống thành mục tiêu negative log-likelihood. Điều này ngụ ý rằng học tập likelihood tối đa là một tập con của mục tiêu R-LOL. Bằng cách điều chỉnh cẩn thận thuật ngữ R(x,y,⋆) trong khi giữ r(θ,ref) = 1, cả lọc dữ liệu (West et al., 2022) và weighted behavior cloning (Wang et al., 2020) cũng có thể được xem như các tập con của phương pháp chúng tôi.

2.4 SO SÁNH VỚI GOLD

Trước đây, Pang & He (2021) đã phát triển thuật toán GOLD sử dụng một dẫn xuất gradient chính sách ngoại tuyến tương tự, nhưng không có xấp xỉ hành động đơn. So với mục tiêu R-LOL (phương trình 4), mục tiêu GOLD có hai điểm khác biệt đặc biệt: (1) nó xấp xỉ trọng số quan trọng bằng cách sử dụng một hằng số thay vì xác suất chính sách tham chiếu, và (2) nó sử dụng log-xác suất theo token của chính sách tham chiếu làm phần thưởng cấp token. Theo trực giác, phương pháp này "khuyến khích thuật toán học tập tập trung vào các ví dụ dễ (likelihood cao dưới mô hình)" (Pang & He, 2021). Tuy nhiên, nó không thể đơn giản bao gồm phần thưởng thưa thớt tùy ý như R-LOL. Để so sánh, chúng tôi sử dụng giả định hành động đơn và thay thế phần thưởng theo token bằng phần thưởng cấp chuỗi để có được mục tiêu Reward GOLD, -EDtr[R(x,y,⋆)πθ(y|x)∇θlnπθ(y|x)], trong đó chúng tôi xấp xỉ trọng số quan trọng và NLL của nó là ∏|y|i=1max(πθ(yi|x,y<i), u)∇θlnπθ(yi|x,y<i) với giới hạn dưới u cho tính ổn định.

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Thuật toán 1: Mã giả Advantage-Leftover Lunch RL
Dữ liệu: tập huấn luyện và xác thực (x,y∈Dtr, Dv), chính sách tham chiếu (πref), chính sách mục tiêu (πθ), phần thưởng nhiệm vụ (R(x,y,⋆)), tham số cắt (ε)
Kết quả: arg maxπθ∼A-LOL ∑x∈Dv,y'∼πθ R(x,y',⋆) ▷Tối đa hóa phần thưởng trên Dv

1 V←mlp(mha(πref),1) ▷lớp giá trị với multi-head attention (mha) trên πref bị đóng băng
2 Vπref←min ∑x∈Dv,y'∼πref(V-R(x,y',⋆))² ▷Huấn luyện ước tính giá trị πref sử dụng phần thưởng trên Dv
3 A⁺πref← {Aπref(x,y,R)} ∀x,y∈Dtr, R(x,y,⋆)-Vπref(x)>0 ▷Lợi thế Tích cực trên Dtr
4 while ∑x∈Dv,y'∼πθ R(x,y',⋆) chưa hội tụ do
5    r(θ,ref)←clip(πθ(y|x)/πref(y|x), 1-ε, 1+ε)
6    ∇θJ(θ)← -EA⁺πref[Aπref(x,y,R)·r(θ,ref)·∇θlnπθ(y|x)] ▷Lấy mẫu sử dụng trọng số A⁺πref
end

3 THIẾT LẬP THỬ NGHIỆM VÀ CÁC ĐƯỜNG CƠ SỞ

Chúng tôi tiến hành thử nghiệm với bốn nhiệm vụ tạo ngôn ngữ khác nhau: hai nhiệm vụ phần thưởng đơn (Helpful and Harmless Assistant, Mục §4 và Commonsense Reasoning, Phụ lục §C.1) và hai nhiệm vụ đa phần thưởng (tạo phản hồi Reddit, Mục §5 và Knowledge Grounded Dialog, Phụ lục §C.2). Trong mỗi thử nghiệm, một LM tham chiếu được thu được, hoạt động như điểm khởi đầu cho tất cả các phương pháp học tập. Chúng tôi tiếp tục tinh chỉnh với các phương pháp khác nhau trong số bước gần như bằng nhau (tùy thuộc vào kích thước Dtr). Nhìn chung, chúng tôi so sánh A-LOL và các biến thể trọng số quan trọng được sửa đổi của nó (A-LOL(ref. free), A-LOL seq., và A-LOL KL) với negative log-likelihood (NLL) và các đường cơ sở RL ngoại tuyến sau:

**Đường cơ sở dựa trên Ưu tiên** Chúng tôi thử nghiệm với ba thuật toán RL ngoại tuyến trực tiếp sử dụng dữ liệu ưu tiên được gắn nhãn bởi con người để giải quyết nhiệm vụ RLHF. DPO (Rafailov et al., 2023) chuyển đổi tối ưu hóa phần thưởng có ràng buộc thành mất mát phân loại ưu tiên bằng cách định nghĩa phần thưởng thay thế là tỷ lệ log-xác suất chính sách mục tiêu và chính sách tham chiếu. Cho mục đích ablation, chúng tôi cũng kiểm tra DPO (ref. free), một biến thể của DPO không có log-xác suất chính sách tham chiếu. Công trình tiếp theo giới thiệu PRO (Song et al., 2024) mở rộng mất mát phân loại của DPO thành mất mát xếp hạng và nội suy nó với negative log-likelihood cho tính ổn định. Chúng tôi không thể so sánh với các phương pháp dựa trên ưu tiên trong các nhiệm vụ có nhiều phần thưởng hoặc nơi ưu tiên được gắn nhãn bởi con người không có sẵn.

**Đường cơ sở dựa trên Phần thưởng** Chúng tôi cũng so sánh với R-LOL (Phương trình 4) và các phương pháp RL ngoại tuyến dựa trên phần thưởng liên quan khác: wBC (§2.3) và Reward GOLD (§2.4).

4 HHA: NHIỆM VỤ HELPFUL AND HARMLESS ASSISTANT

Thử nghiệm chính của chúng tôi sử dụng tập dữ liệu Helpful and Harmless assistant (Bai et al., 2022a; Ganguli et al., 2022) chứa 170K thể hiện cuộc hội thoại người dùng-trợ lý mỗi thể hiện chứa một cặp phản hồi được tạo bởi mô hình. Các phản hồi cuối cùng được gắn nhãn tốt và xấu tương ứng để chỉ ra nhãn ưu tiên của con người. Tập dữ liệu bao gồm bốn tập con: Harmless base chứa các cuộc hội thoại red-teaming cố gắng gợi ra các phản hồi có hại, trong khi ba tập còn lại, Helpful base, Helpful online, và Helpful rejection, chứa các cuộc hội thoại tìm kiếm lời khuyên và hỗ trợ. Chúng tôi sử dụng lại các phân chia dữ liệu từ Song et al. (2024) với việc làm sạch dữ liệu nhỏ. Tổng cộng, nhiệm vụ này có 143K cuộc hội thoại huấn luyện, 280 xác thực, và 8.2K kiểm tra và các phản hồi được gắn nhãn ưu tiên của con người.

**LM Tham chiếu, Mô hình Phần thưởng và Huấn luyện** Chúng tôi chọn kiến trúc cơ sở LLaMA-7B (Touvron et al., 2023a) với adapter QLoRA (Dettmers et al., 2023) được tiền huấn luyện trên tập dữ liệu HHA làm chính sách tham chiếu. Chúng tôi cũng kiểm tra PPO trong thử nghiệm này, cùng với các đường cơ sở RL ngoại tuyến đã nêu và các biến thể A-LOL. Tổng cộng, chúng tôi thực hiện 36 lần chạy huấn luyện khác nhau cho 12 phương pháp học tập, mỗi phương pháp với ba hạt giống ngẫu nhiên. Đối với tất cả các thuật toán sử dụng phần thưởng, chúng tôi sử dụng một bộ phân loại 1.4B tham số được huấn luyện trên nhãn ưu tiên của con người làm mô hình phần thưởng.

Trong khi các phương pháp RL dựa trên ưu tiên sử dụng tất cả các so sánh cặp huấn luyện, các phương pháp khác chỉ sử dụng tập con phản hồi tốt trong quá trình huấn luyện. Thực tế, các phương pháp A-LOL hiệu quả nhất về dữ liệu, bằng cách bỏ qua ≈33% phản hồi tốt được xác định là lợi thế âm. Chúng tôi phân bổ khoảng một epoch bước huấn luyện cho mỗi phương pháp RL ngoại tuyến, tùy thuộc vào yêu cầu dữ liệu huấn luyện của chúng. Chúng tôi huấn luyện PPO trong 2.6× số bước huấn luyện của các phương pháp ngoại tuyến (loại trừ chi phí tính toán tạo dữ liệu trực tuyến). Cuối cùng, như một điểm chuẩn, chúng tôi cũng đánh giá một mô hình 6B bên ngoài được huấn luyện với PPO trên tập dữ liệu HHA. Chúng tôi trình bày chi tiết triển khai của tất cả các phương pháp được kiểm tra trong Phụ lục B.1. Chúng tôi tiến hành phân tích ablation bổ sung về các sửa đổi thuật toán của A-LOL trong Phụ lục B.3 đến B.5.

4.1 KẾT QUẢ HHA

**So sánh Tính ổn định của các thuật toán RL Ngoại tuyến** Hình 2 cho thấy quỹ đạo phần thưởng xác thực đạt được bởi tất cả các phương pháp RL ngoại tuyến được tính trung bình qua ba hạt giống ngẫu nhiên. Biểu đồ bên trái cho thấy các phương pháp dựa trên ưu tiên, đặc biệt là DPO và DPO (ref. free), gặp phải phương sai cao qua các hạt giống ngẫu nhiên so với huấn luyện NLL. Ngược lại, các phương pháp dựa trên phần thưởng có phương sai tương đương hoặc thậm chí thấp hơn NLL (giữa). Trong biểu đồ bên phải, chúng ta quan sát rằng các phương pháp A-LOL cũng cho thấy tính ổn định tương tự như các phương pháp dựa trên phần thưởng, trong khi đạt được phần thưởng cao hơn.

Trong số các phương pháp dựa trên lợi thế của chúng tôi, A-LOL(ref. free) đạt được hiệu suất xác thực thấp hơn so với các biến thể khác. A-LOL KL, mặt khác, có thể trở nên không ổn định bằng cách tối thiểu hóa thuật ngữ penalty KL thay vì mất mát policy-gradient, như được chứng minh bởi sự sụt giảm hiệu suất về cuối quá trình huấn luyện. So sánh, A-LOL và A-LOL seq. cải thiện đều đặn trong suốt quá trình huấn luyện. Chúng tôi cũng vẽ riêng ba quỹ đạo của PPO trong Hình 3 của Phụ lục.

**Đánh giá Tự động và Phân tích** Chúng tôi sử dụng mô hình phần thưởng 6.5B tham số lớn hơn để đánh giá các checkpoint tốt nhất từ mỗi lần chạy trên tập kiểm tra. Chúng tôi cũng so sánh phân phối phản hồi sử dụng độ dài trung bình và các biện pháp đa dạng (Distinct-1, 2, và 3), được tính như tỷ lệ các unigram, bigram, và trigram duy nhất (Li et al., 2016). Các metric đánh giá trung bình thu được bởi mỗi phương pháp qua ba hạt giống ngẫu nhiên được báo cáo trong Bảng 2. Trong ba hàng đầu tiên, chúng tôi thiết lập hiệu suất phản hồi tốt và xấu của tập kiểm tra và chính sách tham chiếu (πref).

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Hình 2: Xu hướng xác thực HHA của các thuật toán RL ngoại tuyến dựa trên ưu tiên (trái), phần thưởng (giữa), và lợi thế (phải) so với huấn luyện negative log-likelihood (NLL) qua ba hạt giống ngẫu nhiên.

Nhìn chung, A-LOL và các biến thể của nó liên tục đạt được phần thưởng trung bình cao với phương sai tương đối thấp, ngay cả sau khi loại bỏ ≈33% điểm dữ liệu huấn luyện. Phân tích định tính thêm cho thấy các thể hiện lợi thế âm thấp nhất thường chỉ ra dữ liệu chất lượng xấu (Phụ lục B.4). Các phương pháp A-LOL hoạt động tương đương với DPO và vượt trội so với tất cả các đường cơ sở dựa trên ưu tiên và phần thưởng khác trong khi tạo ra các phản hồi đa dạng nhất. Thú vị là, A-LOL seq. (sử dụng trọng số quan trọng theo token) đạt được sự đa dạng tốt nhất trong tất cả các mô hình, căn chỉnh phân phối của nó gần nhất với phản hồi kiểm tra. Đối với các đường cơ sở dựa trên ưu tiên, chúng ta nhận thấy mối tương quan trực tiếp giữa phương sai cao của hiệu suất xác thực (Hình 2 trái) và phương sai cao của phần thưởng trung bình tập kiểm tra và độ dài phản hồi. Mặc dù có phần thưởng trung bình cao, DPO (và DPO ref. free) có xu hướng làm lệch phân phối phản hồi về phản hồi bất thường dài và ít đa dạng hơn. Cuối cùng, trong các thử nghiệm với PPO, các mô hình có xu hướng tạo ra phản hồi ngắn hơn nhiều trung bình bằng cách chủ yếu tập trung vào việc tạo phản hồi an toàn (Harmless base). Điều này nhấn mạnh rằng PPO yêu cầu một chính sách tham chiếu ban đầu tốt tạo ra dữ liệu khám phá chất lượng cao và một mô hình phần thưởng được hiệu chỉnh tốt, điều này không phải là hạn chế của các phương pháp RL ngoại tuyến. Đánh giá với các mô hình dựa trên PPO bên ngoài cũng không cho thấy hiệu suất mạnh.

**Đánh giá GPT-4 và Con người** Để điều tra thêm chất lượng của các phương pháp hiệu suất hàng đầu, chúng tôi tiến hành đánh giá GPT-4 (OpenAI, 2023) và con người bổ sung. Theo công trình trước (Rafailov et al., 2023; Song et al., 2024), chúng tôi thực hiện so sánh cặp giữa các phương pháp tốt nhất và phản hồi tốt kiểm tra để xác định tỷ lệ thắng về tính hữu ích và an toàn. Cụ thể, cho mỗi so sánh giữa hai phản hồi, chúng tôi yêu cầu GPT-4 và con người chọn từ bốn tùy chọn (A, B, hòa hoặc không ai) để chỉ ra phản hồi thắng. Chúng tôi yêu cầu chọn phản hồi an toàn hơn trong các thể hiện từ Harmless base và phản hồi hữu ích hơn cho ba phân đoạn kiểm tra khác. Tổng cộng, chúng tôi lấy mẫu 400 thể hiện cho đánh giá GPT-4 và 200 thể hiện cho đánh giá con người (kích thước bằng nhau từ 4 phân đoạn kiểm tra). Để giảm thiểu bias vị trí trong GPT-4 (Zheng et al., 2023; Wang et al., 2023), chúng tôi truy vấn nó hai lần (xáo trộn thứ tự phản hồi) và chỉ tổng hợp các đánh giá khi nó chọn cùng ưu tiên. Trong đánh giá con người, chúng tôi yêu cầu ba người chú thích đánh giá mỗi so sánh cặp và tổng hợp các đánh giá nếu đạt được đa số. Kết quả cuối cùng từ cả hai đánh giá được trình bày trong Bảng 3.

Để thiết lập độ tin cậy đánh giá GPT-4, chúng tôi đầu tiên so sánh chính sách tham chiếu (πref) và phản hồi xấu Tập kiểm tra với phản hồi tốt Tập kiểm tra trong hai hàng đầu tiên. Trong cả hai so sánh, GPT-4 xem xét phản hồi tốt Tập kiểm tra hữu ích và an toàn hơn trong đa số mẫu. Nhìn chung, A-LOL và A-LOL seq. đạt được tỷ lệ thắng cao nhất trong cả an toàn và tính hữu ích (thắng + hòa), với A-LOL KL và DPO đi sau. Con người chọn nhiều thể hiện là hòa hơn GPT-4, nhưng chúng ta lại nhận thấy xu hướng tỷ lệ thắng tương tự với các phương pháp A-LOL dẫn đầu trong cả tính hữu ích và an toàn. Đối với tất cả các thể hiện trong đánh giá con người có nhãn đa số, chúng tôi so sánh với nhãn ưu tiên tương ứng của GPT-4 và thấy 72.1% thỏa thuận. Chúng tôi trình bày một vài ví dụ cuộc hội thoại từ tất cả các mô hình hàng đầu trong Bảng 9 đến 13 trong Phụ lục.

5 NHIỆM VỤ TẠO PHẢN HỒI REDDIT

Dữ liệu ưu tiên con người được hỗ trợ bởi rất ít nhiệm vụ tạo ngôn ngữ. Việc chú thích của chúng cũng khó khăn và tốn kém. Hơn nữa, ưu tiên vốn dĩ là một chiều và không thể được mở rộng một cách tầm thường cho các nhiệm vụ mà nhiều hơn một khía cạnh là quan trọng (Rafailov et al., 2023; Song et al., 2024). Ngược lại, các phương pháp dựa trên policy-gradient có thể sử dụng nhiều hàm phần thưởng trong quá trình huấn luyện RL mà không cần dữ liệu ưu tiên.

Để kiểm tra khả năng tổng quát hóa đa phần thưởng của A-LOL, chúng tôi tạo ra một nhiệm vụ tạo phản hồi Reddit mới với hỗn hợp năm hàm phần thưởng. Nhiệm vụ là học một chatbot từ các cặp bình luận-phản hồi Reddit sao cho trôi chảy, an toàn, hấp dẫn, thú vị và giống con người (See et al., 2019). Do đó, chúng tôi định nghĩa phần thưởng nhiệm vụ là tổng của năm hàm tính điểm: (1) bộ phân loại tính trôi chảy CoLA, (2) bộ phân loại an toàn ngữ cảnh ToxiChat (Baheti et al., 2021), (3) bộ phân loại tham gia đối thoại (Gao et al., 2020), (4) mô hình xếp hạng xác suất upvote Reddit (Gao et al., 2020), và (5) đa dạng TF-IDF có penalty độ dài. Phạm vi của mỗi hàm tính điểm là [0,1].

Để kiểm tra khả năng chống chịu với nhiễu, chúng tôi tạo ra hai tập dữ liệu huấn luyện khác nhau cho nhiệm vụ này: (1) 88K cặp bình luận Reddit được upvote (điểm ∈[66,9582]), phản ánh dữ liệu chất lượng tốt, và (2) 87K cặp bình luận Reddit được downvote (điểm ∈[-2946,-6]), dữ liệu chất lượng xấu. Đối với cả hai trường hợp thể hiện, chúng tôi tạo các tập xác thực và kiểm tra cân bằng, mỗi tập với 1000 cặp bình luận được upvote và 1000 được downvote. Chúng tôi sử dụng mô hình DialoGPT-medium (355M tham số) (Zhang et al., 2020) được huấn luyện bằng mục tiêu NLL làm chính sách tham chiếu.

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 2: Đánh giá tổng hợp phần thưởng và đa dạng tập kiểm tra HHA của NLL, PPO, và các phương pháp RL ngoại tuyến khác được tính trung bình qua ba hạt giống ngẫu nhiên. Để so sánh, chúng tôi cũng báo cáo hiệu suất của phản hồi Kiểm tra và chính sách tham chiếu (πref) trong ba hàng đầu tiên, cùng với một mô hình PPO* bên ngoài khác trong hàng cuối cùng. Nhìn chung, các phương pháp A-LOL tốt nhất đạt được phần thưởng tương đương với DPO, trong khi tiệm cận phân phối của phản hồi tốt Kiểm tra về độ dài và đa dạng. Thật kỳ lạ, DPO và DPO (ref. free) có xu hướng tạo ra phản hồi dài hơn và ít đa dạng hơn trong khi PPO tạo ra ngắn nhất.

| Thuật toán | Harmless base | Helpful base | Helpful online | Helpful rejection | Phần thưởng Trung bình | Độ dài Trung bình | Đa dạng: Distinct-1,2,3 Trung bình |
|------------|---------------|--------------|----------------|-------------------|----------------------|------------------|-----------------------------------|
| #thể hiện (2210) | (2278) | (1085) | (2634) | (8207) |
| Phản hồi tốt Kiểm tra | 54.8 | 40.3 | 61.6 | 50.5 | 50.3 | 46.7 | .099/.471/.773 |
| Phản hồi xấu Kiểm tra | 50.0 | 34.3 | 59.4 | 45.3 | 45.4 | 45.3 | .099/.468/.771 |
| πref (LLaMA-7B) | 54.8 | 36.5 | 49.4 | 41.5 | 44.7 | 51.1 | .067/.246/.404 |
| + MLE hoặc NLL | 60.7 | 44.0 | 56.5 | 49.5 | 51.9±0.5 | 43.3±3.5 | .084/.336/.552 |
| **RL ngoại tuyến dựa trên ưu tiên** |
| + PRO | 63.0 | 46.4 | 57.6 | 51.8 | 54.1±0.6 | 43.4±2.5 | .084/.339/.560 |
| + DPO (ref. free) | 53.6 | 50.1 | 54.4 | 52.3 | 52.3±1.4 | 90.5±1.1 | .049/.226/.432 |
| + DPO | 60.9 | 55.2 | 61.0 | 58.5 | 58.6±0.8 | 66.7±5.9 | .065/.288/.503 |
| **RL ngoại tuyến dựa trên phần thưởng** |
| + wBC | 62.4 | 47.0 | 59.7 | 53.2 | 54.8±0.4 | 42.7±1.1 | .091/.380/.622 |
| + R GOLD | 62.7 | 46.7 | 59.0 | 52.6 | 54.5±0.3 | 44.2±3.1 | .086/.355/.584 |
| + R-LOL | 63.7 | 47.0 | 59.4 | 53.1 | 55.1±0.6 | 38.6±2.4 | .095/.394/.640 |
| **RL ngoại tuyến dựa trên lợi thế** |
| + A-LOL (ref. free) | 63.8 | 49.2 | 60.7 | 54.7 | 56.4±0.6 | 40.7±1.7 | .095/.400/.651 |
| + A-LOL | 64.3 | 50.3 | 61.1 | 55.8 | 57.3±0.3 | 42.3±1.9 | .094/.403/.657 |
| + A-LOL seq. | 64.4 | 50.9 | 62.2 | 55.8 | 57.6±0.5 | 40.1±2.4 | .105/.464/.727 |
| + A-LOL KL | 65.7 | 50.7 | 61.1 | 56.2 | 57.9±0.3 | 44.0±1.7 | .090/.387/.639 |
| **RL Trực tuyến** |
| + PPO | 64.0 | 39.7 | 45.9 | 42.8 | 48.0±0.2 | 16.9±0.6 | .123/.381/.546 |
| PPO* (pythia 6B) | 48.6 | 32.1 | 33.6 | 33.3 | 37.1 | 13.3 | .094/.301/.447 |

Bảng 3: Tỷ lệ thắng an toàn và hữu ích của GPT-4 và Con người của đường cơ sở hiệu suất hàng đầu (DPO) và các biến thể A-LOL tốt nhất so với phản hồi tốt Kiểm tra. Để so sánh, chúng tôi cũng báo cáo tỷ lệ thắng GPT-4 của phản hồi xấu Kiểm tra và chính sách tham chiếu (πref) so với phản hồi tốt Kiểm tra trong hai hàng đầu tiên.

| Đường cơ sở hoặc Phương pháp | An toàn | Hữu ích |
|------------------------------|---------|---------|
| | #mẫu | thắng% | hòa% | thua% | không ai% | #mẫu | thắng% | hòa% | thua% | không ai% |
| **Tỷ lệ thắng đánh giá an toàn và tính hữu ích của GPT-4 so với phản hồi tốt Kiểm tra** |
| Phản hồi xấu Kiểm tra | 83 | 25.3 | 4.8 | 56.6 | 13.3 | 240 | 30.4 | 2.9 | 65.8 | 0.8 |
| πref (LLaMA-7B) | 81 | 35.8 | 6.2 | 53.1 | 4.9 | 267 | 24.3 | 3.0 | 71.9 | 0.7 |
| + DPO | 83 | 60.2 | 3.6 | 36.1 | 0.0 | 260 | 41.2 | 3.1 | 55.0 | 0.8 |
| + A-LOL | 76 | 68.4 | 9.2 | 17.1 | 5.3 | 249 | 53.8 | 1.2 | 45.0 | 0.0 |
| + A-LOL seq. | 82 | 73.2 | 7.3 | 17.1 | 2.4 | 247 | 54.7 | 2.0 | 42.9 | 0.4 |
| + A-LOL KL | 80 | 66.2 | 11.2 | 21.2 | 1.2 | 249 | 45.4 | 3.2 | 51.0 | 0.4 |
| **Tỷ lệ thắng đánh giá an toàn và tính hữu ích của Con người so với phản hồi tốt Kiểm tra** |
| + DPO | 43 | 53.5 | 4.7 | 30.2 | 11.6 | 138 | 47.8 | 7.2 | 42.8 | 2.2 |
| + A-LOL | 45 | 46.7 | 15.6 | 24.4 | 13.3 | 127 | 53.5 | 15.0 | 30.7 | 0.8 |
| + A-LOL seq. | 49 | 63.3 | 14.3 | 14.3 | 8.2 | 134 | 49.3 | 9.0 | 38.1 | 3.7 |
| + A-LOL KL | 43 | 53.5 | 9.3 | 23.3 | 14.0 | 137 | 48.9 | 11.7 | 34.3 | 5.1 |

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

trong 6 epoch làm chính sách tham chiếu. Sau đó chúng tôi thực hiện huấn luyện thêm trong 3 epoch với các biến thể A-LOL và các đường cơ sở RL ngoại tuyến dựa trên phần thưởng khác. Phần thưởng trung bình, độ dài và metric đa dạng được báo cáo trong Bảng 4.

**Kết quả** Trong cả hai phân chia huấn luyện upvote và downvote, các biến thể A-LOL đạt được phần thưởng kiểm tra cao hơn so với mọi đường cơ sở dựa trên phần thưởng khác. Chúng cho thấy cải thiện đặc biệt cao trong an toàn, tham gia và xác suất upvote. Trong khi hiệu suất của A-LOL tương đương với A-LOL(ref. free), hai biến thể khác, với trọng số quan trọng cấp chuỗi và penalty KL, vượt trội so với hiệu suất của chúng. Phù hợp với kết quả của thử nghiệm trước, chúng ta quan sát rằng A-LOL sequence (với giả định trọng số quan trọng theo token) đạt được đa dạng cao nhất trong cả hai phân chia huấn luyện. Thử nghiệm với PPO dẫn đến một chính sách tạo ra phản hồi chung chung, từ đó tối ưu hóa tính trôi chảy, an toàn và tfidf trong khi bỏ qua hai thành phần khác (chi tiết thêm trong Phụ lục C.3).

Đáng ngạc nhiên, các LM được huấn luyện trên dữ liệu downvote với A-LOL gần như thu hẹp khoảng cách với các đối tác được huấn luyện trên dữ liệu upvote. Khi kiểm tra kỹ hơn, chúng tôi thấy rằng khoảng 36% phản hồi upvote và 48% phản hồi downvote trong các tập huấn luyện tương ứng của chúng nhận được lợi thế âm và do đó, không bao giờ được lấy mẫu khi tinh chỉnh với A-LOL. Bằng cách lọc các điểm dữ liệu không thuận lợi, A-LOL trích xuất tín hiệu huấn luyện hữu ích ngay cả từ dữ liệu không tối ưu. Chúng tôi gọi phương pháp của mình là Leftover Lunch RL chính xác vì khả năng chống chịu với dữ liệu huấn luyện không thuận lợi. Chúng tôi hiển thị phân phối phần thưởng theo thành phần trong Hình 5 trong phụ lục.

6 KẾT LUẬN

Chúng tôi giới thiệu Advantage-Leftover Lunch RL, một tập hợp các thuật toán gradient chính sách ngoại tuyến dựa trên lợi thế dễ triển khai trên negative log-likelihood tiêu chuẩn và ổn định hơn so với RL ngoại tuyến dựa trên ưu tiên và PPO. Trên bốn nhiệm vụ khác nhau, A-LOL liên tục cho thấy hiệu suất tương tự hoặc tốt hơn so với các phương pháp RL ngoại tuyến dựa trên ưu tiên và phần thưởng khác. Đáng chú ý nhất, A-LOL khai thác ước tính lợi thế của LM tham chiếu để loại bỏ dữ liệu không thuận lợi. Khả năng độc đáo này của A-LOL làm cho nó có khả năng chống chịu với nhiễu và cho phép nó ăn bữa trưa thừa từ ngay cả dữ liệu huấn luyện không tối ưu.

Khai thác tính linh hoạt của importance weighting, chúng tôi tạo ra bốn biến thể của A-LOL đạt được hiệu suất hàng đầu trong hầu hết mọi đánh giá. Trong số đó, chúng tôi thấy rằng các phương pháp sử dụng trọng số quan trọng thường vượt trội so với biến thể không tham chiếu. Thực tế, sử dụng giả định trọng số quan trọng theo token, biến thể A-LOL sequence không chỉ cải thiện hiệu suất kiểm tra mà còn cả sự đa dạng.

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024

Bảng 4: Đánh giá tạo phản hồi Reddit trên năm thuộc tính đối thoại: Tính trôi chảy, An toàn, Tham gia, Xác suất nhận upvote, và đa dạng TF-IDF. Ngay cả khi huấn luyện trên các phản hồi được downvote, các biến thể A-LOL đạt được điểm trung bình cao trong tất cả các hàm phần thưởng và đạt gần nhất với hiệu suất của mô hình phản hồi upvote hàng đầu. Chúng tôi nhấn mạnh sự đa dạng thấp của đường cơ sở R GOLD. Chúng tôi không hiển thị kết quả kiểm tra cho các phương pháp mà việc tinh chỉnh thêm chính sách tham chiếu không tăng phần thưởng xác thực.

| Mô hình/Thuật toán | Phần thưởng | Trôi chảy | An toàn | Tham gia | X.suất Upvote | TF-IDF | Độ dài | Distinct-1/2/3 |
|-------------------|-------------|-----------|---------|----------|---------------|---------|---------|---------------|
| **Mô hình được huấn luyện trên tập huấn luyện phản hồi Reddit được upvote** |
| πref (DialoGPT) | 2.93 | .92 | .84 | .47 | .43 | .26 | 14.7 | .137/.409/.609 |
| + NLL | 2.95 | .92 | .84 | .48 | .44 | .25 | 15.2 | .143/.426/.626 |
| + wBC | 2.97 | .92 | .85 | .49 | .45 | .25 | 15.3 | .154/.448/.648 |
| + R GOLD | 3.03 | .94 | .89 | .50 | .44 | .26 | 13.9 | .120/.300/.412 |
| + R-LOL | 2.95 | .93 | .86 | .44 | .44 | .27 | 9.6 | .159/.423/.598 |
| + A-LOL (ref. free) | 3.15 | .95 | .90 | .55 | .48 | .27 | 14.1 | .146/.405/.587 |
| + A-LOL | 3.15 | .93 | .88 | .55 | .51 | .29 | 11.4 | .186/.502/.698 |
| + A-LOL seq | 3.28 | .93 | .88 | .62 | .57 | .27 | 15.9 | .191/.519/.707 |
| + A-LOL KL | 3.18 | .94 | .87 | .58 | .53 | .27 | 14.5 | .168/.467/.669 |
| **Mô hình được huấn luyện trên tập huấn luyện phản hồi Reddit được downvote** |
| πref (DialoGPT) | 2.87 | .91 | .81 | .49 | .39 | .27 | 13.6 | .128/.369/.548 |
| + wBC | 2.93 | .92 | .80 | .53 | .42 | .26 | 14.8 | .145/.422/.614 |
| + R GOLD | 3.05 | .94 | .87 | .52 | .42 | .29 | 11.0 | .123/.297/.401 |
| + R-LOL | 2.91 | .91 | .83 | .49 | .41 | .28 | 10.6 | .179/.488/.666 |
| + A-LOL (ref. free) | 3.13 | .94 | .87 | .56 | .47 | .29 | 11.3 | .165/.441/.622 |
| + A-LOL | 3.14 | .93 | .87 | .57 | .47 | .30 | 10.0 | .199/.517/.688 |
| + A-LOL seq | 3.18 | .93 | .89 | .58 | .48 | .30 | 10.2 | .207/.527/.713 |
| + A-LOL KL | 3.18 | .93 | .88 | .60 | .49 | .28 | 17.4 | .127/.333/.454 |

7 TUYÊN BỐ TÍNH TÁI TẠO

Trong các thử nghiệm chính của chúng tôi với nhiệm vụ HHA (§4), sử dụng các thử nghiệm LM 7B tham số lớn nhất, chúng tôi chạy mọi đường cơ sở và phương pháp A-LOL với ba hạt giống ngẫu nhiên cho tính tái tạo. Chúng tôi cũng cung cấp chi tiết triển khai của mọi phương pháp cùng với các siêu tham số trong Phụ lục §B.1. Trong các thử nghiệm đa phần thưởng khác, chúng tôi kiểm tra tất cả các phương pháp và đường cơ sở với cả thiết lập dữ liệu chất lượng tốt và chất lượng xấu. Chúng tôi cũng trình bày mã giả tổng quát của A-LOL trong Thuật toán 1. Cuối cùng, chúng tôi chia sẻ mã nguồn trên GitHub tại https://github.com/abaheti95/LoL-RL.

8 LỜI CẢM ƠN

Chúng tôi muốn cảm ơn Xuhui Zhou và Akhila Yerukola vì những gợi ý của họ về bản thảo trước đó của bài báo. Chúng tôi cũng cảm ơn các nhà đánh giá ẩn danh vì đã cung cấp phản hồi có giá trị để cải thiện chất lượng trình bày.

TÀI LIỆU THAM KHẢO

[Các tài liệu tham khảo được liệt kê từ trang 10-17, bao gồm đầy đủ các trích dẫn học thuật với tên tác giả, tiêu đề bài báo, tên hội nghị/tạp chí, năm xuất bản, và các thông tin bibliographic khác]

--- TRANG 18 ---
[Tiếp tục danh sách tài liệu tham khảo đến trang 31]

PHỤ LỤC

A MỐI QUAN HỆ CỦA A-LOL VỚI PPO

[Phần phụ lục tiếp tục với các phân tích kỹ thuật chi tiết, bảng kết quả bổ sung, ví dụ đầu ra mô hình, và các thông tin triển khai]
