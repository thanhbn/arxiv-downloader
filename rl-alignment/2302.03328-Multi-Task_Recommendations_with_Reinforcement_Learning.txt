# 2302.03328.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2302.03328.pdf
# File size: 973475 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Multi-Task Recommendations with Reinforcement Learning
Ziru Liu
City University of Hong Kong
ziruliu2-c@my.cityu.edu.hkJiejie Tian
City University of Hong Kong
jiejitian2-c@my.cityu.edu.hkQingpeng Cai*
Kuaishou Technology
cqpcurry@gmail.com
Xiangyu Zhao*
City University of Hong Kong
xianzhao@cityu.edu.hkJingtong Gao
City University of Hong Kong
jt.g@my.cityu.edu.hkShuchang Liu
Kuaishou Technology
liushuchang@kuaishou.com
Dayou Chen
City University of Hong Kong
dayouchen2-c@my.cityu.edu.hkTonghao He
City University of Hong Kong
tonghaohe2-c@my.cityu.edu.hkDong Zheng
Kuaishou Technology
zhengd07@qq.com
Peng Jiang
Kuaishou Technology
jp2006@139.comKun Gai
Unaffiliated
gai.kun@qq.com
ABSTRACT
In recent years, Multi-task Learning (MTL) has yielded immense
success in Recommender System (RS) applications [ 41]. However,
current MTL-based recommendation models tend to disregard the
session-wise patterns of user-item interactions because they are
predominantly constructed based on item-wise datasets. Moreover,
balancing multiple objectives has always been a challenge in this
field, which is typically avoided via linear estimations in existing
works. To address these issues, in this paper, we propose a Rein-
forcement Learning (RL) enhanced MTL framework, namely RMTL,
to combine the losses of different recommendation tasks using dy-
namic weights. To be specific, the RMTL structure can address the
two aforementioned issues by ( i) constructing an MTL environ-
ment from session-wise interactions and ( ii) training multi-task
actor-critic network structure, which is compatible with most ex-
isting MTL-based recommendation models, and ( iii) optimizing
and fine-tuning the MTL loss function using the weights generated
by critic networks. Experiments on two real-world public datasets
demonstrate the effectiveness of RMTL with a higher AUC against
state-of-the-art MTL-based recommendation models. Additionally,
we evaluate and validate RMTL’s compatibility and transferability
across various MTL models.
CCS CONCEPTS
•Information systems →Recommender systems .
* Corresponding authors.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
WWW ’23, May 1–5, 2023, Austin, TX, USA
©2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9416-1/23/04. . . $15.00
https://doi.org/10.1145/3543507.3583467KEYWORDS
Multi-task Learning; Recommendation; Reinforcement Learning
ACM Reference Format:
Ziru Liu, Jiejie Tian, Qingpeng Cai*, Xiangyu Zhao*, Jingtong Gao, Shuchang
Liu, Dayou Chen, Tonghao He, Dong Zheng, Peng Jiang, and Kun Gai. 2023.
Multi-Task Recommendations with Reinforcement Learning. In Proceedings
of the ACM Web Conference 2023 (WWW ’23), May 1–5, 2023, Austin, TX, USA.
ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3543507.3583467
1 INTRODUCTION
The evolution of the Internet industry has led to a tremendous
increase in the information volume of online services [ 1], such
as social media and online shopping. In this scenario, the Recom-
mender System (RS), which distributes various types of items to
match users’ interests, has made significant contributions to the
enhancement of user online experiences in a variety of application
fields, such as products recommendation in e-commerce platforms,
short video recommendation in social media applications [ 31,46].
In recent years, researchers have proposed numerous techniques
for recommendations, including collaborative filtering [ 35], matrix
factorization based approaches [ 23], deep learning powered recom-
mendations [ 9,51], etc. The primary objective of RS is to optimize
a specific recommendation object, such as click-through rate and
user conversion rate. However, users usually have varying inter-
action behaviors on a single item. In short-video recommendation
services, for instance, users exhibit a wide range of behavior indi-
cators, such as clicks, thumbs, and continuous dwelling time [ 18];
while in e-commerce platforms, the developers not only focus on
the users’ clicks but also on the final purchases to guarantee profits.
All these potential issues prompted the development of Multi-Task
Learning (MTL) techniques for recommender systems in research
and industry communities [30, 46, 47].
MTL-based recommendation models learn multiple recommen-
dation tasks simultaneously by training in a shared representationarXiv:2302.03328v2  [cs.IR]  10 Mar 2023

--- PAGE 2 ---
WWW ’23, May 1–5, 2023, Austin, TX, USA Liu, et al.
and transferring information among tasks [ 5], which has been de-
veloped for a wide range of machine learning applications, includ-
ing computer vision [ 39], natural language processing [ 15], click-
through rate (CTR) and click-through&conversion rate (CTCVR)
prediction [ 30]. The objective functions for most existing MTL
works are typically linear scalarizations of the multiple-task loss
functions [ 30,31,46], which fix the weight with a constant. This
item-wise multi-objective loss function is incapable of ensuring
the convergence of the global optimum and typically yields limited
prediction performance. On the other hand, at the representation
level, the input of most existing MTL models is assumed to be the
feature embeddings and user-item interaction (called item-wise),
despite the fact that sequentially organized data (i.e., session-wise
inputs) are relatively more prevalent in real-world RS applications.
For example, the click and conversion behaviors of short video
users typically occur during a specific session, so their inputs are
also timing-related. However, this will downgrade the MTL model
performance, while some tasks may have conflicts between session-
wise and item-wise labels [ 5]. Exiting MTL models concentrate
on the design of network structures to improve the generalization
ability of the model, while the study of proposing a new method
that enhances the multi-task prediction weights considering the
session-wise patterns has not received sufficient attention.
To address the two above-mentioned problems, we propose an
RL-enhanced multi-task recommendation framework, RMTL, which
is capable of incorporating the sequential property of user-item in-
teractions into MTL recommendations and automatically updating
the task-wise weights in the overall loss function. Reinforcement
Learning (RL) algorithms have recently been applied in the RS
research, which models the sequential user behaviors as Markov
Decision Process (MDP) and utilizes RL to generate recommenda-
tions at each decision step [ 32,58]. The RL-based recommender
system is capable of handling the sequential user-item interaction
and optimizing long-term user engagement [ 2]. Therefore, our
RL-enhanced framework RMTL can convert the session-wise RS
data into MDP manner, and train an actor-critic framework to gen-
erate dynamic weights for optimizing the MTL loss function. To
achieve multi-task output, we employ a two-tower MTL backbone
model as the actor network, which is optimized by two distinct
critic networks for each task. In contrast to existing MTL models
with item-wise input and constant loss function weight design, our
RMTL model extracts sequential patterns from session-wise MDP
input and updates the loss function weights automatically for each
batch of data instances. In this paper, we focus on the CTR/CTCVR
prediction, which is a crucial metric in e-commerce and short video
platform [ 26]. Experiments against state-of-the-art MTL-based rec-
ommendation models on two real-world datasets demonstrate the
effectiveness of the proposed model.
We summarize the contributions of our work as follows: (i) The
multi-task recommendation problem is converted into an actor-
critic reinforcement learning scheme, which is capable of achieving
session-wise multi-task prediction; (ii) We propose an RL-enhanced
Multi-task learning framework RMTL, which can generate adap-
tively adjusted weights for loss function design. RMTL is compatible
with most existing MTL-based recommendation models; (iii) Ex-
tensive experiments on two real-world datasets demonstrate thesuperior performance of RMTL than SOTA MTL models, we also
verify RMTL’s transferability across various MTL models.
2 THE PROPOSED FRAMEWORK
This section will give a detailed description of our method, the
RMTL framework, which effectively addresses the bottlenecks of
exiting works by realizing session-wise multi-task prediction with
dynamically adjusted loss function weights.
2.1 Preliminary and Notations
Session-wise Multi-task Recommendations. We note that the
choice of loss function as an item-wise multi-objective combina-
tion may lack the ability to extract sequential patterns from data.
In our work, we propose the session-wise multi-objective loss,
which optimizes the objective by minimizing weighted cumula-
tive loss at each session. Given a 𝐾-task1prediction dataset with
𝐷:=
𝑢𝑠𝑒𝑟𝑛,𝑖𝑡𝑒𝑚𝑛,(𝑦𝑛,1,...,𝑦𝑛,𝐾)	𝑁
𝑛=1, which consist of 𝑁user-
item interaction records, where 𝑢𝑠𝑒𝑟𝑛and𝑖𝑡𝑒𝑚𝑛means the𝑛-th
user-item𝑖𝑑. Each data record has 𝐾binary 0-1 labels 𝑦1,···,𝑦𝐾
for the corresponding task. We define the session as {𝝉𝑚}𝑀
𝑚=1,
each session 𝝉𝑚contains𝑇𝑚different discrete timestamps: 𝜏𝑚=
𝜏𝑚,1,···,𝜏𝑚,𝑡,···,𝜏𝑚,𝑇𝑚	
, where𝜏𝑚,𝑡stands for the timestamp 𝑡
in session𝑚. The corresponding label is denoted byn
(𝑦𝑠
𝑡,1,...,𝑦𝑠
𝑡,𝐾)o
.
The session-wise loss function for all sessions parameterized by
𝜃𝑠
1,...,𝜃𝑠
𝐾is defined as:
arg min
𝜃𝑠
1,...,𝜃𝑠
𝐾	L𝑠(𝜃𝑠
1,···,𝜃𝑠
𝐾)=𝐾∑︁
𝑘=1(𝑀∑︁
𝑚=1𝑇𝑚∑︁
𝑡=1𝜔𝑠
𝑘,𝜏𝑚,𝑡𝐿𝑠
𝑘,𝜏𝑚,𝑡(𝜃𝑠
𝑘))
𝑠.𝑡. 𝐿𝑠
𝑘,𝜏𝑚,𝑡(𝜃𝑠
𝑘)=−[𝑦𝑠
𝑘,𝜏𝑚,𝑡𝑙𝑜𝑔(𝑧𝑠
𝑘,𝜏𝑚,𝑡(𝜃𝑠
𝑘))+
(1−𝑦𝑠
𝑘,𝜏𝑚,𝑡)𝑙𝑜𝑔(1−𝑧𝑠
𝑘,𝜏𝑚,𝑡(𝜃𝑠
𝑘))](1)
whereL𝑠(𝜃𝑠
1,···,𝜃𝑠
𝐾)is the session-wise total loss function, and
𝜔𝑠
𝑘,𝜏𝑚is the corresponding weight for the BCE loss, which adap-
tively adjusted by the system. 𝑧𝑠
𝑛,𝑘(𝜃𝑠
𝑘)is the session-wise prediction
value at𝑛-th data point for task 𝑘parameterized by 𝜃𝑠
𝑘. In order
to obtain such weight, we take advantage of the reinforcement
learning framework to estimate the weight using several dynamic
critic networks that could tune their outputs at different solving
steps of the objective function.
2.2 Framework Overview
We present the framework of RMTL in Figure 1 with a state rep-
resentation network, an actor network, and critic networks. The
learning process of RMTL can be summarized as two steps:
•The Forward Step. Given the user-item combined features, the
state representation network generates the state 𝑠𝑡based on the
input feature at timestamp 𝑡. Then, we get the action (𝑎1,𝑡,𝑎2,𝑡)
from the actor network extracting state information. The action
value(𝑎1,𝑡,𝑎2,𝑡)and user-item combined features are further
processed by the MLP layer and embedding layer as the input of
the critic network for calculating Q-value from critic network
𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘)for each task 𝑘. Finally, the overall loss function for
1Note that for multi-task recommendations in CTR/CTCVR prediction setting, we
have𝐾=2.

--- PAGE 3 ---
Multi-Task Recommendations with Reinforcement Learning WWW ’23, May 1–5, 2023, Austin, TX, USA
State 
Representation
s
Categorical 
features
s
Numerical 
features
Embedding
MLP
Embedding
MLP
Environment
ENV
Actor
  
Critic
Target 
Actor
Current 
Actor
Target 
Critic 
1
Target 
Critic 
2
Current 
Critic 
1
Current 
Critic 
2
Action
Q 
value
MLP
Action
State
Action
TD 
Error
Total
Loss
Task 
Labels
Action
Q 
value
Reward
Figure 1: Overview of the RMTL framework.
multi-taskL(𝜃1,𝜃2)can be estimated according to BCE loss for
each task and adapted weight 𝜔𝑘,𝑡controlled by 𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘).
•The Backward Step. We first update the critic network param-
eters𝜙𝑘based on TD error 𝜎and gradients of Q-value. Then, we
optimize the parameters of the actor network 𝜃𝑘with respect to
the overall loss function.
2.3 Markov Decision Process (MDP) Design
We translate the multi-task prediction data into an MDP format
(S,A,P,R,𝛾)to analyze in the reinforcement learning framework.
•State spaceSis the set of state 𝑠𝑡∈R𝑑containing the user-item
combined features, where 𝑑is the combined feature dimension;
•Action space In our CTR/CTCVR prediction setting, Ais the
set of continuous action pairs (𝑎1,𝑡,𝑎2,𝑡)∈[ 0,1]2containing
the actions for two specific prediction tasks, where each element
inArepresents prediction value for CTR and CTCVR;
•Reward𝑟𝑘,𝑡=𝑟(𝑎𝑘,𝑡,𝑦𝑘,𝑡)∈R,𝑘=1,2, is the feedback from
the environment related to the current actions and the ground-
truth click/pay label pair (𝑦1,𝑡,𝑦2,𝑡)of the current item. In order
to be consistent with the definition of BCE loss, we define the
reward function for each step using the negative BCE value, i.e.,
𝑟𝑘,𝑡=𝑦𝑘,𝑡𝑙𝑜𝑔(𝑎𝑘,𝑡)+(1−𝑦𝑘,𝑡)𝑙𝑜𝑔(1−𝑎𝑘,𝑡), 𝑘=1,2(2)
•TransitionP𝑠𝑎𝑠′=𝜒(𝑠′=𝑠𝑡+1)is the transition probability
from state𝑠to state𝑠′based on the user interaction sequence,
and𝜒is the indicator function that set the probability as 1when
the next state (item) corresponds to the sequence;
•Discount𝛾the discount rate is set as 0.95 for our case.
The RMTL model learns a optimal policy 𝜋(𝑠𝑡;𝜃∗)which con-
verts the item-user feature 𝑠𝑡∈Sinto the continuous action value
(𝑎1,𝑡,𝑎2,𝑡)∈A to maximize the weighted total rewards:
max
{𝜃1,𝜃2}2∑︁
𝑘=1(𝑀∑︁
𝑚=1𝑇𝑚∑︁
𝑡=1𝜔𝑟
𝑘,𝜏𝑚,𝑡𝑅𝑘,𝜏𝑚,𝑡(𝜃𝑘))
𝑠.𝑡. 𝑅𝑘,𝜏𝑚,𝑡(𝜃𝑘)=𝑟(𝑎𝑘,𝜏𝑚,𝑡,𝑦𝑘,𝜏𝑚,𝑡)𝑘=1,2(3)
where𝜔𝑟
𝑘,𝜏𝑚,𝑡is the weight for reward at session 𝜏𝑚in timestamp
𝑡for task𝑘. Since the reward function defined by Equation (2)is the negative BCE, and action value is generated from policy
parameterized by 𝜃𝑘. The optimization problem above is equivalent
to minimizing the session-wise loss function in Equation (1).
2.4 Session MDP Construction
Based on the MDP design in Section 2.3, we construct session MDP
for the RL training, which can improve the performance of the MTL
model. Classic MTL methods usually face challenges in introducing
sequential user behaviors into the modeling, where the timestamps
of user behaviors are highly correlated. Reinforcement learning
built upon the MDP sequences can be a solution to address this
problem. However, the alignment order of MDP may have great
influences on the performance of RL, since the decision-making of
the next action depends on the temporal difference (TD). In this
paper, we construct the session MDP, which is organized by session
id. For each session 𝜏𝑚, the transition records are separated by
the timestamps stored in the original dataset. This construction
generates session MDP sequences organized by sequential user
behaviors and has the advantage of overall loss weight updating.
For CTR/CTCVR prediction task, i.e., 𝐾=2, assume we have a
session-wise dataset 𝐷𝑠={𝑆𝜏𝑚1𝜏𝑚,𝑈𝜏𝑚1𝜏𝑚,I𝜏𝑚,(y1,𝜏𝑚,y2,𝜏𝑚)}𝑀
𝑚=1
consisting of 𝑀session records, where 𝑆𝜏𝑚and𝑈𝜏𝑚are the𝑚-th
session and user 𝑖𝑑.I𝜏𝑚is item𝑖𝑑vector whose dimension equals
the item number that interacted with the user in 𝑚-th session. Each
record stores the user-item information x𝜏𝑚={𝑈𝜏𝑚1𝜏𝑚,I𝜏𝑚}2
and corresponding binary 0-1 label vector (y1,𝜏𝑚,y2,𝜏𝑚)for click
and convert indicators. Use feature vector generated from map-
ping function 𝑓for further processing by state function 𝐹, then
input to agent policy 𝜋(𝑠;𝜃𝑘), which is a pretrained MTL net-
work. The replay buffer Bcontains𝑀session MDP sequences
{(𝑠𝜏𝑚,1,···,𝑠𝜏𝑚,𝑇𝑚)}𝑀
𝑚=1, each of them have 𝑇𝑚transition infor-
mation based on session 𝜏. The detail of session MDP construction
is shown in Algorithm 1. Specifically, we separate each session-wise
data based on timestamps. Then, randomly sample a session and
generate state information by the functions 𝑓and𝐹(line 2). The
state information of records in a specific session are sequentially
input into the agent policy 𝜋(𝑠;𝜃𝑘), which generates estimated
21𝜏𝑚is the unit vector with dimension equals the item number in 𝑚-th session.

--- PAGE 4 ---
WWW ’23, May 1–5, 2023, Austin, TX, USA Liu, et al.
Algorithm 1 Session-wise CTR/CTCVR Prediction Setup
Input : Session-wise dataset organized as
{𝑆𝜏1:(x𝜏1,y1,𝜏1,y2,𝜏1),···,𝑆𝜏𝑀:(x𝜏𝑀,y1,𝜏𝑀,y2,𝜏𝑀)}
Reorganize : Separate each session with state-label pairs denoted
as(𝑓(𝑥𝜏𝑚,𝑡),𝑦1,𝜏𝑚,𝑡,𝑦2,𝜏𝑚,𝑡), where𝑓is a mapping function that
returns the feature vector for any given user-item 𝑖𝑑
1:while Environment not closed do
2: Reset : randomly sample a session 𝝉𝑚and initialize state 𝑠1
with𝐹(𝑓(𝑥1)), where𝐹is state representation function.
3: while Session not done do
4: Input: Use action value 𝑎1,𝑡,𝑎2,𝑡from agent policy
𝜋(𝑠;𝜃𝑘)as prediction values for CTR/CTCVR
5: Step: Compute the reward 𝑟1,𝑡,𝑟2,𝑡according to 2 with
actions and labels, return next state 𝑠𝑡+1. Store the
transition(𝑠𝑡,𝑎1,𝑡,𝑎2,𝑡,𝑠𝑡+1,𝑟1,𝑡,𝑟2,𝑡)into replay buffer
B
6: if𝑡+1>𝑇𝑚then
7: Finish the session 𝝉𝑚
8: end if
9: end while
10:end while
action pairs(𝑎1,𝑡,𝑎2,𝑡)(line 4). Then, we further calculate the re-
ward value according to the Equation (2) and store the transition
(𝑠𝑡,𝑎1,𝑡,𝑎2,𝑡,𝑠𝑡+1,𝑟1,𝑡,𝑟2,𝑡)at timestamp 𝑡into the experience re-
play bufferB, which provides a more stable training environment
(line 5). This completes the session-wise CTR/CTCVR prediction
setup for the RL training.
2.4.1 State Representation Network. The original features of
item-user pairs are categorical (represented by one-hot encoding
format) or numerical. The state representation network is the com-
bination of embedding layer and multi-layer perceptron to extract
the user-item features, which corresponds to the mapping function
𝐹in Algorithm 1. The categorical features are firstly converted into
binary vectors with length 𝑙𝑘, and then input to the embedding layer
𝑅𝑙𝑘→𝑅𝑑𝑒with embedding dimension 𝑑𝑒. Besides, the numerical
features are converted to the same dimension with 𝑑𝑒by a linear
transformation. The features translated by the above process are
combined and further used as input for the MLP network, which is
the fully-connected neural network with multiple layers:
h𝑙+1=𝜎(W𝑙h𝑙+b𝑙)𝑛=0,1,...,𝑁−1
h𝐿=𝜎∗(W𝐿−1h𝐿−1+b𝐿−1)(4)
where h𝑙is the𝑙-th hidden layer with weight W𝑙and bias b𝑙, the
corresponding activation function 𝜎is ReLU. The output layer is
denoted by h𝐿with the sigmoid function 𝜎∗. The output h𝐿is
embedded with the extracted feature and represented by 𝐹(𝑓(𝑥𝑡)).
2.5 RMTL Framework
The RMTL aims to optimize the overall loss function with adaptive
dynamic weights. In this direction, existing works [ 46] try to design
the constant rate updating strategy for the loss of weight by:
𝜔′
𝑘,𝑡=𝜔′
𝑘,0𝛾′𝑡(5)
Update 
CTR/CTCVR
Current 
Actor 
Network
Tower 
1
...
...
Tower 
1
Tower 
2
Tower 
1
Tower 
2
Target 
Actor 
Network
Soft 
Update
MLP
a1
a2
Batch 
of  
MDP
MDP  
1
MDP  
i
MDP  
b
Y1
 
Y2
Labels
Total 
Loss
Q-value
ActionFigure 2: Structure of Actor Network.
where𝜔′
𝑘,𝑡is the weight for task 𝑘at timestamp 𝑡, and𝛾′is the
constant decay rate. However, this weight is only controlled by time
𝑡and has no interaction with the type of task. To address this prob-
lem, we apply the critic network for evaluating the estimated total
reward𝑉𝑘of action𝑎𝑘generated from agent policy for each task.
Then, the value of loss function weight for each task is calculated
as the linear transformation with polish variable 𝜆. This novel over-
all loss design can optimize the MTL model parameter in a better
direction, enhancing the efficiency and prediction performance.
The reinforcement learning-based CTR/CTCVR prediction train-
ing process consists of two major ingredients: actor network and
task-specific critic network. The actor network is the main struc-
ture for predicting CTR/CTCVR in our work, which can be thought
of as the agent policy 𝜋(𝑠𝑡;𝜃𝑘)parameterized by 𝜃𝑘. Based on this
actor network, the agent can choose an action for each state. The
critic network denoted by 𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘)is the differentiable action
value network updated by the gradient of Q-value and TD error
𝛿, which can observe the potential rewards of the current state by
learning the relationship between the environment and the rewards.
Besides, it generates the adaptively adjusted weights of BCE loss
for the objective function and updates the actor network parameter
𝜃𝑘along the direction of better policy decisions.
2.5.1 Actor Network. The actor network in the RL setting can
be referred to as a policy that is represented by a probability dis-
tribution𝜋(𝑠;𝜃𝑘)=P(𝑎|𝑠,𝜃𝑘)b. In practice, this method usually
suffers from two problems: (i) The actor critic network performs
poorly in a setting with a large action space, and (ii) the estimated
agent policy is trained by its own expected action value that is not
the ground truth data. In this section, we address the above two
problems by applying a specific MTL model as the agent policy with
deterministic action output, which reduces the capacity of action
space. In addition, we update the neural network based network
parameters using ground truth task labels. The structure of actor
network is shown in Figure 2, which has a similar structure to spe-
cific MTL models, we may specify it using the ESMM model in this
subsection. The shared-bottom layer in the original actor network
design is removed and set as a state representation network men-
tioned in Sub-subsection 2.4.1 for generating replay buffer B. Given
a batch of MDP sequences from the replay buffer B, we input the
state information 𝑠𝑡into the actor network, which is two parallel
neural networks denoted by two tower layers parameterized by 𝜃1

--- PAGE 5 ---
Multi-Task Recommendations with Reinforcement Learning WWW ’23, May 1–5, 2023, Austin, TX, USA
and𝜃2,
𝜋(𝑠𝑡;𝜃𝑘)=𝑎𝑘,𝑡, 𝑘 =1,2 (6)
The output of each tower layer is the deterministic action value,
which represents the prediction for the specific task. In our setting,
one tower outputs the CTR prediction value, and the other outputs
the CTCVR prediction value. After the training process for the
batch of MDP sequences, we calculate the overall loss function
based on the weighted summation of BCE loss:
L(𝜃1,𝜃2)=∑︁
(𝑠𝑡,···,𝑠𝑡+1,···)∈𝑏2∑︁
𝑘=1𝜔𝑘,𝑡𝐵𝐶𝐸(𝜋(𝑠𝑡;𝜃𝑘),𝑦𝑘,𝑡)) (7)
This differentiable overall loss function L(𝜃1,𝜃2)is controlled by
binary cross entropy between estimated action and real task labels
𝑦𝑘,𝑡, which enhances the decision accuracy of the policy.
2.5.2 Critic Network. The traditional critic network estimates
the action value 𝑄generated from actor network and then updates
the actor network parameters based on value 𝑄. While we choose
the MTL model as the actor network, the problem is how to design
a suitable critic network structure for multi-parameter updating.
In this paper, we propose a multi-critic structure with two parallel
MLP networks sharing one bottom layer. This design is capable of
updating MTL model parameters in the actor network and polishing
the loss function weights for specific tasks. The structure of the
critic network is shown in Figure 3. The first part of the critic
network is one shared-bottom layer, which transforms the user-
item features and action information simultaneously. Similar to the
state representation network in Sub-subsection 2.4.1, we apply one
embedding layer and an MLP structure for feature extraction. We
then combine the user-item feature and action information as the
input of two differentiable action value networks parameterized by
𝜙𝑘, which output the estimated Q-value based on the state-action
information for each task. Given the current state 𝑠𝑡and action𝑎𝑘,𝑡,
the Q-value is calculated by:
𝑄′(𝑠𝑡,𝑎𝑘,𝑡)=E[𝑟𝑘,𝑡+𝛾𝑉(𝑠𝑡+1)|𝑠𝑡,𝑎𝑘,𝑡] (8)
=𝑟𝑘,𝑡+𝛾∑︁
𝑠𝑡+1∈S𝑝𝑠𝑡,𝑎𝑡,𝑠𝑡+1·max𝑄′(𝑠𝑡+1,𝑎𝑘,𝑡+1)
where𝑉(𝑠𝑡+1)is state value function. In our case, the action value
𝑎𝑡,𝑘for task𝑘is estimated by the actor network, and the next state
𝑠𝑡+1is determined with a probability equal to 1. Therefore, the
Q-value function in multi critic structure can be calculated as:
𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘)=𝑟𝑘,𝑡+𝛾𝑄(𝑠𝑡+1,𝑎𝑘,𝑡+1;𝜙𝑘) (9)
The weight of the objective loss function in Equation (1)is reversely
adjusted along the direction of the Q value to improve the optimiza-
tion process of actor network, which is the linear transformation
with punish variable 𝜆:
𝜔𝑘,𝑡=1−𝜆∗𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘) (10)
where𝑎𝑘,𝑡is an action for task 𝑘at timestamp 𝑡.𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘)is
action value of state 𝑠𝑡and action𝑎𝑘,𝑡.
2.6 Optimization
The general framework of the actor-critic network usually encoun-
ters an inevitable problem: the convergence of the critic network is
not guaranteed. To overcome this problem, we leverage the idea
s
Categorical 
features
s
Numerical 
features
Feature 
Embedding
s
Actions
Critic 
Input 
Tower 
1
Current 
Critic 
Network
Target 
Critic 
Network
Soft 
update
Critic-1
Critic-2
Critic-1
Critic-2
TD 
Error
 
TD 
Error
 
Q1'
Q1
Reward1
Q2
Update
Update
Q-value 
Vector
Feature 
Extraction
Feature 
Input
Reward2
 
Q2'Figure 3: Structure of Critic Network.
of Deterministic Policy Gradient Algorithms [ 41], introducing the
target network into the learning framework. The target networks
have exactly the same structure as the proposed actor and critic net-
works (i.e., estimation networks ), and we denote them as 𝜋(𝑠𝑡;e𝜃𝑘)
and𝑄(𝑠𝑡,𝑎𝑘,𝑡;e𝜙k), which have lagging parameters e𝜃𝑘ande𝜙𝑘. In
the rest part of this paper, we specify the main actor-critic network
by estimation actor networks parameterized by 𝜃𝑘and estimation
critic networks parameterized by 𝜙𝑘. The traditional actor-critic
network usually suffers from the problem of parameter divergence
and the convergence of the critic network is not guaranteed. The
target network can address these problems by generating stationary
target values from logging parameterized neuron networks, which
smooths the training process. Next, we detail the optimization pro-
cess for model parameters of the estimation actor-critic network,
and also the soft update for target networks.
Estimation critic network updates. 𝜙𝑘is the crucial parame-
ter in the critic network, which determines action Q-value. Given
transition(𝑠𝑡,𝑎1,𝑡,𝑎2,𝑡,𝑠𝑡+1,𝑟1,𝑡,𝑟2,𝑡)from replay bufferB, the TD
target of the target critic network for 𝑘-th task is derived from:
𝑇𝐷𝑘,𝑡=𝑟𝑘,𝑡+𝛾𝑄(𝑠𝑡+1,𝑎𝑘,𝑡+1;e𝜙𝑘) (11)
where𝑎𝑘,𝑡+1=𝜋(𝑠𝑡+1;e𝜃𝑘)is the estimated next action from target
actor network. The Q-value generated from the estimation critic
network, which estimates the current action value is defined as:
𝑄𝑘,𝑡=𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘) (12)
After the training process among 𝑏batch of transitions from the
replay buffer, we calculate the average TD error 𝛿:
𝛿=1
2|𝑏|∑︁
(𝑠𝑡,···,𝑠𝑡+1,···)∈𝑏2∑︁
𝑘=1(𝑇𝐷𝑘,𝑡−𝑄𝑘,𝑡) (13)
=1
2|𝑏|∑︁
(𝑠𝑡,···,𝑠𝑡+1,···)∈𝑏2∑︁
𝑘=1
[𝑟𝑘,𝑡+𝛾𝑄(𝑠𝑡+1,𝑎𝑘,𝑡+1;e𝜙𝑘)−𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘)]

--- PAGE 6 ---
WWW ’23, May 1–5, 2023, Austin, TX, USA Liu, et al.
Then we update the current critic network for each task by the
following gradient decent method:
𝜙𝑡+1
𝑘=𝜙𝑡
𝑘−𝛼𝜙I𝛿∇𝜙k𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘) (14)
where∇𝜙k𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘)is the gradient of 𝑘-th target Q-value. This
completes the optimization of the estimation critic networks.
Estimation actor network updates. Before the TD error 𝛿con-
verges to threshold 𝜖, we update the current actor networks pa-
rameterized by 𝜃𝑘through the gradients back-propagation of loss
function for each tower layer after the forward process of each
batch transitions from B:
𝜃𝑡+1
𝑘=𝜃𝑡
𝑘+𝛼𝜃I∇𝜃𝑘J(𝜃𝑘) (15)
where the loss for tower layers is defined by the negative of average
Q-valueJ(𝜃𝑘)=−1
𝑏Í
(𝑠𝑡,···,𝑠𝑡+1,···)∈𝑏𝑄′(𝑠𝑡,𝜋(𝑠𝑡;𝜃𝑘)). We further
update the current actor networks with respect to the gradients of
the overall loss function of CTR/CTCVR prediction:
𝜃𝑡+1
𝑘=𝜃𝑡
𝑘−𝛼𝜃I∇𝜃𝑘L(𝜃1,𝜃2) (16)
L(𝜃1,𝜃2)=∑︁
(𝑠𝑡,···,𝑠𝑡+1,···)∈𝑏2∑︁
𝑘=1𝜔𝑘,𝑡𝐵𝐶𝐸(𝜋(𝑠𝑡;𝜃𝑘),𝑦𝑘,𝑡)(17)
where the weights 𝜔𝑘,𝑡=1−𝜆∗𝑄(𝑠𝑡,𝑎𝑘,𝑡;𝜙𝑘)controlled by the
corresponding current critic network, which is adaptively adjusted
to the negative direction of estimated action Q-value at 𝑡.
Soft update of the target network. The target actor network
and two specific target critic networks are updated until the cur-
rent critic network reaches the convergence condition towards the
direction of parameters in current networks:
e𝜃𝑘=𝛽e𝜃𝑘+(1−𝛽)𝜃𝑘
e𝜙𝑘=𝛽e𝜙𝑘+(1−𝛽)𝜙𝑘(18)
where𝛽∈[0,1]is the soft update rate.
3 EXPERIMENT
In this section, we conduct several experiments using two real-
world datasets to evaluate the effectiveness of the RMTL framework.
3.1 Experimental Setup
3.1.1 Dataset .From our survey, there is only one open-source
dataset, RetailRocket3, that contains sequential labels of click and
pay. In order to have a comprehensive comparison of RMTL, we
also apply the “Kuairand-1K” dataset4[14] for further analysis.
Each dataset is split into training, validation, and testing sets with
proportion 6:2:2 sorted by timestamp 𝑡for model learning.
3.1.2 Baselines. Since our RMTL structure modifies the MTL ob-
jective loss function, we choose the MTL models with their default
losses and one RL-based model as our baselines. Single Task [29]:
The naive method to learn each task separately, which is widely
used to compare the performance of MTL models. Shared Bottom
[30]: The basic structure of MTL models with a shared bottom
layer. ESMM [31]: A model specified in CTR/CVR predictions that
focus on the different sample spaces for each task. MMoE [30]:
3https://www.kaggle.com/datasets/retailrocket/ecommerce-dataset
4https://kuairand.com/MMoE uses gates to control the relationship between the shared
bottom and the task-specific towers. PLE [46]: PLE is able to cap-
ture complicated task correlations by using shared experts as well
as task-specified expert layers. D-PLE [41]: We apply DDPG (deep
deterministic policy gradient) with the PLE model as the RL-based
baseline. Actually, all the loss functions for the baseline models are
obtained by taking the unchangeable weighted average.
3.1.3 Evaluation Metrics.
•AUC andLogloss : These are the straightforward metrics to
evaluate CTR/CTCVR prediction tasks. According to [ 16], it is
statistically significant in recommendation tasks that AUC or
Logloss changes more than 0.001-level.
•s-Logloss : This metric is defined as the averaged Logloss with
respect to all sessions.
3.2 Implementation Details
We implement the baseline models using the public library5, which
integrates 7 standard MTL models (without ESMM) and gives a
reference performance of each model. All the models have the
same basic network structure, i.e., an input embedding layer with
dimension 128, a 128×512×256Multi-Layer Perceptron (MLP) as
the bottom layer, and a 256×128×64×1MLP as the tower layer.
Specifically, for the models using expert layers (same structure as
the bottom layer), we fix the number of experts as 8. The activation
function for the hidden layers is ReLU and Sigmoid for the output.
We also set a 0.2dropout rate. We use Adam optimizer ( 𝑙𝑟=1𝑒−3)
to learn all the models. We also implement ESMM sharing the
hyperparameters of the above models.
Since the actor network in our RMTL model is a two-tower design
with multi-task output, it is compatible with most existing MTL-
based recommendation models. In this paper, we apply our RMTL
structure on three representative MTL models: ESMM, MMoE, and
PLE. As to the RMTL structure, we set the Actor network with the
same structure as specified MTL models. The Critic networks also
share the same bottom layer and have their out through one tower
layer. Moreover, we add a 1×128action layer to be consistent
with the input features, and we change the output activation to
negative ReLU in that our reward only has negative values. In order
to improve data efficiency, we initialize the Actor with pretrained
parameters from the MTL models and keep them frozen until the
Critics converge. Then we multiply the critic value in the total loss
and retrain the MTL model. The default learning rates of the actor
𝛼𝜃and critic network 𝛼𝜙are set as 0.001. We set the default soft
update rate 𝛽=0.2, punish variable 𝜆=0.7. The implementation
code is available online to ease reproducibility6.
3.3 Overall Performance and Comparison
We compare the performance of five baseline MTL models with
RMTL models for CTR/CTCVR prediction tasks on two different
datasets. The overall performance on CTR/CTCVR tasks for differ-
ent methods is shown in Table 1. It can be observed that:
5https://github.com/easezyc/Multitask-Recommendation-Library.git
6https://github.com/Applied-Machine-Learning-Lab/RMTL

--- PAGE 7 ---
Multi-Task Recommendations with Reinforcement Learning WWW ’23, May 1–5, 2023, Austin, TX, USA
Table 1: Performance on CTR/CTCVR tasks for different methods.
Dataset Task MetricMethods
Single Shared ESMM MMoE PLE D-PLE RMTL- RMTL- RMTL-
Task Bottom ESMM MMoE PLE
RetailRocket CTRAUC↑ 0.7273 0.7287 0.7282 0.7309 0.7308 0.7308 0.7338* 0.7350* 0.7339*
Logloss↓ 0.2065 0.2048 0.2031 0.2021 0.2056 0.2058 0.2024 0.1995 0.2013
s-Logloss↓0.0846 0.0839 0.0852 0.0853 0.0827 0.0830 0.0836 0.0848 0.0824
RetailRocket CTCVRAUC↑ 0.7250 0.7304 0.7316 0.7347 0.7387 0.7386 0.7341 0.7396 0.7419*
Logloss↓ 0.0489 0.0493 0.0486 0.0496 0.0486 0.0490 0.0485 0.0490 0.0480
s-Logloss↓0.0150 0.0149 0.0150 0.0145 0.0147 0.0149 0.0150 0.0143 0.0146
Kuairand CTRAUC↑ 0.7003 0.7018 0.7009 0.7014 0.7026 0.7025 0.7031 0.7029 0.7053*
Logloss↓ 0.6127 0.6114 0.6128 0.6119 0.6111 0.6123 0.6111 0.6105 0.6092*
s-Logloss↓0.6263 0.6250 0.6261 0.6255 0.6252 0.6257 0.6252 0.6243 0.6242
Kuairand CTCVRAUC↑ 0.7342 0.7310 0.7350 0.7324 0.7339 0.7339 0.7377* 0.7345 0.7367*
Logloss↓ 0.5233 0.5249 0.5220 0.5237 0.5235 0.5237 0.5200* 0.5225 0.5221
s-Logloss↓0.5449 0.5468 0.5436 0.5450 0.5454 0.5451 0.5433 0.5446 0.5447
↑: higher is better;↓: lower is better. Underline : the best baseline model. Bold : the best performance among all models.
“*”: the statistically significant improvements (i.e., two-sided t-test with 𝑝<0.05) over the best baseline.
•The SingleTask model achieves the worst performance in both
prediction tasks on two datasets. The feature representation net-
work and loss function optimization are separated for each task,
which fails to detect the intrinsic relation between multiple tasks.
Thus, we can understand that the shared-bottom method, which
shares the feature extraction parameters, can outperform the
SingleTask method for both datasets.
•The PLE model achieves almost the best performance among all
MTL baseline models in most cases. On the basis of the MMoE
model, which controls the parameter interaction between the
shared bottom and specific task tower, the PLE model specifies
the parameter sharing between expert layers to extract hidden
interaction patterns between each task. This demonstrates that
the PLE baseline model can improve the efficiency of information
sharing among tasks to achieve better prediction performance.
•Each version of our proposed RMTL model outperforms the cor-
responding non-RL version baseline model (e.g., RMTL-ESMM
v.s. ESMM) in both prediction tasks with AUC and logloss metrics
on two datasets. Especially on the RetialRocket dataset, the RMTL
model achieves around 0.003-0.005 AUC gains compared with the
corresponding baseline model. By leveraging the sequential prop-
erty of the reinforcement learning framework, the RL-enhanced
method is capable of processing session-wise recommendation
data and achieves significant improvement in CTR/CTCVR pre-
diction tasks by adaptively adjusted loss function weights.
•The CTCVR prediction result on the Kuairand dataset presented
in the fourth row of Table 1, where the ESMM baseline model
and its advanced RL version achieve better performance than
other MTL models. For short video recommendations, click-and-
convert behaviors usually have a relatively higher correlation.
In this phenomenon, the ESMM model outperforms MMoE and
PLE by overcoming the sample selection bias.
•The RMTL models achieve s-Logloss improvement of less than
0.001 compared with the corresponding baseline models, which
ESMM
RMTL-ESMM mmoe-ESMMple-ESMM0.7250.7270.7290.7310.7330.7350.737(a) ESMM/AUC
MMOE
esmm-MMOE RMTL-MMOEple-MMOE0.7300.7310.7320.7330.7340.7350.7360.737(b) MMOE/AUC
PLE
esmm-PLE mmoe-PLE RMTL-PLE0.7300.7310.7320.7330.7340.735(c) PLE/AUC
ESMM
RMTL-ESMMmmoe-ESMMple-ESMM0.20150.20200.20250.2030(d) ESMM/Logloss
MMOE
esmm-MMOE RMTL-MMOEple-MMOE0.1990.2000.2010.2020.203(e) MMOE/Logloss
PLE
esmm-PLE mmoe-PLE RMTL-PLE0.2010.2020.2030.2040.205(f) PLE/LoglossFigure 4: Transferability study results.
is lower than that of Logloss. This phenomenon may be caused
by the fact that session-based metric has similar performance
with online A/B test, which is more difficult to be improved [ 19].
Note that the ratio of logloss and s-logloss is different for the two
datasets since they have different average session lengths while
the average session Logloss is affected by the average session
length for a specific dataset.
To summarize, the RMTL model outperforms the state-of-the-art
MTL models on both CTR and CTCVR prediction tasks for different
real-world datasets. In addition, it can be employed in most MTL
models, validating its desirable compatibility.
3.4 Transferability Study
This subsection presents the transferability study for the RMTL
method for the CTR task on the RetailRocket dataset. We try to
figure out whether the critic network learned from different logging

--- PAGE 8 ---
WWW ’23, May 1–5, 2023, Austin, TX, USA Liu, et al.
Table 2: Ablation study on RetailRocket.
Task MetricMethods
CW WL NLC RMTL-PLE
CTRAUC↑ 0.7308 0.7323 0.7325 0.7339*
Logloss↓0.2056 0.2218 0.2020 0.2013 *
CTCVRAUC↑ 0.7387 0.7411 0.7418 0.7419*
Logloss↓0.0486 0.0485 0.0481 0.0480*
“*” indicates the best performance among all variants.
policies can be applied to the same MTL baseline model and im-
prove the prediction performance. For each MTL baseline model, we
leverage the model parameters from the critic network pretrained
by ESMM, MMoE, and PLE model on the RetailRocket dataset. The
results of AUC and logloss are presented in Figure 4 (a)-(c) and
(d)-(f), where “mmoe-ESMM” means ESMM model applying critic
network trained from MMoE and “ple-ESMM” means ESMM model
applying critic network trained from PLE. It can be observed that:
(i) The pretrained critic network from three MTL models can signif-
icantly increase AUC for each baseline model. (ii) The pretrained
critic network from three MTL models can significantly decrease
logloss for each baseline model.
To summarize, the pretrained critic network is capable of im-
proving the prediction performance of most MTL models, which
demonstrates the excellent transferability of the RMTL model.
3.5 Ablation Study
The most essential design of the RMTL model is the overall loss
function with learnable weights, which is the linear combination
of Q-value from the critic network. In this subsection, in order
to figure out the importance of this loss function in the proposed
model, we change some components and define three variants
below: (i) CW: The CW variant indicates applying constant weights
for the overall loss function and no gradient policy update for
the actor network, which eliminates the contribution of the critic
network. We assign all weights equal to constant 1. (ii) WL: The
WL variant indicates the adjustment of weights 𝜔is controlled
by session behavior labels 𝑦𝑘multiplied by Q-value. (iii) NLC: No
linear transformation is performed to the loss weights, instead, we
directly assign the negative Q-value to loss weights.
The ablation study results for the PLE model on the RetailRocket
dataset are shown in Table 2. It can be observed that: (i) CW has
the worst performance for AUC and logloss metrics on both pre-
diction tasks. It may be caused by equivalently assigned BCE loss
weights, which fail to capture the hidden dynamics of multi-tasks.
(ii) WL and NLC have almost the same performance in this study
that outperform the CW variant with 0.002-0.003 AUC gain. This
demonstrates the effect of weights updating from the critic network.
(iii) RMTL-PLE with our proposed total loss setting achieves the
best performance on both tasks, which illustrates the validity of
our linear combination weight design. We may summarize that our
proposed MTL model can achieve better CTR/CTCVR prediction
performance, where total loss design has great contributions.4 RELATED WORK
In this section, we give a brief introduction to existing works related
to RMTL, i.e., RL-based and MTL-based recommender systems.
Reinforcement Learning Based Recommender Systems. Plenty
of research has been done combining Reinforcement Learning
(RL) [ 2,43,48,50,53,55,57] into the field of Recommender Systems
(RS). Compared to traditional learning-to-rank solutions of RS [ 27]
that optimize the immediate user feedback, the RL-based RS focuses
on optimizing the cumulative reward function that estimates mul-
tiple rounds of interactions between the recommendation policy
and the user response environment. The general problem formu-
lates the sequential user-system interactions as a Markov Decision
Process (MDP) [ 40]. RL solutions have been studied under this for-
mulation including early-age tabular-based methods [ 22,32,34]
that optimize an evaluation table for a set of state-action pairs,
value-based methods [ 20,45,56,59] that learn to evaluate the qual-
ity of an action or a state, policy gradient methods [ 6,7,42] that
optimize the recommendation policy based on the long-term re-
ward, and the actor-critic methods [ 24] based on policy gradient
method [ 4,10,38,44] that simultaneously learn an action evalua-
tor and an action generator. Our method falls into the actor-critic
category and extends it toward the multi-task problem setting.
The main challenges of applying RL for the recommendation task
consist of the large combinatorial state/actions space [ 11,25], the
uncertainty of user environment [ 21,54], the stability and efficiency
of exploration [ 3,8], and the design of a user’s reward function over
heterogeneous behaviors [ 60]. Our work is related to the reward
function design but also enhances the performance of each task.
Multi-Task Learning in Recommender Systems. As stated in [ 52],
Multi-Task Learning (MTL) is a machine learning framework that
learns a task-invariant representation of an input data in a bot-
tom network, while each individual task is solved in one’s respec-
tive task-specific network and boosted by the knowledge transfer
across tasks. Recently, MTL has received increasing interest in rec-
ommender systems [ 17,28,31,36,37] due to its ability to share
knowledge among different tasks especially its ability to capture
heterogeneous user behaviors. A series of works seek to improve
on it by designing different types of shared layer architectures.
These works either introduce constraints on task-specific parame-
ters [ 12,33,49] or separate the shared and the task-specific parame-
ters [ 30,46]. The general idea is to disentangle and share knowledge
through the representation of the input feature. Additionally, there
is also research on applying multi-agent RL for the multi-scenario
setting [ 13] where the recommendation task is bundled with other
tasks like search, and target advertising. Different from the above
ideas, we resort to knowledge distillation to transfer ranking knowl-
edge across tasks on task-specific networks and we combine RL to
improve the long-term satisfaction of users. Notably, our model is
a general framework and could be leveraged as an extension for
most off-the-shelf MTL models.
5 CONCLUSION
In this paper, we propose a novel multi-task learning framework,
RMTL, to improve the prediction performance of multi-tasks by
generating dynamic total loss weights in an RL manner. The RMTL
model can adaptively modify the weights of BCE for each prediction

--- PAGE 9 ---
Multi-Task Recommendations with Reinforcement Learning WWW ’23, May 1–5, 2023, Austin, TX, USA
task by Q-value output from the critic network. By constructing a
session-wise MDP environment, we estimate the multi-actor-critic
networks using a specific MTL agent and then polish the optimiza-
tion of the MTL overall loss function using dynamic weight, which
is the linear transformation of the critic network output. We con-
duct several experiments on two real-world commercial datasets to
verify the effectiveness of our proposed method with five baseline
MTL-based recommendation models. The results demonstrate that
RMTL is compatible with most existing MTL-based recommenda-
tion models and can improve multi-task prediction performance
with excellent transferability.
ACKNOWLEDGEMENTS
This research was partially supported by APRC - CityU New Re-
search Initiatives (No.9610565, Start-up Grant for New Faculty of
City University of Hong Kong), SIRG - CityU Strategic Interdis-
ciplinary Research Grant (No.7020046, No.7020074), HKIDS Early
Career Research Grant (No.9360163), Huawei Innovation Research
Program and Ant Group (CCF-Ant Research Fund).
REFERENCES
[1]Giuseppe Aceto, Valerio Persico, and Antonio Pescapé. 2020. Industry 4.0 and
health: Internet of things, big data, and cloud computing for healthcare 4.0.
Journal of Industrial Information Integration 18 (2020), 100129.
[2]M Mehdi Afsar, Trafford Crump, and Behrouz Far. 2021. Reinforcement learning
based recommender systems: A survey. ACM Computing Surveys (CSUR) (2021).
[3]Xueying Bai, Jian Guan, and Hongning Wang. 2019. A model-based reinforcement
learning with adversarial training for online recommendation. Advances in Neural
Information Processing Systems 32 (2019).
[4]Shalabh Bhatnagar, Mohammad Ghavamzadeh, Mark Lee, and Richard S Sutton.
2007. Incremental natural actor-critic algorithms. Advances in neural information
processing systems 20 (2007).
[5] Rich Caruana. 1997. Multitask learning. Machine learning 28, 1 (1997), 41–75.
[6]Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming Tang,
Yuzhou Zhang, and Yong Yu. 2019. Large-scale interactive recommendation with
tree-structured policy gradient. In Proceedings of the AAAI Conference on Artificial
Intelligence , Vol. 33. 3312–3320.
[7]Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and
Ed H Chi. 2019. Top-k off-policy correction for a REINFORCE recommender
system. In Proceedings of the Twelfth ACM International Conference on Web Search
and Data Mining . 456–464.
[8]Minmin Chen, Bo Chang, Can Xu, and Ed H Chi. 2021. User response models
to improve a reinforce recommender system. In Proceedings of the 14th ACM
International Conference on Web Search and Data Mining . 121–129.
[9]Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al .
2016. Wide & deep learning for recommender systems. In Proceedings of the 1st
workshop on deep learning for recommender systems . 7–10.
[10] Thomas Degris, Patrick M Pilarski, and Richard S Sutton. 2012. Model-free
reinforcement learning with continuous action in practice. In 2012 American
Control Conference (ACC) . IEEE, 2177–2182.
[11] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy
Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, and
Ben Coppin. 2015. Deep reinforcement learning in large discrete action spaces.
arXiv preprint arXiv:1512.07679 (2015).
[12] Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. 2015. Low resource
dependency parsing: Cross-lingual parameter sharing in a neural network parser.
InProceedings of the 53rd annual meeting of the Association for Computational
Linguistics and the 7th international joint conference on natural language processing
(volume 2: short papers) . 845–850.
[13] Jun Feng, Heng Li, Minlie Huang, Shichen Liu, Wenwu Ou, Zhirong Wang,
and Xiaoyan Zhu. 2018. Learning to collaborate: Multi-scenario ranking via
multi-agent reinforcement learning. In Proceedings of the 2018 World Wide Web
Conference . 1939–1948.
[14] Chongming Gao, Shijun Li, Yuan Zhang, Jiawei Chen, Biao Li, Wenqiang Lei,
Peng Jiang, and Xiangnan He. 2022. KuaiRand: An Unbiased Sequential Rec-
ommendation Dataset with Randomly Exposed Videos. In Proceedings of the
31st ACM International Conference on Information and Knowledge Management
(Atlanta, GA, USA) (CIKM ’22) . 5 pages. https://doi.org/10.1145/3511808.3557624[15] Yoav Goldberg. 2017. Neural network methods for natural language processing.
Synthesis lectures on human language technologies 10, 1 (2017), 1–309.
[16] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.
DeepFM: a factorization-machine based neural network for CTR prediction. arXiv
preprint arXiv:1703.04247 (2017).
[17] Guy Hadash, Oren Sar Shalom, and Rita Osadchy. 2018. Rank and rate: multi-task
learning for recommender systems. In Proceedings of the 12th ACM Conference on
Recommender Systems . 451–454.
[18] Yanxiang Huang, Bin Cui, Jie Jiang, Kunqian Hong, Wenyu Zhang, and Yiran Xie.
2016. Real-time video recommendation exploration. In Proceedings of the 2016
International Conference on Management of Data . 35–46.
[19] Guangda Huzhang, Zhen-Jia Pang, Yongqing Gao, Wen-Ji Zhou, Qing Da, Anx-
iang Zeng, and Yang Yu. 2020. Validation Set Evaluation can be Wrong: An
Evaluator-Generator Approach for Maximizing Online Performance of Ranking
in E-commerce. CoRR abs/2003.11941 (2020). https://arxiv.org/abs/2003.11941
[20] Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu,
Heng-Tze Cheng, Tushar Chandra, and Craig Boutilier. 2019. SlateQ: A tractable
decomposition for reinforcement learning with recommendation sets. (2019).
[21] Eugene Ie, Chih wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing
Wang, Rui Wu, and Craig Boutilier. 2019. RecSim: A Configurable Simulation
Platform for Recommender Systems. (2019). arXiv:1909.04847 [cs.LG]
[22] Thorsten Joachims, Dayne Freitag, Tom Mitchell, et al .1997. Webwatcher: A tour
guide for the world wide web. In IJCAI (1) . Citeseer, 770–777.
[23] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-
niques for recommender systems. Computer 42, 8 (2009), 30–37.
[24] Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen,
Huifeng Guo, and Yuzhou Zhang. 2018. Deep reinforcement learning based
recommendation with explicit user-item interactions modeling. arXiv preprint
arXiv:1810.12027 (2018).
[25] Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen,
Huifeng Guo, Yuzhou Zhang, and Xiuqiang He. 2020. State representation mod-
eling for deep reinforcement learning based recommendation. Knowledge-Based
Systems 205 (2020), 106170.
[26] Hu Liu, Jing Lu, Xiwei Zhao, Sulong Xu, Hao Peng, Yutong Liu, Zehua Zhang,
Jian Li, Junsheng Jin, Yongjun Bao, et al .2020. Kalman filtering attention for user
behavior modeling in ctr prediction. Advances in Neural Information Processing
Systems 33 (2020), 9228–9238.
[27] Tie-Yan Liu et al .2009. Learning to rank for information retrieval. Foundations
and Trends ®in Information Retrieval 3, 3 (2009), 225–331.
[28] Yichao Lu, Ruihai Dong, and Barry Smyth. 2018. Coevolutionary recommendation
model: Mutual learning between ratings and reviews. In Proceedings of the 2018
World Wide Web Conference . 773–782.
[29] Minh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser.
2015. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114
(2015).
[30] Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018.
Modeling task relationships in multi-task learning with multi-gate mixture-of-
experts. In Proceedings of the 24th ACM SIGKDD international conference on
knowledge discovery & data mining . 1930–1939.
[31] Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun
Gai. 2018. Entire space multi-task model: An effective approach for estimating
post-click conversion rate. In The 41st International ACM SIGIR Conference on
Research & Development in Information Retrieval . 1137–1140.
[32] Tariq Mahmood and Francesco Ricci. 2007. Learning and adaptivity in interactive
recommender systems. In Proceedings of the ninth international conference on
Electronic commerce . 75–84.
[33] Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. 2016.
Cross-stitch networks for multi-task learning. In Proceedings of the IEEE conference
on computer vision and pattern recognition . 3994–4003.
[34] Omar Moling, Linas Baltrunas, and Francesco Ricci. 2012. Optimal radio channel
recommendations with explicit and implicit feedback. In Proceedings of the sixth
ACM conference on Recommender systems . 75–82.
[35] Raymond J Mooney and Loriene Roy. 2000. Content-based book recommending
using learning for text categorization. In Proceedings of the fifth ACM conference
on Digital libraries . 195–204.
[36] Huashan Pan, Xiulin Li, and Zhiqiang Huang. 2019. A Mandarin Prosodic Bound-
ary Prediction Model Based on Multi-Task Learning.. In Interspeech . 4485–4488.
[37] Changhua Pei, Xinru Yang, Qing Cui, Xiao Lin, Fei Sun, Peng Jiang, Wenwu Ou,
and Yongfeng Zhang. 2019. Value-aware recommendation based on reinforcement
profit maximization. In The World Wide Web Conference . 3123–3129.
[38] Jan Peters and Stefan Schaal. 2008. Natural actor-critic. Neurocomputing 71, 7-9
(2008), 1180–1190.
[39] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn:
Towards real-time object detection with region proposal networks. Advances in
neural information processing systems 28 (2015).
[40] Guy Shani, David Heckerman, Ronen I Brafman, and Craig Boutilier. 2005. An
MDP-based recommender system. Journal of Machine Learning Research 6, 9
(2005).

--- PAGE 10 ---
WWW ’23, May 1–5, 2023, Austin, TX, USA Liu, et al.
[41] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
Martin Riedmiller. 2014. Deterministic policy gradient algorithms. In International
conference on machine learning . Pmlr, 387–395.
[42] Yueming Sun and Yi Zhang. 2018. Conversational recommender system. In The
41st international acm sigir conference on research & development in information
retrieval . 235–244.
[43] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-
duction . MIT press.
[44] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. 1999.
Policy gradient methods for reinforcement learning with function approximation.
Advances in neural information processing systems 12 (1999).
[45] Nima Taghipour, Ahmad Kardan, and Saeed Shiry Ghidary. 2007. Usage-based
web recommendations: a reinforcement learning approach. In Proceedings of the
2007 ACM conference on Recommender systems . 113–120.
[46] Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020. Progressive
layered extraction (ple): A novel multi-task learning (mtl) model for personalized
recommendations. In Fourteenth ACM Conference on Recommender Systems . 269–
278.
[47] Nelson Vithayathil Varghese and Qusay H Mahmoud. 2020. A survey of multi-task
deep reinforcement learning. Electronics 9, 9 (2020), 1363.
[48] Yuyan Wang, Mohit Sharma, Can Xu, Sriraj Badam, Qian Sun, Lee Richardson,
Lisa Chung, Ed H Chi, and Minmin Chen. 2022. Surrogate for Long-Term User
Experience in Recommender Systems. In Proceedings of the 28th ACM SIGKDD
Conference on Knowledge Discovery and Data Mining . 4100–4109.
[49] Yongxin Yang and Timothy Hospedales. 2016. Deep multi-task representation
learning: A tensor factorisation approach. arXiv preprint arXiv:1605.06391 (2016).
[50] Qihua Zhang, Junning Liu, Yuzhuo Dai, Yiyan Qi, Yifan Yuan, Kunlun Zheng, Fan
Huang, and Xianfeng Tan. 2022. Multi-Task Fusion via Reinforcement Learning
for Long-Term User Satisfaction in Recommender Systems. In Proceedings of
the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining .
4510–4520.
[51] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based rec-
ommender system: A survey and new perspectives. ACM Computing Surveys(CSUR) 52, 1 (2019), 1–38.
[52] Yu Zhang and Qiang Yang. 2021. A survey on multi-task learning. IEEE Transac-
tions on Knowledge and Data Engineering (2021).
[53] Xiangyu Zhao, Changsheng Gu, Haoshenglun Zhang, Xiwang Yang, Xiaobing
Liu, Hui Liu, and Jiliang Tang. 2021. DEAR: Deep Reinforcement Learning for
Online Advertising Impression in Recommender Systems. In Proceedings of the
AAAI Conference on Artificial Intelligence , Vol. 35. 750–758.
[54] Xiangyu Zhao, Long Xia, Jiliang Tang, and Dawei Yin. 2019. " Deep reinforce-
ment learning for search, recommendation, and online advertising: a survey"
by Xiangyu Zhao, Long Xia, Jiliang Tang, and Dawei Yin with Martin Vesely as
coordinator. ACM sigweb newsletter Spring (2019), 1–15.
[55] Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang
Tang. 2018. Deep Reinforcement Learning for Page-wise Recommendations. In
Proceedings of the 12th ACM Recommender Systems Conference . ACM, 95–103.
[56] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin.
2018. Recommendations with negative feedback via pairwise deep reinforcement
learning. In Proceedings of the 24th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining . 1040–1048.
[57] Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Dawei Yin, Yihong Zhao, and Jiliang
Tang. 2017. Deep Reinforcement Learning for List-wise Recommendations. arXiv
preprint arXiv:1801.00209 (2017).
[58] Yufan Zhao, Donglin Zeng, Mark A Socinski, and Michael R Kosorok. 2011.
Reinforcement learning strategies for clinical trials in nonsmall cell lung cancer.
Biometrics 67, 4 (2011), 1422–1433.
[59] Guanjie Zheng, Fuzheng Zhang, Zihan Zheng, Yang Xiang, Nicholas Jing Yuan,
Xing Xie, and Zhenhui Li. 2018. DRN: A deep reinforcement learning framework
for news recommendation. In Proceedings of the 2018 world wide web conference .
167–176.
[60] Lixin Zou, Long Xia, Zhuoye Ding, Jiaxing Song, Weidong Liu, and Dawei Yin.
2019. Reinforcement learning to optimize long-term user engagement in recom-
mender systems. In Proceedings of the 25th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining . 2810–2818.
