# 2311.08692.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2311.08692.pdf
# File size: 967943 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Routing to the Expert: Efficient Reward-guided Ensemble of Large
Language Models
Keming Lu, Hongyi Yuan∗, Runji Lin∗
Junyang Lin, Zheng Yuan, Chang Zhou, Jingren Zhou
Alibaba Inc.
{lukeming.lkm,yuanhongyi.yhy,linrunji.lrj}@alibaba-inc.com
{junyang.ljy,yuanzheng.yuanzhen}@alibaba-inc.com
{ericzhou.zc,jingren.zhou}@alibaba-inc.com
Abstract
The complementary potential of Large Lan-
guage Models (LLM) assumes off-the-shelf
LLMs have heterogeneous expertise in a wide
range of domains and tasks so that an ensem-
ble of LLMs can achieve consistently better
performance. Existing ensemble methods for
LLMs mainly focus on reward model ranking
of outputs, leading to significant computation
overhead. To combat this issue, we revisit the
complementary potential of LLMs and further
elaborate it by mining latent expertise with off-
the-shelf reward models. We propose ZOOTER ,
a reward-guided routing method distilling re-
wards on training queries to train a routing func-
tion, which can precisely distribute each query
to the LLM with expertise about it. We also
integrate a tag-based label enhancement to miti-
gate noise from uncertainty when using rewards
as silver supervision. ZOOTER shows compu-
tation efficiency in inference as it only intro-
duces minor computation overhead of a routing
function compared with reward model ranking
methods. We evaluate ZOOTER on a compre-
hensive benchmark collection with 26 subsets
on different domains and tasks. ZOOTER out-
performs the best single model on average and
ranks first on 44% of tasks, even surpassing
multiple reward model ranking methods.1
1 Introduction
Large Language Models (LLMs) aligned with hu-
man preference rapidly emerge and are almost daily
released (Touvron et al., 2023a,b; Anil et al., 2023;
Bai et al., 2023). These off-the-shelf LLMs are
further finetuned or aligned with human preference
to be generalists (Xu et al., 2023; Touvron et al.,
2023b,a) or specialists (Yuan et al., 2023a; Luo
et al., 2023a,b; Roziere et al., 2023) for solving
versatile tasks. It is worth noticing that LLMs are
pretrained and aligned with various data, leading
∗ ∗Work done during internship at Alibaba Inc.
1Work in progress.
Share ideas for adapting art masterpieces into interactive experiences for children. List 5 specific artworks and associated ideas.Certainly! As a helpful and respectful assistant, I'm happy to provide ideas for adapting art masterpieces ... 1. "The Starry Night" by Vincent van Gogh:\n\n Interactive Experience Idea: Create a digital or ...1. The Mona Lisa by Leonardo da Vinci: Create a virtual reality experience where children can explore the painting and interact with it in different ways, such as …… \n\n2. The Scream by Edvard Munch: Create a ……1. The Scream by Edvard Munch: This painting is a perfect opportunity to discuss emotions with children. …… Then, you can guide them through a process ……1. "The Starry Night" by Vincent van Gogh:\n Interactive Experience: Create a virtual reality (VR) experience ……. They can learn about the constellations and stars, and even create their own ……1. "The Starry Night" by Vincent van Gogh -Create a virtual reality experience ……. They can learn about the different constellations and the techniques van Gogh used to create the swirling effects ……Creating interactive experiences based on art masterpieces can be a great way to introduce children to the world of art and spark their creativity. ... 1. "The Starry Night" by Vincent van Gogh\nIdea: Create a virtual reality (VR) experience …WizardCoderLLaMa-2
WizardMathWizardLMVicunaOpenChatRewardOracleZooter0.10102.190.0210-0.030.007-0.360.12102.630.1391.770.63104.22Figure 1: An example of the large language model en-
semble. Reward model ranking marked in blue needs
to generate responses from all models while ZOOTER
routers the given query to the best model and only infers
one model. This case is collected from the MT-Bench
benchmark and we also present oracle judgements of
each response.
to diverse strengths and weaknesses in versatile
downstream tasks (Jiang et al., 2023). Therefore,
the ensemble of LLMs harnesses the complemen-
tary potential among them and may achieve better
performance than a single best-on-average model
across diverse tasks.
One of the key challenges in the LLM ensemble
is computation efficiency due to the large parame-
ter size of existing LLMs. Previous research (Jiang
et al., 2023; Shnitzer et al., 2023) provides solid
methods to merge generation outputs of LLMs as
an ensemble. Such methods require tremendous
inference cost that makes it unscalable and thus
not competitive to the best-on-average model un-
der low-resource scenarios. To efficiently assemble
off-the-shelf LLMs, we first dive deeper into the
considerably straightforward but still understudiedarXiv:2311.08692v1  [cs.CL]  15 Nov 2023

--- PAGE 2 ---
assumption: Off-the-shelf aligned LLMs, even for
those aligned as “generalists”, have heterogeneous
expertise in a wide range of domains and topics .
However, analyzing the expertise of an LLM is also
challenged as the latent expertise of LLMs is highly
related to the pretrained and alignment data, which
is very vague and inaccessible even for popular
open-source LLMs such as LLAMA -2-C HAT (Tou-
vron et al., 2023b) and WIZARD LM (Xu et al.,
2023).
If this assumption strongly holds, off-the-shelf
LLMs can be assembled efficiently by assigning
queries to the model that is proficient in the query
without additional inference costs on each model.
Such an efficient routing strategy only requires in-
ference cost for a single model for each query and
the overhead cost of a much smaller query router.
However, probing the detailed expertise of off-the-
shelf LLMs and generating supervision for train-
ing routers also require annotations. Developing a
data-efficient training method for routing queries
is significantly understudied.
To combat these issues, we propose ZOOTER , a
reward-guided query routing method for efficiently
assembling off-the-shelf LLMs. ZOOTER obtains
and enhances silver supervision from existing re-
ward models (RM) for query router training and
distributes queries in advance to “expertise”. As
shown in Fig. 1, the reward distribution implies
the oracle judgments and reveals a latent exper-
tise between LLMs. And ZOOTER captures the
expertise from reward distributions and provides
query distribution during inference. Specifically,
we first conduct a comprehensive study involving
four groups of benchmarks across more than 26
subsets in various domains and tasks. We investi-
gate six widely used open-source LLMs and show
the complementary potential of such wide-range
downstream tasks by aggregating them via reward
model ranking. We then collect a diverse training
query set and distill rewards of model expertise as
indirect supervision for training an LLM router and
develop tag-based label enhancement to overcome
the shortage of such silver labels from reward mod-
els further. With comprehensive experiments, we
show ZOOTER can benefit from RM silver supervi-
sion to learn the latent expertise among LLMs and
conduct efficient routing for the model ensemble.
Our contributions are mainly three-fold:
•We revisit the complementary potential of open-
source LLMs, which proves the effectiveness ofLLM ensemble, and show rewards from off-the-
shelf RMs can be silver supervision for model
expertise.
•We propose ZOOTER , an efficient reward-guided
routing method distilling rewards from off-the-
shelf reward model for probing model expertise.
Then, we develop a tag-based label enhancement
to mitigate noise from the uncertainty of reward
models.
•We comprehensively evaluate ensemble meth-
ods, including reward model ranking and
ZOOTER on four groups of benchmarks with
26 subsets on different tasks and domains. Our
evaluation shows ZOOTER can effectively assem-
ble LLMs and even outperforms reward model
ranking methods with significantly fewer com-
putation overhead.
2 Related Works
Instruction Tuning and Alignment. Instruction
tuning (Longpre et al., 2023) helps LLMs to fol-
low versatile instructions, which is widely adopted
to align LLMs with human preference (Chiang
et al., 2023; Xu et al., 2023; Bai et al., 2023). In
this work, we focus on assembling aligned LLMs,
such as Llama-2-Chat (Touvron et al., 2023b), Wiz-
ardLM (Xu et al., 2023), Vicuna (Chiang et al.,
2023), and so on. And we evaluate them on a wide
range of alignment evaluation tasks.
Large Language Model Ensemble. The ensem-
ble of LLMs is an emerging topic due to the ex-
plosion of open-source LLMs. LLM ensemble
aims to merge off-the-shelf LLMs to achieve con-
sistently better performance across diverse down-
stream tasks. Few works explore the complemen-
tary potential assumption of LLMs and how to as-
semble LLMs with it. Jiang et al. (2023) presents
an ensembling framework consisting of a pair
ranker and a generation fuser. Chen et al. (2023)
sequentially infers off-the-shelf LLMs and stops
until the response meets a sufficient quality. Wang
et al. (2023b) proposes a fusing-of-experts problem
that fuses outputs of expert models with comple-
mentary knowledge of the data distribution and
formulates it as supervised learning. Shnitzer et al.
(2023) show the utility and limitations of learning
model routers from various benchmark datasets.
Although these works all focus on reward ranking
or routing strategies to assemble LLMs, ZOOTER
distinguishes from these concurrent works in two

--- PAGE 3 ---
aspects. First, our concurrent works require output
generations or the forward process to get prompt
representations of all candidates, leading to sig-
nificant computation overhead. ZOOTER infers
model expertise by distilling rewards on a prede-
fined training query set to avoid such inference
overhead. Then, all these works are developed and
evaluated on a set of benchmarks, while ZOOTER
can be developed with only queries without golden
responses, and ZOOTER aims for more diverse
alignment tasks. Therefore, ZOOTER stands out
for its efficiency in data and computation. We also
evaluate ZOOTER on more diverse alignment tasks
to comprehensively examine the complementary
potential of LLMs.
Reward Model Guided Generation. Reward
models in the context of large language models
are commonly used to improve alignment perfor-
mance by reinforcement learning (Schulman et al.,
2017; Ouyang et al., 2022) or preference learning
(Yuan et al., 2023b; Rafailov et al., 2023; Song
et al., 2023). Reward models can also improve
the performance during the generation phase. The
math reasoning ability of language models can be
improved by using reward models ranking multiple
generated reasoning paths (Cobbe et al., 2021; Ue-
sato et al., 2022; Lightman et al., 2023). Liu et al.
(2023) uses reward models to formulate reward-
guided decoding. Inspired by these successful ap-
plications of reward models in alignment, ZOOTER
also takes advantage of off-the-shelf reward models
to investigate the latent expertise of LLMs.
3 Methods
We first revisit the complementary potential of
LLMs (§3.1) and then introduce ZOOTER as an
efficient LLM ensemble method (§3.2).
3.1 Complementary Potential of LLMs
In this section, we present the preliminaries about
the assumption: Off-the-shelf aligned LLMs have
heterogeneous expertise in a wide range of domains
and topics . We also briefly introduce two LLM
ensemble strategies, reward model ranking, and
query routing.
Complementary Potential Assumption. Consid-
ering a set of LLMs denoted as M={mi|i∈
Z+}and a set of downstream queries denoted
asQ={qi|i∈ Z+}, we assume that for each
LLM miinM, there exists a non-empty query
subset Qmisuch that the LLM can achieve uni-formly better performance than other LLMs in
Mfor any query qj∈ Q mi, which is mi=
argmaxm∈MP(qj, m(qj)).Pcan be any prefer-
ence or metric for performance assessment. In this
work, we further enhance this assumption and aim
to show that the complementary between LLMs re-
veals their expertise in different domains and tasks,
so that we can categorize queries and choose the
best LLM for each category.
Reward Model Ranking. Reward model rank-
ing (RMR) leverages the complementary poten-
tial to ensemble LLMs and achieve surpass per-
formance. RMR tries to find a reward function
ˆPto estimate the oracle preference Pso that we
can obtain the best model for each query (Jiang
et al., 2023). However, RMR infers all candidate
models to get outputs and then rank them with a
reward function, introducing a large computation
overhead.
Query Routing. Query routing mitigates effi-
ciency concerns in the LLM ensemble, especially
compared with existing RMR methods. In gen-
eral, query routing tries to find a routing function
Z(q, m i)with respect to qj∈ Q exists, so that
mi= argmaxm∈MZ(qj, m). The routing func-
tion distributes queries based on themselves with-
out generating outputs. If the complementary po-
tential of LLMs holds, the routing function predicts
the probability that a query qbelongs to the exper-
tise of an LLM Qm.
3.2 Zooter
In this section, we propose ZOOTER , a reward-
guided query routing method for efficiently assem-
bling large language models. ZOOTER learns from
the reward model ranking to interpret the latent
expertise of each model. So, as shown in Fig. 2,
ZOOTER first infers all candidate LLMs on a train-
ing set containing diverse queries to generate re-
sponses. Then, all responses will be rewarded by
an off-the-shelf reward model providing scalar re-
wards, marked in blue dash lines in Fig. 2. The
rewards are first enhanced by a tag-based prior
for smoothing and denoising. The normalized re-
ward distribution is then used as supervision in the
knowledge distillation training of the routing func-
tion, shown in the green dash lines in Fig. 2. During
inference, the routing function categorizes the in-
put query to an LLM with the strongest expertise
potential in this query, and the LLM will gener-
ate an expert response. By training such a routing

--- PAGE 4 ---
Response 1Response 2Response N-1.730.337.28
LLM 2LLM 1
LLM N...Off-the-shelf
Query......
ResponsesRewardsQuery RoutingReward Model Ranking
ZooterReward Model
Tag-based Label EnhancementReward DistributionKnowledge DistillationTraining
ZooterTrainingReward-guided Query Routing (Light)Reward Model Ranking (Heavy)
DistributeFigure 2: Overview of ZOOTER .ZOOTER aims to assemble a set of off-the-shelf LLMs by first conducting a reward
model ranking on a diverse training set to obtain supervision of model expertise, highlighted in blue in the figure.
Instruction tags are then used to mitigate the uncertainty in reward estimation. ZOOTER uses the normalized rewards
as supervision to train a routing function by knowledge distillation. The training circle is marked in green, and the
inference is marked in orange. ZOOTER is much lighter in computation as it routes the query to the corresponding
expert LLM during inference time, while reward model ranking has to generate outputs for all candidates.
function, Z OOTER achieves a much more efficient
ensemble as it only needs to infer one expert LLM,
plus a small computation overhead of the routing
function. In this section, we introduce the two key
components along with the design motivations.
Reward Distillation. As we discussed in §3.1,
query routing aims to find a routing function pre-
dicting the probability that a query qbelongs to
the expertise of an LLM Qm, where Qmis a set of
queries that an LLM mconsistently achieves maxi-
mum preference among all candidates. Recalling
the reward model ranking, we notice the estimated
preferences ˆP(q, m i(q)), i.e., reward, can be inter-
preted as the relative advantages of an LLM mi
among all candidates on the query q. Therefore,
the normalized reward can be used as a silver su-
pervision for the routing function:
Z(q)i=P(q∈Qmi)
:=exp( ˆP(q, m i(q)))P
mi∈Mexp( ˆP(q, m i(q))),
as the higher advantages inherently present the ex-
pertise of an LLM on a query compared with itscounterparts.
To estimate the expertise of each model and
train the routing function, we need to apply the
reward preference ranking on a diverse training
setˆQ. We first infer all candidate models on each
query ˆq∈ˆQ, and then assign rewards by an off-
the-shelf reward model to obtain a scalar reward
for each query and model
ri={ˆP(ˆqi, mj(ˆqi))}|M|
j=1, i= 1, . . . ,|ˆQ|.
. Then, we train the router function Zon the train-
ing set by knowledge distillation with a Kullback-
Leibler divergence as the loss function:
L(qi,ri) =KL(Z(qi),softmax (ri)).
ZOOTER is a data-efficient and low-resource
method as the training set ˆQonly contains queries
without annotations of responses. However, queries
in the training set are expected to be as diverse as
possible to maximize the generalization abilities of
the routing function. The distillation process helps
ZOOTER to learn the latent expertise of each model.
So, we can mitigate the computation cost by only

--- PAGE 5 ---
judging whether a query belongs to the expertise
set with our routing function during inference.
Tag-based Label Enhancement. Although reward
distillation provides a feasible way for routing func-
tions to leverage silver supervision from reward
model ranking, the language reward model pro-
vides rewards with uncertainty, introducing certain
noises (Gleave and Irving, 2022). We first empir-
ically analyze this uncertainty in §4.3. Existing
off-the-shelf reward models will all involve noises
in terms of uncertainty, as shown in Fig. 3. There-
fore, we leverage instruction tagging to enhance
rewards on the training queries further. The tag-
based label enhancement we proposed is similar
to the widely used label smoothing techniques and
proven effective in knowledge distillation (Yuan
et al., 2020). Specifically, we first tag each query
ˆqi∈ˆQwith a local tagger T(·)to obtain a set
of tags T(qi). Then, we aggregate all rewards on
queries with the same tags for the tag-wise rewards
as follows:
Qt={qi|t∈ T(qi), i= 1, . . . ,|ˆQ|}
rt=1
|Qt|X
i∈Qtri
Then, we enhance rewards for each query with tag-
wise rewards by a linear combination:
r∗
i=βri+ (1−β)rt;t=T(qi), i= 1, . . . ,|ˆQ|
,where βis a hyper-parameter for the trade-off be-
tween coarse-grained tag-wise rewards and fine-
grained sample-level rewards. Then, we replace
original rewards in the KL divergence loss training
with tag-based enhanced rewards r∗during routing
function training.
4 Experiments
In this section, we report experimental
setup (§4.1), main results (§4.2), and analy-
sis about Z OOTER (§4.3).
4.1 Experimental Setup
Candidate LLMs. We select six LLAMA-based
LLMs of the same 13B size as the candidate LLMs
for query routing. (a) WizardLM (Xu et al., 2023)
is aligned with queries and responses augmented
byEVOLINSTRUCT , (b) WizardCoder (Luo et al.,
2023b) is a coding expert LLM using the same
techniques as WizardLM, (c) WizardMath (Luo
et al., 2023a) is a math expert LLM alignedwith query augmentation, ChatGPT rewards and
PPO optimization, (d) Vicuna (Chiang et al.,
2023) is aligned on tremendous conversations be-
tween users and proprietary chatbots, (e) Open-
Chat (Wang et al., 2023a) is aligned with a selected
set of ShareGPT with additional training strate-
gies, (f) Llama-2-Chat (Touvron et al., 2023b)
is first aligned by supervised fine-tuning and then
multi-turn rejection sampling. Both baselines and
ZOOTER are experimented and evaluated based on
these six candidates.
Training Datasets. We create a diverse mix in-
struction dataset from the open-source data to max-
imize the generalization abilities of ZOOTER . We
first collect and tag open-source data from 13
datasets with a local tagger developed by Lu et al.
(2023). For trustworthy evaluation results, we de-
contaminate all samples containing queries that
have a 6-gram overlap with any samples in our
benchmarks described below to avoid data leakage.
Then, we randomly select ten samples for each
unique tag to form a diverse mix instruction dataset
DIVINSTRUCT with 47,986 instructions and sam-
ples across 6,270 different tags. Detailed statistics
of D IVINSTRUCT is in Appx. §A.
Benchmarks. We actively involve four sets of
benchmarks to evaluate ZOOTER on various down-
stream tasks comprehensively. We first include
three widely-used alignment benchmarks with
GPT-4 judge:
•AlpcaEval (Li et al., 2023b) consists of 5 subsets
from the koala, vicuna, and others evaluation
sets. It contains 805 samples in total.
•FLASK (Ye et al., 2023) is a fine-grained evalu-
ation for alignment. We evaluate 10 domains in
FLASK and report the average score across all
domains as a final score.
•MT-Bench (Chiang et al., 2023) is a multi-turn
evaluation across eight aspects, including mathe-
matics and coding. We only train and route with
the first-turn query but evaluate in the multi-turn
manner as the original recipe.
However, as reported by Wang et al. (2023c),
GPT-4 judgments may have bias and significant
disagreement with humans. Therefore, we also
include a group of benchmarks consisting of
MMLU (Hendrycks et al., 2021), GSM8K (Cobbe
et al., 2021), and HumanEval (Chen et al., 2021).
Metrics. Comparing ensemble models on various
benchmarks is challenging as the scale of scores

--- PAGE 6 ---
Model#Param AlpacaEval (5) FLASK (10) MT-Bench (8) Benchmarks (3) All (26)
Ranker Infer Avg. MTR Avg. MTR Avg. MTR Avg. MTR MTR % Uplift
Routing Candidates
WIZARD CODER −− 13B 0.42 5.6 3.12 5.2 4.44 5.38 30.9 4.33 5.3 0.06
WIZARD LM −− 13B 0.89 2.0 3.89 1.8 7.15 2.0 44.2 2.0 1.83 0.25
WIZARD MATH −− 13B 0.47 5.0 3.28 5.0 5.73 4.38 34.8 4.0 4.6 0.03
LLAMA -2-CHAT −− 13B 0.91 1.6 3.88 1.5 6.72 2.88 32.3 3.67 2.23 0.31
OPENCHAT −− 13B 0.89 2.2 3.79 3.1 7.12 2.0 31.2 3.33 2.67 0.19
VICUNA −− 13B 0.8 3.8 3.7 3.5 6.58 3.25 33.6 2.67 3.4 0.06
BMA −− 13B 0.91 1.6 3.88 1.5 6.72 2.88 32.3 3.67 2.23 0.31
ZOOTER
Ours 86M 13B 0.93 1.17 3.89 1.82 7.11 2.33 34.2 3.0 1.94 0.44
Reward Model Ranking (RMR)
W/ OA SSIST RM 300M 6 ×13B 0.79 4.0 3.75 3.73 6.59 3.22 35.1 3.25 3.42 0.19
W/ LLM-B LENDER 300M 6 ×13B 0.83 3.67 3.77 3.36 6.21 4.0 36.4 2.75 3.39 0.17
W/ AUTO-J 13B 6 ×13B 0.89 2.67 3.92 1.64 7.03 2.22 32.2 3.5 2.25 0.42
W/ ULTRA RM 13B 6 ×13B 0.92 1.17 4.06 1.0 7.18 1.89 40.1 3.25 1.53 0.72
W/ QWEN RM 7B 6 ×13B 0.92 1.33 4.04 1.0 7.26 2.11 38.6 3.0 1.58 0.67
W/ ORACLE −− 6×13B 0.98 1.0 4.56 1.0 8.25 1.0 75.3 1.0 1.0 1.0
Proprietary Models
GPT-3.5-turbo −− −− 0.89 2.67 4.06 1.91 7.94 1.78 73.0 1.0 1.78 0.61
GPT-4 −− −− 0.94 1.0 4.37 1.0 8.99 1.0 88.3 1.0 1.0 1.0
Table 1: Main results of both ZOOTER and reward model ranking. We report performance across four groups of
benchmarks and report the number of subsets beside the name of benchmarks. We also report the parameters of
ranker and total inference models for both candidates and ensemble methods. MTR denotes the mean task rate, and
%Uplift denotes the rate of uplift. The average scores and uplift rate are as higher as better while MTR is as lower
as better. We mark better scores in darker blue for better visualization and easier interpretation.
is different on each benchmark. To combat this
issue, we do not only report the scores on each
benchmark but also the mean task rank (MTR). All
benchmarks we evaluate have multiple subsets, we
define MTR as the rank of the evaluated model
among all baselines average on all subsets. MTR
is only about the rank among baselines so it can
be easily adopted across benchmarks that have dif-
ferent score scales. Similarly, we also propose an
uplift rate, denoting the rate of subsets that the
evaluated model achieves the best performance of
benchmarks. We report these two metrics on a total
of 26 evaluation subsets in all benchmarks. Lower
MTR and higher uplift rates show the evaluated
model has consistently higher performance among
versatile downstream tasks.
Baselines. We also compare ZOOTER with existing
reward model ranking (RMR) methods. We set up
RMR baselines with the latest rewards models, in-
cluding OA SSIST RM,AUTO-J (Li et al., 2023a),
ULTRA RM (Cui et al., 2023), QWEN RM (Bai
et al., 2023), and an Oracle ranking for refer-
ence. We also consider the pair ranking in LLM-
Blender (Jiang et al., 2023) as one of the RMRmethods. Besides, we also report the performance
of proprietary models across our benchmark collec-
tions for reference, including GPT-3.5-turbo and
GPT-4.
Configurations. We train our routing function
from mdeberta-v3-base . And we use QwenRM
to generate rewards on training queries as supervi-
sion for our routing function, as it achieves the best
performance in reward model ranking with consid-
erably smaller model parameters described in §4.2.
And we run all training and inference on 8 A100
GPUs. We infer and evaluate all benchmarks with
corresponding configurations and GPT-4 settings.
We use greedy decoding for MMLU, GSM8K, and
HumanEval.
4.2 Results
We present the main results in Tab. 1. We report the
performance of six routing candidates across our
benchmarks, and the best model on average (BMA)
isLLAMA-2-C HAT. And we report ZOOTER with
β= 0.3in tag-based label enhancement. We fur-
ther analyze the results in the following two as-
pects:

--- PAGE 7 ---
Complementary Potential. We evaluate the en-
semble with reward model ranking (RMR) on five
different off-the-shelf reward models. RMR with
UltraRM achieves the best performance in MTR
and uplift rate on the aggregation of all bench-
marks, which ranks at 1.53 and achieves the best
model across 72% subtasks. RMR with QwenRM
achieves the second best and has similar perfor-
mance with UltraRM with smaller parameter sizes,
followed by RMR with Auto-J, LLM-Blender, and
OAssistRM. RMR with QwenRM, UltraRM, and
Auto-J outperform that of BMA, showing the effec-
tiveness of RMR. Furthermore, we also calculate
the score of RMR with an Oracle ranker, which
consistently outperforms all candidates and even
outperforms GPT-4 on AlpacaEval and FLASK.
Such results provide solid evidence for the com-
plementary potential of off-the-shelf LLMs and
also support the key motivation behind ZOOTER ,
i.e., using rewards from off-the-shelf reward mod-
els as silver supervision for the routing function
training. However, we notice RMR fails on bench-
marks, such as MMLU, GSM8K, and HumanEval,
showing that precisely judging knowledge, mathe-
matics, and coding problems are still challenging
for existing RMs.
Zooter Performance. We then compare the per-
formance of ZOOTER with that of BMA and RMR.
ZOOTER outperforms BMA on AlpacaEval, MT-
Bench, and Benchmarks, and achieves similar per-
formance on FLASK. The most significant im-
provement is witnessed on MT-Bench, where the
performance of ZOOTER is higher than that of
BMA by 0.39. In general, ZOOTER achieves top-
1 on 44% subtasks while BMA is only on 31%.
With the evidence above, ZOOTER successfully uti-
lizes the complementary potential between LLMs
to achieve the best performance more consistently
over our benchmarks, with computation overhead
from only 86M ranker. At the same time, ZOOTER
outperforms RMR with OAssistRM, LLM-Blender,
and Auto-J, by significantly less computation over-
head. However, though ZOOTER outperforms
RMR with QwenRM on AlpacaEval, there are still
obvious gaps between ZOOTER and RMR with
QwenRM in general.
4.3 Analysis
We provide further analysis on how RM uncertainty
may influence the training of Z OOTER .
RM Uncertainty. As presented in the previous
2 4 6 8 10
MT-Bench Score0.81.01.21.41.61.8Reward Entropy
Figure 3: Analysis between reward entropy and scores
of reward preference ranking on MT-bench.
βAlpacaEval FLASK MT-Bench Benchmarks All
0 1.4 2.2 2.25 3.67 2.06
0.1 1.2 2.1 2.38 3.67 2.00
0.3 1.2 1.9 2.50 3.67 1.97
0.5 1.2 2.2 3.12 3.67 2.23
0.7 1.2 2.2 3.38 4.00 2.31
0.9 1.2 2.3 3.12 4.00 2.31
1.0 1.2 2.3 3.25 4.00 2.34
Table 2: Mean task rank (MTR) of different βin tag-
based label enhancement across all benchmarks. The
best value of βis marked in blue.
research, RM may have uncertainty on its scalar
rewards, which may introduce noise in the routing
training since we use RM scores as silver supervi-
sion. In this subsection, we first present the exis-
tence of this uncertainty to explain the motivation
behind tag-based label enhancement, the method
we propose to mitigate such uncertainty in the rout-
ing function training. We calculate the entropy of
rewards from QwenRM among all candidate LLMs
for each query in MT-Bench and draw it with the
MT-Bench scores of each sample by reward prefer-
ence ranking with QwenRM. As shown in Fig. 3,
samples with lower reward entropy tend to have
high MT-bench scores. We interpret this observa-
tion as higher reward entropy reveals more uncer-
tainty in the reward. Therefore, we propose tag-
based label enhancement to leverage a tag-based
prior to adjust reward entropy.
Label Enhancement. The tag-based label en-
hancement proposed in §3.2 contains a hyper-
parameter β, which represents the trade-off be-
tween fine-grained sample-level rewards and
coarse-grained tag-level rewards. We conduct ex-
periments to tune this hyperparameter and ana-
lyze how rewards in different granularities may

--- PAGE 8 ---
influence the training of our routing function. As
shown in Tab. 2, ZOOTER achieves the best perfor-
mance when βequals 0.3, proving a combination
of sample-level and tag-level rewards will benefit
the reward distillation. The ablation also shows the
necessity of tag-based label enhancement. Fur-
thermore, distilling tag-level rewards ( β= 0 )
shows significantly better performance than distill-
ing sample-level rewards ( β= 1), supporting the
analysis that noises from the uncertainty of RMs in
sample-level rewards damage reward distillation.
5 Conclusion
In this work, we revisit the complementary po-
tential of open-source LLMs and reward model
ranking of multiple off-the-shelf reward models,
providing evidence to the effectiveness of LLM en-
semble. We propose Z OOTER , an efficient reward-
guided routing method for ensemble off-the-shelf
LLMs. Comprehensive evaluation shows ZOOTER
can outperform the best single model on average
and even ensemble models by reward model rank-
ing with significantly fewer computation overhead.
Valuable future works include diving deep into the
interpretation of latent expertise in each LLM.
References
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-
son, Dmitry Lepikhin, Alexandre Passos, Siamak
Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng
Chen, et al. 2023. Palm 2 technical report. arXiv
preprint arXiv:2305.10403 .
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al. 2023. Qwen technical report. arXiv
preprint arXiv:2309.16609 .
Lingjiao Chen, Matei Zaharia, and James Zou. 2023.
Frugalgpt: How to use large language models while
reducing cost and improving performance.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluating
large language models trained on code.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, et al. 2021. Training verifiers to solve math
word problems. arXiv preprint arXiv:2110.14168 .
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,
Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and
Maosong Sun. 2023. Ultrafeedback: Boosting lan-
guage models with high-quality feedback.
Adam Gleave and Geoffrey Irving. 2022. Uncer-
tainty estimation for language reward models. arXiv
preprint arXiv:2203.07472 .
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2021. Measuring massive multitask language under-
standing. In International Conference on Learning
Representations .
Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023.
Llm-blender: Ensembling large language models
with pairwise ranking and generative fusion. arXiv
preprint arXiv:2306.02561 .
Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan,
Hai Zhao, and Pengfei Liu. 2023a. Generative
judge for evaluating alignment. arXiv preprint
arXiv:2310.05470 .
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan
Taori, Ishaan Gulrajani, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. 2023b. Al-
pacaeval: An automatic evaluator of instruction-
following models. https://github.com/
tatsu-lab/alpaca_eval .
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri
Edwards, Bowen Baker, Teddy Lee, Jan Leike,
John Schulman, Ilya Sutskever, and Karl Cobbe.
2023. Let’s verify step by step. arXiv preprint
arXiv:2305.20050 .
Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru,
Yejin Choi, Hannaneh Hajishirzi, and Asli Celiky-
ilmaz. 2023. Don’t throw away your value model!
making ppo even better via value-guided monte-carlo
tree search decoding.

--- PAGE 9 ---
Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V . Le,
Barret Zoph, Jason Wei, and Adam Roberts. 2023.
The flan collection: Designing data and methods for
effective instruction tuning.
Keming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Jun-
yang Lin, Chuanqi Tan, and Chang Zhou. 2023. #
instag: Instruction tagging for diversity and complex-
ity analysis. arXiv preprint arXiv:2308.07074 .
Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-
guang Lou, Chongyang Tao, Xiubo Geng, Qingwei
Lin, Shifeng Chen, and Dongmei Zhang. 2023a. Wiz-
ardmath: Empowering mathematical reasoning for
large language models via reinforced evol-instruct.
Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo
Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qing-
wei Lin, and Daxin Jiang. 2023b. Wizardcoder:
Empowering code large language models with evol-
instruct.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
Ermon, Christopher D. Manning, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model.
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.
Code llama: Open foundation models for code. arXiv
preprint arXiv:2308.12950 .
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv:1707.06347 .
Tal Shnitzer, Anthony Ou, Mírian Silva, Kate Soule,
Yuekai Sun, Justin Solomon, Neil Thompson, and
Mikhail Yurochkin. 2023. Large language model
routing with benchmark datasets.
Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei
Huang, Yongbin Li, and Houfeng Wang. 2023. Pref-
erence ranking optimization for human alignment.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models.Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023b. Llama 2: Open foundation and
fine-tuned chat models.
Jonathan Uesato, Nate Kushman, Ramana Kumar, Fran-
cis Song, Noah Siegel, Lisa Wang, Antonia Creswell,
Geoffrey Irving, and Irina Higgins. 2022. Solv-
ing math word problems with process-and outcome-
based feedback. arXiv preprint arXiv:2211.14275 .
Guan Wang, Sijie Cheng, Qiying Yu, and Changling
Liu. 2023a. OpenChat: Advancing Open-source Lan-
guage Models with Imperfect Data.
Hongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik
Kundu, Eric Xing, and Mikhail Yurochkin. 2023b.
Fusing models with complementary expertise.
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack
Hessel, Tushar Khot, Khyathi Raghavi Chandu,
David Wadden, Kelsey MacMillan, Noah A. Smith,
Iz Beltagy, and Hannaneh Hajishirzi. 2023c. How
far can camels go? exploring the state of instruction
tuning on open resources.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin
Jiang. 2023. Wizardlm: Empowering large language
models to follow complex instructions.
Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeon-
bin Hwang, Seungone Kim, Yongrae Jo, James
Thorne, Juho Kim, and Minjoon Seo. 2023. Flask:
Fine-grained language model evaluation based on
alignment skill sets.
Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and
Jiashi Feng. 2020. Revisiting knowledge distillation
via label smoothing regularization. In Proceedings of
the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , pages 3903–3911.
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting
Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and

--- PAGE 10 ---
Jingren Zhou. 2023a. Scaling relationship on learn-
ing mathematical reasoning with large language mod-
els.
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,
Songfang Huang, and Fei Huang. 2023b. Rrhf: Rank
responses to align language models with human feed-
back without tears.Dataset Amount
ultrachat 18,588
sharedgpt 10432
wizardlm(sharedgpt) 5325
wizardlm(alpaca) 5145
alpaca 2186
repair 1034
openchat 1033
flan 862
math 849
unnatural 582
dmcc 573
dolly 560
oasst 183
lima 70
mbpp 43
Table 3: Composition of D IVINSTRUCT
A Datasets
DIVINSTRUCT is a diverse mix instruction set from
multiple open-source datasets with careful decon-
tamination on all benchmarks evaluated in this
work. The detailed composition of DIVINSTRUCT
is report in Tab. 3.
