Đang được xem xét như một bài báo hội nghị

TỐI ƯU HÓA CHÍNH SÁCH PROXIMAL THEO CẶP: KHAI THÁC PHẢN HỒI TƯƠNG ĐỐI CHO VIỆC ĐIỀU CHỈNH LLM

Tianhao Wu∗, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen, Kannan Ramchandran & Jiantao Jiao
Đại học California, Berkeley

TÓM TẮT

Các Mô hình Ngôn ngữ Lớn (LLM) có thể thu được kiến thức rộng lớn về thế giới thông qua việc tiền huấn luyện trên các kho dữ liệu lớn. Tuy nhiên, do tiếp xúc với dữ liệu chất lượng thấp, LLM có thể thể hiện hành vi có hại mà không phù hợp với các giá trị của con người. Phương pháp chiếm ưu thế để định hướng LLM hướng tới hành vi có lợi liên quan đến Học Tăng cường với Phản hồi của Con người (RLHF), với Tối ưu hóa Chính sách Proximal (PPO) phục vụ như bộ tối ưu RL mặc định. Mặc dù hiệu quả, PPO có những hạn chế khi tối ưu hóa phần thưởng được huấn luyện từ loss dựa trên so sánh. Chủ yếu, PPO không bất biến đối với các hàm phần thưởng tương đương chứa thông tin ưu tiên giống hệt nhau do nhu cầu hiệu chỉnh quy mô phần thưởng. Ngoài ra, nhu cầu của PPO về các cập nhật theo từng token đưa ra độ phức tạp trong cả xấp xỉ hàm và thiết kế thuật toán so với tối ưu hóa theo quỹ đạo. Bài báo này đề xuất một khung mới, học tăng cường với phản hồi tương đối, và một thuật toán gradient chính sách theo quỹ đạo mới, Tối ưu hóa Chính sách Proximal Theo cặp (P3O) hoạt động trực tiếp trên phần thưởng so sánh. Chúng tôi chứng minh về mặt lý thuyết rằng P3O bất biến đối với phần thưởng tương đương và tránh được độ phức tạp của PPO. Các đánh giá thực nghiệm cho thấy P3O vượt trội hơn PPO trong sự đánh đổi KL-Phần thưởng và có thể phù hợp với sở thích của con người cũng như hoặc tốt hơn các phương pháp trước đó. Tóm lại, công trình này giới thiệu một phương pháp đơn giản nhưng hiệu quả để điều chỉnh LLM với sở thích của con người thông qua phản hồi tương đối.

1 GIỚI THIỆU

Các Mô hình Ngôn ngữ Lớn (LLM) đã đạt được tiến bộ đáng kể, ảnh hưởng sâu sắc đến cộng đồng AI (Chowdhery et al., 2022; Brown et al., 2020; Touvron et al., 2023; Bubeck et al., 2023). Tuy nhiên, do sự phụ thuộc vào các kho dữ liệu khổng lồ từ internet, bao gồm tỷ lệ cao dữ liệu chất lượng thấp, LLM có khả năng biểu hiện hành vi không mong muốn. Những điều này bao gồm việc bịa đặt sự thật, tạo ra văn bản thiên vị hoặc độc hại, và thậm chí nội dung có hại đối với con người (Perez et al., 2022; Ganguli et al., 2022). Do đó, việc điều chỉnh LLM với các giá trị của con người là rất quan trọng, ví dụ: hữu ích, trung thực, vô hại (Bai et al., 2022a).

Một phương pháp chiếm ưu thế trong lĩnh vực Điều chỉnh AI cho LLM có tên là Học Tăng cường với Phản hồi của Con người (RLHF) liên quan đến quy trình ba giai đoạn: tinh chỉnh có giám sát, học phần thưởng, và tinh chỉnh học tăng cường (RL) (Ziegler et al., 2019; Ouyang et al., 2022). Trong giai đoạn thứ ba quan trọng, Tối ưu hóa Chính sách Proximal (PPO) được áp dụng rộng rãi như bộ tối ưu RL mặc định (Schulman et al., 2017). Mặc dù hiệu quả được ca ngợi của PPO, các nghiên cứu gần đây đã nêu bật một số câu hỏi thú vị và các vấn đề tiềm ẩn cần được chú ý thêm:

Tính bất ổn của PPO. Mặc dù hiệu quả được ca ngợi, các nghiên cứu gần đây đã xác định tính bất ổn liên quan đến PPO. Các yếu tố như chuẩn hóa phần thưởng, mở rộng phần thưởng, cắt phần thưởng, kiểm soát KL, chuẩn hóa lợi thế và khởi tạo critic (Zheng et al., 2023; Engstrom et al., 2020) có thể góp phần vào tính bất ổn này. Hơn nữa, chúng tôi xác định một nguồn bất ổn khác: có sự không nhất quán khi tối ưu hóa phần thưởng được huấn luyện với mô hình phần thưởng so sánh Bradley-Terry Loss (BTL). Về bản chất, BTL bất biến đối với sự dịch chuyển hằng số trong khi PPO thì không. Điều này có nghĩa là ngay cả khi hai mô hình phần thưởng mang thông tin giống hệt nhau về sở thích của con người, việc tối ưu hóa chúng bằng PPO có thể tạo ra kết quả khác nhau. (Phần 4.4)

Theo quỹ đạo so với Theo token. Một căng thẳng phổ biến trong việc tinh chỉnh LLM bằng RL xoay quanh việc liệu việc điều chỉnh nên theo quỹ đạo hay theo token. Sự hai mặt này phát sinh chủ yếu từ bản chất của giai đoạn huấn luyện mô hình phần thưởng trong đó mô hình phần thưởng được huấn luyện bằng cách sử dụng tập dữ liệu so sánh với những người gán nhãn con người xếp hạng nhiều hoàn thành của một lời nhắc duy nhất. Lý do rõ ràng để thực hiện phương pháp gián tiếp này dựa trên so sánh theo quỹ đạo là thường khó khăn cho những người gán nhãn để thực hiện phân công điểm trực tiếp hoặc cung cấp so sánh ngoài góc độ theo quỹ đạo. Một ngoại lệ có thể là các nhiệm vụ cụ thể với ít mơ hồ hơn, chẳng hạn như các bài toán (Lightman et al., 2023), nơi việc tính điểm một phần cho các câu trả lời đúng một phần là khả thi. Khó khăn này phát sinh từ thực tế rằng mô hình phần thưởng chỉ trả về một phần thưởng vô hướng duy nhất sau khi nhận toàn bộ phản hồi (khi mô hình gặp token <eos>). Với độ phức tạp này, hai quan điểm chiếm ưu thế xuất hiện: Liệu việc tạo ngôn ngữ nên được tiếp cận như một Contextual Bandit (CB) hay một Quá trình Quyết định Markov (MDP)? Mặc dù phương pháp phổ biến là sử dụng PPO để thực hiện cập nhật theo token, nó đưa ra độ phức tạp bổ sung cả trong xấp xỉ hàm và trong thiết kế thuật toán, tức là PPO yêu cầu học thêm một hàm giá trị V và sử dụng Ước lượng Lợi thế Tổng quát (GAE) (Schulman et al., 2015b) để phân phối lại phần thưởng thưa thớt cho tất cả các token.

Trong bài báo này, chúng tôi cung cấp những hiểu biết mới để giải quyết hai vấn đề trên. Chúng tôi tóm tắt các đóng góp chính của chúng tôi như:

(1) Chúng tôi định nghĩa một mối quan hệ tương đương cho các hàm phần thưởng được huấn luyện từ sở thích của con người. Chúng tôi xác định rằng BTL bất biến dưới mối quan hệ tương đương này, trong khi PPO thì không.

(2) Chúng tôi quan sát thấy rằng Tối ưu hóa Ưu tiên Trực tiếp (DPO) (Rafailov et al., 2023) có thể được sử dụng kết hợp với một mô hình phần thưởng được huấn luyện, dẫn đến DPO trực tuyến. Hơn nữa, chúng tôi chứng minh rằng DPO trực tuyến vẫn bất biến dưới mối quan hệ tương đương phần thưởng. Bằng chứng thực nghiệm cho thấy DPO trực tuyến có thể đạt được phần thưởng cao hơn trong khi kém hiệu quả KL hơn PPO.

(3) Chúng tôi giới thiệu một thuật toán gradient chính sách mới, Tối ưu hóa Chính sách Proximal Theo cặp (P3O) dưới khung Học Tăng cường với Phản hồi Tương đối (Hình 1). Thuật toán của chúng tôi phù hợp hoàn hảo với bản chất so sánh của mô hình phần thưởng, tránh các phức tạp như ước lượng hàm V, GAE và các kỹ thuật chuẩn hóa khác nhau (Zheng et al., 2023). Các đánh giá thực nghiệm cho thấy P3O luôn vượt trội hơn PPO và DPO về sự đánh đổi KL-Phần thưởng và Đánh giá GPT-4.

2 CÔNG TRÌNH LIÊN QUAN

Những nỗ lực đáng kể đã được thực hiện hướng tới việc điều chỉnh LLM với các giá trị của con người. Các chiến lược điều chỉnh này rộng rãi thuộc về hai loại: huấn luyện ngoại tuyến và huấn luyện trực tuyến.

Huấn luyện ngoại tuyến thường liên quan đến một tập dữ liệu tĩnh, và không yêu cầu đánh giá hoặc tạo thêm. Ví dụ, Thoppilan et al. (2022); Gunasekar et al. (2023) sử dụng tinh chỉnh hướng dẫn để cập nhật mô hình trên một tập dữ liệu chất lượng cao được thiết kế cho một nhiệm vụ downstream cụ thể quan tâm. Snell et al. (2022) đề xuất sử dụng Q-Learning ngoại tuyến để học một thuật ngữ bổ sung cho việc giải mã. Trong khi Rafailov et al. (2023) giới thiệu DPO, một phương pháp ngoại tuyến có thể trực tiếp điều chỉnh LM với dữ liệu ưu tiên của con người, rút ra từ nghiệm dạng đóng của bài toán Contextual Bandit với kiểm soát KL. Cũng có các phương pháp như PRO (Song et al., 2023) và RRHF (Yuan et al., 2023) tinh chỉnh mô hình dựa trên xếp hạng của phần thưởng.

Công trình của chúng tôi được phân loại dưới huấn luyện trực tuyến, bao gồm một vòng lặp tạo phản hồi mới từ chính sách được cập nhật, đánh giá chúng với mô hình phần thưởng và cập nhật chính sách. Phương pháp chiếm ưu thế hiện tại RLHF dựa trên các phương pháp RL trực tuyến như PPO (Schulman et al., 2017), A2C (Mnih et al., 2016) hoặc các biến thể của chúng (Ramamurthy et al., 2022; Zhu et al., 2023b). Cũng có một số phương pháp khác lệch khỏi tiêu chuẩn này. Ví dụ, Gulcehre et al. (2023) giới thiệu ReST, sử dụng RL ngoại tuyến thay vì RL trực tuyến trong giai đoạn cải thiện chính sách; Dong et al. (2023) đề xuất RAFT, tinh chỉnh lặp đi lặp lại chính sách trên các phản hồi được tạo bởi chính sách Best-of-N. Một paradigm khác song song với RLHF là Học Tăng cường với Phản hồi AI (RLAIF) (Bai et al., 2022b; Lee et al., 2023), nhằm tự cải thiện AI. RLAIF thay thế vai trò của con người bằng AI trong phản hồi và mang lại kết quả tương đương với các mô hình quy mô nhỏ hơn.

Ngoài bối cảnh ngôn ngữ, học chính sách dựa trên ưu tiên đã được khám phá trong cả bandit và RL. Contextual dueling bandit (Dudík et al., 2015; Yue et al., 2012) sử dụng ưu tiên hoặc xếp hạng của hành động để điều chỉnh chính sách, thay vì phần thưởng. Tương tự, PbRL (Jain et al., 2013; Busa-Fekete et al., 2014; Christiano et al., 2017; Sadigh et al., 2017; Kupcsik et al., 2018) học từ ưu tiên nhị phân được tạo bởi một hàm điểm số không xác định nào đó. Công trình của chúng tôi có điểm tương đồng với gradient chính sách theo cặp được phác thảo trong Xu et al. (2020), mặc dù chúng tôi tích hợp kỹ thuật cắt quan trọng và kiểm soát KL phù hợp hơn với bối cảnh tạo ngôn ngữ.

3 KIẾN THỨC CƠ BẢN

Trong phần này, chúng tôi xem xét đường ống RLHF và thảo luận về cách tạo ngôn ngữ phù hợp với bối cảnh CB và RL (Sutton & Barto, 2018; Yang et al., 2021; Lattimore & Szepesvári, 2020; Wu et al., 2022).

• Giai đoạn SFT (Tinh chỉnh Có giám sát): Giai đoạn này bắt đầu với một LM được tiền huấn luyện, và sau đó được tinh chỉnh với học có giám sát (thường là loss likelihood tối đa) trên một tập dữ liệu chất lượng cao cho nhiệm vụ downstream quan tâm. Kết quả của giai đoạn này được ký hiệu là πSFT.

• Giai đoạn Học Phần thưởng. Trong giai đoạn thứ hai, mô hình SFT được nhắc với các lời nhắc x để tạo ra các cặp câu trả lời y1,y2∼πSFT(y|x). Các cặp phản hồi sau đó được trình bày cho những người gán nhãn con người biểu thị ưu tiên cho một câu trả lời, được ký hiệu là yw≻yl|x, trong đó yw là câu được người gán nhãn ưa thích và yl là câu ít được ưa thích hơn. Dưới những ưu tiên này là mô hình phần thưởng tiềm ẩn không thể truy cập r∗(y,x). Theo mô hình Bradley-Terry (Bradley & Terry, 1952), phân phối ưu tiên con người p∗ có thể được biểu thị như:

p∗(y1≻y2|x) = exp(r∗(y1|x)) / (exp(r∗(y1|x)) + exp(r∗(y2|x))) = 1 / (1 + exp(r∗(y1|x)−r∗(y2|x)))

Giả sử truy cập vào một tập dữ liệu {(x(i),y(i)w,y(i)l)}Ni=1 được lấy mẫu từ p∗. Chúng tôi tham số hóa phần thưởng là rϕ và ước lượng nó qua likelihood tối đa:

LR = (1/N) ∑i=1 to N log σ(rϕ(y(i)w|x(i))−rϕ(y(i)l|x(i)))     (1)

trong đó σ là hàm sigmoid. rϕ được khởi tạo với πSFT được tăng cường bởi các lớp tuyến tính bổ sung ở trên cùng. Các ràng buộc như E[r(y|x)] = 0 có thể được tích hợp để giảm phương sai.

• Giai đoạn Tinh chỉnh RL. Các công trình trước đây công thức hóa bài toán tối ưu như:

max πθ Ex∼D,y∼πθ(·|x) [rϕ(y|x)−βDKL(πθ(·|x)∥πSFT(·|x))]     (2)

Thuật ngữ βDKL(πθ(·|x)∥πSFT(·|x)) được sử dụng để điều chỉnh độ lệch khỏi mô hình SFT, điều quan trọng để ngăn mô hình hoàn toàn quên kiến thức thế giới thu được trong giai đoạn tiền huấn luyện. Phương pháp tiêu chuẩn là trực tiếp sử dụng PPO (Schulman et al., 2017; Ouyang et al., 2022) để tối ưu phần thưởng được sửa đổi rϕ(y|x)−β(log πθ(y|x)−log πSFT(y|x)).

3.1 MDP SO VỚI CB

Tạo Ngôn ngữ được mô hình hóa như một MDP: Một mô hình ngôn ngữ nhận một chuỗi token (t1, ..., th) làm đầu vào và tạo ra phân phối cho token tiếp theo th+1. Không gian hành động của MDP này bao gồm tất cả các token tiềm năng, với trạng thái là sự nối của tất cả các token lịch sử cũng như lời nhắc. Quá trình chuyển tiếp trong mô hình là xác định, trạng thái tiếp theo bằng với sự nối của trạng thái hiện tại và token vừa được tạo. Chính thức,

P(sh+1 = (t1, ..., th+1)|sh = (t1, ..., th), ah+1 = th+1) = 1

Một episode kết thúc khi tổng token được tạo đạt đến giới hạn được định trước hoặc khi mô hình ngôn ngữ tạo ra token đặc biệt <eos>. Khi kết thúc một episode, toàn bộ chuỗi được chấm điểm bởi một mô hình phần thưởng, tạo ra một phần thưởng vô hướng r. PPO tuân theo công thức này, trong đó phần thưởng vô hướng duy nhất sẽ được gán cho token cuối cùng và kết hợp với hình phạt KL theo token để tạo thành phần thưởng cuối cùng. Để giải quyết thách thức phản hồi thưa thớt, PPO sử dụng Ước lượng Lợi thế Tổng quát (Schulman et al., 2015b) để phân bổ lại phần thưởng vô hướng trên tất cả các token.

Tạo Ngôn ngữ được mô hình hóa như một CB: Một mô hình ngôn ngữ nhận một lời nhắc x = (t1, ..., th) và tạo ra một phản hồi y = (th+1, ..., th'). Phản hồi hoặc kết thúc với token <eos> hoặc tiếp tục đến khi đạt giới hạn token được định trước. Trong bối cảnh này, mọi phản hồi có thể được coi như một hành động, và không có quá trình chuyển tiếp nào cả. Toàn bộ chuỗi được chấm điểm bởi một mô hình phần thưởng, tạo ra một phần thưởng vô hướng. Công thức này được áp dụng bởi (Rafailov et al., 2023; Zhu et al., 2023a) cũng như bài báo của chúng tôi.

4 THUẬT TOÁN

4.1 TỐI ƯU HÓA CHÍNH SÁCH PROXIMAL THEO CẶP (P3O)

Để dẫn xuất P3O, chúng tôi bắt đầu từ Vanilla Policy Gradient (VPG, Pseudocode 1) (Sutton et al., 1999; Schulman et al., 2017). Để đơn giản, chúng tôi bỏ qua lời nhắc x trong công thức và tập trung vào dẫn xuất cho bối cảnh bandit và mở rộng sang contextual bandit tổng quát hơn trong định lý 1.

Trong bối cảnh bandit, giả sử chúng ta đang cập nhật một chính sách được tham số hóa πθ với các hành động được ký hiệu là y. VPG nhằm ước lượng công thức sau với các mẫu:

∇LVPG = Ey∼πθ [r(y)∇log πθ(y)] = ∑y r(y)∇πθ(y)     (3)

Các công trình trước về PG đề xuất trừ một baseline b để giảm phương sai, dẫn đến

∇LVPG = ∑y (r(y)−b)∇πθ(y)

Chúng tôi thay lựa chọn phổ biến của baseline, b = ∑y r(y)πθ(y):

∇LVPG = ∑y1 r(y1)∇πθ(y1)−∑y1,y2 r(y2)πθ(y2)∇πθ(y1)
       = ∑y1,y2 r(y1)πθ(y2)∇πθ(y1)−∑y1,y2 r(y2)πθ(y2)∇πθ(y1)
       = ∑y1,y2 (r(y1)−r(y2))πθ(y2)∇πθ(y1)     (4)

Phương trình (4) đưa ra một biểu thức phụ thuộc trực tiếp vào sự khác biệt tương đối r(y1)−r(y2). Chúng tôi tiếp tục giới thiệu πθold và viết lại Eq (4) với importance sampling:

∇LVPG = Ey1,y2∼πθold [(r(y1)−r(y2)) πθ(y2)/πθold(y2) ∇πθ(y1)/πθold(y1)]

Hoán đổi hành động y1,y2 và lấy trung bình cùng nhau, chúng ta có biểu thức gradient đối xứng mà chúng tôi gọi là Pairwise Policy Gradient (PPG):

∇LPPG = Ey1,y2∼πθold [(r(y1)−r(y2)) (πθ(y2)/πθold(y2) ∇πθ(y1)/πθold(y1) − πθ(y1)/πθold(y1) ∇πθ(y2)/πθold(y2))/2]     (5)

= Ey1,y2∼πθold [(r(y1)−r(y2)) πθ(y1)/πθold(y1) πθ(y2)/πθold(y2) ∇(log πθ(y1)/πθ(y2))/2]     (6)

Tổng quát hóa trực tiếp của nó đối với contextual bandit là định lý sau:

Định lý 1 (Pairwise Policy Gradient với Importance Sampling). Đối với bất kỳ lời nhắc x nào, gradient chính sách có thể được biểu thị là ∇LVPG = Ex∼D [∇LVPG(x)], trong đó ∇LVPG(x) có thể được biểu thị là:

Ey1,y2∼πθold [(r(y1|x)−r(y2|x)) (πθ(y2|x)/πθold(y2|x) ∇πθ(y1|x)/πθold(y1|x) − πθ(y1|x)/πθold(y1|x) ∇πθ(y2|x)/πθold(y2|x))/2]

= Ey1,y2∼πθold [(r(y1|x)−r(y2|x)) πθ(y1|x)/πθold(y1|x) πθ(y2|x)/πθold(y2|x) ∇(log πθ(y1|x)/πθ(y2|x))/2]

4.2 KẾT HỢP VỚI CLIPPING

Một sửa đổi đáng kể của PPO so với VPG là cắt hàm loss, điều này không khuyến khích các cập nhật lớn cho chính sách. Cụ thể, PPO yêu cầu tỷ lệ πθ/πθold nên gần với 1, được hướng dẫn bởi dấu hiệu của hàm Advantage Adv. Hiểu điều này theo nghĩa trực quan, nếu Adv(y|x)>0, điều này có nghĩa là thực hiện hành động y có lợi trên trung bình. Do đó, chúng ta muốn tăng xác suất πθ(y|x). Tuy nhiên, nếu tỷ lệ chính sách πθ/πθold vượt quá 1+ε, chúng ta coi sự thay đổi là đủ và dừng gradient; nếu không, gradient được tính toán để học thêm. Ngược lại, nếu Adv(y|x)<0, chúng ta cố gắng tối ưu tỷ lệ hướng tới 1−ε thay vì 1+ε. Trực giác này hướng dẫn chúng ta dẫn xuất hai biến thể của thuật toán, được phân biệt bởi việc clipping được áp dụng riêng biệt hay chung cho các hành động y1 và y2.

Clipping Riêng biệt (Phiên bản 1): Đối với {i,j}={1,2},

LP3Oi(x) = Ey1,y2∼πθold [sg((r(yi|x)−r(yj|x)) πθ(yj|x)/πθold(yj|x) πθ(yi|x)/πθold(yi|x))]

LP3Oi,clip(x) = Ey1,y2∼πθold [sg((r(yi|x)−r(yj|x)) πθ(yj|x)/πθold(yj|x) clip(πθ(yi|x)/πθold(yi|x), 1−ε, 1+ε))]

LP3Osep = Ex∼D [(min(LP3O1(x), LP3O1,clip(x)) + min(LP3O2(x), LP3O2,clip(x)))/2]

Clipping Chung (Phiên bản 2):

LP3O(x) = Ey1,y2∼πθold [sg((r(y1|x)−r(y2|x)) πθ(y1|x)/πθold(y1|x) πθ(y2|x)/πθold(y2|x) log(πθ(y1|x)/πθ(y2|x)))]

LP3Oclip(x) = Ey1,y2∼πθold [sg((r(y1|x)−r(y2|x)) πθ(y1|x)/πθold(y1|x) πθ(y2|x)/πθold(y2|x) × clip(log(πθ(y1|x)/πθ(y2|x)), log(πθold(y1|x)/πθold(y2|x))−ε, log(πθold(y1|x)/πθold(y2|x))+ε))]

LP3Ojoi = Ex∼D [min(LP3O(x), LP3Oclip(x))]

sg và clip tương ứng đề cập đến toán tử stop-gradient và clip. Chúng tôi trình bày toàn bộ đường ống của P3O trong Pseudocode 2.

4.3 MỐI QUAN HỆ VỚI PPO VÀ DPO

Trong phần này, chúng tôi thảo luận ngắn gọn về mối quan hệ của thuật toán được đề xuất và hai thuật toán hiện có, PPO và DPO. Chúng tôi giải thích thêm về các tính chất lý thuyết của chúng trong Phần 4.4.

So sánh với PPO: Mục tiêu của PPO, trước khi clipping được áp dụng và giảm xuống bối cảnh contextual bandit, có thể được biểu thị như sau:

LPPOno clip = −Ey∼πθold(·|x) [(r(y|x)−Vϕ(x)) πθ(y|x)/πθold(y|x)]

Trong đó Vϕ(x) là một proxy cho hàm giá trị Vπθold = Ey∼πθold [r(y|x)]. Trái ngược với PPO, P3O mở rộng Vϕ(x) bằng cách sử dụng một mẫu khác y' để xây dựng một ước lượng không thiên lệch, bỏ qua nhu cầu học một hàm giá trị V bổ sung, dẫn đến kết quả ổn định hơn.

So sánh với DPO: Giống như P3O, DPO cũng áp dụng công thức contextual bandit (CB). Gradient của hàm mục tiêu DPO có dạng sau:

∇LDPO(x,yw,yl) = −βσ(β log πθ(yl|x)/πSFT(yl|x) − β log πθ(yw|x)/πSFT(yw|x)) ∇(log πθ(yw|x)/πθ(yl|x))/2

Hướng gradient của DPO tương tự với công thức của chúng tôi trong Định lý 1. Tuy nhiên, DPO sử dụng một trọng số khác trước gradient. Chúng tôi giả thuyết rằng lý do DPO thường đạt được phần thưởng cao hơn nhưng kém về kiểm soát KL (Hình 2 và 3) so với PPO là do DPO điều chỉnh chính sách hướng tới chính sách mục tiêu trong khi không trực tiếp xem xét phần thưởng. Không giống PPO và P3O, áp dụng gradient chính sách dựa trên ý tưởng cải thiện chính sách nghiêm ngặt cho mỗi cập nhật gradient (Schulman et al., 2015a), DPO điều chỉnh chính sách qua một "khoảng cách" thay thế, trong đó các bước trung gian không được đảm bảo tối đa hóa sự đánh đổi KL-Phần thưởng. Chúng tôi lưu ý rằng P3O kết hợp lợi ích của PPO và DPO, cung cấp cải thiện chính sách được đảm bảo tương tự gradient chính sách.

4.4 TƯƠNG ĐƯƠNG PHẦN THƯỞNG & TÍNH BẤT ỔN CỦA PPO

Trong phần này, chúng tôi định nghĩa chính thức khái niệm tương đương phần thưởng (Định nghĩa 1). Chúng tôi chỉ ra rằng BTL bất biến dưới mối quan hệ tương đương này trong bổ đề 1. Sau đó chúng tôi thảo luận tại sao nó dẫn đến một tính chất mong muốn có tên là bất biến (Định nghĩa 2) mà chúng ta muốn các thuật toán RL thỏa mãn. Cuối cùng, chúng tôi trình bày định lý chính (Định lý 2) cho thấy PPO không thỏa mãn tính chất này, góp phần vào tính bất ổn của nó.

Định nghĩa 1 (Tương đương Phần thưởng). Hai hàm phần thưởng r(y|x) và r'(y|x) được gọi là tương đương, ký hiệu là r∼r', nếu và chỉ nếu tồn tại một hàm δ(x) chỉ phụ thuộc vào lời nhắc x, sao cho đối với mọi cặp lời nhắc và phản hồi (x,y),

r(y|x)−r'(y|x) = δ(x)

Lớp tương đương liên quan đến phần thưởng r được biểu thị là [r].

Lưu ý rằng các loss so sánh như loss Bradley-Terry và loss Plackett-Luce, không bị ảnh hưởng bởi sự dịch chuyển trong phần thưởng của lời nhắc như trong định nghĩa 1. Quan sát này dẫn đến Bổ đề sau:

Bổ đề 1 (Bất biến của BTL). Đối với hai hàm phần thưởng thỏa mãn r∼r', cả hai đều mang lại loss giống hệt nhau cho bất kỳ cặp phản hồi nào (hoặc K phản hồi) dưới Bradley-Terry Loss (hoặc Plackett-Luce Loss).

Bổ đề 1 nhấn mạnh rằng thông tin duy nhất chúng ta có thể học từ dữ liệu ưu tiên là sự khác biệt phần thưởng của hai phản hồi cho cùng một lời nhắc. Điều này có nghĩa là việc so sánh trực tiếp các phản hồi xuất phát từ các lời nhắc khác nhau nên được tránh. Điều này là do chúng ta có thể tạo ra một hàm tùy ý được ký hiệu là δ và thay thế r̂ bằng r̂+δ giống hệt, trong khi lật dấu của r̂(y|x)−r̂'(y'|x'). Kết quả là, một thuật toán lý tưởng nên chỉ tập trung vào thông tin liên quan trong hàm phần thưởng, lọc ra tiếng ồn được đại diện bởi δ. Điều này dẫn đến định nghĩa sau:

Định nghĩa 2 (Bất biến). Một thuật toán được gọi là bất biến đối với quan hệ tương đương "∼", nếu đối với bất kỳ hai hàm phần thưởng tương đương r∼r' và một tập cố định các cặp lời nhắc và phản hồi, thuật toán thực hiện các cập nhật giống hệt nhau cho chính sách.

Để minh họa định nghĩa 2, giả sử chúng ta có hai hàm phần thưởng tương đương r̂ và r̂' = r̂+δ. Đáng chú ý, ngay cả khi được khởi tạo với cùng một seed ngẫu nhiên, PPO có thể dẫn đến các cập nhật khác biệt cho một batch giống hệt nhau. Hành vi này có thể được quy cho sự phụ thuộc của PPO vào việc học một hàm V để ước lượng lợi thế. Trong trường hợp đơn giản nhất, nơi lợi thế được ước lượng qua TD một bước (Adv(y|x) = r(y|x)−V(x), tương ứng với λGAE = 0) và y là một token duy nhất, chúng ta nên mong đợi hàm lợi thế vẫn không thay đổi. Tuy nhiên, theo dẫn xuất

Advr̂(y|x) = Advr̂'(y|x)
⇐⇒ r̂(y|x)−Vr̂(x) = r̂'(y|x)−Vr̂'(x)
⇐⇒ Vr̂'(x)−Vr̂(x) = δ(x)

Chúng ta có thể thấy rằng ngay cả khi r̂ và r̂' tương đương, chúng mang lại các cập nhật khác nhau cho hàm V. Điều này dẫn đến định lý chính của chúng tôi (chứng minh đầy đủ trong Phụ lục B.2):

Định lý 2 (Không bất biến của PPO). P3O và DPO bất biến đối với "∼". Ngược lại, PPO thì không, với cùng khởi tạo của V.

5 THỰC NGHIỆM

Trong phần này, chúng tôi nghiên cứu thực nghiệm mức độ P3O có thể phù hợp với sở thích của con người. Chúng tôi tiến hành thực nghiệm trên hai nhiệm vụ RLHF được áp dụng rộng rãi, tóm tắt và hỏi đáp, và chúng tôi thấy rằng P3O đạt được hiệu suất tốt hơn về cả sự đánh đổi KL-Phần thưởng và chất lượng tạo, so với một số baseline mạnh. Chúng tôi sẽ giới thiệu ngắn gọn các nhiệm vụ, phương pháp so sánh, và đánh giá trong thực nghiệm của chúng tôi, và sau đó trình bày chi tiết về những phát hiện này.

Nhiệm vụ. Chúng tôi khám phá hai nhiệm vụ tạo văn bản mở khác nhau, tức là tóm tắt và hỏi đáp. Đối với cả hai nhiệm vụ, các thuật toán được cung cấp một mô hình phần thưởng được tiền huấn luyện từ một tập dữ liệu ưu tiên D={x(i),y(i)w,y(i)l}, và mục tiêu là có được một chính sách π(y|x) có thể tạo ra phản hồi y chất lượng cao với lời nhắc x. Trong tóm tắt, chúng tôi sử dụng tập dữ liệu TL;DR "quá dài; không đọc được" (Völske et al., 2017), trong đó x là một bài đăng diễn đàn từ Reddit, và y là tóm tắt tương ứng. Chúng tôi sử dụng mô hình SFT 6B CarperAI/openai summarize tldr sft làm chính sách ban đầu và EleutherAI/gpt-j-6b làm mô hình phần thưởng. Trong hỏi đáp, x là một truy vấn của con người, có thể đến từ các chủ đề đa dạng, và chính sách nên học tạo ra một phản hồi y hấp dẫn và hữu ích. Theo công trình trước, chúng tôi sử dụng tập dữ liệu Anthropic Helpful and Harmless (HH) (Bai et al., 2022a). Chúng tôi tinh chỉnh hai chính sách có kích thước {1B,6B}, Dahoas/pythia-1B-static-sft và Dahoas/pythia-6B-static-sft. Cả hai mô hình đã trải qua tinh chỉnh có giám sát với các cặp lời nhắc-phản hồi được gán nhãn, tương tự như giao thức trong Ouyang et al. (2022) và Ramamurthy et al. (2022). Đối với mô hình phần thưởng, chúng tôi sử dụng mô hình 6B Dahoas/gptj-rm-static được huấn luyện từ cùng tập dữ liệu dựa trên EleutherAI/gpt-j-6b như một proxy của sở thích con người.

Phương pháp. Chúng tôi so sánh hai phiên bản của P3O, P3O-V1 và P3O-V2, đại diện cho clipping riêng biệt và chung tương ứng, với một số phương pháp hiệu quả và đại diện cho việc điều chỉnh LLM. Chúng tôi bắt đầu với chính sách SFT được huấn luyện bằng tinh chỉnh có giám sát theo token. Nó chưa trải qua điều chỉnh thêm; Mọi phương pháp khác sử dụng mô hình SFT làm khởi tạo. Đối với các thuật toán RL, chúng tôi xem xét phương pháp chiếm ưu thế PPO (Schulman et al., 2017; Ouyang et al., 2022) với phần thưởng được chỉ định trong Eq (2). Chúng tôi tuân theo triển khai của trlx (Castricato et al., 2023). Bên cạnh đó, chúng tôi cũng xem xét DPO mới được đề xuất (Rafailov et al., 2023), một phương pháp tối ưu trực tiếp chính sách hướng tới nghiệm dạng đóng của việc tối đa hóa phần thưởng có ràng buộc KL. Mặc dù DPO được đề xuất như một phương pháp điều chỉnh ngoại tuyến, chúng tôi nhận thấy rằng chúng ta có thể làm cho nó trực tuyến với sự trợ giúp của một hàm phần thưởng proxy. (Chi tiết thêm có thể tìm thấy trong Phụ lục A.2)

Đánh giá. Lệch quá nhiều khỏi chính sách tham chiếu (ví dụ: mô hình SFT) sẽ dẫn đến chính sách trực tuyến cắt góc của mô hình phần thưởng và tạo ra các tiếp tục không mạch lạc, như được chỉ ra bởi các công trình trước (Ziegler et al., 2019). Gao et al. (2023) nghiên cứu luật tỷ lệ của việc tối ưu quá mức phần thưởng trong một thiết lập tổng hợp, nơi nhãn được cung cấp bởi một mô hình phần thưởng "tiêu chuẩn vàng". Họ phát hiện thực nghiệm rằng phần thưởng vàng có thể được xấp xỉ bởi một dạng hàm đơn giản liên quan đến phân kỳ KL căn bậc hai từ chính sách tham chiếu. Do đó, việc cân bằng sự đánh đổi giữa phân kỳ KL và phần thưởng tiệm cận là quan trọng, và chúng tôi đo hiệu quả của mỗi thuật toán bằng biên của phần thưởng đạt được và phân kỳ KL từ chính sách tham chiếu (Biên KL-Phần thưởng). Để đánh giá trực tiếp chất lượng của các phản hồi được tạo, chúng tôi cũng thực hiện So sánh Trực tiếp giữa mọi cặp thuật toán trong tập dữ liệu HH. Chúng tôi sử dụng hai thước đo để đánh giá: (1) Phần thưởng, mục tiêu được tối ưu trong quá trình RL trực tuyến, (2) GPT-4, như một proxy trung thực cho đánh giá con người về tính hữu ích của phản hồi. Đối với thước đo sau, chúng tôi sẽ chỉ ra rằng các nghiên cứu trước cho thấy LLM có thể là những đánh giá viên tự động tốt hơn các thước đo hiện có (Chen et al., 2023), và các phán quyết GPT-4 tương quan mạnh với con người, với sự đồng ý của con người với GPT-4 thường tương tự hoặc cao hơn sự đồng ý giữa các người chú thích (Rafailov et al., 2023). Chi tiết bổ sung có thể tìm thấy trong Phụ lục C.

5.1 BIÊN KL-PHẦN THƯỞNG

Chúng tôi tiến hành thực nghiệm trên cả tập dữ liệu TL;DR và HH để đánh giá hiệu quả của các thuật toán điều chỉnh trong việc tối ưu phần thưởng trong khi hạn chế độ lệch chính sách khỏi tham chiếu. Hình 2 và 3 chứng minh biên KL-Phần thưởng cho TL;DR và HH tương ứng. Mỗi điểm đại diện cho đánh giá trung bình trên các lời nhắc kiểm tra ở mỗi khoảng 500 bước. Trục x đại diện cho phân kỳ KL cấp chuỗi trung bình DKL(πθ∥πSFT), trong khi trục y biểu thị phần thưởng trung bình được cung cấp bởi mô hình phần thưởng proxy. Đối với nhiệm vụ tóm tắt, chúng tôi thấy rằng P3O-V1 có thể đạt được phần thưởng cao hơn một chút so với P3O-V2, trong khi có sự đánh đổi KL-Phần thưởng tệ hơn. Do đó, chỉ P3O-V2 được bao gồm trong Hình 2 để so sánh. Chúng tôi thấy rằng P3O-V2 có thể tạo ra gần như cùng phần thưởng cao nhất trong khi duy trì hiệu quả KL vượt trội. DPO, mặc dù hội tụ nhanh hơn, thể hiện phân kỳ KL cao hơn 25% so với P3O-V2 dưới cùng phần thưởng. Đối với nhiệm vụ hỏi đáp, P3O-V1 và P3O-V2 có biên nghiêm ngặt chiếm ưu thế hơn PPO và DPO tương ứng trong cả hai kích thước mô hình, được hiển thị bởi Hình 3. Những phát hiện thực nghiệm thiết lập sự đánh đổi vượt trội của P3O giữa KL và Phần thưởng so với các baseline khác, cung cấp phần thưởng cao hơn đáng kể trong phạm vi 0.1-0.3.

5.2 SO SÁNH TRỰC TIẾP

Để xác minh độ tin cậy của kết quả trước, chúng tôi tiến hành so sánh trực tiếp giữa mỗi cặp thuật toán trong số {P3O, DPO, PPO, SFT}. Vì biên KL-Phần thưởng chỉ ra rằng clipping chung (P3O-V2) tạo ra kết quả ổn định hơn so với clipping riêng biệt (P3O-V1), chúng tôi chỉ xem xét P3O-V2 trong phần này và gọi nó là P3O. Chúng tôi lấy mẫu hoàn thành từ các chính sách khác nhau trên tập kiểm tra của tập dữ liệu HH ở nhiệt độ mặc định 1.0, và chúng tôi tính tỷ lệ thắng theo cặp trung bình bằng cách sử dụng (1) phần thưởng và (2) GPT-4 làm đánh giá viên. Các nghiên cứu trước (Chen et al., 2023; Rafailov et al., 2023) đã cho thấy GPT-4 là một proxy trung thực cho sở thích con người và được áp dụng rộng rãi cho các so sánh. Lời nhắc được sử dụng để đánh giá được trình bày trong Phụ lục C.4.

Hình 4 trình bày kết quả so sánh theo cặp toàn diện, cả qua phần thưởng proxy và GPT-4. Xếp hạng phân kỳ KL và phần thưởng trung bình của các mô hình này là DPO > P3O > PPO > SFT. Mặc dù DPO vượt trội một chút so với P3O về phần thưởng, nó có phân kỳ KL cao hơn đáng kể (Bảng 1), điều này có thể có hại cho chất lượng tạo. Kết quả là, DPO có tỷ lệ thắng phần thưởng 49.5% so với P3O nhưng chỉ 45.4% khi được đánh giá bởi GPT-4. So với các phương pháp khác, P3O thể hiện tỷ lệ thắng GPT-4 57.0% so với PPO và 69.3% so với SFT. Kết quả này nhất quán với những phát hiện của chúng tôi từ phần biên KL-Phần thưởng, khẳng định rằng P3O có thể phù hợp với sở thích con người tốt hơn so với các baseline trước đó.

6 KẾT LUẬN & CÔNG TRÌNH TƯƠNG LAI

Công trình này trình bày những hiểu biết mới về việc điều chỉnh các mô hình ngôn ngữ lớn với sở thích của con người thông qua học tăng cường. Chúng tôi giới thiệu khung Học Tăng cường với Phản hồi Tương đối, thống nhất các nguyên tắc cốt lõi của mô hình phần thưởng và tinh chỉnh RL. Dưới khung này, chúng tôi thiết kế một thuật toán gradient chính sách mới, P3O, dựa trên phản hồi theo cặp. Các đánh giá thực nghiệm chứng minh rằng P3O vượt trội hơn các phương pháp trước về biên KL-Phần thưởng cũng như tỷ lệ thắng GPT-4. P3O kế thừa lợi thế của các phương pháp gradient chính sách, trong khi duy trì sự đơn giản trong cả thiết kế thuật toán và xấp xỉ hàm.

Nhìn về phía trước, một số câu hỏi thú vị phát sinh để khám phá trong tương lai. Thứ nhất, chúng tôi nhằm hiểu tác động của việc tối ưu quá mức phần thưởng đối với các thuật toán RL dựa trên quỹ đạo và các thuật toán RL dựa trên token. Thứ hai, chúng tôi quan tâm đến việc liệu chúng ta có thể tổng quát hóa thuật toán gradient chính sách để chứa nhiều hơn hai phản hồi được xếp hạng, có khả năng cho phép sự đánh đổi tốt hơn giữa nỗ lực con người và điều chỉnh AI. Cuối cùng, chúng tôi muốn khám phá lợi ích của việc áp dụng thuật toán P3O trong các bối cảnh ngoài việc huấn luyện mô hình ngôn ngữ với phản hồi con người. Chúng tôi háo hức mong đợi điều tra những câu hỏi này trong công trình tương lai của chúng tôi.

TÀI LIỆU THAM KHẢO

[Phần tài liệu tham khảo được giữ nguyên tiếng Anh vì đây là danh sách trích dẫn học thuật tiêu chuẩn]

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.

[Tiếp tục với tất cả các tài liệu tham khảo khác...]
