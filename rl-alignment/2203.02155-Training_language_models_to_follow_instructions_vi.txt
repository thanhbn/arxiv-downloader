# 2203.02155.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2203.02155.pdf
# Kích thước tệp: 1797405 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Huấn luyện mô hình ngôn ngữ để tuân theo hướng dẫn với phản hồi từ con người

Long Ouyang Jeff Wu Xu Jiang Diogo Almeida Carroll L. Wainwright
Pamela Mishkin Chong Zhang Sandhini Agarwal Katarina Slama Alex Ray
John Schulman Jacob Hilton Fraser Kelton Luke Miller Maddie Simens
Amanda Askell Peter Welinder Paul Christiano
Jan Leike Ryan Lowe
OpenAI

Tóm tắt
Việc làm cho mô hình ngôn ngữ lớn hơn không nhất thiết làm cho chúng tốt hơn trong việc tuân theo ý định của người dùng. Ví dụ, các mô hình ngôn ngữ lớn có thể tạo ra đầu ra không trung thực, độc hại, hoặc đơn giản là không hữu ích cho người dùng. Nói cách khác, các mô hình này không được căn chỉnh với người dùng của chúng. Trong bài báo này, chúng tôi trình bày một hướng đi để căn chỉnh các mô hình ngôn ngữ với ý định của người dùng trên một loạt các nhiệm vụ bằng cách tinh chỉnh với phản hồi từ con người. Bắt đầu với một tập hợp các lời nhắc do người gán nhãn viết và các lời nhắc được gửi qua OpenAI API, chúng tôi thu thập một bộ dữ liệu các minh họa của người gán nhãn về hành vi mong muốn của mô hình, mà chúng tôi sử dụng để tinh chỉnh GPT-3 bằng học có giám sát. Sau đó chúng tôi thu thập một bộ dữ liệu xếp hạng các đầu ra của mô hình, mà chúng tôi sử dụng để tinh chỉnh thêm mô hình được giám sát này bằng học tăng cường từ phản hồi con người. Chúng tôi gọi các mô hình kết quả là InstructGPT. Trong các đánh giá của con người trên phân phối lời nhắc của chúng tôi, các đầu ra từ mô hình InstructGPT 1.3B tham số được ưa thích hơn các đầu ra từ GPT-3 175B, mặc dù có ít hơn 100 lần tham số. Hơn nữa, các mô hình InstructGPT cho thấy sự cải thiện về tính trung thực và giảm việc tạo ra đầu ra độc hại trong khi có suy giảm hiệu suất tối thiểu trên các bộ dữ liệu NLP công cộng. Mặc dù InstructGPT vẫn mắc những lỗi đơn giản, kết quả của chúng tôi cho thấy rằng tinh chỉnh với phản hồi từ con người là một hướng đi đầy hứa hẹn để căn chỉnh các mô hình ngôn ngữ với ý định của con người.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LM) có thể được "nhắc nhở" để thực hiện một loạt các nhiệm vụ xử lý ngôn ngữ tự nhiên (NLP), với một số ví dụ về nhiệm vụ làm đầu vào. Tuy nhiên, các mô hình này thường thể hiện những hành vi không mong muốn như bịa đặt sự thật, tạo ra văn bản thiên vị hoặc độc hại, hoặc đơn giản là không tuân theo hướng dẫn của người dùng (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021; Gehman et al., 2020). Điều này là do mục tiêu mô hình hóa ngôn ngữ được sử dụng cho nhiều LM lớn gần đây—dự đoán token tiếp theo trên một trang web từ internet—khác với mục tiêu "tuân theo hướng dẫn của người dùng một cách hữu ích và an toàn" (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022). Do đó, chúng tôi nói rằng mục tiêu mô hình hóa ngôn ngữ bị lệch căn chỉnh. Việc tránh những hành vi không mong muốn này đặc biệt quan trọng đối với các mô hình ngôn ngữ được triển khai và sử dụng trong hàng trăm ứng dụng.

Chúng tôi đạt được tiến bộ trong việc căn chỉnh các mô hình ngôn ngữ bằng cách huấn luyện chúng hành động theo ý định của người dùng (Leike et al., 2018). Điều này bao gồm cả những ý định rõ ràng như tuân theo hướng dẫn và những ý định ngầm như giữ sự trung thực, và không thiên vị, độc hại, hoặc có hại khác. Sử dụng ngôn ngữ của Askell et al. (2021), chúng tôi muốn các mô hình ngôn ngữ hữu ích (chúng nên giúp người dùng giải quyết nhiệm vụ của họ), trung thực (chúng không nên bịa đặt thông tin hoặc đánh lừa người dùng), và vô hại (chúng không nên gây ra tổn hại vật lý, tâm lý, hoặc xã hội cho con người hoặc môi trường). Chúng tôi trình bày việc đánh giá các tiêu chí này trong Phần 3.6.

Chúng tôi tập trung vào các phương pháp tinh chỉnh để căn chỉnh các mô hình ngôn ngữ. Cụ thể, chúng tôi sử dụng học tăng cường từ phản hồi con người (RLHF; Christiano et al., 2017; Stiennon et al., 2020) để tinh chỉnh GPT-3 để tuân theo một lớp rộng các hướng dẫn bằng văn bản (xem Hình 2). Kỹ thuật này sử dụng sở thích của con người làm tín hiệu phần thưởng để tinh chỉnh các mô hình của chúng tôi. Đầu tiên chúng tôi thuê một nhóm 40 nhà thầu để gán nhãn dữ liệu của chúng tôi, dựa trên hiệu suất của họ trong một bài kiểm tra sàng lọc (xem Phần 3.4 và Phụ lục B.1 để biết thêm chi tiết). Sau đó chúng tôi thu thập một bộ dữ liệu các minh họa do con người viết về hành vi đầu ra mong muốn trên các lời nhắc (chủ yếu bằng tiếng Anh) được gửi đến OpenAI API và một số lời nhắc do người gán nhãn viết, và sử dụng điều này để huấn luyện các đường cơ sở học có giám sát của chúng tôi. Tiếp theo, chúng tôi thu thập một bộ dữ liệu so sánh có nhãn con người giữa các đầu ra từ các mô hình của chúng tôi trên một tập hợp lớn hơn các lời nhắc API. Sau đó chúng tôi huấn luyện một mô hình phần thưởng (RM) trên bộ dữ liệu này để dự đoán đầu ra mô hình nào mà người gán nhãn của chúng tôi sẽ ưa thích. Cuối cùng, chúng tôi sử dụng RM này như một hàm phần thưởng và tinh chỉnh đường cơ sở học có giám sát của chúng tôi để tối đa hóa phần thưởng này bằng thuật toán PPO (Schulman et al., 2017). Chúng tôi minh họa quá trình này trong Hình 2. Thủ tục này căn chỉnh hành vi của GPT-3 với sở thích đã nêu của một nhóm người cụ thể (chủ yếu là người gán nhãn và nhà nghiên cứu của chúng tôi), thay vì bất kỳ khái niệm rộng hơn nào về "giá trị con người"; chúng tôi thảo luận thêm về điều này trong Phần 5.2. Chúng tôi gọi các mô hình kết quả là InstructGPT.

Chúng tôi chủ yếu đánh giá các mô hình của mình bằng cách để người gán nhãn của chúng tôi đánh giá chất lượng đầu ra mô hình trên tập kiểm tra của chúng tôi, bao gồm các lời nhắc từ khách hàng được giữ lại (những người không được đại diện trong dữ liệu huấn luyện). Chúng tôi cũng tiến hành các đánh giá tự động trên một loạt các bộ dữ liệu NLP công cộng. Chúng tôi huấn luyện ba kích thước mô hình (1.3B, 6B, và 175B tham số), và tất cả các mô hình của chúng tôi sử dụng kiến trúc GPT-3. Các phát hiện chính của chúng tôi như sau:

Người gán nhãn ưa thích đáng kể các đầu ra InstructGPT hơn các đầu ra từ GPT-3. Trên tập kiểm tra của chúng tôi, các đầu ra từ mô hình InstructGPT 1.3B tham số được ưa thích hơn các đầu ra từ GPT-3 175B, mặc dù có hơn 100 lần ít tham số hơn. Các mô hình này có cùng kiến trúc, và chỉ khác nhau ở chỗ InstructGPT được tinh chỉnh trên dữ liệu con người của chúng tôi. Kết quả này vẫn đúng ngay cả khi chúng tôi thêm lời nhắc few-shot vào GPT-3 để làm cho nó tốt hơn trong việc tuân theo hướng dẫn. Các đầu ra từ InstructGPT 175B của chúng tôi được ưa thích hơn các đầu ra GPT-3 175B 85±3% thời gian, và được ưa thích 71±4% thời gian so với GPT-3 175B few-shot. Các mô hình InstructGPT cũng tạo ra các đầu ra phù hợp hơn theo người gán nhãn của chúng tôi, và tuân theo các ràng buộc rõ ràng trong hướng dẫn một cách đáng tin cậy hơn.

Các mô hình InstructGPT cho thấy sự cải thiện về tính trung thực so với GPT-3. Trên benchmark TruthfulQA, InstructGPT tạo ra các câu trả lời trung thực và thông tin khoảng gấp đôi so với GPT-3. Kết quả của chúng tôi cũng mạnh mẽ trên tập con các câu hỏi không được chọn một cách đối kháng chống lại GPT-3. Trên các nhiệm vụ "miền đóng" từ phân phối lời nhắc API của chúng tôi, nơi đầu ra không nên chứa thông tin không có trong đầu vào (ví dụ: tóm tắt và QA miền đóng), các mô hình InstructGPT bịa đặt thông tin không có trong đầu vào ít hơn một nửa so với GPT-3 (tỷ lệ ảo giác 21% so với 41%, tương ứng).

InstructGPT cho thấy sự cải thiện nhỏ về độc tính so với GPT-3, nhưng không về thiên vị. Để đo độc tính, chúng tôi sử dụng bộ dữ liệu RealToxicityPrompts (Gehman et al., 2020) và tiến hành cả đánh giá tự động và con người. Các mô hình InstructGPT tạo ra khoảng 25% ít đầu ra độc hại hơn GPT-3 khi được nhắc hành xử một cách tôn trọng. InstructGPT không cải thiện đáng kể so với GPT-3 trên các bộ dữ liệu Winogender (Rudinger et al., 2018) và CrowSPairs (Nangia et al., 2020).

Chúng tôi có thể giảm thiểu suy giảm hiệu suất trên các bộ dữ liệu NLP công cộng bằng cách sửa đổi thủ tục tinh chỉnh RLHF của chúng tôi. Trong quá trình tinh chỉnh RLHF, chúng tôi quan sát suy giảm hiệu suất so với GPT-3 trên một số bộ dữ liệu NLP công cộng, đáng chú ý là SQuAD (Rajpurkar et al., 2018), DROP (Dua et al., 2019), HellaSwag (Zellers et al., 2019), và dịch thuật WMT 2015 Pháp sang Anh (Bojar et al., 2015). Đây là một ví dụ về "thuế căn chỉnh" vì thủ tục căn chỉnh của chúng tôi có cái giá là hiệu suất thấp hơn trên một số nhiệm vụ mà chúng tôi có thể quan tâm. Chúng tôi có thể giảm đáng kể suy giảm hiệu suất trên các bộ dữ liệu này bằng cách trộn các cập nhật PPO với các cập nhật tăng log likelihood của phân phối pretraining (PPO-ptx), mà không làm tổn hại điểm số sở thích của người gán nhãn.

Các mô hình của chúng tôi tổng quát hóa cho sở thích của người gán nhãn "được giữ lại" không tạo ra bất kỳ dữ liệu huấn luyện nào. Để kiểm tra khả năng tổng quát hóa của các mô hình của chúng tôi, chúng tôi tiến hành một thí nghiệm sơ bộ với người gán nhãn được giữ lại, và thấy rằng họ ưa thích các đầu ra InstructGPT hơn các đầu ra từ GPT-3 với tỷ lệ tương tự như người gán nhãn huấn luyện của chúng tôi. Tuy nhiên, cần nhiều công việc hơn để nghiên cứu cách các mô hình này hoạt động trên các nhóm người dùng rộng hơn, và cách chúng hoạt động trên các đầu vào mà con người không đồng ý về hành vi mong muốn.

Các bộ dữ liệu NLP công cộng không phản ánh cách các mô hình ngôn ngữ của chúng tôi được sử dụng. Chúng tôi so sánh GPT-3 được tinh chỉnh trên dữ liệu sở thích con người của chúng tôi (tức là InstructGPT) với GPT-3 được tinh chỉnh trên hai bộ biên dịch khác nhau của các nhiệm vụ NLP công cộng: FLAN (Wei et al., 2021) và T0 (Sanh et al., 2021) (đặc biệt là biến thể T0++). Các bộ dữ liệu này bao gồm nhiều nhiệm vụ NLP khác nhau, kết hợp với hướng dẫn ngôn ngữ tự nhiên cho mỗi nhiệm vụ. Trên phân phối lời nhắc API của chúng tôi, các mô hình FLAN và T0 của chúng tôi hoạt động hơi tệ hơn đường cơ sở SFT của chúng tôi, và người gán nhãn ưa thích InstructGPT đáng kể hơn các mô hình này (InstructGPT có tỷ lệ thắng 73.4±2% so với đường cơ sở của chúng tôi, so với 26.8±2% và 29.8±2% cho phiên bản T0 và FLAN của chúng tôi, tương ứng).

Các mô hình InstructGPT cho thấy khả năng tổng quát hóa đầy hứa hẹn với các hướng dẫn bên ngoài phân phối tinh chỉnh RLHF. Chúng tôi thăm dò định tính khả năng của InstructGPT, và thấy rằng nó có thể tuân theo hướng dẫn để tóm tắt mã, trả lời câu hỏi về mã, và đôi khi tuân theo hướng dẫn bằng các ngôn ngữ khác nhau, mặc dù các hướng dẫn này rất hiếm trong phân phối tinh chỉnh. Ngược lại, GPT-3 có thể thực hiện các nhiệm vụ này nhưng yêu cầu nhắc nhở cẩn thận hơn, và thường không tuân theo hướng dẫn trong các miền này. Kết quả này thú vị vì nó cho thấy rằng các mô hình của chúng tôi có thể tổng quát hóa khái niệm "tuân theo hướng dẫn." Chúng giữ lại một số căn chỉnh ngay cả trên các nhiệm vụ mà chúng nhận được rất ít tín hiệu giám sát trực tiếp.

InstructGPT vẫn mắc những lỗi đơn giản. Ví dụ, InstructGPT vẫn có thể không tuân theo hướng dẫn, bịa đặt sự thật, đưa ra câu trả lời né tránh dài dòng cho các câu hỏi đơn giản, hoặc không phát hiện ra hướng dẫn với tiền đề sai.

Nhìn chung, kết quả của chúng tôi chỉ ra rằng tinh chỉnh các mô hình ngôn ngữ lớn bằng sở thích con người cải thiện đáng kể hành vi của chúng trên một loạt rộng các nhiệm vụ, mặc dù vẫn còn nhiều công việc cần làm để cải thiện tính an toàn và độ tin cậy của chúng.

Phần còn lại của bài báo này được cấu trúc như sau: Đầu tiên chúng tôi trình bày chi tiết các công trình liên quan trong Phần 2, trước khi đi sâu vào phương pháp và chi tiết thí nghiệm của chúng tôi trong Phần 3, bao gồm phương pháp luận cấp cao của chúng tôi (3.1), chi tiết nhiệm vụ và bộ dữ liệu (3.3 và 3.2), thu thập dữ liệu con người (3.4), cách chúng tôi huấn luyện các mô hình của mình (3.5), và thủ tục đánh giá của chúng tôi (3.6). Sau đó chúng tôi trình bày kết quả của mình trong Phần 4, chia thành ba phần: kết quả trên phân phối lời nhắc API (4.1), kết quả trên các bộ dữ liệu NLP công cộng (4.2), và kết quả định tính (4.3). Cuối cùng chúng tôi đưa ra một cuộc thảo luận mở rộng về công việc của chúng tôi trong Phần 5, bao gồm ý nghĩa đối với nghiên cứu căn chỉnh (5.1), chúng tôi đang căn chỉnh với cái gì (5.2), hạn chế (5.3), câu hỏi mở (5.4), và tác động rộng hơn của công việc này (5.5).

2 Công trình liên quan

Nghiên cứu về căn chỉnh và học từ phản hồi con người. Chúng tôi xây dựng dựa trên các kỹ thuật trước đây để căn chỉnh các mô hình với ý định con người, đặc biệt là học tăng cường từ phản hồi con người (RLHF). Ban đầu được phát triển để huấn luyện robot đơn giản trong môi trường mô phỏng và trò chơi Atari (Christiano et al., 2017; Ibarz et al., 2018), gần đây nó đã được áp dụng để tinh chỉnh các mô hình ngôn ngữ để tóm tắt văn bản (Ziegler et al., 2019; Stiennon et al., 2020; Böhm et al., 2019; Wu et al., 2021). Công việc này lần lượt được ảnh hưởng bởi các công việc tương tự sử dụng phản hồi con người làm phần thưởng trong các miền như đối thoại (Jaques et al., 2019; Yi et al., 2019; Hancock et al., 2019), dịch thuật (Kreutzer et al., 2018; Bahdanau et al., 2016), phân tích cú pháp ngữ nghĩa (Lawrence và Riezler, 2018), tạo truyện (Zhou và Xu, 2020), tạo đánh giá (Cho et al., 2018), và trích xuất bằng chứng (Perez et al., 2019). Madaan et al. (2022) sử dụng phản hồi bằng văn bản từ con người để bổ sung lời nhắc và cải thiện hiệu suất của GPT-3. Cũng có công việc về căn chỉnh các agent trong môi trường dựa trên văn bản sử dụng RL với prior chuẩn mực (Nahian et al., 2021). Công việc của chúng tôi có thể được xem như một ứng dụng trực tiếp của RLHF để căn chỉnh các mô hình ngôn ngữ trên một phân phối rộng các nhiệm vụ ngôn ngữ.

Câu hỏi về ý nghĩa của việc các mô hình ngôn ngữ được căn chỉnh cũng đã nhận được sự chú ý gần đây (Gabriel, 2020). Kenton et al. (2021) lập danh mục các vấn đề hành vi trong LM là kết quả của sự sai lệch căn chỉnh, bao gồm tạo ra nội dung có hại và lạm dụng các mục tiêu được chỉ định sai. Trong công việc đồng thời, Askell et al. (2021) đề xuất các trợ lý ngôn ngữ như một bài kiểm tra cho nghiên cứu căn chỉnh, nghiên cứu một số đường cơ sở đơn giản, và các tính chất tỷ lệ của chúng.

Huấn luyện các mô hình ngôn ngữ để tuân theo hướng dẫn. Công việc của chúng tôi cũng liên quan đến nghiên cứu về tổng quát hóa qua nhiệm vụ trong các mô hình ngôn ngữ, nơi LM được tinh chỉnh trên một loạt rộng các bộ dữ liệu NLP công cộng (thường được gắn tiền tố với hướng dẫn phù hợp) và được đánh giá trên một tập khác các nhiệm vụ NLP. Đã có một loạt công việc trong miền này (Yi et al., 2019; Mishra et al., 2021; Wei et al., 2021; Khashabi et al., 2020; Sanh et al., 2021; Aribandi et al., 2021), khác nhau về dữ liệu huấn luyện và đánh giá, định dạng hướng dẫn, kích thước mô hình pretrained, và các chi tiết thí nghiệm khác. Một phát hiện nhất quán qua các nghiên cứu là việc tinh chỉnh LM trên một loạt các nhiệm vụ NLP, với hướng dẫn, cải thiện hiệu suất downstream của chúng trên các nhiệm vụ được giữ lại, cả trong setting zero-shot và few-shot.

Cũng có một dòng công việc liên quan về tuân theo hướng dẫn cho điều hướng, nơi các mô hình được huấn luyện để tuân theo hướng dẫn ngôn ngữ tự nhiên để điều hướng trong môi trường mô phỏng (Bahdanau et al., 2018; Abramson et al., 2020; Zhao et al., 2021).

Đánh giá tác hại của các mô hình ngôn ngữ. Một mục tiêu của việc sửa đổi hành vi của các mô hình ngôn ngữ là để giảm thiểu tác hại của các mô hình này khi chúng được triển khai trong thế giới thực. Những rủi ro này đã được ghi nhận rộng rãi (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021; Weidinger et al., 2021; Tamkin et al., 2021). Các mô hình ngôn ngữ có thể tạo ra đầu ra thiên vị (Dhamala et al., 2021; Liang et al., 2021; Manela et al., 2021; Caliskan et al., 2017; Kirk et al., 2021), rò rỉ dữ liệu riêng tư (Carlini et al., 2021), tạo ra thông tin sai lệch (Solaiman et al., 2019; Buchanan et al., 2021), và được sử dụng một cách độc hại; để có một đánh giá toàn diện, chúng tôi hướng độc giả đến Weidinger et al. (2021). Việc triển khai các mô hình ngôn ngữ trong các miền cụ thể tạo ra những rủi ro và thách thức mới, ví dụ trong các hệ thống đối thoại (Henderson et al., 2018; Xu et al., 2020; Dinan et al., 2019b). Có một lĩnh vực mới nhưng đang phát triển nhằm xây dựng các benchmark để đánh giá cụ thể những tác hại này, đặc biệt là xung quanh độc tính (Gehman et al., 2020), khuôn mẫu (Nadeem et al., 2020), và thiên vị xã hội (Dhamala et al., 2021; Nangia et al., 2020; Rudinger et al., 2018). Việc đạt được tiến bộ đáng kể trong những vấn đề này khó khăn vì các can thiệp có ý tốt vào hành vi LM có thể có tác dụng phụ (Welbl et al., 2021; Blodgett et al., 2020); ví dụ, nỗ lực giảm độc tính của LM có thể giảm khả năng mô hình hóa văn bản từ các nhóm dưới đại diện, do các mối tương quan thiên vị trong dữ liệu huấn luyện (Xu et al., 2021).

Sửa đổi hành vi của các mô hình ngôn ngữ để giảm thiểu tác hại. Có nhiều cách để thay đổi hành vi tạo sinh của các mô hình ngôn ngữ. Solaiman và Dennison (2021) tinh chỉnh LM trên một bộ dữ liệu nhỏ, hướng đến giá trị, điều này cải thiện khả năng tuân thủ các giá trị này của mô hình trong một nhiệm vụ trả lời câu hỏi. Ngo et al. (2021) lọc bộ dữ liệu pretraining bằng cách loại bỏ các tài liệu mà một mô hình ngôn ngữ có xác suất có điều kiện cao để tạo ra một tập các cụm từ kích hoạt do nhà nghiên cứu viết. Khi được huấn luyện trên bộ dữ liệu được lọc này, LM của họ tạo ra ít văn bản có hại hơn, với cái giá là giảm nhẹ hiệu suất mô hình hóa ngôn ngữ. Xu et al. (2020) sử dụng nhiều phương pháp khác nhau để cải thiện tính an toàn của chatbot, bao gồm lọc dữ liệu, chặn một số từ hoặc n-gram trong quá trình tạo sinh, token kiểm soát cụ thể về an toàn (Keskar et al., 2019; Dinan et al., 2019a), và thu thập dữ liệu human-in-the-loop (Dinan et al., 2019b). Các phương pháp khác để giảm thiểu thiên vị được tạo ra bởi LM sử dụng chính quy hóa word embedding (Liu et al., 2019; Huang et al., 2019), tăng cường dữ liệu (Liu et al., 2019; Dinan et al., 2019a; Sheng et al., 2019), chiếu không gian null để làm cho phân phối trên các token nhạy cảm trở nên đều hơn (Liang et al., 2021), các hàm mục tiêu khác nhau (Qian et al., 2019), hoặc phân tích trung gian nhân quả (Vig et al., 2020). Cũng có công việc về điều khiển việc tạo sinh của các mô hình ngôn ngữ bằng một mô hình ngôn ngữ thứ hai (thường nhỏ hơn) (Dathathri et al., 2019; Krause et al., 2020), và các biến thể của ý tưởng này đã được áp dụng để giảm độc tính của mô hình ngôn ngữ (Schick et al., 2021).

--- TRANG 6 ---
Bảng 1: Phân phối các danh mục trường hợp sử dụng từ bộ dữ liệu lời nhắc API của chúng tôi.

Trường hợp sử dụng (%)
Tạo sinh 45.6%
QA mở 12.4%
Động não 11.2%
Trò chuyện 8.4%
Viết lại 6.6%
Tóm tắt 4.2%
Phân loại 3.5%
Khác 3.5%
QA đóng 2.6%
Trích xuất 1.9%

Bảng 2: Các lời nhắc minh họa từ bộ dữ liệu lời nhắc API của chúng tôi. Đây là những ví dụ hư cấu được lấy cảm hứng từ việc sử dụng thực tế—xem thêm ví dụ trong Phụ lục A.2.1.

Trường hợp sử dụng: Lời nhắc
Động não: Liệt kê năm ý tưởng về cách lấy lại sự nhiệt tình cho sự nghiệp của tôi
Tạo sinh: Viết một câu chuyện ngắn về một con gấu đi đến bãi biển, kết bạn với một con hải cẩu, và sau đó về nhà.
Viết lại: Đây là tóm tắt của một vở kịch Broadway:
"""
{tóm tắt}
"""
Đây là đề cương của quảng cáo cho vở kịch đó:
"""

3 Phương pháp và chi tiết thí nghiệm

3.1 Phương pháp luận cấp cao

Phương pháp luận của chúng tôi tuân theo của Ziegler et al. (2019) và Stiennon et al. (2020), những người đã áp dụng nó trong các miền tiếp tục phong cách và tóm tắt. Chúng tôi bắt đầu với một mô hình ngôn ngữ pretrained (Radford et al., 2019; Brown et al., 2020; Fedus et al., 2021; Rae et al., 2021; Thoppilan et al., 2022), một phân phối các lời nhắc mà chúng tôi muốn mô hình của mình tạo ra đầu ra được căn chỉnh, và một nhóm người gán nhãn con người được huấn luyện (xem Phần 3.4 để biết chi tiết). Sau đó chúng tôi áp dụng ba bước sau (Hình 2).

Bước 1: Thu thập dữ liệu minh họa, và huấn luyện một chính sách có giám sát. Người gán nhãn của chúng tôi cung cấp các minh họa về hành vi mong muốn trên phân phối lời nhắc đầu vào (xem Phần 3.2 để biết chi tiết về phân phối này). Sau đó chúng tôi tinh chỉnh một mô hình GPT-3 pretrained trên dữ liệu này bằng học có giám sát.

Bước 2: Thu thập dữ liệu so sánh, và huấn luyện một mô hình phần thưởng. Chúng tôi thu thập một bộ dữ liệu so sánh giữa các đầu ra mô hình, nơi người gán nhãn chỉ ra đầu ra nào họ ưa thích cho một đầu vào nhất định. Sau đó chúng tôi huấn luyện một mô hình phần thưởng để dự đoán đầu ra được con người ưa thích.

Bước 3: Tối ưu hóa một chính sách chống lại mô hình phần thưởng bằng PPO. Chúng tôi sử dụng đầu ra của RM làm phần thưởng scalar. Chúng tôi tinh chỉnh chính sách có giám sát để tối ưu hóa phần thưởng này bằng thuật toán PPO (Schulman et al., 2017).

Bước 2 và 3 có thể được lặp lại liên tục; thu thập thêm dữ liệu so sánh trên chính sách tốt nhất hiện tại, được sử dụng để huấn luyện RM mới và sau đó là chính sách mới. Trong thực tế, hầu hết dữ liệu so sánh của chúng tôi đến từ các chính sách có giám sát của chúng tôi, với một số đến từ các chính sách PPO của chúng tôi.

3.2 Bộ dữ liệu

Bộ dữ liệu lời nhắc của chúng tôi chủ yếu bao gồm các lời nhắc văn bản được gửi đến OpenAI API, cụ thể là những lời nhắc sử dụng phiên bản trước đó của các mô hình InstructGPT (được huấn luyện thông qua học có giám sát trên một tập con dữ liệu minh họa của chúng tôi) trên giao diện Playground. Khách hàng sử dụng Playground được thông báo rằng dữ liệu của họ có thể được sử dụng để huấn luyện các mô hình tiếp theo thông qua thông báo định kỳ bất cứ khi nào các mô hình InstructGPT được sử dụng. Trong bài báo này, chúng tôi không sử dụng dữ liệu từ khách hàng sử dụng API trong sản xuất. Chúng tôi loại bỏ trùng lặp lời nhắc một cách heuristic bằng cách kiểm tra các lời nhắc có chung tiền tố dài, và chúng tôi giới hạn số lượng lời nhắc xuống 200 cho mỗi ID người dùng. Chúng tôi cũng tạo các split train, validation, và test dựa trên ID người dùng, để các tập validation và test không chứa dữ liệu từ người dùng có dữ liệu trong tập huấn luyện. Để tránh việc các mô hình học các chi tiết khách hàng có thể nhạy cảm, chúng tôi lọc tất cả lời nhắc trong split huấn luyện để tìm thông tin nhận dạng cá nhân (PII).

Để huấn luyện các mô hình InstructGPT đầu tiên, chúng tôi yêu cầu người gán nhãn tự viết lời nhắc. Điều này là do chúng tôi cần một nguồn ban đầu các lời nhắc giống hướng dẫn để khởi động quá trình, và những loại lời nhắc này không thường được gửi đến các mô hình GPT-3 thông thường trên API. Chúng tôi yêu cầu người gán nhãn viết ba loại lời nhắc:

• Đơn giản: Chúng tôi đơn giản yêu cầu người gán nhãn nghĩ ra một nhiệm vụ tùy ý, trong khi đảm bảo các nhiệm vụ có sự đa dạng đủ.

• Few-shot: Chúng tôi yêu cầu người gán nhãn nghĩ ra một hướng dẫn, và nhiều cặp query/response cho hướng dẫn đó.

• Dựa trên người dùng: Chúng tôi có một số trường hợp sử dụng được nêu trong các đơn xin tham gia danh sách chờ OpenAI API. Chúng tôi yêu cầu người gán nhãn nghĩ ra các lời nhắc tương ứng với các trường hợp sử dụng này.

Từ các lời nhắc này, chúng tôi tạo ra ba bộ dữ liệu khác nhau được sử dụng trong thủ tục tinh chỉnh của chúng tôi: (1) bộ dữ liệu SFT của chúng tôi, với các minh họa của người gán nhãn được sử dụng để huấn luyện các mô hình SFT của chúng tôi, (2) bộ dữ liệu RM của chúng tôi, với xếp hạng của người gán nhãn về các đầu ra mô hình được sử dụng để huấn luyện RM của chúng tôi, và (3) bộ dữ liệu PPO của chúng tôi, không có nhãn con người, được sử dụng làm đầu vào cho tinh chỉnh RLHF. Bộ dữ liệu SFT chứa khoảng 13k lời nhắc huấn luyện (từ API và do người gán nhãn viết), bộ dữ liệu RM có 33k lời nhắc huấn luyện (từ API và do người gán nhãn viết), và bộ dữ liệu PPO có 31k lời nhắc huấn luyện (chỉ từ API). Thêm chi tiết về kích thước bộ dữ liệu được cung cấp trong Bảng 6.

Để cho cảm nhận về thành phần của bộ dữ liệu của chúng tôi, trong Bảng 1 chúng tôi hiển thị phân phối các danh mục trường hợp sử dụng cho các lời nhắc API của chúng tôi (cụ thể là bộ dữ liệu RM) như được gán nhãn bởi các nhà thầu của chúng tôi. Hầu hết các trường hợp sử dụng là tạo sinh, thay vì phân loại hoặc QA. Chúng tôi cũng hiển thị một số lời nhắc minh họa (được viết bởi các nhà nghiên cứu để bắt chước các loại lời nhắc được gửi đến các mô hình InstructGPT) trong Bảng 2; thêm lời nhắc được gửi đến các mô hình InstructGPT được hiển thị trong Phụ lục A.2.1, và lời nhắc được gửi đến các mô hình GPT-3 được hiển thị trong Phụ lục A.2.2. Chúng tôi cung cấp thêm chi tiết về bộ dữ liệu của mình trong Phụ lục A.

3.3 Nhiệm vụ

Các nhiệm vụ huấn luyện của chúng tôi đến từ hai nguồn: (1) một bộ dữ liệu các lời nhắc được viết bởi người gán nhãn của chúng tôi và (2) một bộ dữ liệu các lời nhắc được gửi đến các mô hình InstructGPT sớm trên API của chúng tôi (xem Bảng 6). Các lời nhắc này rất đa dạng và bao gồm tạo sinh, trả lời câu hỏi, đối thoại, tóm tắt, trích xuất, và các nhiệm vụ ngôn ngữ tự nhiên khác (xem Bảng 1). Bộ dữ liệu của chúng tôi có hơn 96% tiếng Anh, tuy nhiên trong Phần 4.3 chúng tôi cũng thăm dò khả năng của mô hình chúng tôi để phản hồi các hướng dẫn bằng ngôn ngữ khác và hoàn thành các nhiệm vụ lập trình.

Đối với mỗi lời nhắc ngôn ngữ tự nhiên, nhiệm vụ thường được chỉ định trực tiếp thông qua hướng dẫn ngôn ngữ tự nhiên (ví dụ: "Viết một câu chuyện về một con ếch khôn ngoan"), nhưng cũng có thể được chỉ định gián tiếp thông qua các ví dụ few-shot (ví dụ: đưa ra hai ví dụ về câu chuyện ếch, và nhắc mô hình tạo ra một câu chuyện mới) hoặc tiếp tục ngầm định (ví dụ: cung cấp phần đầu của một câu chuyện về một con ếch). Trong mỗi trường hợp, chúng tôi yêu cầu người gán nhãn của mình cố gắng hết sức để suy luận ý định của người dùng đã viết lời nhắc, và yêu cầu họ bỏ qua các đầu vào mà nhiệm vụ rất không rõ ràng. Hơn nữa, người gán nhãn của chúng tôi cũng tính đến các ý định ngầm định như tính trung thực của phản hồi, và các đầu ra có thể có hại như ngôn ngữ thiên vị hoặc độc hại, được hướng dẫn bởi các hướng dẫn chúng tôi cung cấp cho họ (xem Phụ lục B) và phán đoán tốt nhất của họ.

3.4 Thu thập dữ liệu con người

Để tạo ra dữ liệu minh họa và so sánh của chúng tôi, và để tiến hành các đánh giá chính của chúng tôi, chúng tôi thuê một nhóm khoảng 40 nhà thầu trên Upwork và thông qua ScaleAI. So với công việc trước đây thu thập dữ liệu sở thích con người về nhiệm vụ tóm tắt (Ziegler et al., 2019; Stiennon et al., 2020; Wu et al., 2021), các đầu vào của chúng tôi bao trùm một phạm vi nhiệm vụ rộng hơn nhiều, và đôi khi có thể bao gồm các chủ đề gây tranh cãi và nhạy cảm. Mục tiêu của chúng tôi là chọn một nhóm người gán nhãn nhạy cảm với sở thích của các nhóm nhân khẩu học khác nhau, và giỏi trong việc xác định các đầu ra có thể có hại. Do đó, chúng tôi đã tiến hành một bài kiểm tra sàng lọc được thiết kế để đo hiệu suất của người gán nhãn trên các trục này. Chúng tôi chọn những người gán nhãn có hiệu suất tốt trong bài kiểm tra này; để biết thêm thông tin về quy trình lựa chọn và nhân khẩu học của người gán nhãn, xem Phụ lục B.1.

Trong quá trình huấn luyện và đánh giá, các tiêu chí căn chỉnh của chúng tôi có thể xung đột: ví dụ, khi người dùng yêu cầu phản hồi có thể có hại. Trong quá trình huấn luyện, chúng tôi ưu tiên tính hữu ích cho người dùng (việc không làm như vậy đòi hỏi một số quyết định thiết kế khó khăn mà chúng tôi để lại cho công việc tương lai; xem Phần 5.4 để thảo luận thêm). Tuy nhiên, trong các đánh giá cuối cùng của chúng tôi, chúng tôi yêu cầu người gán nhãn ưu tiên tính trung thực và vô hại (vì đây là điều chúng tôi thực sự quan tâm).

Như trong Stiennon et al. (2020), chúng tôi hợp tác chặt chẽ với người gán nhãn trong suốt dự án. Chúng tôi có quy trình giới thiệu để huấn luyện người gán nhãn về dự án, viết hướng dẫn chi tiết cho mỗi nhiệm vụ (xem Phụ lục B.2), và trả lời câu hỏi của người gán nhãn trong phòng chat chung.

Như một nghiên cứu ban đầu để xem các mô hình của chúng tôi tổng quát hóa tốt như thế nào với sở thích của người gán nhãn khác, chúng tôi thuê một nhóm người gán nhãn riêng biệt không tạo ra bất kỳ dữ liệu huấn luyện nào. Những người gán nhãn này được tìm nguồn từ cùng các nhà cung cấp, nhưng không trải qua bài kiểm tra sàng lọc.

Mặc dù sự phức tạp của nhiệm vụ, chúng tôi thấy rằng tỷ lệ đồng ý giữa các người chú thích khá cao: người gán nhãn huấn luyện đồng ý với nhau 72,6±1,5% thời gian, trong khi đối với người gán nhãn được giữ lại con số này là 77,3±1,3%. Để so sánh, trong công việc tóm tắt của Stiennon et al. (2020), sự đồng ý giữa nhà nghiên cứu với nhà nghiên cứu là 73±4%.

3.5 Mô hình

Chúng tôi bắt đầu với các mô hình ngôn ngữ pretrained GPT-3 từ Brown et al. (2020). Các mô hình này được huấn luyện trên một phân phối rộng dữ liệu Internet và có thể thích ứng với một loạt rộng các nhiệm vụ downstream, nhưng có hành vi được đặc trưng kém. Bắt đầu từ các mô hình này, chúng tôi sau đó huấn luyện các mô hình với ba kỹ thuật khác nhau:

Tinh chỉnh có giám sát (SFT). Chúng tôi tinh chỉnh GPT-3 trên các minh họa của người gán nhãn bằng học có giám sát. Chúng tôi huấn luyện trong 16 epoch, sử dụng suy giảm learning rate cosine, và residual dropout 0.2. Chúng tôi thực hiện lựa chọn mô hình SFT cuối cùng dựa trên điểm RM trên tập validation. Tương tự như Wu et al. (2021), chúng tôi thấy rằng các mô hình SFT của chúng tôi overfit trên validation loss sau 1 epoch; tuy nhiên, chúng tôi thấy rằng huấn luyện trong nhiều epoch hơn giúp cả điểm RM và đánh giá sở thích con người, mặc dù overfit này.

Mô hình hóa phần thưởng (RM). Bắt đầu từ mô hình SFT với lớp unembedding cuối được loại bỏ, chúng tôi huấn luyện một mô hình để nhận lời nhắc và phản hồi, và đưa ra phần thưởng scalar. Trong bài báo này chúng tôi chỉ sử dụng RM 6B, vì điều này tiết kiệm rất nhiều tính toán, và chúng tôi thấy rằng huấn luyện RM 175B có thể không ổn định và do đó ít phù hợp để được sử dụng làm hàm giá trị trong RL (xem Phụ lục C để biết thêm chi tiết).

Trong Stiennon et al. (2020), RM được huấn luyện trên một bộ dữ liệu so sánh giữa hai đầu ra mô hình trên cùng đầu vào. Họ sử dụng cross-entropy loss, với các so sánh làm nhãn—sự khác biệt trong phần thưởng đại diện cho log odds rằng một phản hồi sẽ được ưa thích hơn phản hồi khác bởi một người gán nhãn con người. Để tăng tốc thu thập so sánh, chúng tôi trình bày cho người gán nhãn từ K=4 đến K=9 phản hồi để xếp hạng. Điều này tạo ra (K choose 2) so sánh cho mỗi lời nhắc được hiển thị cho người gán nhãn. Vì các so sánh rất tương quan trong mỗi nhiệm vụ gán nhãn, chúng tôi thấy rằng nếu chúng tôi đơn giản xáo trộn các so sánh thành một bộ dữ liệu, một lần đi qua bộ dữ liệu khiến mô hình phần thưởng overfit. Thay vào đó, chúng tôi huấn luyện trên tất cả (K choose 2) so sánh từ mỗi lời nhắc như một phần tử batch duy nhất. Điều này hiệu quả hơn nhiều về mặt tính toán vì nó chỉ yêu cầu một lần forward pass của RM cho mỗi completion (thay vì (K choose 2) forward pass cho K completion) và, vì nó không còn overfit nữa, nó đạt được độ chính xác validation và log loss được cải thiện nhiều.

Cụ thể, hàm loss cho mô hình phần thưởng là:

loss(θ) = -(1/(K choose 2)) E_{(x,y_w,y_l)~D}[log(σ(r_θ(x,y_w) - r_θ(x,y_l)))] (1)

trong đó r_θ(x,y) là đầu ra scalar của mô hình phần thưởng cho lời nhắc x và completion y với tham số θ, y_w là completion được ưa thích trong cặp y_w và y_l, và D là bộ dữ liệu so sánh con người.

Cuối cùng, vì RM loss bất biến với các dịch chuyển trong phần thưởng, chúng tôi chuẩn hóa mô hình phần thưởng bằng một bias để các minh họa của người gán nhãn đạt điểm trung bình 0 trước khi thực hiện RL.

Học tăng cường (RL). Một lần nữa theo Stiennon et al. (2020), chúng tôi tinh chỉnh mô hình SFT trên môi trường của chúng tôi bằng PPO (Schulman et al., 2017). Môi trường là môi trường bandit trình bày lời nhắc khách hàng ngẫu nhiên và mong đợi phản hồi cho lời nhắc. Với lời nhắc và phản hồi, nó tạo ra phần thưởng được xác định bởi mô hình phần thưởng và kết thúc episode. Ngoài ra, chúng tôi thêm phạt KL per-token từ mô hình SFT tại mỗi token để giảm thiểu over-optimization của mô hình phần thưởng. Hàm giá trị được khởi tạo từ RM. Chúng tôi gọi các mô hình này là "PPO."

Chúng tôi cũng thử nghiệm việc trộn gradient pretraining vào gradient PPO, để sửa chữa suy giảm hiệu suất trên các bộ dữ liệu NLP công cộng. Chúng tôi gọi các mô hình này là "PPO-ptx." Chúng tôi tối đa hóa hàm mục tiêu kết hợp sau trong huấn luyện RL:

objective(φ) = E_{(x,y)~D_{PPO}}[r_θ(x,y) - β log(π^{RL}_φ(y|x)/π^{SFT}(y|x))] + γ E_{x~D_{pretrain}}[log π^{RL}_φ(x)] (2)

trong đó π^{RL}_φ là chính sách RL được học, π^{SFT} là mô hình được huấn luyện có giám sát, và D_{pretrain} là phân phối pretraining. Hệ số phần thưởng KL, β, và hệ số loss pretraining, γ, kiểm soát cường độ của phạt KL và gradient pretraining tương ứng. Đối với các mô hình "PPO", γ được đặt bằng 0.

Trừ khi được chỉ định khác, trong bài báo này InstructGPT đề cập đến các mô hình PPO-ptx.

Đường cơ sở. Chúng tôi so sánh hiệu suất của các mô hình PPO của chúng tôi với các mô hình SFT của chúng tôi và GPT-3. Chúng tôi cũng so sánh với GPT-3 khi nó được cung cấp tiền tố few-shot để 'nhắc' nó vào chế độ tuân theo hướng dẫn (GPT-3-prompted). Tiền tố này được thêm vào trước hướng dẫn do người dùng chỉ định.

Chúng tôi cũng so sánh InstructGPT với việc tinh chỉnh GPT-3 175B trên các bộ dữ liệu FLAN (Wei et al., 2021) và T0 (Sanh et al., 2021), cả hai đều bao gồm nhiều nhiệm vụ NLP khác nhau, kết hợp với hướng dẫn ngôn ngữ tự nhiên cho mỗi nhiệm vụ (các bộ dữ liệu khác nhau về các bộ dữ liệu NLP được bao gồm, và phong cách hướng dẫn được sử dụng). Chúng tôi tinh chỉnh chúng trên khoảng 1 triệu ví dụ tương ứng và chọn checkpoint đạt được điểm mô hình phần thưởng cao nhất trên tập validation. Xem Phụ lục C để biết thêm chi tiết huấn luyện.

3.6 Đánh giá

Để đánh giá mức độ "căn chỉnh" của các mô hình của chúng tôi, trước tiên chúng tôi cần làm rõ ý nghĩa của căn chỉnh trong bối cảnh này. Định nghĩa về căn chỉnh trong lịch sử là một chủ đề mơ hồ và gây nhầm lẫn, với nhiều đề xuất cạnh tranh khác nhau (Chen et al., 2021; Leike et al., 2018; Gabriel, 2020). Theo Leike et al. (2018), mục tiêu của chúng tôi là huấn luyện các mô hình hành động phù hợp với ý định của người dùng. Thực tế hơn, cho mục đích của các nhiệm vụ ngôn ngữ của chúng tôi, chúng tôi sử dụng một khung tương tự như Askell et al. (2021), những người định nghĩa các mô hình được căn chỉnh nếu chúng hữu ích, trung thực, và vô hại.

Để hữu ích, mô hình nên tuân theo hướng dẫn, nhưng cũng suy luận ý định từ lời nhắc few-shot hoặc một mẫu có thể giải thích khác như "Q: {câu hỏi}\nA: ". Vì ý định của một lời nhắc nhất định có thể không rõ ràng hoặc mơ hồ, chúng tôi dựa vào phán đoán từ người gán nhãn của chúng tôi, và thước đo chính của chúng tôi là đánh giá sở thích của người gán nhãn. Tuy nhiên, vì người gán nhãn của chúng tôi không phải là người dùng tạo ra lời nhắc, có thể có sự khác biệt giữa những gì người dùng thực sự có ý định và những gì người gán nhãn nghĩ là có ý định chỉ từ việc đọc lời nhắc.

Không rõ làm thế nào để đo tính trung thực trong các mô hình tạo sinh thuần túy; điều này đòi hỏi so sánh đầu ra thực tế của mô hình với "niềm tin" của nó về đầu ra đúng, và vì mô hình là một hộp đen lớn, chúng ta không thể suy luận niềm tin của nó. Thay vào đó, chúng tôi đo tính trung thực—liệu các câu nói của mô hình về thế giới có đúng hay không—bằng hai thước đo: (1) đánh giá xu hướng của mô hình chúng tôi để bịa đặt thông tin trên các nhiệm vụ miền đóng ("ảo giác"), và (2) sử dụng bộ dữ liệu TruthfulQA (Lin et al., 2021). Không cần nói, điều này chỉ nắm bắt một phần nhỏ của ý nghĩa thực sự của tính trung thực.

Tương tự như tính trung thực, việc đo tác hại của các mô hình ngôn ngữ cũng đặt ra nhiều thách thức. Trong hầu hết các trường hợp, tác hại từ các mô hình ngôn ngữ phụ thuộc vào cách đầu ra của chúng được sử dụng trong thế giới thực. Ví dụ, một mô hình tạo ra đầu ra độc hại có thể có hại trong bối cảnh của một chatbot được triển khai, nhưng thậm chí có thể hữu ích nếu được sử dụng để tăng cường dữ liệu để huấn luyện một mô hình phát hiện độc tính chính xác hơn. Sớm hơn trong dự án, chúng tôi để người gán nhãn đánh giá liệu một đầu ra có 'có thể có hại' hay không. Tuy nhiên, chúng tôi đã ngừng điều này vì nó đòi hỏi quá nhiều suy đoán về cách các đầu ra cuối cùng sẽ được sử dụng; đặc biệt là vì dữ liệu của chúng tôi cũng đến từ khách hàng tương tác với giao diện API Playground (thay vì từ các trường hợp sử dụng sản xuất).

Do đó, chúng tôi sử dụng một bộ tiêu chí proxy cụ thể hơn nhằm nắm bắt các khía cạnh khác nhau của hành vi trong một mô hình được triển khai có thể cuối cùng có hại: chúng tôi để người gán nhãn đánh giá liệu một đầu ra có không phù hợp trong bối cảnh của một trợ lý khách hàng, xúc phạm một lớp được bảo vệ, hoặc chứa nội dung tình dục hoặc bạo lực. Chúng tôi cũng benchmark mô hình của mình trên các bộ dữ liệu nhằm đo thiên vị và độc tính, như RealToxicityPrompts (Gehman et al., 2020) và CrowS-Pairs (Nangia et al., 2020).

Để tóm tắt, chúng tôi có thể chia các đánh giá định lượng của mình thành hai phần riêng biệt:

Đánh giá trên phân phối API. Thước đo chính của chúng tôi là đánh giá sở thích con người trên một tập các lời nhắc được giữ lại từ cùng nguồn với phân phối huấn luyện của chúng tôi. Khi sử dụng lời nhắc từ API để đánh giá, chúng tôi chỉ chọn lời nhắc của khách hàng mà chúng tôi chưa bao gồm trong huấn luyện. Tuy nhiên, cho rằng lời nhắc huấn luyện của chúng tôi được thiết kế để sử dụng với các mô hình InstructGPT, có thể chúng bất lợi cho các đường cơ sở GPT-3. Do đó, chúng tôi cũng đánh giá trên lời nhắc được gửi đến các mô hình GPT-3 trên API; các lời nhắc này thường không theo phong cách 'tuân theo hướng dẫn', nhưng được thiết kế cụ thể cho GPT-3. Trong cả hai trường hợp, đối với mỗi mô hình chúng tôi tính toán tần suất đầu ra của nó được ưa thích so với một chính sách cơ sở; chúng tôi chọn mô hình SFT 175B của chúng tôi làm cơ sở vì hiệu suất của nó gần ở giữa nhóm. Ngoài ra, chúng tôi yêu cầu người gán nhãn đánh giá chất lượng tổng thể của mỗi phản hồi trên thang Likert 1-7 và thu thập một loạt metadata cho mỗi đầu ra mô hình (xem Bảng 3).

Đánh giá trên các bộ dữ liệu NLP công cộng. Chúng tôi đánh giá trên hai loại bộ dữ liệu công cộng: những bộ nắm bắt một khía cạnh của tính an toàn mô hình ngôn ngữ, đặc biệt là tính trung thực, độc tính, và thiên vị, và những bộ nắm bắt hiệu suất zero-shot trên các nhiệm vụ NLP truyền thống như trả lời câu hỏi, đọc hiểu, và tóm tắt. Chúng tôi cũng tiến hành đánh giá con người về độc tính trên bộ dữ liệu RealToxicityPrompts (Gehman et al., 2020). Chúng tôi đang phát hành các mẫu từ mô hình của mình trên tất cả các nhiệm vụ NLP dựa trên sampling.

4 Kết quả

Trong phần này, chúng tôi cung cấp bằng chứng thí nghiệm cho các tuyên bố của chúng tôi trong Phần 1, được sắp xếp thành ba phần: kết quả trên phân phối lời nhắc API, kết quả trên các bộ dữ liệu NLP công cộng, và kết quả định tính.

4.1 Kết quả trên phân phối API

Người gán nhãn ưa thích đáng kể các đầu ra InstructGPT hơn các đầu ra từ GPT-3. Trên tập kiểm tra lời nhắc của chúng tôi, người gán nhãn của chúng tôi ưa thích đáng kể các đầu ra InstructGPT qua các kích thước mô hình. Các kết quả này được hiển thị trong Hình 1. Chúng tôi thấy rằng các đầu ra GPT-3 hoạt động tệ nhất, và người ta có thể đạt được những cải thiện đáng kể từng bước bằng cách sử dụng lời nhắc few-shot được tạo tốt (GPT-3 (prompted)), sau đó bằng cách huấn luyện trên minh họa bằng học có giám sát (SFT), và cuối cùng bằng cách huấn luyện trên dữ liệu so sánh bằng PPO. Việc thêm cập nhật trên pretraining mix trong PPO không dẫn đến thay đổi lớn trong sở thích của người gán nhãn. Để minh họa mức độ của những lợi ích của chúng tôi: khi so sánh trực tiếp, các đầu ra InstructGPT 175B được ưa thích hơn các đầu ra GPT-3 85±3% thời gian, và được ưa thích 71±4% thời gian so với GPT-3 few-shot.

Chúng tôi cũng thấy rằng kết quả của chúng tôi không thay đổi đáng kể khi đánh giá trên lời nhắc được gửi đến các mô hình GPT-3 trên API (xem Hình 3), mặc dù các mô hình PPO-ptx của chúng tôi hoạt động hơi tệ hơn ở các kích thước mô hình lớn hơn.

Trong Hình 4, chúng tôi cho thấy người gán nhãn cũng đánh giá các đầu ra InstructGPT một cách thuận lợi theo một số trục cụ thể hơn. Cụ thể, so với GPT-3, các đầu ra InstructGPT phù hợp hơn trong bối cảnh của một trợ lý khách hàng, thường tuân theo các ràng buộc rõ ràng được định nghĩa trong hướng dẫn (ví dụ: "Viết câu trả lời của bạn trong 2 đoạn văn hoặc ít hơn."), ít có khả năng không tuân theo hướng dẫn đúng hoàn toàn, và bịa đặt sự thật ('ảo giác') ít hơn trong các nhiệm vụ miền đóng. Các kết quả này cho thấy rằng các mô hình InstructGPT đáng tin cậy hơn và dễ kiểm soát hơn GPT-3. Chúng tôi thấy rằng các danh mục metadata khác của chúng tôi xảy ra quá ít thường xuyên trong API của chúng tôi để có được sự khác biệt có ý nghĩa thống kê giữa các mô hình của chúng tôi.

Các mô hình của chúng tôi tổng quát hóa cho sở thích của người gán nhãn "được giữ lại" không tạo ra bất kỳ dữ liệu huấn luyện nào. Người gán nhãn được giữ lại có sở thích xếp hạng tương tự như những worker mà chúng tôi sử dụng để tạo ra dữ liệu huấn luyện (xem Hình 3). Đặc biệt, theo các worker được giữ lại, tất cả các mô hình InstructGPT của chúng tôi vẫn vượt trội hơn đáng kể so với các đường cơ sở GPT-3. Do đó, các mô hình InstructGPT của chúng tôi không chỉ đơn giản là overfit với sở thích của người gán nhãn huấn luyện của chúng tôi.

Chúng tôi thấy bằng chứng thêm về điều này từ khả năng tổng quát hóa của các mô hình phần thưởng của chúng tôi. Chúng tôi chạy một thí nghiệm trong đó chúng tôi chia người gán nhãn của mình thành 5 nhóm, và huấn luyện 5 RM (với 3 seed khác nhau) bằng cross validation 5-fold (huấn luyện trên 4 nhóm, và đánh giá trên nhóm được giữ lại). Các RM này có độ chính xác 69,6±0,9% trong việc dự đoán sở thích của người gán nhãn trong nhóm được giữ lại, một sự giảm nhỏ từ độ chính xác 72,4±0,4% của họ trong việc dự đoán sở thích của người gán nhãn trong tập huấn luyện của họ.

Các bộ dữ liệu NLP công cộng không phản ánh cách các mô hình ngôn ngữ của chúng tôi được sử dụng. Trong Hình 5, chúng tôi cũng so sánh InstructGPT với các đường cơ sở GPT-3 175B của chúng tôi được tinh chỉnh trên các bộ dữ liệu FLAN (Wei et al., 2021) và T0 (Sanh et al., 2021) (xem Phụ lục C để biết chi tiết). Chúng tôi thấy rằng các mô hình này hoạt động tốt hơn GPT-3, ngang với GPT-3 với lời nhắc được chọn tốt, và tệ hơn đường cơ sở SFT của chúng tôi. Điều này chỉ ra rằng các bộ dữ liệu này không đủ đa dạng để cải thiện hiệu suất trên phân phối lời nhắc API của chúng tôi. Trong so sánh trực tiếp, các đầu ra mô hình InstructGPT 175B của chúng tôi được ưa thích hơn mô hình FLAN của chúng tôi 78±4% thời gian và hơn mô hình T0 của chúng tôi 79±4% thời gian. Điểm Likert cho các mô hình này được hiển thị trong Hình 5.

Chúng tôi tin rằng mô hình InstructGPT của chúng tôi vượt trội hơn FLAN và T0 vì hai lý do. Thứ nhất, các bộ dữ liệu NLP công cộng được thiết kế để nắm bắt các nhiệm vụ dễ đánh giá bằng các thước đo tự động, như phân loại, trả lời câu hỏi, và đến một mức độ nhất định là tóm tắt và dịch thuật. Tuy nhiên, phân loại và QA chỉ là một phần nhỏ (khoảng 18%) của những gì khách hàng API sử dụng các mô hình ngôn ngữ của chúng tôi, trong khi tạo sinh mở và động não chiếm khoảng 57% bộ dữ liệu lời nhắc của chúng tôi theo người gán nhãn (xem Bảng 1). Thứ hai, có thể khó khăn cho các bộ dữ liệu NLP công cộng để có được sự đa dạng đầu vào rất cao (ít nhất, trên các loại đầu vào mà người dùng thế giới thực sẽ quan tâm sử dụng). Tất nhiên, các nhiệm vụ được tìm thấy trong các bộ dữ liệu NLP đại diện cho một loại hướng dẫn mà chúng ta muốn các mô hình ngôn ngữ có thể giải quyết, vì vậy loại mô hình tuân theo hướng dẫn rộng nhất sẽ kết hợp cả hai loại bộ dữ liệu.

4.2 Kết quả trên các bộ dữ liệu NLP công cộng

Các mô hình InstructGPT cho thấy sự cải thiện về tính trung thực so với GPT-3. Như được đo bởi các đánh giá con người trên bộ dữ liệu TruthfulQA, các mô hình PPO của chúng tôi cho thấy sự cải thiện nhỏ nhưng đáng kể trong việc tạo ra đầu ra trung thực và thông tin so với GPT-3 (xem Hình 6). Hành vi này là mặc định: các mô hình của chúng tôi không phải được hướng dẫn cụ thể để nói sự thật để thể hiện tính trung thực được cải thiện. Thú vị là, ngoại lệ là mô hình PPO-ptx 1.3B của chúng tôi, hoạt động hơi tệ hơn mô hình GPT-3 cùng kích thước. Khi đánh giá chỉ trên các lời nhắc không được chọn một cách đối kháng chống lại GPT-3, các mô hình PPO của chúng tôi vẫn trung thực và thông tin hơn đáng kể so với GPT-3 (mặc dù sự cải thiện tuyệt đối giảm vài điểm phần trăm).

Theo Lin et al. (2021), chúng tôi cũng đưa ra lời nhắc "Instruction+QA" hữu ích hướng dẫn mô hình phản hồi bằng "Tôi không có bình luận" khi nó không chắc chắn về câu trả lời đúng. Trong trường hợp này, các mô hình PPO của chúng tôi có xu hướng trung thực và không thông tin thay vì tự tin nói điều sai lầm; các mô hình GPT-3 cơ sở không giỏi điều này.

Sự cải thiện về tính trung thực của chúng tôi cũng được chứng minh bởi thực tế là các mô hình PPO của chúng tôi ảo giác (tức là bịa đặt thông tin) ít hơn trong các nhiệm vụ miền đóng từ phân phối API của chúng tôi, mà chúng tôi đã hiển thị trong Hình 4.

InstructGPT cho thấy sự cải thiện nhỏ về độc tính so với GPT-3, nhưng không về thiên vị. Đầu tiên chúng tôi đánh giá các mô hình của mình trên bộ dữ liệu RealToxicityPrompts (Gehman et al., 2020). Chúng tôi làm điều này theo hai cách: chúng tôi chạy các mẫu mô hình qua Perspective API để có được điểm độc tính tự động, đây là thủ tục đánh giá tiêu chuẩn cho bộ dữ liệu này, và chúng tôi cũng gửi các mẫu này đến người gán nhãn để có được đánh giá về độc tính tuyệt đối, độc tính tương đối so với lời nhắc, tính liên tục, và sở thích đầu ra tổng thể. Chúng tôi lấy mẫu lời nhắc từ bộ dữ liệu này đều theo độc tính lời nhắc để đánh giá tốt hơn cách các mô hình của chúng tôi hoạt động với độc tính đầu vào cao (xem Hình 39 trong Phụ lục E); điều này khác với việc lấy mẫu lời nhắc tiêu chuẩn cho bộ dữ liệu này, và do đó số độc tính tuyệt đối của chúng tôi bị thổi phồng.

Kết quả của chúng tôi trong Hình 7. Chúng tôi thấy rằng, khi được hướng dẫn tạo ra đầu ra an toàn và tôn trọng ("lời nhắc tôn trọng"), các mô hình InstructGPT tạo ra đầu ra ít độc hại hơn so với GPT-3 theo Perspective API. Lợi thế này biến mất khi lời nhắc tôn trọng được loại bỏ ("không có lời nhắc"). Thú vị là, khi được nhắc rõ ràng để tạo ra đầu ra độc hại, các đầu ra InstructGPT độc hại hơn nhiều so với GPT-3 (xem Hình 39).

Các kết quả này được xác nhận trong các đánh giá con người của chúng tôi: InstructGPT ít độc hại hơn GPT-3 trong setting "lời nhắc tôn trọng", nhưng hoạt động tương tự trong setting "không có lời nhắc". Chúng tôi cung cấp kết quả mở rộng trong Phụ lục E. Để tóm tắt: tất cả các mô hình của chúng tôi được đánh giá là ít độc hại hơn mong đợi với lời nhắc (chúng nhận được điểm âm trên thang từ -1 đến 1, trong đó 0 là 'độc hại như mong đợi'). Đường cơ sở SFT của chúng tôi ít độc hại nhất trong tất cả các mô hình của chúng tôi, nhưng cũng có tính liên tục thấp nhất và ít được ưa thích nhất trong xếp hạng của chúng tôi, điều này có thể chỉ ra rằng mô hình tạo ra các phản hồi rất ngắn hoặc suy thoái.

Để đánh giá xu hướng của mô hình tạo ra lời nói thiên vị (xem Phụ lục E), chúng tôi cũng đánh giá InstructGPT trên các phiên bản sửa đổi của bộ dữ liệu Winogender (Rudinger et al., 2018) và CrowS-Pairs (Nangia et al., 2020). Các bộ dữ liệu này bao gồm các cặp câu có thể làm nổi bật thiên vị tiềm năng. Chúng tôi tính toán xác suất tương đối của việc tạo ra các câu trong mỗi cặp và entropy (trong bit) của các phân phối xác suất nhị phân liên quan. Các mô hình hoàn toàn không thiên vị sẽ không có sở thích giữa các câu trong mỗi cặp và do đó sẽ có entropy tối đa. Theo thước đo này, các mô hình của chúng tôi không ít thiên vị hơn GPT-3. Mô hình PPO-ptx cho thấy thiên vị tương tự như GPT-3, nhưng khi được hướng dẫn hành xử một cách tôn trọng, nó thể hiện entropy thấp hơn và do đó thiên vị cao hơn. Mẫu của thiên vị không rõ ràng; có vẻ như các mô hình được hướng dẫn chắc chắn hơn về đầu ra của chúng bất kể đầu ra của chúng có thể hiện hành vi khuôn mẫu hay không.

Chúng tôi có thể giảm thiểu suy giảm hiệu suất trên các bộ dữ liệu NLP công cộng bằng cách sửa đổi thủ tục tinh chỉnh RLHF của chúng tôi. Theo mặc định, khi chúng tôi huấn luyện một mô hình PPO trên phân phối API của chúng tôi, nó chịu "thuế căn chỉnh", vì hiệu suất của nó trên một số bộ dữ liệu NLP công cộng giảm. Chúng tôi muốn một thủ tục căn chỉnh tránh thuế căn chỉnh, vì nó khuyến khích việc sử dụng các mô hình không được căn chỉnh nhưng có khả năng hơn trong các nhiệm vụ này.

Trong Hình 29, chúng tôi cho thấy việc thêm các cập nhật pretraining vào tinh chỉnh PPO của chúng tôi (PPO-ptx) giảm thiểu các suy giảm hiệu suất này trên tất cả các bộ dữ liệu, và thậm chí vượt qua GPT-3 trên HellaSwag. Hiệu suất của mô hình PPO-ptx vẫn chậm hơn GPT-3 trên DROP, SQuADv2, và dịch thuật; cần nhiều công việc hơn để nghiên cứu và tiếp tục loại bỏ các suy giảm hiệu suất này.

Việc trộn các cập nhật pretraining hoạt động tốt hơn giải pháp đơn giản hơn là tăng hệ số KL. Trong Hình 33, chúng tôi cho thấy có một giá trị của hệ số pretraining mix vừa đảo ngược suy giảm hiệu suất trên SQuADv2 và DROP (các bộ dữ liệu chúng tôi sử dụng để kiểm tra), vừa có sự giảm tối thiểu trong phần thưởng validation. Ngược lại, việc tăng hệ số KL (Hình 34) dẫn đến giảm đáng kể phần thưởng validation và không bao giờ phục hồi hoàn toàn trên DROP và SQuAD. Thay đổi mô hình KL từ PPO init sang GPT-3 đưa ra kết quả tương tự.

4.3 Kết quả định tính

Các mô hình InstructGPT cho thấy khả năng tổng quát hóa đầy hứa hẹn với các hướng dẫn bên ngoài phân phối tinh chỉnh RLHF. Đặc biệt, chúng tôi thấy rằng InstructGPT cho thấy khả năng tuân theo hướng dẫn bằng ngôn ngữ không phải tiếng Anh, và thực hiện tóm tắt và trả lời câu hỏi cho mã. Điều này thú vị vì các ngôn ngữ không phải tiếng Anh và mã tạo thành một thiểu số nhỏ trong dữ liệu tinh chỉnh của chúng tôi, và nó cho thấy rằng, trong một số trường hợp, các phương pháp căn chỉnh có thể tổng quát hóa để tạo ra hành vi mong muốn trên các đầu vào mà con người không trực tiếp giám sát.

Chúng tôi không theo dõi các hành vi này một cách định lượng, nhưng chúng tôi hiển thị một số ví dụ định tính trong Hình 8. Mô hình PPO-ptx 175B của chúng tôi có thể trả lời câu hỏi về mã một cách đáng tin cậy, và cũng có thể tuân theo hướng dẫn bằng ngôn ngữ khác; tuy nhiên, chúng tôi nhận thấy rằng nó thường tạo ra đầu ra bằng tiếng Anh ngay cả khi hướng dẫn bằng ngôn ngữ khác. Để so sánh, chúng tôi thấy rằng GPT-3 có thể thực hiện các nhiệm vụ này nhưng yêu cầu nhắc nhở cẩn thận hơn, và hiếm khi tuân theo hướng dẫn trong các miền này.

InstructGPT vẫn mắc những lỗi đơn giản. Trong tương tác với mô hình PPO-ptx 175B của chúng tôi, chúng tôi đã nhận thấy nó vẫn có thể mắc những lỗi đơn giản, mặc dù hiệu suất mạnh mẽ của nó trên nhiều nhiệm vụ ngôn ngữ khác nhau. Để đưa ra một vài ví dụ: (1) khi được đưa ra hướng dẫn với tiền đề sai, mô hình đôi khi sai lầm cho rằng tiền đề là đúng, (2) mô hình có thể né tránh quá mức; khi được đưa ra một câu hỏi đơn giản, nó đôi khi có thể nói rằng không có một câu trả lời nào cho câu hỏi và đưa ra nhiều câu trả lời có thể, ngay cả khi có một câu trả lời khá rõ ràng từ bối cảnh, và (3) hiệu suất của mô hình suy giảm khi hướng dẫn chứa nhiều ràng buộc rõ ràng (ví dụ: "liệt kê 10 bộ phim được làm trong những năm 1930 tại Pháp") hoặc khi các ràng buộc có thể thách thức đối với các mô hình ngôn ngữ (ví dụ: viết tóm tắt trong một số câu được chỉ định).

Chúng tôi hiển thị một số ví dụ về các hành vi này trong Hình 9. Chúng tôi nghi ngờ rằng hành vi (2) xuất hiện một phần vì chúng tôi hướng dẫn người gán nhãn thưởng cho sự khiêm tốn epistemological; do đó, họ có thể có xu hướng thưởng các đầu ra né tránh, và điều này được mô hình phần thưởng của chúng tôi nhận ra. Chúng tôi nghi ngờ rằng hành vi (1) xảy ra vì có ít lời nhắc trong tập huấn luyện giả định tiền đề sai, và các mô hình của chúng tôi không tổng quát hóa tốt cho các ví dụ này. Chúng tôi tin rằng cả hai hành vi này có thể được giảm đáng kể với thu thập dữ liệu đối kháng (Dinan et al., 2019b).

5 Thảo luận

5.1 Ý nghĩa đối với nghiên cứu căn chỉnh

Nghiên cứu này là một phần của chương trình nghiên cứu rộng hơn của chúng tôi để căn chỉnh các hệ thống AI với ý định con người (Christiano et al., 2017; Ziegler et al., 2019; Stiennon et al., 2020). Mặc dù công việc này tập trung vào các hệ thống mô hình ngôn ngữ hiện tại của chúng tôi, chúng tôi tìm kiếm các phương pháp tổng quát và có thể mở rộng hoạt động cho các hệ thống AI tương lai (Leike et al., 2018). Các hệ thống chúng tôi làm việc ở đây vẫn còn khá hạn chế, nhưng chúng nằm trong số các mô hình ngôn ngữ lớn nhất hiện nay và chúng tôi áp dụng chúng trên một loạt rộng các nhiệm vụ ngôn ngữ, bao gồm phân loại, tóm tắt, trả lời câu hỏi, viết sáng tạo, đối thoại, và những thứ khác.

Cách tiếp cận của chúng tôi đối với nghiên cứu căn chỉnh trong công việc này là lặp đi lặp lại: chúng tôi đang cải thiện sự căn chỉnh của các hệ thống AI hiện tại thay vì tập trung một cách trừu tượng vào việc căn chỉnh các hệ thống AI chưa tồn tại. Một bất lợi của cách tiếp cận này là chúng tôi không trực tiếp đối mặt với các vấn đề căn chỉnh chỉ xảy ra khi căn chỉnh các hệ thống siêu nhân (Bostrom, 2014). Tuy nhiên, cách tiếp cận của chúng tôi cung cấp cho chúng tôi một vòng phản hồi thực nghiệm rõ ràng về những gì hoạt động và những gì không. Chúng tôi tin rằng vòng phản hồi này rất cần thiết để tinh chỉnh các kỹ thuật căn chỉnh của chúng tôi, và nó buộc chúng tôi phải theo kịp tiến bộ trong học máy. Hơn nữa, kỹ thuật căn chỉnh chúng tôi sử dụng ở đây, RLHF, là một khối xây dựng quan trọng trong một số đề xuất để căn chỉnh các hệ thống siêu nhân (Leike et al., 2018; Irving et al., 2018; Christiano et al., 2018). Ví dụ, RLHF là một phương pháp trung tâm trong công việc gần đây về tóm tắt sách, một nhiệm vụ thể hiện một số khó khăn của việc căn chỉnh các hệ thống AI siêu nhân vì khó cho con người đánh giá trực tiếp (Wu et al., 2021).

Từ công việc này, chúng tôi có thể rút ra bài học cho nghiên cứu căn chỉnh nói chung:

1. Chi phí tăng sự căn chỉnh mô hình là khiêm tốn so với pretraining. Chi phí thu thập dữ liệu của chúng tôi và tính toán cho các lần chạy huấn luyện, bao gồm các lần chạy thí nghiệm là một phần nhỏ của những gì đã được chi để huấn luyện GPT-3: huấn luyện mô hình SFT 175B của chúng tôi yêu cầu 4.9 petaflops/s-ngày và huấn luyện mô hình PPO-ptx 175B của chúng tôi yêu cầu 60 petaflops/s-ngày, so với 3,640 petaflops/s-ngày cho GPT-3 (Brown et al., 2020). Đồng thời, kết quả của chúng tôi cho thấy rằng RLHF rất hiệu quả trong việc làm cho các mô hình ngôn ngữ hữu ích hơn cho người dùng, hơn cả việc tăng kích thước mô hình 100 lần. Điều này cho thấy rằng hiện tại việc tăng đầu tư vào căn chỉnh các mô hình ngôn ngữ hiện có hiệu quả hơn về chi phí so với huấn luyện các mô hình lớn hơn—ít nhất là đối với phân phối nhiệm vụ ngôn ngữ tự nhiên của khách hàng chúng tôi.

2. Chúng tôi đã thấy một số bằng chứng rằng InstructGPT tổng quát hóa 'tuân theo hướng dẫn' cho các setting mà chúng tôi không giám sát nó, ví dụ trên các nhiệm vụ ngôn ngữ không phải tiếng Anh và nhiệm vụ liên quan đến mã. Đây là một tính chất quan trọng vì việc có con người giám sát các mô hình trên mọi nhiệm vụ chúng thực hiện là quá đắt đỏ. Cần nhiều nghiên cứu hơn để nghiên cứu mức độ tổng quát hóa này mở rộng như thế nào với khả năng tăng; xem Christiano et al. (2021) cho nghiên cứu gần đây theo hướng này.

3. Chúng tôi đã có thể giảm thiểu hầu hết suy giảm hiệu suất được đưa ra bởi tinh chỉnh của chúng tôi. Nếu điều này không phải là trường hợp, các suy giảm hiệu suất này sẽ tạo thành một thuế căn chỉnh—một chi phí bổ sung để căn chỉnh mô hình. Bất kỳ kỹ thuật nào có thuế cao có thể không được áp dụng. Để tránh động cơ cho các hệ thống AI có khả năng cao trong tương lai duy trì không được căn chỉnh với ý định con người, cần có các kỹ thuật căn chỉnh có thuế căn chỉnh thấp. Với mục đích này, kết quả của chúng tôi là tin tức tốt cho RLHF như một kỹ thuật căn chỉnh thuế thấp.

4. Chúng tôi đã xác thực các kỹ thuật căn chỉnh từ nghiên cứu trong thế giới thực. Nghiên cứu căn chỉnh trong lịch sử khá trừu tượng, tập trung vào kết quả lý thuyết (Soares et al., 2015), các miền tổng hợp nhỏ (Christiano et al., 2018; Leike et al., 2017), hoặc huấn luyện các mô hình ML trên các bộ dữ liệu NLP công cộng (Ziegler et al., 2019; Stiennon et al., 2020). Công việc của chúng tôi cung cấp nền tảng cho nghiên cứu căn chỉnh trong các hệ thống AI đang được sử dụng trong sản xuất trong thế giới thực với khách hàng. Điều này cho phép một vòng phản hồi quan trọng về hiệu quả và hạn chế của các kỹ thuật.

5.2 Chúng tôi đang căn chỉnh với ai?

Khi căn chỉnh các mô hình ngôn ngữ với ý định con người, hành vi cuối cùng của chúng là một hàm của mô hình cơ bản (và dữ liệu huấn luyện của nó), dữ liệu tinh chỉnh, và phương pháp căn chỉnh được sử dụng. Trong phần này, chúng tôi mô tả một số yếu tố ảnh hưởng đến dữ liệu tinh chỉnh cụ thể, để cuối cùng xác định chúng tôi đang căn chỉnh với cái gì và ai. Sau đó chúng tôi xem xét các lĩnh vực cải thiện trước khi thảo luận lớn hơn về các hạn chế của công việc chúng tôi trong Phần 5.3.

Văn học thường đóng khung căn chỉnh sử dụng các thuật ngữ như "sở thích con người" hoặc "giá trị con người." Trong công việc này, chúng tôi đã căn chỉnh với một tập hợp sở thích của người gán nhãn được ảnh hưởng, trong số những thứ khác, bởi các hướng dẫn họ được đưa ra, bối cảnh họ nhận chúng (như một công việc được trả lương), và ai đã đưa chúng cho họ. Một số cảnh báo quan trọng áp dụng:

Thứ nhất, chúng tôi đang căn chỉnh với các minh họa và sở thích được cung cấp bởi người gán nhãn huấn luyện của chúng tôi, những người trực tiếp tạo ra dữ liệu mà chúng tôi sử dụng để tinh chỉnh các mô hình của mình. Chúng tôi mô tả quy trình thuê và nhân khẩu học của người gán nhãn trong Phụ lục B; nói chung, họ chủ yếu là những người nói tiếng Anh sống ở Hoa Kỳ hoặc Đông Nam Á được thuê qua Upwork hoặc Scale AI. Họ không đồng ý với nhau về nhiều ví dụ; chúng tôi thấy sự đồng ý giữa người gán nhãn khoảng 73%.

Thứ hai, chúng tôi đang căn chỉnh với sở thích của chúng tôi, như các nhà nghiên cứu thiết kế nghiên cứu này (và do đó bằng proxy với tổ chức nghiên cứu rộng hơn của chúng tôi, OpenAI): chúng tôi viết các hướng dẫn gán nhãn mà người gán nhãn sử dụng như một hướng dẫn khi viết minh họa và chọn đầu ra ưa thích của họ, và chúng tôi trả lời câu hỏi của họ về các trường hợp biên trong phòng chat chung. Cần nhiều nghiên cứu hơn về tác động chính xác của các bộ hướng dẫn khác nhau và thiết kế giao diện đối với dữ liệu được thu thập từ người gán nhãn và tác động cuối cùng của nó đối với hành vi mô hình.

Thứ ba, dữ liệu huấn luyện của chúng tôi được xác định bởi các lời nhắc được gửi bởi khách hàng OpenAI đến các mô hình trên OpenAI API Playground, và do đó chúng tôi ngầm căn chỉnh với những gì khách hàng nghĩ là có giá trị và, trong một số trường hợp, những gì người dùng cuối của họ nghĩ là có giá trị để hiện tại sử dụng API. Khách hàng và người dùng cuối của họ có thể không đồng ý hoặc khách hàng có thể không tối ưu hóa cho sự phúc lợi của người dùng cuối; ví dụ, một khách hàng có thể muốn một mô hình tối đa hóa lượng thời gian người dùng dành trên nền tảng của họ, điều này không nhất thiết là những gì người dùng cuối muốn. Trong thực tế, người gán nhãn của chúng tôi không có khả năng nhìn thấy các bối cảnh mà một lời nhắc hoặc completion nhất định sẽ được nhìn thấy.

Thứ tư, khách hàng của OpenAI không đại diện cho tất cả người dùng tiềm năng hoặc hiện tại của các mô hình ngôn ngữ—chứ đừng nói đến tất cả các cá nhân và nhóm bị ảnh hưởng bởi việc sử dụng mô hình ngôn ngữ. Trong hầu hết thời gian của dự án này, người dùng OpenAI API được chọn từ danh sách chờ. Các hạt giống ban đầu cho danh sách chờ này là nhân viên OpenAI, thiên vị nhóm cuối cùng theo mạng lưới của chúng tôi.

Lùi lại, có nhiều khó khăn trong việc thiết kế một quy trình căn chỉnh công bằng, minh bạch, và có các cơ chế trách nhiệm phù hợp. Mục tiêu của bài báo này là chứng minh rằng kỹ thuật căn chỉnh này có thể căn chỉnh với một nhóm tham chiếu con người cụ thể cho một ứng dụng cụ thể. Chúng tôi không tuyên bố rằng các nhà nghiên cứu, người gán nhãn chúng tôi thuê, hoặc khách hàng API của chúng tôi là nguồn sở thích đúng. Có nhiều bên liên quan cần xem xét—tổ chức huấn luyện mô hình, khách hàng sử dụng mô hình để phát triển sản phẩm, người dùng cuối của các sản phẩm này, và dân số rộng hơn có thể bị ảnh hưởng trực tiếp hoặc gián tiếp. Không chỉ là vấn đề làm cho quy trình căn chỉnh tham gia hơn; không thể huấn luyện một hệ thống được căn chỉnh với sở thích của mọi người cùng một lúc, hoặc nơi mọi người sẽ ủng hộ các đánh đổi.

Một con đường tiến về phía trước có thể là huấn luyện các mô hình có thể được điều kiện hóa trên sở thích của các nhóm nhất định, hoặc có thể dễ dàng được tinh chỉnh hoặc nhắc để đại diện cho các nhóm khác nhau. Các mô hình khác nhau sau đó có thể được triển khai và sử dụng bởi các nhóm ủng hộ các giá trị khác nhau. Tuy nhiên, các mô hình này vẫn có thể ảnh hưởng đến xã hội rộng hơn và có rất nhiều quyết định khó khăn cần đưa ra liên quan đến sở thích của ai để điều kiện hóa, và làm thế nào để đảm bảo rằng tất cả các nhóm có thể được đại diện và có thể từ chối các quy trình có thể có hại.

5.3 Hạn chế

Phương pháp luận. Hành vi của các mô hình InstructGPT của chúng tôi được xác định một phần bởi phản hồi con người thu được từ các nhà thầu của chúng tôi. Một số nhiệm vụ gán nhãn dựa trên các phán đoán giá trị có thể bị ảnh hưởng bởi danh tính của các nhà thầu của chúng tôi, niềm tin, nền tảng văn hóa, và lịch sử cá nhân của họ. Chúng tôi thuê khoảng 40 nhà thầu, được hướng dẫn bởi hiệu suất của họ trong một bài kiểm tra sàng lọc nhằm đánh giá mức độ họ có thể xác định và phản hồi các lời nhắc nhạy cảm, và tỷ lệ đồng ý của họ với các nhà nghiên cứu trong một nhiệm vụ gán nhãn với hướng dẫn chi tiết (xem Phụ lục B). Chúng tôi giữ nhóm nhà thầu của mình nhỏ vì điều này tạo điều kiện giao tiếp băng thông cao với một nhóm nhà thầu nhỏ hơn đang làm nhiệm vụ toàn thời gian. Tuy nhiên, nhóm này rõ ràng không đại diện cho toàn bộ phổ người sẽ sử dụng và bị ảnh hưởng bởi các mô hình được triển khai của chúng tôi. Như một ví dụ đơn giản, người gán nhãn của chúng tôi chủ yếu nói tiếng Anh và dữ liệu của chúng tôi bao gồm gần như hoàn toàn các hướng dẫn tiếng Anh.

Cũng có nhiều cách chúng tôi có thể cải thiện thiết lập thu thập dữ liệu của mình. Ví dụ, hầu hết so sánh chỉ được gán nhãn bởi 1 nhà thầu vì lý do chi phí. Việc có các ví dụ được gán nhãn nhiều lần có thể giúp xác định các lĩnh vực mà các nhà thầu của chúng tôi không đồng ý, và do đó nơi một mô hình duy nhất không có khả năng căn chỉnh với tất cả họ. Trong các trường hợp không đồng ý, việc căn chỉnh với sở thích người gán nhãn trung bình có thể không mong muốn. Ví dụ, khi tạo ra văn bản ảnh hưởng không tỷ lệ đến một nhóm thiểu số, chúng tôi có thể muốn sở thích của người gán nhãn thuộc nhóm đó được cân nhắc nặng hơn.

Các mô hình. Các mô hình của chúng tôi không hoàn toàn được căn chỉnh cũng không hoàn toàn an toàn; chúng vẫn tạo ra đầu ra độc hại hoặc thiên vị, bịa đặt sự thật, và tạo ra nội dung tình dục và bạo lực mà không cần nhắc rõ ràng. Chúng cũng có thể không tạo ra đầu ra hợp lý trên một số đầu vào; chúng tôi hiển thị một số ví dụ về điều này trong Hình 9. Có lẽ hạn chế lớn nhất của các mô hình của chúng tôi là, trong hầu hết các trường hợp, chúng tuân theo hướng dẫn của người dùng, ngay cả khi điều đó có thể dẫn đến tổn hại trong thế giới thực. Ví dụ, khi được đưa ra lời nhắc hướng dẫn các mô hình có thiên vị tối đa, InstructGPT tạo ra đầu ra độc hại hơn so với các mô hình GPT-3 có kích thước tương đương. Chúng tôi thảo luận các biện pháp giảm thiểu tiềm năng trong các phần sau.

5.4 Câu hỏi mở

Công việc này là bước đầu tiên hướng tới việc sử dụng các kỹ thuật căn chỉnh để tinh chỉnh các mô hình ngôn ngữ để tuân theo một loạt rộng các hướng dẫn. Có nhiều câu hỏi mở để khám phá để căn chỉnh thêm hành vi mô hình ngôn ngữ với những gì mọi người thực sự muốn chúng làm.

Nhiều phương pháp có thể được thử để giảm thêm xu hướng của các mô hình tạo ra đầu ra độc hại, thiên vị, hoặc có hại khác. Ví dụ, người ta có thể sử dụng thiết lập đối kháng nơi người gán nhãn tìm ra các hành vi trường hợp xấu nhất của mô hình, sau đó được gán nhãn và thêm vào bộ dữ liệu (Dinan et al., 2019b). Người ta cũng có thể kết hợp phương pháp của chúng tôi với các cách lọc dữ liệu pretraining (Ngo et al., 2021), hoặc cho huấn luyện các mô hình pretrained ban đầu, hoặc cho dữ liệu chúng tôi sử dụng cho phương pháp pretraining mix của chúng tôi. Tương tự, người ta có thể kết hợp cách tiếp cận của chúng tôi với các phương pháp cải thiện tính trung thực của mô hình, như WebGPT (Nakano et al., 2021).

Trong công việc này, nếu người dùng yêu cầu phản hồi có thể có hại hoặc không trung thực, chúng tôi cho phép mô hình của chúng tôi tạo ra các đầu ra này. Huấn luyện mô hình của chúng tôi để vô hại mặc dù hướng dẫn của người dùng quan trọng, nhưng cũng khó khăn vì liệu một đầu ra có hại phụ thuộc vào bối cảnh nó được triển khai; ví dụ, có thể có lợi khi sử dụng các mô hình ngôn ngữ để tạo ra đầu ra độc hại như một phần của pipeline tăng cường dữ liệu. Các kỹ thuật của chúng tôi cũng có thể được áp dụng để làm cho các mô hình từ chối một số hướng dẫn của người dùng, và chúng tôi dự định khám phá điều này trong các lần lặp tiếp theo của nghiên cứu này.

Việc làm cho các mô hình làm những gì chúng ta muốn có liên quan trực tiếp đến văn học về khả năng điều khiển và kiểm soát (Dathathri et al., 2019; Krause et al., 2020). Một con đường tương lai đầy hứa hẹn là kết hợp RLHF với các phương pháp khả năng điều khiển khác, ví dụ sử dụng mã kiểm soát (Keskar et al., 2019), hoặc sửa đổi thủ tục lấy mẫu tại thời gian suy luận bằng một mô hình nhỏ hơn (Dathathri et al., 2019).

Trong khi chúng tôi chủ yếu tập trung vào RLHF, có nhiều thuật toán khác có thể được sử dụng để huấn luyện chính sách trên dữ liệu minh họa và so sánh của chúng tôi để có được kết quả thậm chí tốt hơn. Ví dụ, người ta có thể khám phá expert iteration (Anthony et al., 2017; Silver et al., 2017), hoặc các phương pháp behavior cloning đơn giản hơn sử dụng một tập con dữ liệu so sánh. Người ta cũng có thể thử các phương pháp tối ưu hóa có ràng buộc (Achiam et al., 2017) tối đa hóa điểm từ một mô hình phần thưởng với điều kiện tạo ra một số lượng nhỏ hành vi có hại.

So sánh cũng không nhất thiết là cách hiệu quả nhất để cung cấp tín hiệu căn chỉnh. Ví dụ, chúng tôi có thể để người gán nhãn chỉnh sửa phản hồi mô hình để làm cho chúng tốt hơn, hoặc tạo ra các phê bình về phản hồi mô hình bằng ngôn ngữ tự nhiên. Cũng có một không gian rộng lớn các tùy chọn để thiết kế giao diện cho người gán nhãn cung cấp phản hồi cho các mô hình ngôn ngữ; đây là một vấn đề tương tác người-máy tính thú vị.

Đề xuất của chúng tôi để giảm thiểu thuế căn chỉnh, bằng cách kết hợp dữ liệu pretraining vào tinh chỉnh RLHF, không hoàn toàn giảm thiểu suy giảm hiệu suất, và có thể làm cho một số hành vi không mong muốn nhất định có khả năng xảy ra hơn đối với một số nhiệm vụ (nếu các hành vi này có trong dữ liệu pretraining). Đây là một lĩnh vực thú vị cho nghiên cứu thêm. Một sửa đổi khác có thể cải thiện phương pháp của chúng tôi là lọc dữ liệu pretraining mix cho nội dung độc hại (Ngo et al., 2021), hoặc bổ sung dữ liệu này bằng hướng dẫn tổng hợp.

Như được thảo luận chi tiết trong Gabriel (2020), có những khác biệt tinh tế giữa việc căn chỉnh với hướng dẫn, ý định, sở thích được tiết lộ, sở thích lý tưởng, lợi ích, và giá trị. Gabriel (2020) ủng hộ một cách tiếp cận dựa trên nguyên tắc đối với căn chỉnh: nói cách khác, để xác định "các nguyên tắc căn chỉnh công bằng nhận được sự ủng hộ phản ánh mặc dù có sự khác biệt rộng rãi trong niềm tin đạo đức của mọi người." Trong bài báo của chúng tôi, chúng tôi căn chỉnh với ý định người dùng được suy luận để đơn giản, nhưng cần nhiều nghiên cứu hơn trong lĩnh vực này.

Thật vậy, một trong những câu hỏi mở lớn nhất là làm thế nào để thiết kế một quy trình căn chỉnh minh bạch, đại diện có ý nghĩa cho những người bị ảnh hưởng bởi công nghệ, và tổng hợp giá trị của mọi người theo cách đạt được sự đồng thuận rộng rãi giữa nhiều nhóm. Chúng tôi thảo luận một số cân nhắc liên quan trong Phần 5.2.

5.5 Tác động rộng hơn

Công việc này được thúc đẩy bởi mục tiêu tăng tác động tích cực của các mô hình ngôn ngữ lớn bằng cách huấn luyện chúng làm những gì một tập hợp con người nhất định muốn chúng làm. Theo mặc định, các mô hình ngôn ngữ tối ưu hóa mục tiêu dự đoán từ tiếp theo, chỉ là một proxy cho những gì chúng ta muốn các mô hình này làm. Kết quả của chúng tôi chỉ ra rằng các kỹ thuật của chúng tôi có triển vọng để làm cho các mô hình ngôn ngữ hữu ích, trung thực, và vô hại hơn. Về lâu dài, các thất bại căn chỉnh có thể dẫn đến hậu quả nghiêm trọng hơn, đặc biệt nếu các mô hình này được triển khai trong các tình huống quan trọng về an toàn. Chúng tôi hy vọng rằng khi việc mở rộng mô hình tiếp tục, cần phải chăm sóc hơn để đảm bảo rằng chúng được căn chỉnh với ý định con người (Bostrom, 2014).

Tuy nhiên, việc làm cho các mô hình ngôn ngữ tốt hơn trong việc tuân theo ý định người dùng cũng làm cho chúng dễ bị lạm dụng hơn. Có thể dễ dàng hơn để sử dụng các mô hình này để tạo ra thông tin sai lệch thuyết phục, hoặc nội dung thù địch hoặc lạm dụng.

Các kỹ thuật căn chỉnh không phải là một liều thuốc chữa bách bệnh để giải quyết các vấn đề an toàn liên quan đến các mô hình ngôn ngữ lớn; thay vào đó, chúng nên được sử dụng như một công cụ trong một hệ sinh thái an toàn rộng hơn. Ngoài việc lạm dụng cố ý, có nhiều miền mà các mô hình ngôn ngữ lớn chỉ nên được triển khai với sự cẩn thận lớn, hoặc không được triển khai chút nào. Các ví dụ bao gồm các miền có rủi ro cao như chẩn đoán y tế, phân loại người dựa trên các đặc điểm được bảo vệ, xác định tính đủ điều kiện cho tín dụng, việc làm, hoặc nhà ở, tạo ra quảng cáo chính trị, và thực thi pháp luật. Nếu các mô hình này được mã nguồn mở, việc hạn chế các ứng dụng có hại trong các miền này và khác trở nên thách thức mà không có quy định phù hợp. Mặt khác, nếu quyền truy cập mô hình ngôn ngữ lớn bị hạn chế đối với một vài tổ chức có tài nguyên cần thiết để huấn luyện chúng, điều này loại trừ hầu hết mọi người khỏi quyền truy cập vào công nghệ ML tiên tiến. Một tùy chọn khác là một tổ chức sở hữu cơ sở hạ tầng đầu cuối của việc triển khai mô hình, và làm cho nó có thể truy cập thông qua API. Điều này cho phép thực hiện các giao thức an toàn như hạn chế trường hợp sử dụng (chỉ cho phép mô hình được sử dụng cho các ứng dụng nhất định), giám sát lạm dụng và thu hồi quyền truy cập đối với những người lạm dụng hệ thống, và giới hạn tỷ lệ để ngăn chặn việc tạo ra thông tin sai lệch quy mô lớn. Tuy nhiên, điều này có thể đi kèm với cái giá là giảm tính minh bạch và tăng tập trung quyền lực vì nó đòi hỏi nhà cung cấp API phải đưa ra quyết định về nơi vẽ ranh giới cho từng câu hỏi này.

Cuối cùng, như đã thảo luận trong Phần 5.2, câu hỏi về việc các mô hình này được căn chỉnh với ai là cực kỳ quan trọng, và sẽ ảnh hưởng đáng kể đến việc liệu tác động ròng của các mô hình này có tích cực hay tiêu cực.

Lời cảm ơn

Đầu tiên, chúng tôi muốn cảm ơn Lilian Weng, Jason Kwon, Boris Power, Che Chang, Josh Achiam, Steven Adler, Gretchen Krueger, Miles Brundage, Tyna Eloundou, Gillian Hadfield, Irene Soliaman, Christy Dennison, Daniel Ziegler, William Saunders, Beth Barnes, Cathy Yeh, Nick Cammaratta, Jonathan Ward, Matt Knight, Pranav Shyam, Alec Radford, và những người khác tại OpenAI cho các cuộc thảo luận trong suốt quá trình dự án đã giúp định hình hướng nghiên cứu của chúng tôi. Chúng tôi cảm ơn Brian Green, Irina Raicu, Subbu Vincent, Varoon Mathur, Kate Crawford, Su Lin Blodgett, Bertie Vidgen, và Paul Röttger cho các cuộc thảo luận và phản hồi về cách tiếp cận của chúng tôi. Cuối cùng, chúng tôi cảm ơn Sam Bowman, Matthew Rahtz, Ben Mann, Liam Fedus, Helen Ngo, Josh Achiam, Leo Gao, Jared Kaplan, Cathy Yeh, Miles Brundage, Gillian Hadfield, Cooper Raterink, Gretchen Krueger, Tyna Eloundou, Rafal Jakubanis, và Steven Adler đã cung cấp phản hồi về bài báo này. Chúng tôi cũng muốn cảm ơn Owain Evans và Stephanie Lin đã chỉ ra thực tế rằng các thước đo TruthfulQA tự động đang phóng đại các lợi ích của các mô hình PPO của chúng tôi.

Cảm ơn những người đã đóng góp theo nhiều cách khác nhau vào cơ sở hạ tầng được sử dụng để huấn luyện và triển khai các mô hình của chúng tôi, bao gồm: Daniel Ziegler, William Saunders, Brooke Chan, Dave Cummings, Chris Hesse, Shantanu Jain, Michael Petrov, Greg Brockman, Felipe Such, Alethea Power, và toàn bộ nhóm siêu máy tính OpenAI. Chúng tôi cũng muốn cảm ọn Suchir Balaji đã giúp tái hiệu chuẩn, Alper Ercetin và Justin Wang đã thiết kế sơ đồ chính trong bài báo này, và nhóm Comms OpenAI đã giúp phát hành, bao gồm: Steve Dowling, Hannah Wong, Natalie Summers, và Elie Georges.

Cuối cùng, chúng tôi muốn cảm ơn những người gán nhãn của chúng tôi, những người mà không có họ công việc này sẽ không thể thực hiện được: Meave Fryer, Sara Tirmizi, James Carroll, Jian Ouyang, Michelle Brothers, Conor Agnew, Joe Kwon, John Morton, Emma Duncan, Delia Randolph, Kaylee Weeks, Alexej Savreux, Siam Ahsan, Rashed Sorwar, Atresha Singh, Muhaiminul Rukshat, Caroline Oliveira, Juan Pablo Castaño Rendón, Atqiya Abida Anjum, Tinashe Mapolisa, Celeste Fejzo, Caio Oleskovicz, Salahuddin Ahmed, Elena Green, Ben Harmelin, Vladan Djordjevic, Victoria Ebbets, Melissa Mejia, Emill Jayson Caypuno, Rachelle Froyalde, Russell M. Bernandez, Jennifer Brillo, Jacob Bryan, Carla Rodriguez, Evgeniya Rabinovich, Morris Stuttard, Rachelle Froyalde, Roxanne Addison, Sarah Nogly, Chait Singh.

[Phần tài liệu tham khảo và các phụ lục tiếp theo sẽ được dịch tương tự...]
