# 2310.02743.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2310.02743.pdf
# Kích thước tệp: 4630232 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
CÁC ENSEMBLE MÔ HÌNH PHẦN THƯỞNG GIÚP GIẢM THIỂU
OVEROPTIMIZATION
Thomas Coste1, Usman Anwar1, Robert Kirk2, David Krueger1
1University of Cambridge,2University College London
{tc628,ua237,dsk30 }@cam.ac.uk ,robert.kirk.20@ucl.ac.uk
TÓM TẮT
Học tăng cường từ phản hồi của con người (RLHF) là một phương pháp tiêu chuẩn để
tinh chỉnh các mô hình ngôn ngữ lớn nhằm tuân theo hướng dẫn. Như một phần của quá trình này,
các mô hình phần thưởng đã học được sử dụng để mô hình hóa gần đúng sở thích của con người. Tuy nhiên,
như những biểu diễn không hoàn hảo của phần thưởng "thực", những mô hình phần thưởng đã học này
dễ bị overoptimization. Gao et al. (2023) nghiên cứu hiện tượng này
trong một thiết lập phản hồi con người tổng hợp với một mô hình phần thưởng "vàng" lớn hơn đáng kể
đóng vai trò là phần thưởng thực (thay vì con người) và cho thấy rằng overoptimization
vẫn là một vấn đề dai dẳng bất kể kích thước của mô hình phần thưởng proxy
và dữ liệu huấn luyện được sử dụng. Sử dụng một thiết lập tương tự, chúng tôi tiến hành một nghiên cứu
hệ thống để đánh giá hiệu quả của việc sử dụng các mục tiêu tối ưu hóa bảo thủ dựa trên ensemble,
cụ thể là tối ưu hóa trường hợp xấu nhất (WCO) và tối ưu hóa có trọng số bất định (UWO),
để giảm thiểu overoptimization mô hình phần thưởng khi sử dụng
hai phương pháp tối ưu hóa: (a) lấy mẫu best-of-n (BoN) (b) tối ưu hóa chính sách gần đúng (PPO).
Chúng tôi bổ sung mở rộng thiết lập của Gao et al. (2023) để bao gồm
25% nhiễu nhãn để phản ánh tốt hơn các điều kiện thế giới thực. Cả với và không có
nhiễu nhãn, chúng tôi thấy rằng tối ưu hóa bảo thủ thực tế loại bỏ overoptimization
và cải thiện hiệu suất lên đến 70% cho lấy mẫu BoN. Đối với PPO,
tối ưu hóa bảo thủ dựa trên ensemble luôn giảm overoptimization và
vượt trội hơn tối ưu hóa mô hình phần thưởng đơn. Hơn nữa, kết hợp nó với
một hình phạt KL nhỏ thành công ngăn chặn overoptimization mà không có chi phí hiệu suất.
Nhìn chung, kết quả của chúng tôi chứng minh rằng tối ưu hóa bảo thủ dựa trên ensemble
có thể chống lại overoptimization một cách hiệu quả.

1 GIỚI THIỆU
Với sự ra đời của các mô hình ngôn ngữ lớn, học tăng cường từ phản hồi của con người (RLHF) đã
nổi lên như một kỹ thuật mạnh mẽ để tinh chỉnh và tăng cường hành vi của mô hình (Ziegler et al., 2019;
Ouyang et al., 2022; Bai et al., 2022a). Tuy nhiên, mặc dù thành công về mặt thực nghiệm, RLHF vẫn
là một phương pháp khó nắm bắt gặp phải nhiều chế độ thất bại (Casper et al., 2023). Một chế độ thất bại như vậy
là overoptimization, một hiện tượng trong đó tối ưu hóa chính sách dường như đang tiến bộ
theo mô hình phần thưởng đã học, nhưng thực tế bắt đầu thoái lui so với hàm phần thưởng thực sự
(Ziegler et al., 2019; Stiennon et al., 2020; Gao et al., 2023). Trong khi nhiều công trình về RLHF
chứa bằng chứng giai thoại về overoptimization (Ziegler et al., 2019; Stiennon et al., 2020; Dubois
et al., 2023), Gao et al. (2023) là công trình duy nhất nghiên cứu overoptimization một cách hệ thống.
Vì làm việc trực tiếp với những người gắn nhãn của con người rất tốn kém, Gao et al. (2023) giới thiệu một thiết lập tổng hợp
để nghiên cứu overoptimization trong đó một mô hình ngôn ngữ lớn hơn nhiều được huấn luyện trước như một mô hình phần thưởng "vàng"
và sau đó được sử dụng để tạo nhãn sở thích cho việc huấn luyện các mô hình phần thưởng proxy.

Trong công trình này, chúng tôi tiến hành một nghiên cứu hệ thống điều tra liệu việc kết hợp ensemble với
tối ưu hóa bảo thủ có thể giúp giảm thiểu overoptimization hay không. Kết quả của chúng tôi chỉ ra rằng không chỉ
tối ưu hóa bảo thủ dựa trên ensemble giúp giảm thiểu overoptimization, mà nó còn dẫn đến
hiệu suất được cải thiện. Thiết lập của chúng tôi giống hệt với Gao et al. (2023) với một sửa đổi:
việc bổ sung nhiễu nhãn. Gao et al. giả định rằng các nhãn sở thích được sử dụng để huấn luyện mô hình phần thưởng proxy
không chứa bất kỳ nhiễu nào. Tuy nhiên, điều này không phản ánh thiết lập RLHF thế giới thực,
trong đó tỷ lệ đồng ý giữa các người chú thích con người thường là giữa 60−75% (Ziegler et al.,
Mã nguồn có sẵn tại: https://github.com/tlc4418/llm_optimization.
1arXiv:2310.02743v2 [cs.LG] 10 Mar 2024

--- TRANG 2 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Hình 1: Pipeline RLHF được sử dụng trong công trình này - các sửa đổi của chúng tôi trên thiết lập RLHF tiêu chuẩn
được sử dụng trong Gao et al. (2023) được tô sáng bằng màu xanh lá.
2019; Stiennon et al., 2020; Dubois et al., 2023). Để mô phỏng sự bất đồng đó và phản ánh tốt hơn
RLHF thế giới thực, chúng tôi mở rộng thiết lập để bao gồm 25% nhiễu nhãn. Trong cả hai trường hợp
không có nhiễu nhãn và 25% nhiễu nhãn, chúng tôi cung cấp bằng chứng mạnh mẽ rằng các phương pháp tối ưu hóa bảo thủ
dựa trên ensemble có hiệu quả trong việc giảm thiểu overoptimization và cải thiện hiệu suất.

Quy luật mở rộng cho overoptimization mô hình phần thưởng được khám phá bởi Gao et al. (2023) chỉ ra rằng
tăng kích thước của mô hình phần thưởng proxy cũng giảm overoptimization. Tuy nhiên, các mô hình phần thưởng
được dẫn xuất từ các mô hình ngôn ngữ được huấn luyện trước. Do đó, việc có được một mô hình phần thưởng lớn hơn sẽ
yêu cầu pretraining đáng kể, điều này không phải lúc nào cũng khả thi và có thể rất tốn kém (Morgan, 2022;
Venigalla & Li, 2022). Tuy nhiên, phương pháp của chúng tôi, sử dụng ensemble của các mô hình phần thưởng, chỉ yêu cầu
tinh chỉnh nhiều bản sao của một mô hình phần thưởng đã được huấn luyện trước, điều này tương đối không tốn kém.
Hơn nữa, kết quả mở rộng mô hình và dữ liệu của chúng tôi (Hình 8 và 9) chỉ ra rằng các lợi ích do
phương pháp của chúng tôi cung cấp là trực giao với các lợi ích đạt được bằng cách tăng kích thước mô hình phần thưởng;
do đó, hai phương pháp có thể được kết hợp một cách liền mạch để có kết quả tốt hơn nữa.

Các đóng góp chính của chúng tôi như sau:
• Chúng tôi tiến hành nghiên cứu đầu tiên về việc sử dụng ensemble để chống lại overoptimization trong
tinh chỉnh mô hình ngôn ngữ dựa trên RLHF.
• Kết quả của chúng tôi chỉ ra rằng việc sử dụng ensemble và tối ưu hóa bảo thủ loại bỏ overoptimization
cho BoN và dẫn đến cải thiện lên đến 70% trong một số trường hợp.
• Đối với PPO, tối ưu hóa bảo thủ dựa trên ensemble thường vượt trội hơn tối ưu hóa mô hình phần thưởng đơn,
và khi kết hợp với trọng số hình phạt KL phù hợp thành công loại bỏ overoptimization.
• Chúng tôi tiến hành thêm các nghiên cứu để thiết lập tính mạnh mẽ của các phương pháp tối ưu hóa bảo thủ
dựa trên ensemble đối với bất kỳ siêu tham số mới nào mà nó giới thiệu (ví dụ: kích thước của ensemble).

2 KIẾN THỨC NỀN TẢNG
Trong phần này, chúng tôi xem xét ngắn gọn hai phương pháp tối ưu hóa chính sách thường được sử dụng: lấy mẫu best-of-n
(BoN) và tối ưu hóa chính sách gần đúng (PPO), tiếp theo là thảo luận về overoptimization.

2.1 LẤY MẪU BEST-OF-N (BON)
Lấy mẫu best-of-n (BoN), còn được gọi là lấy mẫu từ chối, là một phương pháp tối ưu hóa thời gian suy luận đơn giản
(Ouyang et al., 2022; Nakano et al., 2021). Đối với một prompt đã cho, n phản hồi được tạo ra
từ mô hình chính sách, và câu trả lời có điểm số mô hình phần thưởng proxy cao nhất được trả về. Để
đánh giá mức độ tối ưu hóa, khoảng cách KL được định nghĩa phân tích như một hàm của n:
KL bon=logn−n−1
n(1)
2

--- TRANG 3 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
2.2 TỐI ƯU HÓA CHÍNH SÁCH GẦN ĐÚNG (PPO)
Tối ưu hóa chính sách gần đúng (Schulman et al., 2017) là một phương pháp học tăng cường trực tuyến dựa trên
policy-gradient mà tối đa hóa một hàm phần thưởng đã cho bằng cách liên tục thực hiện các cập nhật
gia tăng nhỏ cho chính sách. PPO là thuật toán tiêu chuẩn được sử dụng trong tinh chỉnh các mô hình ngôn ngữ
dựa trên phản hồi của con người (Ouyang et al., 2022; Bai et al., 2022a; Stiennon et al., 2020; Zheng et al.,
2023). Khi sử dụng PPO để tinh chỉnh một mô hình ngôn ngữ, một hạng phạt KL được thêm vào trong
tính toán phần thưởng để điều chỉnh chính sách bằng cách ngăn nó lệch xa khỏi chính sách ban đầu:
RPPO(q, a) =R(q, a)−βlogπPPO(a|q)
πinit(a|q)
(2)
trong đó πPPO là chính sách đang được tối ưu hóa và πinit là mô hình ngôn ngữ ban đầu (được huấn luyện trước).
Mức độ tối ưu hóa được đo bằng khoảng cách KL giữa chính sách ban đầu và chính sách
đang được tối ưu hóa, với chi tiết tính toán này được cung cấp trong Phụ lục C.

2.3 OVEROPTIMIZATION
Trong học tăng cường từ phản hồi của con người (RLHF), một mô hình phần thưởng được sử dụng để xấp xỉ
sở thích của con người, để loại bỏ nhu cầu truy vấn con người cho mỗi lần tạo chính sách. Vì
mô hình phần thưởng đã học chỉ là một proxy cho hàm phần thưởng thực sự, việc tối ưu hóa nó có thể không
luôn dẫn đến cải thiện theo sở thích thực sự của con người. Trong thực tế, việc tối ưu hóa một mô hình phần thưởng
đã học (cố định) hầu như luôn dẫn đến cải thiện theo mô hình phần thưởng đã học này,
nhưng chỉ cải thiện theo mô hình phần thưởng thực sự (tức là con người) trong một khoảng thời gian ban đầu,
sau đó hiệu suất thường bắt đầu thoái lui. Hiện tượng này được gọi là overoptimization,
một ví dụ được thể hiện trong Hình 2.

Hình 2: Một ví dụ về overoptimization.
Độ phân kỳ KL là khoảng cách
giữa chính sách ban đầu và hiện tại,
đo mức độ tối ưu hóa. Để nghiên cứu vấn đề overoptimization, Gao et al.
(2023) giới thiệu một thiết lập tổng hợp trong đó, thay vì
các người chú thích con người, một mô hình phần thưởng vàng được sử dụng để chấm điểm
phản hồi và tạo ra sở thích để huấn luyện các mô hình phần thưởng proxy.
Mô hình phần thưởng vàng thường được chọn
lớn hơn nhiều so với mô hình phần thưởng proxy, để mô phỏng
thực tế rằng trong một thiết lập thực tế, sở thích của con người quá
phức tạp để được nắm bắt bởi một mạng neural với dung lượng
hữu hạn. Trong thiết lập này, Gao et al. (2023) khám phá ra
overoptimization là một vấn đề dai dẳng, mặc dù họ
lưu ý rằng kích thước mô hình phần thưởng proxy lớn hơn và lượng
dữ liệu huấn luyện nhiều hơn có thể giúp giảm overoptimization.
Tuy nhiên, việc mở rộng quy mô không phải lúc nào cũng là một giải pháp
khả thi vì các mô hình phần thưởng proxy thường được dẫn xuất
từ các mô hình ngôn ngữ được huấn luyện trước rộng rãi.

3 PHƯƠNG PHÁP
RLHF tiêu chuẩn thường học một mô hình phần thưởng đơn để ước lượng phần thưởng thực sự, sau đó được sử dụng
để tối ưu hóa chính sách. Tuy nhiên, nhiều công trình trong machine learning rộng hơn đã chỉ ra rằng việc học
nhiều ước lượng và kết hợp chúng có thể giúp cải thiện tính mạnh mẽ (Lakshminarayanan et al.,
2017; Ovadia et al., 2019; Yu et al., 2020b). Lấy cảm hứng từ cái nhìn sâu sắc này, chúng tôi đề xuất
học một ensemble của các mô hình phần thưởng {R1, ..., Rk} trong giai đoạn huấn luyện mô hình phần thưởng.
Trong quá trình tối ưu hóa chính sách, chúng tôi kết hợp các ước lượng phần thưởng từ các mô hình phần thưởng
khác nhau trong ensemble theo ba phương pháp sau.

Tối ưu hóa trung bình: Tối ưu hóa trung bình (Boyd & Vandenberghe, 2004; Wright, 2006) đơn giản lấy
trung bình của các đầu ra của các thành viên ensemble khác nhau:
Rµ(q, a) :=1
kkX
i=1Ri(q, a) (3)
trong đó q là prompt được đưa cho mô hình ngôn ngữ và a là phản hồi tương ứng được lấy mẫu từ
mô hình ngôn ngữ.
3

--- TRANG 4 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Chúng tôi lưu ý rằng tối ưu hóa trung bình không phải là một kỹ thuật tối ưu hóa bảo thủ; nếu ít nhất một thành viên
của ensemble ước lượng quá cao phần thưởng (ngay cả khi các thành viên khác ước lượng chính xác), tối ưu hóa trung bình
sẽ không ngăn chính sách khai thác mô hình phần thưởng bị lỗi.

Tối ưu hóa trường hợp xấu nhất: Tối ưu hóa trường hợp xấu nhất (WCO) (Boyd & Vandenberghe, 2004;
Wright, 2006) tạo ra một ước lượng bảo thủ bằng cách chọn phần thưởng thấp nhất từ ensemble
ở mỗi bước. Việc chọn phần thưởng thấp nhất giúp đảm bảo rằng miễn là ít nhất một thành viên ensemble
không ước lượng quá cao phần thưởng thực sự, tối ưu hóa chính sách sẽ không dẫn đến overoptimization.
RWCO(q, a) := min
iRi(q, a) (4)
Một lợi thế chính của WCO là nó không có bất kỳ siêu tham số nào có thể cần điều chỉnh.
Tuy nhiên, đôi khi nó có thể dẫn đến hình phạt hiệu suất do bản chất cực kỳ bảo thủ của nó.

Tối ưu hóa có trọng số bất định: Trong tối ưu hóa có trọng số bất định (UWO) (Wu et al.,
2021b; Yu et al., 2020b; Brantley et al., 2019), phần thưởng cho một mẫu được tính bằng cách kết hợp
phần thưởng trung bình trên tất cả các mô hình trong ensemble với phương sai nội ensemble, được cân nhắc
bởi một hệ số λ. Theo trực giác, UWO hoạt động bằng cách phạt chính sách vì tạo ra các phản hồi
mà có sự bất đồng cao giữa các mô hình phần thưởng trong ensemble. Điều này giúp ngăn chặn
việc khai thác một mô hình phần thưởng bị lỗi đơn lẻ có thể đang gán sai
phần thưởng cao cho các phản hồi không chính xác hoặc chất lượng thấp. Về mặt toán học, mục tiêu này được cho bởi:
RUWO(q, a) :=1
kX
iRi(q, a)
|{z }
trung bình−λ1
kX
i 
Ri(q, a)−1
kX
iRi(q, a)!2
| {z }
phương sai(5)
trong đó λ là một siêu tham số điều khiển trọng số của thành phần bất định.

4 THIẾT LẬP THỰC NGHIỆM
Chúng tôi dựa thí nghiệm của mình trên thiết lập được đề xuất bởi Gao et al. (2023), tuy nhiên, chúng tôi sử dụng các mô hình
mã nguồn mở (Biderman et al., 2023; Dubois et al., 2023), bộ dữ liệu mã nguồn mở (Taori et al., 2023), và
huấn luyện các mô hình phần thưởng proxy của chúng tôi dưới 25% nhiễu nhãn. Chúng tôi cũng trình bày sự tái tạo định tính
kết quả của Gao et al. trong Phụ lục E.

4.1 DỮ LIỆU
Để huấn luyện các mô hình phần thưởng proxy, chúng tôi sử dụng bộ dữ liệu Alpaca (Taori et al., 2023), với 52,000
hướng dẫn bao gồm một loạt các lệnh và các trình diễn tương ứng được tạo bởi
text-davinci-003 của OpenAI (OpenAI, 2023a). Mỗi mục được tạo thành từ một hướng dẫn, một
đầu vào bổ sung tùy chọn, và một đầu ra trình diễn. Cụ thể hơn, chúng tôi sử dụng
biến thể AlpacaFarm (Dubois et al., 2023) của bộ dữ liệu, vì nó cung cấp các phần chia để sử dụng trong
các giai đoạn RLHF khác nhau và để xác thực, cũng như các chú thích sở thích của con người. Các chi tiết
thêm về các phần chia, định dạng prompt, và ví dụ được đưa ra trong Phụ lục D.

4.2 CÁC MÔ HÌNH ĐÃ ĐƯỢC HUẤN LUYỆN TRƯỚC
Đối với mô hình chính sách và mô hình phần thưởng (proxy), chúng tôi sử dụng các mô hình ngôn ngữ được huấn luyện trước
được cung cấp trong bộ Pythia (Biderman et al., 2023). Mô hình chính sách được sử dụng ở khắp mọi nơi trong
công trình này là mô hình Pythia 1.4B. Đối với các mô hình phần thưởng proxy, chúng tôi loại bỏ các lớp unembedding
từ các mô hình Pythia có kích thước 14M, 70M, và 1.4B và thêm một đầu scalar để đầu ra phần thưởng
để có được các mô hình phần thưởng proxy có kích thước 7M, 44M, và 1.3B.

Cuối cùng, mô hình phần thưởng sở thích con người 7B từ AlpacaFarm (Dubois et al., 2023) được sử dụng như
mô hình phần thưởng vàng. Nó có kích thước rất tương tự với mô hình phần thưởng vàng 6.9B (nguồn đóng)
được sử dụng trong Gao et al. (2023) và lớn hơn đáng kể so với bất kỳ mô hình phần thưởng proxy nào của chúng tôi
(với mô hình lớn nhất là 1.3B) và do đó là một lựa chọn hợp lý cho tiêu chuẩn vàng.
4

--- TRANG 5 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
4.3 PIPELINE RLHF
Pipeline RLHF của chúng tôi tương tự như của Gao et al. (2023) với một số sửa đổi được tô sáng trong
Hình 1. Chúng tôi giải thích thiết lập đầy đủ dưới đây để hoàn thiện.

Tinh chỉnh có giám sát: Như bước đầu tiên trong pipeline RLHF của chúng tôi, cả mô hình chính sách và
mô hình phần thưởng proxy đều trải qua tinh chỉnh có giám sát sử dụng 10k hướng dẫn trình diễn từ
phần "sft" của bộ dữ liệu AlpacaFarm (xem Phần 4.1 và Phụ lục D.2 để biết chi tiết). Điều này tinh chỉnh
các mô hình để có khả năng tuân theo hướng dẫn tốt hơn.

Tạo sở thích và gán nhãn: Để tạo ra một bộ dữ liệu sở thích, mô hình SFT được nhắc
sử dụng các hướng dẫn từ bộ dữ liệu AlpacaFarm, mà nó tạo ra hai phản hồi cho mỗi
hướng dẫn. Các siêu tham số liên quan để lấy mẫu các phản hồi này được đưa ra trong Phụ lục D.1. Hai
phản hồi sau đó được chấm điểm với mô hình phần thưởng vàng, mà gán một điểm số cho mỗi phản hồi.
Các người chú thích con người có xu hướng có tỷ lệ bất đồng cao, thường khoảng 25% (Stiennon et al., 2020) hoặc
nhiều hơn (Ziegler et al., 2019; Dubois et al., 2023). Để mô phỏng điều này, chúng tôi tùy chọn gán nhãn sai 25%
bộ dữ liệu. Điều này dẫn đến hai bộ dữ liệu: một không có nhiễu nhãn và một có 25% nhiễu nhãn.

Huấn luyện mô hình phần thưởng proxy: Chúng tôi huấn luyện mô hình phần thưởng proxy của mình bằng cách tối thiểu hóa
mất mát cross-entropy trên bộ dữ liệu sở thích được tạo ra trong bước trước. Trừ khi được đề cập khác,
chúng tôi sử dụng bộ dữ liệu đầy đủ gồm 46,000 prompt để huấn luyện mô hình phần thưởng và huấn luyện tất cả các mô hình phần thưởng
trong 5 epoch. Chúng tôi đưa ra các siêu tham số khác cho việc huấn luyện mô hình phần thưởng trong Phụ lục D.1 và
ví dụ về đường cong mất mát xác thực cho một tập hợp các mô hình phần thưởng trong Phụ lục F.1. Các mô hình phần thưởng được huấn luyện
đạt từ 60-75% độ chính xác xác thực.

Tạo ensemble: Để tạo ra một ensemble, chúng tôi huấn luyện một số cố định các mô hình phần thưởng proxy sử dụng
dữ liệu và siêu tham số giống hệt nhau nhưng với các seed ngẫu nhiên khác nhau. Điều này dẫn đến sự khởi tạo ngẫu nhiên
khác nhau cho đầu phần thưởng scalar được thêm trên đầu mô hình ngôn ngữ được huấn luyện trước và một thứ tự
xáo trộn dữ liệu khác nhau. Chúng tôi sử dụng tất cả dữ liệu huấn luyện có sẵn cho việc huấn luyện mỗi mô hình phần thưởng vì
huấn luyện trên ít dữ liệu hơn dẫn đến mất mát xác thực cao hơn và hiệu suất kém hơn (Lakshminarayanan
et al., 2017). Trừ khi được nêu khác, chúng tôi huấn luyện một ensemble gồm năm mô hình phần thưởng.

Tối ưu hóa chính sách: Tương tự như Gao et al. (2023), chúng tôi sử dụng lấy mẫu BoN và PPO như các thuật toán tối ưu hóa.
Đối với BoN, chi phí đánh giá cho KL nats lớn hơn tăng theo cấp số nhân. Do đó,
do hạn chế về tính toán có sẵn, chúng tôi chỉ đánh giá BoN cho tối đa nmax= 12,500
mẫu1, mà gần bằng 8.4 nats của KL. Đối với PPO, chúng tôi huấn luyện trong 3000 bước PPO. Chúng tôi đưa ra
chi tiết thêm về việc triển khai và các siêu tham số khác trong Phụ lục D.1.

5 KẾT QUẢ
Trong phần này, chúng tôi báo cáo kết quả của các thí nghiệm. Để tóm tắt, các phát hiện chính của chúng tôi là:
• Sử dụng một ensemble, với bất kỳ mục tiêu nào được tô sáng trong Phần 3, hầu như luôn giúp ích hơn
việc sử dụng một mô hình phần thưởng đơn.
• Đối với BoN, chúng tôi không quan sát bất kỳ overoptimization nào khi sử dụng WCO và UWO như các phương pháp tối ưu hóa;
tuy nhiên, trong trường hợp 25% nhiễu nhãn, tối ưu hóa trung bình thực sự overoptimize (Hình 3).
• Đối với BoN, cả ba phương pháp tối ưu hóa dựa trên ensemble đều dẫn đến cải thiện lên đến ∼30% trong
hiệu suất cuối cùng so với hiệu suất trung bình đạt được bằng cách tối ưu hóa các mô hình phần thưởng riêng lẻ
trong trường hợp không có nhiễu nhãn và cải thiện lên đến ∼75% trong trường hợp 25% nhiễu nhãn (Hình 3).
• Đối với PPO, WCO, và UWO thực sự giảm overoptimization (Hình 5), nhưng việc giảm thiểu hoàn toàn
overoptimization yêu cầu kết hợp chúng với một hình phạt KL nhỏ (Hình 4).
• Đối với PPO, WCO, và UWO luôn khớp hoặc vượt trội hơn tối ưu hóa mô hình phần thưởng đơn
về hiệu suất cuối cùng cho tất cả các giá trị của hình phạt KL (Hình 7).
• Các phát hiện của chúng tôi mạnh mẽ với việc mở rộng kích thước mô hình và kích thước bộ dữ liệu huấn luyện (Hình 8 và 9).

Để trình bày kết quả của chúng tôi, chúng tôi chủ yếu dựa vào các đường cong tối ưu hóa chính sách nơi chúng tôi hiển thị
điểm số mô hình phần thưởng vàng trên trục y bên trái và điểm số mô hình phần thưởng proxy trên trục y bên phải. Đối với
1Tạo ra n= 12,500 câu trả lời cho 1000 prompt và sau đó gán nhãn lại chúng với mô hình phần thưởng proxy và vàng
mất khoảng 700 giờ GPU A100.
5

--- TRANG 6 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
(a) Không có nhiễu nhãn.
 (b) Với 25% nhiễu nhãn.
Hình 3: Kết quả BoN cho kích thước mô hình phần thưởng 44M. Trong Hình BoN này và tương lai, độ phân kỳ KL được
định nghĩa như trong Eq. 1 và đo mức độ tối ưu hóa.

(a) Không có nhiễu nhãn. Đối với cả bốn phương pháp,
hình phạt KL 0.01 được sử dụng.
(b) Với 25% nhiễu nhãn. Hình phạt KL 0.1 cho
trung bình và 0.01 cho tất cả các phương pháp khác được sử dụng.
Hình 4: Kết quả PPO cho kích thước mô hình phần thưởng 44M, trên ba seed PPO. Thang đo độ phân kỳ KL
khác với BoN do sự khác biệt trong thuật toán và tính toán KL (Phụ lục C). Đối với
mỗi phương pháp, chúng tôi chọn hình phạt KL mang lại hiệu suất cuối cùng tốt nhất.

trục x, theo thực tế thông thường (Bai et al., 2022a; Gao et al., 2023), chúng tôi sử dụng
độ phân kỳ KL như được định nghĩa trong Eq. 1 hoặc Phụ lục C, trên thang căn bậc hai để rõ ràng. Đối với PPO, hầu hết
các lần chạy không vượt quá độ phân kỳ KL là 150, nhưng những lần chạy vượt quá được cắt bớt để duy trì biểu đồ dễ đọc hơn.
Ngoài ra, các lần chạy trong Hình 4, minh họa tối ưu hóa PPO chính, được tính trung bình trên ba
seed PPO, với độ lệch chuẩn được hiển thị. Đối với các hình tối ưu hóa PPO khác, do phương sai thấp
được quan sát, một seed đơn được sử dụng. Tuy nhiên, kết quả cho các mô hình phần thưởng đơn được tính trung bình trên năm mô hình phần thưởng
tạo thành ensemble cho các phương pháp khác. Khi so sánh nhiều kết quả thí nghiệm cùng một lúc, chúng tôi trình bày
biểu đồ cột hiển thị hiệu suất cuối cùng trên mô hình phần thưởng vàng và biến quan tâm trên trục x. Như một
chỉ số đánh giá bổ sung cho chất lượng, Phụ lục F.9 so sánh tỷ lệ thắng của chính sách cuối cùng của các phương pháp ensemble khác nhau.
Cuối cùng, chúng tôi cũng bao gồm một số mẫu định tính trong Phụ lục F.10.

5.1 LẤY MẪU BEST-OF-N
Trong Hình 3, chúng tôi trình bày kết quả cho lấy mẫu BoN cho mô hình phần thưởng kích thước 44M. Trên cả
thiết lập không nhiễu và nhiễu, ensemble giúp cải thiện hiệu suất lên ∼30% và ∼75% tương ứng và
thành công tránh overoptimization, ngoại trừ tối ưu hóa trung bình trong trường hợp nhãn nhiễu. Đối với
UWO, chúng tôi hiển thị kết quả cho λ= 0.5 mà chúng tôi thấy là hiệu suất nhất; tuy nhiên, trong Phần 5.4,
chúng tôi chỉ ra rằng nhiều giá trị hợp lý của λ hoạt động tốt. Kết quả bổ sung cho các kích thước khác nhau của
mô hình phần thưởng được trình bày trong Phụ lục F.3 và F.4.

5.2 TỐI ƯU HÓA CHÍNH SÁCH GẦN ĐÚNG
Đối với PPO chúng tôi quan sát rằng WCO và UWO giảm overoptimization và vượt trội hơn các phương pháp khác
về hiệu suất cuối cùng khi không có hình phạt KL, mặc dù chúng không loại bỏ overoptimization
hoàn toàn (như được hiển thị trong Hình 5). Tuy nhiên, trong Hình 4, chúng tôi chỉ ra rằng với một
hệ số hình phạt KL nhỏ là 0.01, WCO và UWO đều thành công ngăn chặn overoptimization, mà không có
sự giảm đáng chú ý nào trong hiệu suất. Mặt khác, khi sử dụng hình phạt KL một mình, một
trọng số hình phạt KL lớn hơn 20 lần là 0.2 được yêu cầu để loại bỏ overoptimization mà phải chịu
hình phạt hiệu suất đáng kể (Hình 6). Trong Hình 7, chúng tôi chỉ ra rằng đối với tất cả các hình phạt, WCO và UWO khớp
hoặc vượt trội hơn hiệu suất cuối cùng trung bình đạt được bằng cách tối ưu hóa một mô hình phần thưởng đơn. Hơn nữa,
đối với các hình phạt KL nhỏ, chúng vượt trội toàn diện hơn tối ưu hóa mô hình phần thưởng đơn.

6

--- TRANG 7 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
(a) Không có nhiễu nhãn.
 (b) Với 25% nhiễu nhãn.
Hình 5: Kết quả PPO cho các mục tiêu tối ưu hóa khác nhau mà không sử dụng hình phạt KL.

Hình 6: Tối ưu hóa mô hình phần thưởng
riêng lẻ (tức là không có ensemble) cho
các giá trị khác nhau của hình phạt KL
trong trường hợp 25% nhiễu nhãn.

Hình 7: Hiệu quả của các phương pháp
tối ưu hóa bảo thủ trên các trọng số
hình phạt KL trong trường hợp 25%
nhiễu nhãn.

5.3 KẾT QUẢ MỞ RỘNG MÔ HÌNH VÀ DỮ LIỆU
Công trình trước đây (Gao et al., 2023) đã chỉ ra rằng việc tăng kích thước mô hình phần thưởng hoặc kích thước bộ dữ liệu huấn luyện
giúp cải thiện hiệu suất được đo bằng mô hình phần thưởng vàng. Trong Hình 8, chúng tôi thay đổi kích thước
của các mô hình phần thưởng (giữ kích thước bộ dữ liệu cố định ở 46k mẫu và kích thước chính sách cố định ở 1.4B) và
vẽ biểu đồ hiệu suất cuối cùng của mỗi phương pháp cho ngân sách huấn luyện cố định. Nhớ lại rằng đối với BoN, đây là
n= 12,500 mẫu và đối với PPO, đây là 3000 bước thời gian. Kết quả của chúng tôi chỉ ra rằng sự cải thiện
do việc sử dụng ensemble là trực giao với sự cải thiện hiệu suất đạt được bằng cách mở rộng
kích thước mô hình phần thưởng. Đặc biệt, với mô hình phần thưởng 1.3B, chúng tôi tô sáng rằng ngay cả khi
chính sách và mô hình phần thưởng có số lượng tham số tương tự, WCO và UWO vẫn cung cấp
lợi ích không tầm thường so với việc sử dụng một mô hình phần thưởng đơn. Một minh họa chi tiết hơn về ví dụ này có thể
được tìm thấy trong Phụ lục F.4.

Tương tự, trong Hình 9, chúng tôi thay đổi kích thước của bộ dữ liệu huấn luyện (giữ kích thước mô hình phần thưởng cố định
ở 44M) và vẽ biểu đồ hiệu suất cuối cùng cho mỗi phương pháp cho ngân sách huấn luyện cố định. Kết quả
một lần nữa chỉ ra rằng việc sử dụng ensemble cung cấp cải thiện bổ sung ngoài việc tăng
kích thước của bộ dữ liệu huấn luyện.

7

--- TRANG 8 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
(a) BoN.
 (b) PPO.
Hình 8: Hiệu suất mô hình phần thưởng vàng cuối cùng đạt được bởi các mục tiêu khác nhau khi tối ưu hóa
các mô hình phần thưởng với các kích thước tham số khác nhau, nhưng được huấn luyện với cùng bộ dữ liệu chứa 25% nhiễu nhãn.
Các mô hình PPO 1.3B được huấn luyện đặc biệt trong 6000 bước PPO, do tốc độ tối ưu hóa chính sách chậm hơn với
các mô hình phần thưởng lớn (Phụ lục F.4).

(a) BoN.
 (b) PPO.
Hình 9: Hiệu suất mô hình phần thưởng vàng cuối cùng đạt được bởi các mục tiêu khác nhau khi tối ưu hóa
các mô hình phần thưởng (44M) được huấn luyện dưới lượng dữ liệu khác nhau. Tỷ lệ nhiễu nhãn 25% được
duy trì cho tất cả các kích thước bộ dữ liệu.

5.4 ĐÁNH GIÁ TÍNH MẠNH MẼ ĐỐI VỚI CÁC SIÊU THAM SỐ
Việc sử dụng phương pháp dựa trên ensemble giới thiệu một siêu tham số bổ sung: cardinality, hoặc
kích thước, của ensemble. Trong Hình 11, chúng tôi vẽ biểu đồ hiệu suất cuối cùng cho BoN và PPO cho ba
phương pháp dựa trên ensemble: tối ưu hóa trung bình, WCO, và UWO. Chúng tôi lưu ý rằng trong khi có một
khoảng cách đáng chú ý giữa ensemble 3 thành viên và 4 thành viên, trong hầu hết các trường hợp hiệu suất rất
tương tự cho ensemble 4 thành viên và 5 thành viên, chỉ ra rằng ensemble 4 thành viên hoặc 5 thành viên
có khả năng hoạt động tốt nhất, và sẽ thấy lợi ích giảm dần sau điểm này.

Đối với UWO, giá trị của hình phạt bất định là một siêu tham số khác. Kết quả của chúng tôi trong Hình 12
chỉ ra rằng hầu hết các giá trị hợp lý của hình phạt bất định thực sự hoạt động tốt, chỉ ra rằng có thể
không cần điều chỉnh siêu tham số này.

5.5 TÁC ĐỘNG CỦA NHIỄU NHÃN VÀ HÌNH PHẠT BẤT ĐỊNH
Hình 10: Phương sai nội ensemble cho
các mục tiêu tối ưu hóa khác nhau dưới
các mức độ nhiễu khác nhau cho PPO.Trong Hình 10, chúng tôi hiển thị phương sai nội ensemble cho một
ensemble gồm các mô hình phần thưởng tham số 44M được tối ưu hóa
thông qua các mục tiêu UWO và tối ưu hóa trung bình sử dụng PPO.
Phương sai giữa các thành viên ensemble bắt đầu ở một
giá trị nhỏ trong trường hợp không có nhiễu nhãn, và tăng
một lượng tương đối nhỏ trong quá trình huấn luyện cho UWO.
Tuy nhiên, bất định tăng gần 3 lần
cho tối ưu hóa trung bình trong quá trình huấn luyện.

Đối với trường hợp 25% nhiễu nhãn, phương sai bắt đầu
cao hơn nhiều và trong quá trình huấn luyện, tăng
khoảng 2.5 lần cho tối ưu hóa trung bình.
Tuy nhiên, phương sai chỉ tăng khoảng 20% cho
UWO (sử dụng λ= 0.1). Hơn nữa, việc sử dụng hình phạt KL
0.01 dẫn đến sự giảm nhẹ trong phương sai ở cuối
huấn luyện. Điều này gợi ý thực tế rằng trong khi tối ưu hóa trung bình có thể khai thác bất kỳ mô hình phần thưởng nào
ước lượng quá cao phần thưởng, hình phạt bất định trong UWO ngăn chặn điều đó - dẫn đến
hiệu suất cuối cùng tốt hơn (được hiển thị trong Hình 7) và giảm overoptimization.

8

--- TRANG 9 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
(a) BoN.
 (b) PPO.
Hình 11: Tác động của cardinality của ensemble lên hiệu suất cuối cùng của trung bình, UWO, và
WCO cho trường hợp 25% nhiễu nhãn.

(a) BoN.
 (b) PPO.
Hình 12: Tác động của trọng số hình phạt bất định lên hiệu suất cuối cùng cho số lượng khác nhau của
các mô hình phần thưởng trong ensemble. Chúng tôi lưu ý rằng có tính mạnh mẽ đáng kể đối với giá trị của
hình phạt bất định.

6 THẢO LUẬN
Chúng tôi đã chứng minh rằng các phương pháp tối ưu hóa bảo thủ dựa trên ensemble cải thiện hiệu suất
và rất hiệu quả trong việc chống lại vấn đề overoptimization trong RLHF. Công trình này mở ra
khả năng của một số nghiên cứu tiếp theo thú vị. Thứ nhất, công trình tương lai nên xem xét nhân rộng
các phát hiện của chúng tôi trong các môi trường khác (tức là các bộ dữ liệu RLHF khác) và với các mô hình ngôn ngữ
quy mô lớn hơn. Thứ hai, thiết lập của chúng tôi dựa trên RLHF offline, trong đó phản hồi của con người được thu thập
trước và không có cập nhật nào được thực hiện cho mô hình phần thưởng trong suốt quá trình tối ưu hóa chính sách.
Điều này trái ngược với RLHF online (Bai et al., 2022a; Christiano et al., 2017), nơi các mô hình phần thưởng
được huấn luyện lại định kỳ trên dữ liệu mới được thu thập từ con người. Liệu ensemble có dẫn đến
lợi ích tương tự trong thiết lập này hay không?

7 CÁC CÔNG TRÌNH LIÊN QUAN
Overoptimization trong RLHF: Một số công trình (Ibarz et al., 2018; Ziegler et al., 2019; Stiennon et al.,
2020) đã cung cấp bằng chứng giai thoại về overoptimization trong RLHF. Tuy nhiên, theo hiểu biết tốt nhất của chúng tôi,
Gao et al. (2023) là những người duy nhất nghiên cứu nó một cách hệ thống trong thiết lập tinh chỉnh
LLM. Công trình của chúng tôi sử dụng thiết lập của họ, tái tạo kết quả của họ, và thực hiện một nghiên cứu hệ thống
đánh giá hiệu quả của việc sử dụng ensemble để giảm thiểu overoptimization.

Sử dụng ensemble trong (Model-Based) RL: Ensemble thường được sử dụng trong deep learning như một công cụ để
ước lượng bất định (Lakshminarayanan et al., 2017; Dietterich, 2000) và thường có thể vượt trội hơn
các phương pháp Bayesian deep learning phức tạp hơn (Ovadia et al., 2019). Việc học một ensemble của
các mô hình động lực học đặc biệt phổ biến trong model-based reinforcement learning nơi thường một
ước lượng bất định đáng tin cậy về không gian trạng thái là quan trọng để tránh sự thay đổi phân phối (Depeweg et al., 2016;
Gal et al., 2016; Chua et al., 2018; Yu et al., 2020a). Sự bất đồng giữa các thành viên ensemble cũng là
một phương pháp phổ biến để thúc đẩy việc khám phá của các agent học trong một môi trường (Henaff, 2019;
Pathak et al., 2019; Shyam et al., 2019; Lee et al., 2021). Brantley et al. (2019) sử dụng một phương pháp
dựa trên ensemble để ước lượng hỗ trợ của các chính sách chuyên gia trong một thiết lập imitation learning. Trong
thiết lập mô hình ngôn ngữ, Gleave & Irving (2022) khám phá việc sử dụng ensemble cho active learning dựa trên bất định
để cải thiện hiệu quả mẫu của các mô hình phần thưởng. Tuy nhiên, theo hiểu biết tốt nhất của chúng tôi,
không có công trình trước đây nào khám phá việc sử dụng ensemble để cải thiện tính mạnh mẽ
của RLHF; đặc biệt trong thiết lập tinh chỉnh mô hình ngôn ngữ.

9

--- TRANG 10 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
TÀI LIỆU THAM KHẢO
Josh Abramson, Arun Ahuja, Federico Carnevale, Petko Georgiev, Alex Goldin, Alden Hung,
Jessica Landon, Jirka Lhotka, Timothy Lillicrap, Alistair Muldal, et al. Improving multi-
modal interactive agents with reinforcement learning from human feedback. arXiv preprint
arXiv:2211.11602 , 2022. A

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless
assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 ,
2022a. 1, 2.2, 5, 6, A

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harm-
lessness from ai feedback. arXiv preprint arXiv:2212.08073 , 2022b. A

Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O'Brien, Eric
Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.
Pythia: A suite for analyzing large language models across training and scaling. In International
Conference on Machine Learning , pp. 2397–2430. PMLR, 2023. 4, 4.2, E

Stephen P Boyd and Lieven Vandenberghe. Convex optimization . Cambridge university press, 2004.
3, 3

Kiante Brantley, Wen Sun, and Mikael Henaff. Disagreement-regularized imitation learning. In
International Conference on Learning Representations , 2019. 3, 7

Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, J ´er´emy Scheurer, Javier
Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems
and fundamental limitations of reinforcement learning from human feedback. arXiv preprint
arXiv:2307.15217 , 2023. 1

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. Advances in neural information processing sys-
tems, 30, 2017. 6, A

Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learn-
ing in a handful of trials using probabilistic dynamics models. Advances in neural information
processing systems , 31, 2018. 7

Jack Clark and Dario Amodei. Faulty reward functions in the wild. https://openai.com/
research/faulty-reward-functions , 2016. A

Stefan Depeweg, Jos ´e Miguel Hern ´andez-Lobato, Finale Doshi-Velez, and Steffen Udluft. Learning
and policy search in stochastic dynamical systems with bayesian neural networks. arXiv preprint
arXiv:1605.07127 , 2016. 7

Thomas G Dietterich. Ensemble methods in machine learning. In International workshop on multi-
ple classifier systems , pp. 1–15. Springer, 2000. 7

Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum,
and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment.
arXiv preprint arXiv:2304.06767 , 2023. A

Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos
Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for
methods that learn from human feedback. arXiv preprint arXiv:2305.14387 , 2023. 1, 4, 4.1, 4.2,
4.3, D.2, E, 13

Yarin Gal, Rowan McAllister, and Carl Edward Rasmussen. Improving PILCO with Bayesian neural
network dynamics models. In Data-Efficient Machine Learning workshop, International Confer-
ence on Machine Learning , 2016. 7

10

--- TRANG 11 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In
International Conference on Machine Learning , pp. 10835–10866. PMLR, 2023. (document), 1,
1, 2.3, 4, 4.2, 4.3, 5, 5.3, 7, E, 13, 14

Adam Gleave and Geoffrey Irving. Uncertainty estimation for language reward models. arXiv
preprint arXiv:2203.07472 , 2022. 7

Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek
Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training
(rest) for language modeling. arXiv preprint arXiv:2308.08998 , 2023. A

Mikael Henaff. Explicit explore-exploit algorithms in continuous state spaces. Advances in Neural
Information Processing Systems , 32, 2019. 7

Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving, Shane Legg, and Dario Amodei. Reward
learning from human preferences and demonstrations in atari. Advances in neural information
processing systems , 31, 2018. 7

Andreas K ¨opf, Yannic Kilcher, Dimitri von R ¨utte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich ´ard Nagyfi, et al. Openassistant
conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327 ,
2023. D.3

Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ra-
mana Kumar, Zac Kenton, Jan Leike, and Shane Legg. Specification gaming: The
flip side of ai ingenuity. April 2020. URL https://www.deepmind.com/blog/
specification-gaming-the-flip-side-of-ai-ingenuity . A

Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. Advances in neural information processing systems ,
30, 2017. 3, 4.3, 7

Kimin Lee, Laura Smith, and Pieter Abbeel. Pebble: Feedback-efficient interactive rein-
forcement learning via relabeling experience and unsupervised pre-training. arXiv preprint
arXiv:2106.05091 , 2021. 7

Joel Lehman, Jeff Clune, Dusan Misevic, Christoph Adami, Lee Altenberg, Julie Beaulieu, Peter J
Bentley, Samuel Bernard, Guillaume Beslon, David M Bryson, et al. The surprising creativity of
digital evolution: A collection of anecdotes from the evolutionary computation and artificial life
research communities. Artificial life , 26(2):274–306, 2020. A

Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint
arXiv:2305.20050 , 2023. A

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri
Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement
with self-feedback. arXiv preprint arXiv:2303.17651 , 2023. A

Timothy Prickett Morgan. Counting the cost of training large language mod-
els, 2022. URL https://www.nextplatform.com/2022/12/01/
counting-the-cost-of-training-large-language-models/ . 1

Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-
pher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted
question-answering with human feedback. arXiv preprint arXiv:2112.09332 , 2021. 2.1, A, B

OpenAI. Openai models. https://platform.openai.com/docs/models/ , 2023a. 4.1

OpenAI. Gpt-4 technical report. arXiv , pp. 2303–08774, 2023b. A

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in Neural Information Processing Systems , 35:
27730–27744, 2022. 1, 2.1, 2.2, A

11

--- TRANG 12 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty?
evaluating predictive uncertainty under dataset shift. Advances in neural information processing
systems , 32, 2019. 3, 7

Alexander Pan, Kush Bhatia, and Jacob Steinhardt. The effects of reward misspecification: Mapping
and mitigating misaligned models. arXiv preprint arXiv:2201.03544 , 2022. A

Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement.
InInternational conference on machine learning , pp. 5062–5071. PMLR, 2019. 7

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea
Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv
preprint arXiv:2305.18290 , 2023. A

William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan
Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802 ,
2022. A

J´er´emy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun
Cho, and Ethan Perez. Training language models with language feedback at scale. arXiv preprint
arXiv:2303.16755 , 2023. A

John Schulman. Approximating kl divergence, 2020. URL http://joschu.net/blog/
kl-approx.html . C

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017. 2.2

Pranav Shyam, Wojciech Ja ´skowski, and Faustino Gomez. Model-based active exploration. In
International conference on machine learning , pp. 5779–5788. PMLR, 2019. 7

Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and character-
izing reward gaming. Advances in Neural Information Processing Systems , 35:9460–9471, 2022.
A

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances
in Neural Information Processing Systems , 33:3008–3021, 2020. 1, 2.2, 4.3, 7, A

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.
https://github.com/tatsu-lab/stanford_alpaca , 2023. 4, 4.1, D.2

Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia
Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and
outcome-based feedback. arXiv preprint arXiv:2211.14275 , 2022. A

Abhinav Venigalla and Linden Li. Mosaic llms (part 2): Gpt-3 quality for <500k, 2022. URL
https://www.mosaicml.com/blog/gpt-3-quality-for-500k . 1

Stephen J Wright. Numerical optimization . 2006. 3, 3

Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Chris-
tiano. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862 ,
2021a. A

Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and
Hanlin Goh. Uncertainty weighted actor-critic for offline reinforcement learning. arXiv preprint
arXiv:2105.08140 , 2021b. 3

Wanqiao Xu, Shi Dong, Dilip Arumugam, and Benjamin Van Roy. Shattering the agent-environment
interface for fine-tuning inclusive language models. arXiv preprint arXiv:2305.11455 , 2023. A

12

--- TRANG 13 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn,
and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information
Processing Systems , 33:14129–14142, 2020a. 7

Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn,
and Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information
Processing Systems , 33:14129–14142, 2020b. 3, 3

Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf:
Rank responses to align language models with human feedback without tears. arXiv preprint
arXiv:2304.05302 , 2023. A

Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf:
Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425 , 2023.
A

Rui Zheng, Shihan Dou, Songyang Gao, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu,
Limao Xiong, Lu Chen, et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint
arXiv:2307.04964 , 2023. 2.2

Simon Zhuang and Dylan Hadfield-Menell. Consequences of misaligned ai. Advances in Neural
Information Processing Systems , 33:15763–15773, 2020. A

Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul
Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv
preprint arXiv:1909.08593 , 2019. 1, 4.3, 7, A

13

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
PHỤ LỤC

A CÁC CÔNG TRÌNH LIÊN QUAN BỔ SUNG
RLHF: Học tăng cường từ phản hồi của con người (RLHF), trong hình thức hiện đại của nó, được phổ biến bởi
Christiano et al. (2017). Ziegler et al. (2019) là công trình đầu tiên chứng minh hiệu quả
của RLHF cho việc tinh chỉnh các mô hình ngôn ngữ. Điều này dẫn đến một số ứng dụng của RLHF cho việc tinh chỉnh
LLM tập trung vào cải thiện một số khía cạnh của các mô hình ngôn ngữ ví dụ như tuân theo hướng dẫn (Ouyang
et al., 2022; Abramson et al., 2022), khả năng tóm tắt (Stiennon et al., 2020; Wu et al.,
2021a), duyệt web (Nakano et al., 2021), dịch thuật (Gulcehre et al., 2023) và tính hữu ích &
vô hại (Bai et al., 2022a). Điều này cũng gây ra rất nhiều quan tâm đến việc cải thiện RLHF
như một phương pháp. Một hướng nghiên cứu chính ở đây là các phương pháp cố gắng tăng cường tín hiệu học cho
mô hình phần thưởng dưới một hình thức nào đó ví dụ như phản hồi ngôn ngữ (Scheurer et al., 2023) hoặc giám sát quá trình
(Lightman et al., 2023; Uesato et al., 2022). Tuy nhiên, điều này thường phát sinh chi phí bổ sung trong
việc gán nhãn. Điều này đã thúc đẩy nghiên cứu về việc trích xuất tín hiệu phản hồi từ một mô hình ngôn ngữ được huấn luyện trước
thông qua prompting phù hợp (Bai et al., 2022b; OpenAI, 2023b; Madaan et al., 2023; Saunders
et al., 2022), tuy nhiên, điều này yêu cầu prompting hiệu quả và một mô hình ngôn ngữ có khả năng cao để
thành công. Một hướng nghiên cứu khác tập trung vào việc sử dụng các thuật toán thay thế cho PPO cho
bước tối ưu hóa chính sách (Gulcehre et al., 2023; Rafailov et al., 2023; Zhao et al., 2023; Yuan et al.,
2023; Dong et al., 2023; Xu et al., 2023). Trực giao với cả hai hướng nghiên cứu này, chúng tôi khám phá việc sử dụng
ensemble để cải thiện RLHF.

Reward Hacking: Overoptimization trong RLHF là một trường hợp của reward hacking. Nhiều ví dụ về
reward hacking tồn tại trong tài liệu (Clark & Amodei, 2016; Lehman et al., 2020; Krakovna et al.,
2020). Skalse et al. (2022) cung cấp một định nghĩa chính thức cho reward hacking và nghiên cứu lý thuyết
liệu các proxy không thể hack có thể tồn tại hay không dưới các điều kiện khác nhau. Zhuang & Hadfield-Menell
(2020) nghiên cứu reward hacking có thể phát sinh do observability từng phần. Pan et al. (2022)
cung cấp ví dụ về các hàm phần thưởng proxy trong các môi trường reinforcement learning khác nhau nơi
các chính sách dung lượng thấp không tạo ra reward hacking nhưng các chính sách có dung lượng lớn hơn thì có.

B ƯỚC LƯỢNG BON
Cách đơn giản để ước lượng phần thưởng trung bình của chính sách BoN dưới một mô hình phần thưởng vàng, khi
sử dụng một mô hình phần thưởng proxy như hàm xếp hạng, là sử dụng một ước lượng Monte Carlo trong đó n
câu trả lời {A1, ..., A2} được lấy mẫu từ một mô hình ngôn ngữ f và sau đó được chấm điểm cho một prompt đã cho q như
sau:
Rgold
n(q) :=EA1,A2,...,An∼f(q)"
Rgold 
arg max
a∈{A1,A2,...,An}Rproxy(a|q)|q!#
(6)
Điều này rất lãng phí vì nó không tái sử dụng câu trả lời cho các giá trị khác nhau của n. Do đó, chúng tôi thay vào đó
sử dụng ước lượng không thiên vị sau đây (Nakano et al., 2021) mà lấy mẫu N≥nmax đầu ra cho một
đầu vào đã cho q:
Rgold
n(q) =1N
nX
1≤i1<···<in≤NRgold
 argmax
a∈{Ai1,...,Ain}Rproxy(a|q)|q
 (7)
Điều này có thể được tính toán hiệu quả bằng cách đầu tiên sắp xếp tất cả câu trả lời A1, ..., AN dưới mô hình phần thưởng proxy
để có được điểm số S1, ..., SN và sau đó tính toán:
1N
nX
1≤i1<···<in≤NRgold
 argmax
a∈{Si1,...,Sin}Rproxy(a|q)|q
=NX
i=ni−1
n−1
N
nRgold(Si|q)(8)
Để có được điểm số mô hình phần thưởng vàng cho một phạm vi giá trị đã cho n= 1,2, ..., nmax, chúng tôi đơn giản
đánh giá trung bình thực nghiệm của ước lượng trên cho tất cả câu hỏi cho mỗi n.

14

--- TRANG 15 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
C TÍNH TOÁN KHOẢNG CÁCH KL CHO PPO
Cách đơn giản để tính toán khoảng cách KL giữa chính sách được tối ưu hóa PPO πRL và mô hình được huấn luyện trước
như sau:
KLRL(πRL, πinit) =Ex∼πRL
logπRL
πinit
(9)
Tuy nhiên, ước lượng này có phương sai cao và có thể âm, không giống như KL thực tế. Do đó, chúng tôi sử dụng
ước lượng sau đây (Schulman, 2020):
KLRL(πRL, πinit) =Ex∼πRL"
1
2
logπRL
πinit2#
(10)

D CHI TIẾT THỰC NGHIỆM BỔ SUNG
D.1 CÁC SIÊU THAM SỐ
Chúng tôi đưa ra các siêu tham số ở đây cho các thành phần khác nhau của pipeline RLHF của chúng tôi:

Bảng 1: Siêu tham số SFT.
Tham số Giá trị
Tốc độ học 8e-6
Epoch 3
Kích thước batch 4

Bảng 2: Siêu tham số RM.
Tham số Giá trị
Tốc độ học 1e-5
Epoch 5
Kích thước batch 32

Bảng 3: Siêu tham số PPO.
Tham số Giá trị
Tốc độ học 1e-6
Bộ lập lịch cosine annealing 1e-7
Epoch PPO 4
Kích thước batch 32
Số lượng rollout 256
Kích thước chunk 32
Phạm vi clipping & giá trị 0.2
GAE lambda 0.95

D.2 CHI TIẾT BỘ DỮ LIỆU ALPACA FARM
Bộ dữ liệu AlpacaFarm (Dubois et al., 2023) được sử dụng trong các thí nghiệm của chúng tôi sử dụng dữ liệu Alpaca
(Taori et al., 2023) bao gồm 52,000 mẫu. Dữ liệu này được chọn do kích thước lớn và thành công trong
việc huấn luyện các mô hình tuân theo hướng dẫn. AlpacaFarm chứa năm phần chia: một phần "sft" được gán nhãn 10k cho
15

--- TRANG 16 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Bảng 4: Siêu tham số tạo sinh.
Tham số Giá trị
Độ dài hướng dẫn tối đa 520
Token mới tối đa (độ dài câu trả lời) 256
Epoch PPO 4
Top-p 0.9 (1.0 cho huấn luyện PPO)
Top-k 0
Nhiệt độ 1.0

Bảng 5: Ví dụ về prompt tạo câu trả lời và mô hình hóa phần thưởng với định dạng phù hợp.
Prompt tạo câu trả lời Prompt mô hình hóa phần thưởng
<|prompter|>Hình dạng hình học nào
có 5 cạnh và
5 góc?<|endoftext|>
<|assistant|><|prompter|>Hình dạng hình học nào
có 5 cạnh và
5 góc?<|endoftext|>
<|assistant|>Hình dạng
hình học là một
ngũ giác.<|endoftext|>
<|prompter|>Phân tích
câu sau và xác định
động từ và thì của nó. \nCô ấy
chơi piano.<|endoftext|>
<|assistant|><|prompter|>Phân tích
câu sau và xác định
động từ và thì của nó. \nCô ấy
chơi piano.<|endoftext|>
<|assistant|>Động từ: Chơi
Thì: Quá khứ<|endoftext|>

tinh chỉnh có giám sát, một phần "pref" 10k chứa các nhãn sở thích theo cặp, một phần "unlabeled"
20k để huấn luyện các thuật toán như PPO, một phần xác thực 2k, và một phần 10k không sử dụng.

D.3 ĐỊNH DẠNG PROMPT
Mặc dù chúng tôi sử dụng các hướng dẫn từ bộ dữ liệu AlpacaFarm, điều này chỉ cung cấp nội dung để prompting
LLM và vẫn yêu cầu định dạng. Chúng tôi chọn sự tối giản, theo định dạng v2 được sử dụng trong
OpenAssistant (Köpf et al., 2023). Định dạng này theo lớp GPTNeoXTokenizer được sử dụng để
huấn luyện trước các LLM của chúng tôi và giới thiệu hai token đặc biệt: <|prompter|> và <|assistant|>.

Để tạo câu trả lời, mô hình nên được prompting với hướng dẫn và đầu vào. Các đầu vào,
nếu có, được nối vào hướng dẫn sau một dòng mới để tạo thành prompt. Prompt sau đó được
đặt trước với token <|prompter|> và đóng lại với token end-of-text (EOT)
<|endoftext|> tuyên bố kết thúc hướng dẫn, và token <|assistant|> để
bắt đầu câu trả lời. Một ví dụ được hiển thị trong Bảng 5.

Để mô hình hóa phần thưởng, prompt cũng nên chứa một câu trả lời để đánh giá. Trong trường hợp này,
văn bản câu trả lời (từ một trình diễn bộ dữ liệu AlpacaFarm hoặc một lần tạo câu trả lời trước đó) được nối
vào prompt ban đầu chứa hướng dẫn và đóng lại với token EOT. Điều này tạo thành
prompt RM đầy đủ, với một ví dụ được hiển thị trong Bảng 5.

16

--- TRANG 17 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
E TÁI TẠO KẾT QUẢ CỦA GAO ET AL. (2023)
Chúng tôi cũng thành công tái tạo kết quả của Gao et al. trong thiết lập của chúng tôi được hiển thị trong Hình 13 và 14.
Lưu ý rằng thiết lập của chúng tôi sử dụng các mô hình mã nguồn mở được dẫn xuất từ bộ Pythia (Biderman et al., 2023) như
cơ sở cho chính sách và các mô hình phần thưởng proxy và mô hình phần thưởng sở thích con người AlpacaFarm
(Dubois et al., 2023) được sử dụng như mô hình phần thưởng vàng. Ngược lại, Gao et al. sử dụng các mô hình nguồn đóng
dựa trên dòng GPT. Việc tái tạo thành công kết quả của họ trong thiết lập của chúng tôi gợi ý về
bản chất chung của overoptimization.

(a) Kết quả BoN cho các kích thước mô hình phần thưởng khác nhau,
với kích thước dữ liệu được giữ cố định (46k).
(b) Kết quả BoN cho các kích thước dữ liệu huấn luyện mô hình phần thưởng khác nhau,
với kích thước RM được giữ cố định (44M).
Hình 13: Điểm số mô hình phần thưởng vàng cho BoN theo xu hướng tương tự như được quan sát bởi Gao et al.
(2023) khi thay đổi kích thước mô hình phần thưởng và dữ liệu. Các lần chạy được tính trung bình trên năm seed, với độ lệch chuẩn
bổ sung được hiển thị. 25% nhiễu nhãn được sử dụng ở đây để tô sáng overoptimization, vì chúng tôi
thấy khó quan sát với khả năng tối ưu hóa BoN hạn chế của chúng tôi. Điều này phù hợp với các thí nghiệm loại bỏ nhiễu nhãn
từ Dubois et al. (2023).

Hình 14: Kết quả tối ưu hóa PPO cho các kích thước dữ liệu huấn luyện mô hình phần thưởng khác nhau, với kích thước mô hình phần thưởng
được giữ cố định (44M). Trung bình và độ lệch chuẩn trên ba lần chạy được hiển thị. Một xu hướng tương tự
với Gao et al. (2023) được quan sát, với overoptimization lớn hơn khi tối ưu hóa được đẩy xa hơn.

17

--- TRANG 18 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
F KẾT QUẢ BỔ SUNG
F.1 ĐƯỜNG CONG MẤT MÁT XÁC THỰC RM

Hình 15: Mất mát xác thực mô hình phần thưởng trong quá trình huấn luyện cho các mô hình phần thưởng 44M không có nhiễu nhãn.
Các mô hình phần thưởng được huấn luyện trong 5 epoch, với mỗi epoch kéo dài 359 bước. Chúng tôi lưu ý rằng trong khi có
sự biến đổi nhỏ trong mất mát xác thực, sự biến đổi này không dự đoán được liệu mô hình phần thưởng
có mạnh mẽ với overoptimization hay không. Ví dụ, trong Hình 16, chúng tôi thấy rằng mô hình phần thưởng với
mất mát xác thực cao nhất không gặp overoptimization nhưng các mô hình phần thưởng khác với mất mát xác thực thấp hơn thì có.

18

--- TRANG 19 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
F.2 BIỂU ĐỒ TỐI ƯU HÓA HUẤN LUYỆN MÔ HÌNH PHẦN THƯỞNG RIÊNG LẺ
Đối với kết quả mô hình phần thưởng đơn, chúng tôi tối ưu hóa từng mô hình phần thưởng riêng biệt và trình bày
trung bình của chúng trong Hình 3 và 4. Ở đây, chúng tôi trình bày đường cong huấn luyện cho từng mô hình phần thưởng riêng biệt.

(a) Không có nhiễu nhãn.
 (b) Trường hợp 25% nhiễu nhãn.
Hình 16: BoN cho mô hình phần thưởng 44M. Trung bình của các đường cong này được trình bày trong Hình 3.

(a) Không có nhiễu nhãn.
 (b) Trường hợp 25% nhiễu nhãn.
Hình 17: Kết quả PPO cho mô hình phần thưởng 44M. Trung bình của các đường cong này được trình bày trong Hình 4.

19

--- TRANG 20 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
F.3 KẾT QUẢ TỐI ƯU HÓA HUẤN LUYỆN MÔ HÌNH PHẦN THƯỞNG 7M

(a) BoN.
 (b) PPO.
Hình 18: Kết quả BoN và PPO với các mô hình phần thưởng 7M để bổ sung biểu đồ cột trong Hình 8.

F.4 KẾT QUẢ TỐI ƯU HÓA HUẤN LUYỆN MÔ HÌNH PHẦN THƯỞNG 1.3B

(a) BoN.
(b) PPO, 3000 bước.
 (c) PPO, 6000 bước.
Hình 19: Kết quả BoN và PPO với các mô hình phần thưởng 1.3B để bổ sung biểu đồ cột trong Hình 8.
Mặc dù tất cả các thí nghiệm PPO được chạy trong 3000 bước, ở đây chúng tôi cũng bao gồm một thí nghiệm với 6000
bước cho mô hình phần thưởng 1.3B, do chính sách được tối ưu hóa với tốc độ chậm hơn đối với các mô hình phần thưởng lớn.
Sự khác biệt có thể được quan sát ở độ phân kỳ KL cao hơn, thường được đạt đến trong
các bước sau của huấn luyện PPO. Đặc biệt, sự khác biệt về hiệu suất giữa các mô hình phần thưởng đơn và
các phương pháp ensemble rõ ràng hơn khi chính sách được huấn luyện lâu hơn.

20

--- TRANG 21 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
F.5 KẾT QUẢ BỔ SUNG KHÔNG CÓ NHIỄU NHÃN

Hình 20: Hiệu quả của các phương pháp tối ưu hóa bảo thủ trên các trọng số hình phạt KL trong trường hợp
không có nhiễu nhãn.

(a) BoN
 (b) PPO
Hình 21: Hiệu suất mô hình phần thưởng vàng cuối cùng đạt được bởi các mục tiêu khác nhau khi tối ưu hóa
các mô hình phần thưởng với các kích thước tham số khác nhau, nhưng được huấn luyện với cùng bộ dữ liệu không có nhiễu nhãn.

(a) BoN
 (b) PPO
Hình 22: Hiệu suất mô hình phần thưởng vàng cuối cùng đạt được bởi các mục tiêu khác nhau khi tối ưu hóa
các mô hình phần thưởng (44M) được huấn luyện dưới lượng dữ liệu khác nhau. Không có nhiễu nhãn được sử dụng trong dữ liệu.

(a) BoN.
 (b) PPO.
Hình 23: Tác động của cardinality của ensemble lên hiệu suất cuối cùng của trung bình, UWO, và
WCO cho trường hợp không có nhiễu nhãn.

21

--- TRANG 22 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
(a) BoN.
 (b) PPO.
Hình 24: Tác động của trọng số hình phạt bất định lên hiệu suất cuối cùng cho số lượng khác nhau của
các mô hình phần thưởng trong ensemble. Chúng tôi lưu ý rằng có tính mạnh mẽ đáng kể đối với giá trị của
hình phạt bất định. Không có nhiễu nhãn được bao gồm trong dữ liệu.

F.6 KẾT QUẢ NHIỄU NHÃN 35%

(a) BoN.
 (b) PPO.
Hình 25: Kết quả BoN và PPO cho các mô hình phần thưởng 44M với hình phạt KL 0.01, khi có 35%
nhiễu nhãn trong dữ liệu huấn luyện mô hình phần thưởng. Kết quả cho thấy rằng sự tăng hiệu suất và
giảm thiểu overoptimization của ensemble mạnh mẽ với một mức độ nhiễu khác.

F.7 KẾT QUẢ PPO DÀI HƠN

Hình 26: Kết quả PPO cho các mô hình phần thưởng 44M với hình phạt KL 0.01 và 25% nhiễu nhãn. Tối ưu hóa
được mở rộng đến 10k bước cho thí nghiệm này. Những kết quả này cho thấy tính ổn định của các phương pháp ensemble
ngay cả đối với mức độ tối ưu hóa cao. Do đó, ensemble của chúng tôi loại bỏ nhu cầu dừng sớm
và rủi ro overoptimization. Có vẻ như bất kể mô hình được tối ưu hóa đến đâu,
hiệu suất sẽ duy trì gần với mức tối ưu.

22

--- TRANG 23 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
F.8 KẾT QUẢ PPO ĐỘ PHÂN KỲ KL HOÀN CHỈNH KHÔNG CẮT XÉN

Hình 27: Tối ưu hóa mô hình phần thưởng riêng lẻ (tức là không có ensemble) cho các giá trị khác nhau của hình phạt KL
trong trường hợp 25% nhiễu nhãn. Hình này không cắt trục x ở 150 độ phân kỳ KL, để
minh họa hành vi của Hình 6 vượt ra ngoài điểm này. Như đã đề cập trong Phần 5, chúng tôi thường dừng
các đường cong tối ưu hóa ở 150 độ phân kỳ KL vì các xu hướng overoptimization đã hiển thị,
và làm như vậy cung cấp tính nhất quán trên các hình. Hơn nữa, biểu đồ rõ ràng hơn vì nó không được làm
hẹp bởi các giá trị cực đoan hơn.

F.9 ĐÁNH GIÁ TỶ LỆ THẮNG CỦA CÁC PHƯƠNG PHÁP ENSEMBLE
Như một thước đo chất lượng bổ sung, Bảng 6 và 7 hiển thị tỷ lệ thắng của các chính sách cuối cùng được huấn luyện với
các phương pháp ensemble khác nhau, khi so sánh với các mô hình phần thưởng đơn, trên nhiều thang đo mô hình phần thưởng.
Những bảng này tương ứng với các lần chạy được hiển thị trước đó trong Hình 3, 4, 18 và 19.
Đối với BoN, chúng tôi sử dụng ước lượng không thiên vị ở n= 12500 và so sánh điểm số vàng giữa mỗi
phương pháp và các mô hình phần thưởng đơn cho mỗi prompt. Đối với PPO, chúng tôi so sánh điểm số vàng của
các câu trả lời được tạo ra bởi chính sách sau 3000 bước huấn luyện PPO. Tỷ lệ thắng được tính bằng cách tính trung bình
các tỷ lệ thắng riêng lẻ chống lại mỗi trong năm mô hình phần thưởng đơn, với độ lệch chuẩn cũng
được trình bày. Kết quả cho PPO cũng được tính trung bình trên ba seed PPO.

Bảng 6: Tỷ lệ thắng chính sách cuối cùng (tính theo phần trăm) của các phương pháp ensemble chống lại các mô hình phần thưởng đơn, khi
sử dụng các mô hình phần thưởng 44M. Độ lệch chuẩn trên năm mô hình phần thưởng đơn được chỉ ra.
Các mô hình tương ứng với những mô hình được hiển thị trong Hình 3 và 4.
BoN PPO
Không có nhiễu 25% nhiễu Không có nhiễu 25% nhiễu
Trung bình 54.9±0.9 57.8±1.3 58.1±3.3 60.2±3.5
WCO 57.1±0.6 58.3±0.8 59.4±3.3 62.2±2.9
UWO λ= 0.5 57.2±0.9 58.2±1.0 60.2±3.4 63.0±3.1

Bảng 7: Tỷ lệ thắng chính sách cuối cùng (tính theo phần trăm) của các phương pháp ensemble chống lại các mô hình phần thưởng đơn,
khi sử dụng các mô hình phần thưởng nhỏ hơn (7M) và lớn hơn (1.3B). Độ lệch chuẩn trên năm mô hình phần thưởng đơn
được hiển thị. Mức độ nhiễu được cố định ở 25%, sao cho các mô hình tương ứng với những mô hình trong
Hình 18 và 19. Chính sách cuối cùng 6000 bước được sử dụng cho PPO trong trường hợp mô hình phần thưởng 1.3B.
BoN PPO
7M RM 1.3B RM 7M RM 1.3B RM
Trung bình 57.7±1.4 72.3±1.0 50.8±3.4 56.2±3.5
WCO 58.8±1.4 71.2±1.2 57.9±3.7 62.9±2.9
UWO λ= 0.5 61.3±1.4 73.8±0.8 59.0±3.7 63.1±3.2

23

--- TRANG 24 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
F.10 MẪU ĐỊNH TÍNH
Các chỉ số định lượng được sử dụng trong suốt các thí nghiệm của chúng tôi vì chúng cho phép đánh giá nhanh chóng và
toàn diện các phương pháp khác nhau. Ở đây chúng tôi cung cấp một cái nhìn sâu sắc ngắn gọn về khía cạnh định tính của
các phương pháp được thảo luận trong công trình này. Đối với một prompt đã cho, các câu trả lời từ chính sách cuối cùng (n= 12500 cho
BoN, và 3000 bước cho PPO) của mỗi phương pháp được cung cấp. Các chính sách từ các lần chạy trong Hình 3 và
4, mà cung cấp kết quả cuối cùng mạnh nhất, được sử dụng.

Các điểm chính như sau. Thứ nhất, các câu trả lời BoN ngắn hơn, vì tối ưu hóa chính sách
không xảy ra theo cách giống như nó làm đối với PPO. Thứ hai, các mô hình phần thưởng đơn cho thấy dấu hiệu thất bại
và overoptimization. Đối với BoN, điều này dịch sang các câu trả lời kém và thậm chí là ảo giác trong trường hợp
của Bảng 8. Đối với PPO, điều này cũng dịch sang các câu trả lời kém, rất dài và lặp lại cao.
Đây là những trường hợp rõ ràng của overoptimization, nơi mô hình phần thưởng dường như đã học để
tạo ra các câu trả lời dài bất kể nội dung, mà mang lại hiệu suất mô hình phần thưởng proxy tốt
nhưng hiệu suất mô hình vàng kém. Cuối cùng, chúng tôi lưu ý rằng các phương pháp tối ưu hóa bảo thủ
tạo ra kết quả định tính mạnh, ngay cả khi các mô hình phần thưởng đơn đang gặp khó khăn.

Bảng 8: Tập hợp đầu tiên các câu trả lời ví dụ cho một prompt đánh giá.
Prompt Tìm sân bay gần nhất đến vị trí đã cho.
Seattle, Washington
Mô hình phần thưởng đơn (BoN) Sân bay gần nhất đến vị trí đã cho là Sân bay Sea-Tac, với
một chuyến bay trực tiếp từ sân bay SFO ở San Francisco, CA đến Sân bay
Quốc tế Seattle-Tacoma có sẵn vào các ngày cập nhật.
Trung bình (BoN) Sân bay gần nhất đến Seattle, Washington là Sân bay Quốc tế
Sea-Tac (SEA). Nó cách khoảng 17.6 dặm và có
thời gian đến ước tính khoảng 12:30 PM.
WCO (BoN) Sân bay gần nhất đến vị trí đã cho là Sân bay Quốc tế Seattle-Tacoma
(SITE). Nó cách khoảng 10.7 dặm
từ vị trí đã cho.
UWO (BoN) Sân bay gần nhất đến vị trí đã cho là Sân bay Quốc tế Seattle-Tacoma.
Nó cách 23.7 dặm, và thời gian đến ước tính là 2 giờ, 28 phút.
Mô hình phần thưởng đơn (PPO) Sân bay gần nhất đến vị trí đã cho là Sân bay Quốc tế Seattle-Tacoma,
nằm trong khu vực Seattle, Washington. Đây là một sân bay lớn với một
hub chính, phục vụ hơn 15 triệu hành khách mỗi năm. Nó chỉ cách trung tâm thành phố
10 dặm và được kết nối với thành phố bằng các tuyến đường sắt đô thị Bunker Hill và Space Needle.
Đây là sân bay chính cho khu vực và là một trong những hub quan trọng nhất trong nước.
Đây là một hub chính cho nhiều hãng hàng không, và là một điểm đến phổ biến cho
du khách từ khắp nơi trên thế giới. Đây là một hub cho nhiều chuyến bay
nội địa và quốc tế, và là hub chính cho khu vực. Đây là một hub chính cho
một số hãng hàng không, và là một điểm đến phổ biến cho du khách từ khắp nơi trên thế giới.
Đây là một hub chính cho nhiều chuyến bay nội địa và quốc tế, và là hub chính cho
khu vực. Đây là một hub chính cho nhiều hãng hàng không, và là một điểm đến phổ biến
cho du khách từ khắp nơi trên thế giới. Đây là một hub chính cho nhiều chuyến bay
nội địa và quốc tế, và là hub chính cho khu vực. Đây là một hub chính cho
nhiều hãng hàng không, và là một điểm đến phổ biến cho du khách từ khắp nơi trên thế giới.
Tiếp tục trang tiếp theo
24

--- TRANG 25 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Prompt Tìm sân bay gần nhất đến vị trí đã cho.
Seattle, Washington
Trung bình (PPO) Sân bay gần nhất đến Seattle là Boeing Field, nằm ở Seattle,
Washington. Nó được phục vụ bởi một số hãng hàng không, bao gồm Alaska
Airlines, United Airlines, Delta Air Lines, và American Airlines.
Nó cách khoảng 35 dặm từ vị trí đã cho. Sân bay cũng được phục vụ bởi
một số công ty cho thuê xe, bao gồm Hertz, Avis, và Budget.
WCO (PPO) Sân bay gần nhất đến vị trí đã cho là Sân bay Quốc tế Seattle-Tacoma
(SEA). Nó cách khoảng 43 dặm từ vị trí. Thời gian trung bình để đến
sân bay là khoảng 2 giờ. Chi phí để đến sân bay là khoảng $110 cho một
chiếc xe một chỗ ngồi. Đây là sân bay phổ biến nhất trong khu vực và được phục vụ bởi
nhiều hãng hàng không. Sân bay cung cấp nhiều dịch vụ, bao gồm khu vực nhận hành lý,
quầy cho thuê xe, và trao đổi tiền tệ. Cũng có một số điểm tham quan gần đó,
bao gồm Space Needle, Thủy cung Thái Bình Dương, và Seattle Center. Sân bay được
phục vụ tốt bởi giao thông công cộng, giúp dễ dàng di chuyển xung quanh.
UWO (PPO) Sân bay gần nhất đến vị trí đã cho là Sân bay Quốc tế Seattle-Tacoma
(SEA). Sân bay nằm trong khu vực Seattle-Tacoma, Washington và được phục vụ bởi
nhiều hãng hàng không nội địa và quốc tế. Nó cách khoảng 70 dặm về phía đông
của thành phố Seattle và có thể tiếp cận bằng xe hơi, xe buýt, tàu hỏa, và phà.
Sân bay có một số nhà ga, bao gồm nhà ga nội địa, nhà ga quốc tế, và trung tâm
cho thuê xe. Sân bay có hai đường băng, cho phép cả chuyến bay nội địa và quốc tế.
Sân bay được phục vụ bởi nhiều hãng hàng không quốc tế, bao gồm Alaska, American,
Delta, United, và Virgin. Nó cũng là nhà của nhiều hãng hàng không nội địa,
bao gồm American, Delta, và United.

Bảng 9: Tập hợp thứ hai các câu trả lời ví dụ cho một prompt đánh giá.
Prompt Tạo một phép ẩn dụ để giải thích điện toán đám mây.
Mô hình phần thưởng đơn (BoN) Điện toán đám mây giống như bầu trời; đó là một khoảng rộng lớn, hoang sơ
đầy những hình dạng và kết cấu đầy màu sắc. Giống như mây di chuyển và
thay đổi dựa trên thời tiết, điện toán đám mây cũng vậy. Nó liên tục
phát triển và thích ứng để đáp ứng nhu cầu của công nghệ hiện đại.
Trung bình (BoN) Điện toán đám mây giống như một ngôi nhà rộng lớn, với nhiều phòng
và tầng mà mỗi cái đại diện cho một loại tài nguyên cụ thể. Mỗi
phòng phục vụ một mục đích cụ thể, như lưu trữ, xử lý, hoặc
truy cập mạng, và mỗi tầng có các tính năng riêng như lưu trữ,
sức mạnh tính toán, hoặc kết nối mạng. Đám mây cung cấp một trung tâm
nơi tài nguyên có thể được truy cập từ bất kỳ đâu trong ngôi nhà, và cho phép
người dùng truy cập các tài nguyên họ cần một cách nhanh chóng và dễ dàng.
Tiếp tục trang tiếp theo
25

--- TRANG 26 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Prompt Tạo một phép ẩn dụ để giải thích điện toán đám mây.
WCO (BoN) Điện toán đám mây giống như một phòng máy chủ khổng lồ trong nhà bạn, với
nhiều máy tính được kết nối với nhau để lưu trữ, xử lý, và truy xuất dữ liệu.
Các máy tính giống như các máy chủ nhỏ trong phòng máy chủ, thực hiện các
nhiệm vụ khác nhau như chạy máy ảo, thực hiện phân tích, và thực hiện các
nhiệm vụ khác. Mỗi máy tính được kết nối với máy chủ, và dữ liệu được lưu trữ
trong máy chủ được truy xuất và sử dụng bởi các máy tính. Dữ liệu được lưu trữ
trong một cơ sở dữ liệu, cho phép dữ liệu được truy cập, cập nhật, và xử lý
một cách nhanh chóng và dễ dàng.
UWO (BoN) Điện toán đám mây giống như một chồng bánh kếp; mỗi bánh kếp là
một máy xử lý dữ liệu và cung cấp sức mạnh tính toán. Khi bạn muốn làm
bánh kếp, bạn đơn giản chọn các nguyên liệu, đặt chúng vào chảo, và lật
chúng để làm bánh kếp. Tương tự, khi bạn sử dụng điện toán đám mây, bạn chọn
các dịch vụ bạn cần sử dụng, sau đó trả một khoản phí hàng tháng để sử dụng
chúng. Khi bạn hoàn thành công việc, bạn có thể tắt máy và xóa các tệp,
hoặc nếu bạn cần thêm sức mạnh, bạn có thể đơn giản mua thêm sức mạnh và trả
một khoản phí định kỳ để sử dụng nó.
Mô hình phần thưởng đơn (PPO) Phép ẩn dụ là một máy tính là một mạng lưới các máy chủ kết nối với nhau,
mỗi cái có sức mạnh xử lý và bộ nhớ riêng, và đám mây như trung tâm
nơi dữ liệu được lưu trữ, cho phép các máy chủ khác nhau truy cập dữ liệu
một cách hiệu quả và tiết kiệm chi phí hơn. Chính đám mây cho phép các máy chủ
khác nhau có thể truy cập cùng một dữ liệu theo cùng một cách, và dữ liệu
được lưu trữ và quản lý trong đám mây, do đó cho phép các máy chủ khác nhau
làm việc cùng nhau để cung cấp một giải pháp hiệu quả và tiết kiệm chi phí hơn.
Chính đám mây là xương sống của công nghệ hiện đại và khả năng truy cập dữ liệu
từ các máy chủ khác nhau, một cách tiết kiệm chi phí và hiệu quả hơn, làm cho
điện toán đám mây trở thành giải pháp lý tưởng cho công nghệ hiện đại. Chính đám mây
là tương lai của công nghệ và khả năng truy cập dữ liệu từ các máy chủ khác nhau,
một cách tiết kiệm chi phí và hiệu quả hơn, là nền tảng của công nghệ hiện đại.
Chính đám mây là tiêu chuẩn mới và khả năng truy cập dữ liệu từ các máy chủ
khác nhau, một cách tiết kiệm chi phí và hiệu quả hơn, là nền tảng của tương lai
của công nghệ. Chính đám mây là tương lai của công nghệ, và khả năng
Trung bình (PPO) Điện toán đám mây giống như một đám mây khí khổng lồ cung cấp tất cả
sức mạnh tính toán cần thiết để vận hành một doanh nghiệp. Đám mây cung cấp
truy cập đến các tài nguyên tính toán cần thiết để cho phép doanh nghiệp duy trì
khả năng cạnh tranh và đi trước đối thủ. Điện toán đám mây là một cách
hiệu quả về chi phí để truy cập sức mạnh tính toán cần thiết để vận hành
một doanh nghiệp, và nó có thể giảm thời gian cần thiết để phát triển và
triển khai phần mềm mới. Nó cũng cho phép doanh nghiệp mở rộng hoạt động
một cách nhanh chóng, với khả năng nhanh chóng thêm hoặc loại bỏ tài nguyên
khi cần thiết. Điện toán đám mây là một cách tuyệt vời để doanh nghiệp duy trì
khả năng cạnh tranh và đi trước đối thủ.
Tiếp tục trang tiếp theo
26

--- TRANG 27 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
Prompt Tạo một phép ẩn dụ để giải thích điện toán đám mây.
WCO (PPO) Điện toán đám mây giống như một căn phòng khổng lồ, mở với nhiều bức tường
và sàn khác nhau. Các bức tường cung cấp bảo mật và quyền riêng tư, trong khi
các sàn cung cấp tài nguyên và truy cập dữ liệu. Trần nhà là bầu trời, và
bầu trời là bầu trời, cung cấp một lượng lớn tài nguyên, sức mạnh, và cơ hội.
Đám mây là một căn phòng đầy tài nguyên, và các tài nguyên là con người, cung cấp
một cộng đồng đa dạng về chuyên môn và tài nguyên. Các bức tường, sàn, và bầu trời
đều quan trọng, và cùng nhau chúng cung cấp một tài nguyên vô giá. Phép ẩn dụ này
giúp giải thích tầm quan trọng của điện toán đám mây, và cách nó có thể có lợi
cho các tổ chức thuộc mọi quy mô. Nó cho thấy sức mạnh của đám mây và các cơ hội
nó cung cấp, cho phép các tổ chức mở rộng quy mô một cách nhanh chóng và hiệu quả.
Nó cũng minh họa tầm quan trọng của việc cung cấp một cơ sở hạ tầng an toàn,
đáng tin cậy, và hiệu quả về chi phí.
UWO (PPO) Điện toán đám mây giống như một thư viện hiện đại, nơi dữ liệu được lưu trữ
và xử lý trên một máy chủ trung tâm. Dữ liệu được truy cập và thao tác từ
bất kỳ thiết bị nào kết nối với internet, cho phép các dịch vụ nhanh hơn và
hiệu quả hơn. Dữ liệu được lưu trữ trong đám mây và có thể truy cập từ bất kỳ
thiết bị nào, cho phép người dùng truy cập dữ liệu một cách nhanh chóng và hiệu quả.
Nền tảng điện toán đám mây dễ sử dụng, cung cấp cho người dùng sự linh hoạt
để truy cập dữ liệu một cách nhanh chóng và dễ dàng, và có hiệu quả về chi phí.
Nền tảng điện toán đám mây cung cấp cho người dùng khả năng truy cập dữ liệu
một cách nhanh chóng và dễ dàng, cho phép họ đưa ra quyết định dựa trên dữ liệu.
Nền tảng điện toán đám mây cũng cung cấp cho người dùng sự linh hoạt để truy cập
dữ liệu một cách nhanh chóng và dễ dàng, cho phép họ đưa ra quyết định dựa trên dữ liệu.
Điều này cho phép nền tảng điện toán đám mây trở thành một giải pháp hiệu quả về chi phí
cho người dùng, trong khi cung cấp cho họ khả năng truy cập dữ liệu một cách nhanh chóng và dễ dàng.

27
