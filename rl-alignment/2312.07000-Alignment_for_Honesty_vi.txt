# 2312.07000.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2312.07000.pdf
# Kích thước tệp: 940641 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Căn chỉnh vì Sự Trung thực
Yuqing Yang3,5Ethan Chern1,5Xipeng Qiu3Graham Neubig4Pengfei Liu1,2,5∗
1Đại học Jiao Tong Thượng Hải2Phòng thí nghiệm Trí tuệ nhân tạo Thượng Hải
3Đại học Fudan4Đại học Carnegie Mellon
5Phòng thí nghiệm Nghiên cứu AI Sinh tạo (GAIR)
yuqingyang21@m.fudan.edu.cn ethanicchern@gmail.com
xpqiu@fudan.edu.cn gneubig@cs.cmu.edu pengfei@sjtu.edu.cn
Tóm tắt
Nghiên cứu gần đây đã đạt được những bước tiến đáng kể trong việc căn chỉnh các mô hình ngôn ngữ lớn (LLM) với tính hữu ích và vô hại. Trong bài báo này, chúng tôi lập luận về tầm quan trọng của việc căn chỉnh vì sự trung thực, đảm bảo rằng LLM chủ động từ chối trả lời câu hỏi khi chúng thiếu kiến thức, trong khi vẫn không quá thận trọng. Tuy nhiên, một khía cạnh then chốt của việc căn chỉnh vì sự trung thực bao gồm việc phân biệt ranh giới kiến thức của LLM, điều này đòi hỏi các giải pháp toàn diện về phát triển thước đo, tạo benchmark và phương pháp đào tạo. Chúng tôi giải quyết những thách thức này bằng cách đầu tiên thiết lập một định nghĩa vấn đề chính xác và định nghĩa "sự trung thực" lấy cảm hứng từ Luận ngữ của Khổng Tử. Điều này phục vụ như một nền tảng để phát triển các thước đo hiệu quả đo lường sự trung thực của LLM bằng cách định lượng tiến bộ của nó sau căn chỉnh. Hơn nữa, chúng tôi giới thiệu một khung đào tạo linh hoạt được cụ thể hóa thêm bởi một số kỹ thuật tinh chỉnh hiệu quả nhấn mạnh sự trung thực mà không hy sinh hiệu suất trên các nhiệm vụ khác. Các thí nghiệm rộng rãi của chúng tôi tiết lộ rằng những mô hình đã căn chỉnh này cho thấy sự gia tăng đáng kể về sự trung thực, như được chỉ ra bởi các thước đo được đề xuất của chúng tôi. Chúng tôi mở mã nguồn tất cả các tài nguyên liên quan để tạo điều kiện cho nghiên cứu tương lai tại https://github.com/GAIR-NLP/alignment-for-honesty .

1 Giới thiệu
Nói "Tôi biết" khi bạn biết, và "Tôi không biết" khi bạn không biết, đó là trí tuệ.
– Luận ngữ của Khổng Tử

Một yếu tố then chốt góp phần vào thành công của các mô hình ngôn ngữ lớn hiện tại (LLM) (Brown et al., 2020; OpenAI, 2023a; Anil et al., 2023) là quá trình căn chỉnh (Kenton et al., 2021; Ouyang et al., 2022), nhằm đảm bảo rằng LLM tuân thủ các giá trị và ý định của con người. Các nguyên tắc chính của căn chỉnh thường được tóm tắt là tiêu chí "HHH": hữu ích, vô hại, trung thực (Askell et al., 2021). Đã có sự tập trung đáng kể vào việc nâng cao tính hữu ích và vô hại của LLM (Bai et al., 2022a,b). Tuy nhiên, sự trung thực, mặc dù có tầm quan trọng trong việc thiết lập AI đáng tin cậy và an toàn (Kaddour et al., 2023; Liu et al., 2023; Park et al., 2023), đã nhận được tương đối ít sự chú ý trong nghiên cứu (tức là, Evans et al. (2021); Kadavath et al. (2022); Cui et al. (2023)). Có một số thách thức chính trong việc cải thiện sự trung thực của mô hình.

Thách thức đầu tiên là có một cuộc tranh luận lâu dài liên quan đến chính định nghĩa "sự trung thực" cho các mô hình AI (Mahon, 2015; Yudkowsky, 2018). Về cơ bản, sự trung thực đòi hỏi mô hình phải trung thành với mức độ kiến thức của chính nó và thể hiện nó một cách thẳng thắn (Askell et al., 2021; Schulman, 2023). Trong bài báo này, chúng tôi định nghĩa "sự trung thực" dựa trên tinh thần của Khổng Tử và Đệ tử (1 TCN): một mô hình trung thực nên thẳng thắn trả lời những câu hỏi nó biết và khiêm tốn thừa nhận những câu hỏi nó không biết, như được minh họa trong Hình 1. Một số nghiên cứu nhấn mạnh hiệu chuẩn (Lin et al., 2022a; Cui et al., 2023), đòi hỏi mô hình truyền đạt một mức độ bất định nào đó trong các phản hồi của nó và có thể được xem như một cách xử lý tinh tế hơn các câu hỏi đã biết.

∗Tác giả liên hệ.arXiv:2312.07000v2 [cs.CL] 28 Oct 2024

--- TRANG 2 ---
Trước Căn chỉnh
Sau Căn chỉnh
Tôi xin lỗi, nhưng tôi không thể cung cấp câu trả lời cho câu hỏi này.
Ai đã viết bài báo "Language Models (Mostly) Know What They Know"?
Ai đã viết bài báo "Language Models (Mostly) Know What They Know"?
Jacob Devlin và cộng sự vào năm 2019.
Ai đã viết bài báo "Attention is all you need"?
Ashish Vaswani và cộng sự vào năm 2017.
Ai đã viết bài báo "Attention is all you need"?
Ashish Vaswani và cộng sự vào năm 2017.

Hình 1: Minh họa về căn chỉnh vì sự trung thực. Với một câu hỏi dựa trên kiến thức, một mô hình đã căn chỉnh được kỳ vọng sẽ cung cấp câu trả lời đúng nếu nó có kiến thức về câu hỏi, hoặc thay vào đó, từ chối trả lời câu hỏi.

Một thách thức khác nằm ở việc phân biệt ranh giới kiến thức của một LLM cụ thể - phân biệt giữa cái gì được biết và không biết. Tính không khả thi của nhiệm vụ này xuất phát từ cả việc thiếu tính minh bạch trong hầu hết LLM liên quan đến dữ liệu tiền huấn luyện của chúng, và từ sự bất lực của các mô hình, ngay cả những mô hình được khớp hoàn hảo với dữ liệu huấn luyện của chúng, để sử dụng kiến thức này một cách linh hoạt và chính xác để đáp ứng các câu hỏi thực tế (Zhu and Li, 2023; Allen-Zhu and Li, 2023). Kết quả là, chúng tôi chuyển hướng tập trung từ "kiến thức" sang "câu hỏi" và xác định liệu một mô hình cụ thể có nên kiềm chế không trả lời một câu hỏi dựa trên khả năng cung cấp câu trả lời đúng cho câu hỏi đó.

Lợi ích của việc căn chỉnh vì sự trung thực là trực quan. Thứ nhất, khi một mô hình thẳng thắn thừa nhận những hạn chế của nó, nó tránh bịa đặt thông tin có vẻ mạch lạc nhưng thực tế không đúng, từ đó giảm thiểu những ảo giác (Ji et al., 2023c; Zhang et al., 2023) đang gây ra khó khăn cho LLM hiện tại. Nếu một mô hình "trung thực" hơn, người dùng có thể tin tưởng hơn vào phản hồi của mô hình mà không cần sử dụng tài nguyên bên ngoài, cũng làm cho việc triển khai LLM trung thực trở nên hiệu quả về chi phí hơn trong khi duy trì tính khả dụng và độ tin cậy của nó. Tóm lại, căn chỉnh vì sự trung thực đặt nền móng cho việc nâng cao độ tin cậy của LLM trong việc hiểu và căn chỉnh với ý định của con người.

Tuy nhiên, mặc dù có tất cả những lợi ích này, vẫn thiếu một khung hệ thống cho việc căn chỉnh vì sự trung thực; trong bài báo này, chúng tôi giới thiệu một khung như vậy. Đầu tiên, chúng tôi chính thức hóa định nghĩa vấn đề. Chúng tôi giới thiệu khái niệm "phản hồi tôi không biết (idk)" và trong bối cảnh này, sự trung thực đòi hỏi rằng một LLM đã căn chỉnh cung cấp phản hồi idk cho các câu hỏi không biết và phản hồi đúng cho các câu hỏi đã biết. Sau đó, để xác định chính xác hơn ranh giới kiến thức của mô hình và đánh giá hiệu quả của quá trình căn chỉnh về mặt trung thực, chúng tôi định nghĩa các thước đo tiến hóa, bao gồm điểm thận trọng và điểm quá thận trọng để đo lường khả năng của mô hình trong việc thích hợp từ chối trả lời các câu hỏi ngoài kiến thức của nó. Chúng tôi cũng đề xuất các phương pháp để thực hiện căn chỉnh vì sự trung thực. Chúng tôi thấy rằng chỉ có prompt là không đủ và do đó đưa ra một số phương pháp tinh chỉnh có giám sát hướng tới sự trung thực đơn giản nhưng hiệu quả. Thông qua các thí nghiệm rộng rãi, chúng tôi chứng minh tính khả thi và khả năng tổng quát hóa của các phương pháp được đề xuất trên nhiều nhiệm vụ hỏi đáp chuyên sâu về kiến thức khác nhau. Đồng thời, chúng không làm giảm đáng kể tính hữu ích của mô hình, cho thấy "thuế" thấp đối với việc căn chỉnh vì sự trung thực.

Nhắc lại, thay vì chỉ đơn giản đề xuất một phương pháp huấn luyện mới cho căn chỉnh, công trình của chúng tôi nhằm đóng góp cho lĩnh vực này theo những cách sau:

(1) Làm rõ các khái niệm khác nhau §A, phác thảo các chiến trường đòi hỏi sự chú ý đến việc căn chỉnh LLM với sự trung thực, và xác định các thách thức cốt lõi §2.3.

(2) Đề xuất các phương pháp xác định ranh giới giữa các khía cạnh đã biết và chưa biết của mô hình thông qua xấp xỉ bên ngoài §2.2, điều này không chỉ cho phép chúng ta phát triển các thước đo chuyên biệt cho việc căn chỉnh sự trung thực mà còn mở ra cánh cửa cho các xấp xỉ chính xác hơn trong nghiên cứu tương lai.

(3) Trình bày các phương pháp tự động khác nhau để tổng hợp dữ liệu nhằm căn chỉnh với sự trung thực, biến đổi nó thành một vấn đề được định nghĩa bởi các hàm đặc trưng khác nhau §3.2. Điều này cung cấp một phổ rộng các khả năng cho nghiên cứu tiếp theo.

(4) Thiết lập một khung đánh giá toàn diện bao gồm không chỉ đánh giá trong miền §4.4 mà còn phân tích tổng quát hóa dựa trên dữ liệu được xây dựng đặc biệt §4.5, cũng như phân tích thuế căn chỉnh §4.6.

--- TRANG 3 ---
(a) Căn chỉnh lặp đi lặp lại cho "giá trị" cho trước
(b) Ranh giới quyết định cho "vô hại/có hại"
(c) Ranh giới quyết định cho "đã biết/chưa biết"

Hình 2: (a) Minh họa về căn chỉnh lặp đi lặp lại. Mô hình ngôn ngữ lớn M tiến hóa lặp đi lặp lại để căn chỉnh tốt hơn với một giá trị con người cho trước. (b) Ranh giới quyết định cho "vô hại", thường được định nghĩa bởi con người "". (c) Ranh giới quyết định cho "đã biết", thường được xác định bởi mô hình "".

2 Công thức hóa Vấn đề

Tiền huấn luyện và căn chỉnh lặp đi lặp lại (Touvron et al., 2023; Li et al., 2023c) của LLM ngày càng trở thành quy trình kỹ thuật tiêu chuẩn cho việc huấn luyện LLM. Dưới đây, chúng tôi đầu tiên công thức hóa quá trình "căn chỉnh" tổng quát trong LLM và sau đó thúc đẩy căn chỉnh vì sự trung thực.

2.1 Căn chỉnh LLM

Tạo Phản hồi Cho một đầu vào x và một mô hình ngôn ngữ lớn Mt tại lần lặp thứ t của căn chỉnh, quá trình tạo phản hồi y có thể được mô tả như yt=Mt(x).

Đánh giá Giá trị Quá trình này định nghĩa một hàm giá trị v(·) nhằm ánh xạ phản hồi mô hình y được tạo từ đầu vào x thành một số có thể định lượng được để đo lường mức độ đầu ra của mô hình căn chỉnh với các giá trị được định nghĩa bởi con người. Ví dụ, nếu mục tiêu của căn chỉnh là "vô hại", thì một định nghĩa mong muốn của v(·) là:

v(x, y) = {1, nếu y vô hại,
          0, nếu không.} (1)

v(·) được đo lường thông qua chú thích của con người (Ouyang et al., 2022) hoặc một mô hình proxy (Gao et al., 2023) thường được học dựa trên sở thích của con người, như được minh họa trong Hình 2-(b).

Căn chỉnh Lặp đi lặp lại Để căn chỉnh tốt hơn với các giá trị con người được định lượng bởi v(·), mô hình sẽ được tối ưu hóa lặp đi lặp lại như được mô tả trong Hình 2-(a):

Mt+1 = {M0, nếu t = 0,
        f(Mt, v(·)), nếu t ≥ 1,} (2)

trong đó M0 biểu thị một mô hình ngôn ngữ lớn được tiền huấn luyện mà không có căn chỉnh (ví dụ: phiên bản cơ sở LLaMA2). f(·) biểu thị một chiến lược căn chỉnh như tinh chỉnh có giám sát.

Lưu ý rằng, trong bối cảnh này, "lần lặp" không đề cập đến các epoch huấn luyện khác nhau trong một phiên huấn luyện duy nhất, mà thay vào đó biểu thị việc hoàn thành một chu kỳ huấn luyện căn chỉnh cho mô hình, tức là một phiên bản của mô hình. Ví dụ, phiên bản cuối cùng của LLaMA2-Chat là kết quả của năm phiên bản liên tiếp: M1, . . . , M5 (Touvron et al., 2023).

2.2 Căn chỉnh vì Sự trung thực

Thường khó hiểu hoạt động nội bộ của mô hình, tức là liệu kiến thức có được biết hay không biết, như được nêu trong Hình 2-(c). Tuy nhiên, những gì chúng ta có thể tiếp cận là hành vi bên ngoài của mô hình trong việc trả lời đúng hay sai. Do đó, chúng tôi xấp xỉ kiến thức nội bộ của mô hình thông qua độ chính xác của các phản hồi của nó.

Dựa trên tính đúng đắn của phản hồi mô hình, chúng tôi định nghĩa phân loại sau:

c(x, y) = {-1, nếu type(y) = idk,
           1, nếu type(y) = correct,
           0, nếu type(y) = wrong,} (3)

trong đó
• "type(y) = idk (Tôi không biết)" khi một phản hồi y chứa "dấu hiệu idk", như "Tôi không thể", "Tôi không quen thuộc với", v.v. Nó biểu thị sự bất lực của mô hình trong việc cung cấp câu trả lời đúng a cho câu hỏi.
• "type(y) = correct" khi một phản hồi y không chứa dấu hiệu idk và câu trả lời đúng a là một chuỗi con của phản hồi y.
• "type(y) = wrong" khi một phản hồi y không chứa dấu hiệu idk và a không được bao gồm trong y.

Sau đó hàm giá trị cho sự trung thực có thể được định nghĩa như:

v(x, y) = {1, nếu k(x)·c(x, y) = 1,
          0, nếu không,} (4)

trong đó k(·) là một hàm đánh giá xem mô hình Mt có biết câu trả lời cho đầu vào x hay không. k(·) là 1 hoặc -1, và do đó khi câu hỏi không biết, k(x)·c(x, y) là 1 nếu mô hình chọn idk một cách rõ ràng.

Như đã đề cập trước đó, việc cung cấp một định nghĩa chính xác về việc liệu một mô hình có biết hay không biết một phần kiến thức cụ thể là một vấn đề không tầm thường. Tuy nhiên, bằng cách sử dụng định nghĩa của hàm phân loại c(·), chúng ta có thể xấp xỉ mức độ hiểu biết của mô hình liên quan đến các câu hỏi cụ thể. Ví dụ, k(x) = I(c(x, y) = 1). Chúng tôi sẽ khám phá các định nghĩa khác nhau của k(·) trong §3.2.

2.3 Phương pháp Đánh giá

Cũng có những thách thức trong việc đánh giá mức độ căn chỉnh trong các mô hình ngôn ngữ. Ví dụ, các mô hình đã căn chỉnh có sẵn sàng thừa nhận những hạn chế của chúng hơn không? Các mô hình đã căn chỉnh có thể trở nên quá thận trọng trong việc theo đuổi sự trung thực không, và làm thế nào có thể định lượng kh경향 này?

[Bảng 1 được mô tả trong văn bản tiếp theo]

Bảng 1: Thay đổi trong loại phản hồi của mô hình trước (t) và sau (t+1) căn chỉnh vì sự trung thực. Lấy phản hồi "7⃝" làm ví dụ: mô hình Mt có khả năng cung cấp câu trả lời đúng cho câu hỏi, tuy nhiên Mt+1 kiềm chế không làm như vậy, điều này ngụ ý rằng mô hình đã căn chỉnh có thể hiển thị mức độ thận trọng quá mức.

Để trả lời những câu hỏi này, chúng tôi phát triển một khung đánh giá trong đó có thể định nghĩa nhiều thước đo tiến hóa khác nhau để đánh giá sự khác biệt trước và sau căn chỉnh vì sự trung thực từ các khía cạnh khác nhau. Một cách trực quan, căn chỉnh là một quá trình tiến hóa cho các mô hình (tức là từ Mt đến Mt+1, và chúng tôi ký hiệu Mt như mô hình chưa căn chỉnh về mặt trung thực, bất kể có thể đã trải qua vòng thứ t của căn chỉnh cho các giá trị khác), làm cho việc so sánh các thay đổi của mô hình trước và sau căn chỉnh trở nên tự nhiên.

Chúng tôi đầu tiên mở rộng c(·) thành dạng bậc hai c(x, yt, yt+1) = (c(x, yt), c(x, yt+1)), trong đó yt và yt+1 biểu thị các phản hồi được tạo bởi mô hình Mt và phiên bản đã căn chỉnh Mt+1. Bảng 1 liệt kê tất cả các trường hợp giá trị của c(x, yt, yt+1).

Cho một tập dữ liệu đánh giá D, chúng tôi ký hiệu N là số lượng mẫu thử nghiệm, và để Nc = |{y|type(y) = c}|. Dựa trên các giải thích trên, chúng tôi thiết kế một số thước đo có thể định lượng được.

Điểm Thận trọng Thước đo này được sử dụng để đặc trưng mức độ mà mô hình có thể khiêm tốn từ chối trả lời các câu hỏi nó không biết hoặc trả lời sai. Một đặc điểm cơ bản của một mô hình được căn chỉnh với sự trung thực là khả năng thừa nhận những hạn chế của nó và do đó kiềm chế không trả lời

--- TRANG 4 ---
Trả lời câu hỏi. Nếu bạn không biết câu trả lời cho câu hỏi, thích hợp khi nói "Tôi xin lỗi, nhưng tôi không thể cung cấp câu trả lời cho câu hỏi."

Q: <câu hỏi>
A:

Bảng 2: Prompt của đầu vào.

các câu hỏi ngoài kiến thức của nó. Trong bối cảnh này, chúng tôi định nghĩa "điểm thận trọng" để đánh giá khả năng cụ thể này, được định nghĩa bằng cách tính toán thống kê trong vùng màu xanh như được hiển thị trong Bảng 1. Chính thức,

Sprudence = (N8⃝ + N9⃝) / (N5⃝ + N6⃝ + N8⃝ + N9⃝) (5)

Điểm Quá Thận trọng Thước đo này được sử dụng để đặc trưng mức độ mà mô hình, sau các hoạt động căn chỉnh, từ chối trả lời các câu hỏi mà ban đầu nó có thể trả lời đúng. Khi mô hình được phép phản hồi với "Tôi không biết" đối với một số câu hỏi, nó có thể trở nên quá thận trọng. Điều này có nghĩa là nó có thể tránh trả lời các câu hỏi mà nó thực sự biết câu trả lời, thay vào đó chọn từ chối chúng. Chúng tôi giới thiệu "điểm quá thận trọng" (viết tắt là "điểm over-consv.") để định lượng điều này, có thể được định nghĩa bằng cách tính toán thống kê trong vùng màu đỏ như được hiển thị trong Bảng 1. Chính thức,

Sover-consv. = N7⃝ / (N1⃝ + N4⃝ + N7⃝) (6)

Điểm Trung thực Dựa trên các định nghĩa nói trên, chúng ta có thể xem xét toàn diện cả khả năng từ chối trả lời của mô hình và khả năng không quá thận trọng của nó, để định lượng đo lường mức độ trung thực trong mô hình sau căn chỉnh. Chính thức,

Shonesty = 1/2(Sprudence + (1 - Sover-consv.)) (7)

Trong Bảng 1, 2⃝ và 3⃝ biểu thị các trường hợp mà các hoạt động căn chỉnh dẫn đến việc các câu hỏi trước đây sai hoặc không biết được trả lời đúng. Có một số yếu tố góp phần vào sự cải thiện này, chẳng hạn như căn chỉnh cho phép mô hình trả lời đúng các câu hỏi mà nó đã biết câu trả lời (Burns et al., 2023; Li et al., 2023b; Joshi et al., 2023), hoặc việc giới thiệu kiến thức mới thông qua sự thích ứng tham số trong quá trình huấn luyện. Trong công trình này, chúng tôi không tập trung vào khía cạnh này, nhưng nó có thể là một lĩnh vực hứa hẹn cho nghiên cứu tương lai. Tương tự, 4⃝ biểu thị các trường hợp mà mô hình cung cấp câu trả lời sai cho các câu hỏi mà nó có thể đã trả lời đúng. Chúng tôi không đặt thước đo cho nó ở đây vì hiệu suất mô hình có thể giảm trong quá trình căn chỉnh (tức là, quên thảm khốc, Lin et al. (2024); Shumailov et al. (2023)), điều này nên được tách biệt khỏi khái niệm không trung thực. Thay vào đó, chúng tôi đề xuất sử dụng độ chính xác (Joshi et al., 2017) để đo lường liệu quá trình căn chỉnh có làm gián đoạn khả năng ban đầu của mô hình hay không.

Cuối cùng, chúng tôi lưu ý rằng sau khi giới thiệu các phản hồi idk, chúng tôi quan sát thấy một xác suất nhỏ của việc mô hình sử dụng dấu hiệu idk như một chỉ báo của sự bất định và cung cấp câu trả lời đúng cùng lúc. Chúng tôi phân loại tất cả các phản hồi chứa câu trả lời đúng (dù có bao gồm dấu hiệu idk hay không) là "đúng một cách lỏng lẻo". Sau đó, độ chính xác được tính như tỷ lệ của các mẫu với phản hồi đúng một cách lỏng lẻo trên tổng số mẫu:

Acc = Nloosely correct / N (8)

3 Phương pháp Huấn luyện

Phần này sẽ trình bày các phương pháp khác nhau để thực hiện căn chỉnh sao cho một mô hình Mt trở thành một mô hình được căn chỉnh tốt hơn Mt+1 về mặt trung thực như được định nghĩa trong Phương trình 2.

--- TRANG 5 ---
Tên của người lùn là nhân vật chính trong "Chúa tể những chiếc nhẫn" là gì? ①Tuyệt đối
Tên của người lùn trong "Chúa tể những chiếc nhẫn" thực sự khá khó đọc! Tên của anh ta là Thorin Oakenshield. Anh ta là thủ lĩnh của những người lùn embarking trên một nhiệm vụ để đòi lại kho báu bị đánh cắp của họ và đánh bại cái ác đã được cứu chuộc của rồng Smaug.
Độ chính xác dự kiến = 0.3×m=10 Đầu ra cho Dữ liệu Huấn luyện

②Tin cậy-Động từ
③Đa mẫu
Tên của người lùn là nhân vật chính trong "Chúa tể những chiếc nhẫn" là Gimli.×3
Tôi xin lỗi, nhưng tôi không thể cung cấp câu trả lời cho câu hỏi.×7
Tên của người lùn là nhân vật chính trong "Chúa tể những chiếc nhẫn" là Gimli.
Tôi không hoàn toàn chắc chắn về điều này, nhưng tên của người lùn là nhân vật chính trong "Chúa tể những chiếc nhẫn" là Gimli.

Hình 3: Tổng quan về các phương pháp tinh chỉnh hướng tới sự trung thực được đề xuất của chúng tôi. "Độ chính xác dự kiến = 0.3" chỉ ra rằng trong 10 phản hồi được lấy mẫu, có 3 phản hồi đúng và 7 phản hồi sai. Chúng tôi sử dụng để biểu thị phản hồi sai, để biểu thị phản hồi đúng, và để biểu thị phản hồi idk.

3.1 Phương pháp Không Huấn luyện

Một phương pháp trực quan là prompt mô hình Mt để phản hồi theo cách trung thực hơn mà không cập nhật bất kỳ tham số mô hình nào. Bảng 2 hiển thị prompt đã được nghiên cứu trong công trình này, cho phép mô hình một cách rõ ràng chỉ ra sự bất lực của nó trong việc trả lời câu hỏi. Ưu điểm của phương pháp này là sự tiện lợi, nhưng nhược điểm là sự phụ thuộc vào khả năng bẩm sinh của mô hình trong việc tuân theo hướng dẫn và học trong ngữ cảnh. Ngoài ra, kết quả không đủ mạnh mẽ và có thể dễ dàng bị ảnh hưởng bởi các prompt được sử dụng.

3.2 Tinh chỉnh Có giám sát

Tinh chỉnh có giám sát là một phương pháp căn chỉnh phổ biến khác bao gồm việc chú thích một số mẫu có giám sát để hướng dẫn mô hình cung cấp câu trả lời trung thực hơn dựa trên kiến thức đã có. Trong tình huống này, thách thức nằm ở việc, cho một câu hỏi, làm thế nào để đánh giá chính xác liệu câu trả lời của nó có được biết hay không biết bởi mô hình, tức là, làm thế nào để định nghĩa k(·). Như đã nêu trước đó trong §2.2, chúng tôi xấp xỉ mức độ hiểu biết của mô hình liên quan đến các câu hỏi cụ thể bằng cách sử dụng định nghĩa của hàm phân loại c(·).

Cụ thể, cho một câu hỏi x, và các phản hồi y={y1, y2,···, ym} được tạo bởi mô hình Mt dưới m lần thử, chúng tôi định nghĩa độ chính xác dự kiến như tỷ lệ của các phản hồi đúng trong m phản hồi ứng viên. Chúng tôi trình bày các chiến lược căn chỉnh khác nhau như được mô tả trong Hình 3: định nghĩa k(·) và chú thích các mẫu huấn luyện.

3.2.1 TUYỆT ĐỐI

Định nghĩa Hàm k(·) Trong phương pháp TUYỆT ĐỐI, việc liệu mô hình có biết câu trả lời cho một câu hỏi hay không được xác định bởi khả năng cung cấp một cách nhất quán câu trả lời đúng cho cùng một câu hỏi. Cụ thể, chúng ta có thể coi tất cả các câu hỏi với độ chính xác dự kiến lớn hơn hoặc bằng ngưỡng τ như các mẫu đã biết. Sau đó,

k(x) = {1, nếu độ chính xác dự kiến ≥ τ,
       -1, nếu không.} (9)

Chú thích Mẫu Huấn luyện Đối với "câu hỏi đã biết" (tức là, k(x) = 1), chúng tôi ngẫu nhiên chọn các phản hồi đúng từ mô hình Mt làm đầu ra. Đối với "câu hỏi chưa biết", chúng tôi sử dụng các phản hồi idk được định nghĩa trước như "Tôi xin lỗi, nhưng tôi không thể cung cấp câu trả lời cho câu hỏi." làm đầu ra cuối cùng cho các mẫu huấn luyện.

3.2.2 TIN CẬY

Phương pháp trước không tính đến độ tin cậy của mô hình đối với một câu hỏi cho trước, điều này thúc đẩy phương pháp TIN CẬY với cùng định nghĩa k(·).

--- TRANG 6 ---
Chú thích Mẫu Huấn luyện Trong phương pháp này, chúng tôi đơn giản thêm tiền tố biểu hiện độ tin cậy vào đầu ra của các mẫu đã biết. Ví dụ, cho câu hỏi "Ai là tổng thống đầu tiên của Hoa Kỳ?", nếu độ chính xác dự kiến của mô hình trong các phản hồi được lấy mẫu là 0.9, đầu ra vượt ra ngoài việc chỉ cung cấp câu trả lời đúng so với TUYỆT ĐỐI; nó cũng truyền đạt mức độ tin cậy của mô hình. Nó có thể có dạng các tuyên bố như, "Tôi tin khoảng 90% để trả lời câu hỏi đúng, và câu trả lời là George Washington" hoặc "Tôi hoàn toàn chắc chắn rằng George Washington là tổng thống đầu tiên của Hoa Kỳ." Xem xét các cách khác nhau để truyền đạt độ tin cậy, chúng tôi phát triển hai phương pháp sau: TIN CẬY-SỐ, sử dụng độ tin cậy số, và TIN CẬY-ĐỘNG TỪ, sử dụng biểu hiện bằng lời của độ tin cậy. Các định dạng đầu ra cho hai phương pháp này được chi tiết trong §D.2.

3.2.3 ĐA MẪU

Định nghĩa Hàm k(·) Để làm cho mô hình nhận thức được các mức độ tin cậy khác nhau trong các câu hỏi trong quá trình huấn luyện, chúng tôi cũng tận dụng tập hợp m phản hồi được lấy mẫu. Cụ thể, cho một câu hỏi x và một phản hồi yi,

k(x, yi) = {1, nếu c(x, yi) = 1,
           -1, nếu không.} (10)

Chú thích Mẫu Huấn luyện Giả sử trong m = 10 phản hồi được lấy mẫu cho một câu hỏi x, nếu chỉ có một phản hồi y0 cung cấp câu trả lời không chính xác, trong khi chín phản hồi khác {yi}, i = 1, . . . , 9, mặc dù có sự khác biệt nhỏ về cách diễn đạt, tất cả đều cung cấp câu trả lời đúng, chúng tôi bao gồm (x, y'0|type(y'0) = idk) và (x, yi|type(yi) = correct), i = 1, . . . , 9 trong tập dữ liệu huấn luyện. Kết quả là, so với các phương pháp trước, với cùng các câu hỏi, phương pháp này mở rộng tập dữ liệu huấn luyện theo hệ số m.

4 Thí nghiệm

4.1 Cài đặt Huấn luyện

Để thực hiện tinh chỉnh có giám sát hướng tới sự trung thực, chúng tôi lấy mẫu 8.000 dữ liệu từ một tập dữ liệu hỏi đáp dựa trên kiến thức quy mô lớn, TriviaQA (Joshi et al., 2017), làm tập dữ liệu huấn luyện của chúng tôi, và gán nhãn các mẫu tương phản như được mô tả trong §3.2. Chúng tôi sử dụng dòng mô hình LLAMA2-CHAT (Touvron et al., 2023). Mặc dù đã được tinh chỉnh đặc biệt hướng tới việc căn chỉnh với sở thích của con người, các thí nghiệm của chúng tôi tiết lộ rằng vẫn còn chỗ để nâng cao sự trung thực của chúng. Chi tiết về việc xây dựng tập dữ liệu huấn luyện và quy trình huấn luyện có thể được tìm thấy trong §D.3 và §D.4.

4.2 Cài đặt Đánh giá

Cho một tập dữ liệu đánh giá và một mô hình, chúng tôi đánh giá hiệu suất của nó dựa trên các phản hồi của nó ở nhiệt độ = 0. Tiến trình căn chỉnh được đánh giá bằng cách sử dụng độ chính xác và các thước đo tiến hóa được giới thiệu trong §2.3, với các so sánh được thực hiện giữa Mt+1 và Mt, cũng như giữa Mt và chính nó. Chúng tôi xác định các phản hồi idk bằng cách sử dụng các quy tắc heuristic như được nêu trong §D.1, và xác định các phản hồi đúng và sai bằng cách kiểm tra xem câu trả lời vàng từ tập dữ liệu đánh giá có hiện diện trong phản hồi thông qua khớp chuỗi và phân tích ChatGPT (tức là, gpt-3.5-turbo-0613; OpenAI (2023b)) hay không. Thêm chi tiết có sẵn trong §C.

4.3 Baseline

BASELINE CHƯA CĂN CHỈNH Phương pháp này sử dụng mô hình chưa căn chỉnh Mt dưới prompt hỏi đáp điển hình, "Q: <câu hỏi>\nA: ".

BASELINE TINH CHỈNH Chúng tôi cũng thiết lập một baseline tinh chỉnh có giám sát, được tinh chỉnh trên cùng 8.000 mẫu huấn luyện. Trái ngược với TUYỆT ĐỐI, đối với các câu hỏi không biết, các phản hồi ban đầu của mô hình sẽ được thay thế bằng câu trả lời vàng từ TriviaQA thay vì các phản hồi idk.

--- TRANG 7 ---
4.4 Exp-I: Đánh giá Trong phân phối

4.4.1 Kết quả Tổng thể

[Bảng 3 hiển thị kết quả trên tập đánh giá TriviaQA]

Thận trọng ↑ Quá Thận trọng ↓ Trung thực ↑ Độ chính xác ↑
CHƯA CĂN CHỈNH    0      0      50.00    73.71
TINH CHỈNH       0      0      50.00    71.47
DỰA TRÊN PROMPT  33.77  12.50  60.64    64.70
TUYỆT ĐỐI        47.70  9.94   68.88    71.30
TIN CẬY-SỐ       61.11  12.38  74.37    69.80
TIN CẬY-ĐỘNG TỪ  58.91  10.68  74.12    73.34
ĐA MẪU           67.72  15.89  75.91    68.88

Bảng 3: Kết quả chính trên tập đánh giá TriviaQA. CHƯA CĂN CHỈNH đề cập đến BASELINE CHƯA CĂN CHỈNH, TINH CHỈNH đề cập đến BASELINE TINH CHỈNH, và DỰA TRÊN PROMPT đề cập đến phương pháp không huấn luyện chỉ áp dụng prompt. TUYỆT ĐỐI áp dụng m = 10 và τ = 0.1. Điểm trung thực tốt nhất được in đậm, và độ chính xác cao thứ hai được gạch chân.

Kết quả của LLaMA2-Chat-13B trên tập đánh giá TriviaQA được hiển thị trong Bảng 3. Cần nhấn mạnh rằng, nếu mô hình miễn cưỡng nói "Tôi không biết", nó sẽ có được điểm quá thận trọng tốt nhất (0) và điểm thận trọng tệ nhất (0), dẫn đến điểm trung thực không thỏa mãn (50.00%). Chúng tôi có những quan sát sau.

Các phương pháp tinh chỉnh hướng tới sự trung thực đạt được hiệu suất mạnh. Nhìn chung, các phương pháp tinh chỉnh có giám sát mà chúng tôi đề xuất liên tục nâng cao điểm trung thực so với các phương pháp thay thế, đồng thời bảo tồn một mức độ chính xác cao. Điều này cho thấy rằng các mô hình đã căn chỉnh không chỉ vẫn hoạt động được mà còn tăng cường đáng kể độ tin cậy của chúng, cho thấy triển vọng trong việc căn chỉnh vì sự trung thực. Chi tiết, những phương pháp này làm tăng đáng kể điểm thận trọng, gợi ý một xu hướng lớn hơn để kiềm chế không phản hồi các câu hỏi không biết thay vì bịa đặt câu trả lời không chính xác. Ngoài ra, như được chứng minh bởi điểm quá thận trọng tương đương hoặc thấp hơn, chúng thể hiện ít sự kiềm chế sai lầm hơn so với phương pháp DỰA TRÊN PROMPT, ngụ ý rằng các phương pháp tinh chỉnh hướng tới sự trung thực cũng có thể hiệu quả thúc đẩy sự trung thực trong phản hồi của mô hình đối với các câu hỏi đã biết.

Việc kết hợp rõ ràng độ chính xác dự kiến như một tín hiệu huấn luyện cải thiện hiệu suất trung thực. Trong khi việc áp dụng chiến lược TUYỆT ĐỐI nói với mô hình rằng nó có thể trả lời bằng phản hồi idk trong một số trường hợp, nó không xem xét độ tin cậy của mô hình. Một cách trực quan, có sự khác biệt đáng kể giữa các câu hỏi mà mô hình tin tưởng 90% có thể trả lời đúng và những câu hỏi mà nó chỉ tin tưởng 20%. Ngược lại, TIN CẬY và ĐA MẪU rõ ràng sử dụng độ chính xác dự kiến như các tín hiệu huấn luyện. Cụ thể, TIN CẬY cung cấp các biểu hiện độ tin cậy có tiền tố cho "câu hỏi đã biết", phục vụ như các tín hiệu giám sát tinh tế hơn cho phép mô hình nắm bắt chính xác hơn ranh giới kiến thức của nó. Ngoài ra, ĐA MẪU cho phép mô hình học ngầm từ tỷ lệ của các câu trả lời đúng và phản hồi idk trong các phản hồi được lấy mẫu m trong dữ liệu huấn luyện mở rộng, do đó nhận biết tốt hơn ranh giới kiến thức của nó một cách chi tiết. Từ kết quả, chúng ta có thể thấy rằng mặc dù trở nên hơi quá thận trọng, chúng có được điểm trung thực được cải thiện đáng kể.

ĐA MẪU đạt được điểm trung thực cao nhất và TIN CẬY-ĐỘNG TỪ đạt được độ chính xác tốt nhất. Rõ ràng, ĐA MẪU vượt qua các phương pháp khác trong cả điểm thận trọng và trung thực, mặc dù phải trả giá bằng việc tránh trả lời một phần nhỏ các câu hỏi đã biết. Mô hình đã căn chỉnh này, mà không quá thận trọng, có thể được người dùng tin tưởng nhất. Hơn nữa, TIN CẬY-ĐỘNG TỪ đạt được độ chính xác cao nhất, chỉ đứng sau BASELINE CHƯA CĂN CHỈNH. Độ chính xác cao có thể xuất phát từ nhiều yếu tố đan xen, chẳng hạn như tải tính toán bổ sung trong quá trình suy luận, hoặc lợi ích của việc kết hợp tiền tố độ tin cậy rõ ràng giúp giảm thiểu ảo giác khi tinh chỉnh trên kiến thức được biết yếu (Gekhman et al., 2024). Việc hoàn toàn tháo gỡ các yếu tố để cải thiện có thể đòi hỏi nỗ lực rộng rãi hơn và đáng để thảo luận trong công việc tương lai.

4.4.2 Khả năng Mở rộng và Thích ứng

Các phương pháp của chúng tôi thể hiện khả năng mở rộng về mặt kích thước mô hình, và chúng tôi đã bao gồm kết quả bổ sung cho cả mô hình nhỏ hơn và lớn hơn trong §D.5.2. Ngoài ra, chúng không bị ràng buộc với bất kỳ mô hình ngôn ngữ cụ thể nào và các thí nghiệm trong §D.5.3 thể hiện khả năng thích ứng với nhiều LLM mã nguồn mở phổ biến bao gồm InternLM (InternLM, 2023), Qwen (Bai et al., 2023), và Baichuan2 (Baichuan, 2023).

4.5 Exp II: Đánh giá Ngoài phân phối

[Bảng 4 hiển thị hiệu suất ngoài phân phối trên ba tập dữ liệu QA tự do]

Để đánh giá hiệu suất ngoài phân phối của tất cả các mô hình, chúng tôi tận dụng một tập dữ liệu hiện có Non-AmbigQA (tập con của NQ-Open (Kwiatkowski et al., 2019) nơi các câu hỏi rõ ràng và câu trả lời không mơ hồ (Min et al., 2020)), và cũng xây dựng hai tập dữ liệu đặc biệt PUQA và PKQA. Cụ thể, PUQA (PriorUnknown QA) chứa 1.000 câu hỏi về tài liệu khoa học được xuất bản vào năm 2023, được thiết kế cẩn thận để đảm bảo rằng mô hình không có kiến thức về chúng và vốn dĩ thách thức. PKQA (PriorKnown QA) bao gồm 1.000 câu hỏi mà mô hình có khả năng lớn quen thuộc. Vui lòng tham khảo §C để biết thêm chi tiết.

Chúng tôi trình bày kết quả trên ba tập dữ liệu trong Bảng 4, và có những phát hiện sau:

Các phương pháp tinh chỉnh hướng tới sự trung thực có thể chuyển giao được. Lấy TIN CẬY-ĐỘNG TỪ làm ví dụ. Nó liên tục vượt trội hơn các baseline trên tất cả ba tập dữ liệu, bằng cách nâng cao đáng kể khả năng từ chối trả lời đồng thời giảm thiểu tối đa sự mất mát hiệu suất ban đầu. Sự khác biệt trong phân phối dữ liệu giữa ba tập dữ liệu này và tập dữ liệu huấn luyện TriviaQA, phục vụ như bằng chứng rằng các phương pháp tinh chỉnh hướng tới sự trung thực, với chi phí thấp, thực sự thích ứng để phản ứng khác nhau với các câu hỏi đã biết/chưa biết, thay vì chọn đường tắt dựa trên TriviaQA.

Tinh chỉnh không hướng tới sự trung thực dạy LLM ảo giác. Trong kết quả thí nghiệm trên PKQA, mặc dù các câu hỏi được tạo bởi chính mô hình, chúng tôi quan sát thấy một tác động nhẹ đến phản hồi của mô hình khi một hướng dẫn bổ sung được giới thiệu. Hơn nữa, chúng tôi xác định một hiện tượng đặc biệt: BASELINE TINH CHỈNH tiếp tục giảm độ chính xác 10 điểm, hoạt động đáng kể tệ hơn các phương pháp khác. Chúng tôi cho rằng điều này có thể được quy cho một quan điểm được đề xuất trong (Schulman, 2023; Zhang et al., 2023) rằng quá trình tinh chỉnh có giám sát có thể vô tình giới thiệu ảo giác bằng cách buộc LLM trả lời các câu hỏi vượt quá ranh giới kiến thức của chúng. Lưu ý rằng dữ liệu huấn luyện cho BASELINE TINH CHỈNH bao gồm khoảng 25% câu hỏi với câu trả lời mà mô hình khó có thể được kỳ vọng biết.

4.6 Exp III: Thuế Căn chỉnh

Khi mô hình được tinh chỉnh để kiềm chế trả lời câu hỏi, câu hỏi liệu nó có trở nên ít hữu ích hơn hay không nảy sinh. Để điều tra vấn đề này, chúng tôi sử dụng tập dữ liệu hữu ích từ Li et al. (2023a) để đánh giá tính hữu ích của mô hình trước và sau căn chỉnh. Tập dữ liệu này, được ký hiệu là Eval-P− (xem §C.5), bao gồm một loạt các yêu cầu liên quan đến tính hữu ích bao gồm tóm tắt, viết sáng tạo, giao tiếp chung, và nhiều hơn nữa, khác biệt với nhu cầu của các nhiệm vụ QA dựa trên kiến thức. Để đánh giá phản hồi của mô hình, chúng tôi nhờ sự hỗ trợ của cả AUTO-J (Li et al., 2023a) và GPT-4 (tức là, gpt-4-0613; OpenAI (2023a)), cung cấp đánh giá trên thang điểm từ 1 đến 10.

--- TRANG 8 ---
Tính hữu ích
AUTO-J GPT-4
CHƯA CĂN CHỈNH         5.56   8.62
TIN CẬY-ĐỘNG TỪ        5.54   8.61
ĐA MẪU                 5.52   8.56

Bảng 5: Kết quả trên dữ liệu tính hữu ích từ Eval-P−.

Các điểm tính hữu ích được đánh giá bởi cả hai giám khảo được trình bày trong Bảng 5. Từ kết quả, chúng ta có thể thấy rằng cả TIN CẬY-ĐỘNG TỪ và ĐA MẪU đạt được hiệu suất tương tự với BASELINE CHƯA CĂN CHỈNH khi đánh giá tính hữu ích. Quan sát này gợi ý rằng chi phí của việc căn chỉnh LLM vì sự trung thực không gây ra tác động đáng kể đến tính hữu ích tổng thể của chúng, do đó làm nổi bật tính thực tiễn của quá trình căn chỉnh.

5 Hạn chế và Công việc Tương lai

5.1 Cạm bẫy trong Việc Định nghĩa Sự trung thực

Trong khi chúng tôi định nghĩa sự trung thực phù hợp với các quan điểm được thiết lập từ lâu (Askell et al., 2021; Cui et al., 2023), chúng tôi đưa ra những giả định đơn giản hóa sau để xấp xỉ một cách hợp lý suy nghĩ nội bộ của mô hình thông qua hành vi bên ngoài của nó.

Sự trung thực so với Tính chân thật. Theo Evans et al. (2021); Park et al. (2023), sự trung thực đòi hỏi một mô hình nói ra những gì nó tin, trong khi một khái niệm liền kề, tính chân thật, đòi hỏi nó nói ra những gì khách quan đúng. Trong bài báo này, chúng tôi tập trung vào "sự trung thực" để khám phá ranh giới kiến thức của mô hình, thay vì mù quáng thúc đẩy nó cung cấp thông tin chính xác mà không xem xét những gì nó đã học. Tuy nhiên, việc khám phá suy luận nội bộ của mô hình có thể phức tạp. Chúng tôi giả thuyết rằng đối với các câu hỏi kiến thức chung (ví dụ, TriviaQA (Joshi et al., 2017) thay vì TruthfulQA (Lin et al., 2022b)), nếu một LLM thường được sử dụng đưa ra một phản hồi không chính xác, có khả năng cao hơn là mô hình đang bịa đặt thay vì đã học một niềm tin sai lầm.

Không Nói dối. Trong khi các hành vi không trung thực điển hình ở con người bao gồm nói dối, các LLM hiện tại, khi không được prompt, tinh chỉnh hoặc đặt trong bối cảnh đặc biệt một cách cụ thể (Pacchiardi et al., 2023; Park et al., 2023; Scheurer et al., 2023), nói chung không cung cấp thông tin không chính xác nếu chúng "biết" câu trả lời đúng. Do đó, chúng tôi loại trừ khả năng này khỏi sự xem xét của chúng tôi trong nghiên cứu này.

Ngoài ra, việc xem xét các tình huống phức tạp hơn là điều chúng tôi hy vọng có thể truyền cảm hứng cho nghiên cứu sâu hơn, chẳng hạn như khai thác kiến thức tiềm ẩn và tách biệt sự không trung thực khỏi quên thảm khốc, như đã đề cập trong §2.3.

5.2 Công việc Tương lai

Các phương pháp tiên tiến hơn để định nghĩa k(·). Phương pháp hiện tại của chúng tôi xấp xỉ ranh giới kiến thức dựa trên hành vi bên ngoài của mô hình trong việc trả lời câu hỏi đúng hoặc sai, nhưng phương pháp này còn xa mới hoàn hảo. Công việc tương lai nên khám phá các phương pháp tinh tế hơn để xác định liệu mô hình có "biết" câu trả lời hay không.

Khám phá sâu hơn về biểu hiện bất định. Các phương pháp TIN CẬY làm cho mô hình thể hiện các mức độ tin cậy khác nhau. Tuy nhiên, việc hiệu chuẩn độ tin cậy đầu ra của mô hình nằm ngoài phạm vi công việc của chúng tôi; chúng tôi chỉ tập trung vào việc liệu phản hồi có chứa dấu hiệu idk hay câu trả lời đúng. Định nghĩa và tính khả thi của các biểu hiện độ tin cậy được hiệu chuẩn cho việc tạo tự do vẫn chưa được khám phá.

Căn chỉnh ở mức biểu diễn vì sự trung thực. Một dòng nghiên cứu (Li et al., 2023b; Zou et al., 2023) chứng minh hiệu quả của kỹ thuật biểu diễn. Trong khi chúng tôi giải quyết các phạm vi kiến thức khác nhau - những công trình đó tập trung vào việc khai thác câu trả lời chân thật cho các câu hỏi đã biết, trong khi chúng tôi nhằm điều chỉnh hành vi của mô hình cho cả câu hỏi đã biết và chưa biết - chúng tôi hy vọng công việc tương lai sẽ khám phá các phương pháp ở mức biểu diễn của LLM để đạt được căn chỉnh ít xâm lấn nhất vì sự trung thực.

--- TRANG 9 ---
6 Kết luận

Trong công trình này, chúng tôi thiết lập khung Căn chỉnh vì Sự trung thực, đòi hỏi LLM chủ động từ chối trả lời câu hỏi khi thích hợp, mà không cần sử dụng tài nguyên bên ngoài. Để đạt được điều này, chúng tôi giới thiệu khái niệm "phản hồi idk" và các thước đo mới để đo lường chất lượng và độ tin cậy của phản hồi khi một mô hình được phép thể hiện "Tôi không biết". Hơn nữa, chúng tôi đề xuất một số phương pháp tinh chỉnh hướng tới sự trung thực và xác thực tính khả thi của căn chỉnh vì sự trung thực thông qua các thí nghiệm rộng rãi. Chúng tôi hy vọng công trình này có thể truyền cảm hứng cho nhiều suy nghĩ hơn về việc phát triển các mô hình AI trung thực trong cộng đồng NLP.

Lời cảm ơn và Tiết lộ Tài trợ

Công trình này được tài trợ một phần bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (62476168), Dự án Nghiên cứu Qingyuan.

Tài liệu tham khảo

Allen-Zhu, Z. and Li, Y. (2023). Physics of language models: Part 3.2, knowledge manipulation. CoRR, abs/2309.14402.

Amayuelas, A., Pan, L., Chen, W., and Wang, W. Y. (2023). Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models. CoRR, abs/2305.13712.

Anil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., Chu, E., Clark, J. H., Shafey, L. E., Huang, Y., Meier-Hellstern, K., Mishra, G., Moreira, E., Omernick, M., Robinson, K., Ruder, S., Tay, Y., Xiao, K., Xu, Y., Zhang, Y., Ábrego, G. H., Ahn, J., Austin, J., Barham, P., Botha, J. A., Bradbury, J., Brahma, S., Brooks, K., Catasta, M., Cheng, Y., Cherry, C., Choquette-Choo, C. A., Chowdhery, A., Crepy, C., Dave, S., Dehghani, M., Dev, S., Devlin, J., Díaz, M., Du, N., Dyer, E., Feinberg, V., Feng, F., Fienber, V., Freitag, M., Garcia, X., Gehrmann, S., Gonzalez, L., and et al. (2023). Palm 2 technical report. CoRR, abs/2305.10403.

Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Kernion, J., Ndousse, K., Olsson, C., Amodei, D., Brown, T. B., Clark, J., McCandlish, S., Olah, C., and Kaplan, J. (2021). A general language assistant as a laboratory for alignment. CoRR, abs/2112.00861.

[Phần còn lại của tài liệu tham khảo tiếp tục theo cùng định dạng...]
