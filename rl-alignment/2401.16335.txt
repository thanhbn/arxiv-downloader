# 2401.16335.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2401.16335.pdf
# File size: 1281400 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Iterative Data Smoothing: Mitigating Reward Overfitting and
Overoptimization in RLHF
Banghua Zhu1, Michael I. Jordan1, and Jiantao Jiao1
1University of California, Berkeley
Abstract
Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language
models closely with human-centric values. The initial phase of RLHF involves learning human values
using a reward model from ranking data. It is observed that the performance of the reward model
degrades after one epoch of training, and optimizing too much against the learned reward model
eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical
insights to design improved reward learning algorithm termed ’Iterative Data Smoothing’ (IDS).
The core idea is that during each training epoch, we not only update the model with the data, but
also update the date using the model, replacing hard labels with soft labels. Our empirical findings
highlight the superior performance of this approach over the traditional methods.
1 Introduction
Recent progress on Large Language Models (LLMs) is having a transformative effect not only in natural
language processing but also more broadly in a range of AI applications (Radford et al., 2019; Chowdhery
et al., 2022; Brown et al., 2020; Touvron et al., 2023; Bubeck et al., 2023; Schulman et al., 2022; OpenAI,
2023; Anthropic, 2023). A key ingredient in the roll-out of LLMs is the fine-tuning step, in which the
models are brought into closer alignment with specific behavioral and normative goals. When no adequately
fine-tuned, LLMs may exhibit undesirable and unpredictable behavior, including the fabrication of facts or
the generation of biased and toxic content (Perez et al., 2022; Ganguli et al., 2022). The current approach
towards mitigating such problems is to make use of reinforcement learning based on human assessments.
In particular, Reinforcement Learning with Human Feedback (RLHF) proposes to use human assessments
as a reward function from pairwise or multi-wise comparisons of model responses, and then fine-tune the
language model based on the learned reward functions (Ziegler et al., 2019; Ouyang et al., 2022; Schulman
et al., 2022).
Following on from a supervised learning stage, a typical RLHF protocol involves two main steps:
•Reward learning: Sample prompts from a prompt dataset and generate multiple responses for the
same prompt. Collect human preference data in the form of pairwise or multi-wise comparisons of
different responses. Train a reward model based on the preference data.
•Policy learning: Fine-tune the current LLM based on the learned reward model with reinforcement
learning algorithms.
Although RLHF has been successful in practice (Bai et al., 2022; Ouyang et al., 2022; Dubois et al.,
2023), it is not without flaws, and indeed the current reward training paradigm grapples with significant
value-reward mismatches. There are two major issues with the current paradigm:
•Reward overfitting: During the training of the reward model, it has been observed that the test
cross-entropy loss of the reward model can deteriorate after one epoch of training Ouyang et al.
(2022).
1arXiv:2401.16335v1  [cs.LG]  29 Jan 2024

--- PAGE 2 ---
•Reward overoptimization: When training the policy model to maximize the reward predicted
by the learned model, it has been observed that the ground-truth reward can increase when the
policy is close in KL divergence to the initial policy, but decrease with continued training (Gao
et al., 2023).
In this paper, we investigate these issues in depth. We simplify the formulation of RLHF to a
multi-armed bandit problem and reproduce the overfitting and overoptimization phenomena. We leverage
theoretical insights in the bandit setting to design new algorithms that work well under practical fine-tuning
scenarios.
1.1 Main Results
As our first contribution, we pinpoint the root cause of both reward overfitting and overoptimization. We
show that it is the inadequacy of the cross-entropy loss for long-tailed preference datasets .
As illustrated in Figure 1, even a simple 3-armed bandit problem can succumb to overfitting and
overoptimization when faced with such imbalanced datasets. Consider a scenario where we have three
arms with true rewards given by r⋆
1= 1, r⋆
2=r⋆
3= 0, and the preference distribution is generated by the
Bradley-Terry-Luce (BTL) model (Bradley and Terry, 1952), i.e. P(i≻j) =exp(r⋆
i)/(exp(r⋆
i) +exp(r⋆
j)).
Suppose our preference dataset compares the first and second arms 1000 times but only compares the first
and third arm once, and let n(i≻j) denote the number of times that arm iis preferred over arm j. The
standar empirical cross-entropy loss used in the literature for learning the reward model (Ouyang et al.,
2022; Zhu et al., 2023a) can be written as follows:
−X
i,jn(i≻j) logexp(ri)
exp(ri) + exp( rj)
.
We know that the empirical values n(1≻2) and n(2≻1) concentrate around their means. However, we
have with probability 0 .73,n(1≻3) = 1 and n(3≻1) = 0, and with probability 0 .27,n(1≻3) = 0 and
n(3≻1) = 1. In either case, the minimizer of the empirical entropy loss will satisfy either ˆr1−ˆr3=−∞
orˆr1−ˆr3= +∞. This introduces a huge effective noise when the coverage is imbalanced. Moreover, the
limiting preference distribution is very different from the ground truth distribution, leading to reward
overfitting. Furthermore, since there is 0 .27 probability that ˆr1−ˆr3=−∞, we will take arm 3 as the
optimal arm instead of arm 1. This causes reward overoptimization during the stage of policy learning
since the final policy converges to the wrong arm with reward zero.
Figure 1. Illustration of the problem of the vanilla empirical cross-entropy minimization for learning the
ground truth reward. With a small number of samples comparing arm 1 and 3, the minimization converges
to a solution which assigns ˆr1−ˆr3=−∞ with constant probability. With the proposed Iterative Data
Smoothing (IDS) algorithm, the estimator is able to recover the ground truth reward.
To mitigate these effects, we leverage the pessimism mechanism from bandit learning to analyze and
design a new algorithm, Iterative Data Smoothing (IDS), that simultaneously addresses both reward
overfitting and reward overoptimization. The algorithm design is straightforward: in each epoch, beyond
updating the model with the data, we also adjust the data using the model. Theoretically, we investigate
the two phenomena in the tabular bandit case. We show that the proposed method, as an alternative
to the lower-confidence-bound-based algorithm (Jin et al., 2021; Xie et al., 2021; Rashidinejad et al.,
2

--- PAGE 3 ---
2021; Zhu et al., 2023a), learns the ground truth distribution for pairs that are compared enough times,
and ignores infrequently covered comparisons thereby mitigating issues introduced by long-tailed data.
Empirically, we present experimental evidence that the proposed method improves reward training in
both bandit and neural network settings.
1.2 Related Work
RLHF and Preference-based Reinforcement Learning. RLHF, or Preference-based Reinforcement
Learning (PbRL), has delivered significant empirical success in the fields of game playing, robot training,
stock prediction, recommender systems, clinical trials and natural language processing (Novoseller et al.,
2019; Sadigh et al., 2017; Christiano et al., 2017b; Kupcsik et al., 2018; Jain et al., 2013; Wirth et al.,
2017; Knox and Stone, 2008; MacGlashan et al., 2017; Christiano et al., 2017a; Warnell et al., 2018; Brown
et al., 2019; Shin et al., 2023; Ziegler et al., 2019; Stiennon et al., 2020; Wu et al., 2021; Nakano et al.,
2021; Ouyang et al., 2022; Menick et al., 2022; Glaese et al., 2022; Gao et al., 2022; Bai et al., 2022;
Ganguli et al., 2022; Ramamurthy et al., 2022). In the setting of the language models, there has been
work exploring the efficient fine-tuning of the policy model (Snell et al., 2022; Song et al., 2023a; Yuan
et al., 2023; Zhu et al., 2023b; Rafailov et al., 2023; Wu et al., 2023).
In the case of reward learning, Ouyang et al. (2022) notes that in general the reward can only be
trained for one epoch in the RLHF pipeline, after which the test loss can go up. Gao et al. (2023) studies
the scaling law of training the reward model, and notes that overoptimization is another problem in reward
learning. To address the problem, Zhu et al. (2023a) propose a pessimism-based method that improves
the policy trained from the reward model when the optimal reward lies in a linear family. It is observed
in Song et al. (2023b) that the reward model tends to be identical regardless of whether the prompts are
open-ended or closed-ended during the terminal phase of training, and they propose a prompt-dependent
reward optimization scheme.
Another closely related topic is the problem of estimation and ranking from pairwise or K-wise
comparisons. In the literature of dueling bandit , one compares two actions and aims to minimize regret
based on pairwise comparisons (Yue et al., 2012; Zoghi et al., 2014b; Yue and Joachims, 2009, 2011; Saha
and Krishnamurthy, 2022; Ghoshal and Saha, 2022; Saha and Gopalan, 2018a; Ailon et al., 2014; Zoghi
et al., 2014a; Komiyama et al., 2015; Gajane et al., 2015; Saha and Gopalan, 2018b, 2019; Faury et al.,
2020). Novoseller et al. (2019); Xu et al. (2020) analyze the sample complexity of dueling RL agents in the
tabular case, which is extended to linear case and function approximation by the recent work of Pacchiano
et al. (2021); Chen et al. (2022). Chatterji et al. (2022) studies a related setting where in each episode
only binary feedback is received. Most of the theoretical work of learning from ranking focuses on regret
minimization, while RLHF focuses more on the quality of the final policy.
Knowledge Distillation The literature of knowledge distillation focuses on transferring the knowledge
from a teacher model to a student model (Hinton et al., 2015; Furlanello et al., 2018; Cho and Hariharan,
2019; Zhao et al., 2022; Romero et al., 2014; Yim et al., 2017; Huang and Wang, 2017; Park et al., 2019;
Tian et al., 2019; Tung and Mori, 2019; Qiu et al., 2022; Cheng et al., 2020). It is observed in this literature
that the soft labels produced by the teacher network can help train a better student network, even when
the teacher and student network are of the same size and structure Hinton et al. (2015). Furlanello et al.
(2018) present a method which iteratively trains a new student network after the teacher network achieves
the smallest evaluation loss. Both our iterative data smoothing algorithm and these knowledge distillation
methods learn from soft labels. However, iterative data smoothing iteratively updates the same model
and data, while knowledge distillation method usually focuses on transferring knowledge from one model
to the other.
2 Formulation
We begin with the notation that we use in the paper. Then we introduce the general formulation of RLHF,
along with our simplification in the multi-armed bandit case.
3

--- PAGE 4 ---
Notations. We use calligraphic letters for sets, e.g., SandA. Given a set S, we write |S|to represent
the cardinality of S. We use [ K] to denote the set of integers from 1 to K. We use µ(a, a′) to denote
the probability of comparing aanda′in a preference dataset, and µ(a) =P
a′∈Aµ(a, a′) to denote the
probability of comparing awith any other arms. Similarly, we use n(a), n(a, a′) to denote the number
of samples that compare awith any other arms, and the number of samples that compare awith a′,
respectively. We use a1≻a2to denote the event that the a1is more preferred compared to a2.
2.1 General Formulation of RLHF
The key components in RLHF consist of two steps: reward learning and policy learning. We briefly
introduce the general formulation of RLHF below.
In the stage of reward learning, one collects a preference dataset based on a prompt dataset and
responses to the prompts. According to the formulation of Zhu et al. (2023a), for the i-th sample, a state
(prompt) siis first sampled from some prompt distribution ρ. Given the state si,Mactions (responses)
(a(1)
i, a(2)
i,···, a(M)
i) are sampled from some joint distribution P(a(1),···, a(M)|si), Let σi: [M]7→[M]
be the output of the human labeller, which is a permutation function that denotes the ranking of the
actions. Here σi(1) represents the most preferred action, and σi(M) is the least preferred action. A
common model for the distribution of σunder multi-ary comparisons is a Plackett-Luce model (Plackett,
1975; Luce, 2012). The Plackett-Luce model defines the probability of a state-action pair ( s, ai) being the
largest among a given set {(s, ai)}M
i=1as
P(ai≻aj,∀j̸=i|s) =exp(r⋆(s, ai))PM
j=1exp(r⋆(s, aj)),
where r⋆:S × A 7→ Ris the ground-truth reward for the response given the prompt. Moreover, the
probability of observing the permutation σis defined as1
P(σ|s,{a(i)}M
i=1) =MY
i=1exp(r(s, a(σ(i))))PM
j=iexp(r(s, a(σ(j)))).
When M= 2, this reduces to the pairwise comparison considered in the Bradley-Terry-Luce (BTL)
model (Bradley and Terry, 1952), which is used in existing RLHF algorithms. In this case, the permutation
σcan be reduced to a Bernoulli random variable, representing whether one action is preferred compared to
the other. Concretely, for each queried state-actions pair ( s, a, a′), we observe a sample cfrom a Bernoulli
distribution with parameterexp(rθ⋆(s,a))
exp(rθ⋆(s,a))+exp( rθ⋆(s,a′)). Based on the observed dataset, the cross-entropy
loss is minimized to estimate the ground-truth reward for the case of pairwise comparison. The minimizer
of cross-entropy loss is the maximum likelihood estimator:
ˆrMLE∈arg min
rLCE(D, r),
LCE(D, r) =−nX
i=1logyi·exp(r(si, a(1)
i))
exp(r(si, a(1)
i)) + exp( r(si, a(2)
i))+(1−yi)·exp(r(si, a(2)
i))
exp(r(si, a(1)
i)) + exp( r(si, a(2)
i))
.
After learning the reward, we aim to learn the optimal policy under KL regularization with respect to
an initial policy π0under some state (prompt) distribution ρ′.
ˆπ= arg max
πEs∼ρ′,a∼π[ˆrMLE(s, a)]−λ·Es∼ρ′[KL(π(· |s)∥π0(· |s))].
2.2 RLHF in Multi-Armed Bandits
To understand the overfitting and overoptimization problems, we simplify the RLHF problem to consider a
single-state multi-armed bandit formulation with pairwise comparisons. Instead of fitting a reward model
and policy model with neural networks, we fit a tabular reward model in a K-armed bandit problem.
1In practice, one may introduce an extra temperature parameter σand replace all r⋆with r⋆/σ. Here we take σ= 1.
4

--- PAGE 5 ---
Consider a multi-armed bandit problem with Karms. Each arm has a deterministic ground-truth
reward r⋆(k)∈R, k∈[K]. In this case, the policy becomes a distribution supported on the Karms
π∈∆([K]). The sampling process for general RLHF reduces to the following: we first sample two actions
ai,a′
ifrom a joint distribution µ∈∆([K]×[K]), and then observe a binary comparison variable yi
following a distribution
P(yi= 1) =exp(r⋆(ai))
exp(r⋆(ai)) + exp( r⋆(a′
i)),P(yi= 0) = 1 −P(yi= 1).
Assume that we are given nsamples, which are sampled i.i.d. from the above process. Let n(a, a′) be
the total number of comparisons between actions aanda′in the nsamples. Let the resulting dataset be
D={ai, a′
i, yi}n
i=1. The tasks in RLHF for multi-armed bandit setting can be simplified as:
1.Reward learning: Estimate true reward r⋆with a proxy reward ˆrfrom the comparison dataset D.
2.Policy learning: Find a policy π∈∆([K]) that maximizes the proxy reward under KL constraints.
In the next two sections, we discuss separately the reward learning phase and policy learning phase, along
with the reasons behind overfitting and overoptimization.
2.3 Overfitting in Reward Learning
For reward learning, the commonly used maximum likelihood estimator is the estimator that minimizes
empirical cross-entropy loss:
ˆrMLE= arg min
rˆLCE(D, r),where (1)
ˆLCE(D,ˆr) =−1
nnX
i=1yilogexp(ˆr(ai))
exp(ˆr(ai)) + exp(ˆ r(a′
i))
+ (1−yi) logexp(ˆr(a′
i))
exp(ˆr(ai)) + exp(ˆ r(a′
i))
.
By definition, ˆrMLEis convergent point when we optimize the empirical cross entropy fully. Thus the
population cross-entropy loss of ˆrMLEis an indicator for whether overfitting exists during reward training.
We define the population cross entropy loss as
LCE(r) =−E(a,a′)∼µ,y∼Ber
exp(r⋆(a))
exp(r⋆(a))+exp( r⋆(a′))"
ylogexp(r(a))
exp(r(a)) + exp( r(a′))
+ (1−y) logexp(r(a′))
exp(r(a)) + exp(ˆ r(a′))#
=−E(a,a′)∼µ"
exp(r⋆(a))
exp(r⋆(a)) + exp( r⋆(a′))logexp(r(a))
exp(r(a)) + exp( r(a′))
+exp(r⋆(a′))
exp(r⋆(a)) + exp( r⋆(a′))logexp(r(a′))
exp(r(a)) + exp(ˆ r(a′))#
.
For a fixed pairwise comparison distribution µ, it is known that the maximum likelihood estimator
ˆrMLEconverges to the ground truth reward r⋆as the number of samples ngoes to infinity.
Theorem 2.1 (Consistency of MLE, see, e.g., Theorem 6.1.3. of Hogg et al. (2013)) .Fixr⋆(K) =ˆr(K) = 0
for the uniqueness of the solution. For any fixed µ, and any given ground-truth reward r⋆, we have that
ˆrMLEconverges in probability to r⋆; i.e., for any ϵ >0,
lim
n→+∞P(∥ˆrMLE−r⋆∥∞≥ϵ) = 0 .
Here we view ˆrMLEandr⋆asK-dimensional vectors.
5

--- PAGE 6 ---
The proof is deferred to Appendix D. This suggests that the overfitting phenomenon does not arise
when we have an infinite number of samples. However, in the non-asymptotic regime when the comparison
distribution µmay depend on n, one may not expect convergent result for MLE. We have the following
theorem.
Theorem 2.2 (Reward overfitting of MLE in the non-asymptotic regime) .Fixr⋆(a) = 1(a= 1) and
ˆr(K) = 0 for uniqueness of the solution. For any fixed n >500, there exists some 3-armed bandit problem
such that with probability at least 0.09,
LCE(ˆrMLE)− L CE(r⋆)≥C
for any arbitrarily large C.
The proof is deferred to Appendix E. Below we provide a intuitive explanation. The constructed hard
instance is a bandit where r⋆(a) = 1(a= 1). For any fixed n, we set µ(1,2) = 1 −1/n,µ(1,3) = 1 /n.
In this hard instance, there is constant probability that arm 3 is only compared with 1 once. And
with constant probability, the observed comparison result between arm 1 and arm 3 will be different from
the ground truth. The MLE will assign r(3) = + ∞since the maximizer of log(exp(x)/(1 + exp(x))) is
infinity when xis not bounded. Thus when optimizing the empirical cross entropy fully, the maximum
likelihood estimator will result in a large population cross-entropy loss. We also validate this phenomenon
in Section 4.1 with simulated experiments.
This lower bound instance simulates the high-dimensional regime where the number of samples is
comparable to the dimension, and the data coverage is unbalanced across dimensions. One can also extend
the lower bound to more than 3 arms, where the probability of the loss being arbitrarily large will be
increased to close to 1 instead of a small constant.
2.4 Overoptimization in Policy Learning
After obtaining the estimated reward function ˆr, we optimize the policy π∈∆([K]) to maximize the
estimated reward. In RLHF, one starts from an initial (reference) policy π0, and optimizes the new policy
πto maximize the estimated reward ˆrunder some constraint in KL divergence between πandπ0. It is
observed in Gao et al. (2022) that as we continue optimizing the policy to maximize the estimated reward,
the true reward of the policy will first increase then decrease, exhibiting the reward overoptimization
phenomenon.
Consider the following policy optimization problem for a given reward model ˆ r:
max
π∈∆([K])Ea∼π(·)[ˆr(a)]−1
λ·KL(π∥π0). (2)
Assuming that the policy gradient method converges to the optimal policy for the above policy optimization
problem, which has a closed-form solution:
πλ(a) =π0(a)·exp(λ·ˆr(a))P
a′∈Aπ0(a′)·exp(λ·ˆr(a′)). (3)
In the tabular case, we can derive a closed form solution for how the KL divergence and ground-truth
reward change with respect to λ, thus completely characterizing the reward-KL tradeoff. We compute the
KL divergence and ground-truth reward of the policy as
KL(πλ∥π0) =P
a∈Aπ0(a)·exp(λ·ˆr(a))·log(exp( λ·ˆr(a))/(P
a′∈Aπ0(a′)·exp(λ·ˆr(a′))))P
a′∈Aπ0(a′)·exp(λ·ˆr(a′))
=P
a∈Aπ0(a)·exp(λ·ˆr(a))·λ·ˆr(a)P
a′∈Aπ0(a′)·exp(λ·ˆr(a′))−log X
a′∈Aπ0(a′)·exp(λ·ˆr(a′))!
,
Ea∼πλ[r⋆(a)] =P
a∈Aπ0(a)·exp(λ·ˆr(a))·λ·r⋆(a)P
a′∈Aπ0(a′)·exp(λ·ˆr(a′)).
6

--- PAGE 7 ---
The above equation provides a precise characterization of how the mismatch between ˆrandr⋆leads to
the overoptimization phenomenon, which can be validated from the experiments in Section 4. To simplify
the analysis and provide better intuition, we focus on the case when λ→ ∞ , i.e., when the optimal policy
selects the best empirical arm without considering the KL constraint. In this case, the final policy reduces
to the empirical best arm, π∞(a) = 1(a= arg maxa′ˆr(a′)).
By definition, π∞is the convergent policy when we keep loosening the KL divergence constraint in
Equation (2). Thus the performance of π∞is a good indicator of whether overoptimization exists during
policy training. We thus define a notion fo sub-optimality to characterize the performance gap between
the convergent policy and the optimal policy:
SubOpt (ˆπ):= max
aE[r⋆(a)−r⋆(ˆπ)].
We know from Theorem 2.1 that, asymptotically, the MLE for reward ˆrMLEconverges to the ground
truth reward r⋆. As a direct result, when using the MLE as reward, the sub-optimality of the policy π∞
also converges to zero with an infinite number of samples.
However, as a corollary of Theorem 2.2 and a direct consequence of reward overfitting, π∞may have
large sub-optimality in the non-asymptotic regime when trained from ˆ rMLE.
Corollary 2.3 (Reward overoptimization of MLE in the non-asymptotic regime) .Fixr⋆(a) = 1(a= 1).
For any fixed n, there exists some 3-armed bandit problem such that with probability at least 0.09,
SubOpt (ˆπ∞)≥1.
The proof is deferred to Appendix F. This suggests that ˆrMLEalso leads to the reward overoptimization
phenomenon in the non-asymptotic regime. In Section 4, we conduct simulation in the exact same setting
to verify the theoretical results.
3 Methods: Pessimistic MLE and Iterative Data Smoothing
The problem of overfitting and overoptimization calls for a design of better and practical reward learning
algorithm that helps mitigate both issues. We first discuss the pessimistic MLE algorithm in Zhu et al.
(2023a), which is shown to converge to a policy with vanishing sub-optimality under good coverage
assumption.
3.1 Pessimistic MLE
In the tabular case, the pessimistic MLE corrects the original MLE by subtracting a confidence interval.
Precisely, we have
ˆrPE(a) = ˆrMLE(a)−λ·r
1
n, (4)
where nis the total number of samples and λ=∥(L+ϵI)−1/2
j∥2is the norm of the j-th column of the
matrix ( L+ϵI)−1/2, where Lis the matrix that satisfies La,a=n(a)/n,La,a′=−n(a, a′)/n,∀a̸=a′, and
ϵis a small constant. Intuitively, for those arms that are compared fewer times, we are more uncertain
about their ground-truth reward value. Pessimistic MLE penalizes these arms by directly subtracting the
length of lower confidence interval of their reward, making sure that the arms that are less often compared
will be less likely to be chosen. It is shown in Zhu et al. (2023a) that the sub-optimality of the policy
optimizing ˆ rPEconverges to zero under the following two conditions:
•The expected number of times that one compares optimal arm (or the expert arm to be compared
with in the definition of sub-optimality) is lower bounded by some positive constant µ(a⋆)≥C.
•The parameterized reward family lies in a bounded space |ˆr(a)| ≤B,∀a∈[K].
7

--- PAGE 8 ---
This indicates that pessimistic MLE can help mitigate the reward overoptimization phenomenon.
However, for real-world reward training paradigm, the neural network parameterized reward family may
not be bounded. Furthermore, estimating the exact confidence interval for a neural-network parameterized
model can be hard and costly. This prevents the practical use of pessimistic MLE, and calls for new
methods that can potentially go beyond these conditions and apply to neural networks.
3.2 Iterative Data Smoothing
We propose a new algorithm, Iterative Data Smoothing (IDS), that shares similar insights as pessimistic
MLE. Intuitively, pessimistic MLE helps mitigate the reward overoptimization issue by reducing the
estimated reward for less seen arms. In IDS, we achieve this by updating the label of the data we train on.
Algorithm 1 Iterative Data Smoothing ( D, θ0, α, β )
Input: The pairwise comparison dataset D={ai, a′
i, yi}n
i=1. A parameterized reward model family
{rθ:A 7→R|θ∈Θ}with initialization θ0∈Θ. Two step sizes α, β. An empirical loss function
Lθ({yi},D) =−1
nnX
i=1yi·logexp(rθ(ai))
exp(rθ(ai)) + exp( rθ(a′
i))
+ (1−yi)·logexp(rθ(a′
i))
exp(rθ(ai)) + exp( rθ(a′
i))
Initialize t= 0 and yi,0=yi,∀i∈[n].
while rθtdoes not converge do
θt+1←θt−α· ∇L θ({yi,t},D)
yi,t+1←(1−β)·yi,t+β·exp(rθt+1(ai))
exp(rθt+1(ai)) + exp( rθt+1(a′
i))
t←t+ 1
end while
Return: rθt
As is shown in Algorithm 1, we initialize yi,0as the labels for the samples yi. In the t-th epoch, we first
update the model using the current comparison dataset with labels {yi,t}n
i=1. After the model is updated,
we also update the data using the model by predicting the probability P(yi= 1) for each comparison
(ai, a′
i) using the current reward estimate ˆrθt. We update each label yi,tas a weighted combination of its
previous value and the new predicted probability.
Intuitively, yi,trepresents a proxy of the confidence level of labels predicted by interim model checkpoints.
The idea is that as the model progresses through multiple epochs of training, it will bring larger change to
rewards for frequently observed samples whose representation is covered well in the dataset. Meanwhile,
for seldom-seen samples, the model will make minimal adjustments to the reward.
3.2.1 Benefit of one-step gradient descent
Before we analyze the IDS algorithm, we first discuss why training for one to two epochs in the traditional
reward learning approach works well (Ouyang et al., 2022). We provide the following analysis of the
one-step gradient update for the reward model. The proof is deferred to Appendix G.
Theorem 3.1. Consider the same multi-armed bandit setting where the reward is initialized equally for
allKarms. Then after one-step gradient descent, one has
∀a, a′∈[K],ˆr(a)−ˆr(a′) =α·(n+(a)−n−(a)−(n+(a′)−n−(a′))),
where n+(a), n−(a)refers to the total number of times that ais preferred and not preferred, respectively.
8

--- PAGE 9 ---
Remark 3.2.The result shows that why early stopping in the traditional reward learning works well in
a simple setting. After one gradient step, the empirical best arm becomes the the arm whose absolute
winning time is the largest. This can be viewed as another criterion besides pessimism that balances
both the time of comparisons and the time of being chosen as the preferred arm. When the arm ais only
compared few times, the difference n+(a)−n−(a) will be bounded by the total number of comparisons,
which will be smaller than those that have been compared much more times. Thus the reward model will
penalize those arms seen less. After updating the label with the model prediction, the label of less seen
samples will be closer to zero, thus getting implicitly penalized.
3.2.2 Benefit of iterative data smoothing
Due to under-optimization, the estimator from a one-step gradient update might still be far from the
ground-truth reward. We provide an analysis here why IDS can be better. Consider any two arms a, a′
with n(a, a′) observations among ntotal observations. By computing the gradient, we can write the IDS
algorithm as
ˆrt+1(a)−ˆrt+1(a′) = ˆrt(a)−ˆrt(a′) +α·n(a, a′)
n·
(ˆµ(a≻a′)·yt+ ˆµ(a≺a′)·(1−yt))·exp(ˆrt(a′))
exp(ˆrt(a)) + exp(ˆ rt(a′))
−(ˆµ(a≺a′)·yt+ ˆµ(a≻a′)·(1−yt))·exp(ˆrt(a))
exp(ˆrt(a)) + exp(ˆ rt(a′))
yt+1= (1−β)·yt+β·exp(ˆrt+1(a))
exp(ˆrt+1(a)) + exp(ˆ rt+1(a′)),
where we define ˆµ(a≻a′) =n(a≻a′)/n(a, a′). One can see that the effective step size for updating ˆris
α·n(a, a′)/n, while the effective step size for updating yisβ. Assume that we choose α, β, l, m such that
α·l/n≪β≪α·m/n.
Consider the following two scales:
•When there are sufficient observations, n(a, a′)≥m, we know that β≪α·n(a, a′)/n. In this
case, the update step size of ytis much slower than ˆrt. One can approximately take yt≈0 or 1
as unchanged during the update. Furthermore, since n(a, a′)≥mis large enough, ˆµconcentrates
around the ground truth µ. In this case, one can see that the reward converges to the ground truth
reward ˆ rt→r⋆.
•When the number of observations is not large, i.e., n(a, a′)≤l, we know that α·l/n≪β. In this
case, the update of ˆris much slower than yt. When the ˆr0are initialized to be zero, ytwill first
converge to 1 /2, leading to ˆ rt(a)≈ˆrt(a′) when tis large.
To formalize the above argument, we consider the following differential equations:
˙d(t) =αn·
(µ·y(t) + (1 −µ)·(1−y(t)))·1
1 + exp( d(t))−((1−µ)·y(t) +µ·(1−y(t)))·exp(d(t))
1 + exp( d(t))
˙y(t) =β·exp(d(t))
1 + exp( d(t))−y(t)
. (5)
Here drepresents the difference of reward between two arms a, a′, and µrepresents the empirical frequency
ˆµ(a≻a′). Let the initialization be d(0) = 0 , y(0) = 1. We have the following theorem.
Theorem 3.3. The differential equations in Equation (5)have one unique stationary point d(t) = 0 , y(t) =
1
2. On the other hand, for any α, β, n, T withβT≤ϵ≪1≪αnT, one has
exp(d(T))
1 + exp( d(T))−µ≤max(2(1 −exp(−ϵ)),exp(−µ(1−µ)αnT))
y(T)≥exp(−ϵ).
9

--- PAGE 10 ---
The proof is deferred to Appendix H. Note that the above argument only proves convergence to the
empirical measure µ. One can combine standard concentration argument to prove the convergence to the
ground truth probability. The result shows that when choosing α, βcarefully, for the pair of arms with a
large number of comparisons, the difference of reward will be close to the ground truth during the process
of training. As a concrete example, by taking α=n−1/2, β=n−1T−2, ϵ=βT, we have
exp(d(T))
1 + exp( d(T))−µ≤max(2 n−1T−1,exp(−µ(1−µ)n1/2T)).
One can see that for those pairs of comparisons with a large sample size n, the estimated probability is
close to the ground truth probability. On the other hand, for those pairs that are compared less often, the
difference d(t) is updated less frequently and remains close to the initialized values. Thus the algorithm
implicitly penalizes the less frequently seen pairs, while still estimating the commonly seen pairs accurately.
In summary, the IDS algorithm enjoys several benefits:
•For a sufficient number of observations, the estimated reward approximately converges to the ground
truth reward; while for an insufficient number of observations, the estimated reward remains largely
unchanged at the initialization. Thus the reward model penalizes the less observed arms with higher
uncertainty.
•It is easy to combine with neural networks, allowing arbitrary parametrization of the reward model.
•It utilizes the soft labels starting from the second epoch, which can be more effective than hard
labels according to the literature on knowledge distillation (Hinton et al., 2015; Zhao and Zhu, 2023).
We also present an alternative formulation of IDS in Appendix B.
0 25 50 75 100 125 150
Epochs0.610.620.630.640.65Cross Entropy LossGround Truth Cross Entropy Loss with K=10
Data Refinement
Pessimistic MLE
MLE
0.0 0.5 1.0 1.5 2.0
KL0.00.20.40.60.81.0Ground Truth Reward
KL-Reward tradeoff for learned policies with K=10
0 25 50 75 100 125 150
Epochs0.650.660.670.680.690.700.710.720.73Cross Entropy LossGround Truth Cross Entropy Loss with K=20
Data Refinement
Pessimistic MLE
MLE
0.0 0.5 1.0 1.5 2.0 2.5 3.0
KL0.00.20.40.60.81.0Ground Truth Reward
KL-Reward tradeoff for learned policies with K=20
Figure 2: Comparisons of the three methods in the multi-armed bandit setting.
10

--- PAGE 11 ---
4 Experiments
In this section, we present the results of experiments with both multi-armed bandits and neural networks.
4.1 Multi-Armed Bandit
In the bandit setting, we focus on the hard example constructed in Theorem 2.2. We take total samples
n= 60 and the number of arms Kas 10 and 20. We compare the performance of the vanilla MLE,
pessimistic MLE and IDS in both the reward learning phase and the policy learning phase.
In the reward learning phase, we run stochastic gradient descent with learning rate 0 .01 on the reward
model for multiple epochs and monitor how the loss changes with respect to the number of training epochs.
For pessimistic MLE, we subtract the confidence level in the reward according to Equation (4). For IDS,
we take the two step sizes as α= 0.01, β= 0.001. As is shown in left part of Figure 2, both MLE and
pessimistic MLE suffer from reward overfitting, while the test cross-entropy loss for the IDS algorithm
continues to decrease until convergence. Since the training loss changes with the updated labels, we plot
the population cross-entropy loss which is averaged over all pairs of comparisons.
In the right part of the figure, we plot the KL-reward tradeoff when training a policy based on the
learned reward. We vary the choice of λin Equation (3) to derive the optimal policy under diverse levels
of KL constraint, where we take the reference policy π0as the uniform policy. One can see that IDS is
able to converge to the optimal reward when KL is large, while both MLE and pessimistic MLE suffer
from overoptimization.
We remark here that the reason pessimistic MLE suffers from both overfitting and overoptimization
might be due to the design of unbounded reward in the multi-armed bandit case. When the reward family
is bounded, pessimistic MLE is also guaranteed to mitigate the overoptimization issue. Furthermore, we
only run one random seed for this setting to keep the plot clean since the KL-reward trade-off heavily
depends on the observed samples.
4.2 Neural Network
We also conduct experiments with neural networks. We use the human-labeled Helpfulness and Harmlessnes
(HH) dataset from Bai et al. (2022).2We take Dahoas/pythia-125M-static-sft3as the policy model
with three different reward models of size 125M, 1B and 3B. When training reward model, we take a
supervised fine-tuned language model, remove the last layer and replace it with a linear layer. When
fine-tuning the language model, we use the proximal policy optimization (PPO) algorithm (Schulman
et al., 2017).
We take a fully-trained 6B reward model Dahoas/gptj-rm-static trained from the same dataset
based on EleutherAI/gpt-j-6b as the ground truth. We use the model to label the comparison samples
using the BTL model (Bradley and Terry, 1952). And we train the 125M, 1B and 3B reward model with
the new labeled comparison samples. The reward training results are shown in Figure 3. One can see
that the MLE begins to overfit after 1-2 epochs, while the loss of the IDS algorithm continues to decrease
stably until convergence.
For both MLE and IDS algortihms, we take the reward with the smallest evaluation loss and optimize
a policy against the selected reward model. We compare results for policy learning as shown in Figure 4.
One can see that MLE suffers from reward overoptimization with few thousand steps, while the ground
truth reward continues to grow when using our IDS algorithm. We select step sizes α= 10−5andβ= 0.7
for all experiments. We observe that larger model leads to more improvement after one epoch, potentially
due to more accurate estimation of the labels. We provide more details of the experiment along with the
experiments on a different dataset, TLDR, in Appendix C.
In the implementation, we find that it is helpful to restore the best checkpoint at the end of each
epoch. This is due to that an inappropriate label {yi}n
i=1at certain epoch may hurt the performance of
the model. To prevent overfitting to the test set, we choose a large validation and test dataset, and we
select the best checkpoint according to the smallest loss in the validation set, and plot the loss on the
2https://huggingface.co/datasets/Dahoas/static-hh
3https://huggingface.co/Dahoas/pythia-125M-static-sft
11

--- PAGE 12 ---
0 1 2 3 4
Epochs0.520.540.560.580.600.620.640.66Test AccuracyTest Accuracy with 3B model
Data Refinement
MLE
0 1 2 3 4
Epochs0.6000.6250.6500.6750.7000.7250.7500.7750.800Test Cross Entropy LossTest Cross Entropy Loss with 3B model
Data Refinement
MLE
0 1 2 3 4 5
Epochs0.520.540.560.580.600.620.640.66Test AccuracyTest Accuracy with 1B model
Data Refinement
MLE
0 1 2 3 4 5
Epochs0.6000.6250.6500.6750.7000.7250.7500.7750.800Test Cross Entropy LossTest Cross Entropy Loss with 1B model
Data Refinement
MLE
0 1 2 3 4 5
Epochs0.520.540.560.580.600.620.64Test AccuracyTest Accuracy with 125M model
Data Refinement
MLE
0 1 2 3 4 5
Epochs0.6000.6250.6500.6750.7000.7250.7500.7750.800Test Cross Entropy LossTest Cross Entropy Loss with 125M model
Data Refinement
MLE
Figure 3: Comparisons of MLE and IDS when the reward is parameterized by a neural network.
12

--- PAGE 13 ---
0 2 4 6 8 10
Steps / k3.0
2.5
2.0
1.5
1.0
0.5
0.00.5Ground Truth RewardGround Truth Reward for Trained Policy
Data Refinement
MLE
Figure 4: Comparison of MLE and IDS for policy learning
test set. During the whole training procedure including checkpoint restoration, we do not use any of the
sample in the test set.
5 Conclusions
We have presented analyses and methodology aimed at resolving the problems of overfitting and
overoptimization in reward training for RLHF. We show that our proposed algorithm, IDS, helps mitigate
these two issues in both the multi-armed bandit and neural network settings. Note that while we identify
the underlying source of reward overfitting and overoptimization as the variance of the human preference
data, it is also possible that bias also contributes to these phenomena. In future work, plan to pursue
further formal theoretical analysis of the IDS algorithm, and explore potential applications beyond reward
training in the generic domains of classification and prediction.
Acknowledgements
The authors would like to thank John Schulman for discussions and suggestions throughout the projects,
which initiated the idea of reproducing reward overfitting and overoptimization in the bandit setting,
and inspired the idea of combining soft labels during training for better training. The authors would
also like to thank Lester Mackey for helpful discussions. The work is supported by NSF Cloudbank
and the Mathematical Data Science program of the Office of Naval Research under grant number
N00014-21-1-2840.
13

--- PAGE 14 ---
References
N. Ailon, Z. S. Karnin, and T. Joachims. Reducing dueling bandits to cardinal bandits. In ICML ,
volume 32, pages 856–864, 2014.
Anthropic. Model card and evaluations for claude models, 2023. URL https://www-files.anthropic.
com/production/images/Model-Card-Claude-2.pdf . Accessed: Sep. 27,2023.
Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan,
et al. Training a helpful and harmless assistant with reinforcement learning from human feedback.
arXiv preprint arXiv:2204.05862 , 2022.
R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired
comparisons. Biometrika , 39(3/4):324–345, 1952.
D. Brown, W. Goo, P. Nagarajan, and S. Niekum. Extrapolating beyond suboptimal demonstrations via
inverse reinforcement learning from observations. In International Conference on Machine Learning ,
pages 783–792. PMLR, 2019.
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,
G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information
processing systems , 33:1877–1901, 2020.
S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li,
S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint
arXiv:2303.12712 , 2023.
N. S. Chatterji, A. Pacchiano, P. L. Bartlett, and M. I. Jordan. On the theory of reinforcement learning
with once-per-episode feedback, 2022.
X. Chen, H. Zhong, Z. Yang, Z. Wang, and L. Wang. Human-in-the-loop: Provably efficient preference-
based reinforcement learning with general function approximation. In International Conference on
Machine Learning , pages 3773–3793. PMLR, 2022.
X. Cheng, Z. Rao, Y. Chen, and Q. Zhang. Explaining knowledge distillation by quantifying the
knowledge. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,
pages 12925–12935, 2020.
J. H. Cho and B. Hariharan. On the efficacy of knowledge distillation. In Proceedings of the IEEE/CVF
international conference on computer vision , pages 4794–4802, 2019.
A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung,
C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint
arXiv:2204.02311 , 2022.
P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning
from human preferences. Advances in neural information processing systems , 30, 2017a.
P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning
from human preferences. In Advances in Neural Information Processing Systems , pages 4299–4307,
2017b.
Y. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. Liang, and T. B. Hashimoto.
Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint
arXiv:2305.14387 , 2023.
L. Faury, M. Abeille, C. Calauz` enes, and O. Fercoq. Improved optimistic algorithms for logistic bandits.
InInternational Conference on Machine Learning , pages 3052–3060. PMLR, 2020.
14

--- PAGE 15 ---
T. Furlanello, Z. Lipton, M. Tschannen, L. Itti, and A. Anandkumar. Born again neural networks. In
International Conference on Machine Learning , pages 1607–1616. PMLR, 2018.
P. Gajane, T. Urvoy, and F. Cl´ erot. A relative exponential weighing algorithm for adversarial utility-based
dueling bandits. In Proceedings of the 32nd International Conference on Machine Learning , pages
218–227, 2015.
D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann, E. Perez, N. Schiefer,
K. Ndousse, et al. Red teaming language models to reduce harms: Methods, scaling behaviors, and
lessons learned. arXiv preprint arXiv:2209.07858 , 2022.
L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. arXiv preprint
arXiv:2210.10760 , 2022.
L. Gao, J. Schulman, and J. Hilton. Scaling laws for reward model overoptimization. In International
Conference on Machine Learning , pages 10835–10866. PMLR, 2023.
S. Ghoshal and A. Saha. Exploiting correlation to achieve faster learning rates in low-rank preference
bandits. In International Conference on Artificial Intelligence and Statistics , pages 456–482. PMLR,
2022.
A. Glaese, N. McAleese, M. Tr e
·bacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger,
M. Chadwick, P. Thacker, et al. Improving alignment of dialogue agents via targeted human judgements.
arXiv preprint arXiv:2209.14375 , 2022.
G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint
arXiv:1503.02531 , 2015.
R. V. Hogg, J. W. McKean, A. T. Craig, et al. Introduction to mathematical statistics . Pearson Education
India, 2013.
Z. Huang and N. Wang. Like what you like: Knowledge distill via neuron selectivity transfer. arXiv
preprint arXiv:1707.01219 , 2017.
A. Jain, B. Wojcik, T. Joachims, and A. Saxena. Learning trajectory preferences for manipulators via
iterative improvement. In Advances in neural information processing systems , pages 575–583, 2013.
Y. Jin, Z. Yang, and Z. Wang. Is pessimism provably efficient for offline RL? In International Conference
on Machine Learning , pages 5084–5096. PMLR, 2021.
W. B. Knox and P. Stone. Tamer: Training an agent manually via evaluative reinforcement. In 7th IEEE
International Conference on Development and Learning , pages 292–297. IEEE, 2008.
J. Komiyama, J. Honda, H. Kashima, and H. Nakagawa. Regret lower bound and optimal algorithm in
dueling bandit problem. In COLT , pages 1141–1154, 2015.
A. Kupcsik, D. Hsu, and W. S. Lee. Learning dynamic robot-to-human object handover from human
feedback. In Robotics research , pages 161–176. Springer, 2018.
R. D. Luce. Individual choice behavior: A theoretical analysis . Courier Corporation, 2012.
J. MacGlashan, M. K. Ho, R. Loftin, B. Peng, G. Wang, D. L. Roberts, M. E. Taylor, and M. L. Littman.
Interactive learning from policy-dependent human feedback. In International Conference on Machine
Learning , pages 2285–2294. PMLR, 2017.
J. Menick, M. Trebacz, V. Mikulik, J. Aslanides, F. Song, M. Chadwick, M. Glaese, S. Young, L. Campbell-
Gillingham, G. Irving, et al. Teaching language models to support answers with verified quotes. arXiv
preprint arXiv:2203.11147 , 2022.
15

--- PAGE 16 ---
R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju,
W. Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv
preprint arXiv:2112.09332 , 2021.
E. R. Novoseller, Y. Sui, Y. Yue, and J. W. Burdick. Dueling posterior sampling for preference-based
reinforcement learning. arXiv preprint arXiv:1908.01289 , 2019.
OpenAI. Gpt-4 technical report, 2023.
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,
A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744, 2022.
A. Pacchiano, A. Saha, and J. Lee. Dueling rl: reinforcement learning with trajectory preferences. arXiv
preprint arXiv:2111.04850 , 2021.
W. Park, D. Kim, Y. Lu, and M. Cho. Relational knowledge distillation. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition , pages 3967–3976, 2019.
E. Perez, S. Huang, F. Song, T. Cai, R. Ring, J. Aslanides, A. Glaese, N. McAleese, and G. Irving. Red
teaming language models with language models. arXiv preprint arXiv:2202.03286 , 2022.
R. L. Plackett. The analysis of permutations. Journal of the Royal Statistical Society Series C: Applied
Statistics , 24(2):193–202, 1975.
Z. Qiu, X. Ma, K. Yang, C. Liu, J. Hou, S. Yi, and W. Ouyang. Better teacher better student: Dynamic
prior knowledge for knowledge distillation. arXiv preprint arXiv:2206.06067 , 2022.
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised
multitask learners. OpenAI blog , 1(8):9, 2019.
R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization:
Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290 , 2023.
R. Ramamurthy, P. Ammanabrolu, K. Brantley, J. Hessel, R. Sifa, C. Bauckhage, H. Hajishirzi, and
Y. Choi. Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and
building blocks for natural language policy optimization. arXiv preprint arXiv:2210.01241 , 2022.
P. Rashidinejad, B. Zhu, C. Ma, J. Jiao, and S. Russell. Bridging offline reinforcement learning and
imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems , 34:
11702–11716, 2021.
A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio. Fitnets: Hints for thin deep
nets. arXiv preprint arXiv:1412.6550 , 2014.
D. Sadigh, A. D. Dragan, S. Sastry, and S. A. Seshia. Active preference-based learning of reward functions.
InRobotics: Science and Systems , 2017.
A. Saha and A. Gopalan. Battle of bandits. In Uncertainty in Artificial Intelligence , 2018a.
A. Saha and A. Gopalan. Active ranking with subset-wise preferences. International Conference on
Artificial Intelligence and Statistics (AISTATS) , 2018b.
A. Saha and A. Gopalan. PAC Battling Bandits in the Plackett-Luce Model. In Algorithmic Learning
Theory , pages 700–737, 2019.
A. Saha and A. Krishnamurthy. Efficient and optimal algorithms for contextual dueling bandits under
realizability. In International Conference on Algorithmic Learning Theory , pages 968–994. PMLR, 2022.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347 , 2017.
16

--- PAGE 17 ---
J. Schulman, B. Zoph, C. Kim, J. Hilton, J. Menick, J. Weng, J. F. C. Uribe, L. Fedus, L. Metz, M. Pokorny,
et al. Chatgpt: Optimizing language models for dialogue. OpenAI blog , 2022.
N. Shah, S. Balakrishnan, J. Bradley, A. Parekh, K. Ramchandran, and M. Wainwright. Estimation from
pairwise comparisons: Sharp minimax bounds with topology dependence. In Artificial Intelligence and
Statistics , pages 856–865. PMLR, 2015.
D. Shin, A. D. Dragan, and D. S. Brown. Benchmarks and algorithms for offline preference-based reward
learning. arXiv preprint arXiv:2301.01392 , 2023.
C. Snell, I. Kostrikov, Y. Su, M. Yang, and S. Levine. Offline rl for natural language generation with
implicit language q learning. arXiv preprint arXiv:2206.11871 , 2022.
F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang. Preference ranking optimization for human
alignment. arXiv preprint arXiv:2306.17492 , 2023a.
Z. Song, T. Cai, J. D. Lee, and W. J. Su. Reward collapse in aligning large language models. arXiv
preprint arXiv:2305.17608 , 2023b.
N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano.
Learning to summarize with human feedback. Advances in Neural Information Processing Systems , 33:
3008–3021, 2020.
Y. Tian, D. Krishnan, and P. Isola. Contrastive representation distillation. arXiv preprint arXiv:1910.10699 ,
2019.
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi` ere, N. Goyal,
E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
arXiv:2302.13971 , 2023.
F. Tung and G. Mori. Similarity-preserving knowledge distillation. In Proceedings of the IEEE/CVF
international conference on computer vision , pages 1365–1374, 2019.
G. Warnell, N. Waytowich, V. Lawhern, and P. Stone. Deep tamer: Interactive agent shaping in high-
dimensional state spaces. In Proceedings of the AAAI Conference on Artificial Intelligence , volume 32,
2018.
C. Wirth, R. Akrour, G. Neumann, and J. F¨ urnkranz. A survey of preference-based reinforcement learning
methods. The Journal of Machine Learning Research , 18(1):4945–4990, 2017.
J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, and P. Christiano. Recursively
summarizing books with human feedback. arXiv preprint arXiv:2109.10862 , 2021.
T. Wu, B. Zhu, R. Zhang, Z. Wen, K. Ramchandran, and J. Jiao. Pairwise proximal policy optimization:
Harnessing relative feedback for llm alignment, 2023.
T. Xie, C.-A. Cheng, N. Jiang, P. Mineiro, and A. Agarwal. Bellman-consistent pessimism for offline
reinforcement learning. Advances in Neural Information Processing Systems , 34:6683–6694, 2021.
Y. Xu, R. Wang, L. Yang, A. Singh, and A. Dubrawski. Preference-based reinforcement learning
with finite-time guarantees. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin, editors, Advances in Neural Information Processing Systems , volume 33, pages 18784–
18794. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
d9d3837ee7981e8c064774da6cdd98bf-Paper.pdf .
J. Yim, D. Joo, J. Bae, and J. Kim. A gift from knowledge distillation: Fast optimization, network
minimization and transfer learning. In Proceedings of the IEEE conference on computer vision and
pattern recognition , pages 4133–4141, 2017.
17

--- PAGE 18 ---
Z. Yuan, H. Yuan, C. Tan, W. Wang, S. Huang, and F. Huang. Rrhf: Rank responses to align language
models with human feedback without tears. arXiv preprint arXiv:2304.05302 , 2023.
Y. Yue and T. Joachims. Interactively optimizing information retrieval systems as a dueling bandits
problem. In Proceedings of the 26th Annual International Conference on Machine Learning , pages
1201–1208. ACM, 2009.
Y. Yue and T. Joachims. Beat the mean bandit. In Proceedings of the 28th International Conference on
Machine Learning (ICML-11) , pages 241–248, 2011.
Y. Yue, J. Broder, R. Kleinberg, and T. Joachims. The k-armed dueling bandits problem. Journal of
Computer and System Sciences , 78(5):1538–1556, 2012.
B. Zhao, Q. Cui, R. Song, Y. Qiu, and J. Liang. Decoupled knowledge distillation. In Proceedings of the
IEEE/CVF Conference on computer vision and pattern recognition , pages 11953–11962, 2022.
Q. Zhao and B. Zhu. Towards the fundamental limits of knowledge transfer over finite domains. arXiv
preprint arXiv:2310.07838 , 2023.
B. Zhu, J. Jiao, and M. I. Jordan. Principled reinforcement learning with human feedback from pairwise
ork-wise comparisons. arXiv preprint arXiv:2301.11270 , 2023a.
B. Zhu, H. Sharma, F. V. Frujeri, S. Dong, C. Zhu, M. I. Jordan, and J. Jiao. Fine-tuning language
models with advantage-induced policy alignment. arXiv preprint arXiv:2306.02231 , 2023b.
D. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Christiano, and G. Irving.
Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 , 2019.
M. Zoghi, S. Whiteson, R. Munos, M. d. Rijke, et al. Relative upper confidence bound for the k-armed
dueling bandit problem. In JMLR Workshop and Conference Proceedings , number 32, pages 10–18.
JMLR, 2014a.
M. Zoghi, S. A. Whiteson, M. De Rijke, and R. Munos. Relative confidence sampling for efficient on-line
ranker evaluation. In Proceedings of the 7th ACM international conference on Web search and data
mining , pages 73–82. ACM, 2014b.
18

--- PAGE 19 ---
A Extension to Multi-wise Comparison
Here we discuss potential extensions from pairwise comparisons to multi-wise comparison. When there is
Mranked responses for each prompt, there are two losses that one can choose from, namely MLE 2and
MLE Mfrom Zhu et al. (2023a).
ˆθMLE 2∈arg min
rL2(D, r),
where L2(D, r) =−1
nnX
i=1MX
j=1MX
k=j+1log 
exp(r(si, a(σi(j))
i ))
exp(r(si, a(σi(j))
i )) + exp( r(si, a(σi(k))
i ))!
ˆθMLEM∈arg min
rLM(D, r),
where LM(D, r) =−1
nnX
i=1MX
j=1log 
exp(r(si, a(σi(j))
i ))
PM
k=jexp(r(si, a(σi(k))
i ))!
.
Here we discuss how to incorporate the iterative data smoothing algorithm for the two losses above.
The loss MLE 2splits the M-wise comparisons into pairwise comparisons, thus it is straightforward to
predict the new label yj,k
ifor each pair of the comparisons between j-th and k-th response. The loss used
for iterative data smoothing can be written as
LDR
2(D, r) =−1
nnX
i=1MX
j=1MX
k=j+1yσi(j),σi(k)
i log 
exp(r(si, a(σi(j))
i ))
exp(r(si, a(σi(j))
i )) + exp( r(si, a(σi(k))
i ))!
+ (1−yσi(j),σi(k)
i ) log 
exp(r(si, a(σi(k))
i ))
exp(r(si, a(σi(j))
i )) + exp( r(si, a(σi(k))
i ))!
.
yj,k
i,t+1= (1−β)·yj,k
i,t+β·exp(rθt+1(si, aj
i))
exp(rθt+1(si, aj
i)) + exp( rθt+1(si, ak
i)).
On the other hand, adapting the loss MLE Mfor iterative data smoothing requires more efforts since it
requires changing the ranking labels to soft labels. The design of MLE Mdecomposes the probability of
the observed ranking to the product of the probability that each response is the most preferred one among
the rest of the responses. One of the options is to directly change the labels for the current rankings by
the following update rules:
LM(D, r) =−1
nnX
i=1MX
j=1yσi(j)
ilog 
exp(r(si, a(σi(j))
i ))
PM
k=jexp(r(si, a(σi(k))
i ))!
.
And the update rule for the labels yiis
yσi(j)
i,t+1= (1−β)·yσi(j)
i,t+β·exp(rθt+1(si, a(σi(j))
i ))
PM
k=jexp(rθt+1(si, a(σi(k))
i )).
However, the above update method does not directly reduce to the the case of pairwise comparisons when
setting M= 2. In order to recover the pairwise loss, one needs to consider all possible rankings and get
the soft labels for all the rankings. The loss will become
L′
M(D, r) =−1
nnX
i=1X
σ∈Π(M)MX
j=1yj,σ
ilog 
exp(r(si, a(σ(j))
i ))
PM
k=jexp(r(si, a(σ(k))
i ))!
.
19

--- PAGE 20 ---
Here Π( M) is the set of all permutations of the Melements. And the label is initialized as yj,σ
i,0= 1 if
σ=σi, and 0 otherwise. And the update rule for the labels yiis
yj,σ
i,t+1= (1−β)·yj,σ
i,t+β·exp(rθt+1(si, a(σ(j))
i ))
PM
k=jexp(rθt+1(si, a(σ(k))
i )).
The loss is consistent with the pairwise cross entropy loss when M= 2. However, it requires enumerating
over all possible permutations, which are not very efficient when Mis large. It requires more study to
decide which loss is more appropriate for M-wise iterative data smoothing.
B An Alternative Formulation of Iterative Data Smoothing
Besides the formulation shown in Algorithm 1, we also propose an alternative formulation that directly
multiplies a confidence ciin front of the original loss for each sample, as is shown in Algorithm 2. We note
here that although the algorithm has better asymptotic convergence result, its performance in practice is
not as good as Algorithm 1.
Algorithm 2 Iterative Data Smoothing V2 ( D, θ0, α, β )
Input: The pairwise comparison dataset D={ai, a′
i, yi}n
i=1. A parameterized reward model family
{rθ:A 7→R|θ∈Θ}with initialization θ0∈Θ. Two step sizes α, β. An empirical loss function
Lθ({ci},D) =−1
nnX
i=1max(2 ci−1,0)·
yi·logexp(rθ(ai))
exp(rθ(ai)) + exp( rθ(a′
i))
+ (1−yi)·logexp(rθ(a′
i))
exp(rθ(ai)) + exp( rθ(a′
i))
Initialize t= 0 and ci,0= 1,∀i.
while rθtdoes not converge do
θt+1←θt−α· ∇L θ({ci,t},D)
ci,t+1←(1−β)·ci,t+β·exp(rθt+1(ai))
exp(rθt+1(ai)) + exp( rθt+1(a′
i))
t←t+ 1
end while
Return: rθt
We multiply a max(2ci−1,0) in front of the loss for each sample as an approximation of how confident
the current model predicts the preference label. When the reward is approximately similar, the coefficient
goes to 0, putting less weights on those samples. Below we show that asymptotically, the new iterative
data smoothing V2 algorithm is better at preserving the preference distribution compared with the original
version.
Theorem B.1. Consider the multi-armed bandit problem with the number of samples going to infinity
and a fixed sampling distribution µ. Assume that µ(a, a′)>0for any a, a′>0. Then we have
•Any stationary point for Algorithm 1 satisfies ∀a, a′,ˆr(a) = ˆr(a′);
•There is one stationary point for Algorithm 2 that satisfies
∀a, a′,ˆr(a)−ˆr(a′) =r⋆(a)−r⋆(a′).
Proof. The stationary points for Algorithm 1 and 2 are the points where the gradients equal 0. For
20

--- PAGE 21 ---
Algorithm 1, this is equivalent to ˆ y=exp(ˆr(a))
exp(ˆr(a))+exp(ˆ r(a′)), and
(µ(a≻a′)·ˆy+µ(a≺a′)·(1−ˆy))·exp(ˆr(a′))
exp(ˆr(a′)) + exp(ˆ r(a′))
=(µ(a≺a′)·ˆy+µ(a≻a′)·(1−ˆy))·exp(ˆr(a))
exp(ˆr(a)) + exp(ˆ r(a′)).
Here ˆycan be different for different ( a, a′). Solving the above equation gives that the only stationary
point is ˆ y= 1/2 and ˆ r(a)−ˆr(a′) = 0.
On the other hand, for Algorithm 2, the stationary point condition is equivalent to ˆc(a, a′) =
exp(ˆr(a))
exp(ˆr(a))+exp(ˆ r(a′)), and
X
a′max(2ˆ c(a, a′)−1,0)·
µ(a≻a′)·exp(ˆr(a′))
exp(ˆr(a′)) + exp(ˆ r(a′))−µ(a≺a′)·exp(ˆr(a))
exp(ˆr(a)) + exp(ˆ r(a′))
= 0.
Thus one can verify that ˆr(a)−ˆr(a′) =r⋆(a)−r⋆(a′) satisfies the stationary condition. This proves the
result.
Remark B.2.Although the asymptotic stationary points of Algorithm 1 do not contain the ground truth,
the two-scale analysis discussed in Section 3.2.2 shows that when one of the step size is much larger than
the other such that one of the updates in ˆyorˆris slower (and thus does not hit the stationary point),
the reward still converges to the ground truth for those sufficiently observed arms. However, preliminary
experiments on Algorithm 2 show that the result is worse than that of Algorithm 1, and also suffer from
reward overfitting. This together with the failure of MLE may suggest that asymptotic result does not
reflect the practical performance with smaller sample size compared with number of parameters.
Remark B.3.The condition of µ(a, a′)>0 can be relaxed to that the comparison graph induced by the
Laplace matrix Lis connected, since the reward is identifiable in this case (Shah et al., 2015).
C Additional Experiments
The hyper-parameters for the neural network experiments are listed in table 1.
Model Parameter Value
Reward modellearning rate α 10−5
label update parameter β 0.7
batch size 128
eval & save steps 100
Policy modelmax sequence length 1024
max output length 500
generation temperature 1.0
batch size 64
fixed KL coefficient 0.001
number of rollouts 128
PPO epochs 4
value coefficient 0.5
GAE coefficient λ 0.95
discount factor 1.0
clip range 2
Table 1: Hyper-parameters for the neural network experiments
We also include additional experiments on a different dataset, TLDR4, in Figure 5 and 6 of this
section. The settings follow the same as HH in Section 4.2. One can see that in the case of TLDR, the
4https://huggingface.co/datasets/CarperAI/openai_summarize_comparisons
21

--- PAGE 22 ---
test accuracy does not drop significantly like HH. However, even with small difference in loss and the
accuracy, the resulting policy reward difference is still significant.
0 1 2 3 4 5
Epochs0.560.580.600.620.640.66Test AccuracyTest Accuracy with 3B model
Data Refinement
MLE
0 1 2 3 4 5
Epochs0.6000.6250.6500.6750.7000.7250.7500.7750.800Test Cross Entropy LossTest Cross Entropy Loss with 3B model
Data Refinement
MLE
0 1 2 3 4 5
Epochs0.540.560.580.600.620.640.66Test AccuracyTest Accuracy with 1B model
Data Refinement
MLE
0 1 2 3 4 5
Epochs0.6000.6250.6500.6750.7000.7250.7500.7750.800Test Cross Entropy LossTest Cross Entropy Loss with 1B model
Data Refinement
MLE
0 1 2 3 4 5
Epochs0.560.580.600.620.640.66Test AccuracyTest Accuracy with 125M model
Data Refinement
MLE
0 1 2 3 4 5
Epochs0.6000.6250.6500.6750.7000.7250.7500.7750.800Test Cross Entropy LossTest Cross Entropy Loss with 125M model
Data Refinement
MLE
Figure 5. Comparisons of MLE and Iterative Data Smoothing when the reward is parameterized by a
neural network.
D Proof of Theorem 2.1
Proof. LetPr(a, a′, c) =Pr(a≻a′) ifc= 1, and Pr(a′≻a) ifc= 0 be the density function of the
observations. According to Theorem 6.1.3. of Hogg et al. (2013), it suffices to verify the following
conditions for the consistency of MLE:
22

--- PAGE 23 ---
0 5 10 15 20 25
Steps / k1.0
0.5
0.00.51.0Ground Truth RewardGround Truth Reward for Trained Policy
Data Refinement
MLE
Figure 6: Comparison of MLE and Iterative Data Smoothing for policy learning.
•The CDFs are distinct, i.e. Pr(a, a′, c) =Pr′(a, a′, c) almost everywhere implies that r=r′.
This is true since the distribution is supported on discrete space, and the equality implies that
r(i)−r(j) =r(i)′−r(j)′for any i, j, and r(M) =r′(M) = 1.
•The PDFs have common support for all r. This is true since the probability is positive for any
a, a′, c.
•The point r⋆is an interior point in RK. This is true by definition, since any open ball of radius ϵ
around r⋆is a subset of the space.
E Proof of Theorem 2.2
Proof. The construction is in similar spirit to Rashidinejad et al. (2021) and Zhu et al. (2023a). Consider
a bandit problem where r⋆(a) = 1(a= 1). For any fixed n, we set µ(1,2) = 1 −1/n,µ(1,3) = 1 /n.
In this hard instance, there is constant probability that arm 3 is only compared with arm 1 once.
Concretely, we have
P(n(1,3) = 1) = n·(1−µ(1,3))n−1·µ(1,3) = (1 −1/n)n−1.
When n≥500, we have P(n(1,3) = 1) ≥0.36. Under this case, we know that arm 3 is preferred with
probability at least exp(r(3))/(exp(r(1)) + exp(r(3))) >0.26. When there is only one comparison between
arm 1 and 3, and arm 3 is preferred, the MLE assigns r(3) as infinity. Even when the reward for arm 1 is
estimated perfectly, this leads to a population cross-entropy loss arbitrarily large.
F Proof of Corollary 2.3
Proof. The proof follows immediately from Theorem 2.2. Under the same construction, we know that
ˆrMLE(3) = + ∞with probability at least 0 .09. Thus, the sub-optimality of the resulting optimal policy is
at least 1.
23

--- PAGE 24 ---
G Proof of Theorem 3.1
Proof. Letˆribe the reward for the i-th arm, and ˆr= [ˆr1,ˆr2,···,ˆrK] as the vector for the reward. One
can calculate the gradient of the reward as
∇ˆriLCE(D,ˆr) =−1
nnX
i=1∇ˆri 
yilog 
exp(ˆrai)
exp(ˆrai) + exp(ˆ ra′
i)!
+ (1−yi)) log 
exp(ˆra′
i)
exp(ˆrai) + exp(ˆ ra′
i)!!
=−1
nnX
i=1 
yiexp(ˆra′
i)
exp(ˆrai)) + exp(ˆ ra′
i)−(1−yi) exp(ˆ rai)
exp(ˆrai) + exp(ˆ ra′
i)!
=−1
2·(n+(i)−n−(i)).
Here the last equality is due to that all the reward is initialized at the same value. And n+(i) (or n−(i))
refers the total number of winning (or losing) of arm iin the observations.
We assume all the reward is initialized at 0 without loss of generality. After one step gradient, we have
ˆr(i) =α(n+(i)−n−(i)).
This proves the result.
H Proof of Theorem 3.3
Proof. From the differential equations in (5), we know that
˙y(t)
y(t)≥ −β.
Taking integration on both sides give us
y(t)≥exp(−βt)≥exp(−ϵ).
Now we set a Lyapunov function V(t) =
exp(d(t))
1+exp( d(t))−µ2
. We know that
˙V(t) = 2exp(d(t))
1 + exp( d(t))−µ
·exp(d(t))
(1 + exp( d(t)))2·˙d(t)
= 2αnexp(d(t))
1 + exp( d(t))−µ
·exp(d(t))
(1 + exp( d(t)))2
·
(µ·y(t) + (1 −µ)·(1−y(t)))·1
1 + exp( d(t))−((1−µ)·y(t) +µ·(1−y(t)))·exp(d(t))
1 + exp( d(t))
= 2αnexp(d(t))
1 + exp( d(t))−µ
·exp(d(t))
(1 + exp( d(t)))2·
(2µ−1)·y(t) + 1−µ−exp(d(t))
1 + exp( d(t))
=−2αn·exp(d(t))
(1 + exp( d(t)))2·exp(d(t))
1 + exp( d(t))−µ2
+ 2αn·exp(d(t))
(1 + exp( d(t)))2·exp(d(t))
1 + exp( d(t))−µ
·(2µ−1)·(y(t)−1)
(i)
≤2αn·exp(d(t))
(1 + exp( d(t)))2· 
−exp(d(t))
1 + exp( d(t))−µ2
+ 1−exp(−ϵ)!
= 2αn·exp(d(t))
(1 + exp( d(t)))2·(−V(t) + 1−exp(−ϵ)).
24

--- PAGE 25 ---
Here (i) uses the fact that y(t), µ,exp(d(t))
1+exp( d(t))∈[0,1]. Now consider two scenarios. The first is that for any
time t∈[0, T], one always has V(t)≥2(1−exp(−ϵ)). In this case, we know that
˙V(t)≤2αn·exp(d(t))
(1 + exp( d(t)))2·(−V(t) + 1−exp(−ϵ))
≤ −αn·exp(d(t))
(1 + exp( d(t)))2·V(t)
≤0. (6)
This shows that V(t) is a non-increasing function. Without loss of generality, assume that µ≥1/2. We
know that
V(t)≤V(t′),∀t > t′. (7)
Now we prove that there must beexp(d(t))
1+exp( d(t))≤µ. If one can find some t0such thatexp(d(t))
1+exp( d(t))> µ, by
the continuity ofexp(d(t))
1+exp( d(t))and the fact thatexp(d(0))
1+exp( d(0))= 1/2, one can find some t1< t0such that
exp(d(t1))
1+exp( d(t1))=µ. This gives that
V(t1) = 0 < V(t0),
which contradicts Equation (7). Thus we know thatexp(d(t))
1+exp( d(t))≤µholds for any t. Furthermore, since
we know that V(t) is non-increasing, we know thatexp(d(t))
1+exp( d(t))≥1/2. This also implies that
exp(d(t))
(1 + exp( d(t)))2≥µ(1−µ).
Similarly, we can prove the same condition holds when µ <1/2. Thus we have
˙V(t)
V(t)≤ −µ(1−µ)αn.
By integrating over ton both sides, we have
V(t)≤exp(−µ(1−µ)αnt)·V(0)≤exp(−µ(1−µ)αnt).
Here the last inequality uses the fact that V(0)∈[0,1].
On the other hand, assume that at some time point t0∈[0, T], we have V(t0)<2(1−exp(−ϵ)). When
V(T)>2(1−exp(−ϵ)), by the continuity of the function V(·), we know that there exists some t1such
thatV(t1) = 2(1 −exp(−ϵ)), and for any t∈[t1, T],V(t)≥2(1−exp(−ϵ)). From Equation (6), we know
that in the regime of t∈[t1, T],V(t) is non-increasing. This contradicts with the fact that V(T)> V(t1).
Thus we know that
V(T)≤2(1−exp(−ϵ)).
25
