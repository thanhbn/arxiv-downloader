# 2211.03959.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2211.03959.pdf
# File size: 736923 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Pretraining in Deep Reinforcement Learning: A Survey
Zhihui Xie FFFFFARMER @SJTU .EDU.CN
Shanghai Jiao Tong University
Zichuan Lin ZICHUANLIN @TENCENT .COM
Tencent
Junyou Li JUNYOULI @TENCENT .COM
Tencent
Shuai Li SHUAILI 8@ SJTU .EDU.CN
Shanghai Jiao Tong University
Deheng Ye DERICYE @TENCENT .COM
Tencent
Abstract
The past few years have seen rapid progress in combining reinforcement learning (RL) with
deep learning. Various breakthroughs ranging from games to robotics have spurred the interest
in designing sophisticated RL algorithms and systems. However, the prevailing workÔ¨Çow in RL
is to learn tabula rasa , which may incur computational inefÔ¨Åciency. This precludes continuous
deployment of RL algorithms and potentially excludes researchers without large-scale computing
resources. In many other areas of machine learning, the pretraining paradigm has shown to be
effective in acquiring transferable knowledge, which can be utilized for a variety of downstream
tasks. Recently, we saw a surge of interest in Pretraining for Deep RL with promising results.
However, much of the research has been based on different experimental settings. Due to the
nature of RL, pretraining in this Ô¨Åeld is faced with unique challenges and hence requires new
design principles. In this survey, we seek to systematically review existing works in pretraining
for deep reinforcement learning, provide a taxonomy of these methods, discuss each sub-Ô¨Åeld, and
bring attention to open problems and future directions.
1. Introduction
Reinforcement learning (RL) provides a general-purpose mathematical formalism for sequential
decision-making (Sutton & Barto, 2018). By utilizing RL algorithms together with deep neural
networks, various milestones in different domains have achieved superhuman performances via
optimizing user-speciÔ¨Åed reward functions in a data-driven manner (Silver et al., 2016; Akkaya
et al., 2019; Vinyals et al., 2019; Ye et al., 2020, 2020, 2022; Chen et al., 2021b). As such, we have
seen a growing interest recently in this research direction.
However, while RL has been proven effective at solving well-speciÔ¨Åed tasks, the issue of sample
efÔ¨Åciency (Jin et al., 2021) and generalization (Kirk et al., 2021) still hinder its application to real-
world problems. In RL research, a standard paradigm is to let the agent learn from its own or
others‚Äô collected experience, usually on a single task, and to optimize neural networks tabula rasa
with random initializations. For humans, in contrast, prior knowledge about the world contributes
greatly to the decision-making process. If the task is related to previously seen tasks, humans tend
to reuse what has been learned to quickly adapt to a new task, without learning from exhaustive
1arXiv:2211.03959v1  [cs.LG]  8 Nov 2022

--- PAGE 2 ---
Adaptation
üí™: Pretrained Policy
üëÄ: Pretrained Repr .
üß†: OthersPretraining
Intrinsic 
Motivation
Downstream Env.
Knowledge TransferUpstream Env.
Upstream Env.Reward 
Signals
Images Videos TextsAgent
Agent Offline Data
Multi -modal & Multi -task DataActionObservation, Reward
Agent
Policy TransferùùÖùíÇùíî,ùíõ ùùÖùíÇùíî
Repr . TransferObservationLatent
StateFigure 1: An illustrating example of the RL pretraining pipeline.
interactions from scratch. Therefore, as compared to humans, RL agents usually suffer from great
data inefÔ¨Åciency (Kapturowski et al., 2022) and are prone to overÔ¨Åtting (Zhang et al., 2018).
Recent advances in other areas of machine learning, however, actively advocate for leveraging
prior knowledge built from large-scale pretraining. By training on broad data at scale, large generic
models, also known as foundation models (Bommasani et al., 2021), can quickly adapt to various
downstream tasks. This pretrain-Ô¨Ånetune paradigm has been proven effective in areas like computer
vision (Chen et al., 2020; He et al., 2020; Grill et al., 2020) and natural language processing (De-
vlin et al., 2019; Brown et al., 2020). However, pretraining has not yet had a signiÔ¨Åcant impact
on the Ô¨Åeld of RL. Despite its promise, designing principles for large-scale RL pretraining faces
challenges from many sources: 1) the diversity of domains and tasks; 2) the limited data sources;
3) the difÔ¨Åculty of fast adaptation to solve downstream tasks. These factors stem from the nature of
RL and are inevitably necessary to be considered.
This survey aims to present a bird‚Äôs-eye view of current research on Pretraining in Deep RL .
Principled pretraining in RL has a variety of potential beneÔ¨Åts. First of all, the substantial computa-
tional cost incurred by RL training remains a hurdle for industrial applications. For example, repli-
cating the results of AlphaStar (Vinyals et al., 2019) approximately costs millions of dollars (Agar-
wal et al., 2022). Pretraining can ameliorate this issue, either with pretrained world models (Sekar
et al., 2020) or pretrained representations (Schwarzer et al., 2021b), by enabling quick adaptation
to solve tasks in zero or few-shot manner. Besides, RL is notoriously task- and domain-speciÔ¨Åc. It
has already been shown that pretraining with massive task-agnostic data can enhance these kinds
of generalizations (Lee et al., 2022). Finally, we believe that pretraining with proper architec-
tures can unlock the power of scaling laws (Kaplan et al., 2020), as shown by recent success in
games (Schwarzer et al., 2021b; Lee et al., 2022). By scaling up general-purpose models with
increased computation, we are able to further achieve superhuman results, as taught in the ‚Äúbitter
lesson‚Äù (Sutton, 2019).
Pretraining in deep RL has undergone several breakthroughs in recent years. Naive pretrain-
ing with expert demonstrations , using supervised learning to predict the actions taken by experts,
has been exhibited with the famed AlphaGo (Silver et al., 2016). To pursue large-scale pretraining
with less supervision, the Ô¨Åeld of unsupervised RL has been growing rapidly in recent years (Burda
et al., 2019a; Laskin et al., 2021), which allows the agent to learn from interacting with the environ-
ment in the absence of reward signals. In accordance with recent advances in ofÔ¨Çine RL (Levine,
2

--- PAGE 3 ---
Notation Description
M Markov decision process
S State space
A Action space
T Transition function
0 Initial state distribution
r Reward function
 Discount factor
D OfÔ¨Çine dataset
 Trajectory
Q Q function
J Expected total discounted reward function
 Neural network parameters
 Feature encoder
z Skill latent vector
Z Skill latent space
H Entropy
I Mutual information
Table 1: Notations used in the survey.
et al., 2020), researchers further consider how to leverage unlabeled and sub-optimal ofÔ¨Çine data
for pretraining (Stooke et al., 2021; Schwarzer et al., 2021b), which we term ofÔ¨Çine pretraining .
The ofÔ¨Çine paradigm with task-irrelevant data further paves the way towards generalist pretraining ,
where diverse datasets from different tasks and modalities as well as general-purpose models with
great scaling properties are combined to build generalist models (Reed et al., 2022; Lee et al., 2022).
Pretraining has the potential to play a big role for RL and this survey could serve as a starting
point for those interested in this direction. In this paper, we seek to provide a systematic review of
existing works in pretraining for deep reinforcement learning. To the best of our knowledge, it is
one of the pioneering efforts to systematically study pretraining in deep RL.
Following the development trend of pretraining in RL, we organize the paper as follows. After
going through the preliminaries of reinforcement learning and pretraining (Section 2), we start with
online pretraining in which an agent learns from interacting with the environment without reward
signals (Section 3). And then, we consider ofÔ¨Çine pretraining, the scenario where unlabeled training
data is collected once with any policy (Section 4). In Section 5, we discuss recent advances in
developing generalist agents for a variety of orthogonal tasks. We further discuss how to adapt to
downstream RL tasks (Section 6). Finally, we conclude this survey together with a few prospects
(Section 7).
2. Preliminaries
2.1 Reinforcement learning
Reinforcement learning considers the problem of Ô¨Ånding a policy that interacts with the environment
under uncertainty to maximize its collected reward. Mathematically, this problem can be formulated
3

--- PAGE 4 ---
via a Markov Decision Process (MDP) deÔ¨Åned by tuple ( S,A,T,0,r,), with a state space S, an
action spaceA, a state transition distribution T:SAS! [0;1], an initial state distribution
0:S! [0;1], a reward function r:SA! R, and a discount factor 2(0;1). The objective
is to Ô¨Ånd such a policy (ajs)parameterized by that maximizes
J() =E;T;0"1X
t=0tr(st;at)#
;
known as the discounted returns. The notation used in the paper is summarized in Table 1.
2.2 Pretraining
Pretraining aims at obtaining transferable knowledge from large-scale training data to facilitate
downstream tasks. In the context of RL, transferable knowledge typically includes good represen-
tations that facilitate the agent to perceive the world (i.e., a better state space) and reusable skills
from which the agent can quickly build complex behaviors given task descriptions (i.e., a better
action space). Training data can be one bottleneck for effective RL pretraining. Unlike what we
have witnessed in Ô¨Åelds like computer vision and natural language processing where a wealth of
unlabeled data can be collected with minimal supervision, RL usually requires highly task-speciÔ¨Åc
reward design, which hinders scaling up pretraining for large-scale applications.
Therefore, the focus of this survey is unsupervised pretraining , in which task-speciÔ¨Åc rewards
are unavailable during pretraining but it is still allowed to learn from online interaction, unlabeled
logged data, or task-irrelevant data from other modalities. We omit supervised pretraining given that
with task-speciÔ¨Åc rewards this scenario roughly degenerates to existing RL settings (Levine et al.,
2020). Figure 1 demonstrates an overview of the pretraining and adaptation process.
The objective is to acquire useful prior knowledge in various forms like good visual representa-
tions, exploratory policies (ajs), latent-conditioned policies (ajs;z), or simply logged datasets.
Depending on what data is available during pretraining, it requires different considerations to obtain
useful knowledge (Section 3-5) and adapt it accordingly to downstream tasks (Section 6).
3. Online Pretraining
Most of the previous successes in RL have been achieved given dense and well-designed reward
functions. Despite its primacy in providing excel performances for a speciÔ¨Åc task, the traditional
RL paradigm faces two critical challenges when scaling it up to large-scale pretraining. Firstly,
it is notoriously easy for an RL agent to overÔ¨Åt (Zhang et al., 2018). As a result, a pretrained
agent trained with sophisticated task rewards can hardly generalize to unseen task speciÔ¨Åcations.
Furthermore, it remains a practical challenge to design reward functions which is usually costly and
requires expert knowledge.
Online pretraining without these reward signals can potentially be a good solution to learning
generic skills and eliminate the supervision requirement. Online pretraining aims at acquiring prior
knowledge by interacting with the environment in the absence of human supervision. During the
pretraining phase, the agent is allowed to interact with the environment for a long period without
access to extrinsic rewards. When the environment is accessible, playing with it facilitates skill
learning that will be useful later when a task is assigned to the agent. This solution, also known as
4

--- PAGE 5 ---
Type Algorithm Intrinsic Reward Visual
Curiosity-driven
ExplorationICM (Pathak et al., 2017) rt/kf((st);at) (st+1)k23
RND (Burda et al., 2019b) rt/kf((st);at) (st+1)k23
Disagreement (Pathak et al., 2019) rt/Var (f((st);at)) 3
Plan2Explore (Sekar et al., 2020) rt/Var (f((st);at)) 3
Skill DiscoveryVIC (Gregor et al., 2016) r/logq(zj(sH)) logp(z) 3
V ALOR (Achiam et al., 2018) r/logq(zjs1:H) logp(z) 7
DIAYN (Eysenbach et al., 2019) rt/logq(zjst) logp(z) 7
VISR (Hansen et al., 2020) rt/logq(zj(st)) logp(z) 3
DADS (Sharma et al., 2020) rt/logq(st+1jst;z) logq(st+1jst) 7
EDL (Campos et al., 2020) rt/logq(stjz) 7
APS (Liu & Abbeel, 2021a) rt/logq(stjz) +P
i2Irandomlogk(st) hik 3
HIDIO (Zhang et al., 2021c) rt/logq(zjat k+1:t;st k:t) 7
UPSIDE (Kamienny et al., 2022) rt/logq(zjst) logp(z) 7
LSD (Park et al., 2022) rt/((st+1) (st))>z 7
Data Coverage
MaximizationCBB (Bellemare et al., 2016) rt/^N(st) 1
2 7
MaxEnt (Hazan et al., 2019) rt/rR
^dt
7
SMM (Lee et al., 2019) rt/log ^p(st) logp(st) 7
APT (Liu & Abbeel, 2021b) rt/P
i2Irandomlogk(st) hik 3
Proto-RL (Yarats et al., 2021) rt/P
i2Iprototypelogk(st) hik 3
RE3 (Seo et al., 2021) rt/log (k(st) KNN ((st))k+ 1) 3
Table 2: Categorization of representative online pretraining approaches.
unsupervised RL , has been actively studied in recent years (Burda et al., 2019a; Srinivas & Abbeel,
2021).
To encourage the agent to build its own knowledge without any supervision, we need principled
mechanisms to provide the agent with intrinsic drives. Psychologists found that babies can discover
both the tasks to be learned and the solution to those tasks through interacting with the environ-
ment (Smith & Gasser, 2005). With experiences accumulated, they are capable of more difÔ¨Åcult
tasks later on. This motivates a wealth of research that studies how to build self-taught agents with
intrinsic rewards (Schmidhuber, 1991; Singh et al., 2004; Oudeyer et al., 2007). Intrinsic rewards,
in contrast to task-specifying extrinsic rewards, refer to general learning signals that encourage the
agent either to collect diverse experiences or to develop useful skills. It has been shown that pre-
training an agent with intrinsic rewards and standard RL algorithms can lead to fast adaptation once
the downstream task is given (Laskin et al., 2021).
Based on how to design intrinsic rewards, we classify existing approaches of unsupervised RL
into three categories1: curiosity-driven exploration, skill discovery, and maximal data coverage.
Table 2 presents a categorization of representative online pretraining algorithms together with their
used intrinsic rewards.
3.1 Curiosity-driven Exploration
In the psychology of motivation, curiosity represents motivation to reduce uncertainty about the
world (Silvia, 2012). Inspired by this line of psychological theory, similar ideas have been studied
to build curiosity-driven approaches for online pretraining. Curiosity-driven approaches seek to
explore interesting states that can possibly bring knowledge about the environment. Intuitively, if
the agent falls short of accurately predicting the environment, it gains knowledge by interacting and
then reducing this part of the uncertainty. The deÔ¨Åning characteristic of a curiosity-driven agent
1. This taxonomy of unsupervised RL was originally proposed by Srinivas and Abbeel (2021).
5

--- PAGE 6 ---
ùë†ùë° ùúô‚Ñéùë°ùëéùë°
‡∑†‚Ñéùë°+1
sùë°+1ùúô‚Ñéùë°+1ùëì
ùëüùë°=|‡∑†‚Ñéùë°+1‚àí‚Ñéùë°+1|22Figure 2: The process of computing intrinsic rewards using curiosity-driven exploration ap-
proaches.
is how to compute the degree of curiosity to these interesting states, which directly serves as the
intrinsic reward for learning. A concrete example is ICM (Pathak et al., 2017), which applies the
intrinsic reward proportional to the prediction error as shown in Figure 2:
rt/kf((st);at) (st+1)k2
2;
wherefandrepresent the learned forward dynamics model and feature encoder, respectively.
To measure curiosity, a broad class of approaches (Pathak et al., 2017; Haber et al., 2018) lever-
ages this kind of learned dynamics models to predict future states in an auxiliary feature space. there
are mainly two kinds of estimation: prediction error and prediction uncertainty. Despite that these
dynamics-based approaches perform quite well across common scenarios, they usually suffer from
action-dependent noisy TVs (Burda et al., 2019a), which will be discussed later in Section 3.1.1.
This deÔ¨Åciency encourages the following work to design dynamics-free curiosity estimation (Burda
et al., 2019b) and more sophisticated uncertainty estimation methods (Pathak et al., 2019; Sekar
et al., 2020).
Another important design choice is associated with the feature encoder , especially for high-
dimensional observations. A proper feature encoder can make the prediction task more tractable
and Ô¨Ålter out irrelevant aspects so that the agent can only focus on the informative ones. Early
studies (Stadie et al., 2015; Burda et al., 2019a) leverage auto-encoding embeddings to recover the
original high-dimensional inputs, but the induced feature space is usually too informative about
irrelevant details and hence susceptible to noise. To address this issue, Pathak et al. (2017) utilize an
inverse dynamics model for feature encoding to make sure that the agent is unaffected by nuisance
factors in the environment. The proposed ICM shows impressive zero-shot performance in playing
video games. Burda et al. (2019b) further relax the design burden by simply replacing the feature
model with a Ô¨Åxed randomly initialized neural network, which is proven effective by a following
large-scale empirical study (Burda et al., 2019a). Despite that random feature encoders are sufÔ¨Åcient
for good performance at training, learned features (e.g., based on inverse dynamics) generalize
better (Burda et al., 2019a). Inspired by recent advances in representation learning, Du et al. (2021)
6

--- PAGE 7 ---
ùëüùë°=ùëôùëúùëîùëûùëß‚Ñéùë°+1‚àíùëôùëúùëîùëùùëß ùëß
ùë†ùë°+1ùúô‚Ñéùë°+1ùëûùëôùëúùëîùëûùëß‚Ñéùë°+1ùëù(ùëß)Figure 3: The process of computing intrinsic rewards using skill discovery approaches.
directly link curiosity and representation learning loss by formulating a minimax game between a
generic representation learning algorithm and a reinforcement learning policy.
3.1.1 C HALLENGES & F UTURE DIRECTIONS
This kind of approach has several deÔ¨Åciencies. One of the most important issues is how to distin-
guish epistemic andaleatoric uncertainty. Epistemic uncertainty refers to uncertainty caused by a
lack of knowledge. Aleatoric uncertainty, in contrast, refers to the variability in the outcome due to
inherently random effects. A concrete phenomenon in RL is the noisy TV problem (Mavor-Parker
et al., 2022), which refers to the cases where the agent gets trapped by its curiosity in highly stochas-
tic environments. To mitigate this issue, some work attempts to use intrinsic rewards proportional
to a reduction in uncertainty (Houthooft et al., 2016; Pathak et al., 2019). However, tractable epis-
temic uncertainty estimation in high dimension remains challenging (H ¬®ullermeier & Waegeman,
2021) due to its sensitivity to imperfect data.
Another issue with the above approaches is that they only receive retrospective signals after
the agent has achieved epistemic uncertainty, which might cause inefÔ¨Åciency in exploration. Based
on this intuition, Sekar et al. (2020) design a model-based method that can prospectively look for
uncertainty in the environment.
3.2 Skill Discovery
Apart from curiosity-driven approaches that tackle unsupervised RL in a model-based perspective,
one can also consider model-free learning of primitive skills2that can be composed to solve down-
stream tasks. This kind of approach is usually referred to as skill discovery approach. The main
intuition behind this is that the learned skill should control which states the agent visits, which can
be seen as a notion of empowerment.
Generally speaking, the objective for skill discovery can be formalized as maximizing the mu-
tual information (MI) between skill latent variable zand states:
I(s;z) =H(z) H(zjs) =H(s) H(sjz); (1)
where we deÔ¨Åne skills oroptions as the policies conditioned on z. There are two components for a
skill discovery agent to determine: 1) a skill distribution p(z); 2) a skill policy (ajs;z). Before
each episode, skill latent zis sampled from distribution p(z), followed by skill (ajs;z)to interact
2. In this work, we use skill,option , and behavior prior interchangeably.
7

--- PAGE 8 ---
with the environment. Learning skills that maximize MI is a challenging optimization problem,
upon which a variety of approaches sharing the same spirit have been applied.
Among the existing MI-based skill discovery methods, the majority (Gregor et al., 2016; Ey-
senbach et al., 2019; Hansen et al., 2020) apply the former form of Equation 1 with the following
variational lower bound (Agakov, 2004):
I(s;z) =Es;zp(s;z)[logp(zjs)] Ezp(z)[logp(z)]
Es;zp(s;z)[logq(zjs)] Ezp(z)[logp(z)]:
In this case, a parametric model q(zjs)is trained together with other variables to estimate the
conditional distribution p(zjs). Maximizing H(z)can be achieved by sampling zfrom a learned
distribution (Gregor et al., 2016) or directly from a Ô¨Åxed uniform distribution (Eysenbach et al.,
2019). As shown in Figure 3, the intrinsic reward is given by rt= logq(zjst) logp(z), upon
which one can apply standard RL algorithms to learn skills.
Another line of research (Sharma et al., 2020; Campos et al., 2020; Liu & Abbeel, 2021a; Laskin
et al., 2022) considers the latter form and similarly derives a lower bound:
I(s;z) =Es;zp(s;z)[logp(sjz)] Esp(s)[logp(s)]
Es;zp(s;z)[logq(sjz)] Esp(s)[logp(s)]:
In this formulation, maximizing the state entropy H(s)encourages exploration while minimizing
the conditional entropy results in directed behaviors. The difÔ¨Åculty lies in the density estimation
ofs, especially for high-dimensional state spaces. A common practice is to maximize H(s)via
maximum entropy estimation (Hazan et al., 2019; Lee et al., 2019; Liu & Abbeel, 2021b), which
will be elaborated more in Section 3.3.
Although different work uses slightly different approaches to optimize Equation 1, it could be
more important to decide other design factors when using skill discovery for online pretraining.
For instance, while most studies consider the episodic setting, some efforts have been made to
extend MI-based skill discovery to non-episodic settings (Xu et al., 2020; Lu et al., 2021b). It
is also promising to consider a curriculum with an increasing number of skills to learn (Achiam
et al., 2018). Several other factors are also worth mentioning, such as whether skill latent zis
discrete (Achiam et al., 2018) or continuous (Warde-Farley et al., 2019), whether the reward signals
are dense (Eysenbach et al., 2019) or sparse (Gregor et al., 2016), and whether it works for image-
based observations (Warde-Farley et al., 2019).
Skill discovery can be also reinterpreted as goal-conditioned policy learning, where zasself-
generated andabstract goal is sampled from a distribution instead of provided by the task. One
can also consider generating concrete goals in a self-supervised manner (Warde-Farley et al., 2019;
Pong et al., 2020) and derive a goal-conditioned reward function similarly from MI maximiza-
tion. DISCERN (Warde-Farley et al., 2019) designs a non-parametric approach for goal sampling,
maintaining a buffer of past observations that drifts as the agent collects new experiences. Skew-
Fit (Pong et al., 2020) instead learns a maximum entropy goal distribution by increasing the entropy
of a generative model in an iterative manner. Choi et al. (2021) provide a more formal connection
mainly from the perspective of goal-conditioned RL. We refer the interested reader to Colas et al.
(2022) for further discussion.
8

--- PAGE 9 ---
3.2.1 C HALLENGES & F UTURE DIRECTIONS
A major issue for MI-based skill discovery approaches is that the objective does not necessarily lead
to strong state coverage as one can maximize I(s;z)even with the smallest state variations (Campos
et al., 2020; Park et al., 2022). This lack of coverage can greatly limit their applicability to down-
stream tasks with complex environments (Campos et al., 2020). To resolve this issue, some existing
work explicitly uses x-ycoordinates as features to enforce state coverage induced by skills (Eysen-
bach et al., 2019; Sharma et al., 2020). It is also explored to separate the learning process to Ô¨Årst
maximizeH(s)via maximum entropy estimation, followed by behavior learning (Campos et al.,
2020; Liu & Abbeel, 2021a).
Moreover, it is empirically shown that skill discovery methods underperform other kinds of on-
line pretraining methods, which may be due to restricted skill spaces (Laskin et al., 2021). This
calls attention to dissecting what skills are learned. In order to live up to their full potential, the
discovered skills must strike a balance between generality (i.e., the applicability to a large variety
of downstream tasks) and speciÔ¨Åcity (i.e., the quality of being useful to induce speciÔ¨Åc behav-
iors) (Gehring et al., 2021). It is also desired to avoid learning trivial skills (Sharma et al., 2020;
Baumli et al., 2021).
3.3 Data Coverage Maximization
Previously we have discussed how to obtain knowledge or skills, measured by the agent‚Äôs own ca-
pability, from unsupervised interaction. Albeit indirectly related to the agent‚Äôs ability, data diversity
induced by online pretraining plays an essential role in deciding how well the agent obtains prior
knowledge. In the Ô¨Åeld of supervised learning, recent advances have shown that diverse data can en-
hance out-of-distribution generalization (Hendrycks et al., 2020b) and robustness (Hendrycks et al.,
2020a). Another supporting evidence is that most of the famed datasets are large and diverse (Deng
et al., 2009; Wang et al., 2019). Motivated by the above considerations, it is desired to use data cov-
erage maximization, usually measured by state visitation, as an objective to stimulate unsupervised
learning.
3.3.1 C OUNT -BASED EXPLORATION
The Ô¨Årst category of data coverage maximization is count-based exploration. Count-based explo-
ration methods directly use visit counts to guide the agent towards underexplored states (Bellemare
et al., 2016; Ostrovski et al., 2017). For tabular MDPs, Model-based Interval Estimation with
Exploration Bonuses (Strehl & Littman, 2008) provably turn state-action N(s;a)counts into an
exploration bonus reward:
rt/N(st;at) 1=2: (2)
Built on Equation 2, a series of work has studied how to tractably generalize count bonuses to
high-dimensional state spaces (Bellemare et al., 2016; Ostrovski et al., 2017; Tang et al., 2017).
To approximate these counts in high dimensions, Bellemare et al. (2016) introduce pseudo-counts
derived from a density model. SpeciÔ¨Åcally, the pseudo-count is deÔ¨Åned as:
^N(s) =t(s) (1 0
t(s))
0
t(s) t(s);
9

--- PAGE 10 ---
whereis a density model over state space S,t(s)is the density assigned to safter training on a
sequence of states s1;:::;st, and0
t(s)is the density of sifwere to be trained on sone additional
time. Based on similar ideas, it has been shown that a better density model (Ostrovski et al., 2017) or
a hash function (Tang et al., 2017; Rashid et al., 2020) for computing state statistics can further im-
prove performance. Besides, a self-supervised inverse dynamics model as discussed in Section 3.1
can also be used to bias the count-based bonuses towards what the agent can control (Badia et al.,
2020).
3.3.2 E NTROPY MAXIMIZATION
To encourage novel state visitation, an alternative objective is to directly maximize the entropy of
state visitation distribution dinduced by policy (ajs):
2arg max
2H(d);
whereH()can be Shannon entropy (Hazan et al., 2019; Lee et al., 2019; Seo et al., 2021), R ¬¥enyi
entropy (Zhang et al., 2021b), or geometry-aware entropy (Guo et al., 2021). The state distribution
dcan either be a discounted distribution (Hazan et al., 2019), a marginal distribution (Lee et al.,
2019), or a stationary distribution (Tarbouriech & Lazaric, 2019).
Albeit compelling, the objective relies on maximizing state entropy, which is notoriously hard
to estimate and optimize. Hazan et al. (2019) contribute a provably efÔ¨Åcient algorithm in the tabu-
lar setting using the conditional gradient method (Frank & Wolfe, 1956) to avoid direct optimiza-
tion. Lee et al. (2019) propose a similar approach that can be viewed from the perspective of state
marginal matching between the state distribution and a given target distribution (e.g., a uniform
distribution). Both Hazan et al. (2019) and Lee et al. (2019) propose to learn a mixture of policies
that maximizes the induced state entropy in an iterative manner. While impressive, these parametric
approaches struggle to scale up to high dimensional spaces. To address this issue, Mutti et al. (2020)
instead optimize a non-parametric, particle-based estimate of state distribution entropy (Kontoyian-
nis et al., 1998), but restrict its use to state-based tasks.
For unsupervised online pretraining with visual observations, entropy maximization becomes
more tricky as exploration is now inextricably intertwined with representation learning. This leads
to achicken-and-egg problem (Yarats et al., 2021; Tam et al., 2022), where learning useful represen-
tations requires diverse data, while effective exploration can only be achieved with good represen-
tations. Based on particle-based entropy estimators, several approaches successfully apply entropy
maximization in image-based tasks with self-supervised representations learned by inverse dynam-
ics prediction (Seo et al., 2021), contrastive learning (Liu & Abbeel, 2021b; Yarats et al., 2021), or
the information bottleneck (Tao et al., 2020).
3.3.3 C HALLENGES & F UTURE DIRECTIONS
Although count-based approaches are shown effective for exploration, it has been shown in previous
work (Ecoffet et al., 2021) that they usually suffer from detachment , in which the agent loses track
of interesting areas to explore, and derailment , in which the exploratory mechanism prevents it from
returning to previously visited states. Count-based approaches also tend to be short-sighted, driving
the agent to get stuck in local minima (Burda et al., 2019b).
10

--- PAGE 11 ---
Type Algorithm Objective Visual Expert Data
Skill ExtractionSPiRL (Pertsch et al., 2021a) Variational Auto-encoder 3 7
OPAL (Ajay et al., 2021) Variational Auto-encoder 7 3
Parrot (Singh et al., 2021) Normalizing Flow 3 3
SkiLD (Pertsch et al., 2021b) Variational Auto-encoder 7 7
TRIAL (Yang et al., 2022) Energy-based Model 7 3
FIST (Hakhamaneshi et al., 2022) Variational Auto-encoder 7 3
Representation
LearningWorld Model (Ha & Schmidhuber, 2018) Reconstruction 3 7
ST-DIM (Anand et al., 2019) Forward Pixel Prediction 3 7
ATC (Stooke et al., 2021) Forward Dynamics Modeling 3 3
SGI (Schwarzer et al., 2021b) Forward Dynamics Modeling 3 7
Markov (Allen et al., 2021) Inverse Dynamics Modeling 3 7
Table 3: Categorization of representative ofÔ¨Çine pretraining approaches.
When applying state entropy maximization approaches for pretraining, it is worth pointing out
that many of them aim at maximizing the entropy of all states visited during the process, and hence
the Ô¨Ånal policy is not necessarily exploratory (Lee et al., 2019). It has also been shown theoretically
that the class of Markovian policies is insufÔ¨Åcient for the maximum state entropy objective, while
non-Markovian policies are essential to guarantee good exploration.
Instead of learning an exploratory policy, another line of research considers collecting unlabeled
records as a prerequisite for ofÔ¨Çine RL (Yarats et al., 2022; Lambert et al., 2022), which is an
interesting direction for understanding and utilizing task-agnostic agents.
4. OfÔ¨Çine Pretraining
Despite its attractive effectiveness of learning without human supervision, online pretraining is still
limited for large-scale applications. Eventually, it is difÔ¨Åcult to reconcile online interaction with
the need to train on large and diverse datasets (Levine, 2021). To address this issue, it is desired to
decouple data collection and pretraining and directly leverage historical data collected from other
agents or humans.
A feasible solution is ofÔ¨Çine RL (Lange et al., 2012; Levine et al., 2020), which has been gaining
attention recently. OfÔ¨Çine RL aims to obtain a reward-maximizing policy purely from ofÔ¨Çine data.
A fundamental challenge of ofÔ¨Çine RL is the distributional shift , which refers to the distribution
discrepancy between training data and those seen during testing. Existing ofÔ¨Çine RL approaches
focus on how to address this challenge when using function approximation. For instance, policy
constraint approaches (Kumar et al., 2019; Siegel et al., 2020) explicitly require the learned policy
to avoid taking unseen actions in the dataset. Value regularization methods (Kumar et al., 2020)
alleviate the overestimation problem of value functions by Ô¨Åtting them to some forms of lower
bounds. However, it remains under-explored whether policies trained ofÔ¨Çine can generalize to new
contexts unseen in the ofÔ¨Çine dataset (Kirk et al., 2021).
Another scenario is ofÔ¨Çine-to-online RL (Nair et al., 2020; Lu et al., 2022; Lee et al., 2022;
Kostrikov et al., 2022), where ofÔ¨Çine RL is used for pretraining, followed by online Ô¨Ånetuning.
It has been shown in this scenario that ofÔ¨Çine RL can accelerate online RL (Nair et al., 2020).
However, both ofÔ¨Çine RL and ofÔ¨Çine-to-online RL require the ofÔ¨Çine experience to be annotated
with rewards, which are challenging to provide for large real-world datasets (Pertsch et al., 2021a).
A compelling alternative direction for leveraging ofÔ¨Çine data is to sidestep policy learning, but
instead learn prior knowledge that is beneÔ¨Åcial for downstream tasks in terms of convergence speed
11

--- PAGE 12 ---
or Ô¨Ånal performances. What is more intriguing, if our model were able to utilize data without
human supervision, it could potentially beneÔ¨Åt from web-scale data for decision-making. We refer
to this setting as ofÔ¨Çine pretraining, where the agent can extract important information (e.g., good
representations and behavior priors) from ofÔ¨Çine data. In Table 3, we categorize existing ofÔ¨Çine
pretraining approaches as well as summarize each approach‚Äôs key properties.
4.1 Skill Extraction
Learning useful behaviors from ofÔ¨Çine data has a long history (Pomerleau, 1988; Argall et al., 2009).
When the ofÔ¨Çine data comes from expert demonstrations, it is straightforward to pretrain policies
via imitation learning (Silver et al., 2016; Rajeswaran et al., 2018; Gupta et al., 2020), which is
often used in real-world applications like robotic manipulation (Zhu et al., 2018; Zhang et al., 2018)
and self-driving (Codevilla et al., 2019). However, imitation learning approaches often assume that
the training data contains complete solutions. They therefore usually fall short of obtaining good
policies when demonstrations are collected from a series of sources.
An alternative solution is to learn useful behavior priors from ofÔ¨Çine data (Lynch et al., 2020;
Shankar & Gupta, 2020; Chebotar et al., 2021), similar to what we have discussed in Section 3.2.
Compared with its online counterpart, ofÔ¨Çine skill extraction assumes a Ô¨Åxed set of trajectories.
These approaches learn a spectrum of behavior policies conditioned on latent z, which provide a
more compact action space for learning high-level policies that can quickly adapt to downstream
tasks. SpeciÔ¨Åcally, temporal skill extraction (Yang et al., 2022) for few-shot imitation (Ajay et al.,
2021) and RL (Ajay et al., 2021; Pertsch et al., 2021a, 2021b) considers how to distill ofÔ¨Çine trajec-
tories into primitive policies (ajs;z), wherez2Z denotes a skill latent learned via unsupervised
learning. By leveraging stochastic latent variable models, we aim at learning a skill latent zi2Z
for a sequence of state-action pairs fst;at;:::;st+H 1;at+H 1g, whereHis a Ô¨Åxed horizon or a
variable one (Kipf et al., 2019; Shankar et al., 2020). For example, Ajay et al. (2021) propose the
following auto-encoding objective to learn primitive skills:
min
;;!J(;;! ) =ED;zq(zj)"
 H 1X
t=0log(atjst;z)#
s.t.ED[DKL(q(zj)k!(zjs0))]KL;
whereq(zj)encodes the trajectory into skill latent zand skill policy (ajs;z)serves as a
decoder to translate skill latent zinto action sequences. To transfer skills into downstream tasks, it
is feasible to learn a hierarchical policy that generates high-level behaviors with (zjs)trained on
downstream tasks (Pertsch et al., 2021a), which will be elaborated in Secition 6.2.
Various latent variable models have been used for pretraining behavior priors. For instance,
variational auto-encoders (Kingma & Welling, 2014) are widely considered (Pertsch et al., 2021a;
Lynch et al., 2020; Ajay et al., 2021). Following work (Singh et al., 2021; Florence et al., 2022)
also explores normalizing Ô¨Çow (Dinh et al., 2017) and energy-based models (LeCun et al., 2006) to
learn action priors.
The scenario of pretraining behavior priors also bears resemblance to few-shot imitation learn-
ing (Dance et al., 2021; Hakhamaneshi et al., 2022). However, for few-shot imitation learning, it
is often assumed that expert data is collected from a single behavior policy. Furthermore, due to
error accumulation (Ross et al., 2011), few-shot imitation learning is often limited to short-horizon
12

--- PAGE 13 ---
Type SufÔ¨Åciency Compactness
Reconstruction FFF F
Forward Pixel Prediction FFF F
Forward Dynamics Modeling FF FF
Inverse Dynamics Modeling F FFF
Table 4: Comparison between different representation learning approaches.
problems (Hakhamaneshi et al., 2022). In this regard, learning behavior priors from diverse and
sub-optimal data appears to be a promising direction.
4.1.1 C HALLENGES & F UTURE DIRECTIONS
Despite its potential to extract useful primitive skills, it is still challenging to pretrain on highly
sub-optimal ofÔ¨Çine data containing random actions (Ajay et al., 2021). Besides, RL with learned
skills does not usually generalize to downstream tasks efÔ¨Åciently, requiring millions of online inter-
actions to converge (Hakhamaneshi et al., 2022). A possible solution is to combine with successor
features (Barreto et al., 2017; Hansen et al., 2020) for fast task inference. However, strategies that
directly use the pretrained policies for exploitation may result in sub-optimal solutions in such a
scenario (Campos et al., 2021).
4.2 Representation Learning
While pretraining behavior priors focus on reducing the complexity of the action space, there exists
another line of work that aims to pretrain good state representations from ofÔ¨Çine data to promote
transfer. If the agent effectively reduces the representation gap between the learned state represen-
tations and the ground-truth endogenous states, it can better focus on factors that are essential for
control. Table 4 compares different kinds of representation learning objectives in terms of sufÔ¨Å-
ciency (i.e., whether the representations contain sufÔ¨Åcient state information) and compactness (i.e.,
whether the representations discard irrelevant information).
Learning good state representations for RL is a mature research area with a range of tools (Nachum
et al., 2019; van den Oord et al., 2018; Zhang et al., 2022b). Traditionally, the problem is formulated
to group states into clusters based on certain properties (Li et al., 2006). Existing representation
learning approaches generally propose some predictive properties that the desired representations
have, with regard to states, actions, and rewards across different time-steps. One of the most rep-
resentative concepts is bisimulation (Larsen & Skou, 1991; Givan et al., 2003), which originally
requires two equivalent states to have the same reward and equivalent distributions over the next
bisimilar states. The objective turns out to be very restrictive and is further relaxed by following
work (Ferns et al., 2004; Castro & Precup, 2010) with a deÔ¨Åned pseudo-metric space to measure be-
havioral similarity. Despite their recent advances (Gelada et al., 2019; Zhang et al., 2021a; Agarwal
et al., 2021) in effective representation learning using deep neural networks, bisimulation methods
fail to provide good abstraction when the rewards are sparse or even absent. In this case, solely
relying on a forward model can lead to representation collapse (Allen et al., 2021).
To alleviate representation collapse, one can instead set the targets to pixel observations. This
includes reconstruction-based approaches (Lange & Riedmiller, 2010; Ha & Schmidhuber, 2018)
and those based on pixel prediction (Watter et al., 2015; Kaiser et al., 2020). Reconstruction-based
13

--- PAGE 14 ---
approaches typically train an auto-encoder on image observations to learn a low-dimensional repre-
sentation, using which a policy is learned subsequently. Approaches based on pixel prediction force
the representations to contain sufÔ¨Åcient information about future pixel observations. Despite these
learned representations preserving sufÔ¨Åcient information about the observation, it lacks compact-
ness and does not guarantee to capture of useful information for the control task.
Instead of predicting the future, it is also beneÔ¨Åcial to model the inverse dynamics of the sys-
tem (Christiano et al., 2016; Pathak et al., 2017). Inverse dynamics modeling learns a representation
that is predictive of the action taken between a pair of consecutive states. It has been shown that
the learned representation can Ô¨Ålter out all uncontrollable aspects of the observations (Efroni et al.,
2021). However, it can also wrongly ignore controllable information and cause over-abstraction
over the state space (Rakelly et al., 2021; Efroni et al., 2021).
With the rise of self-supervised learning developed for CV and NLP, a natural direction is to
adapt these task-agnostic techniques to RL. For instance, a large body of works has explored con-
trastive learning (Gutmann & Hyv ¬®arinen, 2010) as an effective framework to learn good representa-
tions (Nachum et al., 2019; Laskin et al., 2020; Mazoure et al., 2020; Zhan et al., 2020). Contrastive
learning typically uses the InfoNCE loss (Poole et al., 2019) to maximize mutual information be-
tween two variables:
LInfoNCE =E"
logexp (f(xi;yi))
1
KPK
j=1exp (f(xi;yj))#
;
wherefis a bilinear function f(xi;yi) =(xi)>W(yi)with learned parameter W2Rnnand
Kis the number of negative samples. These approaches usually incorporate temporal information,
aiming to distinguish between sequential and non-sequential states (Anand et al., 2019; Stooke
et al., 2021). Following works (Schwarzer et al., 2021a, 2021b) further consider bootstrapped latent
representations (Grill et al., 2020) that get rid of negative samples.
Aside from the above representation learning objectives, some other work considers imposing
Lipschitz smoothness (Gelada et al., 2019; Zhang et al., 2021a), kinematic inseparability (Misra
et al., 2020), or the Markov property (Allen et al., 2021). It has also been shown that a combined
objective can also lead to better performance (Schwarzer et al., 2021b).
4.2.1 C HALLENGES & F UTURE DIRECTIONS
While unsupervised representations have been shown to bring signiÔ¨Åcant improvements to down-
stream tasks, the absence of reward signals typically leads the pretrained encoder to focus on task-
irrelevant features instead of task-relevant ones in visually complex environments (Yamada et al.,
2022). To alleviate this issue, one might incorporate additional inductive bias (Janny et al., 2022) or
labeled data that are cheaper to obtain. We will discuss the latter solution in Section 5.
Another challenge for unsupervised representation learning is how to measure its effectiveness
without access to downstream tasks. Such evaluation is beneÔ¨Åcial because it can provide a proxy
metric to predict performance and promote a deeper understanding of the semantic meanings of pre-
trained representations. To achieve this, it is desired to analyze these representations with probing
techniques and determine which properties they encode. Although previous work has made efforts
in this direction (Zhang et al., 2022a), it remains unclear what properties are most indispensable for
pretrained representations.
14

--- PAGE 15 ---
5. Towards Generalist Agents with RL
So far we have discussed online and ofÔ¨Çine scenarios that are generally restricted to a single modal-
ity and single environment. Recently, there is a surge of interest in building a single generalist
model (Reed et al., 2022; Lee et al., 2022; Fan et al., 2022) to handle tasks in different environments
across different modalities. To enable the agent to learn from and adapt to various open-ended tasks,
it is desired to leverage considerable prior knowledge in different forms such as visual perception
and language understanding. Intuitively, the aim is to bridge the worlds of RL and other Ô¨Åelds of
machine learning, combining previous success together to build a large decision-making model ca-
pable of a diverse set of tasks. In this section, we look at various considerations for handling data
and tasks from different modalities to acquire useful prior knowledge.
5.1 Visual Pretraining
Perception is an unavoidable prerequisite for real-world applications. With an increased number of
image-based decision-making tasks, pretrained visual encoders that were exposed to a wide distri-
bution of images can provide RL agents with robust and resilient representations as a basis to learn
optimal policies.
The Ô¨Åeld of computer vision has seen tremendous progress in pretraining visual encoders from
large-scale image datasets (Deng et al., 2009) and video corpora (Abu-El-Haija et al., 2016). Given
that these data are cheap to access, several works have explored the use of pretrained visual encoders
on large-scale image datasets as means to improve the generalization and sample efÔ¨Åciency of RL
agents. Shah and Kumar (2021) equip standard deep RL algorithms with ResNet encoders pre-
trained on ImageNet and observe that the pretrained representations lead to impressive performances
in Adroit (Rajeswaran et al., 2018) but struggle in the DeepMind control suite (Tassa et al., 2018)
due to large domain gap. Parisi et al. (2022) further investigate various design choices including
datasets, augmentations, and layers, and report positive results on all four considered control tasks.
Tr¬®auble et al. (2022) conduct a large-scale study on how different properties of pretrained V AE-
based embeddings affect out-of-distribution generalization, concluding that some of them (e.g., the
GS metric (Dittadi et al., 2021)) can be good proxy metrics to predict generalization performance.
Instead of extracting visual information from static image datasets, another intriguing direc-
tion is to capture temporal relations from unlabeled videos. Sermanet et al. (2018) design a self-
supervised approach to learning temporal variance and multi-view invariance on multi-view video
data. Xiao et al. (2022) empirically Ô¨Ånd that, without exploiting temporal information, in-the-wild
images collected from YouTube or Egocentric videos lead to better self-supervised representations
for manipulation tasks that ImageNet images. Seo et al. (2022) introduce a two-phase learning
framework, which Ô¨Årst learns useful representations via generative pretraining on videos and then
uses the pretrained model for learning action-conditional world models. Baker et al. (2022) suc-
cessfully extract behavioral priors from internet-scale videos with an inverse dynamics model to
uncover the underlying actions followed by behavior cloning, Ô¨Ånding that the pretrained model ex-
hibits impressive zero-shot capabilities and Ô¨Ånetuning results for playing Minecraft. Zhang et al.
(2022) also leverage inverse dynamics models to predict action labels from action-free videos, upon
which a new contrastive learning framework is proposed to pretrain action-conditioned policies.
15

--- PAGE 16 ---
5.2 Natural Language Pretraining
Human beings are not only able to perceive the visual world through their eyes, but understand
high-level natural language instructions and ground the rich knowledge from texts to complete
tasks. In this vein, there has been a long history of how to connect language and actions (Kol-
lar et al., 2010; Tellex et al., 2011). Especially due to the rapid development of large language
models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022) that exhibit great capability of en-
coding semantic knowledge, it appears to be a promising direction to leverage advanced LLMs as
generic computation engines to facilitate decision making (Lu et al., 2021a).
5.2.1 L ANGUAGE -CONDITIONED POLICY LEARNING
To extract and harness the knowledge of well-informed pretrained LLMs, a feasible solution is to
condition the policies on text descriptions processed by LLMs. This kind of language-conditioned
policy learning could be extremely useful for robotic tasks where high-level language instructions
are available. For example, Ahn et al. (2022) use pretrained LLMs to split high-level instruc-
tions into sub-tasks via prompt engineering for grounding value functions in real-world robotic
tasks. Huang et al. (2022b) further enable grounded closed-loop feedback generated by addi-
tional perception models as the source of corrections for LLMs‚Äô predictions. Tam et al. (2022)
instead consider effective exploration in 3D environments, showing that pretrained representations
from vision-language models (Radford et al., 2021) form a semantically meaningful state space
for curiosity-driven intrinsic rewards. Mahmoudieh et al. (2022) also connect reward speciÔ¨Åcation
to vision-language supervision, introducing a framework that leverages text descriptions and pixel
observations to produce reward signals.
5.2.2 P OLICY INITIALIZATION
Recent advances bridge the gap between reinforcement learning and sequential modeling (Chen
et al., 2021a; Janner et al., 2021; Zheng et al., 2022; Furuta et al., 2022), opening up opportunities
to borrow sequential models to RL tasks. Despite the clear distinction, pretrained LLMs could
arguably provide reusable knowledge via weight initialization. Reid et al. (2022) investigate whether
pretrained LLMs can provide good weight initialization for Transformer-based ofÔ¨Çine RL models,
and conclude with very positive results. Li et al. (2022b) also demonstrate that pretrained LLMs
can be used to initialize policies and facilitate behavior cloning as well as online reinforcement
learning for embodied tasks. They also suggest using sequential input representations and Ô¨Åntuning
the pretrained weights for better generalization.
5.3 Multi-task and Multi-modal Pretraining
With recent advances in building powerful sequence models to handle different modalities and
tasks (Lu et al., 2019; Jaegle et al., 2022; Wang et al., 2022), the wave of using large general-propose
models (Bommasani et al., 2021) has been sweeping through the Ô¨Åeld of supervised learning. The
key ingredient is Transformer (Vaswani et al., 2017), a highly capable neural architecture built on
the self-attention mechanism (Bahdanau et al., 2015) that excels at capturing long-range depen-
dencies in sequential data. Due to its strong generality where various tasks in different domains
can be formulated as sequence modeling, Transformer is believed to be a uniÔ¨Åed architecture for
developing foundation models (Bommasani et al., 2021).
16

--- PAGE 17 ---
Recently, Transformer-based architectures have also been extended to the Ô¨Åeld of ofÔ¨Çine RL (Chen
et al., 2021a; Janner et al., 2021) and then online RL (Zheng et al., 2022), in which the agent
is trained auto-regressively in a supervised manner via likelihood maximization. This opens up
the possibility of replicating previous success achieved with Transformer in the Ô¨Åeld of supervised
learning. SpeciÔ¨Åcally, it is expected that by combining large-scale data, open-ended objectives, and
Transformer-based architectures, we are ready to build general-purpose decision-making agents that
are capable of various downstream tasks in different environments.
Pioneering work in this direction is Gato (Reed et al., 2022), a generalist agent trained on various
tasks from control environments, vision datasets, and language datasets in a supervised manner. To
handle multi-task and multi-modal data, Gato uses demonstrations as prompt sequences (Wei et al.,
2022) at inference time. Lee et al. (2022) extend Decision Transformer (Chen et al., 2021a) to train a
generalist agent called Multi-Game DT that can play 41 Atari games simultaneously. Both Gato and
Multi-Game DT show impressive scaling law properties. Fan et al. (2022) make use of large-scale
multi-modal data from YouTube videos, Wikipedia pages, and Reddit posts to train an agent able
to solve various tasks in Minecraft. To provide dense reward signals, a pretrained vision-language
model based on CLIP (Radford et al., 2021) is introduced as a proxy of human evaluation.
5.4 Challenges & Future Directions
In spite of some promising results, how generalist models beneÔ¨Åt from multi-modal and multi-
task data remains unclear. More speciÔ¨Åcally, these models might suffer from detrimental gradient
interference (Yu et al., 2020) between modalities and tasks due to the incurred optimization chal-
lenges. To mitigate this issue, it is desired to incorporate more analysis tools for optimization
landscapes (Goodfellow & Vinyals, 2015) and gradients (Yu et al., 2020) to tease out the precise
principles.
Another compelling direction is to compose separate pretrained models (e.g., GPT-3 (Brown
et al., 2020) and CLIP (Radford et al., 2021)) together. By leveraging expert knowledge from
different models, this kind of framework can solve complex multi-modal tasks (Li et al., 2022a).
6. Task Adaptation
While pretraining on unsupervised experiences can result in rich transferable knowledge, it remains
challenging to adapt the knowledge to downstream tasks in which reward signals are exposed. In
this section, we discuss brieÔ¨Çy various considerations for downstream task adaptation. We limit
the scope to online adaptation, while adaptation with ofÔ¨Çine RL or imitation learning is also feasi-
ble (Yang & Nachum, 2021).
In online task adaptation, a pretrained model is given, which can be composed of various com-
ponents such as policies and representations, together with a target MDP that can interact with.
Given that pretraining could result in different forms of knowledge, it brings difÔ¨Åculties to design-
ing principled adaptation techniques. Nevertheless, considerable efforts have been made to study
this aspect.
6.1 Representation Transfer
In the Ô¨Åeld of supervised learning, recent advances (Devlin et al., 2019; He et al., 2020; Chen
et al., 2020) have demonstrated that good representations can be pretrained on large-scale unlabeled
17

--- PAGE 18 ---
dataset, as evidenced by their impressive downstream performances. The most common practice is
to freeze the weights of the pretrained feature encoder and train a randomly initialized task-speciÔ¨Åc
network on top of that during adaptation. The success of this paradigm is essentially based on the
promise that related tasks can usually be solved using similar representations.
For RL, it has been shown that directly reusing pretrained task-agnostic representations can
signiÔ¨Åcantly improve sample efÔ¨Åciency on downstream tasks. For instance, Schwarzer et al. (2021b)
conduct experiments on the Atari 100K benchmark and Ô¨Ånd that frozen representations pretrained
on exploratory ofÔ¨Çine data already form a basis of data-efÔ¨Åcient RL. This success also extends to the
cases where domain discrepancy exists between upstream and downstream tasks (Shah & Kumar,
2021; Parisi et al., 2022). However, the issue of negative transfer in the face of domain discrepancy
might be exacerbated for RL due to its complexity (Shah & Kumar, 2021).
When adapting to tasks that have the same environment dynamics as that of the upstream task(s),
successor features (Barreto et al., 2017) can be a powerful tool to aid task adaptation. The frame-
work of successor features is based on the following decomposition of reward functions:
r 
s;a;s0
= 
s;a;s0>w; (3)
where(s;a;s0)2Rdrepresents features of transition (s;a;s0)andw2Rdencodes reward-
specifying weights. This leads to a representation of the value function that decouples the dynamics
of the environment from the rewards:
Q(s;a) =Est=s;at=a"1X
i=ti t 
si+1;ai+1;s0
i+1#>
w= (s;a)>w;
where we call  (s;a)thesuccessor features of(s;a)under. Intuitively,  summarizes the
dynamics induced by and has been studied within the framework of online pretraining (Hansen
et al., 2020; Liu & Abbeel, 2021a) by combining with skill discovery approaches to implicitly learn
controllable successor features  (s;a). Given a learned  (s;a), the problem of task adaptation
reduces to a linear regression derived from Equation 3.
6.2 Policy Transfer
A compelling alternative for task adaptation is to transfer learned behaviors. As discussed in pre-
vious sections, existing work has explored how to pretrain primitive skills that can be reused to
face new tasks or a single exploratory policy that facilitates exploration at the beginning of task
adaptation. The differences in pretrained behaviors result in different adaptation strategies.
To achieve high rewards on the downstream task with skill-conditioned policy (ajs;z), a
straightforward strategy is to simply choose the skill zwith the best outcome and further enhance
it with Ô¨Ånetuning. However, a single best-performing skill can not fulÔ¨Åll its potential. To better
combine diverse skills for task solving, one can view them from the perspective of hierarchical
RL(Barto & Mahadevan, 2003; Kulkarni et al., 2016). In hierarchical RL, the decision-making
task is typically decomposed into a two-level hierarchy, where a meta-controller (zjs)decides
which low-level policy to use for task solving, depending on the current state. This hierarchical
scheme is agnostic to how the low-level policies are learned. Therefore, it is sufÔ¨Åcient to train a
meta-controller on top of the discovered skills, which has been proven effective for few-shot adap-
tation (Hakhamaneshi et al., 2022) and zero-shot adaptation (Sharma et al., 2020).
18

--- PAGE 19 ---
Exploratory policies, as another form of prior knowledge, beneÔ¨Åt downstream tasks in a different
way. Due to the importance of exploration, exploratory policies can provide good initialization
for the agent to gather diverse experiences and reach high-rewarding states. For example, Campos
et al. (2021) validate the effectiveness of transferring exploratory policies trained by curiosity-driven
approaches, in particular for domains that require structured exploration.
While it is always feasible to Ô¨Ånetune pretrained policies, considerations should be taken in
order to prevent catastrophic forgetting when learning in the downstream task. Catastrophic for-
getting refers to the tendency of neural networks to disregard their previously obtained knowledge
when new information is acquired. To mitigate this issue, one might apply knowledge distillation-
like regularization together with RL objectives (Schmitt et al., 2018):
LKD=H(^(ajs)k(ajs));
whereHis cross entropy and ^is the teacher policy. We refer the reader to Khetarpal et al. (2020)
for more discussions on catastrophic forgetting in reinforcement learning.
6.3 Challenges & Future Directions
Parameter EfÔ¨Åciency. Despite that existing pretrained models for RL have much fewer parame-
ters as compared with those in the Ô¨Åeld of supervised learning, the issue of parameter efÔ¨Åciency is
still important with the ever-increasing number of model parameters. More concretely, it is desired
to design parameter-efÔ¨Åcient transfer learning that updates only a small fraction of parameters while
keeping most of the pretrained parameters intact. It has been actively studied in natural language
processing (He et al., 2022) with solutions like adding small neural modules as adapters (Houlsby
et al., 2019) and prepending learnable preÔ¨Åx tokens as soft prompts (Lester et al., 2021). Built on
these techniques, several efforts have been made to enable parameter-efÔ¨Åcient transfer with prompt-
ing (Xu et al., 2022; Reed et al., 2022), which we believe has a large room to improve with tailored
methods.
Domain adaptation. In this section, we mainly consider task adaptation where unseen tasks are
given in the same environment. A more challenging but practical scenario is domain adaptation. In
domain adaptation, there exist environmental shifts between the upstream and downstream tasks.
Despite that these environmental shifts are commonly seen in real-world applications, it remains
a challenging problem to transfer across different domains (Eysenbach et al., 2021; Huang et al.,
2022a). However, we believe that this direction will rapidly evolve by bringing related techniques
from supervised learning to reinforcement learning.
Continually-developed models. For practical applications, we can take a step forward and con-
sider building large pretrained models continually to support added features (e.g., a modiÔ¨Åed action
space, more powerful architectures, etc). While such consideration was already underway during
the development of large-scale RL models (Berner et al., 2019), it requires a more principled way of
combining updates into RL models. We refer the reader to recent work in this direction in the Ô¨Åeld
of supervised learning (Raffel, 2021) and reinforcement learning (Campos et al., 2021; Agarwal
et al., 2022).
19

--- PAGE 20 ---
7. Conclusions and Future Perspectives
In this section, we conclude this survey and highlight several future directions which we believe
will be important topics for future work.
This paper introduces pretraining in deep RL by discussing recent trends to obtain general prior
knowledge for decision-making. In contrast to its supervised learning counterpart, pretraining faces
a variety of challenges unique to RL. In this survey, we present several promising research directions
to tackle these challenges and we believe this Ô¨Åeld will evolve rapidly in the coming years.
There are still several open questions that are important and remain to be addressed.
Benchmarks and evaluation metrics. Evaluation serves as a means for comparing various meth-
ods and driving further improvements. In the Ô¨Åeld of natural language processing, GLUE (Wang
et al., 2019) is a widely used benchmark to evaluate the performance of models across various nat-
ural language understanding tasks. Recently, there has been a surge of research on improving eval-
uation for RL in terms of evaluation metrics (Agarwal et al., 2021) and benchmark datasets (Cobbe
et al., 2020). To the best of our knowledge, URLB (Laskin et al., 2020) is the only benchmark for
pretraining in deep RL. It presents a uniÔ¨Åed evaluation protocol for online pretraining based on the
DeepMind control suite (Tassa et al., 2018). However, a principled evaluation framework for ofÔ¨Çine
pretraining and generalist pretraining is still missing. We expect existing ofÔ¨Çine RL benchmarks
like D4RL (Fu et al., 2020) and RL Unplugged (Gulcehre et al., 2020) can serve as the basis for
developing pretraining benchmarks, but more challenging tasks should better illustrate the value of
pretraining.
Architecture. As discussed in previous sections, there has been a surge of leveraging large trans-
formers for RL tasks. We expect other recent advances in model architecture can bring more im-
provements. For example, Mustafa et al. (2022) learn large sparse models with a mixture of experts
that simultaneously handle images and text with modality-agnostic routing. This has the promise
to solve complex tasks at scale. Besides, one can also rethink existing architectures that have the
potential to support large-scale pretraining (e.g., Progress Neural Networks (Rusu et al., 2016)).
Multi-agent RL. Multi-agent RL (Zhang et al., 2019) is an important sub-Ô¨Åeld of RL. Extending
existing pretraining techniques to the multi-agent scenario is non-trivial. Multi-agent RL typically
requires socially desirable behaviors (Ndousse et al., 2021) and representations (Li et al., 2021).
To the best of our knowledge, Meng et al. (2021) present the only effort in pretraining multi-agent
RL with supervision. How to enable unsupervised pretraining for multi-agent RL remains unclear,
which we believe is a promising research direction.
Theoretical results. For RL, the signiÔ¨Åcant gap between theory and practice has been a long-
standing problem, and bringing large-scale pretraining to RL may exacerbate this even further. For-
tunately, recent theoretical studies have made efforts in terms of representation transfer (Agarwal
et al., 2022) and skill-conditioned policy transfer (Eysenbach et al., 2022). Increasing focus on the-
oretical results is likely to have profound effects on the development of more advanced pretraining
methods.‚Äò
20

--- PAGE 21 ---
References
Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici, G., Varadarajan, B., & Vijaya-
narasimhan, S. (2016). Youtube-8m: A large-scale video classiÔ¨Åcation benchmark. CoRR ,
abs/1609.08675 .http://arxiv.org/abs/1609.08675 .
Achiam, J., Edwards, H., Amodei, D., & Abbeel, P. (2018). Variational option discovery algorithms.
CoRR ,abs/1807.10299 .http://arxiv.org/abs/1807.10299 .
Agakov, D. B. F. (2004). The im algorithm: a variational approach to information maximization.
Advances in neural information processing systems ,16(320), 201.
Agarwal, A., Song, Y ., Sun, W., Wang, K., Wang, M., & Zhang, X. (2022). Provable
beneÔ¨Åts of representational transfer in reinforcement learning. CoRR ,abs/2205.14571 .
DOI: 10.48550/arXiv.2205.14571.
Agarwal, R., Machado, M. C., Castro, P. S., & Bellemare, M. G. (2021). Contrastive behavioral
similarity embeddings for generalization in reinforcement learning. In 9th International Con-
ference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 .
OpenReview.net. https://openreview.net/forum?id=qda7-sVg84 .
Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A., & Bellemare, M. G. (2022). Rein-
carnating reinforcement learning: Reusing prior computation to accelerate progress..
DOI: 10.48550/ARXIV .2206.01626.
Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., & Bellemare, M. (2021).
Deep reinforcement learning at the edge of the statistical precipice. In Ran-
zato, M., Beygelzimer, A., Dauphin, Y ., Liang, P., & Vaughan, J. W. (Eds.), Ad-
vances in Neural Information Processing Systems , V ol. 34, pp. 29304‚Äì29320. Cur-
ran Associates, Inc. https://proceedings.neurips.cc/paper/2021/file/
f514cec81cb148559cf475e7426eed5e-Paper.pdf .
Ahn, M., Brohan, A., Brown, N., Chebotar, Y ., Cortes, O., David, B., Finn, C., Gopalakrishnan,
K., Hausman, K., Herzog, A., et al. (2022). Do as i can, not as i say: Grounding language
in robotic affordances. ArXiv preprint ,abs/2204.01691 .https://arxiv.org/abs/
2204.01691 .
Ajay, A., Kumar, A., Agrawal, P., Levine, S., & Nachum, O. (2021). OPAL: ofÔ¨Çine primitive
discovery for accelerating ofÔ¨Çine reinforcement learning. In 9th International Conference
on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenRe-
view.net. https://openreview.net/forum?id=V69LGwJ0lIN .
Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A., Paino, A., Plap-
pert, M., Powell, G., Ribas, R., et al. (2019). Solving rubik‚Äôs cube with a robot hand. ArXiv
preprint ,abs/1910.07113 .https://arxiv.org/abs/1910.07113 .
Allen, C., Parikh, N., Gottesman, O., & Konidaris, G. (2021). Learning markov state abstractions
for deep reinforcement learning. Advances in Neural Information Processing Systems ,34,
8229‚Äì8241.
Anand, A., Racah, E., Ozair, S., Bengio, Y ., C ÀÜot¬¥e, M., & Hjelm, R. D. (2019). Unsu-
pervised state representation learning in atari. In Wallach, H. M., Larochelle, H.,
Beygelzimer, A., d‚ÄôAlch ¬¥e-Buc, F., Fox, E. B., & Garnett, R. (Eds.), Advances in
21

--- PAGE 22 ---
Neural Information Processing Systems 32: Annual Conference on Neural Informa-
tion Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada , pp. 8766‚Äì8779. https://proceedings.neurips.cc/paper/2019/
hash/6fb52e71b837628ac16539c1ff911667-Abstract.html .
Argall, B. D., Chernova, S., Veloso, M., & Browning, B. (2009). A survey of robot
learning from demonstration. Robotics and Autonomous Systems ,57(5), 469‚Äì483.
DOI: https://doi.org/10.1016/j.robot.2008.10.024.
Badia, A. P., Sprechmann, P., Vitvitskyi, A., Guo, D., Piot, B., Kapturowski, S., Tieleman, O.,
Arjovsky, M., Pritzel, A., Bolt, A., & Blundell, C. (2020). Never give up: Learning di-
rected exploration strategies. In 8th International Conference on Learning Representa-
tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net. https:
//openreview.net/forum?id=Sye57xStvB .
Bahdanau, D., Cho, K., & Bengio, Y . (2015). Neural machine translation by jointly learning to align
and translate. In Bengio, Y ., & LeCun, Y . (Eds.), 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Pro-
ceedings .http://arxiv.org/abs/1409.0473 .
Baker, B., Akkaya, I., Zhokhov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton, B., Sampedro, R.,
& Clune, J. (2022). Video pretraining (vpt): Learning to act by watching unlabeled online
videos. ArXiv preprint ,abs/2206.11795 .https://arxiv.org/abs/2206.11795 .
Barreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T., Silver, D., & van Hasselt, H.
(2017). Successor features for transfer in reinforcement learning. In Guyon, I., von
Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V . N., & Gar-
nett, R. (Eds.), Advances in Neural Information Processing Systems 30: Annual Conference
on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
USA, pp. 4055‚Äì4065. https://proceedings.neurips.cc/paper/2017/hash/
350db081a661525235354dd3e19b8c05-Abstract.html .
Barto, A. G., & Mahadevan, S. (2003). Recent advances in hierarchical reinforcement learning.
Discret. Event Dyn. Syst. ,13(1-2), 41‚Äì77. DOI: 10.1023/A:1022140919877.
Baumli, K., Warde-Farley, D., Hansen, S., & Mnih, V . (2021). Relative variational intrinsic con-
trol. Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence ,35(8), 6732‚Äì6740.
DOI: 10.1609/aaai.v35i8.16832.
Bellemare, M. G., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., & Munos, R.
(2016). Unifying count-based exploration and intrinsic motivation. In Lee,
D. D., Sugiyama, M., von Luxburg, U., Guyon, I., & Garnett, R. (Eds.), Ad-
vances in Neural Information Processing Systems 29: Annual Conference on Neu-
ral Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain ,
pp. 1471‚Äì1479. https://proceedings.neurips.cc/paper/2016/hash/
afda332245e2af431fb7b672a68b659d-Abstract.html .
Berner, C., Brockman, G., Chan, B., Cheung, V ., Debiak, P., Dennison, C., Farhi, D., Fischer,
Q., Hashme, S., Hesse, C., J ¬¥ozefowicz, R., Gray, S., Olsson, C., Pachocki, J., Petrov, M.,
de Oliveira Pinto, H. P., Raiman, J., Salimans, T., Schlatter, J., Schneider, J., Sidor, S.,
Sutskever, I., Tang, J., Wolski, F., & Zhang, S. (2019). Dota 2 with large scale deep rein-
forcement learning. CoRR ,abs/1912.06680 .http://arxiv.org/abs/1912.06680 .
22

--- PAGE 23 ---
Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S.,
Bohg, J., Bosselut, A., Brunskill, E., et al. (2021). On the opportunities and risks of foundation
models. ArXiv preprint ,abs/2108.07258 .https://arxiv.org/abs/2108.07258 .
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam,
P., Sastry, G., Askell, A., Agarwal, S., Herbert-V oss, A., Krueger, G., Henighan, T., Child,
R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin,
M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., &
Amodei, D. (2020). Language models are few-shot learners. In Larochelle, H., Ranzato,
M., Hadsell, R., Balcan, M., & Lin, H. (Eds.), Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual .https://proceedings.neurips.cc/paper/
2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html .
Burda, Y ., Edwards, H., Pathak, D., Storkey, A. J., Darrell, T., & Efros, A. A. (2019a). Large-
scale study of curiosity-driven learning. In 7th International Conference on Learning Repre-
sentations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net. https:
//openreview.net/forum?id=rJNwDjAqYX .
Burda, Y ., Edwards, H., Storkey, A. J., & Klimov, O. (2019b). Exploration by random network dis-
tillation. In 7th International Conference on Learning Representations, ICLR 2019, New Or-
leans, LA, USA, May 6-9, 2019 . OpenReview.net. https://openreview.net/forum?
id=H1lJJnR5Ym .
Campos, V ., Sprechmann, P., Hansen, S. S., Barreto, A., Kapturowski, S., Vitvitskyi, A., Badia,
A. P., & Blundell, C. (2021). Beyond Ô¨Åne-tuning: Transferring behavior in reinforcement
learning. In ICML 2021 Workshop on Unsupervised Reinforcement Learning .https://
openreview.net/forum?id=4NUhTHom2HZ .
Campos, V ., Trott, A., Xiong, C., Socher, R., Gir ¬¥o-i-Nieto, X., & Torres, J. (2020). Explore,
discover and learn: Unsupervised discovery of state-covering skills. In Proceedings of the
37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Vir-
tual Event , V ol. 119 of Proceedings of Machine Learning Research , pp. 1317‚Äì1327. PMLR.
http://proceedings.mlr.press/v119/campos20a.html .
Castro, P. S., & Precup, D. (2010). Using bisimulation for policy transfer in mdps. In Fox,
M., & Poole, D. (Eds.), Proceedings of the Twenty-Fourth AAAI Conference on ArtiÔ¨Åcial
Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010 . AAAI Press. http:
//www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1907 .
Chebotar, Y ., Hausman, K., Lu, Y ., Xiao, T., Kalashnikov, D., Varley, J., Irpan, A., Eysenbach,
B., Julian, R., Finn, C., & Levine, S. (2021). Actionable models: Unsupervised ofÔ¨Çine re-
inforcement learning of robotic skills. In Meila, M., & Zhang, T. (Eds.), Proceedings of
the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Vir-
tual Event , V ol. 139 of Proceedings of Machine Learning Research , pp. 1518‚Äì1528. PMLR.
http://proceedings.mlr.press/v139/chebotar21a.html .
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., &
Mordatch, I. (2021a). Decision transformer: Reinforcement learning via sequence modeling.
Advances in neural information processing systems ,34, 15084‚Äì15097.
23

--- PAGE 24 ---
Chen, S., Zhu, M., Ye, D., Zhang, W., Fu, Q., & Yang, W. (2021b). Which heroes to pick? learning
to draft in moba games with neural networks and tree search. IEEE Transactions on Games ,
13(4), 410‚Äì421.
Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. E. (2020). A simple framework for contrastive
learning of visual representations. In Proceedings of the 37th International Conference on
Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , V ol. 119 of Proceedings
of Machine Learning Research , pp. 1597‚Äì1607. PMLR. http://proceedings.mlr.
press/v119/chen20j.html .
Choi, J., Sharma, A., Lee, H., Levine, S., & Gu, S. S. (2021). Variational empowerment as repre-
sentation learning for goal-conditioned reinforcement learning. In Meila, M., & Zhang, T.
(Eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021,
18-24 July 2021, Virtual Event , V ol. 139 of Proceedings of Machine Learning Research , pp.
1953‚Äì1963. PMLR. http://proceedings.mlr.press/v139/choi21b.html .
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung,
H. W., Sutton, C., Gehrmann, S., et al. (2022). Palm: Scaling language modeling with path-
ways. ArXiv preprint ,abs/2204.02311 .https://arxiv.org/abs/2204.02311 .
Christiano, P. F., Shah, Z., Mordatch, I., Schneider, J., Blackwell, T., Tobin, J., Abbeel, P., &
Zaremba, W. (2016). Transfer from simulation to real world through learning deep inverse
dynamics model. CoRR ,abs/1610.03518 .http://arxiv.org/abs/1610.03518 .
Cobbe, K., Hesse, C., Hilton, J., & Schulman, J. (2020). Leveraging procedural generation to bench-
mark reinforcement learning. In III, H. D., & Singh, A. (Eds.), Proceedings of the 37th
International Conference on Machine Learning , V ol. 119 of Proceedings of Machine Learn-
ing Research , pp. 2048‚Äì2056. PMLR. https://proceedings.mlr.press/v119/
cobbe20a.html .
Codevilla, F., Santana, E., Lopez, A. M., & Gaidon, A. (2019). Exploring the limitations of behavior
cloning for autonomous driving. In Proceedings of the IEEE/CVF International Conference
on Computer Vision (ICCV) .
Colas, C., Karch, T., Sigaud, O., & Oudeyer, P.-Y . (2022). Autotelic agents with intrinsically
motivated goal-conditioned reinforcement learning: A short survey. J. Artif. Int. Res. ,74.
DOI: 10.1613/jair.1.13554.
Dance, C. R., Perez, J., & Cachet, T. (2021). Demonstration-conditioned reinforcement learning for
few-shot imitation. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th International
Conference on Machine Learning , V ol. 139 of Proceedings of Machine Learning Research ,
pp. 2376‚Äì2387. PMLR. https://proceedings.mlr.press/v139/dance21a.
html .
Deng, J., Dong, W., Socher, R., Li, L., Li, K., & Li, F. (2009). Imagenet: A large-scale hierarchical
image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pat-
tern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA , pp. 248‚Äì255. IEEE
Computer Society. DOI: 10.1109/CVPR.2009.5206848.
Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirec-
tional transformers for language understanding. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
24

--- PAGE 25 ---
Technologies, Volume 1 (Long and Short Papers) , pp. 4171‚Äì4186, Minneapolis, Minnesota.
Association for Computational Linguistics. DOI: 10.18653/v1/N19-1423.
Dinh, L., Sohl-Dickstein, J., & Bengio, S. (2017). Density estimation using real NVP. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings . OpenReview.net. https://openreview.
net/forum?id=HkpbnH9lx .
Dittadi, A., Tr ¬®auble, F., Locatello, F., Wuthrich, M., Agrawal, V ., Winther, O., Bauer, S., &
Sch¬®olkopf, B. (2021). On the transfer of disentangled representations in realistic settings.
In9th International Conference on Learning Representations, ICLR 2021, Virtual Event,
Austria, May 3-7, 2021 . OpenReview.net. https://openreview.net/forum?id=
8VXvj1QNRl1 .
Du, Y ., Gan, C., & Isola, P. (2021). Curious representation learning for embodied intelligence.
InProceedings of the IEEE/CVF International Conference on Computer Vision , pp. 10408‚Äì
10417.
Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., & Clune, J. (2021). First return, then explore.
Nature ,590(7847), 580‚Äì586.
Efroni, Y ., Misra, D., Krishnamurthy, A., Agarwal, A., & Langford, J. (2021). Provable RL with
exogenous distractors via multistep inverse dynamics. CoRR ,abs/2110.08847 .https:
//arxiv.org/abs/2110.08847 .
Eysenbach, B., Chaudhari, S., Asawa, S., Levine, S., & Salakhutdinov, R. (2021). Off-dynamics
reinforcement learning: Training for transfer with domain classiÔ¨Åers. In 9th International
Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 .
OpenReview.net. https://openreview.net/forum?id=eqBwg3AcIAK .
Eysenbach, B., Gupta, A., Ibarz, J., & Levine, S. (2019). Diversity is all you need: Learning
skills without a reward function. In 7th International Conference on Learning Represen-
tations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net. https:
//openreview.net/forum?id=SJx63jRqFm .
Eysenbach, B., Salakhutdinov, R., & Levine, S. (2022). The information geometry of unsupervised
reinforcement learning. In The Tenth International Conference on Learning Representations,
ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. https://openreview.
net/forum?id=3wU2UX0voE .
Fan, L., Wang, G., Jiang, Y ., Mandlekar, A., Yang, Y ., Zhu, H., Tang, A., Huang, D.-A., Zhu, Y .,
& Anandkumar, A. (2022). Minedojo: Building open-ended embodied agents with internet-
scale knowledge. ArXiv preprint ,abs/2206.08853 .https://arxiv.org/abs/2206.
08853 .
Ferns, N., Panangaden, P., & Precup, D. (2004). Metrics for Ô¨Ånite markov decision processes.. In
UAI, V ol. 4, pp. 162‚Äì169.
Florence, P., Lynch, C., Zeng, A., Ramirez, O. A., Wahid, A., Downs, L., Wong, A., Lee, J., Mor-
datch, I., & Tompson, J. (2022). Implicit behavioral cloning. In Conference on Robot Learn-
ing, pp. 158‚Äì168. PMLR.
Frank, M., & Wolfe, P. (1956). An algorithm for quadratic programming. Naval Research Logistics
Quarterly ,3(1-2), 95‚Äì110. DOI: https://doi.org/10.1002/nav.3800030109.
25

--- PAGE 26 ---
Fu, J., Kumar, A., Nachum, O., Tucker, G., & Levine, S. (2020). D4RL: datasets for deep data-driven
reinforcement learning. CoRR ,abs/2004.07219 .https://arxiv.org/abs/2004.
07219 .
Furuta, H., Matsuo, Y ., & Gu, S. S. (2022). Generalized decision transformer for ofÔ¨Çine hindsight
information matching. In International Conference on Learning Representations .https:
//openreview.net/forum?id=CAjxVodl_v .
Gehring, J., Synnaeve, G., Krause, A., & Usunier, N. (2021). Hierarchical skills for efÔ¨Åcient explo-
ration. Advances in Neural Information Processing Systems ,34, 11553‚Äì11564.
Gelada, C., Kumar, S., Buckman, J., Nachum, O., & Bellemare, M. G. (2019). Deepmdp: Learning
continuous latent space models for representation learning. In Chaudhuri, K., & Salakhut-
dinov, R. (Eds.), Proceedings of the 36th International Conference on Machine Learning,
ICML 2019, 9-15 June 2019, Long Beach, California, USA , V ol. 97 of Proceedings of
Machine Learning Research , pp. 2170‚Äì2179. PMLR. http://proceedings.mlr.
press/v97/gelada19a.html .
Givan, R., Dean, T., & Greig, M. (2003). Equivalence notions and model minimization in markov
decision processes. ArtiÔ¨Åcial Intelligence ,147(1-2), 163‚Äì223.
Goodfellow, I. J., & Vinyals, O. (2015). Qualitatively characterizing neural network optimization
problems. In Bengio, Y ., & LeCun, Y . (Eds.), 3rd International Conference on Learning
Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Pro-
ceedings .http://arxiv.org/abs/1412.6544 .
Gregor, K., Rezende, D. J., & Wierstra, D. (2016). Variational intrinsic control. ArXiv preprint ,
abs/1611.07507 .https://arxiv.org/abs/1611.07507 .
Grill, J., Strub, F., Altch ¬¥e, F., Tallec, C., Richemond, P. H., Buchatskaya, E., Doer-
sch, C., Pires, B. ¬¥A., Guo, Z., Azar, M. G., Piot, B., Kavukcuoglu, K., Munos,
R., & Valko, M. (2020). Bootstrap your own latent - A new approach to self-
supervised learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., &
Lin, H. (Eds.), Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-
12, 2020, virtual .https://proceedings.neurips.cc/paper/2020/hash/
f3ada80d5c4ee70142b17b8192b2958e-Abstract.html .
Gulcehre, C., Wang, Z., Novikov, A., Paine, T., G ¬¥omez, S., Zolna, K., Agarwal, R., Merel,
J. S., Mankowitz, D. J., Paduraru, C., Dulac-Arnold, G., Li, J., Norouzi, M., Hoffman, M.,
Heess, N., & de Freitas, N. (2020). Rl unplugged: A suite of benchmarks for ofÔ¨Çine re-
inforcement learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., & Lin, H.
(Eds.), Advances in Neural Information Processing Systems , V ol. 33, pp. 7248‚Äì7259. Cur-
ran Associates, Inc. https://proceedings.neurips.cc/paper/2020/file/
51200d29d1fc15f5a71c1dab4bb54f7c-Paper.pdf .
Guo, Z. D., Azar, M. G., Saade, A., Thakoor, S., Piot, B., Pires, B. A., Valko, M., Mesnard,
T., Lattimore, T., & Munos, R. (2021). Geometric entropic exploration. ArXiv preprint ,
abs/2101.02055 .https://arxiv.org/abs/2101.02055 .
Gupta, A., Kumar, V ., Lynch, C., Levine, S., & Hausman, K. (2020). Relay policy learning:
Solving long-horizon tasks via imitation and reinforcement learning. In Kaelbling, L. P.,
26

--- PAGE 27 ---
Kragic, D., & Sugiura, K. (Eds.), Proceedings of the Conference on Robot Learning , V ol.
100 of Proceedings of Machine Learning Research , pp. 1025‚Äì1037. PMLR. https:
//proceedings.mlr.press/v100/gupta20a.html .
Gutmann, M., & Hyv ¬®arinen, A. (2010). Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Teh, Y . W., & Titterington, M. (Eds.), Proceedings
of the Thirteenth International Conference on ArtiÔ¨Åcial Intelligence and Statistics , V ol. 9
ofProceedings of Machine Learning Research , pp. 297‚Äì304, Chia Laguna Resort, Sardinia,
Italy. PMLR. https://proceedings.mlr.press/v9/gutmann10a.html .
Ha, D., & Schmidhuber, J. (2018). World models. CoRR ,abs/1803.10122 .http://arxiv.
org/abs/1803.10122 .
Haber, N., Mrowca, D., Wang, S., Li, F., & Yamins, D. L. (2018). Learning to
play with intrinsically-motivated, self-aware agents. In Bengio, S., Wallach, H. M.,
Larochelle, H., Grauman, K., Cesa-Bianchi, N., & Garnett, R. (Eds.), Advances in
Neural Information Processing Systems 31: Annual Conference on Neural Informa-
tion Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr ¬¥eal, Canada ,
pp. 8398‚Äì8409. https://proceedings.neurips.cc/paper/2018/hash/
71e63ef5b7249cfc60852f0e0f5bf4c8-Abstract.html .
Hakhamaneshi, K., Zhao, R., Zhan, A., Abbeel, P., & Laskin, M. (2022). Hierarchical few-shot imi-
tation with skill transition models. In International Conference on Learning Representations .
https://openreview.net/forum?id=xKZ4K0lTj_ .
Hansen, S., Dabney, W., Barreto, A., Warde-Farley, D., de Wiele, T. V ., & Mnih, V . (2020). Fast
task inference with variational intrinsic successor features. In 8th International Conference
on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . Open-
Review.net. https://openreview.net/forum?id=BJeAHkrYDS .
Hazan, E., Kakade, S. M., Singh, K., & Soest, A. V . (2019). Provably efÔ¨Åcient maximum entropy
exploration. In Chaudhuri, K., & Salakhutdinov, R. (Eds.), Proceedings of the 36th Inter-
national Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, Cali-
fornia, USA , V ol. 97 of Proceedings of Machine Learning Research , pp. 2681‚Äì2691. PMLR.
http://proceedings.mlr.press/v97/hazan19a.html .
He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., & Neubig, G. (2022). Towards a uniÔ¨Åed view of
parameter-efÔ¨Åcient transfer learning. In The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. https:
//openreview.net/forum?id=0RDcd5Axok .
He, K., Fan, H., Wu, Y ., Xie, S., & Girshick, R. B. (2020). Momentum contrast for unsupervised
visual representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020 , pp. 9726‚Äì9735. IEEE.
DOI: 10.1109/CVPR42600.2020.00975.
Hendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan, R., & Song, D. (2020a). Pretrained
transformers improve out-of-distribution robustness. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics , pp. 2744‚Äì2751, Online. Association for
Computational Linguistics. DOI: 10.18653/v1/2020.acl-main.244.
27

--- PAGE 28 ---
Hendrycks, D., Mu, N., Cubuk, E. D., Zoph, B., Gilmer, J., & Lakshminarayanan, B. (2020b). Aug-
mix: A simple data processing method to improve robustness and uncertainty. In 8th Interna-
tional Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-
30, 2020 . OpenReview.net. https://openreview.net/forum?id=S1gmrxHFvB .
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan,
M., & Gelly, S. (2019). Parameter-efÔ¨Åcient transfer learning for NLP. In Chaudhuri, K.,
& Salakhutdinov, R. (Eds.), Proceedings of the 36th International Conference on Machine
Learning , V ol. 97 of Proceedings of Machine Learning Research , pp. 2790‚Äì2799. PMLR.
https://proceedings.mlr.press/v97/houlsby19a.html .
Houthooft, R., Chen, X., Duan, Y ., Schulman, J., Turck, F. D., & Abbeel, P. (2016). VIME: vari-
ational information maximizing exploration. In Lee, D. D., Sugiyama, M., von Luxburg,
U., Guyon, I., & Garnett, R. (Eds.), Advances in Neural Information Processing Systems 29:
Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016,
Barcelona, Spain , pp. 1109‚Äì1117. https://proceedings.neurips.cc/paper/
2016/hash/abd815286ba1007abfbb8415b83ae2cf-Abstract.html .
Huang, B., Feng, F., Lu, C., Magliacane, S., & Zhang, K. (2022a). Adarl: What, where, and how to
adapt in transfer reinforcement learning. In The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. https:
//openreview.net/forum?id=8H5bpVwvt5 .
Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I.,
Chebotar, Y ., et al. (2022b). Inner monologue: Embodied reasoning through planning with
language models. ArXiv preprint ,abs/2207.05608 .https://arxiv.org/abs/2207.
05608 .
H¬®ullermeier, E., & Waegeman, W. (2021). Aleatoric and epistemic uncertainty in machine learning:
An introduction to concepts and methods. Machine Learning ,110(3), 457‚Äì506.
Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C., Ionescu, C., Ding, D., Koppula, S., Zoran,
D., Brock, A., Shelhamer, E., Henaff, O. J., Botvinick, M., Zisserman, A., Vinyals, O., &
Carreira, J. (2022). Perceiver IO: A general architecture for structured inputs & outputs. In
International Conference on Learning Representations .https://openreview.net/
forum?id=fILj7WpI-g .
Janner, M., Li, Q., & Levine, S. (2021). OfÔ¨Çine reinforcement learning as one big sequence model-
ing problem. Advances in neural information processing systems ,34, 1273‚Äì1286.
Janny, S., Baradel, F., Neverova, N., Nadri, M., Mori, G., & Wolf, C. (2022). Filtered-cophy:
Unsupervised learning of counterfactual physics in pixel space. In The Tenth International
Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . Open-
Review.net. https://openreview.net/forum?id=1L0C5ROtFp .
Jin, C., Liu, Q., & MiryooseÔ¨Å, S. (2021). Bellman eluder dimension: New rich classes of rl prob-
lems, and sample-efÔ¨Åcient algorithms. In Ranzato, M., Beygelzimer, A., Dauphin, Y ., Liang,
P., & Vaughan, J. W. (Eds.), Advances in Neural Information Processing Systems , V ol. 34,
pp. 13406‚Äì13418. Curran Associates, Inc. https://proceedings.neurips.cc/
paper/2021/file/6f5e4e86a87220e5d361ad82f1ebc335-Paper.pdf .
28

--- PAGE 29 ---
Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R. H., Czechowski, K., Erhan, D.,
Finn, C., Kozakowski, P., Levine, S., Mohiuddin, A., Sepassi, R., Tucker, G., & Michalewski,
H. (2020). Model based reinforcement learning for atari. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenRe-
view.net. https://openreview.net/forum?id=S1xCPJHtDB .
Kamienny, P.-A., Tarbouriech, J., Lazaric, A., & Denoyer, L. (2022). Direct then diffuse: Incre-
mental unsupervised skill discovery for state covering and goal reaching. In International
Conference on Learning Representations .https://openreview.net/forum?id=
25kzAhUB1lz .
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Rad-
ford, A., Wu, J., & Amodei, D. (2020). Scaling laws for neural language models. CoRR ,
abs/2001.08361 .https://arxiv.org/abs/2001.08361 .
Kapturowski, S., Campos, V ., Jiang, R., Raki ¬¥cevi¬¥c, N., van Hasselt, H., Blundell, C., & Badia, A. P.
(2022). Human-level atari 200x faster.. DOI: 10.48550/ARXIV .2209.07550.
Khetarpal, K., Riemer, M., Rish, I., & Precup, D. (2020). Towards continual reinforcement learning:
A review and perspectives. CoRR ,abs/2012.13490 .https://arxiv.org/abs/2012.
13490 .
Kingma, D. P., & Welling, M. (2014). Auto-encoding variational bayes. In Bengio, Y ., & LeCun,
Y . (Eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,
Canada, April 14-16, 2014, Conference Track Proceedings .http://arxiv.org/abs/
1312.6114 .
Kipf, T., Li, Y ., Dai, H., Zambaldi, V . F., Sanchez-Gonzalez, A., Grefenstette, E., Kohli, P., &
Battaglia, P. W. (2019). Compile: Compositional imitation learning and execution. In
Chaudhuri, K., & Salakhutdinov, R. (Eds.), Proceedings of the 36th International Confer-
ence on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA ,
V ol. 97 of Proceedings of Machine Learning Research , pp. 3418‚Äì3428. PMLR. http:
//proceedings.mlr.press/v97/kipf19a.html .
Kirk, R., Zhang, A., Grefenstette, E., & Rockt ¬®aschel, T. (2021). A survey of generalisation in deep
reinforcement learning. CoRR ,abs/2111.09794 .https://arxiv.org/abs/2111.
09794 .
Kollar, T., Tellex, S., Roy, D., & Roy, N. (2010). Toward understanding natural language directions.
In2010 5th ACM/IEEE International Conference on Human-Robot Interaction (HRI) , pp.
259‚Äì266. DOI: 10.1109/HRI.2010.5453186.
Kontoyiannis, I., Algoet, P., Suhov, Y ., & Wyner, A. (1998). Nonparametric entropy estimation for
stationary processes and random Ô¨Åelds, with applications to english text. IEEE Transactions
on Information Theory ,44(3), 1319‚Äì1327. DOI: 10.1109/18.669425.
Kostrikov, I., Nair, A., & Levine, S. (2022). OfÔ¨Çine reinforcement learning with implicit q-learning.
InThe Tenth International Conference on Learning Representations, ICLR 2022, Virtual
Event, April 25-29, 2022 . OpenReview.net. https://openreview.net/forum?id=
68n2s9ZJWF8 .
Kulkarni, T. D., Narasimhan, K., Saeedi, A., & Tenenbaum, J. (2016). Hierar-
chical deep reinforcement learning: Integrating temporal abstraction and intrinsic
29

--- PAGE 30 ---
motivation. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., & Garnett,
R. (Eds.), Advances in Neural Information Processing Systems , V ol. 29. Curran
Associates, Inc. https://proceedings.neurips.cc/paper/2016/file/
f442d33fa06832082290ad8544a8da27-Paper.pdf .
Kumar, A., Fu, J., Soh, M., Tucker, G., & Levine, S. (2019). Stabilizing off-policy q-learning
via bootstrapping error reduction. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch ¬¥e-
Buc, F., Fox, E., & Garnett, R. (Eds.), Advances in Neural Information Processing Systems ,
V ol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper/
2019/file/c2073ffa77b5357a498057413bb09d3a-Paper.pdf .
Kumar, A., Zhou, A., Tucker, G., & Levine, S. (2020). Conservative q-learning for ofÔ¨Çine re-
inforcement learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., & Lin, H.
(Eds.), Advances in Neural Information Processing Systems , V ol. 33, pp. 1179‚Äì1191. Cur-
ran Associates, Inc. https://proceedings.neurips.cc/paper/2020/file/
0d2b2061826a5df3221116a5085a6052-Paper.pdf .
Lambert, N., Wulfmeier, M., Whitney, W., Byravan, A., Bloesch, M., Dasagi, V ., Hertweck, T.,
& Riedmiller, M. (2022). The challenges of exploration for ofÔ¨Çine reinforcement learning.
ArXiv preprint ,abs/2201.11861 .https://arxiv.org/abs/2201.11861 .
Lange, S., Gabel, T., & Riedmiller, M. (2012). Batch reinforcement learning. In Reinforcement
learning , pp. 45‚Äì73. Springer.
Lange, S., & Riedmiller, M. A. (2010). Deep auto-encoder neural networks in reinforcement learn-
ing. In International Joint Conference on Neural Networks, IJCNN 2010, Barcelona, Spain,
18-23 July, 2010 , pp. 1‚Äì8. IEEE. DOI: 10.1109/IJCNN.2010.5596468.
Larsen, K. G., & Skou, A. (1991). Bisimulation through probabilistic testing. Information and
computation ,94(1), 1‚Äì28.
Laskin, M., Liu, H., Peng, X. B., Yarats, D., Rajeswaran, A., & Abbeel, P. (2022). CIC: con-
trastive intrinsic control for unsupervised skill discovery. CoRR ,abs/2202.00161 .https:
//arxiv.org/abs/2202.00161 .
Laskin, M., Srinivas, A., & Abbeel, P. (2020). CURL: contrastive unsupervised representations
for reinforcement learning. In Proceedings of the 37th International Conference on Ma-
chine Learning, ICML 2020, 13-18 July 2020, Virtual Event , V ol. 119 of Proceedings of
Machine Learning Research , pp. 5639‚Äì5650. PMLR. http://proceedings.mlr.
press/v119/laskin20a.html .
Laskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang, C., Pinto, L., & Abbeel, P. (2021).
Urlb: Unsupervised reinforcement learning benchmark. ArXiv preprint ,abs/2110.15191 .
https://arxiv.org/abs/2110.15191 .
LeCun, Y ., Chopra, S., Hadsell, R., Ranzato, M., & Huang, F. (2006). A tutorial on energy-based
learning. Predicting structured data ,1(0).
Lee, K.-H., Nachum, O., Yang, M., Lee, L., Freeman, D., Xu, W., Guadarrama, S., Fischer, I.,
Jang, E., Michalewski, H., et al. (2022). Multi-game decision transformers. ArXiv preprint ,
abs/2205.15241 .https://arxiv.org/abs/2205.15241 .
30

--- PAGE 31 ---
Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., & Salakhutdinov, R. (2019). EfÔ¨Å-
cient exploration via state marginal matching. ArXiv preprint ,abs/1906.05274 .https:
//arxiv.org/abs/1906.05274 .
Lee, S., Seo, Y ., Lee, K., Abbeel, P., & Shin, J. (2022). OfÔ¨Çine-to-online reinforcement learning via
balanced replay and pessimistic q-ensemble. In Conference on Robot Learning , pp. 1702‚Äì
1712. PMLR.
Lester, B., Al-Rfou, R., & Constant, N. (2021). The power of scale for parameter-efÔ¨Åcient prompt
tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing , pp. 3045‚Äì3059, Online and Punta Cana, Dominican Republic. Association for
Computational Linguistics. DOI: 10.18653/v1/2021.emnlp-main.243.
Levine, S. (2021). Understanding the world through action. In 5th Annual Conference on
Robot Learning, Blue Sky Submission Track .https://openreview.net/forum?id=
L55-yn1iwrm .
Levine, S., Kumar, A., Tucker, G., & Fu, J. (2020). OfÔ¨Çine reinforcement learning: Tutorial, review,
and perspectives on open problems. ArXiv preprint ,abs/2005.01643 .https://arxiv.
org/abs/2005.01643 .
Li, C., Wang, T., Wu, C., Zhao, Q., Yang, J., & Zhang, C. (2021). Celebrating diversity in shared
multi-agent reinforcement learning. In Ranzato, M., Beygelzimer, A., Dauphin, Y ., Liang, P.,
& Vaughan, J. W. (Eds.), Advances in Neural Information Processing Systems , V ol. 34, pp.
3991‚Äì4002. Curran Associates, Inc. https://proceedings.neurips.cc/paper/
2021/file/20aee3a5f4643755a79ee5f6a73050ac-Paper.pdf .
Li, L., Walsh, T. J., & Littman, M. L. (2006). Towards a uniÔ¨Åed theory of state abstraction for mdps..
InAI&M .
Li, S., Du, Y ., Tenenbaum, J. B., Torralba, A., & Mordatch, I. (2022a). Compos-
ing ensembles of pre-trained models via iterative consensus. CoRR ,abs/2210.11522 .
DOI: 10.48550/arXiv.2210.11522.
Li, S., Puig, X., Du, Y ., Wang, C., Akyurek, E., Torralba, A., Andreas, J., & Mordatch, I.
(2022b). Pre-trained language models for interactive decision-making. ArXiv preprint ,
abs/2202.01771 .https://arxiv.org/abs/2202.01771 .
Liu, H., & Abbeel, P. (2021a). APS: active pretraining with successor features. In Meila, M., &
Zhang, T. (Eds.), Proceedings of the 38th International Conference on Machine Learning,
ICML 2021, 18-24 July 2021, Virtual Event , V ol. 139 of Proceedings of Machine Learn-
ing Research , pp. 6736‚Äì6747. PMLR. http://proceedings.mlr.press/v139/
liu21b.html .
Liu, H., & Abbeel, P. (2021b). Behavior from the void: Unsupervised active pre-training. Advances
in Neural Information Processing Systems ,34, 18459‚Äì18473.
Lu, J., Batra, D., Parikh, D., & Lee, S. (2019). Vilbert: Pretraining task-agnostic visiolinguistic
representations for vision-and-language tasks. In Wallach, H., Larochelle, H., Beygelzimer,
A., d'Alch ¬¥e-Buc, F., Fox, E., & Garnett, R. (Eds.), Advances in Neural Information Process-
ing Systems , V ol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/
paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf .
31

--- PAGE 32 ---
Lu, K., Grover, A., Abbeel, P., & Mordatch, I. (2021a). Pretrained transformers as universal com-
putation engines. ArXiv preprint ,abs/2103.05247 .https://arxiv.org/abs/2103.
05247 .
Lu, K., Grover, A., Abbeel, P., & Mordatch, I. (2021b). Reset-free lifelong learning with skill-
space planning. In International Conference on Learning Representations .https://
openreview.net/forum?id=HIGSa_3kOx3 .
Lu, Y ., Hausman, K., Chebotar, Y ., Yan, M., Jang, E., Herzog, A., Xiao, T., Irpan, A., Khansari,
M., Kalashnikov, D., & Levine, S. (2022). Aw-opt: Learning robotic skills with imitation and
reinforcement at scale. In Faust, A., Hsu, D., & Neumann, G. (Eds.), Proceedings of the 5th
Conference on Robot Learning , V ol. 164 of Proceedings of Machine Learning Research , pp.
1078‚Äì1088. PMLR. https://proceedings.mlr.press/v164/lu22a.html .
Lynch, C., Khansari, M., Xiao, T., Kumar, V ., Tompson, J., Levine, S., & Sermanet, P. (2020).
Learning latent plans from play. In Conference on robot learning , pp. 1113‚Äì1132. PMLR.
Mahmoudieh, P., Pathak, D., & Darrell, T. (2022). Zero-shot reward speciÔ¨Åcation via grounded
natural language. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., &
Sabato, S. (Eds.), Proceedings of the 39th International Conference on Machine Learn-
ing, V ol. 162 of Proceedings of Machine Learning Research , pp. 14743‚Äì14752. PMLR.
https://proceedings.mlr.press/v162/mahmoudieh22a.html .
Mavor-Parker, A., Young, K., Barry, C., & GrifÔ¨Ån, L. (2022). How to stay curious while avoiding
noisy tvs using aleatoric uncertainty estimation. In International Conference on Machine
Learning , pp. 15220‚Äì15240. PMLR.
Mazoure, B., des Combes, R. T., Doan, T., Bachman, P., & Hjelm, R. D. (2020). Deep re-
inforcement and infomax learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan,
M., & Lin, H. (Eds.), Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual .https://proceedings.neurips.cc/paper/2020/hash/
26588e932c7ccfa1df309280702fe1b5-Abstract.html .
Meng, L., Wen, M., Yang, Y ., Le, C., Li, X., Zhang, W., Wen, Y ., Zhang, H., Wang, J., & Xu, B.
(2021). OfÔ¨Çine pre-trained multi-agent decision transformer: One big sequence model tackles
all SMAC tasks. CoRR ,abs/2112.02845 .https://arxiv.org/abs/2112.02845 .
Misra, D., Henaff, M., Krishnamurthy, A., & Langford, J. (2020). Kinematic state abstrac-
tion and provably efÔ¨Åcient rich-observation reinforcement learning. In Proceedings of the
37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Vir-
tual Event , V ol. 119 of Proceedings of Machine Learning Research , pp. 6961‚Äì6971. PMLR.
http://proceedings.mlr.press/v119/misra20a.html .
Mustafa, B., Riquelme, C., Puigcerver, J., Jenatton, R., & Houlsby, N. (2022). Multimodal con-
trastive learning with limoe: the language-image mixture of experts. CoRR ,abs/2206.02770 .
DOI: 10.48550/arXiv.2206.02770.
Mutti, M., Pratissoli, L., & Restelli, M. (2020). A policy gradient method for task-agnostic
exploration. In 4th Lifelong Machine Learning Workshop at ICML 2020 .https://
openreview.net/forum?id=d9j_RNHtQEo .
32

--- PAGE 33 ---
Nachum, O., Gu, S., Lee, H., & Levine, S. (2019). Near-optimal representation learning for hi-
erarchical reinforcement learning. In 7th International Conference on Learning Represen-
tations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net. https:
//openreview.net/forum?id=H1emus0qF7 .
Nair, A., Dalal, M., Gupta, A., & Levine, S. (2020). Accelerating online reinforcement learning with
ofÔ¨Çine datasets. ArXiv preprint ,abs/2006.09359 .https://arxiv.org/abs/2006.
09359 .
Ndousse, K. K., Eck, D., Levine, S., & Jaques, N. (2021). Emergent social learning via multi-
agent reinforcement learning. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th
International Conference on Machine Learning , V ol. 139 of Proceedings of Machine Learn-
ing Research , pp. 7991‚Äì8004. PMLR. https://proceedings.mlr.press/v139/
ndousse21a.html .
Ostrovski, G., Bellemare, M. G., van den Oord, A., & Munos, R. (2017). Count-based exploration
with neural density models. In Precup, D., & Teh, Y . W. (Eds.), Proceedings of the 34th
International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11
August 2017 , V ol. 70 of Proceedings of Machine Learning Research , pp. 2721‚Äì2730. PMLR.
http://proceedings.mlr.press/v70/ostrovski17a.html .
Oudeyer, P.-Y ., Kaplan, F., & Hafner, V . V . (2007). Intrinsic motivation systems for autonomous
mental development. IEEE transactions on evolutionary computation ,11(2), 265‚Äì286.
Parisi, S., Rajeswaran, A., Purushwalkam, S., & Gupta, A. (2022). The unsurprising effectiveness of
pre-trained vision models for control. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C.,
Niu, G., & Sabato, S. (Eds.), Proceedings of the 39th International Conference on Machine
Learning , V ol. 162 of Proceedings of Machine Learning Research , pp. 17359‚Äì17371. PMLR.
https://proceedings.mlr.press/v162/parisi22a.html .
Park, S., Choi, J., Kim, J., Lee, H., & Kim, G. (2022). Lipschitz-constrained unsupervised
skill discovery. In International Conference on Learning Representations .https://
openreview.net/forum?id=BGvt0ghNgA .
Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017). Curiosity-driven exploration by self-
supervised prediction. In Precup, D., & Teh, Y . W. (Eds.), Proceedings of the 34th Inter-
national Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 Au-
gust 2017 , V ol. 70 of Proceedings of Machine Learning Research , pp. 2778‚Äì2787. PMLR.
http://proceedings.mlr.press/v70/pathak17a.html .
Pathak, D., Gandhi, D., & Gupta, A. (2019). Self-supervised exploration via disagreement. In
Chaudhuri, K., & Salakhutdinov, R. (Eds.), Proceedings of the 36th International Confer-
ence on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA ,
V ol. 97 of Proceedings of Machine Learning Research , pp. 5062‚Äì5071. PMLR. http:
//proceedings.mlr.press/v97/pathak19a.html .
Pertsch, K., Lee, Y ., & Lim, J. (2021a). Accelerating reinforcement learning with learned skill
priors. In Kober, J., Ramos, F., & Tomlin, C. (Eds.), Proceedings of the 2020 Conference
on Robot Learning , V ol. 155 of Proceedings of Machine Learning Research , pp. 188‚Äì204.
PMLR. https://proceedings.mlr.press/v155/pertsch21a.html .
33

--- PAGE 34 ---
Pertsch, K., Lee, Y ., Wu, Y ., & Lim, J. J. (2021b). Demonstration-guided reinforcement learn-
ing with learned skills. In 5th Annual Conference on Robot Learning .https://
openreview.net/forum?id=JSC4KMlENqF .
Pomerleau, D. A. (1988). Alvinn: An autonomous land vehicle in a neural network. Advances in
neural information processing systems ,1.
Pong, V ., Dalal, M., Lin, S., Nair, A., Bahl, S., & Levine, S. (2020). Skew-Ô¨Åt: State-covering
self-supervised reinforcement learning. In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , V ol. 119 of Proceedings
of Machine Learning Research , pp. 7783‚Äì7792. PMLR. http://proceedings.mlr.
press/v119/pong20a.html .
Poole, B., Ozair, S., Van Den Oord, A., Alemi, A., & Tucker, G. (2019). On variational bounds of
mutual information. In Chaudhuri, K., & Salakhutdinov, R. (Eds.), Proceedings of the 36th
International Conference on Machine Learning , V ol. 97 of Proceedings of Machine Learn-
ing Research , pp. 5171‚Äì5180. PMLR. https://proceedings.mlr.press/v97/
poole19a.html .
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A.,
Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning transferable visual models
from natural language supervision. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th
International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event ,
V ol. 139 of Proceedings of Machine Learning Research , pp. 8748‚Äì8763. PMLR. http:
//proceedings.mlr.press/v139/radford21a.html .
Raffel, C. (2021). A call to build models like we build open-
source software.. https://colinraffel.com/blog/
a-call-to-build-models-like-we-build-open-source-software.
html .
Rajeswaran, A., Kumar, V ., Gupta, A., Vezzani, G., Schulman, J., Todorov, E., & Levine, S. (2018).
Learning complex dexterous manipulation with deep reinforcement learning and demonstra-
tions. In Kress-Gazit, H., Srinivasa, S. S., Howard, T., & Atanasov, N. (Eds.), Robotics:
Science and Systems XIV , Carnegie Mellon University, Pittsburgh, Pennsylvania, USA, June
26-30, 2018 . DOI: 10.15607/RSS.2018.XIV .049.
Rakelly, K., Gupta, A., Florensa, C., & Levine, S. (2021). Which mutual-information representation
learning objectives are sufÔ¨Åcient for control?. Advances in Neural Information Processing
Systems ,34, 26345‚Äì26357.
Rashid, T., Peng, B., Boehmer, W., & Whiteson, S. (2020). Optimistic exploration even with
a pessimistic initialisation. In 8th International Conference on Learning Representa-
tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net. https:
//openreview.net/forum?id=r1xGP6VYwH .
Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M.,
Sulsky, Y ., Kay, J., Springenberg, J. T., et al. (2022). A generalist agent. ArXiv preprint ,
abs/2205.06175 .https://arxiv.org/abs/2205.06175 .
Reid, M., Yamada, Y ., & Gu, S. S. (2022). Can wikipedia help ofÔ¨Çine reinforcement learning?.
ArXiv preprint ,abs/2201.12122 .https://arxiv.org/abs/2201.12122 .
34

--- PAGE 35 ---
Ross, S., Gordon, G., & Bagnell, D. (2011). A reduction of imitation learning and structured predic-
tion to no-regret online learning. In Gordon, G., Dunson, D., & Dud ¬¥ƒ±k, M. (Eds.), Proceedings
of the Fourteenth International Conference on ArtiÔ¨Åcial Intelligence and Statistics , V ol. 15 of
Proceedings of Machine Learning Research , pp. 627‚Äì635, Fort Lauderdale, FL, USA. PMLR.
https://proceedings.mlr.press/v15/ross11a.html .
Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick, J., Kavukcuoglu, K., Pas-
canu, R., & Hadsell, R. (2016). Progressive neural networks. CoRR ,abs/1606.04671 .
http://arxiv.org/abs/1606.04671 .
Schmidhuber, J. (1991). Curious model-building control systems. In Proc. international joint con-
ference on neural networks , pp. 1458‚Äì1463.
Schmitt, S., Hudson, J. J., Z ¬¥ƒ±dek, A., Osindero, S., Doersch, C., Czarnecki, W. M., Leibo, J. Z.,
K¬®uttler, H., Zisserman, A., Simonyan, K., & Eslami, S. M. A. (2018). Kickstarting deep rein-
forcement learning. CoRR ,abs/1803.03835 .http://arxiv.org/abs/1803.03835 .
Schwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville, A. C., & Bachman, P. (2021a). Data-
efÔ¨Åcient reinforcement learning with self-predictive representations. In 9th International
Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 .
OpenReview.net. https://openreview.net/forum?id=uCQfPZwRaUu .
Schwarzer, M., Rajkumar, N., Noukhovitch, M., Anand, A., Charlin, L., Hjelm, R. D., Bachman,
P., & Courville, A. C. (2021b). Pretraining representations for data-efÔ¨Åcient reinforcement
learning. Advances in Neural Information Processing Systems ,34, 12686‚Äì12699.
Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., & Pathak, D. (2020). Planning to ex-
plore via self-supervised world models. In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event , V ol. 119 of Proceedings
of Machine Learning Research , pp. 8583‚Äì8592. PMLR. http://proceedings.mlr.
press/v119/sekar20a.html .
Seo, Y ., Chen, L., Shin, J., Lee, H., Abbeel, P., & Lee, K. (2021). State entropy maximization with
random encoders for efÔ¨Åcient exploration. In Meila, M., & Zhang, T. (Eds.), Proceedings of
the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Vir-
tual Event , V ol. 139 of Proceedings of Machine Learning Research , pp. 9443‚Äì9454. PMLR.
http://proceedings.mlr.press/v139/seo21a.html .
Seo, Y ., Lee, K., James, S., & Abbeel, P. (2022). Reinforcement learning with action-free pre-
training from videos. ArXiv preprint ,abs/2203.13880 .https://arxiv.org/abs/
2203.13880 .
Sermanet, P., Lynch, C., Chebotar, Y ., Hsu, J., Jang, E., Schaal, S., Levine, S., & Brain, G. (2018).
Time-contrastive networks: Self-supervised learning from video. In 2018 IEEE international
conference on robotics and automation (ICRA) , pp. 1134‚Äì1141. IEEE.
Shah, R. M., & Kumar, V . (2021). RRL: resnet as representation for reinforcement learning. In
Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th International Conference on Ma-
chine Learning, ICML 2021, 18-24 July 2021, Virtual Event , V ol. 139 of Proceedings of
Machine Learning Research , pp. 9465‚Äì9476. PMLR. http://proceedings.mlr.
press/v139/shah21a.html .
35

--- PAGE 36 ---
Shankar, T., & Gupta, A. (2020). Learning robot skills with temporal variational inference. In
III, H. D., & Singh, A. (Eds.), Proceedings of the 37th International Conference on Machine
Learning , V ol. 119 of Proceedings of Machine Learning Research , pp. 8624‚Äì8633. PMLR.
https://proceedings.mlr.press/v119/shankar20b.html .
Shankar, T., Tulsiani, S., Pinto, L., & Gupta, A. (2020). Discovering motor programs by re-
composing demonstrations. In 8th International Conference on Learning Representa-
tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net. https:
//openreview.net/forum?id=rkgHY0NYwr .
Sharma, A., Gu, S., Levine, S., Kumar, V ., & Hausman, K. (2020). Dynamics-aware unsupervised
discovery of skills. In 8th International Conference on Learning Representations, ICLR 2020,
Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net. https://openreview.
net/forum?id=HJgLZR4KvH .
Siegel, N., Springenberg, J. T., Berkenkamp, F., Abdolmaleki, A., Neunert, M., Lampe, T., Hafner,
R., Heess, N., & Riedmiller, M. (2020). Keep doing what worked: Behavior modelling priors
for ofÔ¨Çine reinforcement learning. In International Conference on Learning Representations .
https://openreview.net/forum?id=rke7geHtwH .
Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser,
J., Antonoglou, I., Panneershelvam, V ., Lanctot, M., et al. (2016). Mastering the game of go
with deep neural networks and tree search. nature ,529(7587), 484‚Äì489.
Silvia, P. J. (2012). Curiosity and Motivation. In The Oxford Handbook of Human Motivation .
Oxford University Press. DOI: 10.1093/oxfordhb/9780195399820.013.0010.
Singh, A., Liu, H., Zhou, G., Yu, A., Rhinehart, N., & Levine, S. (2021). Parrot: Data-driven
behavioral priors for reinforcement learning. In 9th International Conference on Learn-
ing Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net.
https://openreview.net/forum?id=Ysuv-WOFeKR .
Singh, S. P., Barto, A. G., & Chentanez, N. (2004). Intrinsically motivated reinforcement
learning. In Advances in Neural Information Processing Systems 17 [Neural Informa-
tion Processing Systems, NIPS 2004, December 13-18, 2004, Vancouver, British Columbia,
Canada] , pp. 1281‚Äì1288. https://proceedings.neurips.cc/paper/2004/
hash/4be5a36cbaca8ab9d2066debfe4e65c1-Abstract.html .
Smith, L., & Gasser, M. (2005). The development of embodied cognition: Six lessons from babies.
ArtiÔ¨Åcial life ,11(1-2), 13‚Äì29.
Srinivas, A., & Abbeel, P. (2021). Unsupervised learning for reinforcement learning.. https:
//icml.cc/media/icml-2021/Slides/10843_QHaHBNU.pdf .
Stadie, B. C., Levine, S., & Abbeel, P. (2015). Incentivizing exploration in reinforcement learning
with deep predictive models. ArXiv preprint ,abs/1507.00814 .https://arxiv.org/
abs/1507.00814 .
Stooke, A., Lee, K., Abbeel, P., & Laskin, M. (2021). Decoupling representation learning from
reinforcement learning. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th Inter-
national Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event ,
V ol. 139 of Proceedings of Machine Learning Research , pp. 9870‚Äì9879. PMLR. http:
//proceedings.mlr.press/v139/stooke21a.html .
36

--- PAGE 37 ---
Strehl, A. L., & Littman, M. L. (2008). An analysis of model-based interval estimation for markov
decision processes. Journal of Computer and System Sciences ,74(8), 1309‚Äì1331.
Sutton, R. (2019). The bitter lesson. Incomplete Ideas (blog) ,13, 12.
Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction . MIT press.
Tam, A. C., Rabinowitz, N. C., Lampinen, A. K., Roy, N. A., Chan, S. C., Strouse, D., Wang, J. X.,
Banino, A., & Hill, F. (2022). Semantic exploration from language abstractions and pretrained
representations. ArXiv preprint ,abs/2204.05080 .https://arxiv.org/abs/2204.
05080 .
Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, X., Duan, Y ., Schulman, J.,
Turck, F. D., & Abbeel, P. (2017). #exploration: A study of count-based explo-
ration for deep reinforcement learning. In Guyon, I., von Luxburg, U., Bengio,
S., Wallach, H. M., Fergus, R., Vishwanathan, S. V . N., & Garnett, R. (Eds.), Ad-
vances in Neural Information Processing Systems 30: Annual Conference on Neu-
ral Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA,
USA, pp. 2753‚Äì2762. https://proceedings.neurips.cc/paper/2017/hash/
3a20f62a0af1aa152670bab3c602feed-Abstract.html .
Tao, R. Y ., Francois-Lavet, V ., & Pineau, J. (2020). Novelty search in representational space
for sample efÔ¨Åcient exploration. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan,
M., & Lin, H. (Eds.), Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual .https://proceedings.neurips.cc/paper/2020/hash/
5ca41a86596a5ed567d15af0be224952-Abstract.html .
Tarbouriech, J., & Lazaric, A. (2019). Active exploration in markov decision processes. In Chaud-
huri, K., & Sugiyama, M. (Eds.), The 22nd International Conference on ArtiÔ¨Åcial Intelligence
and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan , V ol. 89 of Proceed-
ings of Machine Learning Research , pp. 974‚Äì982. PMLR. http://proceedings.mlr.
press/v89/tarbouriech19a.html .
Tassa, Y ., Doron, Y ., Muldal, A., Erez, T., Li, Y ., de Las Casas, D., Budden, D., Abdolmaleki, A.,
Merel, J., Lefrancq, A., Lillicrap, T. P., & Riedmiller, M. A. (2018). Deepmind control suite.
CoRR ,abs/1801.00690 .http://arxiv.org/abs/1801.00690 .
Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A., Teller, S., & Roy, N. (2011).
Understanding natural language commands for robotic navigation and mobile manipula-
tion. Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence ,25(1), 1507‚Äì1514.
DOI: 10.1609/aaai.v25i1.7979.
Tr¬®auble, F., Dittadi, A., Wuthrich, M., Widmaier, F., Gehler, P. V ., Winther, O., Locatello, F.,
Bachem, O., Sch ¬®olkopf, B., & Bauer, S. (2022). The role of pretrained representations for
the OOD generalization of RL agents. In International Conference on Learning Representa-
tions .https://openreview.net/forum?id=8eb12UQYxrG .
van den Oord, A., Li, Y ., & Vinyals, O. (2018). Representation learning with contrastive predictive
coding. CoRR ,abs/1807.03748 .http://arxiv.org/abs/1807.03748 .
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., & Polo-
sukhin, I. (2017). Attention is all you need. In Guyon, I., Luxburg, U. V ., Bengio, S., Wallach,
37

--- PAGE 38 ---
H., Fergus, R., Vishwanathan, S., & Garnett, R. (Eds.), Advances in Neural Information Pro-
cessing Systems , V ol. 30. Curran Associates, Inc. https://proceedings.neurips.
cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf .
Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H.,
Powell, R., Ewalds, T., Georgiev, P., et al. (2019). Grandmaster level in starcraft ii using
multi-agent reinforcement learning. Nature ,575(7782), 350‚Äì354.
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. R. (2019). GLUE: A multi-task
benchmark and analysis platform for natural language understanding. In 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .
OpenReview.net. https://openreview.net/forum?id=rJ4km2R5t7 .
Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O. K.,
Singhal, S., Som, S., & Wei, F. (2022). Image as a foreign language: Beit pretraining for all
vision and vision-language tasks. CoRR ,abs/2208.10442 . DOI: 10.48550/arXiv.2208.10442.
Warde-Farley, D., de Wiele, T. V ., Kulkarni, T. D., Ionescu, C., Hansen, S., & Mnih, V . (2019).
Unsupervised control through non-parametric discriminative rewards. In 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .
OpenReview.net. https://openreview.net/forum?id=r1eVMnA9K7 .
Watter, M., Springenberg, J., Boedecker, J., & Riedmiller, M. (2015). Embed to control: A lo-
cally linear latent dynamics model for control from raw images. In Cortes, C., Lawrence,
N., Lee, D., Sugiyama, M., & Garnett, R. (Eds.), Advances in Neural Information Process-
ing Systems , V ol. 28. Curran Associates, Inc. https://proceedings.neurips.cc/
paper/2015/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf .
Wei, J., Bosma, M., Zhao, V . Y ., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., & Le, Q. V .
(2022). Finetuned language models are zero-shot learners. In The Tenth International Con-
ference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenRe-
view.net. https://openreview.net/forum?id=gEZrGCozdqR .
Xiao, T., Radosavovic, I., Darrell, T., & Malik, J. (2022). Masked visual pre-training for motor
control. ArXiv preprint ,abs/2203.06173 .https://arxiv.org/abs/2203.06173 .
Xu, K., Verma, S., Finn, C., & Levine, S. (2020). Continual learning of control primitives : Skill
discovery via reset-games. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., & Lin,
H. (Eds.), Advances in Neural Information Processing Systems , V ol. 33, pp. 4999‚Äì5010. Cur-
ran Associates, Inc. https://proceedings.neurips.cc/paper/2020/file/
3472ab80b6dff70c54758fd6dfc800c2-Paper.pdf .
Xu, M., Shen, Y ., Zhang, S., Lu, Y ., Zhao, D., Tenenbaum, B. J., & Gan, C. (2022). Prompting deci-
sion transformer for few-shot policy generalization. In Thirty-ninth International Conference
on Machine Learning .
Yamada, J., Pertsch, K., Gunjal, A., & Lim, J. J. (2022). Task-induced representation learning.
InThe Tenth International Conference on Learning Representations, ICLR 2022, Virtual
Event, April 25-29, 2022 . OpenReview.net. https://openreview.net/forum?id=
OzyXtIZAzFv .
38

--- PAGE 39 ---
Yang, M., Levine, S., & Nachum, O. (2022). TRAIL: Near-optimal imitation learning with
suboptimal data. In International Conference on Learning Representations .https:
//openreview.net/forum?id=6q_2b6u0BnJ .
Yang, M., & Nachum, O. (2021). Representation matters: OfÔ¨Çine pretraining for sequential decision
making. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th International Conference
on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , V ol. 139 of Proceedings
of Machine Learning Research , pp. 11784‚Äì11794. PMLR. http://proceedings.mlr.
press/v139/yang21h.html .
Yarats, D., Brandfonbrener, D., Liu, H., Laskin, M., Abbeel, P., Lazaric, A., & Pinto, L. (2022).
Don‚Äôt change the algorithm, change the data: Exploratory data for ofÔ¨Çine reinforcement
learning. In ICLR 2022 Workshop on Generalizable Policy Learning in Physical World .
https://openreview.net/forum?id=Su-zh4a41Z5 .
Yarats, D., Fergus, R., Lazaric, A., & Pinto, L. (2021). Reinforcement learning with prototyp-
ical representations. In Meila, M., & Zhang, T. (Eds.), Proceedings of the 38th Interna-
tional Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event , V ol.
139 of Proceedings of Machine Learning Research , pp. 11920‚Äì11931. PMLR. http:
//proceedings.mlr.press/v139/yarats21a.html .
Ye, D., Chen, G., Zhang, W., Chen, S., Yuan, B., Liu, B., Chen, J., Liu, Z., Qiu, F., Yu, H., Yin,
Y ., Shi, B., Wang, L., Shi, T., Fu, Q., Yang, W., Huang, L., & Liu, W. (2020). Towards
playing full MOBA games with deep reinforcement learning. In Larochelle, H., Ranzato,
M., Hadsell, R., Balcan, M., & Lin, H. (Eds.), Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual .https://proceedings.neurips.cc/paper/
2020/hash/06d5ae105ea1bea4d800bc96491876e9-Abstract.html .
Ye, D., Chen, G., Zhao, P., Qiu, F., Yuan, B., Zhang, W., Chen, S., Sun, M., Li, X., Li, S., Liang, J.,
Lian, Z., Shi, B., Wang, L., Shi, T., Fu, Q., Yang, W., & Huang, L. (2022). Supervised learning
achieves human-level performance in MOBA games: A case study of honor of kings. IEEE
Trans. Neural Networks Learn. Syst. ,33(3), 908‚Äì918. DOI: 10.1109/TNNLS.2020.3029475.
Ye, D., Liu, Z., Sun, M., Shi, B., Zhao, P., Wu, H., Yu, H., Yang, S., Wu, X., Guo, Q., Chen, Q.,
Yin, Y ., Zhang, H., Shi, T., Wang, L., Fu, Q., Yang, W., & Huang, L. (2020). Mastering com-
plex control in MOBA games with deep reinforcement learning. In The Thirty-Fourth AAAI
Conference on ArtiÔ¨Åcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications
of ArtiÔ¨Åcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational
Advances in ArtiÔ¨Åcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020 , pp.
6672‚Äì6679. AAAI Press. https://ojs.aaai.org/index.php/AAAI/article/
view/6144 .
Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., & Finn, C. (2020). Gradient surgery
for multi-task learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., &
Lin, H. (Eds.), Advances in Neural Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-
12, 2020, virtual .https://proceedings.neurips.cc/paper/2020/hash/
3fe78a8acf5fda99de95303940a2420c-Abstract.html .
39

--- PAGE 40 ---
Zhan, A., Zhao, P., Pinto, L., Abbeel, P., & Laskin, M. (2020). A framework for efÔ¨Åcient robotic ma-
nipulation. ArXiv preprint ,abs/2012.07975 .https://arxiv.org/abs/2012.07975 .
Zhang, A., Ballas, N., & Pineau, J. (2018). A dissection of overÔ¨Åtting and generalization in continu-
ous reinforcement learning. CoRR ,abs/1806.07937 .http://arxiv.org/abs/1806.
07937 .
Zhang, A., McAllister, R. T., Calandra, R., Gal, Y ., & Levine, S. (2021a). Learning invariant repre-
sentations for reinforcement learning without reconstruction. In 9th International Conference
on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenRe-
view.net. https://openreview.net/forum?id=-2FCwDKRREu .
Zhang, C., Cai, Y ., Huang, L., & Li, J. (2021b). Exploration by maximizing r ¬¥enyi entropy for
reward-free rl framework. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence ,
pp. 10859‚Äì10867.
Zhang, J., Yu, H., & Xu, W. (2021c). Hierarchical reinforcement learning by discovering intrinsic
options. In 9th International Conference on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021 . OpenReview.net. https://openreview.net/forum?
id=r-gPPHEjpmw .
Zhang, K., Yang, Z., & Basar, T. (2019). Multi-agent reinforcement learning: A selective overview
of theories and algorithms. CoRR ,abs/1911.10635 .http://arxiv.org/abs/1911.
10635 .
Zhang, Q., Peng, Z., & Zhou, B. (2022). Learning to drive by watching youtube videos: Action-
conditioned contrastive policy pretraining.. DOI: 10.48550/ARXIV .2204.02393.
Zhang, T., McCarthy, Z., Jow, O., Lee, D., Chen, X., Goldberg, K., & Abbeel, P. (2018). Deep
imitation learning for complex manipulation tasks from virtual reality teleoperation. In
2018 IEEE International Conference on Robotics and Automation (ICRA) , pp. 5628‚Äì5635.
DOI: 10.1109/ICRA.2018.8461249.
Zhang, W., GX-Chen, A., Sobal, V ., LeCun, Y ., & Carion, N. (2022a). Light-weight prob-
ing of unsupervised representations for reinforcement learning. CoRR ,abs/2208.12345 .
DOI: 10.48550/arXiv.2208.12345.
Zhang, X., Song, Y ., Uehara, M., Wang, M., Agarwal, A., & Sun, W. (2022b). EfÔ¨Åcient reinforce-
ment learning in block MDPs: A model-free representation learning approach. In Chaudhuri,
K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., & Sabato, S. (Eds.), Proceedings of the
39th International Conference on Machine Learning , V ol. 162 of Proceedings of Machine
Learning Research , pp. 26517‚Äì26547. PMLR. https://proceedings.mlr.press/
v162/zhang22aa.html .
Zheng, Q., Zhang, A., & Grover, A. (2022). Online decision transformer. In Chaudhuri, K., Jegelka,
S., Song, L., Szepesvari, C., Niu, G., & Sabato, S. (Eds.), Proceedings of the 39th Inter-
national Conference on Machine Learning , V ol. 162 of Proceedings of Machine Learning
Research , pp. 27042‚Äì27059. PMLR. https://proceedings.mlr.press/v162/
zheng22c.html .
Zhu, Y ., Wang, Z., Merel, J., Rusu, A., Erez, T., Cabi, S., Tunyasuvunakool, S., Kram ÀúA¬°r, J., Hadsell,
R., de Freitas, N., & Heess, N. (2018). Reinforcement and imitation learning for diverse vi-
40

--- PAGE 41 ---
suomotor skills. In Proceedings of Robotics: Science and Systems , Pittsburgh, Pennsylvania.
DOI: 10.15607/RSS.2018.XIV .009.
41
