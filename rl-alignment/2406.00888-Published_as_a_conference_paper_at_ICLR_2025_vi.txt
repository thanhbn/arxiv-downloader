# 2406.00888.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2406.00888.pdf
# Kích thước tệp: 1159818 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025
CĂNN CHỈNH CÁC MÔ HÌNH NGÔN NGỮ VỚI
PHẢN HỒI MINH HỌA
Omar Shaikh∗
Đại học Stanford
oshaikh@stanford.eduMichelle S. Lam∗
Đại học Stanford
mlam4@cs.stanford.eduJoey Hejna∗
Đại học Stanford
jhejna@cs.stanford.edu
Yijia Shao
Đại học StanfordHyundong Cho
USCMichael S. Bernstein
Đại học StanfordDiyi Yang
Đại học Stanford

TÓM TẮT
Các mô hình ngôn ngữ được căn chỉnh để bắt chước tiếng nói chung của nhiều người, dẫn đến 
các đầu ra phù hợp với không ai cụ thể nào. Việc hướng dẫn LLM tránh khỏi đầu ra chung 
chung là có thể thông qua tinh chỉnh có giám sát hoặc RLHF, nhưng đòi hỏi các bộ dữ liệu 
lớn một cách cấm đoán cho các nhiệm vụ tạm thời mới. Chúng tôi lập luận rằng thay vào đó 
có thể căn chỉnh một LLM với một bối cảnh cụ thể bằng cách tận dụng một số lượng rất nhỏ 
(<10) các minh họa như phản hồi. Phương pháp của chúng tôi, Tối ưu hóa Nhiệm vụ Lặp lại 
Minh họa (DITTO), trực tiếp căn chỉnh các đầu ra mô hình ngôn ngữ với các hành vi được 
minh họa của người dùng. Xuất phát từ các ý tưởng từ học tập bắt chước trực tuyến, DITTO 
tạo ra dữ liệu so sánh trực tuyến một cách rẻ tiền bằng cách coi các minh họa của người dùng 
là được ưa thích hơn đầu ra từ LLM và các điểm kiểm tra trung gian của nó. Cụ thể, DITTO 
hoạt động bằng cách để một LLM tạo ra các ví dụ được cho là kém hơn các minh họa chuyên 
gia. Phương pháp này lặp đi lặp lại xây dựng các mối quan hệ ưu tiên theo cặp giữa các mẫu 
được tạo bởi LLM này và các minh họa chuyên gia, có thể bao gồm các so sánh giữa các điểm 
kiểm tra huấn luyện khác nhau. Các cặp ưu tiên được xây dựng này sau đó được sử dụng để 
huấn luyện mô hình bằng thuật toán tối ưu hóa ưu tiên (ví dụ DPO). Chúng tôi đánh giá khả 
năng của DITTO trong việc học căn chỉnh phong cách và nhiệm vụ tinh vi qua các lĩnh vực 
như bài báo tin tức, email và bài viết blog. Ngoài ra, chúng tôi tiến hành một nghiên cứu 
người dùng thu thập một loạt các minh họa từ những người tham gia (N= 16). Trên các điểm 
chuẩn và nghiên cứu người dùng của chúng tôi, chúng tôi thấy rằng tỷ lệ thắng cho DITTO 
vượt trội hơn so với gợi ý vài lần, tinh chỉnh có giám sát và các phương pháp tự chơi khác 
trung bình 19% điểm. Bằng cách sử dụng trực tiếp các minh họa như phản hồi, DITTO cung 
cấp một phương pháp mới để tùy chỉnh hiệu quả các LLM.¹

1 GIỚI THIỆU
Các mô hình ngôn ngữ lớn (LLM) được huấn luyện cho mục đích sử dụng chung. Tuy nhiên, 
trong thực tế, chúng thường được áp dụng cho các nhiệm vụ rất cụ thể cho những người dùng rất 
cụ thể. Xem xét một nhiệm vụ đơn giản như viết email: email ưa thích của chúng ta phụ thuộc 
vào phong cách viết cá nhân, nhiệm vụ email cụ thể, hoặc đối tượng mục tiêu (một người bạn, 
người lạ, v.v.). Kết quả là, có thể có sự không khớp giữa phong cách phổ quát (Santurkar et al., 
2023; Chakrabarty et al., 2023) được huấn luyện vào một LLM thông qua hướng dẫn và tinh 
chỉnh ưu tiên, và phong cách cụ thể cần thiết cho các ứng dụng. Các đầu ra LLM cảm thấy không 
có ý kiến và chung chung vì sự không khớp này.

Trong khi các phương pháp hiện có như tinh chỉnh có giám sát hoặc ưu tiên là hiệu quả, chúng 
có thể đòi hỏi một kho tài liệu lớn về hành vi (không) chấp nhận được (theo thứ tự ≈1K mẫu 
(Zhou et al., 2024; Ouyang et al., 2022)), điều này lại đòi hỏi nỗ lực cao bất hợp lý từ một 
cá nhân. Các phương pháp RLAIF như Constitutional AI (Bai et al., 2022) tự động hóa việc 
thu thập ưu tiên theo cặp với một LLM, nhưng căn chỉnh các mô hình với các nguyên tắc chung 
có thể không nắm bắt được các ưu tiên tinh vi. Mặc dù việc gợi ý là hiệu quả về dữ liệu, việc 
tìm một gợi ý hiệu quả có thể tẻ nhạt—người dùng cuối thường dựa vào các phương pháp gợi ý 
mong manh (Zhou et al., 2022; Zamfirescu-Pereira et al., 2023). Làm thế nào chúng ta có thể 
truyền đạt hiệu quả các ưu tiên và căn chỉnh một mô hình ngôn ngữ với một cá nhân hoặc nhiệm 
vụ mới?

∗Đóng góp ngang nhau
¹Mã nguồn: https://github.com/SALT-NLP/demonstrated-feedback
1arXiv:2406.00888v2 [cs.CL] 18 Apr 2025

--- TRANG 2 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

Hình 1: DITTO lặp đi lặp lại căn chỉnh LLM với hành vi được minh họa. Khi người dùng cung cấp 
các minh họa (thông qua chỉnh sửa đầu ra của mô hình, lịch sử tương tác ưa thích trong quá khứ, 
hoặc viết ví dụ từ đầu), DITTO coi các minh họa này là được ưa thích hơn tất cả hành vi của mô 
hình, bao gồm cả các lần lặp sớm hơn của mô hình được huấn luyện. Việc sử dụng các minh họa 
như phản hồi cho phép tạo ra dữ liệu so sánh trực tuyến rẻ tiền và cho phép căn chỉnh vài lần 
với chỉ một số ít mẫu.

Bài báo này giới thiệu một khung để căn chỉnh LLM với các bối cảnh cụ thể bằng cách cung cấp 
một số lượng nhỏ các minh họa (Hình 1). Thay vì sử dụng gợi ý, nguyên tắc, hoặc ưu tiên theo 
cặp, chúng tôi cho thấy rằng chúng ta có thể đạt được căn chỉnh mạnh với các cá nhân bằng cách 
tận dụng một số lượng nhỏ các ví dụ hành vi mong muốn được cung cấp bởi người dùng. Các ví 
dụ này có thể được rút ra từ nhật ký tương tác hiện có của người dùng, hoặc từ các chỉnh sửa 
trực tiếp được thực hiện đối với đầu ra LLM. Phương pháp của chúng tôi, DITTO, hỗ trợ một 
số ít các minh họa này (<10) thành một bộ dữ liệu đáng kể các so sánh ưu tiên, bằng cách coi 
các minh họa của người dùng là được ưa thích hơn đầu ra mô hình từ cả LLM gốc và các lần lặp 
huấn luyện sớm hơn của mô hình. Bộ dữ liệu tăng cường này của các so sánh dựa trên minh họa 
sau đó có thể được sử dụng để cập nhật mô hình ngôn ngữ bằng thuật toán căn chỉnh như DPO 
(Rafailov et al., 2023). Chúng tôi cũng cho thấy rằng DITTO có thể được hiểu như một thuật toán 
học tập bắt chước trực tuyến, nơi dữ liệu được lấy mẫu từ LLM được sử dụng để phân biệt hành 
vi chuyên gia. Quan điểm này cho phép chúng tôi chứng minh rằng DITTO có thể ngoại suy vượt 
ra ngoài hiệu suất của người minh họa (§3).

Vì DITTO tập trung vào căn chỉnh cụ thể theo người dùng/nhiệm vụ, chúng tôi đánh giá DITTO 
thông qua (1) một đánh giá trên các bộ dữ liệu viết cụ thể theo tác giả (§4.1) và (2) một đánh 
giá người dùng (§4.2) trên các nhiệm vụ thế giới thực được định nghĩa bởi những người tham gia 
con người. Các bộ dữ liệu cụ thể theo tác giả của chúng tôi bao gồm việc viết từ bài viết blog 
đến email đến bài báo. Chúng tôi thấy rằng tỷ lệ thắng cho DITTO vượt trội hơn các phương 
pháp như SFT (tăng trung bình 11% điểm), các phương pháp tự chơi như SPIN (20.2% điểm), 
và gợi ý vài lần (33.4% điểm) trên Mistral 7B. Lợi thế của DITTO vẫn giữ nguyên ngay cả khi 
gợi ý vài lần được thực hiện trên một LLM mạnh hơn (GPT-4, 18% điểm). Tiếp theo, chúng tôi 
tiến hành một nghiên cứu người dùng (N= 16), yêu cầu các cá nhân chỉnh sửa các thế hệ từ GPT-4 
trong một nhiệm vụ viết email. Chúng tôi sử dụng các minh họa cuối cùng như đầu vào cho DITTO. 
Trong các đánh giá người dùng thực tế này, lợi thế của DITTO trở nên rõ ràng hơn: DITTO tiếp 
tục vượt trội hơn các đường cơ sở, bao gồm gợi ý vài lần (23.9% điểm), gợi ý do người dùng xây 
dựng (27.9% điểm), và SFT (12% điểm). Cuối cùng, trong một so sánh trực tiếp giữa các minh 
họa và phản hồi theo cặp, chúng tôi cho thấy rằng việc sử dụng các minh họa với DITTO hiệu 
quả hơn về mẫu một bậc độ lớn so với việc thu thập ưu tiên theo cặp cho các cá nhân.

2 CÔNG TRÌNH LIÊN QUAN

LLM và Tinh chỉnh Ưu tiên. Các mô hình ngôn ngữ lớn được huấn luyện trên lượng dữ liệu khổng 
lồ đã được biết đến là hoạt động tốt với việc gợi ý cẩn thận (Brown et al., 2020b; Wei et al., 
2022). Tuy nhiên, việc gợi ý có thể cực kỳ tẻ nhạt (Zamfirescu-Pereira et al., 2023) để thiết 
kế và thường nhạy cảm với các biến thể. Do đó, việc tinh chỉnh các mô hình này trên các bộ 
dữ liệu tuân theo hướng dẫn được tuyển chọn lớn (Mishra et al., 2022; Thoppilan et al., 2022; 
Chung et al., 2022) và/hoặc sử dụng RLHF, nơi LLM được huấn luyện để tối đa hóa một hàm 
thưởng được học từ ưu tiên con người như một bandit theo ngữ cảnh (Ziegler et al., 2019) đã 
trở nên cần thiết. Thông thường, điều này được thực hiện bằng các phương pháp kiểu gradient 
chính sách (Williams, 1992; Schulman et al., 2017) mặc dù các công trình gần đây hơn học trực 
tiếp từ dữ liệu ưu tiên (Rafailov et al., 2023; Hejna et al., 2024; Azar et al., 2023). Trong 
khi các phương pháp này hiệu quả trong các nhiệm vụ như tóm tắt (Stiennon et al., 2020; Wu 
& Hu, 2018; Wu et al., 2024) và tuân theo hướng dẫn (Ouyang et al., 2022; Nakano et al., 2021), 
chúng đòi hỏi hàng nghìn đến hàng trăm nghìn so sánh theo cặp để có được một ước tính chất 
lượng của thưởng. Điều này khiến chúng trở nên đắt đỏ cấm đoán cho một loạt rộng các ứng dụng, 
như huấn luyện một trợ lý viết tùy chỉnh hoặc xây dựng một chatbot cụ thể theo lĩnh vực. Tối 
ưu hóa Ưu tiên Nhóm (GPO) (Zhao et al., 2023) thực hiện một bước hứa hẹn hướng tới căn chỉnh 
vài lần của LLM; tuy nhiên, các nhóm ưu tiên phải được định nghĩa trước cho meta-learning, điều 
này đòi hỏi một bộ dữ liệu lớn. Mặt khác, Gao et al. (2024) sử dụng các chỉnh sửa trực tiếp 
để chưng cất các ưu tiên tiềm ẩn thành các nguyên tắc dựa trên gợi ý. Thay vì các nguyên tắc 
hoặc phản hồi theo cặp, DITTO trực tiếp học các ưu tiên từ một tập hợp các minh họa, tương tự 
như chỉnh sửa mô hình từ các ví dụ chuẩn (Hewitt et al., 2024). Rút ra từ các nghiên cứu trước 
về lập trình bằng minh họa và lập trình người dùng cuối trong HCI (Cypher, 1991; Cypher & 
Halbert, 1993), công trình của chúng tôi nhằm thu thập phản hồi ở mức độ tinh vi hơn so với 
ưu tiên nhị phân, nguyên tắc, hoặc gợi ý.

Tự Cải thiện. Các công trình gần đây sử dụng lấy mẫu lặp để cải thiện LLM. Các phương pháp 
như STaR (Zelikman et al., 2022; 2024; Andukuri et al., 2024) được giám sát bằng cách xác 
minh tính đúng đắn của đầu ra, trong khi Yuan et al. (2024) và Burns et al. (2023) sử dụng 
các mô hình ngôn ngữ (có thể mạnh hơn) như những người phê bình. Không giống như các phương 
pháp này, DITTO không đòi hỏi tín hiệu bên ngoài ngoài các minh họa, tương tự như các phương 
pháp tự chơi như SPIN (Chen et al., 2024). Không giống như SPIN—sử dụng hàng nghìn minh 
họa và được nhắm mục tiêu nhiều hơn về phía các bộ dữ liệu quy mô SFT—DITTO được thiết kế 
cho thích ứng nhanh trong bối cảnh hạn chế dữ liệu và do đó có một vài khác biệt chính. Cụ 
thể, DITTO không cập nhật chính sách tham chiếu và sử dụng các so sánh giữa các mô hình để 
chống quá khớp. Chúng tôi thấy rằng những thay đổi này quan trọng để có được hiệu suất tốt 
chỉ với một số ít minh họa. Trong các bối cảnh dữ liệu phong phú, các công trình khác đã chỉ 
ra rằng một hàm thưởng tiên tri (Gulcehre et al., 2023) hoặc mô hình (Lee et al., 2023; Song 
et al., 2024) là đủ để cung cấp phản hồi. Chúng tôi xem xét các nhiệm vụ như cá nhân hóa, 
mà không có dữ liệu phong phú hoặc tiên tri.

Học tập Bắt chước Trực tuyến. DITTO xây dựng dựa trên học tập bắt chước trực tuyến, điều 
này thu hút sự thành công lâu dài của việc học các hàm thưởng từ các so sánh (Fürnkranz et 
al., 2012; Akrour et al., 2012). Brown et al. (2019) đầu tiên cho thấy rằng với các minh họa 
được xếp hạng, người ta có thể cải thiện một chính sách vượt ra ngoài hiệu suất của người minh 
họa. Các tiếp theo sử dụng tiêm nhiễu tự động để loại bỏ các xếp hạng của con người (Brown et 
al., 2020a). Các phương pháp đương đại khác cho học tập bắt chước trực tuyến dựa trên các trò 
chơi đối kháng giữa những người chơi thưởng và chính sách (Ziebart et al., 2008; Ho & Ermon, 
2016). Trong trường hợp của chúng tôi, chúng tôi sử dụng một công thức bị ràng buộc KL, giống 
như Watson et al. (2023). Sikchi et al. (2022) tổng quát hóa trò chơi đối kháng thành một trò 
chơi xếp hạng và do đó sử dụng các so sánh được tạo ra như DITTO. Tuy nhiên, không giống như 
DITTO, các phương pháp này yêu cầu một cách rõ ràng việc học một hàm thưởng và được thiết kế 
cho điều khiển liên tục—không phải cho LLM.

3 DITTO

Trong khi công trình trước đây sử dụng hàng nghìn so sánh để căn chỉnh LLM, DITTO thay vào đó 
chỉ sử dụng một số ít minh họa chuyên gia để thay đổi hành vi của mô hình. Loại thích ứng nhanh 
và rẻ tiền này được kích hoạt bởi hiểu biết cốt lõi của chúng tôi: rằng dữ liệu so sánh trực tuyến 
có thể dễ dàng thu được từ các minh họa.

3.1 KÝ HIỆU VÀ NỀN TẢNG

Một mô hình ngôn ngữ có thể được xem như chính sách π(y|x) tạo ra một phân phối trên các hoàn 
thành y cho một gợi ý x. Trong RLHF, mục tiêu của chúng ta là huấn luyện một LLM để tối đa hóa 
một hàm thưởng r(x, y) đo chất lượng của một cặp gợi ý-hoàn thành (x, y). Thông thường, một 
ràng buộc phân kỳ KL được thêm vào để ngăn mô hình được cập nhật đi lệch quá xa khỏi một LM 
cơ sở (Ziegler et al., 2019), mà chúng tôi ký hiệu là πref. Tổng thể, các phương pháp RLHF tối 
ưu hóa mục tiêu sau,

JKL(π) = Ey∼π(·|x),x∼p[r(x, y) − α log(π(y|x)/πref(y|x))]                    (1)

mà tối đa hóa thưởng mong đợi trên phân phối gợi ý p dưới một ràng buộc KL được điều chỉnh bởi 
α. Thường thì, mục tiêu này được tối ưu hóa bằng một bộ dữ liệu so sánh có dạng {(x, yw, yl)}, 
nơi hoàn thành "thắng" yw được ưa thích hơn hoàn thành "thua" yl, mà chúng tôi viết là yw ⪰ yl.

Trong khi mục tiêu này phổ biến trong công trình trước đây (Ouyang et al., 2022; Rafailov et 
al., 2023), nó thường được áp dụng trong bối cảnh các hàm thưởng dựa trên dân số được học 
từ các bộ dữ liệu so sánh lớn được thu thập thông qua vô số người chú thích. Ngược lại, chúng 
tôi xem xét r(x, y) là mục tiêu của một cá nhân duy nhất. Trong chế độ này, việc thu thập 
hàng nghìn so sánh từ một người dùng là không khả thi. Thay vào đó, chúng tôi giả sử quyền 
truy cập vào một bộ dữ liệu nhỏ các minh họa chuyên gia, được ký hiệu DE. Chúng tôi giả sử 
các minh họa này được tạo ra từ chính sách chuyên gia πE = arg maxπ Ey∼π(·|x),x∼p[r(x, y)], 
mà tối đa hóa thưởng trong kỳ vọng. Trong khi các minh họa thường được sử dụng cho SFT, các 
phương pháp như vậy thường gặp khó khăn trong các bối cảnh hạn chế dữ liệu. Mặt khác, có thể 
khó khăn để gợi ý một mô hình "vượt qua" các tiên nghiệm được tạo ra bởi huấn luyện RLHF của 
nó. DITTO, như được mô tả trong phần tiếp theo, giải quyết những vấn đề này bằng cách trực 
tiếp tạo ra dữ liệu so sánh bằng đầu ra LM và các minh họa chuyên gia. Điều này có nghĩa là 
không giống như các mô hình tạo dữ liệu tổng hợp (Lee et al., 2023), DITTO không đòi hỏi một 
mô hình hoạt động tốt tại nhiệm vụ đã cho một cách tiên nghiệm.

--- TRANG 4 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

3.2 DITTO

Thuật toán 1: DITTO
Đầu vào: LM πref, minh họa DE={(xi, yE_i)}i∈N,
kích thước mẫu M, tần suất mẫu K
Khởi tạo: π0←SFT(πref,DE), t= 0
while chưa hội tụ do
    Dt← ∪N_i=1{(xi, yj∼πt(·|xi))}M_j=1
    for k= 1,2,3, ..., K do
        Lấy mẫu lô B={(x, yw, yl)} của
        so sánh từ xếp hạng được tạo:
        DE⪰ D_t⪰ D_t−1⪰...⪰ D_0
        πt←DPO(πt, B)# Cập nhật chính sách
    t←t+ 1

Hiểu biết chính của DITTO là bản thân LM, cùng với các minh họa chuyên gia, có thể tạo ra các bộ dữ liệu so sánh cho việc căn chỉnh, loại bỏ nhu cầu thu thập một số lượng lớn ưu tiên theo cặp. Điều này dẫn đến một mục tiêu giống như đối chiếu, nơi các minh họa chuyên gia là những tích cực. Ở đây chúng tôi cung cấp một giải thích trực quan về DITTO; sau này chúng tôi cung cấp một dẫn xuất lý thuyết hơn trong §3.3.

Tạo So sánh. Xem xét một hoàn thành được lấy mẫu từ chính sách chuyên gia, yE∼πE(·|x). Bởi vì là "chuyên gia", yE có thể có thưởng cao, vì πE theo định nghĩa là bộ tối đa hóa thưởng trong kỳ vọng. Do đó, chúng ta sẽ mong đợi các mẫu từ bất kỳ chính sách khác π nào có thưởng ít hơn hoặc bằng những của πE, tức là, ∀π, EπE[r(x, y)]≥Eπ[r(x, y)]. Sử dụng quan sát này, chúng ta có thể xây dựng các so sánh (x, yE, yπ) nơi yE⪰yπ bằng cách đơn giản lấy mẫu các hoàn thành yπ∼π(·|x) cho mỗi cặp minh họa-gợi ý trong DE. Mặc dù các so sánh như vậy được dẫn xuất từ các chính sách thay vì các ví dụ cá nhân, chúng đã chứng minh hiệu quả trong công trình trước đây (Brown et al., 2020a). Một phương pháp ngây thơ cho DITTO sau đó sẽ tối ưu hóa Eq. (1) bằng bộ dữ liệu này và một thuật toán RLHF có sẵn. Làm như vậy sẽ tăng xác suất của các phản hồi chuyên gia trong khi giảm xác suất của các mẫu mô hình hiện tại, không giống như tinh chỉnh tiêu chuẩn chỉ làm cái trước. Quan trọng, việc sử dụng các mẫu từ π cho phép chúng ta xây dựng một bộ dữ liệu ưu tiên không giới hạn chỉ với một vài minh họa. Tuy nhiên, chúng ta có thể làm tốt hơn bằng cách xem xét khía cạnh thời gian của quá trình học tập.

Từ So sánh đến Xếp hạng. Việc sử dụng các so sánh chỉ giữa chuyên gia và chính sách đơn π có thể không đủ để có được hiệu suất tốt. Làm như vậy giảm khả năng chỉ tại π cụ thể đó, dẫn đến các vấn đề quá khớp làm phiền SFT trong các chế độ dữ liệu thấp. Tương tự như phát lại trong RL (Mnih et al., 2015), chúng ta có thể xem xét dữ liệu được tạo ra từ tất cả các chính sách được học theo thời gian. Tại lần lặp đầu tiên, hãy để chính sách ban đầu là π0. Chúng ta có thể lấy mẫu từ chính sách này để lắp ráp một bộ dữ liệu D0={(x, yπ0)}. Sau đó, chúng ta có thể tạo ra dữ liệu so sánh cho RLHF như yE⪰yπ0, mà chúng tôi ký hiệu là DE⪰D0 để ngắn gọn. Sử dụng các so sánh được tạo ra này, chúng ta cập nhật π0 để có được một chính sách mới π1. Theo định nghĩa, EπE[r(x, y)]≥Eπ1[r(x, y)] cũng vậy. Nó theo đó là chúng ta cũng có thể tạo ra các so sánh bằng π1 như DE⪰D1. Tiếp tục quy trình này, chúng ta tạo ra một bộ dữ liệu so sánh ngày càng đa dạng hơn bằng tất cả các chính sách trước đây. Chúng tôi gọi đây là các so sánh "phát lại".

Trong khi phương pháp này về mặt lý thuyết nhất quán, nó giảm khả năng của LM ở mọi nơi ngoại trừ tại các minh họa chuyên gia. Mặc dù cho phép trong các tình huống dữ liệu phong phú, điều này cũng có thể dẫn đến quá khớp với một DE nhỏ. Tuy nhiên, nếu chúng ta giả sử rằng chính sách cải thiện tại mỗi lần lặp, tức là Eπt+1[r(x, y)]≥Eπt[r(x, y)], thì chúng ta cũng có thể xem xét các so sánh giữa các chính sách trong quá trình học tập. Không giống như các so sánh với chuyên gia, chúng ta không đảm bảo rằng điều này giữ; tuy nhiên, trong thực tế, chúng tôi thấy rằng các mô hình có xu hướng cải thiện với mỗi lần lặp, có thể do tính lồi của cả mô hình thưởng và Eq. (1). Điều này cho phép chúng ta lấy mẫu các so sánh giữa xếp hạng hoàn chỉnh của các chính sách:

DE⪰Dt⪰Dt−1⪰...⪰D1⪰D0.                                                         (2)

Hiệu ứng của việc thêm các so sánh "giữa các mô hình" và "phát lại" này là khả năng của các mẫu sớm hơn (ví dụ, những trong D1) bị đẩy xuống nhiều hơn so với những của các mẫu sau này (ví dụ, những trong Dt), làm mịn cảnh quan thưởng ngầm. Việc triển khai thực tế của chúng tôi tổng hợp một số ít các so sánh giữa các mô hình này ngoài các so sánh với chuyên gia.

Một Thuật toán Thực tế. Trong thực tế, thuật toán DITTO là một quy trình lặp bao gồm ba thành phần đơn giản như được nêu trong Thuật toán 1. Đầu tiên, chúng tôi bắt đầu bằng cách chạy tinh chỉnh có giám sát trên tập hợp các minh họa chuyên gia cho một số bước gradient hạn chế. Chúng tôi đặt đây là chính sách ban đầu π0. Thứ hai, chúng tôi lấy mẫu các so sánh: tối đa K lần trong quá trình huấn luyện, chúng tôi xây dựng một bộ dữ liệu mới Dt bằng cách lấy mẫu M hoàn thành từ πt cho mỗi trong số N minh họa trong DE và thêm nó vào xếp hạng trên các chính sách Eq. (2). Khi lấy mẫu các so sánh từ Eq. (2) mỗi lô B bao gồm 70% so sánh "trực tuyến" DE⪰Dt, 20% so sánh "phát lại" có dạng DE⪰Di<t, và 10% "so sánh giữa các mô hình" có dạng Di≤t⪰Dj<i. Cuối cùng, chúng tôi cập nhật chính sách bằng RLHF. Cụ thể, sử dụng các lô được lấy mẫu thông qua quy trình nói trên, chúng tôi cập nhật chính sách πt để có được πt+1 bằng hàm mất mát DPO (Rafailov et al., 2023)

LDPO(π,D) = −E(x,yw,yl)∼D[log σ(α log(π(yw|x)/πref(yw|x)) − α log(π(yl|x)/πref(yl|x)))]

nơi σ là hàm logistic từ mô hình ưu tiên Bradley-Terry. Trong mỗi cập nhật, chúng tôi không cập nhật mô hình tham chiếu πref từ chính sách SFT để tránh đi lệch quá xa khỏi khởi tạo. DITTO có thể hỗ trợ bất kỳ phương pháp tối ưu hóa ưu tiên trực tiếp nào như một phần của bước cuối cùng (ví dụ KTO Ethayarajh et al. (2024) hoặc SimPO Meng et al. (2024)). Trong thực tế, chúng tôi thấy rằng lựa chọn chính xác của thuật toán tối ưu hóa ưu tiên có hiệu ứng hạ lưu hạn chế, vì vậy chúng tôi mặc định DPO cho tất cả các thí nghiệm.

3.3 DẪN XUẤT DITTO NHƯ HỌC TẬP BẮT CHƯỚC TRỰC TUYẾN

DITTO có thể được dẫn xuất thông qua một quan điểm học tập bắt chước trực tuyến, nơi các minh họa chuyên gia được sử dụng cùng với dữ liệu trực tuyến để đồng thời học một hàm thưởng và chính sách. Cụ thể, người chơi chính sách tối đa hóa thưởng mong đợi maxπ J(π, r), khi người chơi thưởng tối thiểu hóa mất mát của nó minr L(Dπ, r) trên một bộ dữ liệu trực tuyến Dπ. Cụ thể, chúng tôi khởi tạo vấn đề tối ưu hóa này bằng mục tiêu chính sách trong Eq. (1) và mất mát mô hình thưởng tiêu chuẩn

min_r {−E(x,yw,yl)∼Dπ[log σ(r(x, yw) − r(x, yl))]
s.t. π = arg max_π JKL(π, r)}                                                    (3)

Như được thực hiện trong công trình trước đây (Sikchi et al., 2022), chúng tôi lấy Dπ là một bộ dữ liệu các so sánh sao cho yπ⪰yπ′ nếu Eπ[r(x, y)]≥Eπ′[r(x, y)]. Chỉ số π chỉ ra rằng Dπ chứa các so sánh trực tuyến giữa π và chuyên gia πE. Bằng cách sử dụng các lựa chọn khác nhau của bộ điều chỉnh và dữ liệu so sánh, người ta có thể đạt được các mục tiêu IRL nghịch đảo khác nhau (Ho & Ermon, 2016).

Dẫn xuất DITTO. Bước đầu tiên trong việc đơn giản hóa Eq. (3) là giải quyết tối đa hóa chính sách bên trong. May mắn thay, từ Ziebart (2010) chúng ta biết rằng mục tiêu chính sách JKL có một giải pháp dạng đóng có dạng π⋆(y|x) = πref(y|x)e^(r(x,y)/α)/Z(x) nơi Z(x) là hàm phân vùng chuẩn hóa phân phối. Đáng chú ý, điều này thiết lập một song ánh giữa các chính sách và hàm thưởng mà chúng ta có thể sử dụng để loại bỏ tối ưu hóa bên trong. Bằng cách sắp xếp lại giải pháp này, chúng ta có thể viết hàm thưởng r như

r(x, y) = α log(π⋆(y|x)/πref(y|x)) − α log Z(x).

Hơn nữa, công trình trước đây (Rafailov et al., 2024) cho thấy rằng việc tham số hóa lại này có thể biểu thị bất kỳ hàm thưởng nào. Do đó, chúng ta có thể thực hiện một thay đổi biến từ r sang π bằng thay thế vào Eq. (3), cho chúng ta mục tiêu DITTO

min_π −EDπ[log σ(α log(π(yw|x)/πref(yw|x)) − α log(π(yl|x)/πref(yl|x)))].

Lưu ý rằng giống như DPO, chúng tôi ước tính ngầm hàm thưởng. Không giống như DPO, DITTO phụ thuộc vào một bộ dữ liệu ưu tiên trực tuyến Dπ. Ít nhất, bộ dữ liệu ưu tiên trực tuyến nên chứa các so sánh πE⪰π, ∀π. Tuy nhiên, bất kỳ ưu tiên nào nhất quán với hàm thưởng thực tế cũng có thể được sử dụng thêm. Chúng tôi để lại khám phá này cho công trình tương lai.

Tại sao DITTO hoạt động tốt hơn chỉ SFT? Một lý do cho hiệu suất tương đối cao của DITTO là nó sử dụng nhiều dữ liệu hơn SFT bằng cách tạo ra các so sánh. Một lý do khác là các phương pháp học tập bắt chước trực tuyến có thể, trong một số trường hợp, hoạt động tốt hơn người minh họa trong khi SFT chỉ bắt chước các minh họa. Trong khi điều này được biết đến trong cộng đồng IRL, chúng tôi cho thấy kết quả sau trong Phụ lục B để liên kết khả năng ngoại suy của DITTO vượt ra ngoài người minh họa với hai đo lường phân kỳ.

Bổ đề 3.1. (Thích ứng từ Brown et al. (2020a)) Hãy để π⋆ là chính sách tối ưu cho Eq. (1) và π̂ là chính sách được ước tính bởi DITTO bằng các minh họa chuyên gia DE. Ngoại suy vượt ra ngoài người minh họa, tức là Eπ̂[r(x, y)] > EDE[r(x, y)] được đảm bảo nếu JKL(π⋆) − EDE[r(x, y)] > αDKL(π̂||π⋆) − αDKL(π̂||πref).

--- TRANG 6 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

4 THÍ NGHIỆM

Chúng tôi đầu tiên phác thảo các điểm chuẩn, tập trung vào các nhiệm vụ với ưu tiên chủ quan (ví dụ, viết email, tiểu luận, bài báo). Sau đó chúng tôi thảo luận về đánh giá tự động, so sánh DITTO với một số đường cơ sở, và phác thảo kết quả. Cuối cùng, chúng tôi tiến hành một nghiên cứu người dùng với DITTO, thu thập các minh họa từ những người tham gia.

4.1 ĐIỂM CHUẨN TĨNH

Dữ liệu Đo lường căn chỉnh vài lần với DITTO đòi hỏi các minh họa từ các cá nhân thay vì các bộ dữ liệu tổng hợp. Do đó chúng tôi xây dựng dựa trên các bộ dữ liệu Xác định Tác giả (AA) trước đây. Nhiệm vụ AA đòi hỏi người ta xác định tác giả a nào từ một tập hợp tác giả A đã viết một tài liệu cụ thể. Chúng ta có thể tái khung nhiệm vụ phân loại AA trước đây như căn chỉnh hiệu quả: việc căn chỉnh một LLM với một tác giả cụ thể sẽ dẫn đến các thế hệ có nhiều khả năng được gán cho cùng tác giả đó. Chúng tôi thu thập dữ liệu từ 20 tác giả riêng biệt từ hai nguồn: (1) email và bài viết blog từ bộ dữ liệu CMCC (Goldstein et al., 2008) chỉ chứa một tác giả và (2) bài báo tin tức từ bộ dữ liệu CCAT (Lewis et al., 2004). Các điểm chuẩn AA của chúng tôi bao gồm một loạt đa dạng các nhiệm vụ ở cấp độ tác giả; các nhiệm vụ trải dài từ viết bài biên tập tài chính đến các bài viết ý kiến về các chủ đề gây tranh cãi. Hiệu suất cao trên CMCC / CCAT50 đòi hỏi khái quát hóa không tầm thường qua các gợi ý. Để biết thêm chi tiết bộ dữ liệu và ví dụ nhiệm vụ, chúng tôi gửi độc giả đến Phụ lục C.

Phân chia và Tiền xử lý Một số điểm chuẩn của chúng tôi có nhiều mẫu viết cho mỗi tác giả hơn những cái khác. Trong khi CCAT gốc có thể có hơn 50 mẫu mỗi tác giả, CMCC có thể có ít nhất 12. Để kiểm soát số lượng mẫu, chúng tôi chọn ngẫu nhiên tập hợp nhỏ nhất các minh họa có sẵn từ mỗi tác giả qua các phân chia huấn luyện của chúng tôi (12) cho các thí nghiệm của chúng tôi. Chúng tôi chọn ngẫu nhiên 10 tác giả từ mỗi bộ dữ liệu, sử dụng 7 mẫu để huấn luyện, và chia phần còn lại thành kiểm tra và xác thực. Bảng 4 trong Phụ lục mô tả số lượng cuối cùng train/val/test qua mỗi điểm chuẩn.

Mô hình và Đường cơ sở Cùng với DITTO, chúng tôi đánh giá tinh chỉnh có giám sát (SFT), kiểm tra xem việc tinh chỉnh đơn giản trên các minh họa chuyên gia DE lâu hơn có hiệu quả không. Chúng tôi cũng đánh giá SPIN (Chen et al., 2024), một phương pháp tự chơi lặp được thiết kế để thay thế SFT. Cuối cùng, chúng tôi kiểm tra gợi ý zero-shot và few-shot, bao gồm các minh họa trực tiếp trong ngữ cảnh của mô hình. Đối với gợi ý few-shot, chúng tôi thêm toàn bộ tập huấn luyện của các minh họa tác giả trong ngữ cảnh. Chúng tôi cũng đã thử kỹ thuật gợi ý với các ràng buộc zero-shot (ví dụ, gợi ý mô hình không nghe như một LM), với thành công hạn chế (xem Phụ lục E). Các thí nghiệm của chúng tôi đòi hỏi một LLM tuân theo hướng dẫn. Chúng tôi sử dụng Mistral Instruct v0.2 7B như một điểm khởi đầu (Jiang et al., 2023) và huấn luyện bằng LoRA (Hu et al., 2021). Chúng tôi đã thử tinh chỉnh đầy đủ cho một số ít tác giả nhưng quan sát không có sự khác biệt đáng kể. Do đó chúng tôi sử dụng LoRA cho tất cả các thí nghiệm. Cuối cùng, chúng tôi so sánh với gợi ý zero/few-shot với một LLM mạnh hơn (GPT-4). Chi tiết siêu tham số có trong Phụ lục D.

Đánh giá Tự động Cho rằng các bộ dữ liệu của chúng tôi chứa tổng cộng 20 tác giả, chúng tôi phải huấn luyện và đánh giá một tập hợp lớn các mô hình (20 tác giả x 7 mô hình huấn luyện = 140 mô hình). Để tạo thuận lợi cho quá trình đánh giá, chúng tôi sử dụng GPT-4² để so sánh các đầu ra của mô hình qua các điều kiện khác nhau. Công trình trước đây đã sử dụng GPT để cả chú thích và đánh giá văn bản (Zheng et al., 2024). Nói chung, hiệu suất tụt hậu so với đánh giá con người; tuy nhiên, để phát hiện tác giả và tương tự phong cách, công trình trước đây đã chỉ ra rằng phân loại dựa trên mô hình thực sự đáng tin cậy hơn so với những người không chuyên (Krishna et al., 2020; Hallinan et al., 2023; Liu et al., 2024; Liu & May, 2024) và đánh giá GPT-4 thường vượt trội hơn các số liệu tự động khác (Kim et al., 2023), cho phép chúng tôi mở rộng tìm kiếm siêu tham số và chạy đánh giá một cách hiệu quả hơn về chi phí.

Trong bối cảnh của chúng tôi, chúng tôi sử dụng GPT-4 để xác định xem một văn bản có nghe giống như một tác giả cụ thể nhiều hơn hay ít hơn. Cho một văn bản do tác giả viết t và hai cặp văn bản được tạo ra từ các điều kiện khác nhau a và b, chúng tôi gợi ý GPT-4 để chọn văn bản khớp gần nhất với văn bản xác thực hoặc kiểm tra t, và tính toán tỷ lệ thắng đối đầu trung bình. Để tính đến thiên vị thứ tự, chúng tôi hoán đổi thứ tự và trung bình các phán quyết. Gợi ý đánh giá của chúng tôi và chi tiết điểm chuẩn hiệu suất được nêu trong Phụ lục F.

Kết quả Kết quả chính của chúng tôi, được đánh giá với đánh giá GPT-4, được tóm tắt trong Bảng 1. Trung bình trên tất cả tác giả, DITTO vượt trội hơn tất cả đường cơ sở, với tỷ lệ thắng trung bình 77.09% qua cả CMCC (71.67%) và CCAT50 (82.50%). Trên CCAT50, DITTO vượt trội hơn tất cả đường cơ sở qua các tác giả trừ một. Trên CMCC, DITTO vượt trội hơn tất cả đường cơ sở khác cho 5/10 tác giả, theo sau bởi gợi ý few-shot

²Chúng tôi sử dụng phiên bản gpt-4-0613 của GPT-4. Chúng tôi quan sát rằng các phiên bản Turbo của GPT-4 thiên vị hơn về phía đầu ra của riêng chúng. Các truy vấn được chạy từ ngày 20 tháng 12, 2023 đến ngày 10 tháng 5, 2024.

--- TRANG 7 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

[Bảng 1: Đánh giá GPT-4: Tỷ lệ thắng đối đầu giữa các phương pháp qua các phân chia kiểm tra điểm chuẩn. DITTO vượt trội hơn tất cả phương pháp đường cơ sở trung bình và qua đa số các tác giả cá nhân. a1...a10 đại diện cho một mô hình đơn được huấn luyện trên một trong mười tác giả được lấy mẫu từ mỗi bộ dữ liệu (xem §4). Kết quả được trung bình qua 3 lần chạy, với 3 mẫu được tạo ra từ mỗi mô hình với nhiệt độ 1.0. Chúng tôi cũng báo cáo tỷ lệ thắng trung bình qua các tác giả, cùng với lỗi chuẩn của trung bình (avgsem).]

cho 3/10. Trong khi SFT phục vụ như một đường cơ sở mạnh (56.78% trên CMCC, 73.89% trên CCAT), DITTO cung cấp một cải thiện tỷ lệ thắng trung bình ↑11.7% điểm so với chỉ SFT.

Các đường cơ sở được gợi ý cũng tụt hậu xa so với DITTO, đặc biệt là các mô hình zero-shot (bao gồm nguồn đóng) (giảm trung bình ↓54.4% điểm trên Mistral, ↓51.5% điểm trên GPT-4). Trong khi GPT-4 zero-shot đã được tinh chỉnh sử dụng RLHF, chúng tôi nghi ngờ rằng phản hồi huấn luyện này khác biệt đáng kể so với của các tác giả trong cả CMCC và CCAT50. Thêm tất cả các trường hợp huấn luyện như một gợi ý few-shot có giúp: tỷ lệ thắng cho gợi ý few-shot tăng so với zero-shot cho cả LLM dựa trên Mistral (↑20.94% điểm) và GPT-4 (↑22.95% điểm). Tuy nhiên, bao gồm các ví dụ few-shot vẫn tụt hậu so với việc áp dụng DITTO (giảm trung bình ↓37.35% điểm cho Mistral; ↓26.99% điểm cho GPT-4). Thay đổi số lượng minh họa trong gợi ý few-shot cũng không mang lại cải thiện (Hình F.2 trong Phụ lục). Chúng tôi nghi ngờ các tiên nghiệm RLHF cơ bản cho LLM có sẵn khá mạnh. Về mặt định tính, các thế hệ few-shot vẫn nghe có vẻ được tạo ra bởi GPT so với DITTO (Bảng 7 trong Phụ lục).

Trong khi chúng tôi có kiểm tra một phương pháp huấn luyện tự cải thiện khác (SPIN), chúng tôi thấy rằng hiệu suất thấp hơn DITTO (giảm trung bình ↓9.3% điểm)—chúng tôi nghi ngờ rằng các quyết định thiết kế cho SPIN (ví dụ, cập nhật chính sách tham chiếu, loại trừ các so sánh giữa chính sách / phát lại) được nhắm mục tiêu về phía các bộ dữ liệu quy mô SFT. Chúng tôi phân tích các quyết định này trong §5.1 và đề xuất lý do cho sự suy giảm hiệu suất.

Cuối cùng, chúng tôi chạy một kiểm định ANOVA để xác định xem có sự khác biệt đáng kể giữa các điều kiện không, và sau đó chạy một kiểm định Tukey để xác định điều kiện cụ thể nào là đáng kể. Các cải thiện của DITTO là đáng kể (p < 0.05) so với tất cả điều kiện khác trong Bảng 1, loại trừ GPT few-shot trên CMCC.

4.2 NGHIÊN CỨU NGƯỜI DÙNG: KIỂM TRA KHÁI QUÁT HÓA CHO CÁC NHIỆM VỤ TỰ NHIÊN

Các điểm chuẩn tĩnh của chúng tôi đã tập trung vào các bộ dữ liệu xác định tác giả có sẵn trước, sử dụng GPT-4 để đo căn chỉnh. Tuy nhiên, đánh giá GPT-4 thể hiện thiên vị tự tăng cường, có thể thổi phồng hiệu suất cho các thế hệ giống LLM (Zheng et al., 2024; Panickssery et al., 2024). Do đó chúng tôi đánh giá DITTO trong một bối cảnh tự nhiên hơn; chúng tôi tiến hành một nghiên cứu người dùng để đánh giá DITTO và yêu cầu người dùng cung cấp các minh họa cho một loạt các nhiệm vụ. Như các đường cơ sở, chúng tôi sử dụng GPT-4 được gợi ý zero-shot và few-shot, cùng với SFT. Ngoài ra, chúng tôi yêu cầu những người tham gia tự gợi ý các mô hình bằng cách lặp đi lặp lại tác giả gợi ý của riêng họ để hướng dẫn đầu ra mô hình. Zero-shot, few-shot, và tự gợi ý mô phỏng những gì hầu hết người dùng sẽ làm ngày nay để hướng dẫn LLM, và SFT cung cấp một đường cơ sở tinh chỉnh mạnh.

Chúng tôi tuyển dụng 16 người tham gia từ truyền thông xã hội (Twitter). Nhiều người tham gia của chúng tôi là sinh viên tiến sĩ quen thuộc với việc gợi ý LLM; do đó, đường cơ sở tự gợi ý của chúng tôi cung cấp một đường cơ sở mạnh cho kỹ thuật gợi ý bổ sung. Những người tham gia được trả $30 / giờ; nghiên cứu của chúng tôi được phê duyệt bởi một IRB.

Tóm tắt Nghiên cứu Người dùng Nghiên cứu người dùng bao gồm hai phần. Trong phần đầu tiên, chúng tôi yêu cầu những người tham gia chỉ định bốn nhiệm vụ viết email (ví dụ, Viết một email cho cố vấn của bạn yêu cầu phản hồi). Những người tham gia được yêu cầu cung cấp hai minh họa cho hai trong số các nhiệm vụ (tổng cộng 4 minh họa huấn luyện). Để giúp động não các nhiệm vụ, chúng tôi tạo ra các gợi ý nhiệm vụ cụ thể với GPT-4; những người tham gia có thể chọn từ những cái này hoặc cung cấp các nhiệm vụ tùy chỉnh của riêng họ. Chúng tôi chia ngẫu nhiên hai gợi ý nhiệm vụ thành huấn luyện, và lưu hai cho kiểm tra; những người tham gia đưa ra hai minh họa mỗi cho cả gợi ý huấn luyện, để mô phỏng một người dùng sẵn sàng chỉ bỏ ra nỗ lực tối thiểu. Người dùng được cung cấp các thế hệ mặc định từ GPT-4 để hỗ trợ tác giả minh họa, mà họ có thể chỉnh sửa hoặc bỏ qua. Trong phần thứ hai, chúng tôi sử dụng hai nhiệm vụ từ tập kiểm tra và cho những người tham gia thấy các thế hệ qua tất cả phương pháp. Chúng tôi lấy mẫu một đầu ra từ mỗi phương pháp (tự gợi ý, zero-shot, few-shot, SFT, và DITTO), và thu thập 10 ưu tiên theo cặp cho mỗi gợi ý kiểm tra (dẫn đến 20 ưu tiên tổng cộng cho mỗi người dùng). Tổng cộng, chúng tôi thu thập tổng cộng 320 ưu tiên theo cặp qua 16 người dùng. Tất cả so sánh được thực hiện mù về điều kiện. Chi tiết nghiên cứu người dùng bổ sung (ví dụ, giao diện, ví dụ về phản hồi được minh họa, gợi ý để tạo ra nhiệm vụ, v.v.) có trong Phụ lục G.

[Bảng 2: Kết quả Nghiên cứu Người dùng. Trong tỷ lệ thắng đối đầu được chú thích bởi con người, DITTO vượt trội hơn các đường cơ sở tự gợi ý, few-shot, và zero-shot GPT-4, cùng với SFT.]

Kết quả Kết quả nghiên cứu người dùng của chúng tôi chứng thực các phát hiện từ các điểm chuẩn tĩnh. DITTO vượt trội hơn các phương pháp đường cơ sở trong việc căn chỉnh với các ưu tiên được minh họa (Bảng 2), với DITTO (68.8% tỷ lệ thắng) > SFT (55.5%) > few-shot (51.6%) > tự gợi ý (46.9%) > zero-shot (27.3%). DITTO tốt hơn đáng kể so với tất cả phương pháp khác (kiểm định ANOVA + Tukey, p < 0.05). Ngoài ra, người dùng thường gặp khó khăn với việc diễn đạt ưu tiên thành gợi ý: tự gợi ý hơi kém hiệu suất so với việc cung cấp minh họa trong gợi ý few-shot, và kém hiệu suất đáng kể so với DITTO. Chúng tôi cũng quan sát định tính rằng người dùng thường chỉnh sửa gần một nửa đầu ra mặc định từ GPT-4 khi tác giả minh họa (ví dụ trong Phụ lục G), với khoảng cách chỉnh sửa Levenshtein chuẩn hóa trung bình = 0.43. Các chỉnh sửa lớn cho đầu ra một mình làm nổi bật hiệu quả của phản hồi được minh họa như một tương tác.

Để hiểu rõ hơn tại sao người dùng trong nghiên cứu của chúng tôi chọn đầu ra DITTO, chúng tôi khớp một mô hình Fightin'-Words để xác định sự khác biệt từ vựng giữa các thế hệ từ GPT-4 và DITTO (Monroe et al., 2008). Fightin'-Words được sử dụng để xác định các từ khác biệt đáng kể về tần suất giữa các kho tài liệu. Nó tạo ra tỷ lệ log-odds và z-scores, đo lường khả năng một từ xuất hiện trong một kho tài liệu so với kho khác. Nhiều từ xuất hiện trong đầu ra được tạo ra bởi GPT so với DITTO (Bảng 6) đến từ các cụm từ sáo rỗng: "greatly appreciate your time and understanding" hoặc "hope this message finds you well." So với DITTO, GPT cũng thường xuyên tạo ra các câu đề cập đến "trust" và "initiative" trong email được soạn cho các cộng tác viên gần gũi. Chúng tôi nghi ngờ phong cách viết của GPT được liên kết chặt chẽ với các tiên nghiệm RLHF của nó.

5 KHI NÀO DITTO HOẠT ĐỘNG?

[Bảng 3: Tỷ lệ thắng đối đầu qua các phân tích thuật toán DITTO trên CMCC. Chúng tôi thí nghiệm với việc lấy mẫu tất cả phủ định ở đầu, phân tích replay và comparisons interpolicy, và cập nhật chính sách tham chiếu.]

Một người dùng phải quyết định về một số điều kiện tiên quyết trước khi sử dụng DITTO, từ họ có bao nhiêu minh họa đến họ phải lấy mẫu bao nhiêu phủ định từ LM. Chúng tôi khám phá tác động của các quyết định này và tập trung vào CMCC, vì nó bao gồm một loạt rộng hơn các nhiệm vụ so với CCAT. Chúng tôi cũng phân tích hiệu quả mẫu của minh họa so với phản hồi theo cặp trong nghiên cứu người dùng của chúng tôi.

5.1 BIẾN ĐỔI THUẬT TOÁN

DITTO bao gồm một số siêu tham số: cụ thể, số lần lặp DITTO N={1..4} và mẫu phủ định M={2...10} được tạo ra từ chuỗi chính sách của chúng tôi. Riêng biệt, chúng tôi phân tích các thành phần của DITTO, như việc sử dụng so sánh giữa chính sách (Di≤t⪰Dj<i) và phát lại (DE⪰Di<t). Chúng tôi cũng kiểm tra một phân tích nơi chúng tôi không lấy mẫu lại dữ liệu trong quá trình huấn luyện và thay vào đó lấy mẫu tất cả phủ định chỉ ở đầu, và nơi chúng tôi cập nhật πref=πt tại mỗi lần lặp như SPIN (Chen et al., 2024). Lưu ý rằng hiệu suất DITTO thay đổi từ người dùng này sang người dùng khác. Để tính đến sự thay đổi giữa các tỷ lệ thắng cấp độ tác giả, chúng tôi chuyển đổi tỷ lệ thắng trung bình của mỗi tác giả thành % cải thiện từ tỷ lệ thắng phân tích ban đầu của họ (ví dụ, % cải thiện từ 1 lần lặp DITTO, 2 phủ định được tạo ra, hoặc 1 minh họa huấn luyện).

--- TRANG 9 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

[Hình 2: Tỷ lệ thắng đối đầu qua các biến đổi siêu tham số DITTO trên CMCC. Đầu tiên, tăng số lần lặp DITTO cải thiện hiệu suất đánh giá GPT-4 (trái). Tăng số phủ định được tạo ra cũng giảm sự thay đổi DITTO qua người dùng trong khi cải thiện hiệu suất DITTO (giữa). Cuối cùng, tăng minh họa cũng cải thiện hiệu suất, nhưng chúng tôi quan sát lợi nhuận giảm dần (phải). Thanh lỗi tương ứng với lỗi chuẩn của trung bình qua các tác giả.]

Đầu tiên, tăng số lần lặp DITTO thường cải thiện hiệu suất (Hình 2). So sánh Lần lặp 1 với Lần lặp 4, chúng tôi quan sát tăng tương đối 31.5% trong tỷ lệ thắng đánh giá GPT-4. Cải thiện là không đơn điệu—trong Lần lặp 2, hiệu suất giảm nhẹ (-3.4%). Các lần lặp sớm có thể tạo ra các mẫu nhiễu hơn, có thể giảm hiệu suất. Mặt khác, tăng mẫu phủ định cải thiện hiệu suất DITTO một cách đơn điệu. Tạo ra 10 phủ định cho mỗi minh họa trong tập huấn luyện, ví dụ, mang lại cải thiện tỷ lệ thắng 21.09% so với chỉ 2. Hơn nữa, khi chúng ta lấy mẫu nhiều phủ định hơn tăng, sự thay đổi trong hiệu suất DITTO giảm. Tuy nhiên, có một sự đánh đổi liên quan đến việc tăng số mẫu phủ định: thời gian chạy của DITTO cũng sẽ tăng. Ngoài ra, chúng tôi thấy rằng các lần lặp được thêm vào (>6) cuối cùng dẫn đến suy giảm hiệu suất, có thể do quá khớp. Lấy mẫu nhiều phủ định hơn (>10) cũng mang lại hiệu suất đạt đỉnh.

Chúng tôi cũng thấy rằng việc phân tích các thành phần của DITTO dẫn đến giảm hiệu suất (Bảng 3). Nếu chúng ta lấy mẫu tất cả phủ định ở đầu—thay vì lấy mẫu lại một cách lặp trong một thời trang trực tuyến—chúng tôi quan sát rằng tỷ lệ thắng so với việc sử dụng DITTO giảm từ 70.1% xuống 57.3%. Trong khi lấy mẫu lại một cách lặp cải thiện hiệu suất, liên tục cập nhật πref trong quá trình trực tuyến này có thể làm suy giảm đáng kể hiệu suất: tỷ lệ thắng giảm từ 70.1% xuống 45.8%. Chúng tôi nghi ngờ cập nhật πref dẫn đến quá khớp tiềm năng. Cuối cùng, cả so sánh phát lại và giữa chính sách đều giúp DITTO. Loại bỏ so sánh phát lại và giữa chính sách giảm tỷ lệ thắng từ DITTO lần lượt 6.5 và 2 điểm.

Một yếu tố gây nhiễu tiềm năng cho hiệu suất của DITTO là tiên nghiệm tuân theo hướng dẫn quá mạnh. Gợi ý few-shot đặc biệt không thể hoàn tác phong cách được huấn luyện vào một LLM tuân theo hướng dẫn. Để cô lập hiệu ứng này, chúng tôi cũng so sánh DITTO với một biến thể cơ sở được tinh chỉnh và gợi ý few-shot của Mistral. So với DITTO, chúng tôi vẫn thấy suy giảm đáng kể—nhiều hơn so với mô hình tuân theo hướng dẫn. Tỷ lệ thắng chống lại DITTO là 9.4 và 10.4 cho few-shot và SFT tương ứng. Chúng tôi nghi ngờ rằng khả năng tuân theo hướng dẫn chung được yêu cầu như một "điểm khởi đầu." Học đồng thời tuân theo hướng dẫn và phản hồi được minh họa là một nhiệm vụ quá khó để học từ một số ít minh họa.

5.2 HIỆU QUẢ MẪU

Một khả năng chính của DITTO là hiệu quả mẫu của nó. Trong §4, chúng tôi đã kiểm tra hiệu suất của DITTO trên tập đầy đủ 7 minh họa từ mỗi tác giả. Trong thực tế, một người dùng chỉ có thể cung cấp một hoặc hai minh họa. Do đó, chúng tôi đánh giá hiệu quả mẫu qua DITTO được huấn luyện các tập con nhỏ hơn của tập huấn luyện đầy đủ N={1...7}. Giống như với các biến đổi thuật toán của chúng tôi, chúng tôi báo cáo tỷ lệ thắng chuẩn hóa theo người dùng (Hình 2). Đầu tiên, chúng tôi quan sát rằng tỷ lệ thắng DITTO tăng nhanh ở đầu. Từ 1≤N≤3, hiệu suất chuẩn hóa gần như tăng gấp đôi cho mỗi minh họa bổ sung (0%→5%→11.9%). Tuy nhiên, chúng tôi quan sát lợi nhuận giảm dần khi cung cấp minh họa thêm (4≤N≤7, 11.9%→15.39%): hiệu suất bão hòa khi minh họa tăng. Một quyết định thiết kế quan trọng trong việc sử dụng DITTO nằm trong việc lựa chọn minh họa; chúng tôi cũng nghi ngờ rằng chất lượng của các minh họa được cung cấp có thể cũng ảnh hưởng đến hiệu suất DITTO. Chúng tôi tiến hành một phân tích sơ bộ về tính gắn kết minh họa trên hiệu suất hạ lưu, kiểm tra cách tương tự minh họa ảnh hưởng đến hiệu suất DITTO (Phụ lục H.2). Chúng tôi cũng xem xét lại điều này trong công trình tương lai.

5.3 CÁC ƯU TIÊN THEO CẶP SO SÁNH VỚI CÁC MINH HỌA NHƯ THẾ NÀO?

Một giả định cốt lõi của DITTO nằm trong hiệu quả mẫu đến từ các minh họa. Về mặt lý thuyết, một người dùng có thể đạt được hiệu suất tương tự bằng cách gán nhãn nhiều ưu tiên theo cặp với một tập hợp lý tưởng các minh họa trong đầu. Như một xấp xỉ sơ bộ, một tác giả đã cung cấp các minh họa cho nghiên cứu người dùng và cũng chú thích 500 cặp ưu tiên bằng đầu ra được lấy mẫu từ Mistral 7B tuân theo hướng dẫn (minh họa trong Phụ lục G.4).

[Hình 3: Các minh họa hiệu quả hơn về mẫu so với ưu tiên theo cặp cho một người dùng cá nhân. Chúng tôi so sánh DITTO với 4 minh họa với prefs theo cặp được lấy mẫu từ (1) LM tuân theo hướng dẫn cơ sở πref và (2) πref được tinh chỉnh trên minh họa. Áp dụng DPO trên 500 ưu tiên theo cặp—với các mẫu từ πref—không mang lại cải thiện so với DITTO. Ngay cả khi minh họa được sử dụng để tinh chỉnh πref trước khi lấy mẫu, người ta phải thu thập nhiều ưu tiên theo cặp để tiếp cận DITTO.]

Tổng thể, chúng tôi xây dựng một bộ dữ liệu ưu tiên theo cặp Dpref = {(x, yi, yj)}, nơi yi ≻ yj. Sau đó chúng tôi tính toán tỷ lệ thắng giữa 20 cặp được lấy mẫu từ Mistral được huấn luyện trên (a) 4 minh họa với DITTO, và (b) trên {0...500} cặp ưu tiên chỉ với DPO. Khi chúng tôi lấy mẫu ưu tiên theo cặp từ chỉ πref, chúng tôi quan sát rằng các cặp được tạo ra nằm ngoài phân phối so với các minh họa—ưu tiên theo cặp không đạt đến hành vi được minh họa của người dùng (kết quả trong Hình 3: "Chính sách cơ sở," màu xanh). Ngay cả khi chúng tôi tinh chỉnh πref trên các minh họa của người dùng, chúng tôi vẫn cần >500 ưu tiên để phù hợp với hiệu suất DITTO (Hình 3: "Chính sách tinh chỉnh minh họa," màu cam). Điều này đặc biệt áng chừng cho các phương pháp căn chỉnh LLM bằng các mẫu được tạo ra từ chỉ πref (ví dụ Constitutional AI)—ưu tiên được tạo ra trên các mẫu OOD (so với thưởng thực của người dùng) về cơ bản không liên quan.

6 KẾT LUẬN

Các chế độ hiện tại để thu thập phản hồi—như nguyên tắc hoặc chú thích theo cặp—phục vụ cho ưu tiên cấp độ dân số. Trong công trình này, chúng tôi thay vào đó làm nổi bật hiệu quả của việc sử dụng minh họa như phản hồi, và cho thấy rằng một số lượng hạn chế các hành vi được minh họa có thể cung cấp một tín hiệu mạnh cho ưu tiên cụ thể của một cá nhân. Chúng tôi cũng giới thiệu một kỹ thuật mới, DITTO, mà tạo ra dữ liệu so sánh trực tuyến một cách rẻ tiền từ các minh họa, và kiểm tra hiệu quả của DITTO qua các điểm chuẩn tĩnh và một nghiên cứu người dùng. Tập trung thu thập phản hồi ở cấp độ minh họa có thể cung cấp một cái nhìn tổng quan đa dạng hơn về ưu tiên cá nhân, và khuyến khích một đánh giá lại các giao diện và tương tác được sử dụng để thu thập phản hồi con người.

Hạn chế Một hạn chế liên quan đến tốc độ DITTO: DITTO chậm hơn các phương pháp không huấn luyện (gợi ý) và SFT (15 phút với DITTO so với 2 phút với SFT trên 7 minh họa). Một nút thắt cổ chai nằm trong lấy mẫu, mặc dù chúng tôi nghi ngờ một hỗn hợp của trước đây (ví dụ, vLLM Kwon et al. (2023)) và công trình tương lai trong tối ưu hóa suy luận LLM có thể cải thiện tốc độ của DITTO. Các mô hình DITTO-ed cũng có xu hướng "quên" khả năng tổng quát hơn. Ví dụ, chúng tôi quan sát rằng các mô hình thường từ chối viết chương trình (tạo ra "Tôi không biết cách viết mã."). Trong Phụ lục H.1, chúng tôi đánh giá việc quên trên các nhiệm vụ mã hóa với HumanEval Chen et al. (2021), quan sát một số suy giảm. Tuy nhiên, chúng tôi hoàn toàn giảm thiểu tất cả suy giảm bằng cách có chọn lọc bỏ adapter LoRA của DITTO, và định tuyến hướng dẫn giữa mô hình tuân theo hướng dẫn chung và adapter LoRA DITTO chuyên biệt (ala MoE). Cuối cùng, vì các ràng buộc đánh giá và tính toán, chúng tôi không kiểm tra qua các họ mô hình hoặc kích thước. Khám phá cách DITTO mở rộng là một hướng cho công trình tương lai.

Công trình Tương lai Một hướng liên quan đến việc phân tích sự đánh đổi giữa các loại dữ liệu ưu tiên (ví dụ, minh họa so với ưu tiên so với nguyên tắc). Trong khi chúng tôi đề xuất minh họa như một phương thức phản hồi, mỗi loại phản hồi đòi hỏi các mức độ nỗ lực khác nhau, và hiệu quả phụ thuộc vào người dùng cung cấp phản hồi. Ngoài ra, ưu tiên được cung cấp thông qua minh họa thường là địa phương về chất lượng—người dùng cung cấp ưu tiên được minh họa trong bối cảnh của các lĩnh vực cụ thể. Hiểu cách mở rộng lượng phản hồi được minh họa địa phương ảnh hưởng đến hành vi mô hình mục đích chung là một hướng cho công trình tương lai. Một quyết định thiết kế quan trọng khác trong việc sử dụng DITTO nằm trong việc lựa chọn minh họa; chúng tôi cũng nghi ngờ rằng chất lượng của các minh họa được cung cấp có thể cũng ảnh hưởng đến hiệu suất DITTO. Trong khi chúng tôi khám phá hiệu ứng của tính gắn kết minh họa trong Phụ lục H.2 (tương tự minh họa ảnh hưởng đến hiệu suất DITTO như thế nào), hiểu cách chọn một tập hợp tối ưu các minh họa cho DITTO từ một người dùng là một hướng cho công trình tương lai. Cho khả năng căn chỉnh các mô hình với một số ít minh họa, DITTO có thể hỗ trợ các tương tác mới cho người dùng cuối cá nhân để điều phối nhiều mô hình cụ thể nhiệm vụ được tuyển chọn theo nhu cầu của họ; hoặc có thể thúc đẩy các phương pháp căn chỉnh chỉ suy luận cho LLM hộp đen mà không đòi hỏi tinh chỉnh.

--- TRANG 11 ---
Xuất bản như một bài báo hội nghị tại ICLR 2025

TUYÊN BỐ ĐẠO ĐỨC

Phản hồi được minh họa là một con dao hai lưỡi. Trong khi DITTO có thể cho phép cá nhân hóa hiệu quả của các mô hình ngôn ngữ, chúng tôi cũng nghi ngờ rằng DITTO sẽ đặc biệt hữu ích cho việc bỏ căn chỉnh mô hình, trong số một loạt các rủi ro khác (Kirk et al., 2023). Tuy nhiên, hiện trạng hiện tại của căn chỉnh mô hình ngôn ngữ nằm với các tập đoàn lớn thực hành tính minh bạch hạn chế. Các mô hình như GPT-4 đã ủng hộ những định kiến tích cực nguy hiểm hoặc mang lại lợi ích không công bằng cho các nhóm đặc quyền do các vấn đề đại diện trong quá trình thu thập phản hồi (Cheng et al., 2023; Ryan et al., 2024).

LỜI CẢM ƠN

Chúng tôi cảm ơn Eric Zelikman, Matt Jörke, Jan-Philipp Fränken, Michael Y. Li, Michael Ryan, Will Held, Shan Rizvi, Suvir Mirchandani, và Jensen Gao cho các cuộc thảo luận và phản hồi hữu ích. Chúng tôi cũng cảm ơn các thành viên của SALT Lab và các nhóm Stanford HCI / NLP.

TÀI LIỆU THAM KHẢO

[Danh sách tài liệu tham khảo tiếp tục với tất cả các trích dẫn được dịch sang tiếng Việt...]

--- CÁC TRANG TIẾP THEO ---

[Phần còn lại của tài liệu tiếp tục với các phụ lục và chi tiết bổ sung, tất cả được dịch sang tiếng Việt theo cùng phong cách và định dạng...]
