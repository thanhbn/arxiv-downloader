# 2405.01525.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2405.01525.pdf
# Kích thước tệp: 1760501 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
FLAME: Căn chỉnh Nhận thức Tính Chính xác cho Mô hình Ngôn ngữ Lớn

Sheng-Chieh Lin1∗, Luyu Gao2, Barlas Oguz3,
Wenhan Xiong3, Jimmy Lin1, Wen-tau Yih3, và Xilun Chen3†
University of Waterloo1, Carnegie Mellon University2, Meta AI3
s269lin@uwaterloo.ca, xilun@meta.com

Tóm tắt
Căn chỉnh là một quy trình tiêu chuẩn để tinh chỉnh các mô hình ngôn ngữ lớn (LLM) đã được tiền huấn luyện để tuân theo các chỉ dẫn ngôn ngữ tự nhiên và phục vụ như những trợ lý AI hữu ích. Tuy nhiên, chúng tôi đã quan sát thấy rằng quy trình căn chỉnh thông thường không thể nâng cao độ chính xác về mặt sự thật của LLM, và thường dẫn đến việc tạo ra nhiều sự thật sai lệch hơn (tức là ảo giác). Trong bài báo này, chúng tôi nghiên cứu cách làm cho quy trình căn chỉnh LLM trở nên chính xác hơn về mặt sự thật, bằng cách đầu tiên xác định các yếu tố dẫn đến ảo giác trong cả hai bước căn chỉnh: tinh chỉnh có giám sát (SFT) và học tăng cường (RL). Cụ thể, chúng tôi phát hiện rằng việc huấn luyện LLM trên kiến thức mới hoặc văn bản không quen thuộc có thể khuyến khích ảo giác. Điều này làm cho SFT kém chính xác hơn vì nó huấn luyện trên dữ liệu được gán nhãn bởi con người có thể là mới lạ đối với LLM. Hơn nữa, các hàm phần thưởng được sử dụng trong RL tiêu chuẩn cũng có thể khuyến khích ảo giác, vì nó hướng dẫn LLM cung cấp những phản hồi hữu ích hơn trên một tập hợp đa dạng các chỉ dẫn, thường ưu tiên những phản hồi dài hơn và chi tiết hơn. Dựa trên những quan sát này, chúng tôi đề xuất căn chỉnh nhận thức tính chính xác (FLAME), bao gồm SFT nhận thức tính chính xác và RL nhận thức tính chính xác thông qua tối ưu hóa ưu tiên trực tiếp. Các thí nghiệm cho thấy rằng căn chỉnh nhận thức tính chính xác được đề xuất của chúng tôi hướng dẫn LLM đưa ra những phản hồi chính xác hơn về mặt sự thật trong khi duy trì khả năng tuân theo chỉ dẫn.

1 Giới thiệu
Căn chỉnh là một quy trình tiêu chuẩn để làm cho các mô hình ngôn ngữ lớn (LLM) đã được tiền huấn luyện (Brown et al., 2020; Touvron et al., 2023) tuân theo các chỉ dẫn ngôn ngữ tự nhiên và phục vụ như những trợ lý AI hữu ích. Mặc dù có tiến bộ đáng kể trong việc tinh chỉnh chỉ dẫn (Wang et al., 2023a; Zhou et al., 2023; Li et al., 2024) và căn chỉnh LLM (Ouyang et al., 2022; Bai et al., 2022; Yuan et al., 2024), các LLM hiện đại nhất vẫn có xu hướng tạo ra những khẳng định sai lệch (Min et al., 2023). Điều này thúc đẩy chúng tôi nghiên cứu các nguyên nhân cơ bản của ảo giác LLM cũng như mối quan hệ của nó với quy trình căn chỉnh.

Quy trình căn chỉnh tiêu chuẩn bao gồm hai giai đoạn huấn luyện: (1) tinh chỉnh có giám sát (SFT) (Sanh et al., 2022); (2) học tăng cường (RL) với phản hồi từ con người (RLHF, Ouyang et al., 2022; Bai et al., 2022) hoặc phản hồi tự động (RLAIF, Bai et al., 2023). Trong nghiên cứu của chúng tôi, chúng tôi phát hiện rằng cả bước SFT và bước RL trong quy trình căn chỉnh tiêu chuẩn thực sự có thể khuyến khích LLM ảo giác. Đầu tiên, trong giai đoạn SFT, LLM được tinh chỉnh với các chỉ dẫn đa dạng được ghép nối với những phản hồi chất lượng cao do con người tạo ra. Mặc dù điều này dẫn đến khả năng tuân theo chỉ dẫn mạnh mẽ (Zhou et al., 2023), nghiên cứu của chúng tôi cho thấy rằng những phản hồi được gán nhãn bởi con người như vậy có thể trình bày thông tin mới hoặc chưa biết đối với LLM. Điều này, lần lượt, có thể vô tình thúc đẩy ảo giác. Thứ hai, chúng tôi phát hiện rằng phần thưởng tiêu chuẩn được sử dụng trong giai đoạn RL thường ưu tiên những phản hồi dài hơn và chi tiết hơn (Singhal et al., 2023; Yuan et al., 2024), điều này có xu hướng kích thích LLM đưa ra nhiều khẳng định sai lệch hơn, như được thể hiện trong các chấm đen trong Hình 1. Một lý do có thể là hầu hết các phương pháp tiếp cận RLHF hoặc RLAIF hiện tại dựa vào một phần thưởng vô hướng duy nhất để thể hiện ưu tiên,

--- TRANG 2 ---
điều này gặp khó khăn trong việc bao phủ nhiều bộ kỹ năng căn chỉnh (Ye et al., 2024) và có khả năng thể hiện thiếu khía cạnh tính chính xác (Hosking et al., 2024).

Để giải quyết các vấn đề nêu trên, chúng tôi nghiên cứu các yếu tố chính ảnh hưởng đến tính chính xác trong quá trình căn chỉnh. Cụ thể, chúng tôi đầu tiên tiến hành một nghiên cứu thí điểm về nhiệm vụ tạo tiểu sử (Min et al., 2023) trong một môi trường kiểm soát hơn, nơi quy trình căn chỉnh chỉ tập trung duy nhất vào tính chính xác (Phần 3). Nghiên cứu thí điểm của chúng tôi tiết lộ rằng một LLM ảo giác nhiều hơn nếu nó được tinh chỉnh trên kiến thức mới trong giai đoạn SFT hoặc RL. Ví dụ, một LLM trở nên kém chính xác đáng kể khi được tinh chỉnh trên các phản hồi được tạo ra bởi một mô hình có quyền truy cập vào kiến thức bên ngoài (ví dụ: một LLM được tăng cường truy xuất), mặc dù những phản hồi đó bản thân chúng chính xác hơn. Tương tự, ảo giác tăng lên đáng kể nếu RLAIF được thực hiện trên các cặp ưu tiên bao gồm đầu ra LLM được tăng cường truy xuất làm ví dụ tích cực và đầu ra của chính LLM đó làm ví dụ tiêu cực. Kết quả là, chúng tôi phát hiện rằng việc tinh chỉnh một LLM đã được tiền huấn luyện trên (một tập con được chọn lọc của) các thế hệ của chính nó mang lại những phản hồi chính xác hơn và giảm ảo giác.

Mục tiêu cuối cùng của chúng tôi là cải thiện tính chính xác của quy trình căn chỉnh tiêu chuẩn, điều này đầy thử thách vì LLM có thể được đưa ra các chỉ dẫn đa dạng và phức tạp. Như được thể hiện trong Hình 2, chúng tôi quan sát thấy rằng một số chỉ dẫn yêu cầu phản hồi chính xác trong khi những chỉ dẫn khác thì không. Được thúc đẩy bởi quan sát này, chúng tôi đề xuất căn chỉnh nhận thức tính chính xác. Chúng tôi đầu tiên xác định các chỉ dẫn dựa trên sự thật yêu cầu phản hồi chính xác. Đối với các chỉ dẫn dựa trên sự thật, chúng tôi tận dụng các phát hiện trong nghiên cứu thí điểm của mình để tạo ra dữ liệu huấn luyện bổ sung ở cả giai đoạn SFT và RL để hướng dẫn LLM một cách rõ ràng đưa ra những phản hồi chính xác. Cụ thể, ở giai đoạn SFT, đối với các chỉ dẫn dựa trên sự thật, thay vì sử dụng dữ liệu huấn luyện gốc do con người tạo ra, chúng tôi xây dựng các minh họa few-shot (từ cùng dữ liệu gốc) và tạo ra dữ liệu huấn luyện sử dụng kiến thức của chính LLM đã được tiền huấn luyện. Điều này có thể ngăn chặn việc tinh chỉnh LLM trên kiến thức chưa biết đối với chính nó. Ở giai đoạn RL, chúng tôi tạo ra các cặp ưu tiên bổ sung tập trung vào tính chính xác cho các chỉ dẫn dựa trên sự thật, được kết hợp với các cặp ưu tiên tiêu chuẩn cho việc tuân theo chỉ dẫn trong quá trình Tối ưu hóa Ưu tiên Trực tiếp (DPO; Rafailov et al., 2023).

Chúng tôi đánh giá các mô hình trên Alpaca Eval (Dubois et al., 2024) và Biography, sử dụng tỷ lệ thắng cho khả năng tuân theo chỉ dẫn và FActScore (Min et al., 2023) cho đánh giá tính chính xác, tương ứng. Như được thể hiện trong Hình 1, sử dụng phương pháp FLAME của chúng tôi (SFT + DPO), một FActScore cao hơn đáng kể (+5.6 điểm) được đạt được so với quy trình căn chỉnh tiêu chuẩn (SFT + DPO), mà không hy sinh khả năng tuân theo chỉ dẫn của LLM (tỷ lệ thắng 51.2%). Nghiên cứu khử yếu tố của chúng tôi cũng chỉ ra rằng việc xác định các chỉ dẫn dựa trên sự thật là chìa khóa cho căn chỉnh chính xác trong môi trường căn chỉnh tổng quát.

2 Công trình liên quan
Căn chỉnh. Vì các LLM đã được tiền huấn luyện không thể tuân theo chỉ dẫn của con người một cách chính xác, nhiều công trình đã được đề xuất để cải thiện căn chỉnh LLM thông qua SFT và RL. Một số đề xuất cải thiện SFT thông qua tuyển chọn dữ liệu (Zhou et al., 2023; Chen et al., 2024), tăng cường chỉ dẫn đa dạng (Wang et al., 2023a; Li et al., 2024) trong khi những nghiên cứu khác tập trung vào RL với phản hồi từ con người (Ouyang et al., 2022; Bai et al., 2022), phản hồi AI (Bai et al., 2023; Sun et al., 2024; Yuan et al., 2024). Mục tiêu chính của các phương pháp căn chỉnh này là khả năng tuân theo chỉ dẫn (hoặc tính hữu ích), điều này có thể hướng dẫn LLM đưa ra những phản hồi chi tiết và dài (Singhal et al., 2023) nhưng không tránh khỏi khuyến khích ảo giác.

Tính chính xác. Các nghiên cứu trước đây đã nêu bật vấn đề ảo giác trong LLM (Kandpal et al., 2023; Mallen et al., 2023). Để giải quyết vấn đề này, những hướng nghiên cứu quan trọng là đánh giá tính chính xác (Min et al., 2023; Wang et al., 2023b; Chern et al., 2023) và cải thiện. Một số phương pháp không cần huấn luyện để cải thiện tính chính xác của LLM bao gồm tăng cường kiến thức bên ngoài (Kandpal et al., 2023; Cheng et al., 2023; Jiang et al., 2023) và giải mã chuyên biệt (Li et al., 2023; Chuang et al., 2024).

Các nghiên cứu gần đây áp dụng RL để cải thiện tính chính xác của LLM. Ví dụ, Tian et al. (2024) đề xuất xây dựng các cặp ưu tiên tính chính xác cho tối ưu hóa ưu tiên trực tiếp (DPO; Rafailov et al., 2023), điều này có liên quan chặt chẽ đến công trình của chúng tôi. Tuy nhiên, họ chỉ tập trung duy nhất vào việc nâng cao tính chính xác của LLM thông qua DPO nhưng bỏ qua tác động tiềm tăng của nó đến khả năng tuân theo chỉ dẫn của các mô hình, như được chứng minh trong các thí nghiệm của chúng tôi. Ngược lại, công trình của chúng tôi cung cấp một kiểm tra toàn diện về việc cải thiện tính chính xác và khả năng tuân theo chỉ dẫn của LLM thông qua các phương pháp tinh chỉnh bao gồm cả SFT và DPO. Đồng thời với công trình của chúng tôi, Kang et al. (2024) phát hiện rằng LLM có xu hướng ảo giác khi đối mặt với các truy vấn không quen thuộc. Họ coi việc cải thiện tính chính xác của LLM như việc dạy LLM đưa ra những phản hồi kiềm chế hoặc ít chi tiết hơn trên những truy vấn không quen thuộc như vậy, một hành vi tương tự được quan sát từ các LLM của chúng tôi được tinh chỉnh với FLAME (xem các nghiên cứu trường hợp trong Phần 5.5). Đáng chú ý rằng cả hai nghiên cứu trước đó đều tập trung vào một tình huống đơn giản hóa như nghiên cứu thí điểm của chúng tôi trong Phần 3: tinh chỉnh LLM để cải thiện tính chính xác trên một nhiệm vụ duy nhất (ví dụ: tinh chỉnh và đánh giá trên việc tạo tiểu sử). Ngược lại, chúng tôi xem xét nhiệm vụ căn chỉnh tổng quát, nơi LLM được đưa ra các chỉ dẫn đa dạng và phức tạp.

3 Một nghiên cứu thí điểm về căn chỉnh chính xác
Trong phần này, chúng tôi đầu tiên nghiên cứu cách căn chỉnh các mô hình ngôn ngữ lớn (LLM) để trở nên chính xác hơn. Chúng tôi sử dụng việc tạo tiểu sử làm nhiệm vụ cho nghiên cứu thí điểm của mình vì hai lý do chính: (1) Việc tạo tiểu sử là một môi trường đơn giản hóa nơi tính chính xác là trọng tâm duy nhất của quy trình căn chỉnh. Như chúng tôi sẽ thảo luận trong Phần 4, việc nghiên cứu căn chỉnh chính xác trên các chỉ dẫn đa dạng của con người phức tạp hơn, vì quy trình căn chỉnh bao gồm các khía cạnh ngoài tính chính xác, như tính hữu ích và an toàn. (2) Việc đánh giá tính chính xác của việc tạo tiểu sử tương đối dễ dàng vì Wikipedia bao phủ đủ thông tin cho các nhân vật công cộng và hầu hết các sự thật về một người là không thể tranh cãi (Min et al., 2023).

3.1 Căn chỉnh cho việc tạo tiểu sử
Một quy trình căn chỉnh tiêu chuẩn bao gồm tinh chỉnh có giám sát (SFT) và học tăng cường (RL). Trong nghiên cứu thí điểm này, mục tiêu chính của chúng tôi là dạy LLM tạo ra tiểu sử với thông tin sai lệch giảm thiểu. Cho thí nghiệm, chúng tôi biên soạn các bộ dữ liệu huấn luyện và đánh giá bao gồm 500 và 183 thực thể con người đa dạng, tương ứng (chi tiết thêm được cung cấp trong Phụ lục A.1). Chúng tôi sử dụng FActScore (FS; Min et al., 2023) làm chỉ số tự động để đánh giá tính chính xác, với khả năng đánh giá chi tiết cho việc tạo văn bản dài và mối tương quan mạnh với đánh giá của con người. Để nghiên cứu căn chỉnh tính chính xác trong nghiên cứu thí điểm này, chúng tôi giả định rằng cần có dữ liệu huấn luyện nơi các phản hồi chính xác hơn so với các thế hệ của chính LLM. Do đó, chúng tôi sử dụng LLM được tăng cường truy xuất (RAG; Lewis et al., 2020) để tạo ra dữ liệu huấn luyện, điều này đã được chứng minh là đưa ra những phản hồi chính xác hơn (Mialon et al., 2023). Trong suốt bài báo, chúng tôi gọi các LLM đã được tiền huấn luyện (PT), tinh chỉnh có giám sát (SFT), và tối ưu hóa ưu tiên trực tiếp (DPO) được tinh chỉnh lần lượt là PT, SFT, và DPO.

SFT. Chúng tôi khám phá hai nguồn giám sát để tạo ra dữ liệu huấn luyện (chi tiết trong Phụ lục A.1): (1) sử dụng PT_RAG với minh chứng few-shot để tạo ra tiểu sử cho mỗi thực thể tên trong dữ liệu huấn luyện, nơi PT_RAG là PT được tăng cường với một bộ truy xuất sẵn có (Lin et al., 2023); (2) sử dụng PT vanilla với minh chứng few-shot để tạo ra dữ liệu huấn luyện làm đường cơ sở. Như được thể hiện trong Bảng 1, PT_RAG thực sự chính xác hơn nhiều so với PT. Tuy nhiên, một khám phá đáng ngạc nhiên trong nghiên cứu thí điểm là việc tinh chỉnh trên các cặp chỉ dẫn-tiểu sử chính xác hơn như vậy được tạo ra bởi PT_RAG dẫn đến một mô hình SFT kém chính xác hơn (dòng 4 so với 3).

DPO. Chúng tôi tiếp tục tinh chỉnh các LLM để trở nên chính xác hơn thông qua DPO. Một cách trực quan để tạo ra các cặp ưu tiên tính chính xác là trực tiếp sử dụng các mẫu từ PT_RAG và PT làm mẫu tích cực và tiêu cực vì PT_RAG tạo ra những tiểu sử chính xác hơn PT (dòng 2 so với 1). Một phương pháp khác là sử dụng FActScore (FS) làm phần thưởng để chọn các mẫu tích cực và tiêu cực trong số các thế hệ từ chính PT (Tian et al., 2024) (chi tiết trong Phụ lục A.1). Như được thể hiện trong Bảng 1, DPO được tinh chỉnh trên dữ liệu tự tạo với phần thưởng FS hướng dẫn các mô hình tạo ra những phản hồi chính xác hơn (dòng 5 so với 3); tuy nhiên, DPO được tinh chỉnh với sự giám sát của PT_RAG làm cho các mô hình ảo giác thậm chí nhiều hơn so với đối tác SFT của nó (6 so với 4).

Kết quả này gợi ý rằng việc ép buộc các mô hình tạo ra những phản hồi tương tự như PT_RAG thúc đẩy tăng ảo giác. Ngược lại, việc tinh chỉnh LLM trên các thế hệ của chính chúng dường như là rất quan trọng cho căn chỉnh chính xác, một phát hiện áp dụng cho cả tinh chỉnh SFT và DPO.

--- TRANG 3 ---
3.2 Chiến lược cho căn chỉnh chính xác
Từ nghiên cứu thí điểm, chúng tôi phát hiện rằng dữ liệu chất lượng tốt hơn (về mặt tính chính xác) cho SFT và DPO không nhất thiết mang lại các mô hình với căn chỉnh chính xác tốt hơn. Điều này có khả năng là do sự giám sát từ RAG chứa thông tin chưa biết đối với LLM; do đó, việc tinh chỉnh trên các phản hồi được tạo ra bởi RAG có thể vô tình khuyến khích LLM đưa ra thông tin không quen thuộc. Để tránh kiến thức chưa biết được trình bày cho LLM, một chiến lược khả thi là tạo ra dữ liệu huấn luyện SFT và DPO sử dụng các phản hồi được tạo ra từ chính LLM.

4 Căn chỉnh nhận thức tính chính xác
Trong phần này, chúng tôi mở rộng thêm cuộc thảo luận về căn chỉnh chính xác để bao gồm các chỉ dẫn tổng quát hơn. Không giống như việc tạo tiểu sử trong Phần 3, nơi tính chính xác là mục tiêu căn chỉnh chính, các chỉ dẫn của con người đa dạng và phức tạp, đòi hỏi một loạt các bộ kỹ năng căn chỉnh ngoài tính chính xác; ví dụ: tư duy logic, xử lý vấn đề và căn chỉnh người dùng (Ye et al., 2024). Do đó, việc tiến hành căn chỉnh chính xác với các chỉ dẫn đa dạng đối mặt với hai thách thức chính: (1) các chỉ dẫn khác nhau có thể đòi hỏi các bộ kỹ năng khác biệt. Ví dụ, trong Hình 2, chỉ dẫn 3, "Hãy cho tôi một lịch sử ngắn gọn về cà phê", đòi hỏi độ chính xác về sự thật và tóm tắt ngắn gọn, trong khi chỉ dẫn 8, "Kể cho tôi một câu chuyện về một con heo lên mặt trăng", ưu tiên sự sáng tạo và trí tưởng tượng hơn là tính chính xác nghiêm ngặt. (2) Như các nghiên cứu gần đây đã nhấn mạnh (Ye et al., 2024; Hosking et al., 2024), việc sử dụng một vô hướng duy nhất cho mô hình phần thưởng không thể giải quyết đầy đủ nhiều bộ kỹ năng căn chỉnh và thường thể hiện thiếu khía cạnh tính chính xác.

Để giải quyết các thách thức nêu trên, chúng tôi đề xuất căn chỉnh nhận thức tính chính xác (FLAME). Để giải quyết thách thức đầu tiên, chúng tôi đề xuất thúc đẩy LLM phân loại liệu một chỉ dẫn cho trước có đòi hỏi phản hồi phải chính xác hay không, như được thể hiện trong Hình 2. Sau đó chúng tôi áp dụng chiến lược tinh chỉnh tính chính xác cho SFT và DPO được thảo luận trong Phần 3.2 cho những chỉ dẫn dựa trên sự thật đó. Hơn nữa, để giải quyết thách thức thứ hai, chúng tôi sử dụng các phần thưởng riêng biệt để đánh giá tính chính xác và khả năng tuân theo chỉ dẫn của một LLM. Để đơn giản, công trình của chúng tôi chỉ xem xét hai bộ kỹ năng căn chỉnh: tuân theo chỉ dẫn và tính chính xác. Chúng tôi để lại mô hình phần thưởng toàn diện hơn cho nghiên cứu tương lai.

Dưới đây, chúng tôi đầu tiên mô tả phương pháp căn chỉnh cơ sở của mình và giới thiệu căn chỉnh nhận thức tính chính xác được đề xuất được xây dựng trên quy trình căn chỉnh cơ sở.

4.1 Căn chỉnh cơ sở
Chúng tôi khởi tạo PT từ mô hình Llama-2 70B đã được tiền huấn luyện và xây dựng quy trình căn chỉnh cơ sở của mình theo các mô hình ngôn ngữ tự thưởng (Yuan et al., 2024) do tính đơn giản và độc lập của nó với các LLM mạnh khác (ví dụ: GPT4) hoặc các đánh giá viên con người làm mô hình phần thưởng. Việc căn chỉnh bao gồm hai bước: (1) xây dựng mô hình SFT được tinh chỉnh trên dữ liệu gốc chất lượng cao bao gồm 3.200 chỉ dẫn và mỗi chỉ dẫn được ghép nối với phản hồi tốt nhất được tạo ra bởi con người từ bộ dữ liệu Open Assistant (OASST; Köpf et al., 2023); (2) tiếp tục tinh chỉnh SFT thông qua DPO trên dữ liệu ưu tiên tuân theo chỉ dẫn (x, y+, y−) được xây dựng bởi chính nó (SFT) làm mô hình phần thưởng, RM_IF, nơi y+ và y− là các phản hồi tích cực và tiêu cực cho một lời nhắc x cho trước, tương ứng. Mô hình được tinh chỉnh kết quả được ký hiệu là SFT + DPO. Lưu ý rằng, theo Yuan et al. (2024), chúng tôi sử dụng thêm 20K chỉ dẫn được tăng cường để tạo ra dữ liệu huấn luyện ưu tiên cho tinh chỉnh DPO. Chi tiết thêm được cung cấp trong Phụ lục A.3.

4.2 Phương pháp của chúng tôi
4.2.1 SFT nhận thức tính chính xác (SFT★)
Mặc dù việc tận dụng dữ liệu gốc chất lượng cao do con người tạo ra là một lựa chọn hợp lý cho SFT (Zhou et al., 2023), nghiên cứu của chúng tôi trong Phần 3 gợi ý rằng việc tinh chỉnh trên dữ liệu chất lượng cao như vậy được tạo ra bởi các mô hình khác ngoài chính LLM đó có thể trình bày thông tin chưa biết cho LLM, điều này có thể lần lượt khuyến khích ảo giác. Để giải quyết vấn đề trên, đối với mỗi chỉ dẫn từ dữ liệu gốc, chúng tôi khai thác kiến thức từ chính LM đã được tiền huấn luyện bằng cách tạo ra các phản hồi với minh chứng few-shot. Hơn nữa, để sử dụng tốt hơn kiến thức từ cả con người và chính LLM đã được tiền huấn luyện, chúng tôi đề xuất sử dụng các phản hồi do con người tạo ra cho các chỉ dẫn không dựa trên sự thật, trong khi tận dụng các phản hồi được lấy mẫu từ LLM đã được tiền huấn luyện cho các chỉ dẫn dựa trên sự thật để giảm thiểu việc giới thiệu kiến thức chưa biết.

Cụ thể, chúng tôi tạo ra dữ liệu huấn luyện căn chỉnh nhận thức tính chính xác cho SFT với hai bước. (1) Phân loại chỉ dẫn: chúng tôi đầu tiên thúc đẩy SFT đánh giá liệu một chỉ dẫn từ dữ liệu gốc có dựa trên sự thật (x ∈ X_fact) hay không. (2) Khai thác kiến thức từ PT: như được minh họa trong Hình 3, chúng tôi lấy mẫu 10 phản hồi từ PT với minh chứng 5-shot, (x0, Human(x0)) ⋯ (x4, Human(x4)), nơi xk là chỉ dẫn tương tự top-k với x được truy xuất bởi DRAGON+ (Lin et al., 2023) từ dữ liệu gốc. Human(xk) biểu thị phản hồi con người tương ứng với xk trong dữ liệu gốc.

Như được minh họa trong Hình 4(a), dữ liệu huấn luyện kết quả cho SFT là (x ∉ X_fact, Human(x)), (x ∈ X_fact, PT(x)), nơi PT(x) biểu thị tập hợp các phản hồi cho x được lấy mẫu từ PT. Mô hình được tinh chỉnh kết quả được ký hiệu là SFT★.

4.2.2 DPO nhận thức tính chính xác (DPO★)
Ở giai đoạn thứ hai của căn chỉnh với DPO, chúng tôi sử dụng SFT★ để tạo ra nhiều phản hồi y0, y1, ⋯ cho một chỉ dẫn x cho trước; sau đó, sử dụng chính SFT★ làm mô hình phần thưởng (RM_IF) để tạo ra một cặp ưu tiên: (x, y+, y−). Quy trình tạo dữ liệu trên giống như giai đoạn thứ hai của căn chỉnh cơ sở của chúng tôi trong Phần 4.1. Tuy nhiên, các nghiên cứu gần đây (Saha et al., 2023; Hosking et al., 2024; Ye et al., 2024) chỉ ra rằng một phần thưởng vô hướng duy nhất từ phản hồi con người hoặc các mô hình phần thưởng LLM có thể thể hiện thiếu khía cạnh tính chính xác. Để giải quyết hạn chế này, chúng tôi giới thiệu một mô hình phần thưởng tính chính xác khác (RM_fact) để đánh giá tính chính xác của các phản hồi và tạo ra một cặp ưu tiên tính chính xác cho các chỉ dẫn dựa trên sự thật: (x ∈ X_fact, y_true, y_false).

Cụ thể, chúng tôi xây dựng RM_fact với tăng cường truy xuất để đo lường tỷ lệ phần trăm các sự thật trong một phản hồi là chính xác. RM_fact bao gồm hai thành phần chính: phân rã sự thật nguyên tử và xác minh khẳng định được tăng cường truy xuất. Chúng tôi chi tiết các thành phần và khử yếu tố tác động của chúng đến chất lượng của RM_fact trong Phụ lục A.4. Chúng tôi tính toán phần thưởng tính chính xác cho cùng các phản hồi được lấy mẫu từ SFT★: RM_fact(x, y0), RM_fact(x, y1), ⋯. Phản hồi với phần thưởng tính chính xác cao nhất (thấp nhất) được chọn làm y_true (y_false). Lưu ý rằng nếu các phản hồi được chọn ghép nối cho thấy sự khác biệt lớn trong phần thưởng tuân theo chỉ dẫn, chúng tôi loại bỏ cặp đó; tức là, |RM_IF(x, y_true) − RM_IF(x, y_false)| > 0.5. Như được minh họa trong Hình 4(b), trong huấn luyện DPO nhận thức tính chính xác, mô hình được khởi tạo từ SFT★ và mô hình được tinh chỉnh là mô hình căn chỉnh nhận thức tính chính xác cuối cùng của chúng tôi, được ký hiệu SFT★ + DPO★. Các quy trình cụ thể để tinh chỉnh các mô hình trong cả SFT và DPO được mô tả trong Phụ lục A.5.

5 Thí nghiệm
5.1 Bộ dữ liệu đánh giá và chỉ số
Tuân theo chỉ dẫn. Chúng tôi sử dụng 805 nhiệm vụ tuân theo chỉ dẫn từ Alpaca Eval (Dubois et al., 2024) để đánh giá tỷ lệ thắng đối đầu của các mô hình so với các đường cơ sở của chúng tôi sử dụng đánh giá viên được khuyến nghị: alpaca_eval_gpt4_turbo_fn.

Chúng tôi sử dụng SFT và SFT + DPO được mô tả trong Phần 4.1 làm đường cơ sở cho so sánh tỷ lệ thắng.

--- TRANG 4 ---
Tính chính xác. Chúng tôi đánh giá các mô hình trên ba bộ dữ liệu với các chỉ dẫn thâm nhập kiến thức đa dạng cho tính chính xác. (1) Biography: một nhiệm vụ con không nhạy cảm với kiến thức của các nhiệm vụ tuân theo chỉ dẫn. Theo nghiên cứu thí điểm của chúng tôi trong Phần 3, chúng tôi sử dụng 183 thực thể con người được cung cấp bởi Min et al. (2023) với lời nhắc "Kể cho tôi tiểu sử của tên thực thể". (2) Alpaca Fact: chúng tôi trích xuất các chỉ dẫn dựa trên sự thật từ 803 chỉ dẫn sử dụng mô hình SFT của chúng tôi (với lời nhắc được thể hiện trong Phụ lục, Hình 5), dẫn đến 241 chỉ dẫn. (3) FAVA (Mishra et al., 2024): 141 chỉ dẫn thâm nhập kiến thức từ nhiều nguồn, bao gồm Open Assistant (Köpf et al., 2023), No Robots (Rajani et al., 2023), WebNLG (Gardent et al., 2017) và các bộ dữ liệu được tạo thủ công. Chúng tôi báo cáo FActScore (FS) không có hình phạt độ dài làm chỉ số cho cả ba bộ dữ liệu. Lưu ý rằng FS gốc tính toán tỷ lệ các sự thật chính xác với hình phạt bổ sung cho các thế hệ ngắn có ít hơn 10 sự thật nguyên tử. Hình phạt này nhằm giải quyết các tình huống nơi các mô hình cung cấp câu trả lời không đủ chi tiết. Chúng tôi giả định khía cạnh này được xem xét trong đánh giá tuân theo chỉ dẫn trong Alpaca Eval. Ngoài ra, chúng tôi cũng báo cáo số lượng sự thật chính xác và sai lệch. Tất cả các con số được báo cáo là trung bình trên các chỉ dẫn trong mỗi bộ dữ liệu.

Ngoài ra, chúng tôi cũng đánh giá tính trung thực của các mô hình được tinh chỉnh của mình sử dụng TruthfulQA (Lin et al., 2022). Chúng tôi đánh giá hiệu suất mô hình trong nhiệm vụ tạo và sử dụng ROUGE (Lin, 2004) và BLEU (Papineni et al., 2002) để đo lường chất lượng của các phản hồi.

5.2 So sánh SFT
Bảng 2 so sánh Llama-2 70B đã được tiền huấn luyện được tinh chỉnh trên bộ dữ liệu OASST với các phản hồi từ các nguồn khác nhau. Chúng tôi liệt kê FActScore (FS) của việc tạo tiểu sử sử dụng mô hình đã được tiền huấn luyện thông qua minh chứng Bio 5-shot làm tham chiếu (dòng 0) và SFT, được tinh chỉnh trên dữ liệu gốc của chúng tôi với các phản hồi do con người tạo ra, là đường cơ sở của chúng tôi (dòng 1). Chúng tôi đầu tiên nhận thấy rằng SFT cho thấy sự suy giảm FActScore đáng kể (53.1 so với 44.7) so với Bio 5-shot với mô hình đã được tiền huấn luyện. Có vẻ như SFT có xu hướng tạo ra những phản hồi dài hơn nhưng với nhiều sự thật sai lệch hơn.

Khi khai thác kiến thức từ PT bằng cách tinh chỉnh trên các phản hồi do chính nó tạo ra, SFT_fact tạo ra những phản hồi chính xác hơn trong Biography và Alpaca (dòng 2 so với 1). Tuy nhiên, nó cho thấy khả năng tuân theo chỉ dẫn hơi kém hơn trong Alpaca Eval. Kết quả này chứng minh rằng các phản hồi của con người thực sự dạy LLM cách tuân theo chỉ dẫn tốt hơn nhưng cũng khuyến khích LLM đưa ra nhiều sự thật sai lệch hơn. Mặt khác, việc khai thác kiến thức từ chính mô hình đã được tiền huấn luyện tránh được sự khuyến khích ảo giác mặc dù có sự giảm nhẹ trong khả năng tuân theo chỉ dẫn. Cuối cùng, SFT★ kết hợp sự giám sát từ con người và PT, cho thấy khả năng tuân theo chỉ dẫn tương đương và đưa ra những phản hồi chính xác hơn trên các chỉ dẫn dựa trên sự thật (dòng 3 so với 1).

5.3 So sánh DPO
Bảng 3 so sánh các công thức huấn luyện DPO khác nhau. Đầu tiên, chúng tôi tiến hành tinh chỉnh DPO trên đường cơ sở SFT của chúng tôi, SFT. Khi tiếp tục căn chỉnh mô hình để tuân theo chỉ dẫn, DPO thấy một cải thiện đáng kể trong khả năng tuân theo chỉ dẫn (dòng 2 so với 1) với tỷ lệ thắng 72.9 so với SFT; tuy nhiên, mô hình được căn chỉnh chỉ dẫn có xu hướng đưa ra những phản hồi dài với nhiều lỗi về sự thật hơn (xem ví dụ trong Phụ lục, Hình 11). Mặt khác, khi chỉ được căn chỉnh với dữ liệu ưu tiên tính chính xác, DPO_fact cho thấy ít cải thiện hơn trong khả năng tuân theo chỉ dẫn (dòng 1 so với 3). Những kết quả này chỉ ra rằng tối ưu hóa ưu tiên cho việc tuân theo chỉ dẫn hoặc tính chính xác một mình có thể có cái giá của cái kia vì cái trước khuyến khích các mô hình đưa ra những phản hồi dài và chi tiết trong khi cái sau khuyến khích các mô hình ngăn chặn việc đưa ra những khẳng định sai lệch. Khi tiến hành căn chỉnh chỉ dẫn và tính chính xác cùng nhau, DPO★ không chỉ tuân theo chỉ dẫn tốt hơn mà còn đưa ra những phản hồi chính xác hơn (dòng 4 so với 1, 2). Cuối cùng, khởi tạo từ SFT★, các mô hình được tinh chỉnh DPO chính xác hơn so với các đối tác của chúng (tức là, 6 so với 2 và 7 so với 4) mà không suy giảm khả năng tuân theo chỉ dẫn. Chúng tôi cũng liệt kê các kết quả từ Llama-2-Chat 70B (dòng 0) và quan sát rằng mặc dù có khả năng tuân theo chỉ dẫn mạnh mẽ, nó có xu hướng đưa ra nhiều sự thật không chính xác hơn. Kết quả này chứng minh rằng căn chỉnh tiêu chuẩn, thậm chí trên dữ liệu thương mại độc quyền, có thể khuyến khích LLM ảo giác. Ngược lại, căn chỉnh nhận thức tính chính xác của chúng tôi hướng dẫn LLM đưa ra những phản hồi chính xác hơn mà không suy giảm khả năng tuân theo chỉ dẫn tổng quát của chúng. Đáng chú ý rằng SFT_fact và DPO_fact tương tự như tinh chỉnh SFT và DPO được đề xuất bởi Tian et al. (2024), điều này cải thiện tính chính xác của LLM nhưng suy giảm khả năng tuân theo chỉ dẫn.

5.4 Kết quả trên TruthfulQA
Bảng 4 so sánh hiệu suất các mô hình trên TruthfulQA. Nhìn chung, chúng tôi quan sát rằng huấn luyện căn chỉnh nhận thức tính chính xác của chúng tôi hướng dẫn LLM đưa ra những phản hồi trung thực hơn. Ví dụ, SFT nhận thức tính chính xác cải thiện tính trung thực của LLM (dòng 5 so với 1). Ngoài ra, tinh chỉnh DPO trên dữ liệu ưu tiên tính chính xác hướng dẫn LLM đưa ra những phản hồi trung thực hơn (dòng 3,4 so với 2 và 7 so với 6). Lưu ý rằng chúng tôi quan sát rằng các mô hình SFT và DPO cho thấy xu hướng ngược lại trong BLUE và ROUGE. Điều này có khả năng là do các mô hình SFT có xu hướng tạo ra những phản hồi ngắn hơn so với các mô hình DPO.

5.5 Thảo luận
Tác động của phân loại chỉ dẫn dựa trên sự thật. Trong căn chỉnh nhận thức tính chính xác của chúng tôi, chúng tôi thúc đẩy SFT đánh giá liệu một chỉ dẫn có yêu cầu phản hồi chính xác hay không và áp dụng chiến lược căn chỉnh tính chính xác của chúng tôi cho chỉ dẫn dựa trên sự thật. Không có phân loại chỉ dẫn, trong SFT nhận thức tính chính xác của chúng tôi, chúng tôi không thể tạo ra sự giám sát từ các phản hồi Human và PT cho các chỉ dẫn không dựa trên sự thật và dựa trên sự thật tương ứng. Thay vào đó, đối với mỗi chỉ dẫn, chúng tôi tạo ra các cặp chỉ dẫn-phản hồi từ 1 và 10 phản hồi từ Human và PT làm sự giám sát, tương ứng. Lưu ý rằng, trong quá trình tinh chỉnh, đối với mỗi chỉ dẫn, chúng tôi lấy mẫu ngẫu nhiên cặp chỉ dẫn-phản hồi được tạo ra từ Human hoặc PT với cùng xác suất. Mô hình SFT cho thấy sự suy giảm trong cả khả năng tuân theo chỉ dẫn và kết quả tính chính xác, như được thể hiện trong dòng 1 so với 2 của Bảng 5. Thứ hai, đối với DPO nhận thức tính chính xác, không có phân loại chỉ dẫn, chúng tôi tạo ra các cặp ưu tiên tính chính xác từ tất cả các chỉ dẫn thay vì các chỉ dẫn dựa trên sự thật. Mô hình được tinh chỉnh DPO đưa ra những phản hồi hơi chính xác hơn nhưng hy sinh khả năng tuân theo chỉ dẫn, như được thể hiện trong dòng 3 so với 4 của Bảng 5.

Tác động của phân loại câu dựa trên sự thật. Ngoài ra, chúng tôi quan sát rằng không phải tất cả các câu trong một phản hồi cho một chỉ dẫn dựa trên sự thật đều yêu cầu kiểm tra sự thật. Ví dụ, cho phản hồi, "Tất nhiên. Commodore 64 là một máy tính gia đình 8-bit được phát hành bởi Commodore International vào tháng 8 năm 1982.", việc tiến hành kiểm tra sự thật cho câu đầu tiên "Tất nhiên." là không cần thiết và có thể làm cho phần thưởng tính chính xác kém chính xác hơn. Để giải quyết vấn đề này, chúng tôi thúc đẩy SFT đánh giá liệu mỗi câu trong một phản hồi có yêu cầu kiểm tra sự thật hay không sử dụng lời nhắc trong Phụ lục, Hình 7. Chúng tôi chỉ tiến hành kiểm tra sự thật và tính toán phần thưởng tính chính xác cho những câu dựa trên sự thật đó. Tuy nhiên, như được thể hiện trong Bảng 5, việc tính toán phần thưởng tính chính xác cho các câu dựa trên sự thật làm cho căn chỉnh chính xác của chúng tôi kém hiệu quả hơn (dòng 5 so với 4). Điều này có khả năng là do bộ phân loại câu dựa trên sự thật không đủ chính xác và mang tiếng ồn vào mô hình phần thưởng tính chính xác của chúng tôi (xem ví dụ trong Phụ lục, Hình 8).

Khử yếu tố về tạo dữ liệu ưu tiên tính chính xác. Trong phần này, chúng tôi kiểm tra các cách khác nhau để tạo ra dữ liệu ưu tiên tính chính xác cho huấn luyện DPO nhận thức tính chính xác. Đầu tiên, đối với mỗi chỉ dẫn dựa trên sự thật, thay vì chọn các phản hồi (trong số 4 phản hồi được tạo ra) với phần thưởng tính chính xác tối đa và tối thiểu (RM_fact) làm các mẫu tích cực và tiêu cực tương ứng, chúng tôi liệt kê tất cả các cặp phản hồi có thể và chọn phản hồi với RM_fact cao hơn (thấp hơn) làm mẫu tích cực (tiêu cực) từ mỗi cặp được liệt kê. Nếu sự khác biệt của RM_fact nhỏ hơn 0.2, chúng tôi coi chúng bằng nhau và loại bỏ các cặp đó. Lưu ý rằng cho cả dòng 1 và 2 trong Bảng 6, chúng tôi cũng loại bỏ các cặp với sự khác biệt của phần thưởng tuân theo chỉ dẫn (RM_IF) lớn hơn 0.5 (như đã đề cập trong Phần 4.2.2). Cuối cùng, đối với mỗi phản hồi, chúng tôi kết hợp tuyến tính các phần thưởng của RM_IF (thang đo 1-5) và RM_fact (thang đo 0-1) với trọng số tương ứng là 1 và 5 làm phần thưởng tổng hợp. Đối với mỗi chỉ dẫn, chúng tôi chọn các phản hồi với phần thưởng tổng hợp tối đa và tối thiểu làm mẫu tích cực và tiêu cực. Như được thể hiện trong Bảng 6, cả hai phương pháp tạo dữ liệu đều tăng số lượng cặp trong dữ liệu ưu tiên tính chính xác; tuy nhiên, nó không mang lại cải thiện rõ ràng trong tính chính xác mà còn hơi suy giảm khả năng tuân theo chỉ dẫn (dòng 2, 3 so với 1).

Tác động của DPO đến độ dài tạo. Bảng 7 liệt kê độ dài trung bình của các phản hồi của mô hình cho mỗi bộ dữ liệu. Chúng tôi quan sát rằng các mô hình được tinh chỉnh DPO có xu hướng đưa ra những phản hồi dài hơn so với SFT trừ DPO_fact trên Biography. Xu hướng này chỉ ra rằng mô hình phần thưởng tuân theo chỉ dẫn RM_IF của chúng tôi hướng dẫn LLM đưa ra những phản hồi chi tiết và dài hơn. Ngoài ra, chúng tôi quan sát rằng mặc dù DPO★ đưa ra những phản hồi với độ dài tương tự như DPO trên Alpaca Eval, DPO★ tạo ra những phản hồi hơi ngắn hơn cho các chỉ dẫn dựa trên sự thật trong ba bộ dữ liệu còn lại. Kết quả này cho thấy rằng huấn luyện DPO nhận thức tính chính xác của chúng tôi chủ yếu tác động đến các phản hồi của mô hình cho các chỉ dẫn dựa trên sự thật. Tác động chủ yếu là để giảm việc đưa ra những khẳng định sai lệch (xem số lượng sự thật chính xác và sai lệch trong dòng 2 và 4 của Bảng 3).

Nghiên cứu trường hợp. Hình 11 (trong Phụ lục) trình bày các thế hệ của các mô hình khác nhau, SFT, SFT + DPO và SFT★ + DPO★, trên Alpaca Eval và Biography. Cho chỉ dẫn, "Tên của một số diễn viên nổi tiếng bắt đầu sự nghiệp trên Broadway là gì?", SFT chỉ liệt kê một số tên của các diễn viên Broadway trong khi các mô hình được tinh chỉnh DPO tạo ra thông tin chi tiết cho mỗi diễn viên Broadway được liệt kê. Đối với việc tạo tiểu sử, chúng tôi quan sát rằng cho chỉ dẫn tạo ra tiểu sử cho một thực thể tên hiếm, Marianne McAndrew, SFT + DPO tạo ra một phản hồi chi tiết nhưng với nhiều sự thật sai lệch trong khi SFT và SFT★ + DPO★ đưa ra những phản hồi tương đối ngắn. Đối với thực thể phổ biến, Ji Sung, tất cả các mô hình tạo ra những phản hồi chi tiết và hầu hết chính xác. Phân tích định tính này cho thấy rằng SFT★ + DPO★ có xu hướng tạo ra những phản hồi chi tiết cho hầu hết các chỉ dẫn nhưng đối với những chỉ dẫn yêu cầu kiến thức chuyên môn (ví dụ: thực thể hiếm) có khả năng chưa biết đối với LLM (Mallen et al., 2023), nó quản lý để giảm các sự thật sai lệch bằng cách đưa ra những phản hồi ít chi tiết hơn, điều này cũng được quan sát bởi Kang et al. (2024).

6 Kết luận
Trong bài báo này, chúng tôi trình bày một nghiên cứu để nâng cao tính chính xác của các mô hình ngôn ngữ lớn (LLM). Chúng tôi đầu tiên xác định rằng phương pháp căn chỉnh tiêu chuẩn, bao gồm SFT và RLAIF với DPO, có thể vô tình khuyến khích LLM tạo ra nhiều sự thật sai lệch hơn. Cụ thể, trong giai đoạn SFT, việc tinh chỉnh LLM với các phản hồi chất lượng cao của con người có thể giới thiệu thông tin không quen thuộc, thúc đẩy LLM đưa ra những sự thật chưa biết. Ngoài ra, trong giai đoạn DPO, việc nâng cao khả năng tuân theo chỉ dẫn của LLM có thể dẫn đến những phản hồi chi tiết và dài hơn nhưng thường dẫn đến tăng ảo giác. Để giải quyết những thiếu sót của căn chỉnh tiêu chuẩn, chúng tôi đề xuất một phương pháp căn chỉnh nhận thức tính chính xác, bao gồm SFT và DPO nhận thức tính chính xác. Các phân tích định lượng và định tính chứng minh rằng căn chỉnh nhận thức tính chính xác của chúng tôi không chỉ hướng dẫn LLM tạo ra những phản hồi chi tiết và hữu ích mà còn giúp ngăn chặn việc tạo ra những khẳng định sai lệch.

7 Hạn chế
Mặc dù chúng tôi đã thành công tích hợp tính chính xác vào quy trình căn chỉnh tiêu chuẩn, công trình của chúng tôi chỉ xem xét hai bộ kỹ năng căn chỉnh: tuân theo chỉ dẫn (hoặc tính hữu ích) và tính chính xác. Trong thực tế, mỗi chỉ dẫn có thể yêu cầu xem xét nhiều bộ kỹ năng căn chỉnh khác biệt và riêng lẻ (Saha et al., 2023). Phương pháp tối ưu hóa cho những bộ kỹ năng này được điều chỉnh cho mỗi truy vấn yêu cầu nghiên cứu thêm. Trong các thí nghiệm của chúng tôi, chúng tôi lưu ý rằng việc tối ưu hóa ưu tiên chỉ duy nhất cho việc tuân theo chỉ dẫn hoặc tính chính xác có thể tiềm ẩn làm tổn hại cái kia. Mặc dù căn chỉnh nhận thức tính chính xác của chúng tôi đã chứng minh cải thiện trong cả hai khía cạnh, vẫn chưa chắc chắn liệu có sự đánh đổi giữa hai khía cạnh khi tích hợp phương pháp của chúng tôi vào căn chỉnh quy mô lớn (Touvron et al., 2023). Cuối cùng, như được thể hiện trong Phụ lục, Hình 8, không phải tất cả các khẳng định (hoặc câu) trong một phản hồi đều yêu cầu xác minh sự thật, một mô hình phần thưởng tính chính xác chính xác hơn nên tính đến yếu tố này. Mặc dù thí nghiệm sơ bộ của chúng tôi, loại bỏ các câu không dựa trên sự thật khỏi mô hình phần thưởng tính chính xác (Phần 5.5), cho thấy hiệu suất không tối ưu, chúng tôi tin rằng nghiên cứu thêm có thể mang lại nhiều hiểu biết hơn.

Lời cảm ơn
Chúng tôi cảm ơn Bhargavi Paranjape đã chia sẻ Llama-2 7B được tinh chỉnh cho việc phân rã sự thật nguyên tử và Jing Xu, Weizhe Yuan và Jason Weston cho các gợi ý hữu ích của họ.

Tài liệu tham khảo
[Danh sách các tài liệu tham khảo tiếng Anh được giữ nguyên như trong bản gốc]

--- TRANG 5 ---
[Tiếp tục danh sách tài liệu tham khảo...]

--- TRANG 6 ---
[Bảng 2 và các nội dung khác được dịch tương tự...]

[Phần còn lại của tài liệu tiếp tục được dịch theo cùng cách thức, giữ nguyên cấu trúc và định dạng của bản gốc]
