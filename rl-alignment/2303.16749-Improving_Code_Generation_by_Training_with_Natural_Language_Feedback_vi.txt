# Cải thiện tạo mã bằng cách huấn luyện với phản hồi ngôn ngữ tự nhiên

Angelica Chen1 Jérémy Scheurer1 2 Tomasz Korbak1 2 3 Jon Ander Campos1 4 Jun Shern Chan1 2
Samuel R. Bowman1 Kyunghyun Cho1 5 6 Ethan Perez1 2 7

## Tóm tắt

Tiềm năng của các mô hình ngôn ngữ lớn đã được huấn luyện trước (LLM) để sử dụng phản hồi ngôn ngữ tự nhiên tại thời điểm suy luận đã là một phát triển thú vị gần đây. Chúng tôi xây dựng dựa trên quan sát này bằng cách chính thức hóa một thuật toán để học từ phản hồi ngôn ngữ tự nhiên tại thời điểm huấn luyện, mà chúng tôi gọi là Học bắt chước từ Phản hồi Ngôn ngữ (ILF). ILF chỉ yêu cầu một lượng nhỏ phản hồi do con người viết trong quá trình huấn luyện và không yêu cầu cùng phản hồi đó tại thời điểm kiểm tra, làm cho nó vừa thân thiện với người dùng vừa hiệu quả về mẫu. Chúng tôi tiếp tục chỉ ra rằng ILF có thể được xem như một dạng tối thiểu hóa phân kỳ KL đến phân phối chân lý cơ bản và chứng minh một bằng chứng khái niệm trên nhiệm vụ tổng hợp chương trình thần kinh. Chúng tôi sử dụng ILF để cải thiện tỷ lệ pass@1 của mô hình CODEGEN-MONO 6.1B lên 38% tương đối (và 10% tuyệt đối) trên benchmark Mostly Basic Python Problems (MBPP), vượt trội hơn cả tinh chỉnh trên MBPP và tinh chỉnh trên các chương trình được sửa chữa bởi con người. Nhìn chung, kết quả của chúng tôi cho thấy việc học từ phản hồi ngôn ngữ tự nhiên do con người viết vừa hiệu quả hơn vừa tiết kiệm mẫu hơn so với huấn luyện độc quyền trên các minh họa để cải thiện hiệu suất của LLM trên các nhiệm vụ tạo mã.

## 1. Giới thiệu

Một nhiệm vụ quan trọng cho lĩnh vực kỹ thuật phần mềm là tổng hợp chương trình, việc tự động tạo ra các chương trình máy tính từ một đặc tả đầu vào (ví dụ: mô tả nhiệm vụ bằng ngôn ngữ tự nhiên hoặc một tập hợp các ví dụ đầu vào-đầu ra) (Manna & Waldinger, 1971). Tổng hợp chương trình hiệu quả không chỉ có thể cải thiện hiệu quả của các nhà phát triển phần mềm (Ziegler et al., 2022), mà còn tăng khả năng tiếp cận việc viết mã nói chung. Gần đây, các mô hình ngôn ngữ lớn đã được huấn luyện trước đã chứng minh thành công ấn tượng trong tổng hợp chương trình (Chen et al., 2021; Li et al., 2022; Austin et al., 2021; Nijkamp et al., 2022; Xu et al., 2022, trong số các nghiên cứu khác) nhưng vẫn gặp khó khăn trong việc tạo ra mã chính xác một cách nhất quán, ngay cả với việc huấn luyện trước quy mô lớn (Chen et al., 2021).

Chúng tôi đặt giả thuyết rằng những thất bại này có thể được quy cho các thiết lập huấn luyện trước LLM hiện đại. Chẳng hạn, các bộ dữ liệu huấn luyện trước mã chủ yếu bao gồm mã chưa được lọc được thu thập từ Internet, chứa một số lượng đáng kể các lỗ hổng bảo mật (Kang et al., 2022) và lỗi (Chen et al., 2021). Tín hiệu huấn luyện này cũng chỉ bao gồm các minh họa ngoại tuyến, không có bất kỳ tín hiệu nào từ thử-và-sai hoặc hướng dẫn tương tác để phạt các đầu ra có lỗi của mô hình. Do đó, chúng tôi đặt giả thuyết rằng việc giám sát LLM bằng phản hồi rõ ràng do con người viết về các đầu ra của chính mô hình có thể hiệu quả hơn trong việc huấn luyện mô hình tạo ra mã chức năng chính xác.

Đặc biệt, một dạng phản hồi trực quan và phong phú để cung cấp cho LLM là phản hồi ngôn ngữ tự nhiên. Chúng tôi lập luận rằng LLM có khả năng tự nhiên kết hợp phản hồi bằng văn bản, điều này đã được chứng minh là cải thiện đáng kể tỷ lệ pass của mô hình tạo mã khi phản hồi được cung cấp tại thời điểm kiểm tra (Nijkamp et al., 2022; Austin et al., 2021). Trong công trình của chúng tôi, chúng tôi xây dựng dựa trên quan sát này bằng cách khám phá việc sử dụng phản hồi ngôn ngữ tự nhiên trong quá trình huấn luyện chính nó, thay vì chỉ trong quá trình suy luận. Chúng tôi phỏng đoán rằng phản hồi như vậy cung cấp thông tin biểu cảm và có mục tiêu về những thất bại hiện tại của mô hình tạo mã một cách tiết kiệm mẫu. Rộng hơn, cách tiếp cận này cũng đại diện cho một phiên bản yếu của giám sát có thể mở rộng (Bowman et al., 2022), trong đó những người giám sát mô hình có thể cải thiện mô hình chỉ bằng cách đánh giá các đầu ra của nó, mà không cần tạo ra các minh họa mới theo cách thủ công, theo cách tận dụng các khả năng đang được giám sát.

Để huấn luyện LLM với phản hồi ngôn ngữ, chúng tôi đề xuất một thuật toán gọi là Học bắt chước từ Phản hồi Ngôn ngữ (ILF; Thuật toán 1), mở rộng công trình của Scheurer et al. (2022), những người nghiên cứu tác động của việc học từ phản hồi ngôn ngữ trên các mô hình tóm tắt văn bản. Scheurer et al. (2022) cải thiện mô hình tóm tắt bằng cách huấn luyện mô hình cơ sở trên các bản tóm tắt được cải thiện tạo ra từ các bản tóm tắt gốc của mô hình và phản hồi do con người viết. Công trình của chúng tôi xây dựng dựa trên Scheurer et al. (2022) theo nhiều cách: (1) bằng cách chính thức hóa thuật toán và tổng quát hóa nó thành một dạng có thể áp dụng cho bất kỳ nhiệm vụ nào (thuật toán ILF của chúng tôi trong Phần 2.2), (2) bằng cách chi tiết hóa cách hàm phần thưởng có thể được điều chỉnh cho tạo mã, và (3) bằng cách chứng minh một bằng chứng khái niệm của ILF cho tạo mã. ILF cải thiện tính chính xác của các chương trình được tạo ra bởi mô hình tạo mã cơ sở πθ bằng cách huấn luyện một mô hình riêng biệt πRefine để sử dụng phản hồi ngôn ngữ để sửa chữa các chương trình không chính xác được tạo bởi πθ. (Chúng tôi gọi các chương trình được sửa chữa là các tinh chỉnh.) Sau đó chúng tôi cải thiện πθ bằng cách tinh chỉnh nó trên các tinh chỉnh được tạo bởi πRefine mà vượt qua các bài kiểm tra đơn vị, tạo ra một mô hình cải thiện cuối cùng πθ∗. Quy trình này có thể được chạy lặp đi lặp lại để tiếp tục cải thiện mô hình, mà chúng tôi chỉ ra có thể được xem như tối thiểu hóa phân kỳ KL mong đợi từ một phân phối chân lý cơ bản mục tiêu (Phần 2).

Chúng tôi chứng minh một bằng chứng khái niệm của ILF cho tạo mã bằng cách chỉ ra rằng nó cải thiện tỷ lệ pass@1 của mô hình CODEGEN-MONO 6.1B trên benchmark Mostly Basic Python Problems (MBPP) (Odena et al., 2021) lên 38% tương đối (10% tuyệt đối) so với hiệu suất zero-shot của nó. Nó cũng vượt trội hơn tinh chỉnh trên mã được cung cấp bởi MBPP 64% (14% tuyệt đối, xem Phần 3.2). Chúng tôi tiếp tục thấy rằng các tinh chỉnh được tạo ra trong quá trình ILF thực sự tận dụng phản hồi do con người viết (Phần 3.1) – khi phản hồi không hữu ích hoặc không liên quan, chúng tôi quan sát sự giảm mạnh trong tính chính xác của mã. Chất lượng của phản hồi cũng quan trọng – phản hồi được tạo bởi LLM cho tỷ lệ pass cuối cùng thấp hơn nhiều so với phản hồi do con người viết (Phần 3.3). Mặc dù thành công của cách tiếp cận của chúng tôi, chúng tôi vẫn quan sát những hạn chế cụ thể – chẳng hạn, πRefine kém hiệu quả hơn trong việc kết hợp phản hồi khi phản hồi giải quyết nhiều lỗi (Phần 3.5), điều này gợi ý không gian cải tiến cho nghiên cứu tương lai hoặc các LLM có khả năng hơn để làm cơ sở cho πRefine. Nhìn chung, kết quả của chúng tôi – cũng như kết quả bổ sung của chúng tôi về tóm tắt văn bản, sử dụng một kỹ thuật tương tự trong Scheurer et al. (2023) – cho thấy phản hồi do con người viết là một dạng giám sát mạnh mẽ, giàu thông tin cho LLM.

## 2. Phương pháp

### 2.1. Kiến thức cơ bản

Ở đây, chúng tôi mô tả chính thức vấn đề mà chúng tôi muốn giải quyết, trước khi giới thiệu thuật toán của chúng tôi. Giả sử chúng tôi bắt đầu với từ vựng V và một mô hình ngôn ngữ đã được huấn luyện trước πθ được tham số hóa bởi θ. πθ: V* → [0,1] là một phân phối xác suất trên các chuỗi token x ∈ V*, trong đó V* là bao đóng Kleene của V. Chúng tôi cũng có một tập dữ liệu các nhiệm vụ D = {(t, u)}. Một nhiệm vụ (t, u) bao gồm một mô tả nhiệm vụ t ∈ T (ví dụ: "Viết một hàm tính phân tích thừa số nguyên tố của một số nguyên đầu vào.") và một bộ u = UNITTESTS(t) ∈ U các bài kiểm tra đơn vị liên quan đến nhiệm vụ t. Cuối cùng, hãy EVAL: V* × T → {0,1} là một hàm xác minh bài kiểm tra đơn vị cho biết liệu một chương trình x ∼ πθ(·|t) có vượt qua tất cả các bài kiểm tra đơn vị trong UNITTESTS(t) không:

EVAL(x, t) := 1, nếu x vượt qua bộ kiểm tra UNITTESTS(t),
0, ngược lại                                                    (1)

Chúng tôi cũng định nghĩa một hàm tinh chỉnh FINETUNE(πθ, D) áp dụng một thuật toán tối ưu hóa dựa trên gradient cho πθ sử dụng mục tiêu mất mát liên quan được tính toán trên tập dữ liệu D.

### 2.2. Học bắt chước từ Phản hồi Ngôn ngữ

Mục tiêu của chúng tôi là lấy mẫu một tập hợp đa dạng các chương trình chất lượng cao x1 ∼ πθ(·|t) cho bất kỳ nhiệm vụ t nào được lấy mẫu từ phân phối nhiệm vụ p(t). Chúng tôi làm điều này bằng cách khớp một LLM tự hồi quy πθ để xấp xỉ một phân phối chân lý cơ bản π*t(x1) gán một xác suất cho x1 tỷ lệ thuận với chất lượng của nó, được đo bằng hàm phần thưởng R. Khớp πθ để xấp xỉ π*t có thể được xem như tối thiểu hóa phân kỳ KL mong đợi từ π*t đến πθ trên phân phối nhiệm vụ p(t):

min θ E t∼p(t) [KL(π*t, πθ(·|t))]                              (2)

trong đó

π*t(x1) ∝ exp(βR(x1, t))                                       (3)

Trong công trình này, chúng tôi sử dụng hàm xác minh bài kiểm tra đơn vị EVAL trực tiếp như hàm phần thưởng R của chúng tôi, nhưng R cũng có thể là một hàm của bất kỳ số lượng tín hiệu nào khác, như stack trace hoặc đầu ra của trình biên dịch.

Tối thiểu hóa mục tiêu trong Phương trình 2 tương đương với học có giám sát, tức là tối thiểu hóa mất mát cross-entropy:

L(θ) = -E t∼p(t) [Lθ(t)],                                      (4)

trong đó

Lθ(t) = Σ x1 π*t(x1) log πθ(x1|t).                            (5)

Thay vì tính toán mất mát này trên không gian lớn theo cấp số mũ của tất cả các x1 có thể, chúng tôi thay vào đó sử dụng lấy mẫu Monte-Carlo trên một tập nhỏ các x1 được rút ra từ π*t. Tuy nhiên, điều này vẫn không thể thực hiện được vì chúng ta không thể lấy mẫu trực tiếp từ π*t. Thay vào đó, chúng tôi xấp xỉ π*t bằng cách sử dụng lấy mẫu quan trọng với phân phối đề xuất qt(x1):

Lθ(t) = Σ x1 qt(x1) π*t(x1)/qt(x1) log πθ(x1|t)              (6)

gán trọng số cao hơn cho các chương trình chất lượng cao hơn x1.

### 2.3. Phân phối đề xuất q

Một cách trực quan, chúng tôi muốn thiết kế qt gần với π*t nhất có thể, điều mà chúng tôi thực hiện bằng cách kết hợp các phần phản hồi ngôn ngữ tự nhiên ft cung cấp thông tin về cách chuyển đổi một chương trình phần thưởng thấp x0 thành một chương trình phần thưởng cao hơn x1. Điều này có thể đạt được bằng cách (i) xác định một chương trình x0 ∼ πθ(·|t) hiện tại không vượt qua bộ kiểm tra (tức là EVAL(x0, t) = 0), (ii) yêu cầu phản hồi ngôn ngữ tự nhiên f về các lỗi trong x0, (iii) sử dụng f để chuyển đổi chương trình gốc x0 thành một tinh chỉnh x1 kết hợp phản hồi và vượt qua bộ kiểm tra (tức là EVAL(x1, t) = 1), và (iv) gán trọng số cao hơn cho x1.

Chúng tôi có thể chính thức hóa quy trình này như sau. Hãy πψ(x1|t, x0, f) là một phân phối trên các chương trình x1 cải thiện x0 bằng cách kết hợp phản hồi f và pF(f|t, x0, EVAL(x0, t) = 0) là phân phối các phần phản hồi f cho chương trình không chính xác x0 và nhiệm vụ t. Sau đó chúng ta có thể định nghĩa phân phối đề xuất của chúng ta như:

qt(x1) = Σ x0,f πθ(x0|t) × δ0(EVAL(x0, t)|x0, t)
         × pF(f|t, x0, EVAL(x0, t) = 0)
         × πψ(x1|t, x0, f)
         × δ1(EVAL(x1, t)|t, x1),                              (7)

trong đó δ0 và δ1 là các phân phối delta Dirac tập trung tại 0 và 1, tương ứng. Khi đó phân phối đề xuất này được đảm bảo đặt khối lượng xác suất cao hơn trên các chương trình chất lượng cao hơn (về mặt tỷ lệ vượt qua bài kiểm tra đơn vị) so với πθ vì thuật ngữ δ1(EVAL(x1, t)|t, x1) bằng 0 cho các chương trình không chính xác x1.

Chúng tôi xấp xỉ việc lấy mẫu từ q bằng cách xem xét từng thuật ngữ trong Phương trình 7 theo thứ tự:

1. Trước tiên chúng tôi lấy mẫu từ πθ(x0|t) × δ0(EVAL(x0, t)|x0, t) bằng lấy mẫu từ chối từ πθ. Nói cách khác, chúng tôi lấy mẫu các chương trình x0 từ πθ cho nhiệm vụ t và chỉ giữ những chương trình thất bại trong bộ kiểm tra (tức là EVAL(x0, t) = 0; bước 2 của Thuật toán 1).

2. Chúng tôi xấp xỉ việc lấy mẫu từ pF(f|t, x0, EVAL(x0, t) = 0) bằng cách để con người chú thích các chương trình x0 (được ghép nối với các mô tả nhiệm vụ t và bộ kiểm tra u tương ứng) với phản hồi ngôn ngữ tự nhiên (bước 3 của Thuật toán 1).

3. Chúng tôi xấp xỉ việc lấy mẫu từ πψ(x1|t, x0, f) bằng cách lấy mẫu từ πRefine, một mô hình có khả năng tạo ra các tinh chỉnh cho mô tả nhiệm vụ, chương trình gốc và phản hồi do con người viết.

4. Cuối cùng, thuật ngữ δ1(EVAL(x1, t)|t, x1) tương ứng với một bộ lọc khác: chúng tôi chỉ giữ các chương trình được tinh chỉnh x1 vượt qua bộ kiểm tra.

Tiếp theo, chúng tôi xem xét các chi tiết cụ thể hơn về cách thực hiện việc lấy mẫu này.

**Huấn luyện πRefine** ILF giả định tính khả dụng của phản hồi nhưng không nhất thiết phải có mã được sửa chữa/tinh chỉnh, vì nhiều lý do khác nhau. Chúng tôi giả định rằng tổng hợp chương trình có thể là một nhiệm vụ mà việc viết phản hồi ngôn ngữ tự nhiên cấp cao thường ít công sức hơn so với việc thực hiện sửa chữa chương trình. Mặc dù việc viết phản hồi bao gồm việc xác định ở mức cao những gì sai với chương trình và cách nó nên được sửa, việc sửa chữa chương trình có thể bao gồm các bước bổ sung như tái cấu trúc, xem qua tài liệu và kiểm tra. Hơn nữa, công trình trước đây (Austin et al., 2021; Nijkamp et al., 2022) đã chỉ ra rằng một số LLM lớn có thể kết hợp phản hồi một cách thành thạo tại thời điểm suy luận, giả định quyền truy cập vào phản hồi chính xác và chất lượng cao. Do đó, ILF giả định quyền truy cập vào một số mô hình πRefine có khả năng tạo ra một tinh chỉnh cho chương trình gốc và phản hồi.

πRefine có thể có nhiều dạng khác nhau, nhưng chúng tôi tinh chỉnh một mô hình CODEGEN-MONO 6.1B đã được huấn luyện trước làm πRefine của chúng tôi. Chúng tôi tạo một tập dữ liệu huấn luyện cho πRefine bằng cách tiếp tục chú thích một tập con của Cannotated với các tinh chỉnh x1 sửa chữa các chương trình không chính xác x0 bằng cách kết hợp phản hồi f, sao cho EVAL(x1, t) = 1 cho (x0, f, t) ∈ Cannotated. Chi tiết thêm về tập dữ liệu và quy trình chú thích của chúng tôi có trong Phần 3.

## 3. Thí nghiệm và Kết quả

Sau khi mô tả cách tiếp cận cấp cao của chúng tôi, bây giờ chúng tôi giải thích thiết lập thí nghiệm mà chúng tôi sử dụng để kiểm tra ILF.

**Tập dữ liệu** Chúng tôi huấn luyện và đánh giá các mô hình của chúng tôi trên tập dữ liệu Mostly Basic Python Problems (MBPP) (Odena et al., 2021). MBPP chứa 974 nhiệm vụ lập trình Python được thiết kế để có thể giải quyết bởi các lập trình viên cấp độ đầu. Mỗi nhiệm vụ chứa một mô tả nhiệm vụ ngôn ngữ tự nhiên t (ví dụ: "Viết một hàm để trả về phân tích thừa số nguyên tố của đầu vào."), một giải pháp vàng và một bộ u gồm ba bài kiểm tra đơn vị. Vì các mô tả nhiệm vụ đôi khi mơ hồ, chúng tôi bao gồm một bài kiểm tra đơn vị trong mô tả nhiệm vụ. Việc thêm bài kiểm tra đơn vị giúp chỉ định định dạng đầu vào và đầu ra của mỗi nhiệm vụ. Chúng tôi giữ lại các bài kiểm tra đơn vị còn lại để đánh giá các chương trình được tạo ra của chúng tôi.

MBPP bao gồm một phân chia prompt/huấn luyện/validation/kiểm tra được chỉ định của tập dữ liệu, nhưng chúng tôi phân chia lại tập dữ liệu thành các phân chia sau:

• **MBPP Refine**: Đây là các nhiệm vụ có ID trong khoảng 111-310 mà CODEGEN-MONO 6.1B không tạo ra bất kỳ hoàn thành chính xác nào. Phân chia này được sử dụng để huấn luyện πRefine.

• **MBPP Train**: Đây là các nhiệm vụ có ID trong khoảng 311-974 mà CODEGEN-MONO 6.1B không tạo ra bất kỳ hoàn thành chính xác nào. Phân chia này trước tiên được sử dụng để đánh giá tính chính xác của các tinh chỉnh được tạo bởi πRefine. Sau đó, các tinh chỉnh chính xác trong phân chia này được sử dụng để huấn luyện πθ để có được πθ* (bước 5 trong Thuật toán 1).

• **MBPP Test**: Đây là các nhiệm vụ có ID trong khoảng 11-110 mà chúng tôi sử dụng để đánh giá hiệu suất cuối cùng của πθ*. Không giống như hai phân chia trước, chúng tôi sử dụng tất cả các nhiệm vụ trong phân chia này, thay vì chỉ các nhiệm vụ mà CODEGEN-MONO 6.1B ban đầu không tạo ra chương trình chính xác. Điều này cho phép chúng tôi so sánh tốt hơn hiệu suất cơ sở của πθ với πθ*.

Chúng tôi sử dụng phân chia đã sửa đổi này để một phần lớn hơn của tập dữ liệu có thể được sử dụng để huấn luyện mô hình cuối cùng πθ*, trong khi các phần nhỏ hơn được phân bổ để huấn luyện πRefine và đánh giá πθ*. Chúng tôi không sử dụng phân chia prompt (ID 1-10).

**Mô hình** Trong toàn bộ bài báo này, chúng tôi sử dụng mô hình CODEGEN-MONO 6.1B đã được huấn luyện trước (Nijkamp et al., 2022) làm πθ của chúng tôi. Nó được huấn luyện trước tuần tự trên THEPILE (Gao et al., 2020), BIGQUERY (Nijkamp et al., 2022) và BIGPYTHON (Nijkamp et al., 2022). Chúng tôi chọn mô hình này vì nó là mã nguồn mở, có thể được tinh chỉnh trên một nút 4×100A100 (80 GB) duy nhất và cho thấy điểm pass@k có thể so sánh với CODEX-12B (Chen et al., 2021; Nijkamp et al., 2022).

Để triển khai thuật toán của chúng tôi, chúng tôi tinh chỉnh độc lập hai phiên bản riêng biệt của CODEGEN-MONO 6.1B để tạo ra πRefine và mô hình cuối cùng πθ*. Chúng tôi huấn luyện πRefine sử dụng các cặp chương trình không chính xác và phản hồi do con người viết làm đầu vào, với các tinh chỉnh do con người viết làm mục tiêu (sử dụng định dạng trong Hình 2). Ngược lại, chúng tôi huấn luyện πθ* sử dụng các mô tả nhiệm vụ ngôn ngữ tự nhiên từ MBPP làm đầu vào và các tinh chỉnh được tạo bởi πRefine làm mục tiêu. Chi tiết huấn luyện thêm có trong Phụ lục A.1.

**Đánh giá** Chúng tôi đánh giá tất cả các tạo mã trong bài báo này bằng cách sử dụng chỉ số pass@k được giới thiệu trong Kulal et al. (2019). Nó ước tính tỷ lệ mà ≥1 trong k mẫu mô hình vượt qua tất cả các bài kiểm tra đơn vị. Chúng tôi sử dụng ước tính thực nghiệm của đại lượng này từ Chen et al. (2021), một ước tính không thiên vị được cho bởi:

pass@k := E task [1 - (n-c choose k)/(n choose k)]           (8)

cho n chương trình tổng cộng (trong đó n ≥ k) và c chương trình chính xác cho nhiệm vụ đã cho.

**Chú thích con người** Chúng tôi thuê các chú thích viên thông qua Surge AI để viết cả phản hồi ngôn ngữ tự nhiên và tinh chỉnh cho các chương trình không chính xác được tạo bởi CODEGEN-MONO 6.1B. Đối với mỗi nhiệm vụ mà CODEGEN-MONO 6.1B không tạo ra chương trình chính xác nào, chúng tôi yêu cầu các công nhân trước tiên chọn một trong những chương trình không chính xác để viết phản hồi và tinh chỉnh. Chúng tôi chỉ định rằng các công nhân nên chọn một mẫu có vẻ tương đối dễ sửa (tức là có thể được sửa chữa tối thiểu để vượt qua các bài kiểm tra đơn vị). Sau đó, họ được yêu cầu viết phản hồi mô tả những gì sai với mã hiện tại và cách sửa nó. Đối với tinh chỉnh, họ được yêu cầu sao chép mã gốc và thực hiện số lượng chỉnh sửa tối thiểu cần thiết để kết hợp phản hồi và vượt qua tất cả các bài kiểm tra đơn vị. Bộ hướng dẫn đầy đủ cho công nhân có thể được tìm thấy trong Phụ lục A.2.

Chúng tôi giữ tất cả các chú thích mà tinh chỉnh vượt qua tất cả các bài kiểm tra trong bộ kiểm tra của nhiệm vụ, phản hồi chính xác (được xác minh thủ công bởi các tác giả), và khoảng cách chỉnh sửa Levenshtein giữa tinh chỉnh và chương trình gốc nhỏ hơn 50% của max(len(tinh chỉnh), len(chương trình gốc)). Tập dữ liệu cuối cùng bao gồm 195 bộ ba (chương trình không chính xác, phản hồi do con người viết, tinh chỉnh do con người viết). Trung bình, các công nhân được trả $23 mỗi mẫu được chú thích và mất 27 phút/mẫu, với phần trăm thứ 10 là 4 phút và phần trăm thứ 90 là 43 phút.

Mặc dù thuật toán ILF chỉ yêu cầu thu thập phản hồi do con người viết cho các nhiệm vụ trong MBPP Train (giả định quyền truy cập vào một số πRefine đã được tinh chỉnh hoặc có thể tạo ra tinh chỉnh thông qua prompting few-shot), chúng tôi thu thập cả phản hồi do con người viết và tinh chỉnh cho tất cả các phân chia của dữ liệu để có thể tiến hành phân tích thêm về phương pháp của chúng tôi. Chẳng hạn, điều này cho phép chúng tôi so sánh tinh chỉnh trên các tinh chỉnh được tạo bởi πRefine với tinh chỉnh trên các tinh chỉnh do con người viết. Khi mở rộng đến các cặp mô hình và nhiệm vụ khác, ILF yêu cầu các chú thích phản hồi mới, nhưng có thể việc sử dụng ILF trên một tập dữ liệu sẽ cải thiện khả năng của mô hình trên tập dữ liệu khác cho một nhiệm vụ tương tự. Chúng tôi để lại các phân tích về việc mở rộng ILF qua các nhiệm vụ và mô hình khác nhau cho nghiên cứu tương lai.

### 3.1. CODEGEN-MONO 6.1B Kết hợp Phản hồi

Trước tiên chúng tôi xác minh rằng mô hình cơ sở của chúng tôi có thể sử dụng phản hồi để sửa chữa mã không chính xác, một điều kiện tiên quyết để ILF hoạt động. Chúng tôi đánh giá khả năng của CODEGEN-MONO 6.1B trong việc tạo ra các tinh chỉnh cho các cặp (mã không chính xác, phản hồi ngôn ngữ tự nhiên), cả theo cách few-shot và sau khi tinh chỉnh.

Phản hồi chỉ yêu cầu cho các nhiệm vụ mà πθ ban đầu không thể tạo ra phản hồi chính xác, vì vậy trước tiên chúng tôi đánh giá CODEGEN-MONO 6.1B zero-shot trên toàn bộ MBPP, tạo ra 30 chương trình mỗi nhiệm vụ với temperature 0.8. Bảng 1 cho thấy tỷ lệ pass kết quả. Có 321 nhiệm vụ mà CODEGEN-MONO 6.1B zero-shot không cho ra mẫu chính xác nào (từ Bảng 1: (100% - 67%) × 974 nhiệm vụ ≈ 321). Sau đó chúng tôi chú thích một chương trình không chính xác mỗi nhiệm vụ với cả phản hồi và tinh chỉnh, như được mô tả trong Phần 3.

**Kết hợp Phản hồi Few-Shot** Chúng tôi sử dụng các chú thích phản hồi của con người để tạo các prompt phản hồi few-shot, được định dạng như trong Hình 2. Chúng tôi đánh giá khả năng của CODEGEN-MONO 6.1B trong việc tạo ra các tinh chỉnh kết hợp phản hồi và vượt qua các bài kiểm tra đơn vị. Tuy nhiên, việc tạo ra một tinh chỉnh vượt qua các bài kiểm tra đơn vị không đảm bảo rằng phản hồi đã được kết hợp; có thể có nhiều giải pháp cho một nhiệm vụ lập trình, bao gồm những giải pháp có chức năng nhưng hoàn toàn khác biệt và không sử dụng phản hồi để cải thiện mã gốc. Ngoài ra, mô hình có thể đã có khả năng sửa chữa các chương trình mà không cần phản hồi. Do đó, chúng tôi cũng đánh giá tỷ lệ pass sau khi xáo trộn các mẫu phản hồi trong tập dữ liệu, để đánh giá xem khả năng sửa chữa mã của mô hình có giảm khi được trình bày với phản hồi không liên quan hay không.

Kết quả được hiển thị trong Bảng 2. Khả năng của CODEGEN-MONO 6.1B trong việc kết hợp phản hồi liên quan trên tập hợp chương trình cụ thể này là thấp, với pass@10 chỉ đạt 13.8%. Tuy nhiên, khoảng cách về độ chính xác giữa các tinh chỉnh được tạo bởi CODEGEN-MONO 6.1B trên phản hồi liên quan so với không liên quan là đáng kể, với pass@10 giảm 71% (tương đối; 13.8% → 4.0%), cho thấy mô hình thực sự đang sử dụng phản hồi.

**Huấn luyện πRefine** Tiếp theo, chúng tôi kiểm tra xem liệu chúng ta có thể cải thiện khả năng sửa chữa chương trình với phản hồi bằng cách tinh chỉnh một mô hình riêng biệt cụ thể để thực hiện nhiệm vụ này hay không. Các ví dụ huấn luyện của chúng tôi bao gồm các bộ ba chương trình không chính xác, phản hồi do con người viết và tinh chỉnh do con người viết. Chúng tôi huấn luyện mô hình để tối đa hóa khả năng của tinh chỉnh cho chương trình và phản hồi. Các chương trình không chính xác được tạo bởi CODEGEN-MONO 6.1B zero-shot trên các nhiệm vụ MBPP, và phản hồi và tinh chỉnh được viết bởi các chú thích viên con người, như được thảo luận trong Phần 3. Chúng tôi chỉ bao gồm các nhiệm vụ mà không có chương trình nào được tạo bởi CODEGEN-MONO 6.1B là chính xác, tạo ra 44 nhiệm vụ trong tập dữ liệu huấn luyện (tạo thành phân chia MBPP Refine) và 128 nhiệm vụ trong tập dữ liệu đánh giá (tạo thành phân chia MBPP Train). Chúng tôi yêu cầu các chú thích viên con người viết tinh chỉnh của mã gốc kết hợp phản hồi do chính họ viết trước đó, vượt qua các bài kiểm tra đơn vị và chỉ thực hiện các chỉnh sửa tối thiểu đối với mã (xem Phần 3). Định dạng của dữ liệu huấn luyện cũng khớp với định dạng prompt few-shot (Hình 2) nhưng không có các ví dụ tinh chỉnh trong ngữ cảnh. Chúng tôi ký hiệu mô hình này là πRefine, như được mô tả trong Phần 2.3.

Bảng 3 cho thấy tỷ lệ pass cho πRefine trên tập dữ liệu đánh giá, được tạo ra bằng cách lấy mẫu 30 tinh chỉnh mỗi nhiệm vụ với temperature 0.8. Tinh chỉnh cải thiện đáng kể khả năng của CODEGEN-MONO 6.1B trong việc kết hợp phản hồi so với tinh chỉnh 1-shot, tăng tỷ lệ pass hơn ba lần (2 → 19% pass@1, 13.8 → 47% pass@10, từ Bảng 2 và 3). Hơn nữa, 61% nhiệm vụ có ít nhất một tinh chỉnh chính xác. Điều này đặc biệt có ý nghĩa khi xem xét thực tế là chúng tôi chỉ chọn các nhiệm vụ mà mô hình CODEGEN-MONO 6.1B chưa được tinh chỉnh ban đầu không tạo ra bất kỳ chương trình chính xác nào (cột ngoài cùng bên phải trong Bảng 3). Đối với 61% nhiệm vụ xác thực mà πRefine tạo ra tinh chỉnh chính xác, chúng tôi chọn ngẫu nhiên một chương trình chính xác như vậy cho mỗi nhiệm vụ để tạo thành tập dữ liệu huấn luyện cho mô hình cuối cùng πθ* của chúng tôi, tạo ra tập dữ liệu huấn luyện cuối cùng gồm 78 ví dụ.

### 3.2. ILF Tạo ra Tỷ lệ Pass Cao hơn Tinh chỉnh trên Dữ liệu Vàng hoặc Chương trình do Con người Viết

Với việc các tinh chỉnh của chúng tôi cải thiện so với các chương trình ban đầu, bây giờ chúng tôi tinh chỉnh trên các tinh chỉnh để cải thiện mô hình tạo mã của chúng tôi. Như đã thảo luận trước đó, chúng tôi sử dụng các tinh chỉnh chính xác (được đánh giá bằng các bài kiểm tra đơn vị) mà πRefine tạo ra cho tập dữ liệu đánh giá của nó làm tập dữ liệu huấn luyện cho πθ*. Vì πθ* được dự định để tạo mã từ mô tả nhiệm vụ ngôn ngữ tự nhiên (thay vì kết hợp phản hồi vào tinh chỉnh), các đầu vào của tập dữ liệu huấn luyện của chúng tôi là các prompt MBPP và các mục tiêu là 78 tinh chỉnh được tạo bởi πRefine được mô tả trong phần trước. Chúng tôi cũng so sánh hiệu suất của π*θ với CODEGEN-MONO 6.1B được đánh giá theo cách zero-shot, CODEGEN-MONO 6.1B được tinh chỉnh trên các chương trình vàng từ tập dữ liệu MBPP và CODEGEN-MONO 6.1B được tinh chỉnh trên các tinh chỉnh do con người viết của chúng tôi. Đối với tất cả các thí nghiệm tinh chỉnh, chúng tôi huấn luyện trên các chương trình tương ứng với cùng tập ID nhiệm vụ như những chương trình được sử dụng trong tập dữ liệu huấn luyện của πθ*.

Ngoài ra, chúng tôi đánh giá tác động của việc loại bỏ các chú thích con người trong thuật toán của chúng tôi bằng cách sử dụng LLM thay cho con người để tạo ra phản hồi và tinh chỉnh (thay thế bước 3 và 4 trong Thuật toán 1). Đối với LLM, chúng tôi sử dụng GPT-3.5 được tinh chỉnh với Feedback Made Easy (FeedME; text-davinci-002 trên OpenAI API). Chúng tôi gọi mô hình này là InstructGPT, đây là dòng mô hình OpenAI mà FeedME thuộc về (OpenAI, 2022). Chúng tôi sử dụng InstructGPT để tạo ra cả phản hồi và tinh chỉnh trên các chương trình gốc. Sau đó chúng tôi tinh chỉnh CODEGEN-MONO 6.1B trên các tinh chỉnh được tạo bởi mô hình.

Kết quả của thuật toán ILF của chúng tôi so với các đường cơ sở và loại bỏ được hiển thị trong Bảng 4. ILF tạo ra tỷ lệ pass@1 và pass@10 cao nhất, mặc dù chúng tôi sử dụng rất ít mẫu phản hồi và tinh chỉnh. Tỷ lệ pass@1 đặc biệt cho thấy sự gia tăng đáng kể so với đường cơ sở zero-shot, đại diện cho sự gia tăng 10% tuyệt đối (38% tương đối). Cải thiện pass@1 đặc biệt hữu ích để hỗ trợ kỹ thuật phần mềm, nơi việc đề xuất một hoàn thành chính xác duy nhất hữu ích hơn so với 10 hoàn thành có thể để người dùng chọn.

So với các tiêu chuẩn vàng, ILF vượt trội hơn cả tinh chỉnh trên các chương trình vàng MBPP và tinh chỉnh do con người viết trên chỉ số pass@1, tạo ra sự gia tăng 14% tuyệt đối (64% tương đối) và 3% tuyệt đối (9% tương đối) trong tỷ lệ pass@1, tương ứng. Tuy nhiên, huấn luyện trên các tinh chỉnh do con người viết tạo ra tỷ lệ pass@10 có thể so sánh với ILF, điều này không đáng ngạc nhiên vì πRefine được huấn luyện trên các tinh chỉnh do con người viết. Khi phản hồi do con người viết và tinh chỉnh được tạo bởi πRefine bị loại bỏ (phần "Ablations" của Bảng 4), ILF cũng vượt trội hơn huấn luyện trên cả tinh chỉnh được tạo bởi InstructGPT 1-shot và 2-shot lần lượt 17% và 11% tuyệt đối (89% và 44% tương đối).

**Phân tích Nguồn Dữ liệu Huấn luyện** Tuy nhiên, chúng tôi cũng lưu ý thực tế đáng ngạc nhiên rằng việc chỉ huấn luyện trên một mẫu nhỏ các chương trình vàng MBPP không tạo ra sự khác biệt đáng kể về độ chính xác so với suy luận zero-shot. Chúng tôi phỏng đoán rằng các chương trình vàng từ tập dữ liệu MBPP có thể hơi ngoài phân phối đối với CODEGEN-MONO 6.1B. Để kiểm tra giả thuyết này, chúng tôi tính toán perplexity của các chương trình vàng MBPP, các tinh chỉnh được tạo bởi πRefine và các tinh chỉnh do con người viết bằng mô hình CODEGEN-MONO 6.1B đã được huấn luyện trước. Kết quả được hiển thị trong Hình 3. Mặc dù phân phối của cả ba nguồn dữ liệu trông tương tự, tập dữ liệu MBPP chứa nhiều chương trình có perplexity cao hơn (tức là các chương trình có perplexity ≥ 10²) so với các tinh chỉnh được tạo bởi πRefine hoặc các tinh chỉnh do con người viết. Do đó, có khả năng CODEGEN-MONO 6.1B học từ hai tập dữ liệu sau dễ dàng hơn, vì chúng gần với phân phối gốc của CODEGEN-MONO 6.1B hơn trong khi vẫn chính xác về mặt chức năng.

Hơn nữa, ILF đặc biệt hữu ích cho các thiết lập mà không có sẵn lượng lớn mã vàng. Trong thiết lập này, ILF có thể được coi như một phương pháp không chỉ tạo ra nhiều dữ liệu huấn luyện hơn, mà dữ liệu huấn luyện gần với đầu ra gốc của mô hình trong không gian biểu diễn dữ liệu và cụ thể sửa chữa các loại lỗi mà mô hình gốc tạo ra. Do đó, việc tinh chỉnh mô hình trên các tinh chỉnh được tạo bởi πRefine không yêu cầu điều chỉnh trọng số nhiều như việc tinh chỉnh mô hình trên các chương trình vàng MBPP, mặc dù cả hai tập dữ liệu huấn luyện đều chứa cùng số lượng chương trình chính xác về mặt chức năng.

### 3.3. Mở rộng Phản hồi Mô hình Không Mang lại Lợi ích Tương tự như Phản hồi Con người

Vì phản hồi con người chất lượng cao có thể tốn kém để thu thập, chúng tôi cũng đánh giá bao nhiêu phản hồi mô hình có thể mang lại lợi ích tương tự như mẫu phản hồi do con người viết của chúng tôi. Để làm điều này, chúng tôi chọn ngẫu nhiên k nhiệm vụ từ tập hợp các nhiệm vụ MBPP mà CODEGEN-MONO 6.1B ban đầu không tạo ra câu trả lời chính xác và prompt InstructGPT để tạo ra cả phản hồi và tinh chỉnh. Sau đó chúng tôi đánh giá các tinh chỉnh về tính chính xác và huấn luyện CODEGEN-MONO 6.1B trên các tinh chỉnh chính xác. Chúng tôi sử dụng k ∈ {50, 100, 200} và tạo ra 30 mẫu đầu ra với temperature 0.8 cho tất cả các giai đoạn của thí nghiệm. Chúng tôi bị giới hạn ở các giá trị k này do số lượng nhỏ các nhiệm vụ chúng tôi có trong MBPP Train, nhưng nghiên cứu tương lai có thể điều tra việc mở rộng các thí nghiệm này bằng cách sử dụng các tập dữ liệu lớn hơn hoặc tự động tạo ra các nhiệm vụ và bài kiểm tra đơn vị mới cho tập dữ liệu huấn luyện. Chi tiết huấn luyện thêm được liệt kê trong Phụ lục A.1.

Kết quả được hiển thị trong Hình 4. Mặc dù việc tăng số lượng phản hồi được tạo bởi InstructGPT mang lại những cải thiện khiêm tốn trong tỷ lệ pass, những cải thiện này không tạo ra tỷ lệ pass cao như của πθ*, mặc dù πθ* chỉ sử dụng tổng cộng 122 phần phản hồi trong toàn bộ quá trình huấn luyện của nó (44 để huấn luyện πRefine và 78 để tạo ra tinh chỉnh để huấn luyện πθ*). Tuy nhiên, khi các mô hình ngôn ngữ lớn đã được huấn luyện trước tiếp tục cải thiện mạnh mẽ về chất lượng, chúng tôi hy vọng rằng khoảng cách giữa phản hồi do con người và mô hình viết này sẽ ngày càng thu hẹp.

### 3.4. Phản hồi Con người Có nhiều Thông tin hơn Phản hồi InstructGPT

Để hiểu rõ hơn tại sao phản hồi con người tạo ra những cải thiện lớn hơn trong tỷ lệ pass so với phản hồi InstructGPT, chúng tôi chọn ngẫu nhiên 50 mẫu phản hồi cho mỗi nguồn (tức là con người hoặc InstructGPT) và chú thích số lượng và loại lỗi mà mỗi mẫu phản hồi giải quyết. Kết quả được hiển thị trong Bảng 5 và 6. Chúng tôi quan sát thấy InstructGPT thường không đưa ra phản hồi (ví dụ: "Mã là chính xác" hoặc "Làm tốt lắm!"), cung cấp phản hồi không liên quan hoặc không chính xác, hoặc nhắc lại mô tả nhiệm vụ thay vì giải quyết những gì cần được sửa chữa về mã. Mặc dù vậy, các tinh chỉnh của InstructGPT thường chính xác ngay cả khi bản thân phản hồi không như vậy. Phản hồi do con người viết giải quyết nhiều lỗi hơn trung bình và không bao giờ đưa ra phản hồi không liên quan. Chúng tôi cung cấp thêm ví dụ về sự khác biệt giữa phản hồi của con người và InstructGPT trong Phụ lục A.3.

### 3.5. πRefine Gặp khó khăn trong việc Kết hợp Phản hồi Giải quyết Nhiều Lỗi

Cuối cùng, chúng tôi khám phá xem số lượng lỗi được giải quyết trong phản hồi có ảnh hưởng đến khả năng của πRefine trong việc sửa chữa mẫu mã gốc hay không. Kết quả được hiển thị trong Hình 5. Càng nhiều lỗi được giải quyết, tỷ lệ pass trung bình của các tinh chỉnh của πRefine càng thấp. Điều này cho thấy một hướng nghiên cứu hứa hẹn cho công việc tương lai có thể bao gồm việc tự động phân tách phản hồi thành nhiều bước và để πRefine kết hợp phản hồi từng bước một. Thực vậy, Nijkamp et al. (2022) cho thấy rằng các mô hình CODEGEN thường hiệu quả hơn trong việc tuân theo hướng dẫn khi các hướng dẫn được đưa ra qua nhiều lượt, và nghiên cứu Chain-of-Thought gần đây (Wei et al., 2022) minh họa một kỹ thuật prompting tương tự.

## 4. Nghiên cứu Liên quan

**LLM cho Tổng hợp Chương trình** Công trình của chúng tôi xây dựng trên một lượng lớn tài liệu khám phá việc sử dụng LLM đã được huấn luyện trước cho tổng hợp chương trình thần kinh. Nhiều LLM đa năng, mặc dù không được huấn luyện trước cụ thể cho tạo mã, đã chứng minh sự thành thạo ấn tượng trong việc giải quyết các thách thức mã vì chúng được huấn luyện trước trên các kho văn bản lớn như THEPILE (Gao et al., 2020) chứa một tỷ lệ nhỏ nội dung mã (Austin et al., 2021; Wang & Komatsuzaki, 2021; Black et al., 2022; Nijkamp et al., 2022). Tuy nhiên, các LLM gần đây khác cho tổng hợp chương trình được huấn luyện chỉ trên các tệp mã nguồn (Wang et al., 2021; Zan et al., 2022; Li et al., 2022; Xu et al., 2022), hoặc trên cả tài liệu văn bản và mã nguồn – đôi khi hoặc liên tiếp (Chen et al., 2021; Nijkamp et al., 2022; Bai et al., 2022a), trong một kho hỗn hợp (Workshop et al., 2022), hoặc trên các tài liệu hỗn hợp ngôn ngữ tự nhiên-ngôn ngữ lập trình (Feng et al., 2020).

**Học từ Phản hồi Con người** Thuật toán của chúng tôi được lấy cảm hứng từ một số công trình trước đây đã huấn luyện mô hình để học từ phản hồi. Một kỹ thuật phổ biến là học tăng cường từ phản hồi con người (RLHF Ziegler et al., 2019; Stiennon et al., 2020; Ouyang et al., 2022), huấn luyện mô hình để thỏa mãn sở thích con người. Tuy nhiên, thuật toán của chúng tôi gần gũi hơn với các công trình sử dụng phản hồi ngôn ngữ tự nhiên, thay vì so sánh giữa các lựa chọn khác nhau. Elgohary et al. (2020); Austin et al. (2021); Nijkamp et al. (2022) đều chứng minh rằng hiệu suất LLM mã nói chung cải thiện khi được prompt với phản hồi ngôn ngữ tự nhiên, mặc dù Nijkamp et al. (2022) quan sát thấy rằng phản hồi hiệu quả hơn khi được đưa ra từng bước một. Công trình của chúng tôi khác với những nghiên cứu này ở chỗ ILF học từ phản hồi tại thời điểm huấn luyện, không phải tại thời điểm suy luận.

Bai et al. (2022a) cũng sử dụng phản hồi ngôn ngữ tự nhiên trong quá trình huấn luyện, nhưng như một phần của thuật toán RLHF thay vào đó, nơi phản hồi được sử dụng để thu hút các phản hồi khác nhau từ trợ lý kỹ thuật số, các phản hồi được xếp hạng bởi các công nhân đám đông và các xếp hạng được sử dụng để huấn luyện mô hình sở thích. Tuy nhiên, họ lưu ý rằng dạng học từ phản hồi ngôn ngữ tự nhiên này không cải thiện mô hình tạo mã của họ một cách đo được hơn so với việc chỉ prompting đơn thuần.

Ngoài tổng hợp chương trình, chúng tôi cho thấy trong công trình khác của chúng tôi (Scheurer et al., 2023) rằng ILF cũng hiệu quả cho tóm tắt văn bản. Ngoài việc tái công thức hàm phần thưởng R(·) cho tóm tắt, Scheurer et al. (2023) bổ sung chứng minh rằng một LLM được tinh chỉnh hướng dẫn có thể đánh giá các đầu ra của chính nó và chọn ra đầu ra tốt nhất. Tương tự như kết quả của chúng tôi về tạo mã, Scheurer et al. (2023) cho thấy ILF vượt trội hơn tất cả các đường cơ sở tinh chỉnh có giám sát trên tóm tắt văn bản. Điều này phù hợp với nhiều công trình khác đã khám phá giám sát thông qua ngôn ngữ tự nhiên theo những cách khác, chẳng hạn như thông qua giải thích (Camburu et al., 2018; Hase & Bansal, 2021; Pruthi et al., 2021; Lampinen et al., 2022, trong số các nghiên cứu khác) và như một phần của các hệ thống RL (Fidler et al., 2017; Luketina et al., 2019; Lin et al., 2020, trong số các nghiên cứu khác).

## 5. Kết luận

Chúng tôi đã chỉ ra rằng ILF có thể cải thiện đáng kể chất lượng của mô hình tạo mã, ngay cả với chỉ một mẫu nhỏ phản hồi và tinh chỉnh do con người viết. Cách tiếp cận này được biện minh về mặt lý thuyết như tối thiểu hóa phân kỳ KL mong đợi giữa πθ và một phân phối chân lý cơ bản mục tiêu, nơi chúng ta thu được tín hiệu từ phân phối sau thông qua phản hồi ngôn ngữ tự nhiên do con người viết.

Cách tiếp cận này cũng hấp dẫn vì nó không phụ thuộc vào mô hình cụ thể (theo nghĩa là ILF có thể được sử dụng với bất kỳ loại mô hình cơ sở πθ nào, giả định sự tồn tại của một LLM đủ khả năng để hoạt động như πRefine), và có thể được tiến hành trong nhiều vòng để liên tục cải thiện mô hình. Hơn nữa, đáng chú ý rằng cách tiếp cận của chúng tôi tạo ra dữ liệu huấn luyện không chỉ chính xác mà còn nhắm vào các loại lỗi cụ thể mà mô hình có khả năng tạo ra. Về bản chất, nó cung cấp một tín hiệu huấn luyện trực tuyến bị thiếu trong thiết lập huấn luyện trước ngoại tuyến của các LLM hiện đại. Cách tiếp cận của chúng tôi cũng rất hiệu quả về mẫu, tạo ra sự gia tăng 38% và 64% tương đối trong tỷ lệ pass@1 so với đường cơ sở zero-shot và tinh chỉnh trên dữ liệu MBPP, mặc dù tinh chỉnh chỉ trên 78 ví dụ.

Công trình của chúng tôi mở ra nhiều hướng nghiên cứu hứa hẹn cho tương lai. Chẳng hạn, ILF có thể được áp dụng lặp đi lặp lại trong nhiều vòng bất cứ khi nào có thông tin mới (ví dụ: cú pháp Python mới) hoặc phát hiện lỗi mới. Khi tốc độ tiến bộ của nghiên cứu LLM hiện đại tiếp tục tăng tốc, có thể sớm sẽ khả thi để tự động hóa một phần hoặc hoàn toàn việc tạo ra phản hồi ngôn ngữ tự nhiên (tương tự như 'RL từ phản hồi AI' (RLAIF; Bai et al., 2022b) và các thí nghiệm của chúng tôi trong Phần 3.3), giảm đáng kể cả thời gian và chi phí cần thiết để thu thập phản hồi. Hướng công việc này cũng đặc biệt hấp dẫn vì tín hiệu học dựa trên quá trình thay vì dựa trên kết quả, điều này đã được chứng minh là giảm thiểu reward hacking và cải thiện tính chính xác của các bước lý luận trung gian (Uesato et al., 2022). Mặc dù cần có thêm công việc để mở rộng phương pháp của chúng tôi, ILF đại diện cho một bước tiến thú vị trong việc huấn luyện LLM với phản hồi phong phú, tương tác và hiệu quả về mẫu.

## Lời cảm ơn

Chúng tôi biết ơn Nitarshan Rajkumar, Jason Phang, Nat McAleese, Geoffrey Irving, Jeff Wu, Jan Leike, Cathy Yeh, William Saunders, Jonathan Ward, Daniel Ziegler, Seraphina Nix, Quintin Pope, Kay Kozaronek, Peter Hase, Talia Ringer, Asa Cooper Stickland, Jacob Pfau, David Lindner, Lennart Heim, Kath Lumpante và Pablo Morena vì những thảo luận hữu ích và phản hồi về thiết kế và triển khai công việc này. Chúng tôi cũng biết ơn Scott Heiner và Edwin Chen vì sự giúp đỡ rộng rãi trong việc thiết lập quy trình và giao diện chú thích con người của chúng tôi. EP cảm ơn National Science Foundation và Open Philanthropy vì sự hỗ trợ học bổng. JAC được hỗ trợ bởi học bổng tiến sĩ từ MECD Tây Ban Nha. AC, SB và KC được hỗ trợ bởi National Science Foundation Awards 1922658 và 2046556. Bất kỳ ý kiến, phát hiện và kết luận hoặc khuyến nghị nào được thể hiện trong tài liệu này đều thuộc về (các) tác giả và không nhất thiết phản ánh quan điểm của National Science Foundation. KC được hỗ trợ bổ sung bởi 42dot, Hyundai Motor Company (dưới dự án Uncertainty in Neural Sequence Modeling) và Samsung Advanced Institute of Technology (dưới dự án Next Generation Deep Learning: From Pattern Recognition to AI). Dự án này cũng được hưởng lợi từ hỗ trợ tài chính cho SB bởi Eric và Wendy Schmidt (được thực hiện theo khuyến nghị của chương trình Schmidt Futures), Open Philanthropy và Apple. Chúng tôi cũng cảm ơn NYU High-Performance Computing Center vì sự hỗ trợ hiện vật và OpenAI vì việc cung cấp quyền truy cập và tín dụng cho các mô hình của họ thông qua Chương trình Truy cập Học thuật API.
