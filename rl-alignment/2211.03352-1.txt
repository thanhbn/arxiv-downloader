# 2211.03352.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2211.03352.pdf
# File size: 4831225 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
1
Curriculum-based Asymmetric Multi-task
Reinforcement Learning
Hanchi Huang, Deheng Y e, Li Shen, and Wei Liu
Abstract ‚ÄîWe introduce CAMRL, the Ô¨Årst curriculum-based asymmetric multi-task learning (AMTL) algorithm for dealing with multiple
reinforcement learning (RL) tasks altogether. To mitigate the negative inÔ¨Çuence of customizing the one-off training order in
curriculum-based AMTL, CAMRL switches its training mode between parallel single-task RL and asymmetric multi-task RL (MTRL),
according to an indicator regarding the training time, the overall performance, and the performance gap among tasks. To leverage the
multi-sourced prior knowledge Ô¨Çexibly and to reduce negative transfer in AMTL, we customize a composite loss with multiple differentiable
ranking functions and optimize the loss through alternating optimization and the Frank-Wolfe algorithm. The uncertainty-based automatic
adjustment of hyper-parameters is also applied to eliminate the need of laborious hyper-parameter analysis during optimization. By
optimizing the composite loss, CAMRL predicts the next training task and continuously revisits the transfer matrix and network weights.
We have conducted experiments on a wide range of benchmarks in multi-task RL, covering Gym-minigrid, Meta-world, Atari video games,
vision-based PyBullet tasks, and RLBench, to show the improvements of CAMRL over the corresponding single-task RL algorithm and
state-of-the-art MTRL algorithms. The code is available at: https://github.com/huanghanchi/CAMRL.
Index Terms ‚ÄîReinforcement learning, curriculum learning, asymmetric multi-task learning.
F
1 I NTRODUCTION
MULTI -TASK learning (MTL) trains multiple related tasks
simultaneously while taking advantage of similarities
and differences between tasks [38]. However, in MTL, nega-
tive transfer may occur since not all tasks can beneÔ¨Åt from
joint learning. To handle this, asymmetric transfer between
every pair of tasks has been developed [19], such that the
amount of transfers from a conÔ¨Ådent network to a relatively
less conÔ¨Ådent one is larger than the other way around. The
core idea is to learn a sparse weighted directed regularization
graph between every two tasks through a curriculum-based
asymmetric multi-task learning (AMTL) mode, which has
been proven to be very effective in supervised learning [19],
[20], [24], while remaining to be unexplored in reinforcement
learning (RL).
In this paper, inspired by AMTL, we study the problem
of mitigating negative transfer in multi-task reinforcement
learning (MTRL). Note that incorporating AMTL techniques
into the context of RL is not easy. Due to the non-stationarity
when training RL tasks and the lack of prior knowledge on
RL tasks‚Äô properties, it is likely that we end up with learning
a poor regularization graph between tasks, if we directly
adopt curriculum-based AMTL to train tasks one by one
without proper corrections to the training order. Besides,
some important factors for Ô¨Çexiblizing the transfer among
tasks and for avoiding negative transfer, such as indicators
representing relative training progress, the transfer perfor-
mance between tasks, and the behavioral diversity described
in [25], have been neglected by existing works on AMTL [19],
[20], [24], which may lead to slow convergence for a RL task,
Hanchi Huang is with Nanyang Technological University, Singapore.
Email: hhuang036@e.ntu.edu.sg. Deheng Ye and Wei Liu are with Tencent
Inc., China. Email: dericye@tencent.com, wl2223@columbia.edu. Li Shen
is with JD.com Inc., China. Email: mathshenli@gmail.com
Deheng Ye and Wei Liu are the corresponding authors.and cause serious negative transfer before convergence. Also,
it is worth noting that performing curriculum-based AMTL
all the time is time-consuming and can lead to performance
degradation when each task is poorly trained or has already
been well-trained. Consequently, RL tasks require different
demands for curriculum learning at different training stages
and the amount of experience learned from different tasks
needs to be dynamically adjusted.
To deal with the above issues, we propose CAMRL, the
Ô¨Årst Curriculum-based Asymmetric Multi-task algorithm
forReinforcement Learning. To balance both the efÔ¨Åciency
and the amount of curriculum-based transfers across tasks,
we design a composite indicator weighting multiple factors
to decide which training mode to switch between parallel
single-task learning and curriculum-based AMTL. To avoid
customizing the one-off order of training tasks, CAMRL
re-calculates the above indicator at the beginning of every
epoch to switch the training mode. When entering the AMTL
training mode, CAMRL updates the training order, the
regularization graph, and the network weights by optimizing
a composite loss function through alternating optimization
and the Frank-Wolfe algorithm. The loss function regularizes
both the amount of outgoing transfers and the similarity
across multiple network weights. Furthermore, three novel
differentiable ranking functions are proposed to Ô¨Çexibly
incorporate various prior knowledge into the loss, such as
the relative training difÔ¨Åculty, the performance of mutual
evaluation, and the similarity between tasks. Finally, we
discuss the Ô¨Çexibility of CAMRL from multiple perspectives
and its limitations, so as to illuminate further incorporations
of curriculum-based AMTL into RL.
Our contributions are summarized as follows:
We propose the CAMRL (curriculum-based asym-
metric multi-task reinforcement learning) algorithm,arXiv:2211.03352v1  [cs.LG]  7 Nov 2022

--- PAGE 2 ---
2
‚Ä¶SAC1
Task 4Task 2Task 5Weights Transfer
Weights 
SimilarityOutput 
Transfer
K episodes K episodesEntering A New EpochAbsorb prior knowledge as 
much as possible!New Epoch
Parallel Single-
task Learning
SAC2SAC3
SAC4Curriculum-based AMTL
Goal: optimize the 
composite loss
RegularizePartial Ranking Loss
Factor1: (i) inFactor2: (ii) inFactor3: (iii) in 
‚Ä¶‚Ä¶
Convert 
Training Mode
‚Ä¶
‚Ä¶
Fig. 1: The workÔ¨Çow of CAMRL. When an epoch starts, CAMRL switches its training mode to single-task learning or
curriculum-based AMTL. For the latter mode, CAMRL customizes a curriculum for multiple tasks and updates the transfer
graph between tasks via a composite loss. CAMRL keeps the training mode for Kepisodes before entering a new epoch.
which is designed with a training-mode-switching
mechanism and a combinatorial loss containing mul-
tiple differentiable ranking functions. These designs
tackle the common issues in multi-task RL, e.g., nega-
tive transfer and poor utilization of prior knowledge.
CAMRL can be paired with various RL-based al-
gorithms and training modes, as well as absorbing
various prior knowledge and ranking information of
training factors of any amount, which are rarely seen
in previous MTRL works [28], [32], [34]. Moreover, by
dynamically adjusting parameters, CAMRL can adapt
to the arrival of a new task via a simple correction.
Experiments on a series of low-/high- dimensional
RL tasks show that CAMRL remarkably outperforms
the corresponding single-task RL algorithm and state-
of-the-art multi-task RL algorithms.
2 R ELATED WORK
2.1 Multi-Task Reinforcement Learning (MTRL)
Before stepping into the deep RL era, most works [6], [27]
on multi-task-oriented algorithms within RL attempted to
rely on the assistance from transfer learning. Later on,
knowledge sharing in multi-task deep RL was proposed
to distill the common traits among all tasks [12], [28]. Since
negative interference among gradients from various tasks
may occur very often during the distilling process, the
work [34] presented a gradient surgery method that alters
gradients by projecting each onto the normal plane of the
other when conÔ¨Çicts happen between gradients. Researchers
also tried to capture the similarity between gradients from
multiple tasks [7], [39]. Furthermore, apart from driving
gradients to be similar, applying compositional models
is also a natural approach to handle gradients‚Äô conÔ¨Çicts.
By breaking down a multi-task problem into modules
and supporting composability of separate modules in new
modular RL agents, the compositional model enables to
relieve the conÔ¨Çicts among gradients and facilitate better
generalization [9]. However, training sub-policies separately
often requires well-predeÔ¨Åned subtasks, which may be
infeasible in real-world applications. Rather than manually
deÔ¨Åning the modules or sub-policies and the speciÔ¨Åcation
of their combination, the authors of [32] introduced the soft
module, a fully end-to-end method which generates soft
combinations of multiple modules automatically without
specifying the policy structure in advance.2.2 Curriculum Learning
Curriculum learning (CL) describes a learning mode in which
we start with easy tasks and then gradually increase the
task difÔ¨Åculty. The idea of using curricula to train learning
agents was Ô¨Årst proposed in [11] and Ô¨Årst introduced for
deep learning by [5]. Over the years, curricula have been
applied to train complex tasks or tasks with sparse rewards
[2], [3], [31], [33]. Early research efforts focused on the
manual conÔ¨Åguration of the curriculum which can be time-
consuming and requires domain knowledge. To deal with
the above issue, automatic CL has gained popularity recently,
which aims to automatically adapt the distribution of training
data by learning to adjust the selection of curricula according
to some factors such as diversity and learning progress [25].
Our work is, however, based on another line of works [19],
[24] that select the next task to train by performing joint
optimization on the loss of supervised learning tasks with
`2-norm regularizations on task parameters.
3 M ETHODOLOGY
3.1 Overview
The pipeline of CAMRL is shown in Figure 1. In CAMRL, we
customize a learning progress indicator to determine whether
to perform parallel single-task training or curriculum-based
AMTL at the beginning of each epoch. When performing
curriculum-based AMTL, a composite loss function that
learns the transfer between tasks and considers several
factors to avoid negative training is adopted. We apply
the alternating optimization and Frank-Wolfe algorithm to
decide the next task for training and update the transfer
matrix. In the meantime, the hyper-parameters in the loss are
automatically adjusted according to each term‚Äôs historical
uncertainty. When a new task arrives, CAMRL can quickly
adapt to the new scenario without much side effect to the
original tasks, by merely modifying the transfer matrix.
The overall training procedure of CAMRL is summarized
in Algorithm 1. Assume that we have Ttasks with varying
degrees of difÔ¨Åculty. In the beginning, we simply equip each
taskt2[T]with a soft actor-critic (SAC) network SACt[15],
and perform parallel single-task training for several epochs
without interference among losses for different tasks, that
is, eachSACt(t2[T])is trained independently. Denote wt
as the network parameters of the t-th SAC network. Next,
according to an indicator related to the learning progress,

--- PAGE 3 ---
3
Algorithm 1 CAMRL Algorithm
1:Inputs:1;2;i(i= 0;1;2;3;4), number of tasks T.
2:Initialization: The transfer matrix B=ITT; the soft actor-critic network SACtwith parameters wtfort2[T];
W= (w1;w2;:::;wT).
3:forn= 1;2;;Ndo
4: Perform parallel training of all SACtfort2[T]forKepisodes without interference among losses.
5:end for
6:forn=N+ 1;N+ 2;do
7: CalculateImuland sample uUniform ([0;1]).
8: ifu<Imulthen
9: Perform parallel training of SACt(t2[T])forKepisodes without interference among losses.
10: else
11: Predict the next training task tand update bo
t= (Bt1;:::;Bt(t 1);Bt(t+1);;BtT)andwtby optimizing:
(t;bo
t) arg min
t2U;bo
tn
0[(1+1kbo
tk1)L(wt) 2(bo
t)>lo
t]+1X
s2U tws i 1X
j=1B(j)sw(j) Btswt2
2
+2X
j2[q](j y0ij)2+3X
j2[T](rank(1)
j y00j)2+4X
j2[T](rank(2)
j y00j)2o
: (1)
12: Fixbo
t;W and select the task twhich minimizes the objective in Eq. (1).
13: Fixt;W and run vanilla Frank-Wolfe on Eq. (7) for several iterations to optimize bo
t.
14: Fixbo
tand train task twith the policy loss Eq. (1) for Kepisodes.
15: end if
16:end for
CAMRL decides whether to switch the training mode into
curriculum-based AMTL, which trains the tasks one by one
by optimizing a composite loss term regularizing the transfer
matrix between tasks. The indicator Imul(=(i)+(ii)+(iii)
3)
is the positive weighted sum of the following terms: (i)
exp( n=a), wherenrefers to the number of the current
epochs. We use exp( n=a)to encourage larger probability
of multi-task training in the early stage so that hard-to-
train tasks could beneÔ¨Åt from easy-to-train tasks as early
as possible. As we will show in Table 12, the performance of
CAMRL downgrades obviously when we remove this term
inImul; (ii)exp( Lnorb), whereLnormeans the average
normalized policy loss of all tasks during the last epoch.
We useLnorto judge the overall training progress. If the
overall training performance is poor, i.e. with a large Lnor,
then the possibility of performing inter-task transfer tends
to be smaller. Note that in experiments, we set Lnorto be
policy _loss averagepolicy _loss
stdpolicy _loss. See the following remark for the
details of the policy loss; (iii) the percentage of tasks whose
average normalized reward during the last epoch locates
outside the interval [Rnor cstdnor;Rnor+cstdnor],
wherestdnoris the standard deviation of the normalized
rewards for all tasks during that period. This term indicates
that the larger the learning process gap among tasks, the
larger the probability of performing multi-task training.
Remark. Policy loss of SAC in [32]. Due to the implementation
issue of SAC inheriting from the code of the soft-module [32], we
Ô¨Ånd it much more convenient to use the negative normalized
version of the policy loss, the direct output of the ‚Äôtrain‚Äô function
in the code of the soft-module, instead of the normalized return.
Besides, when encountering into difÔ¨Åcult tasks with sparse returns,
the normalized return might keep nearly unchanged from time to
time. In such scenarios, utilizing the negative normalized policyloss can help speed up model training more than the normalized
return. SpeciÔ¨Åcally, the policy loss of the SAC used in [32] can be
characterized as log(atjst) Q(at;st), where(atjst)
is the probability of adopting action atin face of the state stand
Q(at;st)is the estimated value of the state-action pair (at;st).
IfuUniform ([0;1])is smaller than Imul, then the
curriculum-based AMTL training mode will be temporarily
adopted in the next epoch; otherwise, we will perform the
single-task training mode (see line 8-10 in Algorithm 1). Here
we need to note that: Imul is constantly changing, it does
not matter if its value is temporarily greater than 1, which
indicates that performing curriculum-based AMTL training
is necessary at this time. When curriculum-based AMTL
is unnecessary, Imul falls back down again and becomes
smaller than 1.
3.2 Curriculum Multi-task Training
In this subsection, we follow [19] to establish the foundation
of curriculum multi-task training for our CMARL.
LetBbe aTTmatrix symbolizing the count of
transfers between each pair of tasks. For t2[T], letwtbe the
parameters of the t-th soft actor-critic network. We follow the
assumption in [19] that for all t2[T];wtPT
s=1Bstws, that
is,Bstrefers to the positive weight of basis wsrepresenting
wt. DenoteW:= (w1;w2;:::;wT).
The core of CAMRL is adapted from the following
composite loss [19]:
L(W;B ) =TX
t=1f(1 +kbo
tk1)L(wt) +kwt X
s6=tBstwsk2
2g;
(2)
wherebo
t= (Bt1;:::;Bt(t 1);Bt(t+1);:::;BtT)>2RT 1
represents the count of outgoing transfers from task tto

--- PAGE 4 ---
4
other tasks and we use kbo
tk1to satisfy the sparsity property
of the transfer matrix B;L(wt)is the policy loss for task t
underwt, andwtis the parameters of the t-th SAC network;
(;)are the coefÔ¨Åcients of weights for different terms in
Eq. (2).
Since simultaneous training of WandBmay result
in a serious negative transfer and a dimensional disaster
when performing optimization, Lee et al. [19] formulated the
curriculum-based training mode leveraging the paradigm
in Eq. (2), for the purpose of Ô¨Ånding the optimal order of
training tasks and only optimizing bo
tandwtrather than B
andWwhen training the task t.
DenoteSas the permutation space over Telements, and
for2S, denote(i)as thei-th element in permutation .
To perform curriculum learning of multiple tasks, Lee et al.
[19] adapted their goal to cope with Eq. (3):
min
2S;W;B0TX
i=1n
(1 +kbo
(i)k1)L(w(i))
+kw(i) i 1X
j=1B(j)(i)w(j)k2
2o
: (3)
LetT:=f(1);:::; (i 1)gbe trained tasks and
U=f1;:::;Tg T be untrained tasks in the current epoch,
respectively. Lee et al. [19] then found a task t2Uto be
learned next, so as to improve the future learning process
the most:
(t;bo
t) arg min
t2U;bo
tn
(1 +kbo
tk1)L(wt)
+X
s2U tkws i 1X
j=1B(j)sw(j) Btswtk2
2o
:
(4)
After selecting the task to be learned next and updating bo
t
by performing alternating optimization on Eq. (4), Lee et al.
[19] solved problem Eq. (5) to greedily minimize Eq. (3):
wt arg min
wtn
(1 +kbo
tk1)L(wt)
+kwt i 1X
j=1B(j)tw(j)k2
2o
: (5)
3.3 Loss ModiÔ¨Åcation
To reasonably distribute the count of transfers between every
two tasks and avoid too many negative transfers, we have
the following expectations and we modify the loss term in
CAMRL based on these expectations :
[Expectation 1] Denotept;ias the performance, usually
the average rewards over several evaluation episodes, on
training task iby using the network originally for training
taskt. When training task t, if we can test the transferability
among several tasks and approximately obtain pt;i1>pt;i2>
>pt;iqfor some tasks i1;i2;;iq, then we expect that
Bt;i1> Bt;i2>> Bt;iqholds as much as possible,
that is, the better evaluation performance on other tasks,
the larger number of transfers on those tasks. To meet the
above expectation, we expectPq
j=1(j yij)2to be as small
as possible, where jis the ranking of pt;ijamongpt;i1>
0.0 0.2 0.4 0.6 0.8 1.0012345 d=1
d=10
d=20
d=50
d=100Fig. 2: Ranking functions with various d. The points
displayed by the dashed lines refer to our randomly
generatedfpt;ijgj2[q].
pt;i2>> pt;iqandyijis the ranking of Bt;ijamong
Bt;i1;Bt;i2;;Bt;iq.
In the normal case, yijis non-differentiable on
Bt;i1;Bt;i2;;Bt;iq. Therefore, we construct a novel dif-
ferentiable ranking function y0
ij=q+ 1 Pq
s=1f0:5
tanh[d(Bt;ij Bt;is)] + 0:5gto replaceyijand the ranking
loss thus becomesPq
j=1(j y0
ij)2.
Here note that the idea of our differentiable ranking
function is inspired by [10]. Ayman [10] customized an
activation function in the form of multiple tanh functions,
which approximates the step function with equidistant points.
Here we modify the intercept representation according to
different combinations for the point set so as to obtain our
differentiable ranking loss which allows cut-off points with
unequal distance. With our modiÔ¨Åcation, the ranking loss can
incorporate various prior knowledge and training factors to
avoid negative transfer, besides the relative training difÔ¨Åculty
and the performance of mutual evaluation between tasks, as
stated below.
Figure 2 depicts the shapes of our adapted composite
tanh function under different d. At the end of every epoch
with length K, we can randomly select some tasks, measure
the performance of the network which is originally trained
for taskion taskj, and update pi;j.
[Expectation 2] If taskiis easier to train, that is,
with a smallerL(wi), then we expect a smaller number
of transfers from task tto taski. This is because if a task
is easy-to-train, i.e., with a faster training progress than
other tasks, then the number of transfer to that task from
other hard-to-train (i.e., with slower training progress) tasks
should intuitively be smaller. To achieve this, we deÔ¨Åne lo
t
as(L(w1);:::;L(wt 1);L(wt+1);:::;L(wT)). Then add the
 (bo
t)>lo
tterm and the corresponding differentiable ranking
lossP
j2[T](rank(1)
j y00j)2to Eq. (5), whererank(1)
jis the
ranking ofL(wj)amongL(w1);L(w2);;L(wT)andy00j
is the ranking of Bt;jamongBt;1;Bt;2;;Bt;T.
Note that the reason to add the  (bo
t)>lo
tis that to
minimize (bo
t)>lo
t, we need to maximize (bo
t)>lo
t. The
maximum y (bo
t)>lo
tcorresponds to the scenario where larger
Bt;iis multiplied with larger L(wi)and smaller Bt;iis
multiplied with smaller L(wi), which perfectly matches our
second expectation.
[Expectation 3] If taskiis more similar to task t, then
we expect a larger number of transfers from task tto task
i. Denote the similarity between task iand tasktassi;t
andrank(2)
ias the ranking of si;tamong (s1;t;s2;t;;sT;t).

--- PAGE 5 ---
5
Then in summary, the problem in Eq. (4) now becomes:
(t;bo
t) arg min
t2U;bo
tn
0[(1+1kbo
tk1)L(wt) 2(bo
t)>lo
t]
+1X
s2U tws i 1X
j=1B(j)sw(j) Btswt2
2+2X
j2[q](j y0ij)2
+3X
j2[T](rank(1)
j y00j)2+4X
j2[T](rank(2)
j y00j)2o
:
(6)
In order to optimize the objective in Eq. (6), we Ô¨Årst Ô¨Åx
bo
tand select task twith the minimum objective in Eq. (6).
Then Ô¨Åxtand apply the Frank-Wolfe algorithm to optimize
bo
twith a convergence guarantee. Finally, Ô¨Åx bo
tand train task
twith the policy loss Eq. (1) for Kepisodes.
Remark. Similarity measures for expectation 3. Regarding
the expectation 3 for our customized ranking loss function, we take
the following three similarity measures, respectively: a) the cosine
similarity between the critic networks; b) the cosine similarity
between the policy networks; c) the negative embedding distance
in the state space, which will be explained in the Discussion
Section. Among them, the Ô¨Årst measure has the best performance
in initial trials and hence we utilize this measure in all our
experiments. As for the negative embedding distance between task
iand taskj, denote the state space as Sand the embedding
distance between task iand taskjis calculated as follows:
(i) Sample 100 states,fs1;s2;;s100g, fromSuniformly
at random. (ii) Denote the embeddings of fs1;s2;:::;s 100g
calculated by the embedding layers of task iand taskjas
fei;1;ei;2;;ei;100gandfej;1;ej;2;;ej;100g, respectively.
(iii) Then, q
1
100P100
m=1jjei;m ej;mjj2
2is just the negative
embedding distance between task iand taskj.
3.4 Loss Optimization
Optimization of bo
t.To make the loss differentiable and
ensure the sparsity of the transfer matrix B, we replace
1kbo
tk1in Eq. (6)with1P
j2[T] ftgBtjand add the
bo
t0;kbo
tk1radius <1
2constraints to Eq. (6),
whereradius is the upper bound of kbo
tk1(t2[T])
that we set in experiments. DeÔ¨Åne f(bo
t) =0[(1 +
1P
j2[T] ftgBtj)L(wt) 2(bo
t)>lo
t] +1P
s2U tkws Pi 1
j=1B(j)sw(j) Btswtk2
2+2P
j2[q](j y0ij)2+
3P
j2[T](rank(1)
j y00j)2+4P
j2[T](rank(2)
j y00j)2, and
we can transform the optimization on bo
tinto the following
constrained optimization problem with objective f(bo
t):
min
bo
t0;kbo
tk1radiusf(bo
t): (7)
In experiments, we apply the following iterative methods
to solve the constrained optimization problem (7): vanilla
Frank‚ÄìWolfe [13], momentum Frank‚ÄìWolfe [1], projected
gradient descent (PGD), PGDMadry [22], and the General
Iterative Shrinkage and Thresholding algorithm (GIST) [14].
Among them, vanilla Frank‚ÄìWolfe achieves smaller f(bo
t)
within the same number of iterations and better convergence
performance. Therefore, we use vanilla Frank‚ÄìWolfe to
optimizebo
twith the convergence rate at the m-th iteration
1pm, according to [18] and [23]. See Appendix for the proof.Dynamic adjustment of hyper-parameters. We take the
optimization of each term in Eq. (6)as a multi-task learning
problem and automatically adjust fig4
i=0according to
each term‚Äôs standard error over the historical epochs [16].
Note that Kendal et al. [16] derived their multi-task loss
function based on maximizing the Gaussian likelihood with
homoscedastic uncertainty on supervised learning tasks, due
to the popularity and the simplicity of the method in [16].
Here we adapt this method by reÔ¨Åning the denominators in
the coefÔ¨Åcients of the loss to deal with our setting.
To be more speciÔ¨Åc, similar to [16], let i=1
42
i+(i2[4])
and0=1
22
0+, whereiis the standard error of the (i+ 1) -
th term in Eq. (6),(= 10 2)is added to avoid the zero value
of denominators, and we set 0to be1
22
0instead of1
42
0to
pay more attention to the original training loss. Thanks to
this automatic adjustment, our CAMRL eliminates the need
for laborious hyper-parameter analysis.
4 E XPERIMENTS
We compare our CAMRL with existing state-of-the-art al-
gorithms on the Gym-minigrid, Meta-world, Atari games,
Ravens, and RLBench, which are the benchmarks for multi-
task RL widely used in the community [4], [8], [35].
4.1 Environments
Gym-minigrid: Gym-minigrid environments [8] are par-
tially observable grid environments consisting of tasks with
increasing complexity levels. In Gym-minigrid, the agent
should Ô¨Ånd a key to open a locked door in a grid. By setting
different obstacles in the way and adding different require-
ments, we obtain different environments, such as DistShift,
Doorkey, DynamicObstacles, etc. Each environment can be
easily tuned in terms of size/complexity, which contributes to
Ô¨Åne-tuning the difÔ¨Åculty of tasks and performing curriculum
learning. In our experiments, we Ô¨Årst randomly select 9
environments, each of which owns more than 3tasks with
different complexities. Then, we choose the task with the
biggest complexity for each selected environment and form
the9tasks that we conduct experiments on with regard to
Gym-minigrid.
Meta-world: Meta-World [35] is a simulated benchmark
for meta reinforcement learning and multi-task learning,
containing 50tasks related to robotic manipulation. In
Meta-World, MT1,MT10, andMT50are multi-task-RL
environments that have 1,10, and 50tasks, respectively.
The tasks for MT10from Task 1to Task 10are ‚ÄôPick and
place‚Äô, ‚ÄôPushing‚Äô, ‚ÄôReaching‚Äô, ‚ÄôDoor opening‚Äô, ‚ÄôButton press‚Äô,
‚ÄôPeg insertion side‚Äô, ‚ÄôWindow opening‚Äô, ‚ÄôWindow closing‚Äô,
‚ÄôDrawer opening‚Äô, and ‚ÄôDrawer closing‚Äô, respectively.
The tasks for MT50from Task 1to Task 50are ‚ÄôTurn on
faucet‚Äô, ‚ÄôSweep‚Äô, ‚ÄôStack‚Äô, ‚ÄôUnstack‚Äô, ‚ÄôTurn off faucet‚Äô, ‚ÄôPush
back‚Äô, ‚ÄôPull lever‚Äô, ‚ÄôTurn dial‚Äô, ‚ÄôPush with stick‚Äô, ‚ÄôGet coffee‚Äô,
‚ÄôPull handle side‚Äô, ‚ÄôBasketball‚Äô, ‚ÄôPull with stick‚Äô, ‚ÄôSweep into
hole‚Äô, ‚ÄôDisassemble nut‚Äô, ‚ÄôPlace onto shell‚Äô, ‚ÄôPush mug‚Äô, ‚ÄôPress
handle side‚Äô, ‚ÄôHammer‚Äô, ‚ÄôSlide plate‚Äô, ‚ÄôSlide plate side‚Äô, ‚ÄôPress
button wall‚Äô, ‚ÄôPress handle‚Äô, ‚ÄôPull handle‚Äô, ‚ÄôSoccer‚Äô, ‚ÄôRetrieve
plate side‚Äô, ‚ÄôRetrieve plate‚Äô, ‚ÄôClose drawer‚Äô, ‚ÄôPress button top‚Äô,
‚ÄôReach‚Äô, ‚ÄôPress button top w/wall‚Äô, ‚ÄôReach with wall‚Äô, ‚ÄôInsert
peg side‚Äô, ‚ÄôPush‚Äô, ‚ÄôPush with wall‚Äô, ‚ÄôPick & place w/wall‚Äô,

--- PAGE 6 ---
6
TABLE 1: Averaged reward for Gym-minigrid tasks (each experiment repeated 5times, the average scores with standard
errors (in brackets) reported). The best results are bolded. The higher the metric, the better performance of the model.
Single AC Single AC+MRCL YOLOR-SAC+MRCL Distral+MRCL Gradient Surgery+MRCL CAMRL+MRCL(ours)
DistShiftEnv 0.05 (0.02) 0.78 (0.05) 0.09 (0.03) 0.07 (0.04) 0.08 (0.02) 0.95 (0.03)
DoorKeyEnv16x16 0.00 (0.00) 0.08 (0.03) 0.01 (0.01) 0.00 (0.00) 0.01 (0.01) 0.13 (0.02)
DynamicObstaclesEnv16x16 -0.99 (0.03) -0.83 (0.04) -0.88 (0.03) -0.98 (0.01) -0.97 (0.02) -0.01 (0.02)
EmptyEnv16x16 0.10 (0.04) 1.10 (0.04) 0.12 (0.03) 0.08 (0.03) 0.11 (0.03) 1.17 (0.05)
FetchEnv 0.07 (0.03) 0.83 (0.05) 0.14 (0.03) 0.09 (0.02) 0.09 (0.03) 0.89 (0.04)
KeyCorridor 0.01 (0.01) 0.02 (0.01) 0.09 (0.03) 0.02 (0.01) 0.08 (0.02) 0.51 (0.02)
LavaCrossingS9N3Env 0.02 (0.01) 0.13 (0.02) 0.02 (0.01) 0.01 (0.01) 0.02 (0.01) 0.04 (0.01)
LavaGapS7Env 0.03 (0.02) 0.53 (0.04) 0.21 (0.02) 0.03 (0.01) 0.05 (0.02) 0.87 (0.02)
MemoryS17Random 0.02 (0.01) 0.61 (0.04) 0.13 (0.02) 0.04 (0.02) 0.20 (0.04) 0.20 (0.03)
TABLE 2: Averaged reward for MT10 tasks. The higher the metric, the better performance of the model.
Single AC Single SAC YOLOR-SAC Distral Gradient Surgery Soft Module CAMRL(ours)
Pick and place 24277.65 (2349.23) 38526.19 (1930.41) 1744.85 (98.42) 12791.42 (938.4) 4347.38 (294.3) 324.91 (40.91) 47235.31 (1043.29)
Pushing -70.93 (10.26) 5.39 (4.93) -10.93 (8.54) -63.57 (4.82) -127.42 (4.25) -53.84 (4.82) 213.27 (10.51)
Reaching -109.39 (9.74) -49.73 (5.91) -53.31 (5.04) -123.61 (6.25) -117.74 (4.02) -38.36 (5.39) -50.36 (2.55)
Door opening -50.31 (8.22) 17.72 (7.14) 10.48 (6.41) -55.41 (4.92) -38.73 (5.49) 13.86 (6.03) 143.74 (10.48)
Button press -28.94 (3.04) 2539.78 (214.49) 409.21 (4.38) -15.87 (3.97) -63.25 (7.93) -9.41 (3.15) 1395.77 (82.91)
Peg insertion side -78.84 (5.21) 25.64 (6.92) 20.31 (4.27) -124.82 (7.92) -79.74 (5.37) -73.74 (6.93) 52.91 (3.81)
Window opening 13.82 (2.48) 1937.28 (102.41) 105.38 (18.49) 11.98 (3.51) 8.93 (3.85) 9.21 (4.14) 15683.29 (1702.03)
Window closing 9.74 (2.31) 8.96 (2.18) 15.87 (4.66) 10.71 (2.04) 12.82 (3.02) 37726.31 (2049.4) 23.64 (3.25)
Drawer opening -19.83 (3.59) 385.72 (39.03) 582.13 (48.93) -17.82 (6.15) -9.32 (3.61) 573.82 (30.98) 1329.39 (28.94)
Drawer closing 1532.98 (234.26) 310.83 (14.09) 193.84 (16.02) -40.86 (12.06) -39.94 (4.83) 611.83 (38.94) 622.74 (10.93)
TABLE 3: Averaged reward for MT50 tasks. The higher the metric, the better performance of the model.
Single AC Single SAC YOLOR-SAC Distral Gradient Surgery Soft Module CAMRL(ours)
Turn on faucet 49832.92 (3903.31) 4793.37 (203.94) 1495.39 (103.47) 7193.81 (102.78) 2893.73 (49.01) 2281.66 (56.93) 7644.89 (61.81)
Sweep -83.03 (20.93) -39.84 (7.93) -94.11 (8.33) -112.93 (13.95) -83.56 (15.03) -18.38 (5.03) -10.31 (3.19)
Stack -110.28 (19.04) -110.38 (8.94) -87.39 (7.37) -152.85 (14.91) -103.95 (7.53) -46.78 (5.82) -42.32 (4.01)
Unstack -47.39 (8.05) -47.17 (9.02) -56.31 (6.49) -74.93 (6.93) -62.38 (10.05) -54.21 (6.18) -31.26 (5.09)
Turn off faucet -50.89 (4.95) -8.38 (3.75) 83.92 (4.28) -12.93 (5.03) -2.31 (4.62) 24.49 (6.83) 1253.73 (21.15)
Push back -52.04 (10.03) -38.93 (6.08) -39.02 (4.39) -122.84 (4.95) -98.53 (7.93) 27.36 (12.05) -27.74 (3.16)
Pull lever -82.94 (10.92) -63.41 (8.38) -68.41 (8.02) -92.38 (7.31) -113.26 (13.85) -69.45 (9.96) -31.84 (7.06)
Turn dial -68.31 (9.83) -50.39 (7.17) -50.37 (8.36) -130.36 (11.03) -58.37 (7.84) -29.32 (4.01) -27.69 (7.68)
Push with stick 703.34 (38.98) -5.81 (1.83) 102.49 (14.07) -15.83 (2.97) -14.92 (4.04) -32.89 (5.81) 1376.83 (38.44)
Get coffee -74.95 (10.95) 72.18 (10.84) -42.46 (9.33) -142.18 (7.89) -40.35 (4.82) -12.96 (7.37) -52.93 (4.91)
Pull handle side -51.59 (10.02) -17.29 (8.05) -59.02 (5.12) 4938.29 (219.77) -62.83 (13.05) -58.72 (14.73) -48.93 (5.89)
Basketball 3295.06 (203.9) 2507.48 (114.85) 1830.94 (88.26) -163.21 (30.46) 4817.61 (248.49) 2194.36 (305.19) 16394.92 (319.85)
Pull with stick -123.59 (8.21) -40.09 (6.73) -38.05 (5.62) -113.48 (7.99) -118.37 (8.38) -38.91 (7.74) -28.92 (5.13)
Sweep into hole -82.83 (8.48) -38.34 (7.92) -10.99 (4.78) 10.76 (8.19) -38.96 (11.01) -28.53 (6.93) 16.74 (5.96)
Disassemble nut 39.09 (7.32) 3947.53 (393.07) 102.58 (7.49) 20.48 (5.68) -3.15 (4.82) 2160.02 (117.83) 1873.93 (79.22)
Place onto shell 11093.45 (302.38) 2.94 (4.02) 183.73 (19.41) -50.31 (5.24) 6973.13 (294.05) 11.97 (5.93) 274.01 (23.91)
Push mug -30.19 (5.31) 40.35 (7.08) -102.29 (9.28) -227.48 (5.39) -93.28 (8.29) 119.93 (16.34) 638.91 (11.34)
Press handle side -127.49 (8.91) -61.31 (5.05) 10.21 (6.77) -173.83 (6.03) -117.31 (10.93) -61.28 (4.91) 24.18 (5.03)
Hammer -74.82 (5.04) -73.29 (4.91) -79.63 (8.29) -147.21 (5.83) -91.22 (6.85) -71.98 (4.05) -60.93 (3.92)
Slide plate -78.25 (7.93) -39.14 (7.03) -0.92 (5.28) -122.84 (10.04) -107.31 (8.86) -33.71 (5.19) 425.64 (10.62)
Slide plate side -81.43 (6.37) -10.28 (3.09) -72.28 (6.99) -103.95 (7.01) -92.18 (10.51) -25.62 (4.97) -23.54 (3.09)
Press button wall -104.18 (6.55) -56.49 (4.72) -67.27 (8.19) -110.38 (4.96) -89.17 (7.31) 64.83 (7.37) -27.73 (4.16)
Press handle -107.74 (7.18) -39.26 (4.27) -68.81 (11.23) -106.91 (4.08) -65.28 (6.83) -48.36 (5.15) -49.18 (4.92)
Pull handle -82.94 (5.03) 34.58 (4.86) -82.17 (7.28) -134.02 (5.82) -77.29 (5.93) -22.16 (4.17) 46.84 (5.86)
Soccer -72.38 (8.93) 429.94 (20.28) 102.36 (9.66) -108.83 (5.37) -9.56 (3.85) 9.68 (3.01) 4 85.03 (20.75)
Retrieve plate side -87.36 (16.83) -3.91 (4.92) -1.38 (3.94) -105.31 (6.77) -78.93 (6.42) -96.43 (8.93) 127.94 (12.84)
Retrieve plate -116.93 (10.35) -1.63 (5.03) -45.08 (5.42) -123.98 (7.94) -62.18 (7.39) 24.91 (5.06) -42.65 (6.27)
Close drawer -58.81 (5.61) -0.98 (4.75) 29.03 (6.12) -114.05 (7.13) -94.77 (5.74) 31.58 (4.97) 428.93 (29.04)
Press button top -107.47 (7.09) -42.38 (5.38) -30.81 (6.17) -118.37 (4.93) -106.31 (5.28) -37.23 (7.08) -29.84 (4.17)
Reach -101.75 (8.94) -37.01 (4.93) -48.92 (7.55) -121.94 (5.88) -102.67 (6.24) -38.77 (4.03) -23.47 (4.22)
Press button top w/wall -109.28 (5.93) -45.02 (6.02) -60.39 (7.82) -162.38 (6.85) -94.29 (4.91) -37.39 (5.42) -41.84 (4.38)
Reach with wall 0.27 (0.13) 2.97 (1.02) 30.44 (5.18) 20.36 (4.39) 3.68 (1.81) 23.61 (4.95) 897.42 (39.57)
Insert peg side 10.84 (4.05) 280.17 (25.91) 9.39 (3.17) -12.89 (4.03) 237.51 (18.32) 832.85 (68.03) 12.93 (3.94)
Push -33.48 (5.06) -23.65 (4.09) 7.49 (5.04) -64.58 (6.28) -23.87 (4.02) -72.56 (8.92) 573.94 (28.05)
Push with wall -75.82 (8.96) -42.71 (4.62) -49.07 (7.33) -105.37 (6.94) -76.32 (5.35) -48.38 (6.52) 23.81 (4.13)
Pick & place w/wall -53.14 (5.53) -43.95 (4.29) -55.39 (6.31) -89.96 (7.32) -57.94 (8.56) -47.32 (7.44) -33.27 (5.24)
Press button 7795.45 (602.42) 7882.14 (595.06) 109.27 (13.49) -10.46 (4.03) 28.35 (5.91) 9.86 (5.68) 847.93 (37.26)
Pick & place -92.49 (6.75) -40.29 (5.62) -40.88 (7.32) -118.91 (8.52) -84.74 (6.31) -44.58 (4.74) -23.15 (4.51)
Pull mug -37.03 (5.67) -18.93 (4.67) -50.36 (6.58) -104.29 (7.43) -53.27 (5.46) -34.01 (4.09) -3.03 (3.56)
Unplug peg -82.37 (8.53) -73.28 (6.75) -49.13 (8.33) -116.72 (8.52) -117.64 (5.99) -63.28 (4.76) -52.94 (4.73)
Close window -58.32 (5.93) -32.19 (4.14) -40.92 (5.17) -113.96 (6.46) -54.73 (5.35) -28.94 (4.78) -21.19 (4.22)
Open window -143.54 (6.67) -39.65 (5.46) -57.41 (6.35) -78.67 (6.74) -129.86 (7.46) -33.42 (5.91) -27.36 (5.57)
Open door -83.84 (5.35) -64.36 (6.77) -76.57 (10.27) -153.26 (5.04) -98.69 (6.53) -72.91 (5.86) -62.27 (5.02)
Close door -23.95 (6.43) -35.63 (7.93) -34.93 (6.74) -87.69 (5.64) -88.72 (6.08) -27.83 (5.83) -23.01 (4.98)
Open drawer -62.17 (5.59) -28.95 (6.43) -71.49 (8.37) -91.24 (6.61) -67.62 (6.72) -25.98 (5.79) -60.03 (5.63)
Open box -92.43 (5.89) -105.74 (6.75) -41.05 (6.07) -183.28 (7.62) -171.39 (6.35) -67.84 (5.82) -27.09 (5.31)
Close box -99.31 (7.86) -26.93 (6.74) -66.38 (7.31) -163.62 (9.63) -133.86 (6.42) -28.21 (7.03) -47.15 (6.46)
Lock door -107.16 (10.55) -48.69 (9.93) -53.19 (4.19) -73.25 (5.61) -86.32 (5.69) -47.24 (6.45) -45.39 (4.13)
Unlock door -38.43 (4.99) -44.31 (5.42) -41.48 (4.28) -64.59 (5.67) -57.24 (7.53) -46.29 (5.96) -33.06 (6.62)
Pick bin -82.96 (5.64) -28.93 (4.84) -45.38 (5.46) -107.51 (6.32) -39.18 (4.99) -16.27 (5.47) -42.94 (5.14)

--- PAGE 7 ---
7
TABLE 4: Averaged reward for Atari tasks. The higher the metric, the better performance of the model.
Single AC Single SAC YOLOR-SAC Distral Gradient Surgery Soft Module CAMRL(ours)
YarsRevenge 1208.71 (50.96) 1293.97 (56.52) 982.31 (42.38) 1219.05 (49.64) 1108.92 (52.69) 694.21 (38.01) 1637.94 (42.83)
Jamesbond 7.92 (2.91) 8.12 (2.86) 7.37 (3.02) 7.53 (3.06) 7.51 (2.17) 7.58 (3.42) 8.23 (2.64)
FishingDerby -10.18 (4.03) -11.26 (4.57) -12.33 (4.27) -11.49 (3.98) -10.38 (3.63) -11.56 (4.02) -11.41 (3.71)
Venture 0.02 (0.01) 0.07 (0.02) 0.02 (0.01) 0.04 (0.01) 0.01 (0.01) 0.09 (0.03) 0.22 (0.03)
DoubleDunk -1.39 (0.94) -1.34 (1.02) -1.36 (0.83) -0.96 (0.46) -1.44 (0.58) -1.42 (0.46) -0.89 (0.31)
Kangaroo 6.95 (2.05) 7.21 (3.47) 4.38 (2.12) 6.24 (2.74) 5.21 (2.95) 9.82 (3.84) 5.74 (2.08)
IceHockey -0.46 (0.28) -0.45 (0.31) -0.52 (0.17) -0.51 (0.24) -0.32 (0.26) -0.53 (0.31) -0.49 (0.27)
ChopperCommand 193.91 (19.48) 212.54 (24.95) 217.38 (17.39) 232.54 (22.81) 201.96 (24.04) 242.91 (27.42) 351.68 (29.05)
Krull 12.59 (3.51) 13.24 (4.09) 8.28 (3.27) 8.38 (3.44) 37.82 (8.43) 10.92 (3.49) 9.75 (3.18)
Robotank 0.37 (0.06) 0.37 (0.05) 0.36 (0.04) 0.36 (0.04) 0.35 (0.05) 0.36 (0.05) 0.38 (0.04)
TABLE 5: Averaged success rate for Ravens tasks. The higher the metric, the better performance of the model.
Single AC Single SAC YOLOR-SAC Distral Gradient Surgery Soft Module CAMRL (ours)
Block-insertion 80.92 (3.94) 87.95 (1.63) 88.31 (2.59) 89.91 (3.47) 91.82 (3.41) 89.95 (6.55) 94.26 (5.37)
Place-red-in-green 84.23 (5.05) 90.32 (6.15) 89.94 (2.08) 91.83 (2.16) 89.18 (3.02) 90.14 (2.98) 93.44 (6.57)
Towers-of-hanoi 86.85 (3.93) 88.47 (2.77) 87.61 (2.76) 88.32 (3.38) 92.37 (2.41) 89.73 (3.34) 95.12 (4.64)
Align-box-corner 84.39 (4.49) 86.49 (2.74) 89.01 (2.46) 87.65 (2.84) 90.13 (2.55) 91.27 (2.83) 93.43 (1.35)
Stack-block-pyramid 79.56 (2.55) 78.52 (4.43) 77.03 (1.98) 77.43 (4.01) 78.91 (3.82) 78.24 (4.28) 77.15 (2.41)
Palletizing-boxes 87.25 (2.29) 90.48 (1.98) 91.38 (2.02) 91.78 (2.44) 91.64 (2.27) 94.49 (2.53) 94.32 (2.63)
Assembling-kits 82.26 (6.31) 85.92 (3.31) 88.48 (2.76) 87.02 (3.89) 88.03 (3.47) 92.91 (2.31) 93.95 (4.36)
Packing-boxes 71.55 (2.69) 75.42 (1.97) 78.21 (3.79) 77.94 (4.02) 77.38 (3.08) 79.21 (3.57) 79.38 (3.01)
Manipulating-rope 83.04 (5.42) 87.06 (5.39) 87.28 (3.47) 87.13 (2.94) 84.26 (4.46) 84.02 (3.59) 89.27 (2.14)
Sweeping-piles 85.61 (2.04) 88.93 (3.63) 89.14 (2.71) 90.15 (2.36) 90.69 (2.35) 90.84 (2.31) 93.64 (3.21)
TABLE 6: Averaged success rate for RLBench tasks. The higher the metric, the better performance of the model.
Single AC Single SAC YOLOR-SAC Distral Gradient Surgery Soft Module CAMRL(ours)
Reach Target 96.49 (0.83) 96.41 (0.76) 96.57 (0.63) 95.92 (0.67) 95.08 (0.91) 96.31 (0.42) 99.87 (0.05)
Push Button 86.49 (2.47) 87.41 (2.76) 88.57 (3.04) 87.38 (2.54) 84.48 (3.42) 89.48 (2.35) 97.24 (0.85)
Pick And Lift 86.36 (2.02) 86.49 (2.76) 87.03 (1.84) 85.47 (2.63) 85.31 (2.42) 87.21 (2.43) 91.58 (1.54)
Pick Up Cup 81.47 (3.04) 81.84 (2.71) 82.59 (2.46) 83.02 (2.47) 83.48 (1.59) 84.03 (2.41) 86.48 (2.57)
Put Knife on Chopping Board 45.49 (3.19) 44.54 (3.58) 46.61 (3.59) 46.02 (4.47) 45.58 (3.17) 46.94 (3.41) 50.91 (4.48)
Take Money Out Safe 61.48 (3.18) 60.45 (3.82) 61.41 (3.46) 59.48 (3.77) 57.57 (3.28) 59.43 (3.91) 66.84 (3.56)
Put Money In Safe 75.35 (3.17) 74.35 (3.46) 72.59 (5.57 75.48 (3.51) 73.18 (3.05) 74.13 (3.06) 80.38 (2.51)
Pick Up Umbrella 72.32 (3.31) 73.58 (3.87) 75.03 (2.86) 73.93 (3.01) 76.57 (3.25) 76.43 (2.76) 81.49 (2.95)
Stack Wine 17.49 (1.38) 17.52 (1.75) 20.53 (2.52) 19.57 (3.22) 18.49 (2.63) 17.56 (2.55) 24.59 (2.04)
Slide Block To Target 71.58 (2.59) 73.69 (3.02) 75.57 (2.99) 73.68 (3.57) 71.01 (3.55) 80.17 (3.52) 79.47 (2.57)
‚ÄôPress button‚Äô, ‚ÄôPick & place‚Äô, ‚ÄôPull mug‚Äô, ‚ÄôUnplug peg‚Äô, ‚ÄôClose
window‚Äô, ‚ÄôOpen window‚Äô, ‚ÄôOpen door‚Äô, ‚ÄôClose door‚Äô, ‚ÄôOpen
drawer‚Äô, ‚ÄôOpen box‚Äô, ‚ÄôClose box‚Äô, ‚ÄôLock door‚Äô, ‚ÄôUnlock door‚Äô,
and ‚ÄôPick bin‚Äô, respectively.
Atari: Atari Learning Environment (ALE) was Ô¨Årst pro-
posed in [4] which includes exploration, planning, reactive
play, and complex visual input [12]. In our experiments, we
randomly select 10tasks with visual observations in ALE
to perform experiments, namely, YarsRevenge, Jamesbond,
FishingDerby, Venture, DoubleDunk, Kangaroo, IceHockey,
ChopperCommand, Krull, and Robotank.
Ravens: Ravens is a vision-based robotic manipulation
environment based on PyBullet. We follow [37] to select 10
typical discrete-time tabletop manipulation tasks in Ravens,
namely, Block-insertion, Place-red-in-green, Towers-of-hanoi,
Align-box-corner, Stack-block-pyramid, Palletizing-boxes,
Assembling-kits, Packing-boxes, Manipulating-rope, and
Sweeping-piles. Similar to [37], we generate 1000 expert
demonstrations for each task by the oracle provided by
Ravens and use these demonstrations to perform imitation
learning for each adopted baseline. After the imitation learn-
ing phase, we train each baseline without demonstrations for
another 10000 epochs and compare the average success rate
of each baseline.
RLBench: RLBench is a large-scale environment designedto speed up vision-guided manipulation research. We follow
[21] to test our selected baselines on 10typical RLBench
tasks, i.e., Reach Target, Push Button, Pick And Lift, Pick Up
Cup, Put Knife on Chopping Board, Take Money Out Safe,
Put Money In Safe, Pick Up Umbrella, Stack Wine, and Slide
Block To Target.
4.2 Baselines
To demonstrate the efÔ¨Åciency of the proposed CAMRL
algorithm, we utilize actor critic (AC) [17], soft actor critic
(SAC) [15], mastering rate based online curriculum learning
(MRCL) [30], YOLOR [29], distral [28], gradient surgery [34],
and soft module [32] as our baselines for comparison. The
criteria of choosing baselines depend on whether they are
commonly-used or state-of-the-art methods.
Among the above algorithms, regarding the mastering
rate based online curriculum learning (MRCL), [30] made
the assumption that the good next tasks are those that
are learnable but not learned thoroughly yet, and thus
introduced a new algorithm based on the notion of mastering
rate so as to choose which task to train next in an online
manner. Due to the fact that the rewards of tasks in gym-
minigrid are pretty sparse, we customize a curriculum by
the size of grid for each task, apply MRCL to switch the
sub-tasks in each curriculum from the micro perspective, and

--- PAGE 8 ---
8
/uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni0000001a /uni0000001b /uni0000001c
/uni0000000b/uni00000044/uni0000000c/uni00000003/uni00000030/uni00000037/uni00000014/uni00000013/uni0000001d/uni00000003/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000048/uni00000055/uni00000003/uni00000030/uni00000044/uni00000057/uni00000055/uni0000004c/uni0000005b/uni00000003/uni00000025/uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni0000001a /uni0000001b /uni0000001c/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000015 /uni00000013 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000015
/uni00000013 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000015 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001b/uni00000016 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001b/uni00000016 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001b/uni00000016 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001b/uni00000016
/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001c/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001c/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001c/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000014/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001c/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014
/uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000014/uni00000017 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001b/uni00000015 /uni00000013/uni00000011/uni00000013/uni00000014/uni00000017 /uni00000013/uni00000011/uni00000013/uni00000014/uni00000017 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014
/uni00000013/uni00000011/uni00000013/uni00000015/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000014/uni00000017 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000014/uni00000017 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000015/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000014/uni00000017
/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000015 /uni00000013 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000015 /uni00000013 /uni00000013 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000015 /uni00000013
/uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013 /uni00000013 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014
/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000015 /uni00000013 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000015 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000014/uni00000015 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni00000016/uni00000019 /uni00000013
/uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013
/uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001b/uni00000015 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001b/uni00000015 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001b/uni00000015 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000014 /uni00000013/uni00000011/uni00000013/uni00000013/uni0000001b/uni00000015 /uni00000013
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000011/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni0000001a/uni00000018/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni00000015/uni00000018/uni00000013/uni00000011/uni00000013/uni00000014/uni00000018/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni0000001a/uni00000018/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000013
/uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni0000001a /uni0000001b /uni0000001c
/uni0000000b/uni00000045/uni0000000c/uni00000003/uni00000030/uni00000037/uni00000014/uni00000013/uni0000001d/uni00000003/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004e/uni00000003/uni00000052/uni00000049/uni00000003/uni00000030/uni00000058/uni00000057/uni00000058/uni00000044/uni0000004f/uni00000003/uni00000037/uni00000048/uni00000056/uni00000057/uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni0000001a /uni0000001b /uni0000001c/uni00000015 /uni0000001a /uni0000001a /uni00000014 /uni00000018 /uni0000001c /uni00000017 /uni00000018 /uni00000018 /uni00000017
/uni00000018 /uni00000017 /uni00000017 /uni0000001a /uni00000016 /uni00000019 /uni00000016 /uni00000016 /uni00000016 /uni00000016
/uni00000016 /uni00000014 /uni00000018 /uni00000017 /uni00000015 /uni00000015 /uni00000015 /uni00000015 /uni00000015 /uni00000015
/uni0000001b /uni00000016 /uni00000014 /uni00000018 /uni00000017 /uni00000014 /uni00000019 /uni00000017 /uni00000019 /uni0000001b
/uni00000019 /uni00000019 /uni0000001c /uni0000001c /uni00000019 /uni00000016 /uni0000001b /uni0000001b /uni00000017 /uni0000001a
/uni00000014 /uni00000015 /uni00000015 /uni00000016 /uni00000014 /uni00000018 /uni00000014 /uni00000014 /uni00000014 /uni00000014
/uni0000001a /uni0000001c /uni00000019 /uni00000019 /uni0000001c /uni0000001a /uni0000001a /uni0000001a /uni0000001b /uni00000019
/uni0000001c /uni0000001b /uni0000001b /uni00000014/uni00000013 /uni0000001b /uni00000017 /uni00000014/uni00000013 /uni00000019 /uni0000001c /uni00000014/uni00000013
/uni00000017 /uni00000018 /uni00000016 /uni00000015 /uni0000001a /uni0000001b /uni00000018 /uni0000001c /uni0000001a /uni00000018
/uni00000014/uni00000013 /uni00000014/uni00000013 /uni00000014/uni00000013 /uni0000001b /uni00000014/uni00000013 /uni00000014/uni00000013 /uni0000001c /uni00000014/uni00000013 /uni00000014/uni00000013 /uni0000001c
/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013
/uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni0000001a /uni0000001b /uni0000001c
/uni0000000b/uni00000046/uni0000000c/uni00000003/uni00000030/uni00000037/uni00000018/uni00000013/uni0000001d/uni00000003/uni00000037/uni00000055/uni00000044/uni00000051/uni00000056/uni00000049/uni00000048/uni00000055/uni00000003/uni00000030/uni00000044/uni00000057/uni00000055/uni0000004c/uni0000005b/uni00000003/uni00000025/uni00000003/uni0000000b/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000029/uni0000004c/uni00000055/uni00000056/uni00000057/uni00000003/uni00000014/uni00000013/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000056/uni0000000c/uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni0000001a /uni0000001b /uni0000001c/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000018
/uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013
/uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013
/uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013
/uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013
/uni00000013 /uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013 /uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000018 /uni00000013/uni00000011/uni00000013/uni00000018
/uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013
/uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013
/uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013
/uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013 /uni00000013
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000014/uni00000013/uni00000011/uni00000013/uni00000015/uni00000013/uni00000011/uni00000013/uni00000016/uni00000013/uni00000011/uni00000013/uni00000017
/uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni0000001a /uni0000001b /uni0000001c
/uni0000000b/uni00000047/uni0000000c/uni00000003/uni00000030/uni00000037/uni00000018/uni00000013/uni0000001d/uni00000003/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004e/uni00000003/uni00000052/uni00000049/uni00000003/uni00000030/uni00000058/uni00000057/uni00000058/uni00000044/uni0000004f
/uni00000037/uni00000048/uni00000056/uni00000057/uni00000003/uni0000000b/uni00000057/uni0000004b/uni00000048/uni00000003/uni00000029/uni0000004c/uni00000055/uni00000056/uni00000057/uni00000003/uni00000014/uni00000013/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000056/uni0000000c/uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018 /uni00000019 /uni0000001a /uni0000001b /uni0000001c/uni0000001b /uni00000017 /uni00000019 /uni00000018 /uni0000001c /uni00000019 /uni0000001b /uni00000015 /uni00000014/uni00000013 /uni0000001c
/uni00000014 /uni00000014 /uni00000014 /uni00000014 /uni00000014 /uni00000016 /uni00000014 /uni0000001c /uni00000014 /uni00000014
/uni00000016 /uni00000016 /uni00000016 /uni00000016 /uni00000016 /uni00000017 /uni00000016 /uni0000001a /uni00000016 /uni00000016
/uni00000017 /uni00000019 /uni00000018 /uni00000019 /uni00000017 /uni00000014 /uni0000001a /uni00000014 /uni00000018 /uni00000017
/uni00000018 /uni0000001a /uni0000001a /uni00000017 /uni00000018 /uni00000018 /uni00000017 /uni00000018 /uni00000017 /uni00000018
/uni00000014/uni00000013 /uni00000018 /uni00000017 /uni0000001b /uni00000014/uni00000013 /uni0000001c /uni00000014/uni00000013 /uni00000016 /uni0000001b /uni00000014/uni00000013
/uni00000015 /uni00000015 /uni00000015 /uni00000015 /uni00000015 /uni00000015 /uni00000015 /uni0000001b /uni00000015 /uni00000015
/uni0000001a /uni00000014/uni00000013 /uni00000014/uni00000013 /uni00000014/uni00000013 /uni00000019 /uni00000014/uni00000013 /uni00000019 /uni00000014/uni00000013 /uni0000001a /uni0000001a
/uni0000001c /uni0000001b /uni0000001c /uni0000001a /uni0000001b /uni0000001b /uni0000001c /uni00000019 /uni0000001c /uni0000001b
/uni00000019 /uni0000001c /uni0000001b /uni0000001c /uni0000001a /uni0000001a /uni00000018 /uni00000017 /uni00000019 /uni00000019
/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c/uni00000014/uni00000013
Fig. 3: Comparisons of B ITTand the performance rank matrix of mutual evaluation across tasks: (a) MT10: B I1010;
(b) MT10:PR of Mutual Test; (c) MT50: B I1010(the First 10 Tasks); (d) MT50: PR of Mutual Test (the First 10 Tasks).
utilize other baselines to perform multi-task or single-task
training for all the tasks in the macro aspect.
4.3 Evaluation Metric
We adopt the reward metric in the Gym-minigrid, meta-
world, and ALE environments and the success rate metric
in the Ravens and RLBench environments. Although with
regard to the meta-world environments, the success rate
has been utilized in [32], [34], this evaluation metric can
be zero very often, which leads to sparse feedback and
is bad for comparing the difÔ¨Åculties of training different
tasks. Besides, [32], [34] displayed performance of their
algorithms with the averaged success rate of all tasks instead
of individual success rates for each task, which makes the
success rate less informative than the reward metric for each
task that we adopt in this paper. Last but not the least, the
simulation steps in [34], the number of samples in [32], and
the number of epochs in this paper are not in the same
magnitude, which makes our reference for the success rate
in [32], [34] less meaningful. As a result, in the meta-world
environment, we use reward instead of the averaged success
rate to formulate the evaluation metric, the policy loss in
Imul, and the composite loss terms.
4.4 Implementation Details
We adopt the same network structure for each single actor
network and single critic network in each baseline. For soft
module and gradient surgery, we follow the same parameters
as in their public codes. For distral, we follow [28] to set
= 0:5and use a set of 9hyper-parameters (1=;)2310 4;10 3;310 3	210 4;410 4;810 4	
for
the entropy costs and the initial learning rate. Hyper-
parameters with the optimal result are utilized with regard
to the distral algorithm.
Concerning our CAMRL, we set a= 1000;b=1
40;c=
2;d= 200;1=2= 0:01, andradius = 0:05in all tasks.
At the end of every epoch, instead of performing mutual test
for each pair of tasks which is computationally-inefÔ¨Åcient, we
1) randomly select two tasks iandj; 2) test the performance
ofSACion taskjand the performance of SACjon taski;
3) updatepi;jandpj;i; and 4) repeat the above mutual test
procedure for 3times. Besides, to avoid the abrupt increase
in loss when switching to the curriculum-based AMTL mode,
the actual loss for task tis set to beL(wt)plus 0.01 times theobjective in Eq. (1). In addition, the proposed algorithms are
run over 5seeds to report the averaged results.
4.5 Results
Results on gym-minigrid. As shown in Table 1, among the
cooperation between mastering rate based online curriculum
learning (MRCL) [30] and the single-task actor critic/distral/-
gradient surgery/CAMRL algorithm, CAMRL achieves the
best overall performance over the 9minigrid tasks, without
showing obvious signs of negative transfer.
Results on MT10 and MT50. As can be seen in Table
2, CAMRL signiÔ¨Åcantly beats all baselines over 6tasks out
of the 10tasks in MT10. The second-best algorithm overall
is single SAC and is obviously worse than CAMRL in 7
tasks out of all 10tasks. In Table 3, the best and second-best
algorithms overall are CAMRL and soft module, respectively.
The latter is at least 20% worse than CAMRL in 32tasks out
of the total 50tasks and at least 20% better than CAMRL in 8
tasks out of all tasks. To sum up, in existing experiments, our
CAMRL works well whether the task scale is large or small.
Moreover, it seldom exposes serious negative transfer, and
can sometimes contribute greatly to the tasks that are hard
to train for other baselines.
Results on Atari, Ravens, and RLBench. See the perfor-
mance of all algorithms on the selected Atari, Ravens, and
RLBench tasks in Tables 4, 5, and 6. From Table 4, 5, and 6,
we can see that our CAMRL algorithm ranks Ô¨Årst in 6tasks
out of the total selected 10Atari tasks, 8tasks out of the total
selected 10Ravens tasks, and 9tasks out of the total selected
10RLBench tasks. No signiÔ¨Åcant negative transfer is found
when performing CAMRL in all these tasks.
Analysis on the transfer matrix B.As stated in Section 3,
Bis aTTasymmetric matrix representing the number of
transfers between tasks. Figure 3(a) visualizes the B I1010
matrix where I1010is the identity matrix in R1010, and
Figure 3(c) visualizes the B I1010matrix of the Ô¨Årst 10
tasks in MT50.
To discover some in-depth rules related to B, we calculate
PM2RTT, the performance matrix of mutual evaluation
across tasks, and compare its variant PR, a performance rank
matrix, with B I1010: First, test the average performance
of the single SAC net trained for task sover 10kepisodes
on taskt, and regard it as PMs;t(s;t2[T]) (the evaluation
is run for 20episodes); next, for each task t2[T], rank
PMs;t(s2[T])by the order from minimum to maximum

--- PAGE 9 ---
9
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a /uni0000000b/uni0000004c/uni0000000c/uni00000012/uni00000016
/uni0000000b/uni0000004c/uni0000004c/uni0000000c/uni00000012/uni00000016
/uni0000000b/uni0000004c/uni0000004c/uni0000004c/uni0000000c/uni00000012/uni00000016
Imul
Fig. 4: Changes of terms in Imulfor MT10.
and regard the rank as the t-th column of the performance
rank matrix PR2RTT. The visualization of PR for MT10
is shown in Figure 3(b), and Figure 3(d) displays the PR
for the Ô¨Årst 10tasks out of the 50tasks in MT50. The bigger
the number in PRs;t, the better evaluation performance that
the SAC network originally trained for task shas on task
t. As shown in Figure 3, when PRs;tis small enough, say
no larger than 2(see Figure 3 (b)(d)), Bs;t ITT
s;t is also
obviously smaller than other elements (see Figure 3 (a)(c)).
This, to some extent from the opposite side, conÔ¨Årms our
previous demand that the better evaluation performance on
other tasks, the larger number of transfers on those tasks.
Incorporation of prior knowledge. To show the capa-
bility of incorporating prior knowledge for CAMRL, we
perform extra experiments that formulate a new differen-
tiable ranking loss for tasks where the relative magnitudes
of the task difÔ¨Åculty are readily apparent in part. SpeciÔ¨Åcally,
we follow [36], [36], [26], [37], and [21] to obtain the public
performance of existing state-of-the-arts methods for MT10,
MT50, Atari, Ravens, and RLBench, respectively. Then we use
the relative ranking of the public performance to formulate a
newtanh -based differentiable ranking loss and incorporate
it with Eq. (1), in the hope that for task t2[T], if taskiis
easier to train, that is, with better public performance, then
taskttends to perform a smaller number of transfers to task
i. The results of adding this new differentiable ranking loss
term are shown in Tables 7, 8, 9, 10, and 11. We can see that
after adding the new loss term, the performance of CAMRL
beats its original version over 5tasks out of the 10tasks in
MT10, 38tasks out of the 50tasks in MT50, 8tasks out of the
10tasks in Atari, 7tasks out of the 10tasks in Ravens, and
9tasks out of the 10tasks in RLBench, which demonstrates
CAMRL‚Äôs capability of incorporating prior knowledge as
well as the great performance of the differentiable ranking
loss in CAMRL.
Changes of each term in Imul.To demonstrate the
switching process of CAMRL‚Äôs training mode and visualize
the magnitude change of each term in Imul, we plot the
changes ofImuland its three terms for CAMRL in the MT10
environment. From Figure 4 we can see that the magnitude
of each term in Imuldoes not differ much and Imulkeeps
ranging from 0:2to0:8, which does not show abnormal
Ô¨Çuctuation and magnitude much.
Hyper-parameter analysis. The results of the hyper-
parameter analysis for our CAMRL on MT10 are shown
in Figure 8. We Ô¨Årst set 0= 1;1=2=1=2=
3=4= 0:01;a= 1000;b=1
40;c= 2;d= 200 , andradius = 0:05. And the evaluation metric is deÔ¨Åned as the
average reward of all 10tasks over the Ô¨Årst 10kepisodes by
the CAMRL algorithm. Then, we
(i)Ô¨Åx2,1,2,3,4,a,b,c,d, andradius . Set parame-
ter1= 0;0:001;0:002;0:005;0:01;0:02;0:05;0:1;0:2;0:5;1,
respectively. The results under different values of 1are
shown in Figure 8 (line 1);
(ii)repeat the same procedure for i(i2[4]),a,1
b,c,d,
and2as that for1.
From Figure 8(a) we can see that, as for i(i2[2])
andi(i2[4]), when all of these hyper-parameters are
around 0:01, decreasing one parameter among them to 0
may hurt the performance. In the meantime, if one of the
parameters is increased too much, the great performance of
CAMRL may not be guaranteed, which may result from
the abrupt change of loss when switching the training
mode and from the exceeded negative transfer. In summary,
according to the above experimental evaluation, as for the
time-invariant design of i(i2[4]), a reasonable setting, e.g.,
i0:01;j0:01fori2[4];j2[2], is indeed helpful in
improving SAC‚Äôs performance, compared with the scenarios
that some hyper-parameters are set to zeros. Furthermore, as
shown in Figure 8, when we automatically adjust i(i2[4])
based on each term‚Äôs uncertainty, the performance of CAMRL
outweighs most conÔ¨Ågurations of i(i2[4]).
With regard to hyper-parameters a,1
b,c, andd, as can
be seen in Figure 8(b), our CAMRL is the most sensitive
tod, the co-efÔ¨Åcient of the inner term in the tanh ranking
function, which strongly demonstrates the effectiveness of
our customized differentiable ranking mechanism. Moreover,
increasing a,1
b, andcto an appropriate value is also
beneÔ¨Åcial to CAMRL‚Äôs performance and does not show
a signiÔ¨Åcant sign of performance deterioration due to the
magnitude problem of terms in the index Imul.
Remark. In fact, due to our elaborate design, each term in Imul
normally will not exceed1
3. And if one of the terms becomes very
small and fails to work out in the index, the other terms will still
function so that CAMRL‚Äôs performance will not deteriorate too
quickly when performing a hyper-parameter analysis on a,b, and
c.
Ablation study. For the ablation study, on tasks MT10, we
test our method in the lack of the mode switching component
(i.e., entirely performing curriculum learning), and when
setting1
a=b=c=d=i(i= 2;3;4)to be 0, respectively. Also, we
test the performance of AMRL which lacks the mode switch-
ing mechanism and multiple ranking functions compared
with our CAMRL. As shown in Table 12, the lack of any com-
ponent of our CAMRL and setting1
a=b=c=d=i(i= 2;3;4)
to be 0or positive inÔ¨Ånity (so that the corresponding
term inImul is0) bring varying degrees of reduction in
algorithm performance, which consistently demonstrates
the precision of algorithm design and the importance of
tacit cooperation among different components. The obvious
performance deterioration when only performing AMRL and
when lacking the mode switching component speciÔ¨Åcally
shows the effectiveness and efÔ¨Åciency of our design of
the indexImul and the customized differentiable ranking
functions.

--- PAGE 10 ---
10
5 D ISCUSSION
In this section, we discuss the advantages of CAMRL over
existing MTRL algorithms, some clariÔ¨Åcations of CAMRL,
and direct ways to improve CAMRL.
5.1 Advantages over Existing MTRL Algorithms
When there exist many tasks, it is difÔ¨Åcult to Ô¨Ånd common
traits owned by all tasks, and thus, the distillation-like
algorithms may perform poorly. On the contrary, except
for distilling common traits of all tasks by a large network,
ourBmatrix probes the transfer relationship between every
two tasks and enables us to perform asymmetric transfer
between tasks so that CAMRL will not be troubled by this
issue.
For multi-task learning with a modular paradigm, nega-
tive transfer can be serious since the task relationship in
the early period is incorrect and the bad inÔ¨Çuence may
accumulate. Moreover, it may take a long time to approx-
imately capture the correct task relationship and by the
time the relationship is well learned, while the effect caused
by negative transfer might have already been irreversible.
On the contrary, by initializing Bas a unit matrix (no
transfer at the beginning), adding constraints to B, and
switching between various training modes, CAMRL can not
only mitigate negative transfer, but also learn a good Bat a
relatively early stage, allowing the training on difÔ¨Åcult tasks
to be positively affected by other tasks.
We summarize the advantages of CAMRL as Ô¨Åve-fold.
First, CAMRL is Ô¨Çexible: (i)CAMRL can freely switch
between parallel single-task training and curriculum-based
AMTL modes according to the indicator related to the
learning progress. (ii) The sub-components of CAMRL
have variants and can be paired with different RL baseline
algorithms and training modes. CAMRL does not impose
restrictions on the network architecture and learning mode.
For example, Wcould be parameters of the whole network.
When there is a large number of tasks, we could share the
principle part of the whole network and let Wbe only a small
subset of the network in order to save RAM. Second, CAMRL
is promising in mitigating negative transfer. For one thing,
by considering the 1-norm constraint on Band applying
Frank-Wolfe to meet the constraint, CAMRL can avoid
excessive transfer to some extent. For another, by customizing
and utilizing indicators about the learning progress and
the performance of mutual tests across tasks, CAMRL can
alleviate negative transfer. Third, the loss function of CAMRL
can take information from multiple aspects to improve
the efÔ¨Åciency of RL tasks‚Äô training. SpeciÔ¨Åcally, the loss
function in Eq. (4)considers the positivity and sparsity of
the transfer matrix, the training difÔ¨Åculty for different tasks,
the performance of mutual tests, and the similarity between
every two tasks. Thanks to our customized ranking loss
which enables to absorb partial ranking information, CAMRL
could take full advantage of various existing prior knowledge
and training factors, regardless of their amount. Fourth,
CAMRL eliminates the need for laborious hyper-parameter
analysis. SpeciÔ¨Åcally, CAMRL allows the hyper-parameters
in the composite loss to dynamically auto-adjust and regards
the auto-adjusting process as a multi-task learning problem.
An uncertainty-based multi-task learning method is utilizedto update the hyper-parameters automatically. Fifth, CAMRL
can be auto-adapted to a new task without affecting original
tasks by changing BtoB0= [B;(0;:::; 0)>; (0;:::; 0);1].
5.2 Some ClariÔ¨Åcations
Computation amount of mutual evaluation. If the mutual
evaluation between tasks requires a big amount of compu-
tation, we could reduce the frequency of mutual evaluation
between tasks and reuse the evaluation results between each
evaluation. Besides, in each epoch, we could just randomly
select a few tasks for mutual evaluation and only conduct
partial ranking with regard to the ranking loss.
Complexity of the loss term. Although our customized
loss contains multiple terms which might lead to strenuous
hyper-parameter tuning, we adapt the automatic hyper-
parameter adjustment technology in [16] to handle the above
issue. Moreover, in the future, it might be valuable to discover
other factors that make a more signiÔ¨Åcant difference to
CAMRL and simplify the loss term by replacing the existing
factors in the loss with the most remarkable factors.
Performances of the trade-off mechanism among differ-
entiable loss functions. Although we adopt differentiable
loss functions in order to keep the ranking alignment
betweenBt;i(i2[T])andpt;i(orL(wi)), there are multiple
terms in the loss and thus we have to make a compromise
between different terms. As a result, it is difÔ¨Åcult to achieve
perfect alignment concerning each ranking loss, and to
mitigate negative transfer we mainly hope that Bt;ican
be small when pt;iandL(wi)are small enough. Actually,
as shown in Figure 3, when PRs;tis small enough, say no
larger than 2,Bs;t ITT
s;t is also obviously smaller than
other elements, which meets our expectation for relieving
serious negative transfer.
Details of integrating MRCL with the baselines. Since
the tasks we select from Gym-minigrid are of the biggest size
or complexity, directly overcoming these tasks would be very
difÔ¨Åcult. Therefore, to lower the barrier of tasks‚Äô learning, we
follow the mastering rate based online curriculum learning
(MRCL) algorithm [30] to perform curriculum learning for
each selected task. SpeciÔ¨Åcally, for each selected task i, we
take taskiand other easier tasks in Gym-minigrid with the
same property as task ias the curriculum of task i. During
the training phase of each task i, we adopt MRCL to learn
the curriculum of task ias quickly as possible, with the
learning algorithm set to be our selected baseline. At each
training step, MRCL adaptively adjusts the next task in the
curriculum of task ifor training according to the dynamic
attention for each task, under the assumption that the good
next tasks are the ones which are learnable but not learned
yet. Thanks to the delicate attention mechanism of MRCL, the
learning algorithm could overcome the selected tasks in Gym-
minigrid little by little instead of being stuck somewhere for
a long time.
ConÔ¨Åguration method of hyper-parameters. (i) For
hyper-parameters a;b, andc, we adjust them according
to the magnitude of the variables in each item of the
indexImul, with the hope that the composite magnitude
of each item in Imul does not differ too far. (ii) For the
hyper-parameter d, we plot various ranking functions with
differentdand selectdwith a relatively smooth ranking

--- PAGE 11 ---
11
function as well as being as small as possible, since a small
dcould contribute to the convergence speed of vanilla
Frank-Wolfe during the optimization of bo
t. (iii) For hyper-
parameters i(i2f0;1;2;3;4g), we automatically adjust
them according to an uncertainty-based multi-task learning
method in [16].
5.3 Potential Improvements
Below we list several directions for CAMRL to be improved,
so as to illuminate further incorporation of curriculum-based
AMTL into RL.
Consider more auto-tuning approaches of hyper-
parameters j(j= 1;2)andj(j= 1;2;3;4), as
well as allowing jto be the time-varying function of
the learning progress and other factors.
Enrich the loss by integrating the intermediate difÔ¨Å-
culty, diversity, surprise and energy [25], etc.
Design techniques to select the optimal subset of all
the networks‚Äô parameters as W.
TransformBinto a non-linear function and update it
with theoretical guidance.
Apply the sliding window, the exponential dis-
counted factor, the appropriate entropy, or other tricks
to mitigate the non-stationarity during the AMTL
training.
6 C ONCLUSIONS AND FUTURE WORK
In this paper, we proposed a novel multi-task reinforce-
ment learning algorithm, called CAMRL (curriculum-based
asymmetric multi-task reinforcement learning). CAMRL
switches the training mode between parallel single-task and
curriculum-based AMTL. CAMRL employs a composite loss
function to reduce negative transfer in multi-task learning.
Apart from regularizing tasks‚Äô outgoing transfer and net-
work weights‚Äô similarity, we introduced three differentiable
ranking functions into the loss to incorporate various prior
knowledge Ô¨Çexibly. An alternating optimization with Frank-
Wolfe has also been utilized to optimize the loss, and
an uncertainty-based automatic adjustment mechanism of
hyper-parameters has been adopted to eliminate laborious
hyper-parameter analysis. We have conducted extensive
experiments to conÔ¨Årm the effectiveness of CAMRL and
have analyzed its Ô¨Çexibility from various perspectives.
In the future, we plan to study CAMRL with more
theoretical insights and consider its potential improvements
listed in the Discussion Section, such as incorporating more
prior knowledge into the loss, designing the non-linear
version of the transfer matrix, and overcoming the non-
stationarity during the AMTL training.
REFERENCES
[1] Constopt-pytorch: a library for constrained optimization built on
pytorch, 2020. https://github.com/GeoffNN/constopt-pytorch.
[2] M. Asada, S. Noda, S. Tawaratsumida, and K. Hosoda. Purposive
behavior acquisition for a real robot by vision-based reinforcement
learning. Machine learning , 23(2-3):279‚Äì303, 1996.
[3] T. Bansal, J. Pachocki, S. Sidor, I. Sutskever, and I. Mor-
datch. Emergent complexity via multi-agent competition. CoRR ,
abs/1710.03748:1‚Äì12, 2017.[4] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade
learning environment: An evaluation platform for general agents.
Journal of ArtiÔ¨Åcial Intelligence Research , 47:253‚Äì279, 2013.
[5] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum
learning. In Proceedings of the 26th annual international conference on
machine learning , pages 41‚Äì48, 2009.
[6] E. Brunskill and L. Li. Sample complexity of multi-task reinforce-
ment learning. arXiv preprint arXiv:1309.6821 , 2013.
[7] Z. Chen, V . Badrinarayanan, C.-Y. Lee, and A. Rabinovich. Grad-
norm: Gradient normalization for adaptive loss balancing in deep
multitask networks. In International Conference on Machine Learning ,
pages 794‚Äì803. PMLR, 2018.
[8] M. Chevalier-Boisvert, D. Bahdanau, et al. Babyai: A platform to
study the sample efÔ¨Åciency of grounded language learning. In
International Conference on Learning Representations , 2018.
[9] C. Devin, A. Gupta, et al. Learning modular neural network
policies for multi-task and multi-robot transfer. In IEEE International
Conference on Robotics and Automation (ICRA) , 2017.
[10] A. Elgharabawy. Preference neural network. arXiv , 2019.
[11] J. L. Elman. Learning and development in neural networks: The
importance of starting small. Cognition , 48(1):71‚Äì99, 1993.
[12] L. Espeholt, H. Soyer, et al. Impala: Scalable distributed deep-rl with
importance weighted actor-learner architectures. In International
Conference on Machine Learning , pages 1407‚Äì1416. PMLR, 2018.
[13] R. M. Freund and P . Grigas. New analysis and results for the
frank‚Äìwolfe method. Mathematical Programming , 2016.
[14] P . Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye. A general iterative
shrinkage and thresholding algorithm for non-convex regularized
optimization problems. In international conference on machine
learning , pages 37‚Äì45, 2013.
[15] T. Haarnoja, A. Zhou, P . Abbeel, and S. Levine. Soft actor-critic:
Off-policy maximum entropy deep reinforcement learning with a
stochastic actor. In Proceedings of the 35th International Conference
on Machine Learning , volume 80 of Proceedings of Machine Learning
Research , pages 1856‚Äì1865. PMLR, 2018.
[16] A. Kendall, Y. Gal, and R. Cipolla. Multi-task learning using
uncertainty to weigh losses for scene geometry and semantics.
InProceedings of the IEEE conference on computer vision and pattern
recognition , pages 7482‚Äì7491, 2018.
[17] V . R. Konda and J. N. Tsitsiklis. Actor-critic algorithms. In Advances
in neural information processing systems , pages 1008‚Äì1014, 2000.
[18] S. Lacoste-Julien. Convergence rate of frank-wolfe for non-convex
objectives. CoRR , abs/1607.00345, 2016.
[19] G. Lee, E. Yang, and S. Hwang. Asymmetric multi-task learning
based on task relatedness and loss. In International Conference on
Machine Learning , pages 230‚Äì238, 2016.
[20] H. B. Lee, E. Yang, and S. J. Hwang. Deep asymmetric multi-task
feature learning. In International Conference on Machine Learning ,
pages 2956‚Äì2964. PMLR, 2018.
[21] S. Liu, S. James, A. J. Davison, and E. Johns. Auto-lambda:
Disentangling dynamic task relationships. arXiv:2202.03091 , 2022.
[22] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu.
Towards deep learning models resistant to adversarial attacks.
arXiv preprint arXiv:1706.06083 , pages 1‚Äì28, 2017.
[23] Y. E. Nesterov. Complexity bounds for primal-dual methods
minimizing the model of objective function. Math. Program. , 171(1-
2):311‚Äì330, 2018.
[24] A. Pentina, V . Sharmanska, and C. H. Lampert. Curriculum learning
of multiple tasks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , pages 5492‚Äì5500, 2015.
[25] R. Portelas, C. Colas, L. Weng, K. Hofmann, and P . Oudeyer.
Automatic curriculum learning for deep RL: A short survey. In
Proceedings of the Twenty-Ninth International Joint Conference on
ArtiÔ¨Åcial Intelligence, IJCAI 2020 , pages 4819‚Äì4825. ijcai.org, 2020.
[26] C. Shenton. Atari reinforcement learning leaderboard, 2018. https:
//github.com/cshenton/atari-leaderboard.
[27] A. Taylor, I. Dusparic, M. Gu√©riau, and S. Clarke. Parallel transfer
learning in multi-agent systems: What, when and how to transfer?
InInternational Joint Conference on Neural Networks, IJCNN 2019
Budapest, Hungary, July 14-19, 2019 , pages 1‚Äì8. IEEE, 2019.
[28] Y. Teh, V . Bapst, W. M. Czarnecki, J. Quan, J. Kirkpatrick, R. Hadsell,
N. Heess, and R. Pascanu. Distral: Robust multitask reinforcement
learning. In Advances in Neural Information Processing Systems , pages
4496‚Äì4506, 2017.
[29] C.-Y. Wang, I.-H. Yeh, and H.-Y. M. Liao. You only learn one
representation: UniÔ¨Åed network for multiple tasks. arXiv preprint
arXiv:2105.04206 , 2021.

--- PAGE 12 ---
12
[30] L. Willems, S. Lahlou, and Y. Bengio. Mastering rate based
curriculum learning. CoRR , abs/2008.06456:1‚Äì15, 2020.
[31] Y. Wu and Y. Tian. Training agent for Ô¨Årst-person shooter game
with actor-critic curriculum learning. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings , 2017.
[32] R. Yang, H. Xu, Y. Wu, and X. Wang. Multi-task reinforcement
learning with soft modularization. In NeurIPS , 2020.
[33] D. Ye, G. Chen, W. Zhang, S. Chen, B. Yuan, B. Liu, J. Chen, Z. Liu,
F. Qiu, H. Yu, Y. Yin, B. Shi, L. Wang, T. Shi, Q. Fu, W. Yang,
L. Huang, and W. Liu. Towards playing full moba games with deep
reinforcement learning. In Advances in Neural Information Processing
Systems , volume 33, pages 621‚Äì632, 2020.
[34] T. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn.
Gradient surgery for multi-task learning. In NeurIPS 2020 , 2020.
[35] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and
S. Levine. Meta-world: A benchmark and evaluation for multi-task
and meta reinforcement learning. In Conference on Robot Learning
(CoRL) , pages 1‚Äì18, 2019.
[36] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and
S. Levine. Meta-world: A benchmark and evaluation for multi-task
and meta reinforcement learning. In Conference on Robot Learning ,
pages 1094‚Äì1100. PMLR, 2020.
[37] A. Zeng, P . Florence, J. Tompson, S. Welker, J. Chien, M. Attarian,
T. Armstrong, I. Krasin, D. Duong, V . Sindhwani, et al. Transporter
networks: Rearranging the visual world for robotic manipulation.
arXiv preprint arXiv:2010.14406 , 2020.
[38] Y. Zhang and Q. Yang. A survey on multi-task learning. CoRR ,
abs/1707.08114:1, 2017.
[39] Y. Zhang and D.-Y. Yeung. A regularization approach to learning
task relationships in multitask learning. ACM Transactions on
Knowledge Discovery from Data (TKDD) , 8(3):1‚Äì31, 2014.
Hanchi Huang received her bachelor‚Äôs degree
from Shanghai Jiao Tong University in 2021. She
is now a graduate student in the School of Com-
puter Science and Engineering, Nanyang Tech-
nological University, Singapore. Her research
interests include theory and algorithms for re-
inforcement learning, combinatorial optimization,
and recommendation system.
Deheng Ye Ô¨Ånished his Ph.D. from the School
of Computer Science and Engineering, Nanyang
Technological University, Singapore, in 2016. He
is now a Principal Researcher and Team Manager
with Tencent, Shenzhen, China, where he leads
a group of engineers and researchers develop-
ing large-scale learning platforms and intelligent
AI agents. He is interested in applied machine
learning, reinforcement learning, and software
engineering. He has been serving as a PC/SPC
for NeurIPS, ICML, ICLR, AAAI, and IJCAI.
Li Shen received his Ph.D. from the School of
Mathematics, South China University of Technol-
ogy, in 2017. He is now a Research Scientist at
JD Explore Academy, China. Previously, he was
a Research Scientist at Tencent, China. His re-
search interests include theory and algorithms for
large scale convex/nonconvex/minimax optimiza-
tion problems, and their applications in statistical
machine learning, deep learning, reinforcement
learning, and game theory.
Wei Liu (M‚Äô14-SM‚Äô19) is currently a Distin-
guished Scientist of Tencent and the Director of
Ads Multimedia AI at Tencent Data Platform. Prior
to that, he has been a research staff member of
IBM T. J. Watson Research Center, USA. Dr. Liu
has long been devoted to fundamental research
and technology development in core Ô¨Åelds of
AI, including deep learning, machine learning,
computer vision, pattern recognition, information
retrieval, big data, etc. To date, he has published
extensively in these Ô¨Åelds with more than 270
peer-reviewed technical papers, and also issued 23 US patents. He
currently serves on the editorial boards of IEEE TPAMI, TNNLS, IEEE
Intelligent Systems, and Transactions on Machine Learning Research.
He is an Area Chair of top-tier computer science and AI conferences, e.g.,
NeurIPS, ICML, IEEE CVPR, IEEE ICCV, IJCAI, and AAAI. Dr. Liu is a
Fellow of the IAPR, AAIA, IMA, RSA, and BCS, and an Elected Member
of the ISI.

--- PAGE 13 ---
13
7 A PPENDIX
7.1 Architecture
For tasks without visual observations, we directly adopt the
deep neural network as the actor network as well as the
critic network. Below is the architecture of the deep neural
network.
# The architecture of the actor network:
nn.Sequential(
nn.Linear(n_o, 128),
nn.ReLU(),
nn.Linear(128, 64),
nn.ReLU(),
nn.Linear(64, 32),
nn.ReLU(),
nn.Linear(32, 16),
nn.ReLU(),
nn.Linear(16, n_a)
)
# The architecture of the critic network:
nn.Sequential(
nn.Linear(n_o, 128),
nn.ReLU(),
nn.Linear(128, 64),
nn.ReLU(),
nn.Linear(64, 32),
nn.ReLU(),
nn.Linear(32, 16),
nn.ReLU(),
nn.Linear(16, 1)
)
wherenois the dimension of the observation and na
represents the number of the optional actions for tasks with
discrete action space or the dimension of the action for tasks
with continuous action space.
For tasks with visual observations, we perform two con-
volutional layers, self.conv1 and self.conv2, on the original
observation, Ô¨Çatten the output by self.conv2, and then obtain
the reshaped observation. After this, we perform the same
operation as tasks without visual observations.
# in_channel, out_channel, kennel_size, stride
self.conv1 = nn.Conv2d(3, 1, (3, 4), (12,16))
self.conv2 = nn.Conv2d(1, 1, (3, 4), (3,4))
7.2 Results visualization
We plot the performance of our selected baselines for gym-
minigrid, MT10, MT50, Atari, Ravens, and RLBench in
Figures 6, 7, 9, 10, 11, and 12.7.3 Convergence of Vanilla Frank-Wolfe Algorithm
7.3.1 Experimental Analysis
We randomly select 10moments of performing the Vanilla
Frank-Wolfe algorithm when optimizing bo
tand visualize the
optimization process in Figure 5. The evaluation metric is
the objective function in Eq. (4) with the task tÔ¨Åxed.
7.3.2 Theoretical Analysis
Corollary. Denotexmas the solution generated by vanilla
Frank‚ÄìWolfe at the m-th round. Then
min
1jmmax
s0;ksk1radiushrf(xm);xm si (8)
maxf2h0;4f0(1+2)D1+1D2
2+2(2+3+4)dTgTgpm+ 1;
whereh0=f(x0)  min
x0;kxk1radiusf(x),D1is the upper bound
offjL(wi)jgi2[T], andD2is the 2-norm upper bound of the neural
network weights. Note that, in our experiments, D220.
Whenfis convex and differentiable,
min
1jmmax
s0;ksk1radiushrf(xm);xm si
8f0(1+2)D1+1D2
2+ 2(2+3+4)dTgT
m+ 1:
(9)
To sum up, the convergence rate of vanilla Frank‚ÄìWolfe on
problem Eq. (7)is between1
mand1pm.
Remark. max
s0;ksk1radiushrf(xm);xm siis the standard
evaluation metric of vanilla Frank‚ÄìWolfe.
Proof. Recall thatf(bo
t) =0[(1+1P
j2[T]nftgBtj)L(wt) 
2(bo
t)>lo
t]+1P
s2Untkws Pi 1
j=1B(j)sw(j) Btswtk2
2+
2P
j2[q](j y0ij)2+3P
j2[T](rank(1)
j y00j)2+
4P
j2[T](rank(2)
j y00j)2.
Among the terms in f(bo
t),(1+1P
j2[T]nftgBtj)L(wt) 
2(bo
t)>lo
tis(1+2)D1T-Lipschitz with respect to bo
t
and1P
s2Untkws Pi 1
j=1B(j)sw(j) Btswtk2
2is1D2
2T-
Lipschitz with respect to bo
t, respectively.
According to [tanh(x)]0= 1 (tanh(x))2, it can be easily
deduced that 2P
j2[q](j y0ij)2+3P
j2[T](rankj y00j)2+
4P
j2[T](rank(2)
j y00j)2is2(2+3+4)dT2-Lipschitz
with respect to bo
t.
By integrating the above terms, we obtain the f0(1+
2)D1+1D2
2+ 2(2+3+4)dTgT-Lipschitz property
off(bo
t). Also note that the diameter of bo
t‚Äôs feasible solution
space is no greater than 1, so by Equation 3 in [18] and Equa-
tion 3.11 in [23] we can easily deduce the two inequalities in
Eq. (8) and Eq. (9).

--- PAGE 14 ---
14
TABLE 7: Reward before and after the incorporation of prior knowledge for MT10.
Ranking of performance by soft-module [32] CAMRL w/o prior success rate CAMRL w/ prior success rate
Pick and place 9 47235.31 42798.2
Pushing 8 213.27 227.94
Reaching 2 -50.36 -51.28
Door opening 6 143.74 147.64
Button press 4 1395.77 1449.87
Peg insertion side 10 52.91 50.03
Window opening 3 15683.29 14948.5
Window closing 7 23.64 26.97
Drawer opening 5 1329.39 1403.84
Drawer closing 1 622.74 618.76
TABLE 8: Reward before and after the incorporation of prior knowledge for MT50.
Ranking of performance by MT-SAC of meta-world benchmark [36] CAMRL w/o prior success rate CAMRL w/ prior success rate
Turn on faucet 1 7644.89 7928.39
Sweep 37 -10.31 -6.38
Stack NA (not found) -42.32 -43.21
Unstack NA (not found) -31.26 -30.48
Turn off faucet 1 1253.73 1308.41
Push back 34 -27.74 -26.37
Pull lever 36 -31.84 -30.37
Turn dial 1 -27.69 -28.36
Push with stick 37 1376.83 1427.37
Get coffee 31 -52.93 -49.31
Pull handle side 1 -48.93 -49.76
Basketball 37 16394.92 17389.21
Pull with stick 37 -28.92 -26.43
Sweep into hole NA (not found) 16.74 17.38
Disassemble nut 37 1873.93 1793.35
Place onto shell NA (not found) 274.01 304.74
Push mug NA (not found) 638.91 652.48
Press handle side 1 24.18 25.31
Hammer 26 -60.93 -58.42
Slide plate 1 425.64 445.31
Slide plate side 24 -23.54 -24.01
Press button wall 1 -27.73 -26.46
Press handle 1 -49.18 -47.54
Pull handle 1 46.84 47.14
Soccer 31 485.03 472.77
Retrieve plate side 1 127.94 130.58
Retrieve plate 1 -42.65 -44.51
Close drawer 1 428.93 441.93
Press button top 1 -29.84 -24.18
Reach 1 -23.47 -20.45
Press button top w/wall 1 -41.84 -38.53
Reach with wall 1 897.42 910.58
Insert peg side 25 12.93 10.56
Push 30 573.94 570.53
Push with wall 35 23.81 30.51
Pick & place w/wall 37 -33.27 -30.56
Press button 1 847.93 856.25
Pick & place 37 -23.15 -21.16
Pull mug NA (not found) -3.03 -2.74
Unplug peg 29 -52.94 -48.61
Close window 1 -21.19 -20.68
Open window 22 -27.36 -29.63
Open door 22 -62.27 -58.64
Close door 1 -23.01 -25.96
Open drawer 27 -60.03 -56.61
Open box 33 -27.09 -24.45
Close box 28 -47.15 -44.58
Lock door 1 -45.39 -40.99
Unlock door 1 -33.06 -36.81
Pick bin 37 -42.94 -40.51
TABLE 9: Reward before and after the incorporation of prior knowledge for Atari.
Ranking of performance in the Atari Learderboard CAMRL w/o prior reward CAMRL w/ prior reward
YarsRevenge 1 1637.94 1684.8
Jamesbond NA (not found in the Atari leaderboard [26]) 8.23 7.96
FishingDerby 7 -11.41 -7.43
Venture 5 0.22 0.24
DoubleDunk NA (not found) -0.89 -0.75
Kangaroo 3 5.74 5.94
IceHockey 8 -0.49 -0.52
ChopperCommand 2 351.68 374.95
Krull 4 9.75 9.49
Robotank 6 0.38 0.42

--- PAGE 15 ---
15
TABLE 10: Success rate before and after the incorporation of prior knowledge for Ravens.
Ranking of performance by Transporter network [37] CAMRL w/o prior success rate CAMRL w/ prior success rate
Block-insertion 2 94.26 94.78
Place-red-in-green 1 93.44 93.73
Towers-of-hanoi 4 95.12 95.34
Align-box-corner 3 93.43 94.01
Stack-block-pyramid 10 77.15 74.24
Palletizing-boxes 5 94.32 94.68
Assembling-kits 7 93.95 93.99
Packing-boxes 9 79.38 79.06
Manipulating-rope 8 89.27 90.24
Sweeping-piles 6 93.64 93.87
TABLE 11: Success rate before and after the incorporation of prior knowledge for RLBench.
Ranking of performance by [21] CAMRL w/o prior success rate CAMRL w/ prior success rate
Reach Target 1 99.87 99.93
Push Button 2 97.24 98.02
Pick And Lift 3 91.58 92.04
Pick Up Cup 4 86.48 86.53
Put Knife on Chopping Board 9 50.91 55.46
Take Money Out Safe 8 66.84 66.79
Put Money In Safe 6 80.38 82.44
Pick Up Umbrella 5 81.49 82.76
Stack Wine 10 24.59 26.98
Slide Block To Target 7 79.47 81.46
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013/uni00000015/uni00000011/uni0000001b/uni00000013/uni00000015/uni00000011/uni0000001b/uni00000014/uni00000015/uni00000011/uni0000001b/uni00000015/uni00000015/uni00000011/uni0000001b/uni00000016/uni00000015/uni00000011/uni0000001b/uni00000017/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000003/uni00000049/uni00000058/uni00000051/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000005a/uni0000004b/uni00000048/uni00000051/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni0000004c/uni00000051/uni0000004a/uni00000003bo
t
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013/uni00000016/uni00000011/uni00000017/uni00000015/uni00000019/uni00000016/uni00000011/uni00000017/uni00000015/uni0000001b/uni00000016/uni00000011/uni00000017/uni00000016/uni00000013/uni00000016/uni00000011/uni00000017/uni00000016/uni00000015/uni00000016/uni00000011/uni00000017/uni00000016/uni00000017
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013/uni00000016/uni00000011/uni00000015/uni00000017/uni00000018/uni00000013/uni00000013/uni00000016/uni00000011/uni00000015/uni00000017/uni00000018/uni00000015/uni00000018/uni00000016/uni00000011/uni00000015/uni00000017/uni00000018/uni00000018/uni00000013/uni00000016/uni00000011/uni00000015/uni00000017/uni00000018/uni0000001a/uni00000018/uni00000016/uni00000011/uni00000015/uni00000017/uni00000019/uni00000013/uni00000013/uni00000016/uni00000011/uni00000015/uni00000017/uni00000019/uni00000015/uni00000018
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013/uni00000016/uni00000011/uni00000014/uni0000001b/uni0000001b/uni00000016/uni00000011/uni00000014/uni0000001b/uni0000001c/uni00000016/uni00000011/uni00000014/uni0000001c/uni00000013/uni00000016/uni00000011/uni00000014/uni0000001c/uni00000014/uni00000016/uni00000011/uni00000014/uni0000001c/uni00000015/uni00000016/uni00000011/uni00000014/uni0000001c/uni00000016/uni00000016/uni00000011/uni00000014/uni0000001c/uni00000017
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013/uni00000016/uni00000011/uni00000016/uni00000013/uni00000014/uni00000018/uni00000016/uni00000011/uni00000016/uni00000013/uni00000015/uni00000013/uni00000016/uni00000011/uni00000016/uni00000013/uni00000015/uni00000018/uni00000016/uni00000011/uni00000016/uni00000013/uni00000016/uni00000013
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013/uni00000016/uni00000011/uni00000013/uni00000018/uni00000018/uni00000016/uni00000011/uni00000013/uni00000019/uni00000013/uni00000016/uni00000011/uni00000013/uni00000019/uni00000018/uni0000002f/uni00000052/uni00000056/uni00000056/uni00000003/uni00000049/uni00000058/uni00000051/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni0000005a/uni0000004b/uni00000048/uni00000051/uni00000003/uni00000032/uni00000053/uni00000057/uni0000004c/uni00000050/uni0000004c/uni0000005d/uni0000004c/uni00000051/uni0000004a/uni00000003bo
t
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013/uni00000016/uni00000011/uni00000014/uni0000001c/uni00000016/uni00000016/uni00000011/uni00000014/uni0000001c/uni00000017/uni00000016/uni00000011/uni00000014/uni0000001c/uni00000018/uni00000016/uni00000011/uni00000014/uni0000001c/uni00000019/uni00000016/uni00000011/uni00000014/uni0000001c/uni0000001a/uni00000016/uni00000011/uni00000014/uni0000001c/uni0000001b
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013
/uni0000002c/uni00000057/uni00000048/uni00000055/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni00000003/uni00000052/uni00000049/uni00000003/uni00000039/uni00000044/uni00000051/uni0000004c/uni0000004f/uni0000004f/uni00000044/uni00000003/uni00000029/uni00000055/uni00000044/uni00000051/uni0000004e/uni00000003/uni0000003a/uni00000052/uni0000004f/uni00000049/uni00000048/uni00000015/uni00000011/uni00000018/uni00000019/uni00000015/uni00000011/uni00000018/uni0000001a/uni00000015/uni00000011/uni00000018/uni0000001b/uni00000015/uni00000011/uni00000018/uni0000001c/uni00000015/uni00000011/uni00000019/uni00000013/uni00000015/uni00000011/uni00000019/uni00000014
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013/uni00000016/uni00000011/uni00000016/uni00000015/uni00000016/uni00000016/uni00000011/uni00000016/uni00000015/uni00000017/uni00000016/uni00000011/uni00000016/uni00000015/uni00000018
/uni00000013 /uni00000014/uni00000013 /uni00000015/uni00000013/uni00000016/uni00000011/uni00000016/uni0000001c/uni00000017/uni00000016/uni00000011/uni00000016/uni0000001c/uni00000019/uni00000016/uni00000011/uni00000016/uni0000001c/uni0000001b/uni00000016/uni00000011/uni00000017/uni00000013/uni00000013
Fig. 5: Convergence of the vanilla Frank-Wolfe algorithm to optimize bo
t.
TABLE 12: Ablation study. Here ‚Äô-‚Äô means ‚Äôwithout‚Äô, and ‚Äô+‚Äô means ‚ÄôCAMRL with a speciÔ¨Åc setting‚Äô. a,b, andcare the
coefÔ¨Åcients of the indicator Imul.i(i2[4])are coefÔ¨Åcients of each term (except the Ô¨Årst term) in Eq. (6).
Method \ Reward Pick and place Pushing Reaching Door opening Button press Peg insertion side Window opening Window closing Drawer opening and Drawer closing Avg. reward
CAMRL 45052.2 204.3 -53.3 138.5 1382.4 53.3 6764.8 10.3 1321.7 1512.9 5638.7
AMRL 10841.9 10.1 -62.2 30.4 984.2 2.9 5481.7 21.1 104.6 1033.8 1844.9
- mode switch 18849.3 19.7 -50.4 80.5 1163.8 30.6 5791.4 8.9 498.8 1402.1 2779.5
+ a=positive inÔ¨Ånity 35318.4 170.4 -55.3 125.5 1303.8 44.1 6191.7 9.2 1184.2 1421.4 4571.3
+ b=positive inÔ¨Ånity 35024.8 149.8 -58.9 104.8 1284.3 41.7 5938.1 9.1 872.9 1284.5 4465.1
+ c=0 38529.1 192.6 -53.8 134.9 1389.4 47.9 6268.8 10.6 1288.7 1482.9 4929.1
+ d=0 23029.4 21.8 -61.9 94.7 1228.6 36.1 5893.3 8.4 841.2 1465.5 3255.7
+2=0 36931.3 109.4 -57.7 112.9 1255.7 42.6 4193.5 14.9 1201.8 1449.2 4525.4
+3=0 43093.8 215.4 -56.3 129.4 1372.5 47.7 6288.3 12.6 1279.1 1485.6 5386.8
+4=0 44226.7 202.5 -53.8 130.8 1368.4 48.1 6631.2 9.3 1249.5 1474.5 5528.7

--- PAGE 16 ---
16
Fig. 6: Averaged reward curve of the Ô¨Årst 10Kepisodes for Gym-minigrid tasks (each experiment repeated 5times). The
higher the metric, the better performance of the model.
Fig. 7: Averaged reward curve of the Ô¨Årst 10Kepisodes for MT10 tasks (each experiment repeated 5times). The higher the
metric, the better performance of the model.
/uni00000015 /uni00000017 /uni00000014/uni00000013 /uni00000015/uni00000013 /uni00000017/uni00000013 /uni00000014/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013
/uni0000000b/uni00000045/uni0000000c/uni00000003/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000003/uni00000052/uni00000049/uni00000003/uni0000004b/uni0000005c/uni00000053/uni00000048/uni00000055/uni00000010/uni00000053/uni00000044/uni00000055/uni00000044/uni00000050/uni00000048/uni00000057/uni00000048/uni00000055/uni00000056/uni00000003/uni0000000ba/uni0000000f/uni000000031
b/uni0000000f/uni00000003c/uni0000000f/uni00000003d/uni0000000c/uni00000016/uni00000018/uni00000013/uni00000013/uni00000016/uni0000001a/uni00000018/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000017/uni00000015/uni00000018/uni00000013/uni00000017/uni00000018/uni00000013/uni00000013/uni00000017/uni0000001a/uni00000018/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000018/uni00000015/uni00000018/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000055/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000052/uni00000049/uni00000003/uni00000044/uni0000004f/uni0000004f/uni00000003/uni00000057/uni00000044/uni00000056/uni0000004e/uni00000056/uni00000003/uni00000052/uni00000059/uni00000048/uni00000055/uni00000003/uni00000014/uni00000013/uni0000004e/uni00000003/uni00000048/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056a
1
b
c
d
Fig. 8: Hyper-parameter analysis of CAMRL.

--- PAGE 17 ---
17
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000014/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000036/uni0000004c/uni00000051/uni0000004a/uni0000004f/uni00000048/uni00000003/uni00000024/uni00000046/uni00000057/uni00000052/uni00000055/uni00000003/uni00000026/uni00000055/uni0000004c/uni00000057/uni0000004c/uni00000046
/uni00000036/uni0000004c/uni00000051/uni0000004a/uni0000004f/uni00000048/uni00000003/uni00000036/uni00000024/uni00000026
/uni0000003c/uni00000032/uni0000002f/uni00000032/uni00000035/uni00000010/uni00000036/uni00000024/uni00000026
/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000055/uni00000044/uni0000004f
/uni0000002a/uni00000055/uni00000044/uni00000047/uni00000042/uni00000056/uni00000058/uni00000055/uni0000004a/uni00000048/uni00000055/uni0000005c
/uni00000036/uni00000052/uni00000049/uni00000057/uni00000003/uni00000030/uni00000052/uni00000047/uni00000058/uni0000004f/uni00000048
/uni00000026/uni00000024/uni00000030/uni00000035/uni0000002f/uni0000000b/uni00000052/uni00000058/uni00000055/uni00000056/uni0000000c
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000015/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000016/uni00000014/uni00000019/uni00000013
/uni00000014/uni00000017/uni00000013
/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000017/uni0000001b/uni00000013
/uni0000001a/uni00000013
/uni00000019/uni00000013
/uni00000018/uni00000013
/uni00000017/uni00000013
/uni00000016/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000018/uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000019/uni00000014/uni00000015/uni00000018
/uni00000014/uni00000013/uni00000013
/uni0000001a/uni00000018
/uni00000018/uni00000013
/uni00000015/uni00000018
/uni00000013/uni00000015/uni00000018
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni0000001a/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni0000001b/uni00000014/uni00000017/uni00000013
/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni0000001c/uni00000013/uni00000015/uni00000018/uni00000013/uni00000018/uni00000013/uni00000013/uni0000001a/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000018/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000014/uni00000013/uni00000014/uni00000013/uni00000013
/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000014/uni00000014/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000014/uni00000015/uni00000013/uni00000015/uni00000018/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni0000001a/uni00000018/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000018/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000013/uni00000013/uni00000014/uni0000001a/uni00000018/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000014/uni00000016/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000014/uni00000017/uni00000014/uni00000018/uni00000013
/uni00000014/uni00000015/uni00000018
/uni00000014/uni00000013/uni00000013
/uni0000001a/uni00000018
/uni00000018/uni00000013
/uni00000015/uni00000018
/uni00000013/uni00000015/uni00000018
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000014/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000014/uni00000019/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000014/uni0000001a/uni00000015/uni00000013/uni00000013
/uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000014/uni0000001b/uni00000014/uni00000018/uni00000013
/uni00000014/uni00000013/uni00000013
/uni00000018/uni00000013
/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000014/uni0000001c/uni00000014/uni00000017/uni00000013
/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000015/uni00000013/uni00000014/uni00000013/uni00000013
/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000015/uni00000014/uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000015/uni00000015/uni00000014/uni00000013/uni00000013
/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000015/uni00000016/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000015/uni00000017/uni00000014/uni00000013/uni00000013
/uni00000018/uni00000013
/uni00000013/uni00000018/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000015/uni00000018/uni0000001b/uni00000013/uni00000013
/uni00000019/uni00000013/uni00000013
/uni00000017/uni00000013/uni00000013
/uni00000015/uni00000013/uni00000013
/uni00000013/uni00000015/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000015/uni00000019/uni00000014/uni00000018/uni00000013
/uni00000014/uni00000013/uni00000013
/uni00000018/uni00000013
/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000015/uni0000001a/uni00000014/uni00000015/uni00000018
/uni00000014/uni00000013/uni00000013
/uni0000001a/uni00000018
/uni00000018/uni00000013
/uni00000015/uni00000018
/uni00000013/uni00000015/uni00000018
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000015/uni0000001b/uni00000014/uni00000013/uni00000013
/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000015/uni0000001c/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000016/uni00000013/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000016/uni00000014/uni00000014/uni00000019/uni00000013
/uni00000014/uni00000017/uni00000013
/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000016/uni00000015/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000016/uni00000016/uni00000013/uni00000018/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000016/uni00000017/uni00000014/uni00000013/uni00000013
/uni00000013/uni00000014/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000016/uni00000018/uni0000001b/uni00000013/uni00000013
/uni00000019/uni00000013/uni00000013
/uni00000017/uni00000013/uni00000013
/uni00000015/uni00000013/uni00000013
/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000016/uni00000019/uni00000014/uni00000013/uni00000013
/uni0000001c/uni00000013
/uni0000001b/uni00000013
/uni0000001a/uni00000013
/uni00000019/uni00000013
/uni00000018/uni00000013
/uni00000017/uni00000013
/uni00000016/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000016/uni0000001a/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000016/uni0000001b/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000016/uni0000001c/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000017/uni00000013/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000017/uni00000014/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000017/uni00000015/uni00000014/uni00000019/uni00000013
/uni00000014/uni00000017/uni00000013
/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000017/uni00000016/uni00000014/uni00000019/uni00000013
/uni00000014/uni00000017/uni00000013
/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000017/uni00000017/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000017/uni00000018/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000017/uni00000019/uni00000014/uni0000001a/uni00000018
/uni00000014/uni00000018/uni00000013
/uni00000014/uni00000015/uni00000018
/uni00000014/uni00000013/uni00000013
/uni0000001a/uni00000018
/uni00000018/uni00000013
/uni00000015/uni00000018
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000017/uni0000001a/uni00000014/uni0000001a/uni00000018
/uni00000014/uni00000018/uni00000013
/uni00000014/uni00000015/uni00000018
/uni00000014/uni00000013/uni00000013
/uni0000001a/uni00000018
/uni00000018/uni00000013
/uni00000015/uni00000018
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000017/uni0000001b/uni00000014/uni00000015/uni00000013
/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000017/uni0000001c/uni0000001a/uni00000013
/uni00000019/uni00000013
/uni00000018/uni00000013
/uni00000017/uni00000013
/uni00000016/uni00000013
/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013 /uni00000019/uni00000013/uni00000013/uni00000013 /uni0000001b/uni00000013/uni00000013/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000028/uni00000053/uni0000004c/uni00000056/uni00000052/uni00000047/uni00000048/uni00000056/uni00000003/uni0000004c/uni00000003/uni00000052/uni00000049/uni00000003/uni00000037/uni00000044/uni00000056/uni0000004e/uni00000003/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013
/uni0000001b/uni00000013
/uni00000019/uni00000013
/uni00000017/uni00000013
/uni00000015/uni00000013
4
Fig. 9: Averaged reward curve of the Ô¨Årst 10Kepisodes for MT50 tasks (each experiment repeated 5times). The higher the
metric, the better performance of the model.

--- PAGE 18 ---
18
Fig. 10: Averaged reward curve of the Ô¨Årst 10Kepisodes for Atari tasks (each experiment repeated 5times). The higher the
metric, the better performance of the model.
Fig. 11: Averaged reward curve of the Ô¨Årst 10Kepisodes for Ravens tasks (each experiment repeated 5times). The higher
the metric, the better performance of the model.
Fig. 12: Averaged reward curve of the Ô¨Årst 10Kepisodes for RLBench tasks (each experiment repeated 5times). The higher
the metric, the better performance of the model.
