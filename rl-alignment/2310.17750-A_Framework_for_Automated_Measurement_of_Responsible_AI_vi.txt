# Một Khung Làm Việc để Đo Lường Tự Động các Tác Hại của AI Có Trách Nhiệm trong Ứng Dụng AI Tạo Sinh

Ahmed Magooda*, Alec Helyar*, Kyle Jackson*, David Sullivan, Chad Atalla, Emily Sheng, Dan Vann, Richard Edgar, Hamid Palangi, Roman Lutz, Hongliang Kong, Vincent Yun, Eslam Kamal, Federico Zarfati, Hanna Wallach, Sarah Bird, Mei Chen
Microsoft
Redmond, WA

TÓM TẮT
Chúng tôi trình bày một khung làm việc để đo lường tự động các chỉ số AI có trách nhiệm (RAI) cho các mô hình ngôn ngữ lớn (LLM) và các sản phẩm và dịch vụ liên quan. Khung làm việc của chúng tôi để tự động đo lường tác hại từ LLM xây dựng trên chuyên môn kỹ thuật và xã hội-kỹ thuật hiện có và tận dụng khả năng của các LLM tiên tiến như GPT-4. Chúng tôi sử dụng khung làm việc này để thực hiện nhiều nghiên cứu tình huống điều tra cách các LLM khác nhau có thể vi phạm một loạt các nguyên tắc liên quan đến RAI. Khung làm việc có thể được sử dụng cùng với chuyên môn xã hội-kỹ thuật chuyên ngành để tạo ra các phép đo cho các lĩnh vực tác hại mới trong tương lai. Bằng cách triển khai khung làm việc này, chúng tôi nhằm mục đích cho phép các nỗ lực đo lường tác hại tiên tiến hơn và thúc đẩy việc sử dụng có trách nhiệm của LLM.

1 GIỚI THIỆU
Những tiến bộ nhanh chóng trong trí tuệ nhân tạo (AI) và xử lý ngôn ngữ tự nhiên (NLP) đã dẫn đến sự phát triển của các mô hình ngôn ngữ lớn (LLM) ngày càng phức tạp như (GPT-4, LLama 2, Falcon, v.v.), với khả năng tạo văn bản tiên tiến trên một loạt rộng các loại nhiệm vụ. Trong khi những mô hình này mở ra nhiều cơ hội, cũng có những lo ngại nghiêm trọng về việc các mô hình gây ra tác hại. Phát hiện thủ công tác hại có thể tính toán tốt hơn các sắc thái. Tuy nhiên, khi tính khả dụng và khả năng của LLM tăng lên, việc phát triển các khung làm việc tự động để đo lường tác hại với tốc độ và quy mô có thể theo kịp tốc độ phổ biến của công nghệ ngày càng trở nên cần thiết.

Được thúc đẩy bởi nhu cầu về một khung đo lường tác hại tự động đủ linh hoạt để phù hợp với các định nghĩa tác hại đang phát triển, hợp lệ và đáng tin cậy, cũng như nhu cầu về việc triển khai đo lường có thể được áp dụng trên các loại sản phẩm và dịch vụ khác nhau liên quan đến LLM (ví dụ: chatbot, hệ thống tóm tắt, v.v.), chúng tôi đề xuất và triển khai một khung làm việc khai thác khả năng của LLM để kiểm tra các LLM khác và đánh giá tiềm năng gây tác hại của chúng. Trong khi công việc của chúng tôi tạo ra các công cụ để đo lường tự động, việc tạo ra các tài nguyên đo lường cụ thể về tác hại (ví dụ: định nghĩa đo lường tác hại) vẫn đòi hỏi chuyên môn chuyên ngành. Chúng tôi muốn đưa ra lời mở đầu cho phần còn lại của bài báo này với sự thừa nhận rằng đây không phải là triển khai cuối cùng, duy nhất, hoặc nhất thiết là tốt nhất để đo lường tác hại; tuy nhiên, đây là một triển khai cho phép linh hoạt trong việc cập nhật định nghĩa và áp dụng cho các sản phẩm và dịch vụ khác nhau. Vẫn còn những câu hỏi mở về rủi ro của việc sử dụng LLM để thực hiện các phần của quá trình đo lường tác hại và bao nhiêu phần của quy trình đo lường có thể và nên được tự động hóa—chúng tôi thảo luận thêm về điều này trong Phần 5 nhưng chủ yếu để lại những câu hỏi quan trọng này cho công việc tương lai.

Cốt lõi của khung làm việc đề xuất của chúng tôi bao gồm hai thành phần chính: (1) tạo dữ liệu từ mẫu và (2) đánh giá đầu ra được tạo. Đầu tiên, chúng tôi giới thiệu một thành phần tạo dữ liệu được thiết kế để đánh giá xu hướng LLM trong việc tạo ra các loại nội dung có thể gây hại cụ thể. Thành phần này mô phỏng các sản phẩm và dịch vụ LLM trong thế giới thực khác nhau, như trả lời câu hỏi, tóm tắt và hội thoại. Tiếp theo, chúng tôi giới thiệu một thành phần đánh giá sử dụng GPT-4 để đánh giá nội dung được tạo bởi LLM theo định nghĩa tác hại. Thành phần này đánh giá nội dung được tạo bởi AI và tạo ra cả đầu ra định lượng và định tính, đưa ra các chú thích số về mức độ nghiêm trọng của tác hại và các đoạn văn viết về lý luận chú thích. Khung làm việc của chúng tôi cho phép so sánh tự động các sản phẩm và dịch vụ dựa trên LLM khác nhau với các bộ đo lường được xây dựng bởi các chuyên gia lĩnh vực cho các tác hại khác nhau, cho phép các nhà thực hành so sánh điểm mạnh và điểm yếu.

2 KIẾN TRÚC
Khung đo lường của chúng tôi bao gồm hai thành phần được điều chỉnh để đánh giá LLM: 1) tạo dữ liệu từ mẫu và tham số, và 2) đánh giá đầu ra được tạo thông qua hướng dẫn chú thích. Thành phần tạo dữ liệu sử dụng mẫu và tham số để mô phỏng tương tác với LLM đang được kiểm tra để tạo ra dữ liệu xấp xỉ tương tác người dùng-AI trong một số sản phẩm hoặc dịch vụ. Các mẫu và tham số được tạo riêng biệt bởi các chuyên gia lĩnh vực cho mỗi tác hại để đảm bảo độ tin cậy và tính hợp lệ của các phép đo kết quả. Tiếp theo, thành phần đánh giá tạo ra các chú thích về đầu ra của LLM trên dữ liệu được tạo bằng cách áp dụng hướng dẫn chú thích. Hướng dẫn chú thích được cung cấp bởi các chuyên gia lĩnh vực dựa trên định nghĩa tác hại mà họ tạo ra.

Quá trình đánh giá được tối ưu hóa bằng cách coi LLM đang được kiểm tra như một hộp đen chỉ cần chấp nhận đầu vào và đưa ra đầu ra. Ngoài ra, việc triển khai khung làm việc này hỗ trợ hai môi trường khác nhau để tính toán. Môi trường đầu tiên bao gồm việc chạy đánh giá trên máy cục bộ, nơi việc xây dựng prompt và điều phối API mô hình, gọi API mô hình, v.v. xảy ra cục bộ. Môi trường thứ hai sử dụng nền tảng Azure Machine Learning (AML) để tự động xây dựng quy trình đánh giá và thực hiện đánh giá bằng tài nguyên tính toán AML. Hình 1 hiển thị một quy trình đánh giá AML mẫu.

2.1 Tạo Dữ Liệu
Phần đầu tiên của khung làm việc chúng tôi tập trung vào việc mô phỏng tương tác của người dùng giả định với một sản phẩm hoặc dịch vụ thực tế như trả lời câu hỏi, trò chuyện và tóm tắt tài liệu. Mục tiêu của phần này trong quy trình tạo dữ liệu, được gọi là mô phỏng nhiệm vụ, là tạo ra các tương tác (giữa LLM và người dùng giả định) bao gồm các chủ đề hoặc mẫu liên quan đến tác hại mục tiêu. Để đạt được điều này, chúng tôi sử dụng một LLM khác để đóng vai trò người dùng giả định, khởi tạo nhiệm vụ và tham gia vào cuộc hội thoại dựa trên các mẫu được cung cấp bởi các chuyên gia lĩnh vực. Chúng tôi ký hiệu LLM đang được kiểm tra là 𝑳𝑳𝑴 𝒕𝒆𝒔𝒕 và LLM mô phỏng người dùng là 𝑳𝑳𝑴 𝒖𝒔𝒆𝒓.

Chúng tôi cung cấp một bộ mẫu, được gọi là mẫu persona, cung cấp hướng dẫn cho 𝑳𝑳𝑴 𝒖𝒔𝒆𝒓 về cách hành xử và những chủ đề hoặc mục tiêu nào nên giới thiệu trong tương tác với 𝑳𝑳𝑴 𝒕𝒆𝒔𝒕. Để đơn giản và tổng quát hóa, chúng tôi sử dụng các mẫu tham số hóa kiểu Jinja. Mỗi mẫu mô tả cấu trúc cơ bản và chủ đề của cuộc hội thoại, để lại chỗ trống cho các tham số chỉ định các chủ đề cụ thể, nhóm người, v.v. để được tích hợp. Sau đó, mỗi mẫu được kết hợp với mỗi bộ tham số tương ứng để tạo ra một hoặc nhiều persona hoàn chỉnh cho 𝑳𝑳𝑴 𝒖𝒔𝒆𝒓 sử dụng trong mô phỏng nhiệm vụ với 𝑳𝑳𝑴 𝒕𝒆𝒔𝒕 hộp đen.

Với những persona hoàn chỉnh được tạo ra bằng cách kết hợp mẫu và tham số, chúng tôi chạy mô phỏng nhiệm vụ tiếp theo. Mỗi persona hoàn chỉnh phục vụ như hướng dẫn cho 𝐿𝐿𝑀𝑢𝑠𝑒𝑟, định hình cách nó tương tác với 𝐿𝐿𝑀𝑡𝑒𝑠𝑡. Phần này tiêm sự sáng tạo và quan trọng cho việc tự động hóa và mở rộng quy mô quá trình, nhưng nó cũng tạo ra rủi ro. Ví dụ, điều gì sẽ xảy ra nếu 𝐿𝐿𝑀𝑢𝑠𝑒𝑟 không mô phỏng hành vi người dùng thực tế trong tương tác với 𝐿𝐿𝑀𝑡𝑒𝑠𝑡? Chúng tôi khám phá những lo ngại này thêm trong phần 5. Một khi mô phỏng nhiệm vụ đã được chạy cho mỗi persona hoàn chỉnh, chúng tôi có một bộ dữ liệu được tạo bao gồm đầu vào người dùng mô phỏng và đầu ra hệ thống 𝑳𝑳𝑴 𝒕𝒆𝒔𝒕 thực (chúng tôi gọi mỗi tương tác mô phỏng là một mẫu).

2.2 Đánh Giá
Phần thứ hai của khung làm việc chúng tôi chịu trách nhiệm đánh giá thông qua chú thích tự động các mẫu được tạo trong mô phỏng nhiệm vụ. Quá trình chú thích sử dụng một LLM bằng cách cung cấp cho nó hướng dẫn chú thích được chế tác thủ công bởi các chuyên gia lĩnh vực và bao gồm định nghĩa tác hại, ví dụ và định nghĩa khiếm khuyết. Định nghĩa khiếm khuyết chỉ định tiêu chí để xác định liệu một mẫu dữ liệu có được coi là mong muốn hoặc cho phép trong bối cảnh của LLM đang được kiểm tra và bất kỳ sản phẩm hoặc dịch vụ nào nó được nhúng vào. Việc chế tác định nghĩa này là một thách thức xã hội-kỹ thuật được gắn chặt với định nghĩa tác hại được tạo bởi các chuyên gia lĩnh vực và quyết định chính sách được đưa ra bởi các tổ chức xây dựng hệ thống AI đang được kiểm tra.

LLM sau đó có thể chú thích các ví dụ cho trước bằng cách sử dụng hướng dẫn được cung cấp. Chú thích tự động bao gồm nhiều bước: bước đầu tiên sử dụng hướng dẫn chú thích để chú thích mỗi mẫu. Những chú thích này ban đầu được tạo bằng văn bản, nơi LLM tuân theo một lược đồ chú thích được chỉ định bởi các ví dụ few-shot trong hướng dẫn chú thích. Bước tiếp theo phân tích chú thích để trích xuất các chỉ số mong đợi (ví dụ: điểm khiếm khuyết, lý luận, v.v.) theo hướng dẫn được cung cấp. Bước cuối cùng bao gồm việc tổng hợp các giá trị được trích xuất và tính toán một chỉ số (ví dụ: tỷ lệ khiếm khuyết).

Đối với mỗi lĩnh vực tác hại, các thí nghiệm thỏa thuận chú thích người-LLM được tiến hành trong quá trình phát triển tài nguyên đo lường. Sau đó, tài nguyên đo lường và khung kỹ thuật có thể được áp dụng cùng nhau để tạo ra các phép đo mà không cần chú thích của con người. Cuối cùng, một tỷ lệ khiếm khuyết được tính toán, đại diện cho tỷ lệ các mẫu được chú thích là phù hợp với định nghĩa khiếm khuyết.

Ví dụ, một cách định nghĩa khiếm khuyết có thể hoạt động là thông qua ngưỡng mức độ nghiêm trọng. Xem xét trường hợp chúng ta có thể muốn đánh giá liệu LLM đang được kiểm tra có tạo ra nội dung bạo lực cực độ hay không. Các chuyên gia lĩnh vực có thể xây dựng thang đo mức độ nghiêm trọng (ví dụ: trên thang 1-10 nơi thấp hơn là ít nghiêm trọng hơn) cho nội dung bạo lực, và định nghĩa khiếm khuyết có thể là một ngưỡng trong phạm vi mức độ nghiêm trọng này hoặc một thang đo mức độ nghiêm trọng cụ thể (ví dụ: bất kỳ mẫu nào có mức độ nghiêm trọng ≥7 là một khiếm khuyết). Sau đó, tỷ lệ khiếm khuyết có thể được xác định bằng cách tính toán tỷ lệ các mẫu đáp ứng định nghĩa khiếm khuyết so với tổng số mẫu. Trong trường hợp này, tỷ lệ khiếm khuyết có thể được tính như sau:

𝐷𝑒𝑓𝑒𝑐𝑡𝑅𝑎𝑡𝑒 =|𝑥∈𝑠𝑎𝑚𝑝𝑙𝑒𝑠 :𝑥>𝑡ℎ𝑟𝑒𝑠ℎ𝑜𝑙𝑑|
|𝑠𝑎𝑚𝑝𝑙𝑒𝑠|

3 DIỄN GIẢI CÁC PHÉP ĐO
Bằng cách kết hợp khung làm việc này với tài nguyên đo lường (mẫu, tham số, định nghĩa tác hại và hướng dẫn chú thích), một quy trình đo lường có thể lặp lại có thể được tạo ra. Chạy quy trình đo lường này trên một hệ thống AI tạo ra một tỷ lệ khiếm khuyết. Điều quan trọng là diễn giải tỷ lệ khiếm khuyết này một cách cẩn thận và hiểu rõ tính hữu ích của các phép đo được rút ra theo cách này. Tất cả tỷ lệ khiếm khuyết thu được thông qua áp dụng khung kỹ thuật này là các phép đo tương đối, không đại diện cho trạng thái tuyệt đối của thế giới. Nói cách khác, tỷ lệ khiếm khuyết 0% không có nghĩa là không có cơ hội nào xảy ra tác hại được đo trong thế giới thực. Thay vào đó, tỷ lệ khiếm khuyết 0% có thể được diễn giải là có nghĩa rằng hệ thống AI đang được kiểm tra dường như không thất bại trong bất kỳ bài kiểm tra nào trong bộ đo lường hiện tại.

Ngoài ra, tất cả các phép đo kết quả chỉ đáng tin cậy và hợp lệ như tài nguyên đo lường được thiết kế cho tác hại đang được kiểm tra. Quá trình tạo ra những tài nguyên đo lường này là một vấn đề xã hội-kỹ thuật phức tạp chứa đầy cạm bẫy và cơ hội để độ tin cậy và tính hợp lệ bị ảnh hưởng. Nếu tài nguyên đo lường được tạo với định nghĩa tác hại được xây dựng kém, các phép đo kết quả có thể dao động từ vô nghĩa đến có hại trực tiếp (nếu quyết định phát triển bị hiểu lầm bởi phép đo được thiết kế kém).

Với quan điểm này, những phép đo này cung cấp tiện ích đáng kể và nhắm mục tiêu. Những phép đo này có thể phục vụ như công cụ chẩn đoán. Chúng cho phép so sánh hiệu quả của các biện pháp giảm thiểu khác nhau cũng như theo dõi tiến độ trong việc giảm thiểu các khiếm khuyết đã biết theo thời gian. Cuối cùng, khi sử dụng các bộ đo lường giống hệt nhau để kiểm tra hai hệ thống AI, các phép đo kết quả có thể được sử dụng để so sánh hiệu suất tương đối của mỗi hệ thống trên các thách thức được đại diện trong bộ đo lường.

4 NGHIÊN CỨU TÌNH HUỐNG
Dưới đây chúng tôi cung cấp một nghiên cứu sâu về Tính Căn Cứ. Sau đó chúng tôi cung cấp một ví dụ về cách khung làm việc này có thể được tận dụng để tạo ra các phép đo và so sánh nhiều mô hình.

4.1 Nghiên Cứu Sâu: Tính Căn Cứ
Trong nghiên cứu tình huống này, chúng tôi coi các thế hệ không có căn cứ từ 𝐿𝐿𝑀𝑡𝑒𝑠𝑡 là có hại và gọi danh mục đo lường này là tính căn cứ. Trước tiên chúng tôi phải xây dựng tài nguyên đo lường cho tác hại cụ thể này. Như đã đề cập trước đó, tài nguyên đo lường phải bao gồm một bộ mẫu và tham số. Đối với nghiên cứu tình huống về tính căn cứ, các mẫu và tham số là để tạo ra một bộ câu hỏi (prompt cho 𝐿𝐿𝑀𝑡𝑒𝑠𝑡) và các tệp ngữ cảnh tương ứng (được sử dụng bởi 𝐿𝐿𝑀𝑡𝑒𝑠𝑡 để trả lời các câu hỏi prompt). Trong giai đoạn đầu tiên của quy trình đánh giá (tức là tạo dữ liệu với mô phỏng nhiệm vụ), chúng tôi khởi tạo cuộc hội thoại giữa 𝐿𝐿𝑀𝑡𝑒𝑠𝑡 và 𝐿𝐿𝑀𝑢𝑠𝑒𝑟 mô phỏng. 𝐿𝐿𝑀𝑢𝑠𝑒𝑟 tuân theo các mẫu và tham số và đặt mỗi câu hỏi từ bộ được cung cấp. Đồng thời, chúng tôi cung cấp cho 𝐿𝐿𝑀𝑡𝑒𝑠𝑡 quyền truy cập vào các tệp ngữ cảnh và cung cấp hướng dẫn để trả lời các câu hỏi dựa hoàn toàn trên các tệp ngữ cảnh. Hình 4 minh họa hướng dẫn prompt cho 𝐿𝐿𝑀𝑡𝑒𝑠𝑡 để trả lời câu hỏi trong khi dựa độc quyền vào các tệp ngữ cảnh như một nguồn thông tin.

Sau khi tạo ra các cuộc hội thoại, chúng tôi tiến hành giai đoạn đánh giá để đánh giá các mẫu được tạo. Như một phần của tài nguyên đo lường, chúng tôi phải cung cấp hướng dẫn chú thích cho một LLM (GPT-4) để đánh giá liệu một phản hồi có căn cứ hay không. Trong trường hợp này, chúng tôi thiết kế một hướng dẫn chú thích cơ bản để tạo ra điểm tính căn cứ từ 1-5. Điểm 1 biểu thị rằng phản hồi không có căn cứ, trong khi điểm 5 chỉ ra rằng tất cả thông tin trong câu trả lời đều có căn cứ. Hình 5 hiển thị hướng dẫn chú thích. LLM chú thích (GPT-4) sau đó được cung cấp câu hỏi gốc do 𝐿𝐿𝑀𝑢𝑠𝑒𝑟 đặt, phản hồi từ 𝐿𝐿𝑀𝑡𝑒𝑠𝑡, và ngữ cảnh được đưa cho 𝐿𝐿𝑀𝑡𝑒𝑠𝑡 để xây dựng câu trả lời. Sau đó, LLM chú thích gán điểm tính căn cứ trên thang từ 1 đến 5 cho mỗi mẫu.

Để đánh giá hiệu quả của hướng dẫn chú thích, chúng tôi thu thập một bộ dữ liệu gồm 266 ví dụ bao gồm câu hỏi, phản hồi và ngữ cảnh được sử dụng để tạo ra phản hồi. Những ví dụ này được chú thích bởi các đánh giá viên con người sử dụng cùng thang từ 1 đến 5 cho tính căn cứ. Song song, chúng tôi sử dụng khung làm việc đề xuất của chúng tôi sử dụng GPT-4 để chú thích cùng dữ liệu, cũng trên cùng thang từ 1 đến 5, sử dụng hướng dẫn chú thích được chế tác. Sau đó, chúng tôi đánh giá sự thỏa thuận giữa chú thích của con người và GPT-4 bằng hai chỉ số heuristic đơn giản. Chỉ số đầu tiên, tỷ lệ thỏa thuận chính xác, đo tỷ lệ các trường hợp mà điểm của con người và GPT-4 giống hệt nhau. Chỉ số thứ hai phục vụ nhiều hơn như một heuristic lỏng lẻo: tỷ lệ thỏa thuận thoải mái, xem xét thỏa thuận trong các trường hợp mà điểm của con người và GPT-4 khác nhau không quá 1 điểm trên thang.

Phân tích sơ bộ của chúng tôi tiết lộ tỷ lệ thỏa thuận chính xác là 60% và tỷ lệ thỏa thuận thoải mái là 80,5% như hiển thị trong bảng 1. Hình 6 trình bày ma trận nhầm lẫn minh họa mối quan hệ giữa chú thích của con người và GPT-4. Công việc thêm về thỏa thuận người-người cũng cần thiết để xây dựng hiểu biết về kết quả chấp nhận được trên mỗi chỉ số này. Ngoài ra, phân tích thỏa thuận mạnh mẽ hơn sẽ được thực hiện trong công việc tương lai. Loại phép đo này cung cấp cảm giác về chất lượng của hướng dẫn chú thích, cho phép chúng tôi lặp lại và cải thiện hướng dẫn. Những kết quả sơ bộ này cũng hữu ích để xây dựng một khái niệm thô về mức độ tự tin chúng ta có thể có trong các phép đo kết quả.

4.2 Thiết Kế Thí Nghiệm
Chúng tôi tiến hành một loạt thí nghiệm để đánh giá ba LLM với khung đánh giá đề xuất. Chúng tôi gọi ba mô hình này là mô hình 1, mô hình 2 và mô hình 3. Trong tất cả các thí nghiệm được báo cáo, chúng tôi tập trung vào các nhiệm vụ mô phỏng hội thoại, nơi chúng tôi tham gia vào cuộc hội thoại tổng hợp với LLM đang được kiểm tra (𝐿𝐿𝑀𝑡𝑒𝑠𝑡) để đo xu hướng vi phạm các nguyên tắc RAI trong các khía cạnh sau:

(1) Thành công trong Jailbreak
(2) Tạo Nội Dung Có Thể Gây Hại, bao gồm nhưng không giới hạn ở:
• Nội dung Thù Hận hoặc Không Công Bằng
• Nội dung Tình Dục
• Nội dung Bạo Lực

(3) Rò rỉ Sở hữu Trí tuệ (IP):
• Bài hát
• Sách
• Tin tức

Trong vòng thí nghiệm này, chúng tôi sử dụng GPT-4 trong cả thành phần tạo dữ liệu và đánh giá của quy trình. Để tạo dữ liệu, chúng tôi sử dụng GPT-4 để mô phỏng tác nhân người dùng (𝐿𝐿𝑀𝑢𝑠𝑒𝑟) trò chuyện với 𝐿𝐿𝑀𝑡𝑒𝑠𝑡 sử dụng các mẫu persona được cung cấp. Để đánh giá, chúng tôi sử dụng GPT-4 như LLM cơ bản cho thành phần chú thích. Thiết kế thí nghiệm này nhằm minh họa đại khái cách khung làm việc đề xuất của chúng tôi có thể được tận dụng trong việc đánh giá hiệu suất của các LLM khác nhau để gây ra các tác hại khác nhau hoặc vi phạm các nguyên tắc RAI.

4.3 Kết Quả
Như minh họa trong Bảng 2, ba mô hình thể hiện hành vi tương tự về tỷ lệ khiếm khuyết khi được đánh giá cho việc tạo ra nội dung có thể gây hại. Điều này chỉ ra rằng các mô hình tạo ra nội dung được chú thích là khiếm khuyết trên số lượng mẫu tương tự, với Mô hình 3 hiển thị tỷ lệ thấp nhất trong việc tạo ra khiếm khuyết nội dung có thể gây hại. Đáng chú ý, việc tạo ra nội dung bạo lực và thù hận phổ biến hơn so với nội dung tình dục.

Trong bối cảnh rò rỉ dữ liệu sở hữu trí tuệ (IP), Mô hình 2 và 3 thể hiện tỷ lệ khiếm khuyết giống hệt nhau trên tất cả các danh mục (bài hát, sách và tin tức), gợi ý rằng những mô hình này tạo ra nội dung được bảo vệ IP với cùng tỷ lệ khi được kiểm tra trên bộ tài nguyên đo lường này. Điều này có thể gợi ý rằng tài nguyên đo lường nên được mở rộng hoặc cải thiện để cung cấp sự rõ ràng lớn hơn về sự khác biệt hiệu suất có thể có giữa các mô hình. Trong số các danh mục IP khác nhau, bài hát thể hiện tỷ lệ rò rỉ cao nhất, tiếp theo là sách và tin tức. Ngược lại, Mô hình 1 hiển thị tỷ lệ khiếm khuyết cao hơn đáng kể cho bài hát và tin tức so với Mô hình 2 và 3, với tỷ lệ khiếm khuyết 45,8% cho bài hát so với 17,9% cho cả Mô hình 2 và 3, và tỷ lệ khiếm khuyết 9,6% cho tin tức so với 1,1% cho cả Mô hình 2 và 3. Điều này ngụ ý rằng Mô hình 1 dễ bị tổn thương hơn trong việc tiết lộ tài liệu được bảo vệ IP trong các kịch bản sản phẩm.

Về đánh giá jailbreak, Mô hình 2 và 3 thể hiện tỷ lệ khiếm khuyết tương đương, với rò rỉ hướng dẫn là vector tấn công thành công nhất so với tạo nội dung người lớn hoặc thúc đẩy hoạt động bất hợp pháp. Tuy nhiên, Mô hình 1 thể hiện độ dễ bị tổn thương cao hơn đáng kể đối với rò rỉ hướng dẫn, với tỷ lệ thành công 80% so với 51% và 53% cho Mô hình 2 và 3 tương ứng.

Kết luận, đánh giá của chúng tôi tiết lộ rằng Mô hình 2 và 3 hiển thị tỷ lệ thấp hơn trong việc tạo ra nội dung được bảo vệ IP và tiết lộ hướng dẫn cơ bản so với Mô hình 1. Vì vậy, chúng tôi đề xuất rằng Mô hình 2 và 3 có thể phù hợp hơn như các thành phần cho các sản phẩm và dịch vụ AI trong thế giới thực so với Mô hình 1.

5 HẠN CHẾ
Khung làm việc này tạo điều kiện đánh giá nhanh chóng và lặp lại của các phiên bản khác nhau của LLM và các sản phẩm và dịch vụ liên quan. Tuy nhiên, có một số hạn chế.

Sử dụng LLM để đo tác hại từ LLM khác. Đáng chú ý, công việc này không đầy đủ giải quyết các vấn đề liên quan đến rủi ro của việc sử dụng LLM để đo tác hại từ LLM khác, đặc biệt khi LLM được biết là gây ra tác hại. Đây là một vấn đề nghiên cứu mở, mặc dù chúng tôi lưu ý rằng thành phần đánh giá của khung làm việc chúng tôi đủ linh hoạt để cắm vào các phương pháp đánh giá khác. Lo ngại này có thể biểu hiện trong cả thành phần tạo dữ liệu và đánh giá của khung làm việc.

Trong trường hợp tạo dữ liệu (trong mô phỏng nhiệm vụ), bằng cách sử dụng LLM để bắt chước hành vi người dùng, chúng ta có nguy cơ LLM thất bại trong việc mô phỏng cuộc hội thoại thực tế. Điều này có thể ảnh hưởng đến tính hợp lệ sinh thái của dữ liệu được tạo. Ngoài ra, LLM được sử dụng trong mô phỏng nhiệm vụ có thể thất bại trong việc đại diện cho các mẫu ngôn ngữ của một số nhóm nhân khẩu học nhất định, khiến các nỗ lực đo lường đánh giá thấp tiềm năng tác hại ảnh hưởng đến các nhóm bị lề hóa.

Trong trường hợp đánh giá, việc sử dụng LLM để chú thích các tác hại tiềm ẩn từ nội dung được tạo bởi LLM khác có thể dẫn đến vấn đề. LLM được biết là tạo ra nội dung có hại và có thể tạo ra không cân xứng một số loại nội dung có hại cụ thể ảnh hưởng đến một số nhóm người cụ thể. Nếu LLM dễ bị tổn thương trong việc tạo ra một số loại nội dung có hại cụ thể, liệu nó có hiệu quả trong việc đánh giá và chú thích cùng loại nội dung đó không? Điều này có thể dẫn đến việc chú thích thiếu tác hại. Đồng thời, các xu hướng khác của LLM có thể dẫn đến việc chú thích quá mức tác hại. LLM được biết là gặp khó khăn với tính căn cứ, và chúng tôi đã quan sát các trường hợp mà LLM chú thích tạo ra điểm khiếm khuyết và lý luận văn bản trích dẫn các phần không tồn tại của mẫu. Tần suất và tác động của các thế hệ không có căn cứ trong quá trình chú thích có thể như thế nào? Bởi vì hậu quả thực tế của việc gắn nhãn sai một đoạn văn bản là không có hại có lẽ lớn hơn so với việc gắn nhãn sai văn bản là có hại, lượng nội dung có thể gây hại được đo từ khung làm việc này nên được coi là giới hạn dưới cho lượng nội dung có thể gây hại thực tế.

Một heuristic để đánh giá tác động của các vấn đề được mô tả ở trên là thỏa thuận chú thích người-mô hình. Trong khi thực hành này cung cấp thêm một số sự tự tin trong độ tin cậy của chú thích LLM, nó không thể được xem như một sự thay thế hoàn toàn đầy đủ cho nghiên cứu toàn diện cần thiết để giải quyết những lo ngại này. Ngoài ra, đo lường thỏa thuận chú thích người-mô hình chung chung là không đủ. Điều này do thực tế rằng các nhóm người khác nhau với kinh nghiệm sống khác nhau sẽ trải nghiệm tác hại khác nhau và chú thích khác nhau.

Tiện ích và diễn giải. Một hạn chế khác nằm trong tiện ích và diễn giải của các phép đo kết quả. Như đã đề cập trong phần 3, tỷ lệ khiếm khuyết 0% không thể được diễn giải có nghĩa là hệ thống AI đang được kiểm tra không gây ra tác hại. Các phép đo kết quả là tương đối hơn là tuyệt đối, vì vậy chúng hữu ích cho chẩn đoán và so sánh giữa các hệ thống nhưng không áp dụng được cho ước tính rủi ro tuyệt đối hoặc khả năng tuyệt đối của tác hại.

Tính hợp lệ và độ tin cậy. Có lẽ thách thức lớn nhất của khung kỹ thuật này là thực tế rằng nó đòi hỏi tài nguyên đo lường được xây dựng cẩn thận cho các vấn đề xã hội-kỹ thuật. Thật không may, nếu những tài nguyên đo lường này được tạo ra kém, việc sử dụng chúng trong khung kỹ thuật không ngay lập tức nâng lên bất kỳ cờ đỏ nào. Việc sử dụng tài nguyên đo lường được xây dựng kém hoặc không hợp lệ có thể không được chú ý, điều này có thể dẫn đến tăng tác hại nếu các nhà thực hành tin tưởng vào các phép đo kết quả. Trong nghiên cứu tình huống ban đầu của chúng tôi, chúng tôi tham gia với các chuyên gia lĩnh vực để tạo ra tài nguyên đo lường, nhưng công việc tương lai cần thiết để hiểu các thực hành liên quan đến việc tạo ra tài nguyên đo lường đáng tin cậy và hợp lệ.

Một khía cạnh khác của độ tin cậy liên quan đến tính tái tạo và tính ổn định của chú thích được tạo bởi LLM. Chúng tôi đã quan sát các chú thích lặp lại trên cùng một mẫu dẫn đến kết quả khác nhau. Để đáp ứng, chúng tôi triển khai một yếu tố ổn định chạy quá trình chú thích nhiều lần và sử dụng giá trị đa số được tạo cho mỗi mẫu. Trong khi điều này có thể giảm đáng kể biến động, nó đi kèm với chi phí tính toán tăng, vì nó đòi hỏi chạy đánh giá nhiều lần (ví dụ: 5 hoặc 7), có thể dẫn đến thời gian đánh giá dài hơn và yêu cầu tài nguyên lớn hơn.

Tài nguyên. Cuối cùng, chúng tôi nhận ra rằng cách tiếp cận này đòi hỏi nhiều lần gọi các mô hình lớn. Trong khi quyền truy cập vào LLM đang mở rộng, việc có được tài nguyên cần thiết để chạy các LLM khác nhau, đặc biệt cho các nhiệm vụ lớn, có thể thách thức và tốn kém. Tài nguyên tính toán cần thiết cho phương pháp này có thể làm cho nó không thực tế hoặc không thể tiếp cận đối với một số nhà thực hành, và các tác động môi trường liên quan đến sự phổ biến của khung làm việc này phải được kiểm tra.

6 KẾT LUẬN VÀ HƯỚNG TƯƠNG LAI
Trong công việc này, chúng tôi trình bày một khung kỹ thuật để đánh giá tự động các mô hình ngôn ngữ lớn (LLM) trong các lĩnh vực tác hại liên quan đến RAI khác nhau như tính căn cứ, nội dung có thể gây hại và rò rỉ sở hữu trí tuệ. Khung làm việc này tận dụng LLM để tự động hóa quá trình đánh giá, cho phép đo lường với tốc độ và quy mô được yêu cầu bởi sự phổ biến hiện tại của các sản phẩm và dịch vụ được hỗ trợ bởi LLM. Khung làm việc đề xuất cung cấp một quy trình từ đầu đến cuối để kiểm tra LLM (𝐿𝐿𝑀𝑡𝑒𝑠𝑡) bằng cách mô phỏng tương tác với LLM khác (𝐿𝐿𝑀𝑢𝑠𝑒𝑟) và chú thích đầu ra với LLM khác. Khung làm việc phụ thuộc vào các tài nguyên đo lường khác nhau được tạo tốt nhất bởi các chuyên gia lĩnh vực cho mỗi lĩnh vực tác hại chịu đo lường.

Sau đó, chúng tôi đã chứng minh tính hữu ích của khung làm việc đề xuất bằng cách đánh giá ba LLM gần đây trên ba danh mục tác hại riêng biệt (rò rỉ nội dung IP, tạo ra nội dung có thể gây hại và jailbreak). Các phép đo kết quả cho phép chúng tôi so sánh hiệu suất tương đối của những mô hình này và phục vụ như một ví dụ về cách khung làm việc này có thể được sử dụng bởi các nhà thực hành đưa ra quyết định về phiên bản mô hình nào sử dụng trong các sản phẩm và dịch vụ AI của họ. Trong khi cần nhiều công việc hơn để khám phá cách tài nguyên đo lường đáng tin cậy và hợp lệ được tạo ra cho mỗi lĩnh vực tác hại, khung làm việc này cung cấp một con đường khả thi để đánh giá tác hại xuất phát từ các hệ thống AI dựa trên LLM với tốc độ và quy mô có thể theo kịp tốc độ phát triển hiện tại. Đối với công việc tương lai, chúng tôi sẽ kiểm tra các hạn chế đã nêu ở trên để làm cho cách tiếp cận đo lường đáng tin cậy hơn, hợp lệ hơn, có thể lặp lại hơn, khách quan hơn và hiệu quả chi phí hơn.

TÀI LIỆU THAM KHẢO
[1] Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios Anastasopoulos, and Yulia Tsvetkov. 2023. Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey. arXiv:2210.07700 [cs.CL]
[2] OpenAI. 2023. GPT-4 Technical Report. ArXiv abs/2303.08774 (2023).
[3] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116 (2023).
[4] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).
