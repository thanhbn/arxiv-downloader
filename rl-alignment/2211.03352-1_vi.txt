# 2211.03352.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2211.03352.pdf
# Kích thước tệp: 4831225 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
1
Học Tăng Cường Đa Nhiệm Bất Đối Xứng 
Dựa Trên Chương Trình Giảng Dạy
Hanchi Huang, Deheng Ye, Li Shen, và Wei Liu
Tóm tắt —Chúng tôi giới thiệu CAMRL, thuật toán học đa nhiệm bất đối xứng dựa trên chương trình giảng dạy (AMTL) đầu tiên để xử lý đồng thời nhiều nhiệm vụ học tăng cường (RL). Để giảm thiểu ảnh hưởng tiêu cực của việc tùy chỉnh thứ tự huấn luyện một lần trong AMTL dựa trên chương trình giảng dạy, CAMRL chuyển đổi chế độ huấn luyện giữa RL đơn nhiệm song song và học tăng cường đa nhiệm bất đối xứng (MTRL), theo một chỉ số liên quan đến thời gian huấn luyện, hiệu suất tổng thể và khoảng cách hiệu suất giữa các nhiệm vụ. Để tận dụng kiến thức trước đa nguồn một cách linh hoạt và giảm chuyển giao tiêu cực trong AMTL, chúng tôi tùy chỉnh một hàm mất mát tổng hợp với nhiều hàm xếp hạng khả vi và tối ưu hóa hàm mất mát thông qua tối ưu hóa luân phiên và thuật toán Frank-Wolfe. Việc điều chỉnh tự động các siêu tham số dựa trên độ không chắc chắn cũng được áp dụng để loại bỏ nhu cầu phân tích siêu tham số tốn công trong quá trình tối ưu hóa. Bằng cách tối ưu hóa hàm mất mát tổng hợp, CAMRL dự đoán nhiệm vụ huấn luyện tiếp theo và liên tục xem xét lại ma trận chuyển giao và trọng số mạng. Chúng tôi đã tiến hành thí nghiệm trên một loạt các điểm chuẩn trong RL đa nhiệm, bao gồm Gym-minigrid, Meta-world, trò chơi video Atari, nhiệm vụ PyBullet dựa trên thị giác và RLBench, để chứng minh sự cải tiến của CAMRL so với thuật toán RL đơn nhiệm tương ứng và các thuật toán MTRL tiên tiến. Mã nguồn có sẵn tại: https://github.com/huanghanchi/CAMRL.
Từ khóa chỉ mục —Học tăng cường, học chương trình giảng dạy, học đa nhiệm bất đối xứng.
F
1 GIỚI THIỆU
HỌC đa nhiệm (MTL) huấn luyện đồng thời nhiều nhiệm vụ liên quan trong khi tận dụng những điểm tương đồng và khác biệt giữa các nhiệm vụ [38]. Tuy nhiên, trong MTL, chuyển giao tiêu cực có thể xảy ra vì không phải tất cả các nhiệm vụ đều có thể hưởng lợi từ học kết hợp. Để xử lý điều này, chuyển giao bất đối xứng giữa mọi cặp nhiệm vụ đã được phát triển [19], sao cho lượng chuyển giao từ một mạng tin cậy đến một mạng tương đối ít tin cậy hơn lớn hơn theo hướng ngược lại. Ý tưởng cốt lõi là học một đồ thị chính quy hóa có hướng có trọng số thưa thớt giữa mỗi hai nhiệm vụ thông qua chế độ học đa nhiệm bất đối xứng dựa trên chương trình giảng dạy (AMTL), điều này đã được chứng minh là rất hiệu quả trong học có giám sát [19], [20], [24], trong khi vẫn chưa được khám phá trong học tăng cường (RL).

Trong bài báo này, lấy cảm hứng từ AMTL, chúng tôi nghiên cứu vấn đề giảm thiểu chuyển giao tiêu cực trong học tăng cường đa nhiệm (MTRL). Lưu ý rằng việc kết hợp các kỹ thuật AMTL vào bối cảnh RL không dễ dàng. Do tính không ổn định khi huấn luyện các nhiệm vụ RL và thiếu kiến thức trước về các thuộc tính của nhiệm vụ RL, có thể chúng ta sẽ học được một đồ thị chính quy hóa kém giữa các nhiệm vụ, nếu chúng ta trực tiếp áp dụng AMTL dựa trên chương trình giảng dạy để huấn luyện các nhiệm vụ từng cái một mà không có các điều chỉnh thích hợp cho thứ tự huấn luyện. Bên cạnh đó, một số yếu tố quan trọng để linh hoạt hóa việc chuyển giao giữa các nhiệm vụ và tránh chuyển giao tiêu cực, chẳng hạn như các chỉ số đại diện cho tiến trình huấn luyện tương đối, hiệu suất chuyển giao giữa các nhiệm vụ và sự đa dạng hành vi được mô tả trong [25], đã bị bỏ qua bởi các công trình hiện có về AMTL [19], [20], [24], điều này có thể dẫn đến hội tụ chậm cho một nhiệm vụ RL, và gây ra chuyển giao tiêu cực nghiêm trọng trước khi hội tụ. Ngoài ra, đáng chú ý là việc thực hiện AMTL dựa trên chương trình giảng dạy mọi lúc là tốn thời gian và có thể dẫn đến suy giảm hiệu suất khi mỗi nhiệm vụ được huấn luyện kém hoặc đã được huấn luyện tốt. Do đó, các nhiệm vụ RL yêu cầu các nhu cầu khác nhau về học chương trình giảng dạy ở các giai đoạn huấn luyện khác nhau và lượng kinh nghiệm học được từ các nhiệm vụ khác nhau cần được điều chỉnh động.

Để giải quyết các vấn đề trên, chúng tôi đề xuất CAMRL, thuật toán Học Tăng Cường Đa Nhiệm Bất Đối Xứng Dựa Trên Chương Trình Giảng Dạy đầu tiên. Để cân bằng cả hiệu quả và lượng chuyển giao dựa trên chương trình giảng dạy giữa các nhiệm vụ, chúng tôi thiết kế một chỉ số tổng hợp có trọng số nhiều yếu tố để quyết định chế độ huấn luyện nào cần chuyển đổi giữa học đơn nhiệm song song và AMTL dựa trên chương trình giảng dạy. Để tránh tùy chỉnh thứ tự một lần của các nhiệm vụ huấn luyện, CAMRL tính toán lại chỉ số trên ở đầu mỗi epoch để chuyển đổi chế độ huấn luyện. Khi vào chế độ huấn luyện AMTL, CAMRL cập nhật thứ tự huấn luyện, đồ thị chính quy hóa và trọng số mạng bằng cách tối ưu hóa một hàm mất mát tổng hợp thông qua tối ưu hóa luân phiên và thuật toán Frank-Wolfe. Hàm mất mát chính quy hóa cả lượng chuyển giao đi ra và sự tương đồng giữa nhiều trọng số mạng. Hơn nữa, ba hàm xếp hạng khả vi mới được đề xuất để linh hoạt kết hợp các kiến thức trước khác nhau vào hàm mất mát, chẳng hạn như độ khó huấn luyện tương đối, hiệu suất đánh giá lẫn nhau và sự tương đồng giữa các nhiệm vụ. Cuối cùng, chúng tôi thảo luận về tính linh hoạt của CAMRL từ nhiều góc độ và các hạn chế của nó, để làm sáng tỏ việc kết hợp thêm AMTL dựa trên chương trình giảng dạy vào RL.

Đóng góp của chúng tôi được tóm tắt như sau:
Chúng tôi đề xuất thuật toán CAMRL (học tăng cường đa nhiệm bất đối xứng dựa trên chương trình giảng dạy),

--- TRANG 2 ---
2
…SAC1
Nhiệm vụ 4Nhiệm vụ 2Nhiệm vụ 5Chuyển Giao Trọng Số
Tương Đồng 
Trọng SốChuyển Giao 
Đầu RaK episodes K episodesVào Epoch MớiHấp thụ kiến thức trước 
càng nhiều càng tốt!Epoch Mới
Học Đơn Nhiệm 
Song Song
SAC2SAC3
SAC4AMTL Dựa Trên Chương Trình Giảng Dạy
Mục tiêu: tối ưu hóa 
hàm mất mát tổng hợp
Chính Quy HóaHàm Mất Mát Xếp Hạng Từng Phần
Yếu tố1: (i) trongYếu tố2: (ii) trongYếu tố3: (iii) trong 
……
Chuyển Đổi 
Chế Độ Huấn Luyện
…
…
Hình 1: Quy trình làm việc của CAMRL. Khi một epoch bắt đầu, CAMRL chuyển đổi chế độ huấn luyện sang học đơn nhiệm hoặc
AMTL dựa trên chương trình giảng dạy. Đối với chế độ sau, CAMRL tùy chỉnh một chương trình giảng dạy cho nhiều nhiệm vụ và cập nhật
đồ thị chuyển giao giữa các nhiệm vụ thông qua một hàm mất mát tổng hợp. CAMRL giữ nguyên chế độ huấn luyện trong K episodes trước khi vào epoch mới.

được thiết kế với cơ chế chuyển đổi chế độ huấn luyện
và một hàm mất mát tổ hợp chứa nhiều hàm xếp hạng
khả vi. Những thiết kế này giải quyết các vấn đề phổ
biến trong RL đa nhiệm, ví dụ, chuyển giao tiêu cực
và việc sử dụng kém kiến thức trước.
CAMRL có thể được kết hợp với nhiều thuật toán
và chế độ huấn luyện dựa trên RL khác nhau, cũng
như hấp thụ nhiều kiến thức trước và thông tin xếp
hạng của các yếu tố huấn luyện với số lượng bất kỳ,
điều hiếm thấy trong các công trình MTRL trước
đây [28], [32], [34]. Hơn nữa, bằng cách điều chỉnh
tham số động, CAMRL có thể thích ứng với sự xuất
hiện của nhiệm vụ mới thông qua một điều chỉnh đơn giản.
Các thí nghiệm trên một loạt các nhiệm vụ RL có
chiều thấp/cao cho thấy CAMRL vượt trội đáng kể
so với thuật toán RL đơn nhiệm tương ứng và các
thuật toán học tăng cường đa nhiệm tiên tiến.

2 CÔNG TRÌNH LIÊN QUAN
2.1 Học Tăng Cường Đa Nhiệm (MTRL)
Trước khi bước vào kỷ nguyên RL sâu, hầu hết các công trình [6], [27]
về các thuật toán hướng đa nhiệm trong RL đã cố gắng
dựa vào sự hỗ trợ từ học chuyển giao. Sau đó,
chia sẻ kiến thức trong RL sâu đa nhiệm được đề xuất
để chưng cất các đặc điểm chung giữa tất cả các nhiệm vụ [12], [28]. Vì
can thiệp tiêu cực giữa các gradient từ nhiều nhiệm vụ khác nhau
có thể xảy ra rất thường xuyên trong quá trình chưng cất, công trình [34] đã trình bày một phương pháp phẫu thuật gradient để thay đổi
gradient bằng cách chiếu mỗi gradient lên mặt phẳng pháp tuyến
của gradient khác khi xung đột xảy ra giữa các gradient. Các nhà nghiên cứu
cũng đã cố gắng nắm bắt sự tương đồng giữa các gradient từ
nhiều nhiệm vụ [7], [39]. Hơn nữa, ngoài việc thúc đẩy
các gradient trở nên tương tự, việc áp dụng các mô hình tổ hợp
cũng là một cách tiếp cận tự nhiên để xử lý xung đột của gradient.
Bằng cách chia nhỏ một vấn đề đa nhiệm thành các mô-đun
và hỗ trợ khả năng tổ hợp của các mô-đun riêng biệt trong
các agent RL mô-đun mới, mô hình tổ hợp cho phép
giảm bớt xung đột giữa các gradient và tạo điều kiện cho
việc tổng quát hóa tốt hơn [9]. Tuy nhiên, việc huấn luyện các chính sách con riêng biệt thường yêu cầu các nhiệm vụ con được định nghĩa trước rõ ràng, điều này có thể
không khả thi trong các ứng dụng thực tế. Thay vì xác định thủ công
các mô-đun hoặc chính sách con và đặc tả
của sự kết hợp chúng, các tác giả của [32] đã giới thiệu
mô-đun mềm, một phương pháp hoàn toàn đầu cuối tự động tạo ra
các kết hợp mềm của nhiều mô-đun mà không cần
chỉ định cấu trúc chính sách trước.

2.2 Học Chương Trình Giảng Dạy
Học chương trình giảng dạy (CL) mô tả một chế độ học trong đó
chúng ta bắt đầu với các nhiệm vụ dễ và sau đó dần dần tăng
độ khó của nhiệm vụ. Ý tưởng sử dụng chương trình giảng dạy để huấn luyện
các agent học đầu tiên được đề xuất trong [11] và đầu tiên được giới thiệu cho
học sâu bởi [5]. Qua các năm, chương trình giảng dạy đã được
áp dụng để huấn luyện các nhiệm vụ phức tạp hoặc các nhiệm vụ với phần thưởng thưa thớt
[2], [3], [31], [33]. Các nỗ lực nghiên cứu ban đầu tập trung vào
cấu hình thủ công của chương trình giảng dạy có thể tốn
thời gian và yêu cầu kiến thức miền. Để giải quyết
vấn đề trên, CL tự động đã trở nên phổ biến gần đây,
nhằm tự động thích ứng với phân phối dữ liệu huấn luyện
bằng cách học điều chỉnh việc lựa chọn chương trình giảng dạy theo
một số yếu tố như tính đa dạng và tiến trình học [25].
Tuy nhiên, công trình của chúng tôi dựa trên một dòng công trình khác [19],
[24] chọn nhiệm vụ tiếp theo để huấn luyện bằng cách thực hiện
tối ưu hóa kết hợp trên hàm mất mát của các nhiệm vụ học có giám sát với
các chính quy hóa `2-norm trên các tham số nhiệm vụ.

3 PHƯƠNG PHÁP
3.1 Tổng Quan
Quy trình của CAMRL được hiển thị trong Hình 1. Trong CAMRL, chúng tôi
tùy chỉnh một chỉ số tiến trình học để xác định có nên
thực hiện huấn luyện đơn nhiệm song song hay AMTL dựa trên chương trình giảng dạy
ở đầu mỗi epoch. Khi thực hiện AMTL dựa trên chương trình giảng dạy, một hàm mất mát tổng hợp
học việc chuyển giao giữa các nhiệm vụ và xem xét một số
yếu tố để tránh huấn luyện tiêu cực được áp dụng. Chúng tôi áp dụng
tối ưu hóa luân phiên và thuật toán Frank-Wolfe để
quyết định nhiệm vụ tiếp theo để huấn luyện và cập nhật ma trận chuyển giao. Trong khi đó, các siêu tham số trong hàm mất mát được
tự động điều chỉnh theo độ không chắc chắn lịch sử của mỗi số hạng. Khi một nhiệm vụ mới xuất hiện, CAMRL có thể nhanh chóng
thích ứng với tình huống mới mà không có nhiều tác động phụ đến
các nhiệm vụ ban đầu, bằng cách chỉ cần sửa đổi ma trận chuyển giao.

Quy trình huấn luyện tổng thể của CAMRL được tóm tắt
trong Thuật toán 1. Giả sử chúng ta có T nhiệm vụ với
các mức độ khó khác nhau. Ban đầu, chúng tôi chỉ đơn giản trang bị cho mỗi
nhiệm vụ t ∈ [T] một mạng soft actor-critic (SAC) SACt [15],
và thực hiện huấn luyện đơn nhiệm song song trong vài epoch
mà không có sự can thiệp giữa các hàm mất mát cho các nhiệm vụ khác nhau, tức là,
mỗi SACt (t ∈ [T]) được huấn luyện độc lập. Ký hiệu wt
là các tham số mạng của mạng SAC thứ t. Tiếp theo,
theo một chỉ số liên quan đến tiến trình học,

--- TRANG 3 ---
3
Thuật toán 1 Thuật toán CAMRL
1:Đầu vào: λ₁, λ₂, αᵢ(i = 0,1,2,3,4), số nhiệm vụ T.
2:Khởi tạo: Ma trận chuyển giao B = I_T×T; mạng soft actor-critic SACt với tham số wt cho t ∈ [T];
W = (w₁, w₂, ..., wT).
3:for n = 1,2,...,N do
4: Thực hiện huấn luyện song song tất cả SACt cho t ∈ [T] trong K episodes mà không có sự can thiệp giữa các hàm mất mát.
5:end for
6:for n = N+1, N+2,... do
7: Tính Imul và lấy mẫu u ~ Uniform([0,1]).
8: if u < Imul then
9: Thực hiện huấn luyện song song SACt (t ∈ [T]) trong K episodes mà không có sự can thiệp giữa các hàm mất mát.
10: else
11: Dự đoán nhiệm vụ huấn luyện tiếp theo t và cập nhật bo_t = (Bt1,...,Bt(t-1),Bt(t+1),...,BtT) và wt bằng cách tối ưu hóa:
(t*, bo_t*) ← arg min[t∈U,bo_t] {α₀[(1+λ₁||bo_t||₁)L(wt)-λ₂(bo_t)ᵀlo_t]+λ₁∑[s∈U\t]||ws-∑[j=1]^[i-1]B(j)sw(j)-Btsw_t||²₂
+α₂∑[j∈[q]](j-y'ij)²+α₃∑[j∈[T]](rank(1)_j-y''j)²+α₄∑[j∈[T]](rank(2)_j-y''j)²} : (1)
12: Cố định bo_t, W và chọn nhiệm vụ t sao cho tối thiểu hóa mục tiêu trong Eq. (1).
13: Cố định t, W và chạy vanilla Frank-Wolfe trên Eq. (7) trong vài lần lặp để tối ưu hóa bo_t.
14: Cố định bo_t và huấn luyện nhiệm vụ t với hàm mất mát chính sách Eq. (1) trong K episodes.
15: end if
16:end for

CAMRL quyết định có nên chuyển chế độ huấn luyện thành
AMTL dựa trên chương trình giảng dạy hay không, điều này huấn luyện các nhiệm vụ từng cái một
bằng cách tối ưu hóa một số hạng hàm mất mát tổng hợp chính quy hóa ma trận chuyển giao giữa các nhiệm vụ. Chỉ số Imul (= (i)+(ii)+(iii)/3)
là tổng có trọng số dương của các số hạng sau: (i)
exp(-n/a), trong đó n đề cập đến số epoch hiện tại. Chúng tôi sử dụng exp(-n/a) để khuyến khích xác suất lớn hơn
của huấn luyện đa nhiệm trong giai đoạn đầu để các nhiệm vụ khó huấn luyện có thể hưởng lợi từ các nhiệm vụ dễ huấn luyện càng sớm
càng tốt. Như chúng tôi sẽ chỉ ra trong Bảng 12, hiệu suất của
CAMRL giảm rõ rệt khi chúng tôi loại bỏ số hạng này
trong Imul; (ii) exp(-Lnor/b), trong đó Lnor có nghĩa là hàm mất mát chính sách chuẩn hóa trung bình của tất cả các nhiệm vụ trong epoch cuối cùng.
Chúng tôi sử dụng Lnor để đánh giá tiến trình huấn luyện tổng thể. Nếu
hiệu suất huấn luyện tổng thể kém, tức là với Lnor lớn,
thì khả năng thực hiện chuyển giao giữa nhiệm vụ có xu hướng
nhỏ hơn. Lưu ý rằng trong các thí nghiệm, chúng tôi đặt Lnor là
(policy_loss_average - policy_loss)/std_policy_loss. Xem nhận xét sau
để biết chi tiết về hàm mất mát chính sách; (iii) tỷ lệ phần trăm các nhiệm vụ có phần thưởng chuẩn hóa trung bình trong epoch cuối cùng nằm
ngoài khoảng [Rnor - c*stdnor, Rnor + c*stdnor],
trong đó stdnor là độ lệch chuẩn của các phần thưởng chuẩn hóa cho tất cả các nhiệm vụ trong giai đoạn đó. Số hạng này cho biết
rằng khoảng cách tiến trình học giữa các nhiệm vụ càng lớn, xác suất thực hiện huấn luyện đa nhiệm càng lớn.

Nhận xét. Hàm mất mát chính sách của SAC trong [32]. Do vấn đề triển khai
của SAC kế thừa từ mã của soft-module [32], chúng tôi
thấy việc sử dụng phiên bản chuẩn hóa âm của hàm mất mát chính sách, đầu ra trực tiếp của hàm 'train' trong mã của soft-module, thuận tiện hơn nhiều so với việc sử dụng return chuẩn hóa.
Bên cạnh đó, khi gặp phải các nhiệm vụ khó với return thưa thớt,
return chuẩn hóa có thể giữ gần như không thay đổi theo thời gian. Trong những tình huống như vậy, việc sử dụng hàm mất mát chính sách chuẩn hóa âm có thể giúp tăng tốc độ huấn luyện mô hình hơn so với return chuẩn hóa. Cụ thể, hàm mất mát chính sách của SAC được sử dụng trong [32] có thể được đặc trưng là -log π(at|st) - Q(at,st), trong đó π(at|st)
là xác suất áp dụng hành động at khi đối mặt với trạng thái st và
Q(at,st) là giá trị ước tính của cặp trạng thái-hành động (at,st).

Nếu u ~ Uniform([0,1]) nhỏ hơn Imul, thì chế độ huấn luyện
AMTL dựa trên chương trình giảng dạy sẽ được áp dụng tạm thời trong epoch tiếp theo; ngược lại, chúng tôi sẽ thực hiện
chế độ huấn luyện đơn nhiệm (xem dòng 8-10 trong Thuật toán 1). Ở đây chúng ta cần lưu ý rằng: Imul liên tục thay đổi, không quan trọng nếu giá trị của nó tạm thời lớn hơn 1, điều này
cho thấy rằng việc thực hiện huấn luyện AMTL dựa trên chương trình giảng dạy là cần thiết vào thời điểm này. Khi AMTL dựa trên chương trình giảng dạy
không cần thiết, Imul giảm xuống trở lại và trở nên
nhỏ hơn 1.

3.2 Huấn Luyện Đa Nhiệm Chương Trình Giảng Dạy
Trong phần này, chúng tôi theo [19] để thiết lập nền tảng
của huấn luyện đa nhiệm chương trình giảng dạy cho CMARL của chúng tôi.

Gọi B là ma trận T×T biểu thị số lượng
chuyển giao giữa mỗi cặp nhiệm vụ. Đối với t ∈ [T], gọi wt là
các tham số của mạng soft actor-critic thứ t. Chúng tôi theo
giả định trong [19] rằng đối với tất cả t ∈ [T], wt ≈ ∑[s=1]^T Bst*ws, tức là,
Bst đề cập đến trọng số dương của cơ sở ws đại diện cho
wt. Ký hiệu W := (w₁, w₂, ..., wT).

Cốt lõi của CAMRL được điều chỉnh từ hàm mất mát tổng hợp sau [19]:

L(W, B) = ∑[t=1]^T {(1 + ||bo_t||₁)L(wt) + ||wt - ∑[s≠t] Bst*ws||²₂}, (2)

trong đó bo_t = (Bt1, ..., Bt(t-1), Bt(t+1), ..., BtT)ᵀ ∈ R^(T-1)
đại diện cho số lượng chuyển giao đi ra từ nhiệm vụ t đến

--- TRANG 4 ---
4
các nhiệm vụ khác và chúng tôi sử dụng ||bo_t||₁ để thỏa mãn tính chất thưa thớt
của ma trận chuyển giao B; L(wt) là hàm mất mát chính sách cho nhiệm vụ t
dưới wt, và wt là các tham số của mạng SAC thứ t;
(λ, λ) là các hệ số trọng số cho các số hạng khác nhau trong
Eq. (2).

Vì việc huấn luyện đồng thời W và B có thể dẫn đến
chuyển giao tiêu cực nghiêm trọng và thảm họa chiều khi
thực hiện tối ưu hóa, Lee et al. [19] đã công thức hóa
chế độ huấn luyện dựa trên chương trình giảng dạy tận dụng paradigm
trong Eq. (2), với mục đích tìm thứ tự tối ưu của
việc huấn luyện các nhiệm vụ và chỉ tối ưu hóa bo_t và wt thay vì B
và W khi huấn luyện nhiệm vụ t.

Ký hiệu S là không gian hoán vị trên T phần tử, và
đối với σ ∈ S, ký hiệu σ(i) là phần tử thứ i trong hoán vị σ.
Để thực hiện học chương trình giảng dạy của nhiều nhiệm vụ, Lee et al.
[19] đã điều chỉnh mục tiêu của họ để đối phó với Eq. (3):

min[σ∈S,W,B≥0] ∑[i=1]^T {(1 + ||bo_σ(i)||₁)L(wσ(i))
+ ||wσ(i) - ∑[j=1]^(i-1) Bσ(j)σ(i)*wσ(j)||²₂} : (3)

Gọi T := {σ(1), ..., σ(i-1)} là các nhiệm vụ đã huấn luyện và
U = {1, ..., T}\T là các nhiệm vụ chưa huấn luyện trong epoch hiện tại,
tương ứng. Lee et al. [19] sau đó đã tìm một nhiệm vụ t* ∈ U để
học tiếp theo, nhằm cải thiện quá trình học tương lai nhiều nhất:

(t*, bo_t*) ← arg min[t∈U,bo_t] {(1 + ||bo_t||₁)L(wt)
+ ∑[s∈U\t] ||ws - ∑[j=1]^(i-1) B(j)s*w(j) - Bts*wt||²₂} : (4)

Sau khi chọn nhiệm vụ được học tiếp theo và cập nhật bo_t
bằng cách thực hiện tối ưu hóa luân phiên trên Eq. (4), Lee et al.
[19] đã giải quyết vấn đề Eq. (5) để tham lam tối thiểu hóa Eq. (3):

wt* ← arg min[wt] {(1 + ||bo_t||₁)L(wt)
+ ||wt - ∑[j=1]^(i-1) B(j)t*w(j)||²₂} : (5)

3.3 Điều Chỉnh Hàm Mất Mát
Để phân phối hợp lý số lượng chuyển giao giữa mỗi
hai nhiệm vụ và tránh quá nhiều chuyển giao tiêu cực, chúng tôi có
những kỳ vọng sau và chúng tôi điều chỉnh số hạng hàm mất mát trong
CAMRL dựa trên những kỳ vọng này:

[Kỳ vọng 1] Ký hiệu pt,i là hiệu suất, thường là
phần thưởng trung bình qua vài episode đánh giá, trên
nhiệm vụ huấn luyện i bằng cách sử dụng mạng ban đầu để huấn luyện
nhiệm vụ t. Khi huấn luyện nhiệm vụ t, nếu chúng ta có thể kiểm tra khả năng chuyển giao
giữa một số nhiệm vụ và xấp xỉ thu được pt,i₁ > pt,i₂ >
... > pt,iᵩ cho một số nhiệm vụ i₁, i₂, ..., iᵩ, thì chúng ta kỳ vọng rằng
Bt,i₁ > Bt,i₂ > ... > Bt,iᵩ giữ càng nhiều càng tốt,
tức là, hiệu suất đánh giá tốt hơn trên các nhiệm vụ khác,
số lượng chuyển giao lớn hơn trên những nhiệm vụ đó. Để đáp ứng
kỳ vọng trên, chúng ta kỳ vọng ∑[j=1]ᵩ (j - yᵢⱼ)² càng nhỏ
càng tốt, trong đó j là thứ hạng của pt,iⱼ trong pt,i₁ >

0.0 0.2 0.4 0.6 0.8 1.0012345 d=1
d=10
d=20
d=50
d=100Hình 2: Các hàm xếp hạng với d khác nhau. Các điểm
được hiển thị bởi các đường nét đứt đề cập đến {pt,iⱼ}ⱼ∈[ᵩ] được tạo ngẫu nhiên của chúng tôi.

pt,i₂ > ... > pt,iᵩ và yᵢⱼ là thứ hạng của Bt,iⱼ trong
Bt,i₁, Bt,i₂, ..., Bt,iᵩ.

Trong trường hợp bình thường, yᵢⱼ không khả vi trên
Bt,i₁, Bt,i₂, ..., Bt,iᵩ. Do đó, chúng tôi xây dựng một hàm xếp hạng khả vi mới y'ᵢⱼ = q + 1 - ∑[s=1]ᵩ {0.5 -
tanh[d(Bt,iⱼ - Bt,iₛ)] + 0.5} để thay thế yᵢⱼ và hàm mất mát xếp hạng
do đó trở thành ∑[j=1]ᵩ (j - y'ᵢⱼ)².

Ở đây lưu ý rằng ý tưởng của hàm xếp hạng khả vi
của chúng tôi được lấy cảm hứng từ [10]. Ayman [10] đã tùy chỉnh một
hàm kích hoạt dưới dạng nhiều hàm tanh, xấp xỉ
hàm bước với các điểm cách đều. Ở đây chúng tôi điều chỉnh biểu diễn
chặn theo các kết hợp khác nhau cho tập điểm để thu được
hàm mất mát xếp hạng khả vi của chúng tôi cho phép các điểm cắt với
khoảng cách không bằng nhau. Với điều chỉnh của chúng tôi, hàm mất mát xếp hạng có thể
kết hợp nhiều kiến thức trước và yếu tố huấn luyện khác nhau để
tránh chuyển giao tiêu cực, bên cạnh độ khó huấn luyện tương đối
và hiệu suất đánh giá lẫn nhau giữa các nhiệm vụ, như
được nêu dưới đây.

Hình 2 mô tả các hình dạng của hàm tanh tổng hợp
đã điều chỉnh của chúng tôi dưới d khác nhau. Ở cuối mỗi epoch
có độ dài K, chúng ta có thể chọn ngẫu nhiên một số nhiệm vụ, đo lường
hiệu suất của mạng ban đầu được huấn luyện
cho nhiệm vụ i trên nhiệm vụ j, và cập nhật pi,j.

[Kỳ vọng 2] Nếu nhiệm vụ i dễ huấn luyện hơn, tức là,
với L(wi) nhỏ hơn, thì chúng ta kỳ vọng số lượng chuyển giao
nhỏ hơn từ nhiệm vụ t đến nhiệm vụ i. Điều này là do nếu một nhiệm vụ
dễ huấn luyện, tức là, với tiến trình huấn luyện nhanh hơn các
nhiệm vụ khác, thì số lượng chuyển giao đến nhiệm vụ đó từ
các nhiệm vụ khó huấn luyện khác (tức là, với tiến trình huấn luyện chậm hơn) theo trực giác
nên nhỏ hơn. Để đạt được điều này, chúng tôi định nghĩa lo_t
là (L(w₁), ..., L(wt₋₁), L(wt₊₁), ..., L(wT)). Sau đó thêm số hạng
(bo_t)ᵀlo_t và hàm mất mát xếp hạng khả vi tương ứng
∑[j∈[T]] (rank(1)_j - y''_j)² vào Eq. (5), trong đó rank(1)_j là
thứ hạng của L(wⱼ) trong L(w₁), L(w₂), ..., L(wT) và y''_j
là thứ hạng của Bt,j trong Bt,1, Bt,2, ..., Bt,T.

Lưu ý rằng lý do để thêm (bo_t)ᵀlo_t là để
tối thiểu hóa (bo_t)ᵀlo_t, chúng ta cần tối đa hóa -(bo_t)ᵀlo_t. Giá trị tối đa -(bo_t)ᵀlo_t tương ứng với tình huống trong đó Bt,i lớn hơn
được nhân với L(wi) lớn hơn và Bt,i nhỏ hơn được
nhân với L(wi) nhỏ hơn, điều này hoàn toàn phù hợp với
kỳ vọng thứ hai của chúng tôi.

[Kỳ vọng 3] Nếu nhiệm vụ i tương đồng hơn với nhiệm vụ t, thì
chúng ta kỳ vọng số lượng chuyển giao lớn hơn từ nhiệm vụ t đến nhiệm vụ
i. Ký hiệu độ tương đồng giữa nhiệm vụ i và nhiệm vụ t là si,t
và rank(2)_i là thứ hạng của si,t trong (s₁,t, s₂,t, ..., sT,t).

--- TRANG 5 ---
5
Tóm lại, vấn đề trong Eq. (4) bây giờ trở thành:

(t*, bo_t*) ← arg min[t∈U,bo_t] {α₀[(1+λ₁||bo_t||₁)L(wt)-λ₂(bo_t)ᵀlo_t]
+λ₁∑[s∈U\t] ||ws-∑[j=1]^(i-1) B(j)s*w(j)-Bts*wt||²₂+α₂∑[j∈[q]](j-y'ᵢⱼ)²
+α₃∑[j∈[T]](rank(1)_j-y''_j)²+α₄∑[j∈[T]](rank(2)_j-y''_j)²} : (6)

Để tối ưu hóa mục tiêu trong Eq. (6), đầu tiên chúng tôi cố định
bo_t và chọn nhiệm vụ t* với mục tiêu tối thiểu trong Eq. (6).
Sau đó cố định t* và áp dụng thuật toán Frank-Wolfe để tối ưu hóa
bo_t với đảm bảo hội tụ. Cuối cùng, cố định bo_t và huấn luyện nhiệm vụ
t* với hàm mất mát chính sách Eq. (1) trong K episodes.

Nhận xét. Các thước đo tương đồng cho kỳ vọng 3. Liên quan đến
kỳ vọng 3 cho hàm mất mát xếp hạng tùy chỉnh của chúng tôi, chúng tôi lấy
ba thước đo tương đồng sau, tương ứng: a) độ tương đồng cosine
giữa các mạng critic; b) độ tương đồng cosine giữa các mạng chính sách; c) khoảng cách embedding âm trong không gian trạng thái, sẽ được giải thích trong Phần Thảo luận. Trong số đó, thước đo đầu tiên có hiệu suất tốt nhất
trong các thử nghiệm ban đầu và do đó chúng tôi sử dụng thước đo này trong tất cả
các thí nghiệm. Đối với khoảng cách embedding âm giữa nhiệm vụ
i và nhiệm vụ j, ký hiệu không gian trạng thái là S và khoảng cách embedding giữa nhiệm vụ i và nhiệm vụ j được tính như sau:
(i) Lấy mẫu 100 trạng thái, {s₁, s₂, ..., s₁₀₀}, từ S một cách
ngẫu nhiên đồng đều. (ii) Ký hiệu các embedding của {s₁, s₂, ..., s₁₀₀}
được tính bởi các lớp embedding của nhiệm vụ i và nhiệm vụ j là
{eᵢ,₁, eᵢ,₂, ..., eᵢ,₁₀₀} và {eⱼ,₁, eⱼ,₂, ..., eⱼ,₁₀₀}, tương ứng.
(iii) Sau đó, √(1/100 ∑[m=1]¹⁰⁰ ||eᵢ,m - eⱼ,m||²₂) chính là khoảng cách
embedding âm giữa nhiệm vụ i và nhiệm vụ j.

3.4 Tối Ưu Hóa Hàm Mất Mát
Tối ưu hóa bo_t. Để làm cho hàm mất mát khả vi và
đảm bảo tính thưa thớt của ma trận chuyển giao B, chúng tôi thay thế
λ₁||bo_t||₁ trong Eq. (6) bằng λ₁∑[j∈[T]\{t}] Btj và thêm các ràng buộc
bo_t ≥ 0, ||bo_t||₁ ≤ radius < 1/2 vào Eq. (6),
trong đó radius là cận trên của ||bo_t||₁ (t ∈ [T])
mà chúng tôi đặt trong các thí nghiệm. Định nghĩa f(bo_t) = α₀[(1 +
λ₁∑[j∈[T]\{t}] Btj)L(wt) - λ₂(bo_t)ᵀlo_t] + λ₁∑[s∈U\t] ||ws - ∑[j=1]^(i-1) B(j)s*w(j) - Bts*wt||²₂ + α₂∑[j∈[q]] (j - y'ᵢⱼ)² +
α₃∑[j∈[T]] (rank(1)_j - y''_j)² + α₄∑[j∈[T]] (rank(2)_j - y''_j)², và
chúng ta có thể chuyển đổi tối ưu hóa trên bo_t thành vấn đề tối ưu hóa có ràng buộc sau với mục tiêu f(bo_t):

min[bo_t≥0,||bo_t||₁≤radius] f(bo_t): (7)

Trong các thí nghiệm, chúng tôi áp dụng các phương pháp lặp sau
để giải quyết vấn đề tối ưu hóa có ràng buộc (7): vanilla
Frank–Wolfe [13], momentum Frank–Wolfe [1], projected
gradient descent (PGD), PGDMadry [22], và General
Iterative Shrinkage and Thresholding algorithm (GIST) [14].
Trong số đó, vanilla Frank–Wolfe đạt được f(bo_t) nhỏ hơn
trong cùng số lần lặp và hiệu suất hội tụ tốt hơn. Do đó, chúng tôi sử dụng vanilla Frank–Wolfe để
tối ưu hóa bo_t với tốc độ hội tụ tại lần lặp thứ m
là 1/√m, theo [18] và [23]. Xem Phụ lục để biết chứng minh.

Điều chỉnh động các siêu tham số. Chúng tôi coi việc tối ưu hóa mỗi số hạng trong Eq. (6) như một vấn đề học đa nhiệm
và tự động điều chỉnh {αᵢ}⁴ᵢ₌₀ theo
sai số chuẩn của mỗi số hạng qua các epoch lịch sử [16].
Lưu ý rằng Kendal et al. [16] đã rút ra hàm mất mát đa nhiệm
của họ dựa trên việc tối đa hóa likelihood Gaussian với
độ không chắc chắn homoscedastic trên các nhiệm vụ học có giám sát, do
tính phổ biến và sự đơn giản của phương pháp trong [16].
Ở đây chúng tôi điều chỉnh phương pháp này bằng cách tinh chỉnh các mẫu số trong
các hệ số của hàm mất mát để đối phó với thiết lập của chúng tôi.

Cụ thể hơn, tương tự như [16], gọi αᵢ = 1/(4σᵢ²) + ε (i ∈ [4])
và α₀ = 1/(2σ₀²) + ε, trong đó σᵢ là sai số chuẩn của số hạng thứ (i+1)
trong Eq. (6), ε (= 10⁻²) được thêm để tránh giá trị
bằng không của mẫu số, và chúng tôi đặt α₀ là 1/(2σ₀²) thay vì 1/(4σ₀²) để
chú ý nhiều hơn đến hàm mất mát huấn luyện ban đầu. Nhờ
điều chỉnh tự động này, CAMRL của chúng tôi loại bỏ nhu cầu
phân tích siêu tham số tốn công.

4 THÍ NGHIỆM
Chúng tôi so sánh CAMRL của chúng tôi với các thuật toán tiên tiến hiện có trên Gym-minigrid, Meta-world, trò chơi Atari,
Ravens và RLBench, đây là các điểm chuẩn cho RL đa nhiệm được sử dụng rộng rãi trong cộng đồng [4], [8], [35].

4.1 Môi Trường
Gym-minigrid: Các môi trường Gym-minigrid [8] là các môi trường lưới có thể quan sát từng phần bao gồm các nhiệm vụ với
mức độ phức tạp ngày càng tăng. Trong Gym-minigrid, agent
nên tìm một chìa khóa để mở một cửa bị khóa trong một lưới. Bằng cách đặt
các chướng ngại vật khác nhau trên đường đi và thêm các yêu cầu khác nhau, chúng ta thu được các môi trường khác nhau, chẳng hạn như DistShift,
Doorkey, DynamicObstacles, v.v. Mỗi môi trường có thể dễ dàng
được điều chỉnh về kích thước/độ phức tạp, điều này góp phần
tinh chỉnh độ khó của các nhiệm vụ và thực hiện học chương trình giảng dạy. Trong các thí nghiệm của chúng tôi, đầu tiên chúng tôi chọn ngẫu nhiên 9
môi trường, mỗi môi trường sở hữu hơn 3 nhiệm vụ với
độ phức tạp khác nhau. Sau đó, chúng tôi chọn nhiệm vụ có
độ phức tạp lớn nhất cho mỗi môi trường đã chọn và tạo thành
9 nhiệm vụ mà chúng tôi tiến hành thí nghiệm liên quan đến
Gym-minigrid.

Meta-world: Meta-World [35] là một điểm chuẩn mô phỏng
cho meta học tăng cường và học đa nhiệm,
chứa 50 nhiệm vụ liên quan đến thao tác robot. Trong
Meta-World, MT1, MT10 và MT50 là các môi trường học tăng cường đa nhiệm có 1, 10 và 50 nhiệm vụ, tương ứng.
Các nhiệm vụ cho MT10 từ Nhiệm vụ 1 đến Nhiệm vụ 10 là 'Pick and
place', 'Pushing', 'Reaching', 'Door opening', 'Button press',
'Peg insertion side', 'Window opening', 'Window closing',
'Drawer opening' và 'Drawer closing', tương ứng.

Các nhiệm vụ cho MT50 từ Nhiệm vụ 1 đến Nhiệm vụ 50 là 'Turn on
faucet', 'Sweep', 'Stack', 'Unstack', 'Turn off faucet', 'Push
back', 'Pull lever', 'Turn dial', 'Push with stick', 'Get coffee',
'Pull handle side', 'Basketball', 'Pull with stick', 'Sweep into
hole', 'Disassemble nut', 'Place onto shell', 'Push mug', 'Press
handle side', 'Hammer', 'Slide plate', 'Slide plate side', 'Press
button wall', 'Press handle', 'Pull handle', 'Soccer', 'Retrieve
plate side', 'Retrieve plate', 'Close drawer', 'Press button top',
'Reach', 'Press button top w/wall', 'Reach with wall', 'Insert
peg side', 'Push', 'Push with wall', 'Pick & place w/wall',

--- TRANG 6 ---
6
BẢNG 1: Phần thưởng trung bình cho các nhiệm vụ Gym-minigrid (mỗi thí nghiệm lặp lại 5 lần, điểm trung bình với sai số chuẩn (trong ngoặc) được báo cáo). Kết quả tốt nhất được in đậm. Chỉ số càng cao, hiệu suất mô hình càng tốt.

Single AC Single AC+MRCL YOLOR-SAC+MRCL Distral+MRCL Gradient Surgery+MRCL CAMRL+MRCL(ours)
DistShiftEnv 0.05 (0.02) 0.78 (0.05) 0.09 (0.03) 0.07 (0.04) 0.08 (0.02) 0.95 (0.03)
DoorKeyEnv16x16 0.00 (0.00) 0.08 (0.03) 0.01 (0.01) 0.00 (0.00) 0.01 (0.01) 0.13 (0.02)
DynamicObstaclesEnv16x16 -0.99 (0.03) -0.83 (0.04) -0.88 (0.03) -0.98 (0.01) -0.97 (0.02) -0.01 (0.02)
EmptyEnv16x16 0.10 (0.04) 1.10 (0.04) 0.12 (0.03) 0.08 (0.03) 0.11 (0.03) 1.17 (0.05)
FetchEnv 0.07 (0.03) 0.83 (0.05) 0.14 (0.03) 0.09 (0.02) 0.09 (0.03) 0.89 (0.04)
KeyCorridor 0.01 (0.01) 0.02 (0.01) 0.09 (0.03) 0.02 (0.01) 0.08 (0.02) 0.51 (0.02)
LavaCrossingS9N3Env 0.02 (0.01) 0.13 (0.02) 0.02 (0.01) 0.01 (0.01) 0.02 (0.01) 0.04 (0.01)
LavaGapS7Env 0.03 (0.02) 0.53 (0.04) 0.21 (0.02) 0.03 (0.01) 0.05 (0.02) 0.87 (0.02)
MemoryS17Random 0.02 (0.01) 0.61 (0.04) 0.13 (0.02) 0.04 (0.02) 0.20 (0.04) 0.20 (0.03)

BẢNG 2: Phần thưởng trung bình cho các nhiệm vụ MT10. Chỉ số càng cao, hiệu suất mô hình càng tốt.

Single AC Single SAC YOLOR-SAC Distral Gradient Surgery Soft Module CAMRL(ours)
Pick and place 24277.65 (2349.23) 38526.19 (1930.41) 1744.85 (98.42) 12791.42 (938.4) 4347.38 (294.3) 324.91 (40.91) 47235.31 (1043.29)
Pushing -70.93 (10.26) 5.39 (4.93) -10.93 (8.54) -63.57 (4.82) -127.42 (4.25) -53.84 (4.82) 213.27 (10.51)
Reaching -109.39 (9.74) -49.73 (5.91) -53.31 (5.04) -123.61 (6.25) -117.74 (4.02) -38.36 (5.39) -50.36 (2.55)
Door opening -50.31 (8.22) 17.72 (7.14) 10.48 (6.41) -55.41 (4.92) -38.73 (5.49) 13.86 (6.03) 143.74 (10.48)
Button press -28.94 (3.04) 2539.78 (214.49) 409.21 (4.38) -15.87 (3.97) -63.25 (7.93) -9.41 (3.15) 1395.77 (82.91)
Peg insertion side -78.84 (5.21) 25.64 (6.92) 20.31 (4.27) -124.82 (7.92) -79.74 (5.37) -73.74 (6.93) 52.91 (3.81)
Window opening 13.82 (2.48) 1937.28 (102.41) 105.38 (18.49) 11.98 (3.51) 8.93 (3.85) 9.21 (4.14) 15683.29 (1702.03)
Window closing 9.74 (2.31) 8.96 (2.18) 15.87 (4.66) 10.71 (2.04) 12.82 (3.02) 37726.31 (2049.4) 23.64 (3.25)
Drawer opening -19.83 (3.59) 385.72 (39.03) 582.13 (48.93) -17.82 (6.15) -9.32 (3.61) 573.82 (30.98) 1329.39 (28.94)
Drawer closing 1532.98 (234.26) 310.83 (14.09) 193.84 (16.02) -40.86 (12.06) -39.94 (4.83) 611.83 (38.94) 622.74 (10.93)

BẢNG 3: Phần thưởng trung bình cho các nhiệm vụ MT50. Chỉ số càng cao, hiệu suất mô hình càng tốt.

Single AC Single SAC YOLOR-SAC Distral Gradient Surgery Soft Module CAMRL(ours)
Turn on faucet 49832.92 (3903.31) 4793.37 (203.94) 1495.39 (103.47) 7193.81 (102.78) 2893.73 (49.01) 2281.66 (56.93) 7644.89 (61.81)
Sweep -83.03 (20.93) -39.84 (7.93) -94.11 (8.33) -112.93 (13.95) -83.56 (15.03) -18.38 (5.03) -10.31 (3.19)
Stack -110.28 (19.04) -110.38 (8.94) -87.39 (7.37) -152.85 (14.91) -103.95 (7.53) -46.78 (5.82) -42.32 (4.01)
Unstack -47.39 (8.05) -47.17 (9.02) -56.31 (6.49) -74.93 (6.93) -62.38 (10.05) -54.21 (6.18) -31.26 (5.09)
Turn off faucet -50.89 (4.95) -8.38 (3.75) 83.92 (4.28) -12.93 (5.03) -2.31 (4.62) 24.49 (6.83) 1253.73 (21.15)
Push back -52.04 (10.03) -38.93 (6.08) -39.02 (4.39) -122.84 (4.95) -98.53 (7.93) 27.36 (12.05) -27.74 (3.16)
Pull lever -82.94 (10.92) -63.41 (8.38) -68.41 (8.02) -92.38 (7.31) -113.26 (13.85) -69.45 (9.96) -31.84 (7.06)
Turn dial -68.31 (9.83) -50.39 (7.17) -50.37 (8.36) -130.36 (11.03) -58.37 (7.84) -29.32 (4.01) -27.69 (7.68)
Push with stick 703.34 (38.98) -5.81 (1.83) 102.49 (14.07) -15.83 (2.97) -14.92 (4.04) -32.89 (5.81) 1376.83 (38.44)
Get coffee -74.95 (10.95) 72.18 (10.84) -42.46 (9.33) -142.18 (7.89) -40.35 (4.82) -12.96 (7.37) -52.93 (4.91)
Pull handle side -51.59 (10.02) -17.29 (8.05) -59.02 (5.12) 4938.29 (219.77) -62.83 (13.05) -58.72 (14.73) -48.93 (5.89)
Basketball 3295.06 (203.9) 2507.48 (114.85) 1830.94 (88.26) -163.21 (30.46) 4817.61 (248.49) 2194.36 (305.19) 16394.92 (319.85)
Pull with stick -123.59 (8.21) -40.09 (6.73) -38.05 (5.62) -113.48 (7.99) -118.37 (8.38) -38.91 (7.74) -28.92 (5.13)
Sweep into hole -82.83 (8.48) -38.34 (7.92) -10.99 (4.78) 10.76 (8.19) -38.96 (11.01) -28.53 (6.93) 16.74 (5.96)
Disassemble nut 39.09 (7.32) 3947.53 (393.07) 102.58 (7.49) 20.48 (5.68) -3.15 (4.82) 2160.02 (117.83) 1873.93 (79.22)
Place onto shell 11093.45 (302.38) 2.94 (4.02) 183.73 (19.41) -50.31 (5.24) 6973.13 (294.05) 11.97 (5.93) 274.01 (23.91)
Push mug -30.19 (5.31) 40.35 (7.08) -102.29 (9.28) -227.48 (5.39) -93.28 (8.29) 119.93 (16.34) 638.91 (11.34)
Press handle side -127.49 (8.91) -61.31 (5.05) 10.21 (6.77) -173.83 (6.03) -117.31 (10.93) -61.28 (4.91) 24.18 (5.03)
Hammer -74.82 (5.04) -73.29 (4.91) -79.63 (8.29) -147.21 (5.83) -91.22 (6.85) -71.98 (4.05) -60.93 (3.92)
Slide plate -78.25 (7.93) -39.14 (7.03) -0.92 (5.28) -122.84 (10.04) -107.31 (8.86) -33.71 (5.19) 425.64 (10.62)
Slide plate side -81.43 (6.37) -10.28 (3.09) -72.28 (6.99) -103.95 (7.01) -92.18 (10.51) -25.62 (4.97) -23.54 (3.09)
Press button wall -104.18 (6.55) -56.49 (4.72) -67.27 (8.19) -110.38 (4.96) -89.17 (7.31) 64.83 (7.37) -27.73 (4.16)
Press handle -107.74 (7.18) -39.26 (4.27) -68.81 (11.23) -106.91 (4.08) -65.28 (6.83) -48.36 (5.15) -49.18 (4.92)
Pull handle -82.94 (5.03) 34.58 (4.86) -82.17 (7.28) -134.02 (5.82) -77.29 (5.93) -22.16 (4.17) 46.84 (5.86)
Soccer -72.38 (8.93) 429.94 (20.28) 102.36 (9.66) -108.83 (5.37) -9.56 (3.85) 9.68 (3.01) 485.03 (20.75)
Retrieve plate side -87.36 (16.83) -3.91 (4.92) -1.38 (3.94) -105.31 (6.77) -78.93 (6.42) -96.43 (8.93) 127.94 (12.84)
Retrieve plate -116.93 (10.35) -1.63 (5.03) -45.08 (5.42) -123.98 (7.94) -62.18 (7.39) 24.91 (5.06) -42.65 (6.27)
Close drawer -58.81 (5.61) -0.98 (4.75) 29.03 (6.12) -114.05 (7.13) -94.77 (5.74) 31.58 (4.97) 428.93 (29.04)
Press button top -107.47 (7.09) -42.38 (5.38) -30.81 (6.17) -118.37 (4.93) -106.31 (5.28) -37.23 (7.08) -29.84 (4.17)
Reach -101.75 (8.94) -37.01 (4.93) -48.92 (7.55) -121.94 (5.88) -102.67 (6.24) -38.77 (4.03) -23.47 (4.22)
Press button top w/wall -109.28 (5.93) -45.02 (6.02) -60.39 (7.82) -162.38 (6.85) -94.29 (4.91) -37.39 (5.42) -41.84 (4.38)
Reach with wall 0.27 (0.13) 2.97 (1.02) 30.44 (5.18) 20.36 (4.39) 3.68 (1.81) 23.61 (4.95) 897.42 (39.57)
Insert peg side 10.84 (4.05) 280.17 (25.91) 9.39 (3.17) -12.89 (4.03) 237.51 (18.32) 832.85 (68.03) 12.93 (3.94)
Push -33.48 (5.06) -23.65 (4.09) 7.49 (5.04) -64.58 (6.28) -23.87 (4.02) -72.56 (8.92) 573.94 (28.05)
Push with wall -75.82 (8.96) -42.71 (4.62) -49.07 (7.33) -105.37 (6.94) -76.32 (5.35) -48.38 (6.52) 23.81 (4.13)
Pick & place w/wall -53.14 (5.53) -43.95 (4.29) -55.39 (6.31) -89.96 (7.32) -57.94 (8.56) -47.32 (7.44) -33.27 (5.24)
Press button 7795.45 (602.42) 7882.14 (595.06) 109.27 (13.49) -10.46 (4.03) 28.35 (5.91) 9.86 (5.68) 847.93 (37.26)
Pick & place -92.49 (6.75) -40.29 (5.62) -40.88 (7.32) -118.91 (8.52) -84.74 (6.31) -44.58 (4.74) -23.15 (4.51)
Pull mug -37.03 (5.67) -18.93 (4.67) -50.36 (6.58) -104.29 (7.43) -53.27 (5.46) -34.01 (4.09) -3.03 (3.56)
Unplug peg -82.37 (8.53) -73.28 (6.75) -49.13 (8.33) -116.72 (8.52) -117.64 (5.99) -63.28 (4.76) -52.94 (4.73)
Close window -58.32 (5.93) -32.19 (4.14) -40.92 (5.17) -113.96 (6.46) -54.73 (5.35) -28.94 (4.78) -21.19 (4.22)
Open window -143.54 (6.67) -39.65 (5.46) -57.41 (6.35) -78.67 (6.74) -129.86 (7.46) -33.42 (5.91) -27.36 (5.57)
Open door -83.84 (5.35) -64.36 (6.77) -76.57 (10.27) -153.26 (5.04) -98.69 (6.53) -72.91 (5.86) -62.27 (5.02)
Close door -23.95 (6.43) -35.63 (7.93) -34.93 (6.74) -87.69 (5.64) -88.72 (6.08) -27.83 (5.83) -23.01 (4.98)
Open drawer -62.17 (5.59) -28.95 (6.43) -71.49 (8.37) -91.24 (6.61) -67.62 (6.72) -25.98 (5.79) -60.03 (5.63)
Open box -92.43 (5.89) -105.74 (6.75) -41.05 (6.07) -183.28 (7.62) -171.39 (6.35) -67.84 (5.82) -27.09 (5.31)
Close box -99.31 (7.86) -26.93 (6.74) -66.38 (7.31) -163.62 (9.63) -133.86 (6.42) -28.21 (7.03) -47.15 (6.46)
Lock door -107.16 (10.55) -48.69 (9.93) -53.19 (4.19) -73.25 (5.61) -86.32 (5.69) -47.24 (6.45) -45.39 (4.13)
Unlock door -38.43 (4.99) -44.31 (5.42) -41.48 (4.28) -64.59 (5.67) -57.24 (7.53) -46.29 (5.96) -33.06 (6.62)
Pick bin -82.96 (5.64) -28.93 (4.84) -45.38 (5.46) -107.51 (6.32) -39.18 (4.99) -16.27 (5.47) -42.94 (5.14)

--- TRANG 7 ---
7
BẢNG 4: Phần thưởng trung bình cho các nhiệm vụ Atari. Chỉ số càng cao, hiệu suất mô hình càng tốt.

Single AC Single SAC YOLOR-SAC Distral Gradient Surgery Soft Module CAMRL(ours)
YarsRevenge 1208.71 (50.96) 1293.97 (56.52) 982.31 (42.38) 1219.05 (49.64) 1108.92 (52.69) 694.21 (38.01) 1637.94 (42.83)
Jamesbond 7.92 (2.91) 8.12 (2.86) 7.37 (3.02) 7.53 (3.06) 7.51 (2.17) 7.58 (3.42) 8.23 (2.64)
FishingDerby -10.18 (4.03) -11.26 (4.57) -12.33 (4.27) -11.49 (3.98) -10.38 (3.63) -11.56 (4.02) -11.41 (3.71)
Venture 0.02 (0.01) 0.07 (0.02) 0.02 (0.01) 0.04 (0.01) 0.01 (0.01) 0.09 (0.03) 0.22 (0.03)
DoubleDunk -1.39 (0.94) -1.34 (1.02) -1.36 (0.83) -0.96 (0.46) -1.44 (0.58) -1.42 (0.46) -0.89 (0.31)
Kangaroo 6.95 (2.05) 7.21 (3.47) 4.38 (2.12) 6.24 (2.74) 5.21 (2.95) 9.82 (3.84) 5.74 (2.08)
IceHockey -0.46 (0.28) -0.45 (0.31) -0.52 (0.17) -0.51 (0.24) -0.32 (0.26) -0.53 (0.31) -0.49 (0.27)
ChopperCommand 193.91 (19.48) 212.54 (24.95) 217.38 (17.39) 232.54 (22.81) 201.96 (24.04) 242.91 (27.42) 351.68 (29.05)
Krull 12.59 (3.51) 13.24 (4.09) 8.28 (3.27) 8.38 (3.44) 37.82 (8.43) 10.92 (3.49) 9.75 (3.18)
Robotank 0.37 (0.06) 0.37 (0.05) 0.36 (0.04) 0.36 (0.04) 0.35 (0.05) 0.36 (0.05) 0.38 (0.04)

BẢNG 5: Tỷ lệ thành công trung bình cho các nhiệm vụ Ravens. Chỉ số càng cao, hiệu suất mô hình càng tốt.

Single AC Single SAC YOLOR-SAC Distral Gradient Surgery Soft Module CAMRL (ours)
Block-insertion 80.92 (3.94) 87.95 (1.63) 88.31 (2.59) 89.91 (3.47) 91.82 (3.41) 89.95 (6.55) 94.26 (5.37)
Place-red-in-green 84.23 (5.05) 90.32 (6.15) 89.94 (2.08) 91.83 (2.16) 89.18 (3.02) 90.14 (2.98) 93.44 (6.57)
Towers-of-hanoi 86.85 (3.93) 88.47 (2.77) 87.61 (2.76) 88.32 (3.38) 92.37 (2.41) 89.73 (3.34) 95.12 (4.64)
Align-box-corner 84.39 (4.49) 86.49 (2.74) 89.01 (2.46) 87.65 (2.84) 90.13 (2.55) 91.27 (2.83) 93.43 (1.35)
Stack-block-pyramid 79.56 (2.55) 78.52 (4.43) 77.03 (1.98) 77.43 (4.01) 78.91 (3.82) 78.24 (4.28) 77.15 (2.41)
Palletizing-boxes 87.25 (2.29) 90.48 (1.98) 91.38 (2.02) 91.78 (2.44) 91.64 (2.27) 94.49 (2.53) 94.32 (2.63)
Assembling-kits 82.26 (6.31) 85.92 (3.31) 88.48 (2.76) 87.02 (3.89) 88.03 (3.47) 92.91 (2.31) 93.95 (4.36)
Packing-boxes 71.55 (2.69) 75.42 (1.97) 78.21 (3.79) 77.94 (4.02) 77.38 (3.08) 79.21 (3.57) 79.38 (3.01)
Manipulating-rope 83.04 (5.42) 87.06 (5.39) 87.28 (3.47) 87.13 (2.94) 84.26 (4.46) 84.02 (3.59) 89.27 (2.14)
Sweeping-piles 85.61 (2.04) 88.93 (3.63) 89.14 (2.71) 90.15 (2.36) 90.69 (2.35) 90.84 (2.31) 93.64 (3.21)

BẢNG 6: Tỷ lệ thành công trung bình cho các nhiệm vụ RLBench. Chỉ số càng cao, hiệu suất mô hình càng tốt.

Single AC Single SAC YOLOR-SAC Distral Gradient Surgery Soft Module CAMRL(ours)
Reach Target 96.49 (0.83) 96.41 (0.76) 96.57 (0.63) 95.92 (0.67) 95.08 (0.91) 96.31 (0.42) 99.87 (0.05)
Push Button 86.49 (2.47) 87.41 (2.76) 88.57 (3.04) 87.38 (2.54) 84.48 (3.42) 89.48 (2.35) 97.24 (0.85)
Pick And Lift 86.36 (2.02) 86.49 (2.76) 87.03 (1.84) 85.47 (2.63) 85.31 (2.42) 87.21 (2.43) 91.58 (1.54)
Pick Up Cup 81.47 (3.04) 81.84 (2.71) 82.59 (2.46) 83.02 (2.47) 83.48 (1.59) 84.03 (2.41) 86.48 (2.57)
Put Knife on Chopping Board 45.49 (3.19) 44.54 (3.58) 46.61 (3.59) 46.02 (4.47) 45.58 (3.17) 46.94 (3.41) 50.91 (4.48)
Take Money Out Safe 61.48 (3.18) 60.45 (3.82) 61.41 (3.46) 59.48 (3.77) 57.57 (3.28) 59.43 (3.91) 66.84 (3.56)
Put Money In Safe 75.35 (3.17) 74.35 (3.46) 72.59 (5.57) 75.48 (3.51) 73.18 (3.05) 74.13 (3.06) 80.38 (2.51)
Pick Up Umbrella 72.32 (3.31) 73.58 (3.87) 75.03 (2.86) 73.93 (3.01) 76.57 (3.25) 76.43 (2.76) 81.49 (2.95)
Stack Wine 17.49 (1.38) 17.52 (1.75) 20.53 (2.52) 19.57 (3.22) 18.49 (2.63) 17.56 (2.55) 24.59 (2.04)
Slide Block To Target 71.58 (2.59) 73.69 (3.02) 75.57 (2.99) 73.68 (3.57) 71.01 (3.55) 80.17 (3.52) 79.47 (2.57)

'Press button', 'Pick & place', 'Pull mug', 'Unplug peg', 'Close
window', 'Open window', 'Open door', 'Close door', 'Open
drawer', 'Open box', 'Close box', 'Lock door', 'Unlock door',
và 'Pick bin', tương ứng.

Atari: Môi trường Học Atari (ALE) lần đầu được
đề xuất trong [4] bao gồm thám hiểm, lập kế hoạch, chơi phản ứng
và đầu vào thị giác phức tạp [12]. Trong các thí nghiệm của chúng tôi, chúng tôi
chọn ngẫu nhiên 10 nhiệm vụ với quan sát thị giác trong ALE
để thực hiện thí nghiệm, cụ thể là YarsRevenge, Jamesbond,
FishingDerby, Venture, DoubleDunk, Kangaroo, IceHockey,
ChopperCommand, Krull và Robotank.

Ravens: Ravens là môi trường thao tác robot dựa trên thị giác
dựa trên PyBullet. Chúng tôi theo [37] để chọn 10
nhiệm vụ thao tác trên bàn thời gian rời rạc điển hình trong Ravens,
cụ thể là Block-insertion, Place-red-in-green, Towers-of-hanoi,
Align-box-corner, Stack-block-pyramid, Palletizing-boxes,
Assembling-kits, Packing-boxes, Manipulating-rope và
Sweeping-piles. Tương tự như [37], chúng tôi tạo ra 1000
bản demo chuyên gia cho mỗi nhiệm vụ bởi oracle được cung cấp bởi
Ravens và sử dụng những bản demo này để thực hiện học bắt chước
cho mỗi baseline được áp dụng. Sau giai đoạn học bắt chước,
chúng tôi huấn luyện mỗi baseline mà không có bản demo trong
10000 epoch khác và so sánh tỷ lệ thành công trung bình
của mỗi baseline.

RLBench: RLBench là một môi trường quy mô lớn được thiết kế
để tăng tốc nghiên cứu thao tác hướng dẫn bằng thị giác. Chúng tôi theo
[21] để kiểm tra các baseline đã chọn trên 10 nhiệm vụ RLBench
điển hình, tức là Reach Target, Push Button, Pick And Lift, Pick Up
Cup, Put Knife on Chopping Board, Take Money Out Safe,
Put Money In Safe, Pick Up Umbrella, Stack Wine và Slide
Block To Target.

4.2 Baseline
Để chứng minh hiệu quả của thuật toán CAMRL đề xuất,
chúng tôi sử dụng actor critic (AC) [17], soft actor critic
(SAC) [15], mastering rate based online curriculum learning
(MRCL) [30], YOLOR [29], distral [28], gradient surgery [34],
và soft module [32] làm baseline để so sánh. Tiêu chí
chọn baseline phụ thuộc vào việc chúng có phải là các phương pháp thường được sử dụng hay tiên tiến.

Trong số các thuật toán trên, liên quan đến mastering rate based online curriculum learning (MRCL), [30] đã đưa ra
giả định rằng các nhiệm vụ tốt tiếp theo là những nhiệm vụ có thể học được nhưng chưa được học kỹ lưỡng, và do đó
đã giới thiệu một thuật toán mới dựa trên khái niệm mastering
rate để chọn nhiệm vụ nào để huấn luyện tiếp theo một cách trực tuyến. Do thực tế là phần thưởng của các nhiệm vụ trong gym-
minigrid khá thưa thớt, chúng tôi tùy chỉnh một chương trình giảng dạy theo
kích thước lưới cho mỗi nhiệm vụ, áp dụng MRCL để chuyển đổi
các nhiệm vụ con trong mỗi chương trình giảng dạy từ góc độ vi mô, và

--- TRANG 8 ---
8
[Biểu đồ và đồ thị hiển thị kết quả thí nghiệm so sánh hiệu suất của CAMRL với các phương pháp khác trên các dataset khác nhau]

sử dụng các baseline khác để thực hiện huấn luyện đa nhiệm hoặc đơn nhiệm
cho tất cả các nhiệm vụ ở khía cạnh vĩ mô.

4.3 Chỉ Số Đánh Giá
Chúng tôi áp dụng chỉ số phần thưởng trong các môi trường Gym-minigrid, meta-
world và ALE và chỉ số tỷ lệ thành công trong các môi trường Ravens và RLBench. Mặc dù liên quan đến các môi trường meta-world, tỷ lệ thành công
đã được sử dụng trong [32], [34], chỉ số đánh giá này có thể
bằng không rất thường xuyên, dẫn đến phản hồi thưa thớt và
không tốt cho việc so sánh độ khó huấn luyện của các
nhiệm vụ khác nhau. Bên cạnh đó, [32], [34] hiển thị hiệu suất của
thuật toán của họ với tỷ lệ thành công trung bình của tất cả các nhiệm vụ thay vì
tỷ lệ thành công cá nhân cho mỗi nhiệm vụ, điều này làm cho
tỷ lệ thành công ít thông tin hơn so với chỉ số phần thưởng cho mỗi
nhiệm vụ mà chúng tôi áp dụng trong bài báo này. Cuối cùng nhưng không kém phần quan trọng,
các bước mô phỏng trong [34], số lượng mẫu trong [32], và
số epoch trong bài báo này không cùng độ lớn, điều này làm cho
tham chiếu của chúng tôi về tỷ lệ thành công trong [32], [34] ít ý nghĩa hơn. Do đó, trong môi trường meta-world, chúng tôi sử dụng phần thưởng thay vì tỷ lệ thành công trung bình để công thức hóa chỉ số đánh giá, hàm mất mát chính sách trong
Imul và các số hạng hàm mất mát tổng hợp.

4.4 Chi Tiết Triển Khai
Chúng tôi áp dụng cùng một cấu trúc mạng cho mỗi mạng actor đơn
và mạng critic đơn trong mỗi baseline. Đối với soft
module và gradient surgery, chúng tôi theo các tham số
giống như trong các mã công khai của họ. Đối với distral, chúng tôi theo [28] để đặt
β = 0.5 và sử dụng một tập hợp 9 siêu tham số (λ₁, λ₂) ∈ {3×10⁻⁴, 10⁻³, 3×10⁻³} × {2×10⁻⁴, 4×10⁻⁴, 8×10⁻⁴} cho
chi phí entropy và tốc độ học ban đầu. Các siêu tham số với kết quả tối ưu được sử dụng liên quan đến thuật toán distral.

Liên quan đến CAMRL của chúng tôi, chúng tôi đặt a = 1000, b = 1/40, c =
2, d = 200, λ₁ = λ₂ = 0.01, và radius = 0.05 trong tất cả các nhiệm vụ.
Ở cuối mỗi epoch, thay vì thực hiện kiểm tra lẫn nhau
cho mỗi cặp nhiệm vụ không hiệu quả về mặt tính toán, chúng tôi
1) chọn ngẫu nhiên hai nhiệm vụ i và j; 2) kiểm tra hiệu suất
của SACᵢ trên nhiệm vụ j và hiệu suất của SACⱼ trên nhiệm vụ i;
3) cập nhật pᵢ,ⱼ và pⱼ,ᵢ; và 4) lặp lại quy trình kiểm tra lẫn nhau trên 3 lần. Bên cạnh đó, để tránh sự tăng đột ngột
trong hàm mất mát khi chuyển sang chế độ AMTL dựa trên chương trình giảng dạy,
hàm mất mát thực tế cho nhiệm vụ t được đặt là L(wt) cộng với 0.01 lần
mục tiêu trong Eq. (1). Ngoài ra, các thuật toán đề xuất được
chạy trên 5 seed để báo cáo kết quả trung bình.

4.5 Kết Quả
Kết quả trên gym-minigrid. Như được hiển thị trong Bảng 1, trong số
sự hợp tác giữa mastering rate based online curriculum
learning (MRCL) [30] và thuật toán single-task actor critic/distral/-
gradient surgery/CAMRL, CAMRL đạt được
hiệu suất tổng thể tốt nhất trên 9 nhiệm vụ minigrid, mà không
hiển thị dấu hiệu rõ ràng của chuyển giao tiêu cực.

Kết quả trên MT10 và MT50. Như có thể thấy trong Bảng
2, CAMRL vượt trội đáng kể so với tất cả các baseline trên 6 nhiệm vụ trong
số 10 nhiệm vụ trong MT10. Thuật toán tốt thứ hai tổng thể
là single SAC và rõ ràng tệ hơn CAMRL trong 7
nhiệm vụ trong số tất cả 10 nhiệm vụ. Trong Bảng 3, các thuật toán tốt nhất và tốt thứ hai tổng thể là CAMRL và soft module, tương ứng.
Thuật toán sau ít nhất tệ hơn 20% so với CAMRL trong 32 nhiệm vụ trong
số tổng cộng 50 nhiệm vụ và ít nhất tốt hơn 20% so với CAMRL trong 8
nhiệm vụ trong số tất cả các nhiệm vụ. Tóm lại, trong các thí nghiệm hiện tại, CAMRL của chúng tôi hoạt động tốt dù quy mô nhiệm vụ lớn hay nhỏ.
Hơn nữa, nó hiếm khi bộc lộ chuyển giao tiêu cực nghiêm trọng, và
đôi khi có thể đóng góp lớn cho các nhiệm vụ khó
huấn luyện đối với các baseline khác.

Kết quả trên Atari, Ravens và RLBench. Xem hiệu suất của tất cả các thuật toán trên các nhiệm vụ Atari, Ravens và
RLBench đã chọn trong Bảng 4, 5 và 6. Từ Bảng 4, 5 và 6,
chúng ta có thể thấy rằng thuật toán CAMRL của chúng tôi xếp hạng
đầu tiên trong 6 nhiệm vụ trong số tổng cộng 10 nhiệm vụ Atari đã chọn, 8 nhiệm vụ trong số tổng cộng 10 nhiệm vụ Ravens đã chọn, và 9 nhiệm vụ trong số tổng cộng 10 nhiệm vụ RLBench đã chọn. Không tìm thấy chuyển giao tiêu cực đáng kể khi thực hiện CAMRL trong tất cả các nhiệm vụ này.

Phân tích về ma trận chuyển giao B. Như được nêu trong Phần 3,
B là ma trận bất đối xứng T×T đại diện cho số lượng
chuyển giao giữa các nhiệm vụ. Hình 3(a) hiển thị ma trận B - I₁₀×₁₀
trong đó I₁₀×₁₀ là ma trận đơn vị trong R¹⁰×¹⁰, và
Hình 3(c) hiển thị ma trận B - I₁₀×₁₀ của 10
nhiệm vụ đầu tiên trong MT50.

Để khám phá một số quy tắc sâu sắc liên quan đến B, chúng tôi tính toán
PM ∈ R^T×T, ma trận hiệu suất đánh giá lẫn nhau
giữa các nhiệm vụ, và so sánh biến thể PR của nó, một ma trận xếp hạng hiệu suất, với B - I₁₀×₁₀: Đầu tiên, kiểm tra hiệu suất trung bình
của mạng SAC đơn được huấn luyện cho nhiệm vụ s trên 10k episodes
trên nhiệm vụ t, và coi nó là PMₛ,ₜ (s,t ∈ [T]) (đánh giá
được chạy trong 20 episodes); tiếp theo, đối với mỗi nhiệm vụ t ∈ [T], xếp hạng
PMₛ,ₜ (s ∈ [T]) theo thứ tự từ tối thiểu đến tối đa

--- TRANG 9 ---
9
[Biểu đồ và hình ảnh hiển thị các kết quả phân tích và so sánh]

và coi thứ hạng là cột thứ t của ma trận xếp hạng hiệu suất PR ∈ R^T×T. Việc hiển thị PR cho MT10
được thể hiện trong Hình 3(b), và Hình 3(d) hiển thị PR
cho 10 nhiệm vụ đầu tiên trong số 50 nhiệm vụ trong MT50. Số lớn hơn
trong PRₛ,ₜ, hiệu suất đánh giá tốt hơn mà
mạng SAC ban đầu được huấn luyện cho nhiệm vụ s có trên nhiệm vụ
t. Như được hiển thị trong Hình 3, khi PRₛ,ₜ đủ nhỏ, nói
không lớn hơn 2 (xem Hình 3 (b)(d)), Bₛ,ₜ - I^T×T_{ₛ,ₜ} cũng
rõ ràng nhỏ hơn so với các phần tử khác (xem Hình 3 (a)(c)).
Điều này, ở một mức độ nào đó từ phía đối lập, xác nhận
nhu cầu trước đây của chúng tôi rằng hiệu suất đánh giá tốt hơn trên
các nhiệm vụ khác, số lượng chuyển giao lớn hơn trên những nhiệm vụ đó.

Kết hợp kiến thức trước. Để thể hiện khả năng
kết hợp kiến thức trước của CAMRL, chúng tôi
thực hiện thí nghiệm bổ sung công thức hóa một hàm mất mát xếp hạng khả vi mới cho các nhiệm vụ trong đó độ lớn tương đối
của độ khó nhiệm vụ rõ ràng ở một phần. Cụ thể,
chúng tôi theo [36], [36], [26], [37], và [21] để thu được
hiệu suất công khai của các phương pháp tiên tiến hiện tại cho MT10,
MT50, Atari, Ravens và RLBench, tương ứng. Sau đó chúng tôi sử dụng
xếp hạng tương đối của hiệu suất công khai để công thức hóa một
hàm mất mát xếp hạng khả vi dựa trên tanh mới và kết hợp
nó với Eq. (1), với hy vọng rằng đối với nhiệm vụ t ∈ [T], nếu nhiệm vụ i
dễ huấn luyện hơn, tức là, với hiệu suất công khai tốt hơn, thì
nhiệm vụ t có xu hướng thực hiện số lượng chuyển giao nhỏ hơn đến nhiệm vụ
i. Kết quả của việc thêm số hạng hàm mất mát xếp hạng khả vi mới này
được hiển thị trong Bảng 7, 8, 9, 10 và 11. Chúng ta có thể thấy rằng
sau khi thêm số hạng mất mát mới, hiệu suất của CAMRL
vượt trội so với phiên bản gốc trên 5 nhiệm vụ trong số 10 nhiệm vụ trong
MT10, 38 nhiệm vụ trong số 50 nhiệm vụ trong MT50, 8 nhiệm vụ trong số
10 nhiệm vụ trong Atari, 7 nhiệm vụ trong số 10 nhiệm vụ trong Ravens, và
9 nhiệm vụ trong số 10 nhiệm vụ trong RLBench, điều này chứng minh
khả năng kết hợp kiến thức trước của CAMRL cũng như
hiệu suất tuyệt vời của hàm mất mát xếp hạng khả vi trong CAMRL.

Thay đổi của mỗi số hạng trong Imul. Để chứng minh
quá trình chuyển đổi chế độ huấn luyện của CAMRL và hiển thị
sự thay đổi độ lớn của mỗi số hạng trong Imul, chúng tôi vẽ
các thay đổi của Imul và ba số hạng của nó cho CAMRL trong
môi trường MT10. Từ Hình 4 chúng ta có thể thấy rằng độ lớn
của mỗi số hạng trong Imul không khác biệt nhiều và Imul tiếp tục
dao động từ 0.2 đến 0.8, không cho thấy biến động bất thường
và độ lớn quá nhiều.

Phân tích siêu tham số. Kết quả của phân tích siêu tham số
cho CAMRL của chúng tôi trên MT10 được hiển thị
trong Hình 8. Đầu tiên chúng tôi đặt α₀ = 1, λ₁ = λ₂ = α₁ = α₂ =
α₃ = α₄ = 0.01, a = 1000, b = 1/40, c = 2, d = 200, và radius = 0.05. Và chỉ số đánh giá được định nghĩa là phần thưởng trung bình của tất cả 10 nhiệm vụ trên 10k episodes đầu tiên bởi thuật toán CAMRL. Sau đó, chúng tôi
(i) cố định λ₂, λ₁, α₂, α₃, α₄, a, b, c, d, và radius. Đặt tham số λ₁ = 0, 0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1,
tương ứng. Kết quả dưới các giá trị khác nhau của λ₁ được
hiển thị trong Hình 8 (dòng 1);
(ii) lặp lại quy trình tương tự cho αᵢ (i ∈ [4]), a, 1/b, c, d,
và λ₂ như đối với λ₁.

Từ Hình 8(a) chúng ta có thể thấy rằng, đối với αᵢ (i ∈ [2])
và αᵢ (i ∈ [4]), khi tất cả các siêu tham số này ở khoảng 0.01, việc giảm một tham số trong số chúng xuống 0
có thể làm tổn hại hiệu suất. Trong khi đó, nếu một trong
các tham số được tăng quá nhiều, hiệu suất tuyệt vời của
CAMRL có thể không được đảm bảo, điều này có thể do
sự thay đổi đột ngột của hàm mất mát khi chuyển đổi chế độ huấn luyện
và do chuyển giao tiêu cực vượt quá. Tóm lại,
theo đánh giá thí nghiệm trên, đối với thiết kế bất biến theo thời gian của αᵢ (i ∈ [4]), một thiết lập hợp lý, ví dụ,
αᵢ ≈ 0.01, λⱼ ≈ 0.01 cho i ∈ [4], j ∈ [2], thực sự hữu ích trong
việc cải thiện hiệu suất của SAC, so với các tình huống
mà một số siêu tham số được đặt bằng không. Hơn nữa, như
được hiển thị trong Hình 8, khi chúng tôi tự động điều chỉnh αᵢ (i ∈ [4])
dựa trên độ không chắc chắn của mỗi số hạng, hiệu suất của CAMRL
vượt trội so với hầu hết các cấu hình của αᵢ (i ∈ [4]).

Liên quan đến các siêu tham số a, 1/b, c, và d, như có thể
thấy trong Hình 8(b), CAMRL của chúng tôi nhạy cảm nhất
với d, hệ số của số hạng bên trong trong hàm xếp hạng tanh,
điều này chứng minh mạnh mẽ hiệu quả của cơ chế xếp hạng
khả vi tùy chỉnh của chúng tôi. Hơn nữa,
việc tăng a, 1/b, và c đến một giá trị thích hợp cũng
có lợi cho hiệu suất của CAMRL và không cho thấy
dấu hiệu đáng kể của suy giảm hiệu suất do vấn đề độ lớn
của các số hạng trong chỉ số Imul.

Nhận xét. Thực tế, do thiết kế tinh tế của chúng tôi, mỗi số hạng trong Imul
thường sẽ không vượt quá 1/3. Và nếu một trong các số hạng trở nên rất
nhỏ và không hoạt động trong chỉ số, các số hạng khác vẫn sẽ
hoạt động để hiệu suất của CAMRL không suy giảm quá
nhanh khi thực hiện phân tích siêu tham số trên a, b, và c.

Nghiên cứu loại bỏ. Đối với nghiên cứu loại bỏ, trên các nhiệm vụ MT10, chúng tôi
kiểm tra phương pháp của chúng tôi trong trường hợp thiếu thành phần chuyển đổi chế độ
(tức là, hoàn toàn thực hiện học chương trình giảng dạy), và khi
đặt λ₁/a = b = c = d = αᵢ (i = 2,3,4) bằng 0, tương ứng. Ngoài ra, chúng tôi
kiểm tra hiệu suất của AMRL thiếu cơ chế chuyển đổi chế độ
và nhiều hàm xếp hạng so với CAMRL của chúng tôi. Như được hiển thị trong Bảng 12, việc thiếu bất kỳ thành phần
nào của CAMRL và đặt λ₁/a = b = c = d = αᵢ (i = 2,3,4)
bằng 0 hoặc dương vô cùng (để số hạng tương ứng
trong Imul bằng 0) mang lại các mức độ giảm khác nhau trong
hiệu suất thuật toán, điều này nhất quán chứng minh
độ chính xác của thiết kế thuật toán và tầm quan trọng của
sự hợp tác ngầm giữa các thành phần khác nhau. Sự suy giảm hiệu suất rõ ràng khi chỉ thực hiện AMRL và
khi thiếu thành phần chuyển đổi chế độ đặc biệt
cho thấy hiệu quả và hiệu suất của thiết kế
chỉ số Imul và các hàm xếp hạng khả vi tùy chỉnh của chúng tôi.

--- TRANG 10 ---
10
5 THẢO LUẬN
Trong phần này, chúng tôi thảo luận về các ưu điểm của CAMRL so với
các thuật toán MTRL hiện tại, một số làm rõ về CAMRL,
và các cách trực tiếp để cải thiện CAMRL.

5.1 Ưu Điểm So Với Các Thuật Toán MTRL Hiện Tại
Khi có nhiều nhiệm vụ, rất khó tìm ra các đặc điểm chung
được sở hữu bởi tất cả các nhiệm vụ, và do đó, các thuật toán giống như chưng cất có thể hoạt động kém. Ngược lại, ngoài
việc chưng cất các đặc điểm chung của tất cả các nhiệm vụ bằng một mạng lớn,
ma trận B của chúng tôi thăm dò mối quan hệ chuyển giao giữa mỗi
hai nhiệm vụ và cho phép chúng tôi thực hiện chuyển giao bất đối xứng
giữa các nhiệm vụ sao cho CAMRL sẽ không gặp phải vấn đề này.

Đối với học đa nhiệm với paradigm mô-đun, chuyển giao tiêu cực có thể nghiêm trọng vì mối quan hệ nhiệm vụ trong
giai đoạn đầu không chính xác và ảnh hưởng xấu có thể
tích lũy. Hơn nữa, có thể mất thời gian dài để xấp xỉ
nắm bắt mối quan hệ nhiệm vụ chính xác và vào thời điểm
mối quan hệ được học tốt, trong khi hiệu ứng gây ra bởi chuyển giao tiêu cực có thể đã không thể đảo ngược được.
Ngược lại, bằng cách khởi tạo B như một ma trận đơn vị (không
chuyển giao ở đầu), thêm ràng buộc vào B, và
chuyển đổi giữa các chế độ huấn luyện khác nhau, CAMRL không chỉ có thể
giảm thiểu chuyển giao tiêu cực, mà còn học một B tốt ở
giai đoạn tương đối sớm, cho phép việc huấn luyện trên các nhiệm vụ khó
được ảnh hưởng tích cực bởi các nhiệm vụ khác.

Chúng tôi tóm tắt các ưu điểm của CAMRL thành năm khía cạnh.
Đầu tiên, CAMRL linh hoạt: (i) CAMRL có thể tự do chuyển đổi
giữa huấn luyện đơn nhiệm song song và các chế độ AMTL dựa trên chương trình giảng dạy theo chỉ số liên quan đến
tiến trình học. (ii) Các thành phần con của CAMRL
có các biến thể và có thể được kết hợp với các thuật toán baseline RL khác nhau và các chế độ huấn luyện. CAMRL không áp đặt
hạn chế về kiến trúc mạng và chế độ học.
Ví dụ, W có thể là tham số của toàn bộ mạng.
Khi có số lượng lớn nhiệm vụ, chúng ta có thể chia sẻ phần
chính của toàn bộ mạng và để W chỉ là một
tập con nhỏ của mạng để tiết kiệm RAM. Thứ hai, CAMRL
hứa hẹn trong việc giảm thiểu chuyển giao tiêu cực. Một mặt,
bằng cách xem xét ràng buộc 1-norm trên B và áp dụng
Frank-Wolfe để đáp ứng ràng buộc, CAMRL có thể tránh
chuyển giao quá mức ở một mức độ nào đó. Mặt khác, bằng cách tùy chỉnh
và sử dụng các chỉ số về tiến trình học và
hiệu suất của các bài kiểm tra lẫn nhau giữa các nhiệm vụ, CAMRL có thể
giảm bớt chuyển giao tiêu cực. Thứ ba, hàm mất mát của CAMRL
có thể lấy thông tin từ nhiều khía cạnh để cải thiện
hiệu quả của việc huấn luyện các nhiệm vụ RL. Cụ thể, hàm mất mát
trong Eq. (4) xem xét tính dương và thưa thớt của
ma trận chuyển giao, độ khó huấn luyện cho các nhiệm vụ khác nhau,
hiệu suất của các bài kiểm tra lẫn nhau, và sự tương đồng giữa
mỗi hai nhiệm vụ. Nhờ hàm mất mát xếp hạng tùy chỉnh của chúng tôi
cho phép hấp thụ thông tin xếp hạng từng phần, CAMRL
có thể tận dụng tối đa nhiều kiến thức trước và yếu tố huấn luyện hiện có, bất kể số lượng của chúng. Thứ tư,
CAMRL loại bỏ nhu cầu phân tích siêu tham số tốn công. Cụ thể, CAMRL cho phép các siêu tham số
trong hàm mất mát tổng hợp tự động điều chỉnh động và coi
quá trình tự động điều chỉnh như một vấn đề học đa nhiệm.
Một phương pháp học đa nhiệm dựa trên độ không chắc chắn được sử dụng
để cập nhật các siêu tham số tự động. Thứ năm, CAMRL
có thể được tự động điều chỉnh với một nhiệm vụ mới mà không ảnh hưởng đến các nhiệm vụ ban đầu bằng cách thay đổi B thành B' = [B, (0,...,0)ᵀ; (0,...,0), 1].

5.2 Một Số Làm Rõ
Lượng tính toán của đánh giá lẫn nhau. Nếu đánh giá lẫn nhau
giữa các nhiệm vụ yêu cầu một lượng lớn tính toán, chúng ta có thể giảm tần suất đánh giá lẫn nhau
giữa các nhiệm vụ và tái sử dụng kết quả đánh giá giữa mỗi
lần đánh giá. Bên cạnh đó, trong mỗi epoch, chúng ta có thể chỉ
chọn ngẫu nhiên một vài nhiệm vụ để đánh giá lẫn nhau và chỉ tiến hành
xếp hạng từng phần liên quan đến hàm mất mát xếp hạng.

Độ phức tạp của số hạng hàm mất mát. Mặc dù hàm mất mát tùy chỉnh của chúng tôi chứa nhiều số hạng có thể dẫn đến
việc điều chỉnh siêu tham số khó khăn, chúng tôi điều chỉnh công nghệ điều chỉnh siêu tham số tự động trong [16] để xử lý vấn đề trên. Hơn nữa, trong tương lai, có thể có giá trị để khám phá
các yếu tố khác tạo ra sự khác biệt đáng kể hơn đối với
CAMRL và đơn giản hóa số hạng hàm mất mát bằng cách thay thế các yếu tố
hiện tại trong hàm mất mát bằng các yếu tố đáng chú ý nhất.

Hiệu suất của cơ chế cân bằng giữa các hàm mất mát khả vi. Mặc dù chúng tôi áp dụng các hàm mất mát khả vi để
giữ sự căn chỉnh xếp hạng giữa Bt,i (i ∈ [T]) và pt,i (hoặc L(wi)), có nhiều số hạng trong hàm mất mát và do đó chúng tôi phải thỏa hiệp
giữa các số hạng khác nhau. Do đó, khó có thể đạt được
sự căn chỉnh hoàn hảo liên quan đến mỗi hàm mất mát xếp hạng, và để
giảm thiểu chuyển giao tiêu cực, chúng tôi chủ yếu hy vọng rằng Bt,i có thể
nhỏ khi pt,i và L(wi) đủ nhỏ. Thực tế,
như được hiển thị trong Hình 3, khi PRs,t đủ nhỏ, nói
không lớn hơn 2, Bs,t - I^T×T_{s,t} cũng rõ ràng nhỏ hơn so với
các phần tử khác, điều này đáp ứng kỳ vọng của chúng tôi để giảm bớt
chuyển giao tiêu cực nghiêm trọng.

Chi tiết về việc tích hợp MRCL với các baseline. Vì
các nhiệm vụ chúng tôi chọn từ Gym-minigrid có kích thước
hoặc độ phức tạp lớn nhất, việc trực tiếp vượt qua những nhiệm vụ này sẽ rất
khó khăn. Do đó, để hạ thấp rào cản học tập của các nhiệm vụ, chúng tôi
theo thuật toán mastering rate based online curriculum learning
(MRCL) [30] để thực hiện học chương trình giảng dạy cho
mỗi nhiệm vụ đã chọn. Cụ thể, đối với mỗi nhiệm vụ đã chọn i, chúng tôi
lấy nhiệm vụ i và các nhiệm vụ dễ hơn khác trong Gym-minigrid với
cùng thuộc tính như nhiệm vụ i làm chương trình giảng dạy của nhiệm vụ i. Trong
giai đoạn huấn luyện của mỗi nhiệm vụ i, chúng tôi áp dụng MRCL để học
chương trình giảng dạy của nhiệm vụ i càng nhanh càng tốt, với
thuật toán học được đặt là baseline đã chọn của chúng tôi. Tại mỗi bước huấn luyện, MRCL điều chỉnh thích ứng nhiệm vụ tiếp theo trong chương trình giảng dạy của nhiệm vụ i để huấn luyện theo sự chú ý động cho mỗi nhiệm vụ, dưới giả định rằng các nhiệm vụ tốt tiếp theo là những nhiệm vụ có thể học được nhưng chưa được học.
Nhờ cơ chế chú ý tinh tế của MRCL, thuật toán học có thể
vượt qua các nhiệm vụ đã chọn trong Gym-minigrid từng chút một thay vì bị mắc kẹt ở đâu đó trong thời gian dài.

Phương pháp cấu hình siêu tham số. (i) Đối với
các siêu tham số a, b, và c, chúng tôi điều chỉnh chúng theo
độ lớn của các biến trong mỗi mục của
chỉ số Imul, với hy vọng rằng độ lớn tổng hợp
của mỗi mục trong Imul không khác biệt quá xa. (ii) Đối với
siêu tham số d, chúng tôi vẽ các hàm xếp hạng khác nhau với
d khác nhau và chọn d với hàm xếp hạng tương đối mượt
cũng như càng nhỏ càng tốt, vì d nhỏ
có thể đóng góp vào tốc độ hội tụ của vanilla
Frank-Wolfe trong quá trình tối ưu hóa bo_t. (iii) Đối với các siêu tham số αi (i ∈ {0,1,2,3,4}), chúng tôi tự động điều chỉnh
chúng theo phương pháp học đa nhiệm dựa trên độ không chắc chắn trong [16].

5.3 Các Cải Tiến Tiềm Năng
Dưới đây chúng tôi liệt kê một số hướng để CAMRL được cải thiện,
nhằm làm sáng tỏ việc kết hợp thêm AMTL dựa trên chương trình giảng dạy
vào RL.

Xem xét thêm các cách tiếp cận tự động điều chỉnh của các siêu tham số λj (j = 1,2) và αj (j = 1,2,3,4), cũng như
cho phép λj là hàm biến thiên theo thời gian của
tiến trình học và các yếu tố khác.
Làm phong phú hàm mất mát bằng cách tích hợp độ khó trung gian, tính đa dạng, sự bất ngờ và năng lượng [25], v.v.
Thiết kế kỹ thuật để chọn tập con tối ưu của tất cả
các tham số mạng làm W.
Chuyển đổi B thành một hàm phi tuyến và cập nhật nó
với hướng dẫn lý thuyết.
Áp dụng cửa sổ trượt, hệ số chiết khấu theo hàm mũ,
entropy thích hợp, hoặc các thủ thuật khác để giảm thiểu
tính không ổn định trong quá trình huấn luyện AMTL.

6 KẾT LUẬN VÀ CÔNG VIỆC TƯƠNG LAI
Trong bài báo này, chúng tôi đã đề xuất một thuật toán học tăng cường đa nhiệm mới, được gọi là CAMRL (học tăng cường đa nhiệm bất đối xứng dựa trên chương trình giảng dạy). CAMRL
chuyển đổi chế độ huấn luyện giữa đơn nhiệm song song và
AMTL dựa trên chương trình giảng dạy. CAMRL sử dụng một hàm mất mát tổng hợp
để giảm chuyển giao tiêu cực trong học đa nhiệm.
Ngoài việc chính quy hóa chuyển giao đi ra của các nhiệm vụ và sự tương đồng của trọng số mạng, chúng tôi đã giới thiệu ba hàm xếp hạng khả vi vào hàm mất mát để kết hợp linh hoạt nhiều kiến thức trước khác nhau. Một tối ưu hóa luân phiên với Frank-Wolfe cũng đã được sử dụng để tối ưu hóa hàm mất mát, và
một cơ chế điều chỉnh tự động dựa trên độ không chắc chắn của
các siêu tham số đã được áp dụng để loại bỏ việc phân tích siêu tham số tốn công. Chúng tôi đã tiến hành các thí nghiệm mở rộng để
xác nhận hiệu quả của CAMRL và
đã phân tích tính linh hoạt của nó từ nhiều góc độ.

Trong tương lai, chúng tôi dự định nghiên cứu CAMRL với nhiều
hiểu biết lý thuyết hơn và xem xét các cải tiến tiềm năng của nó
được liệt kê trong Phần Thảo luận, chẳng hạn như kết hợp thêm
kiến thức trước vào hàm mất mát, thiết kế phiên bản phi tuyến
của ma trận chuyển giao, và vượt qua tính không ổn định
trong quá trình huấn luyện AMTL.

TÀI LIỆU THAM KHẢO
[1] Constopt-pytorch: a library for constrained optimization built on
pytorch, 2020. https://github.com/GeoffNN/constopt-pytorch.
[2] M. Asada, S. Noda, S. Tawaratsumida, and K. Hosoda. Purposive
behavior acquisition for a real robot by vision-based reinforcement
learning. Machine learning, 23(2-3):279–303, 1996.
[3] T. Bansal, J. Pachocki, S. Sidor, I. Sutskever, and I. Mor-
datch. Emergent complexity via multi-agent competition. CoRR,
abs/1710.03748:1–12, 2017.
[4] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade
learning environment: An evaluation platform for general agents.
Journal of Artificial Intelligence Research, 47:253–279, 2013.
[5] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum
learning. In Proceedings of the 26th annual international conference on
machine learning, pages 41–48, 2009.
[6] E. Brunskill and L. Li. Sample complexity of multi-task reinforce-
ment learning. arXiv preprint arXiv:1309.6821, 2013.
[7] Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich. Grad-
norm: Gradient normalization for adaptive loss balancing in deep
multitask networks. In International Conference on Machine Learning,
pages 794–803. PMLR, 2018.
[8] M. Chevalier-Boisvert, D. Bahdanau, et al. Babyai: A platform to
study the sample efficiency of grounded language learning. In
International Conference on Learning Representations, 2018.
[9] C. Devin, A. Gupta, et al. Learning modular neural network
policies for multi-task and multi-robot transfer. In IEEE International
Conference on Robotics and Automation (ICRA), 2017.
[10] A. Elgharabawy. Preference neural network. arXiv, 2019.
[11] J. L. Elman. Learning and development in neural networks: The
importance of starting small. Cognition, 48(1):71–99, 1993.
[12] L. Espeholt, H. Soyer, et al. Impala: Scalable distributed deep-rl with
importance weighted actor-learner architectures. In International
Conference on Machine Learning, pages 1407–1416. PMLR, 2018.
[13] R. M. Freund and P. Grigas. New analysis and results for the
frank–wolfe method. Mathematical Programming, 2016.
[14] P. Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye. A general iterative
shrinkage and thresholding algorithm for non-convex regularized
optimization problems. In international conference on machine
learning, pages 37–45, 2013.
[15] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine. Soft actor-critic:
Off-policy maximum entropy deep reinforcement learning with a
stochastic actor. In Proceedings of the 35th International Conference
on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pages 1856–1865. PMLR, 2018.
[16] A. Kendall, Y. Gal, and R. Cipolla. Multi-task learning using
uncertainty to weigh losses for scene geometry and semantics.
In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 7482–7491, 2018.
[17] V. R. Konda and J. N. Tsitsiklis. Actor-critic algorithms. In Advances
in neural information processing systems, pages 1008–1014, 2000.
[18] S. Lacoste-Julien. Convergence rate of frank-wolfe for non-convex
objectives. CoRR, abs/1607.00345, 2016.
[19] G. Lee, E. Yang, and S. Hwang. Asymmetric multi-task learning
based on task relatedness and loss. In International Conference on
Machine Learning, pages 230–238, 2016.
[20] H. B. Lee, E. Yang, and S. J. Hwang. Deep asymmetric multi-task
feature learning. In International Conference on Machine Learning,
pages 2956–2964. PMLR, 2018.
[21] S. Liu, S. James, A. J. Davison, and E. Johns. Auto-lambda:
Disentangling dynamic task relationships. arXiv:2202.03091, 2022.
[22] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu.
Towards deep learning models resistant to adversarial attacks.
arXiv preprint arXiv:1706.06083, pages 1–28, 2017.
[23] Y. E. Nesterov. Complexity bounds for primal-dual methods
minimizing the model of objective function. Math. Program., 171(1-
2):311–330, 2018.
[24] A. Pentina, V. Sharmanska, and C. H. Lampert. Curriculum learning
of multiple tasks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 5492–5500, 2015.
[25] R. Portelas, C. Colas, L. Weng, K. Hofmann, and P. Oudeyer.
Automatic curriculum learning for deep RL: A short survey. In
Proceedings of the Twenty-Ninth International Joint Conference on
Artificial Intelligence, IJCAI 2020, pages 4819–4825. ijcai.org, 2020.
[26] C. Shenton. Atari reinforcement learning leaderboard, 2018. https:
//github.com/cshenton/atari-leaderboard.
[27] A. Taylor, I. Dusparic, M. Guériau, and S. Clarke. Parallel transfer
learning in multi-agent systems: What, when and how to transfer?
In International Joint Conference on Neural Networks, IJCNN 2019
Budapest, Hungary, July 14-19, 2019, pages 1–8. IEEE, 2019.
[28] Y. Teh, V. Bapst, W. M. Czarnecki, J. Quan, J. Kirkpatrick, R. Hadsell,
N. Heess, and R. Pascanu. Distral: Robust multitask reinforcement
learning. In Advances in Neural Information Processing Systems, pages
4496–4506, 2017.
[29] C.-Y. Wang, I.-H. Yeh, and H.-Y. M. Liao. You only learn one
representation: Unified network for multiple tasks. arXiv preprint
arXiv:2105.04206, 2021.

--- TRANG 12 ---
12
[30] L. Willems, S. Lahlou, and Y. Bengio. Mastering rate based
curriculum learning. CoRR, abs/2008.06456:1–15, 2020.
[31] Y. Wu and Y. Tian. Training agent for first-person shooter game
with actor-critic curriculum learning. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings, 2017.
[32] R. Yang, H. Xu, Y. Wu, and X. Wang. Multi-task reinforcement
learning with soft modularization. In NeurIPS, 2020.
[33] D. Ye, G. Chen, W. Zhang, S. Chen, B. Yuan, B. Liu, J. Chen, Z. Liu,
F. Qiu, H. Yu, Y. Yin, B. Shi, L. Wang, T. Shi, Q. Fu, W. Yang,
L. Huang, and W. Liu. Towards playing full moba games with deep
reinforcement learning. In Advances in Neural Information Processing
Systems, volume 33, pages 621–632, 2020.
[34] T. Yu, S. Kumar, A. Gupta, S. Levine, K. Hausman, and C. Finn.
Gradient surgery for multi-task learning. In NeurIPS 2020, 2020.
[35] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and
S. Levine. Meta-world: A benchmark and evaluation for multi-task
and meta reinforcement learning. In Conference on Robot Learning
(CoRL), pages 1–18, 2019.
[36] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and
S. Levine. Meta-world: A benchmark and evaluation for multi-task
and meta reinforcement learning. In Conference on Robot Learning,
pages 1094–1100. PMLR, 2020.
[37] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian,
T. Armstrong, I. Krasin, D. Duong, V. Sindhwani, et al. Transporter
networks: Rearranging the visual world for robotic manipulation.
arXiv preprint arXiv:2010.14406, 2020.
[38] Y. Zhang and Q. Yang. A survey on multi-task learning. CoRR,
abs/1707.08114:1, 2017.
[39] Y. Zhang and D.-Y. Yeung. A regularization approach to learning
task relationships in multitask learning. ACM Transactions on
Knowledge Discovery from Data (TKDD), 8(3):1–31, 2014.

Hanchi Huang nhận bằng cử nhân từ Đại học Giao thông Thượng Hải vào năm 2021. Hiện tại cô là sinh viên cao học tại Khoa Khoa học và Kỹ thuật Máy tính, Đại học Công nghệ Nanyang, Singapore. Các lĩnh vực nghiên cứu của cô bao gồm lý thuyết và thuật toán cho học tăng cường, tối ưu hóa tổ hợp và hệ thống gợi ý.

Deheng Ye hoàn thành bằng Tiến sĩ tại Khoa Khoa học và Kỹ thuật Máy tính, Đại học Công nghệ Nanyang, Singapore, vào năm 2016. Hiện tại ông là Nhà nghiên cứu chính và Quản lý Nhóm tại Tencent, Thâm Quyến, Trung Quốc, nơi ông lãnh đạo một nhóm kỹ sư và nhà nghiên cứu phát triển các nền tảng học quy mô lớn và các agent AI thông minh. Ông quan tâm đến học máy ứng dụng, học tăng cường và kỹ thuật phần mềm. Ông đã phục vụ như PC/SPC cho NeurIPS, ICML, ICLR, AAAI và IJCAI.

Li Shen nhận bằng Tiến sĩ từ Khoa Toán học, Đại học Công nghệ Nam Trung Quốc, vào năm 2017. Hiện tại ông là Nhà khoa học Nghiên cứu tại JD Explore Academy, Trung Quốc. Trước đây, ông từng là Nhà khoa học Nghiên cứu tại Tencent, Trung Quốc. Các lĩnh vực nghiên cứu của ông bao gồm lý thuyết và thuật toán cho các vấn đề tối ưu hóa lồi/phi lồi/minimax quy mô lớn, và các ứng dụng của chúng trong học máy thống kê, học sâu, học tăng cường và lý thuyết trò chơi.

Wei Liu (M'14-SM'19) hiện là Nhà khoa học Xuất sắc tại Tencent và Giám đốc Ads Multimedia AI tại Nền tảng Dữ liệu Tencent. Trước đó, ông từng là nhân viên nghiên cứu tại Trung tâm Nghiên cứu IBM T. J. Watson, Hoa Kỳ. Tiến sĩ Liu từ lâu đã cống hiến cho nghiên cứu cơ bản và phát triển công nghệ trong các lĩnh vực cốt lõi của AI, bao gồm học sâu, học máy, thị giác máy tính, nhận dạng mẫu, truy xuất thông tin, dữ liệu lớn, v.v. Cho đến nay, ông đã xuất bản rộng rãi trong các lĩnh vực này với hơn 270 bài báo kỹ thuật được đánh giá ngang hàng, và cũng đã cấp 23 bằng sáng chế Hoa Kỳ. Hiện tại ông phục vụ trong ban biên tập của IEEE TPAMI, TNNLS, IEEE Intelligent Systems và Transactions on Machine Learning Research. Ông là Area Chair của các hội nghị khoa học máy tính và AI hàng đầu, ví dụ: NeurIPS, ICML, IEEE CVPR, IEEE ICCV, IJCAI và AAAI. Tiến sĩ Liu là Fellow của IAPR, AAIA, IMA, RSA và BCS, và là Thành viên Được bầu của ISI.

--- TRANG 13 ---
13
7 PHỤ LỤC
7.1 Kiến Trúc
Đối với các nhiệm vụ không có quan sát thị giác, chúng tôi trực tiếp áp dụng mạng nơ-ron sâu làm mạng actor cũng như mạng critic. Dưới đây là kiến trúc của mạng nơ-ron sâu.

# Kiến trúc của mạng actor:
nn.Sequential(
nn.Linear(n_o, 128),
nn.ReLU(),
nn.Linear(128, 64),
nn.ReLU(),
nn.Linear(64, 32),
nn.ReLU(),
nn.Linear(32, 16),
nn.ReLU(),
nn.Linear(16, n_a)
)

# Kiến trúc của mạng critic:
nn.Sequential(
nn.Linear(n_o, 128),
nn.ReLU(),
nn.Linear(128, 64),
nn.ReLU(),
nn.Linear(64, 32),
nn.ReLU(),
nn.Linear(32, 16),
nn.ReLU(),
nn.Linear(16, 1)
)

trong đó n_o là chiều của quan sát và n_a đại diện cho số lượng hành động tùy chọn cho các nhiệm vụ với không gian hành động rời rạc hoặc chiều của hành động cho các nhiệm vụ với không gian hành động liên tục.

Đối với các nhiệm vụ có quan sát thị giác, chúng tôi thực hiện hai lớp tích chập, self.conv1 và self.conv2, trên quan sát ban đầu, làm phẳng đầu ra bởi self.conv2, và sau đó thu được quan sát được định hình lại. Sau đó, chúng tôi thực hiện cùng một thao tác như các nhiệm vụ không có quan sát thị giác.

# in_channel, out_channel, kennel_size, stride
self.conv1 = nn.Conv2d(3, 1, (3, 4), (12,16))
self.conv2 = nn.Conv2d(1, 1, (3, 4), (3,4))

7.2 Hiển Thị Kết Quả
Chúng tôi vẽ hiệu suất của các baseline đã chọn cho gym-minigrid, MT10, MT50, Atari, Ravens và RLBench trong Hình 6, 7, 9, 10, 11 và 12.

7.3 Hội Tụ của Thuật Toán Vanilla Frank-Wolfe
7.3.1 Phân Tích Thí Nghiệm
Chúng tôi chọn ngẫu nhiên 10 thời điểm thực hiện thuật toán Vanilla Frank-Wolfe khi tối ưu hóa bo_t và hiển thị quá trình tối ưu hóa trong Hình 5. Chỉ số đánh giá là hàm mục tiêu trong Eq. (4) với nhiệm vụ t cố định.

7.3.2 Phân Tích Lý Thuyết
Hệ quả. Ký hiệu x_m là nghiệm được tạo bởi vanilla Frank–Wolfe tại vòng thứ m. Khi đó

min_{1≤j≤m} max_{s≥0,||s||₁≤radius} ⟨∇f(x_m), x_m - s⟩ (8)
≤ max{2h₀, (4f₀(1+λ₂)D₁+λ₁D₂²+2(α₂+α₃+α₄)dT)T}/√m + 1,

trong đó h₀ = f(x₀) - min_{x≥0,||x||₁≤radius} f(x), D₁ là cận trên của {|L(w_i)|}_{i∈[T]}, và D₂ là cận trên 2-norm của các trọng số mạng nơ-ron. Lưu ý rằng, trong các thí nghiệm của chúng tôi, D₂ ≈ 20.

Khi f lồi và khả vi,
min_{1≤j≤m} max_{s≥0,||s||₁≤radius} ⟨∇f(x_m), x_m - s⟩
≤ 8f₀(1+λ₂)D₁+λ₁D₂²+2(α₂+α₃+α₄)dT)T/(m+1): (9)

Tóm lại, tốc độ hội tụ của vanilla Frank–Wolfe trên vấn đề Eq. (7) nằm giữa 1/m và 1/√m.

Nhận xét. max_{s≥0,||s||₁≤radius} ⟨∇f(x_m), x_m - s⟩ là chỉ số đánh giá tiêu chuẩn của vanilla Frank–Wolfe.

Chứng minh. Nhớ lại rằng f(bo_t) = α₀[(1+λ₁∑_{j∈[T]\{t}} B_{tj})L(w_t) - λ₂(bo_t)ᵀlo_t] + λ₁∑_{s∈U\t} ||w_s - ∑_{j=1}^{i-1} B_{(j)s}w_{(j)} - B_{ts}w_t||₂² + α₂∑_{j∈[q]} (j - y'_{ij})² + α₃∑_{j∈[T]} (rank_{j}^{(1)} - y''_j)² + α₄∑_{j∈[T]} (rank_{j}^{(2)} - y''_j)².

Trong các số hạng trong f(bo_t), (1+λ₁∑_{j∈[T]\{t}} B_{tj})L(w_t) - λ₂(bo_t)ᵀlo_t là (1+λ₂)D₁T-Lipschitz đối với bo_t và λ₁∑_{s∈U\t} ||w_s - ∑_{j=1}^{i-1} B_{(j)s}w_{(j)} - B_{ts}w_t||₂² là λ₁D₂²T-Lipschitz đối với bo_t, tương ứng.

Theo [tanh(x)]' = 1 - (tanh(x))², có thể dễ dàng suy ra rằng α₂∑_{j∈[q]} (j - y'_{ij})² + α₃∑_{j∈[T]} (rank_j - y''_j)² + α₄∑_{j∈[T]} (rank_{j}^{(2)} - y''_j)² là 2(α₂+α₃+α₄)dT²-Lipschitz đối với bo_t.

Bằng cách tích hợp các số hạng trên, chúng ta thu được tính chất {f₀(1+λ₂)D₁+λ₁D₂²+2(α₂+α₃+α₄)dT}T-Lipschitz của f(bo_t). Cũng lưu ý rằng đường kính của không gian nghiệm khả thi của bo_t không lớn hơn 1, vì vậy bằng Phương trình 3 trong [18] và Phương trình 3.11 trong [23] chúng ta có thể dễ dàng suy ra hai bất đẳng thức trong Eq. (8) và Eq. (9).

--- TRANG 14 ---
14
BẢNG 7: Phần thưởng trước và sau khi kết hợp kiến thức trước cho MT10.

Xếp hạng hiệu suất bởi soft-module [32] CAMRL không có tỷ lệ thành công trước CAMRL có tỷ lệ thành công trước
Pick and place 9 47235.31 42798.2
Pushing 8 213.27 227.94
Reaching 2 -50.36 -51.28
Door opening 6 143.74 147.64
Button press 4 1395.77 1449.87
Peg insertion side 10 52.91 50.03
Window opening 3 15683.29 14948.5
Window closing 7 23.64 26.97
Drawer opening 5 1329.39 1403.84
Drawer closing 1 622.74 618.76

BẢNG 8: Phần thưởng trước và sau khi kết hợp kiến thức trước cho MT50.

Xếp hạng hiệu suất bởi MT-SAC của meta-world benchmark [36] CAMRL không có tỷ lệ thành công trước CAMRL có tỷ lệ thành công trước
Turn on faucet 1 7644.89 7928.39
Sweep 37 -10.31 -6.38
Stack NA (không tìm thấy) -42.32 -43.21
Unstack NA (không tìm thấy) -31.26 -30.48
Turn off faucet 1 1253.73 1308.41
Push back 34 -27.74 -26.37
Pull lever 36 -31.84 -30.37
Turn dial 1 -27.69 -28.36
Push with stick 37 1376.83 1427.37
Get coffee 31 -52.93 -49.31
Pull handle side 1 -48.93 -49.76
Basketball 37 16394.92 17389.21
Pull with stick 37 -28.92 -26.43
Sweep into hole NA (không tìm thấy) 16.74 17.38
Disassemble nut 37 1873.93 1793.35
Place onto shell NA (không tìm thấy) 274.01 304.74
Push mug NA (không tìm thấy) 638.91 652.48
Press handle side 1 24.18 25.31
Hammer 26 -60.93 -58.42
Slide plate 1 425.64 445.31
Slide plate side 24 -23.54 -24.01
Press button wall 1 -27.73 -26.46
Press handle 1 -49.18 -47.54
Pull handle 1 46.84 47.14
Soccer 31 485.03 472.77
Retrieve plate side 1 127.94 130.58
Retrieve plate 1 -42.65 -44.51
Close drawer 1 428.93 441.93
Press button top 1 -29.84 -24.18
Reach 1 -23.47 -20.45
Press button top w/wall 1 -41.84 -38.53
Reach with wall 1 897.42 910.58
Insert peg side 25 12.93 10.56
Push 30 573.94 570.53
Push with wall 35 23.81 30.51
Pick & place w/wall 37 -33.27 -30.56
Press button 1 847.93 856.25
Pick & place 37 -23.15 -21.16
Pull mug NA (không tìm thấy) -3.03 -2.74
Unplug peg 29 -52.94 -48.61
Close window 1 -21.19 -20.68
Open window 22 -27.36 -29.63
Open door 22 -62.27 -58.64
Close door 1 -23.01 -25.96
Open drawer 27 -60.03 -56.61
Open box 33 -27.09 -24.45
Close box 28 -47.15 -44.58
Lock door 1 -45.39 -40.99
Unlock door 1 -33.06 -36.81
Pick bin 37 -42.94 -40.51

BẢNG 9: Phần thưởng trước và sau khi kết hợp kiến thức trước cho Atari.

Xếp hạng hiệu suất trong Atari Learderboard CAMRL không có phần thưởng trước CAMRL có phần thưởng trước
YarsRevenge 1 1637.94 1684.8
Jamesbond NA (không tìm thấy trong Atari leaderboard [26]) 8.23 7.96
FishingDerby 7 -11.41 -7.43
Venture 5 0.22 0.24
DoubleDunk NA (không tìm thấy) -0.89 -0.75
Kangaroo 3 5.74 5.94
IceHockey 8 -0.49 -0.52
ChopperCommand 2 351.68 374.95
Krull 4 9.75 9.49
Robotank 6 0.38 0.42

--- TRANG 15 ---
15
BẢNG 10: Tỷ lệ thành công trước và sau khi kết hợp kiến thức trước cho Ravens.

Xếp hạng hiệu suất bởi Transporter network [37] CAMRL không có tỷ lệ thành công trước CAMRL có tỷ lệ thành công trước
Block-insertion 2 94.26 94.78
Place-red-in-green 1 93.44 93.73
Towers-of-hanoi 4 95.12 95.34
Align-box-corner 3 93.43 94.01
Stack-block-pyramid 10 77.15 74.24
Palletizing-boxes 5 94.32 94.68
Assembling-kits 7 93.95 93.99
Packing-boxes 9 79.38 79.06
Manipulating-rope 8 89.27 90.24
Sweeping-piles 6 93.64 93.87

BẢNG 11: Tỷ lệ thành công trước và sau khi kết hợp kiến thức trước cho RLBench.

Xếp hạng hiệu suất bởi [21] CAMRL không có tỷ lệ thành công trước CAMRL có tỷ lệ thành công trước
Reach Target 1 99.87 99.93
Push Button 2 97.24 98.02
Pick And Lift 3 91.58 92.04
Pick Up Cup 4 86.48 86.53
Put Knife on Chopping Board 9 50.91 55.46
Take Money Out Safe 8 66.84 66.79
Put Money In Safe 6 80.38 82.44
Pick Up Umbrella 5 81.49 82.76
Stack Wine 10 24.59 26.98
Slide Block To Target 7 79.47 81.46

[Biểu đồ hiển thị hội tụ của thuật toán vanilla Frank-Wolfe để tối ưu hóa bo_t]

BẢNG 12: Nghiên cứu loại bỏ. Ở đây '-' có nghĩa là 'không có', và '+' có nghĩa là 'CAMRL với một thiết lập cụ thể'. a, b, và c là các hệ số của chỉ số Imul. αᵢ (i ∈ [4]) là các hệ số của mỗi số hạng (trừ số hạng đầu tiên) trong Eq. (6).

Phương pháp \ Phần thưởng Pick and place Pushing Reaching Door opening Button press Peg insertion side Window opening Window closing Drawer opening và Drawer closing Phần thưởng trung bình
CAMRL 45052.2 204.3 -53.3 138.5 1382.4 53.3 6764.8 10.3 1321.7 1512.9 5638.7
AMRL 10841.9 10.1 -62.2 30.4 984.2 2.9 5481.7 21.1 104.6 1033.8 1844.9
- chuyển đổi chế độ 18849.3 19.7 -50.4 80.5 1163.8 30.6 5791.4 8.9 498.8 1402.1 2779.5
+ a=dương vô cùng 35318.4 170.4 -55.3 125.5 1303.8 44.1 6191.7 9.2 1184.2 1421.4 4571.3
+ b=dương vô cùng 35024.8 149.8 -58.9 104.8 1284.3 41.7 5938.1 9.1 872.9 1284.5 4465.1
+ c=0 38529.1 192.6 -53.8 134.9 1389.4 47.9 6268.8 10.6 1288.7 1482.9 4929.1
+ d=0 23029.4 21.8 -61.9 94.7 1228.6 36.1 5893.3 8.4 841.2 1465.5 3255.7
+ α₂=0 36931.3 109.4 -57.7 112.9 1255.7 42.6 4193.5 14.9 1201.8 1449.2 4525.4
+ α₃=0 43093.8 215.4 -56.3 129.4 1372.5 47.7 6288.3 12.6 1279.1 1485.6 5386.8
+ α₄=0 44226.7 202.5 -53.8 130.8 1368.4 48.1 6631.2 9.3 1249.5 1474.5 5528.7

--- TRANG 16 ---
16
[Hình ảnh hiển thị các đường cong phần thưởng trung bình của 10K episodes đầu tiên cho các nhiệm vụ Gym-minigrid (mỗi thí nghiệm lặp lại 5 lần). Chỉ số càng cao, hiệu suất mô hình càng tốt.]

[Hình ảnh hiển thị các đường cong phần thưởng trung bình của 10K episodes đầu tiên cho các nhiệm vụ MT10 (mỗi thí nghiệm lặp lại 5 lần). Chỉ số càng cao, hiệu suất mô hình càng tốt.]

[Biểu đồ phân tích siêu tham số của CAMRL với các giá trị khác nhau của các tham số]

--- TRANG 17 ---
17
[Hình ảnh hiển thị các đường cong phần thưởng trung bình của 10K episodes đầu tiên cho các nhiệm vụ MT50 (mỗi thí nghiệm lặp lại 5 lần). Chỉ số càng cao, hiệu suất mô hình càng tốt.]

--- TRANG 18 ---
18
[Hình ảnh hiển thị các đường cong phần thưởng trung bình của 10K episodes đầu tiên cho các nhiệm vụ Atari (mỗi thí nghiệm lặp lại 5 lần). Chỉ số càng cao, hiệu suất mô hình càng tốt.]

[Hình ảnh hiển thị các đường cong phần thưởng trung bình của 10K episodes đầu tiên cho các nhiệm vụ Ravens (mỗi thí nghiệm lặp lại 5 lần). Chỉ số càng cao, hiệu suất mô hình càng tốt.]

[Hình ảnh hiển thị các đường cong phần thưởng trung bình của 10K episodes đầu tiên cho các nhiệm vụ RLBench (mỗi thí nghiệm lặp lại 5 lần). Chỉ số càng cao, hiệu suất mô hình càng tốt.]
