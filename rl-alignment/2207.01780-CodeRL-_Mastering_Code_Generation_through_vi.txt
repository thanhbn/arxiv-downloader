# 2207.01780.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2207.01780.pdf
# Kích thước file: 3779063 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
CodeRL: Làm chủ việc sinh mã thông qua
Các mô hình được tiền huấn luyện và Học tăng cường sâu
Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, Steven C.H. Hoiy
Salesforce Research
https://github.com/salesforce/CodeRL
Tóm tắt
Tổng hợp chương trình hoặc sinh mã nhằm tạo ra một chương trình thỏa mãn một
đặc tả bài toán. Các phương pháp gần đây sử dụng các mô hình ngôn ngữ được tiền huấn luyện
quy mô lớn (LMs) đã cho thấy kết quả đầy hứa hẹn, tuy nhiên chúng có một số hạn chế quan trọng.
Cụ thể, chúng thường tuân theo một quy trình tinh chỉnh có giám sát tiêu chuẩn để huấn luyện một
mô hình sinh mã chỉ từ các cặp mô tả bài toán bằng ngôn ngữ tự nhiên
và các chương trình ground-truth. Mô hình như vậy phần lớn bỏ qua một số tín hiệu quan trọng nhưng
có thể hữu ích trong đặc tả bài toán như các bài kiểm tra đơn vị, điều này
thường dẫn đến hiệu suất kém khi giải quyết các tác vụ mã hóa phức tạp chưa thấy.
Để giải quyết các hạn chế này, chúng tôi đề xuất "CodeRL", một khung làm việc mới cho các tác vụ tổng hợp chương trình thông qua các LMs được tiền huấn luyện và học tăng cường sâu (RL).
Cụ thể, trong quá trình huấn luyện, chúng tôi coi LM sinh mã như một mạng actor,
và giới thiệu một mạng critic được huấn luyện để dự đoán tính đúng đắn chức năng
của các chương trình được sinh ra và cung cấp tín hiệu phản hồi dày đặc cho actor. Trong quá trình suy luận, chúng tôi giới thiệu một quy trình sinh mới với chiến lược lấy mẫu quan trọng
cho phép một mô hình tự động tái sinh các chương trình dựa trên phản hồi từ
các bài kiểm tra đơn vị ví dụ và điểm số critic. Đối với các backbone mô hình, chúng tôi mở rộng
kiến trúc encoder-decoder của CodeT5 với các mục tiêu học tập được cải tiến, kích thước mô hình lớn hơn và dữ liệu tiền huấn luyện tốt hơn. Phương pháp của chúng tôi không chỉ đạt được kết quả SOTA mới trên benchmark APPS đầy thử thách, mà còn cho thấy khả năng chuyển giao zero-shot mạnh mẽ với kết quả SOTA mới trên benchmark MBPP đơn giản hơn.
       Tinh chỉnh RL Actor-Critic với                        Trả về Đặc tả Bài toán 
Một chuỗi là palindrome nếu nó đọc giống nhau từ trái sang phải 
và từ phải sang trái....Nếu có một chuỗi con như vậy trong $s$ mà 
không phải là palindrome, in độ dài tối đa của chuỗi con đó.... 
Ví dụ Đầu vào và Đầu ra:  Đầu vào: 'hannah' Đầu ra: 5 
Chương trình Giải pháp 
Bài kiểm tra Đơn vị 
   Đầu vào: wuffuw Đầu ra: 5
   Đầu vào: iiiiiii Đầu ra: 0...  Môi trường  
Compiler Policy  
LM được tiền huấn luyện (actor) 
Hành động 
Hàm Giá trị 
Critic 
Trạng thái Phần thưởng Giá trị Đặc tả Bài toán 
Bài kiểm tra Đơn vị I/O
Hình 1: Một ví dụ về tác vụ tổng hợp chương trình (Phải): Mỗi tác vụ được định nghĩa bởi một đặc tả bài toán
bằng ngôn ngữ tự nhiên, thường chứa các cặp ví dụ đầu vào và đầu ra. Đầu ra mong đợi là một
chương trình được kiểm tra tính đúng đắn chức năng so với một số bài kiểm tra đơn vị. Tổng quan cấp cao về
khung làm việc CodeRL của chúng tôi cho tổng hợp chương trình (Trái): chúng tôi coi một mô hình ngôn ngữ mã được tiền huấn luyện
(LM) như một chính sách ngẫu nhiên, việc sinh mã như các hành động, và phần thưởng có thể được ước tính dựa trên
kết quả bài kiểm tra đơn vị của các chương trình đầu ra từ compiler (môi trường).
Đóng góp bằng nhau.
yTác giả liên hệ: {hungle, shoi}@salesforce.comarXiv:2207.01780v3  [cs.LG]  3 Nov 2022

--- TRANG 2 ---
Mục lục
1 Giới thiệu 3
2 Công trình Liên quan 4
2.1 Tổng hợp Chương trình . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Học Tăng cường cho Sinh Chuỗi . . . . . . . . . . . . . . . . . . . . . . . 4
2.3 Hoàn thành Chương trình . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3 CodeRL 5
3.1 Tác vụ Tổng hợp Chương trình . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Tiền huấn luyện Mô hình Ngôn ngữ trên Mã . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3 Tổng hợp Chương trình như một Bài toán RL . . . . . . . . . . . . . . . . . . . . . . . . 6
3.3.1 Định nghĩa Return bằng Tín hiệu Bài kiểm tra Đơn vị . . . . . . . . . . . . . . . . . . . . . 7
3.3.2 Return với một Baseline . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3.3.3 Return Trung gian bằng Critic như Bộ dự đoán Lỗi . . . . . . . . . . . . . . . . 8
3.3.4 Sinh Chương trình với Bài kiểm tra Đơn vị Ví dụ và Critic . . . . . . . . . . . . 8
3.4 Chi tiết Cài đặt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4 Thí nghiệm 10
4.1 Thiết lập Thí nghiệm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.2 Benchmarks Tổng hợp Chương trình . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4.3 Kết quả Thí nghiệm trên APPS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
4.4 Nghiên cứu Ablation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
4.5 Kết quả Thí nghiệm trên MBPP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
4.6 Phân tích Định tính . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
5 Hạn chế và Tác động Rộng rãi 19
6 Kết luận 19
A Quy trình Lấy mẫu Critic 24
B Kết quả Thí nghiệm Bổ sung 24
B.1 Kết quả Benchmark CodeXGLUE . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
B.2 Kết quả Benchmark MBPP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
C Phân tích Định tính Bổ sung 27
C.1 Phân tích Thất bại . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
C.2 Ví dụ Chương trình Được sinh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2

--- TRANG 3 ---
1 Giới thiệu
Tổng hợp chương trình hoặc sinh mã là tác vụ thiết kế và xây dựng một chương trình máy tính có thể thực thi
thỏa mãn một đặc tả bài toán (xem Hình 1, phải, để có ví dụ). Nghiên cứu tổng hợp chương trình
đã nhận được nhiều sự chú ý do những tác động đáng kể của nó lên ngành công nghiệp phần mềm, bao gồm
năng suất tốt hơn và khả năng tiếp cận của các công việc và giáo dục lập trình. Phát triển một mô hình AI
có thể tự động sinh các chương trình dựa trên yêu cầu của con người có thể biến đổi đáng kể
các công cụ lập trình và cách chúng ta làm việc với chúng.

Các nỗ lực gần đây sử dụng các phương pháp học sâu, cụ thể là các mô hình ngôn ngữ được tiền huấn luyện
dựa trên Transformer [Vaswani et al., 2017, Brown et al., 2020], ban đầu được thiết kế cho các tác vụ học ngôn ngữ tự nhiên, để sinh các chương trình máy tính độc đáo. Các phương pháp này [Hendrycks et al.,
2021, Chen et al., 2021a, Austin et al., 2021] coi tổng hợp chương trình như một tác vụ sequence-to-sequence,
nhận chuỗi đầu vào là đặc tả bài toán bằng ngôn ngữ tự nhiên và sinh ra một
chuỗi mã làm chương trình đầu ra. Mặc dù những mô hình này đạt được kết quả đầy hứa hẹn, đặc biệt
trong các tác vụ lập trình cơ bản [Chen et al., 2021a, Austin et al., 2021], chúng tôi quan sát thấy chúng vẫn thất bại trong việc
sinh mã để giải quyết các bài toán phức tạp như những bài trong các cuộc thi lập trình [Hendrycks
et al., 2021, Li et al., 2022].

Có hai hạn chế chính. Thứ nhất, các mô hình hiện tại được huấn luyện bằng mục tiêu dự đoán token tiếp theo (NTP) thông thường, tối đa hóa khả năng xảy ra của token ground-truth tiếp theo. Như đã chỉ ra trong các lĩnh vực NLP [Bengio et al., 2015, Ranzato et al., 2016], việc huấn luyện các mô hình chỉ với mục tiêu dự đoán token tiếp theo theo cách "teacher-forcing" thường dẫn đến tích lũy lỗi trong thời gian kiểm tra khi các token được sinh ra bằng cách điều kiện hóa trên các token được lấy mẫu trước đó, không phải các token ground-truth. Vấn đề này trở nên nghiêm trọng hơn trong lĩnh vực tổng hợp chương trình, nơi các điểm số khớp token như BLEU [Papineni et al., 2002, Ren et al., 2020] phù hợp hơn trong các tác vụ tổng hợp chương trình một phần (tức là hoàn thành mã) [Husain et al., 2019] nhưng đã thất bại trong việc đo lường tính đúng đắn chức năng của các chương trình hoàn chỉnh [Hendrycks et al., 2021, Chen et al., 2021a]. Do đó, việc huấn luyện chỉ với mục tiêu NTP không lý tưởng để giải quyết việc sinh chương trình đầy đủ để giải các bài toán lập trình.

Thứ hai, các mô hình hiện tại thất bại trong việc sử dụng các tín hiệu có ý nghĩa tiềm năng từ các bài kiểm tra đơn vị, vốn trực tiếp quyết định hiệu suất mô hình bằng tính đúng đắn chức năng của chương trình. Các phương pháp hiện tại bỏ qua tín hiệu quan trọng này trong quá trình tối ưu hóa mô hình cũng như quy trình sinh. Trong quá trình tối ưu hóa, các bài kiểm tra đơn vị có thể được tính vào các mục tiêu học tập để khớp với mục tiêu cuối cùng là sinh các chương trình đúng về mặt ngữ nghĩa. Trong quá trình suy luận, vì các bài kiểm tra đơn vị thường là các phần của mô tả bài toán (tức là bài kiểm tra đơn vị ví dụ), chúng có khả năng mạnh mẽ để cải thiện thêm các chương trình đầu ra. Các phương pháp liên quan như [Li et al., 2022] sử dụng bài kiểm tra đơn vị ví dụ để lọc và xếp hạng các chương trình đầu ra cuối cùng.
Mặc dù phương pháp này tự nhiên chọn các ứng viên chương trình tốt hơn, nó không cho phép các mô hình cải thiện chương trình dựa trên kết quả bài kiểm tra đơn vị ban đầu (ví dụ).

Để giải quyết các vấn đề trên, chúng tôi giới thiệu "CodeRL", một khung làm việc mới để cải thiện các LMs được tiền huấn luyện cho các tác vụ tổng hợp chương trình thông qua học tăng cường sâu (xem Hình 1, trái, và Phần 3 để biết thêm chi tiết). Cụ thể, chúng tôi đề xuất một chiến lược huấn luyện tối ưu hóa các LMs được tiền huấn luyện cho các tác vụ tổng hợp chương trình trong một phương pháp RL actor-critic [Konda and Tsitsiklis, 1999, Sutton et al., 1999]. Chúng tôi coi LM được tiền huấn luyện như một mạng actor và lấy mẫu tổng hợp các chuỗi từ actor này, bao gồm cả chương trình đúng và sai. Những mẫu chương trình này được chuyển đến một mô hình critic được huấn luyện như một bộ dự đoán lỗi để đánh giá tính đúng đắn chức năng của những mẫu này. Chúng tôi sử dụng các trạng thái ẩn cấp token được trích xuất từ mô hình critic đã học để ước tính các giá trị/điểm số của các token đầu ra của những mẫu tổng hợp này. Mạng actor sau đó được tinh chỉnh trên những mẫu tổng hợp này được cân nhắc theo điểm số critic của chúng.

Trong quá trình suy luận, như một phần của khung làm việc CodeRL, chúng tôi giới thiệu một quy trình sinh mới
khai thác có hệ thống các tín hiệu bài kiểm tra đơn vị ví dụ để cho phép các mô hình cải thiện thêm chương trình.
Thứ nhất, đối với các mẫu vượt qua bài kiểm tra đơn vị ví dụ, chúng tôi sử dụng mô hình critic để lọc và chọn
các chuỗi con. Những chuỗi con này được sử dụng như "hạt giống" điều kiện hóa mô hình để lấy mẫu lại
các token mới và có được các chương trình đầu ra mới. Thứ hai, trong số các chương trình thất bại, critic chọn các chương trình hàng đầu dựa trên khả năng vượt qua bài kiểm tra đơn vị của chúng. Những ứng viên chương trình này được nối với thông tin lỗi nhận được từ compiler và chuyển đến một mô-đun sửa chữa chương trình. Quy trình sinh này cho phép một chiến lược kép để tự động tinh chỉnh và sửa chữa các chương trình đầu ra dựa trên tính đúng đắn chức năng của chúng trong thời gian kiểm tra.
3

--- TRANG 4 ---
Cùng với CodeRL, chúng tôi mở rộng CodeT5 như một mô hình nền tảng với các chiến lược tiền huấn luyện được cải thiện, bao gồm các mục tiêu tiền huấn luyện tốt hơn, kích thước mô hình lớn hơn và dữ liệu tiền huấn luyện lớn. Các thí nghiệm toàn diện của chúng tôi (Phần 4) cho thấy rằng các mô hình của chúng tôi có thể đạt được hiệu suất SOTA trên benchmark APPS đầy thử thách [Hendrycks et al., 2021]. Cụ thể, các mô hình của chúng tôi đạt được hơn 2% pass@1, 6% pass@5, và 20% pass@1000. Vì phương pháp RL của chúng tôi là model-agnostic, chúng tôi cũng áp dụng nó cho các mô hình quy mô lớn khác nhau và đạt được sự cải thiện hiệu suất nhất quán. Chúng tôi tiếp tục kiểm tra khả năng chuyển giao zero-shot của nó trên benchmark MBPP đơn giản hơn [Austin et al., 2021], nơi nó thiết lập kết quả SOTA mới là 63.0% pass@80 vượt qua 61.4% của GPT-137B được tinh chỉnh. Chúng tôi thực hiện phân tích định tính để hiểu các bài toán mà mô hình thành công hoặc thất bại trong việc giải quyết. Cuối cùng, chúng tôi phát hành mô hình CodeT5-large (770M) được cải thiện vượt trội hơn nhiều LMs được tiền huấn luyện có kích thước lớn hơn nhiều.

2 Công trình Liên quan
2.1 Tổng hợp Chương trình
Các tác vụ tổng hợp chương trình có thể truy nguyên từ sớm như việc áp dụng đầu tiên của nghiên cứu học máy [Waldinger and Lee, 1969, Manna and Waldinger, 1971]. Các tác vụ trước đó bao gồm đặc tả bài toán dưới dạng các ví dụ đầu vào-đầu ra (IO) [Summers, 1977, Gulwani et al., 2012] và các phương pháp tổng hợp bị giới hạn ở các phương pháp xác suất [Liang et al., 2010] hoặc các khái niệm lập trình đơn giản [Joulin and Mikolov, 2015, Kurach et al., 2015]. Khi các phương pháp học sâu trở nên phổ biến, các phương pháp sau này áp dụng các mô hình neural để tạo ra các chương trình đầu ra, giả định một bias quy nạp cho đủ số lượng mẫu chương trình [Parisotto et al., 2016, Balog et al., 2016, Devlin et al., 2017].

Gần đây hơn, chúng ta chứng kiến sự xuất hiện của các tác vụ tổng hợp chương trình trong đó các chương trình đầu ra được mở rộng sang các ngôn ngữ lập trình đa năng [Yin and Neubig, 2017, Xu et al., 2018, Chen et al., 2021a] và đặc tả chương trình được mô tả đầy đủ bằng văn bản tiếng Anh tự nhiên [Hendrycks et al., 2021, Austin et al., 2021, Poesia et al., 2022]. Những mở rộng này đã khuyến khích một số lượng ngày càng tăng các ứng dụng của các mô hình ngôn ngữ được tiền huấn luyện (LMs) cho tổng hợp chương trình để khai thác các biểu diễn ngữ cảnh được học từ dữ liệu lớn về mã và ngôn ngữ tự nhiên [Feng et al., 2020, Clement et al., 2020, Wang et al., 2021, Wang and Komatsuzaki, 2021, Chen et al., 2022].
Nijkamp et al. [2022] đề xuất một phương pháp tổng hợp chương trình đối thoại với các mô hình ngôn ngữ được tiền huấn luyện lớn. Mặc dù có kết quả ấn tượng trong các bài toán lập trình cơ bản và triển khai thương mại ban đầu³, các mô hình hiện tại vẫn hoạt động kém khi đối mặt với các bài toán phức tạp như những bài từ các cuộc thi lập trình trên Codeforces [Hendrycks et al., 2021, Li et al., 2022].

Tổng hợp Chương trình trong Bối cảnh Thị giác. Một hướng nghiên cứu liên quan khác là tổng hợp chương trình trong các lĩnh vực thị giác máy tính như hình ảnh và video. Các bài báo đầu như [Kulkarni et al., 2015, Yang et al., 2015] giới thiệu các mạng đồ họa nghịch đảo để suy luận các thuộc tính thị giác như tư thế, hình dạng và ánh sáng của các đối tượng thị giác. Wu et al. [2017], Liu et al. [2019], Ellis et al. [2018] nghiên cứu bài toán render hình ảnh, chuyển đổi hình ảnh thành các biểu diễn có cấu trúc và compact, tức là chương trình cảnh. Tian et al. [2019] mở rộng công trình trước đó để render hình dạng 3D từ hình ảnh thông qua chương trình hình dạng, chứa các tính năng để nắm bắt các priors hình học và cấu trúc. Ganin et al. [2018] giới thiệu một phương pháp dựa trên RL để render hình ảnh thực tế thông qua các chương trình đồ họa cấp cao.
Sun et al. [2018] giới thiệu tổng hợp chương trình từ các video tổng hợp demo để tóm tắt hành vi của các đối tượng trong video.

Mặc dù hướng nghiên cứu này có tác động đáng kể đến các ứng dụng như chỉnh sửa hình ảnh/video, tạo phụ đề và ngoại suy, những phương pháp này bị giới hạn ở các chương trình của ngôn ngữ dành riêng cho lĩnh vực được định nghĩa cho các đối tượng thị giác. Ví dụ, trong [Sun et al., 2018], ngôn ngữ lập trình chứa các hàm cơ bản cho nhận thức đối tượng, hành động và luồng điều khiển. Trong công trình của chúng tôi, chúng tôi tập trung vào tổng hợp chương trình từ đặc tả bài toán ngôn ngữ tự nhiên và các chương trình đầu ra là trong các ngôn ngữ đa năng như Python. Loại tác vụ lập trình này có thể từ các bài toán lập trình cơ bản đến các tác vụ lập trình cấp độ thi đấu đòi hỏi trình độ giải quyết vấn đề cao.

2.2 Học Tăng cường cho Sinh Chuỗi
Liên quan đến các tác vụ tổng hợp chương trình là các lĩnh vực nghiên cứu sinh chuỗi, trong đó các phương pháp RL đã thể hiện những thành tựu đáng kể. Trong những lĩnh vực này, các phương pháp RL được sử dụng để khai thác tín hiệu từ các metric không thể vi phân của tác vụ hiện tại. Công trình đầu như [Ranzato et al., 2016] áp dụng chiến lược này với thuật toán REINFORCE [Williams, 1992] để trực tiếp tối ưu hóa các mô hình cho các metric kiểm tra dựa trên chuỗi như điểm BLEU [Papineni et al., 2002] và ROUGE [Lin, 2004] cho các mô hình dịch. Trong cùng lĩnh vực, Bahdanau et al. [2016] giới thiệu một khung làm việc actor-critic [Sutton, 1984, Konda and Tsitsiklis, 1999]. Trong các lĩnh vực tạo phụ đề thị giác, Rennie et al. [2017], Wang et al. [2018] đề xuất sử dụng RL để tối ưu hóa các mô hình tạo phụ đề hình ảnh bằng các biến thể của điểm CIDEr [Vedantam et al., 2015]. Thay vào đó, Ren et al. [2017] đưa ra một ước tính return hướng mục tiêu mới sử dụng embedding thị giác-ngữ nghĩa. Johnson et al. [2017], Trivedi et al. [2021] giới thiệu sinh chương trình như một tác vụ phụ để học các chính sách có thể diễn giải trong các tác vụ hỏi đáp và điều hướng tổng hợp.

Khác với các lĩnh vực trước đó, trong tổng hợp chương trình, Austin et al. [2021], Chen et al. [2021a], Li et al. [2022] đã chứng minh mối tương quan rất thấp giữa các metric tương tự dựa trên token và tính đúng đắn chức năng của chương trình. Do đó, việc định nghĩa một mục tiêu tối ưu hóa phù hợp trong lĩnh vực này không hề đơn giản. Chúng tôi đề xuất khai thác tín hiệu bài kiểm tra đơn vị, vốn thể hiện trực tiếp tính đúng đắn chức năng của chương trình, trong cả giai đoạn tối ưu hóa mô hình và sinh trong thời gian kiểm tra. Các công trình liên quan hơn đến công trình của chúng tôi là tổng hợp chương trình dựa trên RL [Guu et al., 2017, Bunel et al., 2018, Liang et al., 2018, Zhong et al., 2018] và các phương pháp tổng hợp được hướng dẫn bởi thực thi [Ellis et al., 2019, Chen et al., 2021b]. Tuy nhiên, những phương pháp này bị giới hạn ở các ngôn ngữ lập trình được định nghĩa trong một lĩnh vực ứng dụng cụ thể.

2.3 Hoàn thành Chương trình
Liên quan đến công trình của chúng tôi là nghiên cứu về hoàn thành chương trình tự động hoặc hoàn thành mã. Hoàn thành mã nhằm sinh chương trình được điều kiện hóa trên mã một phần (ví dụ: chữ ký hàm, mã có khoảng trống) và các chương trình đầu ra thường là các đoạn ngắn như các gợi ý mã tiềm năng.
Công trình đầu như [Robbes and Lanza, 2008, Bruch et al., 2009] cho thấy rằng đủ mẫu chương trình và lịch sử chương trình trước đó có thể tạo điều kiện cho các hệ thống hoàn thành mã tốt hơn về mặt liên quan của các gợi ý mã. Raychev et al. [2014], White et al. [2015] giới thiệu các phương pháp dựa trên học sâu bằng cách coi các tác vụ như một bài toán NLP dự đoán xác suất của các token hoặc câu bằng các mô hình ngôn ngữ neural. Svyatkovskiy et al. [2021], Guo et al. [2021] cải thiện các hệ thống hoàn thành mã với chiến lược xếp hạng lại để chọn các ứng viên chương trình tốt hơn và với các dự đoán có cấu trúc để sinh các chương trình đúng về mặt cú pháp hơn. Công trình gần đây như [Clement et al., 2020, Svyatkovskiy et al., 2020] áp dụng các mô hình ngôn ngữ được tiền huấn luyện để khai thác các biểu diễn đã học từ dữ liệu mã nguồn lớn và [Aye et al., 2021] giải quyết hoàn thành mã thực tế.

So với hoàn thành mã, tổng hợp chương trình yêu cầu các hệ thống sinh các chương trình hoàn chỉnh từ đầu và những chương trình này thường được đánh giá bằng tính đúng đắn chức năng của chúng thông qua một số bài kiểm tra đơn vị [Hendrycks et al., 2021, Li et al., 2022]. Trong công trình này, mặc dù chúng tôi tập trung vào tổng hợp chương trình từ các mô tả bài toán tự nhiên, chúng tôi áp dụng một chiến lược tương tự với hoàn thành mã trong quy trình sinh của chúng tôi để cải thiện các chương trình đầu ra.

3 CodeRL
3.1 Tác vụ Tổng hợp Chương trình
Theo phương pháp sequence-to-sequence, tác vụ tổng hợp chương trình chứa một mô tả bài toán làm chuỗi đầu vào D và một chuỗi đầu ra của chương trình ^W = (^w1;:::;^wT);^wt∈V⁴ có thể giải quyết bài toán. Đầu ra tại mỗi bước giải mã t là một phân phối trên từ vựng V, được tính bằng hàm softmax ^wt~softmax(Linear(st)) trong đó st là trạng thái ẩn ngữ cảnh tại bước giải mã t. Theo thông lệ, trong thời gian huấn luyện, các tham số mô hình, θ, được học bằng cách tối đa hóa khả năng xảy ra của các chương trình tham chiếu ground-truth. Ký hiệu W = (w1;:::wT) là chương trình ground-truth, mục tiêu là tối thiểu hóa mất mát cross-entropy:

Lce(θ) = Σt log pθ(W|D) = Σt log[pθ(wt|w1:t−1;D)]                (1)

trong đó xác suất có điều kiện pθ được tham số hóa theo hàm softmax ở trên. Trong thời gian kiểm tra, các mô hình sinh chuỗi chương trình bằng cách lấy mẫu token ^wt một cách tự hồi quy từ

⁴Để đơn giản, chúng tôi sử dụng T như ký hiệu độ dài chuỗi cho tất cả các chuỗi thực tế có thể biến đổi.

5

--- TRANG 5 ---
phân phối p(·|^w1:t−1;D). Các mô hình được đánh giá so với các bài kiểm tra đơn vị tương ứng với bài toán.
Mỗi bài kiểm tra bao gồm một cặp đầu vào và đầu ra ground-truth. Trong các tác vụ tổng hợp chương trình thực tế [Hendrycks et al., 2021], các bài kiểm tra đơn vị ví dụ thường được đưa ra như các phần của đặc tả bài toán.

3.2 Tiền huấn luyện Mô hình Ngôn ngữ trên Mã
Chúng tôi áp dụng các mô hình Transformer làm backbone của các hệ thống tổng hợp chương trình. Cụ thể, bài báo này mở rộng mô hình CodeT5 [Wang et al., 2021] như một mô hình nền tảng cho CodeRL.

CodeT5. CodeT5 [Wang et al., 2021] là một mô hình ngôn ngữ đa ngôn ngữ nhận biết mã được tiền huấn luyện trên corpus mã nguồn quy mô lớn được tuyển chọn từ Github. Với kiến trúc encoder-decoder thống nhất, CodeT5 đạt được hiệu suất tối tân trong một loạt các tác vụ trí tuệ mã trong benchmark CodeXGLUE [Lu et al., 2021] bao gồm cả tác vụ hiểu và sinh mã.

Cải thiện Dữ liệu Tiền huấn luyện. Chúng tôi mở rộng bộ dữ liệu tiền huấn luyện Python bằng cách sử dụng bộ dữ liệu Github Code quy mô lớn được phát hành gần đây⁵. Chúng tôi đã biên dịch thông tin công khai, không cá nhân từ GitHub bao gồm mã Python được cấp phép cho phép (ví dụ: "mit", "apache-2", "bsd-3-clause", "bsd-2-clause", "cc0-1.0", "unlicense", "isc"). Bộ dữ liệu Python kết quả (GCPY) có 10.5B token và lớn gấp 10 lần corpus CodeSearchNet (CSN) [Husain et al., 2019] được sử dụng trong CodeT5 gốc [Wang et al., 2021].

Cải thiện Mục tiêu Tiền huấn luyện. Mặc dù các tác vụ tiền huấn luyện trong CodeT5 như dự đoán span bị che (MSP) có lợi cho các tác vụ hiểu mã, chúng có sự khác biệt lớn với các mục tiêu tổng hợp chương trình. Để giảm thiểu khoảng cách này, chúng tôi giới thiệu một tác vụ tiền huấn luyện dự đoán token tiếp theo (NTP) vào CodeT5. Cụ thể, chúng tôi lấy mẫu đồng nhất một vị trí pivot cho mỗi mẫu mã, sau đó chuyển nội dung trước pivot cho encoder và phần còn lại cho decoder. Để kiểm soát độ dài của chuỗi đầu vào và đầu ra, chúng tôi hạn chế pivot trong khoảng 10% đến 90% của chuỗi gốc.

3.3 Tổng hợp Chương trình như một Bài toán RL
Chúng tôi đề xuất công thức hóa Tổng hợp Chương trình như một bài toán RL (xem Hình 1, trái) và áp dụng một phương pháp RL actor-critic để cải thiện hiệu suất của một LM được tiền huấn luyện bằng cách khai thác các tín hiệu bài kiểm tra đơn vị trong cả quá trình tối ưu hóa mô hình (xem Hình 2) và quy trình sinh (xem Hình 4).

Chính thức hơn, chúng ta có thể xem các tham số đã học của một mô hình LM, θ, như một chính sách ngẫu nhiên πθ, quyết định một hành động như việc dự đoán từng token. Sau mỗi hành động, một mô hình LM cập nhật các biểu diễn trạng thái ẩn của nó được sử dụng bởi chính sách để xác định hành động tiếp theo trong bước giải mã tiếp theo. Ở cuối episode sinh (tức là một token <endoftext> được quan sát), mô hình LM

[THIS IS FIGURE/CHART: Reference to Figure 2 showing an overview of the actor-critic framework to optimize pretrained LMs for program synthesis]

6

--- TRANG 6 ---
nhận được một return r được đo bằng tính đúng đắn chức năng của chương trình được sinh ra. Mục tiêu của việc tinh chỉnh RL là tối thiểu hóa expected return:

Lrl(θ) = E_{Ws~πθ}[r(Ws)]                (2)

trong đó Ws = (ws_1;:::;ws_T) là một mẫu tổng hợp trong đó mỗi token ws_t được lấy mẫu bởi mô hình LM tại bước thời gian giải mã t. Theo thuật toán REINFORCE [Williams, 1992, Sutton and Barto, 2018] và định lý gradient chính sách [Sutton et al., 1999] chúng ta có thể định nghĩa một ước tính của gradient ∇L(θ) của return không thể vi phân r như:

∇Lrl(θ) ≈ E_{Ws~πθ}[r(Ws)∇ log pθ(Ws|D)]
≈ E_{Ws~πθ}[r(Ws)∑_t ∇ log pθ(ws_t|ws_{1:t-1};D)]    (3)

3.3.1 Định nghĩa Return bằng Tín hiệu Bài kiểm tra Đơn vị
Đối với mỗi chuỗi mẫu Ws, return r có thể được định nghĩa một cách heuristic bằng cách kiểm tra tính đúng đắn chức năng của nó. Chúng tôi chuyển các chương trình được sinh ra cùng với các bài kiểm tra đơn vị tương ứng cho compiler. Từ các đầu ra của các bài kiểm tra, chúng ta có thể xác định return r:

r(Ws) = {
-1.0 , nếu Ws không thể được biên dịch (tức là lỗi biên dịch)     (4)
-0.6 , nếu Ws không thể được thực thi với bài kiểm tra đơn vị (tức là lỗi runtime)    (5)
-0.3 , nếu Ws thất bại bất kỳ bài kiểm tra đơn vị nào    (6)
+1.0 , nếu Ws vượt qua tất cả bài kiểm tra đơn vị    (7)
}

Tuy nhiên, trong các lĩnh vực liên quan như nghiên cứu text-to-SQL [Zhong et al., 2018, Xu et al., 2018], chúng tôi lưu ý rằng phương pháp này để ước tính return có thể dẫn đến việc huấn luyện mô hình không ổn định với phương sai cao của ước tính gradient theo Eq. (3) với mini-batches trong huấn luyện.

3.3.2 Return với một Baseline
Để giảm thiểu phương sai này, chúng tôi áp dụng một "baseline" [Sutton and Barto, 2018]. Cụ thể, chúng tôi sử dụng một chiến lược giải mã tham lam làm baseline và bất kỳ mẫu được sinh ra nào vượt trội hơn baseline này được cho ước tính return dương, và ước tính return âm ngược lại. Kỹ thuật chuẩn hóa tương đối này cho phép các mô hình khám phá các chương trình không hoàn hảo, miễn là return của chúng tốt hơn baseline. Cho một mẫu huấn luyện, chúng tôi ký hiệu return của baseline r(Wb) và expected gradient được tính như:

∇Lrl(θ) ≈ E_{Ws~πθ}[(r(Ws) - r(Wb))∑_t ∇ log pθ(ws_t|ws_{1:t-1};D)]    (8)

Lưu ý rằng tại mỗi bước giải mã t, baseline giải mã tham lam của chúng tôi độc lập với hành động ws_t và do đó, số hạng expected gradient ∇Lrl(θ) từ Eq. (3) vẫn giống trong Eq. (8).

3.3.3 Return Trung gian bằng Critic như Bộ dự đoán Lỗi
Chúng tôi quan sát thấy rằng ước tính gradient ở trên chỉ dựa trên một return cuối cùng ở cuối quá trình giải mã. Tuy nhiên, các chương trình thường tuân theo các quy tắc cú pháp cố định trong đó một token duy nhất như một khoảng trắng bổ sung có thể làm cho chương trình bị lỗi. Do đó, Eq. (8) trở nên quá hạn chế.
Một giải pháp đơn giản là sử dụng điểm số tương tự dựa trên token [Papineni et al., 2002, Ren et al., 2020]) giữa mỗi chuỗi con Ws_{1:t} và ground truth. Tuy nhiên, việc khớp mã không phải là một thước đo return lý tưởng do mối tương quan kém với tính đúng đắn chương trình [Hendrycks et al., 2021, Chen et al., 2021a, Austin et al., 2021] chỉ có thể được đo so với các chương trình hoàn chỉnh đầy đủ.

Thay vào đó, chúng tôi giới thiệu một mô hình critic. Hình 3 cho thấy tổng quan về mô hình critic của chúng tôi. Mô hình critic được tham số hóa như một mạng neural với các tham số φ nhận đầu vào là mô tả bài toán D và một chương trình được lấy mẫu Ws = {ws_1;:::;ws_T}. Critic được huấn luyện để suy luận kết quả bài kiểm tra đơn vị; một trong {CompileError;RuntimeError;FailedTest;PassedTest} như được mô tả trong các định nghĩa return trong Eq. (4) đến (7). Mục tiêu huấn luyện của critic có thể được biểu diễn như:

Lcritic(φ) = -log pφ(u|Ws;D)                (9)

trong đó u ký hiệu kết quả bài kiểm tra đơn vị ground-truth được đưa ra bởi compiler sau khi chuyển Ws cho các bài kiểm tra đơn vị tương ứng với bài toán. Chúng tôi sử dụng các mô hình Transformer có kích thước nhỏ hơn mô hình actor làm kiến trúc cơ sở cho mô hình critic. Các trạng thái ẩn ngữ cảnh của các token chương trình (h1;:::;hT) thu được từ decoder mô hình critic được max-pooled theo chiều độ dài chuỗi hpool = Pooling(h1;:::;hT). Dự đoán của critic về kết quả bài kiểm tra đơn vị được tính như û = softmax(Linear(hpool)).

Cho một critic đã học, chúng tôi sử dụng phân phối xác suất v̂t = softmax(Linear(ht)) để ước tính giá trị cấp token q̂ của ws_t liên quan đến đầu ra bài kiểm tra đơn vị ground-truth (lưu ý rằng chúng tôi sử dụng biểu diễn ngữ cảnh cấp token ht ở đây, trước thao tác pooling). Cụ thể, q̂(ws_t) = v̂t[u] trong đó v̂[·] ký hiệu xác suất của một kết quả bài kiểm tra đơn vị cụ thể từ bốn kết quả có thể. Chúng tôi sử dụng ước tính này để huấn luyện mô hình LM actor với return trung gian:

∇Lrl(θ) ≈ E_{Ws~πθ}[(r(Ws) - r(Wb))∑_t q̂(ws_t)∇ log pθ(ws_t|ws_{1:t-1};D)]    (10)

Lưu ý rằng vì mô hình critic của chúng tôi được áp dụng trong môi trường học có giám sát với ground truth có sẵn, chúng tôi cũng sử dụng các mẫu huấn luyện của các chương trình đầu ra hoàn hảo W và gán chúng với kết quả bài kiểm tra mặc định u = PassedTest để huấn luyện critic.

3.3.4 Sinh Chương trình với Bài kiểm tra Đơn vị Ví dụ và Critic
Chúng tôi tận dụng các bài kiểm tra đơn vị được cung cấp trong mô tả bài toán đầu vào để cải thiện quy trình sinh trong quá trình suy luận cũng (xem Hình 4 để tổng quan và Phụ lục A để giải thích từng bước). Đối với mỗi bài toán, chúng tôi sinh N chương trình, mỗi chương trình được chuyển cho các bài kiểm tra đơn vị ví dụ thường được nhúng như các phần của đặc tả bài toán. Trong số N chương trình, chúng tôi ký hiệu những chương trình vượt qua bài kiểm tra ví dụ như một tập P và các chương trình thất bại còn lại như một tập F.

Tinh chỉnh Chương trình. Lưu ý rằng mặc dù các chương trình trong P thành công vượt qua bài kiểm tra ví dụ, không được đảm bảo rằng những chương trình này sẽ thành công so với các bài kiểm tra ẩn cuối cùng. Các bài kiểm tra ẩn thường toàn diện hơn và có thể chứa các trường hợp góc thách thức những chương trình này. Do đó, chúng ta có thể áp dụng một vòng sinh khác để tinh chỉnh thêm các chương trình.

Cụ thể, chúng tôi sử dụng các chuỗi con từ những mẫu chương trình này từ P làm prompts (hoặc chuỗi "hạt giống") cho LM actor. Chúng tôi sử dụng một mô hình critic riêng biệt (φtest) để hướng dẫn lựa chọn các chuỗi con từ những mẫu được lọc này. Mô hình critic này được huấn luyện với mục tiêu tương tự như Eq. (9), nhưng trong thiết lập phân loại nhị phân với nhãn {FailedTest;PassedTest}. Gọi Wpass = {w1;:::;wT} ký hiệu một mẫu được sinh ra vượt qua bài kiểm tra đơn vị ví dụ. Chúng tôi sử dụng mô hình critic để gán một giá trị cho mỗi token:

q̂test(wt) = pφtest(û = PassedTest|w1:t;D)                (11)

8

--- TRANG 7 ---
tương ứng với xác suất dự đoán của critic về chuỗi con đến t vượt qua bài kiểm tra đơn vị.
Chúng tôi chia chuỗi tại vị trí tmax tương ứng với giá trị cao nhất được gán bởi critic và sử dụng
phần chia bên trái làm hạt giống cho giai đoạn tiếp theo. Nếu chuỗi hạt giống này đến tmax chứa một token với
pφtest(FailedTest) > pφtest(PassedTest), chúng tôi tiếp tục cắt bỏ nó tại token này bằng cách loại bỏ các token bên phải. Điều này được thực hiện để chọn các prompts có khả năng sinh ra các chương trình thành công trong vòng tiếp theo.
Chúng tôi sử dụng những hạt giống này để khởi tạo và điều kiện hóa LM (actor) để lấy mẫu lại các token mới cho đến khi chúng tôi gặp token <endoftext>. Trong vòng này, mỗi chuỗi hạt giống có thể được xếp chồng N/|P| lần để upsampling.
Điều này dẫn đến cùng số lượng chương trình đầu ra N như trong vòng sinh đầu tiên. Cuối cùng, chúng tôi đánh giá những N chương trình được tinh chỉnh này so với các bài kiểm tra đơn vị ẩn.

Sửa chữa Chương trình. Sinh chương trình để giải quyết một bài toán, đặc biệt là một bài toán lập trình cấp độ thi đấu, bao gồm một không gian tìm kiếm lớn của các chương trình có thể. Rất thường, chúng tôi quan sát thấy thất bại hoàn toàn khi tất cả các chương trình thất bại so với bài kiểm tra ví dụ, tức là |F| = N. Do đó, đối với những trường hợp này, chúng tôi sử dụng một bước sinh bổ sung để đầu tiên sửa chữa chương trình trước khi tinh chỉnh chúng.

Cụ thể, chúng tôi sử dụng cùng mô hình critic cũng được sử dụng trong tinh chỉnh chương trình, để lấy mẫu các ứng viên hàng đầu từ tập F. Gọi Wfail ký hiệu một mẫu được sinh ra thất bại với bài kiểm tra đơn vị ví dụ. Chúng tôi sử dụng mô hình critic để gán một giá trị cho mẫu này:

q̂test(Wfail) = pφtest(û = PassedTest|Wfail;D)                (12)

tương ứng với xác suất dự đoán của critic về chương trình vượt qua bài kiểm tra đơn vị. Chúng tôi chọn M chương trình hàng đầu với xác suất cao nhất và chuyển chúng cho một mô hình sửa chữa chương trình ω.

Mô hình sửa chữa chương trình này được thiết kế như một mô hình sinh sequence-to-sequence. Chuỗi đầu vào là sự nối của mô tả bài toán D và chương trình lỗi Wfail. Chúng tôi cũng bao gồm các tín hiệu bổ sung nhận được từ kết quả bài kiểm tra đơn vị, bao gồm loại kết quả bài kiểm tra (như được định nghĩa trong các định nghĩa return trong Eq. (4) đến (7), và các loại phụ lỗi (ví dụ: lỗi cú pháp, lỗi out-of-index). Các loại lỗi được trích xuất từ các traces lỗi được trả về bởi compiler. Để huấn luyện mô hình sửa chữa chương trình, chúng tôi khai thác các mẫu tổng hợp ban đầu được sử dụng trong huấn luyện RL của chúng tôi, như các chương trình lỗi Wfail = Ws. Chương trình ground-truth W có thể được sử dụng như chương trình đúng mong đợi.
Mục tiêu huấn luyện của mô hình sửa chữa chương trình là tối thiểu hóa mất mát cross-entropy:

Lrepair_ce(ω) = Σt log pω(W|D;Wfail;u;c) = Σt log[pω(wt|w1:t−1;D;Wfail;u;c)]    (13)

trong đó u là một trong {CompileError;RuntimeError;FailedTest;PassedTest} và c là loại phụ lỗi. Trong thời gian kiểm tra, mỗi chuỗi thất bại được chọn có thể được xếp chồng N/M lần để upsampling.
Điều này dẫn đến cùng số lượng chương trình đầu ra N như trong vòng sinh đầu tiên. Cuối cùng, chúng tôi chuyển những N chương trình được sửa chữa này và áp dụng quy trình tinh chỉnh mã như trước.

Lấy mẫu Critic. Chúng tôi gọi chiến lược kép của sửa chữa chương trình và tinh chỉnh là "Lấy mẫu Critic" (CS). Chiến lược kép này cho phép các mô hình sinh và cải thiện chương trình trong quá trình suy luận, cả từ trường hợp thành công (tinh chỉnh chương trình), và từ trường hợp thất bại (sửa chữa chương trình). Trong thực tế, chúng tôi sử dụng sinh mini-batch để cải thiện hiệu quả trong quá trình suy luận và sử dụng nucleus sampling với kích thước batch N = 200. Lưu ý rằng trong quá trình tinh chỉnh chương trình, mặc dù chúng tôi phát sinh thêm chi phí tính toán để lấy mẫu lại bằng các chuỗi hạt giống, chúng tôi chỉ cần sinh các chương trình một phần trong giai đoạn tái sinh, làm cho giai đoạn này ít tốn kém hơn so với sinh thông thường.

3.4 Chi tiết Cài đặt
Do số lượng trajectories có thể lớn (tức là V^T) để sinh một chuỗi và vòng phản hồi không ổn định giữa actor và critic [Lillicrap et al., 2015, Wang et al., 2018], chúng tôi áp dụng imitation learning để đầu tiên khởi động một mô hình LM được tiền huấn luyện chỉ với Lce trong tối đa 10 epochs. Sau đó chúng tôi lấy mẫu các chuỗi chương trình từ mạng actor này để huấn luyện critic trong khi giữ các tham số của mạng actor cố định. Đối với các thí nghiệm với các mô hình actor CodeT5, chúng tôi sử dụng kiến trúc CodeT5-small cho mô hình critic, và kiến trúc critic GPT2-small khi các mô hình actor là các biến thể GPT. Theo [Bahdanau et al., 2016], vì phương pháp RL của chúng tôi được áp dụng trong một tác vụ học có giám sát, ngoài các chương trình tổng hợp, chúng tôi cũng sử dụng các chương trình ground-truth của các mẫu huấn luyện để huấn luyện critic. Những mẫu này được coi là các chương trình hoàn hảo và luôn có nhãn PassedTest.
Sau khi huấn luyện critic, chúng tôi sau đó áp dụng cả Lce và Lrl với trọng số bằng nhau để tinh chỉnh mạng actor. Để tối ưu hóa mạng LM actor, trong thực tế, theo công trình trước đó [Bahdanau et al., 2016, Rennie et al., 2017, Wang et al., 2018], trong mỗi bước tối ưu hóa huấn luyện, chúng ta có thể đơn giản xấp xỉ expected gradient với một mẫu đơn Ws ~ πθ:

∇Lrl(θ) ≈ (r(Ws) − r(Wb))Σt q̂(wst)∇ log pθ(wst|ws1:t−1;D)    (14)

4 Thí nghiệm
4.1 Thiết lập Thí nghiệm
Tiền huấn luyện. Chúng tôi tiền huấn luyện một mô hình CodeT5-large (770M) từ đầu theo kiến trúc của T5-large [Raffel et al., 2020]. Chúng tôi tuân theo thiết lập tiền huấn luyện trong CodeT5 [Wang et al., 2021] với các sửa đổi như đề xuất trong Phần 3.2. Cụ thể, chúng tôi áp dụng tokenizer dành riêng cho mã từ Wang et al. [2021]. Chúng tôi sử dụng 6 ngôn ngữ lập trình (PLs) trong CodeSearchNet [Husain et al., 2019] (CSN) thay vì 8 PLs trong CodeT5 vì các dataset C/C# không có sẵn công khai. Chúng tôi chỉ áp dụng tác vụ tiền huấn luyện masked span prediction (MSP) từ [Wang et al., 2021] và do đó, không phải parse các chương trình thành abstract syntax trees (ASTs) để có được thông tin identifier.
Bước tiền xử lý cuối cùng được yêu cầu trong các tác vụ tiền huấn luyện gốc khác như masked identifier prediction [Wang et al., 2021]. Để tăng tốc thêm việc huấn luyện, chúng tôi nối các mẫu dữ liệu thành kích thước batch 512 cho tiền huấn luyện với MSP và số lượng token kết quả là 1.1B. Để xác thực lợi ích của việc sử dụng CodeT5 được tiền huấn luyện mới này như mô hình nền tảng của chúng tôi, chúng tôi đánh giá mô hình này trên CodeXGLUE [Lu et al., 2021] và đạt được kết quả SOTA mới (xem Phụ lục B.1).

Chúng tôi thực hiện các thí nghiệm của mình trên kubernetes với 16 GPU A100-40G trên Google Cloud Platform và tổng thời gian tiền huấn luyện khoảng 21 ngày. Trong giai đoạn tiền huấn luyện đầu tiên với MSP, chúng tôi sử dụng tỷ lệ corruption 15%, learning rate (LR) đỉnh 2e-4, và kích thước batch 2048. Chúng tôi tiền huấn luyện trên CSN trong 150 epochs (~10 ngày) và sau đó trên GCPY trong 10 epochs (~5 ngày). Đối với giai đoạn tiền huấn luyện thứ hai với NTP, chúng tôi áp dụng LR đỉnh 1e-4 và kích thước batch 256, và tiền huấn luyện trong 10 epochs (6 ngày). Chúng tôi đặt độ dài tối đa là 768 và 600 cho chuỗi nguồn và đích tương ứng cho mục tiêu này. Đối với tất cả các thí nghiệm, chúng tôi sử dụng optimizer AdamW [Loshchilov and Hutter, 2019] với weight decay 0.05 và bộ lập lịch LR decay tuyến tính với bước warmup 1000.

Đánh giá. Chúng tôi tuân theo [Hendrycks et al., 2021, Chen et al., 2021a] và đánh giá các mô hình bằng metric pass@k, là tỷ lệ phần trăm các bài toán được giải quyết bằng cách sử dụng k chương trình được sinh ra cho mỗi bài toán. Chúng tôi cũng tuân theo [Li et al., 2022] và sử dụng metric n@k chỉ xem xét một tập con n ứng viên từ k chương trình được sinh ra cho mỗi bài toán. Tập con n ứng viên thường được chọn bằng phương pháp lọc bằng cách chuyển các chương trình được sinh ra qua bài kiểm tra ví dụ được đưa ra như phần của mô tả bài toán [Chen et al., 2021a, Li et al., 2022]. Lưu ý rằng trong quy trình lấy mẫu critic của chúng tôi, chúng ta có thể thực hiện nhiều vòng để tinh chỉnh/sửa chữa các chương trình dựa trên kết quả bài kiểm tra ban đầu của chúng. Trong thực tế, chúng tôi giới hạn tối đa một vòng sửa chữa và/hoặc tinh chỉnh.

4.2 Benchmarks Tổng hợp Chương trình
Benchmark APPS. Chúng tôi chọn benchmark tổng hợp chương trình APPS đầy thử thách [Hendrycks et al., 2021], vì nó có các bài toán mã hóa lớn với độ khó khác nhau được thu thập từ nhiều trang web mã hóa. APPS bao gồm 10,000 bài toán mã hóa với chia train-test 50-50. Mỗi bài toán đi kèm với 23.2 chương trình Python đúng và 21.2 bài kiểm tra đơn vị trung bình. Độ dài trung bình mỗi bài toán là 293.2 từ và độ dài trung bình mỗi chương trình là 18.0 dòng. Dataset được phân loại thành ba cấp độ khó: Introductory (3639, train/test=2639/1000), Interview (5000, train/test=2000/3000), và Competition (1361, train/test=361/1000). Mỗi mẫu bao gồm 20 bài kiểm tra đơn vị trung bình để xác thực tính đúng đắn chức năng của chương trình. Chúng tôi tuân theo cùng bước tiền xử lý như [Hendrycks et al., 2021] để công thức hóa các chuỗi đầu vào từ mô tả bài toán.

Trên APPS, chúng tôi tinh chỉnh CodeT5 được tiền huấn luyện của chúng tôi theo khung làm việc CodeRL (Phần 4.3). Để khởi động các mô hình CodeT5 với Lce, chúng tôi áp dụng kích thước batch 64 và warmup LR từ 0 đến 2e-5 trong 500 bước đầu tiên và decay theo đa thức (power=0.5) đến 1e-5 cho đến hết 10 epochs, mất khoảng 30 giờ trên một GPU A100. Chúng tôi đặt độ dài chuỗi nguồn và đích tối đa lần lượt là 600 và 512.

Benchmark MBPP. Chúng tôi bổ sung thêm một dataset tổng hợp chương trình Python nhỏ hơn và đơn giản hơn gọi là MBPP [Austin et al., 2021] (Mostly Basic Programming Problems) để đánh giá. Dataset chứa 974 instances với 374/90/500 instances cho training/validation/testing tương ứng và 10 được dành riêng cho few-shot learning. Các bài toán thường ngắn, thường là mỗi câu mô tả ngôn ngữ tự nhiên. Mỗi bài toán đi kèm với 1 giải pháp đúng (trung bình 6.8 dòng mã) và 3 bài kiểm tra đơn vị dưới dạng các câu lệnh assert để xác thực tính đúng đắn chức năng. Không giống APPS, các bài kiểm tra đơn vị trong MBPP không bị ẩn và được kết hợp rõ ràng vào các chuỗi nguồn cho các mô hình tổng hợp chương trình. Điều này có thể khuyến khích các mô hình overfitting với những câu lệnh assert này thông qua hard-coding một biểu thức if thỉnh thoảng. Tuy nhiên, để so sánh công bằng với các baseline, chúng tôi xây dựng các chuỗi nguồn theo cách tương tự như công trình trước đó.
Cụ thể, chúng tôi áp dụng cùng định dạng prompt như [Austin et al., 2021] để chuẩn bị chuỗi đầu vào như: mô tả bài toán + "Your code should satisfy these tests:" + 3 câu lệnh assert.

Trên MBPP, chúng tôi thí nghiệm trong cả thiết lập zero-shot (Phần 4.5) và tinh chỉnh đầy đủ (Phụ lục B.2). Để tinh chỉnh CodeT5, do tập huấn luyện nhỏ của MBPP, chúng tôi tinh chỉnh các mô hình trong 60 epochs với LR không đổi 2e-5 và kích thước batch 32, mất ít hơn 30 phút trên một A100. Chúng tôi đặt độ dài nguồn và đích tối đa lần lượt là 382 và 306.

4.3 Kết quả Thí nghiệm trên APPS
Baselines. Như được báo cáo bởi Hendrycks et al. [2021], chúng tôi so sánh các mô hình của mình với một số baselines, bao gồm GPT2 [Radford et al., 2019], GPT-Neo [Black et al., 2021], và GPT3 [Brown et al., 2020].
Chúng tôi cũng so sánh kết quả với Codex [Chen et al., 2021a] và AlphaCode [Li et al., 2022]. Lưu ý rằng theo mặc định, kết quả của các LMs được tiền huấn luyện (ngoại trừ Codex và GPT3) là từ các mô hình được tinh chỉnh trên APPS chỉ sử dụng mất mát tiêu chuẩn Lce. Trong các ablations của chúng tôi, vì CodeRL là model-agnostic, chúng tôi cũng có thể tích hợp nó với các biến thể GPT như GPT-J [Wang and Komatsuzaki, 2021] và GPT-Neo.

11

--- TRANG 11 ---
Bảng 1: Kết quả trên benchmark APPS: Nhìn chung, CodeRL có thể mang lại sự cải thiện hiệu suất của các mô hình CodeT5 và đạt được SOTA mới trên cả metric pass@k và n@k. "Intro": introductory, "Inter": interview, "Comp": competition-level tasks.

(a) Hiệu suất theo pass@k với k={1;5;1000}
[Bảng hiển thị kết quả với các cột Model, Size, và hiệu suất pass@1, pass@5, pass@1000 cho Intro, Inter, Comp, All]

(b) Hiệu suất theo n@k với k lên đến 50000 và n={1;5}
[Bảng hiển thị kết quả với các cột Model, Size, k và hiệu suất 1@k, 5@k cho Intro, Inter, Comp, All]

Kết quả Tổng thể. Thứ nhất, Bảng 1a cho thấy rằng CodeRL với mô hình CodeT5 có thể đạt được sự cải thiện hiệu suất đáng kể, vượt trội hơn nhiều LMs được tiền huấn luyện có kích thước lớn hơn nhiều. Cụ thể, phương pháp của chúng tôi đạt được kết quả SOTA mới là 2.69% pass@1, 6.81% pass@5, và 20.98% pass@1000.
Bảng 1b cho thấy rằng khi đánh giá trên một tập con các mẫu mã được lọc, CodeRL+CodeT5 của chúng tôi có thể đạt được kết quả SOTA là 8.48% 1@k và 12.62% 5@k.

Thứ hai, tương tự như công trình trước đó Hendrycks et al. [2021], Austin et al. [2021], Chen et al. [2021a], chúng tôi cũng quan sát thấy lợi ích của upsampling generation khi tăng số lượng mẫu sinh k từ 1 đến 1000. Lưu ý rằng mặc dù CodeRL phát sinh thêm chi phí tính toán trong quá trình suy luận với CS, phương pháp của chúng tôi chỉ yêu cầu k thấp hơn nhiều để đạt được hiệu suất tương đương với các mô hình khác.
Cụ thể, chỉ với k=1000, hiệu suất mô hình của chúng tôi tốt bằng AlphaCode với ngân sách sinh lớn hơn nhiều k=50000. Cuối cùng, từ Bảng 1b, chúng tôi thấy rằng đối với các tác vụ lập trình thử thách ở cấp độ interview và competition, việc tinh chỉnh có thể cải thiện đáng kể hiệu suất mô hình.
Cụ thể, chúng tôi lưu ý rằng Codex, không được tinh chỉnh trên APPS và được kiểm tra trong thiết lập few-shot, có thể đạt được kết quả n@1000 tốt, nhưng mô hình thất bại đáng kể trong các tác vụ tổng hợp ở cấp độ interview và competition. Quan sát này chỉ ra một khoảng cách đáng kể giữa giai đoạn tiền huấn luyện và các tác vụ tổng hợp downstream.

4.4 Nghiên cứu Ablation
Trong phần này, để so sánh công bằng giữa các biến thể của ước tính return và mục tiêu học tập, chúng tôi báo cáo kết quả của pass@k với k={1;5} với giải mã beam search. Để phân tích ablation của CodeRL trong quá trình suy luận với k lớn hơn, chúng tôi báo cáo kết quả có và không có quy trình CS.

Tác động của Ước tính Return. Bảng 2 cho thấy kết quả của CodeT5-770M được huấn luyện bằng các phương pháp khác nhau để ước tính return của các mẫu mã. Nhìn chung, chúng tôi báo cáo rằng mục tiêu CodeRL với ước tính return cấp token tương đối bằng mô hình critic của chúng tôi (Model D) có thể đạt được hiệu suất tốt nhất trên pass@1 và pass@5. Thứ hai, chúng tôi lưu ý rằng việc sử dụng return tuyệt đối không có baseline (Model B) có thể dẫn đến giảm hiệu suất nhiều nhất, vì phương pháp này phạt nặng tất cả các mẫu không đúng (mặc dù chúng vẫn có thể tốt hơn một baseline ngây thơ). Do đó, việc xem xét ước tính return tương đối có thể khai thác hiệu quả các mã không hoàn hảo có thể dẫn đến các hệ thống tổng hợp tốt hơn.

Thứ ba, không có mô hình critic, việc đơn giản gán phần thưởng giống nhau cho tất cả các token trong một mẫu mã (Model A) là bất lợi vì những ước tính return này quá hạn chế để được sử dụng như tín hiệu phản hồi cho huấn luyện RL. Ví dụ, một chương trình được coi là không đúng chỉ vì một ký tự khoảng trống bổ sung, có thể dẫn đến Indentation Error trong chương trình Python. Việc đơn giản gán

12

--- TRANG 12 ---
một phần thưởng giống nhau cho tất cả các token trong chương trình này sẽ phạt nặng các phần đúng của chuỗi chương trình. Cuối cùng, chúng tôi thí nghiệm với một critic dựa trên khoảng cách giả định rằng các giá trị token q̂(ws_t) giảm tuyến tính từ t=1 đến t=T (Model C). Hiệu suất thấp hơn cho thấy lợi ích của việc huấn luyện một mạng critic để tính toán return thay vì dựa vào các phương pháp dựa trên quy tắc.

Tác động của Mục tiêu Học tập. Bảng 3 cho thấy kết quả với các kết hợp khác nhau của Lce và Lrl. Vì CodeRL là model-agnostic, chúng tôi áp dụng các thí nghiệm ablation cho cả CodeT5 và GPT-Neo [Black et al., 2021]. Lưu ý rằng trong những thí nghiệm này, Lce và Lrl được áp dụng trên các mô hình đã được khởi động/tinh chỉnh với Lce trong tối đa 10 epochs. Thứ nhất, khi chúng tôi thí nghiệm chỉ sử dụng Lrl, chúng tôi lưu ý vấn đề vanishing gradients trong quá trình tinh chỉnh, được quan sát tương tự bởi Ranzato et al. [2016], Bahdanau et al. [2016]. Do đó, các mô hình cuối cùng thực sự bị suy giảm và dẫn đến giảm hiệu suất. Thứ hai, chúng tôi lưu ý rằng bằng cách chỉ sử dụng Lce để tinh chỉnh thêm, mặc dù cải thiện losses trong thời gian huấn luyện, hiệu suất mô hình thực sự giảm trong thời gian kiểm tra. Chúng tôi mong đợi những mô hình này đang overfitting với dữ liệu huấn luyện, như được quan sát tương tự trong phân tích của chúng tôi về các mô hình được tiền huấn luyện trong Hình 6.

Thú vị là, chúng tôi thấy rằng một phương pháp ngây thơ của Lce với các mẫu tổng hợp Ws, tất cả đều được coi là mã đúng với r(Ws) = 1, vẫn dẫn đến một số cải thiện hiệu suất với GPT-Neo trên pass@5. Tuy nhiên, trong tất cả các trường hợp khác, chiến lược huấn luyện này không hoạt động tốt bằng việc xem xét một mô hình critic để ước tính return của Ws bằng kết quả kiểm tra của chúng. Cuối cùng, chúng tôi thấy rằng việc sử dụng cả Lce và Lrl dẫn đến cải thiện hiệu suất nhất quán hơn tổng thể trên pass@1 và pass@5 cho các mô hình GPT-Neo và CodeT5.

Tác động của Lấy mẫu Critic. Bảng 4 cho thấy kết quả ablation của critical sampling (CS) trong quá trình suy luận, được áp dụng trên các mô hình CodeT5. Chúng tôi thí nghiệm với các kết hợp khác nhau của các bước tinh chỉnh và sửa chữa chương trình. Nhìn chung, chúng tôi thấy tác động tích cực của CS, kết hợp cả tinh chỉnh và sửa chữa chương trình, trên tất cả các metric, với lợi ích đặc biệt đáng kể hơn trên pass@1000. Chúng tôi

13

--- TRANG 13 ---
lưu ý rằng chỉ tinh chỉnh chương trình một mình có thể giúp mang lại sự cải thiện hiệu suất, nhưng tác động của nó giảm trên metric 1@1000. Lưu ý rằng n@k đo lường tỷ lệ giải quyết trong tập con P được lọc từ k mẫu. Vì tinh chỉnh chương trình về mặt kỹ thuật sẽ tăng kích thước của tập con này, metric n@k sẽ xem xét một số lượng lớn hơn theo cấp số nhân các lựa chọn của n mẫu so với trước đó. Điều này sẽ chuẩn hóa n@k bằng một pool ứng viên n lớn hơn, dẫn đến ít tác động hơn của tinh chỉnh chương trình trên hiệu suất mô hình. Chúng tôi khuyến nghị các bước xử lý hậu kỳ bổ sung như xếp hạng ứng viên [Cobbe et al., 2021] để cải thiện hiệu suất của tinh chỉnh chương trình, đặc biệt trên các metric n@k.

Thứ hai, khi tích hợp tinh chỉnh chương trình với sửa chữa chương trình (cho các bài toán mà P=∅), chúng tôi thấy sự cải thiện hiệu suất thêm trong tất cả các metric. Thú vị là, khi thí nghiệm với các lược đồ chọn top-M khác nhau, chúng tôi thấy hiệu suất tổng thể tốt nhất với M=1 và hiệu suất bắt đầu giảm từ M=2 đến M=4 (ngoại trừ kết quả pass@200). Quan sát này chỉ ra lợi ích của việc sử dụng mô hình critic để tập trung vào các ứng viên tốt nhất cho sửa chữa chương trình thay vì chọn nhiều ứng viên chương trình. Hơn nữa, với M lớn hơn, mỗi ứng viên chương trình sẽ có kích thước batch nhỏ hơn (tức là N/M). Điều này dẫn đến cơ hội thấp hơn cho mô hình sửa chữa chương trình để sửa chữa và sinh đúng chương trình.

Kết quả trên Tác vụ Cấp độ Competition. Chúng tôi chọn điều tra một tập con của split test APPS, chứa các mẫu test có cấp độ khó cao nhất (tức là các tác vụ lập trình competition). Hình 5 cho thấy kết quả của pass@k và n@k với k từ 1 đến 200 và n={1;5}, cho CodeRL+CodeT5 và chỉ CodeT5. Vì CodeRL là model-agnostic, chúng tôi cũng tích hợp nó với GPT-J [Wang and Komatsuzaki, 2021] và báo cáo kết quả. Để tập trung vào tác động của tối ưu hóa RL của chúng tôi, trong thời gian kiểm tra, chúng tôi so sánh các mô hình chỉ sử dụng nucleus sampling và không có quy trình CS. Hình 5 cho thấy rằng sự cải thiện hiệu suất khá nhất quán trên cả GPT-J và CodeT5.
Đặc biệt, khi k tăng, lợi ích hiệu suất của CodeRL đáng kể hơn trên cả mô hình GPT-J và CodeT5. Chúng tôi quy cho những lợi ích này cho mục tiêu học tập CodeRL Lrl khuyến khích các mô hình khám phá các giải pháp mã được rút ra từ phân phối lấy mẫu của mô hình. Trong thời gian kiểm tra

14

--- TRANG 14 ---
với ngân sách lấy mẫu k tăng, các mô hình được phép sinh các giải pháp mã đa dạng và tác động của Lrl trở nên đáng kể hơn.

Tác động của Phương pháp Tiền huấn luyện cho CodeT5. Như thường quan sát trong công trình trước đó [Austin et al., 2021, Chen et al., 2021a, Li et al., 2022], hiệu suất của các hệ thống tổng hợp có tương quan với chất lượng của các mô hình nền tảng. Trong Bảng 5, chúng tôi báo cáo kết quả của CodeT5 với các cấu hình khác nhau về kích thước mô hình, dữ liệu tiền huấn luyện, và mục tiêu tiền huấn luyện. Để so sánh công bằng, tất cả các mô hình chỉ được tinh chỉnh/khởi động với Lce trên APPS trong tối đa 12 epochs. Như quan sát tương tự trong công trình trước đó [Chen et al., 2021a, Austin et al., 2021], chúng tôi thấy rằng việc mở rộng số lượng tham số mô hình (từ 60M đến 770M) có thể cải thiện đáng kể hiệu suất mô hình của các tác vụ tổng hợp downstream. Khi chúng tôi cải thiện dữ liệu tiền huấn luyện bằng cách thêm dataset GCPY (lớn gấp 10 lần dataset CSN), chúng tôi cũng quan sát sự cải thiện hiệu suất tốt, tức là từ 1.3 đến 1.56 pass@1, và 1.72 đến 2.06 pass@5. Cuối cùng, bằng cách kết hợp mục tiêu tiền huấn luyện từ Masked Span Prediction (MSP) và Next Token Prediction (NTP), mô hình có thể thích ứng tốt hơn với tác vụ tổng hợp downstream. Đáng chú ý, việc thêm tác vụ tiền huấn luyện NTP có thể cải thiện hiệu suất từ 2.06 đến 2.9 pass@5.

Kết quả theo Epochs Tinh chỉnh với Mục tiêu NTP. Hình 6 cho thấy hiệu suất của các biến thể mô hình CodeT5 theo epochs tinh chỉnh và theo cấp độ khó của các tác vụ lập trình. Lưu ý rằng trong những thí nghiệm này, chúng tôi chỉ cần so sánh giữa các biến thể mô hình CodeT5 theo chiến lược tiền huấn luyện, và do đó, chỉ sử dụng Lce trong giai đoạn tinh chỉnh trên APPS. Nhất quán với phân tích trước đó của chúng tôi, việc tăng cường cả dữ liệu tiền huấn luyện (với dữ liệu GCPY lớn hơn) và mục tiêu tiền huấn luyện (với mục tiêu NTP) cải thiện hiệu suất mô hình qua các epochs huấn luyện nói chung. Hơn nữa, như được lưu ý bởi phân tích của chúng tôi về các mục tiêu học tập, việc chỉ sử dụng Lce thường dẫn đến hiệu suất overfitting, thường sau epoch 10 trong trường hợp của chúng tôi. Do đó, để tinh chỉnh thêm các LMs quy mô lớn, chúng tôi khuyến nghị áp dụng mục tiêu RL Lrl của chúng tôi để sử dụng các mẫu huấn luyện tổng hợp và tránh overfitting các mô hình.

15

--- TRANG 15 ---
Bảng 6: Kết quả trên benchmark MBPP: chúng tôi kiểm tra khả năng chuyển giao zero-shot của CodeRL. CodeRL+CodeT5 (ZS) được huấn luyện trên APPS với Lrl và đánh giá trên MBPP (Mostly Basic Programming Problems) Benchmark [Austin et al., 2021] trong thiết lập zero-shot, đạt được SOTA mới.

[Bảng hiển thị kết quả so sánh các mô hình GPT với kích thước khác nhau và CodeRL+CodeT5 (ZS)]

Hình 7: Phân tích các dòng trùng lặp giữa MBPP và APPS: Sự chồng chéo dữ liệu giữa APPS và MBPP có vẻ tối thiểu, chỉ có 12.6% chương trình MBPP có >50% dòng trùng lặp trong dữ liệu huấn luyện APPS.

4.5 Kết quả Thí nghiệm trên MBPP
Đánh giá Zero-shot trên Benchmark MBPP. Ngoài benchmark APPS, chúng tôi kiểm tra khả năng chuyển giao zero-shot của CodeRL trên một benchmark tổng hợp chương trình nhỏ hơn và đơn giản hơn khác là MBPP [Austin et al., 2021]. Bảng 6 báo cáo kết quả của CodeRL+CodeT5 của chúng tôi trên benchmark MBPP so với các mô hình GPT được tinh chỉnh có kích thước lên đến 137B. CodeRL+CodeT5 (ZS) của chúng tôi được huấn luyện trên APPS và sau đó được đánh giá trên MBPP trong thiết lập zero-shot. Chúng tôi quan sát thấy rằng CodeRL với CodeT5 có kích thước mô hình nhỏ hơn nhiều mang lại hiệu suất zero-shot tốt đáng ngạc nhiên, thiết lập kết quả SOTA mới là 63.0% pass@80 vượt qua 61.4% pass@80 của GPT-137B. Điều này xác thực khả năng chuyển giao zero-shot mạnh mẽ của CodeRL cho các tác vụ chưa thấy.

Sự chồng chéo giữa MBPP và APPS. Một mối quan tâm chung về transfer learning là các tác vụ nguồn (APPS) và đích (MBPP) có thể có sự chồng chéo trong dữ liệu huấn luyện của chúng, có thể dẫn đến mô hình nguồn có xu hướng ghi nhớ những dữ liệu tương tự đáng kể này khi áp dụng cho tác vụ đích. Để giải quyết mối quan tâm này, chúng tôi phân tích có bao nhiêu dòng mã xuất hiện trong cả tập huấn luyện của APPS và các chương trình của MBPP theo Austin et al. [2021]. Để phân tích này, chúng tôi loại bỏ các comment mã và chuẩn hóa khoảng trắng cho mỗi dòng, và sau đó loại trừ các dòng xuất hiện hơn hai lần bất cứ đâu trong MBPP, vì những dòng này có khả năng là các từ khóa Python phổ biến như return và break.

Hình 7 minh họa số lượng dòng trùng lặp tuyệt đối (Trái) và phần trăm tương đối của dòng trùng lặp (Phải) trong các chương trình MBPP. Như có thể thấy, sự chồng chéo giữa APPS và MBPP có vẻ tối thiểu. Chỉ có 12.6% chương trình MBPP có hơn một nửa số dòng của chúng khớp ở đâu đó trong dữ liệu huấn luyện APPS. Bên cạnh đó, hơn một nửa (514 trong số 974) chương trình có sự chồng chéo bằng không và 90.9% chỉ có không quá 3 dòng chồng chéo với tập huấn luyện APPS. Nếu chúng ta yêu cầu thêm

16

--- TRANG 16 ---
các dòng phải liên tiếp, không có quá 2 dòng trùng lặp liên tiếp. Phân tích thí nghiệm thêm được bao gồm trong Phụ lục B.2.

4.6 Phân tích Định tính
Phân tích theo Kết quả Bài kiểm tra Đơn vị. Hình 8 thể hiện tỷ lệ phần trăm trung bình của các chương trình được sinh ra cho mỗi bài toán, được nhóm theo kết quả kiểm tra của chúng. Cụ thể, chúng tôi sử dụng CodeT5 hoặc CodeRL+CodeT5 để sinh chương trình và ngẫu nhiên chọn 200 chương trình được sinh ra cho mỗi mẫu kiểm tra trong split test APPS. Chúng tôi chuyển các chương trình cho bài kiểm tra đơn vị ví dụ hoặc bài kiểm tra đơn vị ẩn và nhóm các chương trình đầu ra theo kết quả kiểm tra của chúng. Các kết quả được phân loại theo định nghĩa của chúng tôi trong Eq. (4) đến (7), bao gồm CompileError, RuntimeError, FailedTest, và PassedTest.

Thứ nhất, trên cả bài kiểm tra đơn vị ví dụ và bài kiểm tra đơn vị ẩn, chúng tôi quan sát thấy rằng việc tích hợp CodeRL có thể tăng khả năng một chương trình có thể vượt qua các bài kiểm tra, và giảm xác suất nó thất bại một hoặc nhiều bài kiểm tra đơn vị. Xác suất vượt qua bài kiểm tra đơn vị được cải thiện đáng kể hơn trong các bài toán lập trình cấp độ introductory.

Thứ hai, chúng tôi lưu ý rằng tỷ lệ phần trăm có lỗi biên dịch giảm trong các chương trình được sinh bởi CodeRL, với hiệu ứng nhiều hơn trên các bài toán cấp độ interview và competition. Vì lỗi biên dịch ít có khả năng xảy ra với các chương trình CodeRL, những chương trình này vẫn gặp phải lỗi runtime.
Điều này dẫn đến xác suất cao hơn rằng một chương trình CodeRL chứa lỗi runtime. Phân tích thêm về thất bại biên dịch và runtime được mô tả trong Phụ lục C.1.

Chúng tôi lưu ý rằng có những khoảng cách hiệu suất khá đáng kể theo kết quả kiểm tra giữa bài kiểm tra đơn vị ví dụ (Hình 8a) và bài kiểm tra đơn vị ẩn (Hình 8b). Quan sát này cho thấy rằng các bài kiểm tra ví dụ không toàn diện bằng các bài kiểm tra ẩn và do đó, hạn chế tác động tích cực của quy trình sinh CodeRL của chúng tôi do false positives. Chúng tôi khuyến nghị các phương pháp bổ sung để cải thiện bài kiểm tra đơn vị ví dụ, chẳng hạn như thông qua data augmentation bằng cách biến đổi các cặp đầu vào/đầu ra [Austin et al., 2021].

Ví dụ Chương trình Được sinh. Hình 9 cho thấy một ví dụ về bài toán lập trình từ benchmark APPS và các chương trình tương ứng được sinh bởi các biến thể CodeT5. Cụ thể, dựa trên cùng một CodeT5 được tiền huấn luyện nền tảng (được tiền huấn luyện với dữ liệu GCPY và mục tiêu NTP), chúng tôi so sánh mô hình CodeT5 được tinh chỉnh chỉ bằng Lce và một mô hình khác tuân theo khung làm việc CodeRL của chúng tôi. Trong CodeRL+CodeT5, chúng tôi tiếp tục cho thấy các chương trình trước và sau khi áp dụng quy trình CS. Chúng tôi thấy rằng việc áp dụng CodeRL có thể sinh các chương trình phù hợp hơn và việc sử dụng quy trình CS tiếp tục cải thiện tính đúng đắn chức năng của chúng. Ví dụ, trong Hình 9, mô hình CodeT5

17

--- TRANG 17 ---
hiểu sai bài toán và tập trung vào việc tìm ước chung lớn nhất giữa a và b thôi. Thay vào đó, mô hình CodeRL tránh sai lầm này và giải quyết bài toán tìm ước chung lớn nhất giữa giai thừa của a và b.

Chúng tôi cũng thấy rằng CodeRL có thể cải thiện độ phức tạp của các chương trình được sinh ra, một chất lượng quan trọng trong các bài toán lập trình phức tạp. Ví dụ, trong chương trình cấp độ interview trong Hình 9, chúng tôi lưu ý rằng không áp dụng CS, chương trình được sinh ra đúng về mặt chức năng nhưng thất bại trong quá trình thực thi do lỗi timeout. Chương trình này đơn giản tính các giai thừa riêng biệt của cả a và b, điều này sẽ làm chậm việc thực thi trong các tình huống với a hoặb cực lớn. Việc áp dụng quy trình CS có thể điều kiện hóa các mô hình trên các phần của chương trình trước đó và (tái)sinh các token mới để tạo ra một chương trình hiệu quả hơn. Trong ví dụ trong Hình 9, các giai thừa được tính trên min(a,b) để cải thiện hiệu quả của các chương trình. Do đó, chương trình cuối cùng kết quả có thể vượt qua tất cả bài kiểm tra đơn vị ẩn (bao gồm các bài kiểm tra với giá trị cực lớn) mà không có lỗi timeout.

Để có thêm ví dụ chương trình được sinh ra, vui lòng tham khảo Phụ lục C.2.

5 Hạn chế và Tác động Rộng rãi
Tổng hợp chương trình có thể dẫn đến những tác động xã hội tích cực đáng kể, ví dụ: biến đổi các công cụ phát triển phần mềm tương lai, tăng năng suất của các nhà phát triển, và cải thiện khả năng tiếp cận và chất lượng của các khóa học lập trình. Chúng tôi đề xuất CodeRL như một khung làm việc tổng quát để cải thiện hiệu suất của mã được sinh ra từ các mô hình ngôn ngữ. Chúng tôi giới thiệu độc giả đến các hạn chế và tác động rộng rãi được thảo luận dài bởi Chen et al. [2021a] vì những điều này sẽ áp dụng cho các mô hình actor khác nhau mà người ta có thể sử dụng với CodeRL. Các rủi ro và hạn chế rất quan trọng để được xem xét trước khi triển khai những mô hình như vậy ở quy mô lớn.

Một hạn chế chính được nêu bật trong nhiều hệ thống đương đại là các mô hình ngôn ngữ được huấn luyện trên mã có thể sinh mã có thiên kiến, và thậm chí có thể sinh ngôn ngữ tự nhiên độc hại như comment mã.
Tương tự như công trình trước đó trong sinh ngôn ngữ [Ouyang et al., 2022], bên cạnh việc cải thiện tính đúng đắn chức năng, RL có thể được sử dụng để căn chỉnh các mô hình theo sở thích của con người. Các lược đồ giải mã được hướng dẫn hoặc có trọng số [Krause et al., 2021] hoặc các token điều khiển dành riêng cho an toàn [Xu et al., 2020] cũng có thể được khám phá để hướng dẫn việc sinh mã hướng tới các thuộc tính mong muốn như an toàn, đáng tin cậy, nhanh, hiệu quả, công bằng và đại diện.

Một thiên kiến dữ liệu khác chúng ta cần xem xét là liên quan đến bảo mật hệ thống của dữ liệu tiền huấn luyện. Ví dụ, dữ liệu tiền huấn luyện từ các repository mã Github công khai có thể chứa các lỗ hổng và các mô hình tổng hợp kết quả có thể sinh các chương trình với các biện pháp bảo mật yếu [Hammond Pearce et al., 2021]. Do đó, tương tự như các hệ thống sinh mã khác dựa trên LMs lớn, đầu ra của CodeRL đảm bảo cần xác minh bởi các nhà phát triển con người có trình độ.

Một hạn chế khác của phương pháp của chúng tôi là chi phí tính toán của việc huấn luyện mô hình critic để ước tính return ngoài LM gốc (mạng actor). Tuy nhiên, trong thực tế, chúng tôi thấy rằng việc huấn luyện một mô hình critic tốt không yêu cầu các mô hình quy mô lớn để đạt được hiệu suất tốt. Ví dụ, một mô hình critic được tinh chỉnh được khởi tạo từ GPT-2 (small) được tiền huấn luyện có thể đạt được hơn 75% độ chính xác dự đoán lỗi trên các mẫu tổng hợp. Do đó chi phí tinh chỉnh cho mô hình critic là nhỏ so với việc tiền huấn luyện LM gốc. Với CodeRL, chúng tôi có được hiệu suất vượt trội so với các mô hình ngôn ngữ được tiền huấn luyện lớn hơn nhiều. Việc kết hợp CodeRL với các mô hình nhỏ hơn (ví dụ: CodeT5-770M) sẽ có chi phí suy luận ít hơn đáng kể để sinh chương trình.

Cuối cùng, các công trình trước đó về sinh mã đã nêu bật cách các hệ thống được huấn luyện với mục tiêu dự đoán token tiếp theo thể hiện thất bại căn chỉnh - đầu ra mô hình không được căn chỉnh với ý định của người dùng mặc dù mô hình có khả năng làm như vậy. Điều này cũng đúng với CodeRL, vì chúng tôi thực sự chứng kiến mã được sinh ra đôi khi không thỏa mãn yêu cầu người dùng về mặt bài kiểm tra đơn vị. Tuy nhiên, không giống như các công trình trước đó, đầu ra của CodeRL có thể được điều chỉnh rất nhiều bởi người dùng thông qua mô tả bài toán cũng như các bài kiểm tra đơn vị mà giải pháp dự kiến vượt qua. Bằng cách tận dụng bài kiểm tra đơn vị trong quá trình huấn luyện, và trong quá trình suy luận, CodeRL, khi được áp dụng với một hệ thống sinh mã cơ sở, cải thiện sự căn chỉnh của nó với ý định người dùng. Khả năng cải thiện căn chỉnh của CodeRL có thể quan trọng trong việc giải quyết các vấn đề misalignment được dự đoán bởi Chen et al. [2021a] sẽ tệ hơn khi chúng ta mở rộng dữ liệu, tham số và thời gian huấn luyện.

6 Kết luận
Chúng tôi trình bày CodeRL, một khung làm việc mới cho tổng hợp chương trình, sử dụng học tăng cường sâu để cải thiện các LMs được tiền huấn luyện, bằng cách khai thác tín hiệu bài kiểm tra đơn vị trong cả giai đoạn huấn luyện và suy luận.
Cụ thể, chúng tôi giới thiệu một phương pháp huấn luyện actor-critic để tối ưu hóa các LMs được tiền huấn luyện với tín hiệu phản hồi dày đặc trên các mẫu mã tổng hợp. Trong quá trình suy luận, chúng tôi đề xuất một quy trình sinh mới với lấy mẫu critical, cho phép mô hình tự động tái sinh chương trình dựa trên phản hồi từ bài kiểm tra đơn vị và điểm số critic. Chúng tôi tích hợp CodeRL với mô hình CodeT5-large được cải thiện (770M) và đạt được kết quả SOTA mới trên cả benchmark APPS và MBPP, vượt qua SOTA trước đó bằng các LMs được tiền huấn luyện lớn có kích thước mô hình lớn hơn nhiều. Phân tích toàn diện của chúng tôi cho thấy rằng CodeRL đạt được sự cải thiện nhất quán so với các LMs được tiền huấn luyện thông thường cho các tác vụ sinh mã. CodeRL là một khung làm việc tổng quát tích hợp các LMs được tiền huấn luyện và RL một cách toàn diện cho tổng hợp chương trình, và có thể được mở rộng và cải thiện theo nhiều cách khác nhau. Ví dụ, nó có thể dễ dàng được tích hợp với các LMs được tiền huấn luyện tốt hơn khác và có thể được cải thiện với phản hồi chi tiết hơn từ môi trường, chẳng hạn như phản hồi nhận được từ một bộ phân tích mã tĩnh.

Lời cảm ơn
Chúng tôi muốn cảm ơn nhóm Salesforce Research của chúng tôi vì những thảo luận và phản hồi có ích.

Tài liệu tham khảo
[Danh sách các tài liệu tham khảo được liệt kê theo thứ tự alphabetical]

[Trang tiếp theo chứa các phụ lục A, B, C với nội dung chi tiết về quy trình lấy mẫu critic, kết quả thí nghiệm bổ sung và phân tích định tính bổ sung]
