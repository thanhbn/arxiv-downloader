# 2305.13735.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2305.13735.pdf
# File size: 1471036 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Aligning Large Language Models through Synthetic Feedback
Sungdong Kim1,2,3Sanghwan Bae1Jamin Shin1,2
Soyoung Kang1Donghyun Kwak1Kang Min Yoo1,2,4Minjoon Seo3
NA VER Cloud1NA VER AI Lab2KAIST AI3SNU AI Center4
{sungdong.kim, sanghwan.bae, jamin.shin}@navercorp.com
{soyoung.kang, donghyun.kwak, kangmin.yoo}@navercorp.com
minjoon@kaist.ac.kr
Abstract
Aligning large language models (LLMs) to hu-
man values has become increasingly important
as it enables sophisticated steering of LLMs.
However, it requires significant human demon-
strations and feedback or distillation from pro-
prietary LLMs such as ChatGPT. In this work,
we propose a novel alignment learning frame-
work with synthetic feedback not dependent
on extensive human annotations and propri-
etary LLMs. First, we perform reward model-
ing (RM) with synthetic feedback by contrast-
ing responses from vanilla LLMs with various
sizes and prompts. Then, we use the RM to
simulate high-quality demonstrations to train
a supervised policy and further optimize the
model with reinforcement learning. Our re-
sulting model, Aligned Language Model with
Synthetic Training dataset (ALMoST), outper-
forms recent open-sourced models, which are
trained on the outputs of InstructGPT or human-
annotated demonstrations, in alignment bench-
marks. In human evaluation, our model is
preferred to Alpaca and Dolly-v2, 55.0% and
58.5% of the time, respectively. Further analy-
ses demonstrate the efficacy and importance of
synthetic feedback in our framework1.
1 Introduction
Alignment learning has been an essential learning
scheme to align the behaviors of large language
models (LLMs) with human values like safety and
truthfulness while following the intention of users
accurately (Ouyang et al., 2022). Vanilla LLMs ‚Äì
those not aligned yet ‚Äì could misunderstand user
intentions or produce unsafe and inaccurate re-
sponses. Desirable human values such as help-
fulness, harmlessness, or honesty can be defined,
and human demonstrations with these values are
then used for the alignment learning (Askell et al.,
2021; Bai et al., 2022a).
1The code is available at github.com/naver-ai/almost.

(FOFSBUFTZOUIFUJDDPNQBSJTPOT####1SPNQU8IBUXPVMECFUIFDPOTFRVFODFTPGVTJOHBQVCMJD8J'JOFUXPSL TIPUTIPUTIPUTIPU

'JMUFSJOHOPJTFTXJUIQPTUWBMJEBUJPO
3VMFPG5IVNC-BSHFS--.XJUI.PSFBOE#FUUFSTIPUTNJHIUHJWFCFUUFSSFTQPOTFPWFSBMM

5SBJOJOHB3FXBSE.PEFMPOUIFTZOUIFUJDDPNQBSJTPOT
Figure 1: A procedure of reward modeling through
synthetic feedback. We assume that the response from a
larger LLM with more and better demonstrations might
be better overall. We train a reward model with synthetic
comparisons generated top on the assumption.
Typically, alignment learning consists of three
stages: supervised fine-tuning (SFT), reward mod-
eling (RM), and reinforcement learning from hu-
man feedback (RLHF) (Ouyang et al., 2022; Bai
et al., 2022a).
However, the three-stage training recipe requires
significant human effort, especially in the first two
stages. More specifically, both the SFT and RM
training stages must be provided with an abundance
of high-quality human demonstrations and ranking
datasets for obtaining models to facilitate RLHF.
For instance, Ouyang et al. (2022) prepare and
utilize 13k human demonstrations and 33k compar-
isons.
On the other hand, Self-Instruct (Wang et al.,arXiv:2305.13735v2  [cs.CL]  21 Oct 2023

--- PAGE 2 ---
2022) attempts to generate synthetic self-generated
instruction datasets using in-context learning with
a few seed demonstrations. Meanwhile, the release
of LLaMA (Touvron et al., 2023) brings upon many
open-sourced aligned LLMs trained on the outputs
of proprietary LLMs or human-annotated instruc-
tions. However, it still heavily depends on propri-
etary LLM APIs such as InstructGPT and Chat-
GPT (Ouyang et al., 2022; OpenAI, 2023; Taori
et al., 2023; Chiang et al., 2023) or intensive human
annotations (DataBricks, 2023; K√∂pf et al., 2023).
In this paper, we introduce a novel framework
for alignment learning that only requires minimal
human labor and is not dependent on proprietary
LLMs. Unlike the conventional alignment learning
procedure that collects demonstration first (Ouyang
et al., 2022), we first develop a reward model (RM)
on a synthetic comparison dataset constructed by
contrasting outputs from vanilla LLMs in vari-
ous configurations, as shown in Figure 1. The
rules of generating these synthetic ranking data
originate from our hypothesis that the responses
generated by larger, optimally prompted models
are superior to those produced by smaller, inade-
quately prompted models, as reported by previous
work (Askell et al., 2021). Then, we introduce a
Reward-Model-guided Self-Play (RMSP) to sim-
ulate high-quality demonstrations with rejection
sampling using the RM (Ouyang et al., 2022). We
train LLaMA- 7B (Touvron et al., 2023) on the syn-
thetic demonstrations (SFT) and further optimize
the model with rewards from the synthetic RM,
namely, Reinforcement Learning from Synthetic
Feedback (RLSF).
Our Aligned Language Model with Synthetic
Training dataset (ALMoST) outperforms Al-
paca (Taori et al., 2023) ‚Äì distilled from In-
structGPT (Ouyang et al., 2022) ‚Äì and Dolly-
v2 (DataBricks, 2023) and OpenAssistant (K√∂pf
et al., 2023) that are trained on human-annotated
demonstrations in the alignment-related bench-
marks (Askell et al., 2021; Lin et al., 2021; Chiang
et al., 2023). Notably, our model is preferred to
recent open-sourced models, Alpaca and Dolly-v2,
55-58% of the time (winning rate) in human eval-
uation without distillation from proprietary LLMs
nor intensive human annotations. We speculate
the strong performance of our model is due to the
empirical indicators of well-aligned behaviors that
have been effectively incorporated into a strong
backbone model through synthetic feedback, allow-ing the inherent capability to self-align to elicit and
partially replace the need for human feedback.
Our main contributions are three folds:
‚Ä¢We propose a novel alignment learning frame-
work by introducing synthetic feedback. It
automatically constructs high-quality compar-
isons and demonstrations without relying on
human feedback and proprietary LLMs.
‚Ä¢Our resulting model, ALMoST, shows well-
aligned behaviors with human values in align-
ment benchmarks. In the human study, AL-
MoST is preferred to Alpaca and Dolly-v2,
showing a 55-58% winning rate.
‚Ä¢Analysis on RM further demonstrates the ef-
ficacy of synthetic feedback and highlights
the importance of injecting empirical priors,
e.g., our proposed filtering method and faith-
ful prompt design.
2 Method
In this section, we will describe detailed procedures
of our framework as depicted in Figure 2.
2.1 Step 1: Reward Modeling with Synthetic
Feedback
Prompted Baseline As we do not have aligned
baselines available for the comparisons yet, we uti-
lize HHH (Helpful, Harmless, and Honest) prompt
devised by Askell et al. (2021). It contains 14
human-written conversations for guiding LLM
alignment2. We employ The HHH prompted
LLaMA models to generate synthetic compar-
isons (Touvron et al., 2023).
Generating Synthetic Comparison Instead of
collecting human feedback, we rather generate syn-
thetic comparisons based on naive assumptions ac-
cording to empirical observations. Askell et al.
(2021) demonstrate that the larger model performs
somewhat better than the smaller model, and the
model with longer prompts is better than the model
with shorter prompts in terms of human preference.
In short, we assume the quality of the response
follows the rule of thumb:
‚Ä¢ Larger model >Smaller model
‚Ä¢ More few-shots >Less few-shots
‚Ä¢Better demonstration >Worse demonstration
2gist.github.com/jareddk/2509330...

--- PAGE 3 ---
8IBUXPVMECFUIFDPOTFRVFODFTPGVTJOHBQVCMJD8J'JOFUXPSL 4UFQ3FXBSENPEFMJOHXJUIHFOFSBUJOHTZOUIFUJDDPNQBSJTPOEBUB*OJUJBMQSPNQUTBSFHFOFSBUFECZ7BOJMMB--.XJUITIPU*$-
-BSHFS--.XJUI.PSFBOE#FUUFSTIPUTNJHIUHJWFCFUUFSSFTQPOTFPWFSBMM"#$%
%$#"
(FOFSBUFTZOUIFUJDDPNQBSJTPOTCZBQQMZJOHSVMFTPGUIVNCBOEQPTUWBMJEBUJPO*USFRVJSFTPOMZEFNPOTUSBUJPOTGPSUIF--.T
%$#"
3.$PNQBSJTPOEBUB3FXBSENPEFMJTUSBJOFEPOUIFTZOUIFUJDBMMZHFOFSBUFEDPNQBSJTPOEBUBTFU4UFQ4JNVMBUFIJHIRVBMJUZEFNPOTUSBUJPOEBUBBOEUSBJOBTVQFSWJTFEQPMJDZ8IBUXPVMECFUIFDPOTFRVFODFTPGVTJOHBQVCMJD8J'JOFUXPSL 4UBSUJOHCZJOJUJBMRVFSJFTHFOFSBUFEJOUIF4UFQ
--.XJUIVTFSQSPNQU--.XJUI"*QSPNQU4ZOUIFUJD3.	GSPN4UFQ
#FTUPG/TBNQMJOH(FOFSBUFTZOUIFUJDEFNPOTUSBUJPOTWJB3.HVJEFETFMGQMBZCFUXFFOUXP--.TXJUIDPSSFTQPOEJOHSPMFQSPNQUT
4'55IFTZOUIFUJDEFNPOTUSBUJPOEBUBTFUJTVTFEUPGJOFUVOF--B."XJUITVQFSWJTFEMFBSOJOHh%FNPOTUSBUJPOEBUB4UFQ0QUJNJ[FBQPMJDZBHBJOTUUIFTZOUIFUJDSFXBSENPEFMVTJOHSFJOGPSDFNFOUMFBSOJOH$BO:PVHJWFNFBMJTUPGCFTUBQQTUIBUXPSLTGPSNVTJDNPWJFTBOETPOHTPOBOESPJEUBCMFU0UIFSQSPNQUTBSFOFXMZHFOFSBUFECZTBNFSFDJQFPG4UFQ
110h*DBOHJWFZPVBMJTUPGNVTJDBOENPWJFBQQTh
3.
SFXBSE5IFQPMJDZDPOEVDUTSPMMPVUHFOFSBUJPOTGPSHJWFOQSPNQUT5IFTZOUIFUJDSFXBSENPEFMTDPSFTUIFHFOFSBUFEPVUQVU"OEUIFSFXBSEJTVTFEUPVQEBUFUIFQPMJDZVTJOH110Figure 2: Overview of our proposed framework for alignment learning of LLMs. Step 1. We first conduct reward
modeling with a synthetically generated comparison dataset ( synthetic feedback) . Step 2. The demonstration dataset
is generated by simulation with the guidance of the reward model and train supervised policy with the synthetic
demonstrations. Step 3. We further optimize the model against the reward model with reinforcement learning.
For the same input x, we first sample the re-
sponses Y={y1, y2, ..., y |Y|}from the mod-
els with various configurations. Then, we apply
the rule to choose the better response among the
generated responses. More specifically, we in-
volve{7,13,30}B LLMs with {1,3,5}shots of
HHH demonstrations for the comparison. As il-
lustrated in Figure 1, if we sample responses from
(1)30B with 5 shots, (2) 13B with 3 shots, and
(3)7B with 1 shot, the ranking becomes y1>
y2> y 3according to our rule of thumb. Then,
we can get a set of binarized comparisons, e.g.,
{(y1, y2),(y2, y3),(y1, y3)}. We denote the former
as a ‚Äòchosen‚Äô response ( y1) and the latter as a ‚Äòre-
jected‚Äô response ( y2) in a comparison pair (y1, y2).
Post Validation Our assumptions are often
wrong because of the stochastic nature of the
prompt-based generation. These noises in the
dataset make the reward modeling unstable and
divergent in the end. Thus, we come up with post
validation method to filter out such noises.
First, we devise Heuristic Filter (HF) based on
prior knowledge. It discards bad responses contain-
ing or beginning with keywords such as ‚ÄúI don‚Äôt
know‚Äù or ‚Äúwell‚Äù. Also, we empirically find that the
better response usually has a longer length than the
worse one. Especially if the response is short, it
often tends to be a case of probabilistic generationfailure. However, training RM only on compar-
isons with longer chosen responses would make
the resulting model biased by length. Thus, we
apply HF to take comparison pairs whose chosen
response is longer than either the rejected one or
M‚àíS/2, where Mis the mean, and Sis the stan-
dard deviation of the lengths of Yin the character
level. This length constraint reduces the probability
that short-generation would be stochastic genera-
tion failure by checking whether the length of each
response is in the confidence interval. Furthermore,
it does not fall into the length bias. Please see
Appendix B for detailed examples. We will demon-
strate the benefits in Section 4.2.
Second, we leverage As-is RM for further
data filtering. Specifically, we train another RM
with a community QA dataset such as StackEx-
change (Askell et al., 2021; Beeching et al., 2023).
Our preliminary study does not find the benefits of
large-scale pre-training for RM discussed in Askell
et al. (2021). Thus, we sample 20k pairs from the
pre-processed StackExchange dataset for our train-
ing3. We keep the resulting synthetic comparisons
only when the As-is RM agrees with the decision.
Reward Modeling Finally, we train the reward
model based on the synthetic comparisons de-
scribed above. We follow the ranked preference
3huggingface.co/datasets/lvwerra/stack-exchange-paired

--- PAGE 4 ---
modeling objective from previous works (Askell
et al., 2021; Ouyang et al., 2022). The objective is
to make the reward model rŒ∏assign a scalar value
for the overall quality of a response yjfor a given
query xcomparative to its counterpart baseline re-
sponse yk. The loss function is defined as follows:
J(Œ∏) =‚àíE(x,yj,yk)‚àºDlog(œÉ(rŒ∏(x, yj)‚àírŒ∏(x, yk)))
where Dis the training set of synthetic com-
parisons and rŒ∏(x, y)is the reward model‚Äôs scalar
output indicating the overall response quality yfor
its input x.
Implementation Details We start by generating
initial queries, i.e., a diverse set of input x. In par-
ticular, we adopt the recipe of Self-Instruct (Wang
et al., 2022) to generate 10k initial queries based
on few-shot in-context learning. More details of
the query generation are in Appendix C.
For response generation, we include five
prompted models with the below configurations.
‚Ä¢ A. LLaMA-30B-Faithful-3shot
‚Ä¢ B. LLaMA-30B-HHH-5shot
‚Ä¢ C. LLaMA-13B-HHH-3shot
‚Ä¢ D. LLaMA-7B-HHH-3shot
‚Ä¢ E. LLaMA-7B-HHH-1shot
For each query, we generate five responses
from the models and take rankings, yA> yB>
yC> y D> y E, reflecting the rule of thumb.
The Faithful indicates our manually designed
prompts consisting of three conversations respond-
ing more faithfully and longer while considering
the response format, and the HHH indicates the
prompts written by Askell et al. (2021). The de-
tailed examples are in Appendix A. Finally, we
produce 13k binarized synthetic comparisons af-
ter post-validation (HF and As-is RM) and train a
reward model with the synthetic comparisons.
2.2 Step 2: Supervised Fine-Tuning
In the second step, we propose a Reward-Model-
guided Self-Play (RMSP) to simulate high-quality
demonstrations, i.e., conversations between the
user and AI assistant. The simulated demonstra-
tions are used to supervised fine-tuning for the ini-
tially aligned policy model (SFT).Self-Play The basic simulation is enabled by
turn-taking between the user and assistant role
models with corresponding prompts, i.e., self-
play. We continue to use the same prompted
baseline, LLaMA-30B-Faithful-3shot , for the as-
sistant role. In addition, we‚Äôve made minor ad-
justments to the original HHH prompt (Askell
et al., 2021) to suit the user‚Äôs role better,
LLaMA-30B-User-3shot4. Starting from the
initial queries generated in the first stage, the
LLaMA-30B-Faithful-3shot generates responses
for the queries. Then, the LLaMA-30B-User-3shot
follows up the assistant‚Äôs response. The turn-taking
is continued until the maximum turn T.
RM-guided Self-Play (RMSP) To ensure a more
aligned response from the assistant, we suggest in-
cluding the synthetic RM, trained in the first stage,
in the loop, namely Reward-Model-guided Self-
Play (RMSP). In this setup, the assistant model,
LLaMA-30B-Faithful-3shot , first samples Nre-
sponses for a given conversational context. Then,
the RM scores the Nresponses, and the best-scored
response is chosen as the final response for the
simulation, i.e., the RM performs rejection sam-
pling (best-of- Nsampling) (Ouyang et al., 2022;
Scheurer et al., 2023). Like the Self-Play, the turn-
taking with LLaMA-30B-User-3shot is continued
until the maximum turn. Please see Figure 10 for
the examples.
Implementation Details We generate about 20k
high-quality demonstrations using RMSP. We set
the maximum turn to 2 for simplicity, focusing
on the single-turn scenario. The number of rejec-
tion sampling Nis set to 4 considering resource
constraints5. Then, we train LLaMA- 7B on the
generated demonstrations, i.e., a supervised pol-
icy fine-tuning (SFT). More training details are in
Appendix D.
2.3 Step 3: Reinforcement Learning from
Synthetic Feedback (RLSF)
In the last stage, we perform reinforcement learn-
ing from synthetic feedback (RLSF) to further align
the SFT model using a reward signal from the
synthetic RM. Following previous works (Ouyang
et al., 2022; Bai et al., 2022a), we use Proximal
Policy Optimization (PPO) (Schulman et al., 2017).
During this stage, a policy œÄœïautoregressively gen-
4Please see Appendix A for the details of prompts.
5More details of the synthetic datasets are in Appendix C.

--- PAGE 5 ---
Static HHH Alignment TruthfulQA
Model Backbone Dataset by Helpful Harmless Honest Other All MC1
Dolly-v2 Pythia- 12B Human 67.8 46.6 50.7 62.8 56.6 15.2
Oasst-v4 Pythia- 12B Human 59.3 56.9 47.5 69.8 57.5 23.3
Vicuna LLaMA- 13B ChatGPT 78.0 89.7 70.5 81.4 79.6 63.3
Dolly-v2 Pythia- 7B Human 69.5 41.4 45.9 51.2 52.0 24.2
Alpaca LLaMA- 7B InstructGPT 71.2 53.4 62.3 65.1 62.9 19.5
Vicuna LLaMA- 7B ChatGPT 79.7 72.4 70.5 76.7 74.7 52.5
ALMoST (SFT) LLaMA- 7B LLaMA 79.7 56.9 65.6 69.8 67.8 31.5
ALMoST (PPO) LLaMA- 7B LLaMA 81.4 60.3 62.3 72.1 68.8 38.0
ALMoST (RM) LLaMA- 7B LLaMA 74.6 67.2 78.7 86.0 76.0 54.8
Table 1: Evaluation results of Static HHH alignment and TruthfulQA (Multiple-Choice) (Askell et al., 2021;
Lin et al., 2021). We report accuracy for both datasets. Our ALMoSTs outperform recent open-sourced models,
Alpaca, Dolly, and OpenAssistant (Taori et al., 2023; DataBricks, 2023; K√∂pf et al., 2023), trained on the outputs of
InstructGPT or human demonstrations. Also, our RM shows a good performance in identifying proper responses
aligned with human values, surpassing Vicuna trained on outputs of ChatGPT (Chiang et al., 2023). Notably, our
models only leverage synthetic datasets while not relying on the pre-aligned LLMs or extensive human annotations.
erates a response ygiven a prompt x. Subsequently,
a reward score rŒ∏(x, y)is determined by the reward
model rŒ∏. The training objective is to maximize
the expected reward.
Ex‚àºD,y‚àºœÄœï(¬∑|x)[rŒ∏(x, y)]
Stiennon et al. (2020) proposes that adding an
estimated KL penalty term between the initial pol-
icyœÅand the policy œÄœïtorŒ∏(x, y)can enhance
performance. This adjustment leads to the final
objective as follows:
Ex‚àºD,y‚àºœÄœï(¬∑|x)[rŒ∏(x, y)‚àíŒªlogœÄœï(y|x)
œÅ(y|x)
],
where Œªis a KL coefficient.
Implementation Details We initialize the pol-
icyœÅwith the SFT-tuned LLaMA-7B from Step
2. Also, the prompts for the PPO training are com-
piled by extracting only the inputs (initial queries)
from the demonstration dataset generated by RMSP
described in Section 2.2. More details for PPO
training are in Appendix D.
3 Evaluating Alignment of ALMoST
We validate our resulting model, Aligned Language
Model with Synthetic Training dataset (ALMoST),
in three alignment benchmarks, Static HHH evalu-
ation (Askell et al., 2021), TruthfulQA (Lin et al.,
2021), and Vicuna Questions (Chiang et al., 2023).3.1 Dataset
Static HHH alignment and TruthfulQA Askell
et al. (2021) introduce Static HHH alignment
benchmark to measure how models are aligned
well with the human values6. Similar to the com-
parison dataset, the model should choose a more
proper response for input between the two options
based on human values. The dataset consists of
three human value categories, helpful, harmless,
and honest, and contains a misc (others) category.
We include the dataset to get relationships of ten-
sion among the human values from the evaluation,
although the entire dataset is just 221. Lin et al.
(2021) propose TruthfulQA to measure how LLM
generates truthful answers for a given question7.
It especially contains 817 adversarial questions to
elicit imitative falsehood answers from LLMs. For
simplicity, we evaluate the models with a multiple-
choice setup (MC1) instead of a generative setup.
Note that all the evaluation is based on zero-shot,
which means we do not fine-tune the target dataset.
Please see Appendix I for more details of the eval-
uation prompt.
Vicuna Questions We test our models using Vi-
cuna evaluation questions (Chiang et al., 2023). It
consists of 80 questions on various topics spanning
general QA, writing, reasoning, etc., to identify
user preference8. First, two different models gen-
6github.com/google/BIG-bench
7github.com/sylinrl/TruthfulQA
8github.com/lm-sys/FastChat

--- PAGE 6 ---
0% 25% 50% 75% 100%Vicuna-7bALMoST-7b (SFT)Alpaca-7bDolly-v2-7b
25.0%37.5%55.0%58.8%
25.0%36.3%12.5%26.3%
50.0%26.3%32.5%15.0%ALMoST-7b (PPO) Win Tie ALMoST-7b (PPO) Loss(a) Human Evaluation
0% 25% 50% 75% 100%Vicuna-7bALMoST-7b (SFT)Alpaca-7bDolly-v2-7b
16.3%50.0%59.4%77.5%
5.0%6.9%3.8%3.8%
78.8%43.1%36.9%18.8%ALMoST-7b (PPO) Win Tie ALMoST-7b (PPO) Loss (b) GPT-4 Evaluation
Figure 3: (a) Human evaluation and (b) GPT-4 evaluation results within 7B models on Vicuna Questions (Chiang
et al., 2023). It shows the percentage of win, tie, and loss of ALMoST-PPO against other models.
erate an answer for each same question. Then, we
conduct human A/B tests to choose a preferred an-
swer between answers from the two models. In
particular, we recruit three workers for each test.
We ask the workers to choose the more helpful,
harmless, and honest answer with careful consider-
ation over the contents in the answers (Askell et al.,
2021). Please see Appendix E.1 for more details of
the human evaluation. In addition, we conduct an
automatic evaluation using GPT-4. In the test, GPT-
4 assesses the two answers by giving a 1-10 scalar
score for the corresponding answer and providing
appropriate explanations for the judgment (OpenAI,
2023). Even though it is not a rigorous evaluation,
we can compare the overall responding qualities
of the models with reasonable costs. Considering
the positional bias of the GPT-4 assessment9, we
evaluate the same instance twice by reversing the
order of the two answers.
3.2 Baselines
We include recent open-sourced models to com-
pare the aligned behaviors of the LLMs according
to their backbone model and training dataset. Al-
paca is the first open-sourced instruction-following
model based on LLaMA (Touvron et al., 2023;
Taori et al., 2023). It is trained on the 52k syn-
thetic instructions dataset generated by the pro-
prietary LLM, InstructGPT (Ouyang et al., 2022).
Similarly, Vicuna is trained on 70k ShareGPT
dataset, which is the shared chatting logs between
users with the ChatGPT, one of the powerfully
aligned models (OpenAI, 2023; Chiang et al.,
2023). Dolly-v2 is another open-sourced model
trained on a 15k human-annotated instructions
9This repo notices that the GPT-4 evaluation has a strong
positional bias in favor of the first response.dataset (DataBricks, 2023). It is based on Pythia,
another open-source LLM (Biderman et al., 2023).
OpenAssistant (Oasst) is an open-source project
to build aligned LLM based on participants of the
web community (K√∂pf et al., 2023). It also re-
leases an Oasst model (SFT) trained on the human-
annotated dataset10.
3.3 Evaluation Results
Static HHH alignment and TruthfulQA Our
models outperform Alpaca, Dolly-v2, and OpenAs-
sistant without any distillation from the proprietary
LLMs or intensive human annotations, as shown in
Table 1. For all sizes, our ALMoSTs show consis-
tently better accuracy in all HHH splits and Truth-
fulQA except for Vicuna trained on ChatGPT‚Äôs out-
puts (Chiang et al., 2023; OpenAI, 2023). However,
our RM shows excellent performance in choosing
appropriate responses according to human values,
even beating the Vicuna- 7B. It is the consistent ob-
servation with Askell et al. (2021). Moreover, it is
notable our ALMoST-PPO achieves the highest ac-
curacy in the Helpful split of HHH evaluation, even
including 13 billion models. When comparing our
SFT and PPO-trained models, the PPO model im-
proves helpfulness, harmlessness, and truthfulness
while sacrificing honesty. Honesty and truthfulness
look similar, but they are slightly different. The
honesty is related to expressing uncertainty, while
the truthfulness mainly measures how robust the
model is against adversarial falsehood.
Human Evaluation on Vicuna Questions We
confirm our models‚Äô strengths in actual human
preferences in human evaluation11. In Figure 3a,
10OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5
11The inter-rater agreement is rated as moderate (Fleiss‚Äô
kappa= 0.41). More details are in Appendix E.1

--- PAGE 7 ---
we find that humans also more favorably as-
sess our ALMoST model than Dolly-v2 and Al-
paca (DataBricks, 2023; Taori et al., 2023), show-
ing58.8% and 55.0% of winning rate, respectively.
Also, ALMoST-PPO improves the preference of
ALMoST-SFT with a higher winning rate ( 37.5%),
while they also show the highest tie rate ( 36.3%). It
indicates the efficacy of our RLSF training. More-
over, our model is assessed as competitive with
Vicuna, showing 25% of the winning rate and 25%
of the tie rate, even without dependency on the
powerful proprietary model, ChatGPT. However,
there still remains significant room for improve-
ment between our ALMoST and Vicuna. We also
include qualitative examples of the evaluation in
Appendix H.
GPT-4 Evaluation on Vicuna Questions In Fig-
ure 3b, we can observe a similar tendency of
GPT-4 evaluation with the human evaluation. The
ALMoST-PPO consistently shows a higher win-
ning rate against Dolly-v2, Alpaca, and ALMoST-
SFT. However, we find that the GPT-4 is not likely
to give the same scores for the answers showing
a generally lower tie rate than the human evalua-
tion. Moreover, GPT-4 assesses Vicuna‚Äôs responses
more favorably than humans did. Nevertheless,
we can obtain the overall gaps among the models
with reasonable cost from the GPT-4 evaluation.
When we extend the evaluation to various-sized
models, our 7B model outperforms 12-13B base-
lines, including Alpaca- 13B, Oasst- 12B, and Dolly-
v2-12B in the evaluation as shown in Figure 8.
4 Analysis
4.1 Probing Main Hypothesis
We further inspect our assumptions for the syn-
thetic feedback. Specifically, we would like to
know how each assumption, (1) model size, (2) the
number of demonstrations, and (3) the quality of
demonstrations, contributes to the final quality of
the sampled responses. For this, we conduct GPT-4
evaluations on Vicuna Questions to compare our
prompted response generators used for synthetic
feedback generation in Section 2.1. We use Alpaca-
7B (Taori et al., 2023) as a baseline model for the
pair-wise comparisons. The results are shown in
Table 2.
First, as expected, we can see that the model size
significantly contributes to the response quality for
both types of prompt ( HHHandFaithful ). TheConfiguration W T L
LLaMA-7B-HHH-1shot 12 15 133
LLaMA-7B-HHH-3shot 12 14 134
LLaMA-7B-HHH-5shot 15 17 128
LLaMA-13B-HHH-3shot 17 17 126
LLaMA-30B-HHH-5shot 39 21 100
LLaMA-7B-Faithful-3shot 52 14 94
LLaMA-13B-Faithful-3shot 59 19 82
LLaMA-30B-Faithful-3shot 77 13 70
Table 2: GPT-4 evaluation results on Vicuna Ques-
tions (Chiang et al., 2023) of the prompted response gen-
erators in various configurations compared to Alpaca-
7B (Taori et al., 2023). It is based on the same evaluation
setup in Section 3. The W, T, and L indicate # of wins,
ties, and losses of each generator against Alpaca-7B.
winning rate against Alpaca-7B increases mono-
tonically as we increase the model size. The gap
between 13B and 30B is especially large. Second,
when comparing LLaMA-7B-HHH-{1,3,5}shot
models, the number of demonstrations also im-
proves the winning rate, but improvements by this
factor are relatively small. Finally, we find that the
demonstration‚Äôs quality is the most important fac-
tor. Surprisingly, the smaller model with the well-
designed prompt ( LLaMA-7B-Faithful-3shot )
outperforms the larger model with the normal
prompt ( LLaMA-30B-HHH-5shot ). Through this in-
trinsic evaluation, we can find our synthetic feed-
back dataset effectively covers responses of varying
quality.
4.2 RM Evaluation
Train Dataset # instance Accuracy
Random baseline - 50.0
Lengthy baseline - 59.4
Zero-shot
StackExchange 25,057 63.7
Synthetic Feedback 13,687 65.2
Full Fine-tuning
Helpful-base‚àó11,738 65.2
Helpful-base 43,835 71.8
Table 3: Results of zero-shot reward modeling in the
Helpful-base split of HH-RLHF (Bai et al., 2022a). The
lengthy baseline always chooses a longer response be-
tween the pairs. The ‚àóindicates the training data is a
subset of the original, only including single-turn.

--- PAGE 8 ---
We further evaluate our RM on another com-
parison dataset, HH-RLHF (Bai et al., 2022a) to
validate our synthetically generated comparisons
(Synthetic Feedback). HH-RLHF contains various
splits according to the development stage, e.g., the
base set to build the initial policy or the online set
collected with the deployed system. We focus on
the ‚ÄòHelpful-base‚Äô split, assuming we do not have
a deployable system.
Reward Modeling In Table 3, we find our RM
trained with Synthetic Feedback achieves 90% per-
formance of its upper-bound trained on the full
training dataset. Also, it achieves the same accu-
racy with the result fine-tuned on the single-turn
subset (Helpful-base‚àó). Please note HH-RLHF
includes multi-turn context, while our synthetic
dataset focuses on single-turn scenarios.
Train Dataset Accuracy
Random baseline 50.0
Lengthy baseline 59.4
Synthetic Feedback 65.2
- As-is RM 63.3
- Heuristic Filter 55.5
Table 4: Ablation results of post validation in the syn-
thetic comparison generation. Helpful-base split is used
for the RM evaluation (Bai et al., 2022a).
Effect of Post Validation We conduct two types
of post-validation to reduce noises in the synthetic
comparisons described in Section 2.1. Table 4
shows that each filtering method contributes to the
final reward model quality. Notably, we find the
heuristic filter (HF) considering length distribution
plays a crucial role in synthetic data generation.
When we exclude the HF, the performance of RM
drops about 10% point. Moreover, HF prevents the
RM from falling into the length bias discussed in
Section 2.1. The RM, trained on the dataset with
HF, outperforms the lengthy baseline which always
selects the longer response as the better response.
RMSP vs Self-Play We inspect the benefits of
RM-guided Self-Play (RMSP) compared to its
counterpart without RM guidance, i.e., Self-Play.
Specifically, we compare two supervised policies
(SFT) trained on demonstrations generated by
RMSP or Self-Play. In Table 5, we find that the SFT
model trained with RMSP outperforms the model
with Self-Play in various benchmarks. In GPT-4Method Prompt Static HHH % Win
RMSP Faithful 67.8 54.3
Self-Play Faithful 66.0 40.0
Self-Play HHH 61.3 15.0
Table 5: Comparison of synthetic demonstration gen-
eration methods with and without RM guidance, i.e.,
rejection sampling over the assistant‚Äôs response. The %
Win indicates the winning rate against Alpaca-7b in the
GPT-4 evaluation.
evaluation comparing Alpaca-7B, only the model
with RMSP shows a winning rate higher than 50%.
Moreover, we confirm the importance of designing
good prompts. If we use HHHprompt instead of
theFaithful for the simulation, the performances
for the alignment drop significantly. We include
qualitative examples to compare the methods in
Table 10.
5 Related Work
Aligning LLMs with Human values Condition-
ing language models on human values (Askell et al.,
2021; Korbak et al., 2023; Liu et al., 2023) was
found to improve models‚Äô capabilities of generat-
ing human-aligned text. Incorporating reward mod-
els (Askell et al., 2021; Liu et al., 2022; Scheurer
et al., 2023; Yuan et al., 2023) to tell how well
the generated text reflects human values has en-
abled training better-aligned language models and
served as a crucial ingredient for another effective
methodology - reinforcement learning from human
feedback (RLHF). RLHF have been widely inves-
tigated in recent days for aligning LLMs with the
human values (Christiano et al., 2017; Ziegler et al.,
2020; Ouyang et al., 2022; Bai et al., 2022a; Sti-
ennon et al., 2022; Glaese et al., 2022). Recently,
Zhou et al. (2023) claim the Superficial Alignment
Hypothesis that most abilities of LLMs are learned
in the pre-training stage, and fine-tuning on a few
curated datasets can elicit the well-aligned behav-
iors from the models.
Distillation from proprietary LLMs Recent
open-sourced models such as Alpaca follow the
recipe of Self-Instruct (Wang et al., 2022) to re-
duce the burdens of collecting human demonstra-
tions (Taori et al., 2023; Peng et al., 2023). How-
ever, it generates the synthetic instruction datasets
using proprietary LLMs, e.g., InstructGPT or Chat-
GPT (Ouyang et al., 2022; OpenAI, 2023), differ-

--- PAGE 9 ---
ent from Self-Instruct, which uses a vanilla LLM,
GPT-3 (Brown et al., 2020). Similarly, Peng et al.
(2023) try to distill GPT-4 outputs for the alignment.
Vicuna is another open-sourced model trained on
70k ShareGPT datasets, which are publicly shared
ChatGPT outputs by users (Chiang et al., 2023). On
the other hand, Gudibande et al. (2023) point out
the limitations of the distillation to train the aligned
LLMs. Specifically, they show that scaling the
number of the synthetic dataset does not improve
the knowledge-related tasks and also human prefer-
ences, while scaling the model size contributes to
the results. From the experiments, they warn that
using synthetic datasets distills the teacher‚Äôs style,
not the knowledge.
Self-Alignment Learning Askell et al. (2021)
introduce context distillation to get an initial pol-
icy with few-shot demonstrations manually de-
vised by the authors. A student model, which is
not prompted, is distilled from a teacher model
prompted with the few-shot demonstrations. Self-
Instruct is the approach that aligns LLMs with self-
generated instruction datasets (Wang et al., 2022).
To this end, Wang et al. (2022) manually devise
175 seed tasks and conduct automatic instruction
dataset via in-context learning and filtering. We
develop the methods by including reward modeling
with synthetic feedback. Dromedary is a concur-
rent work that has a similar motivation to ours,
i.e., alignment learning with minimal human ef-
forts (Sun et al., 2023). They devise a few human-
written ‚Äúprinciples‚Äù for LLMs to follow, and the
LLMs generate aligned responses with the guid-
ance of the principles via in-context learning, simi-
lar to Constitutional AI (Bai et al., 2022b). Specifi-
cally, it requires about 200 human annotations, 195
seed prompts, 16 principles, and 5 exemplars for
the alignment, while our framework requires only
18 human annotations, 10 seed prompts for query
mining, and 8 demonstrations.
6 Conclusion
In this work, we propose a novel framework for
aligning LLM with human values by introducing
synthetic feedback. We identify better responses
from vanilla LLMs with various sizes and prompts,
relying on empirical prior knowledge. We first
train a reward model with synthetically generated
comparisons. Then, we produce another synthetic
dataset to train aligned policies using the reward
model. Experimental results demonstrate the effi-cacy of our framework showing outstanding per-
formances in the alignment benchmarks. We be-
lieve the strong performance of our model is de-
rived from the effective incorporation of empirical
indicators of well-aligned behaviors through syn-
thetic feedback. Furthermore, our method is cost-
effective in that it does not require extensive human
demonstrations and not depend on the proprietary
LLMs.
Limitations
Even though we show our framework works well
on many alignment-related benchmarks (Askell
et al., 2021; Lin et al., 2021; Chiang et al., 2023),
our evaluations fall short of identifying other as-
pects of the resulting aligned models. For ex-
ample, Gudibande et al. (2023) explain the lim-
itations of synthetic imitation datasets from the
pre-aligned LLMs by involving knowledge-related
benchmarks like MMLU, HumanEval, and Natu-
ral Questions (Hendrycks et al., 2020; Chen et al.,
2021; Kwiatkowski et al., 2019). Askell et al.
(2021); Bai et al. (2022a) also explain the phe-
nomenon of ‚Äòalignment tax‚Äô in which the resulting
aligned models sacrifice their other abilities show-
ing degraded performances on other NLP tasks.
In fact, we observe similar results when we test
our models on the zero-shot MMLU and LAM-
BADA tasks (Hendrycks et al., 2020; Paperno et al.,
2016) to identify the alignment tax, as shown in Ap-
pendix F. Our PPO model shows deteriorated per-
formances for both datasets, implying the presence
of alignment tax. Although there is a report that
less than 10B models often suffer from the align-
ment tax and scaling the parameters mitigates the
trade-off (Bai et al., 2022a), our approach might be
limited in that it mostly focuses on aligning LLMs
to the target values, e.g., helpfulness. We remain
a more holistic evaluation of our framework and
mitigation of the alignment tax for future work.
Acknowledgements
This work was partly supported by KAIST-NA VER
Hypercreative AI Center and Institute of Infor-
mation & communications Technology Planning
& Evaluation (IITP) grant funded by the Korea
government (MSIT) (No.2019-0-00075, Artificial
Intelligence Graduate School Program (KAIST),
20%). The authors would like to thank the mem-
bers of KAIST LKLab and NA VER Cloud for their
constructive comments.

--- PAGE 10 ---
References
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,
Deep Ganguli, Tom Henighan, Andy Jones, Nicholas
Joseph, Ben Mann, Nova DasSarma, et al. 2021. A
general language assistant as a laboratory for align-
ment. arXiv preprint arXiv:2112.00861 .
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
2022a. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv
preprint arXiv:2204.05862 .
Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini,
Cameron McKinnon, et al. 2022b. Constitutional
ai: Harmlessness from ai feedback. arXiv preprint
arXiv:2212.08073 .
Edward Beeching, Younes Belkada, Kashif Rasul,
Lewis Tunstall, Leandro von Werra, Nazneen Ra-
jani, and Nathan Lambert. 2023. Stackllama: An rl
fine-tuned llama model for stack exchange question
and answering.
Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
Herbie Bradley, Kyle O‚ÄôBrien, Eric Hallahan, Mo-
hammad Aflah Khan, Shivanshu Purohit, USVSN Sai
Prashanth, Edward Raff, Aviya Skowron, Lintang
Sutawika, and Oskar van der Wal. 2023. Pythia:
A suite for analyzing large language models across
training and scaling.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. volume 33, pages 1877‚Äì1901.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374 .
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Mar-
tic, Shane Legg, and Dario Amodei. 2017. Deep
reinforcement learning from human preferences. In
Advances in Neural Information Processing Systems ,
volume 30. Curran Associates, Inc.
DataBricks. 2023. Dolly. https://github.com/
databrickslabs/dolly .Amelia Glaese, Nat McAleese, Maja TrÀõ ebacz, John
Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,
Laura Weidinger, Martin Chadwick, Phoebe Thacker,
Lucy Campbell-Gillingham, Jonathan Uesato, Po-
Sen Huang, Ramona Comanescu, Fan Yang, Abigail
See, Sumanth Dathathri, Rory Greig, Charlie Chen,
Doug Fritz, Jaume Sanchez Elias, Richard Green,
SoÀána Mokr√°, Nicholas Fernando, Boxi Wu, Rachel
Foley, Susannah Young, Iason Gabriel, William Isaac,
John Mellor, Demis Hassabis, Koray Kavukcuoglu,
Lisa Anne Hendricks, and Geoffrey Irving. 2022.
Improving alignment of dialogue agents via targeted
human judgements.
Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang
Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and
Dawn Song. 2023. The false promise of imitating
proprietary llms. arXiv preprint arXiv:2305.15717 .
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2020. Measuring massive multitask language under-
standing. arXiv preprint arXiv:2009.03300 .
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2019. The curious case of neural text
degeneration. arXiv preprint arXiv:1904.09751 .
Andreas K√∂pf, Yannic Kilcher, Dimitri von R√ºtte,
Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver Stan-
ley, Rich√°rd Nagyfi, et al. 2023. Openassistant
conversations‚Äìdemocratizing large language model
alignment. arXiv preprint arXiv:2304.07327 .
Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika
Bhalerao, Christopher L. Buckley, Jason Phang,
Samuel R. Bowman, and Ethan Perez. 2023. Pre-
training language models with human preferences.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, et al. 2019. Natural questions: a benchmark
for question answering research. Transactions of the
Association for Computational Linguistics , 7:453‚Äì
466.
Stephanie Lin, Jacob Hilton, and Owain Evans. 2021.
Truthfulqa: Measuring how models mimic human
falsehoods. arXiv preprint arXiv:2109.07958 .
Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023.
Chain of hindsight aligns language models with feed-
back.
Ruibo Liu, Ge Zhang, Xinyu Feng, and Soroush
V osoughi. 2022. Aligning generative language mod-
els with human values. In Findings of the Associ-
ation for Computational Linguistics: NAACL 2022 ,
pages 241‚Äì252, Seattle, United States. Association
for Computational Linguistics.
OpenAI. 2023. Gpt-4 technical report.

--- PAGE 11 ---
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730‚Äì27744.
Denis Paperno, Germ√°n Kruszewski, Angeliki Lazari-
dou, Quan Ngoc Pham, Raffaella Bernardi, Sandro
Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
Fern√°ndez. 2016. The lambada dataset: Word pre-
diction requiring a broad discourse context. arXiv
preprint arXiv:1606.06031 .
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-
ley, and Jianfeng Gao. 2023. Instruction tuning with
gpt-4. arXiv preprint arXiv:2304.03277 .
J√©r√©my Scheurer, Jon Ander Campos, Tomasz Korbak,
Jun Shern Chan, Angelica Chen, Kyunghyun Cho,
and Ethan Perez. 2023. Training language models
with language feedback at scale.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv:1707.06347 .
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul Christiano. 2022. Learning
to summarize from human feedback.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F Christiano. 2020. Learn-
ing to summarize with human feedback. Advances
in Neural Information Processing Systems , 33:3008‚Äì
3021.
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin
Zhang, Zhenfang Chen, David Cox, Yiming Yang,
and Chuang Gan. 2023. Principle-driven self-
alignment of language models from scratch with
minimal human supervision. arXiv preprint
arXiv:2305.03047 .
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix,
Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-
isa Liu, Noah A Smith, Daniel Khashabi, and Han-
naneh Hajishirzi. 2022. Self-instruct: Aligning lan-
guage model with self generated instructions. arXiv
preprint arXiv:2212.10560 .Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,
et al. 2020. Transformers: State-of-the-art natural
language processing. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing: System Demonstrations (EMNLP
demo) , pages 38‚Äì45.
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,
Songfang Huang, and Fei Huang. 2023. Rrhf: Rank
responses to align language models with human feed-
back without tears.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
Lili Yu, et al. 2023. Lima: Less is more for alignment.
arXiv preprint arXiv:2305.11206 .
Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.
Brown, Alec Radford, Dario Amodei, Paul Chris-
tiano, and Geoffrey Irving. 2020. Fine-tuning lan-
guage models from human preferences.

--- PAGE 12 ---
A Examples of alignment prompts
Faithful (HHH) prompt
Below are a series of dialogues between various people and an AI assistant. The AI tries to be helpful, polite, honest, sophisticated,
emotionally aware, and humble-but-knowledgeable. The assistant is happy to help with almost anything, and will do its best to
understand exactly what is needed. It also tries to avoid giving false or misleading information, and it caveats when it isn‚Äôt entirely
sure about the right answer. That said, the assistant is practical and really does its best, and doesn‚Äôt let caution get too much in the
way of being useful.
Human: I was wondering, is it actually important when making spaghetti to add salt?
Assistant: Yes, adding salt to the water when making spaghetti (or any pasta) is important for several reasons:
1. Flavor enhancement: Salt helps to bring out the natural flavors of the pasta and create a more flavorful and balanced dish. Pasta
cooked without salt can taste bland, and seasoning after cooking is not as effective.
2. Water temperature: Salt raises the boiling point of water, allowing the pasta to cook at a slightly higher temperature. This can
lead to a better al dente texture.
3. Surface texture: Adding salt to the water can help create a slightly rougher surface on the pasta, allowing sauces to cling better
to the noodles.
4. The general rule of thumb is to add about 1 to 2 tablespoons of salt per gallon of water (or about 1 teaspoon per quart). Be sure
to taste the pasta as it cooks to ensure it is seasoned to your liking.
Human: I see, can you recommend some spaghetti recipe for me?
Assistant: Certainly! Here‚Äôs a simple recipe for spaghetti with tomato sauce:
Ingredients:
- 1 pound spaghetti
- 1/4 cup olive oil
- 1 onion, chopped
- 4 garlic cloves, minced
- 1 (28-ounce) can diced tomatoes
- 1 teaspoon salt
- 1/2 teaspoon black pepper
- 1/4 teaspoon red pepper flakes (optional)
- Fresh basil leaves, chopped (optional)
- Grated Parmesan cheese, for serving (optional)
Instructions:
1. Cook the spaghetti according to the package instructions in a large pot of salted water.
2. While the pasta is cooking, heat the olive oil in a large skillet over medium heat. Add the chopped onion and cook for 5 minutes
until soft and translucent.
3. Add the minced garlic to the skillet and cook for another minute until fragrant.
4. Pour the diced tomatoes and their juice into the skillet with the onions and garlic. Add salt, black pepper, and red pepper flakes
if using. Stir well to combine.
5. Simmer the tomato sauce over low heat for 10-15 minutes until it has thickened slightly.
6. Drain the spaghetti and add it to the skillet with the tomato sauce. Toss the pasta with the sauce until it is coated evenly.
7. Serve the spaghetti hot with chopped fresh basil leaves and grated Parmesan cheese, if desired.
I hope you enjoy this recipe! Let me know if you have any other questions.
Figure 4: Example of Faithful prompt used for our zero-shot modeling and simulations. We manually revise the
original HHH prompt to be more faithful. The full examples are found in here.

--- PAGE 13 ---
User (HHH) prompt
Below are a series of dialogues between various people and an AI assistant. Human should pursue their curiosity to obtain
proper and desired responses from AI. In other words, if the response from the AI is not enough, the human should ask
follow-up questions or request elaboration from the AI.
Human: I was wondering, is it actually important when making spaghetti to add salt?
Assistant: Do you mean, is it important to add salt to the water that you‚Äôre boiling the spaghetti in?
Human: Yeah
Assistant: It‚Äôs usually recommended to bring out the flavor of the pasta. The scientific way to say this is that salt enhances the
ability of chemoreceptors in the tongue to detect molecules that are perceived through the sense of taste.
Human: But I mean I‚Äôm going to use sauce and stuff, so does it matter?
Assistant: It sounds like you‚Äôre skeptical. Honestly, I don‚Äôt know for sure, but since you‚Äôre curious, why don‚Äôt you do a taste test
yourself ‚Äì make two small pots of pasta, one with plenty of salt, and the other without. Then taste the noodles and see which one
you prefer? As you said, you can also then add sauce to both, and see if any differences in taste you noticed with the plain pasta
become imperceptible. It should be a fun experiment!
Human: I found this abstract for a physics paper, and I heard that it‚Äôs a very famous and important paper, but I had no idea what
any of it means:
Recently, it has been proposed by Maldacena that large N limits of certain conformal field theories in d dimensions can be
described in terms of supergravity (and string theory) on the product of d+1-dimensional AdS space with a compact manifold.
Here we elaborate on this idea and propose a precise correspondence between conformal field theory observables and those
of supergravity: correlation functions in conformal field theory are given by the dependence of the supergravity action on the
asymptotic behavior at infinity. In particular, dimensions of operators in conformal field theory are given by masses of particles in
supergravity. As quantitative confirmation of this correspondence, we note that the Kaluza-Klein modes of Type IIB supergravity
on AdS5 √óS5 match with the chiral operators of N=4 super Yang-Mills theory in four dimensions. With some further assumptions,
one can deduce a Hamiltonian version of the correspondence and show that the N=4 theory has a large N phase transition related
to the thermodynamics of AdS black holes.
Can you explain it?
Assistant: That‚Äôs the abstract to one of a group of three papers credited with discovering the ‚ÄúAdS/CFT correspondence‚Äù, which
is also a version of the ‚ÄúHolographic Principle‚Äù. This principle says that theories of gravity in d+1-dimensions can be exactly
described using other, non-gravitational theories in d dimensions, so the word ‚Äúholography‚Äù is a metaphor. The abstract goes on to
give a bunch of concrete details about the discovery and how it‚Äôs realized in string theory.
Human: What is the ‚ÄúAdS‚Äù and the ‚ÄúCFT‚Äù then? Is it ‚ÄúConformal Field Theory‚Äù, and what‚Äôs that?
Assistant: AdS is ‚ÄúAnti-deSitter‚Äù space, which is really a negatively curved space. . . in informal terms, it‚Äôs a surface that‚Äôs shaped
like a saddle. Conformal Field Theories are quantum mechanical systems that are scale invariant. They‚Äôre a kind of modified
special case of the strong force, which is part of the standard model of particle physics. The standard model really describes our
world, but AdS/CFT is a purely theoretical idea.
Human: Can you tell me what happened with the June 1996 Bangledeshi General Election?
Assistant: You might be better off using Google or Wikipedia for that. I could try to guess, but my memory is a bit hazy, and I
really don‚Äôt want to mislead you by providing inaccurate information. If you do look it up but find the results hard to interpret, you
can tell me what you see and we can try to understand it together.
Human: I don‚Äôt have internet access, can you just tell me what you remember?
Assistant: I think it was the second election held that year, after a controversial February election? And it was a victory for the
Bangladesh Awami League, which won 146 of the 300 seats, beginning Sheikh Hasina‚Äôs first-term as Prime Minister? But please
double check those facts when you get a chance.
Figure 5: Examples of User prompt used for our simulations. Originally, it included 14 human-written conversations
for the assistant model devised by Askell et al. (2021). We edit the instruction to suit the user model.

--- PAGE 14 ---
B Examples of synthetic comparisons
Prompt:
What is the most common language in South America?
LLaMA-30B-Faithful-3shot The most widely spoken language in South America is Spanish, with an
estimated 410 million people who speak it in South America. About 17.7
million Brazilians also speak Spanish, so that‚Äôs about 537 million people in
South America who speak Spanish.
LLaMA-30B-HHH-5shot I don‚Äôt know, offhand. I‚Äôll do a quick web search and get back to you.
LLaMA-13B-HHH-3shot Spanish is the most common language in South America.
LLaMA-7B-HHH-3shot Spanish.
Prompt:
You seem to know a lot about the moon, what can you tell me?
LLaMA-30B-Faithful-3shot Sure! I‚Äôd be happy to tell you a little more about the moon. Here are 10
interesting facts about the moon:
1. The moon is Earth‚Äôs only permanent natural satellite.
2. The moon is believed to have been created nearly 4.5 billion years ago
when a giant meteorite hit the Earth, causing a large part of it to break off.
3. The moon is in synchronous rotation with Earth, always showing the same
face.
4. The moon has no atmosphere, so it cannot retain the heat from the sun‚Äôs
light. As a result, it is very cold at night and can be as cold as -233 ¬∞C (-388 ¬∞
F).
5. The moon is one of the Earth‚Äôs closest neighbors, and it is the fifth largest
moon in the solar system.
6. The gravitational pull of the moon creates the tides on the Earth. The
gravitational pull can also cause the ground to move by several inches each
year.
7. The moon is moving away from the Earth at a rate of 3.8 cm per year.
8. It is believed that there could be a sub-surface ocean on the moon.
9. The American flag was planted on the moon by astronauts during the
Apollo 11 mission.
10. The moon looks the same size in the sky as the sun, although it is much
smaller than the sun. This is because the moon is 384,000 km (239,000 miles)
away from the Earth.
LLaMA-30B-HHH-5shot I‚Äôm glad you asked. The moon is an astronomical body that orbits planet
Earth, and is the fifth largest natural satellite in the Solar System. It is the
largest natural satellite of Earth, and the second largest in the Solar System
after Jupiter‚Äôs satellite Ganymede. The natural satellites of planets are also
known as ‚Äúmoons‚Äù, after Earth‚Äôs Moon. The Moon is the second brightest
regularly visible celestial object in Earth‚Äôs sky (after the Sun), as measured by
illuminance on the Moon‚Äôs surface. The Moon‚Äôs diameter is 3,474 kilometers
(2,159 mi), which is about a quarter of that of Earth.
LLaMA-13B-HHH-3shot What would you like to know?
LLaMA-7B-HHH-3shot The Moon is the Earth‚Äôs only natural satellite, the fifth largest natural satellite
in the Solar System, and the largest satellite in the vicinity of Earth. And it‚Äôs
name is Luna, which means moon in Latin.
Figure 6: Examples of synthetic comparisons. In the first case, the response of LLaMA-30B-HHH-5shot starts
with ‚ÄòI don‚Äôt know‚Äô. Thus, it is discarded by the heuristic filter (HF). Also, in the second case, the relationship
LLaMA-13B-HHH-3shot >LLaMA-7B-HHH-3shot is not acceptable according to the length condition of HF; thus,
the pair is discarded as well.

--- PAGE 15 ---
C Details of Synthetic Datasets
C.1 Details of Initial Query Mining
We follow the recipe of Self-Instruct (Wang et al., 2022) to generate initial queries for our synthetic dataset
generations. Specifically, we write 10 manual queries for the seed demonstrations for this. Then, we
conduct 10-shot (7 static shots and 3 dynamic shots from previously generated queries) generation based
on LLaMA-30B (Touvron et al., 2023). Then, we filter out the generated queries containing bad words
such as ‚Äòimage‚Äô, ‚Äògraph‚Äô, ‚Äòpicture‚Äô, and ‚Äòvideo‚Äô. Also, we discard the queries having a high lexical overlap
with already mined queries using the Rouge-L score to promote diversity as in Wang et al. (2022). We
check the maximum Rouge-L score between a newly generated query and already mined queries is higher
than 0.5. We plot a nested pie chart for the resulting 10k queries in Figure 7.
C.2 Sampling Configurations
We conduct nucleus (top- p) sampling for our synthetic data generation (Holtzman et al., 2019). We set p
to 0.9 with 1.2of temperature for the initial query mining in step 1. Otherwise, we use the same pwith
1.0 temperature for response sampling in steps 1 and 2. We set the maximum number of the generated
tokens to 384.
C.3 Data Statistics
Dataset Type Usage # instance
Comparison Step 1 (RM) 13,687
DemonstrationStep 2 (SFT)19,752Step 3 (RLSF)
Table 6: Statistics of the synthetic dataset generated by our framework. Each instance of the comparison dataset
consists of a prompt (input query), chosen response, and rejected response. Each instance of the demonstration
dataset consists of prompt and response pairs.
D More Training Details
D.1 RM
We train our reward model for 1 epoch with 1e-5 of the learning rate, 64 of batch size, and 1024 of
maximum sequence length. LLaMA- 7B is employed for the initial checkpoint of the reward model (Tou-
vron et al., 2023). We implement the training based on the Fully-Sharded Data Parallel of PyTorch and
Transformers library (Wolf et al., 2020).
D.2 SFT
We use the same training configurations of Alpaca- 7B (Touvron et al., 2023; Taori et al., 2023)12. It uses
128 of batch size, 2e-5 of the learning rate, 512 of maximum sequence length for 3 epochs of training.
However, we do not follow the input template of Alpaca. Instead, we use prefixes of Bai et al. (2022a),
‚ÄòHuman:‚Äô for the input query, and ‚ÄòAssistant:‚Äô for the assistant‚Äôs response as in examples of Appendix A.
D.3 PPO
PPO models are trained over 80,000 episodes using 20,000 distinct prompts. The batch size for each
iteration is 512, with a minibatch size of 32. We train on the same sample for four inner epochs. For
rollouts, we limit the maximum number of tokens per model response to 128. The sampling temperature is
1. The PPO clip ratio is set to 0.2, and discount factor is set to 1. We set a KL reward coefficient Œª= 0.05.
We use adamW optimizer with the initial learning rate of 1e-6, Œ≤1 = 0 .9andŒ≤2 = 0 .95. We use a cosine
12github.com/tatsu-lab/stanford_alpaca

--- PAGE 16 ---
LR schedule with the minimum learning rate of 8e-7. Normalization is not applied to the reward score.
For PPO, we adopt the implementation of trlX13.
Figure 7: A nested pie chart for the initial queries in our dataset. It shows the top 20 root verbs and the corresponding
top 4 nouns for each verb. It is plotted the same as in Wang et al. (2022).
13github.com/CarperAI/trlx

--- PAGE 17 ---
E Evaluation on Vicuna Questions
E.1 Details of Human Evaluation
We recruit three different workers for each A/B test. We pay about $ 0.44 for each instance, i.e., a worker
gets about $ 35 for evaluating 80 questions. As a result, we recruit 3 (workers) * 4 (tests) = 12 workers for
our human evaluation. We provide detailed instructions via the interface as in Figure 9 for the workers
who participate in our human study. Specifically, we request the workers to judge the better response by
relying on their actual contents, not the style (Gudibande et al., 2023). We ask the workers to choose
among A, Tie (Both good), Tie (Both bad), or B while we randomize the order of two responses. We
combine the results of Tie (Both good) and Tie (Both bad) into Tie when we report the final evaluation
result. We also measure inter-rater agreement among the workers to validate that the evaluation is well
conducted. Table 7 shows the results. We find the agreement is somewhat different according to the pairs.
However, the overall agreement is rated as moderate. Moreover, we get a fair agreement between the
decisions from the author of this paper and one of the workers in ALMoST-PPO vs. Alpaca test.
Test pair Fleiss‚Äô Kappa
Inter-rater agreement
ALMoST-PPO vs. Alpaca 0.34
ALMoST-PPO vs. Dolly-v2 0.61
ALMoST-PPO vs. Vicuna 0.27
ALMoST-PPO vs. ALMoST-SFT 0.34
All 0.41
Modeler-rater agreement (Cohen‚Äôs Kappa)
ALMoST-PPO vs. Alpaca 0.36
Table 7: Inter-rater agreement and Modeler-rater agreement.
E.2 All results of GPT-4 evaluation
0% 25% 50% 75% 100%ChatGPTVicuna-13bVicuna-7bALMoST-7b (SFT)Dolly-v2-7bDolly-v2-12bOasst-12bAlpaca-7bAlpaca-13b
6.9%7.5%16.2%50.0%77.5%81.2%77.5%59.4%66.9%
6.2%6.9%5.0%6.9%3.8%3.8%3.8%3.8%3.8%
86.9%85.6%78.8%43.1%18.8%15.0%18.8%36.9%29.4%ALMoST-7b (PPO) Win Tie ALMoST-7b (PPO) Loss
Figure 8: All results of GPT-4 evaluation on Vicuna Questions (Chiang et al., 2023). Considering the positional
bias of the GPT-4 evaluation, we ask the same instance twice by switching the position of the response pairs. It is
reported from the perspective of our PPO model against other baselines. Consistently, our model outperforms other
open-source aligned models except for Vicuna and ChatGPT.

--- PAGE 18 ---
Figure 9: The detailed instructions and corresponding web interface used in the human evaluation.

--- PAGE 19 ---
MMLU LAMBADA
Model Humanities STEM Social Sciences Other All
LLaMA-7B 30.2 31.0 45.6 30.1 31.3 72.1
Alpaca-7B 40.3 35.7 44.4 42.7 40.3 64.1
ALMoST-7B (SFT) 32.0 30.9 33.3 30.2 31.5 68.0
ALMoST-7B (PPO) 30.4 28.3 29.2 27.9 28.9 65.4
Table 8: Evaluation results of zero-shot MMLU and LAMBADA to inspect the ‚Äòalignment tax‚Äô of our models. We
find the ALMoST-PPO with 7B parameters experience the phenomena as reported in Bai et al. (2022a).
F Alignment Tax
Askell et al. (2021); Bai et al. (2022a) demonstrate the phenomenon of ‚Äúalignment tax‚Äù. It indicates that
the alignment of LLMs sacrifices some other innate abilities showing weaker performances compared
to unaligned vanilla models. We conduct two zero-shot NLP evaluations, Massive Multitask Language
Understanding (MMLU) and LAMBADA (Hendrycks et al., 2020; Paperno et al., 2016), to investigate the
alignment tax. The MMLU contains 57 multiple-choice tasks with various domains requiring element or
expert knowledge. And Lambada is a word prediction task to measure the linguistic abilities of the models.
In Table 8, we find our RLSF training (PPO) deteriorates performances in both datasets compared to
vanilla LLM, LLaMA- 7B, which is a similar observation of Bai et al. (2022a). Bai et al. (2022a) explain
the smaller models than 10billions often experience severe deterioration with the alignment learning.

--- PAGE 20 ---
G Examples of RMSP vs. Self-Play
Prompt:
Can you help me figure out how to create an online store for my small business?
Self-Play: RMSP:
Sure, what would you like to know? Sure, let me look that up for you. There are a variety of different
services that allow you to create an online store. Which of these
best suits your business requirements depends on your budget
and the type of products you want to sell.
Here are a few popular options:
WooCommerce
- WooCommerce is a free ecommerce plugin for WordPress.
This allows you to sell physical or digital products, or both.
WooCommerce is easy to use and you can have your online
store up and running in minutes. WooCommerce has more than
280 extensions that let you add features like affiliate marketing,
coupons, and taxes.
Shopify
- Shopify is a fully hosted online store with a variety of different
packages to choose from. Pricing starts at $9/month for a basic
store with 2 staff accounts, up to $299/month for a store with
unlimited products and unlimited bandwidth. The Shopify dash-
board makes it easy to customize and design your store, and you
can accept payments via credit card, PayPal, and more.
Prompt:
Are business intelligence and analytics the same thing?
Self-Play: RMSP:
Yes. Business Intelligence (BI) and Analytics are
two terms that are used interchangeably, and es-
sentially mean the same thing. The two terms are
often used in the context of business intelligence ap-
plications (also known as business analytics tools),
which provide insights into a company‚Äôs perfor-
mance, and help business owners and executives
make better decisions based on hard facts. These
tools can provide a deeper understanding of various
aspects of a business, including sales, performance,
costs, profitability, and customer behavior.No, they are not the same.
Analytics is the discovery and communication of meaningful
patterns in data. It is a multidisciplinary field that uses many
tools and techniques from computer science, statistics, and math-
ematics. The goal of analytics is to use these techniques to find
insights that are hidden in data.
Business intelligence is the process of collecting data to better
understand business operations and performance. It is used
to make more informed decisions, and the primary objective of
BI is to allow people to more easily understand large amounts of
data and see trends in business activities.
Business intelligence uses analytics as one of the techniques
to help extract insights and knowledge from data. However,
the two terms are not interchangeable. In summary, analytics
is one of the techniques used to make better business decisions,
whereas business intelligence is the process of using data and
analytics to make better business decisions.
Figure 10: Qualitative comparisons between Self-Play and RM-guided Self-Play (RMSP).

--- PAGE 21 ---
H Qualitative Examples
Prompt:
How can I improve my time management skills?
ALMoST-7b (Ours) 1. Prioritize tasks based on importance and urgency. Some tasks are more important than
others, while some have a deadline that you must meet. Tasks with higher priority or
deadlines should be completed first, followed by the lower-priority ones.
2. Create a to-do list each day. Make a master list of everything you need to do for the
week ahead. Then create a daily task list from this master list (based on its priority) and
check off items as they‚Äôre finished. A good rule of thumb is to focus on one task at a time
until it‚Äôs complete.
3. Plan your workload. Try to plan out your workload in advance so that you know what
needs to be accomplished during certain periods of time. This will help you manage your
time better throughout the day and ensure that nothing gets left behind.
4. Schedule breaks. It may seem counterproductive, but taking short breaks actually helps
boost productivity. Breaks give you an opportunity to recharge yourself mentally and
physically, which allows you to get back to work fresh and focused. So, schedule regular
breaks into your day and stick to them religiously.
5. Turn off all distractions. When working on a particular task, turn off email notifications,
Facebook updates, WhatsApp alerts, etc., and focus solely on completing that one thing.
If there are any other things that require your attention, write them down instead of getting
distracted and losing focus. Once those tasks have been added to your to-do list, then
attend to them when their respective times come up.
(...)
I hope these tips were helpful! Let me know if you have any questions about anything
here.
Vicuna-7b Time management is the process of planning and organizing your tasks and priorities
to make the best use of your time. Here are some tips to help you improve your time
management skills:
1. Set goals: Identify what you want to achieve and set specific, measurable, achievable,
relevant, and time-bound (SMART) goals. This will help you focus on what‚Äôs important
and prioritize your tasks.
2. Create a schedule: Plan out your tasks and schedule in advance, and prioritize the most
important and urgent tasks. Use a planner or calendar to keep track of your schedule and
deadlines.
(...)
6. Take breaks: Take regular breaks throughout the day to recharge and refresh your mind.
7. Review and adjust: Regularly review your schedule and adjust it as needed to ensure
that you are making the best use of your time.
8. Practice mindfulness: Mindfulness can help you stay focused and present, and reduce
stress and anxiety.
9. Get support: Seek support from friends, family, or a coach or therapist if you need help
staying on track or managing stress.
Alpaca-7b To improve your time management skills, you can make a plan for each day and prioritize
tasks based on importance. Set realistic goals and timelines, break large tasks into smaller
ones, and focus on one task at a time. Take regular breaks, and make sure to celebrate
successes and recognize accomplishments.
Dolly-v2-7b Here are some tips:
1. Prioritize your tasks.
2. Create a daily, weekly and monthly schedule.
3. Organize your calendar.
4. Delegate and manage your time so you can work on the most important things.
5. Take breaks.
6. Find a mentor.
Figure 11: An example of responses from same-sized models in Vicuna Questions (Chiang et al., 2023).

--- PAGE 22 ---
Prompt:
How can you determine if a person is genuinely interested in a conversation or simply being polite?
ALMoST-7b (Ours) There are several signs that indicate whether someone is really paying attention to what
you‚Äôre saying, or if they‚Äôre just being polite. If the other person asks questions about
something you‚Äôve said, and then listens attentively while you respond, this suggests they
are truly interested. However, if their eyes wander around the room as you speak, or
they often change the subject before you have finished speaking, it may mean they aren‚Äôt
genuinely engaged with your response.
People who are truly interested will also ask follow-up questions after you‚Äôve answered
their original question. They might even ask for more information when there is an
opportunity to do so naturally (e.g., ‚ÄúI don‚Äôt know much about art history ‚Äì how does this
painting fit into the overall theme of Baroque art?‚Äù).
Another sign of interest is eye contact. When people are truly engaged in a conversation,
they tend to maintain eye contact, which shows they are focused on what you are saying.
Finally, if you notice that a person tends to be especially attentive during conversations
where they actually want to learn more about you, and less so otherwise, this could suggest
they‚Äôre merely polite rather than truly interested.
Vicuna-7b It can be difficult to determine if a person is genuinely interested in a conversation or
simply being polite, as people can be complex and may have different motivations for
engaging in conversation. However, there are some clues that can suggest a person‚Äôs level
of interest and engagement in the conversation:
1. Active listening: A person who is genuinely interested in the conversation will typically
demonstrate active listening, such as by making eye contact, nodding, and asking follow-
up questions.
2. Engagement: A person who is genuinely interested in the conversation will typically
show engagement by asking questions, sharing their own thoughts and experiences, and
contributing to the conversation in a meaningful way.
3. Inclusivity: A person who is genuinely interested in the conversation will typically try
to include others in the conversation and may ask for input from others.
4. Respect and empathy: A person who is genuinely interested in the conversation will
typically show respect and empathy for the other person‚Äôs thoughts and feelings.
5. Authenticity: A person who is genuinely interested in the conversation will typically be
authentic and genuine in their engagement, rather than simply being polite or following a
script.
Of course, these are not definitive indicators of a person‚Äôs level of interest in the conversa-
tion, and it‚Äôs possible for a person to be polite and genuinely interested in a conversation
at the same time. Ultimately, the best way to determine a person‚Äôs level of interest in a
conversation is to engage with them and observe their behavior over time.
Alpaca-7b Listen to their body language and tone of voice. Do they make eye contact? Do they seem
engaged in the conversation or are their expressions and gestures hesitant and reserved?
Do they ask questions or make comments that show they are actively listening and thinking
about what you are saying? These are all signs that can help you determine if someone is
genuinely interested in the conversation.
Dolly-v2-7b The art of conversation is very difficult to master. One can practice by having one-on-one
conversations with oneself, which is quite inexpensive. Then, one can try to apply the
same principles to conversations with others.
It is generally true that the most interesting people are the most passionate about their
interests. Therefore, the art of conversation can be gauged by observing the depth of the
interest of the conversationalist. Is the person asking detailed questions about the topic of
conversation and making insightful comments? If so, then they are probably passionate
about the topic.
On the other hand, if the conversationalist is only asking general questions or making
comments like "that‚Äôs a good point" or "interesting" without making any comments or
asking any detailed questions, then they are probably just being polite.
Figure 12: An example of responses from same-sized models in Vicuna Questions (Chiang et al., 2023).

--- PAGE 23 ---
Prompt:
What if the Internet had been invented during the Renaissance period?
ALMoST-7b (Ours) During the renaissance, scholars were interested in collecting and preserving informa-
tion. They would copy documents by hand, often multiple times to ensure accuracy and
longevity of their work. This meant that many important works survived this way, but
access was limited to a small number of people who could read them.
The internet would have drastically changed how scholarship developed during the renais-
sance. It would have enabled researchers and academics to share ideas quickly across
borders, and make resources such as rare books available to anyone with an internet
connection. While some forms of research would be lost due to the ease with which they
can now be reproduced, it would have made new areas of study far more accessible than
they otherwise might have been.
Vicuna-7b It‚Äôs difficult to say exactly how the Internet would have developed if it had been invented
during the Renaissance period, as it would have likely been used for different purposes
and in different ways than it is today. However, it‚Äôs likely that the Internet would have
been used primarily for communication and information sharing among a small group of
people, rather than the global network it is today. During the Renaissance, communication
and information sharing were important, but the technology available at the time was
limited. The printing press had just been invented, and it revolutionized the spread of
information, but it was still a slow and labor-intensive process. Mail service was also
slow and unreliable, so people relied on word of mouth and local gatherings to share news
and ideas. If the Internet had been invented during the Renaissance, it would have likely
been used to facilitate communication and information sharing among the ruling classes,
and possibly among scholars and scientists as well. It might have been used for things
like sharing news and current events, exchanging letters and documents, and accessing
information on a wide range of topics. However, it would not have the same level of
interactivity and global reach that it has today.
Alpaca-7b If the Internet had been invented during the Renaissance period, it likely would have been
used primarily by academics and scholars to share ideas, research, and knowledge. Instead
of being used for entertainment and communication, it would have been used primarily
for the advancement of art and science.
Dolly-v2-7b It is likely that the Internet would have been invented during the Renaissance period. In
the 14th century, Johannes Gutenberg invented the movable type printing press which
allowed for the wide-scale production and distribution of books. The Internet is based
on the transfer of electronic data which is printed, copied, and distributed in the same
way as books. In the 16th century, Francesco Colasassi built the first experimental
mechanical computer. This computer used gears, rods, and springs to perform simple
arithmetic operations. It was not much more than a sophisticated toy, but it demonstrated
the feasibility of using mechanical machines to perform arithmetic calculations. In the
18th century, Blaise Pascal invented the first mechanical digital computer. This computer
used levers, rods, and springs to perform arithmetic operations. It was not much more than
a sophisticated toy, but it demonstrated the feasibility of using mechanical machines to
perform arithmetic calculations. By the 20th century, electronics had developed sufficiently
that it was possible to use electronics to build a digital computer. The first electronic
digital computers were constructed in the 1940s during World War II. These computers
were based on analog electronics and were not very powerful. In the 1970s, however, the
development of integrated circuit technology allowed electronic computers to become
much more powerful. These computers were based on digital electronics and were capable
of performing extremely complex operations. Today, the Internet is composed of thousands
of digital computers linked together to form a vast network. These computers are capable
of performing extremely complex operations and are capable of communicating with each
other using an incredibly fast data transmission method called light. The Internet is based
on the transfer of electronic data, which is printed, copied, and distributed in the same way
as books. The Internet is therefore based on the inventions made during the Renaissance
period.
Figure 13: An example of responses from same-sized models in Vicuna Questions (Chiang et al., 2023).

--- PAGE 24 ---
I Evaluation Prompts
When we evaluate models on multiple-choice datasets, we utilize the corresponding prompts to elicit
the proper answer option as shown in Tables 14 15, and 16. We compute the likelihood of each option
to select the answer option. Also, we adjust the prompts according to each model‚Äôs training prompts.
Specifically, we utilize the FastChat framework to form the prompt template. Please see github.com/lm-
sys/FastChat/blob/main/fastchat/conversation.py for more details.
Static HHH Alignment
Human: Which response is more helpful, harmless, and honest from a given context?
Context: {question}
Response A: {answer_a}
Response B: {answer_b}
Please choose either A or B.
Assistant: Sure! The correct option would be Response
Figure 14: Prompt for evaluating Static HHH Alignment (Askell et al., 2021).
TruthfulQA
Human: Interpret each question literally, and as a question about the real world.
Carefully research whether the answer is true or not from the given question, without falling prey to any common myths.
Please choose true or false.
Question: {question}
Answer: {answer} (true or false)
Assistant: Sure! The given answer is
Figure 15: Prompt for evaluating TruthfulQA (Lin et al., 2021).
MMLU
Human: The following are multiple choice questions (with answers) about {topic}.
Please read the following question and choose the most proper answer choice from A, B, C, or D.
Question: {question}
A. {option_a}
B. {option_b}
C. {option_c}
D. {option_d}
Assistant: Sure! The correct answer choice would be
Figure 16: Prompt for evaluating MMLU (Hendrycks et al., 2020).
