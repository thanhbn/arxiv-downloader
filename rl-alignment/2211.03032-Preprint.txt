# 2211.03032.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2211.03032.pdf
# File size: 828293 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Preprint
Decentralized Policy Optimization
Kefan Su
School of Computer Science
Peking University
sukefan@pku.edu.cnZongqing Lu
School of Computer Science
Peking University
zongqing.lu@pku.edu.cn
Abstract
The study of decentralized learning or independent learning in cooperative multi-
agent reinforcement learning has a history of decades. Recently empirical studies
show that independent PPO (IPPO) can obtain good performance, close to or
even better than the methods of centralized training with decentralized execution,
in several benchmarks. However, decentralized actor-critic with convergence
guarantee is still open. In this paper, we propose decentralized policy optimization
(DPO), a decentralized actor-critic algorithm with monotonic improvement and
convergence guarantee. We derive a novel decentralized surrogate for policy
optimization such that the monotonic improvement of joint policy can be guaranteed
by each agent independently optimizing the surrogate. In practice, this decentralized
surrogate can be realized by two adaptive coefﬁcients for policy optimization at
each agent. Empirically, we compare DPO with IPPO in a variety of cooperative
multi-agent tasks, covering discrete and continuous action spaces, and fully and
partially observable environments. The results show DPO outperforms IPPO in
most tasks, which can be the evidence for our theoretical results.
1 Introduction
In cooperative multi-agent reinforcement learning (MARL), centralized training with decentralized
execution (CTDE) has been the primary framework (Lowe et al., 2017; Foerster et al., 2018; Sunehag
et al., 2018; Rashid et al., 2018; Wang et al., 2021a; Zhang et al., 2021; Yu et al., 2021). Such a
framework can settle the non-stationarity problem with the centralized value function, which takes
as input the global information and is beneﬁcial to the training process. Conversely, decentralized
learning has been paid much less attention. The main reason may be that there are few theoretical
guarantees for decentralized learning and the interpretability is insufﬁcient even if the simplest form
of decentralized learning, i.e., independent learning, can obtain good empirical performance in several
benchmarks (Papoudakis et al., 2021). However, decentralized learning itself still deserves attention
as there are still many settings in which the global information cannot be accessed by each agent, and
also for better robustness and scalability (Zhang et al., 2019). Moreover, the idea of decentralized
learning is direct, comprehensible, and easy to realize in practice.
Independent Q-learning (IQL) (Tampuu et al., 2015) and independent PPO (IPPO) (de Witt et al.,
2020) are the straightforward decentralized learning methods for cooperative MARL, where each
agent learns the policy by DQN (Mnih et al., 2015) and PPO (Schulman et al., 2017) respectively.
Empirical studies (de Witt et al., 2020; Yu et al., 2021; Papoudakis et al., 2021) demonstrate that
these two methods can obtain good performance, close CTDE methods. Especially, IPPO can
outperform several CTDE methods in a few benchmarks, including MPE (Lowe et al., 2017) and
SMAC (Samvelyan et al., 2019), which shows great promise for decentralized learning. Unfortunately,
to the best of our knowledge, there is still no theoretical guarantee or rigorous explanation for IPPO,
though there has been some study (Sun et al., 2022).
In this paper, we make a further step and propose decentralized policy optimization (DPO ), a
decentralized actor-critic method with monotonic improvement and convergence guarantee for
cooperative multi-agent reinforcement learning. Similar to IPPO, DPO is actually independent
learning , because in DPO each agent optimizes its own objective individually and independently.
However, unlike IPPO, such an independent policy optimization of DPO can guarantee the monotonic
improvement of the joint policy.
1arXiv:2211.03032v1  [cs.LG]  6 Nov 2022

--- PAGE 2 ---
Preprint
From the essence of fully decentralized learning, we ﬁrst analyze Q-function in the decentralized
setting and further show that the optimization objective of IPPO may not induce the joint policy
improvement. Then, starting from the surrogate of TRPO (Schulman et al., 2015) and together
considering the characteristics of fully decentralized learning, we introduce a novel lower bound
of joint policy improvement as the surrogate for decentralized policy optimization. This surrogate
can be naturally decomposed for each agent, which means each agent can optimize its individual
objective to make sure that the joint policy improves monotonically. In practice, this decentralized
surrogate can be realized by two adaptive coefﬁcients for policy optimization at each agent. The idea
of DPO is simple yet effective, and suitable for fully decentralized learning.
Empirically, we compare DPO and IPPO in a variety of cooperative multi-agent tasks including a
cooperative stochastic game, MPE (Lowe et al., 2017), multi-agent MuJoCo (Peng et al., 2021),
and SMAC (Samvelyan et al., 2019), covering discrete and continuous action spaces, and fully and
partially observable environments. The empirical results show that DPO performs better than IPPO
in most tasks, which can be evidence for our theoretical results.
2 Related Work
CTDE. In cooperative MARL, centralized training with decentralized execution (CTDE) is the most
popular framework (Lowe et al., 2017; Iqbal & Sha, 2019; Foerster et al., 2018; Sunehag et al., 2018;
Rashid et al., 2018; Wang et al., 2021a; Zhang et al., 2021; Peng et al., 2021). CTDE algorithms
can handle the non-stationarity problem in the multi-agent environment by the centralized value
function. One line of research in CTDE is value decomposition (Sunehag et al., 2018; Rashid et al.,
2018; Son et al., 2019; Yang et al., 2020; Wang et al., 2021a), where a joint Q-function is learned and
factorized into local Q-functions by the relationship between optimal joint action and optimal local
actions. Another line of research in CTDE is multi-agent actor-critic (Foerster et al., 2018; Iqbal
& Sha, 2019; Wang et al., 2021b; Zhang et al., 2021; Su & Lu, 2022), where the centralized value
function is learned to provide policy gradients for agents to learn stochastic policies. More recently,
policy optimization has attracted much attention for cooperative MARL. PPO (Schulman et al.,
2017) and TRPO (Schulman et al., 2015) have been extended to multi-agent settings by MAPPO (Yu
et al., 2021), CoPPO (Wu et al., 2021), and HAPPO (Kuba et al., 2021) respectively via learning
a centralized state value function. However, these methods are CTDE and thus not appropriate for
decentralized learning.
Fully decentralized learning. Independent learning (OroojlooyJadid & Hajinezhad, 2019) is the
most straightforward approach for fully decentralized learning and has actually been studied in
cooperative MARL since decades ago. The representatives are independent Q-learning (IQL) (Tan,
1993; Tampuu et al., 2015) and independent actor-critic (IAC) as Foerster et al. (2018) empirically
studied. These methods make agents directly execute the single-agent Q-learning or actor-critic
algorithm individually. The drawback of such independent learning methods is obvious. As other
agents are also learning, each agent interacts with a non-stationary environment, which violates
the stationary condition of MDP. Thus, these methods are not with any convergence guarantee
theoretically, though IQL could obtain good performance in several benchmarks (Papoudakis et al.,
2021). More recently, decentralized learning has also been speciﬁcally studied with communication
(Zhang et al., 2018; Li et al., 2020) or parameter sharing (Terry et al., 2020). However, in this paper,
we consider fully decentralized learning as each agent independently learning its policy while being
not allowed to communicate or share parameters as in Tampuu et al. (2015); de Witt et al. (2020). We
will propose an algorithm with convergence guarantees in such a fully decentralized learning setting.
IPPO. TRPO (Schulman et al., 2015) is an important single-agent actor-critic algorithm that limits
the policy update in a trust region and has a monotonic improvement guarantee by optimizing a
surrogate objective. PPO (Schulman et al., 2017) is a practical but effective algorithm derived from
TRPO, which replaces the trust region constraint with a simpler clip trick. IPPO (de Witt et al.,
2020) is a recent cooperative MARL algorithm in which each agent just learns with independent PPO.
Though IPPO is still with no convergence guarantee, it obtains surprisingly good performance in
SMAC (Samvelyan et al., 2019). IPPO is further empirically studied by Yu et al. (2021); Papoudakis
et al. (2021). Their results show IPPO can outperform a few CTDE methods in several benchmark
tasks. These studies demonstrate the potential of policy optimization in fully decentralized learning,
which we will focus on in this paper. Although there are some value-based algorithms for fully
2

--- PAGE 3 ---
Preprint
decentralized learning (Tan, 1993; Matignon et al., 2007; Palmer et al., 2017), most of them are
heuristic and follow the principle different from policy-based algorithms, so we will not focus on
these algorithms.
3 Method
From the perspective of policy optimization, in fully decentralized learning, we need to ﬁnd an
objective for each agent such that the joint policy improvement can be guaranteed by each agent
independently and individually optimizing its own objective. Thus, we propose a novel lower bound
of the joint policy improvement to enable decentralized policy optimization (DPO). In the following,
we ﬁrst discuss some preliminaries; then we analyze the critic of agent in fully decentralized learning;
next we derive the lower bound and the proof for convergence; ﬁnally we introduce the practical
algorithm of DPO.
3.1 Preliminaries
Dec-POMDP. Decentralized partially observable Markov decision process is a general model for
cooperative MARL. A Dec-POMDP is a tuple G=fS;A;P;Y;O;I;N;r; g.Sis the state space,
Nis the number of agents, 2[0;1)is the discount factor, and I=f1;2Ngis the set of all
agents.A=A1A2ANrepresents the joint action space, where Aiis the individual action
space for agent i.P(s0js;a) :SAS![0;1]is the transition function, and r(s;a) :SA!R
is the reward function of state s2Sand joint action a2A.Yis the observation space, and
O(s;i) :SI!Yis a mapping from state to observation for each agent i. The objective of Dec-
POMDP is to maximize J() =E[P
t=0tr(st;at)];thus we need to ﬁnd the optimal joint policy
= arg maxJ(). To settle the partial observable problem, history i2Ti= (YAi)is often
used to replace observation oi2Y. In fully decentralized learning, each agent iindependently learns
an individual policy i(aiji)and their joint policy can be represented as the product of each i.
Though each agent learns individual policy as i(aiji)in practice, in our analysis, we will assume
that each agent could receive the state s, because the analysis in partially observable environments
is much more difﬁcult and the problem may be undecidable in Dec-POMDP (Madani et al., 1999).
Moreover, the V-function and Q-function of the joint policy are as follows,
V(s) =Ea[Q(s;a)] (1)
Q(s;a) =r(s;a) +Es0P(js;a)[V(s0)]: (2)
Joint TRPO Objective. In Dec-POMDP, we can still obtain a TRPO objective for the joint policy 
from the theoretical results in single-agent RL (Schulman et al., 2015), which is referred to as the
joint TRPO objective,
J(new) J(old)Ljoint
old(new) CDmax
KL(oldknew) (3)
whereLjoint
old(new) =X
sold(s)X
anew(ajs)Aold(s;a); (4)
whereDmax
KL(oldknew) = maxsDKL(old(js)knew(js)),old(s) =P
t=0tPr(st=sjold)
is the discounted stationary distribution of the state given old,Aoldis the advantage function under
old, andCis a constant.
The joint TRPO objective, i.e., RHS of (3), is a lower bound for the difference between the new
joint policynewand the old joint policy oldin term of expected return. Therefore, we can
use this objective as a surrogate, and maximizing this surrogate can guarantee that the policy is
improving monotonically. However, the joint TRPO objective cannot be directly optimized in fully
decentralized learning as this objective is involved in the joint policy, which cannot be accessed in
fully decentralized learning.
We will propose a new lower bound (surrogate) for J(new) J(old), which can be optimized in
fully decentralized learning. Before introducing our new surrogate, we need to ﬁrst analyze the critic
of agent in fully decentralized learning, which is referred to as decentralized critic.
3

--- PAGE 4 ---
Preprint
3.2 Decentralized Critic
In fully decentralized learning, each agent learns independently from its own interactions with the
environment. Therefore, the Q-function of each agent iis actually the following formula:
Qi
 i(s;ai) =r i(s;ai) +Ea i i;s0P(js;ai;a i);a0
ii[Qi
 i(s0;a0
i)]; (5)
wherer i(s;ai) =E i[r(s;ai;a i)], and ianda irespectively denote the joint policy and
joint action of all agents expect agent i. If we take the expectation Ea0
 i i(js0);a i i(js)over
both sides of the Q-function of joint policy (2), then we have
E i[Q(s;ai;a i)] =r i(s;ai) +Ea i i;s0P(js;ai;a i);a0
ii
E i[Q(s0;a0
i;a0
 i)]
:
We can see that Qi
 i(s;ai)andE i[Q(s;ai;a i)]satisfy the same iteration. Moreover, we will
show in the following that Qi
 i(s;ai)andE i[Q(s;ai;a i)]are just the same.
We ﬁrst deﬁne an operator  i
 ias follows,
 i
 iQ(s;ai) =r i(s;ai) +Ea i i;s0P(js;ai;a i);a0
ii[Q(s0;a0
i)]:
Then we will prove that the operator  i
 iis a contraction.
Considering any two individual Q-functions Q1andQ2, we have:
k i
 iQ1  i
 iQ2k1= max
s;aijEa i i;s0P(js;ai;a i);a0
ii[Q1(s0;a0
i) Q2(s0;a0
i)]j
Ea i i;s0P(js;ai;a i);a0
ii[max
s0;a0
ijQ1(s0;a0
i) Q2(s0;a0
i)j]
=max
s0;a0
ijQ1(s0;a0
i) Q2(s0;a0
i)j
=kQ1 Q2k1:
So the operator  i ihas one and only one ﬁxed point, which means
Qi
 i(s;ai) =E i[Q(s;ai;a i)];
Vi
 i(s) =E i[V(s)] =V(s):
With this well-deﬁned decentralized critic, we can further analyze the objective of IPPO (de Witt
et al., 2020). In IPPO, the policy objective of each agent ican be essentially formulated as follows:
Li
old(i
new) =X
sold(s)X
aii
new(aijs)Ai
old(s;ai); (6)
whereAi
old(s;ai) =Qi
old
 i
old(s;ai) Ei
old[Qi
old
 i
old(s;ai)] =E i
old[Aold(s;ai;a i)]:
However, (6)is different from (4)in the joint TRPO objective. Thus, directly optimizing (6)may
not improve the joint policy, and thus cannot provide any guarantee for convergence, to the best of
our knowledge. Nevertheless, it seems that Ai
old(s;ai)is the only advantage formulation that can
be accessed by each agent in fully decentralized learning. So, the policy objective of DPO will be
derived on (6)but with modiﬁcations to guarantee convergence, and we will introduce the detail in
the next section. In the following, we discuss how to compute this advantage in practice in fully
decentralized learning.
As we need to calculate Ai
old(s;ai) =E i
old[r(s;ai;a i) +Vold(s0) Vold(s)]for the policy
update, we can approximate Ai
old(s;ai)with ^Ai(s;ai) =r+Vi
 i(s0) Vi
 i(s), which is an
unbiased estimate of Ai
old(s;ai), though it may be with a large variance. In practice, we can follow the
traditional idea in fully decentralized learning, and let each agent iindependently learn an individual
value function Vi(s). Then, we further have ^Ai(s;ai)r+Vi(s0) Vi(s). The loss for the
decentralized critic is as follows:
Li
critic =E
(Vi(s) yi)2
;whereyi=r+Vi(s0)or Monte Carlo return : (7)
There may be some ways to improve the learning of this critic, which however is beyond the scope of
our discussion.
4

--- PAGE 5 ---
Preprint
3.3 Decentralized Surrogate
We are ready to introduce the decentralized surrogate. First, we derive our novel lower bound of the
joint policy improvement by the following theorem.
Theorem 1. Supposeoldandneware two joint policies. Then, the following bound holds:
J(new) J(old)1
NNX
i=1Li
old(i
new) ~MNX
i=1q
Dmax
KL(i
oldkinew) CNX
i=1Dmax
KL(i
oldki
new);
where ~M=2 max s;ajAold(s;a)j
1 andC=4maxs;ajAold(s;a)j
(1 )2 are two constants.
Proof. We ﬁrst considerLjoint
old(new) Li
old(i
new). According to (4)and(6), we have the following
equation:
Ljoint
old(new) Li
old(i
new)
=X
sold(s)X
aii
new(aijs)X
a i i
new(a ijs)Aold(s;ai;a i) Ai
old(s;ai)
=EoldEinewX
a i 
 i
new(a ijs)  i
old(a ijs)
Aold(s;ai;a i)
:
Then, we have the following inequalities:
jLjoint
old(new) Li
old(i
new)j
EoldEinewX
a ij i
old(a ijs)  i
new(a ijs)jjAold(s;ai;a i)j
EoldEinew
MX
a ij i
old(a ijs)  i
new(a ijs)j
(M= max
s;ajAold(s;a)j)
= 2MEold
DTV( i
old(js)k i
new(js))
2M
1 max
sDTV( i
old(js)k i
new(js))
=~MDmax
TV( i
oldk i
new) ( ~M=2M
1 )
~Mq
Dmax
KL( i
oldk inew) (8)
~MsX
j6=iDmax
KL(j
oldkj
new); (9)
where (8)is from the relationship between the total variance and KL-divergence that DTV(pkq)2
DKL(pkq)(Schulman et al., 2015), and (9)is a property of the KL-divergence, which can be obtained
as follows,
Dmax
KL(oldjjnew) = max
sDKL(old(js)jjnew(js))
= max
sX
iDKL(i
old(js)jji
new(js))
X
imax
sDKL(i
old(js)jji
new(js))
=X
iDmax
KL(i
oldjji
new): (10)
From (9), we can further obtain the following inequality,
Ljoint
old(new) Li
old(i
new)  ~MsX
j6=iDmax
KL(j
oldkj
new): (11)
5

--- PAGE 6 ---
Preprint
Next we will prove this theorem, starting from (3),
J(new) J(old)Ljoint
old(new) CDmax
KL(oldjjnew)
=1
NNX
i=1Ljoint
old(new) CDmax
KL(oldjjnew)
1
NNX
i=1Li
old(i
new) ~M
NNX
i=1sX
j6=iDmax
KL(j
oldkj
new) CDmax
KL(newkold) (12)
1
NNX
i=1Li
old(i
new) ~MvuutN 1
NNX
i=1Dmax
KL(i
oldkinew) CDmax
KL(newkold) (13)
1
NNX
i=1Li
old(i
new) ~MvuutN 1
NNX
i=1Dmax
KL(i
oldkinew) CNX
i=1Dmax
KL(i
oldki
new)(14)
1
NNX
i=1Li
old(i
new) ~MvuutNX
i=1Dmax
KL(i
oldkinew) CNX
i=1Dmax
KL(i
oldki
new)
1
NNX
i=1Li
old(i
new) ~MNX
i=1q
Dmax
KL(i
oldkinew) CNX
i=1Dmax
KL(i
oldki
new): (15)
The inequality (12) is the direct application of the inequality (11). The inequality (13) is from the
Cauthy-Schwarz inequality,
NX
i=1sX
j6=iDmax
KL(j
oldkj
new)s
NX
i=1X
j6=iDmax
KL(j
oldkj
new)
=s
N(N 1)X
i=1Dmax
KL(i
oldkinew):
The inequality (14) is from (10), while the inequality (15) is from the simple inequalitypP
iaiP
ipai(ai0;8i).
The lower bound in Theorem 1 is dedicated to decentralized policy optimization, because it can be
directly decomposed individually for each agent as a decentralized surrogate. From Theorem 1, if we
set the policy optimization objective of each agent ias
i
new= arg max
i1
NLi
old(i) ~Mq
Dmax
KL(i
oldki) CDmax
KL(i
oldki)
; (16)
then we have J(new)J(old)from TRPO (Schulman et al., 2015). Moreover, as the objective
J()is bounded, the convergence of fJ(t)gis guaranteed, where tis the joint policy after
titerations according to (16).Therefore, the joint policy of agents improves monotonically and
converges to sub-optimum by fully decentralized policy optimization, i.e., independent learning.
Note that this result is under the assumption that each agent can obtain the state, and in practice each
agent will take the individual trajectory ias the approximation to the state.
3.4 Algorithm
DPO is with a simple idea that each agent optimizes the decentralized surrogate (16). However, we
face the same trouble as TRPO that the constant ~MandCare large and if we directly optimize this
objective, then the step size of the policy update will be small.
To settle this problem, we absorb the idea of the adaptive coefﬁcient in PPO (Schulman et al., 2017).
We use two adaptive coefﬁcients i
1andi
2to replace the constant ~MandCand additionally replace
6

--- PAGE 7 ---
Preprint
Algorithm 1. DPO
1:forepisode = 1toMdo
2: fort= 1to max_episode_length do
3: select action aii(js)
4: execute action aiand observe reward rand next state s0
5: collecths;ai;r;s0i
6: end for
7: Update decentralized critic according to (7)
8: Update policy according to the surrogate (17)
9: Updatei
1andi
2according to (18).
10:end for
the maximum KL-divergence with the average KL-divergence. In practice, we will actually optimize
the following objective
i
new= arg max
i1
NLi
old(i) i
1q
Davg
KL(i
oldki) i
2Davg
KL(i
oldki)
; (17)
whereDavg
KL(i
oldki) =Esold
DKL(i
old(js)ki(js))
.
As for the adaption of i
1andi
2, we need to deﬁne a hyperparameter dtarget , which can be seen as a
ruler for the average KL-divergence Davg
KL(i
oldki
new)for each agent. If Davg
KL(i
oldki
new)is close
todtarget , then we believe current i
1andi
2are appropriate. If Davg
KL(i
oldki
new)exceedsdtarget
too much, we believe i
1andi
2are small and need to increase and vice versa. In practice, we will
use the following rule to update i
1andi
2:
IfDavg
KL(i
oldki
new)>dtarget;theni
j i
j!8j2f1;2g
IfDavg
KL(i
oldki
new)<dtarget=; theni
j i
j=!8j2f1;2g:(18)
We choose the constants = 1:5and!= 2as the choice in PPO (Schulman et al., 2017). As for the
critic, we just follow the standard method in PPO. Then, we can have the fully decentralized learning
procedure of DPO for each agent iin Algorithm 1.
The practical algorithm of DPO actually uses some approximations to the decentralized surrogate.
Most of these approximations are traditional practice in RL or with no alternative in fully decentralized
learning yet. We admit that the practical algorithm may not maintain the theoretical guarantee.
However, we need to argue that we go one step further to give a decentralized surrogate in fully
decentralized learning with convergence guarantee. We believe and expect that a better practical
method can be found based on this objective in future work.
4 Experiments
In this section, we compare the practical algorithm of DPO with IPPO (de Witt et al., 2020) in a variety
of cooperative multi-agent environments, including a cooperative stochastic game, MPE (Lowe et al.,
2017), multi-agent MuJoCo (Peng et al., 2021), and SMAC (Samvelyan et al., 2019), covering
both discrete and continuous action spaces, and fully and partially observable environments. As we
consider fully decentralized learning, in the experiments agents do not use parameter-sharing as
sharing parameters should be considered as centralized learning (Terry et al., 2020). In all experiments,
the network architectures and common hyperparameters of DPO and IPPO are the same for a fair
comparison. More details about experimental settings and hyperparameters are available in Appendix.
Moreover, all the learning curves are from 5 random seeds and the shaded area corresponds to the
95% conﬁdence interval.
4.1 A Didactic Example
First, we use a cooperative stochastic game as a didactic example. The cooperative stochastic game is
with 100 states, 6 agents and each agent has 5 actions. All the agents share a joint reward function.
The reward function and the transition probability are both generated randomly. This stochastic
7

--- PAGE 8 ---
Preprint
0 100000 200000 300000 400000 500000
steps0100200300400500mean episode rewardsStochastic Game
OPTIMAL
DPO
IPPO
(a) DPO compared with IPPO
0 100000 200000 300000 400000 500000
steps0100200300400mean episode rewardsStochastic Game
dtarget=0.001
dtarget=0.01
dtarget=0.1
dtarget=1 (b) the inﬂuence of dtarget
Figure 1: Empirical studies of DPO on the didactic example: (a) learning curve of DPO compared
with IPPO and the global optimum; (b) the inﬂuence of different values of dtarget on DPO, x-axis is
environment steps.
0.0 0.2 0.4 0.6 0.8 1.0
steps 1e650
40
30
20
mean episode rewardssimple spread
DPO
IPPO
0.0 0.2 0.4 0.6 0.8 1.0
steps 1e650
40
30
20
line control
DPO
IPPO
0.0 0.2 0.4 0.6 0.8 1.0
steps 1e650
40
30
20
10
circle control
DPO
IPPO
Figure 2: Learning curve of DPO compared with IPPO in 5-agent simple spread, 5-agent line control,
and 5-agent circle control in MPE, where x-axis is environment steps.
game has a certain degree of complexity which is helpful to distinguish the performance of DPO and
IPPO. On the other hand, this environment is tabular which means training in this environment is fast
and we can do ablation studies efﬁciently. Moreover, we can ﬁnd the global optimum by dynamic
programming to compare with in this game.
The learning curves in Figure 1a show that DPO performs better than IPPO and learns a better solution
in this environment. The fact that DPO learns a sub-optimal solution agrees with our theoretical
result. However, the sub-optimal solution found by DPO is still away from the global optimum. This
means that there is still improvement space.
On the other hand, we study the inﬂuence of the hyperparameter dtarget on DPO. We choose
dtarget = 0:001;0:01;0:1;1. The empirical results are shown in Figure 1b. We ﬁnd that when
dtarget is small, the coefﬁcient 1and2are more likely to be increased and the step size of the
policy update is limited. So for the case that dtarget = 0:001;0:01, the performance of DPO is
relatively low. And when dtarget is large, the policy update may be out of the trust region. This
can be witnessed by the ﬂuctuating learning curve of the case dtarget = 1. So we need to choose
an appropriate value for dtarget and in this environment we choose dtarget = 0:1, which is also
the learning curve of DPO in Figure 1a. We found that the appropriate value for dtarget changes
in different environments. In the following, we keep dtarget to be the same for tasks of the same
environment. There may be some better choices for dtarget , but it is a bit time-consuming and out of
the range of our discussion.
4.2 MPE
MPE is a popular environment in cooperative MARL. MPE is a 2D environment and the objects
in MPE environment are either agents or landmarks. Landmark is a part of the environment, while
agents can move in any direction. With the relation between agents and landmarks, we can design
different tasks. We use the discrete action space version of MPE and the agents can accelerate or
decelerate in the direction of x-axis or y-axis. We choose MPE for its partial observability. We take
dtarget = 0:01for all MPE tasks.
The MPE tasks we used for the experiments are simple spread, line control, and circle control which
are originally used in Agarwal et al. (2020). In our experiments, we set the number of agents N= 5
in all these three tasks. The empirical results are illustrated in Figure 2. We can ﬁnd that although
DPO may fall behind IPPO at the beginning of the training in some tasks, DPO learns a better policy
in the end for all three tasks.
8

--- PAGE 9 ---
Preprint
0.0 0.5 1.0 1.5 2.0
steps 1e605001000150020002500mean episode rewardsHopper-v2 3x1
DPO
IPPO
0.0 0.5 1.0 1.5 2.0
steps 1e601000200030004000HalfCheetah-v2 3x2
DPO
IPPO
0 1 2 3 4 5
steps 1e60100020003000Walker2d-v2 3x2
DPO
IPPO
0 1 2 3 4
steps 1e6100015002000Ant-v2 4x2
DPO
IPPO
0 1 2 3 4
steps 1e6200300400500Humanoid-v2 17x1
DPO
IPPO
Figure 3: Learning curve of DPO compared with IPPO in 3-agent Hopper, 3-agent HalfCheetah,
3-agent Walker2d, 4-agent Ant, and 17-agent Humanoid in multi-agent MuJoCo, where x-axis is
environment steps.
0.0 0.5 1.0 1.5 2.0
steps 1e60.00.20.40.6win rates2s3z
DPO
IPPO
0.0 0.2 0.4 0.6 0.8 1.0
steps 1e60.00.20.40.60.8win rates8m
DPO
IPPO
0 1 2 3 4 5
steps 1e60.00.20.40.60.8win rates3s5z
DPO
IPPO
0.0 0.5 1.0 1.5 2.0
steps 1e651015mean episode rewards27m vs 30m
DPO
IPPO
0.0 0.5 1.0 1.5 2.0
steps 1e6246810mean episode rewardsMMM2
DPO
IPPO
Figure 4: Learning curve of DPO compared with IPPO in 2s3z, 8m, 3s5z, 27m_vs_30m, and MMM2
in SMAC, where x-axis is environment steps.
4.3 Multi-Agent MuJoCo
Multi-agent MuJoCo is a robotic locomotion control environment for multi-agent settings, which
is built upon single-agent MuJoCo (Todorov et al., 2012). In multi-agent MuJoCo, each agent
controls one part of a robot to carry out different tasks. We choose this environment for the reason of
continuous state and action spaces. We select 5 tasks for our experiments: 3-agent Hopper, 3-agent
HalfCheetah, 3-agent Walker2d, 4-agent Ant and 17-agent Humanoid. We take dtarget = 0:001for
all multi-agent MuJoCo tasks.
The empirical results are illustrated in Figure 3. We can ﬁnd that in all ﬁve tasks, DPO outperforms
IPPO, though in 3-agent HalfCheetah DPO learns slower than IPPO at the beginning. The results on
multi-agent MuJoCo verify that DPO is also effective in facing continuous state and action spaces.
Moreover, the better performance of DPO in the 17-agent Humanoid task could be evidence of the
scalability of DPO.
4.4 SMAC
SMAC is a partially observable and high-dimensional environment that has been used in many
cooperative MARL studies. We select ﬁve maps in SMAC, 2s3z, 8m, 3s5z, 27m_vs_30m, and
MMM2 for our experiments. We take dtarget = 0:02for all SMAC tasks.
The empirical results are illustrated in Figure 4. The two super hard SMAC tasks (27m_vs_30m and
MMM2) are too difﬁcult for both DPO and IPPO to win, so we use episode reward as the metric to
show their difference. DPO performs better than IPPO in all ﬁve maps. We need to argue that though
we have controlled the network architectures of DPO and IPPO to be the same, in our experiments
each agent has its individual parameters which increases the difﬁculty of training. So our results in
SMAC may be different from other works. Although IPPO has been shown to perform well in SMAC
(de Witt et al., 2020; Yu et al., 2021; Papoudakis et al., 2021), DPO can still outperform IPPO, which
veriﬁes the effectiveness of the practical algorithm of DPO in high-dimensional complex tasks and
can also be evidence of our theoretical result. Again, the better performance of DPO in 27m_vs_30m
shows its good scalability in the task with many agents.
5 Conclusion
In this paper, we investigate fully decentralized learning in cooperative multi-agent reinforcement
learning. We derive a novel decentralized lower bound for the joint policy improvement and we
propose DPO, a fully decentralized actor-critic algorithm with convergence guarantee and monotonic
improvement. Empirically, we test DPO compared with IPPO in a variety of environments including
a cooperative stochastic game, MPE, multi-agent MuJoCo, and SMAC, covering both discrete and
continuous action spaces, and fully and partially observable environments. The empirical results
show the advantage of DPO over IPPO, which can be evidence for our theoretical results.
9

--- PAGE 10 ---
Preprint
References
Akshat Agarwal, Sumit Kumar, Katia Sycara, and Michael Lewis. Learning transferable cooperative
behavior in multi-agent team. In International Conference on Autonomous Agents and Multiagent
Systems (AAMAS) , 2020.
Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS
Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft
multi-agent challenge? arXiv preprint arXiv:2011.09533 , 2020.
Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In AAAI Conference on Artiﬁcial Intelligence (AAAI) ,
2018.
Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning (ICML) , 2019.
Jakub Grudzien Kuba, Ruiqing Chen, Munning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong
Yang. Trust region policy optimisation in multi-agent reinforcement learning. arXiv preprint
arXiv:2109.11251 , 2021.
Wenhao Li, Bo Jin, Xiangfeng Wang, Junchi Yan, and Hongyuan Zha. F2a2: Flexible fully-
decentralized approximate actor-critic for cooperative multi-agent reinforcement learning. arXiv
preprint arXiv:2004.11145 , 2020.
Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems (NeurIPS) , 2017.
Omid Madani, Steve Hanks, and Anne Condon. On the undecidability of probabilistic planning and
inﬁnite-horizon partially observable markov decision problems. In AAAI/IAAI , pp. 541–548, 1999.
Laëtitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Hysteretic q-learning: an algo-
rithm for decentralized reinforcement learning in cooperative multi-agent teams. In IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS) , 2007.
V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature , 518(7540):529–533, 2015.
Afshin OroojlooyJadid and Davood Hajinezhad. A review of cooperative multi-agent deep reinforce-
ment learning. arXiv preprint arXiv:1908.03963 , 2019.
Gregory Palmer, Karl Tuyls, Daan Bloembergen, and Rahul Savani. Lenient multi-agent deep
reinforcement learning. arXiv preprint arXiv:1707.04402 , 2017.
Georgios Papoudakis, Filippos Christianos, Lukas Schäfer, and Stefano V Albrecht. Benchmarking
multi-agent deep reinforcement learning algorithms in cooperative tasks. In Advances in Neural
Information Processing Systems (NeurIPS) , 2021.
Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr,
Wendelin Böhmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy
gradients. In Advances in Neural Information Processing Systems (NeurIPS) , 2021.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster,
and Shimon Whiteson. Qmix: monotonic value function factorisation for deep multi-agent
reinforcement learning. In International Conference on Machine Learning (ICML) , 2018.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli,
Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The
starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043 , 2019.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning (ICML) , 2015.
10

--- PAGE 11 ---
Preprint
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to
factorize with transformation for cooperative multi-agent reinforcement learning. In International
Conference on Machine Learning (ICML) , 2019.
Kefan Su and Zongqing Lu. Divergence-regularized multi-agent actor-critic. In International
Conference on Machine Learning (ICML) , 2022.
Mingfei Sun, Sam Devlin, Katja Hofmann, and Shimon Whiteson. Monotonic improvement guaran-
tees under non-stationarity for decentralized ppo. arXiv preprint arXiv:2202.00082 , 2022.
Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinícius Flores Zam-
baldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-
decomposition networks for cooperative multi-agent learning based on team reward. In Interna-
tional Conference on Autonomous Agents and MultiAgent Systems (AAMAS) , 2018.
Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan
Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning.
arXiv preprint arXiv:1511.08779 , 2015.
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In International
Conference on Machine Learning (ICML) , 1993.
Justin K Terry, Nathaniel Grammel, Ananth Hari, Luis Santos, and Benjamin Black. Revisiting
parameter sharing in multi-agent deep reinforcement learning. arXiv preprint arXiv:2005.13625 ,
2020.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
InIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , 2012.
Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling
multi-agent q-learning. In International Conference on Learning Representations (ICLR) , 2021a.
Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Dop: Off-policy multi-
agent decomposed policy gradients. In International Conference on Learning Representations
(ICLR) , 2021b.
Zifan Wu, Chao Yu, Deheng Ye, Junge Zhang, Hankz Hankui Zhuo, et al. Coordinated proximal
policy optimization. Advances in Neural Information Processing Systems (NeurIPS) , 34, 2021.
Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao Tang.
Qatten: A general framework for cooperative multiagent reinforcement learning. arXiv preprint
arXiv:2002.03939 , 2020.
Hsuan-Yu Yao, Ping-Chun Hsieh, Kuo-Hao Ho, Kai-Chun Hu, Liang-Chun Ouyang, I Wu, et al.
Hinge policy optimization: Rethinking policy improvement and reinterpreting ppo. arXiv preprint
arXiv:2110.13799 , 2021.
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955 , 2021.
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Baar. Fully decentralized multi-
agent reinforcement learning with networked agents. In International Conference on Machine
Learning (ICML) , 2018.
Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Multi-agent reinforcement learning: A selective
overview of theories and algorithms. arXiv preprint arXiv:1911.10635 , 2019.
Tianhao Zhang, Yueheng Li, Chen Wang, Guangming Xie, and Zongqing Lu. Fop: Factorizing
optimal joint policy of maximum-entropy multi-agent reinforcement learning. In International
Conference on Machine Learning (ICML) , 2021.
11

--- PAGE 12 ---
Preprint
A Experimental Settings
A.1 MPE
The three tasks are built on the origin MPE (Lowe et al., 2017) (MIT license) and are originally used
in Agarwal et al. (2020) (MIT license). The objective in these three tasks are listed as follows:
•Simple Spread: There areNagents who need to occupy the locations of Nlandmarks.
•Line Control: There areNagents who need to line up between 2 landmarks.
•Circle Control: There areNagents who need to form a circle around a landmark.
The reward in these tasks is the distance between all the agents and their target locations. We set the
number of agents N= 5for these three tasks in our experiment.
A.2 Multi-Agent MuJoCo
Multi-agent MuJoCo (Peng et al., 2021) (Apache-2.0 license) is a robotic locomotion task with
continuous action space for multi-agent settings. The robot could be divided into several parts and
each part contains several joints. Agents in this environment control a part of the robot which could
be different varieties. So the type of the robot and the assignment of the joints decide a task. For
example, the task ‘HalfCheetah-3 2’ means dividing the robot ‘HalfCheetah’ into three parts for
three agents and each part contains 2 joints.
The details about our experiment settings in multi-agent Mujoco are listed in Table 1. The conﬁgura-
tion deﬁnes the number of agents and the joints of each agent. The ‘agent obsk’ deﬁnes the number
of nearest agents an agent can observe.
Table 1: The task settings of multi-agent MuJoCo
task conﬁguration agent obsk
HalfCheetah 32 2
Hopper 31 2
Walker2d 32 2
Ant 42 2
B Training Details
Our code is based on the open-source code1of MAPPO (Yu et al., 2021) (MIT license). We modify
the code for individual parameters and ban the tricks used by MAPPO for SMAC. The network
architectures and base hyperparameters of DPO and IPPO are the same for all the tasks in all the
environments. We use 3-layer MLPs for the actor and the critic and use ReLU as non-linearities. The
number of the hidden units of the MLP is 128. We train all the networks with an Adam optimizer.
The learning rates of the actor and critic are both 5e-4. The number of epochs for every batch of
samples is 15 which is the recommended value in Yu et al. (2021). For IPPO, the clip parameter is
0.2 which is the same as Schulman et al. (2017). For DPO, the initial values of the coefﬁcient i
1and
i
2are 0.01. The value of dtarget is 0.1 for the cooperative stochastic game, 0.01 for MPE, 0.001 for
multi-agent MuJoCo, and 0.02 for SMAC.
The version of the game StarCraft2 in SMAC is 4.10 for our experiments in all the SMAC tasks. We
set the episode length of all the multi-agent MuJoCo tasks as 1000 in all of our multi-agent MuJoCo
experiments. We perform the whole experiment with a total of four NVIDIA A100 GPUs. We have
summarized the hyperparameters in Table 2.
1https://github.com/marlbenchmark/on-policy
12

--- PAGE 13 ---
Preprint
Table 2: Hyperparameters for all the experiments
hyperparameter value
MLP layers 3
hidden size 128
non-linear ReLU
optimizer Adam
actor_lr 5e-4
critic_lr 5e-4
numbers of epochs 15
initiali
1 0.01
initiali
2 0.01
 1.5
! 2
dtarget different for environments as aforementioned
clip parameter for IPPO 0.2
C Additional Results
Schulman et al. (2017) actually proposed two versions of PPO. The ﬁrst version, which is also the
most popular version, is with the clip trick. The second version is directly optimizing the penalty
formula with adaptive coefﬁcients and we refer to this algorithm as PPO-KL. IPPO (de Witt et al.,
2020) is actually extended from the ﬁrst version, while the practical algorithm of DPO is similar
to the second version. The main difference between DPO and PPO-KL is the term of the square
root of the KL-divergence in the policy loss. We modify IPPO by making each agent learn with
PPO-KL to obtain IPPO-KL. On the other hand, independent Q-learning (IQL) (Tan, 1993) is a
classic independent learning algorithm. IQL could obtain good performance in some tasks but it is
not with any theoretical guarantee, to the best of our knowledge.
0 100000 200000 300000 400000 500000
steps0100200300400500mean episode rewardsStochastic Game
OPTIMAL
DPO
IPPOIPPO-KL
IQL
Figure 5: Learning curve of DPO compared with IPPO, IPPO-KL, IQL, and the global optimum in
the cooperative stochastic game, where x-axis is environment steps.
For the completeness of our experiments, we test the performance of IPPO-KL and IQL in the
cooperative stochastic game and the empirical result is illustrated in Figure 5. We ﬁnd that the
performance of IPPO-KL is close to DPO but a little bit lower and more unstable. This can be
explained as the policy loss of IPPO-KL is actually a biased approximation of DPO, which omits
the square root term. The theoretical guarantee of DPO may also help IPPO-KL perform better than
IPPO in this game, while the bias in the policy loss of IPPO-KL makes it perform worse than DPO.
As for IQL, its performance is lower than IPPO though it converges faster. Since IQL performs worse
than IPPO in such a didactic environment and IQL is a value-based algorithm and is out of the scope
of our discussion, we do not take IQL as a baseline in other environments. Moreover, we think the
essential relations among IPPO, IPPO-KL, and DPO can be clearly witnessed from the empirical
results in the cooperative stochastic game. For other tasks, we focus on the mainstream version of
IPPO as in de Witt et al. (2020); Yu et al. (2021); Papoudakis et al. (2021).
Besides our empirical results, we would like to share our views on the difference between DPO
and IPPO and give some intuitive ideas. KL regularization and ratio clipping are similar in the
13

--- PAGE 14 ---
Preprint
single-agent setting, but they are not supposed to be similar in multi-agent settings. The ‘correct’
ratio clipping in multi-agent setting according to the theory of PPO should clip over the joint policy
rationew(ajs)
old(ajs). IPPO just clips individual policy ratioi
new(aijs)
i
old(aijs)for each agent iwhich may not be
enough to realize the ‘correct’ ratio clipping. We could ﬁnd more discussion about this in the CoPPO
(Wu et al., 2021) paper. So IPPO is not supposed to enjoy the theoretical results of DPO.
We could rewrite the objective of IPPO for agent iwith a similar formulation in HPO (Yao et al.,
2021) as follows:
Li;IPPO
old(i
new) =X
sold(s)X
aii
new(aijs)jAi
old(s;ai)jl 
sign(Ai
old(s;ai));ui(s;ai) 1;
;
wherel(y;x; ) = maxf0; yxgis the hinge loss and ui(s;ai) =i
new(aijs)
i
old(aijs)is the ratio.
If we follow the same idea as PPO, then IPPO is the ‘correct’ ratio clipping version for the surrogate
of DPO. However, the effectiveness of this ratio clipping formulation in theory is still open in
decentralized learning since there is not any convergence guarantee for IPPO, to the best of our
knowledge.
Though the effectiveness of IPPO in theory is beyond the scope of our paper, we could provide an
intuitive explanation for the fact that the performance of DPO can surpass IPPO from this formulation
and the analysis in HPO. In the proof of HPO, there is a critical assumption that the sign of the
estimated advantage is the same as that of the true advantage (Assumption 4 in Section 2.3 in Yao
et al. (2021)). And HPO also shows that the sign of the advantage is more important than the value
for this formulation of PPO-clip. In decentralized learning, both DPO and IPPO are facing the
difﬁculty of learning the individual advantage function as there may be noise in the individual value
function. However, the objective of DPO is continuous and the objective of IPPO is discrete for
sign(Ai
old(s;ai)). So the impact of the noise in the value function may be larger on IPPO than DPO.
D Discussion
In the paper, we derive a novel lower bound that can be naturally divided into independent surrogate
(16) for each agent. By each agent optimizing this surrogate, the monotonic improvement of the joint
policy can be guaranteed in fully decentralized settings. However, the practical algorithm of DPO
takes the formula of (17) with several approximations. How to solve the optimization of (16) more
precisely is left as future work.
Moreover, we expect our work could provide some insights for future studies on fully decentralized
multi-agent reinforcement learning, since current methods still have a gap from the optimum as
shown in Figure 5.
14
