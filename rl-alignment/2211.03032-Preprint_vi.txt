# 2211.03032.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2211.03032.pdf
# Kích thước tệp: 828293 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Bản thảo
Tối ưu hóa Chính sách Phi tập trung
Kefan Su
Khoa Khoa học Máy tính
Đại học Bắc Kinh
sukefan@pku.edu.cnZongqing Lu
Khoa Khoa học Máy tính
Đại học Bắc Kinh
zongqing.lu@pku.edu.cn
Tóm tắt
Nghiên cứu về học phi tập trung hoặc học độc lập trong học tăng cường đa tác nhân hợp tác có lịch sử hàng thập kỷ. Gần đây các nghiên cứu thực nghiệm cho thấy PPO độc lập (IPPO) có thể đạt được hiệu suất tốt, gần bằng hoặc thậm chí tốt hơn các phương pháp huấn luyện tập trung với thực thi phi tập trung, trong một số bài kiểm tra. Tuy nhiên, actor-critic phi tập trung với đảm bảo hội tụ vẫn còn là vấn đề mở. Trong bài báo này, chúng tôi đề xuất tối ưu hóa chính sách phi tập trung (DPO), một thuật toán actor-critic phi tập trung với cải thiện đơn điệu và đảm bảo hội tụ. Chúng tôi suy ra một hàm thay thế phi tập trung mới cho tối ưu hóa chính sách sao cho việc cải thiện đơn điệu của chính sách liên kết có thể được đảm bảo bởi mỗi tác nhân tối ưu hóa hàm thay thế một cách độc lập. Trong thực tế, hàm thay thế phi tập trung này có thể được thực hiện bằng hai hệ số thích ứng cho tối ưu hóa chính sách tại mỗi tác nhân. Về mặt thực nghiệm, chúng tôi so sánh DPO với IPPO trong nhiều nhiệm vụ đa tác nhân hợp tác khác nhau, bao gồm không gian hành động rời rạc và liên tục, và môi trường quan sát hoàn toàn và một phần. Kết quả cho thấy DPO vượt trội hơn IPPO trong hầu hết các nhiệm vụ, có thể là bằng chứng cho kết quả lý thuyết của chúng tôi.

1 Giới thiệu
Trong học tăng cường đa tác nhân hợp tác (MARL), huấn luyện tập trung với thực thi phi tập trung (CTDE) đã là khung chính (Lowe et al., 2017; Foerster et al., 2018; Sunehag et al., 2018; Rashid et al., 2018; Wang et al., 2021a; Zhang et al., 2021; Yu et al., 2021). Khung như vậy có thể giải quyết vấn đề không dừng với hàm giá trị tập trung, nhận thông tin toàn cục làm đầu vào và có lợi cho quá trình huấn luyện. Ngược lại, học phi tập trung đã ít được chú ý hơn nhiều. Lý do chính có thể là có ít đảm bảo lý thuyết cho học phi tập trung và tính diễn giải không đủ ngay cả khi dạng đơn giản nhất của học phi tập trung, tức là học độc lập, có thể đạt được hiệu suất thực nghiệm tốt trong một số bài kiểm tra (Papoudakis et al., 2021). Tuy nhiên, học phi tập trung bản thân vẫn đáng được chú ý vì vẫn còn nhiều thiết lập mà thông tin toàn cục không thể được truy cập bởi mỗi tác nhân, và cũng để có tính mạnh mẽ và khả năng mở rộng tốt hơn (Zhang et al., 2019). Hơn nữa, ý tưởng của học phi tập trung là trực tiếp, dễ hiểu và dễ thực hiện trong thực tế.

Q-learning độc lập (IQL) (Tampuu et al., 2015) và PPO độc lập (IPPO) (de Witt et al., 2020) là các phương pháp học phi tập trung đơn giản cho MARL hợp tác, trong đó mỗi tác nhân học chính sách bằng DQN (Mnih et al., 2015) và PPO (Schulman et al., 2017) tương ứng. Các nghiên cứu thực nghiệm (de Witt et al., 2020; Yu et al., 2021; Papoudakis et al., 2021) chứng minh rằng hai phương pháp này có thể đạt được hiệu suất tốt, gần với các phương pháp CTDE. Đặc biệt, IPPO có thể vượt trội hơn một số phương pháp CTDE trong một vài bài kiểm tra, bao gồm MPE (Lowe et al., 2017) và SMAC (Samvelyan et al., 2019), cho thấy tiềm năng lớn cho học phi tập trung. Thật không may, theo hiểu biết của chúng tôi, vẫn chưa có đảm bảo lý thuyết hoặc giải thích nghiêm ngặt nào cho IPPO, mặc dù đã có một số nghiên cứu (Sun et al., 2022).

Trong bài báo này, chúng tôi tiến thêm một bước và đề xuất tối ưu hóa chính sách phi tập trung (DPO), một phương pháp actor-critic phi tập trung với cải thiện đơn điệu và đảm bảo hội tụ cho học tăng cường đa tác nhân hợp tác. Tương tự như IPPO, DPO thực tế là học độc lập, vì trong DPO mỗi tác nhân tối ưu hóa mục tiêu riêng của mình một cách cá nhân và độc lập. Tuy nhiên, không giống như IPPO, việc tối ưu hóa chính sách độc lập của DPO có thể đảm bảo cải thiện đơn điệu của chính sách liên kết.

--- TRANG 2 ---
Bản thảo
Từ bản chất của học hoàn toàn phi tập trung, chúng tôi đầu tiên phân tích hàm Q trong thiết lập phi tập trung và tiếp tục cho thấy rằng mục tiêu tối ưu hóa của IPPO có thể không tạo ra cải thiện chính sách liên kết. Sau đó, bắt đầu từ hàm thay thế của TRPO (Schulman et al., 2015) và cùng xem xét các đặc điểm của học hoàn toàn phi tập trung, chúng tôi giới thiệu một cận dưới mới của cải thiện chính sách liên kết như hàm thay thế cho tối ưu hóa chính sách phi tập trung. Hàm thay thế này có thể được phân tách tự nhiên cho mỗi tác nhân, có nghĩa là mỗi tác nhân có thể tối ưu hóa mục tiêu cá nhân của mình để đảm bảo rằng chính sách liên kết cải thiện đơn điệu. Trong thực tế, hàm thay thế phi tập trung này có thể được thực hiện bằng hai hệ số thích ứng cho tối ưu hóa chính sách tại mỗi tác nhân. Ý tưởng của DPO đơn giản nhưng hiệu quả, và phù hợp cho học hoàn toàn phi tập trung.

Về mặt thực nghiệm, chúng tôi so sánh DPO và IPPO trong nhiều nhiệm vụ đa tác nhân hợp tác khác nhau bao gồm trò chơi ngẫu nhiên hợp tác, MPE (Lowe et al., 2017), MuJoCo đa tác nhân (Peng et al., 2021), và SMAC (Samvelyan et al., 2019), bao gồm không gian hành động rời rạc và liên tục, và môi trường quan sát hoàn toàn và một phần. Kết quả thực nghiệm cho thấy DPO hoạt động tốt hơn IPPO trong hầu hết các nhiệm vụ, có thể là bằng chứng cho kết quả lý thuyết của chúng tôi.

2 Công trình liên quan
CTDE. Trong MARL hợp tác, huấn luyện tập trung với thực thi phi tập trung (CTDE) là khung phổ biến nhất (Lowe et al., 2017; Iqbal & Sha, 2019; Foerster et al., 2018; Sunehag et al., 2018; Rashid et al., 2018; Wang et al., 2021a; Zhang et al., 2021; Peng et al., 2021). Các thuật toán CTDE có thể xử lý vấn đề không dừng trong môi trường đa tác nhân bằng hàm giá trị tập trung. Một hướng nghiên cứu trong CTDE là phân tách giá trị (Sunehag et al., 2018; Rashid et al., 2018; Son et al., 2019; Yang et al., 2020; Wang et al., 2021a), trong đó hàm Q liên kết được học và phân tích thành các hàm Q cục bộ bằng mối quan hệ giữa hành động liên kết tối ưu và các hành động cục bộ tối ưu. Một hướng nghiên cứu khác trong CTDE là actor-critic đa tác nhân (Foerster et al., 2018; Iqbal & Sha, 2019; Wang et al., 2021b; Zhang et al., 2021; Su & Lu, 2022), trong đó hàm giá trị tập trung được học để cung cấp gradient chính sách cho các tác nhân học các chính sách ngẫu nhiên. Gần đây hơn, tối ưu hóa chính sách đã thu hút nhiều sự chú ý cho MARL hợp tác. PPO (Schulman et al., 2017) và TRPO (Schulman et al., 2015) đã được mở rộng cho thiết lập đa tác nhân bởi MAPPO (Yu et al., 2021), CoPPO (Wu et al., 2021), và HAPPO (Kuba et al., 2021) tương ứng thông qua việc học hàm giá trị trạng thái tập trung. Tuy nhiên, các phương pháp này là CTDE và do đó không phù hợp cho học phi tập trung.

Học hoàn toàn phi tập trung. Học độc lập (OroojlooyJadid & Hajinezhad, 2019) là cách tiếp cận trực tiếp nhất cho học hoàn toàn phi tập trung và thực sự đã được nghiên cứu trong MARL hợp tác từ hàng thập kỷ trước. Các đại diện là Q-learning độc lập (IQL) (Tan, 1993; Tampuu et al., 2015) và actor-critic độc lập (IAC) như Foerster et al. (2018) đã nghiên cứu thực nghiệm. Các phương pháp này làm cho các tác nhân trực tiếp thực thi thuật toán Q-learning hoặc actor-critic đơn tác nhân một cách cá nhân. Nhược điểm của các phương pháp học độc lập như vậy là rõ ràng. Khi các tác nhân khác cũng đang học, mỗi tác nhân tương tác với một môi trường không dừng, vi phạm điều kiện dừng của MDP. Do đó, các phương pháp này không có bất kỳ đảm bảo hội tụ nào về mặt lý thuyết, mặc dù IQL có thể đạt được hiệu suất tốt trong một số bài kiểm tra (Papoudakis et al., 2021). Gần đây hơn, học phi tập trung cũng đã được nghiên cứu cụ thể với giao tiếp (Zhang et al., 2018; Li et al., 2020) hoặc chia sẻ tham số (Terry et al., 2020). Tuy nhiên, trong bài báo này, chúng tôi xem xét học hoàn toàn phi tập trung như mỗi tác nhân học chính sách của mình một cách độc lập trong khi không được phép giao tiếp hoặc chia sẻ tham số như trong Tampuu et al. (2015); de Witt et al. (2020). Chúng tôi sẽ đề xuất một thuật toán với đảm bảo hội tụ trong thiết lập học hoàn toàn phi tập trung như vậy.

IPPO. TRPO (Schulman et al., 2015) là một thuật toán actor-critic đơn tác nhân quan trọng giới hạn cập nhật chính sách trong một vùng tin cậy và có đảm bảo cải thiện đơn điệu bằng cách tối ưu hóa một mục tiêu thay thế. PPO (Schulman et al., 2017) là một thuật toán thực tế nhưng hiệu quả được suy ra từ TRPO, thay thế ràng buộc vùng tin cậy bằng một thủ thuật cắt đơn giản hơn. IPPO (de Witt et al., 2020) là một thuật toán MARL hợp tác gần đây trong đó mỗi tác nhân chỉ học với PPO độc lập. Mặc dù IPPO vẫn không có đảm bảo hội tụ, nó đạt được hiệu suất tốt đáng ngạc nhiên trong SMAC (Samvelyan et al., 2019). IPPO được nghiên cứu thực nghiệm thêm bởi Yu et al. (2021); Papoudakis et al. (2021). Kết quả của họ cho thấy IPPO có thể vượt trội hơn một vài phương pháp CTDE trong một số nhiệm vụ kiểm tra. Các nghiên cứu này chứng minh tiềm năng của tối ưu hóa chính sách trong học hoàn toàn phi tập trung, mà chúng tôi sẽ tập trung trong bài báo này. Mặc dù có một số thuật toán dựa trên giá trị cho học hoàn toàn phi tập trung (Tan, 1993; Matignon et al., 2007; Palmer et al., 2017), hầu hết chúng đều là heuristic và tuân theo nguyên tắc khác với các thuật toán dựa trên chính sách, vì vậy chúng tôi sẽ không tập trung vào các thuật toán này.

3 Phương pháp
Từ góc độ tối ưu hóa chính sách, trong học hoàn toàn phi tập trung, chúng ta cần tìm một mục tiêu cho mỗi tác nhân sao cho việc cải thiện chính sách liên kết có thể được đảm bảo bởi mỗi tác nhân tối ưu hóa mục tiêu riêng của mình một cách độc lập và cá nhân. Do đó, chúng tôi đề xuất một cận dưới mới của cải thiện chính sách liên kết để cho phép tối ưu hóa chính sách phi tập trung (DPO). Trong phần sau, chúng tôi đầu tiên thảo luận về một số kiến thức cơ bản; sau đó chúng tôi phân tích critic của tác nhân trong học hoàn toàn phi tập trung; tiếp theo chúng tôi suy ra cận dưới và bằng chứng cho hội tụ; cuối cùng chúng tôi giới thiệu thuật toán thực tế của DPO.

3.1 Kiến thức cơ bản
Dec-POMDP. Quá trình quyết định Markov có thể quan sát một phần phi tập trung là một mô hình tổng quát cho MARL hợp tác. Một Dec-POMDP là một bộ G={S,A,P,Y,O,I,N,r,γ}. S là không gian trạng thái, N là số lượng tác nhân, γ∈[0,1) là hệ số chiết khấu, và I={1,2,...,N} là tập hợp tất cả các tác nhân. A=A₁×A₂×...×Aₙ đại diện cho không gian hành động liên kết, trong đó Aᵢ là không gian hành động cá nhân cho tác nhân i. P(s'|s,a): S×A×S→[0,1] là hàm chuyển tiếp, và r(s,a): S×A→R là hàm phần thưởng của trạng thái s∈S và hành động liên kết a∈A. Y là không gian quan sát, và O(s,i): S×I→Y là một ánh xạ từ trạng thái đến quan sát cho mỗi tác nhân i. Mục tiêu của Dec-POMDP là tối đa hóa J(π)=E[∑ₜ₌₀^∞ γᵗr(sₜ,aₜ)]; do đó chúng ta cần tìm chính sách liên kết tối ưu π*=arg max J(π). Để giải quyết vấn đề có thể quan sát một phần, lịch sử τᵢ∈Tᵢ=(Y×Aᵢ)* thường được sử dụng để thay thế quan sát oᵢ∈Y. Trong học hoàn toàn phi tập trung, mỗi tác nhân i học một cách độc lập một chính sách cá nhân πᵢ(aᵢ|τᵢ) và chính sách liên kết của họ có thể được biểu diễn như tích của mỗi πᵢ.

Mặc dù mỗi tác nhân học chính sách cá nhân như πᵢ(aᵢ|τᵢ) trong thực tế, trong phân tích của chúng tôi, chúng tôi sẽ giả định rằng mỗi tác nhân có thể nhận được trạng thái s, vì phân tích trong môi trường có thể quan sát một phần khó khăn hơn nhiều và vấn đề có thể không thể quyết định trong Dec-POMDP (Madani et al., 1999). Hơn nữa, hàm V và hàm Q của chính sách liên kết như sau,

V^π(s) = E_a^π[Q^π(s,a)]                                                   (1)
Q^π(s,a) = r(s,a) + γE_{s'~P(·|s,a)}[V^π(s')].                           (2)

Mục tiêu TRPO liên kết. Trong Dec-POMDP, chúng ta vẫn có thể thu được một mục tiêu TRPO cho chính sách liên kết π từ các kết quả lý thuyết trong RL đơn tác nhân (Schulman et al., 2015), được gọi là mục tiêu TRPO liên kết,

J(π_{new}) ≥ J(π_{old}) + L^{joint}_{π_{old}}(π_{new}) - CD^{max}_{KL}(π_{old}||π_{new})  (3)

trong đó L^{joint}_{π_{old}}(π_{new}) = ∑_s ρ^{π_{old}}(s) ∑_a π_{new}(a|s)A^{π_{old}}(s,a),     (4)

trong đó D^{max}_{KL}(π_{old}||π_{new}) = max_s D_{KL}(π_{old}(·|s)||π_{new}(·|s)), ρ^{π_{old}}(s) = ∑_{t=0}^∞ γ^t Pr(s_t=s|π_{old}) là phân phối trạng thái dừng được chiết khấu của trạng thái cho π_{old}, A^{π_{old}} là hàm lợi thế dưới π_{old}, và C là một hằng số.

Mục tiêu TRPO liên kết, tức là vế phải của (3), là một cận dưới cho sự khác biệt giữa chính sách liên kết mới π_{new} và chính sách liên kết cũ π_{old} về mặt lợi nhuận kỳ vọng. Do đó, chúng ta có thể sử dụng mục tiêu này như một hàm thay thế, và tối đa hóa hàm thay thế này có thể đảm bảo rằng chính sách đang cải thiện đơn điệu. Tuy nhiên, mục tiêu TRPO liên kết không thể được tối ưu hóa trực tiếp trong học hoàn toàn phi tập trung vì mục tiêu này liên quan đến chính sách liên kết, không thể được truy cập trong học hoàn toàn phi tập trung.

Chúng tôi sẽ đề xuất một cận dưới mới (hàm thay thế) cho J(π_{new})-J(π_{old}), có thể được tối ưu hóa trong học hoàn toàn phi tập trung. Trước khi giới thiệu hàm thay thế mới của chúng tôi, chúng ta cần phân tích critic của tác nhân trong học hoàn toàn phi tập trung trước, được gọi là critic phi tập trung.

--- TRANG 3 ---
Bản thảo

3.2 Critic Phi tập trung
Trong học hoàn toàn phi tập trung, mỗi tác nhân học độc lập từ các tương tác riêng của mình với môi trường. Do đó, hàm Q của mỗi tác nhân i thực sự là công thức sau:

Q^{π_i}_{π_{-i}}(s,a_i) = r_i(s,a_i) + γE_{a_{-i}~π_{-i}, s'~P(·|s,a_i,a_{-i}), a'_i~π_i}[Q^{π_i}_{π_{-i}}(s',a'_i)],  (5)

trong đó r_i(s,a_i) = E_{π_{-i}}[r(s,a_i,a_{-i})], và π_{-i} và a_{-i} tương ứng biểu thị chính sách liên kết và hành động liên kết của tất cả các tác nhân trừ tác nhân i. Nếu chúng ta lấy kỳ vọng E_{a'_{-i}~π_{-i}(·|s'), a_{-i}~π_{-i}(·|s)} trên cả hai vế của hàm Q của chính sách liên kết (2), thì chúng ta có

E_{π_{-i}}[Q^π(s,a_i,a_{-i})] = r_i(s,a_i) + γE_{a_{-i}~π_{-i}, s'~P(·|s,a_i,a_{-i}), a'_{-i}~π_{-i}}E_{π_{-i}}[Q^π(s',a'_i,a'_{-i})].

Chúng ta có thể thấy rằng Q^{π_i}_{π_{-i}}(s,a_i) và E_{π_{-i}}[Q^π(s,a_i,a_{-i})] thỏa mãn cùng một phép lặp. Hơn nữa, chúng tôi sẽ chỉ ra trong phần sau rằng Q^{π_i}_{π_{-i}}(s,a_i) và E_{π_{-i}}[Q^π(s,a_i,a_{-i})] chính là như nhau.

Chúng tôi đầu tiên định nghĩa một toán tử T^{π_{-i}}_i như sau,

T^{π_{-i}}_i Q(s,a_i) = r_i(s,a_i) + γE_{a_{-i}~π_{-i}, s'~P(·|s,a_i,a_{-i}), a'_{-i}~π_{-i}}[Q(s',a'_i)].

Sau đó chúng tôi sẽ chứng minh rằng toán tử T^{π_{-i}}_i là một phép co.

Xem xét bất kỳ hai hàm Q cá nhân Q_1 và Q_2, chúng ta có:

||T^{π_{-i}}_i Q_1 - T^{π_{-i}}_i Q_2||_∞ = max_{s,a_i} |E_{a_{-i}~π_{-i}, s'~P(·|s,a_i,a_{-i}), a'_{-i}~π_{-i}}[Q_1(s',a'_i) - Q_2(s',a'_i)]|

≤ γE_{a_{-i}~π_{-i}, s'~P(·|s,a_i,a_{-i}), a'_{-i}~π_{-i}}[max_{s',a'_i} |Q_1(s',a'_i) - Q_2(s',a'_i)|]

= γ max_{s',a'_i} |Q_1(s',a'_i) - Q_2(s',a'_i)|

= γ||Q_1 - Q_2||_∞.

Vậy toán tử T^{π_{-i}}_i có một và chỉ một điểm cố định, có nghĩa là

Q^{π_i}_{π_{-i}}(s,a_i) = E_{π_{-i}}[Q^π(s,a_i,a_{-i})],

V^{π_i}_{π_{-i}}(s) = E_{π_{-i}}[V^π(s)] = V^π(s).

Với critic phi tập trung được định nghĩa rõ ràng này, chúng ta có thể phân tích thêm mục tiêu của IPPO (de Witt et al., 2020). Trong IPPO, mục tiêu chính sách của mỗi tác nhân i về cơ bản có thể được công thức hóa như sau:

L^i_{π_{old}}(π^i_{new}) = ∑_s ρ^{π_{old}}(s) ∑_{a_i} π^i_{new}(a_i|s) A^i_{π_{old}}(s,a_i),  (6)

trong đó A^i_{π_{old}}(s,a_i) = Q^{π^i_{old}}_{π^{-i}_{old}}(s,a_i) - E^{π^i_{old}}[Q^{π^i_{old}}_{π^{-i}_{old}}(s,a_i)] = E^{π^{-i}_{old}}[A^{π_{old}}(s,a_i,a_{-i})].

Tuy nhiên, (6) khác với (4) trong mục tiêu TRPO liên kết. Do đó, tối ưu hóa trực tiếp (6) có thể không cải thiện chính sách liên kết, và do đó không thể cung cấp bất kỳ đảm bảo nào cho hội tụ, theo hiểu biết của chúng tôi. Tuy nhiên, có vẻ như A^i_{π_{old}}(s,a_i) là công thức lợi thế duy nhất có thể được truy cập bởi mỗi tác nhân trong học hoàn toàn phi tập trung. Vậy, mục tiêu chính sách của DPO sẽ được suy ra trên (6) nhưng với các sửa đổi để đảm bảo hội tụ, và chúng tôi sẽ giới thiệu chi tiết trong phần tiếp theo. Trong phần sau, chúng tôi thảo luận cách tính lợi thế này trong thực tế trong học hoàn toàn phi tập trung.

Vì chúng ta cần tính A^i_{π_{old}}(s,a_i) = E^{π^{-i}_{old}}[r(s,a_i,a_{-i}) + γV^{π_{old}}(s') - V^{π_{old}}(s)] cho cập nhật chính sách, chúng ta có thể xấp xỉ A^i_{π_{old}}(s,a_i) với Â^i(s,a_i) = r + γV^{π_i}_{π_{-i}}(s') - V^{π_i}_{π_{-i}}(s), là một ước lượng không thiên vị của A^i_{π_{old}}(s,a_i), mặc dù nó có thể có phương sai lớn. Trong thực tế, chúng ta có thể theo ý tưởng truyền thống trong học hoàn toàn phi tập trung, và để mỗi tác nhân i học một cách độc lập một hàm giá trị cá nhân V_i(s). Sau đó, chúng ta tiếp tục có Â^i(s,a_i) ≈ r + γV_i(s') - V_i(s). Mất mát cho critic phi tập trung như sau:

L^i_{critic} = E[(V_i(s) - y_i)^2], trong đó y_i = r + γV_i(s') hoặc lợi nhuận Monte Carlo.  (7)

Có thể có một số cách để cải thiện việc học critic này, tuy nhiên điều đó nằm ngoài phạm vi thảo luận của chúng tôi.

--- TRANG 4 ---
Bản thảo

3.3 Hàm thay thế Phi tập trung
Chúng tôi đã sẵn sàng giới thiệu hàm thay thế phi tập trung. Đầu tiên, chúng tôi suy ra cận dưới mới của cải thiện chính sách liên kết bằng định lý sau.

Định lý 1. Giả sử π_{old} và π_{new} là hai chính sách liên kết. Khi đó, cận sau đây giữ:

J(π_{new}) ≥ J(π_{old}) + \frac{1}{N}\sum_{i=1}^N L^i_{π_{old}}(π^i_{new}) - \tilde{M}\sum_{i=1}^N \sqrt{D^{max}_{KL}(π^i_{old}||π^i_{new})} - C\sum_{i=1}^N D^{max}_{KL}(π^i_{old}||π^i_{new}),

trong đó \tilde{M} = \frac{2γ\max_{s,a}|A^{π_{old}}(s,a)|}{1-γ} và C = \frac{4γ\max_{s,a}|A^{π_{old}}(s,a)|}{(1-γ)^2} là hai hằng số.

Chứng minh. Chúng tôi đầu tiên xem xét L^{joint}_{π_{old}}(π_{new}) - L^i_{π_{old}}(π^i_{new}). Theo (4) và (6), chúng ta có phương trình sau:

L^{joint}_{π_{old}}(π_{new}) - L^i_{π_{old}}(π^i_{new})
= \sum_s ρ^{π_{old}}(s) \sum_a π_{new}(a|s) \sum_{a_{-i}} π^{-i}_{new}(a_{-i}|s) A^{π_{old}}(s,a_i,a_{-i}) - A^i_{π_{old}}(s,a_i)
= E^{π_{old}} E^{π^i_{new}} \sum_{a_{-i}} \frac{π^{-i}_{new}(a_{-i}|s)}{π^{-i}_{old}(a_{-i}|s)} A^{π_{old}}(s,a_i,a_{-i}).

Sau đó, chúng ta có các bất đẳng thức sau:

|L^{joint}_{π_{old}}(π_{new}) - L^i_{π_{old}}(π^i_{new})|
≤ E^{π_{old}} E^{π^i_{new}} \sum_{a_{-i}} |π^{-i}_{old}(a_{-i}|s) - π^{-i}_{new}(a_{-i}|s)| |A^{π_{old}}(s,a_i,a_{-i})|
≤ E^{π_{old}} E^{π^i_{new}} M \sum_{a_{-i}} |π^{-i}_{old}(a_{-i}|s) - π^{-i}_{new}(a_{-i}|s)| (M = \max_{s,a} |A^{π_{old}}(s,a)|)
= 2ME^{π_{old}} D_{TV}(π^{-i}_{old}(·|s)||π^{-i}_{new}(·|s))
≤ 2M \frac{γ}{1-γ} \max_s D_{TV}(π^{-i}_{old}(·|s)||π^{-i}_{new}(·|s))
= \tilde{M} D^{max}_{TV}(π^{-i}_{old}||π^{-i}_{new}) (\tilde{M} = \frac{2Mγ}{1-γ})
≤ \tilde{M} \sqrt{D^{max}_{KL}(π^{-i}_{old}||π^{-i}_{new})}  (8)
≤ \tilde{M} \sqrt{\sum_{j≠i} D^{max}_{KL}(π^j_{old}||π^j_{new})}, (9)

trong đó (8) là từ mối quan hệ giữa tổng phương sai và phân kỳ KL rằng D_{TV}(p||q) ≤ \sqrt{2D_{KL}(p||q)} (Schulman et al., 2015), và (9) là một tính chất của phân kỳ KL, có thể được thu được như sau,

D^{max}_{KL}(π_{old}||π_{new}) = \max_s D_{KL}(π_{old}(·|s)||π_{new}(·|s))
= \max_s \sum_i D_{KL}(π^i_{old}(·|s)||π^i_{new}(·|s))
≤ \sum_i \max_s D_{KL}(π^i_{old}(·|s)||π^i_{new}(·|s))
= \sum_i D^{max}_{KL}(π^i_{old}||π^i_{new}). (10)

Từ (9), chúng ta có thể thu được thêm bất đẳng thức sau,

L^{joint}_{π_{old}}(π_{new}) - L^i_{π_{old}}(π^i_{new}) ≤ \tilde{M} \sqrt{\sum_{j≠i} D^{max}_{KL}(π^j_{old}||π^j_{new})}. (11)

--- TRANG 5 ---
Bản thảo

Tiếp theo chúng tôi sẽ chứng minh định lý này, bắt đầu từ (3),

J(π_{new}) ≥ J(π_{old}) + L^{joint}_{π_{old}}(π_{new}) - CD^{max}_{KL}(π_{old}||π_{new})
= \frac{1}{N}\sum_{i=1}^N [J(π_{old}) + NL^{joint}_{π_{old}}(π_{new}) - CD^{max}_{KL}(π_{old}||π_{new})]
≥ \frac{1}{N}\sum_{i=1}^N [L^i_{π_{old}}(π^i_{new}) - \tilde{M}\frac{1}{N}\sum_{i=1}^N \sqrt{\sum_{j≠i} D^{max}_{KL}(π^j_{old}||π^j_{new})} - CD^{max}_{KL}(π_{new}||π_{old})] (12)
≥ \frac{1}{N}\sum_{i=1}^N L^i_{π_{old}}(π^i_{new}) - \tilde{M}\sqrt{\frac{N-1}{N}\sum_{i=1}^N D^{max}_{KL}(π^i_{old}||π^i_{new})} - CD^{max}_{KL}(π_{new}||π_{old}) (13)
≥ \frac{1}{N}\sum_{i=1}^N L^i_{π_{old}}(π^i_{new}) - \tilde{M}\sqrt{\frac{N-1}{N}\sum_{i=1}^N D^{max}_{KL}(π^i_{old}||π^i_{new})} - C\sum_{i=1}^N D^{max}_{KL}(π^i_{old}||π^i_{new}) (14)
≥ \frac{1}{N}\sum_{i=1}^N L^i_{π_{old}}(π^i_{new}) - \tilde{M}\sqrt{\sum_{i=1}^N D^{max}_{KL}(π^i_{old}||π^i_{new})} - C\sum_{i=1}^N D^{max}_{KL}(π^i_{old}||π^i_{new})
≥ \frac{1}{N}\sum_{i=1}^N L^i_{π_{old}}(π^i_{new}) - \tilde{M}\sum_{i=1}^N \sqrt{D^{max}_{KL}(π^i_{old}||π^i_{new})} - C\sum_{i=1}^N D^{max}_{KL}(π^i_{old}||π^i_{new}). (15)

Bất đẳng thức (12) là ứng dụng trực tiếp của bất đẳng thức (11). Bất đẳng thức (13) là từ bất đẳng thức Cauchy-Schwarz,

\sum_{i=1}^N \sqrt{\sum_{j≠i} D^{max}_{KL}(π^j_{old}||π^j_{new})} ≤ \sqrt{N\sum_{i=1}^N \sum_{j≠i} D^{max}_{KL}(π^j_{old}||π^j_{new})}
= \sqrt{N(N-1)\sum_{i=1}^N D^{max}_{KL}(π^i_{old}||π^i_{new})}.

Bất đẳng thức (14) là từ (10), trong khi bất đẳng thức (15) là từ bất đẳng thức đơn giản \sqrt{\sum_i a_i} ≤ \sum_i \sqrt{a_i} (a_i ≥ 0, ∀i).

Cận dưới trong Định lý 1 được dành riêng cho tối ưu hóa chính sách phi tập trung, vì nó có thể được phân tách trực tiếp một cách cá nhân cho mỗi tác nhân như một hàm thay thế phi tập trung. Từ Định lý 1, nếu chúng ta thiết lập mục tiêu tối ưu hóa chính sách của mỗi tác nhân i như

π^i_{new} = \arg\max_{π^i} \frac{1}{N}L^i_{π_{old}}(π^i) - \tilde{M}\sqrt{D^{max}_{KL}(π^i_{old}||π^i)} - CD^{max}_{KL}(π^i_{old}||π^i), (16)

thì chúng ta có J(π_{new}) ≥ J(π_{old}) từ TRPO (Schulman et al., 2015). Hơn nữa, vì mục tiêu J(π) bị chặn, sự hội tụ của {J(π^{(t)})} được đảm bảo, trong đó π^{(t)} là chính sách liên kết sau t lần lặp theo (16). Do đó, chính sách liên kết của các tác nhân cải thiện đơn điệu và hội tụ đến dưới tối ưu bằng tối ưu hóa chính sách hoàn toàn phi tập trung, tức là học độc lập.

Lưu ý rằng kết quả này dưới giả thiết rằng mỗi tác nhân có thể thu được trạng thái, và trong thực tế mỗi tác nhân sẽ lấy quỹ đạo cá nhân τ_i như xấp xỉ cho trạng thái.

3.4 Thuật toán

DPO có ý tưởng đơn giản là mỗi tác nhân tối ưu hóa hàm thay thế phi tập trung (16). Tuy nhiên, chúng ta gặp phải rắc rối tương tự như TRPO rằng các hằng số \tilde{M} và C lớn và nếu chúng ta tối ưu hóa trực tiếp mục tiêu này, thì kích thước bước của cập nhật chính sách sẽ nhỏ.

Để giải quyết vấn đề này, chúng tôi hấp thụ ý tưởng của hệ số thích ứng trong PPO (Schulman et al., 2017). Chúng tôi sử dụng hai hệ số thích ứng β₁ⁱ và β₂ⁱ để thay thế các hằng số \tilde{M} và C và thêm vào thay thế

--- TRANG 6 ---
Bản thảo

Thuật toán 1. DPO
1: for episode = 1 to M do
2:   for t = 1 to max_episode_length do
3:     select action a_i ~ π_i(·|s)
4:     execute action a_i and observe reward r and next state s'
5:     collect ⟨s, a_i, r, s'⟩
6:   end for
7:   Update decentralized critic according to (7)
8:   Update policy according to the surrogate (17)
9:   Update β₁ⁱ and β₂ⁱ according to (18).
10: end for

phân kỳ KL tối đa với phân kỳ KL trung bình. Trong thực tế, chúng tôi sẽ thực sự tối ưu hóa mục tiêu sau

π^i_{new} = \arg\max_{π^i} \frac{1}{N}L^i_{π_{old}}(π^i) - β₁ⁱ\sqrt{D^{avg}_{KL}(π^i_{old}||π^i)} - β₂ⁱD^{avg}_{KL}(π^i_{old}||π^i), (17)

trong đó D^{avg}_{KL}(π^i_{old}||π^i) = E_{s~ρ^{π_{old}}} D_{KL}(π^i_{old}(·|s)||π^i(·|s)).

Đối với việc thích ứng của β₁ⁱ và β₂ⁱ, chúng ta cần định nghĩa một siêu tham số d_{target}, có thể được xem như một thước đo cho phân kỳ KL trung bình D^{avg}_{KL}(π^i_{old}||π^i_{new}) cho mỗi tác nhân. Nếu D^{avg}_{KL}(π^i_{old}||π^i_{new}) gần với d_{target}, thì chúng ta tin rằng β₁ⁱ và β₂ⁱ hiện tại là phù hợp. Nếu D^{avg}_{KL}(π^i_{old}||π^i_{new}) vượt quá d_{target} quá nhiều, chúng ta tin rằng β₁ⁱ và β₂ⁱ nhỏ và cần tăng lên và ngược lại. Trong thực tế, chúng tôi sẽ sử dụng quy tắc sau để cập nhật β₁ⁱ và β₂ⁱ:

Nếu D^{avg}_{KL}(π^i_{old}||π^i_{new}) > d_{target}, thì β^i_j ← β^i_j × ζ ∀j ∈ {1,2}
Nếu D^{avg}_{KL}(π^i_{old}||π^i_{new}) < d_{target}/ψ, thì β^i_j ← β^i_j/ψ ∀j ∈ {1,2}. (18)

Chúng tôi chọn các hằng số ζ = 1.5 và ψ = 2 như lựa chọn trong PPO (Schulman et al., 2017). Đối với critic, chúng tôi chỉ tuân theo phương pháp tiêu chuẩn trong PPO. Sau đó, chúng ta có thể có quy trình học hoàn toàn phi tập trung của DPO cho mỗi tác nhân i trong Thuật toán 1.

Thuật toán thực tế của DPO thực sự sử dụng một số xấp xỉ cho hàm thay thế phi tập trung. Hầu hết các xấp xỉ này đều là thực hành truyền thống trong RL hoặc chưa có lựa chọn thay thế nào trong học hoàn toàn phi tập trung. Chúng tôi thừa nhận rằng thuật toán thực tế có thể không duy trì được đảm bảo lý thuyết. Tuy nhiên, chúng tôi cần lập luận rằng chúng tôi đã tiến thêm một bước để đưa ra một hàm thay thế phi tập trung trong học hoàn toàn phi tập trung với đảm bảo hội tụ. Chúng tôi tin tưởng và mong đợi rằng một phương pháp thực tế tốt hơn có thể được tìm thấy dựa trên mục tiêu này trong công việc tương lai.

4 Thí nghiệm

Trong phần này, chúng tôi so sánh thuật toán thực tế của DPO với IPPO (de Witt et al., 2020) trong nhiều môi trường đa tác nhân hợp tác khác nhau, bao gồm trò chơi ngẫu nhiên hợp tác, MPE (Lowe et al., 2017), MuJoCo đa tác nhân (Peng et al., 2021), và SMAC (Samvelyan et al., 2019), bao gồm cả không gian hành động rời rạc và liên tục, và môi trường quan sát hoàn toàn và một phần. Vì chúng tôi xem xét học hoàn toàn phi tập trung, trong các thí nghiệm các tác nhân không sử dụng chia sẻ tham số vì chia sẻ tham số nên được coi là học tập trung (Terry et al., 2020). Trong tất cả các thí nghiệm, kiến trúc mạng và các siêu tham số chung của DPO và IPPO giống nhau để có sự so sánh công bằng. Thêm chi tiết về thiết lập thí nghiệm và siêu tham số có sẵn trong Phụ lục. Hơn nữa, tất cả các đường cong học từ 5 hạt giống ngẫu nhiên và vùng được tô màu tương ứng với khoảng tin cậy 95%.

4.1 Một Ví dụ Giáo khoa

Đầu tiên, chúng tôi sử dụng một trò chơi ngẫu nhiên hợp tác như một ví dụ giáo khoa. Trò chơi ngẫu nhiên hợp tác có 100 trạng thái, 6 tác nhân và mỗi tác nhân có 5 hành động. Tất cả các tác nhân chia sẻ một hàm phần thưởng liên kết. Hàm phần thưởng và xác suất chuyển tiếp đều được tạo ngẫu nhiên. Trò chơi ngẫu nhiên này có một mức độ phức tạp nhất định giúp phân biệt hiệu suất của DPO và IPPO. Mặt khác, môi trường này là dạng bảng có nghĩa là huấn luyện trong môi trường này nhanh và chúng ta có thể làm các nghiên cứu loại bỏ hiệu quả. Hơn nữa, chúng ta có thể tìm tối ưu toàn cục bằng quy hoạch động để so sánh trong trò chơi này.

--- TRANG 7 ---
Bản thảo

Các đường cong học trong Hình 1a cho thấy DPO hoạt động tốt hơn IPPO và học được một giải pháp tốt hơn trong môi trường này. Thực tế rằng DPO học được một giải pháp dưới tối ưu phù hợp với kết quả lý thuyết của chúng tôi. Tuy nhiên, giải pháp dưới tối ưu được tìm thấy bởi DPO vẫn cách xa tối ưu toàn cục. Điều này có nghĩa là vẫn còn không gian cải thiện.

Mặt khác, chúng tôi nghiên cứu ảnh hưởng của siêu tham số d_{target} trên DPO. Chúng tôi chọn d_{target} = 0.001, 0.01, 0.1, 1. Kết quả thực nghiệm được hiển thị trong Hình 1b. Chúng tôi thấy rằng khi d_{target} nhỏ, hệ số β₁ và β₂ có nhiều khả năng được tăng lên và kích thước bước của cập nhật chính sách bị giới hạn. Vậy cho trường hợp d_{target} = 0.001, 0.01, hiệu suất của DPO tương đối thấp. Và khi d_{target} lớn, cập nhật chính sách có thể vượt ra ngoài vùng tin cậy. Điều này có thể được chứng kiến bằng đường cong học dao động của trường hợp d_{target} = 1. Vậy chúng ta cần chọn một giá trị phù hợp cho d_{target} và trong môi trường này chúng tôi chọn d_{target} = 0.1, đó cũng là đường cong học của DPO trong Hình 1a. Chúng tôi thấy rằng giá trị phù hợp cho d_{target} thay đổi trong các môi trường khác nhau. Trong phần sau, chúng tôi giữ d_{target} giống nhau cho các nhiệm vụ của cùng một môi trường. Có thể có một số lựa chọn tốt hơn cho d_{target}, nhưng nó hơi tốn thời gian và nằm ngoài phạm vi thảo luận của chúng tôi.

4.2 MPE

MPE là một môi trường phổ biến trong MARL hợp tác. MPE là một môi trường 2D và các đối tượng trong môi trường MPE là các tác nhân hoặc các mốc. Mốc là một phần của môi trường, trong khi các tác nhân có thể di chuyển theo bất kỳ hướng nào. Với mối quan hệ giữa các tác nhân và các mốc, chúng ta có thể thiết kế các nhiệm vụ khác nhau. Chúng tôi sử dụng phiên bản không gian hành động rời rạc của MPE và các tác nhân có thể tăng tốc hoặc giảm tốc theo hướng của trục x hoặc trục y. Chúng tôi chọn MPE vì tính có thể quan sát một phần của nó. Chúng tôi lấy d_{target} = 0.01 cho tất cả các nhiệm vụ MPE.

Các nhiệm vụ MPE chúng tôi sử dụng cho các thí nghiệm là simple spread, line control, và circle control ban đầu được sử dụng trong Agarwal et al. (2020). Trong các thí nghiệm của chúng tôi, chúng tôi đặt số lượng tác nhân N = 5 trong tất cả ba nhiệm vụ này. Kết quả thực nghiệm được minh họa trong Hình 2. Chúng ta có thể thấy rằng mặc dù DPO có thể tụt lại phía sau IPPO lúc đầu của quá trình huấn luyện trong một số nhiệm vụ, DPO học được một chính sách tốt hơn cuối cùng cho tất cả ba nhiệm vụ.

--- TRANG 8 ---
Bản thảo

4.3 MuJoCo Đa tác nhân

MuJoCo đa tác nhân là một môi trường điều khiển di chuyển robot cho thiết lập đa tác nhân, được xây dựng dựa trên MuJoCo đơn tác nhân (Todorov et al., 2012). Trong MuJoCo đa tác nhân, mỗi tác nhân điều khiển một phần của robot để thực hiện các nhiệm vụ khác nhau. Chúng tôi chọn môi trường này vì lý do của không gian trạng thái và hành động liên tục. Chúng tôi chọn 5 nhiệm vụ cho các thí nghiệm của chúng tôi: Hopper 3 tác nhân, HalfCheetah 3 tác nhân, Walker2d 3 tác nhân, Ant 4 tác nhân và Humanoid 17 tác nhân. Chúng tôi lấy d_{target} = 0.001 cho tất cả các nhiệm vụ MuJoCo đa tác nhân.

Kết quả thực nghiệm được minh họa trong Hình 3. Chúng ta có thể thấy rằng trong tất cả năm nhiệm vụ, DPO vượt trội hơn IPPO, mặc dù trong HalfCheetah 3 tác nhân DPO học chậm hơn IPPO lúc đầu. Kết quả trên MuJoCo đa tác nhân xác minh rằng DPO cũng hiệu quả khi đối mặt với không gian trạng thái và hành động liên tục. Hơn nữa, hiệu suất tốt hơn của DPO trong nhiệm vụ Humanoid 17 tác nhân có thể là bằng chứng về khả năng mở rộng của DPO.

4.4 SMAC

SMAC là một môi trường có thể quan sát một phần và chiều cao mà đã được sử dụng trong nhiều nghiên cứu MARL hợp tác. Chúng tôi chọn năm bản đồ trong SMAC, 2s3z, 8m, 3s5z, 27m_vs_30m, và MMM2 cho các thí nghiệm của chúng tôi. Chúng tôi lấy d_{target} = 0.02 cho tất cả các nhiệm vụ SMAC.

Kết quả thực nghiệm được minh họa trong Hình 4. Hai nhiệm vụ SMAC siêu khó (27m_vs_30m và MMM2) quá khó khăn cho cả DPO và IPPO để thắng, vì vậy chúng tôi sử dụng phần thưởng tập để làm chỉ số để hiển thị sự khác biệt của chúng. DPO hoạt động tốt hơn IPPO trong tất cả năm bản đồ. Chúng tôi cần lập luận rằng mặc dù chúng tôi đã kiểm soát kiến trúc mạng của DPO và IPPO giống nhau, trong các thí nghiệm của chúng tôi mỗi tác nhân có các tham số cá nhân riêng làm tăng độ khó của việc huấn luyện. Vậy kết quả của chúng tôi trong SMAC có thể khác với các công trình khác. Mặc dù IPPO đã được chỉ ra hoạt động tốt trong SMAC (de Witt et al., 2020; Yu et al., 2021; Papoudakis et al., 2021), DPO vẫn có thể vượt trội hơn IPPO, điều này xác minh hiệu quả của thuật toán thực tế của DPO trong các nhiệm vụ phức tạp chiều cao và cũng có thể là bằng chứng cho kết quả lý thuyết của chúng tôi. Một lần nữa, hiệu suất tốt hơn của DPO trong 27m_vs_30m cho thấy khả năng mở rộng tốt của nó trong nhiệm vụ với nhiều tác nhân.

5 Kết luận

Trong bài báo này, chúng tôi điều tra học hoàn toàn phi tập trung trong học tăng cường đa tác nhân hợp tác. Chúng tôi suy ra một cận dưới phi tập trung mới cho cải thiện chính sách liên kết và chúng tôi đề xuất DPO, một thuật toán actor-critic hoàn toàn phi tập trung với đảm bảo hội tụ và cải thiện đơn điệu. Về mặt thực nghiệm, chúng tôi kiểm tra DPO so sánh với IPPO trong nhiều môi trường khác nhau bao gồm trò chơi ngẫu nhiên hợp tác, MPE, MuJoCo đa tác nhân, và SMAC, bao gồm cả không gian hành động rời rạc và liên tục, và môi trường quan sát hoàn toàn và một phần. Kết quả thực nghiệm cho thấy lợi thế của DPO so với IPPO, có thể là bằng chứng cho kết quả lý thuyết của chúng tôi.

--- TRANG 9 ---
Bản thảo

Tài liệu tham khảo

Akshat Agarwal, Sumit Kumar, Katia Sycara, and Michael Lewis. Learning transferable cooperative behavior in multi-agent team. In International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2020.

Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020.

Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In AAAI Conference on Artificial Intelligence (AAAI), 2018.

Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In International Conference on Machine Learning (ICML), 2019.

Jakub Grudzien Kuba, Ruiqing Chen, Munning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. arXiv preprint arXiv:2109.11251, 2021.

Wenhao Li, Bo Jin, Xiangfeng Wang, Junchi Yan, and Hongyuan Zha. F2a2: Flexible fully-decentralized approximate actor-critic for cooperative multi-agent reinforcement learning. arXiv preprint arXiv:2004.11145, 2020.

Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information Processing Systems (NeurIPS), 2017.

Omid Madani, Steve Hanks, and Anne Condon. On the undecidability of probabilistic planning and infinite-horizon partially observable markov decision problems. In AAAI/IAAI, pp. 541–548, 1999.

Laëtitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Hysteretic q-learning: an algorithm for decentralized reinforcement learning in cooperative multi-agent teams. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2007.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.

Afshin OroojlooyJadid and Davood Hajinezhad. A review of cooperative multi-agent deep reinforcement learning. arXiv preprint arXiv:1908.03963, 2019.

Gregory Palmer, Karl Tuyls, Daan Bloembergen, and Rahul Savani. Lenient multi-agent deep reinforcement learning. arXiv preprint arXiv:1707.04402, 2017.

Georgios Papoudakis, Filippos Christianos, Lukas Schäfer, and Stefano V Albrecht. Benchmarking multi-agent deep reinforcement learning algorithms in cooperative tasks. In Advances in Neural Information Processing Systems (NeurIPS), 2021.

Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr, Wendelin Böhmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy gradients. In Advances in Neural Information Processing Systems (NeurIPS), 2021.

Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning (ICML), 2018.

Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International Conference on Machine Learning (ICML), 2015.

--- TRANG 10 ---
Bản thảo

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning. In International Conference on Machine Learning (ICML), 2019.

Kefan Su and Zongqing Lu. Divergence-regularized multi-agent actor-critic. In International Conference on Machine Learning (ICML), 2022.

Mingfei Sun, Sam Devlin, Katja Hofmann, and Shimon Whiteson. Monotonic improvement guarantees under non-stationarity for decentralized ppo. arXiv preprint arXiv:2202.00082, 2022.

Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinícius Flores Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for cooperative multi-agent learning based on team reward. In International Conference on Autonomous Agents and MultiAgent Systems (AAMAS), 2018.

Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru, and Raul Vicente. Multiagent cooperation and competition with deep reinforcement learning. arXiv preprint arXiv:1511.08779, 2015.

Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In International Conference on Machine Learning (ICML), 1993.

Justin K Terry, Nathaniel Grammel, Ananth Hari, Luis Santos, and Benjamin Black. Revisiting parameter sharing in multi-agent deep reinforcement learning. arXiv preprint arXiv:2005.13625, 2020.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2012.

Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie Zhang. Qplex: Duplex dueling multi-agent q-learning. In International Conference on Learning Representations (ICLR), 2021a.

Yihan Wang, Beining Han, Tonghan Wang, Heng Dong, and Chongjie Zhang. Dop: Off-policy multi-agent decomposed policy gradients. In International Conference on Learning Representations (ICLR), 2021b.

Zifan Wu, Chao Yu, Deheng Ye, Junge Zhang, Hankz Hankui Zhuo, et al. Coordinated proximal policy optimization. Advances in Neural Information Processing Systems (NeurIPS), 34, 2021.

Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong Chen, Wulong Liu, and Hongyao Tang. Qatten: A general framework for cooperative multiagent reinforcement learning. arXiv preprint arXiv:2002.03939, 2020.

Hsuan-Yu Yao, Ping-Chun Hsieh, Kuo-Hao Ho, Kai-Chun Hu, Liang-Chun Ouyang, I Wu, et al. Hinge policy optimization: Rethinking policy improvement and reinterpreting ppo. arXiv preprint arXiv:2110.13799, 2021.

Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.

Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Baar. Fully decentralized multi-agent reinforcement learning with networked agents. In International Conference on Machine Learning (ICML), 2018.

Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Multi-agent reinforcement learning: A selective overview of theories and algorithms. arXiv preprint arXiv:1911.10635, 2019.

Tianhao Zhang, Yueheng Li, Chen Wang, Guangming Xie, and Zongqing Lu. Fop: Factorizing optimal joint policy of maximum-entropy multi-agent reinforcement learning. In International Conference on Machine Learning (ICML), 2021.

--- TRANG 11 ---
Bản thảo

A Thiết lập Thí nghiệm

A.1 MPE

Ba nhiệm vụ được xây dựng trên MPE gốc (Lowe et al., 2017) (giấy phép MIT) và ban đầu được sử dụng trong Agarwal et al. (2020) (giấy phép MIT). Mục tiêu trong ba nhiệm vụ này được liệt kê như sau:

• Simple Spread: Có N tác nhân cần chiếm các vị trí của N mốc.
• Line Control: Có N tác nhân cần xếp hàng giữa 2 mốc.
• Circle Control: Có N tác nhân cần tạo thành một vòng tròn xung quanh một mốc.

Phần thưởng trong các nhiệm vụ này là khoảng cách giữa tất cả các tác nhân và các vị trí mục tiêu của chúng. Chúng tôi đặt số lượng tác nhân N = 5 cho ba nhiệm vụ này trong thí nghiệm của chúng tôi.

A.2 MuJoCo Đa tác nhân

MuJoCo đa tác nhân (Peng et al., 2021) (giấy phép Apache-2.0) là một nhiệm vụ di chuyển robot với không gian hành động liên tục cho thiết lập đa tác nhân. Robot có thể được chia thành nhiều phần và mỗi phần chứa nhiều khớp. Các tác nhân trong môi trường này điều khiển một phần của robot có thể có nhiều biến thể khác nhau. Vậy loại robot và việc phân công các khớp quyết định một nhiệm vụ. Ví dụ, nhiệm vụ 'HalfCheetah-3×2' có nghĩa là chia robot 'HalfCheetah' thành ba phần cho ba tác nhân và mỗi phần chứa 2 khớp.

Chi tiết về thiết lập thí nghiệm của chúng tôi trong MuJoCo đa tác nhân được liệt kê trong Bảng 1. Cấu hình định nghĩa số lượng tác nhân và các khớp của mỗi tác nhân. 'agent obsk' định nghĩa số lượng tác nhân gần nhất mà một tác nhân có thể quan sát.

Bảng 1: Thiết lập nhiệm vụ của MuJoCo đa tác nhân
nhiệm vụ | cấu hình | agent obsk
HalfCheetah | 3×2 | 2
Hopper | 3×1 | 2
Walker2d | 3×2 | 2
Ant | 4×2 | 2

B Chi tiết Huấn luyện

Mã của chúng tôi dựa trên mã nguồn mở¹ của MAPPO (Yu et al., 2021) (giấy phép MIT). Chúng tôi sửa đổi mã cho các tham số cá nhân và cấm các thủ thuật được sử dụng bởi MAPPO cho SMAC. Kiến trúc mạng và các siêu tham số cơ bản của DPO và IPPO giống nhau cho tất cả các nhiệm vụ trong tất cả các môi trường. Chúng tôi sử dụng MLP 3 lớp cho actor và critic và sử dụng ReLU như các phi tuyến tính. Số lượng đơn vị ẩn của MLP là 128. Chúng tôi huấn luyện tất cả các mạng với trình tối ưu Adam. Tốc độ học của actor và critic đều là 5e-4. Số lượng epoch cho mỗi batch mẫu là 15 là giá trị được khuyến nghị trong Yu et al. (2021). Đối với IPPO, tham số clip là 0.2 giống như Schulman et al. (2017). Đối với DPO, các giá trị ban đầu của hệ số β₁ⁱ và β₂ⁱ là 0.01. Giá trị của d_target là 0.1 cho trò chơi ngẫu nhiên hợp tác, 0.01 cho MPE, 0.001 cho MuJoCo đa tác nhân, và 0.02 cho SMAC.

Phiên bản của trò chơi StarCraft2 trong SMAC là 4.10 cho các thí nghiệm của chúng tôi trong tất cả các nhiệm vụ SMAC. Chúng tôi đặt độ dài tập của tất cả các nhiệm vụ MuJoCo đa tác nhân là 1000 trong tất cả các thí nghiệm MuJoCo đa tác nhân của chúng tôi. Chúng tôi thực hiện toàn bộ thí nghiệm với tổng cộng bốn GPU NVIDIA A100. Chúng tôi đã tóm tắt các siêu tham số trong Bảng 2.

¹https://github.com/marlbenchmark/on-policy

--- TRANG 12 ---
Bản thảo

Bảng 2: Siêu tham số cho tất cả các thí nghiệm
siêu tham số | giá trị
lớp MLP | 3
kích thước ẩn | 128
phi tuyến | ReLU
trình tối ưu | Adam
actor_lr | 5e-4
critic_lr | 5e-4
số lượng epoch | 15
β₁ⁱ ban đầu | 0.01
β₂ⁱ ban đầu | 0.01
ζ | 1.5
ψ | 2
d_target | khác nhau cho môi trường như đã đề cập
tham số clip cho IPPO | 0.2

C Kết quả Bổ sung

Schulman et al. (2017) thực sự đề xuất hai phiên bản của PPO. Phiên bản đầu tiên, cũng là phiên bản phổ biến nhất, là với thủ thuật clip. Phiên bản thứ hai là tối ưu hóa trực tiếp công thức phạt với các hệ số thích ứng và chúng tôi gọi thuật toán này là PPO-KL. IPPO (de Witt et al., 2020) thực sự được mở rộng từ phiên bản đầu tiên, trong khi thuật toán thực tế của DPO tương tự như phiên bản thứ hai. Sự khác biệt chính giữa DPO và PPO-KL là số hạng căn bậc hai của phân kỳ KL trong mất mát chính sách. Chúng tôi sửa đổi IPPO bằng cách làm cho mỗi tác nhân học với PPO-KL để có được IPPO-KL. Mặt khác, Q-learning độc lập (IQL) (Tan, 1993) là một thuật toán học độc lập cổ điển. IQL có thể có được hiệu suất tốt trong một số nhiệm vụ nhưng nó không có bất kỳ đảm bảo lý thuyết nào, theo hiểu biết của chúng tôi.

[Hình 5: Đường cong học của DPO so sánh với IPPO, IPPO-KL, IQL, và tối ưu toàn cục trong trò chơi ngẫu nhiên hợp tác, trong đó trục x là bước môi trường.]

Để hoàn thiện các thí nghiệm của chúng tôi, chúng tôi kiểm tra hiệu suất của IPPO-KL và IQL trong trò chơi ngẫu nhiên hợp tác và kết quả thực nghiệm được minh họa trong Hình 5. Chúng tôi thấy rằng hiệu suất của IPPO-KL gần với DPO nhưng thấp hơn một chút và không ổn định hơn. Điều này có thể được giải thích vì mất mát chính sách của IPPO-KL thực sự là một xấp xỉ thiên vị của DPO, bỏ qua số hạng căn bậc hai. Đảm bảo lý thuyết của DPO cũng có thể giúp IPPO-KL hoạt động tốt hơn IPPO trong trò chơi này, trong khi sự thiên vị trong mất mát chính sách của IPPO-KL làm cho nó hoạt động tệ hơn DPO. Đối với IQL, hiệu suất của nó thấp hơn IPPO mặc dù nó hội tụ nhanh hơn. Vì IQL hoạt động tệ hơn IPPO trong môi trường giáo khoa như vậy và IQL là một thuật toán dựa trên giá trị và nằm ngoài phạm vi thảo luận của chúng tôi, chúng tôi không lấy IQL làm đường cơ sở trong các môi trường khác. Hơn nữa, chúng tôi nghĩ rằng các mối quan hệ thiết yếu giữa IPPO, IPPO-KL, và DPO có thể được chứng kiến rõ ràng từ kết quả thực nghiệm trong trò chơi ngẫu nhiên hợp tác. Đối với các nhiệm vụ khác, chúng tôi tập trung vào phiên bản chính thống của IPPO như trong de Witt et al. (2020); Yu et al. (2021); Papoudakis et al. (2021).

Bên cạnh kết quả thực nghiệm của chúng tôi, chúng tôi muốn chia sẻ quan điểm của chúng tôi về sự khác biệt giữa DPO và IPPO và đưa ra một số ý tưởng trực quan. Chính quy hóa KL và cắt tỉ lệ tương tự trong thiết lập đơn tác nhân, nhưng chúng không được cho là tương tự trong thiết lập đa tác nhân. Việc cắt tỉ lệ 'chính xác' trong thiết lập đa tác nhân theo lý thuyết của PPO nên cắt trên tỉ lệ chính sách liên kết π_new(a|s)/π_old(a|s). IPPO chỉ cắt tỉ lệ chính sách cá nhân π^i_new(a_i|s)/π^i_old(a_i|s) cho mỗi tác nhân i có thể không đủ để thực hiện việc cắt tỉ lệ 'chính xác'. Chúng ta có thể tìm thêm thảo luận về điều này trong bài báo CoPPO (Wu et al., 2021). Vậy IPPO không được cho là hưởng kết quả lý thuyết của DPO.

Chúng ta có thể viết lại mục tiêu của IPPO cho tác nhân i với công thức tương tự trong HPO (Yao et al., 2021) như sau:

L^{i,IPPO}_{π_{old}}(π^i_{new}) = ∑_s ρ^{π_{old}}(s) ∑_{a_i} π^i_{new}(a_i|s) |A^i_{π_{old}}(s,a_i)| l_{sign(A^i_{π_{old}}(s,a_i)), u_i(s,a_i)} (1, ε),

trong đó l(y,x,ε) = max{0, y-x-ε} là mất mát hinge và u_i(s,a_i) = π^i_{new}(a_i|s)/π^i_{old}(a_i|s) là tỉ lệ.

Nếu chúng ta theo cùng ý tưởng như PPO, thì IPPO là phiên bản cắt tỉ lệ 'chính xác' cho hàm thay thế của DPO. Tuy nhiên, hiệu quả của công thức cắt tỉ lệ này trong lý thuyết vẫn còn mở trong học phi tập trung vì không có bất kỳ đảm bảo hội tụ nào cho IPPO, theo hiểu biết của chúng tôi.

Mặc dù hiệu quả của IPPO trong lý thuyết nằm ngoài phạm vi bài báo của chúng tôi, chúng tôi có thể cung cấp một giải thích trực quan cho thực tế rằng hiệu suất của DPO có thể vượt qua IPPO từ công thức này và phân tích trong HPO. Trong chứng minh của HPO, có một giả thiết quan trọng rằng dấu của lợi thế được ước lượng giống với dấu của lợi thế thực (Giả thiết 4 trong Phần 2.3 trong Yao et al. (2021)). Và HPO cũng cho thấy rằng dấu của lợi thế quan trọng hơn giá trị cho công thức này của PPO-clip. Trong học phi tập trung, cả DPO và IPPO đều đối mặt với khó khăn của việc học hàm lợi thế cá nhân vì có thể có nhiễu trong hàm giá trị cá nhân. Tuy nhiên, mục tiêu của DPO liên tục và mục tiêu của IPPO rời rạc cho sign(A^i_{π_{old}}(s,a_i)). Vậy tác động của nhiễu trong hàm giá trị có thể lớn hơn trên IPPO so với DPO.

D Thảo luận

Trong bài báo, chúng tôi suy ra một cận dưới mới có thể được chia một cách tự nhiên thành hàm thay thế độc lập (16) cho mỗi tác nhân. Bằng cách mỗi tác nhân tối ưu hóa hàm thay thế này, việc cải thiện đơn điệu của chính sách liên kết có thể được đảm bảo trong thiết lập hoàn toàn phi tập trung. Tuy nhiên, thuật toán thực tế của DPO lấy công thức của (17) với một số xấp xỉ. Cách giải quyết tối ưu hóa của (16) chính xác hơn được để lại như công việc tương lai.

Hơn nữa, chúng tôi mong đợi công việc của chúng tôi có thể cung cấp một số hiểu biết cho các nghiên cứu tương lai về học tăng cường đa tác nhân hoàn toàn phi tập trung, vì các phương pháp hiện tại vẫn có khoảng cách với tối ưu như được hiển thị trong Hình 5.
