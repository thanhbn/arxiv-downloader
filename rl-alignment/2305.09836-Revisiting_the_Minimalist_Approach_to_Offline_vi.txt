# Xem xét lại Phương pháp Tối giản cho Học tăng cường Ngoại tuyến

Denis Tarasov Vladislav Kurenkov Alexander Nikulin Sergey Kolesnikov
Tinkoff
{den.tarasov, v.kurenkov, a.p.nikulin, s.s.kolesnikov}@tinkoff.ai

## Tóm tắt

Những năm gần đây đã chứng kiến những tiến bộ đáng kể trong học tăng cường ngoại tuyến (RL), dẫn đến sự phát triển của nhiều thuật toán với các mức độ phức tạp khác nhau. Mặc dù các thuật toán này đã mang lại những cải tiến đáng chú ý, nhiều thuật toán tích hợp những lựa chọn thiết kế có vẻ nhỏ nhặt nhưng ảnh hưởng đến hiệu quả của chúng vượt ra ngoài những tiến bộ thuật toán cốt lõi. Tuy nhiên, tác động của những lựa chọn thiết kế này lên các baseline đã được thiết lập vẫn chưa được nghiên cứu đầy đủ. Trong công trình này, chúng tôi nhằm mục đích thu hẹp khoảng cách này bằng cách thực hiện phân tích hồi cứu các công trình gần đây trong RL ngoại tuyến và đề xuất ReBRAC, một thuật toán tối giản tích hợp các yếu tố thiết kế như vậy được xây dựng trên nền tảng phương pháp TD3+BC. Chúng tôi đánh giá ReBRAC trên 51 bộ dữ liệu với cả không gian trạng thái proprioceptive và visual sử dụng các benchmark D4RL và V-D4RL, chứng minh hiệu suất tiên tiến của nó trong số các phương pháp không sử dụng ensemble trong cả cài đặt ngoại tuyến và ngoại tuyến sang trực tuyến. Để minh họa thêm hiệu quả của những lựa chọn thiết kế này, chúng tôi thực hiện nghiên cứu ablation quy mô lớn và phân tích độ nhạy siêu tham số trên quy mô hàng nghìn thí nghiệm.

## 1 Giới thiệu

Sự quan tâm của cộng đồng học tăng cường (RL) đối với cài đặt ngoại tuyến đã dẫn đến vô số thuật toán mới được thiết kế riêng để học các chính sách có hiệu suất cao mà không có khả năng tương tác với môi trường (Levine et al., 2020; Prudencio et al., 2022). Tuy nhiên, tương tự như những tiến bộ trong RL trực tuyến (Engstrom et al., 2020; Henderson et al., 2018), nhiều thuật toán đó đi kèm với sự phức tạp gia tăng – các lựa chọn thiết kế và triển khai vượt ra ngoài những đổi mới thuật toán cốt lõi, đòi hỏi nỗ lực tinh tế trong việc tái tạo, điều chỉnh siêu tham số và phân bổ nguyên nhân của những cải thiện hiệu suất.

Thực vậy, vấn đề phức tạp đã được đưa ra trong cộng đồng RL ngoại tuyến bởi Fujimoto & Gu (2021); các tác giả đã nêu bật những điều chỉnh thiết kế và mức triển khai bị che giấu (ví dụ, các kiến trúc khác nhau hoặc tiền huấn luyện actor) và sau đó chứng minh rằng một chuẩn hóa hành vi đơn giản được thêm vào TD3 (Fujimoto et al., 2018) tạo thành một baseline mạnh trong cài đặt ngoại tuyến. Thuật toán tối giản và gọn gàng này, TD3+BC, đã trở thành baseline tiêu chuẩn de-facto để so sánh. Thực vậy, hầu hết các thuật toán mới đều đối chiếu với nó và tuyên bố có những cải thiện đáng kể so với (Akimov et al., 2022; An et al., 2021; Nikulin et al., 2023; Wu et al., 2022; Chen et al., 2022b; Ghasemipour et al., 2022). Tuy nhiên, việc áp dụng những lựa chọn thiết kế và triển khai mới xuất hiện cho baseline này vẫn còn thiếu.

Trong công trình này, chúng tôi xây dựng trên dòng nghiên cứu của Fujimoto & Gu (2021) và đặt câu hỏi: mức độ mà những lựa chọn thiết kế nhỏ mới xuất hiện có thể thúc đẩy thuật toán RL ngoại tuyến tối giản là bao nhiều? Câu trả lời được minh họa trong Hình 1: chúng tôi đề xuất một phần mở rộng cho TD3+BC, ReBRAC (Mục 3), đơn giản chỉ thêm vào những quyết định thiết kế xuất hiện gần đây lên nó. Chúng tôi kiểm tra thuật toán của mình trên cả các bài toán không gian trạng thái proprioceptive và visual sử dụng các benchmark D4RL (Fu et al., 2020) và V-D4RL (Lu et al., 2022) (Mục 4) chứng minh hiệu suất tiên tiến của nó trên các phương pháp không sử dụng ensemble. Hơn nữa, phương pháp của chúng tôi chứng minh hiệu suất tiên tiến trong cài đặt ngoại tuyến sang trực tuyến trên các bộ dữ liệu D4RL (Mục 4.3) mà không được thiết kế cụ thể cho cài đặt này. Để nêu bật thêm hiệu quả của những sửa đổi được đề xuất, sau đó chúng tôi thực hiện nghiên cứu ablation quy mô lớn (Mục 4.4). Chúng tôi hy vọng phương pháp được mô tả có thể phục vụ như một baseline mạnh dưới các ngân sách tìm kiếm siêu tham số khác nhau (Mục 4.6), nhấn mạnh thêm tầm quan trọng của những lựa chọn thiết kế có vẻ nhỏ nhặt được giới thiệu cùng với những đổi mới thuật toán cốt lõi.

## 2 Kiến thức cơ bản

### 2.1 Học tăng cường ngoại tuyến

Một bài toán Học tăng cường tiêu chuẩn được định nghĩa như một Quá trình Quyết định Markov (MDP) với tuple {S, A, P, R, γ}, trong đó S⊂Rn là không gian trạng thái, A⊂Rm là không gian hành động, P:S×A→S là hàm chuyển tiếp, R:S×A→R là hàm phần thưởng, và γ∈(0,1) là hệ số chiết khấu. Mục tiêu cuối cùng là tìm một chính sách π(a|s) tối đa hóa lợi nhuận chiết khấu tích lũy Eπ[∑∞t=0γtR(st, at)]. Chính sách này cải thiện bằng cách tương tác với môi trường, quan sát các trạng thái, và thực hiện các hành động cung cấp phần thưởng.

Trong RL ngoại tuyến, các chính sách không thể tương tác với môi trường và chỉ có thể truy cập vào một bộ dữ liệu giao dịch tĩnh D được thu thập bởi một hoặc nhiều chính sách khác. Cài đặt này đưa ra những thách thức mới, chẳng hạn như ước tính giá trị cho các cặp trạng thái-hành động không được bao gồm trong bộ dữ liệu trong khi không có khả năng khám phá (Levine et al., 2020).

### 2.2 Actor-Critic được Chuẩn hóa Hành vi

Behavior Regularized Actor-Critic (BRAC) là một framework RL ngoại tuyến được giới thiệu trong Wu et al. (2019). Ý tưởng cốt lõi đằng sau BRAC là các thuật toán actor-critic có thể được phạt theo hai cách để giải quyết các tác vụ RL ngoại tuyến: phạt actor và phạt critic. Trong framework này, mục tiêu actor được biểu diễn như trong Phương trình (1), và mục tiêu critic như trong Phương trình (2), trong đó F là một hàm phân kỳ giữa phân phối hành động dữ liệu và phân phối hành động chính sách. Những khác biệt so với actor-critic vanilla được tô màu xanh.

π = argmaxπ E(s,a)∼D[Qθ(s, π(s))−α·F(π(s), a)] (1)
θ = argminθ E(s,a,r,s′,â′)∼D, a′∼π(s′)[(Qθ(s, a)−(r+γ(Qθ(s′, a′)−α·F(a′,â′))))²] (2)

Trong công trình gốc, các lựa chọn khác nhau của F đã được đánh giá khi sử dụng làm thuật ngữ chuẩn hóa cho actor hoặc critic. Các tác giả đã kiểm tra phân kỳ KL, Kernel MMD, và khoảng cách Wasserstein nhưng không quan sát được bất kỳ lợi thế nhất quán nào. Cuối cùng, điều quan trọng cần lưu ý là, ban đầu, cả hai hệ số chuẩn hóa đều có cùng trọng số.

Sau đó, TD3+BC (Fujimoto & Gu, 2021) đã được giới thiệu, sử dụng Mean Squared Error (MSE) làm thuật ngữ chuẩn hóa F cho actor. TD3+BC được coi là phương pháp tối giản cho RL ngoại tuyến vì nó sửa đổi các thuật toán RL hiện có bằng cách đơn giản thêm thuật ngữ behavioral cloning vào loss của actor, dễ triển khai và không mang lại overhead tính toán đáng kể nào.

## 3 ReBRAC: Chưng cất các Lựa chọn Thiết kế Chủ chốt

Trong phần này, chúng tôi mô tả phương pháp được đề xuất cùng với thảo luận về các lựa chọn thiết kế mới được gặp trong tài liệu RL ngoại tuyến (Bảng 1). Phương pháp của chúng tôi là một phiên bản tổng quát hơn của BRAC được xây dựng trên nền tảng thuật toán TD3+BC (Fujimoto & Gu, 2021) với các sửa đổi khác nhau trong thiết kế trong khi vẫn giữ nó đơn giản (Hình 1). Chúng tôi gọi phương pháp của mình là Revisited BRAC (ReBRAC).

**Mạng sâu hơn** Việc sử dụng mạng nơ-ron sâu hơn đã là một yếu tố quan trọng trong thành công của nhiều mô hình Deep Learning, với chất lượng mô hình thường tăng theo độ sâu, miễn là có đủ dữ liệu để hỗ trợ việc mở rộng này (Kaplan et al., 2020). Tương tự, các nghiên cứu gần đây trong RL (Neumann & Gros, 2022; Sinha et al., 2020) và RL ngoại tuyến (Lee et al., 2022; Kumar et al., 2022) đã chứng minh tầm quan trọng của độ sâu trong việc đạt được hiệu suất cao. Mặc dù hầu hết các thuật toán RL ngoại tuyến dựa trên SAC (Haarnoja et al., 2018) hoặc TD3 (Fujimoto et al., 2018), mặc định sử dụng hai lớp ẩn, công trình gần đây (Kumar et al., 2020) sử dụng ba lớp ẩn cho SAC thay vào đó, điều này có vẻ là một thay đổi quan trọng (Fujimoto & Gu, 2021). Thay đổi kích thước mạng đã được các công trình sau này áp dụng (An et al., 2021; Yang et al., 2022; Zhuang et al., 2023; Nikulin et al., 2023) và có thể là một trong những sửa đổi quan trọng cải thiện hiệu suất cuối cùng.

Các thuật toán BRAC và TD3+BC ban đầu chỉ sử dụng hai lớp ẩn cho mạng actor và critic của chúng, trong khi hầu hết các giải pháp tiên tiến sử dụng mạng sâu hơn. Cụ thể, ba lớp ẩn đã trở thành lựa chọn phổ biến cho các thuật toán RL ngoại tuyến gần đây. Trong ReBRAC, chúng tôi tuân theo xu hướng này và sử dụng ba lớp ẩn cho mạng actor và critic. Ngoài ra, chúng tôi cung cấp nghiên cứu ablation trong Mục 4.5 để điều tra tác động của số lượng lớp lên hiệu suất của ReBRAC.

**LayerNorm** LayerNorm (Ba et al., 2016) là một kỹ thuật được sử dụng rộng rãi trong deep learning giúp cải thiện sự hội tụ của mạng. Trong Hiraoka et al. (2021), các tác giả thêm dropout và LayerNorm vào các thuật toán RL khác nhau, đáng chú ý là tăng hiệu suất của chúng. Kỹ thuật này cũng được áp dụng trong Smith et al. (2022), và có vẻ như sự tăng cường đạt được chủ yếu là do LayerNorm. Cụ thể, trong RL ngoại tuyến, nhiều nghiên cứu khác nhau đã kiểm tra tác động của chuẩn hóa giữa các lớp (Bhatt et al., 2019; Kumar et al., 2022; Nikulin et al., 2022, 2023). Một nghiên cứu song song của Ball et al. (2023) chứng minh thực nghiệm rằng LayerNorm giúp ngăn chặn việc ngoại suy giá trị thảm khốc cho hàm Q khi sử dụng bộ dữ liệu ngoại tuyến trong RL trực tuyến. Theo các công trình này, trong phương pháp của chúng tôi, chúng tôi cũng áp dụng LayerNorm giữa mỗi lớp của mạng critic.

**Batch lớn hơn** Một kỹ thuật khác để tăng tốc sự hội tụ của mạng nơ-ron là tối ưu hóa batch lớn (You et al., 2017, 2019). Mặc dù việc nghiên cứu kích thước batch lớn hơn 256 bị hạn chế, một số công trình trước đây đã sử dụng chúng. Ví dụ, sự hội tụ của SAC-N đã được tăng tốc trong Nikulin et al. (2022). Các thuật toán được đề xuất gần đây hơn cũng sử dụng batch lớn hơn để huấn luyện, mặc dù không cung cấp phân tích chi tiết (Akimov et al., 2022; Nikulin et al., 2023).

Việc sử dụng batch lớn trong RL ngoại tuyến vẫn chưa được nghiên cứu đầy đủ, và lợi ích cũng như hạn chế của nó chưa được hiểu đầy đủ. Các thí nghiệm của chúng tôi cho thấy rằng trong một số domain, việc sử dụng batch lớn có thể dẫn đến cải thiện hiệu suất đáng kể, trong khi ở những domain khác, nó có thể không có tác động đáng chú ý hoặc thậm chí làm giảm hiệu suất (xem Bảng 8). Chúng tôi tăng kích thước batch lên 1024 mẫu và điều chỉnh tỷ lệ học tập cho các tác vụ D4RL Gym-MuJoCo, theo phương pháp được đề xuất bởi Nikulin et al. (2022).

**Tách rời phạt actor và critic** Framework BRAC ban đầu đề xuất phạt actor và critic với cùng độ lớn. Hầu hết các thuật toán trước đây chỉ hạn chế actor (Fujimoto & Gu, 2021; Wu et al., 2022) hoặc chỉ critic (Kumar et al., 2020; An et al., 2021; Ghasemipour et al., 2022). TD3-CVAE (Rezaeifar et al., 2022) phạt cả hai bằng cùng hệ số, trong khi một nghiên cứu khác, SAC-RND (Nikulin et al., 2023), cho thấy rằng việc tách rời việc phạt trong RL ngoại tuyến có lợi cho hiệu suất thuật toán, mặc dù thiếu ablation về việc chỉ sử dụng một trong các hình phạt.

Phương pháp của chúng tôi cho phép phạt đồng thời actor và critic với các tham số tách rời. Được truyền cảm hứng bởi TD3+BC (Fujimoto & Gu, 2021), Mean Squared Error (MSE) được sử dụng làm hàm phân kỳ F, mà chúng tôi thấy đơn giản và hiệu quả. Mục tiêu actor được hiển thị trong Phương trình (3), và mục tiêu critic được hiển thị trong Phương trình (4). Những khác biệt so với actor-critic ban đầu được tô màu đỏ. Theo TD3+BC (Fujimoto & Gu, 2021), hàm Q được chuẩn hóa để làm cho thuật toán ít nhạy cảm hơn với các tham số chuẩn hóa. Tuy nhiên, chúng tôi từ bỏ việc sử dụng chuẩn hóa trạng thái, như ban đầu được đề xuất trong TD3 + BC, được thúc đẩy bởi ý định thực thi thuật toán trực tuyến và quan sát rằng điều chỉnh này thường dẫn đến tác động không đáng kể. Vì phương pháp của chúng tôi chủ yếu xây dựng trên TD3+BC, những khác biệt trong hiệu suất của chúng nên được coi là quan trọng nhất và không xuất hiện chỉ vì tìm kiếm siêu tham số bổ sung.

π = argmaxπ E(s,a)∼D[Qθ(s, π(s))−β1·(π(s)−a)²] (3)
θ = argminθ E(s,a,r,s′,â′)∼D, a′∼π(s′)[(Qθ(s, a)−(r+γ(Qθ(s′, a′)−β2·(a′−â′)²)))²] (4)

**Thay đổi giá trị hệ số chiết khấu γ** Việc lựa chọn hệ số chiết khấu là một khía cạnh quan trọng trong việc giải quyết các bài toán RL (Jiang et al., 2015). Một nghiên cứu gần đây (Hu et al., 2022) đề xuất rằng việc giảm giá trị mặc định của γ từ 0.99 có thể dẫn đến kết quả tốt hơn trong cài đặt RL ngoại tuyến. Ngược lại, trong SPOT (Wu et al., 2022), các tác giả tăng giá trị của γ lên 0.995 khi tinh chỉnh các tác vụ AntMaze với phần thưởng thưa thớt, điều này dẫn đến các giải pháp tiên tiến. Tương tự, trong cài đặt ngoại tuyến SAC-RND (Nikulin et al., 2023), việc tăng γ cũng đạt được hiệu suất cao trên cùng domain. Việc lựa chọn γ tăng cho các tác vụ AntMaze được thúc đẩy bởi phần thưởng thưa thớt, tức là giá trị γ thấp có thể không truyền tín hiệu huấn luyện tốt. Tuy nhiên, cần có thêm ablation để hiểu liệu việc thay đổi tham số có trực tiếp chịu trách nhiệm cho hiệu suất được cải thiện hay không. Trong các thí nghiệm của chúng tôi, chúng tôi cũng thấy rằng việc tăng γ từ giá trị mặc định 0.99 lên 0.999 là rất quan trọng để cải thiện hiệu suất trên bộ tác vụ này (xem Bảng 8).

## 4 Thí nghiệm

### 4.1 Đánh giá trên D4RL ngoại tuyến

Chúng tôi đánh giá phương pháp được đề xuất trên ba bộ tác vụ D4RL: Gym-MuJoCo, AntMaze, và Adroit. Đối với mỗi domain, chúng tôi xem xét tất cả các bộ dữ liệu có sẵn. Chúng tôi so sánh kết quả của mình với một số baseline không sử dụng ensemble, bao gồm TD3+BC (Fujimoto & Gu, 2021), IQL (Kostrikov et al., 2021), CQL (Kumar et al., 2020) và SAC-RND (Nikulin et al., 2023).

Phần lớn các siêu tham số được áp dụng từ TD3+BC, trong khi các tham số β1 và β2 từ Phương trình (3) và Phương trình (4) được điều chỉnh. Chúng tôi kiểm tra độ nhạy cảm với các tham số này trong Mục 4.6. Để có cái nhìn tổng quan đầy đủ về thiết lập thí nghiệm và chi tiết, xem Phụ lục A.

Theo Wu et al. (2022), chúng tôi điều chỉnh siêu tham số trên bốn seed (được gọi là training seeds) và đánh giá các tham số tốt nhất trên mười seed mới (được gọi là unseen training seeds), báo cáo hiệu suất trung bình của các checkpoint cuối cùng cho các tác vụ D4RL. Trên V-D4RL, chúng tôi sử dụng lần lượt hai và năm seed. Điều này giúp tránh overfitting trong quá trình tìm kiếm siêu tham số và đưa ra kết quả công bằng và có thể tái tạo hơn. Để so sánh công bằng, chúng tôi điều chỉnh TD3+BC và IQL theo cùng giao thức. Chúng tôi cũng chạy lại SAC-RND trên các tác vụ Gym-MuJoCo và AntMaze và điều chỉnh nó cho domain Adroit.

Kết quả kiểm tra của chúng tôi trên các tác vụ Gym-MuJoCo, AntMaze, và Adroit của D4RL có sẵn trong Bảng 2, Bảng 3, Bảng 4, tương ứng. Kết quả tốt nhất theo trung bình trong số các thuật toán được tô đậm, và hiệu suất tốt thứ hai được gạch chân. Phương pháp của chúng tôi, ReBRAC, đạt được kết quả tiên tiến trên các tác vụ Gym-MuJoCo, AntMaze, và Adroit vượt trội so với tất cả baseline trên trung bình, ngoại trừ SAC-RND trên các tác vụ Gym-MuJoCo, tốt hơn một chút. Hồ sơ hiệu suất và xác suất cải thiện (Agarwal et al., 2021) trong Hình 1b và Hình 1c cũng chứng minh rằng ReBRAC có khả năng cạnh tranh khi so sánh với các thuật toán mà chúng tôi đối chiếu. Phương pháp của chúng tôi cũng có thể so sánh với các phương pháp dựa trên ensemble (xem Phụ lục C để có thêm so sánh).

### 4.2 Đánh giá trên V-D4RL ngoại tuyến

Ngoài việc kiểm tra ReBRAC trên D4RL, chúng tôi đánh giá hiệu suất của nó trên benchmark V-D4RL (Lu et al., 2022). Động lực của chúng tôi để làm như vậy là thực tế rằng điểm số trên các tác vụ D4RL Gym-MuJoCo đã bão hòa trong những năm gần đây, và ngay cả sau khi giới thiệu các phương pháp RL ngoại tuyến dựa trên ensemble bởi An et al. (2021), đã không có tiến bộ đáng chú ý nào trên các tác vụ này. Mặt khác, V-D4RL cung cấp một bộ vấn đề tương tự, với các bộ dữ liệu được thu thập theo cách tương tự như trong D4RL nhưng với quan sát của agent giờ đây là hình ảnh từ môi trường.

Chúng tôi kiểm tra thuật toán của mình trên tất cả các bộ dữ liệu tác vụ đơn có sẵn không có yếu tố phân tâm và so sánh nó với các baseline từ công trình V-D4RL ban đầu (Lu et al., 2022). Kết quả được báo cáo trong Bảng 5. Phương pháp được đề xuất của chúng tôi đạt được kết quả tiên tiến hoặc gần tiên tiến trên hầu hết các tác vụ, và đây là phương pháp duy nhất, trên trung bình, hoạt động đáng chú ý tốt hơn Behavioral Cloning ngây thơ.

### 4.3 Đánh giá trên D4RL ngoại tuyến sang trực tuyến

Việc đánh giá hiệu suất ngoại tuyến sang trực tuyến là một khía cạnh then chốt đối với các thuật toán học tăng cường (RL), đặc biệt trong bối cảnh những phát triển gần đây. Trong bối cảnh này, chúng tôi đã thực hiện các kiểm tra bổ sung trên ReBRAC, vì nó nổi bật như một thuật toán đầy hứa hẹn vì một số lý do thuyết phục.

Trước hết và quan trọng nhất, ReBRAC chứng minh năng lực đáng chú ý sau tiền huấn luyện ngoại tuyến. Thứ hai, thuật toán của chúng tôi có những điểm tương đồng đáng chú ý với TD3+BC, một phương pháp đã thể hiện hiệu quả trong tinh chỉnh trực tuyến như được quan sát bởi Beeson et al. (Beeson & Montana, 2022).

Để đơn giản, chúng tôi chọn vô hiệu hóa việc phạt critic trong quá trình tinh chỉnh trực tuyến. Hơn nữa, chúng tôi giảm tuyến tính hình phạt của actor xuống một nửa giá trị ban đầu của nó, theo phương pháp được mô tả bởi Beeson & Montana (2022). Đáng chú ý, không có điều chỉnh siêu tham số nào được thực hiện trong quá trình này.

Để đánh giá phương pháp của chúng tôi trong cài đặt ngoại tuyến sang trực tuyến, chúng tôi tuân theo phương pháp được nêu bởi Tarasov et al. (2022). Trong phân tích so sánh của chúng tôi, chúng tôi xem xét các thuật toán sau: TD3+BC (Fujimoto & Gu, 2021), IQL (Kostrikov et al., 2021), CQL (Kumar et al., 2020), SPOT (Wu et al., 2022), và Cal-CQL (Nakamoto et al., 2023). Điểm số sau giai đoạn ngoại tuyến và điều chỉnh trực tuyến, được báo cáo trong Bảng 6. Chúng tôi cũng cung cấp finetuning cumulative regret được đề xuất bởi Nakamoto et al. (2023) trong Bảng 7.

ReBRAC thể hiện hiệu suất cạnh tranh, vượt trội so với bốn trong số sáu bộ dữ liệu AntMaze và đạt được kết quả tiên tiến về điểm số cuối cùng trên các tác vụ Adroit. Trên trung bình, ReBRAC vượt trội so với đối thủ gần nhất, Cal-CQL, được thiết kế cụ thể cho vấn đề ngoại tuyến sang trực tuyến trong công trình đồng thời (Nakamoto et al., 2023).

Về regret, ReBRAC vượt trội so với tất cả các thuật toán khác, với Cal-QL là ngoại lệ duy nhất, cho thấy kết quả tốt hơn một chút trên trung bình chỉ trong domain AntMaze.

### 4.4 Ablation các Lựa chọn Thiết kế

Để hiểu rõ hơn nguồn gốc của hiệu suất được cải thiện, chúng tôi đã thực hiện nghiên cứu ablation về các sửa đổi được thực hiện đối với thuật toán. Kết quả có thể được tìm thấy trong Bảng 8. Các nghiên cứu ablation bổ sung cho tất cả các bộ dữ liệu có thể được tìm thấy trong Phụ lục G. Một sửa đổi tại một thời điểm đã được vô hiệu hóa, trong khi tất cả các thay đổi khác được giữ lại, bao gồm layer normalization trong mạng critic, các lớp tuyến tính bổ sung trong mạng actor và critic, thêm hình phạt MSE vào loss của critic và actor. Trong trường hợp AntMaze, chúng tôi cũng cố gắng sử dụng giá trị γ mặc định thay vì giá trị tăng. Để chứng minh thêm hiệu quả của các sửa đổi của chúng tôi, chúng tôi cũng chạy triển khai của mình tương đương với TD3+BC ban đầu, với tất cả các thay đổi bị vô hiệu hóa và siêu tham số được lấy từ bài báo gốc. Thí nghiệm này phục vụ để cho thấy rằng điểm số được cải thiện là do những thay đổi được đề xuất trong thuật toán chứ không chỉ là các triển khai khác nhau. Hơn nữa, chúng tôi tìm kiếm tham số chuẩn hóa cho triển khai TD3+BC của chúng tôi để chứng minh rằng việc điều chỉnh tham số này không phải là nguồn duy nhất của cải thiện. Hơn nữa, chúng tôi kiểm tra TD3 + BC bằng cách thêm từng sửa đổi của ReBRAC một cách độc lập để chứng minh rằng mỗi sửa đổi riêng lẻ không đủ để đạt được hiệu suất của ReBRAC nơi sự kết hợp của các sửa đổi xuất hiện.

Ngoài ra, chúng tôi chạy ablation để xác thực tầm quan trọng của việc tách rời bằng cách vô hiệu hóa nó và tìm kiếm giá trị tham số phạt tốt nhất từ tập hợp tất cả các giá trị đã sử dụng trước đây được liệt kê trong Phụ lục B.

Kết quả ablation cho thấy ReBRAC vượt trội so với TD3+BC không phải vì các triển khai khác nhau hoặc lựa chọn tham số chuẩn hóa actor. Tất cả các domain đều bị ảnh hưởng khi LayerNorm bị vô hiệu hóa, dẫn đến hiệu suất trung bình giảm một nửa. Việc loại bỏ các lớp bổ sung dẫn đến sự sụt giảm đáng chú ý trong các tác vụ AntMaze, trong khi trên Gym-MuJoCo sự giảm nhỏ, và trên các tác vụ Adroit, chúng ta có thể thấy một sự tăng nhẹ. Thuật toán không thể học hầu hết các tác vụ khi hình phạt actor bị vô hiệu hóa. Đáng chú ý, hình phạt critic đóng vai trò nhỏ trong việc cải thiện hiệu suất trên hầu hết các vấn đề cũng như việc tách rời các hình phạt. Việc sử dụng kích thước batch tiêu chuẩn trên các tác vụ Gym-MuJoCo làm giảm đáng kể điểm số cuối cùng, trong khi việc sử dụng hệ số chiết khấu tăng cho AntMaze là rất quan trọng để có được hiệu suất tiên tiến.

Dựa trên các nghiên cứu ablation được thực hiện, cấu hình được đề xuất của các lựa chọn thiết kế dẫn đến hiệu suất tốt nhất trên trung bình. Lưu ý rằng việc điều chỉnh những lựa chọn này cho từng tác vụ một cách độc lập làm cho có thể đạt được điểm số thậm chí cao hơn những gì chúng tôi báo cáo trong Mục 4.1. Chúng tôi cũng có thể thay đổi số lượng lớp ẩn trong mạng, dẫn đến hiệu suất tốt hơn, xem Mục 4.5. Nhưng trong cuộc sống thực, đánh giá thuật toán có thể tốn kém, vì vậy chúng tôi giới hạn bản thân chỉ điều chỉnh các tham số chuẩn hóa.

### 4.5 Xếp chồng Thêm nhiều Lớp

Như các ablation cho thấy, độ sâu của mạng đóng vai trò quan trọng khi giải quyết các tác vụ AntMaze và HalfCheetah (xem Phụ lục G). Chúng tôi thực hiện các thí nghiệm bổ sung để kiểm tra cách hiệu suất phụ thuộc vào độ sâu mạng chi tiết hơn. Ball et al. (2023) sử dụng mạng có độ sâu bốn khi giải quyết các tác vụ AntMaze. Mục tiêu của chúng tôi là tìm điểm mà hiệu suất bão hòa. Để làm điều này, chúng tôi chạy thuật toán của mình trên các tác vụ AntMaze tăng số lượng lớp lên sáu trong khi giữ các tham số khác không thay đổi. Chúng tôi cố gắng tăng actor và critic riêng biệt và cùng một lúc. Chúng tôi cũng giảm kích thước của mỗi mạng một lớp tương tự. Kết quả có thể được tìm thấy trong Hình 2.

Một số kết luận có thể được rút ra từ kết quả. Thứ nhất, việc tăng thêm số lượng lớp có thể dẫn đến kết quả tốt hơn trên domain AntMaze khi các lớp được mở rộng lên năm. Đối với sáu lớp, hiệu suất giảm hoặc không cải thiện. Thứ hai, việc giảm kích thước của critic dẫn đến hiệu suất tệ nhất trên hầu hết các bộ dữ liệu. Cuối cùng, không có mô hình rõ ràng về cách hiệu suất thay đổi ngay cả trong một domain duy nhất. Sự sụt giảm trên sáu lớp là đặc điểm chung duy nhất có thể được nhìn thấy. Trên trung bình, bốn lớp critic và ba lớp actor là tốt nhất. Thay đổi chỉ mạng của actor ổn định hơn trên trung bình.

### 4.6 Phân tích Độ nhạy Penalization

Theo Kurenkov & Kolesnikov (2022), chúng tôi chứng minh độ nhạy cảm của ReBRAC đối với lựa chọn siêu tham số β1 và β2 dưới lựa chọn chính sách đồng nhất trên các tác vụ D4RL sử dụng Expected Online Performance (EOP) và so sánh nó với TD3+BC và IQL. EOP cho thấy hiệu suất tốt nhất được mong đợi tùy thuộc vào số lượng chính sách có thể được triển khai để đánh giá trực tuyến. Kết quả được chứng minh trong Bảng 9. Như có thể thấy, khoảng mười chính sách được yêu cầu để ReBRAC đạt được hiệu suất tiên tiến không sử dụng ensemble. EOP của ReBRAC cao hơn cho bất kỳ số lượng chính sách trực tuyến nào khi so sánh với TD3+BC và tốt hơn IQL trên tất cả các domain khi ngân sách đánh giá lớn hơn hai chính sách. Xem Phụ lục F cho EOP được tách theo tác vụ.

## 5 Công trình liên quan

**Phương pháp RL ngoại tuyến không sử dụng ensemble.** Trong những năm gần đây, nhiều thuật toán học tăng cường ngoại tuyến đã được phát triển. TD3+BC (Fujimoto & Gu, 2021) đại diện cho một phương pháp tối giản cho RL ngoại tuyến, tích hợp một thành phần Behavioral Cloning vào loss của actor, cho phép các thuật toán actor-critic trực tuyến hoạt động trong cài đặt ngoại tuyến. CQL (Kumar et al., 2020) điều khiển mạng critic để gán giá trị thấp hơn cho các cặp trạng thái-hành động ngoài phân phối và giá trị cao hơn cho các cặp trong phân phối. IQL (Kostrikov et al., 2021) đề xuất một phương pháp để học một chính sách mà không lấy mẫu các hành động ngoài phân phối.

Mặc dù vậy, các phương pháp tinh vi hơn có thể cần thiết để đạt được kết quả tiên tiến trong một cài đặt không sử dụng ensemble. Ví dụ, Chen et al. (2022b); Akimov et al. (2022) tiền huấn luyện các dạng encoder khác nhau cho hành động, sau đó tối ưu hóa actor để dự đoán hành động trong không gian tiềm ẩn. SPOT (Wu et al., 2022) tiền huấn luyện Variational Autoencoder và sử dụng độ không chắc chắn của nó để phạt actor vì lấy mẫu hành động OOD trong khi SAC-RND (Nikulin et al., 2023) áp dụng Random Network Distillation và phạt actor và critic.

**Phương pháp RL ngoại tuyến dựa trên ensemble.** Một số lượng đáng kể các công trình trong học tăng cường ngoại tuyến cũng đã tận dụng các phương pháp ensemble để ước tính độ không chắc chắn. Thuật toán SAC-N được giới thiệu gần đây (An et al., 2021) đã vượt trội so với tất cả các phương pháp trước đây trên các tác vụ D4RL Gym-MuJoCo; tuy nhiên, nó đòi hỏi ensemble lớn cho một số tác vụ, chẳng hạn như tác vụ hopper, yêu cầu kích thước ensemble 500 và áp đặt gánh nặng tính toán đáng kể. Để giảm thiểu điều này, thuật toán EDAC đã được giới thiệu trong cùng công trình, sử dụng đa dạng hóa ensemble để giảm kích thước ensemble từ 500 xuống 50. Mặc dù giảm, kích thước ensemble vẫn còn đáng kể so với các lựa chọn thay thế không sử dụng ensemble. Đáng đề cập rằng cả SAC-N và EDAC đều không có khả năng giải quyết các tác vụ AntMaze phức tạp (Tarasov et al., 2022).

Một thuật toán tiên tiến khác trong các tác vụ Gym-MuJoCo là RORL (Yang et al., 2022), là một sửa đổi của SAC-N làm cho hàm Q mạnh mẽ và mượt mà hơn bằng cách làm nhiễu các cặp trạng thái-hành động với việc sử dụng các hành động ngoài phân phối. RORL cũng yêu cầu kích thước ensemble lên đến 20. Mặt khác, MSG (Ghasemipour et al., 2022) sử dụng mục tiêu độc lập cho mỗi thành viên ensemble và đạt được hiệu suất tốt trên các tác vụ Gym-MuJoCo với kích thước ensemble bốn nhưng yêu cầu 64 thành viên ensemble để đạt được hiệu suất tiên tiến trên các tác vụ AntMaze.

**Ablation các lựa chọn thiết kế.** Ablation của các lựa chọn thiết kế khác nhau trên các baseline đã được thiết lập rất hạn chế, đặc biệt trong RL ngoại tuyến. Nó chỉ được chỉ ra bởi Fujimoto & Gu (2021) rằng CQL hoạt động kém nếu các khác biệt không thuật toán được đề xuất bị loại bỏ. Một nghiên cứu song song (Ball et al., 2023) cho thấy rằng một số sửa đổi được xem xét (LayerNorm và độ sâu mạng) quan trọng khi được sử dụng trong cài đặt RL trực tuyến với dữ liệu ngoại tuyến, khác với RL ngoại tuyến thuần túy.

## 6 Kết luận, Hạn chế, và Công việc tương lai

Trong công trình này, chúng tôi xem xét lại những tiến bộ gần đây trong lĩnh vực RL ngoại tuyến trong hai năm qua và tích hợp một bộ cải tiến khiêm tốn vào baseline TD3+BC tối giản đã được thiết lập trước đây. Các thí nghiệm của chúng tôi chứng minh rằng mặc dù những cập nhật hạn chế này, chúng ta có thể đạt được kết quả cạnh tranh hơn trên các benchmark D4RL và V-D4RL ngoại tuyến và ngoại tuyến sang trực tuyến dưới các ngân sách siêu tham số khác nhau.

Mặc dù có những kết quả đáng chú ý, công trình của chúng tôi bị hạn chế ở một phương pháp và một tập con của các thay đổi thiết kế có thể. Việc khám phá tác động tiềm năng của những sửa đổi này đối với các phương pháp RL ngoại tuyến khác (ví dụ, IQL, CQL, MSG) và điều tra các lựa chọn thiết kế khác được sử dụng trong RL ngoại tuyến, ví dụ, lịch trình tỷ lệ học tập, dropout (như trong IQL), mạng rộng hơn, hoặc lựa chọn giữa các chính sách ngẫu nhiên và xác định là bắt buộc.
