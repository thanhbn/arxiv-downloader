# 2312.03814.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2312.03814.pdf
# Kích thước tệp: 4959313 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Journal of Machine Learning Research 25 (2024) 1-30 Nộp bài 2/24; Sửa đổi 6/24; Xuất bản 8/24
Pearl: Một Agent Học Tăng Cường Sẵn Sàng Cho Sản Xuất
Zheqing Zhu*, Rodrigo de Salvo Braz, Jalaj Bhandari, Daniel R. Jiang, Yi Wan, Yonathan Efroni,
Liyuan Wang, Ruiyang Xu, Hongbo Guo, Alex Nikulkov, Dmytro Korenkevych, Urun Dogan, Frank
Cheng, Zheng Wu, Wanqiao Xu
Nhóm Học Tăng Cường Ứng Dụng, AI tại Meta
*Tác giả liên hệ. Vui lòng gửi email đến billzhu@meta.com.
Biên tập viên: Zeyi Wen
Tóm tắt
Học tăng cường (RL) là một khung linh hoạt để tối ưu hóa các mục tiêu dài hạn. Mặc dù nhiều vấn đề thực tế có thể được hình thức hóa bằng RL, việc học và triển khai một chính sách RL hiệu quả đòi hỏi một hệ thống được thiết kế để giải quyết một số thách thức quan trọng, bao gồm nghịch lý khám phá-khai thác, khả năng quan sát một phần, không gian hành động động và các lo ngại về an toàn. Trong khi tầm quan trọng của những thách thức này đã được công nhận rộng rãi, các thư viện RL mã nguồn mở hiện có không giải quyết chúng một cách rõ ràng. Bài báo này giới thiệu Pearl, một gói phần mềm RL Sẵn Sàng Cho Sản Xuất được thiết kế để đối phó với những thách thức này theo cách modular. Ngoài việc trình bày kết quả đánh giá, chúng tôi cũng nêu bật các ví dụ về việc Pearl đang được áp dụng trong ngành công nghiệp để chứng minh lợi thế của nó cho các trường hợp sử dụng sản xuất. Pearl được mở mã nguồn trên GitHub tại github.com/facebookresearch/pearl và trang web chính thức của nó là pearlagent.github.io.
Từ khóa: Học tăng cường, phần mềm mã nguồn mở, Python, PyTorch

1 Giới thiệu
Lĩnh vực học tăng cường (RL) đã đạt được những thành công đáng kể trong những năm gần đây, bao gồm vượt qua hiệu suất của con người trong các trò chơi (Mnih et al., 2015; Silver et al., 2017), điều khiển robot trong các nhiệm vụ thao tác phức tạp (Peng et al., 2018; Levine et al., 2016; Lee et al., 2020), tối ưu hóa các hệ thống gợi ý quy mô lớn (Xu et al., 2023), và tinh chỉnh các mô hình ngôn ngữ lớn (Ouyang et al., 2022). Đồng thời, các thư viện RL mã nguồn mở, như Tianshou (Weng et al., 2022) và Stable-Baselines 3 (Raffin et al., 2021), đã nổi lên như những công cụ quan trọng cho nghiên cứu có thể tái tạo trong RL. Tuy nhiên, những thư viện hiện có này chủ yếu tập trung vào việc triển khai các thuật toán khác nhau để học chính sách, tức là các phương pháp để học một chính sách từ một luồng dữ liệu. Mặc dù đây là một tính năng cốt lõi trong RL, còn nhiều điều hơn nữa để thiết kế một giải pháp RL có thể được sử dụng cho các ứng dụng thực tế và hệ thống sản xuất. Ví dụ, luồng dữ liệu nên được thu thập như thế nào? Điều này được gọi là khám phá, một thách thức nổi tiếng trong RL (Sutton và Barto, 2018). Một câu hỏi quan trọng khác mà các nhà thiết kế hệ thống phải đối mặt khi triển khai các giải pháp RL là: các lo ngại về an toàn nên được tích hợp như thế nào (Dulac-Arnold et al., 2019)?

Trong bài báo này, chúng tôi giới thiệu Pearl, một gói phần mềm mã nguồn mở nhằm cho phép người dùng dễ dàng thiết kế các giải pháp RL thực tế. Ngoài việc học chính sách tiêu chuẩn, Pearl cũng cho phép người dùng dễ dàng tích hợp các tính năng bổ sung vào giải pháp RL của họ, bao gồm khám phá thông minh, ràng buộc an toàn, sở thích rủi ro, ước lượng trạng thái dưới khả năng quan sát một phần, và không gian hành động động. Nhiều tính năng này rất quan trọng để đưa RL từ nghiên cứu đến triển khai và phần lớn bị bỏ qua trong các thư viện hiện có. Để làm điều này, chúng tôi tập trung vào tính modular của agent, tức là thiết kế modular của một agent RL cho phép người dùng kết hợp các tính năng khác nhau. Pearl được xây dựng trên PyTorch gốc, hỗ trợ huấn luyện kích hoạt GPU, tuân thủ các phương pháp hay nhất về kỹ thuật phần mềm, và được thiết kế cho huấn luyện, kiểm tra và đánh giá phân tán. Các phần còn lại của bài báo cung cấp chi tiết về thiết kế và tính năng của Pearl, so sánh với các thư viện RL mã nguồn mở hiện có, và một số ví dụ về việc Pearl đang được áp dụng trong các ứng dụng ngành công nghiệp. Chúng tôi cung cấp các ví dụ sử dụng Pearl trong Phụ lục A. Kết quả của các thí nghiệm đánh giá trên các bộ thử nghiệm khác nhau được hiển thị trong Phụ lục B.

--- TRANG 2 ---
Zhu et al.

Hình 1: Giao diện của Pearl Agent

2 Thiết kế Pearl Agent
Chúng tôi đưa ra một cái nhìn tổng quan ngắn gọn về thiết kế và giao diện của Pearl. Pearl có bốn module chính: module học chính sách, module khám phá, module tóm tắt lịch sử, và module an toàn. Những module này triển khai một số yếu tố quan trọng mà chúng tôi nghĩ là thiết yếu cho việc học hiệu quả trong các vấn đề ra quyết định tuần tự thực tế. Đóng góp chính của Pearl là hỗ trợ những tính năng này theo cách thống nhất: Hình 1 cho thấy cách những module này được thiết kế để tương tác với nhau trong một môi trường RL trực tuyến. Lưu ý rằng trong việc thiết kế Pearl, chúng tôi tạo ra sự phân tách rõ ràng giữa agent RL và môi trường. Tuy nhiên, trong một số trường hợp, người dùng có thể chỉ có quyền truy cập vào dữ liệu được thu thập ngoại tuyến, vì vậy Pearl cũng cung cấp hỗ trợ cho RL ngoại tuyến. Dưới đây, chúng tôi cung cấp chi tiết về các phương pháp được triển khai trong từng module.

Module học chính sách: Pearl triển khai các thuật toán khác nhau để học một chính sách cho cả môi trường contextual bandit và vấn đề RL tuần tự đầy đủ. Đáng chú ý là Pearl hỗ trợ học chính sách dưới không gian hành động động, một tính năng không có trong các thư viện hiện có.

•Contextual bandits: Các phương pháp học bandit bao gồm mô hình hóa phần thưởng và sử dụng module khám phá để khám phá hiệu quả. Pearl hỗ trợ học bandit tuyến tính và neural cùng với các module khám phá khác nhau, cũng như phương pháp SquareCB (Foster và Rakhlin, 2020).

•Phương pháp dựa trên giá trị: Pearl hỗ trợ DQN (Mnih et al., 2015), Double DQN (Van Hasselt et al., 2016), Dueling DQN (Wang et al., 2016), Deep SARSA (Rummery và Niranjan, 1994) và Bootstrapped DQN (Osband et al., 2016)—một biến thể DQN với khám phá sâu dựa trên ensemble.

•Phương pháp actor-critic: SAC (Haarnoja et al., 2018), DDPG (Silver et al., 2014), TD3 (Fujimoto et al., 2018), PPO (Schulman et al., 2017), và REINFORCE (Sutton et al., 1999).

•Phương pháp RL ngoại tuyến: CQL (Kumar et al., 2020), IQL (Kostrikov et al., 2021) và TD3BC (Fujimoto và Gu, 2021).

•Phương pháp RL phân phối: Quantile Regression DQN (QRDQN) (Dabney et al., 2018b).

Module khám phá: Một trong những thách thức chính trong RL là xác định cách agent nên thu thập dữ liệu thông qua khám phá thông minh để học nhanh hơn. Chúng tôi thiết kế exploration_module trong Pearl theo cách hỗ trợ triển khai các thuật toán khám phá thông minh khác nhau cho contextual bandit và môi trường RL tuần tự đầy đủ. Các phương pháp upper confidence bound (UCB) và Thompson sampling (TS) phổ biến cho các vấn đề contextual bandit như LinUCB (Li et al., 2010), Neural-LinUCB (Xu et al., 2021), và LinTS (Agrawal và Goyal, 2013), cùng với phương pháp SquareCB gần đây của Foster và Rakhlin (2020), được bao gồm trong Pearl. Đối với môi trường RL, chúng tôi triển khai ý tưởng khám phá sâu bằng ensemble sampling (Osband et al., 2016), thực hiện khám phá nhất quán theo thời gian sử dụng các mẫu posterior xấp xỉ của hàm giá trị tối ưu. Các chiến lược khám phá đơn giản thường được triển khai trong các thư viện hiện có, như khám phá ϵ-greedy (Sutton và Barto, 2018), khám phá Gaussian cho không gian hành động liên tục (Lillicrap et al., 2015), và khám phá Boltzmann (Cesa-Bianchi et al., 2017) cũng được triển khai trong Pearl.

Module an toàn: RL an toàn là một rào cản quan trọng khác trong việc triển khai RL để giải quyết các vấn đề thực tế (García và Fernández, 2015; Dulac-Arnold et al., 2019). Pearl cung cấp ba tính năng để cho phép học an toàn. Phương thức filter_action trong module an toàn của Pearl cho phép người dùng chỉ định một không gian hành động an toàn cho mỗi tương tác môi trường. Ngoài ra, risk_sensitive_safety_module tạo điều kiện cho việc học nhạy cảm với rủi ro sử dụng các learner chính sách phân phối, xấp xỉ phân phối của phần thưởng tích lũy. Pearl sau đó triển khai các phương pháp khác nhau để tính toán hàm Q-value/giá trị dưới các chỉ số rủi ro khác nhau từ phân phối này. Pearl cũng được thiết kế để cho phép học trong MDPs bị ràng buộc. Điều này được thực hiện thông qua reward_constrained_safety_module cho phép người dùng chỉ định các ràng buộc về chi phí tích lũy, ngoài mục tiêu tối đa hóa phần thưởng. Chúng tôi triển khai phương pháp reward constrained policy optimization (RCPO) của Tessler et al. (2018) trong module này, tương thích với các learner chính sách khác nhau và các ràng buộc chi phí tổng quát.

Module tóm tắt lịch sử: Các ứng dụng RL yêu cầu học dưới khả năng quan sát một phần của trạng thái rất phổ biến (Levinson et al., 2011; Hauskrecht và Fraser, 2000). Để tạo điều kiện cho việc học trong môi trường quan sát một phần, chúng tôi trang bị Pearl với một module tóm tắt lịch sử triển khai hai chức năng. Đầu tiên, nó theo dõi lịch sử (và cập nhật nó) sau mỗi tương tác môi trường. Thay vì quan sát thô, lịch sử được lưu trữ trong tuple tương tác tại mỗi bước thời gian. Thứ hai, trong quá trình học chính sách (và lựa chọn hành động), module này cung cấp các cách khác nhau để tóm tắt lịch sử thành một biểu diễn trạng thái. Pearl hiện tại hỗ trợ sử dụng lịch sử thông qua việc chồng các cặp trạng thái-hành động trong quá khứ cũng như sử dụng mạng neural long-short-term-memory (LSTM) (Hochreiter và Schmidhuber, 1997) để học một biểu diễn trạng thái. Module tóm tắt lịch sử và chính sách có thể được huấn luyện (cùng nhau) theo cách end-to-end sử dụng một backward pass duy nhất.

3 So sánh với Các Thư viện Hiện có

Bảng 1: So sánh các tính năng của Pearl với các thư viện RL mã nguồn mở phổ biến hiện có

Tính năng | ReAgent | RLLib | SB3 | Tianshou | CleanRL | TorchRL | Pearl
--- | --- | --- | --- | --- | --- | --- | ---
Tính modular của agent | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ | ✓
Khám phá thông minh | ✗ | ✗ | ✗ | ✓ | ✗ | ✗ | ✓
Học an toàn | ✗ | ✗ | ✗ | ◦ | ◦ | ✗ | ✓
Tóm tắt lịch sử | ✗ | ✓ | ✗ | ✗ | ✗ | ✗ | ✓
Contextual bandits | ✓ | ◦ | ✗ | ✗ | ✗ | ✗ | ✓
RL ngoại tuyến | ✓ | ✓ | ✓ | ✓ | ✗ | ✓ | ✓
Không gian hành động động | ◦ | ✗ | ✗ | ✗ | ✗ | ✗ | ✓

Để minh họa những đổi mới của Pearl, chúng tôi so sánh các chức năng của nó với những thư viện RL phổ biến khác như ReAgent (Gauci et al., 2018), RLLib (Liang et al., 2018), StableBaselines3 (Raffin et al., 2021), Tianshou (Weng et al., 2022), CleanRL (Huang et al., 2022), và TorchRL (Bou et al., 2023). Về cơ bản, động cơ chính của những thư viện này là tạo điều kiện cho nghiên cứu và đánh giá có thể tái tạo của các thuật toán RL hiện có. Mặt khác, Pearl được thiết kế để triển khai một số khả năng quan trọng rất cần thiết để phát triển một hệ thống dựa trên RL có thể được triển khai trong thực tế. Bảng 1 so sánh một số tính năng này. Như được hiển thị, tất cả các thư viện hiện có cung cấp ít hoặc không hỗ trợ cho việc học an toàn. Ngoại trừ ReAgent, không có thư viện nào khác hỗ trợ không gian hành động động. Hỗ trợ cho các phương pháp khám phá thông minh và khả năng học trong môi trường quan sát một phần cũng thưa thớt trong các thư viện hiện có. Cuối cùng, Pearl thuộc số ít thư viện khác (bên cạnh ReAgent và một phần RLLib) hỗ trợ học trong cả môi trường contextual bandit và RL tuần tự đầy đủ trên cùng một nền tảng – điều này quan trọng vì nhiều ứng dụng ngành công nghiệp quy mô lớn thường được mô hình hóa như contextual bandits.

Chúng tôi nhấn mạnh rằng đổi mới chính đằng sau Pearl là khả năng kết hợp các tính năng nói trên theo cách thống nhất. Người dùng không cần viết mã từ đầu để thử nghiệm các kết hợp tính năng khác nhau. Ví dụ, việc triển khai các thuật toán học chính sách trong các thư viện khác thường đi kèm với một chiến lược khám phá cố định (ví dụ, DQN với khám phá ϵ-greedy). Tuy nhiên, Pearl cho phép khả năng kết hợp các phương pháp học chính sách và khám phá khác nhau, có thể dẫn đến các giải pháp RL hiệu quả hơn. Thiết kế modular của agent RL trong Pearl làm cho điều này và các kết hợp khác của các tính năng giữa khám phá, học an toàn, tóm tắt lịch sử, không gian hành động động, và RL ngoại tuyến trở nên khả thi. Nó cũng cho phép cả nhà nghiên cứu học thuật và ngành công nghiệp nhanh chóng thêm các module mới để thử nghiệm ý tưởng mới một cách hiệu quả.

4 Việc Áp dụng Pearl trong Các Ứng dụng Ngành công nghiệp

Chúng tôi hiện đang tích hợp Pearl để cho phép các giải pháp dựa trên RL trong các hệ thống sản xuất quy mô lớn, để giải quyết ba vấn đề thực tế. Bảng 2 dưới đây nêu bật một số khả năng độc đáo của Pearl, được kích hoạt bởi việc tập trung vào thiết kế agent modular, có thể phục vụ các yêu cầu sản phẩm khác nhau trong mỗi trường hợp.

Hệ thống gợi ý dựa trên đấu giá: Chúng tôi đang sử dụng Pearl để triển khai thiết kế thuật toán gần đây của chúng tôi trong (Xu et al., 2023) để sử dụng RL tối ưu hóa giá trị dài hạn trong một hệ thống gợi ý dựa trên đấu giá. Giải pháp này yêu cầu ước lượng hàm Q-value của chính sách dựa trên đấu giá sử dụng dữ liệu được thu thập trực tuyến, mà chúng tôi sử dụng triển khai Pearl về học temporal-difference ngoại tuyến trên chính sách. Hơn nữa, vì một tập hợp gợi ý độc đáo có sẵn tại mỗi bước thời gian, khả năng của Pearl hoạt động với không gian hành động động rất quan trọng cho việc triển khai của chúng tôi. Thiết kế modular của Pearl dễ dàng hỗ trợ tích hợp các mạng neural quy mô lớn, trong trường hợp này được sử dụng để ước lượng Q-values cho các tương tác người dùng-mục.

Đấu thầu đấu giá quảng cáo: Chúng tôi cũng đang sử dụng Pearl để triển khai một hệ thống RL cho đấu thầu đấu giá thời gian thực trong thị trường quảng cáo, nơi nhiệm vụ là đưa ra các đề xuất hiệu quả tuân theo các ràng buộc ngân sách của nhà quảng cáo. Để làm điều này, chúng tôi theo một giải pháp được nêu trong công trình gần đây của chúng tôi trong Korenkevych et al. (2023), yêu cầu thu thập dữ liệu (với một số khám phá) và sau đó huấn luyện một agent sử dụng RL ngoại tuyến. Chúng tôi tận dụng pipeline huấn luyện ngoại tuyến và module khám phá Gaussian trong Pearl cho việc triển khai của chúng tôi. Hơn nữa, chúng tôi đang sử dụng Pearl để thử nghiệm hai phần mở rộng của công trình trong Korenkevych et al. (2023). Phần mở rộng đầu tiên sử dụng submodule tối ưu hóa chính sách có ràng buộc của Pearl để tính đến các ràng buộc ngân sách của nhà quảng cáo. Phần mở rộng khác sử dụng module tóm tắt lịch sử của Pearl để mã hóa lịch sử tương tác thành biểu diễn trạng thái của agent. Điều này hữu ích vì thị trường quảng cáo có khả năng quan sát một phần (chỉ quan sát được đề xuất thắng cuộc).

Bảng 2: Pearl tích hợp các tính năng quan trọng cho việc áp dụng thực tế

Tính năng Pearl | Auction RecSys | Ads Auction Bidding | Creative Selection
--- | --- | --- | ---
Khám phá trực tuyến | ✓ | | ✓
Học an toàn | | ✓ | 
Tóm tắt lịch sử | | ✓ | 
Contextual bandits | | | ✓
RL ngoại tuyến | ✓ | ✓ | 
Không gian hành động động | ✓ | | ✓

Lựa chọn sáng tạo: Trong dự án này, Pearl đang được nhóm chúng tôi sử dụng để triển khai một giải pháp contextual bandit cho vấn đề lựa chọn sáng tạo. Lựa chọn sáng tạo phổ biến trong mạng xã hội và quảng cáo trực tuyến và đề cập đến vấn đề chọn một kết hợp các thành phần quảng cáo, như hình ảnh và văn bản, cùng với vị trí, phông chữ hoặc màu sắc của chúng, để trình bày cho người dùng. Chúng tôi đưa điều này thành một vấn đề contextual bandit và đang sử dụng Pearl để triển khai một giải pháp với một learner chính sách bandit neural cùng với khám phá neural linear UCB, để học sở thích của người dùng cho các sáng tạo khác nhau. Lựa chọn sáng tạo là một ví dụ khác nơi không gian hành động thay đổi một cách động, vì mỗi phần nội dung có thể được hiển thị sử dụng một tập hợp các sáng tạo có sẵn tương ứng. Một lần nữa, hỗ trợ của Pearl cho việc học với không gian hành động động làm đơn giản hóa đáng kể việc triển khai của chúng tôi.

--- TRANG 5 ---
Pearl: Một Agent Học Tăng Cường Sẵn Sàng Cho Sản Xuất

Phụ lục A. Sử dụng Pearl

Chúng tôi hiển thị một số ví dụ về cách khởi tạo một agent RL trong Pearl, được thực hiện bằng cách sử dụng lớp PearlAgent. Trong Ví dụ Mã 1 dưới đây, chúng tôi tạo một agent đơn giản sử dụng Deep Q-learning (DQN) để tối ưu hóa chính sách và khám phá ϵ-greedy. Đáng nhấn mạnh là exploration_module trong Pearl được truyền như một đầu vào cho policy_learner. Lựa chọn thiết kế này quan trọng để cho phép triển khai các thuật toán khám phá thông minh.

```python
env = GymEnvironment("CartPole-v1")
assert isinstance(env.action_space, DiscreteActionSpace)
num_actions: int = env.action_space.n
agent = PearlAgent(
    policy_learner=DeepQLearning(
        state_dim=env.observation_space.shape[0],
        action_space=env.action_space,
        hidden_dims=[64, 64],
        learning_rate=1e-4,
        exploration_module=EGreedyExploration(0.10),
        training_rounds=5,  # số lần cập nhật gradient mỗi bước học
        action_representation_module=OneHotActionTensorRepresentationModule(
            max_number_actions=num_actions
        ),
    ),
    replay_buffer=FIFOOffPolicyReplayBuffer(10000),
)
```

Ví dụ Mã 1: Một PearlAgent với DQN làm learner chính sách và khám phá ϵ-greedy.

```python
env = GymEnvironment("CartPole-v1")
assert isinstance(env.action_space, DiscreteActionSpace)
num_actions: int = env.action_space.n
agent = PearlAgent(
    policy_learner=QuantileRegressionDeepQLearning(
        state_dim=env.observation_space.shape[0],
        action_space=env.action_space,
        hidden_dims=[64, 64],
        learning_rate=1e-4,
        exploration_module=EGreedyExploration(0.10),
        training_rounds=5,
        action_representation_module=OneHotActionTensorRepresentationModule(
            max_number_actions=num_actions
        ),
    ),
    safety_module=QuantileNetworkMeanVarianceSafetyModule(
        variance_weighting_coefficient=0.2
    ),
    replay_buffer=FIFOOffPolicyReplayBuffer(10000),
)
```

Ví dụ Mã 2: Một PearlAgent với một learner chính sách phân phối (QRDQN) và module an toàn mean variance, ánh xạ phân phối ước lượng trên phần thưởng dài hạn thành Q-values.

Ví dụ Mã 2 minh họa cách Pearl có thể được sử dụng cho việc học nhạy cảm với rủi ro. Trong ví dụ này, QRDQN (một phương pháp RL phân phối) được sử dụng để học chính sách, cùng với một module an toàn nhạy cảm với rủi ro mean-variance. Lưu ý rằng việc học nhạy cảm với rủi ro đòi hỏi sử dụng cả learner chính sách phân phối và module an toàn nhạy cảm với rủi ro—cái trước ước lượng phân phối trên phần thưởng chiết khấu dài hạn và sử dụng thước đo rủi ro được triển khai trong cái sau để tính toán Q-values (được sử dụng để học chính sách). Kiểm tra tương thích được triển khai trong Pearl để tạo ra lỗi khi người dùng vô tình cố gắng sử dụng kết hợp các module không tương thích với nhau. Chúng tôi cũng cung cấp các giá trị mặc định để hỗ trợ người dùng mới muốn sử dụng Pearl, nhưng không quen thuộc với các chi tiết. Để ngắn gọn, chúng tôi không hiển thị thêm ví dụ mã và đề nghị người dùng tham khảo tài liệu và hướng dẫn của Pearl trên trang web chính thức để có thêm minh họa.

Cả hai ví dụ mã 1 và 2 đều cho thấy rằng một replay_buffer cũng cần được chỉ định khi khởi tạo một PearlAgent. Trong Pearl, các replay buffer khác nhau được triển khai để học trong các môi trường vấn đề khác nhau. Ví dụ, các buffer on-policy và off-policy first-in-first-out (FIFO) cho phép học trong các môi trường on-policy và off-policy tương ứng.

Chúng tôi kết thúc phần này với mã mẫu về giao diện agent-môi trường trong Pearl như được hiển thị dưới đây trong Ví dụ Mã 3. Sự phân tách rõ ràng trong Pearl giữa agent và môi trường đáng được nhấn mạnh: người dùng Pearl không phải chỉ định rõ ràng một môi trường. Đây là một tính năng hữu ích cho các ứng dụng thực tế, vì người dùng Pearl có thể xây dựng các agent để tương tác trực tiếp với thế giới thực, ví dụ, con người, với các wrapper API để cho phép học. Ví dụ, hãy xem xét việc thiết kế một agent chatbot trong Pearl có thể xuất ra phản hồi văn bản cho người dùng như một hành động. Agent sau đó có thể nhận truy vấn văn bản tiếp theo như một quan sát, được gói theo API PearlAgent. Ở đây, các tương tác trực tuyến có thể được kích hoạt mà không cần phải chỉ định các phương thức reset() và step() thường được yêu cầu bởi một instance của lớp Environment. Do đó, Pearl không bị giới hạn chỉ được sử dụng với các môi trường mô phỏng (ví dụ, Open AI Gym hoặc MuJoCo).

--- TRANG 7 ---
Pearl: Một Agent Học Tăng Cường Sẵn Sàng Cho Sản Xuất

```python
def online_learning_example(
    agent: PearlAgent,
    env: Environment,
    num_episodes: int,
    learn: bool = True,
    exploit: bool = True,
    learn_after_episode: bool = False,
) -> List[float]:
    """
    Chạy số tập được chỉ định và trả về danh sách với
    lợi nhuận theo tập.
    Args:
        agent (Agent): agent.
        env (Environment): môi trường.
        num_episodes (int): số tập để chạy.
        learn (bool, optional): Chạy 'agent.learn()' sau mỗi bước.
        exploit (bool, optional): yêu cầu agent chỉ khai thác.
        Mặc định là False. Nếu được đặt thành True, thì module khám phá
        của agent được sử dụng để khám phá.
        learn_after_episode (bool, optional): học chỉ vào cuối mỗi tập.
    Returns:
        List[float]: lợi nhuận của tất cả các tập.
    """
    returns = []
    for i in range(num_episodes):
        observation, action_space = env.reset()
        agent.reset(observation, action_space)
        cum_reward = 0
        done = False
        while not done:
            action = agent.act(exploit=exploit)
            action_result = env.step(action)
            cum_reward += action_result.reward
            agent.observe(action_result)
            # để học sau mỗi tương tác môi trường
            if learn and not learn_after_episode:
                agent.learn()
            done = action_result.done
        # để học chỉ vào cuối tập
        if learn and learn_after_episode:
            agent.learn()
        returns.append(cum_reward)
        print(f"Episode {i}: return = {cum_reward}")
    return returns
```

Ví dụ Mã 3: Minh họa cách người dùng có thể sử dụng một agent RL trong Pearl để tương tác với một môi trường và học trực tuyến. Lưu ý rằng môi trường đầu vào chỉ được yêu cầu để triển khai các hàm env.reset() và env.step().

A.1 Kiến trúc, Tính modular và thiết kế API

Như đã giải thích trong văn bản chính, Pearl bao gồm bốn loại module chính: learner chính sách, module khám phá, module tóm tắt lịch sử, và module an toàn. Trong phần này chúng tôi cung cấp thêm chi tiết về kiến trúc của Pearl và cách những module này tương tác.

Learner chính sách, module tóm tắt lịch sử, và module an toàn là các thành phần cấp cao, nghĩa là chúng có mặt ở cấp agent Pearl cấp cao. Module khám phá, mặt khác, là một thành phần của learner chính sách. Luồng thông tin xảy ra theo cách sau:

•Khi một quan sát được cung cấp cho agent Pearl, module tóm tắt lịch sử chuyển đổi nó thành một trạng thái, là biểu diễn nội bộ được sử dụng bởi các module khác. Bất kỳ module tóm tắt lịch sử nào cũng có thể làm điều này, bao gồm phương pháp tầm thường sử dụng quan sát làm trạng thái, hoặc phức tạp hơn tạo ra các biểu diễn trạng thái tính đến các quan sát trước đó. Phiên bản đơn giản nhất của cái sau là cập nhật một cửa sổ trượt của n quan sát quá khứ được chồng lên, trong khi một module tinh vi hơn sử dụng LSTM để tạo ra một trạng thái mã hóa các đặc điểm chính của lịch sử của agent. Các ví dụ về agents sử dụng những module này được hiển thị trong Ví dụ Mã 4 và 5.

```python
agent = PearlAgent(
    policy_learner=DeepQLearning(
        state_dim=observation_dim * 3,  # *3 do tóm tắt lịch sử
        action_space=env.action_space,
        hidden_dims=[64, 64],
        training_rounds=20,
        exploration_module=EGreedyExploration(epsilon=0.05),
    ),
    history_summarization_module=StackingHistorySummarizationModule(
        observation_dim=observation_dim,
        history_length=3
    ),
    ...
)
```

Ví dụ Mã 4: Một PearlAgent được định nghĩa để sử dụng module tóm tắt lịch sử stacking.

```python
agent = PearlAgent(
    policy_learner=DeepQLearning(
        state_dim=observation_dim,
        action_space=env.action_space,
        hidden_dims=[64, 64],
        exploration_module=EGreedyExploration(epsilon=0.05),
    ),
    history_summarization_module=LSTMHistorySummarizationModule(
        observation_dim=observation_dim,
        action_dim=action_dim,
        hidden_dim=128,
        history_length=8,
    ),
    ...
)
```

Ví dụ Mã 5: Một PearlAgent được định nghĩa để sử dụng module tóm tắt lịch sử LSTM.

•Trong quá trình quyết định hành động nào để thực hiện, các vấn đề an toàn phát sinh: các tập con hành động có thể bị phản đối (thông qua thước đo mềm về chi phí hoặc thông qua sở thích rủi ro được chỉ định bởi người dùng) hoặc hoàn toàn bị cấm. Hỗ trợ cho những tình huống này được cung cấp bởi các module an toàn. Hiện tại chúng tôi cung cấp module an toàn ràng buộc phần thưởng (Tessler et al., 2018) và module an toàn mean variance quantile network. Cái sau học một phân phối lợi nhuận cho mỗi cặp trạng thái-hành động; và tính toán Q-values dưới thước đo rủi ro được chỉ định bởi người dùng. Một ví dụ về PearlAgent được định nghĩa để sử dụng module an toàn quantile network mean variance được hiển thị trong Ví dụ Mã 6.

•Learner chính sách sau đó sử dụng trạng thái và không gian các hành động có sẵn để quyết định về một hành động. Bên cạnh thuật toán RL nó triển khai, learner chính sách cũng đưa ra quyết định dựa trên việc nó có ở chế độ khám phá hay không, và nó đang sử dụng chiến lược khám phá nào, được định nghĩa bởi module khám phá cụ thể được cung cấp bởi người dùng.

–Một số chiến lược khám phá, đặc biệt là những chiến lược dựa trên posterior sampling (Osband et al., 2016) yêu cầu truy cập vào các phần thông tin trung gian khác nhau được tính toán bởi learner chính sách, như giá trị của mỗi hành động hoặc ước lượng bất định. Nhu cầu về thông tin này là lý do tại sao module khám phá phải là một thành phần của learner chính sách (trái ngược với một module hậu xử lý bên ngoài learner chính sách chỉ có thể thấy hành động được chọn bởi cái trước). Learner chính sách cung cấp thông tin cần thiết cho module khám phá thông qua một API module khám phá đặc biệt, sau đó phản hồi với hành động thích hợp để thực hiện dựa trên chiến lược khám phá hiện tại. Lưu ý rằng learner chính sách có thể được kết hợp với các loại module khám phá khác nhau vì nó không tham gia vào phép tính khám phá; nó chỉ cung cấp thông tin được yêu cầu bởi module khám phá được chọn bởi người dùng, sau đó tự đưa ra quyết định. Điều này giúp người dùng dễ dàng sử dụng hoặc viết các module khám phá mới mà không cần phải sửa đổi learner chính sách kết quả. Ví dụ Mã 6 cho thấy một PearlAgent được định nghĩa với một module khám phá ϵ-greedy (như một phần của learner chính sách của nó).

--- TRANG 9 ---
Pearl: Một Agent Học Tăng Cường Sẵn Sàng Cho Sản Xuất

```python
agent = PearlAgent(
    policy_learner=QuantileRegressionDeepQLearning(
        state_dim=observation_dim,
        action_space=action_space,
        hidden_dims=[64, 64],
        exploration_module=EGreedyExploration(epsilon=0.05),
        safety_module=QuantileNetworkMeanVarianceSafetyModule(
            variance_weighting_coefficient=0.2,
        ),
        ...
    )
)
```

Ví dụ Mã 6: Một PearlAgent được định nghĩa để sử dụng module an toàn quantile network mean variable (phải được sử dụng cùng với learner chính sách quantile regression deep Q-learning).

A.2 Xử lý Lỗi và Trường hợp Đặc biệt

Pearl kiểm tra đầu vào và đặc tả và tạo ra các ngoại lệ khi chúng không hợp lệ. Các ngoại lệ sau đó có thể được bắt và xử lý theo nhiều cách, chẳng hạn như bằng cách ghi nhật ký một thông điệp hoặc tạo ra một lỗi.

Một ví dụ về kiểm tra lỗi đặc biệt hữu ích bởi Pearl là kiểm tra tương thích được thực hiện bởi PearlAgent trên các module của nó vì một số module chỉ có thể được sử dụng kết hợp với các module khác. Ví dụ, module an toàn quantile network mean variance được thảo luận trong phần A.1 chỉ có thể được sử dụng kết hợp với một learner chính sách phân phối, và nếu người dùng nhầm lẫn sử dụng nó với loại learner chính sách khác, điều này sẽ được chỉ ra ngay khi PearlAgent được khởi tạo.

Các ví dụ khác, trong số nhiều ví dụ, về xử lý lỗi bao gồm kiểm tra tương thích của kích thước batch và kích thước replay buffer, của các loại hành động (rời rạc hoặc liên tục) và thuật toán xử lý một loại hoặc loại khác, và việc sử dụng sai một mạng giá trị (chỉ trạng thái) khi một mạng Q-value (cặp trạng thái-hành động) được yêu cầu.

Phụ lục B. Kết quả Đánh giá

Trong phần này, chúng tôi cung cấp kết quả đánh giá cho Pearl. Chúng tôi thử nghiệm Pearl trong nhiều môi trường vấn đề khác nhau, như vấn đề điều khiển rời rạc và liên tục, học ngoại tuyến, và vấn đề contextual bandit. Để làm điều này, chúng tôi đánh giá Pearl trong một số nhiệm vụ thường được sử dụng từ OpenAI Gym (Brockman et al., 2016) và các phiên bản sửa đổi của một số nhiệm vụ này. Các thí nghiệm của chúng tôi được thực hiện cẩn thận để thử nghiệm kết hợp của tất cả bốn module (học chính sách, khám phá, học an toàn và tóm tắt lịch sử). Trong nhiều trường hợp, chúng tôi tự thiết kế một môi trường đơn giản để nhấn mạnh các thách thức trong RL như khả năng quan sát một phần, sự thưa thớt của phần thưởng, học nhạy cảm với rủi ro, học với ràng buộc, và không gian hành động động. Những điều này giúp làm sáng tỏ động cơ chính của Pearl: tầm quan trọng của việc xem xét các khía cạnh khác nhau của RL (ngoài việc chỉ sử dụng các thuật toán tối ưu hóa chính sách phổ biến) trong việc thiết kế một hệ thống có thể đối phó với các thách thức của ứng dụng thực tế.

Chúng tôi trước tiên trình bày tốc độ và sử dụng bộ nhớ cho Pearl và so sánh nó với một thư viện mã nguồn mở chất lượng cao khác B.1. Sau đó chúng tôi trình bày kết quả đánh giá các phương pháp học chính sách trong Pearl cho các vấn đề RL và contextual bandit trong các phần B.2 và B.3. Phần B.4 thử nghiệm ba module khác trong Pearl, cùng với khả năng của Pearl học trong các vấn đề với không gian hành động động.

B.1 Tốc độ và Sử dụng Bộ nhớ

Trước khi trình bày kết quả đánh giá của chúng tôi, chúng tôi đưa ra một so sánh sơ bộ về tốc độ và sử dụng bộ nhớ của Pearl so với CleanRL (Huang et al., 2022). CleanRL cung cấp các triển khai thuật toán đơn tệp, không modular, tập trung vào các phiên bản đơn giản nhất. Nó không giải quyết các thách thức bổ sung như an toàn và không gian hành động động. Thiết kế đơn giản này cho phép CleanRL, về lý thuyết, đạt được tốc độ nhanh và sử dụng bộ nhớ thấp. Hơn nữa, CleanRL có hiệu suất được ghi lại tốt trên các bộ thử nghiệm khác nhau, thể hiện hiệu quả tương đương với các triển khai RL khác, như được hiển thị trong https://docs.cleanrl.dev/.

Chúng tôi đánh giá thuật toán Soft Actor-Critic trên HalfCheetah-v4, một vấn đề thử nghiệm từ bộ Mujoco với không gian hành động liên tục. Sử dụng các tham số giống hệt nhau cho cả triển khai Pearl và CleanRL, chúng tôi đo thời gian và sử dụng bộ nhớ tại 10.000 bước môi trường. Các thiết lập tham số chi tiết được cung cấp trong Phần B.2.2. Các thí nghiệm được thực hiện trên CPU Intel(R) Xeon(R) Platinum 8339HC (1.80GHz) với GPU Nvidia A100.

Tại mốc 10.000 bước, Pearl yêu cầu 145 giây và tiêu thụ 1.0 GB bộ nhớ, so với CleanRL, mất 123 giây và sử dụng 4.2 GB bộ nhớ. Việc tiêu thụ bộ nhớ cao hơn của CleanRL được gán cho phương pháp khởi tạo replay buffer đầy với một vector zero có kích thước đến dung lượng của buffer. Ngược lại, Pearl cấp phát bộ nhớ động khi cần, lên đến giới hạn buffer. Vào cuối thời gian huấn luyện, khi replay buffer đầy, việc sử dụng bộ nhớ của CleanRL tăng lên 4.4 GB, trong khi việc sử dụng của Pearl tăng lên 5.7 GB. Hiệu suất chậm hơn của Pearl so với CleanRL có thể liên kết với thiết kế modular của Pearl, được thiết kế để giải quyết các thách thức cụ thể như an toàn và không gian hành động động. Ngược lại, CleanRL sử dụng thiết kế đơn giản hơn không giải quyết những vấn đề này.

Việc tăng sử dụng bộ nhớ của Pearl so với CleanRL có thể được gán cho thiết kế replay buffer của Pearl yêu cầu lưu trữ một số thuộc tính khác như available_actions và available_actions_mask cho trạng thái hiện tại và tiếp theo, chi phí liên quan đến một cặp trạng thái-hành động (nếu có) v.v. Thiết kế này cần thiết để cho phép các chức năng khác nhau trong Pearl – ví dụ, xử lý không gian hành động động, học dưới ràng buộc chi phí sử dụng module an toàn ràng buộc phần thưởng v.v. Vì CleanRL chỉ cung cấp các triển khai cụ thể và hạn chế của các thuật toán học chính sách, thiết kế replay buffer của nó đơn giản hơn.

B.2 Đánh giá Học Tăng Cường

B.2.1 Điều khiển rời rạc

Chúng tôi đầu tiên đánh giá các phương pháp điều khiển rời rạc trong Pearl trên Cartpole, một nhiệm vụ học tăng cường cổ điển từ OpenAI Gym. Chúng tôi cung cấp chi tiết về thiết lập thí nghiệm của chúng tôi dưới đây. Trong suốt các thí nghiệm của chúng tôi, chúng tôi đặt hệ số chiết khấu là 0.99.

Thiết lập Thí nghiệm cho Các Phương pháp Điều khiển Rời rạc Dựa trên Giá trị. Chúng tôi thử nghiệm triển khai Pearl của các phương pháp điều khiển rời rạc dựa trên giá trị bao gồm DQN, DoubleDQN, SARSA, DuelingDQN và BootstrappedDQN. Để khám phá, chúng tôi sử dụng module khám phá ϵ-greedy trong Pearl với ϵ = 0.1 và đặt kích thước mini-batch là 32. Tất cả các phương pháp sử dụng optimizer AdamW với tốc độ học 10⁻³, và cập nhật mạng mục tiêu mỗi 10 bước. Kích thước bước của bản cập nhật này là 0.1 cho SARSA và 1.0 cho các phương pháp khác. DQN, DoubleDQN, DuelingDQN, và BootstrappedDQN đều sử dụng replay buffer first-in-first-out (FIFO) của Pearl có kích thước 50.000, và lấy mẫu ngẫu nhiên cho các bản cập nhật mini-batch. SARSA cập nhật các tham số của nó vào cuối mỗi tập sử dụng tất cả các tuple tương tác được thu thập trong tập đó. Về kiến trúc mạng neural, DQN, DoubleDQN, và SARSA sử dụng một mạng neural được kết nối đầy đủ (NN) với hai lớp ẩn có kích thước [64×64] để xấp xỉ hàm Q-value. Đối với Bootstrapped DQN, chúng tôi sử dụng một ensemble của 10 NN như vậy để xấp xỉ phân phối posterior của hàm Q-value.

Thiết lập Thí nghiệm cho Các Phương pháp Điều khiển Rời rạc Dựa trên Chính sách. Chúng tôi cũng thử nghiệm một số phương pháp điều khiển rời rạc dựa trên chính sách của Pearl. Các phương pháp được thử nghiệm là REINFORCE, proximal policy optimization (PPO) và phiên bản điều khiển rời rạc của soft actor critic (SAC). Các mạng chính sách cho cả ba phương pháp được tham số hóa bằng một NN được kết nối đầy đủ với hai lớp ẩn có kích thước [64×64] và được cập nhật bằng optimizer AdamW với tốc độ học 10⁻⁴. Ngoài ra, đối với PPO chúng tôi sử dụng một critic với cùng kiến trúc, một NN được kết nối đầy đủ với hai lớp ẩn có kích thước [64×64]. Đối với Discrete SAC, chúng tôi sử dụng một twin critic, bao gồm hai NN được kết nối đầy đủ, mỗi cái với hai lớp ẩn [64×64]. Đối với cả PPO và Discrete SAC, những critic này được cập nhật bởi optimizer AdamW với tốc độ học 10⁻⁴. Triển khai Discrete SAC của chúng tôi ngoài ra sử dụng một mạng mục tiêu cho critic, được cập nhật bằng soft-updates với kích thước bước 0.005.

Một lần nữa, cả REINFORCE và PPO đều thực hiện cập nhật vào cuối mỗi tập, chỉ sử dụng dữ liệu được thu thập trong tập đó – cho điều này, chúng tôi sử dụng replay buffer theo tập trong Pearl. Discrete SAC, mặt khác, sử dụng cập nhật mini-batch mỗi bước – cho điều này, chúng tôi sử dụng replay buffer FIFO kích thước 50000 với kích thước mini-batch 32. Các ngưỡng cắt trên và dưới cho tỷ lệ lấy mẫu quan trọng trong PPO được chọn là 0.9 và 1.1, tương ứng.

Đối với cả ba phương pháp, chúng tôi sử dụng module khám phá propensity của Pearl để khám phá, về cơ bản có nghĩa là thực hiện các hành động được lấy mẫu từ các chính sách đã học của chúng. Phương pháp Discrete SAC sử dụng điều chỉnh entropy để cải thiện khả năng khám phá của nó – chúng tôi đặt hệ số điều chỉnh entropy trong SAC là 0.1 cho thí nghiệm này.

Kết quả. Chúng tôi vẽ đồ thị các đường cong học của mỗi agent RL như mô tả ở trên trong Hình 2. Ở đây, trục x hiển thị số bước môi trường và trục y hiển thị lợi nhuận theo tập được tính trung bình trong 5000 bước qua khứ. Mỗi thí nghiệm được thực hiện với 5 seed ngẫu nhiên khác nhau. Hình 2 cũng vẽ đồ thị ±1 sai số chuẩn giữa các lần chạy khác nhau. Chúng tôi lưu ý rằng những kết quả này chỉ nhằm phục vụ như một kiểm tra tính hợp lý cho kết hợp các module học chính sách và khám phá thường được tìm thấy trong các triển khai của các thư viện khác – chúng tôi chỉ kiểm tra việc học ổn định, nhất quán của các triển khai của chúng tôi thay vì tìm kiếm các lần chạy huấn luyện tốt nhất với các lựa chọn siêu tham số tối ưu.

--- TRANG 12 ---
Zhu et al.

Hình 2: Lợi nhuận huấn luyện cho các phương pháp điều khiển rời rạc trên nhiệm vụ CartPole. Bảng trái và phải hiển thị lợi nhuận cho các phương pháp dựa trên giá trị và dựa trên chính sách, tương ứng.

B.2.2 Điều khiển liên tục

Chúng tôi cũng đánh giá các triển khai Pearl của ba phương pháp actor-critic phổ biến: Continuous soft-actor critic (SAC), Discrete deterministic policy gradients (DDPG) và Twin delayed deep deterministic policy gradients (TD3), trên bốn nhiệm vụ điều khiển liên tục trong bốn trò chơi MuJoCo (Todorov et al., 2012; Brockman et al., 2016). Đối với tất cả các thí nghiệm, chúng tôi đặt γ = 0.99.

Thiết lập Thí nghiệm cho Các Phương pháp Điều khiển Liên tục. Cả ba phương pháp đều sử dụng một actor và một critic. Chúng tôi tham số hóa cả hai bằng một NN được kết nối đầy đủ với hai lớp ẩn có kích thước [256×256]. Cả mạng actor và critic đều được huấn luyện bằng cập nhật mini-batch có kích thước 256, được rút từ replay buffer FIFO có kích thước 100.000. DDPG và TD3 là các thuật toán với actors xác định. Để khám phá, chúng tôi sử dụng nhiễu Gaussian với độ lệch chuẩn cố định nhỏ được đặt là 0.1. Đối với SAC, chúng tôi đặt hệ số điều chỉnh entropy là 0.25.

Cả ba phương pháp đều sử dụng optimizer AdamW để cập nhật cả actor và critic. Đối với SAC, tốc độ học cho actor và critic là 3×10⁻⁴ và 5×10⁻⁴ tương ứng. Đối với DDPG và TD3, tốc độ học cho actor và critic là 10⁻³ và 3×10⁻⁴ tương ứng. Cả ba phương pháp đều sử dụng mạng mục tiêu cho critic, với tốc độ cập nhật mềm 0.005. Ngoài ra, DDPG và TD3 cũng sử dụng mạng mục tiêu cho các mạng actor với tốc độ cập nhật mềm 0.005.

Kết quả, được tính trung bình trên năm seed ngẫu nhiên khác nhau, được hiển thị dưới đây trong Hình 3. Như trước đây, trục x hiển thị số bước môi trường và trục y hiển thị trung bình chạy của lợi nhuận theo tập trong 5000 bước. Một lần nữa, mục tiêu của chúng tôi ở đây là kiểm tra việc học nhất quán thay vì tìm kiếm các siêu tham số tốt nhất. Các đồ thị trong Hình 3 rõ ràng xác nhận việc triển khai các phương pháp actor-critic này trong Pearl.

Hình 3: Lợi nhuận huấn luyện cho Continuous SAC, DDPG và TD3 trên bốn nhiệm vụ điều khiển liên tục trong MuJoCo.

Các Phương pháp Học Ngoại tuyến. Chúng tôi cũng thử nghiệm Implicit Q-learning (IQL) (Kostrikov et al., 2021), một thuật toán ngoại tuyến được triển khai trong Pearl trên các nhiệm vụ điều khiển liên tục MuJoCo với dữ liệu ngoại tuyến. Thay vì tích hợp với D4RL (Fu et al., 2020) có các phụ thuộc trên các phiên bản cũ hơn của MuJoCo, chúng tôi tạo ra các bộ dữ liệu ngoại tuyến riêng theo giao thức được nêu trong bài báo D4RL. Đối với mỗi môi trường, chúng tôi tạo ra một bộ dữ liệu ngoại tuyến nhỏ gồm 100k chuyển đổi bằng cách huấn luyện một agent RL với soft-actor critic làm learner chính sách và hệ số entropy cao (để khuyến khích khám phá). Các bộ dữ liệu của chúng tôi bao gồm tất cả các chuyển đổi trong replay buffer của agent, tương tự như cách bộ dữ liệu "medium" được tạo ra trong bài báo D4RL. Để huấn luyện, chúng tôi sử dụng cùng các siêu tham số như được đề xuất trong bài báo IQL, thay đổi tốc độ học của critic thành 1×10⁻⁴ và tốc độ cập nhật mềm của mạng critic thành 0.05. Trong Bảng 3 dưới đây, chúng tôi báo cáo các điểm số được chuẩn hóa. Chúng tôi không nhằm mục đích khớp với kết quả của bài báo IQL, vì chúng tôi đã tạo ra các bộ dữ liệu riêng. Tuy nhiên, kết quả trong Bảng 3 phục vụ như một kiểm tra tính hợp lý và xác nhận rằng việc triển khai IQL trong Pearl học được một chính sách hợp lý so với chính sách ngẫu nhiên cơ sở và chính sách chuyên gia.

Bảng 3: Điểm số chuẩn hóa của Implicit Q-learning trên ba môi trường điều khiển liên tục MuJoCo khác nhau.

Môi trường | Lợi nhuận ngẫu nhiên | Lợi nhuận IQL | Lợi nhuận chuyên gia | Điểm số chuẩn hóa
--- | --- | --- | --- | ---
HalfCheetah-v4 | -426.93 | 145.89 | 484.80 | 0.62
Walker2d-v4 | -3.88 | 1225.12 | 2348.07 | 0.52
Hopper-v4 | 109.33 | 1042.03 | 3113.23 | 0.31

Ở đây, "Lợi nhuận ngẫu nhiên" đề cập đến lợi nhuận trung bình của một agent SAC chưa được huấn luyện, "Lợi nhuận IQL" đề cập đến lợi nhuận đánh giá trung bình của agent IQL đã được huấn luyện (lợi nhuận theo tập của agent đã được huấn luyện khi tương tác với môi trường) và "Lợi nhuận chuyên gia" là lợi nhuận theo tập tối đa trong bộ dữ liệu ngoại tuyến. Điểm số chuẩn hóa được tính như tỷ lệ của Lợi nhuận IQL và Lợi nhuận chuyên gia, lấy Lợi nhuận ngẫu nhiên làm cơ sở, như được đề xuất trong Kostrikov et al. (2021).

B.3 Đánh giá Neural Contextual Bandits (CB)

Chúng tôi triển khai và thử nghiệm các điều chỉnh neural của các thuật toán CB phổ biến, bao gồm LinUCB (Li et al., 2010), Thompson Sampling (TS) (Agrawal và Goyal, 2013), và SquareCB (Foster và Rakhlin, 2020). Chúng tôi cung cấp một số chi tiết về các triển khai của chúng tôi. Gọi (x, a) là một cặp (ngữ cảnh, hành động). Các điều chỉnh neural của LinUCB và TS dựa trên việc tính toán một số hạng thưởng khám phá ∥ϕₜ(x, a)∥_{A_t^{-1/2}} trong đó ϕₜ(x, a) là biểu diễn đặc trưng của (x, a) tại bước học t và A_t = I + Σ_{n=1}^{t-1} ϕₜ(x, a)ϕₜ(x, a)ᵀ là ma trận hiệp phương sai đặc trưng.

Đối với LinUCB, chúng tôi rõ ràng thêm số hạng β × ∥ϕₜ(x, a)∥_{A_t^{-1/2}} làm thưởng khám phá cho phần thưởng dự đoán cho mỗi (x, a). Đối với neural TS, cho mỗi (x, a), chúng tôi lấy mẫu phần thưởng từ phân phối Gaussian, với trung bình bằng dự đoán mô hình phần thưởng và phương sai bằng ∥ϕₜ(x, a)∥_{A_t^{-1/2}}. Đối với phương pháp SquareCB, chúng tôi tuân theo cùng quy tắc lựa chọn hành động như được mô tả trong Foster và Rakhlin (2020). Chúng tôi đặt tham số tỷ lệ là γ = 10√(dT) trong đó d là chiều của vector đầu vào đặc trưng-hành động, ϕ(x, a). Đối với cả ba thuật toán, các hành động được biểu diễn bằng mã hóa nhị phân. Do đó, chiều của vector đầu vào đặc trưng-hành động là d_x + log₂(A), trong đó d_x là chiều của đặc trưng ngữ cảnh ϕ(x) và A là số lượng hành động. Lưu ý rằng Foster và Rakhlin (2020) khuyến nghị đặt γ ∝ √(dT) cho vấn đề bandit tuyến tính. Chúng tôi chia tỷ lệ này cho 10 vì chúng tôi quan sát được hiệu suất cải thiện một cách thực nghiệm trong nghiên cứu ablation của chúng tôi.

Để thiết kế các thí nghiệm của chúng tôi, chúng tôi theo một cách phổ biến sử dụng các bộ dữ liệu học có giám sát để xây dựng các bộ dữ liệu ngoại tuyến cho các môi trường CB (Dudík et al., 2011; Foster et al., 2018; Bietti et al., 2021). Chúng tôi sử dụng kho lưu trữ UCI (Kelly et al.) để chọn các bộ dữ liệu học có giám sát. Mỗi bộ dữ liệu này là một tập hợp các đặc trưng x và các nhãn thực tương ứng y. Chúng tôi thực hiện phép biến đổi sau để xây dựng các bộ dữ liệu CB ngoại tuyến. Cho một cặp (x, y) được rút từ một bộ dữ liệu, chúng tôi đặt hàm phần thưởng là r(x, a) = 1{a=y} + ϵ, trong đó ϵ ~ N(0, σ). Nghĩa là, agent nhận được phần thưởng kỳ vọng là 1 khi nó chọn nhãn đúng a = y, và nhận được phần thưởng kỳ vọng là 0 trong trường hợp ngược lại.

Bên cạnh việc thử nghiệm các thuật toán contextual bandit khác nhau, chúng tôi triển khai một learner ngoại tuyến làm cơ sở (xem nhãn "Offline" trong Hình 4). Learner ngoại tuyến chỉ có quyền truy cập vào một bộ dữ liệu ngoại tuyến đa dạng, sử dụng để ước lượng mô hình phần thưởng ṝ(x, a). Trong Hình 4, chúng tôi hiển thị hiệu suất của một chính sách tham lam đối với mô hình phần thưởng, tức là π(x) ∈ arg max_a ṝ(x, a).

Các đồ thị được trình bày trong Hình 4 hiển thị các đường cong học trung bình trên 5 lần chạy cho mỗi trong bốn bộ dữ liệu, letter, yeast, satimage và pendigits. Các khoảng tin cậy đại diện cho một sai số chuẩn. Những đường cong này cho thấy rằng tất cả các thuật toán được thử nghiệm (UCB, TS và Square CB) đều khớp với hiệu suất của cơ sở. Điều này xác nhận việc triển khai các phương pháp khám phá thông minh này để học chính sách trong các vấn đề contextual bandit.

Hình 4: Hiệu suất của learner chính sách contextual bandit neural với các module khám phá dựa trên Linear UCB, Thompson Sampling và SquareCB trong Pearl trên bốn bộ dữ liệu UCI. Chúng tôi cũng thêm một cơ sở ngoại tuyến được coi là gần tối ưu.

Thiết lập Thí nghiệm cho Các Thuật toán Contextual Bandit. Đối với các bộ dữ liệu letter, satimage và pendigits, các thí nghiệm của chúng tôi sử dụng một MLP với hai lớp ẩn có kích thước 64×16. Đối với bộ dữ liệu yeast, các thí nghiệm của chúng tôi sử dụng một MLP với hai lớp ẩn có kích thước 32×16. Chúng tôi sử dụng optimizer Adam với tốc độ học 0.01 và sử dụng kích thước batch 128. Để biểu diễn hành động, chúng tôi sử dụng mã hóa nhị phân của không gian hành động, tức là chúng tôi biểu diễn mỗi hành động như biểu diễn nhị phân của nó cần thiết để biểu diễn tất cả các hành động.

B.4 Đánh giá Tính Linh hoạt

Phần này cung cấp chi tiết về một số thí nghiệm đánh giá mà chúng tôi đã chạy để thử nghiệm các tính năng khác nhau trong Pearl, cụ thể là a) học trong các môi trường quan sát một phần bằng cách tóm tắt lịch sử, b) khám phá hiệu quả và hiệu quả trong các môi trường phần thưởng thưa thớt, c) học các chính sách tránh rủi ro, d) học với các ràng buộc về chi phí dài hạn, và e) học trong các môi trường vấn đề với không gian hành động động. Chúng tôi không thực hiện tìm kiếm siêu tham số để tối ưu hóa hiệu suất và các thí nghiệm chỉ nhằm chứng minh rằng Pearl có thể được sử dụng hiệu quả để học một chính sách tốt trong các kịch bản thách thức khác nhau.

Tóm tắt lịch sử cho các vấn đề với trạng thái quan sát một phần. Để thử nghiệm module tóm tắt lịch sử trong Pearl, chúng tôi điều chỉnh CartPole, một môi trường điều khiển cổ điển trong OpenAI Gym, để thiết kế một biến thể nơi trạng thái có thể quan sát một phần. Mục tiêu vẫn giống nhau — giữ một cột thẳng đứng bằng cách di chuyển một xe được liên kết với cột. Trong môi trường CartPole gốc, agent có thể cảm nhận trạng thái cơ bản thực sự — góc và vận tốc góc của cột, cũng như vị trí và vận tốc của xe. Trong môi trường chúng tôi thiết kế, chỉ góc của cột và vị trí của xe có thể quan sát được. Do đó, một agent RL phải sử dụng các quan sát hiện tại và quá khứ để suy ra các vận tốc, vì cả vận tốc góc của cột và vận tốc của xe đều quan trọng để chọn hành động tối ưu. Để tăng thêm mức độ quan sát một phần, môi trường mới được thiết kế để phát ra quan sát (góc của cột và vị trí của xe) sau mỗi 2 tương tác, thay vì mỗi tương tác. Đối với các tương tác khác, môi trường phát ra một vector không.

Chúng tôi đánh giá hai agent RL trên môi trường mới này—một có và một không có module tóm tắt lịch sử LSTM. Cả hai đều sử dụng DQN làm learner chính sách cùng với module khám phá ϵ-greedy với ϵ = 0.1. Các siêu tham số khác được lấy giống như được mô tả trong Phần B.2. Chúng tôi sử dụng một LSTM với 2 lớp tái phát, mỗi lớp có 128 đơn vị ẩn (tức là chiều của trạng thái ẩn được lấy là 128). Độ dài lịch sử được chọn là 4. Hình 5a vẽ đồ thị trung bình và độ lệch chuẩn của lợi nhuận theo tập trên năm lần chạy. Như có thể thấy rõ ràng, agent sử dụng module tóm tắt lịch sử dựa trên LSTM học để đạt được lợi nhuận cao nhanh chóng trong khi agent khác không học được. Ví dụ đơn giản này minh họa cách một agent RL trong Pearl có thể được trang bị khả năng sử dụng thông tin lịch sử để học trong một môi trường quan sát một phần.

Hình 5: Kết quả Đánh giá Tính Linh hoạt của Agent: đường cong học cho (a) DQN, có và không có LSTM, trong môi trường CartPole quan sát một phần, (b) DQN và Bootstrapped DQN trong môi trường Deep Sea 10×10, và (c) QR-DQN với module an toàn nhạy cảm với rủi ro mean variance. Dưới mức độ tránh rủi ro cao, chính sách đã học có lợi nhuận kỳ vọng nhỏ hơn nhưng cũng có phương sai nhỏ hơn của phân phối lợi nhuận.

Khám phá hiệu quả cho các vấn đề phần thưởng thưa thớt. Trong phần B.3, chúng tôi đánh giá các thuật toán khám phá thông minh trong Pearl được sử dụng để học bandit. Ở đây chúng tôi thử nghiệm thuật toán Bootstrapped DQN (Osband et al., 2016) được triển khai trong Pearl, cho phép khám phá thông minh trong các môi trường RL. Để làm điều này, chúng tôi sử dụng môi trường DeepSea (Osband et al., 2019). DeepSea là một môi trường phần thưởng thưa thớt và nổi tiếng là thách thức cho khám phá. Nó có n×n trạng thái và hoàn toàn xác định. Đối với các thí nghiệm của chúng tôi, chúng tôi chọn n = 10, cho đó cơ hội đạt được trạng thái mục tiêu dưới một chính sách ngẫu nhiên là 2⁻¹⁰.

Chúng tôi thử nghiệm hai agent RL – một với learner chính sách DQN và khám phá ϵ-greedy (ϵ = 0.1) và cái khác với Bootstapped DQN. Các siêu tham số cho DQN và Bootstrapped DQN được lấy giống như trong Phần B.2. Đối với mỗi thí nghiệm, chúng tôi thực hiện 5 lần chạy, mỗi lần với 10⁶ bước. Hình 5b hiển thị các đường cong học của hai agent. Có thể thấy rằng Bootstrapped DQN nhanh chóng học được một chính sách tối ưu trong khi agent khác (DQN với khám phá ϵ-greedy) không học được. Việc tích hợp các thuật toán khám phá thông minh trong Pearl thực sự có thể giúp người dùng thử nghiệm các agent RL trong các môi trường vấn đề thực tế với các thách thức khám phá.

Học chính sách nhạy cảm với rủi ro. Để thử nghiệm khả năng của Pearl học một chính sách dưới các thước đo khác nhau về độ nhạy cảm với rủi ro, chúng tôi thiết kế một môi trường đơn giản gọi là Mean-Variance Bandit. Môi trường này chỉ có một trạng thái và hai hành động. Phần thưởng của mỗi trong hai hành động tuân theo phân phối Gaussian. Phân phối phần thưởng cho hành động 1 có trung bình là 6 và phương sai là 1, trong khi cho hành động 2, trung bình là 10 và phương sai là 9. Dưới mục tiêu tối đa hóa phần thưởng cổ điển, nơi mục tiêu là tối đa hóa lợi nhuận kỳ vọng, dễ thấy rằng chính sách tối ưu cho môi trường này sẽ luôn chọn hành động 2. Tuy nhiên với việc học nhạy cảm với rủi ro, nhà thiết kế hệ thống có thể chọn học một chính sách bằng cách tối ưu hóa cho một mục tiêu khác (khác với việc tối đa hóa lợi nhuận kỳ vọng).

RL nhạy cảm với rủi ro chỉ hoạt động với các learner phân phối, như Quantile regression DQN (QR-DQN) (Dabney et al., 2018b) hoặc implicit quantile networks (IQN) (Dabney et al., 2018a). Ý tưởng cơ bản là học một phân phối trên phần thưởng tích lũy cho mỗi cặp trạng thái hành động (ký hiệu Z(s, a)), thay vì học kỳ vọng của chúng, Q(s, a). Chính thức, đối với một chính sách π cho trước, Z và Q được định nghĩa như sau.

Z^π(s, a) = [Σ_{t=0}^∞ γ^t r(s_t, a_t) | s_0 = s, a_0 = a, a_t ~ π(·|s_t)], Q^π(s, a) = E[Z^π(s, a)].

Lưu ý ở đây rằng Z^π(s, a) là một biến ngẫu nhiên. Sau đó, người ta có thể sử dụng một hàm thước đo rủi ro (Ma et al., 2020; Dabney et al., 2018a) để ánh xạ phân phối này thành các giá trị Q. Trong Pearl, chúng tôi triển khai một hàm thước đo rủi ro tính toán các giá trị Q (Q^π(s, a) = E[Z^π(s, a)]); và một hàm thước đo rủi ro mean-variance tính toán trung bình có trọng số của trung bình và phương sai của Z^π, E[Z^π(s, a)] - β·Var(Z^π(s, a)), trong đó Var(Z^π(s, a)) ký hiệu phương sai của phân phối lợi nhuận và β là một tham số chỉ định mức độ nhạy cảm với rủi ro.

Đối với môi trường Mean-Variance Bandit, chúng tôi thử nghiệm mean_variance_safety_module trong Pearl, sử dụng hàm thước đo rủi ro mean-variance để tính toán các giá trị Q như được hiển thị ở trên. Đối với một giá trị β khác không, một chính sách tốt sẽ tối đa hóa lợi nhuận kỳ vọng trong khi tối thiểu hóa các lần thăm các cặp trạng thái hành động với phương sai cao của phân phối lợi nhuận; và β chỉ định sự cân bằng chính xác giữa hai mục tiêu này. Như có thể dễ dàng thấy, chính sách tối ưu trong Mean-Variance Bandit thay đổi với giá trị của β. Đối với bất kỳ β > 0.5, chính sách tối ưu luôn chọn hành động 1 (hành động với trung bình và phương sai lợi nhuận thấp hơn) trong khi với β < 0.5, luôn chọn hành động 2 (hành động với trung bình và phương sai lợi nhuận cao hơn) là tối ưu.

Đối với thí nghiệm này, chúng tôi sử dụng triển khai Pearl của thuật toán QR-DQN để học phân phối trên phần thưởng tích lũy, và gắn mean_variance_safety_module làm module an toàn để học một chính sách nhạy cảm với rủi ro. Mạng quantile được sử dụng được tham số hóa bằng 10 quantiles và hai lớp ẩn có kích thước 64. Chúng tôi sử dụng optimizer AdamW với tốc độ học 5×10⁻⁴, kích thước batch 32, và replay buffer có kích thước 50.000. Chúng tôi thử nghiệm ba lựa chọn của β, β ∈ {0, 0.1, 1}). Hình 5c hiển thị các đường cong học cho 5000 bước huấn luyện với các giá trị β khác nhau, được tính trung bình trên 5 lần chạy. Chúng tôi cũng hiển thị đường cong học cho DQN đơn giản. Như có thể thấy rõ ràng, đối với một giá trị β cao (tương ứng với mức độ tránh rủi ro cao), chính sách tối ưu đã học chọn hành động 1, với lợi nhuận trung bình là 6. DQN và QR-DQN với các giá trị β nhỏ hơn hội tụ đến chính sách tối ưu, chọn hành động 2, với phần thưởng trung bình là 10.

Trong khi thiết lập môi trường ở đây đơn giản, nó phục vụ như một kiểm tra tính hợp lý cho việc triển khai risk_sensitive_safety_module và learner chính sách phân phối trong Pearl. Nó cũng minh họa cách người dùng có thể sử dụng các thước đo rủi ro khác nhau để chỉ định mức độ tránh rủi ro trong quá trình học chính sách. Hiện tại, chúng tôi chỉ triển khai hàm thước đo rủi ro mean-variance và thuật toán QR-DQN trong Pearl. Chúng tôi dự định thêm nhiều hơn trong bản phát hành tiếp theo, ví dụ, các triển khai của IQN (Dabney et al., 2018a) và Distributional SAC (Ma et al., 2020), cùng với các thước đo khác nhau của kỳ vọng bị biến dạng (Ma et al., 2020).

Học với các ràng buộc chi phí. Nhiều vấn đề thực tế yêu cầu một agent học một chính sách tối ưu tuân theo (các) ràng buộc chi phí. Hơn nữa, đối với các vấn đề nơi tín hiệu phần thưởng không được định nghĩa rõ, thường hữu ích khi chỉ định hành vi mong muốn dưới dạng các ràng buộc. Ví dụ, giới hạn tiêu thụ năng lượng của động cơ có thể là một ràng buộc mong muốn để học di chuyển robot.

Tối ưu hóa phần thưởng kỳ vọng dài hạn tuân theo (các) ràng buộc yêu cầu sửa đổi quy trình học. Môi trường này thường được hình thức hóa như một MDP có ràng buộc. Pearl cho phép học trong MDPs có ràng buộc thông qua reward_constrained_safety_module triển khai thuật toán Reward Constrained Policy Optimization của Tessler et al. (2018).

Để thử nghiệm việc triển khai của chúng tôi, chúng tôi sửa đổi một môi trường gym với hàm chi phí theo bước, c(s, a), ngoài phần thưởng theo bước tiêu chuẩn r(s, a). Chúng tôi chọn chi phí theo bước là c(s, a) = ||a||₂² trong đó a ∈ ℝᵈ, xấp xỉ năng lượng tiêu tốn khi thực hiện hành động a trong trạng thái s. Hình 6 hiển thị kết quả của các thuật toán actor-critic khác nhau với ràng buộc về chi phí tích lũy chiết khấu kỳ vọng (được chuẩn hóa). Cụ thể, đối với các giá trị khác nhau của ngưỡng α, chúng tôi thêm một ràng buộc

(1-γ)E_{s_t~η^π,a_t~π}[Σ_{t=0}^∞ γᵗc(s_t, a_t)|s_0=s, a_0=a] ≤ α.

Chúng tôi thử nghiệm với các điều chỉnh ràng buộc phần thưởng của các learner chính sách actor-critic khác nhau như TD3, DDPG và Continuous SAC (CSAC), bằng cách khởi tạo các agent RL tương ứng trong Pearl và chỉ định module an toàn là reward_constrained_safety_module. Những cái này được gọi là reward constrained TD3 (RCTD3), RCDDPG và RCCSAC tương ứng. Các giá trị siêu tham số cho các learner chính sách được chọn giống như được chỉ định ở trên trong phần B.2. reward_constrained_safety_module chỉ có một siêu tham số, α. Chúng tôi thử nghiệm với các giá trị α khác nhau như được hiển thị trong Hình 6.

Hình 6 hiển thị phần thưởng tích lũy theo tập và chi phí cho bốn nhiệm vụ điều khiển liên tục MuJoCo. Như có thể thấy, chi phí tích lũy nhỏ hơn cho một giá trị ngưỡng α nhỏ hơn. Thú vị, các giá trị α vừa phải trong các môi trường khác nhau không dẫn đến suy giảm hiệu suất đáng kể, mặc dù có thêm ràng buộc chi phí—thực tế, đối với một số môi trường, ràng buộc chi phí dẫn đến các chính sách với lợi nhuận theo tập cải thiện một chút (ví dụ, RCDDPG trong Half-Cheetah).

Thích ứng với không gian hành động động. Trong nhiều vấn đề, các agent được yêu cầu học trong các môi trường nơi không gian hành động thay đổi tại mỗi bước thời gian. Các nhà thực hành thường gặp điều này trong các hệ thống gợi ý, nơi đối với mỗi tương tác người dùng, agent gặp một tập hợp nội dung riêng biệt (để chọn từ đó) để gợi ý. Để đánh giá khả năng thích ứng của Pearl với không gian hành động động, chúng tôi tạo ra hai môi trường dựa trên CartPole và Acrobot. Trong những môi trường này, sau mỗi bốn bước, agent mất quyền truy cập vào hành động 1 trong CartPole và hành động 2 trong Acrobot, tương ứng.

Hình 7 hiển thị các đường cong học cho DQN, SAC, PPO, và REINFORCE trong những môi trường chuyên biệt này. Đối với những thí nghiệm này, chúng tôi sử dụng cùng bộ siêu tham số như trong Phần B.2. Mặc dù độ phức tạp tăng lên do không gian hành động động, hầu hết các agent đã học thành công các chính sách hiệu quả sau 100.000 bước, ngoại trừ REINFORCE luôn hoạt động kém hơn so với các thuật toán khác. Những thí nghiệm đơn giản này minh họa rằng Pearl có thể được sử dụng một cách liền mạch để học trong các môi trường với không gian hành động động.

Hình 6: Đồ thị chi phí theo tập (trên) và lợi nhuận theo tập (dưới) trong quá trình huấn luyện trên các nhiệm vụ điều khiển liên tục với phản hồi chi phí và phần thưởng. Các đồ thị trình bày hiệu suất của TD3, DDPG, và CSAC và các điều chỉnh ràng buộc chi phí của chúng tôi RCTD3, RCDDPG, và RCCSAC cho nhiều giá trị của ngưỡng ràng buộc α. Xem văn bản để biết chi tiết.

Phụ lục C. Chi tiết Bổ sung Về Kiến trúc Pearl

C.1 Hỗ trợ cho Cả Học Tăng Cường Ngoại tuyến và Trực tuyến

Như đã đề cập trong phần 2, Pearl hỗ trợ cả thuật toán học chính sách ngoại tuyến và trực tuyến. Các thuật toán học ngoại tuyến, ví dụ Conservative Q-learning (CQL) (Kumar et al., 2020) và Implicit Q-learning (IQL) (Kostrikov et al., 2021), được thiết kế để học từ dữ liệu ngoại tuyến và tránh các lỗi ngoại suy bằng cách thêm một số hình thức bảo thủ khi học hàm giá trị. Trong Pearl, cả thuật toán học chính sách ngoại tuyến và trực tuyến đều được triển khai như các learner chính sách khác nhau.

Đáng chú ý rằng thiết kế modular của Pearl cho phép người dùng kết hợp học ngoại tuyến và trực tuyến trong pipeline huấn luyện. Một cách đơn giản để triển khai quá trình chuyển đổi RL ngoại tuyến-đến-trực tuyến như vậy là huấn luyện một agent Pearl sử dụng dữ liệu ngoại tuyến và thuật toán học chính sách ngoại tuyến (chẳng hạn, CQL) và sau đó sử dụng chính sách và hàm giá trị đã học (mạng chính sách và mạng hàm giá trị) để khởi tạo một agent Pearl trực tuyến. Theo cách này, người dùng có thể hiệu quả tận dụng dữ liệu ngoại tuyến để khởi động nóng việc học trực tuyến.

Bên cạnh cách đơn giản được đề cập ở trên, có văn hệ gần đây đề xuất các ý tưởng mới để thực hiện quá trình chuyển đổi RL ngoại tuyến đến trực tuyến, xem (Yu và Zhang, 2023; Lee et al., 2022; Nakamoto et al., 2024). Tuy nhiên, đây là một lĩnh vực nghiên cứu tích cực. Như một phần của công việc đang tiến hành của chúng tôi, chúng tôi dự định tích hợp một số ý tưởng này vào Pearl. Chúng tôi nghĩ điều này sẽ dễ thực hiện với thiết kế modular của Pearl.

C.2 Hỗ trợ cho Khám phá Thông minh Ngoài Dithering

Thiết kế module khám phá của chúng tôi trong Pearl cùng với việc tích hợp chặt chẽ với module learner chính sách đã cung cấp hỗ trợ cho các chiến lược khám phá thông minh ngoài dithering. Ví dụ, Bootstrapped DQN (Osband et al., 2016) được triển khai trong tệp Github này. Chúng tôi thiết kế module learner chính sách và module khám phá để hoạt động chặt chẽ với nhau - module learner chính sách trong Pearl tính toán các đại lượng cần thiết, như hàm giá trị, biểu diễn và phân phối chính sách. Một hành động khai thác có thể được thực hiện nếu agent không khám phá. Đối với trường hợp agent muốn khám phá, module khám phá có thể sử dụng các đại lượng được tính toán bởi learner chính sách để xác định hành động khám phá. Ví dụ, trong trường hợp Bootstrapped DQN, learner chính sách học một phân phối posterior xấp xỉ trên ước lượng của hàm giá trị tối ưu bằng bootstrapping và module khám phá triển khai khám phá sâu (Thompson sampling) bằng cách lấy mẫu từ nó để thực hiện hành động trong mỗi tập.

C.3 Hỗ trợ cho Không gian Hành động Động

Chúng tôi muốn lưu ý rằng Pearl là thư viện RL mã nguồn mở đầu tiên cung cấp hỗ trợ về không gian hành động động cho cả phương pháp dựa trên giá trị và actor-critic.

Trước đây, chỉ ReAgent (Gauci et al., 2018) cung cấp hỗ trợ hạn chế cho không gian hành động động chỉ với các phương pháp dựa trên giá trị (Deep Q-learning). Chúng tôi lưu ý rằng việc triển khai không gian hành động động trong ReAgent có những khuyết điểm – các hành động có sẵn trong mỗi trạng thái chỉ được tính đến tại thời điểm suy luận (tức là không gian hành động động chỉ được sử dụng trong quá trình tương tác agent-môi trường thay vì được sử dụng trong cả huấn luyện và suy luận). Ngoài ra, ReAgent không hỗ trợ không gian hành động động với các phương pháp gradient chính sách hoặc actor-critic. Pearl cung cấp hỗ trợ cho không gian hành động động thông qua các đổi mới cho cả phương pháp dựa trên giá trị và actor-critic.

C.3.1 Hỗ trợ Không gian Hành động Động cho Các Phương pháp Học Dựa trên Giá trị

Ý tưởng chính để hỗ trợ không gian hành động động cho các phương pháp dựa trên giá trị là tính toán mục tiêu tối ưu hóa trong phương trình Bellman dựa trên các giá trị Q tối đa trên không gian hành động của trạng thái tiếp theo. Về mặt toán học, gọi (S_t, A_t, r(S_t, A_t), S_{t+1}) là một tuple chuyển đổi. Sau đó, hàm giá trị của chính sách tối ưu có thể được ước lượng bằng cách tối thiểu hóa lặp đi lặp lại lỗi giữa vế trái và vế phải của phương trình Bellman sau,

Q_θ(S_t, A_t) = r(S_t, A_t) + γ max_{a∈A_{S_{t+1}}} Q_θ(S_{t+1}, a),

trong đó chúng tôi ký hiệu A_{S_{t+1}} là không gian hành động liên quan đến trạng thái S_{t+1}. Để đạt được điều này, chúng tôi thiết kế một replay buffer để lưu trữ hai thuộc tính bổ sung cho mỗi trạng thái, available_actions và available_actions_mask. Vui lòng tham khảo Hình 8 hiển thị việc thêm available_actions và available_actions_mask cho trạng thái hiện tại và tiếp theo trong một tuple chuyển đổi.

Hình 8: Thiết kế RL Dựa trên Giá trị với Không gian Hành động Động

Thuộc tính available_actions là một tensor biểu diễn các hành động có sẵn. Ở đây, chúng tôi sử dụng zero padding cho các hành động không có sẵn. available_actions_mask đơn giản là một tensor với mask cho các hành động không có sẵn. Để tính toán các mục tiêu tối ưu hóa (vế phải của phương trình Bellman), max_{a∈A_{S_{t+1}}} Q_θ(S_{t+1}, a), trước tiên chúng tôi tính toán Q_θ(S_{t+1}, a) cho tất cả hành động a (bao gồm các hành động không có sẵn sử dụng biểu diễn zero padded). Sau đó chúng tôi sử dụng available_actions_mask để đặt Q-values của các hành động không có sẵn thành -∞ để đảm bảo rằng những cái này không đóng góp vào cập nhật gradient khi tối thiểu hóa lỗi Bellman.

C.3.2 Hỗ trợ Không gian Hành động Động (Rời rạc) cho Các Phương pháp Actor-Critic

Đối với các phương pháp gradient chính sách và actor-critic, Pearl chỉ hỗ trợ không gian hành động rời rạc động, theo công trình của Chen et al. (2022). Dường như không có văn hệ về tối ưu hóa chính sách với không gian hành động liên tục thay đổi với các trạng thái khác nhau.

Để cho phép học chính sách sử dụng các phương pháp actor-critic trong các vấn đề với không gian hành động động, chúng tôi triển khai một kiến trúc mạng neural chính sách được gọi là dynamic actor network, như được minh họa trong Hình 9, nơi xác suất hành động được tính toán như

π_ϕ(a|s) = f_ϕ(s, a) / Σ_{a'∈A_s} f_ϕ(s, a').

Ở đây, chúng tôi ký hiệu π_ϕ là dynamic actor network xuất ra xác suất hành động cho tất cả hành động trong không gian hành động A_s (không gian hành động phụ thuộc trạng thái) và gọi f_ϕ ký hiệu lớp gần cuối của mạng actor. Xác suất hành động được tính toán bằng cách áp dụng softmax cho lớp cuối, f_ϕ. Với điều này, chúng ta có thể viết phương trình Bellman cho việc học critic như

Q_θ(S_t, A_t) = r(S_t, A_t) + γE_{a~π_ϕ}[Q_θ(S_{t+1}, a)],

và có thể sử dụng critic đã học, Q_θ, để cập nhật mạng chính sách sử dụng các thuật toán gradient chính sách hoặc actor-critic.

Đáng chú ý rằng việc sử dụng dynamic actor networks là quan trọng đối với việc triển khai của chúng tôi. Đối với việc học critic, chúng tôi tính toán E_{a~π_ϕ}[Q_θ(S_{t+1}, a)] bằng cách tận dụng dynamic actor network π_ϕ; ở đây việc sử dụng π_ϕ đảm bảo rằng phân phối chính sách chỉ bao phủ các hành động trong không gian hành động có sẵn khi tính toán kỳ vọng trên hàm giá trị của trạng thái tiếp theo. Tương tự, chúng tôi cũng sử dụng dynamic actor network π_ϕ cho các cập nhật học chính sách. Cụ thể, trong việc tính toán gradient chính sách, chúng tôi chỉ sử dụng giá trị critic cho các hành động trong tập hành động có sẵn của một trạng thái. Vui lòng tham khảo Hình 10 để minh họa các thay đổi chúng tôi thực hiện đối với các phương pháp actor-critic để học với không gian hành động động.

Hình 9: Dynamic Actor Network cho Chính sách Hành động Động

Hình 10: Thiết kế Actor-Critic với Không gian Hành động Động

Phụ lục D. Chi tiết Bổ sung cho Các Ví dụ Áp dụng Ngành công nghiệp

Trong phần này, chúng tôi cung cấp thiết lập chi tiết của các ví dụ chúng tôi trình bày trong Phần 4.

1. Hệ thống gợi ý dựa trên đấu giá (Xu et al., 2023):
   (a) Thiết lập MDP: Chúng tôi sử dụng RL cho một MDP ngữ cảnh nơi mỗi người dùng đại diện cho một ngữ cảnh và mục tiêu là tối ưu hóa phản hồi tích cực tích lũy trong suốt các tương tác của người dùng với hệ thống gợi ý.
   (b) Quan sát: Biểu diễn thời gian thực của người dùng được cung cấp bởi hệ thống tóm tắt các tương tác lịch sử từ người dùng và các đặc trưng bảo vệ quyền riêng tư thời gian thực của họ. Chúng tôi quan sát phản hồi từ người dùng sau khi một gợi ý được đưa ra cho người dùng. Module tóm tắt lịch sử của Pearl cũng có thể được tận dụng nếu biểu diễn chuỗi thời gian người dùng không được cung cấp bởi hệ thống.
   (c) Hành động: Mỗi phần nội dung có sẵn để gợi ý là một hành động và hệ thống cung cấp mỗi gợi ý cho agent như một biểu diễn tóm tắt nội dung và kết quả tương tác lịch sử của nội dung. Vì tại mỗi bước, có một tập hợp gợi ý khác nhau, không gian hành động động cần được hỗ trợ cho một agent RL để giải quyết vấn đề này.
   (d) Phần thưởng: Phản hồi của người dùng đối với gợi ý chúng tôi đưa ra.
   (e) Learner chính sách: Một thách thức chính đối với hệ thống gợi ý dựa trên đấu giá là chính sách cuối cùng được triển khai bị ảnh hưởng mạnh bởi hệ thống đấu giá. Trong Xu et al. (2023), chúng tôi áp dụng Deep SARSA làm learner chính sách để thực hiện cải thiện chính sách trên chính sách sao cho chúng tôi có thể tính đến tác động của hệ thống đấu giá lên chính sách và chúng tôi hiện đang cải thiện thêm kết quả với các thuật toán học ngoại tuyến off-policy như CQL. Chúng tôi tận dụng không gian hành động động dựa trên giá trị, được trình bày trong Phần C.3.1 cho huấn luyện và suy luận.

2. Đấu thầu đấu giá quảng cáo (Korenkevych et al., 2023):
   (a) Thiết lập MDP: RL được tận dụng để tuần tự đặt đề xuất trong một hệ thống đấu giá quảng cáo sao cho agent có thể tối đa hóa chuyển đổi tích lũy trong một chiến dịch quảng cáo.
   (b) Quan sát: Chúng tôi quan sát ngân sách đã tiêu thụ, thống kê dự báo thị trường thô và kết quả đấu thầu đấu giá.
   (c) Hành động: Lượng ngân sách agent đặt vào đấu giá quảng cáo.
   (d) Phần thưởng: Kết quả quảng cáo dựa trên đấu giá thắng cuộc, nếu không thì 0.
   (e) Learner chính sách: Chúng tôi áp dụng TD3BC như một thuật toán RL ngoại tuyến để học từ dữ liệu ngoại tuyến được thu thập từ các hệ thống đấu thầu sản xuất.
   (f) Tóm tắt lịch sử: Để giảm khả năng quan sát một phần của thị trường đấu giá, chúng tôi chồng một số quan sát quá khứ để cung cấp thông tin chi tiết có ý nghĩa hơn vào việc ra quyết định hiện tại.
   (g) Khám phá trực tuyến: Agent RL tận dụng chính sách khám phá phân phối Gaussian với cắt để cung cấp hỗ trợ tốt hơn trong việc học chính sách ngoại tuyến.
   (h) Học an toàn: Các nỗ lực hiện tại của chúng tôi về đấu thầu đấu giá bao gồm tối ưu hóa chính sách có ràng buộc phần thưởng (RCPO) kết hợp với TD3 ngăn agent RL thực hiện các hành động không an toàn.

3. Lựa chọn sáng tạo:
   (a) Thiết lập Contextual bandit: Mỗi ngữ cảnh trong vấn đề lựa chọn sáng tạo là một phần nội dung được giao.
   (b) Quan sát và phần thưởng: Trong lựa chọn sáng tạo, phần thưởng và quan sát giống nhau, đại diện cho phản hồi của người dùng sau khi thấy phần nội dung.
   (c) Hành động: Mỗi hành động là một định dạng tiềm năng của nội dung được giao.
   (d) Học chính sách + Khám phá: chúng tôi sử dụng Neural-LinUCB cho cả việc học kỳ vọng của phần thưởng cũng như khám phá trực tuyến thông qua nguyên tắc "lạc quan đối mặt với bất định".

Phụ lục E. Hạn chế và Công việc Tương lai

Khi so sánh với các thư viện khác, có ba lĩnh vực mà các thư viện khác có thể cung cấp hỗ trợ tốt hơn: (1) RL đa agent, (2) RL dựa trên mô hình và (3) abstractions cấp thấp.

1. RL đa agent: Đối với các trường hợp sử dụng sản xuất của chúng tôi, chúng tôi chưa gặp phải nhu cầu đáng kể để sử dụng các thuật toán RL đa agent, vì vậy bản phát hành đầu tiên của Pearl tập trung vào môi trường đơn agent. Tuy nhiên, chúng tôi tin rằng khi phương pháp RL trưởng thành trong ngành công nghiệp, cuối cùng sẽ có nhu cầu về một thư viện RL đa agent tập trung vào sản xuất (một ví dụ về một lĩnh vực yêu cầu xem xét đa agent là môi trường đấu giá quảng cáo, nơi nhiều người đấu thầu tham gia vào một đấu giá duy nhất). Tại thời điểm đó, chúng tôi tin rằng thiết kế tập trung vào agent của Pearl có thể là một điểm khởi đầu tuyệt vời để hỗ trợ các thuật toán như vậy, và do đó, đây là một hướng tương lai quan trọng tiềm năng của Pearl. Hiện tại, có một số thư viện chuyên biệt tuyệt vời cho môi trường đa agent (ví dụ, Mava (de Kock et al., 2021), MARLLib (Hu et al., 2023), MALib (Zhou et al., 2023), để nêu tên một số).

2. RL dựa trên mô hình: RL dựa trên mô hình là một lĩnh vực nghiên cứu tích cực đang phát triển cho cộng đồng RL. Trái ngược với các phương pháp không dựa trên mô hình, nó bao gồm việc mô hình hóa hàm chuyển đổi và phần thưởng một cách rõ ràng thay vì học từ các tuple tương tác môi trường. Chúng tôi chưa lên kế hoạch tích hợp các thuật toán RL dựa trên mô hình vào Pearl vì hai lý do: 1) các thuật toán trong văn hệ ngày nay mang tính khám phá và vẫn chưa được áp dụng rộng rãi trong thực tế mặc dù các ứng dụng cụ thể trong robot học đang nổi lên (Wu et al., 2023), 2) đối với các trường hợp sử dụng sản xuất khác nhau, thường khó mô hình hóa chính xác các hàm chuyển đổi và phần thưởng chỉ với thông tin một phần từ các tương tác môi trường - ví dụ, trong các hệ thống đấu thầu, chỉ đề xuất thắng cuộc được quan sát trái ngược với tất cả các đề xuất. Hơn nữa, trong nhiều hệ thống có tương tác con người (hệ thống gợi ý v.v.), các hàm chuyển đổi và phần thưởng có thể không ổn định, làm cho việc mô hình hóa chính xác trở nên thách thức hơn. Mặc dù vậy, có văn hệ đang phát triển về các ý tưởng RL dựa trên mô hình - với các mô hình nền tảng lớn được huấn luyện trước phục vụ như mô hình của môi trường như Dreamer (Hafner et al., 2023) và Diffusion World Model (Ding et al., 2024). Khi những phương pháp này trưởng thành và bắt đầu được các nhà thực hành áp dụng, chúng tôi nghĩ việc tích hợp chúng vào Pearl sẽ hữu ích. Hiện tại, chúng tôi khuyến khích người dùng kiểm tra MBRL-Lib (Pineda et al., 2021), một thư viện mã nguồn mở dựa trên PyTorch xuất sắc chuyên về việc triển khai các phương pháp RL dựa trên mô hình.

3. Abstractions cấp thấp: Pearl tập trung chủ yếu vào thiết kế của một agent RL toàn diện và sẵn sàng cho sản xuất, vì vậy đối với bản phát hành đầu tiên, chúng tôi không tập trung vào các đóng góp kỹ thuật cấp thấp. Một số thư viện đặt trọng tâm mạnh vào những cải tiến như vậy: ví dụ, một trong những đổi mới chính của TorchRL (Bou et al., 2023) là khái niệm TensorDict, một container dữ liệu để truyền tensors và các đối tượng khác giữa các thành phần của một thuật toán RL, được thiết kế đặc biệt cho RL. Một phần mở rộng tương lai tiềm năng của Pearl là sử dụng các abstractions cấp thấp như vậy với hy vọng cải thiện hiệu suất.

Về công việc tương lai khác, một số tính năng quan trọng mà chúng tôi có trong đầu có thể có lợi cho cộng đồng. Đầu tiên, chúng tôi tin rằng các kiến trúc transformer (Vaswani et al., 2017) có thể cung cấp một phần mở rộng tuyệt vời cho module tóm tắt lịch sử hiện tại sao cho attention có thể hướng đến các quan sát quá khứ cung cấp thông tin có ý nghĩa cho trạng thái chủ quan của agent. Thứ hai, kế hoạch của chúng tôi trong việc cung cấp hỗ trợ tích hợp mô hình ngôn ngữ với các quyết định cấp token cũng có thể có lợi cho các nhà nghiên cứu quan tâm đến giao điểm giữa RL và các mô hình ngôn ngữ lớn.

Phụ lục F. Chi tiết Bổ sung về Thuật toán và Tiêu chí Lựa chọn

Trong phần này, chúng tôi cung cấp giới thiệu ngắn gọn cho mỗi thuật toán được triển khai trong Pearl và thảo luận khi nào thuật toán nên được chọn.

F.1 Thuật toán Tối ưu hóa Chính sách Ra quyết định Tuần tự Dựa trên Giá trị

• DQN (Mnih et al., 2015): DQN kết hợp Q-learning với mạng neural sâu để xấp xỉ hàm giá trị hành động tối ưu cho việc ra quyết định trong môi trường phức tạp. Nó sử dụng experience replay và mạng mục tiêu để ổn định huấn luyện, cho phép nó học các chính sách hiệu quả từ không gian đầu vào có chiều cao và không gian hành động rời rạc.

• Double DQN (Van Hasselt et al., 2016): Double DQN giải quyết bias ước lượng quá mức vốn có trong Q-learning bằng cách tách biệt quá trình lựa chọn hành động và đánh giá hành động. Trong Double DQN, hành động tối đa hóa Q-value được chọn sử dụng mạng trực tuyến, trong khi giá trị của hành động này được đánh giá sử dụng mạng mục tiêu, dẫn đến việc học chính xác và ổn định hơn. Sự sửa đổi này cải thiện hiệu suất và độ bền của DQN, đặc biệt trong các môi trường nơi ước lượng quá mức có thể ảnh hưởng đáng kể đến quá trình học.

• Dueling DQN (Wang et al., 2016): Dueling DQN giới thiệu một kiến trúc mới tách biệt biểu diễn của giá trị trạng thái và giá trị lợi thế trong mạng. Kiến trúc này bao gồm hai luồng: một ước lượng hàm giá trị trạng thái và một khác ước lượng hàm lợi thế cho mỗi hành động, sau đó được kết hợp để tạo thành hàm Q-value cuối cùng. Sự tách biệt này cho phép mạng học hiệu quả hơn về các trạng thái có giá trị, độc lập với hành động được thực hiện, cải thiện hiệu quả học và hiệu suất trong các môi trường có nhiều hành động có giá trị tương tự.

• Deep SARSA (Rummery và Niranjan, 1994): Deep SARSA (State-Action-Reward-State-Action) mở rộng phương pháp SARSA bằng cách sử dụng mạng neural sâu để xấp xỉ hàm giá trị hành động. Không giống như DQN, sử dụng phần thưởng tương lai tối đa để cập nhật Q-values, Deep SARSA cập nhật Q-values của nó dựa trên hành động thực tế được thực hiện bởi chính sách, tích hợp bản chất on-policy của SARSA vào deep learning. Cách tiếp cận này có thể dẫn đến việc học ổn định hơn trong một số môi trường, đặc biệt khi sử dụng các chính sách ngẫu nhiên hoặc khám phá.

F.2 Thuật toán Tối ưu hóa Chính sách Ra quyết định Tuần tự Actor-critic

• REINFORCE (Sutton et al., 1999): REINFORCE là thuật toán gradient chính sách trực tiếp tối ưu hóa chính sách bằng cách điều chỉnh các tham số của nó để tối đa hóa phần thưởng kỳ vọng. Nó sử dụng gradient của log-probability của các hành động được chọn, được cân bằng bởi phần thưởng tích lũy, để cập nhật các tham số chính sách, cho phép nó học các chính sách phức tạp trong không gian có chiều cao. Mặc dù hiệu quả, REINFORCE thường bị phương sai cao trong ước lượng gradient, làm cho cần thiết phải sử dụng các kỹ thuật như baselines để giảm phương sai và cải thiện tính ổn định học.

• DDPG (Silver et al., 2014): DDPG là thuật toán actor-critic không dựa trên mô hình, off-policy được thiết kế cho không gian hành động liên tục. Nó kết hợp lợi ích của gradient chính sách xác định, giảm phương sai trong ước lượng gradient, với mạng neural sâu để học cả chính sách (actor) và hàm giá trị (critic). DDPG sử dụng experience replay và sử dụng các mạng mục tiêu riêng biệt cho huấn luyện ổn định, cho phép nó hoạt động tốt trong các môi trường phức tạp với không gian trạng thái và hành động có chiều cao.

• TD3 (Fujimoto et al., 2018): TD3 giải quyết các hạn chế quan trọng bằng cách tích hợp ba cải tiến quan trọng: clipped double Q-learning, delayed policy updates, và target policy smoothing. Clipped double Q-learning giảm thiểu bias ước lượng quá mức bằng cách sử dụng minimum của hai ước lượng Q-value, trong khi delayed policy updates làm chậm các thay đổi chính sách để cung cấp ước lượng giá trị chính xác hơn. Target policy smoothing thêm nhiễu vào các hành động mục tiêu để giảm overfitting đến các đỉnh hẹp trong hàm giá trị, dẫn đến việc học bền vững và ổn định hơn trong không gian hành động liên tục.

• SAC (Haarnoja et al., 2018): SAC là thuật toán off-policy nhằm tối đa hóa cả phần thưởng kỳ vọng và entropy của chính sách, thúc đẩy khám phá và độ bền. Nó sử dụng chính sách ngẫu nhiên, được biểu diễn bởi phân phối Gaussian, và sử dụng số hạng entropy trong hàm mục tiêu để khuyến khích các hành động đa dạng và khám phá. SAC kết hợp lợi ích của cả phương pháp actor-critic và entropy-regularized, dẫn đến việc học ổn định và hiệu quả trong không gian hành động liên tục có chiều cao. Chúng tôi cũng triển khai phiên bản không gian hành động rời rạc bằng cách tính toán entropy hàm giá trị kỳ vọng và entropy phân phối chính sách từ một phân phối chính sách rời rạc.

• PPO (Schulman et al., 2017): PPO đạt được sự cân bằng giữa tính ổn định của các phương pháp trust-region và hiệu quả của các kỹ thuật tối ưu hóa bậc nhất. Nó giới thiệu một mục tiêu surrogate được cắt để giới hạn các cập nhật chính sách, đảm bảo rằng các thay đổi đối với chính sách không quá drastic và duy trì trong một trust region được định nghĩa trước. Cách tiếp cận này đơn giản hóa việc triển khai trong khi duy trì hiệu suất bền vững và hiệu quả mẫu, làm cho PPO trở thành lựa chọn phổ biến cho một loạt các nhiệm vụ RL.

F.3 Thuật toán Khám phá

• LinUCB (Li et al., 2010): LinUCB chọn hành động dựa trên upper confidence bounds được suy ra từ các mô hình tuyến tính. Nó cân bằng khám phá và khai thác bằng cách xem xét cả phần thưởng dự đoán và bất định của dự đoán, cho phép nó đưa ra quyết định có thông tin trong các môi trường động. LinUCB đặc biệt hiệu quả trong các kịch bản nơi phần thưởng là hàm tuyến tính của các đặc trưng đã biết, làm cho nó trở thành công cụ mạnh mẽ cho gợi ý cá nhân hóa và ra quyết định thích ứng.

• Neural-LinUCB (Xu et al., 2021): Neural-LinUCB tăng cường thuật toán LinUCB truyền thống bằng cách tích hợp mạng neural sâu để nắm bắt các mối quan hệ phức tạp, phi tuyến trong dữ liệu ngữ cảnh. Trong cách tiếp cận hybrid này, một mạng neural trích xuất biểu diễn đặc trưng có chiều cao, sau đó được đưa vào một mô hình tuyến tính để tính toán upper confidence bounds cho lựa chọn hành động. Sự kết hợp này cho phép Neural-LinUCB tận dụng sức mạnh biểu đạt của mạng neural để dự đoán chính xác hơn trong khi duy trì sự cân bằng khám phá-khai thác có nguyên tắc của LinUCB, làm cho nó hiệu quả cho các nhiệm vụ ra quyết định tinh vi.

• LinTS (Agrawal và Goyal, 2013): LinTS tận dụng suy luận Bayesian để cân bằng khám phá và khai thác bằng cách duy trì phân phối xác suất trên các tham số mô hình. Đối với mỗi hành động, nó lấy mẫu tham số từ phân phối này để ước lượng phần thưởng, hiệu quả tích hợp bất định vào quá trình ra quyết định. Cách tiếp cận này cho phép thuật toán thích ứng học chính sách tối ưu trong các môi trường nơi phần thưởng là hàm tuyến tính của ngữ cảnh, làm cho nó phù hợp cho các ứng dụng như gợi ý trực tuyến và marketing cá nhân hóa.

• SquareCB (Foster và Rakhlin, 2020): SquareCB giải quyết sự cân bằng khám phá-khai thác trong các vấn đề contextual bandit bằng cách giảm chúng thành các nhiệm vụ hồi quy trực tuyến. Nó sử dụng một oracle hồi quy trực tuyến với squared loss để quản lý và cập nhật confidence bounds một cách hiệu quả cho việc ra quyết định. Cách tiếp cận này cho phép SquareCB tối ưu hóa hiệu suất dưới cả môi trường agnostic và realizable, cung cấp giải pháp bền vững cho contextual bandits trong các ứng dụng đa dạng.

• Bootstrapped DQN và Randomized Value Functions (Osband et al., 2016): Bootstrapped DQN tăng cường khả năng khám phá của Deep Q-Network (DQN) tiêu chuẩn bằng cách giới thiệu nhiều heads trong lớp đầu ra của mạng neural, mỗi head đại diện cho một Q-function khác nhau. Những heads này được huấn luyện trên các tập con khác nhau một chút của dữ liệu huấn luyện, hiệu quả tạo ra các ước lượng giá trị đa dạng khuyến khích khám phá trong quá trình học. Cách tiếp cận này giải quyết thách thức khám phá không đủ trong DQN tiêu chuẩn, cải thiện hiệu suất của thuật toán trong các môi trường với phần thưởng thưa thớt.

F.4 Thuật toán Học Ngoại tuyến

• CQL (Kumar et al., 2020): CQL được thiết kế để học hiệu quả từ dữ liệu ngoại tuyến mà không cần tương tác thêm với môi trường. CQL giới thiệu số hạng điều chỉnh phạt Q-values của các hành động chưa thấy, giảm ước lượng quá mức và thúc đẩy ước lượng chính sách bảo thủ. Cách tiếp cận này dẫn đến đánh giá chính sách ổn định và bền vững hơn, đặc biệt trong các môi trường nơi dữ liệu có sẵn bị hạn chế hoặc thiên vị.

• IQL (Kostrikov et al., 2021): IQL nhằm học các chính sách tối ưu từ một bộ dữ liệu cố định mà không cần tương tác môi trường thêm. Nó sử dụng behavior cloning loss để lựa chọn hành động, kết hợp với Q-learning loss tiêu chuẩn để đánh giá chính sách, tách biệt lựa chọn hành động khỏi đánh giá chính sách để ổn định huấn luyện. Cách tiếp cận này giúp xử lý distributional shift giữa chính sách học từ dữ liệu ngoại tuyến và chính sách tối ưu, làm cho IQL hiệu quả trong các môi trường nơi khám phá bị hạn chế.

• TD3BC (Fujimoto và Gu, 2021): TD3BC kết hợp thuật toán TD3 với behavior cloning để giải quyết thách thức distributional shift trong RL ngoại tuyến. Nó tận dụng tính ổn định và cân bằng khám phá-khai thác của TD3 trong khi sử dụng behavior cloning để ràng buộc cập nhật chính sách gần hơn với phân phối dữ liệu. Sự tích hợp này tăng cường hiệu quả mẫu và cải thiện hiệu suất bằng cách giảm tác động có hại của lỗi ngoại suy phổ biến trong các môi trường ngoại tuyến.

F.5 Thuật toán Học An toàn

• Học nhạy cảm với rủi ro qua QRDQN (Dabney et al., 2018b): QRDQN xấp xỉ phân phối đầy đủ của lợi nhuận thay vì chỉ trung bình của chúng bằng cách sử dụng hồi quy quantile. Phương pháp này biểu diễn phân phối lợi nhuận kỳ vọng sử dụng nhiều quantiles, cho phép chính sách ước lượng tốt hơn sự biến đổi và rủi ro liên quan đến các hành động khác nhau. Bằng cách điều chỉnh các quantiles để nhấn mạnh các phần thấp hơn của phân phối, QRDQN có thể được điều chỉnh để ưu tiên hành vi tránh rủi ro, tạo ra các chính sách ưu tiên các hành động dẫn đến biến đổi thấp hơn trong phần thưởng nhận được, do đó cho phép học nhạy cảm với rủi ro.

• Tối ưu hóa chính sách có ràng buộc phần thưởng (RCPO) (Tessler et al., 2018): RCPO là cách tiếp cận để giải quyết các vấn đề tối ưu hóa chính sách có ràng buộc. Nó tích hợp tín hiệu phạt vào hàm phần thưởng để hướng dẫn chính sách đến việc thỏa mãn ràng buộc, trong khi cũng chứng minh hội tụ dưới các giả định nhẹ. RCPO đặc biệt hiệu quả trong các lĩnh vực robot, thể hiện hội tụ nhanh hơn và cải thiện tính ổn định so với các phương pháp truyền thống, làm cho nó thành thạo trong việc xử lý cả ràng buộc tổng quát và cụ thể trong các nhiệm vụ RL. Chúng tôi đã làm cho RCPO tương thích với TD3 (Fujimoto et al., 2018), SAC (Haarnoja et al., 2018) và DDPG (Silver et al., 2014).

F.6 Thuật toán Tóm tắt Lịch sử

Các cách tiếp cận chính của chúng tôi cho tóm tắt lịch sử là stacking quan sát lịch sử và tóm tắt LSTM (Hochreiter và Schmidhuber, 1997).

• Stacking quan sát: Stacking quan sát trong RL, đặc biệt trong các môi trường quan sát một phần, cung cấp cách thực tế để xấp xỉ trạng thái quan sát đầy đủ. Bằng cách tích hợp chuỗi các quan sát, hành động gần đây nhất, hoặc cả hai, các agent có thể suy ra trạng thái cơ bản của hệ thống chính xác hơn. Phương pháp này hiệu quả biến vấn đề thành một vấn đề giống như quá trình quyết định Markov quan sát đầy đủ (MDP), cho phép agent đưa ra quyết định có thông tin hơn dựa trên ngữ cảnh mở rộng được cung cấp bởi dữ liệu lịch sử.

• Tóm tắt lịch sử LSTM: Sử dụng mạng LSTM (long short-term memory) trong RL, đặc biệt trong các môi trường quan sát một phần, tăng cường khả năng của agent ghi nhớ và sử dụng các phụ thuộc dài hạn trong dữ liệu quan sát (Lambrechts et al., 2022). Khả năng này rất quan trọng nơi stacking lịch sử đơn thuần có thể không nắm bắt được ngữ cảnh cần thiết cho việc ra quyết định. LSTM giúp xử lý các chuỗi quan sát quá khứ, duy trì biểu diễn trạng thái động và thông tin hơn, cho phép agent đưa ra dự đoán và quyết định chính xác hơn dựa trên chuỗi mở rộng của các sự kiện quá khứ. Cách tiếp cận này có thể cải thiện đáng kể hiệu suất trong các môi trường nơi trạng thái hiện tại của agent không quan sát đầy đủ thông qua đầu vào ngay lập tức.
