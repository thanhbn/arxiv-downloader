# 2310.13385.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2310.13385.pdf
# File size: 1147994 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Tuna: Instruction Tuning using Feedback from Large Language Models
Haoran Li1,†,Yiran Liu3,‡,Xingxing Zhang2,Wei Lu1,Furu Wei2
1StatNLP Research Group, Singapore University of Technology and Design
2Microsoft Research Asia,3Tsinghua University
haoran2_li@mymail.sutd.edu.sg, wei_lu@sutd.edu.sg
liu-yr21@mails.tsinghua.edu.cn ,{xizhang,fuwei} @microsoft.com
Abstract
Instruction tuning of open-source large lan-
guage models (LLMs) like LLaMA, using di-
rect outputs from more powerful LLMs such
as Instruct-GPT and GPT-4, has proven to
be a cost-effective way to align model behav-
iors with human preferences. However, the
instruction-tuned model has only seen one re-
sponse per instruction, lacking the knowledge
of potentially better responses. In this pa-
per, we propose finetuning an instruction-tuned
LLM using our novel probabilistic ranking
andcontextual ranking approaches to increase
the likelihood of generating better responses.
Probabilistic ranking enables the instruction-
tuned model to inherit the relative rankings of
high-quality and low-quality responses from
the teacher LLM. On the other hand, learn-
ing with contextual ranking allows the model
to refine its own response distribution using
the contextual understanding ability of stronger
LLMs. Furthermore, we apply probabilistic
ranking and contextual ranking sequentially
to the instruction-tuned LLM. The resulting
model, which we call Tuna , consistently im-
proves the performance on Super Natural In-
structions (119 test tasks), LMentry (25 test
tasks), Vicuna QA, and can even obtain better
results than several strong reinforcement learn-
ing baselines. Our code and data are available
athttps://github.com/microsoft/LMOps .
1 Introduction
Large language models (LLMs) have made signif-
icant progress by scaling up model size and data
size (Peters et al., 2018; Devlin et al., 2019; Rad-
ford et al., 2019; Brown et al., 2020; OpenAI, 2023)
for unsupervised pre-training and subsequently ap-
plying reinforcement learning from human feed-
back (RLHF) to align model responses with human
preferences (Christiano et al., 2017; Ouyang et al.,
2022). More recently, instruction tuning (Wei et al.,
†,‡Work done during internship at MSRA.
Instruction -
Tuned LLMs
 R1, -0.9
Text-Davinci
003Learn the 
Ranking
InstructionResponses
R3>R1>R2
GPT-4 
Learn the RankingContextual  Ranking
Instruction
 R1
 R2
 R3Responses
Instruction -
Tuned LLMs
R3>R1>R2Probabilistic  Ranking
Probabilistic 
Ranking
Contextual 
Ranking
Instruction -
Tuned LLMs
TunaContextual Prompt
 R2, -1.2
 R3, -0.7Figure 1: The finetuning process using probabilistic
ranking (top), contextual ranking (middle), and a com-
bination of both (bottom).
2022) with Self-Instruct algorithm (Wang et al.,
2022a) has emerged as a cost-effective method for
aligning with human preferences. In this approach,
open LLMs like LLaMA (Touvron et al., 2023)
can be finetuned on instruction-following data gen-
erated by OpenAI GPT using the Self-Instruct al-
gorithm. The Alpaca model (Taori et al., 2023)
exemplifies this technique, which enables close
alignment with human preferences while reducing
dependence on human-labeled data.
However, instruction tuning offers only a broad
guideline for the base LLMs to transition from
“next token prediction” to a more interactive,
instruction-following style. As a result, the model
may learn some superficial features or styles from
the instruction data but still lacks a deeper under-
standing of what constitutes a preferred response.
For instance, when given a question like “GivearXiv:2310.13385v1  [cs.CL]  20 Oct 2023

--- PAGE 2 ---
three tips for staying healthy”, a base LLM may
generate fluent yet undesirable continuations, while
an instruction-tuned LLM could offer three general
tips. Humans might prefer more detailed tips over
general tips, but such tips are less likely to be sam-
pled since they have lower likelihood within the
current model distribution. This can be attributed
to the fact that they are either unseen during in-
struction tuning or hard to be sampled due to the
exposure bias (Ranzato et al., 2015).
To address this, we propose further finetuning
of an instruction-tuned LLM to discern the qual-
ity of multiple responses more precisely, using our
novel probabilistic ranking (Sec. 2.2; Fig. 1 top)
and contextual ranking (Sec. 2.3; Fig. 1 mid-
dle) approaches. Probabilistic ranking enables the
instruction-tuned LLM to inherit the high-quality
and low-quality responses as well as their rela-
tive rankings from the teacher LLM (e.g., text-
davinci-003). In contrast, contextual ranking aims
to re-balance the instruction-tuned model’s own re-
sponse distribution with the help of stronger LLMs
(e.g., GPT-4), mitigating the exposure bias issue.
We apply probabilistic ranking and contextual
ranking sequentially to an instruction-tuned model,
i.e., Alpaca (Taori et al., 2023), resulting in a model
called Tuna (Sec. 2.4; Fig. 1 bottom). We evaluate
Tuna on various benchmarks, including Super Nat-
ural Instructions (Wang et al., 2022b), which con-
tains 119 diverse test tasks; LMentry (Efrat et al.,
2022), comprising 25 tasks to assess the basic ca-
pabilities and robustness of LLMs; and Vicuna QA
(Chiang et al., 2023) which evaluates the model’s
ability to answer a diverse set of questions with the
assistance of GPT-4. Experimental results demon-
strate that the Tuna model not only consistently
outperforms the standard instruction-tuned mod-
els on all benchmarks, but also surpasses several
strong RLHF baselines (Ouyang et al., 2022).
To summarize, our contributions are as follows:
•We propose probabilistic ranking andcontex-
tual ranking , which enable the instruction-
tuned model to distinguish high-quality and
low-quality responses and assign higher prob-
ability to the former accordingly.
•TheTuna model, obtained by sequentially
applying probabilistic ranking and contex-
tual ranking on an instruction-tuned LLM,
achieves better results than several strong
benchmarks, including RLHF models;•Our model, data and code will be released to
facilitate future research.
2 Methodology
In this section, we describe how to obtain our Tuna
model using the feedback from LLMs. We first
describe the vanilla instruction tuning. We then
introduce our probabilistic ranking and contextual
ranking approaches. Lastly, we describe how to
integrate both ranking approaches.
2.1 Instruction Tuning
LLMs like GPT-3 (Brown et al., 2020) have been
trained on a massive text corpus using maximum
likelihood estimation (MLE):
LMLE(y) =−1
|y|X
tlogp(yt|y<t;θ),(1)
where θrepresents the parameters of the base
model. The pre-training objective function compels
the model to predict the next token ytgiven its pre-
fixy<t= [y0, y1, ..., y t−1]. A sufficiently-trained
LLM can generate fluent continuations given al-
most any prefix. However, the generated continua-
tions may not align well with human preferences.
As the primary goal of an LLM is to assist humans,
it becomes essential to encourage the generation of
content that follows human instructions and aligns
with human preferences. The current dominant
approach to enhance LLMs’ instruction-following
ability is called instruction tuning (Mishra et al.,
2021; Wei et al., 2022; Taori et al., 2023), which
finetunes the base LLMs in a supervised manner
on instruction-response pairs {i, r}(where iis an
instruction and ris its response) using MLE:
LMLE(i, r) =−1
|r|logp(r|i;θ′), (2)
where θ′represents the parameters of the
instruction-tuned model. After instruction tuning,
we expect the model distribution p(·|i;θ′)to allo-
cate higher probabilities to proper responses like r
rather than undesirable continuations.
Note that the responses in instruction-response
pairs can either be annotated by humans1or gen-
erated by strong LLMs, such as Instruct-GPT or
GPT-4 (Wang et al., 2022a). A prevalent and cost-
effective approach for generating instruction tun-
ing data is the Self-Instruct algorithm (Wang et al.,
1https://huggingface.co/datasets/databricks/
databricks-dolly-15k

--- PAGE 3 ---
2022a). Specifically, it uses a strong LLM, e.g.,
text-davinci-003, to create instructions based on a
few seed instructions, and then generates a single
response for each instruction using the same LLM.
2.2 Probabilistic Ranking
Instruction tuning with the data generated by the
Self-Instruct algorithm is essentially a form of
sequence-level distillation (Kim and Rush, 2016).
The rationale behind this class of distillation
method is that the current commercial LLMs have
significantly better capabilities than their open-
source counterparts. Instead of learning from the
single-response data, our probabilistic ranking ap-
proach leverages the relative rankings of multiple
responses based on the teacher model’s probabili-
ties for better pseudo label distillation (see Fig. 1
top).
Letrdenote the original response for in-
struction iin the instruction tuning dataset.
We query strong (teacher) LLMs, such as
text-davinci-003 , to generate Nnew responses
fori. Let r(0), r(1), . . . , r(N−1)denote these new
responses, and p(r(0)|i), p(r(1)|i), . . . , p (r(N−1)|i)
denote their probabilities. While the teacher LLMs
are expected to produce responses of comparable
quality on average, there will inevitably be some
variation in the quality of the generated responses.
This inherent variability manifests itself in vari-
ous aspects, such as differences in accuracy (Wang
et al., 2023a), response length, and level of details
provided (Wang et al., 2023b).
Intuitively, if a model is perfectly distilled,
the relative probabilities assigned to two samples
should be the same as those of the teacher model.
Specifically, let p(r(j)|i;θ′)andp(r(k)|i;θ′)de-
note the probabilities of r(j)andr(k)w.r.t. the
student model. If p(r(j)|i)> p(r(k)|i), then
p(r(j)|i;θ′)> p(r(k)|i;θ′). We use the follow-
ing normalized log-likelihood as the teacher model
quality score to account for differences in response
lengths:
s(i, r(k)) =logp(r(k)|i)
|r(k)|β, k ={0, ..., N −1}
(3)
where |r(k)|is the length of r(k)andβrepresents
the length penalty.
We then rank those responses in decreasing or-
der based on s(i, r(k)). The resulting instruction-
response pairs become {i, r,(r[0], ...r[N−1])},
where i, rare from the original instruction tuningdata, and r[j]is considered to have better quality
thanr[k], ifj < k . Once we obtain the ranked
responses, we can encourage our model to learn
from these rankings using a pairwise ranking ob-
jective, which has been successfully employed in
previous work (Zhong et al., 2020; Liu et al., 2022;
Zhang et al., 2022; Zhao et al., 2023). The ranking
objective function is as follows:
Lrank=X
0≤j<k≤N−1Lj,k
rank (4)
Lj,k
rank= max
0, vk
θ′−vj
θ′+m×(k−j)
, j < k
(5)
where vk
θ′=1
|r[k]|logp 
r[k]|i;θ′
,m > 0is the
margin hyper-parameter. The ranking loss, Lrank,
aims to teach the model to distinguish good re-
sponses from bad ones based on the teacher LLM’s
perspective. In addition to Lrank, we also apply
a cross-entropy loss on the original response as
regularization:
L=Lrank+λLMLE, L MLE=1
|r|logp(r|i;θ′)
(6)
where ris the original response, and λ >0controls
the importance of LMLE, which helps prevent over-
optimization of the ranking loss.
After learning with probabilistic ranking, the
model can better assign probabilities to superior
and inferior responses.
2.3 Contextual Ranking
During the instruction tuning or the probabilistic
ranking stage, the model is finetuned to generate a
good rgiven an instruction i. However, given the
same iduring inference, the model may still gen-
erate a relatively low-quality response r′. This is
related to the exposure bias problem (Ranzato et al.,
2015), where the model fails to generate rdue to
accumulated errors during the auto-regressive gen-
eration process. To address this issue, we use our
contextual ranking approach to refine the distri-
bution of responses generated by the model itself,
assigning higher probabilities to better responses
with the help of strong LLMs (Fig. 1 middle), thus
alleviating exposure bias (Ranzato et al., 2015).
For each instruction, we first sample Nre-
sponses from the instruction-tuned model itself, i.e.,
r(0), r(1), ..., r(N−1)∼p(·|i;θ′). We hope the sam-
ples to be diverse enough so that better responses
are more likely to appear in the sampled results.

--- PAGE 4 ---
To ensure diversity, we impose a constraint on the
ROUGE-L (Lin, 2004) score between each pair of
responses, requiring it to be less than a threshold
τ. If the ROUGE-L score exceeds τ, we increase
the sampling temperature and resample another re-
sponse. If multiple trials still result in a ROUGE-L
score above τ, we retain the least similar response
from the trials. After obtaining Nresponses, we
leverage the contextual understanding ability of
commercial LLMs, such as GPT-4 (OpenAI, 2023),
to rank them based on various aspects. The ranking
process consists of multiple steps. First, we ask
GPT-4 to assess whether the instruction requires
an open-ended answer (e.g., story generation) or
a close-ended answer (e.g., solving a math prob-
lem). We then request GPT-4 to generate its own
response as a reference. Next, GPT-4 compares
the reference response with the Nresponses from
different aspects and assign scores to each response.
For open-ended instructions, GPT-4 evaluates rel-
evance (score 0-5), level of details /justification
(score 0-5), and accuracy (score 0-5) of the model
responses compared to its reference response. For
close-ended instructions, the evaluation criteria are
accuracy (score 0-5), level of details /justification
(score 0-5), and clarity (score 0-5). Finally, GPT-4
ranks responses in decreasing order based on the
sum of their scores (see Appendix E for our com-
plete prompt). We also manually evaluated GPT-4
rankings, which have achieved a strong correlation
with human judgements (see Appendix G, H).
As in Sec. 2.2, the resulting instruction tun-
ing dataset becomes {i, r,(r[0], ...r[N−1])}. Note
that the r[k],0≤k≤N−1, is derived from the
instruction-tuned model itself. Lastly, we use the
same objective function as in Eq. 6 to encourage
the model to assign higher probabilities to better
responses.
2.4 Integrating Probabilistic and Contextual
Ranking
Given an instruction-tuned model, there are sev-
eral options for further finetuning: 1) learning with
probabilistic ranking alone; 2) learning with con-
textual ranking alone; 3) learning with probabilistic
ranking followed by contextual ranking (see Fig.
1 bottom). We refer to the models finetuned with
these three methods as Tuna p,Tuna c, and Tuna ,
respectively.
To optimally integrate both probabilistic rank-
ing and contextual ranking techniques, it is recom-mended to first obtain a Tuna pmodel, followed by
applying contextual ranking to Tuna p’s response
distribution, resulting in the Tuna model. There
are two reasons for this choice. First, although it
is beneficial to learn the ranking of different re-
sponses from the teacher LLM’s perspective (prob-
abilistic ranking), the model might not fully capture
the teacher’s ranking knowledge due to its limited
capacity. Second, contextual ranking enables the
model to better adapt to its own capacity by work-
ing with the model’s own generations. By gener-
ating its own responses, the model can finetune its
understanding with the help of stronger LLMs and
more effectively produce responses that are both
closer to human preferences and compatible with
its capacity constraints, alleviating the exposure
bias issue (Ranzato et al., 2015).
3 Experiments
3.1 Model and Data
In our experiments, we use a 7B LLaMA model
(Touvron et al., 2023) as the base model. The
instruction tuning data is sourced from Alpaca
(Taori et al., 2023), which consists of 52K instruc-
tions paired with responses that are generated by
text-davinci-003 using the Self-Instruct algo-
rithm (Wang et al., 2022a). We perform instruction
tuning on 52K Alpaca data using recommended hy-
perparameters, such as a learning rate of 2e-5 and
the AdamW optimizer (0.9,0.999) (Loshchilov
and Hutter, 2019).2For simplicity, we also refer to
the instruction-tuned model as Alpaca .
For probabilistic ranking, we input 52K instruc-
tions from Alpaca dataset into text-davinci-003
to produce N= 4responses per instruction along
with their log-likelihoods3, with an inference tem-
perature of 1. We calculate response scores using
Eq. 3 with βbeing 1.3, and rank the responses
accordingly. Subsequently, we finetune the Alpaca
model for 1 epoch with a learning rate 1e-5, mar-
ginm= 0.1, and cross entropy regularizer weight
λ= 1.0. We denote the model trained exclusively
with probabilistic ranking as Tuna p.
For contextual ranking, we sample N= 4 re-
sponses from the Alpaca model with temperature
T= 1for each instruction. To avoid similar gen-
erations, we ensure the pairwise ROUGE-L (Lin,
2004) between responses is less than τ= 0.8. Oth-
2https://github.com/AetherCortex/Llama-X
3GPT-4 is more powerful but it does not return log-
likelihoods.

--- PAGE 5 ---
Super NI LMentry Vicuna QA
0-shot 2-shot LMentry Score Win Lose Tie
LLaMA 11.0 23.6 26.3 4% 92% 4%
T5-LM 11B - 30.2 20.6 - - -
T0 11B - 32.3 31.6 - - -
InstructGPT 175B - 52.1 48.4 - - -
Alpaca 36.0 44.5 31.4 - - -
+ PPO-sim 31.9 (-4.1) 37.5 (-7.0) 27.8 (-3.6) 79% 16% 5%
+ PPO-sim-GPT4-20K 37.1 (+1.1) 44.9 (+0.4) 27.8 (-3.6) 74% 22% 4%
Tuna p 39.4 (+3.4) 43.9 (-0.6) 35.0 (+3.6) 68% 27% 5%
Tuna c 37.7 (+1.7) 46.6 (+2.1) 32.2 (+0.8) 74% 20% 6%
Tuna c(PRM) 34.2 (-1.8) 40.1 (-4.4) 32.2 (+0.8) 75% 19% 6%
Tuna 38.7 (+2.7) 45.0 (+0.5) 34.7 (+3.3) 86% 10% 4%
Table 1: Performance comparison of different models on Super NI, LMentry and Vicuna QA. The numbers in bold
indicate the top-2 results. The numbers in parentheses indicate the performance differences compared to Alpaca.
The results of T5-LM 11B (Raffel et al., 2020), T0-11B (Sanh et al., 2022), InstructGPT 175B (Ouyang et al., 2022)
are taken from Wang et al. (2022b); Efrat et al. (2022). The RLHF baselines PPO-sim and PPO-sim-GPT4-20K,
which apply the PPO algorithm (Schulman et al., 2017), are taken from Dubois et al. (2023).
Model Alpaca Alpaca+PPO-sim Tuna pTuna cTuna
Score 2.13 2.95∗2.98∗3.15∗3.80∗†
Table 2: Human evaluation on Vicuna QA. * denotes that the model is significantly ( p <0.01) better than Alpaca,
while†denotes that Tuna is significantly ( p <0.01) better than other models.
erwise, we remove the similar response, increase
the temperature by 0.1, and resample. If three tri-
als fail to produce unique enough responses, we
keep the least similar one. We then employ GPT-
4 to rank responses for the first 13K instruction
data with the GPT-4 inference temperature to be 0.
The contextual ranking prompt is shown in Table
9.4The finetuning hyperprameters follow those
of probabilistic ranking. We refer to the model
trained on 13K contextual ranking data of the Al-
paca model as Tuna c.
Furthermore, we use the 13K GPT-4 ranking
data to train a proxy ranking model (PRM) based on
StableLM-3B.5The PRM is employed to re-rank
Alpaca’s responses on 52K instructions. We refer
to the Alpaca model trained with 52K ranking data
totally generated by the PRM as Tuna c(PRM) .
Lastly, we also collect 13K GPT-4 contextual
ranking data based on Tuna p’s responses instead
of Alpaca’s. We refer to the model finetuned on
Tuna pasTuna .
We also included strong reinforcement learning
4The cost of calling OpenAI API is listed in Appendix B.
5https://github.com/Stability-AI/StableLMbaselines for comparison (i.e., PPO-sim and PPO-
sim-GPT4-20K models from AlpacaFarm (Dubois
et al., 2023)).6
3.2 Evaluation
Super Natural Instruction (Super NI) Super
NI (Wang et al., 2022b) contains 119 test tasks
designed to evaluate a model’s cross-task general-
ization ability. It includes a variety of classification
and generation tasks, such as textual entailment
and title generation. We report both 0-shot and
2-shot performance, where 0-shot provides only
an instruction (referred to as “definition” in their
literature) and 2-shot offers two additional positive
examples. The evaluation metric for all 119 tasks
isROUGE-L (Lin, 2004), which is strongly corre-
lated with human evaluation with a Pearson coef-
ficient of 0.998 according to Wang et al. (2022b).
Greedy decoding is applied during inference.
LMentry LMentry (Efrat et al., 2022) is a bench-
mark that primarily focuses on the accuracy and ro-
6We also trained our own RLHF model, which is not as
good as the ones in AlpacaFarm. The comparison can be
found in Appendix I

--- PAGE 6 ---
bustness aspects of LLMs’ generations. It contains
25 short tasks that are trivial to humans but chal-
lenging for LLMs. The final metric is LMentry
score , which is calculated by multiplying its mean
accuracy on 25 tasks with the robustness score. The
model will be evaluated in a 0-shot manner, and
greedy decoding is applied during inference.
Vicuna QA Vicuna QA (Chiang et al., 2023)
comprises 80 test questions across 9 categories
that measure an LLM’s ability to generate rele-
vant, detailed and accurate responses and it has
been widely adopted in many works. Instead of
having a ground truth for evaluation, it conducts
pairwise comparisons with the help of GPT-4 (Ope-
nAI, 2023). It prompts GPT-4 to compare the out-
puts of our models to the Alpaca model. We report
thewin/lose/tie rate against the Alpaca model.
Human Evaluation Additionally, we conduct
human evaluations on Vicuna QA. Specifically, re-
sponses from five anonymous systems, namely Al-
paca, Alpaca + PPO-sim, Tuna, Tuna p, and Tuna c,
were randomly shuffled and presented to annota-
tors who were then asked to rank these outputs.
The scoring was designed such that the i-th ranked
system receives a score of 6−i, meaning the best-
ranked system receives a score of 5, and the worst-
ranked system receives a score of 1. Each question
was annotated by two different annotators, and the
score was averaged.
3.3 Main Results
The main results are presented in Table 1. Af-
ter instruction tuning, Alpaca demonstrates sig-
nificant performance improvements over LLaMA
on all three benchmarks. This highlights the suc-
cessful transition from the “next token predic-
tion” paradigm to a more interactive instruction-
following paradigm.
Furthermore, both contextual and probabilis-
tic ranking enhance performance across all three
benchmarks. Specifically, Tuna cexhibits more im-
provement on the Super NI72-shot results while
Tuna pperforms better on Super NI 0-shot and
LMentry, narrowing the performance gap with
much larger models like InstructGPT-175B. Since
the 2-shot input is longer than 0-shot, we conjec-
ture that contextual ranking might be more benefi-
cial for longer sequence generation than probabilis-
7ROUGE is used as the default metric on Super NI. How-
ever, our results follow the same trend using BERTScore (see
Appendix J).tic ranking. On the Vicuna QA benchmark, both
Tuna pandTuna coutperform Alpaca significantly
on nearly 70% of the questions, as evaluated by
GPT-4. Upon comparison with the RLHF base-
lines, Tuna pandTuna cconsistently demonstrate
superior performances on both the Super NI and
LMentry benchmarks. However, when it comes to
the Vicuna QA benchmark, their performance is
marginally lower than that of the RLHF baselines.
Moreover, Tuna achieves the best performance on
Vicuna QA while maintaining competitive scores
on Super-NI and LMentry. Human results on Vi-
cuna QA (see Table 2) also confirm that humans
prefer the responses from our models.
Furthermore, Tuna c(PRM) demonstrates com-
parable performance to Tuna con Vicuna QA and
LMentry, but it underperforms both Tuna cand Al-
paca on Super NI. This suggests that although the
PRM has primarily learned ranking from the GPT-
4 contextual ranking data, it also introduces some
noise during the learning process. Overall, it is
more effective to learn directly from GPT-4 contex-
tual ranking data.8
3.4 Ablation Study
In this subsection, we delve deeper into the per-
formance of our approach by examining several
aspects, including: (a) the effect of more responses
in instruction tuning, (b) the order of applying two
ranking methods, (c) the influence of the cross en-
tropy regularization, (d) the amount of probabilistic
ranking data, and (e) the risks of GPT-4 evaluation.
More Responses in Instruction Tuning We ex-
plore whether Tuna’s effectiveness is solely due
to the increased response data by examining the
impact of adding more responses per instruction
during instruction tuning. We create a new model,
Alpaca-Mul, by adding four extra responses from
the probabilistic ranking dataset to the Alpaca
dataset and fine-tuning the LLaMA model using
Eq. 2. The results are presented in Table 3.
Upon evaluation on Super NI, Alpaca-Mul’s per-
formance is nearly identical to that of Alpaca but
falls short when compared to the 0-shot settings
ofTuna pand Tuna. On LMentry, Alpaca-Mul out-
performs Alpaca, yet it still does not reach the
performance levels of Tuna pand Tuna. Interest-
ingly, in the Vicuna QA task, Alpaca-Mul slightly
underperforms compared to Alpaca.
8Experiments with more PRMs can be found in App. D.

--- PAGE 7 ---
Super NI LMentry Vicuna QA
0-shot 2-shot LMentry Score Win Lose Tie
Alpaca 36.0 44.5 31.4 - - -
Alpaca-Mul 34.7 (-1.3) 45.7 (+1.2) 33.9 (+2.5) 42% 53% 5%
Tuna p 39.4 (+3.4) 43.9 (-0.6) 35.0 (+3.6) 68% 27% 5%
Tuna 38.7 (+2.7) 45.0 (+0.5) 34.7 (+3.3) 86% 10% 4%
Tuna c 37.7 (+1.7) 46.6 (+2.1) 32.2 (+0.8) 74% 20% 6%
Tuna cp-13K 35.7 (-0.3) 44.0 (-0.5) 33.5 (+2.1) 58% 37% 5%
Tuna cp-39K 34.8 (-1.2) 43.4 (-1.1) 35.4 (+4.0) 46% 48% 6%
Tuna cp-52K 35.0 (-1.0) 42.6 (-1.9) 33.8 (+2.4) 51% 41% 8%
mix-Tuna-52K 37.7 (+1.7) 44.2 (-0.3) 30.0 (-1.4) 70% 23% 7%
mix-Tuna-104K 36.0 (+0.0) 40.0 (-4.5) 32.6 (+1.2) 55% 40% 5%
Table 3: Different combinations of probabilistic ranking data and contextual ranking data. The numbers in bold
represent the top-2 results. The numbers in parentheses represent the performance difference compared to Alpaca.
These findings suggest that merely adding more
responses without differentiating them does not
necessarily lead to improved response generation.
Overall, the results of Alpaca-Mul indicate that
Tuna’s superior performance cannot be solely at-
tributed to the availability of more response data.
Integration Order An alternative approach to
Tuna involves first training the Tuna cmodel, and
subsequently continuing training the Tuna cmodel
with probabilistic ranking data. The resulting
model is referred to as Tuna cp.
We explore various strategies for training Tuna cp:
1) finetuning Tuna cwith the first 13K probabilis-
tic ranking data ( Tuna cp-13K); 2) finetuing Tuna c
model with last 39K probabilistic ranking data
(Tuna cp-39K); 3) finetuning Tuna cmodel with
52K probabilistic ranking data ( Tuna cp-52K). Ad-
ditionally, we also try to finetune original Alpaca
model with a combination of 13K GPT-4 contextual
ranking data (generated from Alpaca model’s re-
sponses) and the last 39K probabilistic ranking data
(mix-Tuna-52K). We also finetune Alpaca model
with 52K contextual ranking data (13K GPT-4 con-
textual ranking +39K ranking-model-generated
data) plus 52K probabilistic ranking data (mix-
Tuna-104K). The training details are listed in the
Appendix C. The results are listed in Table 3.
None of the combination strategies consistently
outperform both Tuna pandTuna cacross the Vi-
cuna QA and Super NI benchmarks. On LMen-
try, however, finetuning Tuna cwith probabilistic
ranking data is beneficial, especially when no du-
plicate data is present ( Tuna cp-39K). This suggestsrank 1 2 3 4
contextual ranking 66.4 55.2 51.4 44.8
prob. ranking 55.8 54.3 52.5 49.4
PRM 69.2 57.8 50.9 44.7
Table 4: The average ranking lengths of contextual rank-
ing data, probabilistic ranking data and the data gener-
ated by the proxy ranking model (PRM).
that shorter probabilistic ranking data are beneficial
when high accuracy and robustness are top priority.
Interestingly, Tuna cpis not comparable to Tuna,
indicating that the order in which the model is
trained with contextual and probabilistic ranking
matters. One plausible explanation is that both the
original Alpaca data and the probabilistic ranking
data are generated by text-davinci-003 , while
Tuna chas significantly shifted the model distribu-
tion by re-ranking the Alpaca model’s responses,
making it challenging to finetune Tuna cwith prob-
abilistic ranking data again.
The Effect of Cross Entropy Regularizer We
examine the influence of the weight λof the cross
entropy regularizer in Eq. 6 on performance by
varying λacross different values: {0,0.1,1,5,10}
while training the Tuna cmodel. Fig. 2 illustrates
that as λincreases, the performance on accuracy-
oriented benchmarks such as Super NI and LMen-
try improves, while the performance on open ques-
tions does not necessarily follow the same trend.
On one hand, this finding suggests that with a small
λ, learning with contextual ranking may induce

--- PAGE 8 ---
/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000035/uni00000032/uni00000038/uni0000002a/uni00000028/uni00000010/uni0000002f/uni00000013/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057
/uni00000015/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057
/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000017/uni00000013/uni0000002f/uni00000030/uni00000048/uni00000051/uni00000057/uni00000055/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000014 /uni00000014/uni00000011/uni00000013 /uni00000018/uni00000011/uni00000013 /uni00000014/uni00000013/uni00000011/uni00000013
/uni00000013/uni00000018/uni00000013/uni00000014/uni00000013/uni00000013/uni00000035/uni00000044/uni00000057/uni00000048/uni0000003a/uni0000004c/uni00000051
/uni0000002f/uni00000052/uni00000056/uni00000048
/uni00000037/uni0000004c/uni00000048Figure 2: The effect of varying the weight λof cross en-
tropy regularization in Eq. 6 on Tuna c. The win/lose/tie
rate on Vicuna is computed against Alpaca.
long and detailed answers, but those answers are
not always accurate. On the other hand, it implies
that accuracy-oriented benchmarks and open QA
benchmarks are complementary, and researchers
should consider more diverse test cases to thor-
oughly evaluate a model (Wang et al., 2023b).
The Amount of Probabilistic Ranking Data
We investigate the impact of varying the amount
of probabilistic ranking data used for finetuning
theTuna pmodel by testing different data sizes, i.e.,
{0,13000 ,24000 ,52000}.0refers to the Alpaca
model. The results, shown in Fig. 3, reveal that for
probabilistic ranking, 13K data points are sufficient
for Super NI and LMentry, while Vicuna QA re-
quires 24K data points. We conjecture that this satu-
ration phenomenon can be attributed to two reasons.
First, 52K Alpaca instructions generated by Self-
Instruct algorithm are not diverse enough, as new
instructions are produced by text-davinci-003
using prompt instructions sampled from a limited
seed task pool. Second, instruction tuning itself
may only require a limited amount of data to per-
form behavior cloning, as discussed in Zhou et al.
(2023). Thus, we can further reduce the cost of
probabilistic ranking data generation by half.
The Risks in GPT-4 Evaluation We present evi-
dence that evaluating a model on open QA with the
help of GPT-4 may be risky. Table 4 displays the
ranking length of our proxy ranking model (PRM).
It shows that the PRM has inherited GPT-4 rank-
ing’s bias towards longer outputs (Li et al., 2023).
/uni00000016/uni00000013/uni00000017/uni00000013/uni00000018/uni00000013/uni00000035/uni00000032/uni00000038/uni0000002a/uni00000028/uni00000010/uni0000002f
/uni00000013/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057
/uni00000015/uni00000010/uni00000056/uni0000004b/uni00000052/uni00000057
/uni00000016/uni00000013/uni00000016/uni00000018/uni00000017/uni00000013/uni0000002f/uni00000030/uni00000048/uni00000051/uni00000057/uni00000055/uni0000005c/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048
/uni00000013 /uni00000014/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000015/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000016/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000017/uni00000013/uni00000013/uni00000013/uni00000013 /uni00000018/uni00000013/uni00000013/uni00000013/uni00000013
/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni00000013/uni00000018/uni00000013/uni00000035/uni00000044/uni00000057/uni00000048
/uni0000003a/uni0000004c/uni00000051
/uni0000002f/uni00000052/uni00000056/uni00000048
/uni00000037/uni0000004c/uni00000048Figure 3: The effect of varying the number of proba-
bilistic ranking data on Tuna p.
However, as we discussed in Sec. 3.3, the data
generated by the PRM is not as good as the orig-
inal 13K contextual ranking data, as assessed by
more targeted automatic evaluations like Super NI
and LMentry. Despite the inferior quality of the
PRM-generated data, the performance on Vicuna
QA remains almost unaffected (see Tuna c(PRM)
in Table 1). This observation suggests that evaluat-
ing LLMs on open QA with GPT-4 may not always
be as accurate as it appears, echoing the findings of
Wang et al. (2023b). It highlights the need for more
representative test questions or additional targeted
benchmarks for evaluation.
4 Related Work
Instruction Tuning Instruction tuning aims to
improve the usability of base language models
(Brown et al., 2020; Raffel et al., 2020; Chowd-
hery et al., 2022) by finetuning them on instruction-
response pairs in a zero-shot (Wei et al., 2022) or
few-shot manner (Mishra et al., 2021; Wang et al.,
2022b; Mallen et al., 2023). The instruction data
can be sourced from off-the-shelf NLP benchmarks
(Mishra et al., 2021; Wei et al., 2022; Wang et al.,
2022b) or generated by LLMs (Wang et al., 2022a;
Honovich et al., 2022; Taori et al., 2023; Peng et al.,
2023).
Ranking Loss Learning through re-ranking
sequence-level outputs has been studied in
sequence-to-sequence models (Wiseman and Rush,
2016; Edunov et al., 2018; Liu et al., 2022; Zhang
et al., 2022). BRIO and MoCa algorithms (Liu

--- PAGE 9 ---
et al., 2022; Zhang et al., 2022) adopt a pair-
wise ranking loss to guide the model to gener-
ate summaries with higher ROUGE scores (Lin,
2004). In this paper, we use GPT-4’s (OpenAI,
2023) strong contextual understanding ability and
text-davinci-003 ’s (Ouyang et al., 2022) intrin-
sic probability measures for ranking. In parallel
with our work, Yuan et al. (2023) also propose
pairwise ranking loss for finetuning LLMs. Key
differences include: 1) our pipeline finetuning strat-
egy; 2) our focus on ranking the model’s responses;
3) our use of the original response for cross entropy
regularization, while they select the highest-reward
response. Additionally, Liu et al. (2023c) also em-
ploys GPT models for finetuning BART (Lewis
et al., 2019) on the summarization task.
Pre-Trained Model Evaluation Large pre-
trained models are powerful evaluation metrics
due to their strong contextual understanding abil-
ity, such as BERTScore (Zhang* et al., 2020),
BARTScore (Yuan et al., 2021), MoverScore (Zhao
et al., 2019), COMET (Rei et al., 2020), and
GPTScore (Fu et al., 2023). More recently, there
are more evaluation strategies based on GPT-3.5
and GPT-4 (Liu et al., 2023b; Gao et al., 2023).
5 Conclusion
In this paper, we propose to finetune an instruction-
tuned LLM using our probabilistic ranking ap-
proach ( Tuna p), contextual ranking approach
(Tuna c), and a combination of both (Tuna). Our
comprehensive experiments demonstrate consis-
tent performance improvements across three bench-
marks: Super Natural Instructions (119 test tasks),
LMentry (25 test tasks), and vicuna QA. Further-
more, our methods outperform popular reinforce-
ment learning from human feedback baselines that
rely on the proximal policy optimization algo-
rithm. These findings underscore the effectiveness
of our approach in enhancing the performance of
instruction-tuned LLMs and pave the way for fu-
ture research in this area.
Limitations
Despite the promising results achieved by our Tuna
model, there are several limitations that should be
acknowledged. The first limitation is GPT-4 rank-
ing inconsistency. In our experiments, we relied
on GPT-4 for contextual ranking, which may in-
troduce bias due to the inconsistency in its rank-
ing performance. As a powerful LLM, GPT-4 isgenerally expected to provide accurate and reli-
able rankings; however, it may still be sensitive
to the phrasing or structure of prompts (Dubois
et al., 2023). This inconsistency may lead to sub-
optimal rankings and potentially affect the overall
performance of the Tuna model. In future work, it
would be beneficial to design more robust prompts
that can mitigate the impact of GPT-4’s ranking in-
consistencies. Another limitation is the evaluation
benchmark. In this paper, we evaluated the Tuna
model on three benchmarks, which provided a di-
verse range of tasks and challenges. However, it is
unclear how well the Tuna model would generalize
to other types of tasks, domains, or languages. Fur-
ther research is needed to explore the applicability
of the Tuna model to a broader range of problems
and settings. The last limitation is the reliance
on the use of proprietary LLMs, such as GPT-4
andtext-davinci-003 , for generating responses
and rankings. This dependency may limit the ac-
cessibility and reproducibility of our method for
researchers who do not have access to these pro-
prietary models. Developing alternative methods
that can leverage open-source LLMs or other rank-
ing mechanisms would be a valuable direction for
future research.
Acknowledgements
We would like to thank reviewers for their valuable
feedback. This research/project is supported by
Ministry of Education, Singapore, under its Tier 3
Programme (The Award No.: MOET320200004),
the National Research Foundation Singapore and
DSO National Laboratories under the AI Singa-
pore Program (AISG Award No: AISG2-RP-2020-
016), and Ministry of Education, Singapore, under
its Academic Research Fund (AcRF) Tier 2 Pro-
gramme (MOE AcRF Tier 2 Award No: MOE-
T2EP20122-0011). Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the authors and do not reflect
the views of the Ministry of Education, Singapore.
References
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack

--- PAGE 10 ---
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Advances
in Neural Information Processing Systems .
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebas-
tian Gehrmann, Parker Schuh, Kensen Shi, Sasha
Tsvyashchenko, Joshua Maynez, Abhishek Rao,
Parker Barnes, Yi Tay, Noam M. Shazeer, Vinod-
kumar Prabhakaran, Emily Reif, Nan Du, Benton C.
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier García,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pillai,
Marie Pellat, Aitor Lewkowycz, Erica Moreira, Re-
won Child, Oleksandr Polozov, Katherine Lee, Zong-
wei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz,
Orhan Firat, Michele Catasta, Jason Wei, Kathleen S.
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways. ArXiv , abs/2204.02311.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Mar-
tic, Shane Legg, and Dario Amodei. 2017. Deep
reinforcement learning from human preferences. In
Advances in Neural Information Processing Systems .
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: pre-training of
deep bidirectional transformers for language under-
standing. In North American Chapter of the Associa-
tion for Computational Linguistics .
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,
Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy
Liang, and Tatsunori Hashimoto. 2023. Alpacafarm:
A simulation framework for methods that learn from
human feedback. ArXiv , abs/2305.14387.
Sergey Edunov, Myle Ott, Michael Auli, David Grang-
ier, and Marc’Aurelio Ranzato. 2018. Classical
structured prediction losses for sequence to sequence
learning. In North American Chapter of the Associa-
tion for Computational Linguistics .
Avia Efrat, Or Honovich, and Omer Levy. 2022. Lmen-
try: A language model benchmark of elementary
language tasks. ArXiv , abs/2211.02069.
Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei
Liu. 2023. Gptscore: Evaluate as you desire. ArXiv ,
abs/2302.04166.Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin,
Shiping Yang, and Xiaojun Wan. 2023. Human-
like summarization evaluation with chatgpt. ArXiv ,
abs/2304.02554.
Or Honovich, Thomas Scialom, Omer Levy, and Timo
Schick. 2022. Unnatural instructions: Tuning lan-
guage models with (almost) no human labor.
Yoon Kim and Alexander M. Rush. 2016. Sequence-
level knowledge distillation. In Conference on Em-
pirical Methods in Natural Language Processing .
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdel rahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2019. Bart:
Denoising sequence-to-sequence pre-training for nat-
ural language generation, translation, and compre-
hension. In Annual Meeting of the Association for
Computational Linguistics .
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,
Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and
Tatsunori B. Hashimoto. 2023. Alpacaeval: An au-
tomatic evaluator of instruction-following models.
https://github.com/tatsu-lab/alpaca_eval .
Chin-Yew Lin. 2004. Rouge: A package for automatic
evaluation of summaries. In Annual Meeting of the
Association for Computational Linguistics .
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2023a. Lost in the middle: How language
models use long contexts.
Yang Liu, Dan Iter, Yichong Xu, Shuo Wang, Ruochen
Xu, and Chenguang Zhu. 2023b. G-eval: Nlg evalua-
tion using gpt-4 with better human alignment. ArXiv ,
abs/2303.16634.
Yixin Liu, Alexander R. Fabbri, Pengfei Liu,
Dragomir R. Radev, and Arman Cohan. 2023c. On
learning to summarize with large language models as
references. ArXiv , abs/2305.14239.
Yixin Liu, Pengfei Liu, Dragomir R. Radev, and Graham
Neubig. 2022. Brio: Bringing order to abstractive
summarization. In Annual Meeting of the Association
for Computational Linguistics .
Ilya Loshchilov and Frank Hutter. 2019. Decoupled
weight decay regularization. In International Confer-
ence on Learning Representations .
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,
Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
When not to trust language models: Investigating
effectiveness of parametric and non-parametric mem-
ories. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) .
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and
Hannaneh Hajishirzi. 2021. Cross-task generaliza-
tion via natural language crowdsourcing instructions.

--- PAGE 11 ---
InAnnual Meeting of the Association for Computa-
tional Linguistics .
OpenAI. 2023. GPT-4 Technical Report. ArXiv ,
abs/2303.08774.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Gray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. In Advances in Neural Information
Processing Systems .
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-
ley, and Jianfeng Gao. 2023. Instruction tuning with
gpt-4. ArXiv , abs/2304.03277.
Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt
Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. 2018. Deep contextualized word repre-
sentations. In North American Chapter of the Associ-
ation for Computational Linguistics .
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. Journal of Machine Learning Research .
Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
and Wojciech Zaremba. 2015. Sequence level train-
ing with recurrent neural networks. In International
Conference on Learning Representations .
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon
Lavie. 2020. COMET: A neural framework for MT
evaluation. In Conference on Empirical Methods in
Natural Language Processing .
Victor Sanh, Albert Webson, Colin Raffel, Stephen
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine
Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
M Saiful Bari, Canwen Xu, Urmish Thakker,
Shanya Sharma Sharma, Eliza Szczechla, Taewoon
Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti
Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han
Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong,
Harshit Pandey, Rachel Bawden, Thomas Wang, Tr-
ishala Neeraj, Jos Rozen, Abheesht Sharma, An-
drea Santilli, Thibault Fevry, Jason Alan Fries, Ryan
Teehan, Teven Le Scao, Stella Biderman, Leo Gao,
Thomas Wolf, and Alexander M Rush. 2022. Multi-
task prompted training enables zero-shot task gener-
alization. In International Conference on Learning
Representations .
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Radford, and Oleg Klimov. 2017. Proximal policy
optimization algorithms. ArXiv , abs/1707.06347.Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. 2023. Stanford alpaca:
An instruction-following llama model. https://
github.com/tatsu-lab/stanford_alpaca .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. ArXiv ,
abs/2302.13971.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,
Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,
and Denny Zhou. 2023a. Self-consistency improves
chain of thought reasoning in language models. In
International Conference on Learning Representa-
tions .
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack
Hessel, Tushar Khot, Khyathi Raghavi Chandu,
David Wadden, Kelsey MacMillan, Noah A. Smith,
Iz Beltagy, and Hanna Hajishirzi. 2023b. How far
can camels go? exploring the state of instruction
tuning on open resources.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-
isa Liu, Noah A. Smith, Daniel Khashabi, and Han-
naneh Hajishirzi. 2022a. Self-instruct: Aligning lan-
guage model with self generated instructions. ArXiv ,
abs/2212.10560.
Yizhong Wang, Swaroop Mishra, Pegah Alipoor-
molabashi, Yeganeh Kordi, Amirreza Mirzaei,
Anjana Arunkumar, Arjun Ashok, Arut Selvan
Dhanasekaran, Atharva Naik, David Stap, Eshaan
Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Is-
han Purohit, Ishani Mondal, Jacob Anderson, Kirby
Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar
Pal, M. Moradshahi, Mihir Parmar, Mirali Purohit,
Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,
Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur
Sampat, Savan Doshi, Siddharth Deepak Mishra, Su-
jan Reddy, Sumanta Patro, Tanay Dixit, Xudong
Shen, Chitta Baral, Yejin Choi, Noah A. Smith,
Hanna Hajishirzi, and Daniel Khashabi. 2022b.
Super-naturalinstructions: Generalization via declar-
ative instructions on 1600+ nlp tasks. In Conference
on Empirical Methods in Natural Language Process-
ing.
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M.
Dai, and Quoc V Le. 2022. Finetuned language mod-
els are zero-shot learners. In International Confer-
ence on Learning Representations .
Sam Wiseman and Alexander M. Rush. 2016. Sequence-
to-sequence learning as beam-search optimization. In
Conference on Empirical Methods in Natural Lan-
guage Processing .

--- PAGE 12 ---
Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.
Bartscore: Evaluating generated text as text genera-
tion. In Advances in Neural Information Processing
Systems .
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,
Songfang Huang, and Feiran Huang. 2023. Rrhf:
Rank responses to align language models with human
feedback without tears. ArXiv , abs/2304.05302.
Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.
Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-
uating text generation with bert. In International
Conference on Learning Representations .
Xingxing Zhang, Yiran Liu, Xun Wang, Pengcheng He,
Yang Yu, Si-Qing Chen, Wayne Xiong, and Furu Wei.
2022. Momentum calibration for text generation.
ArXiv , abs/2212.04257.
Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-
tian M. Meyer, and Steffen Eger. 2019. Moverscore:
Text generation evaluating with contextualized em-
beddings and earth mover distance. In Conference on
Empirical Methods in Natural Language Processing .
Yao Zhao, Mikhail Khalman, Rishabh Joshi, Shashi
Narayan, Mohammad Saleh, and Peter J Liu. 2023.
Calibrating sequence likelihood improves conditional
language generation. In International Conference on
Learning Representations .
Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang,
Xipeng Qiu, and Xuanjing Huang. 2020. Extractive
summarization as text matching. In Annual Meeting
of the Association for Computational Linguistics .
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
L. Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke
Zettlemoyer, and Omer Levy. 2023. Lima: Less is
more for alignment. ArXiv , abs/2305.11206.
A The Length Penalty βfor Probabilistic
Ranking Data
In our preliminary experiments, we found that the
length penalty β= 1.3is able to induce detailed re-
sponses and validated this choice on LIMA (Zhou
et al., 2023) dataset. We finetune the βparameter
in Eq. 3 using the LIMA training dataset, which
contains 1030 high-quality expert instruction an-
notations, allowing LLaMA-65B to be finetuned
and achieve remarkably strong performance across
a wide range of topics. Note that the training set
also contains 50 modified Super NI examples but
they are from the training tasks while we test our
models on 119 Super NI test tasks. Specifically, we
first obtain Tuna pmodels with probabilistic rank-
ing data scored with different β. Then, we compute
the token-level negative log-likelihood (NLL) ofthe output of each LIMA instance under differ-
entTuna pmodels and average the token likelihood
over the whole LIMA training set. The results are
shown in Table 5. It can be seen that with β= 1.3,
the model can achieve the best NLL on LIMA train-
ing set. Thus, we set β= 1.3in our experiments.
β 0.9 1.0 1.1 1.2 1.3 1.4
NLL 2.14 2.12 2.11 2.10 2.09 2.09
Table 5: The token-level log-likelihood of LIMA train-
ing set under Tuna pmodels trained with probabilistic
ranking data scored with different β.
B OpenAI API Pricing
We list the cost of calling OpenAI API models in
Table 6.9The human labeling cost per 1K examples
is estimated based on the pricing listed in Dubois
et al. (2023), at 0.25$ for each comparison. For
each data example, there are 4 responses and thus
4∗(4−1)/2 = 6 comparisons. Thus, the human
labor cost per 1K examples is 1500$.
C Training Details of Tuna cpand
mix-Tuna
The hyperparameters are listed in Table 7. For
models finetuned from Alpaca, i.e., Tuna c,Tuna p,
and mix-Tuna, the learning rate is 1e-5. The only
exception is mix-Tuna-104K, whose learning rate
is 5e-6 since it contains 52K duplicate data. For
models finetuned from Tuna corTuna p, i.e., Tuna cp
and Tuna, the learning rate is 1e-6. We use 8 Nvidia
V100-32GB GPUs for all experiments in this paper.
D Other Proxy Ranking Models (PRM)
Similar to the PRM introduced in Sec. 3.1, we
use the 13K GPT-4 ranking data to train another
PRM based on LLaMA-7B, which we refer to as
PRM-7B. We denote the PRM based on StableLM-
3B as PRM-3B. These two ranking models are
employed to re-rank Alpaca’s responses on 52K
instructions. The Alpaca model trained with 52K
data totally generated by the ranking models are
referred to as Tuna c(PRM-3B-52K) and Tuna c
(PRM-7B-52K). Note that Tuna c(PRM-3B-52K)
is the Tuna c(PRM) listed in Table 1. We denote the
Alpaca model trained with 13K GPT-4 contextual
ranking data plus the last 39K data generated by
9https://openai.com/pricing

--- PAGE 13 ---
Data Num Model Price
Probabilistic Ranking 52K text-davinci-003 275$
Contextual Ranking 13K GPT-4-0314 380$
Human 1K - 1500$
Table 6: The estimated cost of calling OpenAI API and human labeling.
LR epoch batch size warmup
Tuna c 1e-5 1 128 2
Tuna p 1e-5 1 128 2
mix-Tuna-52K 1e-5 1 128 2
mix-Tuna-104K 5e-6 1 128 2
Tuna 1e-6 1 128 2
Tuna cp-13K 1e-6 1 128 2
Tuna cp-39K 1e-6 1 128 2
Tuna cp-52K 1e-6 1 128 2
Table 7: The hyperparameters of training different models.
the ranking models as Tuna c(PRM-3B-39K) and
Tuna c(PRM-7B-39K).
The results are listed in Table 8. We can observe
that models trained with ranking data generated
by both PRMS do not achieve better results on Su-
per NI compared to Tuna c. The performances of
Tuna c(PRM-3/7B-39K) is close to Tuna c(PRM-
3/7B-52K), implying that the ranking model have
learned 13K contextual ranking data well. Using
a larger ranking model, such as 7B, does not gain
better performance, which indicates that the rank-
ing ability might not necessarily scale with the
pre-training model’s capacity. In general, the best
strategy is still to learn directly from GPT-4 con-
textual ranking data, which contains less noise.
E Contextual Ranking Prompt
We show the prompt that we use for GPT-4 contex-
tual ranking in Table 9.
F Is the Contextual Ranking Prompt Too
Long?
In (Liu et al., 2023a), the authors found the "lost in
the middle" phenomenon occurs at around 2K (20
documents * 100 tokens/document) tokens for GPT-
4 (note we used GPT-4 in contextual ranking). We
computed the average length of the prompt (includ-
ing four responses and the ranking guidelines) used
in GPT-4 ranking. The average length is 650 tokens,
which is significantly shorter than 2K. Thus, theinput length does not seem to be an issue in GPT-4
ranking. Our human experiments above also con-
firm that the GPT-4 ranking is closely aligned with
human assessments (see Appendix G).
G Human Evaluation of GPT-4 Ranking
We conducted human evaluations of GPT-4 rank-
ings on 50 questions used for contextual ranking.
We asked annotators to rank the four system out-
puts produced by our model and we observe that
the ranking quality by GPT-4 is reasonably good
(the Spearman coefficient between the human rank-
ings and GPT-4 rankings is 0.72). Furthermore, we
also manually inspected the explanations given by
GPT-4 for the ranking results. We found these ex-
planations to be well-reasoned and plausible. Per-
haps this is not surprising given the fact that several
recent papers found GPT can be good evaluators
in multiple NLP tasks (Fu et al., 2023; Gao et al.,
2023). We believe the ranking feedback of this
level is sufficient for guiding our model for better
training (our experiments also proved this).
H Why Not Choose Pairwise Ranking in
GPT-4 Ranking
There are several reasons why ranking 4 responses
together is preferred over pairwise rankings.
1.API cost: Pairwise ranking for four responses
requires (4 * 3) / 2 = 6 API calls, significantly
increasing the total cost. Moreover, a loop

--- PAGE 14 ---
Super NI LMentry Vicuna QA
0-shot 2-shot LMentry Score Win Lose Tie
Alpaca 36.0 44.5 31.4 - - -
Tuna p 39.4 (+3.4) 43.9 (-0.6) 35.0 (+3.6) 68% 27% 5%
Tuna c 37.7 (+1.7) 46.6 (+2.1) 32.2 (+0.8) 74% 20% 6%
Tuna c(PRM-3B-39K) 35.6 (-0.4) 40.4 (-4.1) 33.4 (+2.0) 79% 15% 6%
Tuna c(PRM-7B-39K) 33.5 (-2.5) 40.3 (-4.2) 32.5 (+1.1) 73% 20% 7%
Tuna c(PRM-3B-52K) 34.2 (-1.8) 40.1 (-4.4) 32.2 (+0.8) 75% 19% 6%
Tuna c(PRM-7B-52K) 34.6 (-1.4) 41.1 (-3.4) 32.0 (+0.6) 73% 20% 7%
Tuna 38.7 (+2.7) 45.0 (+0.5) 34.7 (+3.3) 86% 10% 4%
Table 8: Performance comparison of different models. The numbers in bold indicate the top-2 results. The numbers
in parentheses indicate the performance difference compared to Alpaca.
(e.g., R1 > R2, R2 > R3, R3 > R1) could
occur when R1/2/3 are of similar qualities, po-
tentially requiring extra API calls for further
validation.
2.The GPT-4 ranking quality is good enough,
see Appendix G.
I Comparison between Our RLHF
Models and
PPO-sim/PPO-sim-GPT4-20K
We compare our RLHF models and PPO-sim/PPO-
sim-GPT4-20K from Dubois et al. (2023) on Vi-
cuna QA. The results can be found in Table
10. PPO-sim/PPO-sim-GPT4-20K have better re-
sponses and thus we choose to report the results of
their models.
J BERTScore Results on Super NI
By default, ROUGE is employed on Super NI. We
additionally reported BERTScore (which is proven
to be a better alternative for ROUGE) in Table 11
and the results follow the same trends (also see
Table 1).
K Examples
We show some examples in 12.

--- PAGE 15 ---
Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that appropriately completes the request.
### Instruction:
{Instruction }
### Input:
{Input }
### Response:
###Response 0:
{Response 0 }
###Response 1:
{Response 1 }
###Response 2:
{Response 2 }
###Response 3:
{Response 3 }
We would like you to rate Response 0/1/2/3 in reply to the given instruction displayed above.
First, identify if the instruction requires open-ended or close-ended responses.
Second, you need to generate one high quality ‘###Response 4’ in answer to the instruction. It
needs to have the same format as other responses and will be used as a reference later.
Third, identify if there are duplicate responses and keep only one of the duplicate responses for the
following steps.
Fourth, compare Response 4 with Response 0/1/2/3/4 and assign each response an overall score
on a scale of 0 to 15 where a higher score indicates better overall quality. For an open-ended
instruction, please rate based on the relevance (score 0 to 5), level of details/justification: (score 0
to 5) and accuracy (score 0 to 5) of each response; for a close-ended instruction, please rate based
on the accuracy (score 0 to 5), level of details/justification (score 0 to 5) and clarity (score 0 to 5) of
each response. The ratings should have the format: ‘Response k: [ sum of the 3 individual
scores you give to response k ]’.
Last, rank the responses in decreasing order of their overall scores. The ranking should have the
format: ‘rank: [i, j ,k, l, m]’. If there are duplicate responses, keep only one of them in the rank,
that is, the ranking may become: ‘rank: [i, j, k, l]’, ‘rank: [i, j, k]’ ‘rank: [i, j]’ or even ‘rank: [i]’.
Table 9: Contextual Ranking Prompt for GPT-4.

--- PAGE 16 ---
Win Lose Tie
PPO-sim 79% 16% 5%
PPO-sim-GPT4-20K 74% 22% 4%
Our PPO (w/ GPT4 ranking data) 55% 35% 10%
Table 10: Vicuna QA results of our RLHF models and
(Dubois et al., 2023).
MODEL 0-shot 2-shot
Alpaca 0.638 0.705
PPO-sim 0.594 0.652
PPO-sim-GPT4-20K 0.641 0.714
Tuna p 0.667 0.690
Tuna c 0.655 0.727
Tuna 0.665 0.699
Table 11: BERTScore evaluation of the model outputs
in Super NI. Higher is better.

--- PAGE 17 ---
Instruction: "Why is open-source important? Answer in 3-5 sentences"
Alpaca :
Open-source software is important because it allows for collaboration and innovation. It allows
developers to work together to create better software, and it allows users to access the source
code and make changes to the software to meet their own needs. Open-source software also
encourages transparency and allows users to see how the software works and how it can be
improved. Additionally, open-source software is often more secure and reliable than proprietary
software, as it has been tested and reviewed by a large number of developers. Finally, open-source
software is often free or low-cost, making it accessible to everyone.
Tuna c:
Open-source software is important because it allows for collaboration and innovation. It allows
developers to work together to improve existing software and create new software, and it allows
users to view and modify the source code to customize the software to their own needs. Open-source
software also encourages transparency and accountability, as anyone can view and inspect the
code to ensure that it is secure and reliable. Additionally, open-source software is often more
cost-effective than proprietary software, as users can access the source code and modify it to suit
their own needs without having to pay for a license. Finally, open-source software is often more
reliable and secure than proprietary software, as it is developed and tested by a large community of
developers.
Tuna p:
Open-source software is important because it allows developers to access the source code of a
program and make changes to it. This allows developers to customize the software to their needs
and to fix any bugs or errors that may be present. Additionally, open-source software is often free to
use and modify, which makes it more accessible to those who may not be able to afford proprietary
software. Open-source software also allows developers to collaborate and share ideas, which can
lead to the development of better and more efficient software. Finally, open-source software can
be used as a learning tool, as it allows developers to see how the software works and how it is put
together. This can help them to better understand programming and coding in general.
Tuna :
Open-source software is important because it allows developers to collaborate on projects, share
code, and create better products. By making the source code available to the public, developers
can easily collaborate on projects, allowing them to create better products and services. This
collaboration also allows developers to quickly identify and fix bugs, as well as add new features
and functionality. Additionally, open-source software is often more secure than proprietary software,
as it can be more easily audited and reviewed by the public. Finally, open-source software is often
more cost-effective than proprietary software, as it does not require licensing fees or other costs
associated with closed-source software. Open-source software is an important part of the software
industry, and is used by many businesses and individuals.
Table 12: Example responses of different models.

--- PAGE 18 ---
Instruction: "Write a Python function that generates a random number between 1 and 10 (inclusive)."
Listing 1: Alpaca
1def random_number():
2 return random.randint(1, 11)
Listing 2: Tuna c
1def random_number():
2 return random.randint(1, 11)
Listing 3: Tuna p
1"""
2Generate a random number between 1 and 10 (inclusive)
3"""
4
5import random
6
7def random_number():
8 return random.randint(1, 10)
9
10if __name__ == '__main__ ':
11 print(random_number())
Listing 4: Tuna
1"""
2Generate a random number between 1 and 10 (inclusive)
3"""
4
5import random
6
7def random_number():
8 return random.randint(1, 10)
9
10if __name__ == '__main__ ':
11 print(random_number())
Table 13: Example responses of different models.
