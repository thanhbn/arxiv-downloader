# Một Mô Hình Lý Thuyết Tổng Quát để Hiểu về Học Tập từ Sở Thích Con Người

Mohammad Gheshlaghi Azar Mark Rowland Bilal Piot
Daniel Guo Daniele Calandriello Michal Valko Rémi Munos
Google DeepMind

Tóm tắt

Việc triển khai phổ biến của học tập từ sở thích con người thông qua học tăng cường (RLHF) dựa trên hai phép xấp xỉ quan trọng: thứ nhất giả định rằng các sở thích theo cặp có thể được thay thế bằng phần thưởng theo điểm. Thứ hai giả định rằng một mô hình phần thưởng được huấn luyện trên các phần thưởng theo điểm này có thể tổng quát hóa từ dữ liệu thu thập được sang dữ liệu ngoài phân phối được lấy mẫu bởi chính sách. Gần đây, Tối ưu hóa Sở thích Trực tiếp (DPO) đã được đề xuất như một phương pháp bỏ qua phép xấp xỉ thứ hai và học trực tiếp một chính sách từ dữ liệu thu thập mà không cần giai đoạn mô hình hóa phần thưởng. Tuy nhiên, phương pháp này vẫn phụ thuộc nhiều vào phép xấp xỉ thứ nhất.

Trong bài báo này, chúng tôi cố gắng có được hiểu biết lý thuyết sâu sắc hơn về các thuật toán thực tế này. Đặc biệt, chúng tôi suy ra một mục tiêu tổng quát mới gọi là ΨPO để học từ sở thích con người được biểu diễn theo các sở thích theo cặp và do đó bỏ qua cả hai phép xấp xỉ. Mục tiêu tổng quát mới này cho phép chúng tôi thực hiện phân tích sâu về hành vi của RLHF và DPO (như các trường hợp đặc biệt của ΨPO) và xác định các lỗ hổng tiềm ẩn của chúng. Sau đó chúng tôi xem xét một trường hợp đặc biệt khác cho ΨPO bằng cách đặt Ψ đơn giản là Đồng nhất, mà chúng tôi có thể suy ra một quy trình tối ưu hóa hiệu quả, chứng minh các đảm bảo hiệu suất và chứng minh tính ưu việt thực nghiệm của nó so với DPO trên một số ví dụ minh họa.

Đang được đánh giá.

1 Giới thiệu

Học tập từ sở thích con người (Christiano et al., 2017) là một mô hình được áp dụng trong tài liệu xử lý ngôn ngữ tự nhiên để căn chỉnh tốt hơn các mô hình ngôn ngữ sinh sẵn được huấn luyện trước (Radford et al., 2018; Ramachandran et al., 2016) và được điều chỉnh theo hướng dẫn (Wei et al., 2022) với mong muốn của con người. Nó bao gồm việc đầu tiên thu thập lượng lớn dữ liệu trong đó mỗi dữ liệu được cấu thành từ một ngữ cảnh, các cặp phần tiếp theo của ngữ cảnh, cũng được gọi là các thế hệ, và một sở thích con người theo cặp chỉ ra thế hệ nào là tốt nhất. Sau đó, một chính sách tạo ra các thế hệ tốt cho một ngữ cảnh được học từ dữ liệu thu thập. Chúng tôi đóng khung bài toán học tập từ sở thích con người như một bài toán bandit ngữ cảnh ngoại tuyến (Lu et al., 2010). Mục tiêu của bài toán bandit này là cho một ngữ cảnh để chọn một hành động (đóng vai trò của thế hệ) được con người đánh giá ưa thích nhất dưới ràng buộc rằng chính sách bandit kết quả nên gần với một chính sách tham chiếu đã biết nào đó. Ràng buộc giữ gần với một chính sách tham chiếu đã biết có thể được thỏa mãn ví dụ bằng cách sử dụng điều hòa KL (Geist et al., 2019) và vai trò của nó là tránh độ lệch mô hình (Lazaridou et al., 2020; Lu et al., 2020).

Một phương pháp nổi bật để giải quyết bài toán học tập từ sở thích con người là thông qua học tăng cường từ phản hồi con người (RLHF, Ouyang et al., 2022; Stiennon et al., 2020) trong đó đầu tiên một mô hình phần thưởng được huấn luyện dưới dạng một bộ phân loại các hành động được ưa thích và không được ưa thích. Sau đó chính sách bandit được huấn luyện thông qua RL để tối đa hóa mô hình phần thưởng học được này trong khi tối thiểu hóa khoảng cách với chính sách tham chiếu. Gần đây RLHF đã được sử dụng thành công trong việc giải quyết bài toán căn chỉnh các mô hình ngôn ngữ sinh với sở thích con người (Ouyang et al., 2022). Hơn nữa, các công trình gần đây như tối ưu hóa sở thích trực tiếp (DPO, Rafailov et al., 2023) và (SLiC-HF, Zhao et al., 2023) đã cho thấy rằng có thể tối ưu hóa chính sách bandit trực tiếp từ sở thích con người mà không cần học một mô hình phần thưởng. Họ cũng đã cho thấy rằng trên một lựa chọn các nhiệm vụ ngôn ngữ tiêu chuẩn, chúng có khả năng cạnh tranh với RLHF hiện đại nhất trong khi chúng đơn giản hơn để triển khai và cần ít tài nguyên hơn.

Mặc dù thành công thực tế này, ít được biết về nền tảng lý thuyết của các phương pháp thực tế này. Các ngoại lệ đáng chú ý, xem xét các trường hợp đặc biệt cụ thể, là (Wang et al., 2023; Chen et al., 2022) và công trình trước đó về bandit và RL dựa trên sở thích (Busa-Fekete et al., 2014, 2013) và đấu đôi (Novoseller et al., 2020; Pacchiano et al., 2023). Tuy nhiên, các công trình lý thuyết này tập trung vào việc cung cấp đảm bảo lý thuyết theo các ràng buộc hối tiếc trong bối cảnh bandit tiêu chuẩn và chúng không giải quyết bối cảnh thực tế của RLHF, DPO và SLiC-HF.

Trong công trình này, trọng tâm của chúng tôi là thu hẹp khoảng cách giữa lý thuyết và thực tế bằng cách giới thiệu một biểu diễn lý thuyết đơn giản và tổng quát của các thuật toán thực tế để học từ sở thích con người. Đặc biệt, chúng tôi cho thấy rằng có thể đặc trưng các hàm mục tiêu của RLHF và DPO như các trường hợp đặc biệt của một mục tiêu tổng quát hơn được biểu diễn độc quyền theo các sở thích theo cặp. Chúng tôi gọi mục tiêu này là mục tiêu tối ưu hóa sở thích Ψ (ΨPO), trong đó Ψ là một ánh xạ không giảm tùy ý. Sau đó chúng tôi phân tích hàm mục tiêu này trong các trường hợp đặc biệt của RLHF và DPO và điều tra các lỗ hổng tiềm ẩn của nó. Điều tra lý thuyết của chúng tôi về RLHF và DPO tiết lộ rằng về nguyên tắc cả hai đều có thể dễ bị overfitting. Điều này là do thực tế rằng các phương pháp đó dựa trên giả định mạnh rằng các sở thích theo cặp có thể được thay thế bằng điểm ELo (phần thưởng theo điểm) thông qua mô hình hóa Bradley-Terry (BT) (Bradley and Terry, 1952). Đặc biệt, giả định này có thể có vấn đề khi các sở thích (được lấy mẫu) là xác định hoặc gần như xác định vì nó dẫn đến overfitting với tập dữ liệu sở thích với cái giá bỏ qua thuật ngữ điều hòa KL (xem Sec. 4.2). Sau đó chúng tôi trình bày một giải pháp đơn giản để tránh vấn đề overfitting, cụ thể là bằng cách đặt Ψ thành đồng nhất trong ΨPO. Phương pháp này được gọi là Identity-PO (IPO) và theo cấu trúc bỏ qua giả định mô hình hóa BT cho các sở thích (xem Sec. 5). Cuối cùng, chúng tôi đề xuất một giải pháp thực tế, thông qua một hàm mất mát được lấy mẫu (xem Sec. 5.2), để tối ưu hóa phiên bản đơn giản hóa này của ΨPO một cách thực nghiệm và chúng tôi so sánh hiệu suất của nó với DPO trên các ví dụ bandit đơn giản, cung cấp hỗ trợ thực nghiệm cho các phát hiện lý thuyết của chúng tôi (xem Sec. 5.3 và Sec. 5.4).

2 Ký hiệu

Trong phần còn lại, chúng tôi xây dựng dựa trên các ký hiệu của DPO (Rafailov et al., 2023). Cho một ngữ cảnh x trong X, trong đó X là không gian hữu hạn của các ngữ cảnh, chúng tôi giả định một không gian hành động hữu hạn Y. Một chính sách π trong ΔX Y liên kết với mỗi ngữ cảnh x trong X một phân phối xác suất rời rạc π(.|x) trong ΔY trong đó ΔY là tập hợp các phân phối rời rạc trên Y. Chúng tôi ký hiệu μ trong ΔX Y là chính sách hành vi. Từ một ngữ cảnh x cho trước, hãy y, y′∼μ(x) là hai hành động được tạo ra độc lập bởi chính sách tham chiếu. Chúng sau đó được trình bày cho các người đánh giá con người những người thể hiện sở thích cho một trong các thế hệ, được ký hiệu là yw≻yl trong đó yw và yl ký hiệu các hành động được ưa thích và không được ưa thích trong số {y, y′} tương ứng.

Sau đó chúng tôi viết sở thích con người thực p*(y≻y′|x) là xác suất của y được ưa thích hơn y′ biết ngữ cảnh x. Xác suất đến từ tính ngẫu nhiên của việc lựa chọn con người mà chúng tôi hỏi về sở thích của họ. Vậy p*(y≻y′|x) = Eh[I{h ưa thích y hơn y′ cho x}], trong đó kỳ vọng là trên con người h. Chúng tôi cũng giới thiệu sở thích kỳ vọng của một thế hệ y trên một phân phối μ biết x, được ghi là p*(y≻μ|x), thông qua phương trình sau:

p*(y≻μ|x) = E y′∼μ(.|x)[p*(y≻y′|x)].

Đối với bất kỳ hai chính sách π, μ trong ΔX Y và một phân phối ngữ cảnh ρ chúng tôi ký hiệu tổng sở thích của chính sách π so với μ là

p*ρ(π≻μ|x) = Ex∼ρ y∼π(.|x)[p*(y≻μ|x)].

Trong thực tế, chúng tôi không quan sát p* trực tiếp, mà các mẫu I(y, y′|x) từ một phân phối Bernoulli với trung bình p*(y≻y′|x) (tức là, I(y, y′|x) là 1 với xác suất p*(y≻y′|x) và 0 ngược lại). Đặc biệt, chúng tôi giả định chúng tôi có quyền truy cập vào các sở thích thông qua một tập dữ liệu các thế hệ được đánh giá D = (xi, yi, y′i)Ni=1 = (xi, yw,i≻yl,i)Ni=i, trong đó N là kích thước tập dữ liệu. Ngoài ra, đối với một tập hữu hạn tổng quát S, một phân phối xác suất rời rạc η trong ΔS và một hàm thực f trong RS, chúng tôi ghi kỳ vọng của f dưới η là Es∼η[f(s)] = Σs trong S f(s)η(s). Đối với một tập dữ liệu hữu hạn D = (si)Ni=1, với si trong S cho mỗi i, và một hàm thực f trong RS, chúng tôi ký hiệu kỳ vọng thực nghiệm của f dưới D là Es∼D[f(s)] = 1/N ΣNi=1 f(si).

3 Kiến thức nền

3.1 Học Tăng cường từ Phản hồi Con người (RLHF)

Mô hình RLHF tiêu chuẩn (Christiano et al., 2017; Stiennon et al., 2020) bao gồm hai giai đoạn chính: (i) học mô hình phần thưởng; (ii) tối ưu hóa chính sách sử dụng phần thưởng đã học. Ở đây chúng tôi cung cấp một bản tóm tắt về các giai đoạn này.

3.1.1 Học Mô hình Phần thưởng

Học một mô hình phần thưởng bao gồm việc huấn luyện một bộ phân loại nhị phân để phân biệt giữa các hành động được ưa thích và không được ưa thích sử dụng mất mát hồi quy logistic. Đối với bộ phân loại, một lựa chọn phổ biến là mô hình Bradley-Terry: đối với một ngữ cảnh x và hành động y cho trước, chúng tôi ký hiệu phần thưởng theo điểm, cũng có thể được hiểu như một điểm Elo, của y cho x bằng r(x, y). Mô hình Bradley-Terry biểu diễn hàm sở thích p(y≻y′|x) (bộ phân loại) như một sigmoid của hiệu các phần thưởng:

p(y≻y′|x) = σ(r(x, y) − r(x, y′)), (1)

trong đó σ(·) ký hiệu hàm sigmoid và đóng vai trò chuẩn hóa. Cho tập dữ liệu D = (xi, yw,i≻yl,i)Ni=1 người ta có thể học hàm phần thưởng bằng cách tối ưu hóa mất mát hồi quy logistic sau

L(r) = −E(x,yw,yl)∼D[log (p(yw≻yl|x))]. (2)

Giả định rằng p*(y≻y′|x) tuân theo mô hình Bradley-Terry, người ta có thể chỉ ra rằng khi kích thước của tập dữ liệu D tăng, p(y≻y′|x) trở thành ước lượng ngày càng chính xác hơn của p*(y≻y′|x) thực và trong giới hạn hội tụ về p*(y≻y′|x).

3.1.2 Tối ưu hóa Chính sách với Phần thưởng Đã học

Sử dụng phần thưởng (điểm Elo) r(x, y), mục tiêu RLHF đơn giản là tối ưu hóa cho chính sách π trong ΔX Y tối đa hóa phần thưởng kỳ vọng trong khi tối thiểu hóa khoảng cách giữa π và một chính sách tham chiếu πref trong ΔX Y thông qua hàm mục tiêu được điều hòa KL sau:

J(π) = Eπ[r(x, y)] − τDKL(π||πref), (3)

trong đó ngữ cảnh x được rút từ ρ và hành động y được rút từ π(.|x). Độ phân kỳ DKL(π||πref) được định nghĩa như sau:

DKL(π||πref) = Ex∼ρ[KL(π(.|x)||πref(.|x))].

trong đó:

KL(π(.|x)||πref(.|x)) = Ey∼π(.|x)[log π(y|x)/πref(y|x)].

Mục tiêu trong Phương trình (3) về cơ bản được tối ưu hóa bởi PPO (Schulman et al., 2017) hoặc các phương pháp tương tự. Sự kết hợp của RLHF + PPO đã được sử dụng với thành công lớn trong thực tế (ví dụ, InsturctGPT và GPT-4 Ouyang et al., 2022; OpenAI, 2023).

3.2 Tối ưu hóa Sở thích Trực tiếp

Một phương pháp thay thế cho mô hình RL được mô tả ở trên là tối ưu hóa sở thích trực tiếp (DPO; Rafailov et al., 2023), tránh hoàn toàn việc huấn luyện mô hình phần thưởng. Mất mát mà DPO tối ưu hóa, cho một tập dữ liệu thực nghiệm D, như một hàm của π, được cho bởi

min π E(x,yw,yl)∼D[-log σ(τ log π(yw|x)/π(yl|x) - τ log πref(yw|x)/πref(yl|x))]. (4)

Trong dạng quần thể của nó, mất mát có dạng

min π Ex∼ρ y,y′∼μ[-p*(y≻y′|x) log σ(τ log π(y|x)/π(y′|x) - τ log πref(y|x)/πref(y′|x))]. (5)

Rafailov et al. (2023) chỉ ra rằng khi (i) mô hình Bradley-Terry trong Phương trình (1) phù hợp hoàn hảo với dữ liệu sở thích và (ii) hàm phần thưởng tối ưu r được thu được từ mất mát trong Phương trình (2), thì các bộ tối ưu toàn cục của mục tiêu RLHF trong Phương trình (3) và mục tiêu DPO trong Phương trình (5) hoàn toàn trùng khớp. Thực tế, sự tương ứng này đúng một cách tổng quát hơn; xem Mệnh đề 4 trong Phụ lục B.

4 Một Mục tiêu Tổng quát cho Tối ưu hóa Sở thích

Một đóng góp khái niệm trung tâm của bài báo là đề xuất một mục tiêu tổng quát cho RLHF, dựa trên việc tối đa hóa một hàm phi tuyến của các sở thích. Để đạt được điều này, chúng tôi xem xét một hàm không giảm tổng quát Ψ : [0,1] → R, một chính sách tham chiếu πref trong ΔX Y, và một tham số điều hòa thực dương τ trong R*+, và định nghĩa mục tiêu tối ưu hóa sở thích Ψ (ΨPO) là

max π Ex∼ρ y∼π(.|x) y′∼μ(.|x)[Ψ(p*(y≻y′|x))] − τDKL(π||πref). (6)

Mục tiêu này cân bằng việc tối đa hóa một hàm có thể phi tuyến của các xác suất sở thích với thuật ngữ điều hòa KL khuyến khích các chính sách gần với tham chiếu πref. Điều này được thúc đẩy bởi dạng của Phương trình (3), và chúng ta sẽ thấy trong phần tiếp theo rằng nó tổng quát hóa nghiêm ngặt cả RLHF và DPO, khi mô hình BT áp dụng.

4.1 Phân tích Sâu hơn về DPO và RLHF

Trong phần còn lại, chúng tôi bỏ qua sự phụ thuộc vào x để dễ ký hiệu. Điều này không mất tính tổng quát và tất cả các kết quả sau đây đều đúng cho tất cả x trong Supp(ρ). Trước tiên chúng tôi kết nối DPO và RLHF với mục tiêu sở thích Ψ trong Phương trình (6), dưới lựa chọn đặc biệt của Ψ(q) = log(q/(1−q)). Chính xác hơn, mệnh đề sau thiết lập kết nối này.

Mệnh đề 1. Giả sử Ψ(q) = log(q/(1−q)). Khi mô hình Bradley-Terry áp dụng cho p*, nghĩa là, tồn tại r: Y → R sao cho

p*(y≻y′) = σ(r(y) − r(y′)),

thì chính sách tối ưu cho Phương trình (6), cho mục tiêu RLHF trong Phương trình (3), và cho mục tiêu DPO tiêu chuẩn trong Phương trình (5) là giống hệt nhau.

Chứng minh. Lưu ý rằng dưới giả định mô hình Bradley-Terry áp dụng, chúng ta có

E y′∼μ[Ψ(p*(y≻y′))] = E y′∼μ[Ψ(er(y)/(er(y) + er(y′)))]
= E y′∼μ[log(er(y)/er(y′))]
= E y′∼μ[r(y) − r(y′)]
= r(y) − E y′∼μ[r(y′)].

Điều này bằng phần thưởng trong Phương trình (3), tới một hằng số cộng, và do đó nó theo sau rằng chính sách tối ưu cho Phương trình (6) và cho việc tối ưu hóa mục tiêu trong Phương trình (3) là giống hệt nhau. Hơn nữa, như được chỉ ra bởi Rafailov et al. (2023), chính sách tối ưu cho mục tiêu DPO trong Phương trình (5) và mục tiêu trong Phương trình (3) là giống hệt nhau, điều này đưa ra khẳng định của mệnh đề.

Áp dụng mệnh đề này vào hàm mục tiêu của Phương trình (6), mà có một giải pháp phân tích, tiết lộ rằng dưới giả định BT, giải pháp dạng đóng cho DPO và RLHF có thể được viết là

π*(y) ∝ πref(y) exp(τ−1 E y′∼μ[Ψ(p*(y≻y′))]). (7)

Các suy dẫn dẫn đến Phương trình 7 là một kết quả nổi tiếng và được cung cấp trong App. A.1 để hoàn chỉnh.

4.2 Điều hòa Yếu và Overfitting

Đáng để lùi lại một bước và hỏi rằng các loại chính sách nào mà mục tiêu trên dẫn chúng ta khám phá. Phép biến đổi phi tuyến cao này của các xác suất sở thích có nghĩa là những tăng nhỏ trong xác suất sở thích đã gần 1 được khuyến khích giống như những tăng lớn hơn trong xác suất sở thích xung quanh 50%, điều này có thể không mong muốn. Việc tối đa hóa logit-preferences, hoặc điểm Elo trong thuật ngữ lý thuyết trò chơi, cũng có thể có những tác động phản trực quan, ngay cả trong các thiết lập bắc cầu (Bertrand et al., 2023).

Xem xét ví dụ đơn giản trong đó chúng ta có hai hành động y và y′ sao cho p*(y≻y′) = 1, tức là, y luôn được ưa thích hơn y′. Khi đó mô hình Bradley-Terry sẽ yêu cầu rằng (r(y) − r(y′)) → +∞ để thỏa mãn (1). Nếu chúng ta cắm điều này vào chính sách tối ưu (7) thì chúng ta sẽ có rằng π*(y′)/π*(y) = 0 (tức là, π*(y′) = 0) bất kể hằng số τ nào được sử dụng cho điều hòa KL. Vì vậy sức mạnh của điều hòa KL trở nên yếu hơn và yếu hơn khi các sở thích càng xác định.

Sự yếu kém của điều hòa KL trở nên rõ ràng hơn trong chế độ dữ liệu hữu hạn, trong đó chúng ta chỉ có quyền truy cập vào một ước lượng mẫu của sở thích p̂(y≻y′). Ngay cả khi sở thích thực là, ví dụ, p*(y≻y′) = 0.8, thực nghiệm có thể rất có thể khi chúng ta chỉ có một vài điểm dữ liệu để ước lượng p̂(y≻y′) = 1, trong trường hợp này chính sách tối ưu thực nghiệm sẽ làm π(y′) = 0 cho bất kỳ τ nào. Điều này có nghĩa là overfitting có thể là một vấn đề thực nghiệm đáng kể, đặc biệt khi không gian ngữ cảnh và hành động cực kỳ lớn như đối với các mô hình ngôn ngữ lớn.

Tại sao RLHF tiêu chuẩn có thể mạnh mẽ hơn đối với vấn đề này trong thực tế? Trong khi một lợi thế được cho là của DPO là nó tránh được nhu cầu phù hợp với một hàm phần thưởng, chúng tôi quan sát rằng trong thực tế khi các xác suất sở thích thực nghiệm ở trong tập {0,1}, hàm phần thưởng cuối cùng bị dưới phù hợp. Các phần thưởng tối ưu trong sự hiện diện của xác suất sở thích {0,1} là vô hạn, nhưng những giá trị này được tránh, và thực sự điều hòa của hàm phần thưởng đã được quan sát là một khía cạnh quan trọng của huấn luyện RLHF trong thực tế (Christiano et al., 2017). Việc dưới phù hợp này của hàm phần thưởng do đó rất quan trọng trong việc thu được một chính sách cuối cùng được điều hòa đủ về phía chính sách tham chiếu πref, và DPO, trong việc tránh huấn luyện hàm phần thưởng, mất đi việc điều hòa chính sách mà hàm phần thưởng dưới phù hợp mang lại.

Trong khi các thực hành thực nghiệm tiêu chuẩn như dừng sớm vẫn có thể được sử dụng như một dạng điều hòa bổ sung để ngăn chặn loại overfitting này, trong phần tiếp theo, chúng tôi sẽ giới thiệu một sửa đổi của mục tiêu ΨPO sao cho chính sách thực nghiệm tối ưu có thể gần πref ngay cả khi các sở thích là xác định.

5 IPO: ΨPO với ánh xạ đồng nhất

Chúng tôi đã quan sát trong phần trước rằng DPO dễ bị overfitting, và điều này xuất phát từ sự kết hợp của tính không bị chặn của Ψ, cùng với việc không huấn luyện một hàm phần thưởng rõ ràng. Không huấn luyện hàm phần thưởng trực tiếp là một lợi thế rõ ràng của DPO, nhưng chúng tôi muốn tránh các vấn đề của overfitting. Phân tích này của DPO thúc đẩy các lựa chọn của Ψ bị chặn, đảm bảo rằng điều hòa KL trong Phương trình 6 vẫn hiệu quả ngay cả trong chế độ các sở thích có giá trị {0,1}, như thường xảy ra khi làm việc với các tập dữ liệu thực nghiệm. Một dạng mục tiêu đặc biệt tự nhiên để xem xét được đưa ra bằng cách lấy Ψ là ánh xạ đồng nhất trong Phương trình (6), dẫn đến tối ưu hóa điều hòa trực tiếp của tổng sở thích:

max π p*ρ(π≻μ) − τDKL(π||πref). (8)

Phương pháp tiêu chuẩn để tối ưu hóa hàm mục tiêu của Phương trình (8) là thông qua RLHF với lựa chọn phần thưởng r(y) = p*(y≻μ). Tuy nhiên cả việc sử dụng RL và ước lượng mô hình phần thưởng r(y) đều có thể tốn kém. Lấy cảm hứng từ DPO, người ta muốn tạo ra một giải pháp thực nghiệm cho bài toán tối ưu hóa của Phương trình (8) có thể học trực tiếp từ tập dữ liệu sở thích. Vì vậy nó sẽ có thể tránh hoàn toàn RL và mô hình hóa phần thưởng.

5.1 Suy dẫn và Thuật toán Hiệu quả Tính toán

Như với DPO, sẽ có lợi khi biểu diễn lại Phương trình (8) như một mục tiêu học ngoại tuyến. Để suy ra biểu thức như vậy, chúng tôi bắt đầu bằng việc theo suy dẫn của Rafailov et al. (2023), thao tác biểu thức phân tích cho chính sách tối ưu thành một hệ thống các bài toán tìm nghiệm. Như trong phần trước, chúng tôi bỏ phụ thuộc vào ngữ cảnh x từ ký hiệu của chúng tôi, vì tất cả các đối số có thể được áp dụng trên cơ sở từng ngữ cảnh.

Bài toán tìm nghiệm. Đặt g(y) = E y′∼μ[Ψ(p*(y≻y′))]. Khi đó chúng ta có

π*(y) ∝ πref(y) exp(τ−1g(y)). (9)

Đối với bất kỳ y, y′ trong Supp(πref), do đó chúng ta có

π*(y)/π*(y′) = πref(y)/πref(y′) exp(τ−1(g(y) − g(y′))). (10)

Bằng cách đặt

h*(y, y′) = log π*(y)πref(y′)/(π*(y′)πref(y))

và sắp xếp lại Phương trình (10), chúng ta thu được

h*(y, y′) = τ−1(g(y) − g(y′)). (11)

Ý tưởng cốt lõi bây giờ là xem xét một chính sách π, định nghĩa

hπ(y, y′) = log π(y)πref(y′)/(π(y′)πref(y)),

và nhắm giải các phương trình:

hπ(y, y′) = τ−1(g(y) − g(y′)). (12)

Mất mát cho IPO. Bây giờ chúng tôi tách khỏi phương pháp phân tích được sử dụng bởi Rafailov et al. (2023), để thu được một công thức ngoại tuyến mới của Phương trình (6), trong trường hợp cụ thể của Ψ như hàm đồng nhất. Trong trường hợp này, Phương trình (12) giảm thành

hπ(y, y′) = τ−1(p*(y≻μ) − p*(y′≻μ)).

Chúng tôi bắt đầu bằng việc biểu diễn lại các bài toán tìm nghiệm này như một bài toán tối ưu hóa duy nhất L(π):

L(π) = E y,y′∼μ[(hπ(y, y′) − (p*(y≻μ) − p*(y′≻μ))/τ)2]. (13)

Người ta có thể dễ dàng chỉ ra rằng đối với lựa chọn π* chúng ta có L(π*) = 0. Vì vậy π* là một bộ tối thiểu toàn cục của L(π). Định lý sau thiết lập tính duy nhất của giải pháp này.

Định lý 2 (Tính duy nhất của Tối ưu Toàn cục/Cục bộ). Giả sử Supp(μ) = Supp(πref) và định nghĩa Π là tập các chính sách π sao cho Supp(π) = Supp(μ). Khi đó π ↦ L(π) có một tối thiểu cục bộ/toàn cục duy nhất trong Π, đó là π*.

Chứng minh. Theo giả định, π* trong Π, và theo định nghĩa đối với tất cả π trong Π, L(π) ≥ 0 vì L(π) là một kỳ vọng của các số hạng bình phương. Hơn nữa, từ Phương trình (11), nó theo ngay rằng L(π*) = 0, và vậy chúng ta suy ra rằng π* là một tối ưu toàn cục cho L. Bây giờ chúng ta chỉ ra rằng không có tối thiểu cục bộ/toàn cục nào khác cho L trong Π.

Chúng ta viết J = Supp(μ). Chúng ta tham số hóa tập Π thông qua các vector logit s trong RJ, đặt πs(y) = exp(s(y))/Σy′ trong J exp(s(y′)) cho y trong J, và πs(y) = 0 ngược lại. Hãy viết L(s) = L(πs) cho mục tiêu như một hàm của logit s.

L(s) = E y,y′∼μ[(p*(y≻μ) − p*(y′≻μ))/τ (14)
− (s(y) − s(y′)) − log πref(y′)/πref(y)]2.

Mục tiêu là bậc hai như một hàm của logit s. Hơn nữa, bằng cách mở rộng bậc hai ở trên, chúng ta thấy rằng mất mát có thể được biểu diễn như một tổng các bình phương

Σy,y′ trong J μ(y)μ(y′)(s(y) − s(y′))2 (15)

cộng các số hạng tuyến tính và hằng số. Do đó đây là một bậc hai nửa xác định dương, và do đó là lồi. Vì vậy chúng ta suy ra rằng tất cả các bộ tối thiểu cục bộ của mất mát L(s) cũng là các bộ tối thiểu toàn cục (Boyd và Vandenberghe, 2004, Chap. 4). Bây giờ chúng ta nhận thấy vì πs là một ánh xạ liên tục toàn ánh từ s tới một người ta có thể dễ dàng chỉ ra từ định nghĩa của tối thiểu cục bộ rằng mọi bộ tối thiểu cục bộ π của L tương ứng với một tập các bộ tối thiểu cục bộ Sπ của L. Vì vậy tất cả các tối thiểu cục bộ của L cũng là các tối thiểu toàn cục.

Cuối cùng, hướng duy nhất s mà bậc hai trong Phương trình (15) không tăng ra khỏi 0 là khi tất cả các số hạng trong ngoặc vẫn là 0; nghĩa là, theo hướng (1, . . . , 1) trong RJ. Vì vậy, L(s) là lồi nghiêm ngặt, ngoại trừ theo hướng (1, . . . , 1). (Boyd và Vandenberghe, 2004, Chap. 3). Tuy nhiên, sửa đổi logit theo hướng e = (1, . . . , 1) không sửa đổi chính sách kết quả πs, vì, đối với y trong J,

πs+λe(y) = es(y)+λ/Σy′ trong J es(y′)+λ = es(y)/Σy′ trong J es(y′) = πs(y).

Tính lồi nghiêm ngặt kết hợp với thực tế rằng π* là một tối thiểu toàn cục chứng minh rằng π* là tối thiểu toàn cục/cục bộ duy nhất trong Π (Boyd và Vandenberghe, 2004, Chap. 4).

5.2 Mất mát Được lấy mẫu cho IPO

Để thu được mất mát được lấy mẫu cho IPO, chúng ta cần chỉ ra rằng chúng ta có thể xây dựng một ước lượng không thiên vị của vế phải của phương trình (13). Để đạt được điều này, chúng tôi xem xét Mất mát IPO Quần thể:

E y,y′∼μ[(hπ(y, y′) − τ−1I(y, y′))2], (16)

trong đó I(y, y′) được rút từ một phân phối Bernoulli với trung bình p*(y≻y′), tức là, I(y, y′) là 1 nếu y được ưa thích hơn y′ (điều này xảy ra với xác suất p*(y≻y′)), và 0 ngược lại. Điều này một cách đơn giản tạo ra một mất mát dựa trên mẫu có thể được sử dụng, bằng cách lấy mẫu một cặp (y, y′) từ tập dữ liệu sở thích, và tham khảo sở thích được ghi lại để thu được một mẫu từ I(y, y′). Mệnh đề sau biện minh cho việc chuyển từ Phương trình (13) sang Phương trình (16), bằng cách chứng minh sự bằng nhau của chúng.

Mệnh đề 3. Các biểu thức trong Phương trình (13) và Phương trình (16) bằng nhau, tới một hằng số cộng độc lập với π.

Chứng minh. Sự tương đương này không hoàn toàn tầm thường, vì nói chung kỳ vọng có điều kiện

E[hπ(Y, Y′) − τ−1I(Y, Y′)|Y = y, Y′ = y′]

không bằng đại lượng tương ứng xuất hiện trong Phương trình (13), cụ thể là

hπ(y, y′) − τ−1(p*(y≻μ) − p*(y′≻μ)).

Thay vào đó chúng ta cần khai thác một số đối xứng giữa các phân phối của y và y′, và sử dụng thực tế rằng hπ(y, y′) phân tách như một hàm cộng của y và y′. Để chỉ ra sự bằng nhau này của các mất mát, đủ để tập trung vào các "số hạng chéo" thu được khi mở rộng các bậc hai trong Phương trình (13) và (16); nghĩa là, để chỉ ra

E y,y′∼μ[hπ(y, y′)I(y, y′)] = E y,y′∼μ[hπ(y, y′)(p*(y≻μ) − p*(y′≻μ))].

Bây giờ, bắt đầu với vế phải, và sử dụng ký hiệu tắt πy = log(π(y)), πRy = log(πref(y)), py = p*(y≻μ), và tương tự cho y′, chúng ta có

E y,y′∼μ[hπ(y, y′)(p*(y≻μ) − p*(y′≻μ))]
= E y,y′∼μ[(πy − πy′ + πRy′ − πRy)(py − py′)]
= E y,y′∼μ[πypy − πypy′ − πy′py + πy′py′ + πRy′py − πRy′py′ − πRypy + πRypy′]
= E y,y′∼μ[(2py − 1)πy − (2py − 1)πRy],

trong đó chúng ta đã sử dụng tính độc lập của y và y′, và Ey∼μ[py] = 1/2. Chuyển sang vế trái, chúng ta có

E y,y′∼μ[hπ(y, y′)I(y, y′)]
= E y,y′∼μ[(πy − πy′ + πRy′ − πRy)I(y, y′)]
= E y∼μ[(πy − πRy)E y′∼μ[I(y, y′)|y]] + E y′∼μ[(-πy′ + πRy′)E y∼μ[I(y, y′)|y′]]
= E y,y′∼μ[πypy − πy′(1 − py′) + πRy′(1 − py′) − πRypy]
= E y,y′∼μ[(2py − 1)πy − (2py − 1)πRy],

trong đó chúng ta sử dụng thực tế rằng Ey′∼μI(y, y′) = py và Ey∼μI(y, y′) = 1 − py′. Điều này chứng minh sự bằng nhau của các mất mát, như yêu cầu.

Bây giờ chúng tôi thảo luận về cách xấp xỉ mất mát trong Phương trình (16) với một tập dữ liệu thực nghiệm. Như trong thảo luận trước đó của chúng tôi, tập dữ liệu thực nghiệm D có dạng (yw,i, yl,i)Ni=i. Lưu ý rằng mỗi điểm dữ liệu (yw,i, yl,i) đóng góp hai số hạng vào một xấp xỉ thực nghiệm của Phương trình (16), với (y, y′, I(y, y′)) = (yw,i, yl,i, 1), và cũng (y, y′, I(y, y′)) = (yl,i, yw,i, 0). Sự đối xứng này quan trọng để khai thác, và dẫn đến giảm phương sai của mất mát. Do đó mất mát thực nghiệm tổng thể được cho bởi

1/2 E(yw,yl)∼D[(hπ(yw, yl) − τ−1)2 + hπ(yl, yw)2]
= 1/2 E(yw,yl)∼D[(hπ(yw, yl) − τ−1)2 + hπ(yw, yl)2],

mà tới một hằng số bằng:

E(yw,yl)∼D[(hπ(yw, yl) − τ−1/2)2]. (17)

Dạng đơn giản hóa này của mất mát cung cấp một số hiểu biết có giá trị về cách IPO tối ưu hóa chính sách π: IPO học từ tập dữ liệu sở thích đơn giản bằng cách hồi quy khoảng cách giữa các tỷ lệ log-likelihood log(π(yw)/π(yl)) và log(πref(yw)/πref(yl)) về τ−1/2. Vậy điều hòa càng yếu, tỷ lệ log-likelihood của yw so với yl càng cao. Nói cách khác IPO, không giống như DPO, luôn điều hòa giải pháp của nó về phía πref bằng cách kiểm soát khoảng cách giữa các tỷ lệ log-likelihood log(π(yw)/π(yl)) và log(πref(yw)/πref(yl)), do đó tránh overfitting với tập dữ liệu sở thích. Chúng tôi tóm tắt IPO được lấy mẫu trong Thuật toán 1:

Thuật toán 1 IPO Được lấy mẫu
Yêu cầu: Tập dữ liệu D gồm các lời nhắc, các thế hệ được ưa thích và không được ưa thích x, yw và yl, tương ứng. Một chính sách tham chiếu πref
1: Định nghĩa
hπ(y, y′, x) = log π(y|x)πref(y′|x)/(π(y′|x)πref(y|x))
2: Bắt đầu từ π = πref tối thiểu hóa
E(yw,yl,x)∼D[(hπ(yw, yl, x) − τ−1/2)2].

5.3 Ví dụ Minh họa

Để minh họa sự khác biệt định tính giữa thuật toán của chúng tôi và DPO, chúng tôi sẽ xem xét một vài trường hợp đơn giản. Để đơn giản, chúng tôi giả sử không có ngữ cảnh x, tức là, chúng tôi ở trong thiết lập bandit.

5.3.1 Thiết lập Tiệm cận

Trước tiên chúng tôi xem xét trường hợp đơn giản trong đó chúng ta chỉ có 2 hành động, y1 và y2, và một sở thích xác định giữa chúng: p*(y1≻y2) = 1. Giả sử chúng ta bắt đầu với πref và μ đồng nhất. Chúng ta biết từ Phần 4.2 rằng DPO sẽ hội tụ về chính sách xác định π*(y1) = 1, π*(y2) = 0 bất kể giá trị của τ. Vì vậy ngay cả khi hệ số điều hòa τ rất lớn, điều này rất khác với πref đồng nhất.

Bây giờ, hãy suy ra chính sách tối ưu cho IPO. Chúng ta có p*(y1≻μ) = 3/4 và p*(y2≻μ) = 1/4. Cắm điều này vào phương trình (9) với Ψ = I chúng ta nhận được rằng π*(y1) = exp(0.75τ−1)/(exp(0.75τ−1) + exp(0.25τ−1)) = σ(0.5τ−1), và π*(y2) = σ(−0.5τ−1), trong đó σ là hàm sigmoid. Do đó chúng ta thấy rằng nếu chúng ta có điều hòa lớn khi τ → +∞, thì π* hội tụ về chính sách đồng nhất πref, và mặt khác khi τ → +0, thì π*(y1) → 1 và π*(y2) → 0, đó là chính sách tối ưu xác định. Tham số điều hòa τ bây giờ thực sự có thể được sử dụng để kiểm soát chúng ta gần πref như thế nào.

5.4 Sở thích Được lấy mẫu

Cho đến nay chúng tôi dựa vào chính sách tối ưu dạng đóng từ Eq. (9) để nghiên cứu tính ổn định của DPO và IPO, nhưng phương trình này không áp dụng cho các thiết lập phức tạp hơn trong đó chúng ta chỉ có quyền truy cập vào sở thích được lấy mẫu thay vì p⋆. Tuy nhiên chúng ta vẫn có thể tìm các xấp xỉ chính xác của chính sách tối ưu bằng cách chọn một tham số hóa πθ và tối ưu hóa θ với một mất mát thực nghiệm trên một tập dữ liệu và các cập nhật dựa trên gradient lặp. Chúng tôi sẽ sử dụng phương pháp này để chỉ ra hai ví dụ không tiệm cận trong đó DPO overfits tập dữ liệu sở thích và bỏ qua πref: khi một hành động yw thắng tất cả các hành động khác, DPO đẩy πθ(y) lên 1 bất kể τ, và ngược lại khi một hành động y không bao giờ thắng các hành động khác, DPO đẩy πθ(y) xuống 0 một lần nữa bất kể τ. Trong các kịch bản tương tự, IPO không hội tụ về các giải pháp suy biến này mà thay vào đó vẫn gần πref dựa trên sức mạnh của điều hòa τ.

Đối với cả hai kịch bản, chúng tôi xem xét một không gian rời rạc Y = {ya, yb, yc} với 3 hành động, và chọn một tập dữ liệu các cặp D = {(yw,i, yl,j)}. Cho D, chúng tôi tận dụng các mất mát thực nghiệm từ Eq. 4 và Eq. 13 để tìm chính sách tối ưu của DPO và IPO. Chúng tôi mã hóa các chính sách như πθ(yi) = softmax(θ)i sử dụng một vector θ trong R3, và tối ưu hóa chúng trong 18000 bước sử dụng Adam (Kingma và Ba, 2014) với tốc độ học 0.01 và kích thước mini-batch 9. Các mini-batch được xây dựng sử dụng lấy mẫu đồng nhất có thay thế từ D. Cả chính sách và mất mát đều được triển khai sử dụng framework python flax (Bradbury et al., 2018; Heek et al., 2023), và triển khai Adam từ optax (Babuschkin et al., 2020).

Hình 1: So sánh Giữa Đường cong Học của Xác suất Hành động của IPO và DPO cho D1

Đối với mỗi tập siêu tham số, chúng tôi lặp lại thí nghiệm 10 lần với các seed khác nhau, và báo cáo trung bình và khoảng tin cậy 95%. Tất cả các thí nghiệm được thực hiện trên một máy ảo đám mây hiện đại với 4 lõi và 32GB ram.

IPO Tránh Các chính sách Tham lam Đối với ví dụ đầu tiên, chúng tôi lấy mẫu mỗi cặp hành động duy nhất một lần để thu thập một tập dữ liệu D chứa 3 sở thích quan sát được. Do tính đối xứng của sở thích theo cặp, việc lấy mẫu chỉ 3 sở thích có thể dẫn đến chỉ hai kết quả (tới các hoán vị của các hành động):

D1 = {(ya, yb), (yb, yc), (ya, yc)},
D2 = {(ya, yb), (yb, yc), (yc, ya)},

trong đó chúng tôi tập trung vào D1, đại diện cho một thứ tự toàn bộ, thay vì D2, đại diện cho một chu kỳ. Kết quả của thí nghiệm được báo cáo trong Hình. 1 trong đó, chúng tôi báo cáo các đường cong học cho các giá trị τ khác nhau. Chúng tôi quan sát rằng DPO luôn hội tụ về chính sách xác định cho tất cả các giá trị của τ. Nói cách khác, DPO hoàn toàn bỏ qua chính sách tham chiếu, bất kể điều hòa mạnh như thế nào, và hội tụ về hành động được ưa thích trong tập dữ liệu. Mặt khác, IPO ngăn chặn chính sách trở nên tham lam khi điều hòa mạnh.

IPO Không Loại trừ Hành động Trong ví dụ đầu tiên, DPO hội tụ về một chính sách xác định vì một hành động hoàn toàn thống trị tất cả các hành động khác và mất mát tiếp tục đẩy lên khả năng của nó cho đến khi nó bão hòa. Hiệu ứng ngược lại xảy ra cho điều kiện ngược lại logic, tức là, khi một hành động không có ít nhất một chiến thắng trong tập dữ liệu, DPO sẽ đặt xác suất của nó về 0 bất kể τ. Trong khi điều này ít gây rối loạn hơn ví dụ đầu tiên (một xác suất duy nhất bị nhiễu loạn trong khi trước đó toàn bộ chính sách bị biến dạng bởi một hành động đạt thành tích quá cao), nó cũng phổ biến hơn nhiều trong dữ liệu thế giới thực. Đặc biệt, bất cứ khi nào không gian hành động lớn nhưng tập dữ liệu nhỏ, một số hành động nhất thiết sẽ được lấy mẫu hiếm hoặc chỉ một lần, làm cho việc không bao giờ quan sát một chiến thắng trở nên có khả năng. Đặc biệt vì chúng ta không có dữ liệu về hiệu suất của chúng, π nên gắn bó gần với πref để an toàn, nhưng mục tiêu của DPO không thúc đẩy điều này.

Trong ví dụ cuối cùng, tập dữ liệu bao gồm hai sở thích quan sát được D3 = {(ya, yb), (yb, ya)} và để cặp (ya, yc) hoàn toàn không được quan sát. Chúng tôi tính toán các giải pháp sử dụng Adam một lần nữa, và báo cáo kết quả trong Hình. 2 cho các giá trị τ khác nhau. Chúng tôi quan sát một lần nữa ở đây rằng DPO bỏ qua hoàn toàn πref trước, bất kể chúng ta điều hòa mục tiêu mạnh như thế nào, trong khi IPO dần dần giảm xác suất của hành động không quan sát được với τ.

Hình 2: So sánh Giữa Đường cong Học của Xác suất Hành động của IPO và DPO cho D3

6 Kết luận và Công việc Tương lai

Chúng tôi đã trình bày một mục tiêu thống nhất, gọi là ΨPO, để học từ sở thích. Nó thống nhất các phương pháp RLHF và DPO. Ngoài ra, chúng tôi giới thiệu một trường hợp cụ thể của ΨPO, gọi là IPO, cho phép học trực tiếp từ sở thích mà không cần giai đoạn mô hình hóa phần thưởng và không dựa vào giả định mô hình hóa Bradley-Terry giả định rằng các sở thích theo cặp có thể được thay thế bằng phần thưởng theo điểm. Điều này quan trọng vì nó cho phép tránh vấn đề overfitting. Đóng góp lý thuyết này chỉ hữu ích trong thực tế nếu một hàm mất mát mẫu thực nghiệm có thể được suy ra. Đây là những gì chúng tôi đã làm trong Sec 5 trong đó chúng tôi chỉ ra rằng IPO có thể được công thức hóa như một bài toán tìm nghiệm từ đó một hàm mất mát mẫu thực nghiệm có thể được suy ra. Hàm mất mát IPO đơn giản, dễ triển khai và được biện minh lý thuyết. Cuối cùng, trong Sec. 5.3 và Sec. 5.4, chúng tôi cung cấp các ví dụ minh họa trong đó chúng tôi nêu bật tính không ổn định của DPO khi các sở thích được biết đầy đủ cũng như khi chúng được lấy mẫu. Những thí nghiệm tối thiểu đó đủ để chứng minh rằng IPO phù hợp hơn để học từ sở thích được lấy mẫu so với DPO. Các công việc tương lai nên mở rộng những thí nghiệm đó sang các thiết lập phức tạp hơn như huấn luyện các mô hình ngôn ngữ trên dữ liệu sở thích con người.
