# 2312.00849.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2312.00849.pdf
# Kích thước tệp: 1878472 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
RLHF-V: Hướng tới các MLLM Đáng tin cậy thông qua Căn chỉnh Hành vi từ
Phản hồi Sửa chữa Chi tiết của Con người
Tianyu Yu1Yuan Yao2*Haoye Zhang1Taiwen He1Yifeng Han1
Ganqu Cui1Jinyi Hu1Zhiyuan Liu1*Hai-Tao Zheng1*Maosong Sun1Tat-Seng Chua2
1Đại học Tsinghua2Đại học Quốc gia Singapore
yiranytianyu@gmail.com yaoyuanthu@gmail.com
https://rlhf-v.github.io
Tóm tắt
Các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM) gần đây đã thể hiện khả năng ấn tượng trong hiểu biết, lý luận và tương tác đa phương thức. Tuy nhiên, các MLLM hiện tại phổ biến gặp phải vấn đề ảo giác nghiêm trọng, tạo ra văn bản không có căn cứ thực tế trong các hình ảnh liên quan. Vấn đề này khiến các MLLM hiện tại không đáng tin cậy và do đó không thực tế trong các ứng dụng thế giới thực (đặc biệt là các ứng dụng có mức độ rủi ro cao). Để giải quyết thách thức này, chúng tôi trình bày RLHF-V, nhằm nâng cao độ tin cậy của MLLM thông qua căn chỉnh hành vi từ phản hồi sửa chữa chi tiết của con người. Cụ thể, RLHF-V thu thập sở thích của con người dưới dạng các chỉnh sửa ở mức đoạn về các ảo giác, và thực hiện tối ưu hóa ưu tiên trực tiếp dày đặc trên phản hồi của con người. Các thí nghiệm toàn diện trên năm bộ kiểm tra trong cả đánh giá tự động và con người cho thấy rằng, RLHF-V có thể tạo ra các hành vi MLLM đáng tin cậy hơn đáng kể với hiệu quả dữ liệu và tính toán đầy hứa hẹn. Đáng chú ý, sử dụng 1.4k mẫu dữ liệu được chú thích, RLHF-V giảm đáng kể tỷ lệ ảo giác của MLLM cơ sở xuống 34.8%, vượt trội hơn LLaVA-RLHF đồng thời được huấn luyện trên 10k dữ liệu chú thích. Mô hình cuối cùng đạt được hiệu suất tốt nhất trong độ tin cậy trong số các MLLM mã nguồn mở, và cho thấy độ bền vững tốt hơn GPT-4V trong việc ngăn chặn các ảo giác phát sinh từ quá tổng quát hóa.

1. Giới thiệu
Thành công gần đây của các Mô hình Ngôn ngữ Lớn Đa phương thức (MLLM) đánh dấu một cột mốc quan trọng trong nghiên cứu AI [4, 6, 14, 16, 27, 35, 37, 54, 63]. Bằng cách kết nối tín hiệu thị giác và Mô hình Ngôn ngữ Lớn (LLM), các MLLM cho thấy khả năng chưa từng có trong hiểu biết, lý luận và tương tác đa phương thức [36, 37, 57]. Các mô hình này thường được tiền huấn luyện trên dữ liệu hình ảnh-văn bản quy mô lớn để học kiến thức và khả năng đa phương thức cơ bản [4, 6, 16, 27].
*Tác giả liên hệ

Để điều hướng hành vi mô hình, hầu hết các MLLM được tinh chỉnh thêm với điều chỉnh hướng dẫn (còn được gọi là tinh chỉnh có giám sát), giám sát các mô hình để sao chép hành vi từ dữ liệu minh họa, cho phép các MLLM tương tác với người dùng trong các nhiệm vụ thế giới mở khác nhau [6, 14, 33, 35, 60].

Tuy nhiên, các hành vi MLLM hiện tại không được căn chỉnh tốt với sở thích của con người. Một vấn đề rõ ràng là xu hướng tạo ra các ảo giác - những phản hồi không có căn cứ thực tế trong các hình ảnh liên quan [29, 33, 37, 48]. Điều này thường bao gồm mô tả về nội dung thị giác không tồn tại và lỗi trong mô tả. Như được hiển thị trong Hình 1, các MLLM hiện tại có thể tạo ảo giác về đối tượng, thuộc tính, số lượng, vị trí, hành động, v.v. Về mặt định lượng, đánh giá của con người cho thấy vấn đề này phổ biến trong các MLLM tiên tiến, nơi ngay cả GPT-4V tiên tiến nhất [37] cũng chứa các ảo giác rõ ràng trong 45.9% phản hồi. Vấn đề này khiến các MLLM hiện tại không đáng tin cậy và do đó không thực tế trong các ứng dụng thế giới thực (đặc biệt là các ứng dụng có mức độ rủi ro cao), chẳng hạn như hướng dẫn người khiếm thị [37] hoặc hệ thống lái xe tự động [55].

Chúng tôi lập luận rằng vấn đề phát sinh từ việc thiếu phản hồi tích cực/tiêu cực của con người trong các mô hình được điều chỉnh hướng dẫn, khiến việc học các ranh giới hành vi chính xác để loại trừ ảo giác trở nên khó khăn. Để giải quyết vấn đề này, chúng tôi đề xuất RLHF-V, một khung mới căn chỉnh hành vi MLLM bằng cách học từ phản hồi của con người. Một cách đơn giản là sử dụng phương pháp Học tăng cường từ Phản hồi Con người (RLHF) truyền thống trong các LLM tiên tiến [38, 51], bao gồm việc người chú thích xếp hạng các phản hồi của mô hình, và sử dụng mô hình thưởng để hướng dẫn việc học LLM chính sách. Tuy nhiên, cách tiếp cận này gặp phải hai thách thức chính: (1) Sự mơ hồ trong chú thích. Việc giúp đỡarXiv:2312.00849v2  [cs.CL]  8 Mar 2024

--- TRANG 2 ---
Lời nhắc: Vui lòng mô tả hình ảnh một cách chi tiết.Đầu ra MLLMPhản hồi Sửa chữa Chi tiết của Con người
MLLM Cuối cùng
ABA*Tối ưu hóa Ưu tiên Trực tiếp Dày đặc (DDPO)B*Đầu vàoA*
(1) Thu thập Phản hồi Con người(2) Học Sở thích Con ngườiHình ảnh có một tháp đồng hồ lớn với mặt đồng hồ ở mỗi bên của nó ... Các đồng hồ trên tháp hiển thị thời gian là 11:50. Tháp được trang trí với những lá cờ màu cam và hồng, tạo thêm nét lễ hội...Hình ảnh cho thấy một tháp đồng hồ ... Đồng hồ hiển thị khoảng 11:20 ... Cũng có một số lá cờ bay ở góc trên bên trái của hình ảnh ... Có một số người rải rác khắp hiện trường...Hình ảnh cho thấy một tháp đồng hồ ... Đồng hồ hiển thị khoảng 15:26 ... Cũng có một số lá cờ bay ở góc trên bên phải của hình ảnh ... Có những cây ở phía sau các tòa nhà...Hình ảnh có một tháp đồng hồ lớn với mặt đồng hồ ở mặt trước của nó ... Đồng hồ trên tháp hiển thị thời gian là 15:26. Có những lá cờ màu đỏ, xanh lá, vàng và xanh dương, tạo thêm nét lễ hội...A
···＜
···
Hình 1. Khung RLHF-V để căn chỉnh hành vi MLLM từ phản hồi con người. (1) Cho hình ảnh đầu vào và lời nhắc, chúng tôi thu được đầu ra từ các MLLM và thu thập phản hồi con người dưới dạng chỉnh sửa ở mức đoạn chi tiết về các ảo giác. (2) Trong quá trình học sở thích con người, chúng tôi thực hiện tối ưu hóa ưu tiên trực tiếp dày đặc trên phản hồi sửa chữa chi tiết của con người.

và các phản hồi hấp dẫn về nội dung hình ảnh phong phú thường dài và phức tạp, khiến việc quyết định phản hồi nào được ưu tiên thường không rõ ràng. Như được hiển thị trong Hình 1 (phản hồi A và B), người chú thích thường phải đối mặt với những tình huống khó xử khi trình bày các phản hồi có ưu điểm và nhược điểm tương ứng. Bên cạnh đó, ngay cả khi được gắn nhãn với sở thích rõ ràng, phản hồi tối ưu vẫn không xác định (ví dụ: thời gian chính xác trên đồng hồ). (2) Hiệu quả học tập. Phản hồi xếp hạng thô sơ khiến việc phân bổ tín dụng chính xác cho các hành vi mong muốn trở nên khó khăn. Xét đến độ phức tạp ngôn ngữ và sự biến thiên của các phản hồi, hành vi mong muốn thường đòi hỏi một lượng lớn dữ liệu được gắn nhãn để học [13, 39, 48]. Hơn nữa, việc phân bổ sai tín dụng cho độ lệch không mạnh mẽ tương quan với dữ liệu thường dẫn đến vấn đề hack thưởng và suy thoái hành vi [7, 51].

RLHF-V giải quyết những thách thức này bằng cách giới thiệu hai đổi mới chính: (1) Ở cấp độ dữ liệu, chúng tôi đề xuất thu thập phản hồi con người dưới dạng chỉnh sửa ở mức đoạn chi tiết. Như được hiển thị trong Hình 1, chúng tôi yêu cầu người chú thích trực tiếp sửa chữa các đoạn ảo giác từ phản hồi mô hình, cung cấp sở thích con người rõ ràng, dày đặc và chi tiết, cũng như các phản hồi tối ưu. Chiến lược này cũng tránh được sự biến thiên ngôn ngữ và độ lệch không mạnh mẽ, đảm bảo rằng phản hồi được phân bổ chính xác cho các hành vi mong muốn, từ đó nâng cao hiệu quả học tập và ngăn chặn vấn đề hack thưởng. (2) Ở cấp độ phương pháp, chúng tôi đề xuất tối ưu hóa ưu tiên trực tiếp dày đặc (DDPO), một biến thể mới của DPO [42] giải quyết mục tiêu RLHF truyền thống theo cách giám sát tương đương đơn giản và hiệu quả. DDPO trực tiếp tối ưu hóa mô hình chính sách đối với sở thích ở mức đoạn dày đặc và chi tiết, nơi các đoạn ảo giác nhận được phản hồi mạnh hơn để có căn cứ thực tế.

Các thí nghiệm toàn diện trên năm bộ kiểm tra cho thấy rằng, RLHF-V có thể nâng cao đáng kể độ tin cậy của các MLLM với hiệu quả dữ liệu và tính toán đầy hứa hẹn. Sử dụng 1.4k dữ liệu sở thích, RLHF-V giảm đáng kể tỷ lệ ảo giác đối tượng của MLLM cơ sở xuống 34.8%, vượt trội hơn LLaVA-RLHF đồng thời [48] được huấn luyện trên 10k dữ liệu sở thích. Chúng tôi cũng cho thấy rằng RLHF-V đạt được độ bền vững tốt hơn GPT-4V mạnh mẽ [37] trong việc ngăn chặn các ảo giác phát sinh từ quá tổng quát hóa.

Đóng góp của công trình này có thể được tóm tắt thành ba phần: (1) Chúng tôi trình bày RLHF-V, một khung mới căn chỉnh hành vi MLLM thông qua phản hồi sửa chữa chi tiết của con người. (2) Chúng tôi thu thập dữ liệu sở thích con người chất lượng cao để cung cấp tín hiệu học tập căn chỉnh con người cho các MLLM. (3) Chúng tôi tiến hành các thí nghiệm toàn diện để chứng minh hiệu quả của khung đề xuất, đạt được hiệu suất tốt nhất trong độ tin cậy trong số các MLLM mã nguồn mở. Tất cả mã, dữ liệu và trọng số mô hình đều được mở nguồn tại https://github.com/RLHF-V/RLHF-V.

2. Thu thập Sở thích Con người
Mục tiêu của dữ liệu sở thích con người là phân biệt các phản hồi chất lượng cao được con người ưa thích với những phản hồi kém hơn, cung cấp tín hiệu học tập căn chỉnh con người để điều hướng các hành vi MLLM. Trước tiên chúng tôi cung cấp phân tích về các yếu tố cơ bản của dữ liệu sở thích con người, dựa trên đó chúng tôi thúc đẩy quy trình thu thập sở thích con người của RLHF-V.

Dữ liệu Sở thích Con người: Các Yếu tố Cơ bản và Thách thức. Cho đầu vào x (bao gồm hình ảnh và lời nhắc), ký hiệu sự khác biệt giữa đầu ra được ưa thích yw và đầu ra kém hơn yl là Y. Sự khác biệt Y có thể được phân tách thành ba yếu tố:
Y = Yp + Ys + Yn, (1)
trong đó Yp là hành vi thực sự được ưa thích như đáng tin cậy và hữu ích, Ys biểu thị độ lệch không mạnh mẽ nông tầng tương quan với dữ liệu nhưng không liên quan đến đánh giá của con người

--- TRANG 3 ---
(ví dụ: yw chứa nhiều cách sử dụng từ cụ thể hơn), và Yn là yếu tố nhiễu ngẫu nhiên biểu thị sự biến thiên ngôn ngữ của ngôn ngữ tự nhiên (ví dụ: các cách khác nhau để thể hiện cùng một ý nghĩa). Yp là yếu tố chúng tôi muốn học từ sự khác biệt Y, trong khi việc khớp với Ys có thể dẫn đến vấn đề hack thưởng và do đó nên tránh. Sự biến thiên ngôn ngữ Yn không làm lệch việc học sở thích nhưng khiến việc học khó khăn hơn, đòi hỏi nhiều dữ liệu được gắn nhãn hơn để học đến yếu tố được ưa thích Yp, và do đó cũng nên tránh nếu có thể.

Các thực hành RLHF phổ biến trong LLM thu thập sở thích con người Y dưới dạng nhãn xếp hạng, cho biết chất lượng tương đối tổng thể của các phản hồi [38, 39, 51]. Theo phân tích trên, thực hành này đối mặt với một số thách thức chính: (1) Sự mơ hồ trong chú thích. Việc chú thích phản hồi nào tốt hơn bằng nhãn xếp hạng tổng thể có thể không rõ ràng do bản chất chi tiết của Yp, đặc biệt đối với các phản hồi phức tạp. Như được hiển thị trong Hình 1, người chú thích thường không thể đồng ý về việc gán xếp hạng tổng thể cho các phản hồi khác nhau có ưu điểm và nhược điểm tương ứng. Chúng tôi quan sát thấy vấn đề này dẫn đến chất lượng chú thích không thỏa đáng của dữ liệu RLHF hiện có. Hơn nữa, ngay cả khi được gắn nhãn với sở thích rõ ràng, các phản hồi tối ưu cho các câu hỏi thường vẫn không xác định. (2) Hiệu quả học tập. Trong quá trình học tăng cường, việc phân bổ chính xác tín dụng thưa thớt và thô sơ từ Y thông qua sự biến thiên ngôn ngữ Yn đến hành vi được ưa thích Yp có thể khó khăn và đòi hỏi nhiều dữ liệu. Việc phân bổ sai cho yếu tố độ lệch không mạnh mẽ Ys sẽ dẫn đến việc các mô hình suy sụp để khai thác phần thưởng tầm thường [7, 51].

Thu thập Sở thích Con người Sửa chữa Chi tiết. Để giải quyết những thách thức này, chúng tôi đề xuất thu thập sở thích con người chi tiết dưới dạng chỉnh sửa ở mức đoạn. Như được hiển thị trong Hình 1, cho một đầu ra có lỗi yl từ các MLLM, chúng tôi yêu cầu người chú thích trực tiếp sửa chữa các đoạn ảo giác, tạo ra đầu ra tối ưu thực tế yw. Việc chú thích đồng thời tạo ra một cặp sở thích tăng dần ở mức đoạn (yw, yl). Quy trình đơn giản này hiệu quả giải quyết các thách thức: (1) Việc chú thích chỉnh sửa tăng dần trong các đoạn rõ ràng hơn và dễ thao tác hơn cho người gắn nhãn. (2) Phản hồi dày đặc và chi tiết được phân bổ trực tiếp cho hành vi được ưa thích Yp, loại trừ sự biến thiên ngôn ngữ Yn và độ lệch không mạnh mẽ Ys, do đó cải thiện hiệu quả học tập và ngăn chặn vấn đề hack thưởng. Trong các thí nghiệm, chúng tôi thấy rằng quy trình này cải thiện đáng kể chất lượng chú thích và hiệu quả dữ liệu, cho phép mô hình của chúng tôi vượt trội hơn các mô hình đồng thời được huấn luyện trên một bậc độ lớn hơn dữ liệu sở thích được gắn nhãn (xem Phần 4.3).

Trong thực tế, chúng tôi thu được tổng cộng 1.4k lời nhắc làm đầu vào từ bộ dữ liệu điều chỉnh hướng dẫn hiện có [60] và các lời nhắc mô tả hình ảnh được tạo bởi GPT-4, và nhận được các phản hồi từ Muffin [60] để chú thích con người. Các phản hồi sau khi chú thích chứa 64.4 từ và 2.65 đoạn được sửa chữa trung bình. Chúng tôi quan sát rằng các chỉnh sửa đa dạng về loại ảo giác, bao gồm đối tượng (41.2%), vị trí (20.3%), số lượng (16.5%), thuộc tính (10.0%), hành động (5.3%) và các loại khác (6.8%).

3. Phương pháp
Chúng tôi giới thiệu cách tiếp cận RLHF-V học phản hồi sửa chữa chi tiết của con người bằng tối ưu hóa ưu tiên trực tiếp dày đặc. Ngoài ra, chúng tôi cũng giảm thiểu các nguồn ảo giác hiện có trong huấn luyện MLLM bằng cách giải quyết vấn đề không khớp thị giác-ngôn ngữ.

3.1. Tối ưu hóa Ưu tiên Trực tiếp Dày đặc
Để tận dụng phản hồi con người dày đặc và chi tiết, chúng tôi trình bày DDPO, một biến thể mới của tối ưu hóa ưu tiên trực tiếp [42] để trực tiếp tối ưu hóa chính sách MLLM đối với sở thích con người dày đặc. Các cách tiếp cận RLHF phổ biến bao gồm việc khớp mô hình thưởng trên dữ liệu sở thích, và sau đó huấn luyện các mô hình phê bình, chính sách và giá trị để tối đa hóa phần thưởng mà không lệch quá xa khỏi mô hình tham chiếu [13, 39, 51]. Quy trình này đòi hỏi huấn luyện nhiều LLM với việc lấy mẫu và huấn luyện rộng rãi, gặp phải các quy trình phức tạp và chi phí tính toán cao.

Tối ưu hóa Ưu tiên Trực tiếp (DPO) [42] giải quyết mục tiêu học tăng cường này theo cách giám sát tương đương đơn giản hơn. Ở đây chúng tôi giới thiệu ngắn gọn về phương pháp DPO, và chuyển độc giả đến bài báo gốc để biết thêm chi tiết. Quan sát chính của DPO là hàm thưởng r(x, y) có thể được biểu thị phân tích bởi mô hình chính sách tối ưu pi*(y|x) và mô hình tham chiếu piref(y|x), và do đó chúng ta có thể trực tiếp tối ưu hóa mô hình chính sách dưới các dạng thích hợp trên dữ liệu sở thích. Cụ thể, mô hình thưởng r(x, y) có thể được biểu diễn như:
r(x, y) = beta log pi*(y|x)/piref(y|x) + beta log Z(x), (2)
trong đó beta là một hằng số và Z(x) là hàm phân vùng. Dựa trên quan sát này, mô hình chính sách có thể được tối ưu hóa trực tiếp trên dữ liệu phản hồi con người:
L = −E(x,yw,yl) log sigma(r(x, yw) − r(x, yl))
= −E(x,yw,yl) log sigma(beta log pi*(yw|x)/piref(yw|x) − beta log pi*(yl|x)/piref(yl|x)), (3)
trong đó mô hình tham chiếu piref(y|x) thường được thực hiện bởi một mô hình cơ sở được điều chỉnh hướng dẫn mà chúng tôi muốn cải thiện, và được giữ cố định trong quá trình huấn luyện DPO. Chỉ có mô hình chính sách pi*(y|x) được cập nhật. Chúng tôi lưu ý rằng DPO đơn giản, hiệu quả và ổn định hơn trong việc căn chỉnh hành vi MLLM so với các cách tiếp cận RLHF truyền thống.

Tận dụng phản hồi ở mức đoạn dày đặc và chi tiết về cơ bản đòi hỏi mô hình đánh giá phần thưởng của các hành động ở mức đoạn. Tuy nhiên, DPO được thiết kế để

--- TRANG 4 ---
học sở thích dưới dạng nhãn xếp hạng phản hồi tổng thể. Cụ thể, điểm hành động của DPO được đưa ra bởi khả năng của phản hồi toàn diện trong thực tế, nơi các đoạn khác nhau được đối xử như nhau:
log pi(y|x) = Σ yi in y log p(yi|x, y<i), (4)
trong đó yi là token thứ i của phản hồi y. Chúng tôi lập luận rằng so với các đoạn không thay đổi yu, các đoạn được sửa chữa yc tiết lộ trực tiếp hơn đánh giá của con người về ảo giác, và do đó nên đóng góp nhiều hơn vào đánh giá hành động tổng thể. Do đó, chúng tôi đề xuất tính điểm phản hồi như một tổng hợp có trọng số của các đoạn chi tiết:1
log pi(y|x) = 1/N [Σ yi in yu log p(yi|x, y<i) + gamma Σ yi in yc log p(yi|x, y<i)], (5)
trong đó gamma > 1 là một siêu tham số trọng số, và gamma lớn hơn có nghĩa là đóng góp nhiều hơn từ các đoạn được sửa chữa. N = |yu| + gamma|yc| là một yếu tố chuẩn hóa, ngăn chặn các phản hồi dài hơn nhận được điểm cao hơn. Theo cách này, các đoạn được sửa chữa được làm nổi bật để nhận phản hồi sở thích con người mạnh hơn để có căn cứ thực tế. Trong các thí nghiệm, chúng tôi thấy rằng DDPO có thể khai thác tốt hơn phản hồi con người chi tiết, dẫn đến các phản hồi đáng tin cậy hơn.

3.2. Giảm thiểu Ảo giác từ Không khớp VL
DDPO giảm ảo giác bằng cách học từ phản hồi con người. Từ góc độ nguyên nhân-kết quả khác, chúng tôi kiểm tra mô hình huấn luyện MLLM chủ đạo, và xác định các nguồn ảo giác trong huấn luyện MLLM. Dựa trên các quan sát, chúng tôi thúc đẩy một công thức huấn luyện đáng tin cậy hơn.

Nhìn chung, các MLLM hiện tại học khả năng đa phương thức trong một mô hình học có giám sát, nơi đầu ra mô hình được giám sát đối với văn bản thật liên quan đến hình ảnh. Trong mô hình như vậy, ảo giác có thể được đưa vào bởi sự không khớp giữa dữ liệu hình ảnh và văn bản. Trong thực tế, sự không khớp có thể đến từ: (1) văn bản chất lượng thấp trong dữ liệu tiền huấn luyện và điều chỉnh hướng dẫn, và (2) tăng cường hình ảnh bất cẩn trong quá trình huấn luyện. Chúng tôi chỉ định các vấn đề và giải pháp trong phần sau.

Giải quyết Ảnh hưởng Văn bản Chất lượng Thấp. Dữ liệu tiền huấn luyện hiện tại của các MLLM được thu thập tự động từ Web [9, 10, 44], không thể tránh khỏi gặp phải nhiễu nghiêm trọng trong văn bản ngay cả sau khi xử lý hậu kỳ rộng rãi. Việc giám sát các MLLM đối với dữ liệu như vậy về cơ bản là dạy chúng tạo ảo giác (ví dụ: mô tả các yếu tố không có trong hình ảnh, hoặc tạo ra các mô tả không nhất quán với hình ảnh). Tương tự, hầu hết các bộ dữ liệu điều chỉnh hướng dẫn thị giác hiện có được tạo bởi ChatGPT/GPT-4 theo các chú thích văn bản trung gian [33, 35, 60], không thể tránh khỏi đưa ảo giác vào dữ liệu hướng dẫn. Mặc dù có thể khó sửa chữa dữ liệu tiền huấn luyện và điều chỉnh hướng dẫn hiện có, chúng tôi thấy rằng ảnh hưởng có thể được chống lại bằng cách đơn giản hậu huấn luyện các MLLM trên các bộ dữ liệu hỏi đáp thị giác chất lượng cao. Một cách trực quan, các bộ dữ liệu được gắn nhãn bởi con người có thể cung cấp tín hiệu học tập chính xác để hiệu chỉnh hành vi mô hình từ ảo giác, và cũng nâng cao khả năng tuân theo hướng dẫn. Trong các thí nghiệm của chúng tôi, chúng tôi thấy rằng việc đơn giản tinh chỉnh mô hình trên VQAv2 [18] có thể giảm đáng kể tỷ lệ ảo giác (xem Phần 4.3).

Giảm thiểu Tăng cường Hình ảnh Không đáng tin cậy. Sự không khớp thị giác-ngôn ngữ cũng có thể đến từ miền hình ảnh. Tăng cường dữ liệu được áp dụng rộng rãi để cải thiện tính đa dạng dữ liệu và độ bền mô hình trong các mô hình đa phương thức khác nhau [14, 27, 41, 53, 60]. Tuy nhiên, chúng tôi lưu ý rằng việc tăng cường như vậy phải được thực hiện cẩn thận trong huấn luyện MLLM. Vấn đề chính là một số thao tác tăng cường hình ảnh có thể thay đổi đáng kể ngữ nghĩa của hình ảnh, có thể khiến hình ảnh được tăng cường không nhất quán với văn bản liên quan. Ví dụ, trong quá trình tăng cường, việc cắt ngẫu nhiên có thể khiến các đối tượng được đề cập trong văn bản biến mất khỏi hình ảnh. Điều này có thể khiến mô hình mô tả các đối tượng không tồn tại, với số lượng sai và ở vị trí sai. Trong huấn luyện mô hình của chúng tôi, chúng tôi loại trừ việc cắt hình ảnh trong tăng cường dữ liệu, điều này cải thiện độ tin cậy của các MLLM (xem Phần 4.3).

4. Thí nghiệm
Trong phần này, chúng tôi điều tra thực nghiệm hiệu quả của RLHF-V trong việc căn chỉnh hành vi MLLM. Ngoài việc đánh giá độ tin cậy và tính hữu ích của cuộc trò chuyện, chúng tôi cũng phân tích hiệu quả dữ liệu và khả năng mở rộng cũng như độ bền vững. Chúng tôi chuyển độc giả đến phụ lục để biết thêm chi tiết về các bộ kiểm tra, đường cơ sở và kết quả.

4.1. Cài đặt Thí nghiệm
Trước tiên chúng tôi giới thiệu cài đặt thí nghiệm, bao gồm đánh giá, đường cơ sở và chi tiết triển khai.

Đánh giá. Chúng tôi đánh giá các mô hình từ hai góc độ, bao gồm độ tin cậy phản ánh mức độ ảo giác, và tính hữu ích phản ánh chất lượng tương tác chung. Tương tự như [48], chúng tôi thấy đánh giá phân loại nhị phân (tức là trả lời có/không) [17, 29] không thể phản ánh tốt các hành vi MLLM trong tương tác dài hình thức mở. Do đó chúng tôi áp dụng các bộ kiểm tra đánh giá trực tiếp các phản hồi dài, gần gũi hơn với các kịch bản sử dụng thực tế của các MLLM. Đối với độ tin cậy, chúng tôi thực hiện đánh giá trên ba bộ kiểm tra:

(1) Object HalBench [43] là một bộ kiểm tra được áp dụng rộng rãi để đánh giá ảo giác đối tượng trong mô tả hình ảnh chi tiết. Nó so sánh các đối tượng trong đầu ra mô hình với nhãn đối tượng được chú thích đầy đủ cho hình ảnh COCO

--- TRANG 5 ---
Mô hình | Object HalBench ↓ | MHumanEval ↓ | MMHal-Bench | LLaVA Bench | VQAv2
Resp. | Mention | Object | Position | Number | All | Info. | Resp. ↓ | Conv. | Detail | Comp. | testdev
LLaVA [35] | 63.0 | 29.5 | 46.6 | 21.2 | 19.9 | 80.8 | 31.9 | 70.8 | 85.4 | 74.3 | 96.3 | -
Muffin [60] | 50.5 | 24.5 | 33.6 | 16.4 | 26.0 | 74.7 | 33.4 | 68.8 | 89.3 | 79.7 | 97.7 | -
LRV [33] | 32.3 | 22.3 | 43.2 | 11.6 | 19.2 | 82.9 | 22.2 | 78.1 | 61.7 | 47.3 | 55.0 | -
LLaVA-RLHF [48] | 38.1 | 18.9 | 37.7 | 17.8 | 18.5 | 72.6 | 39.9 | 65.6 | 93.8 | 74.3 | 111.4 | -
InstructBLIP [14] | 25.9 | 14.3 | 30.8 | 15.1 | 17.1 | 63.7 | 29.5 | 64.4 | 83.2 | 67.6 | 90.6 | -
Qwen-VL-Chat [6] | 43.8 | 20.0 | 34.9 | 16.4 | 15.8 | 61.0 | 38.5 | 52.1 | 81.9 | 77.1 | 92.3 | 79.5
LLaVA 1.5 [34] | 46.3 | 22.6 | 30.8 | 17.8 | 17.1 | 61.0 | 39.2 | 52.1 | 81.6 | 75.5 | 95.2 | 80.0
RLHF-V | 12.2 | 7.5 | 21.9 | 7.5 | 14.4 | 55.5 | 40.0 | 52.1 | 93.1 | 75.3 | 91.6 | 80.0
GPT-4V [37] | 13.6 | 7.3 | 22.6 | 12.3 | 11.0 | 45.9 | 47.6 | 31.3 | 96.0 | 102.5 | 106.7 | 77.2*

Bảng 1. Kết quả thí nghiệm chính về ảo giác. Chúng tôi báo cáo tỷ lệ ảo giác trong các độ chi tiết khác nhau, bao gồm mức phản hồi (Resp.) và mức đề cập (Mention), và tỷ lệ ảo giác mức phản hồi trong các loại khác nhau. Chúng tôi cũng hiển thị điểm về tính thông tin (Info.), cuộc trò chuyện đa phương thức (Conv.), mô tả chi tiết (Detail), và lý luận phức tạp (Comp.). * biểu thị kết quả zero-shot trên VQAv2.2 Kết quả tốt nhất và tốt thứ hai của mã nguồn mở được hiển thị in đậm và gạch chân tương ứng.

[31] để phát hiện ảo giác đối tượng. Để cải thiện tính ổn định đánh giá, chúng tôi tăng cường bộ kiểm tra với 8 lời nhắc đa dạng cho mô tả hình ảnh chi tiết. Chúng tôi báo cáo tỷ lệ ảo giác mức phản hồi (tức là tỷ lệ phần trăm các phản hồi có ảo giác), cũng như tỷ lệ ảo giác mức đề cập (tức là tỷ lệ phần trăm các đề cập đối tượng ảo giác trong tất cả đề cập đối tượng).

(2) MMHal-Bench [48] đánh giá ảo giác và tính thông tin phản hồi. Nó sử dụng GPT-4 để so sánh đầu ra mô hình với phản hồi con người và một số nhãn đối tượng để quyết định điểm số. Trong các thí nghiệm, chúng tôi thấy rằng GPT-4 không thể phát hiện ảo giác một cách đáng tin cậy do tính không đầy đủ của các chú thích văn bản MMHal-Bench. Do đó chúng tôi chỉ báo cáo điểm tính thông tin từ GPT-4, và đánh giá tỷ lệ ảo giác mức phản hồi bằng đánh giá con người.

(3) MHumanEval. Các đánh giá trên hoặc giới hạn ở ảo giác đối tượng thông thường hoặc bị chi phối bởi hỏi đáp ngắn (tức là các câu hỏi có thể được trả lời đầy đủ bằng một vài từ). Để cung cấp đánh giá đáng tin cậy và toàn diện hơn về các loại ảo giác đa dạng, chúng tôi trình bày bộ kiểm tra MHumanEval, bao gồm cả mô tả hình ảnh dài và câu hỏi ngắn. Bộ kiểm tra chứa 146 mẫu thu thập từ Object HalBench (50) và MMHal-Bench (96). Cho các phản hồi mô hình, chúng tôi yêu cầu người chú thích gắn nhãn các đoạn ảo giác và loại ảo giác của các đoạn, bao gồm đối tượng, vị trí, số lượng và loại khác. Chúng tôi báo cáo tỷ lệ ảo giác mức phản hồi về các loại này.

Đối với tính hữu ích, chúng tôi áp dụng hai bộ kiểm tra: (1) LLaVA Bench [35] là một bộ kiểm tra được áp dụng rộng rãi để đánh giá khả năng cuộc trò chuyện đa phương thức, mô tả chi tiết và lý luận phức tạp. Nó chấm điểm đầu ra mô hình đối với phản hồi tham chiếu thông qua GPT-4. (2) VQAv2 [18] là một bộ dữ liệu phổ biến cho hỏi đáp thị giác ngắn.

2Do khả năng tuân theo hướng dẫn hạn chế, hầu hết các MLLM cần phải

Đường cơ sở. Chúng tôi so sánh mô hình của chúng tôi với các đường cơ sở tiên tiến. (1) Đường cơ sở chung. Chúng tôi áp dụng Qwen-VL-Chat [6], LLaVA [35], LLaVA 1.5 [34], Muffin [60], và InstructBLIP [14] làm đường cơ sở chung đại diện. Các mô hình này hầu hết được tiền huấn luyện trên dữ liệu đa phương thức quy mô lớn, và được tinh chỉnh trên dữ liệu hướng dẫn chất lượng cao, đạt được hiệu suất mạnh trên các nhiệm vụ đa phương thức khác nhau. (2) Đường cơ sở được thiết kế riêng cho vấn đề ảo giác. LRV [33] được tinh chỉnh trên 400k dữ liệu hướng dẫn được tạo bởi GPT-4, và giảm thiểu ảo giác bằng cách giới hạn độ dài phản hồi. LLaVA-RLHF đồng thời [48] sử dụng Vicuna v1.5 13B mạnh mẽ [62] (được tinh chỉnh từ LLaMA-2 [51]) làm xương sống LLM. Nó huấn luyện mô hình thưởng trên 10k dữ liệu sở thích được gắn nhãn bởi con người, và thực hiện tối ưu hóa chính sách gần đúng [45] trên 72k dữ liệu được tăng cường thực tế. (3) Đường cơ sở Thương mại. Chúng tôi cũng bao gồm GPT-4V [37] làm tham chiếu mạnh, đánh giá khoảng cách giữa các mô hình mã nguồn mở và các mô hình thương mại tiên tiến.

Chi tiết Triển khai. Chúng tôi triển khai khung RLHF-V dựa trên Muffin [60]. Mô hình sử dụng BEiT-3 [53] làm mô-đun thị giác, và Vicuna v1.0 13B [12] (được tinh chỉnh từ LLaMA [50]) làm xương sống LLM. Siêu tham số beta là 0.5, và hệ số trọng số gamma là 5. Chúng tôi huấn luyện mô hình với DDPO trong 7 epoch, với độ phân giải hình ảnh 448, tốc độ học 5e-7 và kích thước lô 32. Việc huấn luyện RLHF-V có hiệu quả tính toán, mất ít hơn 1 giờ trên 8 GPU A100 tổng cộng.

4.2. Kết quả Chính
Kết quả thí nghiệm chính được báo cáo trong Bảng 1, từ đó chúng tôi quan sát rằng: (1) RLHF-V đạt được hiệu suất tốt nhất trong độ tin cậy trong số các mô hình mã nguồn mở, vượt trội hơn các mô hình chung mạnh và các mô hình được tinh chỉnh đặc biệt để tạo ra phản hồi VQA ngắn, và do đó không thể đạt được hiệu suất zero-shot hợp lý trên VQAv2.

--- TRANG 6 ---
Mô hình | Living Room | Kitchen | Bathroom | Street
∆ | book, person, bed | bottle, bowl, cup | toilet, sink, bottle | person, car, motorcycle
| chair, couch, remote | person, chair, knife | toothbrush, person, cup | traffic light, handbag, truck
| Ha | Hs | ∆ | Ha | Hs | ∆ | Ha | Hs | ∆ | Ha | Hs | ∆
LLaVA-1.5 [34] | 25.2 | 41.8 | +16.6 | 18.9 | 23.9 | +5.0 | 22.4 | 30.4 | +8.0 | 20.6 | 28.0 | +7.4 | +9.2
LLaVA-RLHF [48] | 23.7 | 34.5 | +10.8 | 13.1 | 17.4 | +4.3 | 18.2 | 19.5 | +1.4 | 18.3 | 22.7 | +4.4 | +5.2
QWEN-VL [6] | 24.5 | 34.5 | +10.0 | 16.4 | 20.8 | +4.4 | 21.6 | 17.5 | -4.1 | 22.5 | 32.0 | +9.5 | +5.0
RLHF-V | 5.5 | 8.0 | +2.5 | 3.8 | 5.9 | +2.1 | 4.1 | 4.0 | -0.1 | 2.3 | 4.6 | +2.3 | +1.7
GPT-4V [37] | 8.2 | 19.4 | +11.2 | 4.6 | 5.7 | +1.1 | 5.9 | 13.3 | +7.5 | 4.2 | 4.6 | +0.4 | +5.0

Bảng 2. Kết quả thí nghiệm ảo giác từ quá tổng quát hóa trên Object HalBench. Đối với mỗi cảnh, chúng tôi báo cáo tỷ lệ ảo giác của 10 đối tượng thường gặp nhất trung bình trên toàn bộ bộ kiểm tra (Ha) và dưới cảnh đó (Hs). 6 đối tượng thường gặp nhất được liệt kê cho mỗi cảnh để ngắn gọn. ∆: sự khác biệt tỷ lệ ảo giác, ∆: sự khác biệt trung bình qua các cảnh.

[Biểu đồ với hai đồ thị]
0 200 800 1400 2200 | 56 60 64 68 Tỷ lệ Ảo giác
LLaVA-1.5 | LLaVA-RLHF Data | Our Data

0 200 800 1400 2200 | 110 130 150 170 Số lượng Ảo giác
Data Scale | LLaVA-1.5

Hình 2. Tỷ lệ ảo giác và số lượng trên MHumanEval (tất cả loại) liên quan đến lượng dữ liệu sở thích. Chúng tôi báo cáo kết quả của các mô hình khác nhau được huấn luyện trên dữ liệu RLHF khác nhau.

được thiết kế riêng cho ảo giác. Khung này giảm đáng kể tỷ lệ ảo giác của mô hình cơ sở Muffin xuống 75.8% điểm tương đối cho các đối tượng thông thường trên Object HalBench, và xuống 34.8% cho các đối tượng tổng thể trên MHumanEval. Sự cải thiện nhất quán trong các độ chi tiết khác nhau bao gồm ảo giác mức phản hồi và mức đề cập, và các loại ảo giác khác nhau bao gồm đối tượng, vị trí và số lượng. Việc giảm đáng kể hơn trên các câu trả lời dài thách thức hơn trên Object HalBench và MHumanEval. Kết quả cho thấy rằng RLHF-V có thể học hiệu quả từ phản hồi sửa chữa chi tiết của con người để tạo ra các hành vi MLLM đáng tin cậy hơn. (2) RLHF-V đạt được hiệu suất đầy hứa hẹn trong tính hữu ích phản hồi, nơi các kết quả trên MMHalBench, LLaVA Bench và VQAv2 mạnh và tương đương với mô hình cơ sở. Điều này cho thấy rằng RLHF-V có thể nâng cao độ tin cậy của các MLLM mà không hy sinh tính hữu ích của chúng.

4.3. Phân tích
Trong phần này, chúng tôi tiến hành phân tích về khung xét đến các câu hỏi nghiên cứu sau: (1) Hiệu suất của RLHF-V mở rộng như thế nào với lượng dữ liệu phản hồi? (2) Ưu điểm của dữ liệu sở thích sửa chữa chi tiết so với dữ liệu xếp hạng tổng thể truyền thống là gì? (3) Dữ liệu và phương pháp của RLHF-V có thể được áp dụng để nâng cao độ tin cậy của các MLLM khác không? (4) Phản hồi con người làm giảm ảo giác một cách trực quan như thế nào?

Mở rộng dữ liệu phản hồi dẫn đến kết quả đầy hứa hẹn. Chúng tôi báo cáo tỷ lệ ảo giác và số lượng đoạn ảo giác trên MHumanEval dưới các lượng dữ liệu phản hồi khác nhau trong Hình 2. Chúng tôi quan sát rằng tỷ lệ ảo giác và số lượng của RLHF-V cho thấy sự giảm đáng kể và nhanh chóng khi lượng dữ liệu tăng. Điều này cho thấy rằng phản hồi sửa chữa chi tiết của con người cung cấp tín hiệu học tập hiệu quả và hiệu quả cho việc căn chỉnh hành vi MLLM. Dựa trên xu hướng này, chúng tôi kỳ vọng hiệu suất tốt hơn có thể đạt được với lượng dữ liệu phản hồi tăng lên. Chúng tôi để dành điều này cho công việc tương lai.

Phản hồi sửa chữa chi tiết của con người cho phép hiệu quả học tập tốt hơn. Để định lượng ưu điểm của phản hồi sửa chữa chi tiết của con người, chúng tôi thay thế dữ liệu của chúng tôi bằng 2.2k dữ liệu sở thích con người về ảo giác từ LLaVA-RLHF, cung cấp nhãn xếp hạng tổng thể theo các thực hành RLHF thông thường. Từ kết quả thí nghiệm trong Hình 2, chúng tôi quan sát rằng mô hình được trang bị dữ liệu của chúng tôi cho thấy sự giảm đáng kể và nhanh chóng hơn trong tỷ lệ ảo giác và số lượng. Đáng chú ý, chỉ sử dụng 200 dữ liệu sở thích, mô hình của chúng tôi đạt được tỷ lệ ảo giác tương đương với mô hình sử dụng một bậc độ lớn hơn dữ liệu được gắn nhãn từ LLaVA-RLHF. Hiệu quả dữ liệu vượt trội là do (1) chất lượng dữ liệu tốt hơn vì sự mơ hồ nhãn được giảm thiểu,

--- TRANG 7 ---
Mô hình | MHumanEval ↓ | MHB↓ | VQAv2
| Obj. | Pos. | Num. | All | Resp. | testdev
Muffin [60] | 33.6 | 16.4 | 26.0 | 74.7 | 68.8 | -
RLHF-V | 21.9 | 7.5 | 14.4 | 55.5 | 52.1 | 80.0
w/ vanilla DPO | 21.9 | 11.6 | 11.6 | 57.5 | 54.2 | 80.0
w/ IT-VQA only | 34.3 | 17.1 | 17.1 | 65.1 | 58.3 | 80.0
w/ untrust aug. | 18.5 | 13.7 | 14.4 | 59.6 | 54.2 | 77.1

Bảng 3. Kết quả ablation về các thành phần khác nhau. MHB: MMHal-Bench, IT-VQA: điều chỉnh hướng dẫn trên VQAv2, untrust aug.: tăng cường dữ liệu không đáng tin cậy.

và (2) phản hồi trực tiếp hơn về các đoạn ảo giác, loại trừ độ lệch không mạnh mẽ và sự biến thiên ngôn ngữ.

RLHF-V tổng quát để nâng cao các MLLM khác. Để điều tra khả năng tổng quát của khung, chúng tôi áp dụng dữ liệu và cách tiếp cận của RLHF-V để căn chỉnh hành vi của LLaVA [35], một MLLM đại diện và được sử dụng rộng rãi. Kết quả thí nghiệm cho thấy rằng RLHF-V hiệu quả giảm số lượng ảo giác của LLaVA xuống 13.8 điểm tương đối, cũng như tỷ lệ ảo giác xuống 5.9 điểm tương đối. Chúng tôi cũng áp dụng RLHF-V cho các mô hình cơ sở mạnh hơn và xây dựng OmniLMM-12B [3] đạt được kết quả SoTA mới trên nhiều bộ kiểm tra ảo giác. Ví dụ, OmniLMM-12B chỉ thể hiện 4.5% ảo giác mức đề cập trên Object HalBench. Hơn nữa, OmniLMM-12B cũng cho thấy hiệu suất hàng đầu trong số các mô hình có kích thước tương đương trên nhiều bộ kiểm tra (1637 trên MME-Perception [17], 71.1 trên SeedBench-I [25]). Kết quả chứng minh rằng RLHF-V có thể áp dụng trên các MLLM khác nhau để cải thiện độ tin cậy.

RLHF-V giảm ảo giác từ tương quan và quá tổng quát hóa. Các LLM sở hữu kiến thức thế giới phong phú và khả năng tổng quát hóa mạnh mẽ. Không có phản hồi tích cực/tiêu cực thích hợp của con người, các MLLM có thể quá tổng quát hóa để tạo ra các khái niệm có tương quan cao và hợp lý, dẫn đến ảo giác. Ví dụ, một trường hợp ảo giác phổ biến được quan sát trên các MLLM khác nhau là tuyên bố sự hiện diện của người miễn là họ nhìn thấy hình ảnh của đường phố. Để định lượng vấn đề, chúng tôi chọn một tập hợp các cảnh đại diện {phòng khách, nhà bếp, phòng tắm, đường phố}. Đối với mỗi cảnh, chúng tôi xác định các hình ảnh tương ứng trong COCO bằng cách khớp từ vựng các chú thích với tên cảnh. Sau đó chúng tôi thu được 10 đối tượng thường gặp nhất trong cảnh từ các chú thích đối tượng COCO. Chúng tôi so sánh tỷ lệ ảo giác mức phản hồi cho các đối tượng này (1) trung bình trên tất cả các mẫu kiểm tra, và (2) trên các mẫu dưới cảnh mục tiêu. Các mô hình dễ bị quá tổng quát hóa sẽ mong đợi sự gia tăng đáng kể trong tỷ lệ ảo giác (∆).

Từ kết quả thí nghiệm trong Bảng 2, chúng tôi quan sát rằng: (1) Tất cả các mô hình bao gồm GPT-4V cho thấy sự gia tăng đáng kể trong tỷ lệ ảo giác, chứng minh giả thuyết quá tổng quát hóa. (2) RLHF-V thể hiện sự thay đổi nhỏ nhất trong tỷ lệ ảo giác, thậm chí còn mạnh mẽ hơn GPT-4V. Lý do cho độ bền vững là RLHF-V cung cấp phản hồi sửa chữa chi tiết tích cực/tiêu cực quan trọng của con người cho các MLLM, giúp học các ranh giới hành vi rõ ràng giữa tổng quát hóa hợp lý và quá tổng quát hóa. (3) RLHF-V đạt được tỷ lệ ảo giác thấp nhất cho các đối tượng thông thường này cả trung bình và đặc biệt dưới các cảnh thông thường. Điều này làm cho RLHF-V được ưa thích trong các ứng dụng thế giới thực thực tế.

Nghiên cứu Ablation. Để điều tra đóng góp của từng thành phần, chúng tôi thực hiện nghiên cứu ablation. Từ kết quả thí nghiệm trong Bảng 3, chúng tôi có thể quan sát rằng: (1) Học phản hồi con người với DPO vanilla dẫn đến suy giảm hiệu suất, cho thấy ưu điểm của DDPO trong việc khai thác sở thích con người chi tiết. (2) Tinh chỉnh trên VQAv2 dẫn đến giảm đáng kể trong tỷ lệ ảo giác so với mô hình cơ sở. Điều này tiết lộ giá trị của các bộ dữ liệu được chú thích bởi con người truyền thống từ góc độ mới về giảm thiểu ảo giác. (3) Bao gồm tăng cường dữ liệu không đáng tin cậy (tức là cắt hình ảnh) trong huấn luyện làm tổn hại hiệu suất trên cả ảo giác và VQAv2. Điều này cho thấy rằng tăng cường dữ liệu bất cẩn có thể là con dao hai lưỡi trong huấn luyện MLLM.

Nghiên cứu Trường hợp. Để cung cấp hiểu biết trực quan và so sánh các mô hình khác nhau, chúng tôi cung cấp kết quả định tính trong Hình 3. Chúng tôi hiển thị các trường hợp trong hai kịch bản đại diện: (1) QA ngắn (tức là các câu hỏi có thể được trả lời đầy đủ bằng một vài từ). Mô hình của chúng tôi thường duy trì sự cân bằng tốt giữa tính hữu ích, sự hấp dẫn và độ rõ ràng. So sánh, LLaVA-RLHF thường hấp dẫn hơn nhiều, đưa ra các phần mở rộng dài tuy nhiên có thể ít hợp lý hoặc liên quan hơn. (2) QA dài (tức là các câu hỏi đòi hỏi văn bản dài để trả lời). Chúng tôi quan sát rằng các MLLM dễ bị ảo giác đáng kể hơn trong QA dài, vì nó thường đòi hỏi khả năng toàn diện hơn từ nhiều góc độ. Ví dụ, InstructBLIP và LLaVA-RLHF có thể tự tin mô tả các đối tượng không tồn tại trong một tỷ lệ lớn phản hồi của họ, trong khi RLHF-V đưa ra ít ảo giác đáng kể hơn trong khi cung cấp lượng thông tin hiệu quả tương đương. Chúng tôi chuyển độc giả đến phụ lục để biết thêm kết quả định tính.

5. Công trình Liên quan
Mô hình Ngôn ngữ Lớn Đa phương thức. Xu hướng gần đây trong học đa phương thức đã chứng kiến thành công của việc xây dựng các MLLM bằng cách kết nối bộ mã hóa thị giác với các LLM mạnh mẽ [11, 20, 26, 58, 61]. Mô hình huấn luyện MLLM hiện tại thường bao gồm hai giai đoạn: (1) Tiền huấn luyện. Các mô hình được tiền huấn luyện trên các cặp hình ảnh-văn bản quy mô lớn [6, 14, 27, 54, 60] hoặc dữ liệu xen kẽ [4, 5, 20] để

--- TRANG 8 ---
Vào lúc nào tấm ảnh này được chụp?

Cảnh này có vẻ được chụp vào ban ngày, vì có thể thấy ánh sáng mặt trời chiếu vào qua cửa sổ lớn trong nhà bếp. Ánh sáng sáng tạo ra bầu không khí chào đón và hấp dẫn, làm cho nhà bếp cảm thấy rộng rãi và sống động hơn.

Tấm ảnh được chụp vào ban ngày.

RLHF-V (Công trình này)

InstructBLIP

Tấm ảnh được chụp vào ban ngày, như được chỉ ra bởi sự hiện diện của ánh sáng tự nhiên và không có ánh sáng nhân tạo nào. Ánh sáng mặt trời chiếu vào qua cửa sổ và độ sáng tổng thể của căn phòng cho thấy rằng tấm ảnh được chụp trong giờ ban ngày. Điều này có thể là vào buổi sáng hoặc chiều, tùy thuộc vào thời gian cụ thể trong ngày và vị trí của mặt trời so với cửa sổ.

Sự hiện diện của các đồ vật khác nhau trong phòng, chẳng hạn như chai, cốc, bát và chậu cây, càng hỗ trợ ý tưởng rằng tấm ảnh được chụp vào ban ngày. Những đồ vật này có khả năng được sử dụng hoặc đặt ở vị trí tương ứng của chúng trong ban ngày, vì mọi người có xu hướng tham gia vào nhiều hoạt động hơn và chuẩn bị bữa ăn trong ban ngày.

Kết luận, tấm ảnh được chụp vào ban ngày, như được chứng minh bởi ánh sáng tự nhiên, sự vắng mặt của ánh sáng nhân tạo, và sự hiện diện của các đồ vật khác nhau trong phòng thường được sử dụng trong giờ ban ngày.

LLaVA-RLHF

QA ngắn

Tấm ảnh này về gì? Vui lòng trả lời chi tiết.

Hình ảnh mô tả một con chó đen nhỏ ngồi trên sàn gỗ trong phòng khách, xem màn hình tivi. Tivi có vẻ được gắn trên tường, và đang hiển thị hình ảnh của một con chó khác.

Hình ảnh mô tả một con chó đen ngồi trên sàn trước một màn hình TV lớn. Màn hình TV đang hiển thị hình ảnh của một con chó, có vẻ cùng giống với con chó trong phòng. Con chó dường như đang mê mẩn chương trình TV và tập trung vào màn hình. Có một số con chó khác nhìn thấy trong phòng, nhưng chúng không gần TV như chủ thể chính. Một con chó ở gần phía bên trái của TV, trong khi một con khác ở gần trung tâm phòng. Con chó thứ ba có thể thấy phía bên phải của hình ảnh. Ngoài những con chó, có hai chiếc ghế trong phòng, một cái gần phía bên trái của TV và cái kia gần trung tâm phòng.

RLHF-V (Công trình này)

InstructBLIP

Hình ảnh có một con chó đen ngồi trên sàn gỗ cứng trong một căn phòng, xem tivi. TV nằm ở phía bên trái của phòng, và con chó được đặt gần trung tâm của cảnh.

Cũng có hai người trong phòng, một người ở phía bên trái và người kia ở phía bên phải, có thể đang xem tivi cùng với con chó. Ngoài ra, có hai điều khiển từ xa nhìn thấy trong phòng, một cái gần trung tâm và cái kia ở phía bên phải.

Một chiếc ghế có thể thấy trong phòng, được đặt ở phía bên phải, và một cái bát nằm ở phía bên trái của phòng, có thể để thức ăn hoặc nước cho con chó.

LLaVA-RLHF

QA dài

Hình 3. Kết quả định tính của các mô hình khác nhau về QA ngắn và QA dài. Câu trả lời đúng, phần mở rộng không hợp lý và ảo giác được tô màu tương ứng.

học ánh xạ ngữ nghĩa giữa tín hiệu thị giác và văn bản. (2) Điều chỉnh Hướng dẫn. Để cho phép mô hình có khả năng tuân theo hướng dẫn, các MLLM được tinh chỉnh thêm trên dữ liệu hướng dẫn thị giác, bao gồm các bộ sưu tập bộ dữ liệu được chú thích bởi con người hiện có [14, 28, 34], và dữ liệu được tạo từ ChatGPT/GPT-4 [28, 33, 35, 60]. Mặc dù thành công, các MLLM hiện tại gặp phải vấn đề ảo giác nghiêm trọng [29, 32, 33, 48]. Đáng chú ý, ngay cả sau những nỗ lực rộng rãi, GPT-4V vẫn được thấy là dễ bị ảo giác, tạo ra các lỗi thực tế cơ bản một cách tự tin [37]. Vấn đề này làm suy yếu các ứng dụng thực tế của các MLLM đặc biệt trong các kịch bản có mức độ rủi ro cao, gần đây đã thu hút sự chú ý ngày càng tăng từ cộng đồng.

Căn chỉnh Hành vi cho LLM. Căn chỉnh hành vi tác nhân ngôn ngữ với sở thích con người đã nổi lên như một hướng nghiên cứu đầy hứa hẹn [22, 24]. Các cách tiếp cận quan trọng trong LLM bao gồm điều chỉnh hướng dẫn (hoặc tinh chỉnh có giám sát) và RLHF [39, 47]. Trong khi tinh chỉnh có giám sát phù hợp cho việc căn chỉnh hành vi cơ bản [15, 49], do sự không khớp giữa mục tiêu tối đa hóa khả năng và sở thích con người, nó có thể đưa vào hoặc khuếch đại ảo giác [38, 39]. Do đó, RLHF được chấp nhận rộng rãi cho việc căn chỉnh hành vi và sở thích thêm [8, 13, 38], nơi tối ưu hóa chính sách gần đúng (PPO) [45] được công nhận là kỹ thuật chính. Các thích ứng sau đó cố gắng ổn định quá trình tối ưu hóa [42] và bao gồm các tín hiệu chi tiết hơn [30, 56]. Tuy nhiên, RLHF hiếm khi được khám phá trong các MLLM để căn chỉnh hành vi mô hình với con người.

Giảm Ảo giác cho MLLM. Một số nỗ lực sơ bộ đã được thực hiện để giảm thiểu vấn đề ảo giác trong các MLLM. LRV [33] tạo dữ liệu hướng dẫn với phản hồi tiêu cực, và giảm thiểu ảo giác bằng cách giới hạn độ dài phản hồi. Tuy nhiên, giới hạn độ dài phản hồi không giải quyết vấn đề một cách bản chất, và cũng làm suy yếu tính hữu ích của phản hồi. VIGC [52] lặp lại tinh chỉnh dữ liệu hướng dẫn để điều chỉnh hướng dẫn tốt hơn. Woodpecker [59] đề xuất hậu chỉnh sửa ảo giác bằng cách hợp nhất đầu ra của các MLLM và một mô hình VQA chuyên gia chính xác hơn sử dụng GPT-3.5. Quy trình hậu chỉnh sửa bao gồm các công cụ bên ngoài và LLM lớn hơn nhiều so với MLLM mục tiêu trực tuyến trong nhiều giai đoạn, dẫn đến chi phí suy luận cao và độ trễ. Gunjal et al. [19] phân biệt các phần không chính xác trong phản hồi thông qua chú thích con người, và ngăn chặn nội bộ các phần ảo giác bằng tối ưu hóa ưu tiên trực tiếp. Tuy nhiên, các hành vi tích cực cho các phần ảo giác không được biết, làm cho phản hồi con người không đủ hoàn chỉnh để học ranh giới hành vi. LLaVA-RLHF đồng thời [48] sử dụng cách tiếp cận RLHF truyền thống [39] trên các MLLM, và tăng cường mô hình thưởng với các mô tả văn bản bổ sung phong phú. Do đó nó tương tự gặp thách thức với sự mơ hồ nhãn, hiệu quả học tập và huấn luyện phức tạp. So sánh, RLHF-V trình bày khung học phản hồi sửa chữa chi tiết của con người đầu tiên cho việc căn chỉnh hành vi, và có hệ thống giải quyết các nguồn ảo giác khác nhau trong huấn luyện MLLM, đạt được hiệu suất mạnh trong độ tin cậy.

6. Kết luận
Ảo giác là một vấn đề quan trọng ngăn chặn các ứng dụng thực tế của các MLLM trong các kịch bản thế giới thực. Trong công trình này, chúng tôi trình bày RLHF-V, một khung mới nâng cao độ tin cậy của các MLLM bằng căn chỉnh hành vi từ phản hồi sửa chữa chi tiết của con người. Kết quả thí nghiệm toàn diện cho thấy rằng mô hình của chúng tôi đạt được hiệu suất tốt nhất trong độ tin cậy đặc biệt trong các phản hồi dài thách thức trong khi duy trì tính hữu ích mạnh. Trong công trình này, chúng tôi thu thập phản hồi sửa chữa từ người chú thích. Trong tương lai, với sự tiến bộ của các MLLM đáng tin cậy và có khả năng hơn, chúng tôi sẽ khám phá việc thu thập sở thích chính xác từ các MLLM, có thể hỗ trợ học sở thích quy mô lớn cho việc căn chỉnh hành vi mạnh hơn. Bên cạnh đó, chúng tôi lưu ý rằng khung của RLHF-V có thể giúp giảm ảo giác trong các LLM, mà chúng tôi sẽ khám phá trong tương lai.

Đóng góp
Đóng góp của các tác giả có thể được nêu như sau:
• Trong việc khởi tạo dự án, Yuan Yao và Tianyu Yu thiết kế khung để thu thập phản hồi sửa chữa của con người. Tianyu Yu phát minh thuật toán DDPO. Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun và Tat-Seng Chua cung cấp hướng dẫn vô giá trong thiết kế dự án.
• Trong thu thập dữ liệu, Taiwen He, Haoye Zhang, Tianyu Yu và Yuan Yao phụ trách quá trình chú thích để đảm bảo chất lượng dữ liệu.
• Trong huấn luyện và đánh giá mô hình, Tianyu Yu triển khai khung huấn luyện. Tianyu Yu, Haoye Zhang và Yuan Yao thiết kế khung đánh giá. Tianyu Yu và Haoye Zhang triển khai cơ sở mã đánh giá.
• Trong viết bài, Yuan Yao và Tianyu Yu viết bài báo. Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun và Tat-Seng Chua đưa ra gợi ý để hoàn thiện bài viết.
• Cho khả năng sử dụng công khai, Tianyu Yu, Yifeng Han, Jinyi Hu và Yuan Yao thúc đẩy dự án mã nguồn mở.
• Trong suốt dự án, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun và Tat-Seng Chua cung cấp hướng dẫn và lời khuyên vô giá.

[Phần Tài liệu tham khảo được liệt kê từ [1] đến [63] - tôi sẽ không dịch phần này vì chúng là các trích dẫn học thuật tiêu chuẩn]

--- TRANG 12 ---
A. Nghiên cứu Zoom-in về GPT-4V
Chúng tôi thực hiện nghiên cứu zoom-in về RLHF-V liên quan đến GPT-4V để cung cấp hiểu biết tốt hơn về hành vi của chúng.

A.1. Mẫu Ảo giác
Chúng tôi tiến hành phân tích so sánh các phản hồi được tạo bởi RLHF-V và GPT-4V, và có những quan sát chính sau:

(1) So với RLHF-V, GPT-4V có xu hướng mô tả nhiều chi tiết hơn trong hình ảnh và elabor thêm về mối quan hệ giữa chúng. Về mặt định lượng, chúng tôi sử dụng ChatGPT để trích xuất tất cả các đề cập đối tượng trong phản hồi của GPT-4V, và thấy rằng số lượng trung bình mỗi phản hồi lớn hơn 2.1 lần so với RLHF-V. Chúng tôi chủ yếu quy kết điều này cho độ phân giải cao hơn (lớn hơn 7.6 lần so với RLHF-V) [1] và xương sống LLM mạnh mẽ hơn [37].

(2) Ảo giác của GPT-4V tập trung hơn trong một số phản hồi. Trong HumanEval, tỷ lệ ảo giác của GPT-4V về Object và Position tương đương với RLHF-V. Tuy nhiên, trong chỉ số ALL toàn diện, tỷ lệ ảo giác thấp hơn 17.3% so với RLHF-V. Để hiểu rõ hơn lý do đằng sau hiện tượng này, chúng tôi tiến hành phân tích kỹ lưỡng các kết quả đánh giá. Chúng tôi quan sát rằng các loại ảo giác khác nhau trong GPT-4V thường tập trung trong một tập con nhỏ các phản hồi, trong khi đóng góp vào tỷ lệ ảo giác trên nhiều tiểu mục. Về mặt định lượng, chúng tôi sắp xếp các phản hồi của mỗi mô hình theo số lượng ảo giác giảm dần, và vẽ đường cong tỷ lệ số lượng ảo giác so với tỷ lệ phản hồi ảo giác. Từ kết quả trong Hình 4, chúng ta có thể thấy rằng 45.6% phản hồi ảo giác hàng đầu của GPT-4V đóng góp 75% số lượng ảo giác. So sánh, 64.6% phản hồi ảo giác hàng đầu của RLHF-V đóng góp 75% ảo giác. Chúng tôi chuyển độc giả đến Phần B để biết thêm kết quả định tính.

A.2. Chưng cất đối với GPT-4V
Khi quan sát khả năng nhận thức hình ảnh chi tiết và tạo văn bản vượt trội của GPT-4V, một câu hỏi trực quan là, liệu việc chưng cất khả năng GPT-4V thông qua điều chỉnh hướng dẫn thị giác có có lợi không? Để làm điều này, chúng tôi thu thập 1.2k dữ liệu hướng dẫn thị giác về mô tả hình ảnh dài từ GPT-4V. Sau đó chúng tôi sử dụng phản hồi được tạo bởi GPT-4V để tinh chỉnh mô hình của chúng tôi. Chúng tôi quan sát rằng số lượng trung bình đề cập đối tượng trong phản hồi mô hình tăng đáng kể 1.8 lần so với mô hình gốc. Tuy nhiên, điều này có thể là con dao hai lưỡi: như được hiển thị trong Bảng 4, tỷ lệ ảo giác cũng tăng đáng kể.

Kết quả phù hợp với giả thuyết của [2]: "Nếu chúng ta giám sát mô hình đối với dữ liệu hướng dẫn vượt xa khả năng cơ bản của chính nó, chúng ta về cơ bản đang dạy mô hình tạo ảo giác." Cụ thể, mô hình của chúng tôi học tạo ra nhiều chi tiết hơn và mối quan hệ giữa chúng thông qua chưng cất đối với GPT-4V, trong khi khả năng cơ bản của mô hình không đủ cho nhu cầu này. Kết quả là, vấn đề ảo giác trở nên nghiêm trọng hơn đáng kể. Kết quả cho thấy rằng dữ liệu hướng dẫn thị giác (hoặc mục tiêu chưng cất) không phải càng mạnh càng tốt, mà nên phù hợp với khả năng cơ bản của mô hình.

[Biểu đồ]
0 20 40 60 80 100
Tỷ lệ Phản hồi Ảo giác (%) | 0 20 40 60 80 100 Tỷ lệ Số lượng Ảo giác (%)
RLHF-V | GPT-4V

Hình 4. Phân bố các đoạn ảo giác trên các phản hồi khác nhau. Ảo giác GPT-4V tập trung hơn trên một tập con nhỏ hơn các phản hồi. Hall.: Ảo giác.

B. Kết quả Định tính
Chúng tôi cung cấp thêm kết quả định tính trong phần này để tạo điều kiện cho hiểu biết trực quan và so sánh các mô hình khác nhau tốt hơn. Dựa trên kết quả định tính, chúng tôi có những quan sát sau:

(1) RLHF-V thường thể hiện ít ảo giác hơn trong cả kịch bản QA ngắn và QA dài, so với các mô hình mã nguồn mở như LLaVA-RLHF và InstructBLIP, như được hiển thị trong Hình 5, 6, 7, và 8.

(2) GPT-4V mô tả chi tiết hơn về các chi tiết trong hình ảnh như được hiển thị trong Hình 6, 8, 9 và 10. Ví dụ, trong Hình 9, GPT-4V đề cập đến các chấm đen trên mỗi viên gạch trong khi RLHF-V không mô tả những chi tiết này.

(3) RLHF-V chống lại vấn đề quá tổng quát hóa tốt hơn như được hiển thị trong Hình 9 và Hình 10. Trong Hình 9, GPT-4V sai lầm đề cập đến các đối tượng có liên quan cao đến cảnh trong khi không được hiển thị trong hình ảnh như exhaust, hood, và bottle.

C. Chi tiết Triển khai
Chúng tôi cung cấp thêm chi tiết triển khai trong phần này để tái tạo tốt hơn. Được hưởng lợi từ hiệu quả cao của huấn luyện, chúng tôi làm cho tất cả các tham số có thể huấn luyện trong quá trình huấn luyện, chỉ tốn chưa đến 1 giờ trên 8 GPU A100 tổng cộng. Chúng tôi thấy thực nghiệm rằng áp dụng khởi động dài hơn (10% bước huấn luyện) có thể làm cho huấn luyện ổn định hơn và do đó áp dụng cài đặt này cho tất cả thí nghiệm trong bài báo này. Đối với thu thập dữ liệu, bên cạnh các lời nhắc thu được từ [60], chúng tôi cũng sử dụng các lời nhắc mô tả hình ảnh được tạo bởi GPT-4 trong quá trình chú thích được liệt kê trong Bảng 5.

[Các bảng và hình ảnh còn lại được giữ nguyên định dạng như trong bản gốc]
