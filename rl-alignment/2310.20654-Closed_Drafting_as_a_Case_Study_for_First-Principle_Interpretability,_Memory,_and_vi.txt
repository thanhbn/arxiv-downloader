# 2310.20654.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2310.20654.pdf
# Kích thước tệp: 881226 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Closed Drafting như một Nghiên cứu Trường hợp cho Khả năng Diễn giải Nguyên lý Cơ bản, Bộ nhớ, và Khả năng Tổng quát hóa trong Học Tăng cường Sâu
Ryan Rezai1,*, Jason Wang2,*
1Đại học Waterloo,2Đại học Harvard
rrezai@uwaterloo.ca, jasonwang1@college.harvard.edu

Tóm tắt
Closed drafting hay "pick and pass" là một cơ chế trò chơi phổ biến trong đó mỗi lượt người chơi chọn một thẻ hoặc yếu tố có thể chơi khác từ tay của họ và chuyền phần còn lại cho người chơi tiếp theo. Trong bài báo này, chúng tôi thiết lập các phương pháp nguyên lý cơ bản để nghiên cứu khả năng diễn giải, tổng quát hóa, và bộ nhớ của các mô hình Deep Q-Network (DQN) chơi trò chơi closed drafting. Cụ thể, chúng tôi sử dụng một họ trò chơi closed drafting phổ biến gọi là "Sushi Go Party", trong đó chúng tôi đạt được hiệu suất tiên tiến nhất. Chúng tôi khớp các quy tắc quyết định để diễn giải chiến lược ra quyết định của các tác nhân DRL được huấn luyện bằng cách so sánh chúng với sở thích xếp hạng của các loại người chơi khác nhau. Vì Sushi Go Party có thể được biểu diễn như một tập hợp các trò chơi liên quan chặt chẽ dựa trên bộ thẻ đang chơi, chúng tôi định lượng khả năng tổng quát hóa của các mô hình DRL được huấn luyện trên nhiều bộ thẻ khác nhau, thiết lập một phương pháp để đánh giá hiệu suất tác nhân như một hàm của sự không quen thuộc môi trường. Sử dụng bộ nhớ có thể tính toán rõ ràng về tay của các người chơi khác trong trò chơi closed drafting, chúng tôi tạo ra các thước đo khả năng của các mô hình DRL để học bộ nhớ.

Giới thiệu
Lĩnh vực học tăng cường sâu (DRL) đang trải qua tiến bộ đầy hứa hẹn trong việc giải quyết nhiều vấn đề khó khăn (Li 2018). Đồng thời, một số vấn đề đã được xác định ngăn cản việc triển khai rộng rãi DRL trong các nhiệm vụ thực tế. Một trong những vấn đề được trích dẫn phổ biến nhất là thiếu khả năng diễn giải tác nhân (Glanois et al. 2021). Các chính sách được phát triển bởi các mô hình DRL thường không thể được hiểu bởi con người. Điều này tạo ra vấn đề trong việc khắc phục sự cố, quy định, và giám sát. Một vấn đề liên quan là khả năng của các mô hình DRL hoạt động đáng tin cậy trong các môi trường chưa thấy (Zhang et al. 2018). DRL thường được sử dụng cho các nhiệm vụ có tính quan sát từng phần, trong đó bộ nhớ là một yếu tố quan trọng. Mặc dù vậy, khả năng của con người để diễn giải vai trò của bộ nhớ trong các phương pháp DRL hiện tại là hạn chế (Paischer et al. 2023).

Có những yếu tố của trò chơi closed drafting hữu ích cho việc nghiên cứu khả năng diễn giải, tổng quát hóa, và bộ nhớ trong DRL. Việc diễn giải ra quyết định của mô hình được làm dễ dàng hơn bởi số lượng thẻ hạn chế có sẵn cho mô hình lựa chọn tại một thời điểm nhất định. Điều này có nghĩa là sở thích chung cho thẻ này hơn thẻ khác có thể được đo lường, và chiến lược ra quyết định đã học của các mô hình có thể được tóm tắt theo cách có thể hiểu được đối với người bình thường. Khả năng tổng quát hóa có thể được nghiên cứu thông qua bản chất có thể tùy chỉnh cao của card drafting, trong đó các bộ thẻ khác nhau có thể đang chơi, và mỗi cấu hình liên quan đến cấu hình khác ở mức độ có thể đo lường. Bộ nhớ có thể được nghiên cứu do tính quan sát từng phần có mặt trong trò chơi closed drafting, và do thực tế rằng bộ nhớ về các tay trước đó là một phần của chiến lược thành công, nhưng không được cung cấp trực tiếp cho người chơi. Điều này có nghĩa là chúng ta có thể bao gồm hoặc loại trừ bộ nhớ một cách rõ ràng để nghiên cứu cách/liệu các tác nhân có học được bộ nhớ hay không.

Theo hiểu biết của chúng tôi, không có nghiên cứu trước đây nào đã nghiên cứu closed drafting, mặc dù một số đã tình cờ sử dụng môi trường trò chơi với cơ chế này. Cụ thể, bestseller Sushi Go đã được nghiên cứu lần đầu bởi (Soen 2019) người đã đơn giản hóa setting thành một setting thông tin đầy đủ, (Liu 2020) người đã giới thiệu baseline của một tác nhân dựa trên quy tắc theo một bảng xếp hạng ưu tiên thẻ được thiết lập, và (Klang et al. 2021) người đã đặt Sushi Go trong setting chung của trò chơi trên bàn, mặc dù không có tính cụ thể của closed drafting mà chúng tôi dự định nghiên cứu.

Những đóng góp của chúng tôi trong bài báo này là (1) tạo ra một môi trường DRL đa tác nhân closed drafting Sushi Go Party hỗ trợ việc bao gồm/loại trừ bộ nhớ một cách rõ ràng, (2) thiết lập các thước đo để hiểu hành vi DRL qua các điều kiện bộ nhớ, (3) định lượng động lực tổng quát hóa để xác định tỷ lệ giảm hiệu suất ngoài phân phối, và (4) chứng minh khả năng diễn giải ra quyết định mô hình thấu đáo hơn với các quy tắc quyết định và bảng xếp hạng sở thích được làm khả thi bởi khung closed drafting.

Hình 1: Một Ví dụ về Tay Bài Ban đầu được Chia và Lượt Đầu tiên cho một Trò chơi Sushi Go Party Bốn Người chơi sử dụng Cấu hình "My First Meal"

--- TRANG 2 ---
Thiết lập Sushi Go Party được chơi trong nhóm từ 2 đến 8 người chơi. Các thí nghiệm của chúng tôi sẽ trong nhóm 4 người chơi. Tay bài 9 thẻ được chia cho mỗi người chơi. Những tay bài này được che giấu, và không được nhìn thấy bởi các người chơi khác. Mỗi người chơi chọn một thẻ duy nhất, đặt nó úp mặt xuống trên bàn, sau đó chuyển tay bài còn lại cho người bên cạnh họ. Sau này lượt kết thúc và họ lật và tiết lộ thẻ họ đã đặt úp mặt xuống. Thẻ này vẫn úp mặt lên trên bàn cho đến khi vòng kết thúc. Quá trình này lặp lại cho đến khi tất cả thẻ được chơi, điều này sẽ kết thúc vòng. Do không thể nhìn thấy rõ ràng tay bài của các người chơi khác, quá trình này có tính quan sát từng phần. Điểm được tính từ các thẻ được chơi trên bàn, thường có tương tác (ví dụ, thẻ "Tempura" chỉ cho điểm nếu có hai thẻ đang chơi). Quá trình này lặp lại trong 3 vòng, và người chiến thắng là người chơi có nhiều điểm nhất cuối cùng.

Chúng tôi định nghĩa trò chơi closed drafting nói chung như POMDP hữu hạn chân trời (µ, S, A, P, r, H): bộ của phân phối trạng thái ban đầu, không gian trạng thái, không gian hành động, chuyển tiếp trạng thái, hàm thưởng, và độ dài chân trời. Chúng tôi mã hóa một tay thẻ như một vector tần suất (c1, c2, . . . , cn)T trong đó ci biểu thị số lượng thẻ loại i trong tay, và n biểu thị tổng số thẻ duy nhất. Không gian hành động là loại thẻ để chơi A={1, . . . , n}. Không gian trạng thái được chỉ định bởi tay bài của mỗi người chơi và các thẻ đã chơi, nhưng quan sát loại trừ tay bài của các người chơi khác. Cơ chế closed drafting giới thiệu hạn chế rằng độ dài của mỗi tay bằng nhau, và độ dài của một tay và độ dài của các thẻ đã chơi của người chơi tổng cộng bằng H. Hơn nữa, động lực chuyển tiếp là một hàm đơn giản chỉ đơn giản là xoay các tay bài—điều này cho phép chúng ta tính ra một số thẻ của các tay khác, và chúng ta có thể tùy chọn cung cấp rõ ràng thông tin được điều tra tối ưu trong quan sát. Chính động lực chuyển tiếp đơn giản này làm cho bộ nhớ dễ nghiên cứu hơn đáng kể trong môi trường closed drafting bị hạn chế.

Môi trường Sushi Go Party chỉ cho phép một tập con của tổng số thẻ duy nhất có sẵn được sử dụng trong một trò chơi duy nhất. Trong một trò chơi điển hình, 9 trong số 37 thẻ duy nhất có sẵn sẽ được chọn để sử dụng. Trò chơi có thể trở nên cạnh tranh hơn tùy thuộc vào việc chọn lựa thẻ được chọn. Chúng tôi gọi việc chọn lựa này là cấu hình trò chơi.

Bộ nhớ Chiến lược để chơi trò chơi closed drafting kết hợp một yếu tố ghi nhớ. Tay bài mà người chơi có ở cuối vòng được chuyển cho người chơi lân cận, và đổi lại họ nhận được một tay cùng kích thước từ người chơi lân cận. Điều này là vòng tròn, có nghĩa là người chơi có thể sử dụng kiến thức về các tay trước đó trong việc ra quyết định của họ. Trong môi trường Sushi Go Party của chúng tôi, chúng tôi cho phép các tác nhân DRL giữ lại một đầu vào bộ nhớ rõ ràng của các tay trước đó.

Để diễn giải cách bộ nhớ rõ ràng điều chỉnh hành vi tác nhân, chúng tôi đề xuất hai phương pháp nhắm mục tiêu các khía cạnh khác biệt của ý nghĩa học bộ nhớ. Phương pháp đầu tiên là t-test cổ điển để so sánh liệu các tác nhân được cung cấp bộ nhớ rõ ràng và các tác nhân không có tính năng bộ nhớ có sự khác biệt có ý nghĩa thống kê trong hiệu suất trò chơi. Tuy nhiên, điều này không có nghĩa là bộ nhớ đang giúp theo cách chúng ta nghĩ, đặc biệt nếu bộ nhớ không thực sự được sử dụng bởi mô hình hoặc nếu sự khác biệt chỉ phát sinh vì kích thước đầu vào bổ sung. Do đó chúng tôi phát minh ra một bài kiểm tra ghi nhớ mới dựa trên việc nhiễu loạn phần bộ nhớ của đầu vào và đo KL divergence giữa các phân phối xác suất trên không gian hành động:

MemInfluence(π) = Es′∼Pert(s)[DKL(π(·|s′)||π(·|s))]

trong đó s′∼Pert(s) biểu thị việc thay đổi một thẻ trong bộ nhớ của tay bài người chơi trước đó. Thước đo này giám sát trực tiếp hơn cách thay đổi bộ nhớ thay đổi hành động được chọn của mô hình, bất kể tính hữu ích cho hiệu suất cuối.

Tổng quát hóa Để định lượng mức độ tương tự giữa các cấu hình trò chơi, cho A, B là các tập hợp thẻ đang chơi tương ứng, chúng tôi định nghĩa khoảng cách tập hợp như:

EnvSim(A, B) = |(A∪B)\(A∩B)|.

Sau đó, chúng ta có thể mô tả hiệu suất tổng quát hóa trung bình k bước đi như hiệu suất mong đợi của một mô hình được huấn luyện trên môi trường A và được đánh giá trên môi trường B với EnvSim của k. Đây là một thước đo tự nhiên vì chúng ta mong đợi các trò chơi với các tập hợp thẻ tương tự đang chơi sẽ có động lực trò chơi tương tự, và càng nhiều thẻ khác nhau, càng nhiều sự khác biệt phát sinh. Điều này quan trọng để hiểu rõ hơn và diễn giải các giới hạn của khả năng tác nhân và hành vi của chúng ngoài phân phối.

Khả năng diễn giải Ngoài việc hiểu các hành vi liên quan đến bộ nhớ và khả năng tổng quát hóa, chúng tôi muốn nhất là mô phỏng một chiến lược trung thực, phụ thuộc ngữ cảnh, và đơn giản để mô phỏng tác nhân DRL. Điều này khó khăn nói chung, nhưng có lẽ dễ tiếp cận hơn trong setting closed drafting nơi chúng ta có thể dễ dàng hiểu các kịch bản giả định và có được bảng xếp hạng sở thích rõ ràng khi người chơi chỉ còn hai thẻ để chọn. Cụ thể, để thỏa mãn các desiderata nêu trên, chúng tôi chọn sử dụng quy tắc quyết định, một phương pháp vốn có thể diễn giải ánh xạ các liên từ Boolean ngắn của các đầu vào đến phân loại đầu ra. Những điều kiện Boolean này dễ hiểu và tối ưu hóa rõ ràng cho độ chính xác của những quy tắc này trong việc giải thích hành động của mô hình (việc học chúng bao gồm khớp một ensemble cây và chọn bộ sưu tập quy tắc chính xác nhất nhưng khác biệt từ các nhánh cây quyết định). Chúng tôi sử dụng triển khai của SkopeRules (Goix et al. 2020), tạo ra các quy tắc if-then cho mỗi thẻ được lọc cho độ chính xác và recall. Những cái này mô tả sở thích mô hình cho thẻ này hơn thẻ khác, và chúng ta có thể lấy mẫu datasets ở các tình huống cụ thể để hiểu chiến lược mô hình giải thích hành vi cụ thể ngữ cảnh.

Thí nghiệm
Danh sách Ưu tiên Người chơi Con người Chúng tôi mô tả sở thích của các loại người chơi con người khác nhau bằng cách xây dựng danh sách ưu tiên dựa trên dữ liệu chơi từ trang web Board Game Arena. Dữ liệu từ 172,357 trò chơi Sushi Go Party được thu thập. Số điểm trung bình tính từ mỗi thẻ qua tất cả trò chơi được sử dụng để xây dựng danh sách ưu tiên chúng tôi gọi là "Average Human Player Priority". Nó xấp xỉ giá trị của mỗi thẻ đối với người chơi con người trung bình. Board Game Arena gán điểm ELO cho mỗi người chơi đăng ký

--- TRANG 3 ---
trên trang web. Nhìn vào sáu người chơi hàng đầu với điểm ELO cao nhất và với ít nhất 400 trò chơi đã chơi, chúng tôi xây dựng danh sách ưu tiên gọi là "Elite Human Player Priority". Những người chơi ưu tú này đã chơi tổng cộng 7728 trò chơi.

Tác nhân Giống Con người Để đánh giá hiệu suất của các tác nhân DRL của chúng tôi, chúng tôi chuẩn bị một tác nhân giống con người đơn giản theo danh sách "Average Human Player Priority". Các mô hình DQN được huấn luyện của chúng tôi sẽ chơi chống lại những tác nhân giống con người này trong các thí nghiệm tiếp theo.

Huấn luyện Mô hình Cho tất cả thí nghiệm, chúng tôi huấn luyện các mô hình DQN qua self-play sử dụng cùng kiến trúc mạng neural (4 lớp ẩn của 128 đơn vị mỗi lớp). Các siêu tham số khác được điều chỉnh thủ công cho đến hiệu suất baseline. Hàm thưởng là điểm ghi được cộng 100 ở cuối khi thắng.

Xu hướng theo Khoảng cách Cấu hình Trò chơi Để quan sát động lực tổng quát hóa của các tác nhân DRL, chúng tôi chuẩn bị 5 cấu hình trò chơi. Một cái dựa trên cấu hình "My First Meal" được tìm thấy trong hướng dẫn sử dụng cho Sushi Go Party, dành cho người chơi mới bắt đầu. Một cái khác dựa trên cấu hình "Cutthroat Combo", dành cho người chơi cao cấp. Chúng tôi nội suy 3 cấu hình trò chơi ở giữa "My First Meal" và "Cutthroat Combo" bằng cách từng bước hoán đổi một thẻ duy nhất cho thẻ khác. Điều này cho chúng ta một loạt cấu hình trò chơi từng bước khó khăn hơn.

Chúng tôi huấn luyện 10 mô hình DQN trong 10 epoch trên mỗi trong 5 cấu hình trò chơi, sau đó kiểm tra mỗi mô hình được huấn luyện trên tất cả 5 cấu hình trò chơi chống lại tác nhân giống con người của chúng tôi. Có 25 tổ hợp cấu hình huấn luyện-kiểm tra, với giá trị EnvSim từ 0 đến 4. Mỗi trong 25 tổ hợp cấu hình huấn luyện-kiểm tra được sử dụng để chơi 100 vòng Sushi Go Party, 100 lần. Mỗi batch 100 vòng sử dụng một random seed khác nhau. Tỷ lệ thắng cho 100 vòng được tính toán, và tỷ lệ thắng trung bình được tính tổng và sắp xếp theo EnvSim (xem Hình 2).

Hình 2: Tỷ lệ Thắng Giảm càng Lớn Khoảng cách Tập hợp

Những Phát hiện Bộ nhớ Tinh tế Chúng tôi sử dụng cấu hình "My First Meal" nơi chúng tôi tin rằng hiệu ứng bộ nhớ sẽ mạnh do sự hiện diện của các thẻ "hoàn thành bộ" cần tất cả yếu tố để ghi bất kỳ điểm nào, vì vậy giá trị điểm của chúng về cuối rất phụ thuộc vào các thẻ còn lại. Chúng tôi xấp xỉ thước đo MemInfluence của chúng tôi bằng cách lấy mẫu 10 nhiễu loạn ngẫu nhiên cho một trạng thái nhất định, và lấy mẫu 100 trạng thái từ nửa thứ hai của vòng khi các thẻ đã xoay ít nhất một lần quanh.

Một t-test cho p-value có ý nghĩa thống kê 1.4×10^-65, và sự khác biệt trong thưởng trung bình là đáng kể 14.08. Tuy nhiên, MemInfluence là 1.96×10^-4, có nghĩa là các phân phối trên hành động tiếp theo về cơ bản không thay đổi. Do đó, từ góc độ hiệu suất DQN làm tốt hơn đáng kể với bộ nhớ được cung cấp rõ ràng, gợi ý thất bại của mô hình trong việc nắm bắt hiệu ứng bộ nhớ một mình. Tuy nhiên, chúng tôi thấy rằng thậm chí DQN được huấn luyện với bộ nhớ rõ ràng dường như không bị ảnh hưởng quá nặng bởi nó theo thước đo MemInfluence nhỏ.

Một điểm tích cực là thước đo MemInfluence tìm ra các trạng thái nơi việc nhiễu loạn bộ nhớ gây ra sự thay đổi lớn nhất trong hành động được chọn. Hóa ra điều này xảy ra trong vòng thứ hai đến cuối, phù hợp với trực giác của chúng tôi rằng đây là khi bộ nhớ trở nên đặc biệt quan trọng (ví dụ, trong việc biết liệu bạn có thể hoàn thành một bộ), trong trường hợp đó sự khác biệt tối đa từ một sự thay đổi thẻ duy nhất là một sự thay đổi 6% trong phân phối xác suất để quyết định giữa Tempura (một thẻ hoàn thành bộ) và Soy Sauce (không phải thẻ hoàn thành bộ).

So sánh Ưu tiên Có thể Diễn giải Chúng tôi lấy mẫu một dataset của các cặp quan sát-hành động từ quỹ đạo của ba tác nhân DQN đại diện với hiệu suất khác nhau chống lại các tác nhân giống con người để khớp quy tắc quyết định (xem Hình 3), nhìn vào vòng thứ hai đến cuối với hai thẻ còn lại để có được sở thích từng cặp. Chúng tôi sử dụng điều này để tái tạo bảng xếp hạng ưu tiên gần nhất cho mỗi trong ba tác nhân DQN đại diện. Chúng tôi so sánh những cái này với danh sách ưu tiên của các loại người chơi con người khác nhau (xem Hình 4).

Hình 3: Tỷ lệ Thắng Tác nhân DQN Tệ nhất, Trung bình, và Tốt nhất chống lại Tác nhân Giống Con người Trung bình; 500 Bộ 3 Vòng mỗi Thanh

--- TRANG 4 ---
Hình 4: Ưu tiên Con người so với DQN1; Sở thích Tăng cho Squid khi Hiệu suất Cải thiện được Làm nổi bật

Thảo luận
Chúng tôi chứng minh rằng closed drafting là một lớp môi trường ít được nghiên cứu nhưng rất hữu ích để đặc trưng hóa khả năng diễn giải, tổng quát hóa, và bộ nhớ của các thuật toán DRL. Chúng tôi sử dụng các thuộc tính độc đáo của closed drafting (tức là, tính quan sát từng phần nhưng dễ học) để đề xuất các thước đo để đo lường hiệu ứng bộ nhớ và khả năng của các tác nhân DRL để học bộ nhớ. Thêm vào đó, tính granular của các cấu hình trò chơi cho phép chúng ta đánh giá khả năng tổng quát hóa theo cách môi trường kiểm tra ngoài phân phối như thế nào. Ngoài những phương pháp ngầm này được thiết kế để hiểu rõ hơn các tác động của các hành vi bộ nhớ và khả năng tổng quát hóa cụ thể theo cách đơn giản, chúng tôi xây dựng danh sách ưu tiên rõ ràng đại diện cho các tác nhân DRL được huấn luyện có thể dễ dàng được diễn giải chống lại sở thích của các loại người chơi con người khác nhau để quan sát trực giác sự khác biệt trong việc ra quyết định mô hình DRL. Chúng tôi quan sát sự di chuyển của một yếu tố có thể chơi duy nhất (trong trường hợp này là Squid) qua danh sách ưu tiên như giải thích cho sự khác biệt trong hiệu suất giữa các mô hình chống lại các tác nhân giống con người.

Công việc tương lai bao gồm chính thức hóa một phương pháp chung để xây dựng danh sách ưu tiên và mở rộng phân tích này sang các trò chơi closed drafting khác. Chạy các thí nghiệm của chúng tôi trên nhiều môi trường hơn có thể làm sáng tỏ các xu hướng phổ quát hơn qua các trò chơi closed drafting. Các trò chơi closed drafting đủ đơn giản có thể được giải quyết đầy đủ với lý thuyết trò chơi và cũng đủ dễ tiếp cận cho một thăm dò khả năng diễn giải cơ học để cung cấp thêm bằng chứng cho sự biểu hiện của bộ nhớ.

Tài liệu tham khảo
Glanois, C.; Weng, P.; Zimmer, M.; Li, D.; Yang, T.; Hao, J.;
và Liu, W. 2021. A Survey on Interpretable Reinforcement
Learning. arXiv preprint arXiv:2112.13112 .

Goix, N.; Birodkar, V.; Gardin, F.; Schertzer, J.-M.; Jeong,
H.; Kumar, M.; Gramfort, A.; Staley, T.; la Tour, T. D.; Deng,
B.; C; Pedregosa, F.; Wu, L.; Rokem, A.; Jackson, K.; và
Rahim, M. 2020. scikit-learn-contrib/skope-rules v1.0.1.

Klang, C.-M. E.; Enhörning, V.; Alvarez, A.; và Font, J.
2021. Assessing Simultaneous Action Selection and Com-
plete Information in TAG with Sushi Go! Trong 2021 IEEE
Conference on Games (CoG), 01–04. IEEE.

Li, Y. 2018. Deep Reinforcement Learning. arXiv preprint
arXiv:1810.06339 .

Liu, M. 2020. Reinforcement Learning & Sushi Go!

Paischer, F.; Adler, T.; Hofmarcher, M.; và Hochreiter, S.
2023. Semantic HELM: A Human-Readable Memory for
Reinforcement Learning. arXiv preprint arXiv:2306.09312 .

Soen, A. 2019. Making Tasty Sushi Using Reinforcement
Learning and Genetic Algorithms.

Zhang, C.; Vinyals, O.; Munos, R.; và Bengio, S. 2018.
A Study on Overfitting in Deep Reinforcement Learning.
arXiv preprint arXiv:1804.06893 .
