# 2309.02632.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2309.02632.pdf
# Kích thước tệp: 5714115 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Học Tăng Cường Sâu từ Thiết Kế Ưu Tiên Phân Cấp
Alexander Bukharin1, Yixiao Li1, Pengcheng He2, và Tuo Zhao1
1Viện Công nghệ Georgia
2Microsoft
11 tháng 6, 2024
Tóm tắt
Thiết kế phần thưởng là một khía cạnh cơ bản nhưng đầy thách thức của học tăng cường (RL). Các nhà nghiên cứu thường sử dụng tín hiệu phản hồi từ môi trường để thiết kế thủ công một hàm phần thưởng, nhưng quá trình này không phải lúc nào cũng hiệu quả do quy mô khác nhau và sự phụ thuộc phức tạp của các tín hiệu phản hồi. Bài báo này cho thấy rằng bằng cách khai thác một số cấu trúc nhất định, người ta có thể làm dễ dàng quá trình thiết kế phần thưởng. Cụ thể, chúng tôi đề xuất một khung thiết kế phần thưởng phân cấp - HERON cho các tình huống: (I) Các tín hiệu phản hồi tự nhiên thể hiện tính phân cấp; (II) Phần thưởng thưa thớt, nhưng có phản hồi thay thế ít quan trọng hơn để giúp học chính sách. Cả hai tình huống đều cho phép chúng tôi thiết kế một cây quyết định phân cấp được tạo ra bởi thứ hạng tầm quan trọng của các tín hiệu phản hồi để so sánh các quỹ đạo RL. Với dữ liệu ưu tiên như vậy, chúng tôi có thể huấn luyện một mô hình phần thưởng để học chính sách. Chúng tôi áp dụng HERON cho một số ứng dụng RL, và chúng tôi thấy rằng khung của chúng tôi không chỉ có thể huấn luyện các agent hiệu suất cao trên nhiều nhiệm vụ khó khăn, mà còn cung cấp các lợi ích bổ sung như cải thiện hiệu quả mẫu và tính bền vững. Mã của chúng tôi có sẵn tại https://github.com/abukharin3/HERON .

1 Giới thiệu
Trong thập kỷ qua, những tiến bộ đáng kể trong kỹ thuật học sâu, cùng với sự tăng trưởng chưa từng có về sức mạnh tính toán, đã tạo điều kiện cho những thành tựu đáng chú ý trong lĩnh vực học tăng cường sâu (RL) trên các lĩnh vực đa dạng, bao gồm tài chính, giao thông và lập trình tự động [Deng et al., 2016, Haydari và Yılmaz, 2020, Le et al., 2022]. Một thành phần quan trọng của RL hiện đại là hàm phần thưởng, thường được định nghĩa trước trong các môi trường benchmark như OpenAI gym hoặc game [Mnih et al., 2013, Silver et al., 2016, Brockman et al., 2016]. Tuy nhiên, khi xử lý các môi trường thế giới thực phức tạp, chúng ta không thể truy cập phần thưởng thực tế, hoặc phần thưởng thưa thớt: chúng ta nhận được phần thưởng bằng không hầu hết thời gian. Do đó, việc thiết kế phần thưởng cho agent là cần thiết.

Để xây dựng phần thưởng, các nhà thực hành thường sử dụng nhiều tín hiệu phản hồi z1,...,zm, mỗi tín hiệu nắm bắt các khía cạnh khác nhau của hành vi của agent. Trong các môi trường mà phần thưởng không thể truy cập được, cách tiếp cận phổ biến nhất để sử dụng các tín hiệu này là kết hợp tuyến tính, ví dụ, r=P iωi∗zi

--- TRANG 2 ---
[Booth et al., 2023, Le et al., 2022, Zhang et al., 2019]. Các siêu tham số ωi được điều chỉnh để cung cấp một mô tả toàn diện về hành vi của agent, một quá trình thường được gọi là kỹ thuật phần thưởng [Fu et al., 2017, Wu et al., 2021]. Lấy ví dụ điều khiển đèn giao thông, Zhang et al. [2019] xem xét một tổ hợp có trọng số của độ dài hàng đợi xe, thời gian chờ trung bình và một số tín hiệu phản hồi khác làm phần thưởng. Các siêu tham số ωi được xác định bằng điều chỉnh rộng rãi trong Van der Pol và Oliehoek [2016], Wu et al. [2017], Zhang et al. [2019].

Trong các môi trường phần thưởng thưa thớt, các tín hiệu phản hồi được sử dụng để tạo thành một phần thưởng thay thế, sau đó được kết hợp với phần thưởng thưa thớt. Lấy ví dụ tạo mã, Le et al. [2022] thiết kế một hàm từng phần trong đó vùng được chia theo các tín hiệu phản hồi (ví dụ, lỗi biên dịch, lỗi runtime) và các giá trị được thiết kế bởi các chuyên gia con người.

Mặc dù các tín hiệu phản hồi có thể phục vụ như các tiêu chí hữu ích về hành vi của agent, kỹ thuật phần thưởng không phải lúc nào cũng là một cách hiệu quả để kết hợp các tín hiệu này. Điều này là do các tín hiệu phản hồi có thể có quy mô khác nhau cũng như sự phụ thuộc phức tạp với các tín hiệu phản hồi khác. Trong trường hợp này, việc xác định trọng số bởi con người trở nên thách thức, và nhiều trọng số phải được điều chỉnh đồng thời vì các tín hiệu phản hồi tương ứng của chúng thường có tương quan. Hơn nữa, trong các môi trường phần thưởng thưa thớt, một hàm phần thưởng từng phần thường yêu cầu hàng loạt thử nghiệm để xác định giá trị, và việc chia vùng cũng khó khăn vì mối quan hệ phức tạp giữa các tín hiệu phản hồi khác nhau.

Cách giải quyết các vấn đề nêu trên vẫn chưa rõ ràng. Trong khi hầu hết các công trình hiện tại đã tập trung vào kỹ thuật phần thưởng cho các ứng dụng cụ thể [Liu et al., 2020, Zhang et al., 2019], bài báo này đề xuất một khung thiết kế phần thưởng mới khi các tín hiệu phản hồi thể hiện mối quan hệ phân cấp. Mối quan hệ này tồn tại trong nhiều bài toán RL. Ví dụ, trong điều khiển đèn giao thông, độ dài hàng đợi xe vượt trội đáng kể so với thời gian chờ trung bình và các tín hiệu phản hồi khác. Khung của chúng tôi cũng phù hợp với các môi trường phần thưởng thưa thớt, nơi phần thưởng thưa thớt tự nhiên có tầm quan trọng lớn hơn so với các phần thay thế. Đối với tạo mã, các nhà thực hành tăng cường phần thưởng thưa thớt (liệu mã có vượt qua các bài kiểm tra đơn vị hay không) với các phần thay thế, chẳng hạn như loại lỗi.

Để tận dụng các cấu trúc phân cấp như vậy trong các tình huống nêu trên, chúng tôi đề xuất HERON (Học tăng cường dựa trên ưu tiên phân cấp). HERON huấn luyện một mô hình phần thưởng dựa trên ưu tiên [Bradley và Terry, 1952, Ouyang et al., 2022] thông qua so sánh quỹ đạo theo cặp. Cụ thể, chúng tôi thiết kế một cây quyết định, tại mỗi cấp độ so sánh các quỹ đạo dựa trên một tín hiệu phản hồi. Tín hiệu phản hồi chúng tôi sử dụng tại mỗi cấp độ được xác định bởi thứ hạng tầm quan trọng của nó, như được chỉ định bởi người chú thích con người.

Cây quyết định dựa trên thứ hạng tầm quan trọng này cung cấp một số lợi ích. Đầu tiên, đây là một cách tự nhiên hơn để mô phỏng quá trình quyết định của con người so với kỹ thuật phần thưởng. Khi đưa ra quyết định giữa hai lựa chọn, con người thường bắt đầu với yếu tố quan trọng nhất, sau đó tiến tới yếu tố thứ cấp, và tiếp tục cho đến các yếu tố ít quan trọng hơn còn lại. Thứ hai, việc xếp hạng các tín hiệu phản hồi thường dễ dàng hơn so với việc chỉ định trọng số số học. Thứ ba, quá trình so sánh của HERON không phụ thuộc vào giá trị tuyệt đối của một tín hiệu phản hồi, mà vào số lượng tương đối của chúng. Chúng tôi thấy rằng điều này mang lại tính bền vững bổ sung trong các tình huống mà môi trường huấn luyện thay đổi. Chúng tôi sẽ thảo luận thêm về điều này trong Phần 4.1. Cuối cùng, HERON có thể tận dụng kiến thức được huấn luyện trước, cho phép tạo ra những phần thưởng mạnh mẽ hơn.

Chúng tôi xác nhận thực nghiệm khung HERON thông qua các thí nghiệm rộng rãi trên các ứng dụng thế giới thực:

Điều khiển Đèn Giao thông. Trong điều khiển đèn giao thông [Zhang et al., 2019], có 6 tín hiệu phản hồi với thứ bậc: độ dài hàng đợi > thời gian chờ xe trung bình > các tín hiệu phản hồi khác. HERON liên tục vượt trội hơn các chính sách được huấn luyện với kỹ thuật thiết kế phần thưởng.

Tạo Mã. Tạo mã [Le et al., 2022] là một tình huống phần thưởng thưa thớt. Hầu hết các chương trình được tạo bởi chính sách không thể vượt qua tất cả các bài kiểm tra đơn vị và do đó không nhận được phần thưởng. Do đó, các phần thay thế, như loại lỗi, được thêm vào để tạo thành một hàm phần thưởng từng phần. Trong tạo mã, HERON thể hiện khả năng đạt được điểm Pass@K cao hơn so với hàm phần thưởng từng phần được chế tác thủ công được sử dụng trong các phương pháp tiên tiến.

Căn chỉnh Mô hình Ngôn ngữ. Mặc dù các mô hình ngôn ngữ mạnh mẽ, chúng không phải lúc nào cũng phù hợp với các nguyên tắc của con người [Brown et al., 2020]. Chúng tôi đề xuất sử dụng HERON để căn chỉnh các mô hình ngôn ngữ, bằng cách sử dụng các tập dữ liệu công khai về lời nhắc và đầu ra của mô hình ngôn ngữ được gắn nhãn với tính hữu ích, tính mạch lạc, tính chính xác, tính dài dòng và độ phức tạp của phản hồi. Bằng cách xếp hạng các yếu tố này và áp dụng HERON, chúng tôi có thể huấn luyện một mô hình ngôn ngữ được căn chỉnh.

Điều khiển Robot. Chúng tôi cũng đánh giá HERON trên điều khiển robot [Coumans và Bai, 2016, Brockman et al., 2016], nơi thứ bậc của các tín hiệu phản hồi không rõ ràng. Trong những môi trường này, HERON hoạt động tốt hơn kỹ thuật phần thưởng và thậm chí đạt được hiệu suất tương đương so với phần thưởng thực tế. Điều này cho thấy rằng HERON có thể huấn luyện một chính sách hợp lý ngay cả khi thứ bậc không rõ ràng.

Phần còn lại của bài báo này được tổ chức như sau: Phần 2 giới thiệu các công trình liên quan. Phần 3 giới thiệu khung thiết kế phần thưởng được đề xuất của chúng tôi, bao gồm thu thập dữ liệu, thu thập ưu tiên, học phần thưởng và học chính sách. Chúng tôi tiến hành thí nghiệm trong Phần 4. Chúng tôi thảo luận về hạn chế của phương pháp trong Phần 5.

2 Công trình Liên quan
Ngoài kỹ thuật phần thưởng, có một số công trình cố gắng cải thiện thiết kế phần thưởng cho RL.

Định hình Phần thưởng. Định hình phần thưởng nhằm tăng tốc sự hội tụ của các thuật toán RL bằng cách kết hợp thông tin phần thưởng phụ trợ thông qua các hàm định hình [Ng et al., 1999, Tenorio-Gonzalez et al., 2010, Devlin và Kudenko, 2012]. Những phương pháp này nhằm giảm thiểu tính thưa thớt của một hàm phần thưởng được định nghĩa trước. Trong khi định hình phần thưởng đã chứng minh thành công trong thực tế, nó thường cần điều chỉnh rộng rãi. Để tránh nhu cầu điều chỉnh tốn kém, một số phương pháp đã được đề xuất để tự động định hình phần thưởng bằng cách sử dụng MDP trừu tượng [Marthi, 2007], mã hóa ô [Grzes và Kudenko, 2008], và tối ưu hóa hai cấp [Fu et al., 2019, Hu et al.,

--- TRANG 3 ---
2020]. Ngược lại, công trình của chúng tôi theo đuổi một hướng khác nhau loại bỏ yêu cầu về một hàm phần thưởng được chỉ định trước và không giả định rằng phần thưởng là một kết hợp tuyến tính của các yếu tố phụ trợ.

AutoRL. AutoRL [Afshar et al., 2022, Parker-Holder et al., 2022] tự động hóa các khía cạnh khác nhau của việc lựa chọn siêu tham số trong RL, bao gồm các tham số liên quan đến phần thưởng. Đặc biệt liên quan đến công trình của chúng tôi, Faust et al. [2019] và Chiang et al. [2019] xem trọng số phần thưởng như siêu tham số và tối ưu hóa chúng bằng cách sử dụng huấn luyện dựa trên quần thể.

Học Tăng cường Nghịch đảo. Học tăng cường nghịch đảo (IRL) nhằm học một hàm phần thưởng từ các minh chứng chuyên gia [Ng et al., 2000, Abbeel và Ng, 2004, Boularias et al., 2011]. Mặc dù IRL cho phép học các hành vi phức tạp mà không cần điều chỉnh phần thưởng thủ công, nó yêu cầu hành vi tối ưu được quan sát. Những minh chứng này thường tốn kém để có được, và trong các thí nghiệm của chúng tôi, việc có được chúng sẽ đắt hơn nhiều so với việc có được thứ bậc của các tín hiệu phản hồi. Hơn nữa, các phương pháp IRL thường yêu cầu các thủ tục tối ưu hóa hai cấp không ổn định, mà phương pháp của chúng tôi không liên quan đến.

Học Tăng cường từ Phản hồi Con người. Học tăng cường từ phản hồi con người (RLHF) [Christiano et al., 2017, Ouyang et al., 2022, Bai et al., 2022] nhằm huấn luyện một mô hình chính sách phù hợp với ưu tiên của con người. Mặc dù cả RLHF và phương pháp của chúng tôi đều liên quan đến một mô hình phần thưởng dựa trên ưu tiên, trong RLHF các nhãn ưu tiên đến trực tiếp từ các người chú thích. Mặt khác, HERON tự động so sánh các quỹ đạo bằng thứ hạng tầm quan trọng của các tín hiệu phản hồi. Thứ hạng này có thể dễ dàng được thiết lập bởi một người giám sát con người nếu các tín hiệu phản hồi có thứ bậc, hoặc trong các môi trường phần thưởng thưa thớt. Chúng tôi sẽ thảo luận thêm các khác biệt trong Phần 5.

3 Phương pháp
Chúng tôi xem xét một quá trình quyết định Markov ⟨S,A,P,R,γ⟩ trong đó một agent tương tác với một môi trường qua một loạt các bước thời gian rời rạc. Tại bước thời gian t, agent quan sát st∈S, thực hiện một hành động at∈A theo một chính sách, và nhận được quan sát trạng thái tiếp theo st+1∈S và phần thưởng rt∈R.

Trong hầu hết các ứng dụng thế giới thực, phần thưởng rt không có sẵn. Do đó, mục tiêu của chúng tôi là thiết kế một mô hình phần thưởng phù hợp Rφ:S×A→ R sao cho khi tối đa hóa mục tiêu

J(θ) =Eτ∼πθ X (st,at)∈τγtRφ(st,at), (1)

hành vi của agent, được hướng dẫn bởi mô hình chính sách πθ:S×A→ R, đáp ứng kỳ vọng của chúng tôi.

Để thiết kế phần thưởng, chúng tôi sử dụng một tập hợp n tín hiệu phản hồi z1 t,...,zn t được cung cấp tại mỗi bước thời gian t. Những tín hiệu này phục vụ như nhiều phép đo của một quỹ đạo. Chúng tôi ký hiệu các đoạn của quỹ đạo kết quả là

τ= (st,at,{zi t}n i=1),...,(st+k,at+k,{zi t+k}n i=1),

và chúng tôi quá tải ký hiệu cho zi sao cho nó đại diện cho tín hiệu phản hồi của một đoạn quỹ đạo, zi(τ) =P (st,at)∈τzi(st,at).

--- TRANG 4 ---
Cho các tín hiệu phản hồi của quỹ đạo và thứ hạng tầm quan trọng, HERON xây dựng mô hình phần thưởng dựa trên ưu tiên bằng thu thập ưu tiên và học phần thưởng. HERON sau đó huấn luyện một mô hình chính sách thông qua học chính sách.

Thu thập Ưu tiên. Chúng tôi tạo ra một tập dữ liệu quỹ đạo với một mô hình chính sách. Chúng tôi có thể có được mô hình chính sách ban đầu bằng cách (i) sao chép hành vi từ dữ liệu minh chứng chuyên gia, (ii) huấn luyện trước nó bằng một phần thưởng được chế tác thủ công, hoặc (iii) khởi tạo hoàn toàn ngẫu nhiên. Với dữ liệu quỹ đạo, HERON đầu tiên so sánh chúng dựa trên một dạng trực quan của kiến thức miền: thứ hạng trên các tín hiệu phản hồi. Chúng tôi giả định z1,...,zn đã được sắp xếp theo thứ tự giảm dần về tầm quan trọng bởi một chuyên gia có kiến thức miền. Trong các môi trường phần thưởng thưa thớt, z1 luôn là phần thưởng thưa thớt và các zi còn lại là các phần thay thế. Sau đó chúng tôi thu thập một ưu tiên µ∈{0,1,2} giữa các cặp quỹ đạo (τ1,τ2) với một cây quyết định được tạo ra bởi thứ bậc tín hiệu phản hồi đã cho. Một sự hòa được ký hiệu bởi µ= 0, µ= 1 có nghĩa là τ1 được ưu tiên, và µ= 2 có nghĩa là τ2 được ưu tiên.

Cây quyết định được xây dựng như sau. Chúng tôi đầu tiên đặt cấp độ hiện tại l= 1. Sau đó chúng tôi tính toán

µ=0 nếu |zl(τ1)−zl(τ2)|≤δl
1 nếu zl(τ1)>zl(τ2) +δl
2 nếu zl(τ2)>zl(τ1) +δl

trong đó δl là một siêu tham số lề cho cấp độ l. Tham số lề δl đảm bảo rằng chúng tôi chỉ thu thập một ưu tiên sử dụng zl nếu hai quỹ đạo khác nhau đáng kể theo zl. Lề δl có thể được sử dụng để tiêm thêm kiến thức miền vào thuật toán HERON, nhưng trong các thí nghiệm của chúng tôi, chúng tôi đặt δl bằng độ lệch chuẩn của zl trên dữ liệu được thu thập.

Nếu µ= 0, chúng tôi cập nhật l←l+ 1 và so sánh các quỹ đạo với tín hiệu phản hồi quan trọng tiếp theo. Nếu hai quỹ đạo không khác nhau đáng kể trong bất kỳ tín hiệu phản hồi nào (tức là l>n), chúng tôi loại bỏ cặp quỹ đạo. Chúng tôi minh họa thuật toán trong Hình 6 trong phụ lục.

Học Phần thưởng. Cho một tập dữ liệu có nhãn D của các quỹ đạo (τw,τu) trong đó τw là quỹ đạo được ưu tiên bởi thuật toán thu thập ưu tiên (tức là µ= 1), chúng tôi muốn gán một phần thưởng cao hơn cho quỹ đạo được ưu tiên (chúng tôi loại bỏ tất cả các sự hòa khỏi tập dữ liệu, vì chúng tôi thấy rằng việc bao gồm chúng có tác động không đáng kể đến huấn luyện). Để thực hiện điều này, chúng tôi huấn luyện một mô hình phần thưởng Rφ:S×A→ R trong đó Rφ(τ) =P (st,at)∈τγtRφ(st,at). Để gán một phần thưởng cao hơn cho quỹ đạo được ưu tiên τw, chúng tôi theo phương pháp trong Ouyang et al. [2022] và tối ưu hóa mất mát

L(φ) =−E(τw,τu)∼D[ log σ(Rφ(τw)−Rφ(τu))]. (2)

Chúng tôi lưu ý rằng mất mát này sử dụng mô hình ưu tiên Bradley-Terry [Bradley và Terry, 1952]. Một khi chúng tôi đã huấn luyện mô hình phần thưởng Rφ, chúng tôi có thể gán một phần thưởng cho mỗi quỹ đạo τ là Rφ(τ). Điều quan trọng cần lưu ý là không giống như một số công trình trước đây học các mô hình phần thưởng tuyến tính trên các đặc trưng trạng thái, HERON cho phép các mô hình phần thưởng phức tạp hơn được tham số hóa bởi mạng neural [Sadigh et al., 2017, Bıyık et al., 2020]. Do đó, cũng có thể giới thiệu kiến thức được huấn luyện trước vào mô hình phần thưởng.

--- TRANG 5 ---
Học Chính sách. Với mô hình phần thưởng Rφ, chính sách trong (1) có thể được học thông qua các thuật toán học tăng cường phổ biến như Q-learning hoặc Tối ưu hóa Chính sách Gần kề [Sutton và Barto, 2018, Schulman et al., 2015, 2017].

Tùy chọn: Huấn luyện Đa giai đoạn. Sự thành công của việc học phần thưởng phụ thuộc vào chất lượng của các quỹ đạo được tạo ra bởi mô hình chính sách, nhưng nếu mô hình chính sách ban đầu không tối ưu, ví dụ, được huấn luyện trước từ hàm phần thưởng được chế tác thủ công hoặc được khởi tạo ngẫu nhiên, nó có thể đưa ra độ thiên lệch lấy mẫu đáng kể cho các quỹ đạo. Để giải quyết vấn đề này, chúng tôi có thể lặp lại việc thu thập ưu tiên, học phần thưởng và học chính sách trong nhiều vòng. Trong mỗi vòng mới, các quỹ đạo trong thu thập ưu tiên được tạo ra bởi mô hình chính sách từ vòng cuối, và mô hình phần thưởng sau đó được điều chỉnh sử dụng các so sánh mới.

Mở rộng: Tối ưu hóa Ưu tiên Trực tiếp. Rafailov et al. [2024] đã chỉ ra rằng trong các môi trường bandit ngữ cảnh, bài toán tối ưu hóa ưu tiên được điều hòa KL có thể được tối ưu hóa trực tiếp, không cần mô hình phần thưởng. Trong các môi trường phù hợp, chúng tôi có thể sử dụng DPO để đồng thời huấn luyện chính sách và phần thưởng, giảm chi phí tính toán của HERON.

4 Thí nghiệm
Chúng tôi đánh giá hiệu quả của khung của chúng tôi trong các thí nghiệm điều khiển đèn giao thông, tạo mã, căn chỉnh mô hình ngôn ngữ và điều khiển robot.

4.1 Điều khiển Đèn Giao thông Đa Agent
Môi trường. Trong tình huống thế giới thực này, các agent hợp tác học cách tăng thông lượng và giảm thiểu thời gian chờ của xe qua mạng giao thông. Do tính phức tạp của hệ thống giao thông, thật không may, không có một phần thưởng tối ưu nào, và phần thưởng khác nhau có thể được ưu tiên trong các tình huống khác nhau. Để giải quyết vấn đề này, Wei et al. [2018], Van der Pol và Oliehoek [2016], Zhang et al. [2019] định nghĩa phần thưởng thực tế bằng cách cân bằng sáu tín hiệu phản hồi sau: độ dài hàng đợi (q), thời gian chờ xe (wt), độ trễ xe (dl), số lần dừng khẩn cấp của xe (em), số lần thay đổi pha đèn (fl), và số xe đi qua hệ thống (vl). Phần thưởng thực tế được định nghĩa là R=−0.5q−0.5wt−0.5dl−0.25em−fl+vl.

Cho những thí nghiệm này, chúng tôi đánh giá nhiều thứ bậc phần thưởng khác nhau. Mô hình phần thưởng của chúng tôi được tham số hóa bởi một MLP ba lớp được học bằng huấn luyện đa giai đoạn. Chúng tôi sử dụng QCOMBO [Zhang et al., 2019], một thuật toán dựa trên Q-learning làm thuật toán RL và tiến hành thí nghiệm sử dụng khung Flow [Wu et al., 2017]. Chúng tôi huấn luyện các chính sách RL Đa-agent trên một lưới hai-nhân-hai (bốn agent), mỗi cái được tham số hóa bởi một MLP ba lớp. Để biết thêm chi tiết về môi trường và cài đặt thí nghiệm, xem Phụ lục F.

Baseline. Chúng tôi so sánh phương pháp của chúng tôi với kỹ thuật phần thưởng, trong đó phần thưởng được công thức hóa là Pn i=1Wizi, trong đó Wi là các siêu tham số và zi là các tín hiệu phản hồi được chuẩn hóa. Để tiêm kiến thức miền của HERON và làm cho việc điều chỉnh siêu tham số trở nên khả thi, chúng tôi đặt Wi giảm theo hình học sao cho Wi=βi và chọn β∈{0.1,0.2,...,1.0}. Đây là một baseline kỹ thuật phần thưởng rất thực tế và cạnh tranh. Lưu ý rằng chúng tôi cũng khám phá kỹ thuật phần thưởng mà không có kiến thức phân cấp trước này. Chúng tôi cũng so sánh với các phương pháp ensemble, huấn luyện một chính sách riêng biệt trên mỗi tín hiệu phản hồi và sau đó chọn một hành động tại mỗi bước thời gian bằng một kết hợp có trọng số của mỗi chính sách [Brys et al., 2017].

Trong thí nghiệm này, chúng tôi đánh giá các thiết kế phần thưởng khác nhau bằng cách so sánh phần thưởng thực tế của chính sách liên quan.

Kết quả. Trong Hình 1a, chúng tôi vẽ đồ thị phần thưởng đánh giá của các chính sách trong môi trường điều khiển đèn giao thông. Chúng tôi quan sát thấy rằng chính sách được huấn luyện với HERON hoạt động tốt hơn đáng kể so với các chính sách được huấn luyện với baseline kỹ thuật phần thưởng hoặc thậm chí bằng phần thưởng thực tế được phát triển trong Zhang et al. [2019]. Lợi ích của HERON so với tất cả các phương pháp khác vượt qua kiểm định t với p<0.005. Chúng tôi giả thuyết rằng HERON có thể sử dụng mỗi tín hiệu phần thưởng tốt hơn so với kết hợp tuyến tính; một thay đổi đáng kể trong một tín hiệu phản hồi duy nhất có thể bị chìm trong kết hợp tuyến tính, nhưng HERON có thể kết hợp thông tin này do tính chất phân cấp của nó.

Tính linh hoạt của Mô hình hóa Phần thưởng Phân cấp. Trong các nhiệm vụ khác nhau, không có một phần thưởng lý tưởng nào, và các khía cạnh của hành vi agent nên được ưu tiên phụ thuộc vào sở thích của người thực hành. Do đó, một đặc tính quan trọng mà các thuật toán thiết kế phần thưởng nên có là tính linh hoạt. Cụ thể, việc sửa đổi kiến thức miền được nhập vào nên dẫn đến những thay đổi tương ứng trong hành vi của agent. Để đánh giá tính linh hoạt của HERON, chúng tôi kiểm tra cách thay đổi thứ hạng tín hiệu phản hồi thay đổi hành vi agent trong môi trường điều khiển đèn giao thông.

Trong thí nghiệm này, chúng tôi luôn đặt tín hiệu quan trọng nhất là số xe đi qua, và sau đó chúng tôi sử dụng độ dài hàng đợi, thời gian chờ, hoặc độ trễ làm tín hiệu thứ hai. Kết quả có thể thấy trong Hình 2. Chúng tôi quan sát thấy rằng HERON khá linh hoạt, và bằng cách thay đổi thứ bậc phần thưởng, chúng tôi có thể ảnh hưởng đáng kể đến hành vi của agent: khi ưu tiên các tín hiệu nhất định, hiệu suất của chính sách (được đo theo tín hiệu được ưu tiên) sẽ tăng lên rất nhiều.

Sử dụng Tín hiệu. Chúng tôi cũng hiển thị cấp độ mà cây quyết định được tạo ra bởi HERON đạt được trong Hình 1b. Điều này có thể thay đổi với các thứ bậc phần thưởng khác nhau, nhưng như chúng ta có thể thấy từ hình, một tỷ lệ tương đối tương tự của các quyết định được đưa ra tại mỗi cấp độ của cây quyết định. Điều này xác nhận hiệu quả của việc đặt δl bằng độ lệch chuẩn của zl.

Tính bền vững. Một lợi thế của HERON là không giống như kỹ thuật phần thưởng, nó không phụ thuộc vào độ lớn của các tín hiệu phản hồi khác nhau. Điều này là do thuật toán thu thập ưu tiên sẽ gắn nhãn các cặp quỹ đạo với µ∈{0,1,2}, bất kể quy mô của các tín hiệu khác nhau. Tính bất biến quy mô này có lợi, vì các thuật toán phụ thuộc vào quy mô của các tín hiệu phản hồi có thể dễ bị tổn thương bởi những thay đổi trong môi trường trong quá trình huấn luyện. Ví dụ, nếu quy mô của một tín hiệu phản hồi đột nhiên tăng gấp đôi, (tức là giao thông trên đường cao tốc tăng gấp đôi do giờ cao điểm) thì hai điều sẽ xảy ra: (1) quy mô của tín hiệu phần thưởng có thể tăng mạnh, tương tự như một thay đổi đột ngột trong tốc độ học; (2) vectơ trọng số được sử dụng trong kỹ thuật phần thưởng để kết hợp các tín hiệu phản hồi sẽ bị thay đổi hiệu quả. Hiện tượng đầu tiên có thể gây ra sự bất ổn trong huấn luyện, và hiện tượng thứ hai có thể khiến agent không phù hợp với mong muốn của người giám sát con người.

--- TRANG 6 ---
(a) Đường cong Đánh giá
(b) Sử dụng Tín hiệu

Hình 1: (a) Đường cong đánh giá với các thứ bậc phần thưởng khác nhau trong điều khiển đèn giao thông. Đường cong nằm trong phạm vi ±một độ lệch chuẩn. (b) Sử dụng các tín hiệu khác nhau.

(a) Số Xe Qua
(b) Thời gian Chờ
(c) Độ Trễ
(d) Độ dài Hàng đợi

Hình 2: Đường cong đánh giá với các thứ bậc phần thưởng khác nhau trong điều khiển đèn giao thông. Tầm quan trọng giảm từ trái sang phải trong một nhãn. Đường cong nằm trong phạm vi ±một độ lệch chuẩn.

Để đánh giá tính bền vững của HERON, chúng tôi thay đổi tốc độ của xe giữa chừng quá trình huấn luyện (đây là một môi trường thực tế, vì nhiều khu vực có giới hạn tốc độ phụ thuộc thời gian). Sau đó chúng tôi đánh giá mỗi chính sách sau khi huấn luyện dưới môi trường mới, và xem thuật toán nào có thể thích ứng tốt nhất. Chúng tôi so sánh chính sách được huấn luyện HERON với hai chính sách được huấn luyện với kỹ thuật phần thưởng: một sử dụng tốc độ học tối ưu trong môi trường không thay đổi (1 ×10−3) và một sử dụng tốc độ học nhỏ hơn, ổn định hơn là 1 ×10−5.

Từ Hình 3, chúng ta có thể thấy rằng kỹ thuật phần thưởng khá nhạy cảm với những thay đổi trong môi trường trong quá trình huấn luyện. Điều này có thể được chống lại với tốc độ học nhỏ hơn, nhưng điều này sẽ dẫn đến việc học chậm hơn và phần thưởng dưới tối ưu. Mặt khác, HERON có thể đạt được phần thưởng cao bất kể thay đổi môi trường, hỗ trợ giả thuyết của chúng tôi rằng thiết kế bất biến quy mô của HERON dẫn đến tăng tính bền vững.

4.2 Tạo Mã
Môi trường. RL gần đây đã thu hút sự chú ý đáng kể vì hiệu suất tiên tiến trong các nhiệm vụ tạo văn bản khác nhau. Do đó, chúng tôi điều tra xem HERON có thể đạt được những cải thiện tương tự trong hiệu suất LLM chỉ dựa trên thứ hạng trên các tín hiệu phản hồi hay không. Đầu tiên, chúng tôi xem xét nhiệm vụ tạo mã. Trong nhiệm vụ mã, mục tiêu của agent là viết một chương trình sẽ thỏa mãn các tiêu chí được chỉ định trong một vấn đề đã cho.

Baseline. Gần đây, Le et al. [2022] đã chứng minh hiệu suất tiên tiến có thể đạt được bằng cách huấn luyện với RL. Họ thiết kế thủ công một phần thưởng từng phần hằng số được xác định bởi các tín hiệu phản hồi bao gồm liệu chương trình có vượt qua tất cả bài kiểm tra đơn vị và loại lỗi nếu thất bại (tức là lỗi biên dịch hoặc lỗi runtime). Shojaee et al. [2023] (PPOCoder) xây dựng dựa trên công trình này, tích hợp thêm các tín hiệu phản hồi như độ tương tự cây cú pháp trừu tượng (AST) của chương trình với các minh chứng chuyên gia.

Chi tiết Triển khai. Cây quyết định của chúng tôi dựa trên ba tín hiệu: phần trăm bài kiểm tra mà chương trình vượt qua, loại lỗi mà chương trình gặp phải, và độ tương tự AST với các minh chứng chuyên gia. Để huấn luyện chính sách, chúng tôi theo triển khai của Le et al. [2022]. Chúng tôi khởi tạo các chính sách của chúng tôi với mô hình CodeT5plus-large và mô hình phần thưởng của chúng tôi với CodeT5-small [Wang et al., 2021]. Các chính sách đầu tiên được huấn luyện với sao chép hành vi trên dữ liệu minh chứng chuyên gia. Tiếp theo, chúng tôi tạo ra 20 mẫu cho mỗi chương trình huấn luyện, và tiến hành huấn luyện RL trên những mẫu được tạo ra này. Chúng tôi huấn luyện với mục tiêu gradient chính sách. Chúng tôi đánh giá hiệu suất của mỗi thuật toán sử dụng chỉ số Pass@K, là số bài toán lập trình vượt qua với K bài nộp cho mỗi bài toán [Chen et al., 2021]. Chúng tôi chủ yếu đánh giá HERON trên APPS, một tập dữ liệu lập trình python chứa 5000 bài toán kiểm tra [Hendrycks et al., 2021]. Mỗi câu hỏi trong tập dữ liệu đi kèm với các minh chứng chuyên gia và các trường hợp kiểm tra mà chương trình nên vượt qua. Để đánh giá mỗi thuật toán, chúng tôi tạo ra 200 chương trình cho mỗi bài toán. Tổng cộng, mỗi phương pháp được đánh giá trên 1 triệu chương trình được tạo ra. Để đánh giá khả năng tổng quát hóa của các chính sách, chúng tôi đánh giá mỗi chính sách theo cách zero-shot trên tập dữ liệu MBPP, chứa 974 câu hỏi lập trình python cơ bản [Austin et al., 2021].

Điều chỉnh Quy mô Phần thưởng Sau Huấn luyện. Để tiếp tục kết hợp kiến thức miền và phản hồi môi trường vào phần thưởng, chúng tôi đề xuất điều chỉnh lại quy mô phần thưởng học từ (2). Cụ thể, chúng tôi nhân phần thưởng Rφ(τ) với một hằng số định hình, ký hiệu là αF(τ). Ở đây, α là một siêu tham số và được điều chỉnh trên {1,2,3}, trong khi F(τ) tương ứng với một hàm từng phần của các tín hiệu phản hồi. Chúng tôi định nghĩa nó cho tạo mã là

F(τ) =3 nếu chương trình τ vượt qua tất cả bài kiểm tra đơn vị
2 nếu chương trình τ thất bại một bài kiểm tra đơn vị
1 nếu chương trình τ tạo ra bất kỳ lỗi nào.

Hàm này được thúc đẩy bởi thứ hạng tầm quan trọng của các tín hiệu phản hồi. Nó rõ ràng tăng cường các tín hiệu phản hồi trong học chính sách theo thứ hạng tầm quan trọng của chúng và phục vụ như phần bổ sung của mô hình phần thưởng dựa trên ưu tiên. Cụ thể, bằng cách điều chỉnh α, chúng tôi có thể kiểm soát hiệu quả hình dạng của phần thưởng và mức độ tách biệt giữa các quỹ đạo tốt nhất và tệ nhất. Chúng tôi tập trung nỗ lực điều chỉnh α của chúng tôi độc quyền trên nhiệm vụ tạo mã do tính phức tạp cao của nó.

Kết quả. Chúng tôi hiển thị kết quả cho nhiệm vụ tạo mã trong Bảng 1 và Bảng 2. HERON vượt trội hơn tất cả các phương pháp khác. Đối với K lớn hơn trong Pass@K, lợi ích trong Pass@K vượt qua kiểm định t với p<0.05. Điều này rất có thể là do kỹ thuật phần thưởng chỉ trao phần thưởng lớn cho các chương trình vượt qua tất cả bài kiểm tra đơn vị hoặc tương tự với các minh chứng chuyên gia, trong khi HERON có thể trao phần thưởng lớn cho các chương trình có thể thất bại một số bài kiểm tra đơn vị nhưng mô hình phần thưởng dự đoán là có khả năng thỏa mãn lời nhắc. Điều này có nghĩa là HERON sẽ thúc đẩy một tập hợp chiến lược lập trình đa dạng hơn. Ngoài ra, hàm phần thưởng mượt mà của HERON (trái ngược với hàm từng phần không liên tục trong các môi trường phần thưởng thưa thớt) có thể thuận lợi hơn cho việc học, và do đó dẫn đến hiệu suất cao hơn.

Bảng 1: Pass@K thô trên APPS.
Pass@1 Pass@5 Pass@10 Pass@20 Pass@50
BC 1.59 3.82 5.19 6.74 6.74
CodeRL 1.71 4.12 5.57 7.26 9.81
PPOCoder 1.23 3.08 4.19 5.50 7.62
HERON 1.72 4.19 5.71 7.49 10.19

Bảng 2: Pass@50 trên APPS theo ba cấp độ khó: giới thiệu, phỏng vấn, thi đấu.
Giới thiệu Phỏng vấn Thi đấu
BC 18.8 4.23 2.10
CodeRL 23.7 6.93 4.51
PPOCoder 18.6 5.41 3.23
HERON 24.6 7.28 4.53

Chúng tôi tiếp tục phân tích hiệu suất tạo mã sử dụng chỉ số Pass@K được lọc, chỉ nộp các chương trình vượt qua bài kiểm tra đơn vị được cung cấp trong lời nhắc [Chen et al., 2021]. Như được thấy trong Bảng 3, HERON đồng đều và đáng kể vượt trội hơn các baseline, xác nhận hiệu quả của HERON.

Như trong Le et al. [2022], chúng tôi đánh giá hiệu suất của các chính sách được huấn luyện bởi HERON trên tập dữ liệu MBPP. Kết quả được hiển thị trong Bảng 4. HERON vượt trội hơn các phương pháp khác, cho thấy rằng HERON có thể tạo ra các chính sách có thể tổng quát hóa.

4.3 Căn chỉnh Mô hình Ngôn ngữ
Môi trường. Ngoài tạo mã, chúng tôi cũng đánh giá khả năng của HERON trong việc huấn luyện các mô hình tuân theo hướng dẫn được căn chỉnh với các nguyên tắc con người. Cho thí nghiệm này, chúng tôi sử dụng Phi-2

--- TRANG 7 ---
(a) 25 MPH
(b) 30 MPH
(c) 40 MPH
(d) 45 MPH

Hình 3: Đường cong đánh giá với các môi trường khác nhau: thay đổi giới hạn tốc độ của xe. Giới hạn tốc độ baseline là 35 MPH. Các đường cong nằm trong phạm vi ±một độ lệch chuẩn.

code generation task. Trong nhiệm vụ mã, mục tiêu của agent là viết một chương trình sẽ thỏa mãn các tiêu chí được chỉ định trong một vấn đề đã cho.

Baseline. Gần đây, Le et al. [2022] đã chứng minh hiệu suất tiên tiến có thể đạt được bằng cách huấn luyện với RL. Họ thiết kế thủ công một phần thưởng từng phần hằng số được xác định bởi các tín hiệu phản hồi bao gồm liệu chương trình có vượt qua tất cả bài kiểm tra đơn vị và loại lỗi nếu thất bại (tức là lỗi biên dịch hoặc lỗi runtime). Shojaee et al. [2023] (PPOCoder) xây dựng dựa trên công trình này, tích hợp thêm các tín hiệu phản hồi như độ tương tự cây cú pháp trừu tượng (AST) của chương trình với các minh chứng chuyên gia.

Chi tiết Triển khai. Cây quyết định của chúng tôi dựa trên ba tín hiệu: phần trăm bài kiểm tra mà chương trình vượt qua, loại lỗi mà chương trình gặp phải, và độ tương tự AST với các minh chứng chuyên gia. Để huấn luyện chính sách, chúng tôi theo triển khai của Le et al. [2022]. Chúng tôi khởi tạo các chính sách của chúng tôi với mô hình CodeT5plus-large và mô hình phần thưởng của chúng tôi với CodeT5-small [Wang et al., 2021]. Các chính sách đầu tiên được huấn luyện với sao chép hành vi trên dữ liệu minh chứng chuyên gia. Tiếp theo, chúng tôi tạo ra 20 mẫu cho mỗi chương trình huấn luyện, và tiến hành huấn luyện RL trên những mẫu được tạo ra này. Chúng tôi huấn luyện với mục tiêu gradient chính sách. Chúng tôi đánh giá hiệu suất của mỗi thuật toán sử dụng chỉ số Pass@K, là số bài toán lập trình vượt qua với K bài nộp cho mỗi bài toán [Chen et al., 2021]. Chúng tôi chủ yếu đánh giá HERON trên APPS, một tập dữ liệu lập trình python chứa 5000 bài toán kiểm tra [Hendrycks et al., 2021]. Mỗi câu hỏi trong tập dữ liệu đi kèm với các minh chứng chuyên gia và các trường hợp kiểm tra mà chương trình nên vượt qua. Để đánh giá mỗi thuật toán, chúng tôi tạo ra 200 chương trình cho mỗi bài toán. Tổng cộng, mỗi phương pháp được đánh giá trên 1 triệu chương trình được tạo ra. Để đánh giá khả năng tổng quát hóa của các chính sách, chúng tôi đánh giá mỗi chính sách theo cách zero-shot trên tập dữ liệu MBPP, chứa 974 câu hỏi lập trình python cơ bản [Austin et al., 2021].

Điều chỉnh Quy mô Phần thưởng Sau Huấn luyện. Để tiếp tục kết hợp kiến thức miền và phản hồi môi trường vào phần thưởng, chúng tôi đề xuất điều chỉnh lại quy mô phần thưởng học từ (2). Cụ thể, chúng tôi nhân phần thưởng Rφ(τ) với một hằng số định hình, ký hiệu là αF(τ). Ở đây, α là một siêu tham số và được điều chỉnh trên {1,2,3}, trong khi F(τ) tương ứng với một hàm từng phần của các tín hiệu phản hồi. Chúng tôi định nghĩa nó

--- TRANG 8 ---
cho tạo mã là

F(τ) =3 nếu chương trình τ vượt qua tất cả bài kiểm tra đơn vị
2 nếu chương trình τ thất bại một bài kiểm tra đơn vị
1 nếu chương trình τ tạo ra bất kỳ lỗi nào.

Hàm này được thúc đẩy bởi thứ hạng tầm quan trọng của các tín hiệu phản hồi. Nó rõ ràng tăng cường các tín hiệu phản hồi trong học chính sách theo thứ hạng tầm quan trọng của chúng và phục vụ như phần bổ sung của mô hình phần thưởng dựa trên ưu tiên. Cụ thể, bằng cách điều chỉnh α, chúng tôi có thể kiểm soát hiệu quả hình dạng của phần thưởng và mức độ tách biệt giữa các quỹ đạo tốt nhất và tệ nhất. Chúng tôi tập trung nỗ lực điều chỉnh α của chúng tôi độc quyền trên nhiệm vụ tạo mã do tính phức tạp cao của nó.

Kết quả. Chúng tôi hiển thị kết quả cho nhiệm vụ tạo mã trong Bảng 1 và Bảng 2. HERON vượt trội hơn tất cả các phương pháp khác. Đối với K lớn hơn trong Pass@K, lợi ích trong Pass@K vượt qua kiểm định t với p<0.05. Điều này rất có thể là do kỹ thuật phần thưởng chỉ trao phần thưởng lớn cho các chương trình vượt qua tất cả bài kiểm tra đơn vị hoặc tương tự với các minh chứng chuyên gia, trong khi HERON có thể trao phần thưởng lớn cho các chương trình có thể thất bại một số bài kiểm tra đơn vị nhưng mô hình phần thưởng dự đoán là có khả năng thỏa mãn lời nhắc. Điều này có nghĩa là HERON sẽ thúc đẩy một tập hợp chiến lược lập trình đa dạng hơn. Ngoài ra, hàm phần thưởng mượt mà của HERON (trái ngược với hàm từng phần không liên tục trong các môi trường phần thưởng thưa thớt) có thể thuận lợi hơn cho việc học, và do đó dẫn đến hiệu suất cao hơn.

Bảng 1: Pass@K thô trên APPS.
Pass@1 Pass@5 Pass@10 Pass@20 Pass@50
BC 1.59 3.82 5.19 6.74 6.74
CodeRL 1.71 4.12 5.57 7.26 9.81
PPOCoder 1.23 3.08 4.19 5.50 7.62
HERON 1.72 4.19 5.71 7.49 10.19

Bảng 2: Pass@50 trên APPS theo ba cấp độ khó: giới thiệu, phỏng vấn, thi đấu.
Giới thiệu Phỏng vấn Thi đấu
BC 18.8 4.23 2.10
CodeRL 23.7 6.93 4.51
PPOCoder 18.6 5.41 3.23
HERON 24.6 7.28 4.53

Chúng tôi tiếp tục phân tích hiệu suất tạo mã sử dụng chỉ số Pass@K được lọc, chỉ nộp các chương trình vượt qua bài kiểm tra đơn vị được cung cấp trong lời nhắc [Chen et al., 2021]. Như được thấy trong Bảng 3, HERON đồng đều và đáng kể vượt trội hơn các baseline, xác nhận hiệu quả của HERON.

Như trong Le et al. [2022], chúng tôi đánh giá hiệu suất của các chính sách được huấn luyện bởi HERON trên tập dữ liệu MBPP. Kết quả được hiển thị trong Bảng 4. HERON vượt trội hơn các phương pháp khác, cho thấy rằng HERON có thể tạo ra các chính sách có thể tổng quát hóa.

4.3 Căn chỉnh Mô hình Ngôn ngữ
Môi trường. Ngoài tạo mã, chúng tôi cũng đánh giá khả năng của HERON trong việc huấn luyện các mô hình tuân theo hướng dẫn được căn chỉnh với các nguyên tắc con người. Cho thí nghiệm này, chúng tôi sử dụng Phi-2

--- TRANG 9 ---
[Javaheripi et al.] làm mô hình cơ sở của chúng tôi, và huấn luyện nó trên tập dữ liệu HelpSteer [Wang et al., 2023]. Tập dữ liệu HelpSteer bao gồm các cặp hướng dẫn-phản hồi được chú thích (từ 0-4) trên năm tín hiệu phản hồi: tính chính xác, tính hữu ích, tính mạch lạc, độ phức tạp và tính dài dòng.

Triển khai. Đối với HERON, chúng tôi sử dụng thứ bậc sau: tính chính xác > tính hữu ích > tính mạch lạc > độ phức tạp > tính dài dòng. Sau đó chúng tôi sử dụng HERON-DPO để tối ưu hóa chính sách. Chúng tôi xem xét hai baseline kỹ thuật phần thưởng: tinh chỉnh dựa trên REINFORCE với trọng số phần thưởng bằng nhau trên tất cả tín hiệu và tinh chỉnh dựa trên REINFORCE với trọng số phần thưởng giảm theo cấp số nhân [Williams, 1992]. Mỗi thuật toán được khởi tạo từ một phiên bản của Phi-2 đã trải qua tinh chỉnh có giám sát trên HelpSteer.

Kết quả. Để đánh giá các mô hình kết quả, chúng tôi sử dụng ba mô hình phần thưởng tiên tiến (RLHF-Flow [Dong et al., 2024], PairRM [Jiang et al., 2023], Eurus-7B [Yuan et al., 2024]) cũng như Claude 3 Sonnet để tính tỷ lệ thắng so với chính sách SFT trên tập dữ liệu kiểm tra HelpSteer. Kết quả có thể thấy trong Bảng 5. Chúng tôi thấy rằng HERON-DPO có thể vượt trội đáng kể so với các baseline trên tất cả các giám khảo. Chúng tôi giả thuyết rằng lợi ích này là do cây quyết định của HERON trên những tín hiệu này có thể nắm bắt các nguyên tắc con người tốt hơn so với kết hợp tuyến tính của các tín hiệu phản hồi. Thêm chi tiết có thể tìm thấy trong Phụ lục N.

Bảng 5: Tỷ lệ thắng so với mô hình SFT được tính bởi các giám khảo LLM khác nhau.
RLHF-Flow PairRM Eurus-7B Claude-3 Trung bình
Kỹ thuật Phần thưởng (Trọng số Bằng nhau) 54.67 58.49 53.68 57.46 56.08
Kỹ thuật Phần thưởng (Trọng số Giảm dần) 59.24 59.64 52.49 56.85 57.06
HERON-DPO 66.20 63.02 63.22 63.82 64.57

4.4 Thêm Thí nghiệm
Để chứng minh rằng HERON có thể huấn luyện một chính sách hợp lý ngay cả khi thứ bậc không rõ ràng, chúng tôi thí nghiệm trên bốn nhiệm vụ điều khiển robot: Ant, Half-Cheetah, Hopper và Pendulum. Chúng tôi sử dụng trình mô phỏng PyBullet, nơi phần thưởng thực tế được công thức hóa như một kết hợp tuyến tính của một số tín hiệu như tiềm năng của robot, chi phí năng lượng, liệu các khớp có ở giới hạn của chúng hay không, và liệu chân robot có va chạm hay không [Coumans và Bai, 2016]. Những yếu tố này không nhất thiết hiển thị thứ bậc rõ ràng. Thêm chi tiết về môi trường này có thể tìm thấy trong Phụ lục E. Kết quả có thể tìm thấy trong Bảng 6, nơi chúng tôi quan sát thấy rằng mặc dù HERON không thể luôn hoạt động tốt như phần thưởng thực tế, nó luôn có thể vượt quá hiệu suất của baseline kỹ thuật phần thưởng.

Bảng 6: Phần thưởng thực tế thu được trong các môi trường robot.
Ant Hopper Cheetah Pendulum
Thực tế 0.99(0.0) 0.86(0.01) 0.94(0.10) 1.0(0.01)
Kỹ thuật Phần thưởng 0.88(0.02) 0.72(0.05) 0.61(0.04) 0.99(0.0)
HERON 1.0(0.01) 0.78(0.04) 0.62(0.04) 1.0(0.0)

4.5 Nghiên cứu Loại bỏ
Phân tích Thời gian Huấn luyện. Chi phí tính toán chính của HERON đến từ việc huấn luyện mô hình phần thưởng, vì thu thập dữ liệu đã là một phần của hầu hết các thuật toán RL và thu thập ưu tiên rất nhanh. Để tăng tốc huấn luyện mô hình phần thưởng trong môi trường đa giai đoạn, chúng tôi có thể sử dụng lịch trình huấn luyện được ủ (xem Phụ lục J). Thời gian huấn luyện được chuẩn hóa của HERON, kỹ thuật phần thưởng và học dựa trên ensemble được hiển thị trong Hình 4a. HERON chậm hơn 25% so với kỹ thuật phần thưởng trung bình, điều này khá hợp lý cho rằng chi phí điều chỉnh của kỹ thuật phần thưởng thường lớn.

Siêu tham số. Chúng tôi đặt δi bằng độ lệch chuẩn của zi trên dữ liệu được thu thập trong các thí nghiệm của chúng tôi. Tuy nhiên, chúng tôi đánh giá độ nhạy cảm của HERON đối với những tham số này trong Hình 4. Chúng tôi thấy các giá trị trong [0, 2∗σi] hoạt động tốt, trong đó σi là độ lệch chuẩn của zi.

Chi phí Điều chỉnh. Cuối cùng, chúng tôi so sánh chi phí điều chỉnh của HERON với kỹ thuật phần thưởng trong môi trường điều khiển đèn giao thông. Đối với HERON, chúng tôi xem xét việc điều chỉnh với kiến thức miền chính xác (thứ bậc được cung cấp) và với kiến thức miền không chính xác (3 yếu tố hàng đầu được cung cấp nhưng thứ tự của chúng không được chỉ định). Đối với kỹ thuật phần thưởng, chúng tôi xem xét việc điều chỉnh với kiến thức miền chính xác và không có kiến thức miền (trọng số phần thưởng không có cấu trúc phân cấp). Đối với trường hợp sau, chúng tôi điều chỉnh trọng số với tối ưu hóa Bayesian. Kết quả được hiển thị trong Hình 5. Chúng tôi thấy rằng cả hai phiên bản của HERON đều vượt trội đáng kể so với kỹ thuật phần thưởng. Mặc dù tối ưu hóa Bayesian có thể huấn luyện các chính sách hiệu suất cao, nó yêu cầu 5 đến 15 lần số vòng lặp điều chỉnh của HERON.

5 Thảo luận
Các Tình huống Phù hợp. HERON không nhằm mục đích phục vụ như một giải pháp chung cho tất cả các bài toán RL; tuy nhiên, nó có thể hoạt động khá tốt trong các môi trường cụ thể. Cụ thể, HERON sẽ hữu ích nhất trong các môi trường có một số tín hiệu phản hồi có sẵn một cách rẻ tiền và người giám sát con người có thể xếp hạng những tín hiệu này. Các thí nghiệm của chúng tôi cho thấy rằng trong những môi trường như vậy (tạo mã và điều khiển đèn giao thông) HERON có thể vượt trội hơn các baseline tiên tiến. Hơn nữa, HERON thể hiện triển vọng lớn và tính đa năng vì nó thậm chí có thể đạt được hiệu suất tốt trong các môi trường không lý tưởng (điều khiển robot).

Tính linh hoạt Phần thưởng. HERON cũng có khả năng xử lý các tín hiệu phản hồi có tầm quan trọng gần như bằng nhau. Một giải pháp khả thi là lật thứ hạng tầm quan trọng của các tín hiệu phản hồi có tầm quan trọng bằng nhau với xác suất nhất định trong bước thu thập ưu tiên được mô tả trong Phần 3. Người ta thậm chí có thể thiết kế các thuật toán thu thập ưu tiên phức tạp hơn không yêu cầu thứ bậc nghiêm ngặt trên các tín hiệu phản hồi, mà chúng tôi để dành cho công việc tương lai.

Môi trường Ngân sách Gắn nhãn Thấp. RLHF là một phương pháp hiệu quả để có được một mô hình chính sách mạnh mẽ. Điều này là do con người có thể cung cấp thông tin chi tiết nhiều thông tin hơn cho mô hình phần thưởng so với các tín hiệu phản hồi. Tuy nhiên, việc sử dụng con người để so sánh mọi cặp quỹ đạo thường không khả thi và khó tạo ra hướng dẫn phù hợp về cách so sánh quỹ đạo. Chúng tôi xem xét một môi trường riêng biệt, nơi chỉ có một số tín hiệu phản hồi và một số kiến thức miền về chúng có sẵn. Trong trường hợp này, kỹ thuật phần thưởng trở thành một lựa chọn hợp lý và rẻ tiền, cũng như phương pháp của chúng tôi. Do đó, kỹ thuật phần thưởng là baseline phù hợp nhất để so sánh.

RL Đa mục tiêu. Phương pháp của chúng tôi không được thiết kế cụ thể cho các bài toán RL đa mục tiêu. RL đa mục tiêu chung phức tạp vì một số mục tiêu ảnh hưởng bất lợi đến các mục tiêu khác trong quá trình tối ưu hóa. Trong trường hợp này, các nhà nghiên cứu cố gắng tìm biên Pareto và cân bằng giữa các mục tiêu trong nhiều tình huống khác nhau [Van Moffaert và Nowé, 2014, Mossalam et al., 2016]. Ngược lại, phương pháp của chúng tôi có thể được áp dụng nếu và chỉ nếu các tín hiệu phản hồi có sẵn và có cấu trúc phân cấp.

Công việc Tương lai. Thủ tục so sánh phân cấp được đề xuất của chúng tôi có tính linh hoạt và có thể được mở rộng theo nhiều cách khác nhau. Ví dụ, chúng tôi có thể xem xét cấp độ của phản hồi trong thứ bậc như cường độ ưu tiên. Cụ thể hơn, các kết quả ưu tiên được rút ra dựa trên phản hồi quan trọng hơn tạo ra các ưu tiên mạnh hơn giữa hai quỹ đạo RL. Để khai thác cường độ ưu tiên như vậy, chúng tôi có thể thêm các siêu tham số điều chỉnh hoặc lề bổ sung vào việc học phần thưởng. Vì điều này vượt ra ngoài phạm vi hiện tại của chúng tôi, chúng tôi sẽ để dành cho điều tra tương lai.

Tài liệu tham khảo
Yue Deng, Feng Bao, Youyong Kong, Zhiquan Ren, và Qionghai Dai. Deep direct reinforcement learning for financial signal representation and trading. IEEE transactions on neural networks and learning systems, 28(3):653–664, 2016.

Ammar Haydari và Yasin Yılmaz. Deep reinforcement learning for intelligent transportation systems: A survey. IEEE Transactions on Intelligent Transportation Systems, 23(1):11–32, 2020.

Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, và Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314–21328, 2022.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, và Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, và cộng sự. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, và Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.

Serena Booth, Bradley W Knox, Julie Shah, Scott Niekum, Peter Stone, và Alessandro Allievi. The perils of trial-and-error reward design: misdesign through overfitting and invalid task specifications. Trong AAAI Conference on Artificial Intelligence, 2023.

Zhi Zhang, Jiachen Yang, và Hongyuan Zha. Integrating independent and centralized multi-agent reinforcement learning for traffic signal network optimization. arXiv preprint arXiv:1909.10651, 2019.

--- TRANG 10 ---
Justin Fu, Katie Luo, và Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017.

Zheng Wu, Wenzhao Lian, Vaibhav Unhelkar, Masayoshi Tomizuka, và Stefan Schaal. Learning dense rewards for contact-rich manipulation tasks. Trong 2021 IEEE International Conference on Robotics and Automation (ICRA), trang 6214–6221. IEEE, 2021.

Elise Van der Pol và Frans A Oliehoek. Coordinated deep reinforcement learners for traffic light control. Proceedings of learning, inference and control of multi-agent systems (at NIPS 2016), 8: 21–38, 2016.

Cathy Wu, Aboudy Kreidieh, Kanaad Parvate, Eugene Vinitsky, và Alexandre M Bayen. Flow: A modular learning framework for autonomy in traffic. arXiv preprint arXiv:1710.05465, 2017.

Xiao-Yang Liu, Hongyang Yang, Qian Chen, Runjia Zhang, Liuqing Yang, Bowen Xiao, và Christina Dan Wang. Finrl: A deep reinforcement learning library for automated stock trading in quantitative finance. arXiv preprint arXiv:2011.09607, 2020.

Ralph Allan Bradley và Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324–345, 1952.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, và cộng sự. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730–27744, 2022.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, và cộng sự. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Erwin Coumans và Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. 2016.

Andrew Y Ng, Daishi Harada, và Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. Trong Icml, tập 99, trang 278–287. Citeseer, 1999.

Ana C Tenorio-Gonzalez, Eduardo F Morales, và Luis Villasenor-Pineda. Dynamic reward shaping: training a robot by voice. Trong Advances in Artificial Intelligence–IBERAMIA 2010: 12th Ibero-American Conference on AI, Bahía Blanca, Argentina, November 1-5, 2010. Proceedings 12, trang 483–492. Springer, 2010.

Sam Michael Devlin và Daniel Kudenko. Dynamic potential-based reward shaping. Trong Proceedings of the 11th international conference on autonomous agents and multiagent systems, trang 433–440. IFAAMAS, 2012.

Bhaskara Marthi. Automatic shaping and decomposition of reward functions. Trong Proceedings of the 24th International Conference on Machine learning, trang 601–608, 2007.

--- TRANG 11 ---
Bảng 3: Pass@K được lọc trên APPS.
Pass@1 Pass@10 Pass@20
BC 4.70 6.36 6.44
CodeRL 5.73 8.57 8.96
PPOCoder 5.60 8.61 8.93
HERON 5.74 9.03 9.43

Bảng 4: Pass@K trên MBPP
Pass@1 Pass@2 Pass@5
CodeRL 6.58 10.27 16.24
PPOCoder 6.58 10.09 15.85
HERON 7.40 11.03 16.54

[Javaheripi et al.] làm mô hình cơ sở của chúng tôi, và huấn luyện nó trên tập dữ liệu HelpSteer [Wang et al., 2023]. Tập dữ liệu HelpSteer bao gồm các cặp hướng dẫn-phản hồi được chú thích (từ 0-4) trên năm tín hiệu phản hồi: tính chính xác, tính hữu ích, tính mạch lạc, độ phức tạp và tính dài dòng.

Triển khai. Đối với HERON, chúng tôi sử dụng thứ bậc sau: tính chính xác > tính hữu ích > tính mạch lạc > độ phức tạp > tính dài dòng. Sau đó chúng tôi sử dụng HERON-DPO để tối ưu hóa chính sách. Chúng tôi xem xét hai baseline kỹ thuật phần thưởng: tinh chỉnh dựa trên REINFORCE với trọng số phần thưởng bằng nhau trên tất cả tín hiệu và tinh chỉnh dựa trên REINFORCE với trọng số phần thưởng giảm theo cấp số nhân [Williams, 1992]. Mỗi thuật toán được khởi tạo từ một phiên bản của Phi-2 đã trải qua tinh chỉnh có giám sát trên HelpSteer.

Kết quả. Để đánh giá các mô hình kết quả, chúng tôi sử dụng ba mô hình phần thưởng tiên tiến (RLHF-Flow [Dong et al., 2024], PairRM [Jiang et al., 2023], Eurus-7B [Yuan et al., 2024]) cũng như Claude 3 Sonnet để tính tỷ lệ thắng so với chính sách SFT trên tập dữ liệu kiểm tra HelpSteer. Kết quả có thể thấy trong Bảng 5. Chúng tôi thấy rằng HERON-DPO có thể vượt trội đáng kể so với các baseline trên tất cả các giám khảo. Chúng tôi giả thuyết rằng lợi ích này là do cây quyết định của HERON trên những tín hiệu này có thể nắm bắt các nguyên tắc con người tốt hơn so với kết hợp tuyến tính của các tín hiệu phản hồi. Thêm chi tiết có thể tìm thấy trong Phụ lục N.

Bảng 5: Tỷ lệ thắng so với mô hình SFT được tính bởi các giám khảo LLM khác nhau.
RLHF-Flow PairRM Eurus-7B Claude-3 Trung bình
Kỹ thuật Phần thưởng (Trọng số Bằng nhau) 54.67 58.49 53.68 57.46 56.08
Kỹ thuật Phần thưởng (Trọng số Giảm dần) 59.24 59.64 52.49 56.85 57.06
HERON-DPO 66.20 63.02 63.22 63.82 64.57

4.4 Thêm Thí nghiệm
Để chứng minh rằng HERON có thể huấn luyện một chính sách hợp lý ngay cả khi thứ bậc không rõ ràng, chúng tôi thí nghiệm trên bốn nhiệm vụ điều khiển robot: Ant, Half-Cheetah, Hopper và Pendulum. Chúng tôi sử dụng trình mô phỏng PyBullet, nơi phần thưởng thực tế được công thức hóa như một kết hợp tuyến tính của một số tín hiệu như tiềm năng của robot, chi phí năng lượng, liệu các khớp có ở giới hạn của chúng hay không, và liệu chân robot có va chạm hay không [Coumans và Bai, 2016]. Những yếu tố này không nhất thiết hiển thị thứ bậc rõ ràng. Thêm chi tiết về môi trường này có thể tìm thấy trong Phụ lục E. Kết quả có thể tìm thấy trong Bảng 6, nơi chúng tôi quan sát thấy rằng mặc dù HERON không thể luôn hoạt động tốt như phần thưởng thực tế, nó luôn có thể vượt quá hiệu suất của baseline kỹ thuật phần thưởng.

Bảng 6: Phần thưởng thực tế thu được trong các môi trường robot.
Ant Hopper Cheetah Pendulum
Thực tế 0.99(0.0) 0.86(0.01) 0.94(0.10) 1.0(0.01)
Kỹ thuật Phần thưởng 0.88(0.02) 0.72(0.05) 0.61(0.04) 0.99(0.0)
HERON 1.0(0.01) 0.78(0.04) 0.62(0.04) 1.0(0.0)

4.5 Nghiên cứu Loại bỏ
Phân tích Thời gian Huấn luyện. Chi phí tính toán chính của HERON đến từ việc huấn luyện mô hình phần thưởng, vì thu thập dữ liệu đã là một phần của hầu hết các thuật toán RL và thu thập ưu tiên rất nhanh. Để tăng tốc huấn luyện mô hình phần thưởng trong môi trường đa giai đoạn, chúng tôi có thể sử dụng lịch trình huấn luyện được ủ (xem Phụ lục J). Thời gian huấn luyện được chuẩn hóa của HERON, kỹ thuật phần thưởng và học dựa trên ensemble được hiển thị trong Hình 4a. HERON chậm hơn 25% so với kỹ thuật phần thưởng trung bình, điều này khá hợp lý cho rằng chi phí điều chỉnh của kỹ thuật phần thưởng thường lớn.

Siêu tham số. Chúng tôi đặt δi bằng độ lệch chuẩn của zi trên dữ liệu được thu thập trong các thí nghiệm của chúng tôi. Tuy nhiên, chúng tôi đánh giá độ nhạy cảm của HERON đối với những tham số này trong Hình 4. Chúng tôi thấy các giá trị trong [0, 2∗σi] hoạt động tốt, trong đó σi là độ lệch chuẩn của zi.

Chi phí Điều chỉnh. Cuối cùng, chúng tôi so sánh chi phí điều chỉnh của HERON với kỹ thuật phần thưởng trong môi trường điều khiển đèn giao thông. Đối với HERON, chúng tôi xem xét việc điều chỉnh với kiến thức miền chính xác (thứ bậc được cung cấp) và với kiến thức miền không chính xác (3 yếu tố hàng đầu được cung cấp nhưng thứ tự của chúng không được chỉ định). Đối với kỹ thuật phần thưởng, chúng tôi xem xét việc điều chỉnh với kiến thức miền chính xác và không có kiến thức miền (trọng số phần thưởng không có cấu trúc phân cấp). Đối với trường hợp sau, chúng tôi điều chỉnh trọng số với tối ưu hóa Bayesian. Kết quả được hiển thị trong Hình 5. Chúng tôi thấy rằng cả hai phiên bản của HERON đều vượt trội đáng kể so với kỹ thuật phần thưởng. Mặc dù tối ưu hóa Bayesian có thể huấn luyện các chính sách hiệu suất cao, nó yêu cầu 5 đến 15 lần số vòng lặp điều chỉnh của HERON.

5 Thảo luận
Các Tình huống Phù hợp. HERON không nhằm mục đích phục vụ như một giải pháp chung cho tất cả các bài toán RL; tuy nhiên, nó có thể hoạt động khá tốt trong các môi trường cụ thể. Cụ thể, HERON sẽ hữu ích nhất trong các môi trường có một số tín hiệu phản hồi có sẵn một cách rẻ tiền và người giám sát con người có thể xếp hạng những tín hiệu này. Các thí nghiệm của chúng tôi cho thấy rằng trong những môi trường như vậy (tạo mã và điều khiển đèn giao thông) HERON có thể vượt trội hơn các baseline tiên tiến. Hơn nữa, HERON thể hiện triển vọng lớn và tính đa năng vì nó thậm chí có thể đạt được hiệu suất tốt trong các môi trường không lý tưởng (điều khiển robot).

--- TRANG 12 ---
(a) Thời gian Huấn luyện
(b) δi
(c) α

Hình 4: Thời gian huấn luyện và nghiên cứu loại bỏ cho HERON.

Hình 5: Chi phí điều chỉnh của HERON trong nhiệm vụ điều khiển đèn giao thông.

triển vọng lớn và tính đa năng vì nó thậm chí có thể đạt được hiệu suất tốt trong các môi trường không lý tưởng (điều khiển robot).

Tính linh hoạt Phần thưởng. HERON cũng có khả năng xử lý các tín hiệu phản hồi có tầm quan trọng gần như bằng nhau. Một giải pháp khả thi là lật thứ hạng tầm quan trọng của các tín hiệu phản hồi có tầm quan trọng bằng nhau với xác suất nhất định trong bước thu thập ưu tiên được mô tả trong Phần 3. Người ta thậm chí có thể thiết kế các thuật toán thu thập ưu tiên phức tạp hơn không yêu cầu thứ bậc nghiêm ngặt trên các tín hiệu phản hồi, mà chúng tôi để dành cho công việc tương lai.

Môi trường Ngân sách Gắn nhãn Thấp. RLHF là một phương pháp hiệu quả để có được một mô hình chính sách mạnh mẽ. Điều này là do con người có thể cung cấp thông tin chi tiết nhiều thông tin hơn cho mô hình phần thưởng so với các tín hiệu phản hồi. Tuy nhiên, việc sử dụng con người để so sánh mọi cặp quỹ đạo thường không khả thi và khó tạo ra hướng dẫn phù hợp về cách so sánh quỹ đạo. Chúng tôi xem xét một môi trường riêng biệt, nơi chỉ có một số tín hiệu phản hồi và một số kiến thức miền về chúng có sẵn. Trong trường hợp này, kỹ thuật phần thưởng trở thành một lựa chọn hợp lý và rẻ tiền, cũng như phương pháp của chúng tôi. Do đó, kỹ thuật phần thưởng là baseline phù hợp nhất để so sánh.

RL Đa mục tiêu. Phương pháp của chúng tôi không được thiết kế cụ thể cho các bài toán RL đa mục tiêu. RL đa mục tiêu chung phức tạp vì một số mục tiêu ảnh hưởng bất lợi đến các mục tiêu khác

--- TRANG 13 ---
trong quá trình tối ưu hóa. Trong trường hợp này, các nhà nghiên cứu cố gắng tìm biên Pareto và cân bằng giữa các mục tiêu trong nhiều tình huống khác nhau [Van Moffaert và Nowé, 2014, Mossalam et al., 2016]. Ngược lại, phương pháp của chúng tôi có thể được áp dụng nếu và chỉ nếu các tín hiệu phản hồi có sẵn và có cấu trúc phân cấp.

Công việc Tương lai. Thủ tục so sánh phân cấp được đề xuất của chúng tôi có tính linh hoạt và có thể được mở rộng theo nhiều cách khác nhau. Ví dụ, chúng tôi có thể xem xét cấp độ của phản hồi trong thứ bậc như cường độ ưu tiên. Cụ thể hơn, các kết quả ưu tiên được rút ra dựa trên phản hồi quan trọng hơn tạo ra các ưu tiên mạnh hơn giữa hai quỹ đạo RL. Để khai thác cường độ ưu tiên như vậy, chúng tôi có thể thêm các siêu tham số điều chỉnh hoặc lề bổ sung vào việc học phần thưởng. Vì điều này vượt ra ngoài phạm vi hiện tại của chúng tôi, chúng tôi sẽ để dành cho điều tra tương lai.

Tài liệu tham khảo
Yue Deng, Feng Bao, Youyong Kong, Zhiquan Ren, và Qionghai Dai. Deep direct reinforcement learning for financial signal representation and trading. IEEE transactions on neural networks and learning systems, 28(3):653–664, 2016.

Ammar Haydari và Yasin Yılmaz. Deep reinforcement learning for intelligent transportation systems: A survey. IEEE Transactions on Intelligent Transportation Systems, 23(1):11–32, 2020.

Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, và Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:21314–21328, 2022.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, và Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, và cộng sự. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, và Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.

Serena Booth, Bradley W Knox, Julie Shah, Scott Niekum, Peter Stone, và Alessandro Allievi. The perils of trial-and-error reward design: misdesign through overfitting and invalid task specifications. Trong AAAI Conference on Artificial Intelligence, 2023.

Zhi Zhang, Jiachen Yang, và Hongyuan Zha. Integrating independent and centralized multi-agent reinforcement learning for traffic signal network optimization. arXiv preprint arXiv:1909.10651, 2019.

--- TRANG 14 ---
Justin Fu, Katie Luo, và Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. arXiv preprint arXiv:1710.11248, 2017.

Zheng Wu, Wenzhao Lian, Vaibhav Unhelkar, Masayoshi Tomizuka, và Stefan Schaal. Learning dense rewards for contact-rich manipulation tasks. Trong 2021 IEEE International Conference on Robotics and Automation (ICRA), trang 6214–6221. IEEE, 2021.

Elise Van der Pol và Frans A Oliehoek. Coordinated deep reinforcement learners for traffic light control. Proceedings of learning, inference and control of multi-agent systems (at NIPS 2016), 8: 21–38, 2016.

Cathy Wu, Aboudy Kreidieh, Kanaad Parvate, Eugene Vinitsky, và Alexandre M Bayen. Flow: A modular learning framework for autonomy in traffic. arXiv preprint arXiv:1710.05465, 2017.

Xiao-Yang Liu, Hongyang Yang, Qian Chen, Runjia Zhang, Liuqing Yang, Bowen Xiao, và Christina Dan Wang. Finrl: A deep reinforcement learning library for automated stock trading in quantitative finance. arXiv preprint arXiv:2011.09607, 2020.

Ralph Allan Bradley và Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324–345, 1952.

Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, và cộng sự. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 27730–27744, 2022.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, và cộng sự. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

Erwin Coumans và Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. 2016.

Andrew Y Ng, Daishi Harada, và Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. Trong Icml, tập 99, trang 278–287. Citeseer, 1999.

Ana C Tenorio-Gonzalez, Eduardo F Morales, và Luis Villasenor-Pineda. Dynamic reward shaping: training a robot by voice. Trong Advances in Artificial Intelligence–IBERAMIA 2010: 12th Ibero-American Conference on AI, Bahía Blanca, Argentina, November 1-5, 2010. Proceedings 12, trang 483–492. Springer, 2010.

Sam Michael Devlin và Daniel Kudenko. Dynamic potential-based reward shaping. Trong Proceedings of the 11th international conference on autonomous agents and multiagent systems, trang 433–440. IFAAMAS, 2012.

Bhaskara Marthi. Automatic shaping and decomposition of reward functions. Trong Proceedings of the 24th International Conference on Machine learning, trang 601–608, 2007.

--- TRANG 15 ---
Marek Grzes và Daniel Kudenko. Learning potential for reward shaping in reinforcement learning with tile coding. Trong Proceedings AAMAS 2008 Workshop on Adaptive and Learning Agents and Multi-Agent Systems (ALAMAS-ALAg 2008), trang 17–23, 2008.

Zhao-Yang Fu, De-Chuan Zhan, Xin-Chun Li, và Yi-Xing Lu. Automatic successive reinforcement learning with multiple auxiliary rewards. Trong IJCAI, trang 2336–2342, 2019.

Yujing Hu, Weixun Wang, Hangtian Jia, Yixiang Wang, Yingfeng Chen, Jianye Hao, Feng Wu, và Changjie Fan. Learning to utilize shaping rewards: A new approach of reward shaping. Advances in Neural Information Processing Systems, 33:15931–15941, 2020.

Reza Refaei Afshar, Yingqian Zhang, Joaquin Vanschoren, và Uzay Kaymak. Automated reinforcement learning: An overview. arXiv preprint arXiv:2201.05000, 2022.

Jack Parker-Holder, Raghu Rajan, Xingyou Song, André Biedenkapp, Yingjie Miao, Theresa Eimer, Baohe Zhang, Vu Nguyen, Roberto Calandra, Aleksandra Faust, và cộng sự. Automated reinforcement learning (autorl): A survey and open problems. Journal of Artificial Intelligence Research, 74:517–568, 2022.

Aleksandra Faust, Anthony Francis, và Dar Mehta. Evolving rewards to automate reinforcement learning. arXiv preprint arXiv:1905.07628, 2019.

Hao-Tien Lewis Chiang, Aleksandra Faust, Marek Fiser, và Anthony Francis. Learning navigation behaviors end-to-end with autorl. IEEE Robotics and Automation Letters, 4(2):2007–2014, 2019.

Andrew Y Ng, Stuart Russell, và cộng sự. Algorithms for inverse reinforcement learning. Trong Icml, tập 1, trang 2, 2000.

Pieter Abbeel và Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. Trong Proceedings of the twenty-first international conference on Machine learning, trang 1, 2004.

Abdeslam Boularias, Jens Kober, và Jan Peters. Relative entropy inverse reinforcement learning. Trong Proceedings of the fourteenth international conference on artificial intelligence and statistics, trang 182–189. JMLR Workshop and Conference Proceedings, 2011.

Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, và Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, và cộng sự. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.

Dorsa Sadigh, Anca D Dragan, Shankar Sastry, và Sanjit A Seshia. Active preference-based learning of reward functions. 2017.

--- TRANG 16 ---
Erdem Bıyık, Nicolas Huynh, Mykel J Kochenderfer, và Dorsa Sadigh. Active preference-based gaussian process regression for reward learning. arXiv preprint arXiv:2005.02575, 2020.

Richard S Sutton và Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, và Philipp Moritz. Trust region policy optimization. Trong International conference on machine learning, trang 1889–1897. PMLR, 2015.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, và Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, và Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems, 36, 2024.

Hua Wei, Guanjie Zheng, Huaxiu Yao, và Zhenhui Li. Intellilight: A reinforcement learning approach for intelligent traffic light control. Trong Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, trang 2496–2505, 2018.

Tim Brys, Anna Harutyunyan, Peter Vrancx, Ann Nowé, và Matthew E Taylor. Multi-objectivization and ensembles of shapings in reinforcement learning. Neurocomputing, 263: 48–59, 2017.

Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, và Chandan K Reddy. Execution-based code generation using deep reinforcement learning. arXiv preprint arXiv:2301.13816, 2023.

Yue Wang, Weishi Wang, Shafiq Joty, và Steven CH Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859, 2021.

Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, và cộng sự. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.

Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, và cộng sự. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021.

Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, và cộng sự. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.

Mojan Javaheripi, Sébastien Bubeck, và cộng sự. Phi-2: The surprising power of small language models, 2023. URL https://www. microsoft. com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models.

--- TRANG 17 ---
Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, và cộng sự. Helpsteer: Multi-attribute helpfulness dataset for steerlm. arXiv preprint arXiv:2311.09528, 2023.

Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229–256, 1992.

Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, và Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv e-prints, trang arXiv–2405, 2024.

Dongfu Jiang, Xiang Ren, và Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023.

Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, và Maosong Sun. Advancing llm reasoning generalists with preference trees, 2024.

Kristof Van Moffaert và Ann Nowé. Multi-objective reinforcement learning using sets of pareto dominating policies. The Journal of Machine Learning Research, 15(1):3483–3512, 2014.

Hossam Mossalam, Yannis M Assael, Diederik M Roijers, và Shimon Whiteson. Multi-objective deep reinforcement learning. arXiv preprint arXiv:1610.02707, 2016.

--- TRANG 18 ---
z1(τ1)−z1(τ2)<0z2(τ1)−z2(τ2)z3(τ1)−z3(τ2)≈0≈0μ=2μ=1<0μ=2<0μ=2>0μ=1>0μ=1>0

Hình 6: Thu thập ưu tiên: so sánh quỹ đạo τ1 và τ2 thông qua 3 tín hiệu phản hồi z1,z2,z3.

A Phụ lục / tài liệu bổ sung

B Minh họa Thu thập Ưu tiên

C Chi tiết Thí nghiệm Điều khiển Cổ điển
Đối với các thí nghiệm điều khiển cổ điển, chúng tôi sử dụng OpenAI gym [Brockman et al., 2016]. Để huấn luyện tất cả các chính sách, chúng tôi sử dụng thuật toán DDPG, trong đó các chính sách được tham số hóa bởi các MLP ba lớp với 256 đơn vị ẩn cho mỗi lớp. Chúng tôi sử dụng bộ tối ưu Adam, và tìm kiếm tốc độ học trong [1×10−5,1×10−3].

Đối với mountain car, chúng tôi huấn luyện tổng cộng 15000 bước thời gian và bắt đầu huấn luyện sau 5000 bước thời gian. Đối với pendulum, chúng tôi huấn luyện tổng cộng 50000 bước thời gian và bắt đầu học sau 25000 bước thời gian.

D Baseline

D.1 Baseline Ensemble
Ngoài phần thưởng thực tế, chúng tôi so sánh thuật toán HERON với hai baseline ensemble được lấy cảm hứng từ Brys et al. [2017]. Những baseline ensemble này huấn luyện một chính sách riêng biệt trên mỗi tín hiệu phản hồi, và sau đó kết hợp đầu ra của các chính sách trong một trạng thái đã cho để chọn một hành động. Trong mọi môi trường, chúng tôi huấn luyện mỗi chính sách trong ensemble với các tham số tương tự như được sử dụng cho baseline kỹ thuật phần thưởng và chúng tôi lại điều chỉnh tốc độ học trong [1 ×10−5,1×10−3].

Như được mô tả trong văn bản chính, chúng tôi xem xét hai biến thể của thuật toán dựa trên ensemble này: một trong đó hành động được chọn theo trung bình trên mỗi chính sách (a←argmaxa∈APn k=11 nπk(s,a)) và một trong đó thứ hạng ưu tiên được sử dụng làm đầu vào cho HERON được sử dụng để kết hợp các hành động (a← argmaxa∈APn k=1γkπk(s,a)). Với biến thể thứ hai, γ được chọn từ {0.25,0.35,0.45,···,0.95,0.99,1}.

--- TRANG 19 ---
D.2 Baseline Kỹ thuật Phần thưởng
Chúng tôi cũng kiểm tra hiệu suất của một baseline kỹ thuật phần thưởng trong đó phần thưởng được công thức hóa là Pn i=1βizi, trong đó β là một siêu tham số được chọn từ {0.3,0.4,...,0.9,1.0} và zi là các tín hiệu phản hồi được chuẩn hóa. Các tín hiệu phản hồi được sắp xếp theo thứ bậc phần thưởng HERON, làm cho đây là một baseline kỹ thuật phần thưởng rất thực tế và cạnh tranh. Tuy nhiên, chúng tôi gặp một số thách thức khi cố gắng làm cho thuật toán này hoạt động. Đầu tiên, tất cả các tín hiệu phản hồi cần được chuẩn hóa, điều này yêu cầu các thuật toán phức tạp hoặc nhiều lần triển khai agent trước khi huấn luyện. Ngoài ra, chúng tôi thấy rằng baseline này rất nhạy cảm với β và do đó có chi phí điều chỉnh cao hơn. Ngoài ra, nó thường không thể đánh bại hiệu suất của HERON. Chúng tôi vẽ đồ thị hiệu suất của baseline kỹ thuật phần thưởng trong Hình 7. Lưu ý rằng biểu đồ này hiển thị hiệu suất trong suốt quá trình huấn luyện, và HERON thường hiển thị phần thưởng lớn hơn (tương đối) trong các giai đoạn cuối của huấn luyện.

Như chúng ta có thể thấy từ Hình 7, baseline kỹ thuật phần thưởng yêu cầu điều chỉnh rộng rãi để đạt được hiệu suất tốt. Ngoài ra, việc lựa chọn chiến lược chuẩn hóa rất quan trọng (Hình 7d). Những kết quả này tiếp tục cho thấy những lợi ích của HERON.

(a) Ant
(b) Half-Cheetah
(c) Hopper
(d) Đèn Giao thông
(e) Pendulum
(f) Mountain Car

Hình 7: Nghiên cứu loại bỏ của baseline kỹ thuật phần thưởng.

E Robot
Tất cả các thí nghiệm của chúng tôi được tiến hành với trình mô phỏng PyBullet [Coumans và Bai, 2016]. Các tín hiệu phản hồi trong mỗi môi trường như sau: đối với Ant, đó là liệu robot có còn sống hay không, tiến triển hướng tới trạng thái mục tiêu, liệu các khớp có ở giới hạn của chúng hay không, và liệu chân có va chạm hay không. Đối với HalfCheetah, các tín hiệu là tiềm năng và chi phí năng lượng. Đối với Hopper, các tín hiệu là tiềm năng, phần thưởng còn sống và chi phí năng lượng.

--- TRANG 20 ---
F Điều khiển Đèn Giao thông
Trong các thí nghiệm của chúng tôi, chúng tôi huấn luyện bốn agent trong một lưới hai nhân hai. Độ dài của mỗi đoạn đường là 400 mét và xe đi vào qua mỗi làn đường chảy vào với tốc độ 700 xe/giờ. Lưới giao thông có thể thấy trong Hình 8. Tần số điều khiển là 1 Hz, tức là chúng ta cần nhập một hành động mỗi giây. Phần thưởng dựa trên các thuộc tính sau cho mỗi agent n:

• qn: Tổng độ dài hàng đợi trong tất cả các làn đường vào.
• wtn: Tổng thời gian chờ xe trong tất cả các làn đường vào.
• dln: Tổng độ trễ của tất cả xe trong các làn đường vào.
• emn: Số lần dừng khẩn cấp bởi xe trong tất cả các làn đường vào.
• fln: Một biến Boolean cho biết liệu pha đèn có thay đổi hay không.
• vln: Số xe đi qua giao lộ.

Sau đó chúng ta có thể định nghĩa phần thưởng kỹ thuật phần thưởng là
Rn=−0.5qn−0.5wtn−0.5dln−0.25emn−fln+vln.

Tất cả các thuật toán có cùng chiến lược huấn luyện. Mỗi agent được huấn luyện trong ba tập với 3000 bước thời gian SUMO mỗi tập. Ở đầu quá trình huấn luyện, agent đưa ra quyết định ngẫu nhiên để lấp đầy mạng đường trước khi huấn luyện bắt đầu. Mỗi thuật toán được đánh giá trong 5000 bước thời gian, trong đó 1000 giây đầu được sử dụng để lấp đầy đường một cách ngẫu nhiên. Đối với điều hòa đối kháng, chúng tôi sử dụng chuẩn ℓ2 để ràng buộc các cuộc tấn công δ.

Hình 8: Môi trường điều khiển đèn giao thông.

--- TRANG 21 ---
G So sánh RLHF
Để so sánh rõ ràng RLHF với HERON, chúng tôi so sánh các thuật toán trong môi trường pendulum. Để mô phỏng phản hồi con người, chúng tôi xếp hạng một quỹ đạo trên quỹ đạo khác nếu phần thưởng thực tế đạt được bởi quỹ đạo đó cao hơn phần thưởng thực tế đạt được bởi quỹ đạo khác. Sau đó chúng tôi đánh giá hiệu suất của thuật toán RLHF được mô phỏng này khi có nhiều lượng phản hồi khác nhau được cung cấp. Kết quả có thể thấy trong Hình 9. Trong bảng này, chúng tôi thay đổi số lượng phản hồi trong RLHF, trong khi giữ số lượng phản hồi cho HERON không đổi. Trong môi trường này, HERON có thể hoạt động tốt như RLHF, nhưng hiệu suất tốt như vậy không được đảm bảo trong mọi môi trường.

Hình 9: So sánh RLHF trong Môi trường Pendulum.

H Tính linh hoạt HERON
Trong phần này, chúng tôi đánh giá cách hành vi của các chính sách được huấn luyện bởi HERON thay đổi khi chúng tôi thay đổi thứ bậc phần thưởng. Chúng tôi vẽ đồ thị một số thứ bậc trong Hình 10. Kỹ thuật phần thưởng là đường đen dày. Chúng tôi thử ba tín hiệu làm tín hiệu quan trọng nhất (số xe qua, thời gian chờ và độ trễ). Chúng tôi lưu ý rằng tất cả những quan sát này có thể vượt trội hơn phần thưởng kỹ thuật phần thưởng, mặc dù chúng tôi đo lường lợi nhuận với phần thưởng kỹ thuật phần thưởng. Một sự lệch quan trọng khỏi hiệu suất tốt này là khi thời gian chờ không được xếp hạng cao. Thời gian chờ là một tín hiệu rất quan trọng, và khi chúng tôi không đặt biến này cao trong thứ bậc, hiệu suất trở nên không ổn định khi đo theo phần thưởng kỹ thuật phần thưởng. Điều này là do nếu chúng tôi bỏ qua thời gian chờ của xe, chính sách có thể làm cho một số xe chờ trong thời gian dài, điều này không lý tưởng. Tuy nhiên, điều này có thể dễ dàng được tính đến trong quá trình thiết kế phần thưởng.

Hình 10: Các thứ bậc phần thưởng khác nhau trong HERON.

--- TRANG 22 ---
I Tạo Mã
Trong phần này, chúng tôi mô tả chi tiết cho nhiệm vụ tạo mã.

I.1 Sao chép Hành vi
Để huấn luyện mô hình hành vi ban đầu, chúng tôi sử dụng sao chép hành vi (tinh chỉnh có giám sát) để điều chỉnh CodeT5 được huấn luyện trước cho nhiệm vụ APPS. Cụ thể, chúng tôi huấn luyện với mất mát entropy chéo trong 12000 lần lặp, sử dụng kích thước batch là 64. Chúng tôi sử dụng bộ tối ưu Adam với tốc độ học 2×10−5.

I.2 Lựa chọn Temperature
Một siêu tham số có thể có tác động lớn đến chất lượng tạo ra là tham số temperature, cơ bản là thay đổi mức độ tham lam chúng ta trong bước lấy mẫu token tiếp theo. Trong tất cả các môi trường, chúng tôi theo triển khai của Le et al. [2022], sử dụng temperature 0.6 cho APPS và 1.2 cho MBPP. Ngoài ra, chúng tôi lấy mẫu token một cách tham lam để xây dựng một mẫu baseline cho mỗi bài toán.

I.3 Mô hình Phần thưởng
Đã được lưu ý rằng các mô hình phần thưởng thường overfitting với tập dữ liệu [Ouyang et al., 2022]. Do đó chúng tôi sử dụng một phiên bản nhỏ hơn của CodeT5 cho mô hình phần thưởng của chúng tôi chỉ với 220 triệu tham số. Chúng tôi huấn luyện mô hình này khoảng 40000 bước với kích thước batch là 64. Đây là khoảng một epoch trên tập dữ liệu ưu tiên, bao gồm 20 mẫu cho mỗi bài toán được lấy mẫu từ mô hình hành vi và một số mẫu chuyên gia được cung cấp bởi tập dữ liệu APPS. Chúng tôi sử dụng bộ tối ưu Adam với tốc độ học 2 ×10−5.

I.4 Học Tăng cường
Một khi chúng tôi đã huấn luyện mô hình phần thưởng, chúng tôi gán một phần thưởng cho mỗi chương trình trong tập dữ liệu ưu tiên của chúng tôi và huấn luyện sử dụng học tăng cường trên tập dữ liệu này. Tương tự như Le et al. [2022], chúng tôi huấn luyện trên mất mát gradient chính sách và thêm mất mát entropy chéo như một số hạng điều hòa. Chúng tôi so sánh phương pháp của chúng tôi với hai phần thưởng kỹ thuật phần thưởng:

Phần thưởng CodeRL. Phần thưởng đầu tiên chúng tôi so sánh HERON là từ CodeRL, định nghĩa phần thưởng là
RCodeRL (s) =−1.0 nếu chương trình s thất bại trong biên dịch
−0.6 nếu chương trình s có lỗi runtime
−0.3 nếu chương trình s thất bại một bài kiểm tra đơn vị
1.0 nếu chương trình s vượt qua tất cả bài kiểm tra đơn vị.

Phần thưởng PPOCoder. Phần thưởng thứ hai chúng tôi so sánh HERON dựa trên PPOCoder, có nhận thức về việc bao gồm độ tương tự cú pháp với các mẫu chuyên gia trong phần thưởng. Điều này làm mượt phần thưởng một cách hiệu quả, và do đó có thể làm cho phần thưởng nhiều thông tin hơn. Cụ thể, họ so sánh các cây cú pháp trừu tượng của các chương trình được tạo ra với các chương trình ví dụ chuyên gia. Điều này được tính như
Rast(s,bs) = Count(AST s,ASTbs)/Count(AST s).

Sau đó chúng tôi xây dựng phần thưởng cuối cùng dựa trên PPOCoder là RPPOCoder (s) =RCodeRL (s)+λMEAN bs(Rast(s,bs)), trong đó MEAN là toán tử trung bình. Chúng tôi điều chỉnh λ∈{0.001,0.01,0.1,1}. Chúng tôi lưu ý rằng phần thưởng PPOCoder gốc chứa nhiều tín hiệu phản hồi hơn, nhưng chúng tôi không sử dụng tất cả chúng do chi phí điều chỉnh lớn cần thiết để điều chỉnh chúng tôi.

Đối với cả hai phần thưởng này và phần thưởng HERON, chúng tôi điều chỉnh tốc độ học trong {3×10−6,5×10−6,8× 10−6}.

I.5 Các Chương trình Ví dụ
Để phân tích thêm hiệu suất của HERON, chúng tôi kiểm tra một số chương trình được tạo ra bởi HERON. Những chương trình này được chọn ngẫu nhiên. Chúng tôi hiển thị các lời nhắc và hoàn thành được nối trong Hình 11.

J Huấn luyện Phần thưởng
Trong phần này, chúng tôi chi tiết về việc huấn luyện mô hình phần thưởng của chúng tôi. Đối với các nhiệm vụ điều khiển cổ điển và nhiệm vụ điều khiển đèn giao thông, chúng tôi không có chính sách hành vi ban đầu tốt, vì vậy chúng tôi phải huấn luyện mô hình phần thưởng của chúng tôi theo cách lặp lại. Trong những môi trường này, chúng tôi cập nhật mô hình phần thưởng một cách lặp lại sử dụng các mẫu từ phiên bản hiện tại của chính sách. Theo cách này, mô hình phần thưởng được huấn luyện trên các mẫu được tạo ra từ các chính sách ngày càng tốt hơn.

Như chúng tôi đã đề cập trong cuộc thảo luận về chi phí tính toán của HERON, chi phí huấn luyện mô hình phần thưởng phụ thuộc vào tần số mà mô hình phần thưởng được huấn luyện. Đối với các môi trường điều khiển cổ điển, chúng tôi đơn giản sử dụng lịch trình huấn luyện tuyến tính, trong đó mô hình phần thưởng được cập nhật mỗi 400 bước. Đối với điều khiển đèn giao thông, chúng tôi huấn luyện mô hình phần thưởng với tần số được ủ, trong đó mô hình phần thưởng được huấn luyện mỗi 100 υt bước, trong đó υ được đặt 1.3 và t là bước thời gian hiện tại.

Chúng tôi chứng minh việc huấn luyện mô hình phần thưởng đa bước trong Hình 12. Sự sụt giảm mạnh về độ chính xác xảy ra tại bước thời gian 1000, nơi mô hình hành vi thay đổi từ ngẫu nhiên thành chính sách được huấn luyện. Thay đổi lớn về độ chính xác này cho thấy rằng việc huấn luyện mô hình phần thưởng đa bước là cần thiết, vì các mô hình phần thưởng được huấn luyện trên hành vi ngẫu nhiên không hoạt động tốt khi hành vi thay đổi.

J.1 Siêu tham số Alpha
Mô tả chính thức về tín hiệu định hình: Cho một quỹ đạo τ, hãy so sánh nó với n quỹ đạo khác τ1,...,τn. Gọi F(τ) biểu thị cấp độ trung bình của cây quyết định mà τ thắng. Để cho phép chúng tôi kết hợp kiến thức miền vào HERON, chúng tôi nhân phần thưởng được gán cho τ với một tín hiệu

--- TRANG 23 ---
Hình 11: Các chương trình ví dụ được tạo ra bởi LLM được huấn luyện với HERON.

--- TRANG 24 ---
Hình 12: Độ chính xác mô hình phần thưởng trong suốt quá trình huấn luyện.

αF(τ), trong đó α là một siêu tham số. Khi các tín hiệu phản hồi là phân loại, F(τ) có thể nắm bắt category mà τ nằm trong, và việc nhân phần thưởng với αF(τ) có thể kiểm soát sự tách biệt phần thưởng giữa các category khác nhau.

Mô tả trực quan về tín hiệu định hình: Như đã đề cập trong văn bản chính, siêu tham số α có thể được sử dụng để kiểm soát hình dạng của phần thưởng. Trong Hình 13, chúng tôi hiển thị cách thay đổi α thay đổi hình dạng phần thưởng trong nhiệm vụ tạo mã.

(a) α= 1.0
(b) α= 2.0
(c) α= 3.0
(d) α= 4.0

Hình 13: Hình dạng phần thưởng với các giá trị α khác nhau.

--- TRANG 25 ---
K Thiết lập Tính toán
Đối với các nhiệm vụ điều khiển cổ điển và thí nghiệm điều khiển đèn giao thông, chúng tôi chạy thí nghiệm trên CPU Intel Xeon 6154. Đối với nhiệm vụ tạo mã, chúng tôi huấn luyện với GPU Tesla V100 32GB.

L Đường cong Học Robot
Trong Hình 14, chúng tôi hiển thị các đường cong học trong các môi trường robot.

M Hạn chế
Hạn chế chính của HERON là không phải mọi bài toán sẽ chứa thứ hạng rõ ràng trên các tín hiệu phản hồi, vì một số tín hiệu có thể có tầm quan trọng bằng nhau. Chúng tôi đề xuất giảm thiểu hạn chế này trong các công trình tương lai bằng cách cho phép các sự hòa hoặc sử dụng cây quyết định ngẫu nhiên trong thủ tục thu thập ưu tiên.

(a) Ant
(b) Hopper
(c) HalfCheetah

Hình 14: Đường cong huấn luyện trong các nhiệm vụ robot khác nhau.

--- TRANG 26 ---
N Thí nghiệm Căn chỉnh
Ở đây chúng tôi trình bày thêm chi tiết về các thí nghiệm căn chỉnh mô hình ngôn ngữ của chúng tôi. Chúng tôi sử dụng LoRA [Hu et al., 2020] cho tất cả các thí nghiệm. Đối với mô hình cơ sở SFT, chúng tôi huấn luyện trong hai epoch với tốc độ học 5e-5. Chúng tôi sử dụng kích thước batch 32 và huấn luyện trong 2 epoch. Đối với Reinforce, chúng tôi cũng sử dụng tốc độ học 5e-5, kích thước batch 32, và huấn luyện trong 2 epoch. Đối với DPO, chúng tôi sử dụng tốc độ học 5e-5, kích thước batch 32, β= 0.1, và huấn luyện trong 2 epoch.

Để đánh giá, chúng tôi sử dụng mỗi mô hình phần thưởng như được chỉ định trong phiên bản tương ứng của chúng. Đối với đánh giá dựa trên Claude 3, chúng tôi nhắc nó chọn phản hồi chính xác, hữu ích và vô hại nhất.
