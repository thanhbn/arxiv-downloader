# 2104.10995.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2104.10995.pdf
# Kích thước tệp: 372791 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Được xuất bản như một bài báo hội thảo tại "BRAIN2AI" (ICLR 2021)
MỘT KHOẢNG CÁCH HỌC TẬP GIỮA KHOA HOC THẦN KINH
VÀ HỌC TĂNG CƯỜNG
Samuel T. Wauthier, Pietro Mazzaglia, Ozan C ¸ atal, Cedric De Boom, Tim Verbelen &
Bart Dhoedt
IDLab, Khoa Công nghệ Thông tin
Đại học Ghent – imec
Technologiepark-Zwijnaarde 126, B-9052 Ghent, Bỉ
fsamuel.wauthier, pietro.mazzaglia, ozan.catal, cedric.deboom,
tim.verbelen, bart.dhoedt g@ugent.be
TÓM TẮT
Trong lịch sử, trí tuệ nhân tạo đã rút ra nhiều cảm hứng từ khoa học thần kinh để thúc đẩy các tiến bộ trong lĩnh vực này. Tuy nhiên, tiến bộ hiện tại trong học tăng cường chủ yếu tập trung vào các vấn đề chuẩn mực không thể nắm bắt được nhiều khía cạnh đang được quan tâm trong khoa học thần kinh ngày nay. Chúng tôi minh họa điểm này bằng cách mở rộng một nhiệm vụ mê cung hình T từ khoa học thần kinh để sử dụng với các thuật toán học tăng cường, và cho thấy rằng các thuật toán tiên tiến nhất không thể giải quyết vấn đề này. Cuối cùng, chúng tôi chỉ ra những hiểu biết từ khoa học thần kinh có thể giúp giải thích một số vấn đề gặp phải.

1 GIỚI THIỆU
Khoa học thần kinh và trí tuệ nhân tạo có một lịch sử lâu dài về sự thụ phấn chéo giữa hai lĩnh vực (Hassabis et al., 2017). Đáng chú ý nhất là những tiến bộ trong học tăng cường sâu (RL), được truyền cảm hứng mạnh mẽ từ các tín hiệu lỗi dự đoán phần thưởng được quan sát trong não, cũng như việc bắt chước các mạch thần kinh để tính toán (Mnih et al., 2015). Các thuật toán RL hiện tại thậm chí còn vượt qua hiệu suất của con người trên nhiều môi trường, thường là môi trường trò chơi như các trò chơi Atari (Badia et al., 2020), Dota 2 (Berner et al., 2019) hoặc Go (Silver et al., 2016). Tuy nhiên, những thành tựu mới nhất này chủ yếu được quy cho các cập nhật gia tăng cho thuật toán RL và việc mở rộng đào tạo đến lượng dữ liệu khổng lồ.

Tiến bộ trong trí tuệ nhân tạo thường được thúc đẩy bởi các vấn đề chuẩn mức. Một ví dụ rõ ràng là bộ dữ liệu ImageNet, đang thúc đẩy tiến bộ trong thị giác máy tính (Krizhevsky et al., 2017; He et al., 2016; Khan et al., 2020). Cũng trong RL, các chuẩn mực tiêu chuẩn đã được giới thiệu để đo lường hiệu suất của các thuật toán khác nhau, như Môi trường Học Atari (Bellemare et al., 2013), Open AI Gym (Brockman et al., 2016), DeepMind Lab (Beattie et al., 2016) và Control Suite (Tassa et al., 2018). Các vấn đề chuẩn mực như vậy ngày càng trở nên khó giải quyết hơn, với trọng tâm chính là tăng độ phức tạp của quan sát (tức là từ trạng thái điều khiển đến pixel), và tăng độ khó trong việc tìm phần thưởng (tức là tín hiệu phần thưởng thưa thớt). Tuy nhiên, chúng tôi lập luận rằng nhiều khía cạnh được quan tâm trong khoa học thần kinh (hành vi), như xử lý tính mơ hồ, tính ngẫu nhiên và trí nhớ, ít được nhấn mạnh hơn.

Chúng tôi đề xuất rằng, một lần nữa, chúng ta nên rút ra nhiều cảm hứng hơn từ khoa học thần kinh và xây dựng các vấn đề chuẩn mực mới, một mặt đủ đơn giản để cho phép các lần lặp tương đối nhanh để phát triển thuật toán, nhưng mặt khác, đủ khó để các thuật toán hiện tại có thể giải quyết. Một ví dụ đáng chú ý theo hướng này là bàn thử nghiệm Animal AI (Beyret et al., 2019), tập trung vào điều hướng không gian và các nhiệm vụ liên quan đến suy luận vị trí đối tượng thông qua tính bền vững của đối tượng và loại bỏ không gian. Chúng tôi tin rằng việc đặt nền tảng cho các vấn đề chuẩn mực RL trong các vấn đề được nghiên cứu kỹ lưỡng trong khoa học thần kinh cũng sẽ thúc đẩy các thuật toán mới được truyền cảm hứng từ các lý thuyết khoa học thần kinh và, đồng thời, cung cấp bằng chứng thực nghiệm cho các lý thuyết này.

Tâm lý học và khoa học thần kinh đã nghiên cứu hành vi của chuột trong mê cung từ đầu thế kỷ 20 (Tolman & Honzik, 1930; O'Keefe & Dostrovsky, 1971). Một ví dụ gần đây hơn là mê cung hình T (nhân tạo), được mô tả bởi Friston et al. (2016). Trong môi trường này, một tác nhân nhân tạo, ví dụ

--- TRANG 2 ---
Được xuất bản như một bài báo hội thảo tại "BRAIN2AI" (ICLR 2021)
Hình 1: Môi trường tìm kiếm thức ăn như được định nghĩa bởi Friston et al. (2016). Hình bên trái và bên phải đại diện cho hai bối cảnh có thể. Tác nhân bắt đầu ở giữa. Hình tròn màu đỏ chỉ ra vị trí của phần thưởng. Các hình tròn màu xanh lá cây và xanh dương là các manh mối cho tác nhân biết bối cảnh nào nó đang ở, ví dụ xanh dương và xanh lá cây có nghĩa là phần thưởng sẽ ở bên trái và bên phải, tương ứng.

một con chuột, được đặt trong một mê cung hình T với một phần thưởng duy nhất, ví dụ một ít pho mát, ở nhánh trái hoặc nhánh phải (xem Hình 1). Nhánh dưới chứa một manh mối về vị trí của phần thưởng. Tác nhân bắt đầu ở trung tâm thế giới và có thể chọn nhánh nào để đi. Quan trọng là, sau khi nó đã vào nhánh trái hoặc phải, nó không được phép thoát khỏi nhánh này. Trạng thái ban đầu của thế giới là mơ hồ, vì tác nhân không có cách nào biết bối cảnh của nó, tức là loại thế giới nào nó tìm thấy chính mình: một thế giới có phần thưởng ở nhánh trái hay một thế giới có phần thưởng ở nhánh phải. Ngoài ra, không biết liệu nhánh phải hay nhánh trái chứa phần thưởng, tác nhân chỉ có thể đoán nhánh nào để chọn. Quan sát manh mối ở nhánh dưới cho phép tác nhân giải quyết tính mơ hồ và đưa ra quyết định có thông tin. Mê cung hình T này ban đầu được mô tả theo cách rời rạc, tức là tác nhân có thể ở bốn vị trí có thể (trung tâm, dưới, trái, phải) và có tối đa hai nước đi. Lưu ý rằng tác nhân được phép đi trực tiếp đến nhánh trái hoặc phải từ nhánh dưới. Trong công trình của họ, Friston et al. (2016) nhấn mạnh rằng chìa khóa để giải quyết vấn đề là việc sử dụng các phương pháp dựa trên niềm tin, trong khi các phương pháp không có niềm tin, như lập trình động, thất bại.

Trong phần còn lại của bài báo này, chúng tôi xem xét lại môi trường được mô tả bởi Friston et al. (2016) và triển khai lại nó như một môi trường dựa trên pixel, giống như trò chơi tương tự như chuẩn mực Atari. Hơn nữa, chúng tôi cho thấy rằng một tập hợp các thuật toán học tăng cường tiên tiến không thể giải quyết môi trường này. Chúng tôi kết thúc bằng việc thảo luận một số hạn chế của các phương pháp RL hiện tại và chỉ ra nơi cảm hứng từ khoa học thần kinh có thể giúp ích.

2 MÊ CUNG E: MÊ CUNG T ĐƯỢC THAY ĐỔI
Trong phần này, chúng tôi mở rộng mê cung T của Friston et al. (2016) để tương thích với các chuẩn mực RL hiện tại1. Ví dụ, chúng tôi sử dụng quan sát dựa trên pixel và cho phép tác nhân di chuyển đến các vị trí trung gian giữa các điểm cuối. Nói cách khác, quan sát trở thành hình ảnh, và không gian trạng thái trở nên lớn hơn. Kết quả trực tiếp, số lượng nước đi tối đa cũng tăng lên, vì môi trường không thể được giải quyết trong hai nước đi nữa.

Môi trường của chúng tôi được hiển thị trong Hình 2. Nó bao gồm góc nhìn từ trên xuống của một mê cung hình E nghiêng. Tam giác màu xám đánh dấu tác nhân, trong khi hình tròn màu vàng chỉ ra phần thưởng. Tường được hiển thị màu đỏ, và cửa sổ được hiển thị màu xanh dương. Các khu vực mà tác nhân không thể nhìn thấy được tô màu đen.

1Mã nguồn môi trường có sẵn tại https://github.com/thesmartrobot/ambigym .

Hình 2: (trái) Vị trí bắt đầu trong mê cung E (có và không có khu vực màu đen). (phải) Đường bị chặn sau khi tác nhân vào một nhánh (có và không có khu vực màu đen).

--- TRANG 3 ---
Được xuất bản như một bài báo hội thảo tại "BRAIN2AI" (ICLR 2021)
0.00.51.0PPO PPO+ICM DQN/Rainbow
0 5000 100000.00.51.0PPO w/ orthogonal init
0 5000 10000PPO+RND
0 5000 10000Dreamer
tập returnlượt chơi
Hình 3: Trung bình cuốn của episode return cho các thuật toán khác nhau. Nhiều lần chạy được thực hiện cho mỗi thuật toán và trung bình cuốn được tính trên 100 tập cho mỗi lần chạy, sau đó trung bình trên tất cả các lần chạy được tính. Các vùng tô bóng hiển thị phạm vi giữa trung bình cuốn tối thiểu và tối đa.

Tác nhân có thể nhìn xuyên qua cửa sổ, nhưng không qua tường. Hình 2 cũng hiển thị môi trường với tất cả các khu vực tô màu đen được loại bỏ cho mục đích minh họa. Trường nhìn (FOV) của tác nhân và số lượng nước đi tối đa là các siêu tham số. Các hành động có thể là: không làm gì, di chuyển về phía trước, và quay trái hoặc phải 45 độ. Ngay khi tác nhân vào nhánh trái hoặc phải, nhánh đóng lại, và tác nhân không thể quay lại (xem Hình 2).

Lưu ý rằng môi trường có thể được mô hình hóa như một quá trình quyết định Markov quan sát từng phần (POMDP). Tác nhân không thể suy ra trạng thái cơ bản trực tiếp từ một quan sát duy nhất, vì các khu vực không nằm trong tầm nhìn của tác nhân là vô hình đối với tác nhân. Hơn nữa, từ góc độ RL, thiết lập này tương ứng với môi trường phần thưởng thưa thớt. Thật vậy, không có phần thưởng nào được nhận trừ khi tác nhân có được hình tròn màu vàng. Chỉ khi tác nhân đạt đến hình tròn màu vàng, nó nhận được phần thưởng là 1 và tập kết thúc.

3 THỰC NGHIỆM
Một số thực nghiệm được thiết lập để xác minh hiệu suất của các thuật toán RL khác nhau trên môi trường. Môi trường được khởi tạo theo cùng một cách cho tất cả các thực nghiệm. Nó được đánh giá trên các phương pháp không có mô hình (DQN (Mnih et al., 2015), Rainbow (Hessel et al., 2018), PPO (Schulman et al., 2017)), PPO với tiền thưởng khám phá (ICM (Pathak et al., 2017), RND (Burda et al., 2019b)) và một phương pháp dựa trên mô hình (DreamerV2 (Hafner et al., 2021)). Chi tiết có thể được tìm thấy trong Phụ lục A và B.

Hiệu suất được đo bằng tần suất tác nhân có thể đạt đến phần thưởng. Trong thực tế, trung bình cuốn của returns trên 100 tập được sử dụng để đánh giá, trong đó return biểu thị tổng phần thưởng của tập. Vì return cho môi trường này là 0 hoặc 1, trung bình cuốn ước tính tần suất mà tác nhân có thể đạt đến phần thưởng. Hơn nữa, chúng tôi tính trung bình trên 10 lần chạy đào tạo khác nhau cho mỗi thuật toán. Trong phần sau, chúng tôi sẽ gọi 'hiệu suất' để chỉ số lần trung bình mà tác nhân có thể đạt đến phần thưởng.

Cho tính chất ngẫu nhiên của môi trường, một tác nhân ngẫu nhiên với số lượng nước đi cho phép không giới hạn sẽ đạt đến phần thưởng 50% thời gian. Nói cách khác, với thước đo hiệu suất được định nghĩa ở trên, thuật toán được xem xét phải đạt đến phần thưởng hơn 50% thời gian để có thể nói rằng nó hoạt động tốt hơn một tác nhân ngẫu nhiên. Điều này quan trọng, vì một tác nhân RL không được đảm bảo hiệu suất 50% khi số lượng nước đi tối đa là nhỏ. Ví dụ, nó có thể không bao giờ học cách đi về phía phần thưởng do không bao giờ đạt đến phần thưởng.

Hình 3 hiển thị trung bình cuốn trung bình của return mỗi tập cho các thuật toán đã học cách đi đến phần thưởng trong quá trình đào tạo. Các vùng tô bóng chỉ ra phạm vi giữa trung bình cuốn tối thiểu và tối đa trên tất cả các lần chạy. DQN và Rainbow hiển thị cùng hành vi và được hiển thị trong cùng một biểu đồ.

--- TRANG 4 ---
Được xuất bản như một bài báo hội thảo tại "BRAIN2AI" (ICLR 2021)
Hành vi điển hình của các đường cong trung bình cuốn của các lần chạy riêng lẻ là tác nhân không thể học và ở lại 0%, hoặc tác nhân nhảy lên mức của một tác nhân ngẫu nhiên từ 0% lên khoảng 50%. Điều này được phản ánh trong các giá trị tối thiểu và tối đa. Đối với PPO và các phương pháp khám phá, việc tác nhân không thể học phụ thuộc vào seed. Do đó, trung bình trên tất cả các lần chạy phản ánh số lượng seed mà tác nhân không thể học. Ví dụ, PPO đạt trung bình cuốn trung bình 10% trên 10 lần chạy, cho thấy nó thất bại 8 trên 10 lần.

Trong số ba phương pháp không có mô hình được đánh giá, DQN và Rainbow không thể học, trong khi PPO (không có khởi tạo trực giao hoặc khám phá) đạt hiệu suất rất thấp. Các phương pháp không có mô hình gặp khó khăn nhất với nhiệm vụ phần thưởng thưa thớt và không thể học khi phần thưởng không được đạt đến thông qua các hành động ngẫu nhiên. Việc thêm tiền thưởng khám phá cải thiện hiệu suất cho PPO trung bình 20–30%. Ngoài ra, chúng tôi thấy rằng việc sử dụng khởi tạo trọng số trực giao (Saxe et al., 2013) cải thiện đáng kể hiệu suất trên PPO. Điều này là do thực tế này tạo ra hành vi ngẫu nhiên ban đầu đa dạng hơn. Cuối cùng, DreamerV2 đạt 50% sau khoảng 2000 tập, nhưng không bao giờ tăng lên hơn 50%.

4 THẢO LUẬN
Các kết quả cho thấy rằng các phương pháp không có mô hình thất bại nếu không có khởi tạo hoặc phương pháp khám phá cụ thể. Một mặt, đây là một minh chứng về việc các phương pháp RL nhạy cảm như thế nào với các siêu tham số và cách các "thủ thuật" thông minh phải được sử dụng để có được kết quả tốt. Các kết quả tiên tiến thường được đạt được thông qua các chi tiết quan trọng, như khởi tạo thích hợp, chuẩn hóa đầu vào hoặc các kỹ thuật học thích ứng (Rao et al., 2020). Những chi tiết này thường không được nhấn mạnh là quan trọng cho hiệu suất của thuật toán. Kết quả là, người ta có thể tranh luận liệu có một thiết lập mặc định cho các thuật toán này tổng quát hóa tốt cho bất kỳ môi trường nào. Việc điều chỉnh đúng thường cần thiết để có được kết quả tốt.

Mặt khác, các phương pháp sử dụng mô hình, dù để cung cấp tiền thưởng khám phá hay các phương pháp dựa trên mô hình, ít nhất thành công trong việc đạt đến phần thưởng và có được hiệu suất 50%, bất kể khởi tạo. Từ góc độ khoa học thần kinh, người ta có thể chỉ ra các lý thuyết trong đó não chứa một mô hình của thế giới, tức là não duy trì niềm tin về thế giới thông qua một mô hình sinh tạo, như các phương pháp Bayesian đối với chức năng não (Erickson & Smith, 1988), suy luận tích cực (Friston et al., 2006), và não đối kháng tổng quát (Gershman, 2019). Các mô hình này cho phép chúng ta phản ứng với các sự kiện có tính chất ngẫu nhiên và nơi thông tin không hoàn hảo. Có lẽ việc sử dụng mô hình thế giới là quan trọng cho các thuật toán để tổng quát hóa tốt cho bất kỳ môi trường nào.

Một khía cạnh quan trọng của môi trường là phần thưởng thưa thớt. Khi phần thưởng thưa thớt và khó tìm thông qua khám phá ngẫu nhiên, các phương pháp không có mô hình thường thất bại. Trong trường hợp đó, cần một số khuyến khích để khám phá. Khuyến khích này có thể được đưa ra thông qua tạo hình phần thưởng, phần thưởng nội tại từ các phương pháp khám phá hoặc, như đã đề cập trước đó, khởi tạo thông minh. White (1959) lập luận rằng hành vi khám phá bản thân có thể là một nguồn phần thưởng. Ví dụ, lý thuyết về dòng chảy (Csikszentmihalyi, 1990) tuyên bố rằng một nguồn phần thưởng nội tại quan trọng cho con người là sự quan tâm đến các hoạt động đòi hỏi kỹ năng cao hơn một chút so với họ hiện có. Suy luận tích cực (Friston et al., 2006) định khung việc tối thiểu hóa năng lượng tự do (biến phân) như phần thưởng nội tại chi phối các hành động của sinh vật sống. Việc bao gồm một phần thưởng nội tại, ngoài phần thưởng ngoại tại, có thể là một sự cần thiết.

Một khía cạnh quan trọng khác của môi trường là khoảng thời gian tương đối dài giữa việc nhìn thấy phần thưởng qua cửa sổ và việc có được phần thưởng. Để giải quyết tối ưu mê cung E, tác nhân trước tiên sẽ cần đi về phía cửa sổ để tìm hiểu phần thưởng ở đâu, và sau đó, nhớ vị trí khi nó chọn một nhánh. Rõ ràng, tác nhân sẽ cần một loại trí nhớ nào đó để đạt được điều này. Theo một nghĩa nào đó, các phương pháp dựa trên mô hình chứa một loại trí nhớ ngầm được mã hóa trong trạng thái, mặc dù chúng cũng có thể sử dụng trí nhớ rõ ràng, như bộ nhớ ngắn hạn dài (LSTM), đơn vị tái hiện có cổng (GRU), mạng bộ nhớ (Weston et al., 2014) hoặc máy Turing thần kinh (Graves et al., 2014). Tuy nhiên, như được hiển thị trong phần trước, phương pháp dựa trên mô hình DreamerV2 không hoạt động tốt hơn một tác nhân chọn ngẫu nhiên một nhánh mà không tìm kiếm manh mối. Điều này có thể gợi ý sự cần thiết phải nhìn vào các loại trí nhớ có căn cứ trong khoa học thần kinh, như mạng Hopfield (Little, 1974) hoặc máy Kanerva (Wu et al., 2018).

--- TRANG 5 ---
Được xuất bản như một bài báo hội thảo tại "BRAIN2AI" (ICLR 2021)
Học tăng cường cần nhìn vào khoa học thần kinh để tìm cảm hứng một lần nữa. Các chuẩn mực hiện tại quá hạn chế về các loại vấn đề mà chúng giải quyết. Chúng tôi đã giới thiệu mê cung E, không thể được giải quyết bởi các thuật toán RL sâu tiên tiến. Cuối cùng, chúng tôi lập luận cách các hiểu biết từ khoa học thần kinh có thể hỗ trợ trong việc giải quyết mê cung E. Việc sử dụng các mô hình thế giới, phần thưởng nội tại và trí nhớ, bắt nguồn từ khoa học thần kinh, sẽ có lợi trong vấn đề này.

LỜI CẢM ƠN
Nghiên cứu này nhận được tài trợ từ Chính phủ Flanders theo chương trình "Onderzoeksprogramma Artiﬁci ¨ele Intelligentie (AI) Vlaanderen". Ozan C ¸ atal được tài trợ bởi học bổng Tiến sĩ của Quỹ Nghiên cứu Flanders (FWO).

TÀI LIỆU THAM KHẢO
Adri `a Puigdom `enech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi,
Daniel Guo, and Charles Blundell. Agent57: Outperforming the atari human benchmark, 2020.
Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K ¨uttler,
Andrew Lefrancq, Simon Green, V ´ıctor Vald ´es, Amir Sadik, Julian Schrittwieser, Keith Ander-
son, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis
Hassabis, Shane Legg, and Stig Petersen. Deepmind lab. CoRR , abs/1612.03801, 2016. URL
http://arxiv.org/abs/1612.03801 .
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning envi-
ronment: An evaluation platform for general agents. J. Artif. Int. Res. , 47(1):253–279, May 2013.
ISSN 1076-9757.
Marc G. Bellemare, Will Dabney, and R ´emi Munos. A distributional perspective on reinforcement
learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Con-
ference on Machine Learning , volume 70 of Proceedings of Machine Learning Research , pp.
449–458, International Convention Centre, Sydney, Australia, 06–11 Aug 2017. PMLR.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Christopher Hesse, Rafal J ´ozefowicz,
Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique Pond ´e de Oliveira Pinto,
Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya
Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement
learning. CoRR , abs/1912.06680, 2019. URL http://arxiv.org/abs/1912.06680 .
Benjamin Beyret, Jos ´e Hern ´andez-Orallo, Lucy Cheke, Marta Halina, Murray Shanahan, and
Matthew Crosby. The animal-ai environment: Training and testing animal-like artiﬁcial cog-
nition. CoRR , abs/1909.07483, 2019. URL http://arxiv.org/abs/1909.07483 .
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. CoRR , abs/1606.01540, 2016. URL http://arxiv.org/
abs/1606.01540 .
Yuri Burda, Harrison Edwards, Deepak Pathak, Amos J. Storkey, Trevor Darrell, and Alexei A.
Efros. Large-scale study of curiosity-driven learning. In 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019 . OpenReview.net, 2019a.
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network
distillation. In International Conference on Learning Representations , 2019b.
Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare.
Dopamine: A research framework for deep reinforcement learning, 2018.
Mihaly Csikszentmihalyi. Flow: the Psychology of Optimal Experience by Mihaly Csikszentmihalyi .
Harper & Row, 1990. ISBN 9780060162535.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https:
//github.com/openai/baselines , 2017.

--- TRANG 6 ---
Được xuất bản như một bài báo hội thảo tại "BRAIN2AI" (ICLR 2021)
G. Erickson and C.R. Smith. Maximum-Entropy and Bayesian Methods in Science and Engi-
neering: Foundations . Fundamental Theories of Physics. Springer Netherlands, 1988. ISBN
9789027727930.
Karl Friston, James Kilner, and Lee Harrison. A free energy principle for the brain. Journal of
Physiology-Paris , 100(1):70–87, 2006. ISSN 0928-4257. doi: 10.1016/j.jphysparis.2006.10.001.
Theoretical and Computational Neuroscience: Understanding Brain Functions.
Karl Friston, Thomas FitzGerald, Francesco Rigoli, Philipp Schwartenbeck, John O'Doherty, and
Giovanni Pezzulo. Active inference and learning. Neuroscience & Biobehavioral Reviews , 68:
862–879, Sep 2016. ISSN 01497634. doi: 10.1016/j.neubiorev.2016.06.022.
Samuel J. Gershman. The generative adversarial brain. Frontiers in Artiﬁcial Intelligence , 2:18,
2019. ISSN 2624-8212. doi: 10.3389/frai.2019.00018.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. arXiv e-prints , art.
arXiv:1410.5401, October 2014.
Danijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learn-
ing behaviors by latent imagination. In 8th International Conference on Learning Representa-
tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net, 2020.
Danijar Hafner, Timothy P Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with
discrete world models. In International Conference on Learning Representations , 2021.
Demis Hassabis, Dharshan Kumaran, Christopher Summerﬁeld, and Matthew Botvinick.
Neuroscience-inspired artiﬁcial intelligence. Neuron , 95(2):245–258, 2017. ISSN 0896-6273.
doi: https://doi.org/10.1016/j.neuron.2017.06.011. URL https://www.sciencedirect.
com/science/article/pii/S0896627317305093 .
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 770–778, 2016. doi:
10.1109/CVPR.2016.90.
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In AAAI Conference on Artiﬁcial Intelligence , 2018.
Asifullah Khan, Anabia Sohail, Umme Zahoora, and Aqsa Saeed Qureshi. A survey of the recent
architectures of deep convolutional neural networks. Artiﬁcial Intelligence Review , 53(8):5455–
5516, Dec 2020. ISSN 1573-7462. doi: 10.1007/s10462-020-09825-6.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton.. imagenet classiﬁcation with deep con-
volutional neural networks. Commun. ACM , 60(6):84–90, May 2017. ISSN 0001-0782. doi:
10.1145/3065386.
W.A. Little. The existence of persistent states in the brain. Mathematical Biosciences , 19(1):101–
120, 1974. ISSN 0025-5564. doi: 10.1016/0025-5564(74)90031-5.
V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature , 518(7540):529–533, 2015.
V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd
International Conference on Machine Learning , volume 48 of Proceedings of Machine Learning
Research , pp. 1928–1937, New York, New York, USA, 20–22 Jun 2016. PMLR.
J. O'Keefe and J. Dostrovsky. The hippocampus as a spatial map. preliminary evidence from unit
activity in the freely-moving rat. Brain Research , 34(1):171–175, 1971. ISSN 0006-8993. doi:
10.1016/0006-8993(71)90358-1.

--- TRANG 7 ---
Được xuất bản như một bài báo hội thảo tại "BRAIN2AI" (ICLR 2021)
Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning
Research , pp. 2778–2787, International Convention Centre, Sydney, Australia, 06–11 Aug 2017.
PMLR.
Nirnai Rao, Elie Aljalbout, Axel Sauer, and Sami Haddadin. How to Make Deep RL Work in
Practice. arXiv e-prints , art. arXiv:2010.13083, October 2020.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv e-prints , art. arXiv:1312.6120, December
2013.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay, 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR , abs/1707.06347, 2017.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with
deep neural networks and tree search. Nature , 529(7587):484–489, jan 2016. ISSN 0028-0836.
doi: 10.1038/nature16961.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . 2018.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David
Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy P. Lillicrap, and Mar-
tin A. Riedmiller. Deepmind control suite. CoRR , abs/1801.00690, 2018. URL http:
//arxiv.org/abs/1801.00690 .
E. C. Tolman and C. H. Honzik. Introduction and removal of reward, and maze performance in rats.
University of California Publications in Psychology , 4:257–275, 1930.
Jason Weston, Sumit Chopra, and Antoine Bordes. Memory Networks. arXiv e-prints , art.
arXiv:1410.3916, October 2014.
R. W. White. Motivation reconsidered: the concept of competence. Psychological review , 66:
297–333, 1959.
Yan Wu, Greg Wayne, Alex Graves, and Timothy Lillicrap. The kanerva machine: A genera-
tive distributed memory. In International Conference on Learning Representations , 2018. URL
https://openreview.net/forum?id=S1HlA-ZAZ .

--- TRANG 8 ---
Được xuất bản như một bài báo hội thảo tại "BRAIN2AI" (ICLR 2021)
A MÔ TẢ THUẬT TOÁN
Các thuật toán RL tiên tiến sau đây được sử dụng để đánh giá hiệu suất trong môi trường của chúng tôi.

DQN Mạng Q Sâu (DQN; Mnih et al. (2015)) là một phương pháp tối ưu hóa dựa trên giá trị. Dữ liệu thu thập từ môi trường được sử dụng để đào tạo một mạng thần kinh dự đoán hàm giá trị hành động Q, cung cấp ước tính của returns mong đợi cho tất cả các hành động có thể. Chính sách được triển khai như một quá trình lựa chọn hành động có giá trị Q cao nhất tại mỗi bước thời gian. Đối với DQN, một số khám phá được cung cấp thông qua chiến lược ε-greedy, bao gồm việc chọn một hành động ngẫu nhiên thay vì hành động tốt nhất với xác suất ε.

Rainbow Một số cải tiến so với triển khai DQN ban đầu đã được kết hợp trong (Hessel et al., 2018), dẫn đến một hệ thống toàn diện được gọi là 'Rainbow'. Ba thành phần quan trọng nhất của Rainbow là: sử dụng returns n-bước (Mnih et al., 2016), sử dụng replay kinh nghiệm ưu tiên (Schaul et al., 2016) và mô hình hóa hàm Q như một phân phối (Bellemare et al., 2017).

Cả triển khai DQN và Rainbow của chúng tôi đều dựa vào framework RL Dopamine (Castro et al., 2018), cung cấp kết quả tiên tiến trên chuẩn mực Atari 2600.

PPO Thuật toán Tối ưu hóa Chính sách Gần đúng (PPO; Schulman et al. (2017)) là một phương pháp policy-gradient cho RL sâu. Các phương pháp policy-gradient khai thác định lý Policy Gradient (Sutton & Barto, 2018) để cập nhật trực tiếp chính sách dựa trên returns môi trường. So với Vanilla Policy Gradient (VPG hoặc REINFORCE), PPO triển khai hai khía cạnh bổ sung: ước tính hàm giá trị, được trừ khỏi returns của môi trường để giảm phương sai, và một hàm mục tiêu được cắt, ngăn chặn các cập nhật mạnh mẽ không mong muốn của chính sách.

Triển khai thuật toán PPO của chúng tôi dựa trên triển khai OpenAI Baselines ban đầu (Dhariwal et al., 2017).

Dreamer Dreamer (Hafner et al., 2020) là một phương pháp RL dựa trên mô hình dựa trên hai nguyên tắc chính: (i) học một mô hình 'thế giới' của môi trường, cho phép dự đoán các quan sát và phần thưởng tương lai, (ii) sử dụng mô hình thế giới để học một chính sách tối ưu theo cách RL, bằng cách áp dụng các cập nhật policy-gradient trên các quỹ đạo tưởng tượng được tạo ra bởi mô hình thế giới của Dreamer.

Trong các thực nghiệm của chúng tôi, chúng tôi dựa trên lần lặp thứ hai của tác nhân Dreamer (Hafner et al., 2021), đã được thử nghiệm để vượt qua (Hessel et al., 2018) và các phương pháp không có mô hình tiên tiến khác trên chuẩn mực Atari 2600.

Phương pháp khám phá Những tiến bộ gần đây trong các chiến lược khám phá cho RL đã cho thấy những cải thiện hiệu suất đáng kể trong các nhiệm vụ phần thưởng thưa thớt. Trong phần sau, chúng tôi trình bày hai phương pháp mà chúng tôi đã thử nghiệm trong thiết lập thực nghiệm của mình:

•ICM - Mô-đun Tò mò Nội tại (Pathak et al., 2017) bao gồm hai mạng cho phép tính toán tiền thưởng tò mò để khám phá. Mạng đầu tiên là một mô hình động lực học nghịch đảo, được sử dụng để học các đặc trưng compact để giảm chiều của các đầu vào quan sát. Mạng thứ hai là một động lực học đặc trưng tiến, dự đoán các đặc trưng của các quan sát tương lai bằng cách sử dụng các đặc trưng của quan sát hiện tại và hành động hiện tại. Tiền thưởng tò mò được triển khai như lỗi dự đoán của mô hình đặc trưng tiến, được tính như Lỗi Bình phương Trung bình (MSE) giữa dự đoán và đặc trưng thực, và khuyến khích tác nhân khám phá các chuyển đổi môi trường khác nhau.

•RND - Trong Chưng cất Mạng Ngẫu nhiên (Burda et al., 2019b), họ dựa trên công trình trước đó cho thấy rằng các đặc trưng ngẫu nhiên có thể hoạt động tốt để giảm chiều của quan sát (Burda et al., 2019a). Như một tiền thưởng tò mò, họ sử dụng lỗi dự đoán MSE giữa các đặc trưng được tính bởi một mạng được khởi tạo ngẫu nhiên và một bộ mã hóa đặc trưng mà họ đào tạo, khuyến khích tính đa dạng trong các quan sát môi trường.

Trong các thực nghiệm của chúng tôi, chúng tôi thêm các tiền thưởng nội tại được tính bởi ICM và RND vào phần thưởng, và đào tạo một tác nhân PPO để tối đa hóa returns tổng hợp.

--- TRANG 9 ---
Được xuất bản như một bài báo hội thảo tại "BRAIN2AI" (ICLR 2021)
B CHI TIẾT THỰC NGHIỆM
Vì môi trường giống như trò chơi, quyết định được đưa ra để triển khai tất cả các thuật toán với các tham số ban đầu được sử dụng cho chuẩn mực Atari.

Các thuật toán được chuẩn hóa với Atari 2600 thường sử dụng hình ảnh được tiền xử lý làm đầu vào. Cụ thể hơn, các quan sát từ các trò chơi Atari được thu nhỏ lại thành 84x84 pixel và chuyển đổi thành thang xám. Vì lý do này, hình ảnh từ môi trường của chúng tôi được tiền xử lý theo cùng cách.

Môi trường được khởi tạo theo cách sau. Tác nhân được cho FOV là 1,1 radian (198 độ) và số lượng nước đi tối đa là 250. Số này được chọn để kiểm soát tính thưa thớt. Quá nhiều nước đi có thể làm cho môi trường quá dễ giải quyết, vì vậy bất kỳ tác nhân ngẫu nhiên nào cũng có thể có được phần thưởng 50% thời gian. Quá ít nước đi có thể làm cho môi trường quá khó giải quyết. Vì số lượng nước đi tối thiểu cần thiết để giải quyết môi trường là khoảng 50, số này được đặt thành 250.

Các siêu tham số quan trọng có thể thay đổi giữa các triển khai trong tài liệu được hiển thị trong Bảng 1. Các tham số được điều chỉnh cho môi trường của chúng tôi được nhấn mạnh. Các phương pháp khám phá được thử nghiệm kết hợp với PPO. Các thuật toán khám phá được tắt sau khi tác nhân đạt hiệu suất trung bình 20%. Ngoài phương pháp khởi tạo PPO mặc định, chúng tôi đã chạy một số thực nghiệm PPO bằng cách khởi tạo mạng chính sách sử dụng trọng số trực giao lớn.

Bảng 1: Các siêu tham số quan trọng trong triển khai thuật toán. Một số siêu tham số có thể thay đổi giữa triển khai được tìm thấy trong tài liệu. Bảng này chỉ ra các tham số chính xác được sử dụng. Văn bản được nhấn mạnh chỉ ra các biến đã được chúng tôi điều chỉnh.

DQN
optimizer RMSProp
learning rate 2.5e-4
steps per batch 2500
min. replay history 20000
batch size 32

Rainbow
optimizer Adam
learning rate 6.25e-5
steps per batch 2500
min. replay history 20000
batch size 32

PPO
optimizer Adam
learning rate 2.5e-4
gradient clip 0.1
value loss coefficient 0.5
no. parallel agents 8
steps per batch 250 (x8)
no. mini batches 8
entropy coefficient 0.001

Exploration methods – PPO
exploration off threshold 0.2
