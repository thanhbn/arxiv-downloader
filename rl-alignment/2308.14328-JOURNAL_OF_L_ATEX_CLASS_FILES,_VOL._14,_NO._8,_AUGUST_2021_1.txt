# 2308.14328.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2308.14328.pdf
# File size: 1496907 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1
Reinforcement Learning for Generative AI: A
Survey
Yuanjiang Cao, Quan Z. Sheng, Member, IEEE , Julian McAuley, Lina Yao, Senior Member, IEEE ,
Abstract —Deep Generative AI has been an essential topic in the
machine learning community for a long time, and it can impact
a number of application areas like text generation and computer
vision. The major paradigm for training a generative model is
maximum likelihood estimation. This formulation successfully
establishes the objective of generative tasks, while it cannot
satisfy all the requirements that a user might expect from a
generative model. Reinforcement learning has demonstrated its
power and flexibility to inject new training signals such as
human inductive bias to build a performant model. Thereby,
reinforcement learning has become a trending research field
and has stretched the limits of generative AI in both model
design and application. It is reasonable to summarize advances in
recent years with a comprehensive review. Although there have
been surveys in different application areas recently, this survey
aims to shed light on a high-level review that spans a range
of application areas. We provide a rigorous taxonomy and make
sufficient coverage on various models and applications, including
the fast-developing large language model area. We conclude this
survey by showing the potential directions that might tackle the
limit of current models and expand the frontiers for generative
AI.
Index Terms —reinforcement learning, generative models,
LLMs,
I. I NTRODUCTION
Recent years have witnessed tremendous progress in gen-
erative AI, like variational autoencoders [1], autoregressive
models [2], adversarial generative nets [3], diffusion models
[4], energy-based models [5] and normalizing flows [6]. The
advancement of these models has brought the development of a
broad range of applications, from neural language processing
to image generation and scientific research. Particularly, the
emergence of Large Language Models (LLM), like ChatGPT
[7], has changed the paradigm of industry and academia
regarding how to develop the next generation of machine
learning systems to bridge the gap toward general AI further.
Another fast-developing area is Diffusion models [4], which
is the foundation for large models that generate high quality
images, videos [8], and medical image analysis [9]. whose
training requires large amounts of computing resources for
performant image generation. Generative models also power
scientific research in molecular design and optimization. Al-
phaFold [10] shows that protein structure can be effectively
modeled by machine learning systems [11], [12].
Yuanjiang Cao and Quan Z. Sheng (Michael Sheng) are with Department
of Computing, Macquarie University, Sydney, NSW, AUS. Email: yuan-
jiang.cao@mq.edu.au, michael.sheng@mq.edu.au
Julian McAuley is with University of California San Diego, California,
USA. Email:jmcauley@eng.ucsd.edu
Lina Yao is with CSIRO’s Data61 and University of New South Wales.
Email: lina.yao@data61.csiro.auTraining generative model is one of the cornerstones of gen-
erative AI research, which studies to design objective functions
to guide the learning process. The major objective of gener-
ative models is Maximum Likelihood Estimation (MLE), in
other words, decreasing the Kullback-Leibler(KL) divergence
between generated distribution and target data distribution.
However, in some cases, human want more than what the MLE
can provide. Taking conditional text generation as an example,
for a text generator, we hope not only that it achieves good
performance on texts that exist in the training dataset, but also
that it can output texts that satisfy other desired properties
like diversity, coherence, human-like, and moral considera-
tions. The discrepancy between evaluation metrics and training
objectives can decrease the quality of generated outputs. These
desired properties for a text generator expose that the gap
between distribution fitting and desired properties makes the
maximum likelihood objective insufficient. The generalization
of models connects to the limit of the Negative Log Likelihood
(NLL) objective as well. In some applications of generative AI,
we hope the model can cope with out-of-distribution inputs or
explores out-of-distribution. For example, in novel molecule
design, the goal of the learning process is to explore and
generate unseen molecules instead of those in the dataset. A
code summarizer or generator is expected to produce well-
designed code for novel tasks instead of those in the dataset.
To address the aforementioned limits, the reinforcement
learning has been proposed as a useful optional training
paradigm to improve the performance of generation models.
Reinforcement learning is a training paradigm designed for
learning from interaction [13]. It has flexible objectives in
terms of the reward function, in contrast to the distribution
modeling objective of supervised learning and unsupervised
learning. Furthermore, many generation problems can be re-
defined as decision-making problems, creating the utility of
RL methods on generation problems.
Why this survey There are numerous existing surveys and
reviews of the application of reinforcement learning scattered
in various models [14] and application areas, including neural
language processing [15]–[18], large language model [19],
code generation [20], speech processing [21], [22], computer
vision [23], [24], neural architecture search [25], [26], and
drug discovery [27]–[31]. We have found one paper [32] which
survey the area of RL applying on generative models, while
it only investigates three perspectives in terms of objective
design. In contrast, this work covers topics like using RL
as a sampling method. For application, we explicitly list thearXiv:2308.14328v3  [cs.LG]  24 Feb 2025

--- PAGE 2 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2
Fig. 1: The Overview Structure of This Survey
application area, making readers easier to access topics of their
interests and also inspire new ideas in the model discussion
part. We also cover niche application areas like procedure
generation and graph generation.
Our survey surpasses these previous surveys in terms of
the selection criteria and the comprehensiveness of coverage
in this area. We not only summarize lines of research across
multiple areas but also organize theoretical works that aim
to improve generative models by reinforcement learning tech-
niques. For instance, we cover the recent advancement that the
efficiency of Diffusion Model could be enhanced by shortcut
fine-tuning motivated by one of the reinforcement learning
method: policy gradient, as shown in Section III-B3.
Scope and Paper selection criteria. This study offers a
comprehensive analysis of the potential and obstacles asso-
ciated with reinforcement learning, with a particular focus
on deep reinforcement learning in the realm of generative
AI. The analysis encompasses their intrinsic capabilities, such
as addressing non-differentiable learning issues, infusing gen-
erative AI with innovative training signals, and advances in
sampling and neural architecture search. It also sheds light onthe prevailing challenges, including peaked distribution, the
conundrum of exploration versus exploitation, sparse reward
scenarios, challenges in long-term credit allocation, and is-
sues of generalization. Furthermore, the research delves into
their practical applications across various domains, including
Natural Language Processing (NLP), Computer Vision (CV),
code synthesis, speech decoding, information extraction, rec-
ommendations, robotics, and AI’s role in scientific endeavors.
Emerging trends, such as the intricacies of reward function
formulation, multi-objective optimization strategies, enhancing
and controlling models, modeling human preferences, ensuring
interoperability, and the integration of novel reinforcement
learning techniques with Large Language Models (LLMs) and
foundational models, are also meticulously explored.
In application areas, this survey mainly focuses on se-
quential generation, e.g. conditional text generation and code
generation, but also contains less-mentioned vision tasks such
as 3D point cloud completion. We select papers according to
three selection criteria, impact, venues, and time. For classic
models, we focus on high-quality works that are present in
well-appreciated conferences and journals. For recent papers

--- PAGE 3 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3
TABLE I: Notations in this Survey
Notation Explanation
x, the target random variable that a generative model aims to generate,
x1, .., x i, .., x n a sequence of variables, xirepresents an element of xwhen xrepresents a sequence. This is useful in sequence modelling.
z latent variable
t the time step in a sequence, often serve as an index
st the state at time step tin an MDP
at the action at time step tin an MDP
rt the reward given by the environment at time step tin an MDP
τ a trajectory, a.k.a. a sequence of states, actions, and rewards (s0, a0, s1, r1, ...sn, rn)
Rt Rt=PT
i=tγi−trithe discounted accumulative return over a trajectory
γ , as shown abov, is the discount factor that decreases the impact of future rewards at an exponential rate
π the policy of the RL agent
Vπ(st) the value function of an agent given a state at time step t
Qπ(st, at) the value function of an agent that takes a state and an action as input
p(·), q(·) the probability distribution of a given variable
Ex∼px[·] the expectation of some variable on the distribution of x
DKL(q∥p) the KL Divergence between two distribution pandqon the same variable.
D(·) the discriminator in GAN
G(·) the generator in GAN
that push the boundaries like large language models, we
relaxed the criteria. For some small branches, we list all the
papers we can find.
We propose this survey to systematically review the appli-
cation of reinforcement learning in generative AI, including
models and applications. Our contributions include
•We perform a systematic and thorough examination of
various directions within the field of generative models
and problems using Reinforcement Learning. Recogniz-
ing the multifaceted nature of these areas, we carefully
organize and present an exhaustive review that encom-
passes different methods, structures, and applications.
•We developed a unified taxonomy, meticulously crafted to
organize the extensive and varied literature in generative
studies. This taxonomy not only classifies existing works
according to common themes and methodologies but also
draws attention to potential intersections and divergences
within the field.
•With a specialized focus on Reinforcement Learning
(RL) methodologies within generative AI, our research
provides a detailed exploration of the contemporary chal-
lenges and avenues for development. We dissect the
inherent complexities and hurdles in implementing RL
methods and organize them in a manner that encapsulates
the current state of the art. Furthermore, we identify sev-
eral emerging directions that hold promise for innovative
solutions.
II. P RELIMINARY AND BACKGROUND
In this section, we will provide preliminaries about the
relevant concepts and models, in terms of generative models
and reinforcement learning.
A. Generative Models
The topic of this survey is reinforcement learning applied
in generative AI. Thus it is essential to provide a brief intro-duction to basic generative models, which lays the foundation
for successive chapters. Before we dive into details, we list
the most common notations throughout this survey in Table I.
Generation is a broad research area that scatters in various
application areas. We take three examples for readers to
understand what is generation, then we introduce the formal
definition of key concepts. The first example is a dialogue
system like ChatGPT [7], one of the most popular generative
AI that covers more than 180 million users [33]. If we want
to build a dialogue system, we collect a dataset consists of
pairs of user prompts and human response. The goal is to
train a model that takes a sequence of prompts that consists
of symbols and then return a sequence of symbols as response.
The symbol comprises of alphabet characters, math symbols,
other specific symbols. The second example is text-to-image
generation application like Midjourney [34], where users input
a sequence of symbols and expect to get an image that
satisfying the input. The third example is image-to-image
translation. The user wants to transform a realistic image to
other styles like an impressionistic image. The user needs
to pipe the image and prompt, which is again a sequence
of symbols, into the image generator. From above problem
description, we can see that the goal of generation is typically
requires to pipe the input variable yinto the model, which
could be in the form of a sequential form (y1, y2, ..., y n)and
the model emits a variable x, which could be represented in
the form of a sequence (x1, x2, ..., x m).
Given the input variable and output variable, it is easy
to think that we can directly a probabilistic model p(x|y)
via machine learning methods. Interestingly, this is generally
infeasible for the mismatch between data we have and the
task requirement. For example, we want to build a dialogue
system, however, most text data we can get is acquiring from
webs, these texts are not dialogue data, which means it cannot
be used to learn p(x|y). Instead, we can train a model that
models p(x), the distribution of the data we can get, then we
can adjust the model to the task we need. In the dialogue

--- PAGE 4 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4
example, we can train a model with web dataset, then we can
fine-tune the model based on the dialogue data we collected
from interaction with human. For text-to-image generation,
changing the learning objective from p(x)top(x|y)is also
not difficult. For simplicity, we describes generative models
whose goal is to learn p(x)in the following section.
Given a dataset Xcomposed of a set of samples {xi|1≤
i≥n}, we aim to use a generative model p(ˆx)to generate
a sample ˆxthat follow the true data distribution p(x), where
xis the random variable that represents the true data sample,
nthe number of samples in the dataset. The xcould be a
scalar variable, a vector variable or a sequence of variables
(x1, x2, ..., x n), where the subscript means the ordered index
in the sequence, ximeans one data point of the sequence.
Training and Inference The learning of a model contains
two useful stages: training phase and inference stage. In
training stage, a model is trained. Training a model requires
an objective function or a loss function which reveals the goal
the model optimizes towards. For example, language models
typically use next word prediction as an objective. In this
section, we introduces basic formulation of five generative
models, and we use minimizing NLL as the objective function
without additional description, which is supported by the
maximum likelihood principle [35]. The computation of NLL
entails computing p(x), therefore for all generative models in
this subsection, the formulation is about how to compute p(x).
In inference stage, the model should generate a sample ˆxgiven
a latent variable zthat is sampled from a given distribution
like Gaussian Distribution. The inference stage is how a model
is used to generate samples.
Variational Autoencoder (V AE) [1], [36] learns useful
representations by reconstructing the input xwith a latent
variable zin consideration. Formally, we can decompose the
distribution p(x)by the latent variable:
p(x) =Z
p(x|z)p(z)dz. (1)
lnp(x)≥DKL(q(z|x)|p(z)) +Eq(z|x)lnp(x|z) (2)
But the integral in Equation 1 is intractable in real-world
applications. Therefore, an optional way is to approximate this
conditional distribution by a simpler distribution q(x)with an
evidence lower bound (ELBO) in Equation 2. In inference
stage, V AE could sample following Equation 1, sample a
zfrom Gaussian Distribution, then pipe zinto the decoder
p(x|z)to compute the distribution, then sample a ˆxfrom the
distribution.
Generative Adversarial Networks (GANs) [3] are com-
prised of a discriminator and a generator. The discriminator
is trained to classify where samples come from, real datasets
or generated. The generator aims to trick the discriminator.
Therefore, the two networks form a zero-sum game which
consequently pushes the output distribution of the generator
to approximate the real distribution. Formally, the objective of
GAN can be defined as:
min
Gmax
DEx∼pdata(x)D(x) +Ez∼pg(z)[1−D(G(z))] (3)We can observe that generator could directly generates data
samples by sampling a latent vector zand then pipe the vector
into the generator.
Energy-Based Models (EBM) [37] represents any distri-
bution density with an energy function by
p(x) =e−E(x)
R
x′∼Xe−E(x′)(4)
where E(x)is an energy function that outputs a density of the
a certain sample x. It could be better to understand the energy
function as a compatibility [37] when the energy function
is used to model a joint distribution E(x, y), the energy is
lower when xandyare more compatible. For distribution
p(x), we could treat E(x)as a density. EBMs do not pose
constraint on tractability of the normalizing constant, making
them more flexible [38]. It can be seen from this formulation
that the denominator is an integral over the space X. If we
want to evaluate the probability, we need to evaluate the
energy function all x. If the space is too huge, computing the
probability is intractable. This makes NLL objective infeasible.
Instead, proxy objectives have been developed to optimize the
EBM, a popular one is
∇θL=∇θ−lnpθ(x) (5)
=∇θEθ(x) +∇θlnZ
x′∼Xe−Eθ(x′)(6)
≈ ∇ θEθ(x)−Ex−∼pθ∇θEθ(x−) (7)
where the integral term is approximated by a Markov Chain
Monte Carlo (MCMC) method that samples from the EBM,
forming a contrastive objective [38]. Sampling from EBM
is not trivial as well. Researchers tend to use MCMC for
sampling [38], fortunately, one line of research shows that it
might take a few steps of sampling chain to get an acceptable
sample [39].
Autoregressive Models (AR) decompose the probability
distribution of a variable xinto a sequence of variables by the
chain rule:
p(x) =p(x1, x2, ..., x n) =nY
ip(xi|x1, ..., x i−1) (8)
It can be naturally applied to sequence generation tasks like
text generation and molecule design. It is not so obvious that
AR models could model an image by breaking the image into
a sequence of patches, allowing AR to be used for image
generation. The ordering of generation is fixed and cannot
be changed during training and inference. The computational
complexity of the sampling process is linear in the number of
steps in the generation, and the sequential ordering makes par-
allel computation difficult which leads to less efficiency. The
training objective could minimize the negative log likelihood
ofp(x), and sampling process could generate tokens one by
one, where tokens refer to xiin Equation 8. Although it seems
the sequential generation process prevent parallel computing of
generation and limits the scalability of AR, [40]–[42] etc. have
been proposed to address the scalability and AR has become
one of the most useful generative models.

--- PAGE 5 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5
Normalizing Flows [6], [43], [44] Besides autoregressive
models, another representative generative model that is capable
of direct optimization of NLL objective is Normalizing Flow.
Given a latent variable zof a tractable distribution p(z),
normalizing flow can sample a data point xby transforming z
through an invertible function f, forming x=f(z). Because
thef(·)is an invertible function, the distribution of xcould
be computed by
p(x) =p(z)det∂f(z)
∂z−1
(9)
In Equation 9, normalizing flows use the change of variable
rule of distributions to get the distribution of x. This formula-
tion allows fast sampling and inference by f(z). The constraint
of invertible functions is strong that one step transformation
is generally insufficient when the distribution of xis complex,
which leads to a combination of multiple invertible transfor-
mations:
lnp(xn) = ln p(x1)−X
ilndetfi
xi−1(10)
This formulation requires each transformation xi=fi(xi−1)
to efficiently compute Jacobian determinant as well as to
be expressive and invertible. Capability of tractable sampling
and inference makes training and inference tractable. During
training, normalizing flows can compute p(x)directly by
Equation 9. The sampling process has been explained above.
Diffusion Model is a type of generative model that injects
noise into the data and learn a reverse process to gener-
ate samples. Here we briefly introduce one widely adopted
diffusion model, Denoising Diffusion Probabilistic Models
(DDPM). A DDPM model consists of two Markov chains,
a forward chain q(xK, . . . , x 1|x0) =QK
k=1q(xk|xk−1)that
perturbs the data sample with noises, and a backward chain
pθ(x0, x1, . . . , x K) ==Q1
k=Kq(xk−1|xk)that recover a
sample from the data distribution given a random noise sam-
ple. Note that for simplicity, we slightly abuse the subscription
ofxas a series of perturbing or denoising steps k∈[1, K].
Theθmeans the reverse process is parameterized by a neural
network. The neural network is trained by minimizing the KL
divergence between forward process and reverse process.
DKL(q∥pθ)≥E[−logpθ(x0)] (11)
where the KL divergence is a upper bound of the NLL
objective. Hu et al. links the KL divergence to the following
loss function:
Ek∼[1,K],x0∼q(x0),ϵ∼N(0,I)[λ(k)∥ϵ−ϵθ(xk, k)∥2] (12)
where ϵis the Gaussian noise used in forward computation
andλ(k)is a positive weighting function that is necessary to
link KL divergence to this formulation.
Large Language Model is a language model that uses very
large neural networks as models that typically have billons of
parameters in order to train a generalized model for various
applications. The famous ChatGPT [7] is a chatbot powered by
the large language models. They are typically trained by two
steps, the first step using a next word prediction objective, and
the second step to use various methods to finetune the model
in order to constrain the behaviors of the large models.B. Reinforcement Learning Methods
Reinforcement learning is a computational approach to
automating goal-directed learning and decision-making [13].
In this section, we introduce the Markov Decision Process
(MDP), a formulation that can be widely applied to rein-
forcement learning problems. After formally establishing the
problem, we introduce the categorization of reinforcement
learning methods and the key details of these methods.
1) Markov Decision Process: Markov Decision Process is
a classical formalization of sequential decision making [13]
where actions have impacts on subsequent states and rewards.
The complete formulation model of an MDP contains the
following five elements:
•Sis a set of states of the environment
•Ais a set of actions of the agents
•T:S × A → S is the transition probability distribution
p(st+1|st, at)
•R ⊂Ris the reward function that determines the goal
of the agent
•γ∈[0,1]is the discount factor for cumulative reward
computation
The learning model and decision-maker is the agent, and
the remaining elements outside the agent comprise the environ-
ment. The experience of an agent is a sequence of interactions
of discrete time steps t= 0,1,2,3, .... When an agent interacts
with its environment, it observes a state stat time step t
emitted by the environment, decides the action atbased on
the observation stand responds to the environment. Given the
state of the last time step stand the action at, the environment
transitions to the state of the next time step st+1and sends
an immediate reward rt+1back to the agent. Without further
specification, stcontains sufficient information for the agent
to decide the best action. The agent learns by collecting new
experience data and optimizing a policy πfor action selection.
We consider the finite episodic MDP where the state space
and action space are finite. An episodic MDP has finite length
of sequence. A finite MDP has finite state space and action
space. This is a realistic setting for generation tasks. For
example, in de-novo molecular design, the action space is the
set of all sub-part molecule embeddings, which includes atoms
and bounds between atoms, and the state space is concatena-
tion of actions, forming a string of aotms and bounds. Each
molecule can be represented as a finite character sequence,
therefore it is episodic. The atom space and the space of
bounds between atoms is finite. Another example is image
generation. The common approach is generate RGB pixels that
have discrete and finite spaces. Given the limited size of an
image, the state space and action space of image generation
is finite as well. In an episodic MDP, the environment resets
itself after transitioning Tsteps. The T-step sequence is called
an episode. The goal of an agent is to achieve the highest
cumulative rewards from the environment in an episode. At
each time step t, the agent should select an action that achieves
maximum of the cumulative rewards. The agent maps a state to
an action by a deterministic policy or a probability distribution
π(at|st). Note that the cumulative reward is termed as a return
at time step t,Rt=PT−1−t
k=0γkrk+t+1, which means that

--- PAGE 6 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6
RL Algorithms Related Works
Value-based Q-learning [13], DQN [46], Soft Q-learning [47]
Policy-basedPolicy Gradient/REINFORCE [48],
Actor-Critic [13], TRPO [49], PPO [50], A3C [51]
Hybrid Methods DDPG [52], SAC [53],
TABLE II: Model-free RL algorithms.
the agent should consider future expected rewards instead of
the reward in the current step. γis the discount factor that
decreases the impact of future rewards at an exponential rate.
Then the objective of an agent is formalized as
π∗= argmaxπE[R|π] (13)
One critical assumption of an MDP compared to uncon-
strained RL tasks is the Markov Property, where
p(st+1, rt+1|s0, a0, s1, a1, ..., s t, at) =p(st+1, rt+1|st, at).
It guarantees that state stand action atdetermine the next
state st+1andrt+1. This property is useful in the sense that
the previous state step provides sufficient information, laying
the foundation for the efficiency of the algorithms.
We select research branches that are related to the gen-
eration literature. We go through classic models in the RL
research, model-free RL and model-based RL.
2) Model-free Methods: We first describe Model-free RL.
There are two main approaches: models based on value
functions and models based on policy search. Value functions
are variants of the objective in Equation 13. This objective is
an expectation over a sequence of rewards while the interaction
unfolds step by step. At time step t, the agent’s policy is
to optimize the objective in the current time step Rt. Under
a policy π, the expectation of Rtis the value function
Vπ(st) =Eπ[Rt|st]andQπ(st, at) =Eπ[Rt|st, π(st)].
The value functions have particular recursive properties:
Vπ(st) = Est+1[rt+1+γVπ(st+1)]andQπ(st, at) =
Est+1[rt+1+γQπ(st+1, π(st+1))]which are called the Bell-
man Equation [45] which makes it possible to train a model
through one-step modeling. Bellman Equation could estimate
the value at each time step. Compared to methods that esti-
mates the value at the end of each episode, it could decreases
the variance of the estimation and make it easier for the model
to learn which action has the highest expected return. Bellman
Equation leads to the classic Q-learning that has the following
learning rules:
Qπ(st, at) =Qπ(st, at)+
α[rt+1+γmax
at+1∈AQπ(st+1, at+1)−Qπ(st, at)]
(14)
Classic model-free RL algorithms are listed in the Table II.
DQN [46] extends Q-learning with the neural approxima-
tion of Q values. It devises the experience replay method
where an interaction history is collected, stored, and used
to retrain the parameters of the Q function. This technique
smooths the distribution of transitions, which can stablize
the training process by randomly sampling from memory
instead of correlated episodes sampled from recent states of
environments.Different from value-based methods, Policy gradient or
REINFORCE [48] is a method that directly maximizes the
objective by computing the gradient of the policy:
∇θE[R|π] =Eτ∼πθ[r(τ)· ∇θlogπθ(τ)]
θt+1=θt+αr(τ)· ∇θlogπθ(τ)
r(τ)· ∇θlogπθ(τ) =TX
t=1Rtlogπθ(at|st)(15)
where τis defined as a trajectory variable that includes states
and actions.
REINFORCE with baseline [13], [48] is defined to sub-
tract the value term with a baseline term. The baseline term
can be any function, even a random term that takes the state
as an input:
θi+1=θi+α(Rt−b(st))∇θlogπθ(st) (16)
where irepresents the iteration of model parameter update,
Rtis the expected return term. b(st)is the baseline. This
results in an unbiased estimation of the policy with a reduced
variance [13]. A natural choice for the baseline is the state
value function Vπ(st)that can capture the value fluctuations
of different states.
Actor Critic methods [13] have a similar form as variance-
reduced REINFORCE algorithm does, which is a branch of
policy-based methods. The key difference is that the value
function takes part in the value term of Equation 16, which
can increase the bias of estimation and accelerate learning by
decreasing the variance, which is formulated by
θt+1=θt+α(rt+1+γˆVπ(st+1)−ˆVπ(st))∇θlogπθ(st)
(17)
where the value term uses both a policy model πθ(s)and
an estimated value function ˆVπ(s)for bootstrapping. They
can benefit from both value-based methods and policy-based
method. On the one hand, the selection of actions for value
functions requires the maximum values of all actions. When
there is an infinite number of actions, it can be intractable
to compute all the values. Policy methods can remedy this
by action selection instead of value evaluation in value-
based methods. On the other hand, policy models suffer from
the high variance that can be alleviated by bootstrapping to
accelerate learning.
In actor critic method, another major innovation is the
branch of Trust Region Policy Optimization ( TRPO ) [49] and
Proximal Policy Optimization [50]. TRPO [49] introduces a
trust region for the policy to update itself, in which the policy
improvement is monotonically guaranteed theoretically. The
trust region algorithm is used to compute the gradient of the
following constrained optimization problem:
max
θEs∼pθold,a∼πθoldπθ(a|s)
πθold(a|s)Qθold(s, a)
s.t.Es∼pθold[DKL(πθold(·|s)∥πθ(·|s)]≤δ(18)
where the objective function is the actor critic method that
uses the trajectories of an old policy πθoldand computes the
expected return with an old Q function Qθold. The objective
is constrained by an expectation of KL divergence between

--- PAGE 7 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7
the old policy and the new policy. This problem is estimated
by a quadratic approximation solved by the conjugate gradient
method. PPO [50] uses a clipped surrogate objective to form
a lower bound of the objective of TRPO. Replacing the
complicated quadratic approximation, PPO utilizes only first-
order optimization to solve a constrained problem:
max
θEs∼pθold,a∼πθold[min( ffactored policy (s, a)Qθold(s, a),
clip(ffactored policy (s, a),1−ϵ,1 +ϵ)Qθold(s, a))]
(19)
where ffactored policy (s, a) =πθ(a|s)
πθold(a|s). This formulation
substitutes the KL divergence constraint as a clip operation
to control the divergence between πθoldandπθthat enables
the algorithm to replace the complex optimization process.
Another line of research in actor-critic models is to accel-
erate training through parallel computation. The asynchronous
advantage is actor-critic ( A3C ) [51] is the representative
example. The policy and value parameters are updated asyn-
chronously. The model does not require a replay buffer
because it runs multiple agents in parallel with different
exploration policies that can stabilize training. It achieves good
performance increases compared to DQN, Sarsa, and n-step Q.
Apart from value-based and policy-based methods, hybrid
methods tend to integrate the advantages of both methods to
strike a balance. DPG and DDPG Silver et al. [54] proposed
the Deterministic Policy Gradient (DPG) algorithms. It allows
a more efficient estimation of policy compared to policy
gradient. DPG does not estimate the value Qπby the Monte
Carlo method, it uses an actor-critic method to estimate the Q
value and compute the gradient of actions directly by
∇Jθ=Es∼dπ
∇θπθ(s)∇aQπ(s, a)
a=πθ(s)
(20)
The difference between Monte Carlo based policy gradient and
actor-critic is that the Monte Carlo method requires to estimate
the expected reward of a state by averaging all returns of visits
to a state. This requires to iterate from Tto0, collect returns,
and compute averages. Actor-critic is more easy to compute
by the Bellman Equation. Lillicrap et al. [52] extend DPG
to Deep DPG (DDPG), which combines training techniques
from DQN [46]. DDPG incorporates experience replay and a
soft target that updates the model parameters with a control
variable to slow down the parameter changes.
Soft Q-learning [47] derives an energy-based policy for con-
tinuous states and actions by modeling the policy distribution
as a Boltzmann distribution. The reward function is modified
toRt=PT−1−k
k=0γk(rk+t+1+H(π(·|sk+t+1)))Soft actor-
critic (SAC) [53] integrates the Soft Q-learning into actor-critic
methods and optimizes towards the direction of rewards plus
entropy of policy distributions.
3) Model-based Methods: In RL, models come from prior
knowledge or learning. For example, for an agent playing
Go, the rules of Go are fixed, thus it’s possible to get a
perfect environment model by programming. Influential works
like AlphaGo [55] constructs a Monte-Carlo Tree for forward
prediction. Sometimes it’s not feasible to get a perfect model.In AlphaGo [55], policy learning is integrated with Monte-
Carlo Tree search for the Go game. Generally, Monte-Carlo
Tree search creates a tree whose nodes represent states in
RL. The expansion of the tree is exploring actions and new
states. Decision making is based on values on nodes. AlphaGo
adds a policy network and a value network to the tree. During
inference, nodes are explored and values are obtained along
the tree structure. The parameters of the policy network and
the value network are updated in training by policy gradient
and mean squared error respectively. Actions are selected with
three factors in consideration, a Q-value, a probability and a
number of traversals. The Q-value is computed by mixing a
state value and a reward collected from random fast rollouts.
AlphaZero [56] focuses on self-play to learn a policy from
scratch using an adapted version of AlphaGo models.
Dyna Q-learning [57] employs the model with domain
knowledge as a predictor and data augmentor for the model-
free policy, combining trial-and-error, a domain knowledge
model, planning, and reactive execution into one algorithm.
The dynamics model generates pseudo-experiences that are
incorporated into policy training.
4) Comparison Between Reinforcement Methods: Given the
fact that there are various types of RL methods, it could be
difficult to select from them for a specific generation task.
Generally, model-based RL requires to use a world model or
learn one, in contrast to model-free methods. DQN [46] is
more suitable for discrete actions and policy gradients-based
methods and actor-critic methods could handle continuous
space. TRPO [49] and PPO [50] enhance the stability for
learning. DDPG [52] learns a deterministic policy. We suggest
that researchers should start with basic algorithms like DQN
or policy gradient, then investigate towards more complex
algorithm training method. From the survey we observe that
more works employ policy gradients than DQN. This could
originate the characteristics of policy gradients and DQN.
For example, PPO [50] is popular in fine-tuning a large
language model, while a recent work [58] shows that the
performance of PPO [50] could be matched by well RLOO, a
well designed REINFORCE algorithm [48] by investigating
the components that works for large language model pre-
training. This work argues that the PPO aims to improve
stability of training for environments exploration with high
variance in RL tasks, while it is not suitable for RLHF
fine-tuning which has a relatively stable initial distribution.
It replaces PPO with REINFORCEMENT algorithm without
partial sequence reward learning which accelerates learning
and achieves better performance.
C. Framing Generation tasks as Reinforcement Learning
Problem
We use Figure 2a to show how a generator could be framed
as a reinforcement learning agent in applications. Reinforce-
ment Learning problem is typically defined as a MDP as
shown in Section II-B which is typically a sequential decision
maker. therefore, we use the agent to generate a sequence
x1, x2, x3, ..., x n. At time step t, the previously generated
actions x1, .., x t−1and the task-specific context form the data

--- PAGE 8 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8
(a) Reinforcement learning agent as generator. The
action is integrated into the observation in the next
step.
(b) Non-differentiable setting for generation
models. The blue boxes refer to steps that might
cause non-differentiable condition. Dashed lines
represent the block of gradient and the green
line show that RL uses signal to train the
generator.
Fig. 2: (a) Framing generation tasks as reinforcement learning framework; (b) The reward computation is non-differentiable.
of the environment. For example, when the application is a
visual question answering, x1, ..., x t−1are answer tokens, task
specific context includes the image and the token sequence of
the question.
III. B ENEFITS OF RL- BASED GENERATIVE MODELS
A. Solving the non-differentiable learning problems
One major use of reinforcement learning is that it can prop-
agate gradients through non-differential modules. This extends
the capability of neural networks because it allows the model
to be trained when discrete modules exist in the computation
pipeline. This characteristic is supplementary to supervised
learning and unsupervised learning objectives, which both
require a differentiable training pipeline. In this section, we
introduce two classes of non-differentiable problems.
1) The generated variable is non-differentiable: Discrete
values are prevalent in various generative applications such as
computer vision, neural language processing, and molecule
generation. In language applications and molecular design,
elements of text and molecules are tokenized and embedded
into high-dimensional space in order to capture a better
representation. The tokens are discrete values or one-hot
vectors. In computer vision, a common format of images is the
RGB format which comprises discrete values of three color
channels. Although it is feasible and easy to normalize the
discrete values that are fed into a continuous generative model,
transforming discrete values into continuous values leads to
adverse effects such as weaker robustness [59].
Reinforcement learning is a suitable tool for such problems.
The policy gradient method is a widely adopted approachin the field of machine learning. The formulation lacks any
explicit constraint governing the relationship between the
gradient ∇θπθ(s)and the reward r(τ). The utilization of a
reward signal for policy training has been explored in previous
works such as [60]–[62], circuvmenting the differentiable
requirement of supervised learning. For example, BGAN [60]
aims to tackle the inherent limitation of GANs in handling
discrete data due to the absence of differentiable conditions.
This is achieved by establishing a connection between policy
gradient and the GAN objective. The data distribution is opti-
mized by Monte-Carlo estimation,thereby mitigating variance
in the policy gradient. In language modeling, [61] borrows
the SeqGAN [63] model into a visual dialog system. The
generative agent outputs non-differentiable word sequences.
Hence, policy gradient is leveraged as a means of facilitating
knowledge transfer.
Moreover, the reinforcement learning agent can control the
training instead of being the generator. In this context, the
control values can be discrete. For instance, in InfoNCF [62],
the policy gradient is adopted to optimize the number of
evaluation functions for normalizing flows in the latent space.
2) The training objective is non-differentiable: Apart from
the generative variable, the training objective can be non-
differentiable. Take policy gradient as an example, it allows
directly injecting a non-differentiable objective as reward
without further constraints. Widespread evaluation metrics
for machine translation and text summarization are good
examples. For example, MIXER [64] proposes to address
the exposure bias between the training and testing phase in
sequence generation. Consequently, it uses test metrics, BLEU

--- PAGE 9 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9
Fig. 3: RL can introduce new signals by flexible reward functions
TABLE III: Methods to introduce new training signal
What signals could be incorporated by RL Related Works
Probability from a discriminator [63], [67]–[76],
Task-specific Hand-designed Rules [64], [77]–[80]
Distribution Discrepancy [81]–[89]
Train a reward model with human labelled data [90], [90]–[97]
[65] and ROUGE [66], to directly optimize the model. This
section overlaps with section III-B and we leave other related
discussions later.
B. Introducing new training signal
As described in the last section, reinforcement learning
methods can be employed for non-differentiable problems
that exist in generation applications. In Section III-A1, policy
gradient has no requirements between reward and policy,
in contrast to supervised learning or unsupervised learning,
allowing more flexibility. This flexibility exists in most re-
inforcement learning methods in Section II-B. Thus it is
straightforward to design useful reward functions as additional
training objectives, which can incorporate various training
signals into the generation process, making RL an influential
approach in generation model and application. We demonstrate
four major approaches in this section, as shown in Figure 3.
1) Reward by Discriminator: From the standpoint of train-
ing signal, the discriminator component within the Generative
Adversarial Network (GAN) architecture fulfills a role akin
to that of a reward in the context of reinforcement learning.
SeqGAN [63] first proposes to exploit this similarity by
introducing a GAN to generate the sequence tokens. The
reward signal is derived from the output probability, which
serves as a discriminative measure between real and generated
samples. Subsequent studies have expanded upon or altered
the aforementioned framework through the utilization of a
meticulously crafted discriminator [67], the development of
novel reward formulations [68], the implementation of actor-
critic techniques [69], the incorporation of rank formulations
[70], the utilization of multiple discriminators [71], and other
related modifications.Objective-reinforced GAN [67] extends SeqGAN with
domain-specific objectives to the reward to adapt the generated
samples towards the domain-specific direction. Adversarial
rewards are also employed in GRL [68] that takes the differ-
ence between two probabilities Ex∼pdataD(x)−Ex∼πθD(x).
MaskGAN [69] replaces the baseline in REINFORCE by a
critic, forming an actor-critic algorithm for text generation
instead of a policy gradient in SeqGAN. It uses an in-filling
task to train the agent and uses the probability of real words in
the discriminator as the reward. SAL [71] changes the reward
function with comparison discriminators. The discriminators
take a pair of samples (x1, x2)as input, which is collected
from the current generated sample and previous ones. Three
types of discriminators D(>), D(<), and D(≈)are defined to
describe the relationship between the quality samples. Using
coefficients for a better balance between exploration and
exploitation has been taken into consideration. RankGAN [70]
follows the combination of GAN and RL, which substitutes
the discriminator with a ranker and the reward is provided by
a rank score estimated by the ranker given a sentence and a
reference set and a comparison set. The ranker is trained to
work as a discriminator by increasing the score of sentences
from the dataset and decreasing the score from the generator.
Li et al. [98] introduce adversarial learning into the RL-based
dialogue generation framework, where the reward is defined as
the score produced by a discriminator on the dataset consisting
of a human-generated response or auto-generated response.
This method uses the RL framework to guide the generated
texts by human text distribution. To prevent deteriorating the
model quality, a teacher-forcing method that includes human
supervision in discrimination training is adopted. ColdGAN
[72] explores the exposure bias problem on the GAN-based
model in the text generation tasks. It analyzes the impact of
randomness on discriminators and finds that bad discriminators
might mislead the generators to low-quality areas of the
parameter space. Therefore, it integrates the constraints of
old policies as an importance sampling strategy to balance
the impact of the discriminator. It also proposes a policy
merge method for a cautious generative process. MaliGAN
[73] combines maximum likelihood with gradient descent. It
hypothesizes that the discriminator is easy to learn and can get
optimal results. Under this assumption, the following equation

--- PAGE 10 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10
holds,
Epd[logpθ(x)] =1
Z(θ′)Ep′[r(D) logpθ(x)]
where Z(θ′) =Ep′[r(D)] = 1 . When the discriminator
is not sufficiently trained, the gradient of KL divergence
almost surely is larger than 0. Based on this, an importance
sampling is designed with baseline as rewardrD(x)PrD(x)−b.
MCTS is introduced to cope with high variance. SelfGAN
[74] incorporates MCTS to cooperative decoding to tackle the
instabilities of training the discrete sequential decision-making
problems such as summarization and question answering. Co-
operative decoding means that discriminators are employed in
the decoding phase to rerank the generated sequences as value
functions. As in MCTS, action is selected based on a Q-value,
policy probability, and counts of traversals. Therefore, the
discriminator is integrated into the MCTS decoding procedure
to improve the performance. RL-GAN-Net [75] proposes to
use an RL agent to generate latent code for the GAN model
on point cloud generation. The reward comprises discriminator
loss, point cloud reconstruction loss, and Chamfer distance-
based loss.
TextGAIL [76] incorporates a contrastive discriminator
where the input includes the previous sequence as well as
real data or generated data. The discriminator is required
to evaluate the relative realness between sequences. It also
employs a PPO [50] to decrease the variance of the RL
training.
2) Reward by Hand-designed rules: Novel metrics or
heuristic functions are intended to provide incentives for
training, and as far as we can tell, the majority of RL algorithm
implementations fall within this branch.
Hand-designed rules could lead to non-differentiable objec-
tives [64], [99]–[101]. Like MIXER [64], SCST [99] defines
the reward by the performance of the current model under
the inference algorithm, including CIDEr [102], BLEU4 [65],
ROUGEL [66], and METEOR [103]. TAG [100] incorporates
type auxiliary guiding for code comment generation by rein-
forcement learning. The model takes BLEU [65] and ROUGE
[66] as rewards. ReGen [101] proposes to use reinforcement
learning for non-differentiable evaluations like BLEU [65],
METEOR [103], and chrF++ [104] to guide text generation.
One line of research is incorporating testing-time metrics as
a reward. This line overlaps the branch of non-differentiable
objective when the evaluation metric is non-differentiable,
such as ROUGE [64], [105]–[107] and BLEU [64], [77].
The utilization of testing-time metrics also enhances models
to combat the discrepancy between the training objective
and testing objective. MIXER [64] proposes to address this
problem in sequence generation by reinforcement learning. It
applies REINFORCE to train an agent whose actions are next
words given the current time step context. The reward is the
test metric for neural language processing models. Wan et al.
[77] introduce an actor-critic algorithm to code summarization
by leveraging the exploration feature in reinforcement learn-
ing.
Apart from testing-time metrics, task-specific reward func-
tions are also popular. Li et al. [78] propose to use a policygradient with three considerations in dialogue generation, ease
of answering, information flow, and semantic coherence. The
ease of answering is measured by the negative log-likelihood
−1
NSP
blogp(b|a)where bis a human constructed list of
dull responses that contains sentences like “I don’t know what
you are talking about”, and ais the generated sentence. The
information flow is constructed by penalizing similarity be-
tween two consecutive turns of the same agent. The semantic
coherence is measured by the mutual information between
this turn action and actions from previous turns. PETAL [79]
manages dialog systems by reinforcement learning. It studies
dialog systems in real-world coffee order systems. The reward
contains multiple items, representing personal reward and
general reward. The personal reward reveals the interaction
between the agent and the user, such as accepting or rejecting
the agent’s suggestion. The global reward includes motivation
for getting the user’s information, payment, and shortening
the dialog. In modeling, it breaks down the value function
into two parts, one standing for general value, and one for
personal preference. This achieves a better transfer effect when
a model is tested on new users. Zhao et al. [80] employs
RL to interact with a database in a dialog state tracking and
management task, which is beyond the capability of supervised
learning on this task. The agent is not only asked to respond
to users but also query from a database to better manage the
dialog. Databases are used to generate synthetic data, which
are combined with real data for value function learning and
policy learning, like in Dyna Q-learning [57].
3) Reward by Distribution Discrepancy: Distribution dis-
crepancy can be a useful signal to be integrated into rewards.
Maximizing the divergence leads to more informative gener-
ative data [81], minimizing the divergence between generated
distribution and some distribution could regularize the gener-
ation [82]–[86]. CLARIFYDELPHI [81] uses RL to generate
clarification questions to elicit moral judgment of models. The
question is generated by a PPO network whose reward is
the divergence between two different moral judgments of the
questions. An answer simulation framework is designed to get
the divergence between different answers.
Motivated by the policy gradient, Fan and Lee [82] utilizes
distribution discrepancies to optimize DDPM sampling with
shortcut fine-tuning. The goal is to use gradient-like optimiza-
tion algorithm to explore alternative paths to discover more
efficient paths and boost speed of sampling by replacing the
backward process of DDPM during fine-tuning. The policy
is initialized with a trained DDPM generator, which is then
guided by a generalized critic function that serves as a measure
of discrepancy between the distribution of generated data
and real data, namely a generalized divergence. The gradient
of a DDPM sampler is formulated to a REINFORCE with
baseline mentioned in Section II-B2, The reward is computed
based on the generalized critic function that incorporates 1-
Lipschitz functions as regularization. The critic function is
instantiated as a neural network which is trained to minimize
the reward (maximize the discrepancy between distributions)
and the policy is trained to maximize the reward (minimize
the discrepancy).
When applying reinforcement learning algorithms to gen-

--- PAGE 11 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11
eration models and applications, a line of research combines
reinforcement learning with supervised learning to guarantee
that the model is adjusted by the reward signals but does not
drift away from the supervised training objective to prevent
the model from generating highly rewarded but unrealistic
results. The divergence between a generated distribution and
the distribution defined in the dataset is explored [83]–[86].
KL-control is a technique for non-Markovian systems to
minimize deviation from a prior policy [83], [108]. Sequence
Tutor [83] integrates a KL control method in order to maintain
a policy generation that remains in proximity to the pre-trained
language model. The reward function incorporates previous
knowledge derived from a pre-trained recurrent neural network
(RNN) model.. Ziegler et al. [84] incorporate human prefer-
ence learning into pre-trained language models. It involves
KL-control for coherence and topicality. Jaques et al. [85]
inject KL-control into discrete Q learning to impose an entropy
regularization. GOLD [86] aims to address two problems in
the MLE training paradigm in text generation: diverse but
low-quality samples and exposure bias. Unlike the studies
mentioned above, it adopts an offline reinforcement learning
algorithm. It uses a weighted policy gradient where the weights
come from the training set policy because, in offline reinforce-
ment learning, the agent cannot sample trajectories to estimate
the weight. The weight reveals the conservative method: keep
the actions on the test dataset similar to the training dataset.
For reward, it uses training trajectories to approximate the
probability distribution of human preference. It combines three
kinds of rewards that use a one-zero reward, the product of
MLE probability, and the sum of MLE probability.
4) Reward by data-driven model: With the flexibility of the
reward function, it is also a good way to incorporate models
learned from the reward function. It is feasible to incorporate
various guidance into reinforcement learning by training a
reward model. This branch expresses a similar idea as Inverse
Reinforcement Learning (IRL), which can be integrated in
generative models in two ways.
One is directly embedding previously defined rewards such
as BLEU into a model. Shi et al. [90] conduct experiments in
text generation and empirically prove its effectiveness. They
adopt the maximum entropy IRL to model an approximated
reward function. The training process of the reward model
increases the rewards of real texts and decreases the rewards
of texts that are sampled from the approximated distribution
by a generator with importance sampling. Furthermore, an
entropy term is added to the reward function for the agent
to prevent premature mode collapse and increase the diversity
of generated texts.
The other way is to learn a model for human preference
[109], [110]. This path finally leads to the emergence of
Reinforcement Learning Human Feedback (RLHF) [90]–[95],
which is integrated into the large language model research
and harvest powerful models like ChatGPT [7]. RELIS [109]
proposes to learn a reward function from learning to rank
objectives on a document summarization task. Human pref-
erences of two summaries in the form of ranking are col-
lected and train the reward model by three types of loss:
cross entropy, marginal ranking, and an improved marginalranking. It is theoretically proved that the agent converges
to a near-optimal solution. Nguyen et al. [110] study the
simulated human feedback in the form of ratings in neural
machine translation. They are motivated by the fact that human
feedback is not perfect. For example, expert ratings cannot
perfectly match the goal. There are also granularity, variance,
and skewness problems in the collected ratings. Therefore,
they propose to simulate human feedback and address the
problems mentioned above. They map the feedback to binned
values, use a linear approximation to deal with large variances
in middle ratings and employ a skew perturbation for harsh and
motivational scores. The Open AI Reflection team [91] uses
human preference to guide language models for summarization
tasks. The training consists of three steps. First, trajectories
from a trained policy with various baselines are collected and
these trajectories are evaluated by humans to rank the best
one. Then, they construct a model to learn the rewards that
indicate whether the output is better. Last, they optimize a
policy given the reward model. It is similar to the step in [90]
while it combines datasets from human preferences, which
dramatically outperforms existing methods at that time. The
reward function is trained by the following function,
loss(rθ) =E(x,y0,y1,p)[log(σ(rθ(x, yp)−rθ(x, y1−p)))]
where xis the text before summarization, yis summarized
text, rθis the reward function, ypis the human preferred
text. The loss aims to maximize the distance between two
rewards. Additionally, authors use KL-control [83] to prevent
the mode collapse as well as constrain the policy to be
conservative, not generating weird texts far from the original
supervised pre-trained distribution. InstructGPT [92] follows
the same procedure to fine-tune GPT-3 from human feedback.
Results show that it improves the GPT-3 on truthfulness and
generalization, and decreases the toxicity and performance
regressions. Bai et al. [93] follow the [91] work to test RLHF
on a helpful and harmless dataset. They use a PPO to train the
model and use the same pipeline to learn a preference model
and finetune the language model with reinforcement learning.
It tests the model in an iterative online mode of training
and shows that it improves the performance of the model.
It also identifies a roughly linear trend between the preference
reward and the square root of the KL divergence between the
policy and its initialization. APRIL [94] combines preference
learning with neural TD, an algorithm that replaces the linear
approximation in Linear TD [111] with a neural network.
Preference learning employs the cross-entropy between true
preference and the model used to train a reward model. For
the limits of human feedback collection, a pair-generation
method is proposed to make the process efficient. The pair is
generated on the metric of utility gap, diversity, density, and
uncertainty. Given a human text y′, the utility gap is used to
maximize the gap to get high-quality negative samples. Other
three metrics aim to make the selected sample diverse, located
in a dense part of the distribution, and uncertain. The neural
TD is used instead of DQN because the action space is large
and the maintenance of Q-value is expensive. Kreutzer et al.
[95] discuss the necessity, challenges, and potential solutions
of offline reinforcement learning from human feedback. The

--- PAGE 12 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12
Fig. 4: RL can work as a sampler for models that are hard to sample such as Energy-based Models. The marginal
distribution is difficult to sample for the high cost, RL-based agent provides an alternative way to generate sample sequences.
THe dashed line represents the potential high cost blocks the generation.
necessity of offline RL is that online adjustment of parameters
is too risky and potentially out of control. The challenges
include questionable counterfactual estimation for the lack of
explicit exploration and degeneration problems where low-
reward actions are still encouraged during training. Also,
reliability and learnability are also discussed.
Moreover, the fast development of large language models
inspires researchers to use them as a reward function by proper
prompting. For example, Constitutional AI [96] proposes
RLAIF that trains a harmless but non-evasive AI assistant
that copes with harmful queries via expression of objection
to these queries. Self-critiques and automatic revisions from
an LLM are exploited to modify the dataset and retrain the
LLM on it. Then, an agent is trained by preferences given by
humans and models. Human provides helpfulness evaluations
while the model provides harmlessness evaluations. The label
is generated by an assistant model under the context that
prompts contain human set principles as well as a set of few-
shot examples. Apart from peakiness, Zhu et al. [97] provide
theoretical support for RLHF. They provide a sample complex-
ity for the union problem of RLHF and max-entropy Inverse
Reinforcement Learning. They frame the ranking-based reward
model as a Plackett-Luce (PL) model or a Bradley-Terry-Luce
(BTL) model, providing suboptimality bound for the reward
learning process.
C. Sampling
Although Energy-based Models, one type of mainstream
generative models, enjoy greater expressivity and allow global
constraints, they face challenges in producing samples of
marginal distribution for the unnormalized distributions in the
formulation, which might incur high cost during sampling,
as shown in Figure 4. Reinforcement learning could be an
alternative way to train a sampler for the EBM. A recent line of
research [112]–[116] studies the integration of reinforcement
learning algorithm to distill knowledge in an EBM into an
auto-regressive model by turning distribution matching into
the training signal for the RL algorithm. Distributional Policy
Gradient [112] is a pioneer work that transforms an Energy-TABLE IV: Methods in Sampling and NAS
Methods Related Works
Sampling [112]–[116],
NAS [117]–[127]
Based Model (EBM) training process into a policy gradient
algorithm. Employing reinforcement learning (RL) as a pipe
to train a sampling generator, [112] suggests addressing the
distribution learning problem in Global Autoregressive Models
(GAM), a subset of Energy-Based models. There are two
phases to the training procedure. The MLE criteria are used to
train the autoregressive factor on the dataset in the first step.
The second stage is the focus of [112], where a policy πis
used to approximate a desired distribution pvia the process
of distillation, which facilitates the acquisition of the model
parameters. The policy employs a distribution match as a
reward and is trained using policy gradient via the process
of deductive reasoning based on cross-entropy
∇θCE(p, πθ) =−Ex∼p(·)∇θlogπθ(x)
=−1
ZEx∼πθ(·)P(x)
πθ(x)∇θlogπθ(x)(21)
where the importance sampling is used to form a gradient
descent algorithm with the distribution match computation
as the reward. This step successfully exploits the abundant
expressivity in the GAM into an auto-regressive model, where
the auto-regressive model could serve as a sampler. The policy
gradient method assists the sampling of a generator.
In their seminal work, Khalifa et al. [113] introduce a novel
methodology that harnesses the power of distributional control
in the context of conditional text generation using pre-trained
language models (LLMs). In contrast to the approach proposed
by Parshakova et al. [112], this approach leverages importance
sampling while replacing the sampling distribution with an
optimal distribution. The variable qis subject to updates
exclusively when the condition DKL(p∥πθ)< D KL(p∥q)
holds true.
Korbak et al. [115] propose to discover and exploit similar-
ities between DPG (Distributional Policy Gradient) and policy

--- PAGE 13 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13
gradient. Although the gradient of DPG (Distributional Policy
Gradient) cannot be reduced to a policy gradient, the variance
reduction technique of policy gradients can be transferred to
DPG. Korbak et al. [114] applies KL-adaptive distributional
policy gradient (KL-DPG) [113] on code generation based on
pretraining an auto-regressive model. Go et al. [116] proposes
anf-DPG algorithm that allows using of any f-divergence
as an objective to approximate any target distributions. This
approach unifies the formulation of RLHF and DPG (Dis-
tributional Policy Gradient). The reward of optimization is
defined as the negative gradient of f-divergence between two
distributions f′(πθ(x)
p(x)).
D. Neural Architecture Search
Previous subsections introduce different purposes of re-
inforcement learning, bridging the gap of non-differentiable
learning systems, incorporating new training signals, and serv-
ing as a sampler. Interestingly, the neural network architecture
itself can be viewed as a sequence of tokens, therefore being
the subjective reinforced generator. Uniquely, the agent itself
is a generator but can be applied to almost all feasible tasks
which employ neural networks as learners. In this sense, we
include Neural Architecture Search (NAS) in this survey even
though most applications of NAS are classification tasks.
NAS is used for optimizing neural network architecture
[117]–[127]. Therefore, the reward for NAS is usually the
task metric. For example, when an agent is optimizing the
architecture of a classifier, the accuracy of the classifier is
usually used as reward [117], [119]–[121]. Zoph and Le
[117] proposes to use reinforcement learning to guide neural
architecture design. They employ an RNN network to generate
the architecture description with REINFORCE [48] ENAS
[118] improves the efficiency by parameter sharing. Instead
of building a network from scratch, it constructs the network
on pre-defined convolutional cells. This can reduce the search
space. Similarly, MONAS [119] It removes states in the
training and just considers actions and rewards. It takes power
consumption into reward functions. Various optimization goals
such as mixing, threshold, and surrogate metric are considered
in the reward computation. IRLAS [121] incorporates inverse
reinforcement learning into the NAS. It defines a feature
count that maps an architecture into a trajectory of the agent,
µ=PT
t=1γtϕ(st), where stis the architecture information, γ
is the discount factor, ϕ()is the embedding function. They use
theµto create a linear model for the mirror stimuli function
that aims to use the topology of the expert model such as
ResNet [128] as the guidance.
State space and action space are carefully designed in NAS
in order to make the training tractable. Layer parameters that
are composited as the element of state and action space is
a common choice [122], [125], [129]. MetaQNN [125] con-
strains the space of states and actions to make the generation
tractable. Rijsdijk et al. [129] applies MetaQNN on the NAS
for side-channel-analysis. The reward function is defined con-
sidering the guessing entropy with a different number of attack
traces. BlockQNN [122] employs neural model generation for
image classification tasks. It defines a Network Structure CodeTABLE V: Methods in Neural Language Processing
Sub-areas Related Works
Text Summarization [105], [106], [130]–[136]
Machine Translation [120], [137]–[144]
Dialog System [92], [145]–[157]
Human Value Alignment and Constraints [158]–[160]
Text, Queries, and Knowledge Graph [68], [101], [161], [162]
Large Language Model [58], [91]–[93], [163]–[189]
Other NLP Applications [107], [190]–[195]
(NSC) which quantifies the architecture information such as
layer index, operation type, kernel size, and other related nodes
in the computational graph. The state of E2GAN [123] is the
average value of each sub-module. The action will be how to
extend the architecture,
The sampling efficiency is also explored. Meta-learning
[124], [126] and one-shot learning [127] are two examples.
CATCH [124] employs a meta-reinforcement learning frame-
work to accelerate architecture design on meta-testing tasks.
RL-DARTS [126] uses meta-learning as well. The meta-
optimizer defines the gradient and a control hyperparameter as
the state, the shift of the control hyperparameter as the action,
and the performance on a valid dataset as the reward to meta-
control the direction of the architecture searcher. DQNAS
[127] combines RL-based NAS with one-shot training to get
better performance. The key is using one-shot training to
transfer weights from some layers that are common to quickly
set up the training.
IV. A PPLICATION
Reinforcement learning has been applied to abundant areas.
In this section, we organize and classify the literature accord-
ing to applications, aiming to provide readers a brief introduc-
tion of how RL is applied on different areas. The following
sections include natural language processing, code generation,
computer vision, speech generation, music generation, AI for
science and other small areas.
A. Natural Language Processing
Natural Language Processing (NLP) is one of the largest
application areas of generation models and reinforcement
learning. Reinforcement learning is widely employed in vari-
ous NLP tasks, like text summarization, machine translation,
dialog, etc. We primarily introduce the application directions
but may not cover all directions.
1) Text Summarization: Text summarization is the process
of automatically generating or extracting summaries from a
given input document without losing important information.
RL has been widely applied in this task by generating the
summary [130], [131] or extracting the summary [105], [106],
[132], [133]. Paulus et al. [130] proposes to incorporate self-
critical policy gradient [99] into text summarization. It uses
ROUGE as a reward and trains the NLL objective and RL ob-
jective by a weighted sum operation. Wang et al. [131] change
the model to a convolutional sequence-to-sequence model for
automatically abstractive topic summarization. ROUGE is also
utilized in extractive summarization like Wu and Hu [105],
[132].

--- PAGE 14 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14
RL can be used as a means of hard attention mechanism
which the policy outputs a discrete action to select contents,
unlike in soft attention where a neural network outputs an at-
tention vector that is multiplied with the content. For example,
selecting sentences from a document requires hard attention.
Chen et al. [106] propose to combine RL as hard attention
on the sentence level in the text summarization task. In this
work, an agent is a selector who chooses sentences that are
valuable to do summarization. The state is the set of documents
and the last selected sentence. The action is to choose the
next document sentence for extraction. VTMRL [133] uses
reinforcement learning as a hard attention mechanism to filter
the less topic-coherent and background words in the task of
topic modeling. The reward is defined as a sum of a coherence
score and a topic overlapping value.
Using RL methods for fine-tuning also attracts attention
[134]–[136]. Wu et al. [134] summarize books in a recursive
way. This work uses RLHF as training method. The key
novelty is a book is summarized based on chapters, then these
summaries are fed into the model to do the summarization
again to acquire the final summary of a book. Recursive
summarization could help human to quickly judge the quality
of the summaries. [135] uses policy gradient to adjust a
pre-trained abstractive text summarization model. The reward
comes from two discriminators. One of the discriminators
classify the generated text and human-written text. The other
discriminator constructs a ranking loss to motivate the sum-
marizer produces higher probability on human-written sum-
maries. [136] controls the length of generated texts by PPO-
based fine-tuning on text summarization tasks. The reward
function is rule-based and calculates the difference between
the generated texts and the predefined length limit including
target length, upper bound length, and lower bound length.
2) Machine Translation: Reinforcement learning is applied
to neural machine translation to bridge the train and test metric
gap [120], [137], to direct translation with sentiment preserved
[138], to simplify sentences [139], to balance human efforts
in the interactive system [140], [141], to change the sentiment
[142], to diversify the translation output [143], or to guide
curriculum design [144]. Wu et al. [137] uses BLEU as a
reward in a systematic comparison of decision factors for
RL-based NMT. Pham et al. [120] plus BLEU with a HIT
reward that counts the coverage of translated results and the
annotations into the guidance of training. Kumari et al. [138]
applies RL in sentiment preserved review translation. The
model adopts actor-critic algorithms. The reward is constructed
on a content preservation SBLEU and a sentiment reward
that is a dot product of the sentiment vector and output
probability distribution. DRESS [139] proposes to employ
the policy gradient on the sentence simplification tasks by
introducing three desirarta simplicity, relevance, and fluency
as rewards. BIP-NMT [140] adopts an actor-critic algorithm
for translation model training, which uses a threshold of action
entropy for human feedback acquisition and simulate human
feedback for evaluation. Zhao et al. [141] uses actor-critic. The
reward is BLEU and the negative of the times of request for
human feedback. It regularizes the policy with MLE guidance
of both the right tokens and human feedback. Luo et al.[142] use a similar reward formulation with a harmonic mean
of two rewards to encourage the model to improve both
sentiment reward and content preservation (coherence) reward.
SURF [143] defines a new reward function and explores the
asynchronous training framework. The reward is defined as
wF×eF(ˆy1:t)+wS×eSLSS (ˆy1:t,X)Sentence Fluency and
Sentence-level Semantic Similarity (SLSS). Sentence Fluency
is an average log-likelihood of probabilities given by a pre-
trained model. SLSS is a cosine similarity between embed-
dings of input and generated sentences. [144] uses RL to guide
the curriculum design for the machine translation task. The
action is selecting a batch of samples from the exposed training
dataset. The state is a feature vector that contains the samples
for selection and measurements that reflect the performance
of the machine translation model like sentence-level log-
likelihood. The reward is the performance improvement on
the validation dataset.
3) Dialog System: Dialog systems or conversational agents
are a complicated but fast-developing area in recent years. The
influential InstructGPT [92] is trying to tackle the difficult
open-domain dialog system [145]–[147], [196]. Apart from
it, a task-oriented dialog system is a parallel but important
topic [148]–[151], [153]–[156]. Reinforcement learning has
been explored in both. Also, a hybrid dialog system emerges
recently [157].
MILABOT [145] tackled the Amazon Alexa Prize compe-
tition to learn an open-domain chatbot. RL is used for model
selection. The reward function is a linear regressor trained
from collected ratings. PRG-DM [146] fine-tunes two policies
to generate posts and responses for personalized response gen-
eration by policy gradient. [196] tests the open dialog system
by interacting live with human. The system is trained with
offline Q-learning with KL control to regularize the policy’s
action from generating unrealistic language sequences. Sun et
al. [147] propose an imitation learning approach for complex
dialogue agents. The imitation objective is defined by Donsker
Varadhan’s representation of KL divergence to ease the hard
problem of high dimensional optimization.
HCN [148] explores training a dialog control agent with
reinforcement learning. To avoid degenerated actions, it iter-
atively trains with policy gradient and supervised loss. Lewis
[149] introduce RL on a negotiation task where two agents
both have a set of objects and try to exchange objects to
make each type of object should belong to one agent. The
reward computes whether an agreement is met. Yarats [197]
apply a hierarchical generation framework and substitute the
agent’s state from text tokens to latent variables to improve
the effectiveness of long-term planning in this game. Li et al.
[150] trains a DQN agent to do task completion in neural
dialogue systems. An example is to ask an agent to book
a ticket. Jaques et al. [85] integrates implicit human pref-
erences into a hierarchical open-domain dialogue generation
by reinforcement learning. It employs an off-policy batch
RL approach with dropout-based uncertainty estimates. Co-
Gen [151] proposes to match the latent space of actions
in the external database and natural language response in
conversational search. It uses RL to fine-tune the pre-trained
language model with BLEU as a reward. This fine-tuning helps

--- PAGE 15 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15
the model achieve better results. TrufLL [152] uses RL for
answering questions. Unlike the normal fine-tuning method, it
proposes using a pre-trained language model as a truncation
module that takes the action space of the agent as input to
make the decision-making in a large action space feasible.
[153] applies offline RL (Behaviour Cloning) in dialog agents.
The reward is given by an external program which checks
whether the user goal is satisfied. This work trains on a dataset
that includes the self-generated action candidates which are
evaluated by a critic network to address the sparse and high-
dimensional action space. [154] adopts a hierarchical RL for
the medical dialog system. The reward motivates the agent
to collect more information about the disease and produce a
successful diagnosis. [155] proposes a hierarchical framework
in which a high-level policy determines the semantics at the
utterance level that is in the latent space and a low-level
policy that outputs the next word. The reward is designed to
minimize repetitiveness and toxicity and enhance consistency
and sentiment. [156] incorporates A3C [51] into negotiation
dialog systems. The reward motivates the success of reaching
a compromise and penalizes the result that no deal is struck,
incentivizing both agents to negotiate and achieve a both
preferable result.
READER [157] is designed for mental health counseling
agents by generating dialogue in a hybrid way. The agent
needs to understand various contents from users but provides
effective and appropriate responses.
4) Human Value Alignment and Constraints: The output of
generators is not well matched with human values. Models
sometimes have hallucinations, generating fake information
that they do not understand at all. Sometimes models are
impacted by datasets and spit out sentences that do not match
human values in some cultures. Reinforcement learning can
be used to adjust the model to work better on value matching
[158], impose constraints [159], or even help people to combat
problems like fake news [160]. SENSEI [158] proposes to use
actor-critic to align text generation with human judgments.
The reward is predicted by a binary classifier trained a human-
labeled text data. RCR [159] uses a discriminator to model the
violations and computes a penalty accordingly. This penalty
is added to the reward to regulate the actions of the text
generator. FakeGAN [160] trains a deceptive reviews classifier
with a two-discriminator GAN model. Although it addresses
a classification problem, the method contains generating de-
ceptive reviews, which is a generation subtask.
5) Text, Queries and Knowledge Graph: Natural language
is a good interface for humans but not for search engines and
databases. Therefore, translating natural language into struc-
tured queries and knowledge graphs poses a long-term problem
in NLP. Reinforcement learning is used in these applications
to optimize the query quality [161], create knowledge graphs
[101], complete them [68], and even causal graphs [162]. Mo-
hankumar et al. [161] incorporate human preference rewarded
RL in query rewriting for advertising. The reward is provided
by a fine-tuned model based on another model pre-trained in
100 languages. ReGen [101] proposes to use reinforcement
learning to guide the text and knowledge generation based on
large pre-trained models. GRL [68] integrates GAN and RLin the knowledge graph completion. The model is constructed
on graph neural networks and LSTM. The state space includes
a combination of both entity space and relation space in
the graph. The action is to select the neighboring relational
path to extend the path. CORL [162] proposes to generate a
causal graph by reinforcement learning. The state is defined
as an embedding of a causal graph node, the action is also
in the node space but takes order constraints into account by
imposing a mask to force the parent nodes chosen from a
certain set. The reward is defined as Bayesian Information
Criterion and the episodic reward and dense reward settings
are explored.
6) Large Language Models: Recent years have witnessed a
big rise of large language model [91]–[93]. We collect and or-
ganize advances of RL applications in this direction. Recently,
to further enhance the capability of the large language model,
multi-turn reinforcement learning is proposed [163]–[165],
where the reward is not spontaneous in contrast to single-turn
RL. [163] incorporates self-correction into LLM. To make the
agent revise its action, the action (generated sequence) is piped
into itself to learn a correction step. The reward is derived
from human preferences over two multi-turn conversations. It
adopts a two-stage tuning scheme to model the two attempts
of the inference. The first attempt produces a response, then
revises or corrects the response in the second attempt. The
first stage pushes the second attempt for correction and keeps
the first attempt frozen. The second stage jointly optimizes the
two attempts with reward shaping to penalize falsely changing
the first attempt. [164] addresses the limitation of single-turn
RLHF by introducing a multi-turn algorithm that takes the
history of the multi-turn interactions as states, outputs the
response as actions, and the reward is generated by human
preference. GAE [198] is used to optimize the policy and
KL divergence is leveraged to constrain the optimization near
the original model to prevent policy collapse. [165] transfers
hierarchical RL into mutli-turn langugage model training. The
state space and action space are similar to [164]. The reward
is the success of long-term objective. For example, the agent
is asked to search a book online, the success signal would be
1 if the book is searched. The high-level action is utterances in
response of the state, and the low-level is fine-grained response
of the high level action. The model could be combined with
either online RL algorithms or offline RL algorithms.
New reward formulations are proposed to enhance the
effectiveness [166]–[171]. [166] uses a language model and
a reinforcement learning agent to generate stories towards
specific goals. The language model is responsible for generat-
ing multiple goal-conditioned continuations of a story which
are selected by the knowledge graph-based reinforcement
learning agent. The selected sequence will be appended after
previous contexts for next continuation generation. The reward
caters for goal achievement and coherence reward. [167]
investigates the effectiveness of fine-grained reward for RLHF.
The responses generated by an LLM is segmented into sub-
sentences. Fine-grained reward entails human labeling these
sub-sentences on the exact problems of the LLM response,
such as inveriable facts, irrelevance, or information incom-
pleteness. [168] proposes RLSF which exploits the symbolic

--- PAGE 16 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 16
feedback system to guide the fine-tuning of an LLM. A
certificate of each response the LLM produces is generated
by a symbolic reasoner, which is fed into the reward model
for PPO-based fine-tuning. This work applies the method to
the task that transforms a psuedo-code to C++ code. [169]
proposes a token-level reward model unlike common instance-
level reward model for the LLM tuning. The token-level
capability is acquired by distilling a generative correction
model that generates a probability in deciding a reward for
each token. The reward model is combined with PPO to fine-
tune the LLM. [170] uses token-level feedbacks for LLM
fine-tuning. The reward function provides token-level reward
by computing the difference between probabilities before and
after the word is generated. To increase robustness, this work
also uses a ”First quantize then noise” strategy that uses
quantized rewards and inject noise into rewards but keep
them in the interval. [171] composes an algorithm that can
balance multi-objective during PPO-based fine-tuning. The
reward function is a weighted average of all specific rewards.
The method uses mirror descent and smooth to update the
weights which guarantees that mitigates over-focus on single
objective. This method is used to make the reward composition
fair and stable.
Apart from reward function, new advances in objective
functions [58], [172]–[179], reflection capability [199], [200],
scaling [201], ensemble [202] are proposed to further improve
large language models. Similar to RLOO [58], ReMax [172]
is proposed to leverage a variant of REINFORCE to sub-
stitute PPO [50] in RLHF [91]. [203] proposes to enhance
the reasoning capability of LLM on techniques like Expert
iteration, Return-conditioned RL, and outcome-based reward
modeling. DRO [173] is proposed to exploit single-trajectory
dataset in LLM fine-tuning. Single trajectory dataset means
that one data point consists of a prompt, a response, and
human preference. In contrast, DPO [174] requires each data
point to have two responses for preference learning. DRO
achieves this by changing the objective by a MSE loss between
reward, value function, and the KL divergence regularization
term. [175] fine-tunes an LLM with an advantage-based of-
fline RL on pre-existing dataset. The LLM is trained with
the importance sampling on data collected by a reference
LLM. To increase the robustness of the fine-tuning, this work
only selects data point with a positive reward for training.
[176] fine-tunes LLMs with Inverse RL. This work creates
a link from inverse soft Q-learning to a regularized MLE
objective, which enables to trade-off the impact of regular-
ization that focuses more on long-term impact of reward on
action sequence, increasing the diversity of the action. [177]
explores human preference learning in the pre-training stage
instead of common fine-tuning stage. It tests five objectives
of pretraining, including Reward Weighted Regression (RWR)
and Advantage Weighted Regression (AWR). RWR a variant
of policy gradient method. AWR substitutes the reward with
estimated advantages. [178] proposes to incorporate offline
Q learning into the language generation learning. This work
combines the utility maximization of reinforcement learning
and stability of the supervised learning by Bellman backups of
a value function and a Q function, forming an implicit valuefunction learning. The token is generated based on the history
of all tokens instead of individual transition. The reward
is based on downstream tasks. [179] explores fine-tuning
the LLM by three different offline reinforcement learning
algorithms which can save computing resources compared
to PPO. The first method selects high quality response and
drop others. The second method is similar to policy gradient
with exponentiated rewards. The third method adds reward
in the prompts for better alignment. Reflexion [199] proposes
verbal reinforcement learning which does not train or fine-tune
the LLM but using the in-context learning ability to form a
reinforced loop to achieve the correct answer for a prompt.
The key insight is creating a reflection loop, where an actor
language model gets observations (prompts) from the environ-
ment and reflections from a memory buffer which adds verbal
reflections of actions and outcomes iteratively. The outcome is
generated by an evaluator language model and is transformed
to reflections in verbal form by a self-reflection language
model. The actor language model try multiple times until
the answer is correct or the maximum trial limit is reached.
Similarly, [200] leverages the contemplation ability of a LLM.
This work shows that the LLM is good at self-evaluation of its
own generated texts. Therefore, the self-evaluation is used to
assign rewards for further reinforcement learning fine-tuning.
[201] addresses problems of scaling RLHF to large models up
to 70B (70 billion parameters). This work employs distributed
training strategies entails data parallelism, model sharding,
and pipeline parallelism to efficiently train the large-scale
models. The result shows that PPO is effective but sensitive
to hyper-parameter settings. Offline RL method is easier to
train but achieves sub-optimal performance. [202] proposes to
integrate model ensemble into the reward model of RLHF. This
work discusses three types of reward model ensembles: single
reward model ensemble, linear-layer ensemble, and Low-Rank
Adaptation (LoRA)-based ensemble. The single reward model
ensemble uses multiple reward models directly. In the linear-
layer ensemble, reward models share the same Transformer
model and predicts different rewards on a single linear layer.
LoRA-based ensemble integrates a LoRA layer before the sin-
gle linear layer. This investigation aims to efficiently compute
the reward model for scaling of large language model fine-
tuning.
RL are also used in prompt optimization [180]–[184]. [180]
employs RL for prompt optimization. The state space is an
initial prompt and corresponding task description. The action
space is to select or modify the prompt by generating prompts.
The reward is defined by the downstream tasks and is nor-
malized to stablize the training. [181] utilizes RL for prompt
editing in test-time. This method uses the last hidden states
of the pretrained language model as state representations,
the LLM can choose the objects from instruction, in-context
examplars, and verbalizers. The reward formulation uses the
same one as in [180] but considers difference between edits
to motivates the LLM to accumulates reward at each edition.
Similarly, [182] refines prompt towards truthful, benign, and
helpful outputs of target LLM by reward formulation. This
work employs open-source models and publicly available
dataset to construct reward models for quality, safety, and

--- PAGE 17 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 17
jailbreak prompts. Prompt-OIRL [183] proposes a query-
dependent prompt optimization approach, which is different
from [180], [181] that both search for distributional optimal
prompt (expected quality of answers). [183] seeks a single best
prompt. Besides, this work combines an offline reward model
for inference time evaluation to save the cost of interacting
with LLM for rewards. In [184], reinforcement learning is
utilized to optimize the input space of an LLM. This method
separates the input of an LLM into task description and
individual features. An reinforcement learner is created for
selecting the optimal set of individual features to guide the
LLM towards right responses. The output of the reinforcement
learner is integrated into the LLM for downstream tasks. The
reward is formulated as the action likelihood of the LLM when
the individual feature is in the valid subset.
Safety concerns are discussed in RL fine-tuned LLM [185]–
[189]. [185] demonstrates that the long response bias could
be mitigated by Product-of-Experts, a reward function is
proportional to a product of the human preference reward and
the length bias reward. A bias-only expert is introduced to
capture the length bias reward. The bias-only expert is trained
with perturbations to decrease the impact of semantics. [186]
proposes SafeRLHF to balance helpfulness and harmlessness
in LLM alignment. The reward model is separating helpfulness
and harmlessness by taking harmlessness as a constraint em-
bedded in the Lagrangian optimization. The reward models are
trained separately with different datasets based on preference-
based function and are fused during PPO-based fine-tuning.
The Lagrangian multiplier is updated by a moving average
of harmlessness cost function. [187] demonstrates that RL
fine-tuned LLM still suffers from semantic vulnerabilities
where the radicalized response could be elicited with designed
prompts. [188] shows that LLM might be attacked without
compromising original safety alignment objective. This work
attacks an LLM by data poisoning. Specifically, it flips the
label of the data for RLHF’s reward function but filters out
poison data that could induce significant changes. The attacked
model generates longer response when specified trigger word
appears. [189] attacks the LLM with an RL trained LLM,
where the attacker LLM is trained to search for jail-break
prompts that leads the innocent LLM towards targetted at-
tacked behaviour. This attack process could be used for Trojan
detection by attacking the target LLM and expose the sensitive
prompts.
7) Other Applications in NLP: Reinforcement learning
application in NLP is a wide area but we focus on typically
generating sequences for other purposes like review generation
[190], [191], critique generation [107], mathematical prob-
lems generation [192], keyword-to-sentence generation [193],
paraphrasing [194], or mutiple tasks [195]. Li et al. [190]
combines adversarial training and reinforcement learning for
review generation of commercial purposes. It uses RL in the
same way as SeqGAN [63] do. DP-GAN [191] applies similar
framework like in [63], [78]. It separates the reward into
two levels, word level, and sentence level. The sentence level
reward is an average of all rewards of the word in it. The
total reward is a product of sentence-level reward and the
discounted sum of word-level reward. RL4F [107] proposesTABLE VI: Methods in Code Generation
Sub-areas Related Works
Code Search [204]–[206]
Comment and Annotation Generation [100], [207]
Code Generation [208]–[215]
Unit Test Generation [216]
to enhance the large language model by critique generation.
An critique LLM is used to generate critique for the task
LLM. MWPGen [192] generates mathematical problems from
a math expression and some topic words. It does the problem
generation and then uses the generated problems to get an
answer by a neural network-based solver. It defines the reward
to check the correctness between the expression generated by
the solver and the original expression. Upadhyay et al. [193]
redirect a pre-trained language model towards multiple rewards
to improve performance. It studies unsupervised controlled
text generation and takes text style into consideration. [194]
applies RL fine-tuning in unsupervised paraphrasing tasks.
The method entails three steps, pre-training, transition, RL
tuning. In pretraining, a V AE objective is used. In the transition
phase, the agent learns to generate longer sequence without
supervision. In the RL phase, the agent is trained to maxi-
mize the reward that optimizes the output towards semantic
adequacy, language fluency, and expression diversity. [195]
investigates improving the memory and time efficiency for
RL-based sequence generation. The reward is task-specific,
including BLEU [65] and ROUGE [66]. It improves the
efficiency by reducing redundant sampling and decreasing the
size of computational graphs. This work conduct experiments
on machine translation, text summarization, and RLHF.
B. Code Generation
Given the application of RL on NLP, it is also natural to
consider if the coding process can be automatically executed
by machines. Within them, RL-based models perform well and
improve their performance in multiple directions.
1) Code Search: Code search takes natural language text
as input, and searches for a code snippet that can solve the
problems presented by the text. RL is implemented to use
annotation [204] or enhanced query [205] for code search.
CoaCor [204] applies RL in code annotation, a form of
code generation for code retrieval. They use an actor-critic
algorithm where states are code snippets, actions are generated
code, and rewards are defined according to code retrieval
requirements. QueCos [205] uses the policy gradient method
in the code search application. The agent learns to gener-
ate queries to get high-quality matched code snippets. The
agent is guided by a ranking reward and a BLEU reward.
[206] applies RL-based fine-tuning in query rewriting for E-
commerce applications. The LLM takes a user query and
pattern as input, and rewrites the query to better align with
the product catalog’s terminology. The capability of the LLM
enhances the semantics understanding of the query. The LLM
does Supervised Fine-tuning (SFT) and then uses RL to fine-
tune for better search performance measured by a relevance
reward that measures the relevance between the query and the

--- PAGE 18 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 18
rewrited version, a productive reward measures effectiveness
and an increment reward measures what the rewriting changes
the result.
2) Comment and Annotation Generation: Reinforcement
learning can be used for code summarization as well [100],
[207], which takes code as input and outputs natural language
to summarize the usage of the code. Wang et al. [207] proposes
to use RL to guide code summarization with a hierarchical
attention network. It uses RL to combat the exposure bias
[77] in the code summarization dataset. The reward is BLEU
metric, and the algorithm is actor-critic architecture. TAG
[100] incorporates type auxiliary guiding for code comment
generation by reinforcement learning. It contains two stages
in the decoding process, an operation selection stage, and
a word selection stage. RL is used to guide the operation
selection stage because there is no labelled signal to learn it
directly. Similarly, it uses non-differentiable evaluation metrics
to provide rewards and trains the two stages jointly under the
RL framework.
3) Code Generation: Code generation is different from
code search in the sense that it directly generates the code
instead of searching based on matching. COMPCODER
[208]’s training consists of three stages. In the first stage,
the code generation model is fine-tuned from a language
model. Then, reinforcement learning is used to introduce
the compiler guidance. The last stage uses a discriminator
to learn the compiler feedback on the generated candidates.
The discriminator is trained on whether the code can
be successfully compiled. LearnedSQLGen [209] applies
actor-critic algorithm on SQL generation problem. The state
contains elements of SQL sequence, including reserved words
like Select, From, Where , metadata of tables and attributes,
cell values, operations like =, >, < , and EOF showing the
termination of the sequence. The action space is the same
as the state space. CodeRL [210] incorporates unit tests
into code generation. The code generator is trained with
rewards that are defined based on unit test signals that are
CompileError, RuntimeError, FailedTest, PassedTest .
A critic network is used to predict the probability of four
types of test signals. PPOCoder [211] generates code with
multiple constraints in reward. The reward function is defined
as a sum of test error, syntactic matching score, semantic
matching score, and a KL constraint to prevent RL from
diverging too far. [212] combines a preference-based learning
framework into code optimization. The method integrates unit
test feedback for reward function to improve correctness and
efficiency of LLM fine-tuning. [213] proposes to incorporate
unit test as feedback as well. The reward function comprises
three terms, coarse reward, fine reward, and adaptive reward.
The coarse reward assigns scalar reward according to the
result of the code: pass, failure, syntax error, and other errors.
The fine reward assigns a number according to the specific
error types. The adaptive reward takes the number of passed
test cases into account and stimulates the LLM to pass as
much test cases as possible. [214] uses execution result as
reward as well, the difference is that the reward function
returns high reward only when all private test cases are
passed. [215] proposes a curriculum learning for the codeTABLE VII: Methods in Computer Vision
Sub-areas Related Works
Image Captioning [99], [217]–[223]
Visual Question Answering [224]–[227]
Visual Dialog System [61], [228]
Text-to-Image Generation [96], [229], [230]
3D Generation [231]–[237]
Other Computer Vision Tasks [238]–[242]
generation. The agent is required to generate a small amount
of code snippet given other parts canonical solution. The
curriculum gradually increases difficulty by decreasing the
portion of accessible canonical solution and increasing the
length of generation.
4) Unit Test Generation: [216] improves the quality of unit
test generation by PPO-based fine-tuning. The state consists
of the current code under test and the unit test generated by
the LLM. The later allows the LLM to inspect and refine its
generated results. The action is generating the unit test. The
reward is acquired from a static quality analyzer and is used
to train a reward model that provides rewards for PPO-based
fine-tuning.
C. Computer Vision
Computer vision is another cornerstone of modern machine-
learning research. Generation tasks in computer vision are
also capable of using reinforcement learning algorithms in
many sub-areas, including text generation tasks such as image
captioning, visual question answering, visual dialog, and visual
entity generation like image generation and 3D objects and
scenes generation.
1) Image Captioning: Image captioning is a task where the
model aims to describe related events and entities in an image.
New algorithms are explored, including reward normalization
[99], [220], architecture advancement [217], new reward func-
tion [218], [219], [221]–[223], [243]. SCST [99] incorporates
REINFORCE with a baseline as the training algorithm to
normalize the rewards an agent experiences. It defines the
reward by the performance of a current model under the
inference algorithm. The baseline uses the test dataset to
compute the reward. Ren et al. [217] uses an RL algorithm
whose state comprises an image and up-to-now generated
words. An action is the next word. The reward is defined as
a cosine similarity between the generated captions and the
image. Zhang et al. [218] uses an actor-critic algorithm and a
separation of RNN between the actor-network and the critic
network to do image captioning. TOPIC [219] uses policy
gradient on multi-model product title compression where text
and images are input for title generation. [220] applies RL fine-
tuning in remote sensing image captioning. The RL algorithm
uses [99]. OffPG [243] implements the reward function with
human feedback. The ratings of captions are provided and
incorporated into a policy gradient with baseline to learn a
captioning system in a offline way. Shi et al. [221] improves
the diversity of generated captions by increasing the exposure
of varied caption candidate and a reward function max-CIDEr
that relaxes the similarity constraint to improve diversity. [222]

--- PAGE 19 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 19
proposes a triangle-reward for policy gradient training. Three
rewards measure visual consistency, fluency and correctness,
and semantic coherence respectively. The first and the last
reward is constructed based on graph representations of the
scene. DiscriTune [223] provides reward signal through a pre-
trained fronzen image retriever to a REINFORCE-based image
captioner. The reward is defined as whether the image retriever
gets the original image according to the caption generated by
the captioner.
2) Visual Question Answering: In VQA, topics like reward
design [224], [226], [227] and new architecture [225] are
interesting topics. VQG [224] applies RL on the vision-and-
language problems like the environment GuessWhat [244].
This game comprises three components, a questioner, a guesser
and an oracle. The guesser guesses the targeted object in
an image given the context collected by the questioner. The
oracle contains information about the targeted object. VQG
models the questioner as the agent and proposes to formulate
the reward through three dimensions: goal achieved reward,
progressive reward, and informativeness reward. Zhao et al.
[225] combines multi-model representation learning with a
reinforced GAN-based model for visual question answering.
The representation model contains a pre-trained convolutional
network, a frame-level dynamics network, and a segment-level
attention network. MGN [226] uses REINFORCE algorithm to
fine-tune a graph neural network that takes image and text se-
quence as input. The state is the image and query, the action is
a distribution of tokens that impact a symbolic program which
generates the final answer, and the reward is the final answer
correctness. [227] incorporates the policy gradient method
into a cascade reasoning-based image captioning framework,
allowing the agent to learn through trial and error. The reward
is average normalized Levenshtein similar- ity (ANLS) that
measures the similarity between generated answers and the
ground truths.
3) Visual Dialog System: RL enhances the visual dialog
system by incorporating discriminators [61], [228]. Fan et al.
[61] borrows SeqGAN [63] model into a visual dialog system.
They devise a model that contains two modules, an encoder
to embed images, captions, and questions into the embedding
vector, which is fed into an RL-based decoder as the state.
The decoder is an RL-based GAN. The generator is the agent
that outputs answers as actions. The discriminator learns to
classify the generated answers from real ones in the embedding
space. SCH-GAN [228] learn a cross-modal hashing GAN
with reinforcement learning. Text and image modalities are
considered. The generator tries to retrieve an image from
texts or vice versa. The discriminator aims to distinguish true
examples of the query.
4) Text-to-Image Generation: Recent advancement in im-
age generation is diffusion models, thereby it might be ben-
eficial explore how to combine reinforcement learning with
diffusion models by improvement on images characteristics
that are hard to be described by prompts [229] and online
reinforcement learning methods [230]. The action for the agent
in [229] is the noise vector generated in steps given the last
step output and a context variable. The agent is first pre-
trained on DDPM loss reweighted by exponentiated rewards.The policy gradient algorithms with importance sampling are
incorporated to train the agent. The reward takes file size and
human aesthetic preference acquired from another predictor
into consideration. An extra alignment using a vision language
model is incorporated for RLAIF [96]. [230] proposes to
compare the RL-directed fine-tuning and supervised fine-
tuning in the context of KL divergence as a regularizer. The RL
fine-tuning uses a policy gradient with a KL term to constrain
models on a pre-trained model. The reward in RL fine-tuning
is typically from human preference matching.
5) 3D Generation: Apart from 2D images, 3D generation
is able to introduce reinforcement learning to get a new gen-
eration method [231], better scene generation [232], surface
completion [233], point clouds completion [234], mesh edition
[235], human motion generation [236], [237]. Akizuki et al.
[231] propose to use RL to generate objects in 2D or 3D
space. It models the generation as a link game, where an
agent is required to link from pixel to pixel or from voxel
to voxel. The action space is the direction to extend the
pixel. The reward is based on whether the next pixel or
voxel is in the object. It also successfully learns to generate
Lego structures given fabrication constraints. RLSS [232]
generates 3D indoor scenes with reinforcement learning. The
state contains structures or objects represented by the center
position and bounding boxes as well as the replacement
information. The action is to place an object in a place.
The reward is constructed by programs that include multiple
conditions such as successful condition, count of objects, and
failure conditions. QINet [233] completes the corrupted 3D
point cloud with the actor-critic algorithm. It first generates
masks as pre-processing. Then it converts the discrete point
cloud to the continuous surface. The policy is trained by
rewards defined as IoU between generated cloud and the true
cloud and a latent code constraint to prevent the latent code
drift far away. Zhang et al. [234] complete point cloud with
A3C algorithm. The state is the updated point cloud of each
iteration. The action is the next best view for the completion.
The agent adjusts the camera points in the coordinate system
to change the views. [235] proposes to edit 3D shapes with
two Double DQN agents, a prim-agent that modifies primitives
(e.g., cuboids) and a mesh-agent that changes the vertices
on the mesh. The state is the current configuration of the
3D shape. The action is editing the corner of the primitives
or deleting primitives for the prim-agent, and is changing
groups of vertices. The reward includes the difference between
intersection over union (IoU) of primitives before and after
taking an action. [236] employs PPO to synthesize human
motions in 3D indoor scenes. The state contains configurations
of 3D scene geometry, virtual human body, and intended goals
to interact with. The action is defined in a latent space of pre-
trained motion generation models. The reward enhances goal-
reaching. foot-floor contact, and penetration avoidance. [237]
generates 3D Dance via a transformer-enhanced actor-critic
method. The state encompasses a sequence of dance poses.
The action is the next dance pose. The reward motivates the
agent to align motion-music beats and penalizes inconsistency
movement between upper body and lower body.

--- PAGE 20 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 20
6) Other Computer Vision Tasks: Other tasks like vi-
sual program synthesis [238], dashboard generation [239],
video summarization [240], and vision-language models [241],
[242]. Visual program synthesis is a task that takes a query
as input, the agent learns to write program for visual tasks
by utilizing off-the-shelf computer vision models like image
detection. [238] proposes to address this problem by optimiz-
ing the agent with rewards that are binary variables computed
based on the existing vision-language annotations (not the
target code) to guide the tuning of the language model that
generates code. [239] proposes to employ RL in dashboard
generation that is frequently employed in business intelligence
to help data analysts with data exploration. The state represents
the current configuration of the dashboard, including selected
data fields, chart types, and arrangements. The action is to
add a chart, remove one, or change properties of a chart. The
reward contains three terms: diversity that captures the number
of chart types, parsimony that reveals the number of charts,
and insight that shows the number of correlations revealed in
the chart. Liu et al. [240] propose to use policy gradient in
the video summarization task. The state is the spatio-temporal
features learned from a representation learning network. The
action is selecting frames by binary classification. The reward
is increasing similarity of a selected representative frame, aka
medoid, and other frames in the same cluster and decreasing
similarity between medoids.
Reinforcement Learning could be used in tuning Vision-
Language Model (VLM) [241], [242] as well. [241] proposes
to adapt pre-trained language model for multi-modal tasks
by a RL-based training. This method trains a multi-modal
encoder and fix other parts of the neural network. The encoder
takes outputs of an image from a pre-trained CLIP model
and generates multi-modal representations for text generation.
The reward is formulated based on consine similarity between
CLIP embeddings of the image and the generated text, which
measures the alignment between the input image and the
generated text. The training objective also poses a KL penalty
to keep the RL policy (text generator) close to the original
policy. [242] formulates the VLM as a policy, aiming to
improve its effectiveness the decision making capability via
PPO-based fine-tuning. The state space consists of image
pixels and task descriptions in the text form. The action space
is in text form as well. The reward could be derived from the
success of the task. For example, in the number line task, the
agent is given two poker cards and is required to match the
number of the two cards. The reward could be 1 for success,
-1 for incorrect actions, and 0 otherwise.
D. Speech and Music Generation
Speech and music data can be transformed to sequential
data points. Thereby, it is natural to incorporate reinforcement
learning. In practice,RL is employed to improve the quality
[245], [246], decrease latency [245], control the bit rate in
speech coding [247], and melody generation [248]. Tacotron
2 [245] learns an agent to control the text2speech translation.
The state of the agent is a product of an attentive vector and
hidden vectors, the attentive vectors, and the output sequence.TABLE VIII: Methods in AI For Science
Sub-areas Related Works
Molecule Design [250]–[265]
Reaction Optimization [266]–[270]
Micro-structure Generation [271]
Quantum Architecture Design [272]–[275]
The action is whether to read or speak. When it reads, it
generates an attention vector. When it speaks, it generates the
output in the form of a mel-spectrogram frame. The reward
motivates the agent to produce high-quality translation as well
as low latency. i-ETTS [246] explores reinforced emotional
text-to-speech synthesis. The input of the model is reference
audio and the targeted character sequence. The reference
latent vector encodes tokens that indicate the emotion with
an attention model. Then two modality is fused to decode and
generate an emotional speech that is fed into a speech emotion
recognition classifier. The reward for the speech generator
is recognition accuracy. The agent is trained with a policy
gradient. Gibson and Oh [247] apply RL in speech coding,
an essential technology for digital cellular communications.
The agent is a tree-structured controller for the bit rate in
speech coding. The system uses a reconstruction error as the
penalty. [248] proposes to incorporate LeakGAN [249] into
music melody generation. The music notes are converted to
symbolic representation.
E. AI For Science
Machine learning community also want to help scientists
in other research areas with useful machine-learning tools.
Molecule design is a critical area because the design process is
typically an expensive and long process. Therefore automati-
cally finding patterns from large amounts of data accumulated
in scientific areas become another hot area in recent years. RL
can also play an important role in its flexibility.
1) Molecule Design: Drug discovery or de novo molecule
design can be viewed as policy search in the molecule space.
Thereby it is natural to introduce RL on this application.
Abundant research has been conducted, including various
mainstream methods listed in Section III, like GAN-based
models [276], human prior [277].
For Gan-based models, the direction of architecture design
[250], [276], sample selection [252] are explored. ORGANIC
[276] uses RL directly in the GAN model to discover drugs.
RANC [250] combines ORGANIC style architecture with a
memory network DNC, which enables the model to remember
complex sequences and generate longer sequences. ReLeaSE
proposed by Popova et al. [251] incorporates a prediction
model to bias the generated chemical structures toward those
with the desired physical or biological properties. ATNC [252]
modifies the OGRANITC model by introducing a new sample
selection scheme function that filters out molecules which are
far from training samples and regenerates the sequences until
the number of new sequences is higher than the threshold.
MolGAN [253] uses DDPG on small molecular graph gener-
ation to cope with high dimensional action space. The reward

--- PAGE 21 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 21
is emitted from a differential approximation of the true reward
function.
For human-designed reward methods, various topics are
investigated, like reward design [254], [264], [277], memory
architecture [255], ranking methods [256], training methods
[257]. Inspired by Sequence Tutor [83], REINVENT proposed
by Olivecrona et al. [277] uses RL to tune the MLE pre-trained
RNN on the molecular de-novo generation task. It defines
the reward as the distance between an augmented likelihood
and the agent’s likelihood. The augmented likelihood adds
a reward of the desirable properties of a molecule onto the
log-likelihood of data distribution. [264] computes reward
based on fundamental physical properties such as the energy,
which is approximated by fast quantum-chemical method.
Blacshke et al. [255] augment the REINVENT [277] model
with a memory to cope with the mode collapse problem. The
agent is penalized for generating similar compounds to the
ones in the memory unit. Atance et al. [278] propose the
best agent reminder (BAR) loss for training by motivating
the agent to update the gradient towards the best agent
collected during the training process. It balances between the
best agent loss and the REINVENT loss with a factor. The
reward function considers the average size of the molecules,
drug-like metrics and special molecule DRD2 metrics. AHC
[256] combines the REINVENT loss with samples selected
by the Hill-Climb method [257]. Brown et al. [257] propose
an evaluation framework for de-novo molecular design that
includes two benchmarks, one for an in-distribution learning
test and the other for goal-directed benchmark. [265] generates
3D molecular conformations by pre-training and fine-tuning a
language model, eliminating the need of external software for
graph reconstruction. The state is the current molecular graph
and 3D conformations. The action is generating new parts
of the molecular graph and 3D conformations. The reward
evaluates the binding affinity of target molecular.
Most works above ignore the resolution of molecular design.
Recent works start to combine reinforcement learning meth-
ods to representations of different resolutions, like fragments
[258], tree [259], and population of candidate molecules
[260]. FaST [258] investigates PPO [50] on fragment-based
molecular optimization. The action for the agent is to add or
delete a fragment from the current molecular. The reward is
+1if the novel qualified molecule is discovered and −0.1if
an invalid molecule is explored. RTJ-RL [259] proposes to
apply PPO [50] on a reversible junction tree (RJT) that is
a new representation for molecules. RJT representations are
convertible to valid molecules, which describe the state of the
agent. The action is the modification of the tree, containing
information about node, word, site, and stop. RGA [260]
combines genetic algorithms and reinforcement learning to op-
timize the structure-based drug design. The state is population
at a certain step generation, including the candidate molecules
and their 3D poses. Action space is based on crossover and
mutation, two main steps in the evolution process. The action
is composed of probabilities to choose candidates or ligands
in the population.
For molecular design, it is common to use multiple
constraints or goals to guide the model. Therefore, multi-objective optimization is also an interesting direction , in-
cluding weighted sum [254], [261], alternation [254], [261],
reward weighted sum [261], and Pareto optimization [262],
[263]. The state and action of MoleGuLAR [254] are defined
as sequence generation. The reward is calculated by solute
property measure LogP, drug-likeness metric (QED), and
impact in human factor (TPSA). when conflicts exist between
rewards, they set all rewards as 0 to guide the generation
model towards molecules where the single property is optimal.
Hu et al. [261] use the REINFORCE method for fine-tuning.
They devise a reward-mixing strategy for multiple objective
conflicts. MolSearch [262] proposes to use MCTS on multi-
objective molecular generation and property optimization. The
model maintains a global pool for Pareto molecules which are
defined as molecules that have at least one property at the
best state. DrugEx3 [263] uses a Transformer model for the
generation to allow users to input prior information like the
desired scaffold.
2) Reaction Optimization: It is also feasible to use RL
searching in the formula space for chemical applications.
Zhou et al. [266] propose to use DRL for chemical reaction
optimization. It models a chain of reactions where an agent
has the experimental conditions as states and can change the
conditions by actions such as increasing the temperature. Once
an action is applied, the condition of the reaction changes,
and then the agent is required to further take the following
action. The reward is about the output of the reaction, such as
product yield, selectivity, purity, and cost. RNN as the model
is used to construct the agent. [267] uses a similar formulation
for shell-growth of core-shell semiconductor nanoparticles.
Gottipati et al. [268] integrate forward synthesis into reaction
selection with modified policy gradient method. The state
represents the current molecules generated from a sequence
of commercially accessible reactions. The action is the next
chemical reaction. Reward is designed to measure the desired
properties of generated molecules, such as hydrophilicity and
lipophilicity. [269] applies REINFORCE to control threshold
temperatures and chemical potentials critical for initiating
chemical reactions. The state is current reaction conditions.
The action is modifying the the synthesis parameters like
temperature and gas concentration. The reward promotes the
generation of the target chemical. [270] proposes to incorpo-
rate multi-agent DDPG into the traditional real-time traditional
framework, enabling the optimization to be both economic
and adaptable for reaction optimization. The reward aims to
optimize yield and efficiency and minimize costs.
3) Micro-structure Generation: [271] uses a GAN-based
SAC algorithm to support generation of 3D microstructure
of porous media. The state space contains current design
parameters, current QoI (physical quantities of interests) and
target QoI. The action is a series adjustments of design
parameters. The reward is employed to enhance the alignment
of generated microstructure and a target one.
4) Quantum Architecture Design: [272] propose to do a
quantum architecture search by reinforcement learning. The
state is defined as a multi-qubit entangled state, the action
space is the quantum gate for the design, and the fidelity
of the target state measures the reward. The experiment is

--- PAGE 22 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 22
carried on a simulation environment, which is adapted towards
the OpenAI Gym [279]. [273] designs quantum adiabatic
algorithms by DQN networks. The state is the current progress
of the adiabatic evolution path. The action is changing the
parameters of the numerical operators of energy in the system.
The reward is the success probability of the algorithm. [274]
optimizes Q-learning for quantum circuit architecture. The
goal is to control the energy obtained from a circuit and
reduce the depth of the circuit depth. [275] exploits DQN
to approximate single-qubit unitary operations with sequences
of quantum gates from a finite universal set. The state is
the current composition of quantum circuit. The action is
selecting a gate from a predefined finite universal set. The
reward considers the difference between the target and the
generated sequence for fidelity and the length of the sequence.
F . Recommender System and Information Retrieval
We include most works in this survey about how to generate
content without collaboration with humans. There are also
applications where RL can be used to generate interactions
between two entities, such as a robot and an environment.
For the recommender systems, it is better for readers to read
surveys about how reinforcement learning is injected into the
process [280], [281]. In general, user interaction history is
considered as states and items are defined as actions. The agent
is required to generate items that users might be interested in.
Thereby, the user feedback can be incorporated as rewards to
guide the learning process.
G. Robotics
Robotics is another useful area that deviates from the
generative applications mentioned above. Generally speaking,
robotics can be treated as an interactive agent that generates
responses to humans’ orders. Recent advancement [282] shows
that large language models plus visual foundation models
might lead to a large step towards better robotics control
policies. [283] even proposes a LLM-based agent whose action
is generated code to play MineCraft.
H. Other Areas
Generation models have wide applications. We also list
works that apply RL in niche areas like procedure genera-
tion [284], simulated robotics [285], graph generation [286].
PCGRL [284] proposes an abstract description of formulating
the procedure generation problem into an MDP. The procedure
generation problem is often used for game construction. They
highlight three types of representations for the state and action
modelling. Narrow representation only changes game elements
at predefined locations. Turtle representation provides an agent
with the ability to move around on the map. The broad repre-
sentation enables an agent to change game elements at other
positions. Ha [285] proposes to not only optimize a policy for
a bipedal walker to complete tasks but also optimize the shape
of the walker. It conducts experiments on the OpenAI Gym
[279] BipedalWalkerHardcore-v2 task and learns to generate
an agent that can achieve better performance. Apart fromTABLE IX: Challenges and future directions of
reinforcement learning methods applied in generative model
and applications
Challenges and Future Directions Related Works
Peaked Distribution [288]–[292]
Exploitation-Exploration [55], [293]
Sparse Rewards [143], [294]–[296]
Long-term Credit Assignment [249], [297]
Generalization [298]
knowledge graphs and causal graphs, reinforcement learning
agents can also generate other types of graphs like road graphs
and power grid graphs [286], where the state is the set of
nodes and edges, the action is the selection of a node either
the start node or the end node. The reward is defined for
robustness study expected critical fraction of nodes to the
removal. The reward is estimated by Monte Carlo sampling.
Q-learning is employed for this task. Reinforcement learning is
also used in quantum computer design [272]. Exploratory Data
Analysis (EDA) is another area for RL-based generation. [287]
proposes to generate notebooks during EDA by formulating
the generation process in MDP. The action space consists of
different data operations like filter, group, and backtrack to last
operation. The state space is a vector comprises three steps’
result data including summarization features like the number
of distinct values, the number of null values, the number of
groups, and the groups’ size mean and variance. The reward
is formulated with three terms: interestingness, diversity, and
coherence.
V. C HALLENGES AND FUTURE DIRECTIONS
Despite benefits and wide application of RL-based methods,
challenges also exist, where potential new research directions
emerge, opening new opportunities to further improve current
generative models and applications. In this section, we intro-
duce a range of challenges, the responses from the community,
and less explored promising future directions.
Peaked Distribution Given the fact that reinforcement
learning improves the performance of generation for specific
objectives, Choshen et al. [288] inquire about the reason
behind the performance increase in neural machine translation.
They propose that the peakiness of the distribution is the
true factor for RL to improve performance instead of reward
learning. The peakiness illustrates that RL fine-tuning tends
to increase the probability of the best answer to make the
distribution lower entropy and more discrete. The following
work [289] refutes [288] and show that BLEU increase is
not tied to the peakiness in RL training. Other subsequent
papers methods to address this problem by multi-temperature
sampling [101] and finetuning [292]. More work could be
conducted to improve the diversity of the generated results in
specific areas. Potential solutions may be found in literature
of popularity bias [292].
Exploitation and Exploration In reinforcement learning,
an agent must balance a trade-off between exploitation and
exploration. To maximize the expected reward, the agent must
exploit the best action of what it has experienced before.

--- PAGE 23 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 23
But to collect the potential best action for learning, it must
explore broadly all possible situations to gather enough data
in the learning process. This dilemma is not suited for offline
settings such as classic supervised learning and unsupervised
learning. One solution for exploration is Upper-Confidence-
Bound (UCB) action selection, which injects the consideration
of exploration by the times of selection for action. UCB has
been applied in molecular design [258].
For sequential generation problems, action space is pro-
hibitively large which makes the exploration difficult. There-
fore, pretraining-finetuning architecture is widely exploited to
alleviate this problem. Another way to tackle this problem
is searching in the action space and composing meta-action
to decrease the difficulty of exploration. Similar to AlphaGo
[55], RationaleRL [293] proposes to employ an MCTS in
searching a relatively small and important action space to
improve the performance of the RL-based generator. A new
recent approach of exploring the solution space is applying
MCTS in the inference time of Large Language models [299],
[300], which opens new direction that self-play could enhance
the performance of current LLMs.
Reward function design and multi-objective optimiza-
tion A large number of works have been conducted on how to
guide model training with hand-designed new signals by RL.
Multiple objectives are usually utilized for various constraints
and guidance modelling. It is ideal that the optimal values can
be achieved all at once. But it is often not the case, making
the Pareto optimization a useful tool for analyzing the result.
While according to our exploration, few works have addressed
multi-objective optimization and Pareto optimization in ap-
plication areas. It might be valuable to trade-off in multiple
contradictory losses. Therefore, it is advantageous to explore
how to trade off between them for a more robust model.
Another problem is sparse rewards . It is easy to see that
rewards are training signals that shape the action pattern of
agents. In an ideal environment, rewards are emitted at each
action to provide sufficient guidance for models. If one reward
is computed based on the generated sequence [143], [294],
the environment must produce a reward at the end of each
episode. Some reward evaluation such as BLEU [65] has
this problem. Rewards become sparse and pose challenge for
the learning model to tell which action has a higher impact
on the last rewards. Guo et al. [294] proposes a multi-step
path consistency learning to address this problem by taking
a sequence as a whole, where the consistency is computed
over multiple steps instead of single step to tackle the sparse
reward problem. SURF [143] defines a new reward function
that emits a sequence of normalized rewards computed on
incomplete sentences and the target sequence. Korshunova
et al. [295] address sparse reward problems by fine-tuning,
experience replay, and real-time reward shaping. The fine-
tuning and experience replay could help the policy focus on
promising but rarely explored molecules, and reward shaping
provide dynamic process rewards. Another way to tackle
the sparse reward issue is introducing the label from human
experts as a new signal [296], which incorporates a DAgger-
like method into semantic parsers that interact with users by
texts. DAgger [301] is an imitation learning method that worksby collecting new data from experts and integrate them into the
old dataset, which is used for training a new policy. Despite
mentioned methods, Similar ideas like reward-shaping could
be applied according to specific task configuration, which is
also a promising direction to further enhance the model.
Long-term Credit Assignment The credit assignment
problem arises because the agent need to determines which
previous actions have impact on current rewards. With the
increase of time steps, it becomes more difficult for the
increase of the number of previous actions. Hierarchical RL
is introduced to cope with this problem [297]. It sets up two
modules for learning, a manager and a worker. The manager
takes the latent representation and produces a goal vector in a
low-dimensional space. The worker fuses the latent vector and
the goal to make the decision. LeakGAN [249] uses an agent
with hierarchical structure for image captioning to deal with
the long text generation problem. The hierarchical architecture
is similar to [297]. The difference is that policy gradient algo-
rithm is introduced into a GAN network, where reward comes
from the discriminator in GAN. With the increase of model
capability, more difficult and complex tasks are constructed
to test the boundary of the generative models. Hierarchical
methods could potentially improve the long-horizon problem
solving ability for the generative models.
Generalization Classic reinforcement learning algorithms
are often designed for specific tasks without considering task
adaptation [302]. Classic RL models often perform worse on
unseen tasks [303]. Meta reinforcement learning like MAML
[304] is designed to adapt agents for meta-learning tasks. The
meta-learning setting involves meta-training and meta-testing
environments, mimicking the few-shot supervised learning
setting where at test time, there are a few examples for the
algorithm to adapt to the test tasks. In order to adapt to
new tasks, MAML learns an adaptive initialization of neural
network parameters. This design is model-agnostic and can be
applied in various machine learning tasks such as supervised
learning and reinforcement learning. It can also be introduced
in multi-scenario text generation for adaptation of different
scenarios [298], where Zhao et al. combine MAML with
discriminator-guided text generation.
Recent works also show that it might be difficult to gen-
eralize logic inference tasks even for the best GPT-4 [305].
Thereby, the difficulty of generalization of deep generative
models might be further addressed by the RL algorithms.
More work should be devoted to how to design a model
that can better generalize and achieve better results on out-
of-distribution data. Retraining [306], and causal machine
learning [307] might be interesting ways to complement the
capability of RL-based generators for better learning and
adaptation.
Model Enhancement and Control Recent research has
tailored RL to help solve the difficulty of sampling of EBM
via RL as a distributional approach [112] or increasing the
efficiency by searching backward propagation of DDPM [82].
These approaches pave a new way to exert RL to improve
the generative model, in contrast to classic applications where
RL is a way to introduce new training signals or serve as
an architecture builder. The aforementioned works show the

--- PAGE 24 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 24
feasibility of RL application, more advancement in the RL
community might be transferred to these models to further
improve the model performance. A good example is [115],
which compares the difference between distributional formu-
lation and policy gradient but still exploits the similarity which
lay the foundation for variance reduction to be applied to
distributional methods.
Human Alignment in LLM and foundation models LLM
has demonstrated great transferability on a number of sequence
modeling problems. Combined with vision foundation models,
it is a potential way to achieve more powerful models for
diverse task settings. The fast-developing literature in this area
is drawing the capability of current large models and exposing
new problems for methods like RLHF. This might foster new
directions for how RL is incorporated into generative models.
The RLHF method [91] is a hot research direction since the
high impact of large language models [7]. New studies also
emerge to explore whether the RL is an indispensable factor.
Preference-based models have been proposed to model human
preference. Direct Preference Optimization (DPO) [174] aims
to substitute reinforcement learning by directly utilizing re-
ward functions by preference modelling. More research can be
conducted in this line to find an optimal way for preference
modelling. Also, human preference also is dynamic, so the
capture of dynamics and improvement on current generative
applications might be an interesting direction.
VI. C ONCLUSION
In this survey, we propose a unified taxonomy for RL
applied in generative AI. We collect works from various
directions and extract the key usage of reinforcement learning.
We first briefly introduce the concept of generative models
and reinforcement learning methods. Then we introduce the
key application methods for RL to be incorporated into the
generative models. Furthermore, we extract exemplar works in
a range of application areas for readers who want to narrow
down to a specific area. Finally, we show the promising
directions of future research and conclude the whole survey.
REFERENCES
[1] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv
preprint arXiv:1312.6114 , 2013.
[2] A. Van Den Oord, N. Kalchbrenner, and K. Kavukcuoglu, “Pixel
recurrent neural networks,” in International conference on machine
learning . PMLR, 2016, pp. 1747–1756.
[3] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
S. Ozair, A. Courville, and Y . Bengio, “Generative adversarial nets,”
Advances in neural information processing systems , vol. 27, 2014.
[4] J. Ho et al., “Denoising diffusion probabilistic models,” Advances in
neural information processing systems , vol. 33, pp. 6840–6851, 2020.
[5] R. Collobert and J. Weston, “A unified architecture for natural lan-
guage processing: Deep neural networks with multitask learning,” in
Proceedings of the 25th international conference on Machine learning ,
2008, pp. 160–167.
[6] D. Rezende and S. Mohamed, “Variational inference with normalizing
flows,” in International conference on machine learning . PMLR, 2015,
pp. 1530–1538.
[7] “Chatgpt,” https://openai.com/chatgpt, accessed: 2023-08-01.
[8] W. Harvey, S. Naderiparizi, V . Masrani, C. Weilbach, and F. Wood,
“Flexible diffusion modeling of long videos,” Advances in Neural
Information Processing Systems , vol. 35, pp. 27 953–27 965, 2022.[9] Y . Song, L. Shen, L. Xing, and S. Ermon, “Solving inverse problems in
medical imaging with score-based generative models,” arXiv preprint
arXiv:2111.08005 , 2021.
[10] J. Jumper et al., “Highly accurate protein structure prediction with
alphafold,” Nature , vol. 596, no. 7873, pp. 583–589, 2021.
[11] M. Baek et al., “Accurate prediction of protein structures and interac-
tions using a three-track neural network,” Science , vol. 373, no. 6557,
pp. 871–876, 2021.
[12] Z. Lin et al., “Evolutionary-scale prediction of atomic-level protein
structure with a language model,” Science , vol. 379, no. 6637, pp.
1123–1130, 2023.
[13] R. S. Sutton et al., Reinforcement learning: An introduction . MIT
press, 2018.
[14] M. Mohebbi Moghaddam et al., “Games of gans: Game-theoretical
models for generative adversarial networks,” Artificial Intelligence
Review , pp. 1–37, 2023.
[15] H. Chen et al., “A survey on dialogue systems: Recent advances and
new frontiers,” Acm Sigkdd Explorations Newsletter , vol. 19, no. 2, pp.
25–35, 2017.
[16] G. H. de Rosa et al., “A survey on text generation using generative
adversarial networks,” Pattern Recognition , vol. 119, p. 108098, 2021.
[17] V . Uc-Cetina et al., “Survey on reinforcement learning for language
processing,” Artificial Intelligence Review , vol. 56, no. 2, pp. 1543–
1575, 2023.
[18] J. Ni et al., “Recent advances in deep learning based dialogue systems:
A systematic survey,” Artificial intelligence review , vol. 56, no. 4, pp.
3055–3155, 2023.
[19] H. Sun, “Reinforcement learning in the era of llms: What is essential?
what is needed? an rl perspective on rlhf, prompting, and beyond,”
arXiv preprint arXiv:2310.06147 , 2023.
[20] C. Zhang et al., “A survey of automatic source code summarization,”
Symmetry , vol. 14, no. 3, p. 471, 2022.
[21] X. Tan et al., “A survey on neural speech synthesis,” arXiv preprint
arXiv:2106.15561 , 2021.
[22] B. Lin et al., “Reinforcement learning and bandits for speech and
language processing: Tutorial, review and outlook,” arXiv preprint
arXiv:2210.13623 , 2022.
[23] M. Z. Hossain et al., “A comprehensive survey of deep learning for
image captioning,” ACM Computing Surveys (CsUR) , vol. 51, no. 6,
pp. 1–36, 2019.
[24] N. Le et al., “Deep reinforcement learning in computer vision: a
comprehensive survey,” Artificial Intelligence Review , pp. 1–87, 2022.
[25] Y . Jaafra et al., “Reinforcement learning for neural architecture search:
A review,” Image and Vision Computing , vol. 89, pp. 57–66, 2019.
[26] S. Santra et al., “Gradient descent effects on differential neural ar-
chitecture search: A survey,” IEEE Access , vol. 9, pp. 89 602–89 618,
2021.
[27] Y . et al. Du, “Molgensurvey: A systematic survey in machine learning
models for molecule design,” 2022.
[28] J. C. Fromer et al., “Computer-aided multi-objective optimization in
small molecule discovery,” Patterns , vol. 4, no. 2, 2023.
[29] C. Bilodeau et al., “Generative models for molecular discovery: Recent
advances and challenges,” Wiley Interdisciplinary Reviews: Computa-
tional Molecular Science , vol. 12, no. 5, p. e1608, 2022.
[30] B. Tang et al., “Generative ai models for drug discovery,” in Biophysical
and Computational Tools in Drug Discovery . Springer, 2021, pp. 221–
243.
[31] S. Luukkonen et al., “Artificial intelligence in multi-objective drug
design,” Current Opinion in Structural Biology , vol. 79, p. 102537,
2023.
[32] G. Franceschelli and M. Musolesi, “Reinforcement learning for gener-
ative ai: State of the art, opportunities and open research challenges,”
Journal of Artificial Intelligence Research , vol. 79, pp. 417–446, 2024.
[33] Anna, “Exclusive: Chatgpt traffic slips again for third month in a row,”
The Reuters . [Online]. Available: https://www.reuters.com/technology/
chatgpt-traffic-slips-again-third-month-row-2023-09-07/
[34] I. MidJourney, “Midjourney,” https://www.midjourney.com, 2024, aI
tool.
[35] I. Goodfellow, Y . Bengio, and A. Courville, Deep learning . MIT
press, 2016.
[36] D. J. Rezende, S. Mohamed, and D. Wierstra, “Stochastic backprop-
agation and approximate inference in deep generative models,” in
International conference on machine learning . PMLR, 2014, pp.
1278–1286.
[37] Y . LeCun, S. Chopra, R. Hadsell, M. Ranzato, F. Huang et al. , “A
tutorial on energy-based learning,” Predicting structured data , vol. 1,
no. 0, 2006.

--- PAGE 25 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 25
[38] Y . Song and D. P. Kingma, “How to train your energy-based models,”
arXiv preprint arXiv:2101.03288 , 2021.
[39] M. A. Carreira-Perpinan et al., “On contrastive divergence learning,” in
International workshop on artificial intelligence and statistics . PMLR,
2005, pp. 33–40.
[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”
Advances in neural information processing systems , vol. 30, 2017.
[41] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek,
K. Xiao, S. Agrawal, and J. Dean, “Efficiently scaling transformer
inference,” Proceedings of Machine Learning and Systems , vol. 5, pp.
606–624, 2023.
[42] M. Freitag and Y . Al-Onaizan, “Beam search strategies for neural
machine translation,” arXiv preprint arXiv:1702.01806 , 2017.
[43] L. Dinh, D. Krueger, and Y . Bengio, “Nice: Non-linear independent
components estimation,” arXiv preprint arXiv:1410.8516 , 2014.
[44] I. Kobyzev, S. J. Prince, and M. A. Brubaker, “Normalizing flows:
An introduction and review of current methods,” IEEE transactions on
pattern analysis and machine intelligence , vol. 43, no. 11, pp. 3964–
3979, 2020.
[45] R. Bellman et al., “The theory of dynamic programming,” Bulletin of
the American Mathematical Society , vol. 60, no. 6, pp. 503–515, 1954.
[46] V . et al. Mnih, “Human-level control through deep reinforcement
learning,” Nat., vol. 518, no. 7540, pp. 529–533, 2015. [Online].
Available: https://doi.org/10.1038/nature14236
[47] T. Haarnoja et al., “Reinforcement learning with deep energy-based
policies,” in International conference on machine learning . PMLR,
2017, pp. 1352–1361.
[48] R. J. Williams et al., “Simple statistical gradient-following algorithms
for connectionist reinforcement learning,” Reinforcement learning , pp.
5–32, 1992.
[49] J. Schulman et al., “Trust region policy optimization,” in International
conference on machine learning . PMLR, 2015, pp. 1889–1897.
[50] ——, “Proximal policy optimization algorithms,” arXiv preprint
arXiv:1707.06347 , 2017.
[51] V . Mnih et al., “Asynchronous methods for deep reinforcement learn-
ing,” in International conference on machine learning . PMLR, 2016,
pp. 1928–1937.
[52] T. et al. P. Lillicrap, “Continuous control with deep
reinforcement learning,” in 4th International Conference on Learning
Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
Conference Track Proceedings , Y . Bengio and Y . LeCun, Eds., 2016.
[Online]. Available: http://arxiv.org/abs/1509.02971
[53] T. Haarnoja et al., “Soft actor-critic: Off-policy maximum entropy
deep reinforcement learning with a stochastic actor,” in International
conference on machine learning . PMLR, 2018, pp. 1861–1870.
[54] D. Silver et al., “Deterministic policy gradient algorithms,” in
Proceedings of the 31st International Conference on Machine
Learning , ser. Proceedings of Machine Learning Research, E. P.
Xing and T. Jebara, Eds., vol. 32, no. 1. Bejing, China:
PMLR, 22–24 Jun 2014, pp. 387–395. [Online]. Available: https:
//proceedings.mlr.press/v32/silver14.html
[55] ——, “Mastering the game of go with deep neural networks and tree
search,” nature , vol. 529, no. 7587, pp. 484–489, 2016.
[56] ——, “Mastering the game of go without human knowledge,” nature ,
vol. 550, no. 7676, pp. 354–359, 2017.
[57] R. S. Sutton et al., “Integrated architectures for learning, planning, and
reacting based on approximating dynamic programming,” in Machine
learning proceedings 1990 . Elsevier, 1990, pp. 216–224.
[58] A. Ahmadian, C. Cremer, M. Gall ´e, M. Fadaee, J. Kreutzer, O. Pietquin,
A.¨Ust¨un, and S. Hooker, “Back to basics: Revisiting reinforce style
optimization for learning from human feedback in llms,” arXiv preprint
arXiv:2402.14740 , 2024.
[59] C. et al. Mao, “Discrete representations strengthen vision
transformer robustness,” in International Conference on Learning
Representations , 2022. [Online]. Available: https://openreview.net/
forum?id=8hWs60AZcWk
[60] R. Devon Hjelm et al., “Boundary-seeking generative adversarial
networks,” arXiv e-prints , pp. arXiv–1702, 2017.
[61] H. Fan et al., “Recurrent attention network with reinforced genera-
tor for visual dialog,” ACM Transactions on Multimedia Computing,
Communications, and Applications (TOMM) , vol. 16, no. 3, pp. 1–16,
2020.
[62] T. M. Nguyen et al., “Infocnf: An efficient conditional continuous nor-
malizing flow with adaptive solvers,” arXiv preprint arXiv:1912.03978 ,
2019.[63] L. Yu et al., “Seqgan: Sequence generative adversarial nets with
policy gradient,” in Proceedings of the AAAI conference on artificial
intelligence , vol. 31, no. 1, 2017.
[64] M. Ranzato et al., “Sequence level training with recurrent neural
networks,” arXiv preprint arXiv:1511.06732 , 2015.
[65] K. Papineni et al., “Bleu: a method for automatic evaluation of
machine translation,” in Proceedings of the 40th annual meeting of
the Association for Computational Linguistics , 2002, pp. 311–318.
[66] C.-Y . Lin, “Rouge: A package for automatic evaluation of summaries,”
inText summarization branches out , 2004, pp. 74–81.
[67] G. L. Guimaraes et al., “Objective-reinforced generative adversarial
networks (organ) for sequence generation models,” arXiv preprint
arXiv:1705.10843 , 2017.
[68] Q. Wang et al., “Grl: Knowledge graph completion with gan-based re-
inforcement learning,” Knowledge-Based Systems , vol. 209, p. 106421,
2020.
[69] W. Fedus et al., “Maskgan: better text generation via filling in the ,”
arXiv preprint arXiv:1801.07736 , 2018.
[70] K. Lin et al., “Adversarial ranking for language generation,” Advances
in neural information processing systems , vol. 30, 2017.
[71] W. Zhou et al., “Self-adversarial learning with comparative discrimi-
nation for text generation,” arXiv preprint arXiv:2001.11691 , 2020.
[72] T. Scialom et al., “Coldgans: Taming language gans with cautious sam-
pling strategies,” Advances in Neural Information Processing Systems ,
vol. 33, pp. 18 978–18 989, 2020.
[73] T. Che et al., “Maximum-likelihood augmented discrete generative
adversarial networks,” arXiv preprint arXiv:1702.07983 , 2017.
[74] T. Scialom et al., “To beam or not to beam: That is a question
of cooperation for language gans,” Advances in neural information
processing systems , vol. 34, pp. 26 585–26 597, 2021.
[75] M. Sarmad et al., “Rl-gan-net: A reinforcement learning agent con-
trolled gan network for real-time point cloud shape completion,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) , June 2019.
[76] Q. Wu et al., “Textgail: Generative adversarial imitation learning for
text generation,” in Proceedings of the AAAI Conference on Artificial
Intelligence , vol. 35, no. 16, 2021, pp. 14 067–14 075.
[77] Y . Wan et al., “Improving automatic source code summarization via
deep reinforcement learning,” in Proceedings of the 33rd ACM/IEEE
international conference on automated software engineering , 2018, pp.
397–407.
[78] J. Li et al., “Deep reinforcement learning for dialogue generation,”
arXiv preprint arXiv:1606.01541 , 2016.
[79] K. Mo et al., “Personalizing a dialogue system with transfer reinforce-
ment learning,” in Proceedings of the AAAI Conference on Artificial
Intelligence , vol. 32, no. 1, 2018.
[80] T. Zhao et al., “Towards end-to-end learning for dialog state tracking
and management using deep reinforcement learning,” arXiv preprint
arXiv:1606.02560 , 2016.
[81] V . Pyatkin et al., “Reinforced clarification question generation with
defeasibility rewards for disambiguating social and moral situations,”
arXiv preprint arXiv:2212.10409 , 2022.
[82] Y . Fan et al., “Optimizing ddpm sampling with shortcut fine-tuning,”
arXiv preprint arXiv:2301.13362 , 2023.
[83] N. Jaques et al., “Sequence tutor: Conservative fine-tuning of sequence
generation models with kl-control,” in International Conference on
Machine Learning . PMLR, 2017, pp. 1645–1654.
[84] D. M. Ziegler et al., “Fine-tuning language models from human
preferences,” arXiv preprint arXiv:1909.08593 , 2019.
[85] N. et al. Jaques, “Way off-policy batch deep reinforcement learning
of implicit human preferences in dialog,” CoRR , vol. abs/1907.00456,
2019. [Online]. Available: http://arxiv.org/abs/1907.00456
[86] R. et al. Yuanzhe Pang, “Text generation by learning from demonstra-
tions,” in International Conference on Learning Representations , 2021.
[Online]. Available: https://openreview.net/forum?id=RovX-uQ1Hua
[87] M. Norouzi et al., “Reward augmented maximum likelihood for neural
structured prediction,” Advances In Neural Information Processing
Systems , vol. 29, 2016.
[88] P. Ke et al., “ARAML: A stable adversarial training framework
for text generation,” in Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing and
the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP) . Hong Kong, China: Association for
Computational Linguistics, November 2019, pp. 4271–4281. [Online].
Available: https://aclanthology.org/D19-1436

--- PAGE 26 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 26
[89] S. Lamprier et al., “Generative cooperative networks for natural lan-
guage generation,” in International Conference on Machine Learning .
PMLR, 2022, pp. 11 891–11 905.
[90] Z. et al. Shi, “Toward diverse text generation with inverse reinforcement
learning,” in Proceedings of the Twenty-Seventh International Joint
Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018,
Stockholm, Sweden , J. Lang, Ed. ijcai.org, 2018, pp. 4361–4367.
[Online]. Available: https://doi.org/10.24963/ijcai.2018/606
[91] N. Stiennon et al., “Learning to summarize with human feedback,”
Advances in Neural Information Processing Systems , vol. 33, pp. 3008–
3021, 2020.
[92] L. Ouyang et al., “Training language models to follow instructions with
human feedback,” Advances in Neural Information Processing Systems ,
vol. 35, pp. 27 730–27 744, 2022.
[93] Y . Bai et al., “Training a helpful and harmless assistant with
reinforcement learning from human feedback,” arXiv preprint
arXiv:2204.05862 , 2022.
[94] Y . Gao et al., “Preference-based interactive multi-document summari-
sation,” Information Retrieval Journal , vol. 23, pp. 555–585, 2020.
[95] J. Kreutzer et al., “Offline reinforcement learning from human
feedback in real-world sequence-to-sequence tasks,” in Proceedings of
the 5th Workshop on Structured Prediction for NLP (SPNLP 2021) .
Online: Association for Computational Linguistics, August 2021, pp.
37–43. [Online]. Available: https://aclanthology.org/2021.spnlp-1.4
[96] Y . Bai et al., “Constitutional ai: Harmlessness from ai feedback,” arXiv
preprint arXiv:2212.08073 , 2022.
[97] B. Zhu et al., “Principled reinforcement learning with human
feedback from pairwise or k-wise comparisons,” arXiv preprint
arXiv:2301.11270 , 2023.
[98] J. Li et al., “Adversarial learning for neural dialogue generation,”
inProceedings of the 2017 Conference on Empirical Methods in
Natural Language Processing . Copenhagen, Denmark: Association
for Computational Linguistics, September 2017, pp. 2157–2169.
[Online]. Available: https://aclanthology.org/D17-1230
[99] S. J. Rennie et al., “Self-critical sequence training for image caption-
ing,” in Proceedings of the IEEE conference on computer vision and
pattern recognition , 2017, pp. 7008–7024.
[100] R. Cai et al., “Tag: Type auxiliary guiding for code comment genera-
tion,” arXiv preprint arXiv:2005.02835 , 2020.
[101] P. Dognin et al., “ReGen: Reinforcement learning for text and
knowledge base generation using pretrained language models,” in
Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing . Online and Punta Cana, Dominican
Republic: Association for Computational Linguistics, November 2021,
pp. 1084–1099. [Online]. Available: https://aclanthology.org/2021.
emnlp-main.83
[102] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “Cider: Consensus-
based image description evaluation,” in Proceedings of the IEEE
conference on computer vision and pattern recognition , 2015, pp.
4566–4575.
[103] S. Banerjee and A. Lavie, “Meteor: An automatic metric for mt evalua-
tion with improved correlation with human judgments,” in Proceedings
of the acl workshop on intrinsic and extrinsic evaluation measures for
machine translation and/or summarization , 2005, pp. 65–72.
[104] M. Popovi ´c, “chrf++: words helping character n-grams,” in Proceedings
of the second conference on machine translation , 2017, pp. 612–618.
[105] S. Narayan et al., “Ranking sentences for extractive summarization
with reinforcement learning,” in Proceedings of the 2018 Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long Papers) .
New Orleans, Louisiana: Association for Computational Linguistics,
June 2018, pp. 1747–1759. [Online]. Available: https://aclanthology.
org/N18-1158
[106] Y .-C. Chen et al., “Fast abstractive summarization with reinforce-
selected sentence rewriting,” in Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) . Melbourne, Australia: Association for Computational
Linguistics, July 2018, pp. 675–686. [Online]. Available: https:
//aclanthology.org/P18-1063
[107] A. F. Aky ¨urek et al., “Rl4f: Generating natural language feedback with
reinforcement learning for repairing model outputs,” arXiv preprint
arXiv:2305.08844 , 2023.
[108] E. Todorov et al., “Linearly-solvable markov decision problems,”
Advances in neural information processing systems , vol. 19, 2006.
[109] Y . Gao et al., “Reward learning for efficient reinforcement learning in
extractive document summarisation,” arXiv preprint arXiv:1907.12894 ,
2019.[110] K. Nguyen et al., “Reinforcement learning for bandit neural machine
translation with simulated human feedback,” in Proceedings of
the 2017 Conference on Empirical Methods in Natural Language
Processing . Copenhagen, Denmark: Association for Computational
Linguistics, September 2017, pp. 1464–1474. [Online]. Available:
https://aclanthology.org/D17-1153
[111] R. S. Sutton et al., Temporal credit assignment in reinforcement
learning . University of Massachusetts Amherst, 1984.
[112] T. Parshakova et al., “Distributional reinforcement learning for energy-
based sequential models,” arXiv preprint arXiv:1912.08517 , 2019.
[113] M. Khalifa et al., “A distributional approach to controlled text gener-
ation,” arXiv preprint arXiv:2012.11635 , 2020.
[114] T. Korbak et al., “Energy-based models for code generation under
compilability constraints,” arXiv preprint arXiv:2106.04985 , 2021.
[115] T. et al. Korbak, “On reward maximization and distribution
matching for fine-tuning language models,” 2022. [Online]. Available:
https://openreview.net/forum?id=8f95ajHrIFc
[116] D. Go et al., “Aligning language models with preferences through f-
divergence minimization,” arXiv preprint arXiv:2302.08215 , 2023.
[117] B. et al. Zoph, “Neural architecture search with reinforcement
learning,” in International Conference on Learning Representations ,
2017. [Online]. Available: https://openreview.net/forum?id=r1Ue8Hcxg
[118] H. Pham et al., “Efficient neural architecture search via parameters
sharing,” in International conference on machine learning . PMLR,
2018, pp. 4095–4104.
[119] C. et al. Hsu, “MONAS: multi-objective neural architecture search
using reinforcement learning,” CoRR , vol. abs/1806.10332, 2018.
[Online]. Available: http://arxiv.org/abs/1806.10332
[120] N.-Q. Pham et al., “Towards one-shot learning for rare-word
translation with external experts,” in Proceedings of the 2nd Workshop
on Neural Machine Translation and Generation . Melbourne,
Australia: Association for Computational Linguistics, July 2018, pp.
100–109. [Online]. Available: https://aclanthology.org/W18-2712
[121] M. Guo et al., “Irlas: Inverse reinforcement learning for architecture
search,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2019, pp. 9021–9029.
[122] Z. Zhong et al., “Practical block-wise neural network architecture
generation,” in Proceedings of the IEEE conference on computer vision
and pattern recognition , 2018, pp. 2423–2432.
[123] Y . Tian et al., “Off-policy reinforcement learning for efficient and ef-
fective gan architecture search,” in Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
Part VII 16 . Springer, 2020, pp. 175–192.
[124] X. Chen et al., “Catch: Context-based meta reinforcement learning
for transferrable architecture search,” in Computer Vision–ECCV 2020:
16th European Conference, Glasgow, UK, August 23–28, 2020, Pro-
ceedings, Part XIX 16 . Springer, 2020, pp. 185–202.
[125] B. et al. Baker, “Designing neural network architectures using
reinforcement learning,” in International Conference on Learning
Representations , 2017. [Online]. Available: https://openreview.net/
forum?id=S1c2cvqee
[126] D. Pang et al., “Rl-darts: Differentiable neural architecture search
via reinforcement-learning-based meta-optimizer,” Knowledge-Based
Systems , vol. 234, p. 107585, 2021.
[127] A. Chauhan et al., “Dqnas: Neural architecture search using reinforce-
ment learning,” arXiv preprint arXiv:2301.06687 , 2023.
[128] K. He et al., “Deep residual learning for image recognition,” in
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , June 2016.
[129] J. Rijsdijk et al., “Reinforcement learning for hyperparameter tuning
in deep learning-based side-channel analysis,” IACR Transactions on
Cryptographic Hardware and Embedded Systems , pp. 677–707, 2021.
[130] R. et al. Paulus, “A deep reinforced model for abstractive summariza-
tion,” in International Conference on Learning Representations , 2018.
[Online]. Available: https://openreview.net/forum?id=HkAClQgA-
[131] L. Wang et al., “A reinforced topic-aware convolutional sequence-to-
sequence model for abstractive text summarization,” in Proceedings of
the 27th International Joint Conference on Artificial Intelligence , ser.
IJCAI’18. AAAI Press, 2018, p. 4453–4460.
[132] Y . Wu et al., “Learning to extract coherent summary via deep reinforce-
ment learning,” in Proceedings of the AAAI conference on artificial
intelligence , vol. 32, no. 1, 2018.
[133] L. Gui et al., “Neural topic model with reinforcement learning,” in
Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP) , 2019, pp. 3478–
3483.

--- PAGE 27 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 27
[134] J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, and
P. Christiano, “Recursively summarizing books with human feedback,”
arXiv preprint arXiv:2109.10862 , 2021.
[135] M. Yang, C. Li, Y . Shen, Q. Wu, Z. Zhao, and X. Chen, “Hierarchical
human-like deep neural networks for abstractive text summarization,”
IEEE Transactions on Neural Networks and Learning Systems , vol. 32,
no. 6, pp. 2744–2757, 2020.
[136] R. Jie, X. Meng, L. Shang, X. Jiang, and Q. Liu, “Prompt-based length
controlled generation with reinforcement learning,” arXiv preprint
arXiv:2308.12030 , 2023.
[137] L. Wu et al., “A study of reinforcement learning for neural machine
translation,” in Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing . Brussels, Belgium:
Association for Computational Linguistics, October-November 2018,
pp. 3612–3621. [Online]. Available: https://aclanthology.org/D18-1397
[138] D. Kumari et al., “Sentiment preservation in review translation using
curriculum-based re-inforcement framework,” in Proceedings of Ma-
chine Translation Summit XVIII: Research Track , 2021, pp. 150–162.
[139] X. Zhang et al., “Sentence simplification with deep reinforcement
learning,” in Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing . Copenhagen, Denmark:
Association for Computational Linguistics, September 2017, pp.
584–594. [Online]. Available: https://aclanthology.org/D17-1062
[140] T. K. Lam et al., “A reinforcement learning approach to interactive-
predictive neural machine translation,” in Proceedings of the 21st
Annual Conference of the European Association for Machine
Translation , Alicante, Spain, May 2018, pp. 189–198. [Online].
Available: https://aclanthology.org/2018.eamt-main.17
[141] T. Zhao et al., “Balancing quality and human involvement: An effective
approach to interactive neural machine translation,” in Proceedings of
the AAAI Conference on Artificial Intelligence , vol. 34, no. 05, 2020,
pp. 9660–9667.
[142] F. Luo et al., “Towards fine-grained text sentiment transfer,” in Proceed-
ings of the 57th Annual Meeting of the Association for Computational
Linguistics , 2019, pp. 2013–2022.
[143] A. Anuchitanukul et al., “SURF: Semantic-level unsupervised reward
function for machine translation,” in Proceedings of the 2022
Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies . Seattle,
United States: Association for Computational Linguistics, July 2022,
pp. 4508–4522. [Online]. Available: https://aclanthology.org/2022.
naacl-main.334
[144] M. Zhao, H. Wu, D. Niu, and X. Wang, “Reinforced curriculum learn-
ing on pre-trained neural machine translation models,” in Proceedings
of the AAAI Conference on Artificial Intelligence , vol. 34, no. 05, 2020,
pp. 9652–9659.
[145] I. V . Serban et al., “A deep reinforcement learning chatbot,” arXiv
preprint arXiv:1709.02349 , 2017.
[146] M. Yang et al., “Personalized response generation via domain adapta-
tion,” in Proceedings of the 40th International ACM SIGIR Conference
on Research and Development in Information Retrieval , 2017, pp.
1021–1024.
[147] Z. Sun et al., “Replicating complex dialogue policy of humans via of-
fline imitation learning with supervised regularization,” arXiv preprint
arXiv:2305.03987 , 2023.
[148] J. D. Williams et al., “Hybrid code networks: practical and efficient
end-to-end dialog control with supervised and reinforcement learning,”
inProceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) . Vancouver,
Canada: Association for Computational Linguistics, July 2017, pp.
665–677. [Online]. Available: https://aclanthology.org/P17-1062
[149] M. Lewis et al., “Deal or no deal? end-to-end learning of negotiation
dialogues,” in Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing . Copenhagen, Denmark:
Association for Computational Linguistics, September 2017, pp.
2443–2453. [Online]. Available: https://aclanthology.org/D17-1259
[150] X. Li et al., “End-to-end task-completion neural dialogue systems,” in
Proceedings of the Eighth International Joint Conference on Natural
Language Processing (Volume 1: Long Papers) . Taipei, Taiwan:
Asian Federation of Natural Language Processing, November 2017,
pp. 733–743. [Online]. Available: https://aclanthology.org/I17-1074
[151] C. Ye et al., “Structured and natural responses co-generation for conver-
sational search,” in Proceedings of the 45th International ACM SIGIR
Conference on Research and Development in Information Retrieval ,
2022, pp. 155–164.
[152] A. Martin et al., “Learning natural language generation with truncated
reinforcement learning,” in Proceedings of the 2022 Conference ofthe North American Chapter of the Association for Computational
Linguistics: Human Language Technologies , 2022, pp. 12–37.
[153] Y . Jang, J. Lee, and K.-E. Kim, “Gpt-critic: Offline reinforcement learn-
ing for end-to-end task-oriented dialogue systems,” in International
Conference on Learning Representations , 2022.
[154] C. Zhong, K. Liao, W. Chen, Q. Liu, B. Peng, X. Huang, J. Peng,
and Z. Wei, “Hierarchical reinforcement learning for automatic disease
diagnosis,” Bioinformatics , vol. 38, no. 16, pp. 3995–4001, 2022.
[155] A. Saleh, N. Jaques, A. Ghandeharioun, J. Shen, and R. Picard, “Hierar-
chical reinforcement learning for open-domain dialog,” in Proceedings
of the AAAI conference on artificial intelligence , vol. 34, no. 05, 2020,
pp. 8741–8748.
[156] R. Yang, J. Chen, and K. Narasimhan, “Improving dialog sys-
tems for negotiation with personality modeling,” arXiv preprint
arXiv:2010.09954 , 2020.
[157] A. Srivastava et al., “Response-act guided reinforced dialogue gener-
ation for mental health counseling,” in Proceedings of the ACM Web
Conference 2023 , 2023, pp. 1118–1129.
[158] R. Liu et al., “Aligning generative language models with human
values,” in Findings of the Association for Computational Linguistics:
NAACL 2022 , 2022, pp. 241–252.
[159] R. Zhang et al., “Reward constrained interactive recommendation with
natural language feedback,” arXiv preprint arXiv:2005.01618 , 2020.
[160] H. Aghakhani et al., “Detecting deceptive reviews using generative
adversarial networks,” in 2018 IEEE Security and Privacy Workshops
(SPW) . IEEE, 2018, pp. 89–95.
[161] A. K. Mohankumar et al., “Diversity driven query rewriting in search
advertising,” in Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery & Data Mining , ser. KDD ’21. New York, NY ,
USA: Association for Computing Machinery, 2021, p. 3423–3431.
[Online]. Available: https://doi.org/10.1145/3447548.3467202
[162] X. Wang et al., “Ordering-based causal discovery with reinforcement
learning,” arXiv preprint arXiv:2105.06631 , 2021.
[163] A. Kumar, V . Zhuang, R. Agarwal, Y . Su, J. D. Co-Reyes, A. Singh,
K. Baumli, S. Iqbal, C. Bishop, R. Roelofs et al. , “Training language
models to self-correct via reinforcement learning,” arXiv preprint
arXiv:2409.12917 , 2024.
[164] L. Shani, A. Rosenberg, A. Cassel, O. Lang, D. Calandriello, A. Zipori,
H. Noga, O. Keller, B. Piot, I. Szpektor et al. , “Multi-turn rein-
forcement learning from preference human feedback,” arXiv preprint
arXiv:2405.14655 , 2024.
[165] Y . Zhou, A. Zanette, J. Pan, S. Levine, and A. Kumar, “Archer: Training
language model agents via hierarchical multi-turn rl,” arXiv preprint
arXiv:2402.19446 , 2024.
[166] A. Alabdulkarim, W. Li, L. J. Martin, and M. O. Riedl, “Goal-
directed story generation: Augmenting generative language models
with reinforcement learning,” arXiv preprint arXiv:2112.08593 , 2021.
[167] Z. Wu, Y . Hu, W. Shi, N. Dziri, A. Suhr, P. Ammanabrolu, N. A.
Smith, M. Ostendorf, and H. Hajishirzi, “Fine-grained human feedback
gives better rewards for language model training,” Advances in Neural
Information Processing Systems , vol. 36, pp. 59 008–59 033, 2023.
[168] P. Jha, P. Jana, P. Suresh, A. Arora, and V . Ganesh, “Rlsf:
Reinforcement learning via symbolic feedback,” arXiv preprint
arXiv:2405.16661 , 2024.
[169] Z. Chen, K. Zhou, W. X. Zhao, J. Wan, F. Zhang, D. Zhang, and
J.-R. Wen, “Improving large language models via fine-grained rein-
forcement learning with minimum editing constraint,” arXiv preprint
arXiv:2401.06081 , 2024.
[170] W. Li, W. Wei, K. Xu, W. Xie, D. Chen, and Y . Cheng, “Reinforcement
learning with token-level feedback for controllable text generation,”
arXiv preprint arXiv:2403.11558 , 2024.
[171] J. Li, H. Zhang, F. Zhang, T.-W. Chang, K. Kuang, L. Chen, and
J. Zhou, “Optimizing language models with fair and stable reward
composition in reinforcement learning,” in Proceedings of the 2024
Conference on Empirical Methods in Natural Language Processing ,
2024, pp. 10 122–10 140.
[172] Z. Li, T. Xu, Y . Zhang, Z. Lin, Y . Yu, R. Sun, and Z.-Q. Luo, “Remax:
A simple, effective, and efficient reinforcement learning method for
aligning large language models,” in Forty-first International Conference
on Machine Learning , 2023.
[173] P. H. Richemond, Y . Tang, D. Guo, D. Calandriello, M. G. Azar,
R. Rafailov, B. A. Pires, E. Tarassov, L. Spangher, W. Ellsworth et al. ,
“Offline regularised reinforcement learning for large language models
alignment,” arXiv preprint arXiv:2405.19107 , 2024.
[174] R. et al. Rafailov, “Direct preference optimization: Your language
model is secretly a reward model,” 2023.

--- PAGE 28 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 28
[175] A. Baheti, X. Lu, F. Brahman, R. L. Bras, M. Sap, and M. Riedl,
“Leftover lunch: Advantage-based offline reinforcement learning for
language models,” arXiv preprint arXiv:2305.14718 , 2023.
[176] M. Wulfmeier, M. Bloesch, N. Vieillard, A. Ahuja, J. Bornschein,
S. Huang, A. Sokolov, M. Barnes, G. Desjardins, A. Bewley et al. ,
“Imitating language via scalable inverse reinforcement learning,” in The
Thirty-eighth Annual Conference on Neural Information Processing
Systems , 2024.
[177] T. Korbak, K. Shi, A. Chen, R. V . Bhalerao, C. Buckley, J. Phang,
S. R. Bowman, and E. Perez, “Pretraining language models with
human preferences,” in International Conference on Machine Learning .
PMLR, 2023, pp. 17 506–17 533.
[178] C. Snell, I. Kostrikov, Y . Su, M. Yang, and S. Levine, “Offline rl for
natural language generation with implicit language q learning,” arXiv
preprint arXiv:2206.11871 , 2022.
[179] J. Hu, L. Tao, J. Yang, and C. Zhou, “Aligning language models with
offline reinforcement learning from human feedback,” arXiv preprint
arXiv:2308.12050 , 2023.
[180] M. Deng, J. Wang, C.-P. Hsieh, Y . Wang, H. Guo, T. Shu, M. Song,
E. P. Xing, and Z. Hu, “Rlprompt: Optimizing discrete text prompts
with reinforcement learning,” arXiv preprint arXiv:2205.12548 , 2022.
[181] T. Zhang, X. Wang, D. Zhou, D. Schuurmans, and J. E. Gonzalez,
“Tempera: Test-time prompting via reinforcement learning,” arXiv
preprint arXiv:2211.11890 , 2022.
[182] Z. Huang, X. Wang, F. Zhang, Z. Xu, C. Zhang, X. Zheng, and
X. Huang, “Enhancing the capability and robustness of large language
models through reinforcement learning-driven query refinement,” arXiv
preprint arXiv:2407.01461 , 2024.
[183] H. Sun, “Offline prompt evaluation and optimization with inverse
reinforcement learning,” arXiv preprint arXiv:2309.06553 , 2023.
[184] K. Nottingham, Y . Razeghi, K. Kim, J. Lanier, P. Baldi, R. Fox, and
S. Singh, “Selective perception: Optimizing state descriptions with
reinforcement learning for language model actors,” arXiv preprint
arXiv:2307.11922 , 2023.
[185] W. Shen, R. Zheng, W. Zhan, J. Zhao, S. Dou, T. Gui, Q. Zhang,
and X. Huang, “Loose lips sink ships: Mitigating length bias
in reinforcement learning from human feedback,” arXiv preprint
arXiv:2310.05199 , 2023.
[186] J. Dai, X. Pan, R. Sun, J. Ji, X. Xu, M. Liu, Y . Wang, and Y . Yang,
“Safe rlhf: Safe reinforcement learning from human feedback,” arXiv
preprint arXiv:2310.12773 , 2023.
[187] T. R. McIntosh, T. Susnjak, T. Liu, P. Watters, and M. N. Halgamuge,
“The inadequacy of reinforcement learning from human feedback-
radicalizing large language models via semantic vulnerabilities,” IEEE
Transactions on Cognitive and Developmental Systems , 2024.
[188] J. Wang, J. Wu, M. Chen, Y . V orobeychik, and C. Xiao, “Rlhfpoison:
Reward poisoning attack for reinforcement learning with human feed-
back in large language models,” in Proceedings of the 62nd Annual
Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers) , 2024, pp. 2551–2570.
[189] X. Wang, J. Peng, K. Xu, H. Yao, and T. Chen, “Reinforcement
learning-driven llm agent for automated attacks on llms,” in Proceed-
ings of the Fifth Workshop on Privacy in Natural Language Processing ,
2024, pp. 170–177.
[190] Y . Li et al., “A generative model for category text generation,”
Information Sciences , vol. 450, pp. 301–315, 2018.
[191] J. Xu et al., “Diversity-promoting gan: A cross-entropy based genera-
tive adversarial network for diversified text generation,” in Proceedings
of the 2018 conference on empirical methods in natural language
processing , 2018, pp. 3940–3949.
[192] Q. Wu et al., “Automatic math word problem generation with
topic-expression co-attention mechanism and reinforcement learning,”
IEEE/ACM Transactions on Audio, Speech, and Language Processing ,
vol. 30, pp. 1061–1072, 2022.
[193] B. Upadhyay et al., “Efficient reinforcement learning for unsupervised
controlled text generation,” arXiv preprint arXiv:2204.07696 , 2022.
[194] A. Siddique, S. Oymak, and V . Hristidis, “Unsupervised paraphrasing
via deep reinforcement learning,” in Proceedings of the 26th ACM
SIGKDD international conference on knowledge discovery & data
mining , 2020, pp. 1800–1809.
[195] C. Wang, H. Zhou, Y . Hu, Y . Huo, B. Li, T. Liu, T. Xiao, and J. Zhu,
“Esrl: Efficient sampling-based reinforcement learning for sequence
generation,” in Proceedings of the AAAI Conference on Artificial
Intelligence , vol. 38, no. 17, 2024, pp. 19 107–19 115.
[196] N. Jaques, J. H. Shen, A. Ghandeharioun, C. Ferguson, A. Lapedriza,
N. Jones, S. S. Gu, and R. Picard, “Human-centric dialog trainingvia offline reinforcement learning,” arXiv preprint arXiv:2010.05848 ,
2020.
[197] D. Yarats et al., “Hierarchical text generation and planning for strategic
dialogue,” in International Conference on Machine Learning . PMLR,
2018, pp. 5591–5599.
[198] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-
dimensional continuous control using generalized advantage estima-
tion,” arXiv preprint arXiv:1506.02438 , 2015.
[199] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao,
“Reflexion: Language agents with verbal reinforcement learning,”
Advances in Neural Information Processing Systems , vol. 36, 2024.
[200] J.-C. Pang, P. Wang, K. Li, X.-H. Chen, J. Xu, Z. Zhang, and
Y . Yu, “Language model self-improvement by reinforcement learning
contemplation,” arXiv preprint arXiv:2305.14483 , 2023.
[201] A. Havrilla, M. Zhuravinskyi, D. Phung, A. Tiwari, J. Tow, S. Bi-
derman, Q. Anthony, and L. Castricato, “trlx: A framework for large
scale reinforcement learning from human feedback,” in Proceedings
of the 2023 Conference on Empirical Methods in Natural Language
Processing , 2023, pp. 8578–8595.
[202] S. Zhang, Z. Chen, S. Chen, Y . Shen, Z. Sun, and C. Gan, “Improving
reinforcement learning from human feedback with efficient reward
model ensemble,” arXiv preprint arXiv:2401.16635 , 2024.
[203] A. Havrilla, Y . Du, S. C. Raparthy, C. Nalmpantis, J. Dwivedi-Yu,
M. Zhuravinskyi, E. Hambro, S. Sukhbaatar, and R. Raileanu, “Teach-
ing large language models to reason with reinforcement learning,”
arXiv preprint arXiv:2403.04642 , 2024.
[204] Z. Yao et al., “Coacor: Code annotation for code retrieval with
reinforcement learning,” in The world wide web conference , 2019, pp.
2203–2214.
[205] C. Wang et al., “Enriching query semantics for code search with
reinforcement learning,” Neural Networks , vol. 145, pp. 22–32, 2022.
[206] A. Dai, Z. Zhu, H. Hu, G. Tang, L. Liu, and S. Xu, “Enhancing e-
commerce query rewriting: A large language model approach with
domain-specific pre-training and reinforcement learning,” in Proceed-
ings of the 33rd ACM International Conference on Information and
Knowledge Management , 2024, pp. 4439–4445.
[207] W. Wang et al., “Reinforcement-learning-guided source code summa-
rization using hierarchical attention,” IEEE Transactions on software
Engineering , vol. 48, no. 1, pp. 102–119, 2020.
[208] X. Wang et al., “Compilable neural code generation with compiler
feedback,” arXiv preprint arXiv:2203.05132 , 2022.
[209] L. Zhang et al., “Learnedsqlgen: Constraint-aware sql generation using
reinforcement learning,” in Proceedings of the 2022 International
Conference on Management of Data , ser. SIGMOD ’22. New York,
NY , USA: Association for Computing Machinery, 2022, p. 945–958.
[Online]. Available: https://doi.org/10.1145/3514221.3526155
[210] H. Le et al., “Coderl: Mastering code generation through pretrained
models and deep reinforcement learning,” Advances in Neural Infor-
mation Processing Systems , vol. 35, pp. 21 314–21 328, 2022.
[211] P. Shojaee et al., “Execution-based code generation using deep rein-
forcement learning,” arXiv preprint arXiv:2301.13816 , 2023.
[212] S. Duan, N. Kanakaris, X. Xiao, H. Ping, C. Zhou, N. K. Ahmed,
G. Ma, M. Capota, T. L. Willke, S. Nazarian et al. , “Leveraging rein-
forcement learning and large language models for code optimization,”
arXiv preprint arXiv:2312.05657 , 2023.
[213] J. Liu, Y . Zhu, K. Xiao, Q. Fu, X. Han, W. Yang, and D. Ye,
“Rltf: Reinforcement learning from unit test feedback,” arXiv preprint
arXiv:2307.04349 , 2023.
[214] J. Gehring, K. Zheng, J. Copet, V . Mella, T. Cohen, and G. Synnaeve,
“Rlef: Grounding code llms in execution feedback with reinforcement
learning,” arXiv preprint arXiv:2410.02089 , 2024.
[215] S. Dou, Y . Liu, H. Jia, L. Xiong, E. Zhou, W. Shen, J. Shan,
C. Huang, X. Wang, X. Fan et al. , “Stepcoder: Improve code generation
with reinforcement learning from compiler feedback,” arXiv preprint
arXiv:2402.01391 , 2024.
[216] B. Steenhoek, M. Tufano, N. Sundaresan, and A. Svyatkovskiy, “Rein-
forcement learning from automatic feedback for high-quality unit test
generation,” arXiv preprint arXiv:2310.02368 , 2023.
[217] Z. Ren et al., “Deep reinforcement learning-based image captioning
with embedding reward,” in Proceedings of the IEEE conference on
computer vision and pattern recognition , 2017, pp. 290–298.
[218] L. Zhang et al., “Actor-critic sequence training for image captioning,”
arXiv preprint arXiv:1706.09601 , 2017.
[219] L. Miao et al., “Multi-modal product title compression,” Information
Processing & Management , vol. 57, no. 1, p. 102123, 2020.

--- PAGE 29 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 29
[220] X. Shen, B. Liu, Y . Zhou, J. Zhao, and M. Liu, “Remote sensing image
captioning via variational autoencoder and reinforcement learning,”
Knowledge-Based Systems , vol. 203, p. 105920, 2020.
[221] J. Shi, Y . Li, and S. Wang, “Partial off-policy learning: Balance
accuracy and diversity for human-oriented image captioning,” in Pro-
ceedings of the IEEE/CVF International Conference on Computer
Vision , 2021, pp. 2187–2196.
[222] W. Nie, J. Li, N. Xu, A.-A. Liu, X. Li, and Y . Zhang, “Triangle-
reward reinforcement learning: a visual-linguistic semantic alignment
for image captioning,” in Proceedings of the 29th ACM international
conference on multimedia , 2021, pp. 4510–4518.
[223] R. Dess `ı, M. Bevilacqua, E. Gualdoni, N. C. Rakotonirina, F. Franzon,
and M. Baroni, “Cross-domain image captioning with discriminative
finetuning,” in Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , 2023, pp. 6935–6944.
[224] J. Zhang et al., “Goal-oriented visual question generation via intermedi-
ate rewards,” in Proceedings of the European Conference on Computer
Vision (ECCV) , 2018, pp. 186–201.
[225] Z. Zhao et al., “Open-ended video question answering via multi-
modal conditional adversarial networks,” IEEE Transactions on Image
Processing , vol. 29, pp. 3859–3870, 2020.
[226] R. Saqur and K. Narasimhan, “Multimodal graph networks for com-
positional generalization in visual question answering,” Advances in
Neural Information Processing Systems , vol. 33, pp. 3070–3081, 2020.
[227] F. Liu, G. Xu, Q. Wu, Q. Du, W. Jia, and M. Tan, “Cascade reasoning
network for text-based visual question answering,” in Proceedings of
the 28th ACM International Conference on Multimedia , 2020, pp.
4060–4069.
[228] J. Zhang et al., “Sch-gan: Semi-supervised cross-modal hashing by
generative adversarial network,” IEEE transactions on cybernetics ,
vol. 50, no. 2, pp. 489–502, 2018.
[229] K. Black et al., “Training diffusion models with reinforcement learn-
ing,” arXiv preprint arXiv:2305.13301 , 2023.
[230] Y . Fan et al., “Dpok: Reinforcement learning for fine-tuning text-to-
image diffusion models,” arXiv preprint arXiv:2305.16381 , 2023.
[231] Y . Akizuki et al., “Generative modelling with design constraints–
reinforcement learning for object generation,” in RE: Anthropocene,
Design in the Age of Humans–Proceedings of the 25th CAADRIA
Conference , vol. 1. Association for Computer Aided Architectural
Design Research in Asia, 2020, pp. 445–454.
[232] A. Ostonov et al., “Rlss: A deep reinforcement learning algorithm for
sequential scene generation,” in Proceedings of the IEEE/CVF Winter
Conference on Applications of Computer Vision , 2022, pp. 2219–2228.
[233] R. Zhang et al., “Qinet: Decision surface learning and adversar-
ial enhancement for quasi-immune completion of diverse corrupted
point clouds,” IEEE Transactions on Geoscience and Remote Sensing ,
vol. 60, pp. 1–14, 2022.
[234] Z. Zhang et al., “Point cloud scene completion with joint color and
semantic estimation from single rgb-d image,” IEEE Transactions on
Pattern Analysis and Machine Intelligence , 2023.
[235] C. Lin, T. Fan, W. Wang, and M. Nießner, “Modeling 3d shapes
by reinforcement learning,” in Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
Part X 16 . Springer, 2020, pp. 545–561.
[236] K. Zhao, Y . Zhang, S. Wang, T. Beeler, and S. Tang, “Synthesizing
diverse human motions in 3d indoor scenes,” in Proceedings of the
IEEE/CVF International Conference on Computer Vision , 2023, pp.
14 738–14 749.
[237] L. Siyao, W. Yu, T. Gu, C. Lin, Q. Wang, C. Qian, C. C. Loy,
and Z. Liu, “Bailando: 3d dance generation by actor-critic gpt with
choreographic memory,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , 2022, pp. 11 050–11 059.
[238] Z. Khan, V . K. BG, S. Schulter, Y . Fu, and M. Chandraker, “Self-
training large language models for improved visual program synthesis
with visual reinforcement,” in Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition , 2024, pp. 14 344–
14 353.
[239] D. Deng, A. Wu, H. Qu, and Y . Wu, “Dashbot: Insight-driven dashboard
generation based on deep reinforcement learning,” IEEE Transactions
on Visualization and Computer Graphics , vol. 29, no. 1, pp. 690–700,
2022.
[240] T. Liu, Q. Meng, J.-J. Huang, A. Vlontzos, D. Rueckert, and B. Kainz,
“Video summarization through reinforcement learning with a 3d spatio-
temporal u-net,” IEEE transactions on image processing , vol. 31, pp.
1573–1586, 2022.
[241] Y . Yu, J. Chung, H. Yun, J. Hessel, J. S. Park, X. Lu, R. Zellers, P. Am-
manabrolu, R. Le Bras, G. Kim et al. , “Fusing pre-trained languagemodels with multimodal prompts through reinforcement learning,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition , 2023, pp. 10 845–10 856.
[242] Y . Zhai, H. Bai, Z. Lin, J. Pan, S. Tong, Y . Zhou, A. Suhr, S. Xie,
Y . LeCun, Y . Ma et al. , “Fine-tuning large vision-language models
as decision-making agents via reinforcement learning,” arXiv preprint
arXiv:2405.10292 , 2024.
[243] P. H. Seo, P. Sharma, T. Levinboim, B. Han, and R. Soricut, “Rein-
forcing an image caption generator using off-line human feedback,” in
Proceedings of the AAAI Conference on Artificial Intelligence , vol. 34,
no. 03, 2020, pp. 2693–2700.
[244] H. De Vries et al., “Guesswhat?! visual object discovery through multi-
modal dialogue,” in Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition , 2017, pp. 5503–5512.
[245] D. S. R. Mohan et al., “Incremental text to speech for neural sequence-
to-sequence models using reinforcement learning,” arXiv preprint
arXiv:2008.03096 , 2020.
[246] R. Liu et al., “Reinforcement learning for emotional text-to-speech
synthesis with improved emotion discriminability,” arXiv preprint
arXiv:2104.01408 , 2021.
[247] J. Gibson et al., “A reinforcement learning approach to speech coding,”
Information , vol. 13, no. 7, p. 331, 2022.
[248] Z. Li et al., “A symbolic-domain music generation method based on
leak-gan,” in 2021 3rd International Academic Exchange Conference
on Science and Technology Innovation (IAECST) . IEEE, 2021, pp.
549–552.
[249] J. Guo et al., “Long text generation via adversarial training with
leaked information,” in Proceedings of the AAAI conference on artificial
intelligence , vol. 32, no. 1, 2018.
[250] E. Putin et al., “Reinforced adversarial neural computer for de novo
molecular design,” Journal of chemical information and modeling ,
vol. 58, no. 6, pp. 1194–1204, 2018.
[251] M. Popova et al., “Deep reinforcement learning for de novo drug
design,” Science advances , vol. 4, no. 7, p. eaap7885, 2018.
[252] E. Putin et al., “Adversarial threshold neural computer for molecular
de novo design,” Molecular pharmaceutics , vol. 15, no. 10, pp. 4386–
4397, 2018.
[253] N. De Cao et al., “Molgan: An implicit generative model for small
molecular graphs,” arXiv preprint arXiv:1805.11973 , 2018.
[254] M. Goel et al., “Molegular: molecule generation using reinforcement
learning with alternating rewards,” Journal of Chemical Information
and Modeling , vol. 61, no. 12, pp. 5815–5826, 2021.
[255] T. Blaschke et al., “Memory-assisted reinforcement learning for diverse
molecular de novo design,” Journal of cheminformatics , vol. 12, no. 1,
pp. 1–17, 2020.
[256] M. Thomas et al., “Augmented hill-climb increases reinforcement
learning efficiency for language-based de novo molecule generation,”
Journal of Cheminformatics , vol. 14, no. 1, pp. 1–22, 2022.
[257] N. Brown et al., “Guacamol: benchmarking models for de novo
molecular design,” Journal of chemical information and modeling ,
vol. 59, no. 3, pp. 1096–1108, 2019.
[258] B. Chen et al., “Fragment-based sequential translation for molecular
optimization,” arXiv preprint arXiv:2111.01009 , 2021.
[259] R. Ishitani et al., “Molecular design method using a reversible tree rep-
resentation of chemical compounds and deep reinforcement learning,”
Journal of Chemical Information and Modeling , vol. 62, no. 17, pp.
4032–4048, 2022.
[260] T. Fu et al., “Reinforced genetic algorithm for structure-based drug
design,” Advances in Neural Information Processing Systems , vol. 35,
pp. 12 325–12 338, 2022.
[261] P. Hu et al., “De novo drug design based on stack-rnn with multi-
objective reward-weighted sum and reinforcement learning,” Journal
of Molecular Modeling , vol. 29, no. 4, pp. 1–12, 2023.
[262] M. Sun et al., “Molsearch: Search-based multi-objective molecular
generation and property optimization,” in Proceedings of the 28th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining , 2022,
pp. 4724–4732.
[263] X. Liu et al., “Drugex v3: scaffold-constrained drug design with graph
transformer-based reinforcement learning,” Journal of Cheminformat-
ics, vol. 15, no. 1, p. 24, 2023.
[264] G. Simm, R. Pinsler, and J. M. Hern ´andez-Lobato, “Reinforcement
learning for molecular design guided by quantum mechanics,” in
International Conference on Machine Learning . PMLR, 2020, pp.
8959–8969.
[265] A. Zholus, M. Kuznetsov, R. Schutski, R. Shayakhmetov,
D. Polykovskiy, S. Chandar, and A. Zhavoronkov, “Bindgpt: A

--- PAGE 30 ---
JOURNAL OF L ATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 30
scalable framework for 3d molecular design via language modeling
and reinforcement learning,” arXiv preprint arXiv:2406.03686 , 2024.
[266] Z. Zhou et al., “Optimizing chemical reactions with deep reinforcement
learning,” ACS central science , vol. 3, no. 12, pp. 1337–1344, 2017.
[267] A. A. V olk, R. W. Epps, D. T. Yonemoto, B. S. Masters, F. N.
Castellano, K. G. Reyes, and M. Abolhasani, “Alphaflow: autonomous
discovery and optimization of multi-step chemistry using a self-driven
fluidic lab guided by reinforcement learning,” Nature Communications ,
vol. 14, no. 1, p. 1403, 2023.
[268] S. K. Gottipati, B. Sattarov, S. Niu, Y . Pathak, H. Wei, S. Liu, S. Black-
burn, K. Thomas, C. Coley, J. Tang et al. , “Learning to navigate the
synthetically accessible chemical space using reinforcement learning,”
inInternational conference on machine learning . PMLR, 2020, pp.
3668–3679.
[269] P. Rajak, A. Krishnamoorthy, A. Mishra, R. Kalia, A. Nakano, and
P. Vashishta, “Autonomous reinforcement learning agent for chemical
vapor deposition synthesis of quantum materials,” npj Computational
Materials , vol. 7, no. 1, p. 108, 2021.
[270] K. M. Powell, D. Machalek, and T. Quah, “Real-time optimization
using reinforcement learning,” Computers & Chemical Engineering ,
vol. 143, p. 107077, 2020.
[271] P. C. Nguyen, N. N. Vlassis, B. Bahmani, W. Sun, H. Udaykumar, and
S. S. Baek, “Synthesizing controlled microstructures of porous media
using generative adversarial networks and reinforcement learning,”
Scientific reports , vol. 12, no. 1, p. 9034, 2022.
[272] E.-J. Kuo et al., “Quantum architecture search via deep reinforcement
learning,” arXiv preprint arXiv:2104.07715 , 2021.
[273] J. Lin, Z. Y . Lai, and X. Li, “Quantum adiabatic algorithm design using
reinforcement learning,” Physical Review A , vol. 101, no. 5, p. 052327,
2020.
[274] M. Ostaszewski, L. M. Trenkwalder, W. Masarczyk, E. Scerri, and
V . Dunjko, “Reinforcement learning for optimization of variational
quantum circuit architectures,” Advances in Neural Information Pro-
cessing Systems , vol. 34, pp. 18 182–18 194, 2021.
[275] L. Moro, M. G. Paris, M. Restelli, and E. Prati, “Quantum compiling by
deep reinforcement learning,” Communications Physics , vol. 4, no. 1,
p. 178, 2021.
[276] B. Sanchez-Lengeling et al., “Optimizing distributions over molecu-
lar space. an objective-reinforced generative adversarial network for
inverse-design chemistry (organic),” 2017.
[277] M. Olivecrona et al., “Molecular de-novo design through deep rein-
forcement learning,” Journal of cheminformatics , vol. 9, no. 1, pp.
1–14, 2017.
[278] S. R. Atance et al., “De novo drug design using reinforcement learn-
ing with graph-based deep generative models,” Journal of Chemical
Information and Modeling , vol. 62, no. 20, pp. 4863–4872, 2022.
[279] G. Brockman et al., “Openai gym,” arXiv preprint arXiv:1606.01540 ,
2016.
[280] X. et al. Chen, “A survey of deep reinforcement learning in
recommender systems: A systematic review and future directions,”
CoRR , vol. abs/2109.03540, 2021. [Online]. Available: https://arxiv.
org/abs/2109.03540
[281] S. et al. Wang, “Causal decision transformer for recommender systems
via offline reinforcement learning,” 2023.
[282] W. et al. Huang, “V oxposer: Composable 3d value maps for robotic
manipulation with language models,” 2023.
[283] G. et al. Wang, “V oyager: An open-ended embodied agent with large
language models,” 2023.
[284] A. Khalifa et al., “Pcgrl: Procedural content generation via reinforce-
ment learning,” in Proceedings of the AAAI Conference on Artificial
Intelligence and Interactive Digital Entertainment , vol. 16, no. 1, 2020,
pp. 95–101.
[285] D. Ha et al., “Reinforcement learning for improving agent design,”
Artificial life , vol. 25, no. 4, pp. 352–365, 2019.
[286] V .-A. Darvariu et al., “Goal-directed graph construction using rein-
forcement learning,” Proceedings of the Royal Society A , vol. 477, no.
2254, p. 20210168, 2021.
[287] O. Bar El, T. Milo, and A. Somech, “Automatically generating data ex-
ploration sessions using deep reinforcement learning,” in Proceedings
of the 2020 ACM SIGMOD international conference on management
of data , 2020, pp. 1527–1537.
[288] L. et al. Choshen, “On the weaknesses of reinforcement learning for
neural machine translation,” in International Conference on Learning
Representations , 2020. [Online]. Available: https://openreview.net/
forum?id=H1eCw3EKvH[289] S. Kiegeland et al., “Revisiting the weaknesses of reinforcement learn-
ing for neural machine translation,” arXiv preprint arXiv:2106.08942 ,
2021.
[290] R. Ramamurthy et al., “Is reinforcement learning (not) for natural lan-
guage processing?: Benchmarks, baselines, and building blocks for nat-
ural language policy optimization,” arXiv preprint arXiv:2210.01241 ,
2022.
[291] D. Donato et al., “Mad for robust reinforcement learning in machine
translation,” arXiv preprint arXiv:2207.08583 , 2022.
[292] U. Honda et al., “Switching to discriminative image captioning by
relieving a bottleneck of reinforcement learning,” in Proceedings of
the IEEE/CVF Winter Conference on Applications of Computer Vision ,
2023, pp. 1124–1134.
[293] W. Jin et al., “Multi-objective molecule generation using interpretable
substructures,” in International conference on machine learning .
PMLR, 2020, pp. 4849–4859.
[294] H. Guo et al., “Efficient (soft) q-learning for text generation with
limited good data,” in Findings of the Association for Computational
Linguistics: EMNLP 2022 , 2022, pp. 6969–6991.
[295] M. Korshunova et al., “Generative and reinforcement learning ap-
proaches for the automated de novo design of bioactive compounds,”
Communications Chemistry , vol. 5, no. 1, p. 129, 2022.
[296] Z. Yao et al., “An imitation game for learning semantic parsers from
user interaction,” arXiv preprint arXiv:2005.00689 , 2020.
[297] A. S. Vezhnevets et al., “Feudal networks for hierarchical reinforcement
learning,” in International Conference on Machine Learning . PMLR,
2017, pp. 3540–3549.
[298] T. Zhao et al., “A multi-scenario text generation method based on meta
reinforcement learning,” Pattern Recognition Letters , vol. 165, pp. 47–
54, 2023.
[299] Z. Zhao, W. S. Lee, and D. Hsu, “Large language models as common-
sense knowledge for large-scale task planning,” Advances in Neural
Information Processing Systems , vol. 36, 2024.
[300] D. Zhang, S. Zhoubian, Y . Yue, Y . Dong, and J. Tang, “Rest-mcts*:
Llm self-training via process reward guided tree search,” arXiv preprint
arXiv:2406.03816 , 2024.
[301] S. Ross et al., “A reduction of imitation learning and structured pre-
diction to no-regret online learning,” in Proceedings of the fourteenth
international conference on artificial intelligence and statistics . JMLR
Workshop and Conference Proceedings, 2011, pp. 627–635.
[302] T. Hospedales et al., “Meta-learning in neural networks: A survey,”
IEEE transactions on pattern analysis and machine intelligence ,
vol. 44, no. 9, pp. 5149–5169, 2021.
[303] O. et al. Ended Learning Team, “Open-ended learning leads to
generally capable agents,” CoRR , vol. abs/2107.12808, 2021. [Online].
Available: https://arxiv.org/abs/2107.12808
[304] C. Finn et al., “Model-agnostic meta-learning for fast adaptation of
deep networks,” in International conference on machine learning .
PMLR, 2017, pp. 1126–1135.
[305] H. et al. Liu, “Evaluating the logical reasoning ability of chatgpt and
gpt-4,” 2023.
[306] A. Tripp et al., “Sample-efficient optimization in the latent space of
deep generative models via weighted retraining,” Advances in Neural
Information Processing Systems , vol. 33, pp. 11 259–11 272, 2020.
[307] B. Sch ¨olkopf et al., “Toward causal representation learning,” Proceed-
ings of the IEEE , vol. 109, no. 5, pp. 612–634, 2021.
