# 2311.05596.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2311.05596.pdf
# Kích thước tệp: 6315269 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Các Tác tử Phân cấp được Tăng cường bởi LLM
Bharat Prakash1, Tim Oates1, Tinoosh Mohsenin1, 2
Đại học Maryland, Baltimore County1
Đại học Johns Hopkins2
Tóm tắt
Giải quyết các nhiệm vụ dài hạn, mở rộng về mặt thời gian bằng Học Tăng cường
(RL) là một thách thức, được phức tạp hóa bởi thông lệ thông thường của việc học
mà không có kiến thức trước (hoặc học tabula rasa). Con người có thể tạo ra và
thực hiện các kế hoạch với các hành động mở rộng về mặt thời gian và nhanh chóng
học cách thực hiện các nhiệm vụ mới vì chúng ta hầu như không bao giờ giải quyết
vấn đề từ đầu. Chúng tôi muốn các tác tử tự động có khả năng tương tự này. Gần
đây, các LLM đã được chứng minh là mã hóa một lượng kiến thức khổng lồ về thế
giới và thực hiện việc học trong ngữ cảnh và lý luận ấn tượng. Tuy nhiên, sử dụng
LLM để giải quyết các vấn đề thế giới thực là khó khăn vì chúng không được định
hướng trong nhiệm vụ hiện tại. Trong bài báo này, chúng tôi khai thác khả năng
lập kế hoạch của LLM trong khi sử dụng RL để cung cấp việc học từ môi trường,
tạo ra một tác tử phân cấp sử dụng LLM để giải quyết các nhiệm vụ dài hạn. Thay
vì hoàn toàn dựa vào LLM, chúng hướng dẫn một chính sách cấp cao, làm cho việc
học trở nên hiệu quả hơn đáng kể về mặt mẫu. Phương pháp này được đánh giá
trong các môi trường mô phỏng như MiniGrid, SkillHack, và Crafter, và trên một
cánh tay robot thực trong các nhiệm vụ thao tác khối. Chúng tôi cho thấy rằng các
tác tử được huấn luyện bằng phương pháp của chúng tôi vượt trội hơn các phương
pháp cơ sở khác và, một khi được huấn luyện, không cần truy cập vào LLM trong
quá trình triển khai.

1 Giới thiệu
Con người có thể tạo ra và thực hiện các kế hoạch với các hành động mở rộng về mặt thời gian để thực hiện các nhiệm vụ phức tạp trong một thế giới động và không chắc chắn. Chúng tôi muốn các tác tử tự động có những khả năng tương tự. Những nỗ lực kỹ thuật khổng lồ có thể dẫn đến các tác tử cực kỳ mạnh mẽ, chẳng hạn như xe rover trong không gian, và robot phẫu thuật và công nghiệp. Trong trường hợp không có những tài nguyên như vậy, các kỹ thuật như Học Tăng cường (RL) có thể được sử dụng để trích xuất các chính sách điều khiển mạnh mẽ từ kinh nghiệm. Tuy nhiên, RL có nhiều thách thức, chẳng hạn như khám phá dưới phần thưởng thưa thớt, tổng quát hóa, an toàn, v.v. Điều này làm cho việc học các chính sách tốt một cách hiệu quả về mặt mẫu trở nên khó khăn. Các cách phổ biến để giải quyết những vấn đề này bao gồm sử dụng phản hồi từ chuyên gia [7,27] và tận dụng cấu trúc phân cấp của các nhiệm vụ phức tạp. Có nhiều nghiên cứu trước đó đáng kể về việc học các chính sách phân cấp để chia nhỏ các nhiệm vụ thành các nhiệm vụ con nhỏ hơn [25, 10, 3].

Học Tăng cường Phân cấp (HRL) thực sự giảm thiểu một số vấn đề được đề cập ở trên. Tuy nhiên, khi số lượng tùy chọn hoặc kỹ năng tăng lên, chúng ta lại đối mặt với một số vấn đề tương tự. Sử dụng một số hình thức giám sát, chẳng hạn như cung cấp chi tiết về các nhiệm vụ con hoặc phần thưởng trung gian hoặc hướng dẫn cấp cao của con người, là một phương pháp [21, 16, 18].

Một trong những lý do khiến con người rất giỏi trong việc đối phó với các tình huống không quen thuộc là chúng ta hầu như không bao giờ giải quyết vấn đề từ đầu. Khi được đưa ra một nhiệm vụ mới và một thư viện kỹ năng, chúng ta có thể chọn một tập hợp con các kỹ năng có vẻ phù hợp nhất và khám phá từ đó. Chúng ta có thể thực hiện một số khám phá thử và sai (như trong RL), nhưng chúng ta nhanh chóng học được tập hợp con kỹ năng phù hợp cũng như trình tự chính xác mà chúng cần được thực hiện. Ví dụ, tay nắm cửa trên xe hơi mới hơn nằm phẳng so với cửa, không giống như hầu hết các tay nắm cửa xe hơi khác tồn tại. Điều đó gây ra vấn đề lần đầu tiên bạn cố gắng mở nó. Con người ngay lập tức thu hẹp xuống một vài hành động khám phá,

LangRob Workshop @ Conference on Robot Learning (CoRL 2023).arXiv:2311.05596v1  [cs.LG]  9 Nov 2023

--- TRANG 2 ---
Hình 1: LLM hướng dẫn chính sách cấp cao và tăng tốc việc học. Nó được nhắc với ngữ cảnh, một số ví dụ, và nhiệm vụ cũng như quan sát hiện tại. Đầu ra của LLM thiên lệch việc lựa chọn hành động cấp cao

như cố gắng đưa ngón tay vào dưới tay nắm hoặc ấn vào nó ở những vị trí khác nhau. Chúng ta không, không giống như hầu hết các thuật toán RL có thể làm, gõ vào cửa sổ hoặc kéo gương bên, vì chúng ta tin rằng những tùy chọn như vậy không liên quan về mặt nhân quả dựa trên kiến thức sâu sắc về thế giới.

Các mô hình ngôn ngữ lớn (LLM) đã được chứng minh là mã hóa một lượng kiến thức khổng lồ về thế giới nhờ vào việc được huấn luyện trên lượng lớn văn bản. Chúng tôi đưa ra giả thuyết rằng kiến thức này có thể được tận dụng để tập trung việc huấn luyện các chính sách phân cấp, làm cho chúng hiệu quả hơn đáng kể về mặt mẫu. Cụ thể, chúng tôi khám phá cách các mô hình ngôn ngữ được huấn luyện trước lớn có thể được sử dụng để đưa các tiên nghiệm thông thường vào các tác tử phân cấp.

Trong phương pháp này, chúng tôi giả định quyền truy cập vào một số kỹ năng cấp thấp. Chúng có thể là, ví dụ, các bộ lập kế hoạch được thiết kế hoặc các chính sách được học bằng RL và phần thưởng nhiệm vụ con. Dựa trên mô tả nhiệm vụ cấp cao và trạng thái hiện tại, LLM hướng dẫn tác tử bằng cách đề xuất các hướng hành động có khả năng nhất. Thay vì khám phá ngẫu nhiên, chúng tôi sử dụng những đề xuất này để khám phá thông minh các tùy chọn khác nhau. Vì LLM không được định hướng trong miền, chúng chỉ được sử dụng để thiên lệch việc lựa chọn hành động và ảnh hưởng của chúng được giảm khi quá trình huấn luyện tiến triển. Điều này dẫn đến một chính sách có thể được triển khai mà không phụ thuộc vào LLM trong thời gian chạy. Chúng tôi đánh giá phương pháp này trên một số môi trường mô phỏng (MiniGrid [6], SkillHack [19], và Crafter [13]), cho thấy rằng nó có thể học cách giải quyết các nhiệm vụ phức tạp, dài hạn nhanh hơn nhiều so với các phương pháp cơ sở. Các thí nghiệm với cánh tay robot thực trong các nhiệm vụ thao tác khối sử dụng phiên bản Q-learning dạng bảng của cùng một thuật toán cho thấy rằng nó có thể học các chính sách nhanh hơn nhiều với ít kinh nghiệm hơn trong miền đó. Những đóng góp của chúng tôi được tóm tắt như sau:

• một phương pháp sử dụng LLM để hướng dẫn khám phá bằng cách trích xuất các tiên nghiệm thông thường;
• một tác tử phân cấp sử dụng những tiên nghiệm này để giải quyết các nhiệm vụ dài hạn;
• một đánh giá khung trong mô phỏng cũng như một môi trường thế giới thực đơn giản, cho thấy rằng nó hoạt động tốt hơn đáng kể so với các phương pháp cơ sở;
• một thảo luận về (1) những lợi thế của phương pháp chúng tôi so với các nghiên cứu trước và (2) các nghiên cứu tương lai tiềm năng.

2 Nghiên cứu liên quan
Ngôn ngữ và HRL Có nhiều nghiên cứu trước đó đáng kể về RL phân cấp trong đó MDP tiêu chuẩn được chuyển đổi thành một quá trình quyết định bán-Markov (SMDP). Phương pháp phổ biến nhất là kết hợp các hành động mở rộng về mặt thời gian, còn được gọi là tùy chọn hoặc kỹ năng [3]. Thông thường, một chính sách cấp thấp đạt được các nhiệm vụ con bằng cách thực hiện các hành động nguyên thủy và một chính sách cấp cao lập kế hoạch về các tùy chọn hoặc kỹ năng mở rộng về mặt thời gian. Ngôn ngữ tự nhiên là một cách phổ biến để chỉ định các nhiệm vụ con và đạt được sự tổng quát hóa do tính chất kết hợp và cấu trúc phân cấp vốn có của nó [16,21,28,14]. Hầu hết các phương pháp này chỉ định hoặc tạo ra một kế hoạch cấp cao bằng ngôn ngữ tự nhiên, sau đó được thực hiện tuần tự bởi một chính sách cấp thấp riêng biệt. Những phương pháp này đối mặt với thách thức khi hoạt động trong không gian quan sát có chiều cao. Chúng cũng dựa vào việc thu thập dữ liệu thủ công để huấn luyện các chính sách cấp cao và do đó khó tổng quát hóa cho các nhiệm vụ mới [2, 20, 12].

--- TRANG 3 ---
RL và Mô hình Nền tảng Gần đây, các mô hình ngôn ngữ lớn như GPT-3 đã được sử dụng để xây dựng các tác tử có khả năng hành động trong thế giới thực dựa trên hướng dẫn ngôn ngữ [5,4]. Việc học trong ngữ cảnh và các chiến lược nhắc nhở thông minh được hỗ trợ bởi những mô hình này đã được sử dụng để thiết kế các tác tử phân cấp được hướng dẫn bởi ngôn ngữ. [15] sử dụng LLM như các bộ lập kế hoạch zero-shot để cho phép các tác tử được nhúng hành động trong các kịch bản thế giới thực. Tương tự, [1] sử dụng LLM cùng với các hàm khả năng để tạo ra các kế hoạch khả thi hướng dẫn robot đạt được các mục tiêu được chỉ định trong hướng dẫn ngôn ngữ tự nhiên. Nghiên cứu của chúng tôi có liên quan chặt chẽ với [8], nơi họ cải thiện việc khám phá bằng cách sử dụng LLM để cung cấp phần thưởng trung gian và khuyến khích tác tử tìm kiếm các trạng thái mới.

3 Phương pháp
3.1 Phát biểu vấn đề
Chúng tôi xem xét một hệ thống nhận hướng dẫn dưới dạng ngôn ngữ tự nhiên mô tả một nhiệm vụ, tương tự như [1]. Các hướng dẫn có thể dài, có thể chứa cảnh báo và ràng buộc, và có thể không bao gồm tất cả các bước cá nhân cần thiết. Chúng tôi cũng giả định rằng tác tử có quyền truy cập vào một tập hợp hữu hạn các kỹ năng hoặc chính sách con có thể được thực hiện theo trình tự để giải quyết các nhiệm vụ dài hạn. Những kỹ năng này có thể được mã hóa thủ công, hoặc được huấn luyện bằng học tăng cường hoặc học mô phỏng với thiết kế phần thưởng thủ công. Chúng phải được đi kèm với một mô tả đơn giản bằng ngôn ngữ tự nhiên, chẳng hạn như "nhặt khối đỏ" hoặc "mở cửa xanh". Chúng cũng phải có khả năng phát hiện việc hoàn thành nhiệm vụ con để chuyển quyền điều khiển trở lại chính sách cấp cao. Với một tập hợp hữu hạn các tùy chọn hoặc kỹ năng, mục tiêu của chúng tôi là có được một chính sách quyết định cấp cao lựa chọn trong số chúng.

3.2 Sử dụng LLM để Hướng dẫn Chính sách Cấp cao
Phần này giới thiệu phương pháp của chúng tôi để sử dụng LLM cải thiện việc khám phá trong chính sách cấp cao của hệ thống HRL. Kiến thức ngữ nghĩa và khả năng lập kế hoạch của LLM cải thiện việc lựa chọn hành động cấp cao khi được đưa ra mô tả nhiệm vụ và trạng thái hiện tại dưới dạng ngôn ngữ. Ý tưởng cốt lõi là sử dụng LLM để có được một giá trị xấp xỉ xác suất mà một kỹ năng hoặc nhiệm vụ con nhất định có liên quan để đạt được mục tiêu lớn hơn. Như đã đề cập trước đó, mỗi kỹ năng được đi kèm với một mô tả ngôn ngữ lskill và quỹ đạo hiện tại được dịch sang ngôn ngữ, ltraj. Ngoài ra còn có một hướng dẫn cấp cao, lgoal_inst, mô tả mục tiêu lớn hơn cùng với các ràng buộc tùy chọn.

LLM được sử dụng để đánh giá hàm fLLM(lskill i, lgoal_inst, ltraj) cho mỗi kỹ năng tại mỗi bước quyết định cấp cao. Về cơ bản, LLM trả lời câu hỏi sau: cho nhiệm vụ, lgoal_inst, và quỹ đạo cho đến nay, ltraj, chúng ta có nên chọn kỹ năng lskill i không? Đầu ra của LLM, 'có' hoặc 'không', có thể dễ dàng được chuyển đổi thành int ("0" hoặc "1"). Loại nhắc trả lời câu hỏi dạng đóng này đã được chứng minh hoạt động tốt hơn so với các nhắc mở [8]. Sau khi đánh giá điều này cho mỗi trong số k kỹ năng, chúng ta có FLLM = [fLLM 1, fLLM 2, fLLM 3, ..., f LLM k]. Ví dụ, FLLM = [0,1,0, ...,0,1,0,0].

Một hàm LOG SOFTMAX được áp dụng cho những logit này để có được các tiên nghiệm thông thường từ LLM được ký hiệu bởi pCS=log_softmax (FLLM). Dựa hoàn toàn vào pCS không đủ để giải quyết các nhiệm vụ phức tạp. Đồng thời, sử dụng RL và khám phá mà không có bất kỳ trực giác thông thường nào là không hiệu quả. Do đó, chúng tôi vẫn sử dụng RL và phần thưởng thưa thớt để có được các chính sách cấp cao nhưng cũng sử dụng các tiên nghiệm thông thường, pCS, từ LLM để hướng dẫn việc khám phá. Chi tiết hơn về các thuật toán RL được sử dụng có trong phần Thí nghiệm. Việc lựa chọn hành động trong chính sách khám phá lấy mẫu các hành động từ phân phối phân loại trong đó các logit được thu được bởi đầu chính sách xử lý trạng thái. Những logit này được thiên lệch với các tiên nghiệm thông thường pCS và một yếu tố trọng số λ. Vì vậy việc lựa chọn hành động trông như thế này: a=Categorical [π(st) +λ.pCS(st)]. Ở đây, hành động a là hành động vĩ mô mở rộng về mặt thời gian hoặc kỹ năng. Yếu tố trọng số bắt đầu từ λ= 1 và được giảm dần cho đến khi đạt tới không vào cuối quá trình huấn luyện. Điều này có nghĩa là tác tử được huấn luyện không tiếp tục dựa vào LLM trong quá trình triển khai. Quá trình này được tóm tắt trong Thuật toán 1 và Hình 1.

Truy vấn LLM và Thiết kế Nhắc. Chúng tôi sử dụng gpt-3.5-turbo GPT được cung cấp bởi OpenAI APIs. Để giảm số lượng cuộc gọi API, các phản hồi LLM cho tất cả các kết hợp có thể của lgoal_inst và ltraj được lưu trong bộ nhớ đệm. Một phiên bản đơn giản hóa của ltraj được sử dụng để biểu thị lịch sử quỹ đạo hiện tại bằng cách sử dụng hai hành động quá khứ. Nhắc chính được sử dụng trong các thí nghiệm của chúng tôi có cấu trúc sau Mục tiêu: lgoal_inst, Cho đến nay tôi có: ltraj, Tôi có nên lskill i không?. LLM được hiển thị một vài ví dụ về phản hồi cho những truy vấn như vậy và nhắc chỉ định rằng cần có câu trả lời một từ Có/Không. Các nhắc ví dụ có trong Phụ lục.

--- TRANG 4 ---
Thuật toán 1
high _inst←mục tiêu cấp cao bằng ngôn ngữ
πθ←chính sách cấp cao
fLLM←tiên nghiệm thông thường từ LLM
procedure LLMxHRL(high_inst)
khởi tạo πθ
while πθ chưa hội tụ do
khởi tạo τ← {}
for t←0 to T do
pCS←fLLM(high _inst, τ )
at←cat_dist[πθ(τ) +λ.pCS(τ)].sample ()
st, rt←act(at)
τ←append (st, rt, at)
end for
cập nhật πθ
end while
trả về πθ
end procedure

4 Thí nghiệm
Phần này mô tả thiết lập thí nghiệm và kết quả kiểm tra khung trong ba môi trường mô phỏng và một nhiệm vụ thao tác khối cánh tay robot thế giới thực. Khung dựa vào việc giao tiếp với LLM bằng văn bản. Như đã đề cập trước đó, mỗi kỹ năng tương ứng với một mô tả văn bản lskill i và mục tiêu lớn hơn được mô tả bằng lgoal_inst. Chúng tôi giả định quyền truy cập vào một bộ chú thích ánh xạ lịch sử quan sát hiện tại thành ltraj. Điều này có thể được tự động hóa bằng cách sử dụng các mô hình thị giác sang ngôn ngữ hiện đại như [22], nhưng điều đó được để dành cho nghiên cứu tương lai. Thay vào đó, chúng tôi sử dụng mô hình dựa trên CLIP cùng với LLM trong các thí nghiệm với robot thực để chuyển đổi đầu vào hình ảnh thành trạng thái rời rạc chiều thấp. Chi tiết hơn về việc lấy ltraj có trong Phụ lục. Trong mỗi môi trường, phương pháp của chúng tôi được so sánh với các tác tử phân cấp cơ sở không có bất kỳ hướng dẫn nào từ LLM, và một oracle và tác tử giống SayCan không có khả năng chi trả.

4.1 Thí nghiệm MiniGrid
Thiết lập Các thí nghiệm được mô tả trong phần này được thực hiện trên môi trường MiniGrid bởi [6], đây là một thế giới lưới đơn giản. Môi trường có thể được thiết kế với nhiều phòng có cửa, tường và các đối tượng mục tiêu. Những đối tượng này có thể có màu sắc khác nhau và tác tử và các đối tượng mục tiêu được sinh ra tại các vị trí ngẫu nhiên. Không gian hành động là rời rạc cho phép di chuyển theo 4 hướng la bàn, mở và đóng cửa, và nhặt và thả các đối tượng. Chúng tôi thiết kế nhiều nhiệm vụ trong thiết lập này có thể được chia nhỏ thành các nhiệm vụ con nhỏ hơn.

Phương pháp Mô tả
LLM x HRL (của chúng tôi) Sử dụng LLM để thiên lệch việc lựa chọn hành động cấp cao như được giải thích trong Phần 3. Chỉ nhận phần thưởng khi hoàn thành nhiệm vụ.
Vanilla HRL Tác tử phân cấp cơ sở không có hướng dẫn từ LLM.
Shaped HRL Giống như Vanilla HRL không có hướng dẫn LLM. Nhưng ở đây các tác tử nhận phần thưởng được định hình cho việc hoàn thành nhiệm vụ con thành công. Yêu cầu các hàm phần thưởng được thiết kế thủ công.
Oracle Đây là giới hạn trên. Chính sách cấp cao là một máy trạng thái oracle cung cấp các nhiệm vụ con đúng theo trình tự chính xác. [11]
SayCan w/o Aff Kiến trúc giống SayCan [1] nhưng không có hàm khả năng chi trả mà tin tưởng mù quáng vào LLM. Phương pháp này yêu cầu truy cập LLM trong quá trình triển khai

Bảng 1: Mô tả các phương pháp được sử dụng trong các thí nghiệm

--- TRANG 5 ---
Hình 2: Các biểu đồ cho thấy tỷ lệ thành công của các phương pháp khác nhau trên ba nhiệm vụ trong Môi trường MiniGrid.

• Nhiệm vụ UnlockReach bao gồm một đối tượng ngẫu nhiên trong một phòng nằm sau một cánh cửa bị khóa. Tác tử phải trước tiên tìm chìa khóa đúng dựa trên màu cửa, mở khóa cửa, và sau đó di chuyển đến đối tượng mục tiêu.

• Nhiệm vụ KeyCorridor v0 bao gồm một hành lang với nhiều phòng ở hai bên. Một đối tượng mục tiêu nằm trong một phòng bị khóa có chìa khóa trong một phòng khác. Tác tử phải trước tiên tìm chìa khóa và sau đó mở khóa cửa để cuối cùng đến được mục tiêu.

• KeyCorridor v1 tương tự như v0, nhưng một số phòng có chìa khóa bị lỗi. Hướng dẫn mục tiêu đi kèm với các phòng cần tránh. Nhiệm vụ này khó khăn hơn nhiều đối với các phương pháp HRL tiêu chuẩn.

Mỗi nhiệm vụ có một phần thưởng duy nhất chỉ được cung cấp khi hoàn thành nhiệm vụ thành công. Các tác tử có quyền truy cập vào một số kỹ năng mở rộng về mặt thời gian: GoToObject, PickupObject, UnlockDoor, và OpenBox. Những kỹ năng này được điều kiện hóa theo loại và màu sắc của các đối tượng. Ví dụ Object có thể chỉ một key, ball, hoặc box, và màu sắc có thể là red, green, blue, yellow, v.v. Những nhiệm vụ con cấp thấp này đã được huấn luyện trước và đóng băng sử dụng PPO [24] và đặc tả phần thưởng thủ công. Các chính sách cấp cao cũng được huấn luyện bằng PPO trong đó các nhiệm vụ con được coi như các hành động. Chúng tôi so sánh với Vanilla HRL, Shaped HRL, một Oracle, và phương pháp giống SayCan như được mô tả trong Bảng 1. Kết quả được tóm tắt trong Hình 2. Rõ ràng là phương pháp của chúng tôi vượt trội hơn cả hai phương pháp HRL cơ sở có và không có phần thưởng được định hình. Nó cũng có thể hội tụ đến chính sách tối ưu sớm hơn nhiều so với các phương pháp khác. Oracle và SayCan không được huấn luyện bằng RL nên chúng tôi hiển thị hiệu suất của chúng bằng các đường ngang. Mặc dù chúng có thể so sánh với phương pháp của chúng tôi, một lợi ích của phương pháp chúng tôi là nó không dựa vào LLM trong quá trình triển khai.

4.2 SkillHack
Môi trường Học NetHack [17] là một môi trường RL dựa trên trò chơi kinh điển NetHack. Nó nổi tiếng khó khăn vì số lượng lớn các thực thể và hành động, tạo ra theo thủ tục, và bản chất ngẫu nhiên của trò chơi. MiniHack [23] và SkillHack [19] là các phần mở rộng của NetHack cho phép tạo ra các cấp độ và nhiệm vụ tùy chỉnh. Chúng đơn giản hơn trò chơi đầy đủ trong khi vẫn giữ lại hầu hết các phức tạp thú vị. Bộ SkillHack chứa 16 kỹ năng như PickUp, Navigate, Fight, Wear, Weild, Zap, Apply, v.v. Chi tiết hơn có trong Phụ lục. Những kỹ năng này có thể được thực hiện tuần tự để đạt được các nhiệm vụ lớn hơn. Chúng tôi xem xét hai nhiệm vụ như vậy - Battle, FrozenLavaCross.

• Trong nhiệm vụ Battle, tác tử cần PickUp một Sword được đặt ngẫu nhiên, Wield Sword và cuối cùng Fight và giết một Monster.

• Trong nhiệm vụ FrozenLavaCross, tác tử cần PickUp một WandOfCold hoặc FrostHorn dựa trên những gì có sẵn, sau đó tạo ra một cây cầu qua dung nham bằng ZapWandOfCold hoặc ApplyFrostHorn. Cuối cùng, tác tử phải NavigateLava qua cây cầu mới tạo để đến cầu thang ở phía bên kia.

Trong môi trường này, chúng tôi so sánh với Vanilla HRL và chính sách cấp cao Oracle. Các kỹ năng cấp thấp được huấn luyện bằng IMPALA [9] với mã được cung cấp bởi [19]. Chính sách cấp cao cũng được huấn luyện bằng IMPALA trong đó các kỹ năng chính sách là các hành động vĩ mô. Như thấy trong hai biểu đồ đầu tiên trong Hình 3, ở cả hai nhiệm vụ Battle và FrozenLavaCross, phương pháp của chúng tôi rõ ràng vượt trội hơn tác tử HRL không có hướng dẫn LLM.

--- TRANG 6 ---
Hình 3: 2 biểu đồ bên trái cho thấy tỷ lệ thành công của các phương pháp khác nhau trên SkillHack - Battle và Frozen Lava Cross. 2 biểu đồ bên phải cho thấy tỷ lệ thành công của các phương pháp khác nhau trên Crafter - Get Stone và Make Stone Pickaxe

4.3 Crafter
Crafter [13] là phiên bản 2D của Minecraft có cùng động lực phức tạp nhưng với không gian quan sát đơn giản hơn và tốc độ mô phỏng nhanh hơn. Tương tự như Minecraft, nó liên quan đến việc thu thập và xây dựng các vật phẩm dọc theo cây thành tích. Chúng tôi sửa đổi trò chơi một chút để làm cho nó dễ dàng hơn bằng cách làm chậm sự suy giảm sức khỏe và có ít nguy hiểm hơn để chiến đấu. Chúng tôi đánh giá trên hai nhiệm vụ có cấu trúc phân cấp tự nhiên - MakeWoodPickaxe và MakeStonePickaxe. Chi tiết hơn có trong Phụ lục. Tương tự như các thí nghiệm khác của chúng tôi, chúng tôi huấn luyện trước các chính sách cho nhiều kỹ năng bằng PPO. Chính sách cấp cao sau đó được huấn luyện để lựa chọn trong số những kỹ năng này. Hai biểu đồ cuối cùng trong Hình 3 cho thấy phương pháp của chúng tôi hoạt động tốt hơn phương pháp HRL cơ sở.

4.4 Thí nghiệm Robot Thực uArm
Chúng tôi cũng kiểm tra trên cánh tay robot thực trên phiên bản Q-learning dạng bảng đơn giản hơn của phương pháp chúng tôi. uArm Swift Pro [26] là một cánh tay robot để bàn mã nguồn mở.

Hình 4: Kết quả Cánh tay Robot Chúng tôi thiết kế hai nhiệm vụ thao tác khối - DeskCleanUp và SwapBlocks. Tương tự như các thí nghiệm mô phỏng trước đó, chúng tôi giả định quyền truy cập vào các kỹ năng khác nhau có thể được sử dụng để giải quyết các nhiệm vụ lớn hơn, phức tạp hơn. Trong thiết lập của chúng tôi, video từ camera được sử dụng để chuyển đổi vị trí cánh tay robot và khối thành một mảng các giá trị rời rạc đại diện cho trạng thái. Từ trạng thái đơn giản hóa này, chúng tôi có thể học một chính sách cấp cao với Q-learning dạng bảng. Giống như trước, chúng tôi tính pCS= log_softmax (FLLM) bằng LLM và truy cập vào fLLM(lskill i, lgoal_inst, ltraj). Tham khảo Phụ lục để biết chi tiết hơn.

Hình 4 cho thấy kết quả của các thí nghiệm chúng tôi. Trong nhiệm vụ DeskCleanUp, có ba vị trí với một khay và hai khối (đỏ và xanh lá). Tập được khởi tạo với các khối ở các vị trí ngẫu nhiên. Mục tiêu là nhặt các khối và đặt chúng vào khay, về cơ bản là dọn dẹp bàn. Nhiệm vụ này được huấn luyện trong 100 tập. Trong nhiệm vụ SwapBlocks, cũng có ba vị trí (hoặc vùng) với hai khối ở hai vị trí ngẫu nhiên. Mục tiêu là hoán đổi vị trí của các khối. Trong Hình 4, Swap - 100 biểu thị hiệu suất sau 100 tập và Swap - 300 là sau 300 tập. Chúng ta có thể thấy rằng sử dụng LLM để hướng dẫn khám phá tác tử mang lại hiệu suất tốt hơn trong ít thử nghiệm hơn.

5 Thảo luận
Trong công trình này, chúng tôi trình bày một khung để sử dụng LLM hướng dẫn việc khám phá trong các tác tử phân cấp. Thay vì học từ khám phá ngẫu nhiên mà không có bất kỳ kiến thức trước nào, chúng tôi sử dụng LLM để đề xuất các hành động cấp cao dựa trên nhiệm vụ và trạng thái hiện tại. Chúng tôi đánh giá phương pháp của mình trên các nhiệm vụ dài hạn trong môi trường mô phỏng cũng như với robot thực. Chúng tôi cho thấy rằng phương pháp của chúng tôi hoạt động tốt hơn so với các phương pháp cơ sở và không yêu cầu định hình phần thưởng thủ công. Hơn nữa, một khi tác tử được huấn luyện, nó không còn phụ thuộc vào LLM trong quá trình triển khai không giống như một số phương pháp trước đó.

--- TRANG 7 ---
Công trình này có thể được mở rộng theo nhiều cách để làm cho nó trở nên đầu cuối hơn. Chúng tôi Hiện tại giả định quyền truy cập vào một hàm cung cấp cho chúng tôi các mô tả ngôn ngữ của quỹ đạo và trạng thái hiện tại. Điều này có thể được tự động hóa bằng cách sử dụng những tiến bộ gần đây trong các mô hình ngôn ngữ thị giác (VLM). Sẽ thú vị khi mở rộng khung này cho nhiều hơn một cấp độ hoặc phân cấp để giải quyết các nhiệm vụ dài hơn.

6 Lời cảm ơn
Dự án này được tài trợ bởi Phòng thí nghiệm Nghiên cứu Quân đội Hoa Kỳ theo Thỏa thuận Hợp tác Số W911NF2120076.

Tài liệu tham khảo
[1] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.

[2] J. Andreas, D. Klein, và S. Levine. Modular multitask reinforcement learning with policy sketches. In International Conference on Machine Learning, trang 166–175. PMLR, 2017.

[3] P.-L. Bacon, J. Harb, và D. Precup. The option-critic architecture. In Proceedings of the AAAI Conference on Artificial Intelligence, tập 31, 2017.

[4] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.

[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.

[6] M. Chevalier-Boisvert, L. Willems, và S. Pal. Minimalistic gridworld environment for openai gym. https://github.com/maximecb/gym-minigrid, 2018.

[7] P. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, và D. Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, trang 4299–4307, 2017.

[8] Y. Du, O. Watkins, Z. Wang, C. Colas, T. Darrell, P. Abbeel, A. Gupta, và J. Andreas. Guiding pretraining in reinforcement learning with large language models. arXiv preprint arXiv:2302.06692, 2023.

[9] L. Espeholt, H. Soyer, R. Munos, K. Simonyan, V. Mnih, T. Ward, Y. Doron, V. Firoiu, T. Harley, I. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, trang 1407–1416. PMLR, 2018.

[10] R. Fruit và A. Lazaric. Exploration-exploitation in mdps with options. In Artificial Intelligence and Statistics, trang 576–584. PMLR, 2017.

[11] V. G. Goecks, N. Waytowich, D. Watkins-Valls, và B. Prakash. Combining learning from human feedback and knowledge engineering to solve hierarchical tasks in minecraft. arXiv preprint arXiv:2112.03482, 2021.

[12] P. Goyal, S. Niekum, và R. J. Mooney. Using natural language for reward shaping in reinforcement learning. arXiv preprint arXiv:1903.02020, 2019.

[13] D. Hafner. Benchmarking the spectrum of agent capabilities. arXiv preprint arXiv:2109.06780, 2021.

[14] H. Hu, D. Yarats, Q. Gong, Y. Tian, và M. Lewis. Hierarchical decision making by generating and following natural language instructions. Advances in neural information processing systems, 32, 2019.

[15] W. Huang, P. Abbeel, D. Pathak, và I. Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, trang 9118–9147. PMLR, 2022.

--- TRANG 8 ---
[16] Y. Jiang, S. S. Gu, K. P. Murphy, và C. Finn. Language as an abstraction for hierarchical deep reinforcement learning. Advances in Neural Information Processing Systems, 32, 2019.

[17] H. Küttler, N. Nardelli, A. Miller, R. Raileanu, M. Selvatici, E. Grefenstette, và T. Rocktäschel. The nethack learning environment. Advances in Neural Information Processing Systems, 33: 7671–7684, 2020.

[18] H. Le, N. Jiang, A. Agarwal, M. Dudik, Y. Yue, và H. Daumé III. Hierarchical imitation and reinforcement learning. In International conference on machine learning, trang 2917–2926. PMLR, 2018.

[19] M. Matthews, M. Samvelyan, J. Parker-Holder, E. Grefenstette, và T. Rocktäschel. Hierarchical kickstarting for skill transfer in reinforcement learning, 2022. URL https://arxiv.org/ abs/2207.11584.

[20] S. Mirchandani, S. Karamcheti, và D. Sadigh. Ella: Exploration through learned language abstraction. Advances in Neural Information Processing Systems, 34:29529–29540, 2021.

[21] B. Prakash, N. Waytowich, T. Oates, và T. Mohsenin. Interactive hierarchical guidance using language. arXiv preprint arXiv:2110.04649, 2021.

[22] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, trang 8748–8763. PMLR, 2021.

[23] M. Samvelyan, R. Kirk, V. Kurin, J. Parker-Holder, M. Jiang, E. Hambro, F. Petroni, H. Kuttler, E. Grefenstette, và T. Rocktäschel. Minihack the planet: A sandbox for open-ended reinforcement learning research. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.net/forum? id=skFwlyefkWJ.

[24] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, và O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

[25] R. S. Sutton, D. Precup, và S. Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181–211, 1999.

[26] uArm Developer. uArm-Python-SDK, 2018. URL https://github.com/uArm-Developer/ uArm-Python-SDK.

[27] G. Warnell, N. Waytowich, V. Lawhern, và P. Stone. Deep tamer: Interactive agent shaping in high-dimensional state spaces. AAAI Conference on Artificial Intelligence, trang 1545–1553, 2018. URL https://aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16200.

[28] N. Waytowich, S. L. Barton, V. Lawhern, và G. Warnell. A narration-based reward shaping approach using grounded natural language commands, 2019.

--- TRANG 9 ---
A Phụ lục
A.1 Chi tiết Môi trường
A.1.1 MiniGrid
Minigrid là một môi trường gridworld mã nguồn mở [6]. Chúng tôi sử dụng ba nhiệm vụ UnlockReach, KeyCorridor v0 và KeyCorridor v1. Hình 5 cho thấy bố cục lưới cho ba nhiệm vụ. Quan sát là phiên bản được mã hóa của lưới để nắm bắt loại ô, màu sắc và trạng thái cửa/hộp tùy chọn. Chúng tôi xem xét phiên bản có thể quan sát hoàn toàn của những nhiệm vụ này, có nghĩa là các quan sát bao gồm toàn bộ lưới - 13x13 trong trường hợp của chúng tôi.

(a) Mục tiêu: mở cửa xanh lá bị khóa và đi đến hộp xanh dương
(b) Mục tiêu: nhặt chìa khóa tím, sau đó mở cửa tím và đi đến quả bóng đỏ
(c) Mục tiêu: nhặt chìa khóa tím, sau đó mở cửa tím và đi đến quả bóng đỏ, tránh cửa vàng và xám

Hình 5: Tác tử được đại diện bằng tam giác đỏ. Trái: Nhiệm vụ UnlockReach nơi tác tử cần lấy chìa khóa đúng và mở cửa rồi đi đến đối tượng trong phòng bên phải. Giữa: Nhiệm vụ KeyCorridor-v0 nơi tác tử cần đến quả bóng đỏ trong một trong những phòng bị khóa bên phải. Nó trước tiên cần lấy chìa khóa từ một trong những phòng bên trái. Phải: Tương tự như v0 nhưng một số phòng có chìa khóa bị lỗi được hiển thị màu đỏ. Tác tử không thấy điều này, nó chỉ nhận thông tin này trong mục tiêu văn bản

A.1.2 SkillHack
SkillHack [19] là một phần mở rộng trên [17] nơi bạn có thể thiết kế các cấp độ tùy chỉnh và nhận trạng thái hình ảnh/không gian cùng với mô tả văn bản. Hình 6 cho thấy các nhiệm vụ chúng tôi kiểm tra trong đó mỗi nhiệm vụ yêu cầu giải quyết nhiều nhiệm vụ con. Trạng thái bao gồm bản đồ 2D cùng với thông tin kho và văn bản mô tả hiệu ứng của mỗi hành động. Điều này rất thuận tiện cho phương pháp của chúng tôi vì chúng tôi cần tương tác với LLM bằng ngôn ngữ.

A.1.3 Crafter
Crafter [13] là phiên bản 2D của Minecraft, Hình 6. nó có biểu diễn trạng thái rất đơn giản mã hóa các vật phẩm trên bản đồ, kho và sức khỏe của tác tử. Khá dễ dàng để dịch điều này thành văn bản bằng hàm được mã hóa thủ công. Các kỹ năng cấp cao lskill i mà chúng tôi xem xét, chẳng hạn như chop tree, create table, make wood pickaxe v.v. cũng có thể được mô tả một cách tự nhiên bằng các cụm từ ngôn ngữ.

--- TRANG 10 ---
(a) Battle: Mục tiêu là nhặt kiếm, di chuyển đến quái vật và chiến đấu
(b) FrozenLavaCross: Ở đây mục tiêu là nhặt cây đũa phép, di chuyển đến dung nham để phóng phép, tạo ra cây cầu và sau đó di chuyển đến lối ra
(c) Crafter Env: Môi trường 2D giống Minecraft

Hình 6: Trái và Giữa: Bộ môi trường SkillHack dựa trên trò chơi dựa trên văn bản NetHack. Phải: Crafter: Ở đây, tác tử khám phá môi trường thế giới mở để thu thập tài nguyên như gỗ, đá và tạo ra các đối tượng và công cụ mới

A.1.4 Cánh tay Robot uArm
uArm Swift Pro là một cánh tay robot nghiên cứu mã nguồn mở. Chúng tôi sử dụng điều này để kiểm tra phiên bản Q-learning dạng bảng của phương pháp chúng tôi bằng hai nhiệm vụ [26]. Môi trường được thiết lập trong đó bàn có 3 vùng như được hiển thị trong Hình 7 (b). Robot có thể nhặt và đặt các đối tượng từ bất kỳ vùng nào trong số này. Chúng được coi là các kỹ năng cấp thấp và được mã hóa cứng. Tuy nhiên với nhiều tài nguyên hơn, chúng có thể được huấn luyện bằng RL làm cho chúng mạnh mẽ hơn. Trong nhiệm vụ DeskCleanup, có một khay trong một trong các vùng như thấy trong Hình 7 (a) và (b). Mục tiêu là nhặt các khối và đặt chúng vào khay. Trong nhiệm vụ BlockSwap, hai khối được đặt trong các vùng ngẫu nhiên Hình 7 (c). Mục tiêu là hoán đổi vị trí. Điều này chỉ có thể được thực hiện bằng cách sử dụng vùng trống. Chúng tôi sử dụng camera Intel Realsense để chuyển đổi thông tin hình ảnh thành trạng thái rời rạc đơn giản cho Q-learning dạng bảng. Trạng thái bao gồm 4 giá trị rời rạc, [arm_location, holding, red_location, green_location]. Chúng tôi lấy arm_location từ api robot, đây là một trong 3 vùng. red_location và green_location biểu thị vị trí của khối đỏ và khối xanh lá tương ứng (một trong 3 vùng). holding biểu thị khối mà cánh tay hiện đang cầm nếu có. Chúng tôi trích xuất holding, red_location và green_location từ hình ảnh camera. Thay vì huấn luyện hoặc tinh chỉnh mô hình phát hiện đối tượng riêng biệt, chúng tôi sử dụng CLIP và các cụm từ văn bản mô tả các đối tượng trên các mảnh hình ảnh. Sau đó chúng tôi lấy tọa độ và ánh xạ chúng với các giá trị đúng trong không gian trạng thái.

(a) Nhiệm vụ Dọn dẹp Bàn
(b) Nhiệm vụ Dọn dẹp Bàn  
(c) Nhiệm vụ Hoán đổi Khối

Hình 7: uArm: Môi trường cánh tay robot

A.2 Thiết kế Nhắc
Chúng tôi sử dụng cấu trúc sau cho tất cả các nhắc của chúng tôi.
Mục tiêu: lgoal_inst
Cho đến nay: ltraj
Tôi có nên lskill i không?

ltraj đại diện cho lịch sử quỹ đạo nắm bắt những gì tác tử đã làm cho đến nay. Chúng tôi thấy rằng 2 hành động đủ để nắm bắt điều này vì các nhiệm vụ chúng tôi kiểm tra không cực kỳ phức tạp. Như đã đề cập trong các phần trước, lgoal_inst mô tả mục tiêu cùng với chi tiết, cảnh báo và ràng buộc. Mỗi kỹ năng được liên kết với một mô tả ngôn ngữ lskill i.

A.2.1 Nhắc MiniGrid

Bạn là một tác tử giải mê cung 2D có quyền truy cập vào nhiều kỹ năng cấp thấp như "pick:red:ball", "pick:red:key", "pick:green:ball", "pick:green:key", "pick:blue:ball"...
Mục tiêu: mở cửa xanh lá bị khóa và đi đến hộp xanh dương
Cho đến nay:
Câu hỏi: Tôi có nên pick:red:ball không?
Trả lời: Không
Mục tiêu: mở cửa xanh lá bị khóa và đi đến hộp xanh dương
Cho đến nay:
Câu hỏi: Tôi có nên goto:blue:box không?
Trả lời: Không
Mục tiêu: mở cửa xanh lá bị khóa và đi đến hộp xanh dương
Cho đến nay: pick:green:key
Câu hỏi: Tôi có nên unlock:green:door không?
Trả lời: Có
Mục tiêu: mở cửa xanh lá bị khóa và đi đến hộp xanh dương
Cho đến nay: pick:green:key, unlock:green:door
Câu hỏi: Tôi có nên goto:red:box không?
Trả lời: Không
[.. một vài ví dụ khác ..]
Mục tiêu: mở cửa xanh lá bị khóa và đi đến hộp xanh dương
Cho đến nay: pick:green:key, unlock:green:door
Câu hỏi: Tôi có nên goto:blue:box không?
Trả lời:

A.2.2 Nhắc SkillHack
Môi trường NetHack ban đầu là một trò chơi dựa trên văn bản., Vì vậy may mắn thay chúng tôi nhận được mô tả ngôn ngữ về hành động và quan sát của chúng tôi từ động cơ trò chơi.

Bạn là một Tác tử NetHack được trang bị các kỹ năng sau:
ApplyFrostHorn: Sử dụng sừng băng giá để đóng băng một ít dung nham.
Eat: Ăn một quả táo.
Fight: Đánh một con quái vật.
NavigateLava: Đến cầu thang qua các mảng dung nham ngẫu nhiên.
PickUp: Nhặt một vật phẩm ngẫu nhiên.
PutOn: Đeo một bùa hộ mệnh hoặc khăn.
TakeOff: Cởi quần áo.
Unlock: Sử dụng chìa khóa để mở khóa cửa bị khóa.
Wear: Mặc áo choàng.
Wield: Cầm kiếm.
ZapWandOfCold: Sử dụng cây đũa phép lạnh để đóng băng dung nham.
<liệt kê tên vật phẩm phổ biến>
Battle: PickUp một Sword được đặt ngẫu nhiên, Wield Sword và cuối cùng Fight và giết một Monster.
Mục tiêu: Chiến đấu với Monster
Cho đến nay: Tôi thấy một kiếm bạc
Câu hỏi: Tôi có nên TakeOff không?

--- TRANG 11 ---
Trả lời: Không
Mục tiêu: Chiến đấu với Monster
Cho đến nay: đã nhặt một kiếm bạc
Câu hỏi: Tôi có nên Wield không?
Trả lời: Có
Mục tiêu: Chiến đấu với Monster
Cho đến nay: đã nhặt một kiếm bạc, cầm một kiếm bạc
Câu hỏi: Tôi có nên ZapWandOfCold không?
Trả lời: Không
[.. một vài ví dụ khác ..]
Mục tiêu: Chiến đấu với Monster
Cho đến nay: đã nhặt kiếm, cầm kiếm
Câu hỏi: Tôi có nên Fight không?
Trả lời:

Bạn là một Tác tử NetHack được trang bị các kỹ năng sau:
[.. giống như trên ..]
Frozen Lava Cross: Một WandOfCold hoặc FrostHorn sẽ xuất hiện ở phía gần của dòng dung nham. PickUp vật phẩm và sau đó tạo ra một cây cầu qua dung nham bằng ZapWandOfCold hoặc ApplyFrostHorn. Cuối cùng, NavigateLava qua cây cầu mới tạo của bạn để đến cầu thang ở phía bên kia.
<liệt kê tên vật phẩm phổ biến>
Mục tiêu: Đến cầu thang qua dung nham
Cho đến nay: Tôi thấy một cây đũa phép
Câu hỏi: Tôi có nên PickUp không?
Trả lời: Có
[.. một vài ví dụ khác ..]
Mục tiêu: Đến cầu thang qua dung nham
Cho đến nay: đã nhặt một cây đũa phép, áp dụng phép, dung nham nguội và đông cứng
Câu hỏi: Tôi có nên NavigateLava không?
Trả lời:

A.2.3 Nhắc Crafter
Đối với môi trường này, các nhắc được thiết kế tương tự như [8].

--- TRANG 12 ---
