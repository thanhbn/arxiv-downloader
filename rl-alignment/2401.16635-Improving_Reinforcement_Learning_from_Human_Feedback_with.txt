# 2401.16635.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2401.16635.pdf
# File size: 424631 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Improving Reinforcement Learning from Human Feedback with
Efficient Reward Model Ensemble
Shun Zhang1, Zhenfang Chen1, Sunli Chen2, Yikang Shen1, Zhiqing Sun3,and Chuang Gan1,4
1MIT-IBM Watson AI Lab,2Tsinghua University,3Carnegie Mellon University,4UMass Amherst
Abstract
Reinforcement Learning from Human Feed-
back (RLHF) is a widely adopted approach for
aligning large language models with human
values. However, RLHF relies on a reward
model that is trained with a limited amount
of human preference data, which could lead
to inaccurate predictions. As a result, RLHF
may produce outputs that are misaligned with
human values. To mitigate this issue, we con-
tribute a reward ensemble method that allows
the reward model to make more accurate predic-
tions. As using an ensemble of large language
model-based reward models can be computa-
tionally and resource-expensive, we explore ef-
ficient ensemble methods including linear-layer
ensemble and LoRA-based ensemble. Empir-
ically, we run Best-of- nand Proximal Policy
Optimization with our ensembled reward mod-
els, and verify that our ensemble methods help
improve the alignment performance of RLHF
outputs.
1 Introduction
Large language models (LLMs) (Vaswani et al.,
2017) have become prominent in the field of artifi-
cial intelligence in solving a wide range of complex
tasks in question answering (Ouyang et al., 2022;
Guu et al., 2020), code generation (Li et al., 2022;
Nijkamp et al., 2023), reasoning and planning (Ko-
jima et al., 2023; Liu et al., 2023), and various other
domains. However, as large language models are
trained using data from various sources, they may
generate outputs that are biased, inappropriate, or
even harmful, which are misaligned with human
values. Therefore, it is crucial to align large lan-
guage models with human values for them to be
safely deployed.
Recently, Reinforcement Learning from Human
Feedback (RLHF) (Ouyang et al., 2022) has shown
to be a promising approach to mitigate the misalign-
ment issue. Concretely, RLHF first runs supervisedfine-tuning (SFT) to train a large language model to
generate responses that follow instructions. It then
trains a reward model using a preference dataset
that reflects human values. Lastly, it runs reinforce-
ment learning using the learned reward model to
finetune the SFT model. In this way, the finetuned
model would generate sequences with higher re-
wards, which are presumably more aligned with
human values.
The RLHF algorithm has demonstrated success
in improving the alignment of large language mod-
els (Dubois et al., 2023; Wu et al., 2023). However,
it also has some recognized issues. As the reward
model is trained on offline-collected preference
data, its reward predictions may not be accurate
on out-of-distribution data. If we use reinforce-
ment learning to optimize a large language model
with an inaccurate reward model, it may generate
misaligned outputs with incorrectly high estimated
rewards. This problem is observed in the value
alignment literature and is usually known as re-
ward hacking orreward overoptimization (Amodei
et al., 2016; Leike et al., 2017; Gao et al., 2022;
Casper et al., 2023), and is also a well-known prob-
lem in model-based offline reinforcement learning
(Levine et al., 2020).
In offline reinforcement learning, a prevalent
strategy to mitigate reward overoptimization is to
estimate rewards conservatively under uncertainty
(Kumar et al., 2020). Building upon this strategy,
we consider an ensemble approach that employs
a set of reward models to make better predictions,
in line with concurrent works (Coste et al., 2023;
Eisenstein et al., 2023). To achieve reward model
ensemble, a straightforward approach is to train
multiple reward models independently and then
ensemble them. However, this approach presents
some challenges in our setting. As reward mod-
els are usually based on large language models,
training all the reward models and then loading
all of them during inference time can be computa-arXiv:2401.16635v3  [cs.LG]  22 Oct 2024

--- PAGE 2 ---
tionally expensive and resource-consuming. There-
fore, we contribute to designing efficient ensem-
ble approaches. Lastly, we empirically confirm
the effectiveness of our ensemble methods using
well-accepted evaluation benchmarks, AlpacaEval
(Dubois et al., 2023) and MT-Bench (Zheng et al.,
2023). The contributions of this paper are summa-
rized as follows.
•We design reward ensemble algorithms that
improve reward estimation accuracy, and
hence improve the alignment of large lan-
guage models.
•We propose two ensemble approaches, linear-
layer ensemble and LoRA-based ensemble,
that offer trade-offs between computational
efficiency and alignment performance.
•We use the ensembled reward models for both
Best-of- nand Proximal Policy Optimization
(PPO) algorithms, evaluate them on AlpacaE-
val and MT-Bench, and empirically confirm
that RLHF with our ensembled reward models
outperforms the standard RLHF algorithm.
2 Background and Related Work
Reinforcement learning from human feedback
(RLHF). RLHF was originally considered in the
TAMER framework (Knox and Stone, 2009), in
which an agent learns the reward function from a
human user’s positive or negative feedback. This
setting is later considered in deep reinforcement
learning (Christiano et al., 2017), and recently
employed for finetuning large language models
(Ouyang et al., 2022) to align the model’s be-
havior with human values and preferences. We
overviewed the RLHF framework in the introduc-
tion and leave more details of the standard RLHF
algorithm in Sec. A.1.
Ensemble models and uncertainty estimation.
Model ensembling has been an accepted approach
to improving a model’s accuracy and estimating
a model’s uncertainty. Lakshminarayanan et al.
(2017) quantify predictive uncertainty in deep neu-
ral networks using ensembling, which performs
better than Bayesian neural networks. In reinforce-
ment learning, Liang et al. (2022) use ensembled
reward models to estimate the model’s uncertainty
for more informed exploration. Gleave and Irv-
ing (2022) use ensembled reward models for active
learning.
Concurrently, Coste et al. (2023) also use re-
ward model ensemble to mitigate the reward modeloveroptimization problem and draw similar con-
clusions to our paper. The key difference is
that our work focuses on developing efficient en-
semble approaches, since ensembling multiple
independently-trained reward models can be ex-
pensive. Eisenstein et al. (2023) focus on compar-
ing ensembling during pretraining and finetuning.
Zhai et al. (2023) also consider LoRA-based en-
semble, while their work focuses on an uncertainty-
penalized objective in RL-finetuning. Ramé et al.
(2024) consider a different approach of averaging
the weights of multiple reward models instead of
ensembling their predictions. Wang et al. (2023)
use LoRA ensembles for improving predictive ac-
curacy and uncertainty quantification. Ahmed et al.
(2024) propose a scalable ensemble approach that
shares an encoder backbone but uses separate lin-
ear heads, achieving similar performance to full
ensembling.
Offline reinforcement learning. Uncertainty es-
timation is also a key problem in offline rein-
forcement learning (Levine et al., 2020; Janner
et al., 2019). For example, conservative Q-learning
(CQL) (Kumar et al., 2020) learns a conservative
Q function to mitigate the overestimation bias for
out-of-distribution state, action pairs. Inspired by
this algorithm, we also design an ensemble algo-
rithm that uses conservative predictions by using
the lower confidence bound of the ensembled pre-
dictions.
3 Reward Model Ensemble
The reward model in RLHF is a (large language
model-based) model that takes as input an instruc-
tion and a response, and outputs a reward predic-
tion that indicates the alignment performance of
the response. In this section, we provide a formal
descirption for our ensemble algorithms.
3.1 Architecture Design of Reward Model
Ensemble
In the literature, reward models are commonly fine-
tuned from pretrained large language models. Fol-
lowing the convention (Dubois et al., 2023), we
assume a reward model is a Transformer model
and a linear layer, where the linear layer takes as in-
put the last hidden layer of the Transformer model,
and outputs a reward value.
In this subsection, we discuss possible ways to
ensemble multiple large language model-based re-
ward models and discuss their advantages. The

--- PAGE 3 ---
…Transformer…
(b) Linear-layer Ensemble(c) LoRA-based EnsembleEnsembled RewardEnsembled Reward
(a) Ensemble of Single Reward Models TransformerLinear LayerRewardTransformer…Transformer(❆Frozen)LoRAAdapterTransformer(❆Frozen)LoRAAdapterEnsembled RewardLinear LayerRewardLinear LayerRewardLinear LayerRewardLinear LayerRewardLinear LayerRewardFigure 1: Illustration of the reward model ensemble algorithms.
ensemble algorithms are illustrated in Fig. 1 and
the pseudocode is provided in Alg. 1.
Ensemble of single reward models. A straight-
forward way to achieve reward model ensemble
is to train multiple reward models independently
using different random seeds. At inference time,
we simply load all the reward models and ensem-
ble them. However, such a method can be both
expensive in training and inference – during train-
ing, we need to run reward training for multiple
times; during inference, we need to load multi-
ple large language model-based reward models
simultaneously to GPUs, which can be resource-
consuming. Specifically, an ensemble of kinde-
pendently trained reward models needs to train and
loadk(M+L)parameters, where Mis the number
of parameters of the Transformer model, and Lis
the number of parameters of a linear layer (shown
in Fig. 1 (a)).
Linear-layer ensemble. To make the ensembling
more efficient both in training and inference, we
can make all the ensembled models share the same
Transformer model, while each ensembled model
has its own linear layer that outputs its reward
prediction. During training, both the Transformer
model and the linear layers of all the ensembled
models are being trained. Note that the Trans-
former model is the same for all the ensembled
reward models. In this way, a linear-layer ensemble
model of size konly requires M+kLparameters
(shown in Fig. 1 (b)).
LoRA-based ensemble. In linear-layer ensem-
ble, allowing all the ensembled models to share the
same Transformer model indeed reduces the total
number of parameters requiring training. However,
it may considerably limit the diversity of the ensem-
bled models. Therefore, we allow each ensembledmodel to slightly finetune the Transformer model.
Specifically, each ensembled model trains its own
linear layer, and a LoRA adapter (Hu et al., 2021)
that is added to the Transformer model. The LoRA
adapter only has a small number of parameters
and can be trained efficiently. (The background
on LoRA is provided in Sec. A.2.) In this way, a
LoRA-based ensemble model of size krequires
M+kL+kAparameters, where Ais the number
of parameters of an adapter (shown in Fig. 1 (c)).
Empirically, we find that only LoRA-finetuning
the Transformer model in the reward model does
not perform well, as the Transformer model is not
trained for reward prediction at all. So in our ex-
periments, we first finetune the Transformer model
in the same way as linear-layer ensemble using a
subset of preference data before ensemble model
training (Line 14 in Alg. 1). We then use the rest
of the data for ensemble training. We provide more
details on the dataset split in Sec. 4.
3.2 Predictions of Ensembled Reward Models
Now we need to ensemble the predictions of dif-
ferent ensembled reward models. We explore two
algorithms for ensembling these predictions, which
use the mean predicted value and the lower confi-
dence bound of the predicted values, respectively.
LetRbe the set of ensemble model predictions.
R={r1, r2, . . . , r k}.Mean value prediction
simply uses mean(R), the mean value of the en-
semble reward model prediction. This is inherently
a lower-variance estimation of the reward. On the
other hand, lower confidence bound (LCB) is a
conservative estimation of the reward. It consid-
ers the standard deviation of the ensemble model
predictions, defined as LCB(R) =mean(R)−β·
std(R), where βis a hyperparameter. However, we
empirically find that the performance of LCB is

--- PAGE 4 ---
Algorithm 1 Reward Model Ensemble Algorithms
Require: k: number of ensemble models, M: parameters of
Transformer model
1: Initialize ensemble as empty set: ensemble ← {}
2:▶Option 1: Ensemble of Single Reward Models
3:fori= 1tokdo
4: Mi←clone(M)
5: Initialize linear layer with random parameters Li
6: train (Mi∪Li)
7: Add model Mi∪Litoensemble
8:end for
9:▶Option 2: Linear-layer Ensemble
10:Initialize linear layer with random parameters Lifori∈
[0, k)
11: Concurrently train (M∪Li)fori∈[0, k)
12: Add M∪Li, i∈[0, k)toensemble
13:▶Option 3: LoRA-based Ensemble
14:Finetune M, L i, . . . , L kusing linear-layer ensemble with
a subset of data
15:fori= 1tokdo
16: Add LoRA adapter to the Transformer model Mwith
random parameters Ai
17: train (Ai∪Li)
18: Add M∪Ai∪Litoensemble
19:end for
20:return ensemble
comparable to that of mean value prediction.
4 Empirical Evaluation
In this section, we empirically answer the follow-
ing questions. Q1: Compared with using a sin-
gle reward model, do ensembled reward models
help improve alignment performance in RLHF?
Q2: Which ensemble architecture in Sec. 3.1 has
the best performance? Q3: Which prediction algo-
rithm in Sec. 3.2 has a better performance?
Algorithms. We consider using the ensembled
reward models with Best-of- nand Proximal Pol-
icy Optimization (PPO) (Schulman et al., 2017),
which are standard approaches in RLHF (Ouyang
et al., 2022; Dubois et al., 2023; Coste et al., 2023).
Specifically, Best-of- ngenerates nsamples from
the SFT model for each input and selects the sample
with the highest predicted reward. Proximal Policy
Optimization (PPO) is a reinforcement learning
algorithm that finetunes the SFT model using the
reward model.
For each reward ensemble method, we conduct
the experiments multiple times using different ran-
dom seeds. Specifically, we repeat the experiments
10 times for Best-of- nand 5 times for PPO. For
all experiments except experiments without reward
ensemble, we use three reward models for ensem-
bling ( k= 3).Models. We use the pretrained models provided
in AlpacaFarm (Dubois et al., 2023). Specifically,
we use SFT10k as the base model for generation,
which is a Llama-7b model (Touvron et al., 2023)
finetuned on the alpaca_instructions dataset.
So the model can follow instructions, while it has
not been aligned with human preferences. To be
consistent with Dubois et al. (2023), the Trans-
former model in our reward model is also initial-
ized using SFT10k , which is not yet trained for
reward prediction.
Datasets. We use the AlpacaFarm datasets
(Dubois et al., 2023) for training and evalua-
tion, which provide utilities to evaluate the align-
ment performance of the model outputs using
GPT-4 APIs and can easily compare our mod-
els with other benchmarking models and algo-
rithms. We train all the reward models us-
ing both alpaca_noisy_multi_preference and
alpaca_human_preference datasets, and use
alpaca_farm_evaluation for evaluation.
Specifically, for LoRA-based ensemble, we
use the two training datasets for different phases:
We first use alpaca_noisy_multi_preference
to fully finetune the Transformer model with k
linear layers, in the same way as linear-layer en-
semble (Line 14 in Alg. 1), as we find fully-
finetuning the Transformer is necessary for it
to make reward predictions. Then we use
alpaca_human_preference for training the linear
layers and the adapters for the ensemble members
(Line 15-19 in Alg. 1).
Results. Our main results are reported in Fig-
ure 2. For Best-of- n, we choose n= 50,100,200.
For PPO, we evaluate checkpoints at every 100
training steps. To evaluate the alignment perfor-
mance, we use the win rate metric provided in
AlpacaEval (shown as the vertical axis), which
measures the chances that our methods’ outputs
are preferred by GPT-4 compared with a GPT-3
generated baseline. All the ensemble approaches
use the mean value prediction, which uses the mean
of the predicted rewards of the ensemble members
as their final predictions.
Overall, we find that the win rates are consis-
tently higher when using reward method ensemble
(answering Q1). For Best-of- n, both ensemble of
single reward models and LoRA-based ensemble
have the best performance. For PPO, we are un-
able to run ensemble of single reward models as
it requires loading multiple reward models during

--- PAGE 5 ---
50 75 100 125 150 175 200
n (in Best-of-n)42434445464748Win Rate (%)
(a) Best-of- nresults.
100 150 200 250 300
PPO Training Steps28303234363840Win Rate (%)
 (b) PPO results.
Single Reward Model
Linear-layer Ensemble (Mean)
LoRA-based Ensemble (Mean)
Ensemble of Single Reward Models (Mean)
Figure 2: Win rates of model responses using Best-of- nand PPO on AlpacaEval. Different lines represent different
reward ensemble algorithms. The shaded areas represent standard errors.
Ens. Method First Turn Second Turn Average
Ens. of Single 4.70 ±0.12 3.63 ±0.22 4.16 ±0.14
Linear-layer 4.73 ±0.22 3.67 ±0.19 4.20 ±0.10
LoRA-based 4.86 ±0.09 3.84 ±0.21 4.35 ±0.12
Table 1: Alignment scores on MT-Bench for different
ensemble methods.
PPO training. Nonetheless, we find LoRA-based
ensemble has the best performance.
We also conduct the experiments on MT-Bench
(Zheng et al., 2023), which is a benchmark for
multiturn questions. We evaluate our PPO-trained
models with the most training steps, and report the
alignment scores after the first turn, after the sec-
ond turn, and the average of both. The results are
reported in Table 1, and our findings are consistent
with the AlpacaEval results. The results on both
benchmarks suggest that, although LoRA does not
fully finetune the Transformer models, it is effec-
tive for reward model ensemble and can improve
the alignment performance (answering Q2).
In terms of the prediction methods, we find that
the mean reward prediction and LCB have simi-
lar performance. Detailed results are presented in
Sec. B (answering Q3).
5 Discussion and Conclusion
In summary, our paper presents a novel approach to
enhancing the alignment of large language models
through efficient reward model ensemble in RLHF.
Specifically, the LoRA-based ensemble method
demonstrates effectiveness under computational
constraints. In future work, we will extend this
approach to other steps of LLM training and infer-
ence, such as sample-efficient training of reward
models (Gleave and Irving, 2022).References
Ahmed M. Ahmed, Rafael Rafailov, Stepan Sharkov,
Xuechen Li, and Sanmi Koyejo. 2024. Scalable En-
sembling For Mitigating Reward Overoptimisation.
arXiv preprint arXiv:2406.01013 .
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul
Christiano, John Schulman, and Dan Mané. 2016.
Concrete problems in AI safety. arXiv preprint
arXiv:1606.06565 .
Stephen Casper, Xander Davies, Claudia Shi,
Thomas Krendl Gilbert, Jérémy Scheurer, Javier
Rando, Rachel Freedman, Tomasz Korbak, David
Lindner, Pedro Freire, Tony Wang, Samuel Marks,
Charbel-Raphaël Segerie, Micah Carroll, Andi Peng,
Phillip Christoffersen, Mehul Damani, Stewart
Slocum, Usman Anwar, Anand Siththaranjan, Max
Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii
Krasheninnikov, Xin Chen, Lauro Langosco, Peter
Hase, Erdem Bıyık, Anca Dragan, David Krueger,
Dorsa Sadigh, and Dylan Hadfield-Menell. 2023.
Open Problems and Fundamental Limitations of
Reinforcement Learning from Human Feedback.
ArXiv:2307.15217 [cs].
Paul F. Christiano, Jan Leike, Tom Brown, Miljan Mar-
tic, Shane Legg, and Dario Amodei. 2017. Deep
reinforcement learning from human preferences. In
Advances in Neural Information Processing Systems ,
pages 4299–4307.
Thomas Coste, Usman Anwar, Robert Kirk, and David
Krueger. 2023. Reward Model Ensembles Help Miti-
gate Overoptimization. ArXiv:2310.02743 [cs].
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,
Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. 2023. Alpaca-
Farm: A Simulation Framework for Methods that
Learn from Human Feedback.
Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ah-
mad Beirami, Alex D’Amour, D. J. Dvijotham, Adam
Fisch, Katherine Heller, Stephen Pfohl, Deepak
Ramachandran, Peter Shaw, and Jonathan Berant.
2023. Helping or Herding? Reward Model Ensem-
bles Mitigate but do not Eliminate Reward Hacking.
ArXiv:2312.09244 [cs].

--- PAGE 6 ---
Leo Gao, John Schulman, and Jacob Hilton. 2022. Scal-
ing Laws for Reward Model Overoptimization.
Adam Gleave and Geoffrey Irving. 2022. Uncer-
tainty Estimation for Language Reward Models.
ArXiv:2203.07472 [cs].
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-
supat, and Ming-Wei Chang. 2020. REALM:
Retrieval-Augmented Language Model Pre-Training.
ArXiv:2002.08909 [cs].
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Michael Janner, Justin Fu, Marvin Zhang, and Sergey
Levine. 2019. When to trust your model: Model-
based policy optimization. In Advances in Neural In-
formation Processing Systems , pages 12519–12530.
W. Bradley Knox and Peter Stone. 2009. Interactively
Shaping Agents via Human Reinforcement: The
TAMER Framework. In Proceedings of the Fifth
International Conference on Knowledge Capture , K-
CAP ’09, pages 9–16, New York, NY , USA.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid,
Yutaka Matsuo, and Yusuke Iwasawa. 2023.
Large Language Models are Zero-Shot Reasoners.
ArXiv:2205.11916 [cs].
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey
Levine. 2020. Conservative Q-Learning for Offline
Reinforcement Learning. Neural Information Pro-
cessing Systems .
Balaji Lakshminarayanan, Alexander Pritzel, and
Charles Blundell. 2017. Simple and Scalable Predic-
tive Uncertainty Estimation using Deep Ensembles.
ArXiv:1612.01474 [cs, stat].
Jan Leike, Miljan Martic, Victoria Krakovna, Pedro A
Ortega, Tom Everitt, Andrew Lefrancq, Laurent
Orseau, and Shane Legg. 2017. AI safety gridworlds.
arXiv preprint arXiv:1711.09883 .
Sergey Levine, Aviral Kumar, George Tucker, and Justin
Fu. 2020. Offline Reinforcement Learning: Tuto-
rial, Review, and Perspectives on Open Problems.
arXiv:2005.01643 [cs, stat] . ArXiv: 2005.01643.
Yujia Li, David Choi, Junyoung Chung, Nate Kushman,
Julian Schrittwieser, Rémi Leblond, Tom Eccles,
James Keeling, Felix Gimeno, and Agustin Dal Lago.
2022. Competition-level code generation with al-
phacode. Science , 378(6624):1092–1097. Publisher:
American Association for the Advancement of Sci-
ence.
Xinran Liang, Katherine Shu, Kimin Lee, and Pieter
Abbeel. 2022. Reward Uncertainty for Explo-
ration in Preference-based Reinforcement Learning.
ArXiv:2205.12401 [cs].Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu,
Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023.
LLM+P: Empowering Large Language Models with
Optimal Planning Proficiency. ArXiv:2304.11477
[cs].
Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
Wang, Yingbo Zhou, Silvio Savarese, and Caiming
Xiong. 2023. CodeGen: An Open Large Language
Model for Code with Multi-Turn Program Synthesis.
ArXiv:2203.13474 [cs].
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. ArXiv:2203.02155 [cs].
Alexandre Ramé, Nino Vieillard, Léonard Hussenot,
Robert Dadashi, Geoffrey Cideron, Olivier Bachem,
and Johan Ferret. 2024. WARM: On the
Benefits of Weight Averaged Reward Models.
ArXiv:2401.12187 [cs].
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Radford, and Oleg Klimov. 2017. Proximal Policy
Optimization Algorithms. arXiv:1707.06347 [cs] .
ArXiv: 1707.06347.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971 .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention Is
All You Need. arXiv:1706.03762 [cs] . ArXiv:
1706.03762.
Xi Wang, Laurence Aitchison, and Maja Rudolph. 2023.
LoRA ensembles for large language model fine-
tuning. arXiv preprint arXiv:2310.00035 .
Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane
Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari
Ostendorf, and Hannaneh Hajishirzi. 2023. Fine-
Grained Human Feedback Gives Better Rewards for
Language Model Training. ArXiv:2306.01693 [cs].
Yuanzhao Zhai, Han Zhang, Yu Lei, Yue Yu, Kele Xu,
Dawei Feng, Bo Ding, and Huaimin Wang. 2023.
Uncertainty-Penalized Reinforcement Learning from
Human Feedback with Diverse Reward LoRA En-
sembles. ArXiv:2401.00243 [cs].
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, and Eric Xing. 2023. Judg-
ing LLM-as-a-judge with MT-Bench and Chatbot
Arena. arXiv preprint arXiv:2306.05685 .

--- PAGE 7 ---
A Preliminaries
For the completeness of the paper, we provide more
background details on reinforcement learning from
human feedback and LoRA finetuning in this sec-
tion.
A.1 Reinforcement Learning from Human
Feedback
Reinforcement learning from human feedback
(RLHF) follows a three-step process: supervised
fine-tuning (SFT), reward modeling, and reinforce-
ment learning using the learned reward model.
In this paper, we focus on the second step
of RLHF, which trains a reward model that re-
flects human preferences. We denote the reward
model by rθ, parameterized by θ. To train the re-
ward model, we have a preference dataset, D=
{(x, yw, yl), . . .}, where xis a context (a question
or an instruction); ywis a preferred output, and yl
is a less preferred output. The reward model is then
trained to predict the preference score rθ(x, y)for
an input-output pair, where a larger score indicates
that the output is more preferred by a human. The
loss function for reward model training is
loss(θ) =−E(x,yw,yl)∼D
[log (σ(rθ(x, yw)−rθ(x, yl)))],(1)
where σrepresents the sigmoid function. This ap-
proach effectively captures humans’ preferences,
so that rθpredicts higher rewards for responses that
are preferred by humans.
In the last step of RLHF, we can finetune the
supervised finetuned (SFT) model using reinforce-
ment learning, typically using the Proximal Policy
Optimization (PPO) algorithm. In essence, PPO
iteratively improves the policy by simultaneously
minimizing the divergence between new and old
policies and maximizing the expected cumulative
rewards. We refer readers to find algorithm details
in the original paper (Schulman et al., 2017).
A.2 LoRA Finetuning
When finetuning a large language model, finetuning
all the parameters can be computationally expen-
sive and resource-demanding. To this end, Low-
Rank Adaptation (LoRA) (Hu et al., 2021) is a
well-accepted algorithm to efficiently finetune a
pretrained large language model. Concretely, for
each Transformer layer, LoRA learns
∆W=A1A2, (2)where ∆Wis the change applied to the weight
matrix Wof a transformer layer, and A1andA2
are smaller matrices. Let the dimension of Wbe
d1×d2. Then the dimension of A1isd1×rand
the dimension of A2isr×d2, where ris the rank
of the decomposition, which is smaller than d1and
d2.
When applying LoRA to a transformer layer, the
modified weight matrix W′is used in the forward
pass as follows,
W′=W+ ∆W, (3)
where Wis the original weight matrix of the trans-
former layer. It is worth noting that during training,
only the matrices A1andA2are updated. The
original weights Wremain frozen. This approach
requires training only a substantially smaller set of
parameters in matrices A1andA2, compared to the
original weights W, while all the model weights
will be influenced during training.
B Additional Empirical Results
In addition to our results in the main paper, we also
explored different prediction algorithms, including
using the mean and lower confidence bound (LCB)
of the reward predictions of the ensembled models.
We perform the experiments on AlpacaEval. The
differences between these prediction algorithms are
insignificant, as shown in Figures 3 and 4.
C Hyperparameters
We include the key hyperparameters for our experi-
ments in the following tables.
Hyperparameter Value
seed 42, 43, . . ., 51
num_train_epochs 1
gradient_accumulation_steps 2
learning_rate 3e-6
weight_decay 0.0
warmup_ratio 0.03
optimizer adamw_torch
lr_scheduler_type cosine
Table 2: Parameters for reward modeling.

--- PAGE 8 ---
Hyperparameter Value
seed 42, 43, . . ., 51
num_train_epochs 1
gradient_accumulation_steps 2
learning_rate 5e-5
weight_decay 0.0
warmup_ratio 0.03
optimizer adamw_torch
lr_scheduler_type constant
Table 3: Parameters for reward modeling with LoRA.
Hyperparameter Value
rollout_batch_size 64
step_batch_size 32
learning_rate 1e-5
warmup_steps 5
epoch_num 2
optimizer adamw_torch
kl_divergence_coefficient 0.02
value_function_coefficient 0.1
Table 4: PPO parameters.
Hyperparameter Value
temperature 1.0
max_new_tokens 300
top_p 0.9
Table 5: Decoding parameters for Best-of- n.
Hyperparameter Value
temperature 0.7
max_new_tokens 300
top_p 0.9
Table 6: Decoding parameters for PPO.
LoRA-based Ensemble (Mean)
LoRA-based Ensemble (LCB,  = 0.1)
LoRA-based Ensemble (LCB,  = 0.5)
50 100 150 200
n (in Best-of-n)36384042444648Win Rate (%)
Figure 3: Lora-based ensemble with LCB and different
βvalues.
Linear-layer Ensemble (Mean)
Linear-layer Ensemble (LCB,  = 0.1)
Linear-layer Ensemble (LCB,  = 0.5)
50 100 150 200
n (in Best-of-n)36384042444648Win Rate (%)
Figure 4: Linear-layer ensemble with LCB and different
βvalues.
