# 2206.00059.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2206.00059.pdf
# Kích thước tệp: 1373428 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Phương pháp Hỗn hợp Chuyên gia
cho Quản lý Đối thoại dựa trên RL
Yinlam Chow Aza Tulepbergenov Oﬁr Nachum
Moonkyung Ryu Mohammad Ghavamzadeh Craig Boutilier
Google Research
{yinlamchow, atulep, ofirnachum, mkryu, ghavamza, cboutilier}@google.com

Tóm tắt
Dù đã có những tiến bộ gần đây trong các mô hình ngôn ngữ (LM), việc ứng dụng chúng vào các vấn đề quản lý đối thoại (DM) và khả năng tiến hành các cuộc trò chuyện phong phú vẫn là một thách thức. Chúng tôi sử dụng học tăng cường (RL) để phát triển một tác nhân đối thoại tránh việc thiển cận (xuất ra các phát ngôn chung chung) và tối đa hóa sự hài lòng tổng thể của người dùng. Hầu hết các phương pháp RL hiện có cho DM đều huấn luyện tác nhân ở cấp độ từ, và do đó, phải đối phó với một không gian hành động phức tạp theo tổ hợp ngay cả với một từ vựng có kích thước vừa phải. Do đó, chúng khó có thể tạo ra một cuộc đối thoại thành công và hấp dẫn ngay cả khi được khởi động nóng với một LM được huấn luyện trước. Để giải quyết vấn đề này, chúng tôi phát triển một DM dựa trên RL sử dụng một mô hình ngôn ngữ hỗn hợp chuyên gia (MoE-LM) mới bao gồm (i) một LM có khả năng học các ngữ nghĩa đa dạng cho lịch sử hội thoại, (ii) một số LM chuyên biệt (hoặc chuyên gia) có khả năng tạo ra các phát ngôn tương ứng với một thuộc tính hoặc tính cách cụ thể, và (iii) một DM dựa trên RL thực hiện lập kế hoạch đối thoại với các phát ngôn được tạo ra bởi các chuyên gia. Phương pháp MoE của chúng tôi cung cấp sự linh hoạt lớn hơn để tạo ra các phát ngôn hợp lý với các ý định khác nhau và cho phép RL tập trung vào DM cấp độ hội thoại. Chúng tôi so sánh nó với các đường cơ sở SOTA trên các cuộc đối thoại miền mở và chứng minh hiệu quả của nó cả về tính đa dạng và tính hợp lý của các phát ngôn được tạo ra cũng như hiệu suất DM tổng thể.

1 Giới thiệu
Với những tiến bộ to lớn trong hiểu và tạo sinh ngôn ngữ tự nhiên, ngày càng có nhiều sự chú ý được hướng đến việc xây dựng các tác nhân đối thoại thông minh có thể tiến hành các cuộc trò chuyện phong phú và hấp dẫn với người dùng. Những tương tác như vậy có thể khá mở rộng, chứa nhiều chủ đề khác nhau, và thường liên quan đến một nhiệm vụ cơ bản, chẳng hạn như xây dựng mối quan hệ, đàm phán, trao đổi thông tin và khuyến nghị. Do đó, để làm hài lòng người dùng, một tác nhân đối thoại tốt không chỉ nên tạo ra những phản hồi tự nhiên, mà còn có khả năng theo đuổi các mục tiêu của nhiệm vụ và thích ứng với phản hồi của người dùng một cách linh hoạt.

Một giải pháp tiêu chuẩn là huấn luyện tác nhân đối thoại bằng cách sao chép hành vi, trong đó tác nhân là một mô hình ngôn ngữ (LM) bắt chước các phát ngôn trong tập huấn luyện [Gaši ́c et al., 2011, Fatemi et al., 2016]. Bằng cách tận dụng các mạng nơ-ron sâu, ví dụ RNN [Sutskever et al., 2014] và Transformer [Vaswani et al., 2017], một LM mã hóa cuộc hội thoại thành một trạng thái đối thoại có chiều thấp và dự đoán một phát ngôn, nhưng việc điều khiển việc tạo sinh như vậy cho các mục đích cụ thể vẫn là một câu hỏi mở. Một số công trình đã nghiên cứu các cách để tinh chỉnh một LM để tạo ra văn bản với các ngữ cảnh cụ thể [Ziegler et al., 2019, Ficler and Goldberg, 2017]. Các kết quả khác đã học một LM có thể điều khiển duy nhất có khả năng tạo ra các phát ngôn cho nhiều ý định cụ thể [Gu et al., 2017, Chen et al., 2018, Subramani et al., 2019, Dathathri et al., 2019]. Mặc dù những LM này tạo ra những phản hồi trôi chảy và phù hợp, vẫn chưa rõ làm thế nào để kiểm soát chúng để có hệ thống theo đuổi các mục tiêu trong các cuộc hội thoại nhiều lượt.

Một phương pháp phổ biến khác là xem quản lý đối thoại (DM) như một vấn đề điều khiển và sử dụng học tăng cường (RL) để tối ưu hóa chính sách của tác nhân (thường là chính LM). Việc sử dụng RL cho các hệ thống đối thoại có một lịch sử lâu dài. Công trình sớm dựa vào các trạng thái ngữ nghĩa cụ thể, được chế tác thủ công [Levin and Pieraccini, 1997, Singh et al., 2002, Walker, 2000] hoặc các trạng thái niềm tin có thể quan sát một phần [Williams and Young, 2007, Young et al., 2010], trong đó tác nhân chọn hành động đối thoại được chế tác thủ công tốt nhất ở mỗi lượt, với mục tiêu làm hài lòng người dùng [Shah et al., 2018], hoàn thành nhiệm vụ [Shi and Yu, 2018], hoặc phản hồi truy vấn của người dùng [Serban et al., 2017a]. Tuy nhiên, việc ứng dụng các phương pháp này bị giới hạn vào các vấn đề mà không gian hành động có thể được nắm bắt bằng các biểu diễn được chế tác thủ công, và chúng không thể xử lý các cuộc hội thoại phức tạp. Mặt khác, các phương pháp gần đây hơn sử dụng học sâu để trích xuất các biểu diễn ngữ nghĩa từ lịch sử hội thoại, coi các biểu diễn này như các trạng thái niềm tin đối thoại, và áp dụng RL để học một tác nhân DM tạo sinh ở cấp độ từ [Jaques et al., 2019, Li et al., 2016, 2017, Shin et al., 2020]. Tuy nhiên, vì có vô số khả năng của các phát ngôn ngôn ngữ, và do đó, không gian hành động của bài toán RL cực kỳ lớn, tác nhân thường thực hiện lập kế hoạch kém và tạo ra các phát ngôn không thể hiểu được [Zhao et al., 2019]. Một vấn đề khác là RL chỉ tối ưu hóa một phần thưởng vô hướng, trong khi các phương pháp được đề cập ở trên thường cần tối ưu hóa cả chất lượng của phát ngôn được tạo ra, ví dụ như dễ trả lời [Li et al., 2016] và tính trôi chảy [Li et al., 2017, 2019], và mục tiêu, ví dụ như độ dài hội thoại [Zhou et al., 2020], tình cảm của người dùng [Hancock et al., 2019], và hoàn thành nhiệm vụ [Verma et al., 2022]. Hơn nữa, việc định nghĩa phần thưởng như tổ hợp có trọng số của các chỉ số này không lý tưởng, vì các trọng số được chọn thủ công thường không phản ánh các tiêu chí thành công cơ bản.

Để giải quyết các vấn đề trên liên quan đến việc sử dụng RL trong các hệ thống quản lý đối thoại (DM), chúng tôi đề xuất một tác nhân DM dựa trên RL sử dụng phương pháp hỗn hợp chuyên gia (MoE) mới. Phương pháp MoE của chúng tôi dựa trên một mô hình ngôn ngữ hỗn hợp chuyên gia (MoE-LM), bao gồm ba thành phần chính: 1) một LM (một bộ mã hóa xác suất và một bộ giải mã) có khả năng học các ngữ nghĩa đa dạng cho lịch sử hội thoại, và do đó tạo ra các phát ngôn đa dạng, mà chúng tôi gọi là LM nguyên thủy hoặc LM0, 2) một số LM chuyên biệt (hoặc chuyên gia), {LMi}m_{i=1}, mỗi cái được xây dựng bằng cách sử dụng không gian tiềm ẩn được học bởi LM0, nhưng đã được huấn luyện sao cho nó có khả năng tạo ra các phát ngôn tương ứng với một ý định hoặc tính cách nhất định, và 3) một trình quản lý đối thoại (DM) dựa trên RL mà tại mỗi lượt, với trạng thái tiềm ẩn được chia sẻ bởi các chuyên gia {LMi}m_{i=0} và (các) hành động phát ngôn mà chúng đề xuất, chọn một trong số chúng để tác nhân thực thi. MoE-LM của chúng tôi có thể được xem như một trường hợp đặc biệt của các LM phân cấp (ví dụ, Serban et al. 2017a, Zhao et al. 2019, Saleh et al. 2020), nhưng nó khác với chúng vì nó học cả các LM (chuyên gia) và DM. Hơn nữa, DM trong MoE-LM là một chính sách được điều kiện hóa trên cả trạng thái tiềm ẩn và các hành động được đề xuất bởi các chuyên gia, chứ không chỉ trạng thái như thường thấy trong RL phân cấp. LM nguyên thủy (LM0) đóng vai trò quan trọng trong mô hình này vì nó học các ngữ nghĩa đa dạng cho lịch sử hội thoại và cho phép tác nhân tạo ra nhiều loại phát ngôn khác nhau. Sự đa dạng này cũng được chia sẻ với các LM chuyên biệt (chuyên gia) và mang lại cho chúng sự linh hoạt trong việc tạo ra các phát ngôn (chuyên biệt hơn) của chúng. Một đặc điểm quan trọng khác của MoE-LM là tính mô-đun của nó giúp thuận lợi cho việc thêm và loại bỏ các LM chuyên biệt (chuyên gia). Hơn nữa, kiến trúc phân cấp này cho phép chúng tôi giải quyết một bài toán RL với không gian trạng thái và hành động nhỏ hơn nhiều, điều này khá quan trọng trong chất lượng của chính sách được học. Cuối cùng, vì các phát ngôn ứng viên được tạo ra bởi các chuyên gia với các ý định khác nhau, thay vì kết hợp tất cả các tín hiệu tác nhân-người dùng thành một phần thưởng RL duy nhất, tác nhân DM của chúng tôi có thể tập trung vào việc tối ưu hóa mục tiêu cụ thể của nhiệm vụ hội thoại.

Chúng tôi bắt đầu bài báo với phần giới thiệu ngắn gọn về các LM và việc sử dụng các quá trình quyết định Markov (MDP) trong việc mô hình hóa các vấn đề quản lý đối thoại trong Phần 2. Sau đó chúng tôi mô tả kiến trúc tổng thể của MoE-LM trong Phần 3, tiếp theo là việc triển khai chi tiết từng thành phần chính của nó (được mô tả trong đoạn trên) trong các Phần 4 đến 6. Cuối cùng, trong Phần 7, chúng tôi chứng minh hiệu quả của MoE-LM trong các cuộc đối thoại miền mở, về cả khả năng tạo ra các phát ngôn đa dạng và hợp lý và hiệu suất DM tổng thể.

2 Kiến thức cơ bản
Mô hình Ngôn ngữ (LM) Trong công việc này, chúng tôi sử dụng các LM seq2seq để tạo ra các phát ngôn tiếp theo trong một cuộc đối thoại. Chúng tôi giả định có quyền truy cập vào một tập dữ liệu có dạng D = {(X^{(k)}, Y^{(k)})}^{|D|}_{k=1}, trong đó mỗi X = X^{(k)} là một lịch sử hội thoại L-lượt X = {X_l}^{L-1}_{l=0} và Y là phát ngôn tiếp theo của nó. Chúng tôi ký hiệu

--- TRANG 2 ---
bằng N_X, một cận trên về độ dài (số token) của mỗi phát ngôn X_l trong X.^1 Vai trò của một LM là dự đoán xác suất của phát ngôn tiếp theo Y, bao gồm N token, có điều kiện trên lịch sử hội thoại X, tức là p(Y = {y_n}^N_{n=1} | X). Trong kiến trúc transformer [Wolf et al., 2019], LM trước tiên mã hóa lịch sử hội thoại X bằng một bộ mã hóa thành một chuỗi nhúng có độ dài (L×N_X) {(z_{l,0}, ..., z_{l,N_X-1})}^{L-1}_{l=0}, trong đó mỗi z_{l,n} là một vector trong không gian tiềm ẩn. Để thuận tiện cho ký hiệu, chúng tôi nối các nhúng này thành một nhúng duy nhất z ∈ Z ⊆ ℝ^d và ký hiệu chiều tổng thể của không gian tiềm ẩn là d. Trong kiến trúc RNN [Serban et al., 2016], bộ mã hóa của LM trực tiếp ánh xạ lịch sử hội thoại X thành một trạng thái tiềm ẩn z ∈ Z ⊆ ℝ^d. Trong cả hai kiến trúc, phát ngôn tiếp theo Ŷ = {ŷ_n}^N_{n=1} được lấy mẫu từng token từ bộ giải mã Ψ, tức là Ŷ ~ Ψ(·|z) = ∏^N_{n=1} Ψ(ŷ_n | ŷ_0, ..., ŷ_{n-1}, z), trong đó y_0 là một token ban đầu cố định (bắt đầu câu) [Chien and Kuo, 2019], và trạng thái tiềm ẩn được ký hiệu là z = Φ(X).^2

Các Quá trình Quyết định Markov (MDP) đã được sử dụng để mô hình hóa các vấn đề quản lý đối thoại trong nhiều bối cảnh khác nhau [Li et al., 2016, Asadi and Williams, 2016, Jaques et al., 2019]. Trong các MDP như vậy, được ký hiệu bởi M = (S, A, P, r, s_0, γ), không gian trạng thái S biểu diễn lịch sử hội thoại được token hóa và trạng thái ban đầu s_0 ∈ S là truy vấn ban đầu của người dùng. Không gian hành động A cũng là không gian ngôn ngữ được token hóa với mỗi hành động a ∈ A là phát ngôn tiếp theo của tác nhân (là một chuỗi token có độ dài cố định, N_X). Hạt nhân chuyển tiếp P mô hình phản hồi của người dùng đối với hành động được thực hiện bởi tác nhân (bot). Cuối cùng, hàm phần thưởng r đo lường sự hài lòng của người dùng. Trong các MDP này, chúng ta có thể nghĩ về toàn bộ LM như một chính sách ánh xạ lịch sử hội thoại thành các phát ngôn tiếp theo, và giải quyết chúng bằng cách tìm một chính sách với lợi nhuận chiết khấu kỳ vọng tối đa, tức là π* ∈ arg max_π J_π := E_π[∑^∞_{k=0} γ^t r_t | P, s_0, π]. Lưu ý rằng kích thước của không gian trạng thái và hành động được token hóa tăng theo cấp số nhân với kích thước của từ vựng. Điều này làm cho việc giải quyết MDP trở nên không khả thi ngay cả với từ vựng có kích thước vừa phải. Do đó, sẽ khá mong muốn phát triển một mô hình MDP mới dễ áp dụng hơn cho các hệ thống DM dựa trên RL.

3 Mô hình Ngôn ngữ Hỗn hợp Chuyên gia (MoE)
Chúng tôi bắt đầu bằng cách giải thích làm thế nào một mô hình ngôn ngữ hỗn hợp chuyên gia (MoE-LM) có thể làm phong phú các phát ngôn của bot và cải thiện hiệu suất tổng thể của DM. Mặc dù phương pháp của chúng tôi có thể áp dụng cho bất kỳ hệ thống DM nào, chúng tôi sử dụng đối thoại miền mở [Sankar et al., 2019] như một ví dụ để cho thấy cách các tác nhân dựa trên MoE-LM có thể cải thiện sự hài lòng của người dùng được đo bằng sự cải thiện về tình cảm hoặc sự tham gia. Một cách trực quan, một tác nhân DM tốt nên có các hành vi khác nhau (ví dụ: tò mò, khám phá, phù hợp, an ủi, đồng cảm, khen ngợi, khiêu khích) và nhanh chóng quyết định ý định nào để sử dụng để xoay chuyển cuộc hội thoại, xây dựng mối quan hệ, khơi gợi sự quan tâm của người dùng, cải thiện tâm trạng của họ, v.v.

Để đạt được mục tiêu này, chúng tôi yêu cầu LM có một biểu diễn ngôn ngữ (khám phá nguyên thủy) nắm bắt các ngữ nghĩa khác nhau, để mã hóa các cuộc hội thoại khác nhau và tránh tạo ra các phản hồi tẻ nhạt và lặp đi lặp lại. Chúng tôi cũng cần một cơ chế (xây dựng chuyên gia) để nhúng các ý định khác nhau vào các mô hình con của LM này, để chúng có thể hành xử tương ứng khi được nhắc nhở, và phản hồi hiệu quả. Cuối cùng, với các phát ngôn ứng viên khác nhau có sẵn, mô-đun DM của LM này nên hiểu mức độ hài lòng hiện tại của người dùng và xác định phản hồi nào phù hợp nhất. Được thúc đẩy bởi những quan sát này, chúng tôi xây dựng MoE-LM trong ba bước như được hiển thị trong Hình 1. Chúng tôi đưa ra ý tưởng chính đằng sau mỗi bước ở đây và để lại các mô tả chi tiết của chúng cho các Phần 4, 5, và 6.

Bước 1: Khám phá Nguyên thủy. Chúng tôi trước tiên sử dụng tập dữ liệu D, được giới thiệu trong Phần 2, và học một mô hình ngôn ngữ LM0 = (Φ, G0, Ψ) bao gồm một bộ mã hóa ngẫu nhiên (tức là một bộ mã hóa và một phân phối không gian tiềm ẩn G0 ánh xạ cuộc hội thoại được mã hóa thành một phân phối tiềm ẩn), và một bộ giải mã Ψ. Bộ mã hóa ngẫu nhiên (Φ, G0) bao gồm một bộ mã hóa ánh xạ lịch sử hội thoại được token hóa X thành một không gian tiềm ẩn Z ⊆ ℝ^d, tức là z = Φ(X) ∈ Z, sau đó được sử dụng để xây dựng một phân phối Gaussian có tham số d-chiều G0(z'|z) = N(μ0(z), σ²0(z)I_d×d) trên ℝ^d. Bộ giải mã dự đoán phát ngôn tiếp theo Ŷ0 (từng token) có điều kiện trên điểm z' được lấy mẫu từ phân phối tiềm ẩn, tức là Ψ(Ŷ0|z'); z' ~ G0(·|z). Chúng tôi ký hiệu bởi LM0(Y|X) := E_{z'~G0(·|z), z=Φ(X)}[Ψ(Y|z')], nguyên thủy và học nó bằng cách sử dụng một hàm mất mát mà ngoài việc dự đoán phát ngôn tiếp theo một cách chính xác, còn khuyến khích tính đa dạng và khái quát hóa trong không gian tiềm ẩn Z đã học (xem Eq. 1 và Hình 2). Như chúng tôi sẽ giải thích trong Phần 4, hàm mất mát của chúng tôi được lấy cảm hứng từ các công trình trước đó, và cụ thể hơn là bởi

^1 Nếu phát ngôn thực tế X_l có ít token hơn N_X, nó sẽ được đệm bằng một token cụ thể và được che dấu.
^2 Lưu ý rằng chúng tôi sử dụng Y như phát ngôn tiếp theo trong tập dữ liệu và Ŷ như phát ngôn được dự đoán bởi LM.
^3 Trong thực tế, chúng ta có thể sử dụng cả trạng thái tiềm ẩn như đầu vào cho mô hình bộ giải mã Ψ(Ŷ0|z', z).

--- TRANG 3 ---
mục tiêu trong OPAL [Ajay et al., 2020], tức là một phương pháp học không giám sát để khám phá các kỹ năng nguyên thủy trong các quỹ đạo được sử dụng bởi một số nhiệm vụ RL downstream.

Bước 2: Xây dựng Chuyên gia. Với không gian tiềm ẩn Z, bộ mã hóa (Φ, G0), và bộ giải mã Ψ được học ở Bước 1, giờ đây chúng tôi học m phân phối tiềm ẩn {Gi}^m_{i=1}, mỗi phân phối được định nghĩa là Gi(z'|z) = N(μi(z), σ²i(z)I_d×d). Một cách trực quan, mỗi Gi tương ứng với một thuộc tính, ví dụ như một ý định hoặc tính cách (trong trường hợp chatbot) và tạo ra các mẫu trong các phần cụ thể của không gian tiềm ẩn Z. Điều này dẫn đến việc có m LM, {LMi}^m_{i=1}; LMi = (Φ, Gi, Ψ), mỗi LM tương ứng với một phiên bản chuyên biệt của LM gốc, LM0, và đóng vai trò là một chuyên gia trong MoE-LM của chúng tôi. Khi nhận được lịch sử hội thoại X, mỗi chuyên gia LMi tạo ra một ứng viên (hoặc nhiều hơn) cho phát ngôn tiếp theo Ŷi trong các phần nhất định của không gian ngôn ngữ tương thích với thuộc tính (tính cách) của nó. Như chúng tôi sẽ giải thích trong Phần 5, mỗi Gi được học bằng cách sử dụng một hàm mất mát khuyến khích LM tương ứng của nó, LMi, tạo ra các phát ngôn phù hợp với thuộc tính của nó (xem Eq. 2).

Bước 3: Trình Quản lý Đối thoại (DM). Trình quản lý đối thoại, được ký hiệu bởi μ, nhận đầu vào là lịch sử hội thoại được mã hóa z = Φ(X) và các phát ngôn hành động ứng viên được tạo ra bởi các chuyên gia {Ŷi}^m_{i=0}, và chọn một trong số chúng làm hành động cho bot thực thi, tức là Ŷ ~ μ(·|z, {Ŷi}^m_{i=0}). Chúng tôi sẽ mô tả cách DM được huấn luyện bằng cách sử dụng học tăng cường (RL) trong Phần 6.

4 Khám phá Nguyên thủy trong MoE-LM
Được thúc đẩy bởi tài liệu trong các lĩnh vực học tăng cường và học bắt chước [Ajay et al., 2020], chúng tôi đề xuất học LM nguyên thủy, LM0, trong MoE-LM bằng cách giải quyết bài toán tối ưu có ràng buộc KL sau đây nhằm nắm bắt ngữ nghĩa đa dạng:

min_{(Φ,G0,Ψ)} Ê_{z'~ρ(·|z,Y); z=Φ(X)}[-log Ψ(Y|z')];
s.t. Ê_{z=Φ(X)}[KL(ρ(z'|z,Y)||G0(z'|z))] ≤ β_KL, (1)

trong đó Ê là kỳ vọng thực nghiệm trên (X,Y) trong tập dữ liệu D, ρ là một phân phối trên không gian tiềm ẩn có điều kiện trên lịch sử hội thoại được mã hóa z và phát ngôn mục tiêu Y, và β_KL là một ngưỡng thực dương. Sử dụng (1), chúng tôi học LM0 = (Φ, G0, Ψ) bằng cách tối đa hóa log-likelihood, trong khi thực thi tính nhất quán giữa biến tiềm ẩn z' được dự đoán bởi G0(·|z) và ρ(·|z,Y) thông qua ràng buộc KL. Phân phối ρ(·|z,Y) là một Gaussian N(μ_ρ(z,Φ(Y)), σ²_ρ(z,Φ(Y))I_d×d) trong đó Φ là một bộ mã hóa được huấn luyện trước cho phát ngôn mục tiêu Y, và mean μ_ρ(·,·) và variance σ²_ρ(·,·) là các mô hình có thể huấn luyện. Một lý do để sử dụng một bộ mã hóa riêng biệt cho phát ngôn mục tiêu Y là để tránh overfitting (tức là tránh có gradient lan truyền ngược của Φ với Y như đầu vào).

Kết nối với các mục tiêu giống VAE Trong thực tế, chúng tôi triển khai ràng buộc KL trong (1) như một penalty được trọng số bởi một hệ số được chọn phù hợp. Do đó, người ta có thể diễn giải mục tiêu trong (1) như một biến thể của β-VAE [Burgess et al., 2018]. Do kết nối với VAE, người ta có thể rút ra những điểm tương đồng giữa phương pháp của chúng tôi và các phương pháp đối thoại hiện có như VHRED [Serban et al., 2017b] và VHCR [Park et al., 2018]. Tuy nhiên, chúng tôi nhấn mạnh rằng có những khác biệt chính, và những điều này có thể được giải thích bằng cách trước tiên hiểu cách mục tiêu trong (1) khuyến khích tính đa dạng, điều này là chìa khóa cho việc học nguyên thủy tốt. Cụ thể, điều quan trọng là khám phá nguyên thủy học một bộ mã hóa-giải mã Φ;Ψ có thể được điều chế bởi việc lựa chọn z; tức là thay đổi z' trong khi cố định X nên dẫn đến các phân phối khác nhau trên các phát ngôn được tạo ra. Mục tiêu trong (1) khuyến khích tính đa dạng này bằng cách điều kiện hóa biến tiềm ẩn z' trên cả phát ngôn mục tiêu Y và z = Φ(X), tức là z' ~ ρ(·|z,Y). Ngược lại, ràng buộc KL được sử dụng để đảm bảo rằng bộ mã hóa ngẫu nhiên G0(·|z) của LM nguyên thủy của chúng tôi không quá biến đổi cho các Y khác nhau, và do đó có hiệu ứng hạn chế đối với tính đa dạng. Ví dụ, trong trường hợp cực đoan khi β_KL = 0 (hoặc β → ∞ khi được sử dụng như một regularizer) sẽ không có sự chuyên biệt hóa của không gian tiềm ẩn cho các Y khác nhau. Mặc dù β → ∞ là một trường hợp cực đoan, hành vi suy thoái cũng có thể xảy ra khi β = 1, tức là trong loss biến phân truyền thống được sử dụng bởi VHRED và VHCR. Cụ thể, điều đã biết là loss VAE truyền thống là một cận trên của negative log-likelihood của dữ liệu dưới một tham số hóa bộ mã hóa-giải mã ngẫu nhiên. Do đó nếu dữ liệu có thể được mô hình hóa bởi một LM duy nhất thì một bộ giải mã Ψ tối ưu VAE có thể đơn giản bỏ qua G0, dẫn đến một không gian tiềm ẩn suy thoái như đã quan sát thấy trong công việc trước đó [Park et al., 2018]. Đây chính xác là lý do mà, trong phương pháp của chúng tôi, chúng tôi làm yếu ràng buộc KL (β_KL ≥ 0 hoặc tương đương, β ≤ 1). Điều này cho phép phương pháp của chúng tôi đảm bảo đáng tin cậy hơn rằng một z' duy nhất đại diện cho mỗi cặp hội thoại riêng biệt (X,Y), do đó nắm bắt các phương thức ngữ nghĩa đa dạng và cho phép chuyên biệt hóa downstream dễ dàng hơn.

Trong các kết quả toán học dưới đây, chúng tôi chính thức hóa tuyên bố ở trên, cụ thể là mục tiêu log-likelihood trong (1) dẫn đến một Φ;Ψ được học có thể dễ dàng khôi phục bất kỳ LM mong muốn tùy ý nào bằng cách chuyên biệt hóa không gian tiềm ẩn G. Chúng tôi bắt đầu với một định nghĩa đặc trưng cho phạm vi phủ sóng của một LM tùy ý trên phân phối dữ liệu hội thoại có điều kiện P_D(Y|X).

Định nghĩa 1. LM_{D,ε} là một ε-common LM của dữ liệu D nếu E_D[TV(LM_{D,ε}(Y|X)||P_D(Y|X)))] ≤ ε.

Tận dụng Định lý 4.1 trong Ajay et al. [2020], giờ đây chúng tôi trình bày kết quả lý thuyết đặc trưng cho sức mạnh biểu diễn của cặp bộ mã hóa-giải mã nguyên thủy (Φ,Ψ) của chúng tôi trên dữ liệu D.

Bổ đề 1. Cho (Φ*,G*,Ψ*) là nghiệm của (1) với Ê_{z'~ρ(·|z,Y); z=Φ(X)}[-log Ψ(Y|z')] = ε. Khi đó tồn tại LM* := (Φ*,G*,Ψ*) sao cho E_D[TV(LM_{D,ε}(Y|X)||LM*(Y|X))] ≤ ε + √(1/2(ε + H)), trong đó G*(z'|z) = E_{Y~D}[ρ(z'|z,Y)], và H = E_D[log P_D(Y|X)] là một hằng số phụ thuộc vào D.

Kết quả trên cho thấy rằng, miễn là LM_{D,ε} là ε-common trong D, thì tồn tại một chuyên biệt hóa của không gian tiềm ẩn G mà, khi được ghép nối với Φ;Ψ, có thể khôi phục gần đúng LM_{D,ε}. Chất lượng của xấp xỉ là một hàm của ε - mức độ tối ưu hóa mục tiêu trong (1) - và ε. Trong thực tế, để xây dựng nguyên thủy bằng cách thay thế G bằng G0, tức là LM0 = (Φ,G0,Ψ), vì G0(z'|z) có thể được xem như một distillation của ρ(z'|z,Y). Kết quả lý thuyết này cũng thúc đẩy phần tiếp theo, nơi chúng tôi giải thích "Bước 2: Xây dựng Chuyên gia" của thuật toán. Cụ thể, chúng tôi cho thấy cách sử dụng cặp bộ mã hóa-giải mã được huấn luyện Φ;Ψ để học một phạm vi các chuyên gia chuyên biệt khác nhau được tham số hóa bởi các phân phối tiềm ẩn Gi khác nhau.

5 Xây dựng Chuyên gia với Mô hình Ngôn ngữ Plug-and-play
Để hoàn thành khung MoE, người ta cần tạo ra một cách có hệ thống một dải các chuyên gia LMi khác nhau, ∀i ∈ {1,...,m}, với mỗi chuyên gia tạo ra các phát ngôn ứng viên có ý định khác nhau. Bằng cách xem mỗi chuyên gia như một phân phối của các hành vi cụ thể trong dữ liệu hội thoại D, chúng tôi tận dụng các kết quả của Phần 4 và Bổ đề 1 và áp dụng một bộ mã hóa-giải mã phổ quát (Φ,Ψ) trong tất cả các chuyên gia. Do đó, mỗi chuyên gia i chỉ được tham số hóa bởi một phân phối tiềm ẩn d-chiều tùy ý (ví dụ: Gaussian), và nó lấy mẫu các vùng nhất định của không gian tiềm ẩn Z. Theo thuật ngữ từ Dathathri et al. [2019], tất cả các chuyên gia này có thể được phân loại như các mô hình ngôn ngữ plug-and-play (PPLM). Việc tạo ra các chuyên gia rất tiện lợi vì nó chỉ yêu cầu học các phân phối tiềm ẩn mới, trong khi việc chuyển đổi giữa các chuyên gia tương đương với việc lấy mẫu một phân phối khác.

Ký hiệu bởi ℓi(X,Y) ∈ ℝ một nhãn có giá trị thực đặc trưng cho ý định của chuyên gia i ∈ {1,...,m}, ví dụ được xác định bởi một bộ phân loại sentiment off-the-shelf. Chúng tôi huấn luyện phân phối tiềm ẩn Gi(z) của chuyên gia i bằng cách giải quyết bài toán tối ưu

min_{Gi} Ê_{z'~Gi(·|z); z=Φ(X); Y~Ψ(·|z')}[-ℓi(X,Y)]. (2)

Không giống như phương pháp maximum likelihood có trọng số được xem xét trong Dathathri et al. [2019], gán trọng số ℓi cho các mẫu huấn luyện tương ứng với chuyên gia i, chúng tôi đề xuất học mỗi chuyên gia thông qua tối đa hóa phần thưởng và coi ℓi như một tín hiệu phần thưởng w.r.t. chuyên gia i cần được tối đa hóa. Thú vị là, phương pháp này cũng liên kết với học tăng cường (RL), trong đó cả không gian "trạng thái" và "hành động" đều là không gian tiềm ẩn Z, và "chính sách" là phân phối tiềm ẩn Gi. Lợi ích chính của phương pháp chúng tôi là nó không yêu cầu phát ngôn mục tiêu Y từ dữ liệu D và do đó ít dễ bị ảnh hưởng bởi các vấn đề mất cân bằng dữ liệu trong D về các ý định nhất định. Lưu ý từ (2) rằng bài toán tối đa hóa phần thưởng là cận thị, tức là bài toán RL ở trên có hệ số chiết khấu bằng 0. Động lực chính là, không giống như quản lý đối thoại là một bài toán ra quyết định tuần tự, ở đây chúng tôi muốn mỗi chuyên gia có các hành vi cụ thể, và điều này có thể được thực hiện dễ dàng thông qua tối đa hóa tham lam; Tối ưu hóa đối thoại dài hạn sẽ được xử lý bởi trình quản lý đối thoại chứ không phải các chuyên gia này.

Ví dụ trong trường hợp của một Gaussian Gi, chúng tôi sử dụng thuật toán REINFORCE tiêu chuẩn [Sutton et al., 1999] để học các tham số mô hình (μi, σ²i) của Gi theo

{μi, σi} ← {μi, σi} + α E_{z'~Gi(·|z); Y~Ψ(·|z')}[ℓi(X,Y) ∇_{μi,σi} log P_{Gi}(z'|z)], i ∈ {1,...,m},

trong đó α > 0 là tốc độ học. Để giảm variance của các ước lượng này, chúng tôi cũng áp dụng kỹ thuật giảm baseline [Greensmith et al., 2004] trong policy gradient. Điều này có thể được thực hiện đơn giản bằng cách thay thế ℓi(X,Y) bằng ℓ̃i(X,Y) := ℓi(X,Y) - E_{Y~Ψ(·|Φ(X))}[ℓi(X,Y)]. Để định lượng hiệu suất của chuyên gia LMi, theo các lập luận từ Bổ đề 1 và Bổ đề 4.0.1 trong Ajay et al. [2020], chúng tôi có kết quả về tính dưới tối ưu.

Hệ quả 1. Ký hiệu mục tiêu tối đa hóa phần thưởng thứ i là Li(LM) := Ê_{Y~LM(·|X)}[ℓi(X,Y)]. Giả sử một LM tối ưu cho mục tiêu này LM*_{i,ε} ∈ arg max_{LM} Li(LM) là ε-common trong D. Hơn nữa, cho G*_i trong arg min của (2). Khi đó với chuyên gia LMi = (Φ, G*_i, Ψ) và (ε,H) từ Bổ đề 1, chúng ta có |Li(LMi) - Li(LM*_{i,ε})| ≤ 2||ℓi||₁(ε + √(1/2(ε + H))).

Mặc dù có thể rõ ràng rằng việc tối ưu hóa Gi w.r.t. (2) khuyến khích chuyên gia LMi nắm bắt các hành vi được khuyến khích bởi ℓi, hệ quả này có hai hàm ý xa hơn: (i) Vì tính dưới tối ưu của LMi so với oracle LM*_{i,ε} được giới hạn bởi đại lượng được định nghĩa trong Bổ đề 1, nó biện minh cho việc sử dụng nguyên thủy (Ψ,Φ), tối ưu hóa ε, cho việc xây dựng chuyên gia; (ii) Tính dưới tối ưu càng phụ thuộc vào ε, định lượng mức độ LM*_{i,ε} được biểu diễn trong tập dữ liệu gốc D.

6 Học Tăng cường cho Trình Quản lý Đối thoại MoE-LM
Giờ đây chúng tôi mô tả trình quản lý đối thoại (DM) của MoE-LM và đề xuất các thuật toán RL để huấn luyện nó. Như đã đề cập trong Phần 3, DM là một chính sách nhận lịch sử hội thoại được mã hóa z = Φ(X) và m+1 phát ngôn hành động ứng viên được tạo ra bởi các chuyên gia {Ŷi}^m_{i=0},^4 và chọn ngẫu nhiên một trong số chúng để thực thi, tức là Ŷ ~ μ(·|z, {Ŷi}^m_{i=0}). Lưu ý rằng mỗi chuyên gia i ∈ {0,...,m} là một LM, LMi, hoạt động như một chính sách πi(·|X) và ánh xạ mỗi lịch sử hội thoại X thành một phát ngôn Ŷi. Với kiến trúc này, chúng tôi giải quyết kích thước lớn của không gian trạng thái và hành động trong MDP gốc tăng theo cấp số nhân với kích thước của từ vựng. Như đã mô tả trong Phần 2, không gian trạng thái và hành động của MDP gốc lần lượt là lịch sử hội thoại được token hóa và không gian ngôn ngữ được token hóa, trong khi ở đây DM nên chọn giữa m+1 hành động với không gian tiềm ẩn Z của các hội thoại được mã hóa. Quan trọng là lưu ý rằng MoE-LM của chúng tôi khác với các kiến trúc phân cấp khác [Dayan và Hinton, 2016] trong đó quyết định ở cấp cao là chỉ chọn một bộ điều khiển cấp thấp dựa trên trạng thái hiện tại của hệ thống. Trong MoE-LM, DM quan sát cả trạng thái hiện tại và các hành động được đề xuất bởi các chuyên gia và sau đó chọn một trong số chúng.

Chúng tôi học chính sách DM bằng cách giải quyết một MDP, mà chúng tôi gọi là MoE-MDP, và ký hiệu nó bởi M̃ = (S̃, Ã, P̃, R̃, s̃₀, γ). Không gian trạng thái của MoE-MDP là tích của không gian tiềm ẩn đã học Z và không gian hành động chung của m+1 chuyên gia, tức là S̃ = Z × A^{m+1}. Không gian hành động của nó bao gồm m+1 chuyên gia, tức là Ã = {0,...,m}. Trạng thái ban đầu của nó là mã hóa của truy vấn ban đầu của người dùng và các phát ngôn được đề xuất bởi các chuyên gia để phản hồi truy vấn này. Quá trình chuyển tiếp vẫn mô hình phản hồi của người dùng nhưng giờ đây trên không gian chung của các trạng thái tiềm ẩn và các hành động của chuyên gia. Hàm phần thưởng giống như trong MDP gốc, tức là r̃(s̃,ã) = r(X,aj), trong đó s̃ = (z, {ai}^m_{i=0}) với ai ~ πi(·|X) và z = Φ(X), và ã ∈ {0,...,m} là chuyên gia được chọn bởi DM. Vì MoE-MDP có số lượng hành động hữu hạn, việc học DM tương đương với việc giải quyết một MDP hành động hữu hạn và tìm một chính sách μ trong simplex m-chiều với phần thưởng chiết khấu tích lũy kỳ vọng tối đa.

^4 Để đơn giản, chúng tôi giả định rằng mỗi chuyên gia chỉ tạo ra một phát ngôn ứng viên tại mỗi bước. Sẽ đơn giản để mở rộng điều này thành nhiều (và thậm chí là số lượng biến đổi) phát ngôn ứng viên.

--- TRANG 4 ---
Chúng tôi sử dụng hai thuật toán RL để giải quyết MoE-MDP và học chính sách DM μ. Thuật toán đầu tiên là conservative Q-learning (CQL) [Kumar et al., 2020], một thuật toán RL offline phổ biến. Điều này phù hợp với setting của chúng tôi trong đó chính sách phải được học từ các cuộc hội thoại thu thập được D mà không có tương tác thêm (online) với hệ thống thực. CQL là một scheme regularization học một Q-function conservative cung cấp cận dưới cho Q-function thực. Với dữ liệu hội thoại offline D, chúng tôi tham số hóa Q-function bằng tham số θ và học θ bằng cách tối thiểu hóa Bellman error với behavior regularization: min_θ ∑_{(s,a,r,s')∈D}(E_a[Q_θ(s,a)] - Q_θ(s,a₀)) + (r + γQ_θ^{target}(s', arg max_{a'∈A}Q_θ(s',a')) - Q_θ(s,a))², trong đó a₀ là hành động được đề xuất bởi LM nguyên thủy (xấp xỉ chính sách hành vi của D), α > 0 là tham số regularization, và θ^{target} là tham số Q-function target. Trực quan, CQL regularization tối thiểu hóa sự khác biệt trong Q-values của DM chúng tôi và nguyên thủy. Theo thuật toán CQL, chúng ta có thể đặt DM là μ(a|s) ∝ exp(Q_θ(s,a)), tương ứng với chính sách tối ưu của entropy-regularized Q-learning [Schulman et al., 2017].

Thuật toán RL thứ hai chúng tôi sử dụng là model-based RL (MBRL) [Shah et al., 2018, Wei et al., 2018]. Ở đây chúng tôi trước tiên học một mô hình phát ngôn người dùng P_{user}(X'|X,a) := E_{z=Φ_{user}([X,a])}[Ψ_{user}(X'|z)] thông qua maximum likelihood, sau đó tạo ra dữ liệu D^{MB}, có next-state s̃' mã hóa cuộc hội thoại tiếp theo được tạo ra từ roll-out và các hành động ứng viên tương ứng, và cuối cùng giải quyết việc tối thiểu hóa Bellman error trong MoE-MDP: min_θ ∑_{(s,a,r,s̃')∈D^{MB}}(r + γQ_θ^{target}(s̃', arg max_{a'∈A}Q_θ(s̃',a')) - Q_θ(s,a))². Lợi ích của MBRL so với CQL là hai mặt. Thứ nhất, người ta có thể dễ dàng có được một mô hình phát ngôn người dùng có độ trung thực cao [Peng et al., 2020] bằng cách đơn giản fine-tune một LM lớn (ví dụ: GPT-3 [Floridi and Chiriatti, 2020]). Thứ hai, với đủ dialogue roll-out nắm bắt nhiều scenario khác nhau, MBRL không yêu cầu behavior regularization và có thể ít conservative hơn.

7 Thí nghiệm
Chúng tôi đánh giá phương pháp MoE cho quản lý đối thoại trên hai nhiệm vụ benchmark miền mở. Nhiệm vụ đầu tiên là Cornell [Danescu-Niculescu-Mizil and Lee, 2011], bao gồm các cuộc hội thoại giữa các người nói trong các dòng phim khác nhau và có độ dài hội thoại trung vị là 3 phát ngôn, nhiệm vụ thứ hai là Reddit [Ghandeharioun et al., 2019], là một corpus hội thoại thông thường về các chủ đề khác nhau giữa người dùng có ít nhất 3 lượt với cuộc hội thoại trung vị chứa 7 phát ngôn.

Chúng tôi tiến hành một số thí nghiệm để kiểm tra hiệu quả của các phần khác nhau trong MoE-LM, cụ thể là (i) sức mạnh dự đoán và tính đa dạng của nguyên thủy, (ii) chất lượng của các chuyên gia, và (iii) hiệu suất DM tổng thể. Đối với mỗi metric, chúng tôi báo cáo mean ± standard deviation trên một tập evaluation gồm 100 cuộc hội thoại. Chúng tôi cũng chạy một nghiên cứu ablation trên 4 MoE-LM dựa trên transformer, cụ thể là MoE-1, MoE-2, MoE-3, MoE-4, để hiểu cách hiệu suất bị ảnh hưởng bởi các kiến trúc mô hình khác nhau, bộ mã hóa ngôn ngữ, và bộ tạo tiềm ẩn. MoE-1 và MoE-2 sử dụng kiến trúc đơn giản hơn, trong khi MoE-3 và MoE-4 sử dụng cùng kiến trúc encoder như BERT [Devlin et al., 2018]. MoE-1 sử dụng các mô hình phân phối tiềm ẩn {Gi} nhỏ hơn nhiều so với MoE-2; MoE-3 sử dụng bộ mã hóa BERT được huấn luyện trước, trong khi MoE-4 huấn luyện từ đầu. Chi tiết các mô hình này có thể tìm thấy trong Phụ lục ??.

THÍNH NGHIỆM 1: So sánh Các Mô hình Nguyên thủy Chúng tôi so sánh chất lượng của các biểu diễn tiềm ẩn được học bởi 4 MoE-LM (thông qua tối ưu hóa Eq. 1) và 2 baseline (Transformer tiêu chuẩn [Wolf et al., 2019] và VHRED [Serban et al., 2017b]). Để đánh giá chất lượng của chúng, đối với mỗi cuộc hội thoại test, chúng tôi tạo ra 25 phát ngôn và báo cáo 3 metric sau: (i) Diversity: 1-Sparsity [Hurley and Rickard, 2009] của các singular value của các phát ngôn được nhúng, tức là Diversity({Ŷi}) := 1 - √d||SVD||₁/||SVD||₂ /√d ∈ [0,1], trong đó SVD := SVD({SE(Ŷi)}²⁵ᵢ₌₁), và SE là một sentence encoder được huấn luyện trước (ví dụ: USE [Cer et al., 2018]); (ii) Dist-{1,2,3} [Li et al., 2015]: Tỷ lệ {1,2,3}-gram duy nhất trong các phát ngôn được tạo ra; (iii) Perplexity [Bahl et al., 1983]. Các metric này đo cả độ chính xác và tính đa dạng ngữ nghĩa.

Kết quả của các thí nghiệm trên được báo cáo trong Bảng 1 và 10 (Phụ lục B.1), và các phát ngôn mẫu được tạo ra bởi các LM này có thể tìm thấy trong Phụ lục B.2. So sánh với các baseline (Transformer và VHRED), nhìn chung (i) các LM dựa trên transformer vượt trội VHRED do cơ chế attention của chúng mã hóa rõ ràng thông tin ngữ nghĩa tuần tự, và (ii) các MoE-LM đạt được tính đa dạng tốt hơn nhiều mà không hy sinh nhiều về độ chính xác (tức là điểm perplexity vẫn khá thấp). Về mặt định tính, các phát ngôn mẫu được tạo ra bởi Transformer gần với target hơn so với MoE-2 và MoE-4, có thể vì Transformer có xu hướng ghi nhớ corpus [Kharitonov et al., 2021]. Ngược lại, các MoE-LM tạo ra các phát ngôn có ngữ cảnh tương tự với target nhưng được paraphrase hoặc cấu trúc tương tự nhưng ngữ cảnh khác nhau, chứng minh khả năng tổng quát hóa của chúng.

--- TRANG 5 ---

[THIS IS TABLE: Two tables showing performance metrics. Left table shows accuracy (Perplexity) and diversity metrics for different methods. Right table shows user satisfaction metrics and GPT-Perplexity for different DM approaches.]

[THIS IS FIGURE: Four subfigures showing latent space visualizations: (a) Transformer Primitive TSNE, (b) MoE-4 Primitive TSNE, (c) MoE-4 Sentiment PCA, (d) MoE-4 Emotion PCA]

Trong số các MoE-LM khác nhau, MoE-2 và MoE-4 có hiệu suất tốt nhất, đặc biệt MoE-4 có tính đa dạng tốt hơn trong khi MoE-2 có perplexity thấp hơn. Điều này phù hợp với giả thuyết của chúng tôi rằng (i) việc huấn luyện chung encoder và decoder với Eq. 1 dường như cần thiết để khuyến khích tính đa dạng ngữ nghĩa (trái ngược với việc sử dụng bộ mã hóa BERT được huấn luyện trước, tối đa hóa likelihood), (ii) sức mạnh biểu diễn đầy đủ là cần thiết cho G0 để khớp với phân phối posterior nhằm nắm bắt các ngữ nghĩa khác nhau trong D. Trong Hình 2a và 2b, chúng tôi trực quan hóa không gian tiềm ẩn của 200 mẫu dữ liệu hội thoại cho cả Transformer và MoE-4. Các trạng thái tiềm ẩn của MoE-4 phân tán hơn nhiều trên không gian nhúng, ngụ ý rằng hầu hết các cuộc hội thoại được mã hóa duy nhất. Ngược lại, không gian tiềm ẩn của Transformer có nhiều cluster, cho thấy nó dễ tạo ra các phát ngôn tương tự ngay cả với đầu vào hội thoại khác nhau và do đó ít khả năng tổng quát hóa hơn.

THÍ NGHIỆM 2: Chất lượng Chuyên gia Chúng tôi so sánh hiệu suất của các chuyên gia được học bởi 4 MoE-LM (trong đó các chuyên gia được huấn luyện riêng biệt bằng cách tối ưu hóa Eq. 2) và 2 baseline (WD [Holtzman et al., 2018] và PPLM [Dathathri et al., 2019]). Để nghiên cứu khoảng cách sub-optimality trong Hệ quả 1, chúng tôi cũng bao gồm hiệu suất của các LM chuyên gia end-to-end dựa trên Transformer được tối ưu hóa riêng lẻ với REINFORCE [Li et al., 2016], sử dụng các nhãn chuyên gia {ℓi} như phần thưởng. Chúng tôi sử dụng các hàm nhãn sau để định nghĩa ý định của các chuyên gia: (i) ℓpos-sent(Y), ℓneg-sent(Y), ℓjoy(Y), ℓoptimism(Y), ℓanger(Y), ℓsadness(Y) định lượng 6 tông màu sentiment khác nhau và được xây dựng bởi một detector sentiment dựa trên RoBERTa [Liao et al., 2021] dự đoán liệu một phát ngôn có sentiment tích cực hay tiêu cực, và liệu nó rơi vào bất kỳ cảm xúc tinh tế hơn nào trong 4 cảm xúc: {joy, optimism, sadness, anger}; (ii) ℓsent-coh(X,Y) đo empathy, tức là sự nhất quán sentiment của bot với người dùng; (iii) ℓquestion(Y) xuất 1 khi phát hiện câu hỏi bot và 0 ngược lại; (iv) ℓexp(X,Y) định lượng exploration, tức là xu hướng tránh các ngữ cảnh lặp lại. Xem Phụ lục A.7 để biết chi tiết.

Kết quả của các thí nghiệm trên được báo cáo trong Bảng 3 và 12 (Phụ lục B.1), với các phát ngôn mẫu được báo cáo trong Phụ lục B.3 đến B.9. So sánh với các LM baseline, nhìn chung các chuyên gia được tạo ra dưới khung MoE-LM, đặc biệt dưới MoE-2 và MoE-4, nắm bắt tốt hơn tất cả các ý định ngôn ngữ khác nhau (trong đó WD và PPL dường như nắm bắt sentiment và cảm xúc tiêu cực hiệu quả hơn nhiều so với các hành vi), chứng minh hiệu quả của phương pháp chúng tôi xây dựng các chuyên gia chuyên biệt trên không gian ngôn ngữ đa dạng thông qua tối đa hóa phần thưởng (thay vì weighted MLE). Tương tự như nghiên cứu ablation trong THÍ NGHIỆM 1, tất cả các chuyên gia liên quan đến MoE-2 và MoE-4 cũng nằm trong số những cái tốt nhất trong việc nắm bắt các ý định ngôn ngữ. Thú vị là, với dữ liệu Reddit các chuyên gia trong MoE-4 hoạt động tốt nhất, trong khi với ít dữ liệu hơn nhiều (Cornell) các chuyên gia tốt nhất được xây dựng dựa trên kiến trúc MoE-2 đơn giản hơn. Chúng tôi phỏng đoán sự khác biệt này do các vấn đề over-fitting mà các LM lớn hơn (MoE-4) gặp phải khi không có đủ dữ liệu cho việc fine-tune chuyên gia. Trong Hình 2c và 2d chúng tôi trực quan hóa không gian tiềm ẩn của các chuyên gia dựa trên sentiment trong MoE-4, mỗi cái với 200 mẫu dữ liệu hội thoại. Lưu ý rằng các phân phối tiềm ẩn của chuyên gia sentiment được tách biệt rõ ràng (vì sentiment tích cực và tiêu cực có hành vi đối lập), trong khi phân phối tiềm ẩn của chuyên gia emotion có sự tách biệt dần dần hơn và thậm chí một số overlap (vì ví dụ joy so với optimism khá tương tự, trong khi joy so với anger khá khác nhau). Điều này xác nhận MoE-LM của chúng tôi biểu diễn các hành vi khác nhau trong các vùng riêng biệt của không gian tiềm ẩn và biện minh cho prior cấu trúc của chúng tôi về việc mô hình hóa mỗi chuyên gia như một phiên bản chuyên biệt của LM nguyên thủy, có phân phối tiềm ẩn tập trung vào các vùng cụ thể.

[Continued table showing detailed quality metrics for different expert models]

THÍ NGHIỆM 3: MoE-RL chống lại Người dùng Mô phỏng DialoGPT Chúng tôi so sánh hiệu suất quản lý đối thoại của MoE-LM, có DM được huấn luyện với các phương pháp khác nhau (BC, DQN, CQL, MBRL), với 3 baseline (REINFORCE [Li et al., 2016], KL-control [Jaques et al., 2019], và VHRL [Saleh et al., 2020]). Theo kết quả về chất lượng chuyên gia trong THÍ NGHIỆM 2, chúng tôi chọn khung MoE-2 và MoE-4 lần lượt cho nhiệm vụ Cornell và Reddit. Để đánh giá có hệ thống, chúng tôi thực hiện thí nghiệm bằng cách để các tác nhân RL này tương tác với môi trường người dùng mô phỏng DialoGPT [Zhang et al., 2019] (một LM lớn có khả năng tiến hành các cuộc hội thoại tự nhiên miền mở), tối đa 5 lượt. Nhiệm vụ DM là tối đa hóa tổng sự hài lòng của người dùng ở cấp độ hội thoại, được đo bằng cả (i) sentiment tổng thể của người dùng, và (ii) chuyển tiếp sentiment của người dùng. Để xây dựng một phần thưởng tức thì phục vụ như một đại diện cho sự hài lòng của người dùng, chúng tôi đặt r(X,a,X') = λ1ℓsent(X') + λ2(ℓsent(X') - 1/L ∑^{L-1}_{l=0} γ^l ℓsent(Xl)), trong đó trọng số tổ hợp tuyến tính (λ1,λ2) = (0.75,0.25) tương quan với Ghandeharioun et al. [2019], và ℓsent(X) là cùng một bộ gán nhãn sentiment dựa trên RoBERTa như trong THÍ NGHIỆM 2, gán điểm từ [-1,1] tỷ lệ thuận với sentiment tích cực và tỷ lệ nghịch với xác suất dự đoán sentiment tiêu cực. Để đảm bảo các phương pháp DM RL baseline cũng có thể sở hữu một số tính năng cấp bot, ví dụ: câu hỏi, sentiment tích cực, v.v., ngoài phần thưởng RL trên cho sự hài lòng của người dùng, chúng tôi cũng tối ưu hóa một tổ hợp tuyến tính của các phần thưởng dựa trên bot khi huấn luyện các mô hình baseline, xem Phụ lục B của Saleh et al. [2020] để biết thêm chi tiết. Vì bài toán DM kéo dài tối đa 5 lượt, chúng tôi sử dụng điều này làm horizon hiệu quả và đặt γ = 1^{1/5} = 0.8. Để đánh giá tính trôi chảy của các LM, chúng tôi cũng báo cáo GPT-perplexity, tính điểm perplexity của phát ngôn cuối cùng w.r.t. một LM DialoGPT.

Kết quả của các thí nghiệm trên được báo cáo trong Bảng 2 và 11 (Phụ lục B.1), với các phát ngôn mẫu được báo cáo trong Phụ lục B.10. Các thí nghiệm của chúng tôi cho thấy MoE-LM vượt trội hầu hết các phương pháp baseline về hiệu suất DM. Chúng tôi quy kết quả này cho hai yếu tố: (i) MoE-MDP hạn chế không gian hành động thành một tập nhỏ hơn các phát ngôn ứng viên được tạo ra bởi các chuyên gia (có chất lượng được xác nhận trong THÍ NGHIỆM 2), bài toán RL tương ứng sau đó trở nên đơn giản hơn và yêu cầu ít dữ liệu hơn (đặc biệt trong Cornell) để giải quyết. (ii) Không giống như các phương pháp RL baseline, cần tối ưu hóa cả tín hiệu bot và người dùng, các tác nhân DM MoE tập trung vào việc tối ưu hóa mục tiêu sự hài lòng của người dùng và do đó hiệu quả hơn. Thú vị là, các MoE-LM của chúng tôi cũng có điểm GPT-perplexity thấp hơn (tốt hơn) so với các phương pháp khác. Điều này có thể do thực tế là MoE-LM sử dụng encoder và decoder được huấn luyện trước từ LM nguyên thủy, được tối ưu hóa cho tính tổng quát và độ chính xác, trong khi các phương pháp RL khác có thể làm biến dạng biểu diễn ngôn ngữ của chúng để tạo ra các phát ngôn tối đa hóa phần thưởng nhưng trở nên kém tự nhiên hơn. Trong số các chiến lược MoE-RL khác nhau, MBRL, trước tiên học một mô hình phát ngôn người dùng (mô hình sử dụng cùng encoder từ LM nguyên thủy và học một decoder riêng biệt cho dự đoán phát ngôn người dùng) và thực hiện Q-learning, hoạt động tốt nhất. CQL cải thiện vừa phải so với LM nguyên thủy (behavior policy), trong khi DQN dường như kém hiệu quả hơn trong việc giải quyết bài toán RL offline.

8 Nhận xét Kết luận
Chúng tôi đã phát triển một phương pháp hỗn hợp chuyên gia (MoE) cho quản lý đối thoại (DM) dựa trên RL. MoE language model (MoE-LM) của chúng tôi bao gồm ba thành phần chính: (i) một LM có thể tạo ra ngữ nghĩa đa dạng cho lịch sử hội thoại, (ii) một số LM chuyên biệt (hoặc chuyên gia) có thể tạo ra các phát ngôn tương ứng với một thuộc tính hoặc ý định cụ thể, và (iii) một DM dựa trên RL thực hiện lập kế hoạch đối thoại với các phát ngôn được tạo ra bởi các chuyên gia. Để hiểu mức độ phương pháp MoE của chúng tôi tạo ra các phát ngôn đa dạng và hợp lý, và giải quyết các bài toán DM, chúng tôi đã đánh giá nó bằng hai nhiệm vụ đối thoại miền mở và so sánh với các baseline SOTA. Kết quả của chúng tôi cho thấy MoE-LM (i) cải thiện tính đa dạng của việc tạo văn bản, (ii) có thể tạo ra các phát ngôn với ý định cụ thể, và (iii) mang lại hiệu suất tổng thể tốt hơn. Chúng tôi coi công việc của mình như một bước tiến trong việc tạo ra các LM có thể điều khiển sở hữu các ý định khác nhau và trong việc huấn luyện các DM dựa trên RL có thể tiến hành các cuộc hội thoại phong phú. Công việc tương lai bao gồm cải thiện biểu diễn ngôn ngữ với các phương pháp lý thuyết thông tin, fine-tune các chuyên gia dựa trên mục tiêu DM, mở rộng tác nhân RL để theo dõi hành vi của người dùng (thông qua các trạng thái niềm tin trừu tượng) và lập kế hoạch dựa trên chúng, và đánh giá MoE-LM của chúng tôi trên các bài toán thực tế hơn, chẳng hạn như truy xuất thông tin, khuyến nghị, và đàm phán.

--- TRANG 6 ---
[Phần còn lại của tài liệu tiếp tục với Checklist, References và các phụ lục chi tiết - tôi sẽ tiếp tục dịch nếu cần]
