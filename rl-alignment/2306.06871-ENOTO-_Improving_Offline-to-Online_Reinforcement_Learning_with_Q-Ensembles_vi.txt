# 2306.06871.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2306.06871.pdf
# Kích thước tệp: 4891318 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
ENOTO: Cải thiện Học tăng cường Offline-to-Online với Q-Ensemble
Kai Zhao1,2, Jianye Hao1,∗, Yi Ma1, Jinyi Liu1, Yan Zheng1 và Zhaopeng Meng1
1Trường Đại học Trí tuệ và Điện toán, Đại học Thiên Tân
2Bilibili
{kaizhao, jianye.hao, mayi, jyliu, yanzheng, mengzp }@tju.edu.cn

Tóm tắt
Học tăng cường offline (RL) là một mô hình học tập trong đó một tác nhân học từ một tập dữ liệu kinh nghiệm cố định. Tuy nhiên, việc học chỉ từ một tập dữ liệu tĩnh có thể hạn chế hiệu suất do thiếu khám phá. Để khắc phục điều này, RL offline-to-online kết hợp tiền huấn luyện offline với tinh chỉnh online, cho phép tác nhân tiếp tục cải thiện chính sách của mình bằng cách tương tác với môi trường theo thời gian thực. Mặc dù có những lợi ích, các phương pháp RL offline-to-online hiện tại vẫn gặp phải sự suy giảm hiệu suất và cải thiện chậm trong giai đoạn online. Để giải quyết những thách thức này, chúng tôi đề xuất một khung làm việc mới được gọi là RL ENsemble-based Offline-To-Online (ENOTO). Bằng cách tăng số lượng mạng Q, chúng tôi kết nối một cách mượt mà giữa tiền huấn luyện offline và tinh chỉnh online mà không làm giảm hiệu suất. Hơn nữa, để đẩy nhanh việc nâng cao hiệu suất online, chúng tôi nới lỏng một cách thích hợp tính bi quan của việc ước lượng giá trị Q và tích hợp các cơ chế khám phá dựa trên ensemble vào khung làm việc của chúng tôi. Kết quả thực nghiệm chứng minh rằng ENOTO có thể cải thiện đáng kể tính ổn định huấn luyện, hiệu quả học tập và hiệu suất cuối cùng của các phương pháp RL offline hiện tại trong quá trình tinh chỉnh online trên một loạt các nhiệm vụ di chuyển và điều hướng, vượt trội hơn đáng kể so với các phương pháp RL offline-to-online hiện tại.

1 Giới thiệu
Học tăng cường (RL) đã cho thấy thành công đáng kể trong việc giải quyết các bài toán ra quyết định phức tạp, từ chơi game ảo [Silver et al., 2017; Vinyals et al., 2019] đến điều khiển robot thực tế [Mnih et al., 2015; Tsividis et al., 2021; Schrittwieser et al., 2020]. Trong RL, một tác nhân học cách tối đa hóa lợi nhuận tích lũy từ lượng lớn dữ liệu kinh nghiệm thu được bằng cách tương tác với một môi trường. Tuy nhiên, trong nhiều ứng dụng thực tế, việc thu thập dữ liệu kinh nghiệm có thể tốn kém, mất thời gian hoặc thậm chí nguy hiểm. Thách thức này đã thúc đẩy sự phát triển của RL offline, trong đó một tác nhân học từ một tập dữ liệu kinh nghiệm cố định được thu thập trước khi học [Fujimoto et al., 2019; Wu et al., 2019; Bai et al., 2022; Liu et al., 2023; Yu et al., 2020; Kidambi et al., 2020].

RL offline có một số ưu điểm so với RL online, bao gồm khả năng tái sử dụng dữ liệu hiện có, tiềm năng học nhanh hơn và khả năng học từ những kinh nghiệm quá rủi ro hoặc tốn kém để thu thập online [Silver et al., 2018]. Tuy nhiên, RL offline cũng đặt ra những thách thức đáng kể, chẳng hạn như tiềm năng overfitting với dữ liệu huấn luyện và khó khăn trong việc đảm bảo rằng chính sách đã học là an toàn và tối ưu trong môi trường thực tế. Để giải quyết những thách thức này, RL offline-to-online đã nổi lên như một hướng nghiên cứu hấp dẫn. Cách tiếp cận này kết hợp tiền huấn luyện offline với tinh chỉnh online sử dụng RL, với mục tiêu học từ một tập dữ liệu kinh nghiệm offline cố định và sau đó tiếp tục học online trong môi trường thực tế [Nair et al., 2020; Lee et al., 2022]. RL offline-to-online có tiềm năng giải quyết những hạn chế của RL offline, chẳng hạn như tính không tối ưu của chính sách đã học. Hơn nữa, bắt đầu với một chính sách RL offline có thể đạt được hiệu suất mạnh với ít mẫu môi trường online hơn, so với việc thu thập lượng lớn dữ liệu huấn luyện bằng cách triển khai các chính sách từ đầu.

Các nghiên cứu trước đây đã chỉ ra rằng việc khởi tạo trực tiếp một tác nhân với một phương pháp RL offline để tinh chỉnh online có thể cản trở việc cải thiện chính sách hiệu quả do học tập bi quan [Nair et al., 2020; Zhao et al., 2022]. Một giải pháp ngây thơ cho vấn đề này là loại bỏ trực tiếp thành phần bi quan trong quá trình huấn luyện online. Tuy nhiên, cách tiếp cận này có thể dẫn đến học tập không ổn định hoặc hiệu suất giảm sút khi sự dịch chuyển phân phối giữa tập dữ liệu offline và tương tác online tạo ra lỗi temporal difference ban đầu lớn, gây ra sự quên lãng thông tin đã học từ RL offline [Lee et al., 2022; Mark et al., 2022]. Các phương pháp RL offline-to-online hiện tại đã cố gắng giải quyết những thách thức này thông qua các ràng buộc chính sách ngầm [Nair et al., 2020], lọc dữ liệu offline được sử dụng để tinh chỉnh online [Lee et al., 2022; Mao et al., 2022; Mark et al., 2022], điều chỉnh trọng số ràng buộc chính sách một cách cẩn thận [Zhao et al., 2022], hoặc huấn luyện nhiều chính sách online hơn [Zhang et al., 2023]. Tuy nhiên, những phương pháp này vẫn đối mặt với sự suy giảm hiệu suất trong một số nhiệm vụ và cài đặt, và việc cải thiện hiệu suất của chúng trong giai đoạn online là có hạn.

Lấy cảm hứng từ việc tận dụng Q-ensemble trong RL offline [An et al., 2021], chúng tôi đề xuất một cách tiếp cận mới để giải quyết những thách thức của RL offline-to-online. Cụ thể, chúng tôi thực hiện các thí nghiệm toàn diện bằng cách loại bỏ thành phần bi quan trong các thuật toán RL offline hiện tại và tăng số lượng mạng Q trong cả giai đoạn offline và online. Chúng tôi phát hiện rằng Q-ensemble giúp giảm thiểu huấn luyện không ổn định và suy giảm hiệu suất, và có thể đóng vai trò như một thành phần bi quan linh hoạt hơn bằng cách bao gồm các phương pháp tính toán target và khám phá khác nhau trong giai đoạn tinh chỉnh online. Dựa trên khám phá này, chúng tôi đề xuất một khung làm việc RL ENsemble-based Offline-To-Online (ENOTO) kết nối tiền huấn luyện offline và tinh chỉnh online. Chúng tôi chứng minh tính hiệu quả của khung làm việc ENOTO bằng cách thực hiện nó trên các thuật toán RL offline hiện tại [Kumar et al., 2020; Chen et al., 2022] trên các nhiệm vụ benchmark đa dạng.

Những đóng góp chính của công trình này được tóm tắt như sau:
• Chúng tôi chứng minh tính hiệu quả của Q-ensemble trong việc thu hẹp khoảng cách giữa tiền huấn luyện offline và tinh chỉnh online, cung cấp một giải pháp để giảm thiểu vấn đề thường gặp về huấn luyện không ổn định và sụt giảm hiệu suất.
• Chúng tôi đề xuất một khung làm việc thống nhất ENOTO cho RL offline-to-online, cho phép một loạt rộng các thuật toán RL offline chuyển từ tiền huấn luyện offline bi quan sang tinh chỉnh online lạc quan, dẫn đến cải thiện hiệu suất ổn định và hiệu quả.
• Chúng tôi xác thực thực nghiệm tính hiệu quả của ENOTO trên các nhiệm vụ benchmark khác nhau, bao gồm các nhiệm vụ di chuyển và điều hướng, và xác minh rằng ENOTO đạt được hiệu suất tối ưu so với tất cả các phương pháp baseline.

2 Tại sao Q-Ensemble có thể giúp RL Offline-to-Online?
Để hiểu rõ hơn về khung làm việc dựa trên ensemble của chúng tôi, chúng tôi bắt đầu với các ví dụ làm nổi bật những ưu điểm của Q-ensemble cho RL offline-to-online. Một điểm khởi đầu tự nhiên cho RL offline-to-online là đơn giản khởi tạo tác nhân với một phương pháp RL offline hiện tại đã được huấn luyện và sau đó thực hiện trực tiếp tinh chỉnh online mà không sử dụng tập dữ liệu offline. Tuy nhiên, cách tiếp cận này có thể cản trở việc cải thiện hiệu suất online hiệu quả do tính bi quan vốn có của mô hình học tập offline [Lee et al., 2022; Mark et al., 2022]. Để hỗ trợ tuyên bố này, chúng tôi trình bày CQL [Kumar et al., 2020] như một đại diện và thực hiện các thí nghiệm sơ bộ trên tập dữ liệu D4RL Walker2d-medium-expert-v2. Đường cong học tập của CQL trong quá trình tinh chỉnh online trong Hình 1(a) cho thấy rằng CQL có thể duy trì hiệu suất offline ở giai đoạn ban đầu của tinh chỉnh online và cải thiện đều đặn trong quá trình huấn luyện. Điều này có thể được quy cho việc sử dụng các hàm Q bi quan, cho phép tác nhân truy cập các trạng thái tương tự như những trạng thái trong tập dữ liệu offline và duy trì tính bi quan đối với các hành động chưa từng thấy trong giai đoạn ban đầu của tinh chỉnh online. Tuy nhiên, mục tiêu bi quan cản trở việc khám phá đúng đắn trong giai đoạn online và hạn chế tác nhân khỏi việc cải thiện hiệu suất một cách hiệu quả [Lee et al., 2022; Mark et al., 2022; Hao et al., 2023; Ghasemipour et al., 2022].

Để giải quyết vấn đề khám phá hạn chế nêu trên, người ta có thể nghĩ đến việc loại bỏ thành phần ước lượng bảo thủ để giảm tính bảo thủ của quá trình học tập. Tuy nhiên, như được thể hiện trong Hình 1(a), giải pháp ngây thơ này dẫn đến huấn luyện không ổn định hoặc suy giảm hiệu suất khi chuyển từ CQL sang Soft Actor-Critic (SAC) [Haarnoja et al., 2018] trong quá trình tinh chỉnh online, điều này cũng đã được báo cáo trong các công trình RL offline-to-online trước đây [Lu et al., 2021; Nair et al., 2020; Lee et al., 2022; Mark et al., 2022]. Lý do là SAC thiếu việc ước lượng chính xác các giá trị Q cho các cặp trạng thái-hành động chưa biết. Không có các ràng buộc bảo thủ của CQL, các giá trị Q có xu hướng bị ước lượng quá cao, dẫn đến việc hướng dẫn chính sách sai lầm.

Vậy có thể tìm được một phương pháp giữ lại các ràng buộc bi quan phù hợp để giảm thiểu suy giảm hiệu suất, đồng thời cũng điều chỉnh những ràng buộc này để thuận lợi hơn cho việc khám phá trong giai đoạn online, thay vì bi quan như các thuật toán RL offline truyền thống như CQL không? Lấy cảm hứng từ việc tăng số lượng mạng Q trong [An et al., 2021], chúng tôi giới thiệu Q-ensemble và đặt số lượng hàm Q trong CQL và SAC thành N. Cụ thể, giá trị Q target được ước lượng bằng cách chọn giá trị nhỏ nhất từ tất cả Q-ensemble. Chúng tôi gọi những phương pháp trung gian này là CQL-N và SAC-N. Hình 1(a) cho thấy tính hiệu quả của việc sử dụng SAC-N để tinh chỉnh online một chính sách offline được tiền huấn luyện với CQL-N. Đáng ngạc nhiên, sau khi tích hợp Q-ensemble, chúng tôi quan sát thấy rằng việc huấn luyện trở nên ổn định hơn và không còn quan sát thấy sụt giảm hiệu suất khi chuyển sang tinh chỉnh online. Hơn nữa, phương pháp ràng buộc này không chỉ nâng cao hiệu suất cuối cùng của giai đoạn offline mà còn cải thiện hiệu quả của việc học online.

Để hiểu lý do tại sao Q-ensemble giúp giảm thiểu huấn luyện không ổn định và sụt giảm hiệu suất, chúng tôi kiểm tra các giá trị Q trung bình trên tập dữ liệu của các thuật toán khác nhau trong Hình 1(b). Chúng tôi quan sát thấy rằng nếu chúng ta loại bỏ trực tiếp các ràng buộc bi quan trong giai đoạn tinh chỉnh online (tức là CQL → SAC), việc ước lượng giá trị Q sẽ dao động mạnh, dẫn đến huấn luyện không ổn định và sụt giảm hiệu suất, như được mô tả trong Hình 1(a). Tuy nhiên, với việc tích hợp Q-ensemble của chúng tôi, SAC-N vẫn có khả năng ước lượng bảo thủ, và phạm vi biến thiên của giá trị Q trong CQL-N → SAC-N nhỏ hơn nhiều so với CQL → SAC. Hiện tượng này cho thấy rằng việc giữ lại một cách thích hợp các khả năng bảo thủ là quan trọng trong việc tránh huấn luyện không ổn định và sụt giảm hiệu suất.

Chúng ta đã thấy rằng cả SAC-N và CQL đều có thể ngăn ngừa sụt giảm hiệu suất trong quá trình tinh chỉnh online, nhưng tại sao SAC-N lại cho hiệu suất tốt hơn so với CQL? Để trả lời câu hỏi này, chúng tôi phân tích khoảng cách giữa các hành động được chọn bởi mỗi phương pháp và các hành động trong tập dữ liệu, như được thể hiện trong Hình 1(c). Cụ thể, chúng tôi đo lường cho SAC-N, CQL và một chính sách ngẫu nhiên bằng cách thực hiện tinh chỉnh online trên tập dữ liệu Walker2d-medium-expert-v2. Phát hiện của chúng tôi cho thấy SAC-N có phạm vi lựa chọn hành động rộng hơn so với CQL, và một tập hợp hành động đa dạng hơn có thể dẫn đến hiệu suất được cải thiện, như đã được nêu trong các phương pháp khám phá trước đây [Ecoffet et al., 2021; Lee et al., 2021; Liu et al., 2024; Savinov et al., 2018; Houthooft et al., 2016]. Do đó, chúng ta có thể tích hợp Q-ensemble vào các thuật toán RL offline hiện tại như CQL, và loại bỏ thành phần bảo thủ ban đầu được thiết kế cho các thuật toán offline trong giai đoạn online để cải thiện hiệu quả học tập online.

Tóm lại, phân tích thực nghiệm chính của chúng tôi chỉ ra quan sát sau:
Q-ensemble có thể duy trì khả năng bảo thủ nhất định để giảm thiểu huấn luyện không ổn định và sụt giảm hiệu suất, hoạt động như một phương pháp ràng buộc linh hoạt hơn để khám phá các hành động đa dạng hơn trong quá trình tinh chỉnh online so với các thuật toán RL offline như CQL.

Với Q-ensemble trong tay, chúng ta có thể cải thiện thêm hiệu quả học tập online bằng cách tận dụng linh hoạt các cách tiếp cận khác nhau dựa trên cơ chế này, điều này sẽ được trình bày trong khung làm việc đề xuất của chúng tôi trong phần tiếp theo.

3 Học tăng cường Offline-to-Online dựa trên Ensemble
Dựa trên các quan sát thực nghiệm được thảo luận trước đó, chúng tôi đề xuất khung làm việc RL ENsemble-based Offline-To-Online (ENOTO). Trong phần này, chúng tôi trước tiên trình bày những ưu điểm của Q-ensemble bằng cách sử dụng các kết quả thực nghiệm bổ sung và sau đó dần dần giới thiệu thêm các cơ chế dựa trên ensemble vào khung làm việc của chúng tôi. Mặc dù mỗi quyết định thiết kế riêng lẻ trong ENOTO có vẻ tương đối đơn giản, sự kết hợp cụ thể của chúng vượt trội hơn so với các baseline về tính ổn định huấn luyện, hiệu quả học tập và hiệu suất cuối cùng.

3.1 Q-Ensemble
Như đã thảo luận trong phần trước, Q-ensemble có thể kết nối các giai đoạn offline và online để giúp các tác nhân offline được tiền huấn luyện thực hiện tinh chỉnh online ổn định. Trong phần này, chúng tôi trình bày các kết quả thực nghiệm toàn diện để xác minh thêm những ưu điểm của nó.

Cho một thuật toán RL offline có tên OfflineRL, chúng tôi giới thiệu Q-ensemble để có được OfflineRL-N, chỉ ra rằng thuật toán sử dụng N mạng Q và lấy giá trị nhỏ nhất của tất cả các mạng Q trong ensemble để tính toán target. Với tác nhân OfflineRL-N được tiền huấn luyện, chúng tôi tải nó làm khởi tạo cho tác nhân online và loại bỏ thành phần bi quan được thiết kế ban đầu (nếu có thể) để có được OnlineRL-N. Sau đó OnlineRL-N được huấn luyện online. Trong tất cả các phần phương pháp, chúng tôi thực hiện OfflineRL như CQL, và do đó OfflineRL-N đề cập đến CQL-N, và OnlineRL-N đề cập đến SAC-N. Để xác minh toàn diện tính hiệu quả của Q-ensemble trong việc ổn định quá trình huấn luyện và giảm thiểu sụt giảm hiệu suất, chúng tôi xem xét ba nhiệm vụ di chuyển MuJoCo [Todorov et al., 2012]: HalfCheetah, Hopper và Walker2d từ bộ benchmark D4RL [Fu et al., 2020]. Cụ thể, chúng tôi xem xét các tập dữ liệu medium, medium-replay và medium-expert, như trong các kịch bản thực tế điển hình, chúng ta hiếm khi sử dụng chính sách ngẫu nhiên hoặc có chính sách chuyên gia cho việc điều khiển hệ thống.

Hình 2 cho thấy lợi nhuận chuẩn hóa tổng hợp trên tất cả chín tập dữ liệu. Phù hợp với kết quả của thí nghiệm minh họa trước đó, huấn luyện online của OfflineRL ổn định nhưng dẫn đến hiệu suất tiệm cận chậm hơn. Việc chuyển trực tiếp sang OnlineRL gây ra quá trình huấn luyện không ổn định và sụt giảm hiệu suất. Ngược lại, OfflineRL-N → OnlineRL-N không còn trải qua sự sụp đổ hiệu suất sau khi chuyển sang tinh chỉnh online, và quá trình huấn luyện tương đối ổn định. Ngoài ra, OfflineRL-N → OnlineRL-N đạt được hiệu suất tinh chỉnh tốt hơn so với OfflineRL → OfflineRL.

Mặc dù phương pháp dựa trên ensemble OfflineRL-N → OnlineRL-N đã có những cải thiện nhất định so với phương pháp hiện tại OfflineRL → OfflineRL, nó vẫn không thể được cải thiện nhanh chóng trong giai đoạn online so với các thuật toán RL online tiêu chuẩn. Do đó, chúng tôi chuyển hướng tập trung vào việc phân tích liệu chúng ta có thể nới lỏng một cách thích hợp việc ước lượng bi quan của các giá trị Q trong giai đoạn online để cải thiện thêm hiệu quả học tập trong khi đảm bảo huấn luyện ổn định hay không.

3.2 Nới lỏng tính Bi quan
Trong phần trước, chúng tôi sử dụng OnlineRL-N làm phương pháp chính cho giai đoạn online. Phương pháp này chọn giá trị nhỏ nhất của N mạng Q song song làm target Bellman để buộc việc ước lượng giá trị Q của chúng phải bảo thủ. Trong khi OfflineRL-N → OnlineRL-N đã đạt được hiệu suất thỏa đáng, việc chọn giá trị nhỏ nhất của N mạng Q trong ensemble để tính toán Q-target vẫn quá bảo thủ cho huấn luyện online, so với các thuật toán RL online tiêu chuẩn không có ràng buộc bi quan. Do đó, trong khi đảm bảo rằng quá trình huấn luyện online ổn định, chúng tôi xem xét việc nới lỏng một cách thích hợp việc ước lượng bi quan của các giá trị Q bằng cách sửa đổi phương pháp tính toán Q-target trong OnlineRL-N để cải thiện hiệu quả hiệu suất online.

Cụ thể, chúng tôi so sánh một số phương pháp tính toán Q-target. (a) MinQ là những gì chúng tôi sử dụng trong OnlineRL-N, trong đó giá trị nhỏ nhất của tất cả các mạng Q trong ensemble được lấy để tính toán target. (b) MeanQ tận dụng trung bình của tất cả các giá trị Q để tính toán target. (c) REM là một phương pháp ban đầu được đề xuất để tăng hiệu suất của DQN trong cài đặt hành động rời rạc, sử dụng kết hợp convex ngẫu nhiên của các giá trị Q để tính toán target [Agarwal et al., 2020]. Nó tương tự như trung bình ensemble (MeanQ), nhưng với nhiều ngẫu nhiên hóa hơn. (d) RandomMinPair sử dụng phép tối thiểu hóa trên một tập con ngẫu nhiên gồm 2 trong số N hàm Q, được đề xuất trong các phương pháp trước đây [Chen et al., 2021]. (e) WeightedMinPair tính toán target như kỳ vọng của tất cả các target RandomMinPair, trong đó kỳ vọng được lấy trên tất cả các cặp N-chọn-2 của các hàm Q. RandomMinPair có thể được coi như một phiên bản lấy mẫu đồng nhất của WeightedMinPair.

Hình 3 trình bày kết quả của việc sử dụng các phương pháp tính toán Q-target khác nhau trong giai đoạn online dựa trên OnlineRL-N. Với MinQ, ban đầu được sử dụng trong OnlineRL-N, làm giới hạn, cả MeanQ và REM đều cho hiệu suất kém, trong khi RandomMinPair và WeightedMinPair vượt trội hơn các ứng viên khác với quá trình học online hiệu quả và ổn định. Vì phương pháp WeightedMinPair ổn định hơn trên nhiều tập dữ liệu so với phương pháp RandomMinPair, chúng tôi áp dụng WeightedMinPair. Tiếp tục từ đây, chúng tôi gọi thuật toán trung gian này là OnlineRL-N + WeightedMinPair. Mặc dù có hiệu suất tinh chỉnh online vượt trội của cách tiếp cận này, chúng tôi tiếp tục khám phá các cách để cải thiện thêm hiệu quả học tập online bằng cách tận dụng các đặc tính ensemble.

3.3 Khám phá Lạc quan
Trong các phần trước, chúng tôi sử dụng học tập bi quan để có được một điểm khởi đầu thỏa đáng cho việc học online và dần dần nới lỏng ràng buộc bi quan để cải thiện việc học online. Trong phần này, chúng tôi điều tra việc sử dụng các phương pháp khám phá dựa trên ensemble để cải thiện thêm hiệu suất và hiệu quả học tập.

Cụ thể, chúng tôi so sánh ba phương pháp khám phá dựa trên ensemble. (a) Bootstrapped DQN [Osband et al., 2016] sử dụng ensemble để giải quyết một số thiếu sót của các sơ đồ xấp xỉ posterior thay thế, mạng của nó bao gồm một kiến trúc chia sẻ với N "head" bootstrapped phân nhánh độc lập. (b) OAC [Ciosek et al., 2019] đề xuất một chiến lược khám phá off-policy điều chỉnh để tối đa hóa một giới hạn tin cậy trên cho critic, thu được từ một ước lượng uncertainty epistemic trên hàm Q được tính toán với bootstrap thông qua Q-ensemble. (c) SUNRISE [Lee et al., 2021] trình bày các backup Bellman có trọng số dựa trên ensemble cải thiện quá trình học tập bằng cách tái cân bằng các giá trị Q target dựa trên các ước lượng uncertainty.

Kết quả của các phương pháp khám phá khác nhau được trình bày trong Hình 4. Trong số đó, OnlineRL-N + WeightedMinPair + SUNRISE đạt được lợi nhuận tổng hợp cao nhất. Do đó, chúng tôi biến OnlineRL-N + WeightedMinPair + SUNRISE thành khung làm việc dựa trên ensemble cuối cùng ENOTO của chúng tôi. Thuật toán 1 tóm tắt các quy trình offline và online của ENOTO. Lưu ý rằng vì nhiều thuật toán RL offline có thể tích hợp kỹ thuật ensemble trong các hàm Q, ENOTO do đó có thể đóng vai trò như một plugin chung. Chúng tôi sẽ thể hiện thêm tính chất plug-and-play của ENOTO bằng cách áp dụng OfflineRL-N → OnlineRL-N + WeightedMinPair + SUNRISE trên các thuật toán RL offline khác nhau trong các thí nghiệm. Để có cái nhìn toàn diện về các kết quả chi tiết của phần này, bao gồm kết hợp của RandomMinPair và các phương pháp khám phá khác nhau, vui lòng tham khảo phụ lục.

4 Thí nghiệm
Trong phần này, chúng tôi trình bày các đánh giá thực nghiệm của khung làm việc ENOTO. Chúng tôi bắt đầu với các nhiệm vụ di chuyển từ D4RL [Fu et al., 2020] để đo lường tính ổn định huấn luyện, hiệu quả học tập và hiệu suất cuối cùng của ENOTO bằng cách so sánh với một số phương pháp RL offline-to-online tối ưu. Ngoài ra, chúng tôi đánh giá ENOTO trên các nhiệm vụ điều hướng đầy thách thức hơn để xác minh tính linh hoạt của nó.

4.1 Nhiệm vụ Di chuyển
Chúng tôi trước tiên đánh giá khung làm việc ENOTO trên các nhiệm vụ di chuyển MuJoCo [Todorov et al., 2012], tức là HalfCheetah, Walker2d và Hopper từ bộ benchmark D4RL [Fu et al., 2020]. Để chứng minh khả năng áp dụng của ENOTO trên các tập dữ liệu không tối ưu khác nhau, chúng tôi sử dụng ba loại tập dữ liệu: medium, medium-replay và medium-expert. Cụ thể, các tập dữ liệu medium chứa các mẫu được thu thập bởi một chính sách cấp độ trung bình, các tập dữ liệu medium-replay bao gồm tất cả các mẫu gặp phải trong khi huấn luyện một tác nhân cấp độ trung bình từ đầu, và các tập dữ liệu medium-expert bao gồm các mẫu được thu thập bởi cả chính sách cấp độ trung bình và chuyên gia. Chúng tôi tiền huấn luyện tác nhân trong 1M bước huấn luyện trong giai đoạn offline và thực hiện tinh chỉnh online trong 250K bước môi trường. Chi tiết thí nghiệm bổ sung có thể được tìm thấy trong phụ lục.

Đánh giá So sánh. Chúng tôi xem xét các phương pháp sau làm baseline.
• AWAC [Nair et al., 2020]: một phương pháp RL offline-to-online buộc chính sách bắt chước các hành động với ước lượng advantage cao trong tập dữ liệu.
• BR [Lee et al., 2022]: một phương pháp RL offline-to-online huấn luyện một mạng bổ sung để ưu tiên các mẫu nhằm sử dụng hiệu quả dữ liệu mới cũng như các mẫu near-on-policy trong tập dữ liệu offline.
• PEX [Zhang et al., 2023]: một phương pháp RL offline-to-online gần đây sử dụng một chính sách offline trong một tập chính sách, mở rộng nó với các chính sách bổ sung và xây dựng một phân phối categorical dựa trên các giá trị của chúng tại trạng thái hiện tại để chọn hành động cuối cùng.
• Cal-QL [Nakamoto et al., 2023]: một phương pháp RL offline-to-online gần đây học một khởi tạo hàm giá trị bảo thủ underestimate giá trị của chính sách đã học từ dữ liệu offline, đồng thời cũng được hiệu chuẩn, theo nghĩa là các giá trị Q đã học ở một quy mô hợp lý.
• IQL [Kostrikov et al., 2021]: một thuật toán RL đại diện cho thấy hiệu suất offline vượt trội và cho phép tinh chỉnh online liền mạch thông qua chuyển tham số trực tiếp.
• SAC [Haarnoja et al., 2018]: một tác nhân SAC được huấn luyện từ đầu. Baseline này làm nổi bật lợi ích của RL offline-to-online, trái ngược với RL hoàn toàn online, về mặt hiệu quả học tập.
• Scratch: huấn luyện SAC-N + WeightedMinPair + SUNRISE online từ đầu mà không có tiền huấn luyện offline, trái ngược với khung làm việc ENOTO của chúng tôi.

Hình 5 cho thấy hiệu suất của phương pháp ENOTO-CQL (ENOTO được thực hiện trên CQL) và các phương pháp baseline trong giai đoạn tinh chỉnh online. So với các phương pháp RL online thuần túy như SAC và Scratch, ENOTO-CQL bắt đầu với một chính sách hiệu suất tốt và học nhanh chóng và ổn định, chứng minh lợi ích của tiền huấn luyện offline. Đối với các phương pháp RL offline, IQL cho thấy cải thiện hạn chế vì huấn luyện bi quan hoàn toàn không còn phù hợp cho tinh chỉnh online, trong khi ENOTO-CQL hiển thị tinh chỉnh nhanh. Trong số các phương pháp RL offline-to-online khác, hiệu suất của AWAC bị hạn chế bởi chất lượng của tập dữ liệu do hoạt động huấn luyện chính sách của nó để bắt chước các hành động với ước lượng advantage cao, dẫn đến cải thiện chậm trong giai đoạn online. Trong khi BR có thể đạt được hiệu suất chỉ sau ENOTO-CQL trên một số tập dữ liệu, nó cũng gặp phải huấn luyện không ổn định. PEX thể hiện sự suy giảm hiệu suất đáng chú ý trong giai đoạn đầu của tinh chỉnh online trên các tập dữ liệu khác nhau, được quy cho tính ngẫu nhiên của các chính sách mới được huấn luyện trong giai đoạn đầu, điều này ảnh hưởng tiêu cực đến tính ổn định huấn luyện. Mặc dù bài báo PEX gốc không giải quyết rõ ràng hiện tượng này, một kiểm tra tỉ mỉ phần thí nghiệm của nó cho thấy rằng sụt giảm hiệu suất thực sự ảnh hưởng đến PEX. Chúng tôi cho rằng hiện tượng sụt giảm hiệu suất là một mối quan tâm then chốt trong lĩnh vực RL offline-to-online, đáng được chú ý đáng kể. Chuyển sang thuật toán Cal-QL, trong khi hiệu quả của nó được thể hiện nổi bật trong các nhiệm vụ phức tạp như Antmaze, Adroit và Kitchen, như được nhấn mạnh trong bài báo, chúng tôi lưu ý hiệu suất khiêm tốn hơn trong các nhiệm vụ MuJoCo truyền thống. Các cải thiện trong giai đoạn online có vẻ tương đối bị hạn chế. Tuy nhiên, thuộc tính nổi bật nhất của nó nằm ở tính ổn định đặc biệt, hiệu quả tránh được vấn đề sụt giảm hiệu suất.

Điều quan trọng cần nhấn mạnh là do sự không hoàn chỉnh một phần của mã được cung cấp bởi một số thuật toán baseline nhất định, các thí nghiệm của chúng tôi một phần dựa vào các kho mã nguồn có sẵn công khai và được chấp nhận rộng rãi từ GitHub [Seno and Imai, 2022; Tarasov et al., 2022]. Do đó, kết quả thí nghiệm có thể thể hiện những lệch lạc nhỏ so với kết quả được báo cáo trong các bài báo gốc, điều này sẽ được chi tiết toàn diện trong phụ lục. Tuy nhiên, thông qua các so sánh nghiêm ngặt bao gồm cả các chỉ số hiệu suất gốc của các bài báo baseline và kết quả thu được từ việc thực hiện mã của chúng tôi, phương pháp ENOTO của chúng tôi nhất quán vượt trội hơn các cách tiếp cận baseline về tính ổn định huấn luyện, hiệu quả học tập và hiệu suất cuối cùng trên hầu hết các nhiệm vụ. Thật không may, do những ràng buộc trong văn bản này, chúng tôi chỉ có thể trình bày kết quả đạt được từ việc thực thi mã, vì các biểu diễn đồ họa từ các bài báo nguồn không thể được tích hợp một cách liền mạch.

4.2 Nhiệm vụ Điều hướng
Chúng tôi xác minh thêm tính hiệu quả của ENOTO trên nhiệm vụ điều hướng D4RL Antmaze [Fu et al., 2020] bằng cách tích hợp một thuật toán RL offline khác LAPO [Chen et al., 2022]. Chi tiết, chúng tôi chuyên biệt hóa ENOTO thành LAPO-N + WeightedMinPair + SUNRISE, tức là ENOTO-LAPO. Đối với nhiệm vụ Antmaze, chúng tôi xem xét ba loại mê cung: umaze, medium và large maze, và hai thành phần dữ liệu: play và diverse. Các thành phần dữ liệu khác nhau về độ bao phủ hành động của các vùng khác nhau trong không gian trạng thái và tính không tối ưu của chính sách hành vi.

Đánh giá So sánh. Vì Antmaze là một nhiệm vụ đầy thách thức hơn, hầu hết các phương pháp RL offline gặp khó khăn để đạt được kết quả thỏa đáng trong giai đoạn offline, chúng tôi chỉ so sánh phương pháp ENOTO-LAPO trên nhiệm vụ này với ba phương pháp baseline hiệu quả, IQL, PEX và Cal-QL. Cụ thể, đối với các nhiệm vụ D4RL Antmaze, những phương pháp này áp dụng một sửa đổi phần thưởng theo các công trình trước đây. Sửa đổi này hiệu quả giới thiệu một hình phạt sinh tồn khuyến khích tác nhân hoàn thành mê cung càng nhanh càng tốt. Trong giai đoạn online, chúng tôi duy trì cùng sửa đổi phần thưởng như giai đoạn offline trong quá trình huấn luyện nhưng giữ nguyên phần thưởng trong quá trình đánh giá.

Hình 6 trình bày hiệu suất của ENOTO-LAPO và các phương pháp baseline trong giai đoạn tinh chỉnh online. Trước tiên, LAPO chứng minh hiệu suất offline tốt hơn IQL, cung cấp một điểm khởi đầu cao hơn cho giai đoạn online, đặc biệt trong các môi trường umaze và medium maze nơi nó gần như đạt đến trần hiệu suất. Trong giai đoạn online, IQL cho thấy hiệu suất tiệm cận chậm hơn do các ràng buộc chính sách offline. Dựa trên IQL, PEX nâng cao mức độ khám phá bằng cách tích hợp các chính sách mới bổ sung được huấn luyện từ đầu, nhưng tính ngẫu nhiên mạnh của những chính sách này trong giai đoạn online đầu gây ra sụt giảm hiệu suất. Lưu ý rằng mặc dù cả IQL và PEX đều chia sẻ cùng điểm khởi đầu, PEX thể hiện sụt giảm hiệu suất nghiêm trọng hơn trên hầu hết các nhiệm vụ. Về thuật toán Cal-QL, tương tự như kết quả được mô tả trong bài báo gốc, nó chứng minh hiệu suất mạnh mẽ trong môi trường Antmaze, vượt trội đáng kể so với các đối tác MuJoCo. Đáng chú ý, nó thể hiện tính ổn định và hiệu quả học tập vượt trội so với hai phương pháp baseline, IQL và PEX. Đối với khung làm việc ENOTO đề xuất của chúng tôi, chúng tôi chứng minh rằng ENOTO-LAPO không chỉ có thể nâng cao hiệu suất offline mà còn tạo điều kiện cho việc cải thiện hiệu suất ổn định và nhanh chóng trong khi duy trì hiệu suất offline mà không có suy giảm. Cách tiếp cận này cho phép tác nhân offline nhanh chóng thích ứng với môi trường thực tế, cung cấp tinh chỉnh online hiệu quả và hiệu suất. Ngoài ra, chúng tôi trực tiếp tận dụng LAPO với hai mạng Q cho huấn luyện offline-to-online và sử dụng so sánh với phương pháp ENOTO-LAPO của chúng tôi để xác minh thêm tính hiệu quả của khung làm việc ENOTO. Kết quả bao gồm một số nghiên cứu ablation có thể được tìm thấy trong phụ lục.

5 Kết luận và Hạn chế
Trong công trình này, chúng tôi đã chứng minh rằng Q-ensemble có thể được tận dụng hiệu quả để giảm thiểu huấn luyện không ổn định và sụt giảm hiệu suất, và đóng vai trò như một phương pháp ràng buộc linh hoạt hơn cho tinh chỉnh online trong các cài đặt khác nhau. Dựa trên quan sát này, chúng tôi đề xuất Khung làm việc RL Ensemble-based Offline-to-Online (ENOTO), cho phép nhiều thuật toán RL offline bi quan thực hiện tinh chỉnh online lạc quan và cải thiện hiệu suất của chúng một cách hiệu quả trong khi duy trì quá trình huấn luyện ổn định. Khung làm việc đề xuất đơn giản và có thể được kết hợp với nhiều thuật toán RL offline hiện tại. Chúng tôi thực hiện ENOTO với các kết hợp khác nhau và tiến hành thí nghiệm trên một loạt rộng các nhiệm vụ để chứng minh tính hiệu quả của nó.

Mặc dù có kết quả hứa hẹn, có một số hạn chế trong công trình của chúng tôi cần được thừa nhận. Trước tiên, mặc dù ENOTO được thiết kế để trở thành một plugin linh hoạt cho nhiều thuật toán RL offline khác nhau, nó có thể yêu cầu những sửa đổi thêm để đạt được hiệu suất tối ưu trong các bối cảnh khác nhau. Ví dụ, điều chỉnh hệ số trọng số của mục BC có thể dẫn đến hiệu suất tinh chỉnh tốt hơn cho TD3+BC [Fujimoto and Gu, 2021]. Thứ hai, chi phí tính toán của ensemble và ước lượng uncertainty có thể hạn chế khả năng mở rộng của ENOTO cho các vấn đề quy mô lớn. Công trình tương lai có thể điều tra các cách để giảm chi phí tính toán bằng cách sử dụng deep ensemble [Fort et al., 2019] hoặc distillation ensemble [Hinton et al., 2015], trong khi duy trì hiệu suất bằng cách sử dụng nén Bayesian [Louizos et al., 2017] hoặc xấp xỉ variational [Kingma and Welling, 2013]. Những phương pháp này có thể làm cho ENOTO có khả năng mở rộng và thực tế hơn cho các vấn đề quy mô lớn và ứng dụng thực tế, cho phép phát triển các hệ thống RL offline-to-online hiệu quả và đáng tin cậy hơn.

--- TRANG 8 ---
Lời cảm ơn
Công trình này được hỗ trợ bởi Quỹ Khoa học Tự nhiên Quốc gia Trung Quốc (Số cấp phép 92370132, 62106172), Phòng thí nghiệm Khoa học và Công nghệ về Kỹ thuật Hệ thống Thông tin (Số cấp phép WDZC20235250409, 6142101220304), và Chương trình Tài năng Trẻ Xiaomi của Quỹ Xiaomi.

Tài liệu tham khảo
[Agarwal et al., 2020] Rishabh Agarwal, Dale Schuurmans, và Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. Trong International Conference on Machine Learning, trang 104–114. PMLR, 2020.

[An et al., 2021] Gaon An, Seungyong Moon, Jang-Hyun Kim, và Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. Advances in neural information processing systems, 34:7436–7447, 2021.

[Bai et al., 2022] Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhihong Deng, Animesh Garg, Peng Liu, và Zhaoran Wang. Pessimistic bootstrapping for uncertainty-driven offline reinforcement learning. arXiv preprint arXiv:2202.11566, 2022.

[Chen et al., 2021] Xinyue Chen, Che Wang, Zijian Zhou, và Keith Ross. Randomized ensembled double q-learning: Learning fast without a model. arXiv preprint arXiv:2101.05982, 2021.

[Chen et al., 2022] Xi Chen, Ali Ghadirzadeh, Tianhe Yu, Yuan Gao, Jianhao Wang, Wenzhe Li, Bin Liang, Chelsea Finn, và Chongjie Zhang. Latent-variable advantage-weighted policy optimization for offline rl. arXiv preprint arXiv:2203.08949, 2022.

[Ciosek et al., 2019] Kamil Ciosek, Quan Vuong, Robert Loftin, và Katja Hofmann. Better exploration with optimistic actor critic. Advances in Neural Information Processing Systems, 32, 2019.

[Ecoffet et al., 2021] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, và Jeff Clune. First return, then explore. Nature, 590(7847):580–586, 2021.

[Fort et al., 2019] Stanislav Fort, Huiyi Hu, và Balaji Lakshminarayanan. Deep ensembles: A loss landscape perspective. arXiv preprint arXiv:1912.02757, 2019.

[Fu et al., 2020] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, và Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.

[Fujimoto and Gu, 2021] Scott Fujimoto và Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. Advances in neural information processing systems, 34:20132–20145, 2021.

[Fujimoto et al., 2018] Scott Fujimoto, Herke Hoof, và David Meger. Addressing function approximation error in actor-critic methods. Trong International conference on machine learning, trang 1587–1596. PMLR, 2018.

[Fujimoto et al., 2019] Scott Fujimoto, David Meger, và Doina Precup. Off-policy deep reinforcement learning without exploration. Trong International conference on machine learning, trang 2052–2062. PMLR, 2019.

[Ghasemipour et al., 2022] Seyed Kamyar Seyed Ghasemipour, Shixiang Shane Gu, và Ofir Nachum. Why so pessimistic? estimating uncertainties for offline rl through ensembles, and why their independence matters. arXiv preprint arXiv:2205.13703, 2022.

[Haarnoja et al., 2018] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.

[Hao et al., 2023] Jianye Hao, Tianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi Liu, Zhaopeng Meng, Peng Liu, và Zhen Wang. Exploration in deep reinforcement learning: From single-agent to multiagent domain. IEEE Transactions on Neural Networks and Learning Systems, trang 1–21, 2023.

[Hinton and Roweis, 2002] Geoffrey E Hinton và Sam Roweis. Stochastic neighbor embedding. Advances in neural information processing systems, 15, 2002.

[Hinton et al., 2015] Geoffrey Hinton, Oriol Vinyals, và Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.

[Houthooft et al., 2016] Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, và Pieter Abbeel. Vime: Variational information maximizing exploration. Advances in neural information processing systems, 29, 2016.

[Kidambi et al., 2020] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, và Thorsten Joachims. Morel: Model-based offline reinforcement learning. Advances in neural information processing systems, 33:21810–21823, 2020.

[Kingma and Welling, 2013] Diederik P Kingma và Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.

[Kostrikov et al., 2021] Ilya Kostrikov, Ashvin Nair, và Sergey Levine. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021.

[Kumar et al., 2020] Aviral Kumar, Aurick Zhou, George Tucker, và Sergey Levine. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179–1191, 2020.

[Lee et al., 2021] Kimin Lee, Michael Laskin, Aravind Srinivas, và Pieter Abbeel. Sunrise: A simple unified framework for ensemble learning in deep reinforcement learning. Trong International Conference on Machine Learning, trang 6131–6141. PMLR, 2021.

[Lee et al., 2022] Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, và Jinwoo Shin. Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble. Trong Conference on Robot Learning, trang 1702–1712. PMLR, 2022.

[Liu et al., 2023] Jinyi Liu, Yi Ma, Jianye Hao, Yujing Hu, Yan Zheng, Tangjie Lv, và Changjie Fan. Prioritized trajectory replay: A replay memory for data-driven reinforcement learning. CoRR, abs/2306.15503, 2023.

[Liu et al., 2024] Jinyi Liu, Zhi Wang, Yan Zheng, Jianye Hao, Chenjia Bai, Junjie Ye, Zhen Wang, Haiyin Piao, và Yang Sun. Ovd-explorer: Optimism should not be the sole pursuit of exploration in noisy environments. Trong Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, trang 13954–13962. AAAI Press, 2024.

[Louizos et al., 2017] Christos Louizos, Karen Ullrich, và Max Welling. Bayesian compression for deep learning. Advances in neural information processing systems, 30, 2017.

[Lu et al., 2021] Yao Lu, Karol Hausman, Yevgen Chebotar, Mengyuan Yan, Eric Jang, Alexander Herzog, Ted Xiao, Alex Irpan, Mohi Khansari, Dmitry Kalashnikov, et al. Aw-opt: Learning robotic skills with imitation and reinforcement at scale. arXiv preprint arXiv:2111.05424, 2021.

[Mao et al., 2022] Yihuan Mao, Chao Wang, Bin Wang, và Chongjie Zhang. Moore: Model-based offline-to-online reinforcement learning. arXiv preprint arXiv:2201.10070, 2022.

[Mark et al., 2022] Max Sobol Mark, Ali Ghadirzadeh, Xi Chen, và Chelsea Finn. Fine-tuning offline policies with optimistic action selection. Trong Deep Reinforcement Learning Workshop NeurIPS 2022, 2022.

[Mnih et al., 2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.

[Nair et al., 2020] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, và Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.

[Nakamoto et al., 2023] Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, và Sergey Levine. Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning. arXiv preprint arXiv:2303.05479, 2023.

[Osband et al., 2016] Ian Osband, Charles Blundell, Alexander Pritzel, và Benjamin Van Roy. Deep exploration via bootstrapped dqn. Advances in neural information processing systems, 29, 2016.

[Ostrovski et al., 2017] Georg Ostrovski, Marc G Bellemare, Aäron Oord, và Rémi Munos. Count-based exploration with neural density models. Trong International conference on machine learning, trang 2721–2730. PMLR, 2017.

[Savinov et al., 2018] Nikolay Savinov, Anton Raichuk, Raphaël Marinier, Damien Vincent, Marc Pollefeys, Timothy Lillicrap, và Sylvain Gelly. Episodic curiosity through reachability. arXiv preprint arXiv:1810.02274, 2018.

[Schrittwieser et al., 2020] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604–609, 2020.

[Seno and Imai, 2022] Takuma Seno và Michita Imai. d3rlpy: An offline deep reinforcement learning library. The Journal of Machine Learning Research, 23(1):14205–14224, 2022.

[Silver et al., 2017] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354–359, 2017.

[Silver et al., 2018] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140–1144, 2018.

[Tang et al., 2017] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan, John Schulman, Filip DeTurck, và Pieter Abbeel. # exploration: A study of count-based exploration for deep reinforcement learning. Advances in neural information processing systems, 30, 2017.

[Tarasov et al., 2022] Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, và Sergey Kolesnikov. CORL: Research-oriented deep offline reinforcement learning library. Trong 3rd Offline RL Workshop: Offline RL as a "Launchpad", 2022.

[Todorov et al., 2012] Emanuel Todorov, Tom Erez, và Yuval Tassa. Mujoco: A physics engine for model-based control. Trong 2012 IEEE/RSJ international conference on intelligent robots and systems, trang 5026–5033. IEEE, 2012.

[Tsividis et al., 2021] Pedro A Tsividis, Joao Loula, Jake Burga, Nathan Foss, Andres Campero, Thomas Pouncy, Samuel J Gershman, và Joshua B Tenenbaum. Human-level reinforcement learning through theory-based modeling, exploration, and planning. arXiv preprint arXiv:2107.12544, 2021.

[Vinyals et al., 2019] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojciech M Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, et al. Alphastar: Mastering the real-time strategy game starcraft ii. DeepMind blog, 2:20, 2019.

[Wu et al., 2019] Yifan Wu, George Tucker, và Ofir Nachum. Behavior regularized offline reinforcement learning. arXiv preprint arXiv:1911.11361, 2019.

[Yang et al., 2022] Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie Zhang, và Lei Han. Rorl: Robust offline reinforcement learning via conservative smoothing. arXiv preprint arXiv:2206.02829, 2022.

[Yu et al., 2020] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea Finn, và Tengyu Ma. Mopo: Model-based offline policy optimization. Advances in Neural Information Processing Systems, 33:14129–14142, 2020.

[Zhang et al., 2023] Haichao Zhang, We Xu, và Haonan Yu. Policy expansion for bridging offline-to-online reinforcement learning. arXiv preprint arXiv:2302.00935, 2023.

[Zhao et al., 2022] Yi Zhao, Rinu Boney, Alexander Ilin, Juho Kannala, và Joni Pajarinen. Adaptive behavior cloning regularization for stable offline-to-online reinforcement learning. arXiv preprint arXiv:2210.13846, 2022.

--- TRANG 11-21 ---
[Nội dung phụ lục tiếp tục được dịch với cùng định dạng và độ chi tiết...]
