# 2406.11827.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2406.11827.pdf
# Kích thước tệp: 645308 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
WPO: Nâng cao RLHF với Tối ưu hóa Ưu tiên Có trọng số
Wenxuan Zhou†, Ravi Agrawal, Shujian Zhang, Sathish Reddy Indurthi
Sanqiang Zhao, Kaiqiang Song, Silei Xu, Chenguang Zhu
Zoom Video Communications
Tóm tắt
Học tăng cường từ phản hồi của con người
(RLHF) là một giải pháp đầy hứa hẹn để căn chỉnh các mô hình ngôn ngữ lớn (LLM) gần hơn với các giá trị của con người. Tối ưu hóa ưu tiên off-policy, trong đó dữ liệu ưu tiên được thu thập từ các mô hình khác, được áp dụng rộng rãi do tính hiệu quả về chi phí và khả năng mở rộng. Tuy nhiên, tối ưu hóa ưu tiên off-policy thường gặp phải khoảng cách phân phối giữa chính sách được sử dụng để thu thập dữ liệu và chính sách mục tiêu, dẫn đến tối ưu hóa không tối ưu. Trong bài báo này, chúng tôi đề xuất một chiến lược mới để giảm thiểu vấn đề này bằng cách mô phỏng học tập on-policy với dữ liệu ưu tiên off-policy. Phương pháp Tối ưu hóa Ưu tiên Có trọng số (WPO) của chúng tôi điều chỉnh dữ liệu off-policy để tương tự dữ liệu on-policy hơn bằng cách gán trọng số lại cho các cặp ưu tiên theo xác suất của chúng dưới chính sách hiện tại. Phương pháp này không chỉ giải quyết vấn đề khoảng cách phân phối mà còn nâng cao quá trình tối ưu hóa mà không phát sinh chi phí bổ sung. Chúng tôi xác thực phương pháp của mình trên các benchmark tuân theo hướng dẫn bao gồm Alpaca Eval 2 và MT-bench. WPO không chỉ vượt trội Tối ưu hóa Ưu tiên Trực tiếp (DPO) lên đến 5.6% trên Alpaca Eval 2 mà còn thiết lập tỷ lệ thắng có kiểm soát độ dài đáng chú ý là 76.7% so với GPT-4-turbo dựa trên Gemma-2-9b-it. Chúng tôi phát hành mã nguồn và mô hình tại https://github.com/wzhouad/WPO .

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM; Ouyang et al. 2022; Achiam et al. 2023; Tunstall et al. 2023; Chung et al. 2024) đã thể hiện khả năng đáng chú ý trong việc tạo ra các phản hồi giống con người. Tuy nhiên, chúng vẫn gặp thách thức trong các tình huống đòi hỏi tiêu chuẩn cao về độ tin cậy, an toàn và đạo đức. Để giải quyết những thách thức này, học tăng cường từ phản hồi của con người (RLHF; Christiano et al. 2017; Ouyang et al. 2022; Glaese et al. 2022) là một phương pháp đầy hứa hẹn để căn chỉnh LLM tốt hơn với các giá trị của con người.

Tùy thuộc vào cách các đầu ra được tạo ra, RLHF có thể được phân loại thành các cài đặt on-policy và off-policy. Trong cài đặt on-policy (Schulman et al., 2017; Yuan et al., 2024; Rosset et al., 2024; Wu et al., 2024), mô hình chính sách được sử dụng để tạo đầu ra giống với mô hình chính sách đang được tối ưu hóa. Trong quá trình này, một mô hình chính sách được khởi tạo đầu tiên từ việc tinh chỉnh có giám sát (SFT). Sau đó, một mô hình phần thưởng (Schulman et al., 2017; Gao et al., 2023; Jiang et al., 2023) được thu được dựa trên phản hồi của con người (Schulman et al., 2017) hoặc AI (Lee et al., 2023). Cuối cùng, mô hình chính sách lấy mẫu đầu ra trong quá trình huấn luyện, sau đó được đánh giá sử dụng mô hình phần thưởng. Mô hình chính sách được tối ưu hóa để cải thiện phần thưởng kỳ vọng sử dụng các mục tiêu huấn luyện như Tối ưu hóa Chính sách Gần đúng (PPO; Schulman et al. 2017) và Tối ưu hóa Ưu tiên Trực tiếp (DPO; Rafailov et al. 2023). Tuy nhiên, RL on-policy phụ thuộc nhiều vào việc lấy mẫu chính sách trong quá trình huấn luyện và phần thưởng trực tuyến, có thể phát sinh chi phí cao. Ngược lại, trong cài đặt off-policy (Tunstall et al., 2023; Ivison et al., 2023), các đầu ra được tạo từ các mô hình khác nhau, và mô hình chính sách được tối ưu hóa dựa trên những dữ liệu này thay vì các đầu ra được lấy mẫu của nó. Do đó, RL off-policy mang lại lợi thế đáng kể về mặt chi phí và hiệu quả dữ liệu và dễ mở rộng hơn.

Tuy nhiên, RL off-policy thường cho hiệu suất tệ hơn RL on-policy, do khoảng cách phân phối giữa chính sách được sử dụng để thu thập dữ liệu và chính sách mục tiêu đang được tối ưu hóa, dẫn đến bất ổn và không hiệu quả trong huấn luyện (Fujimoto et al., 2019; Kumar et al., 2019, 2020; Xu et al., 2024; Tang et al., 2024a; Tajwar et al., 2024). Trong tối ưu hóa ưu tiên off-policy, việc tối ưu hóa thường được thực hiện trên dữ liệu ưu tiên được lấy mẫu từ các mô hình khác, và tất cả các phần tử ưu tiên đều được đối xử như nhau. Tuy nhiên, một số dữ liệu ưu tiên, xa cách chính sách hiện tại, ít mang tính thông tin hơn cho việc huấn luyện, dẫn đến tối ưu hóa không hiệu quả và không tối ưu.

--- TRANG 2 ---
Tối ưu hóa Ưu tiên Trực tiếp (DPO) Tối ưu hóa Ưu tiên Có trọng số (WPO)
      : Viết cho tôi một bài thơ về lịch sử jazz
Dữ liệu Ưu tiên       : Viết cho tôi một bài thơ về lịch sử jazz
Policy LM 
 Policy LM 
Dữ liệu Ưu tiên
0.3 
0.9 

Hình 1: Tổng quan về Tối ưu hóa Ưu tiên Có trọng số (WPO). Một số ký hiệu được gắn nhãn cùng với các thành phần tương ứng. DPO hiện tại trực tiếp tối ưu hóa chính sách để thỏa mãn tốt nhất các ưu tiên với dữ liệu off-policy. Ngược lại, WPO điều chỉnh dữ liệu off-policy để tương tự dữ liệu on-policy hơn bằng cách gán trọng số lại cho các cặp ưu tiên theo xác suất của chúng dưới chính sách hiện tại.

Trong bài báo này, chúng tôi đề xuất mô phỏng tối ưu hóa ưu tiên on-policy với dữ liệu ưu tiên off-policy, kết hợp hiệu quả của RL off-policy với lợi ích hiệu suất liên quan đến RL on-policy. Phương pháp của chúng tôi được thúc đẩy bởi quá trình tạo dữ liệu khái niệm sau đây. Quá trình này bắt đầu bằng việc chuyển đổi tập dữ liệu ưu tiên hiện có thành một hàm gán nhãn ưu tiên. Sau đó chúng ta có thể lấy mẫu lại một tập dữ liệu ưu tiên mới thông qua bootstrapping từ dữ liệu hiện có. Quá trình này bao gồm việc lấy mẫu đồng đều các đầu vào từ tập dữ liệu ưu tiên và lấy mẫu trực tuyến các cặp đầu ra mới với mô hình chính sách hiện tại. Mỗi cặp được giữ lại nếu nó có thể được gán nhãn bởi hàm gán nhãn; nếu không, nó sẽ bị loại bỏ. Sau đó chúng tôi thực hiện DPO trên tập dữ liệu ưu tiên được tạo lại. Trong thực tế, quá trình bootstrapping này có thể được triển khai với mục tiêu Tối ưu hóa Chính sách Có trọng số (WPO), trong đó các cặp ưu tiên khác nhau được gán trọng số lại theo xác suất chung của các đầu ra của chúng.

Chúng tôi tiếp tục phát triển một cơ chế căn chỉnh trọng số để đảm bảo rằng tất cả các cặp được tạo on-policy đều được cân trọng đồng đều. Bằng cách này, WPO có thể giảm thiểu hiệu quả khoảng cách phân phối trong RL mà không phát sinh chi phí bổ sung.

Chúng tôi đánh giá WPO trên các benchmark tuân theo hướng dẫn, bao gồm Alpaca Eval 2 (Li et al., 2023) và MT-bench (Zheng et al., 2023). Trong cài đặt off-policy dựa trên Ultrafeedback (Cui et al., 2023), WPO cải thiện tỷ lệ thắng có kiểm soát độ dài so với GPT-4-turbo trên Alpaca Eval 2 lên đến 14.9% so với mô hình SFT, vượt trội DPO lên đến 5.6%. Đặc biệt, trong cài đặt RL lai (hybrid) nơi dữ liệu ưu tiên off-policy được bổ sung thêm với các đầu ra on-policy, WPO (Hình 1) đạt được tỷ lệ thắng có kiểm soát độ dài SOTA mới là 76.7% trên Alpaca Eval 2. Ngoài ra, chúng tôi phát hiện rằng WPO có thể được tích hợp vào các hàm mất mát khác cho tối ưu hóa ưu tiên và cho thấy cải thiện nhất quán. Hơn nữa, chúng tôi so sánh có hệ thống hiệu suất mô hình trong các cài đặt RL khác nhau. Phân tích của chúng tôi tiết lộ rằng cài đặt lai, sử dụng cả dữ liệu ưu tiên on-policy và off-policy, đạt kết quả tốt nhất, và dữ liệu không được ưa thích on-policy quan trọng hơn cho tối ưu hóa ưu tiên.

Tóm lại, đóng góp của chúng tôi có ba khía cạnh:
• Chúng tôi xác định vấn đề khoảng cách phân phối trong tối ưu hóa ưu tiên off-policy, và do đó giới thiệu một phương pháp để mô phỏng RL on-policy sử dụng dữ liệu ưu tiên off-policy.
• Chúng tôi đề xuất mục tiêu WPO, gán trọng số lại cho các cặp ưu tiên dựa trên xác suất của chúng. Điều này đảm bảo rằng các đầu ra phù hợp và có khả năng nhất được ưu tiên trong quá trình tối ưu hóa, giảm thiểu khoảng cách phân phối và cải thiện hiệu quả của tối ưu hóa ưu tiên.
• Chúng tôi tiến hành các benchmark tuân theo hướng dẫn mở rộng. Kết quả của chúng tôi chứng minh rằng WPO vượt trội đáng kể so với DPO và đạt kết quả SOTA mới trên Alpaca Eval 2 trong cài đặt RL lai.

2 Công việc liên quan
Các phương pháp căn chỉnh tổng quát. Sự tiến bộ của ChatGPT đã thúc đẩy những tiến bộ đáng kể trong lĩnh vực các mô hình ngôn ngữ lớn (LLM). Các mô hình đáng chú ý như Zephyr (Tunstall et al., 2023) và GPT-4 (Achiam et al., 2023) đã hiệu quả chứng minh việc áp dụng các kỹ thuật như học tăng cường từ phản hồi của con người (RLHF; Christiano et al. 2017; Ouyang et al. 2022;

--- TRANG 3 ---
Glaese et al. 2022) và tối ưu hóa ưu tiên trực tiếp (DPO; Rafailov et al. 2023), làm nổi bật hiệu quả của chúng trong việc đạt được sự căn chỉnh mô hình được cải thiện. Những phương pháp này, cùng với các phương pháp liên quan như hiệu chỉnh khả năng sequence likelihood (Zhao et al., 2023) và Tối ưu hóa Ưu tiên Tổng quát (GPO) (Tang et al., 2024b), nhằm tinh chỉnh các mục tiêu của RLHF bằng cách nâng cao rõ ràng sự phân biệt giữa các đầu ra được ưa thích hơn và ít được ưa thích hơn. Ngoài ra, việc giới thiệu thuật toán Tối ưu hóa Nash Trực tiếp (DNO) bởi Rosset et al. (2024) đại diện cho một sự đổi mới tiếp theo. Thuật toán này sử dụng cross-entropy để đánh giá khoảng cách giữa tỷ lệ thắng thực tế và dự đoán. Các ứng dụng thực tế thường xuyên dựa vào khung lặp của DPO (Xu et al., 2023). Tuy nhiên, DPO thường tiết lộ sự khác biệt giữa các phân phối đầu ra do chính sách tạo ra và những phân phối trong tập dữ liệu ưu tiên. Để giải quyết điều này, chúng tôi đề xuất mô phỏng học tăng cường on-policy sử dụng dữ liệu off-policy, do đó kết hợp lợi ích của RL on-policy với hiệu quả nâng cao.

Học tăng cường on-policy. Tự-chơi Fine-Tuning (Chen et al., 2024) hoạt động dưới khung lặp tương tự DPO, sử dụng các phản hồi được gán nhãn bởi con người làm "người thắng" và đầu ra từ các lần lặp trước làm "người thua" trong mỗi cặp. Tương tự, Tối ưu hóa Ưu tiên Đối kháng (Cheng et al., 2023) kết hợp các mất mát tương phản, loại bỏ nhu cầu phản hồi trực tiếp từ người chú thích. Phương pháp này giới thiệu một hàm mất mát cấp token được gọi là Cringe Loss (Adolphs et al., 2022), phân biệt token tiếp theo đúng với một token cố ý sai từ từ vựng. Pairwise Cringe Loss (Xu et al., 2023) sử dụng cơ chế mất mát cringe này trong khung huấn luyện lặp cải thiện liên tục. Hơn nữa, việc giới thiệu gần đây của SAMI (Fränken et al., 2024) nhắm mục tiêu tối ưu hóa cận dưới của thông tin tương hỗ có điều kiện giữa prompts và phản hồi thông qua kỹ thuật ước lượng tương phản. Trong phương pháp của chúng tôi, chúng tôi điều chỉnh tầm quan trọng của mỗi cặp trong quá trình huấn luyện bằng cách gán trọng số lớn hơn cho những cặp có khả năng cao hơn được lấy mẫu từ mô hình chính sách, do đó mô phỏng học tăng cường on-policy.

3 Phương pháp
Trong phần này, chúng tôi cung cấp nền tảng lý thuyết của RLHF và DPO trong Phần 3.1. Sau đó chúng tôi giới thiệu vấn đề khoảng cách phân phối và

Thuật toán 1: Tối ưu hóa Ưu tiên Có trọng số (WPO)
Đầu vào: Tập dữ liệu (D) với prompts và phản hồi, policy LM πθ, tổng số lần lặp T, tốc độ học αt,
for t= 0 to T do
    Lấy mẫu một mini-batch của các tuple (x, yw, yl) từ D,
    Tính trọng số căn chỉnh thông qua Eq. (2),
    Tính LWPO thông qua Eq. (1),
    Cập nhật tham số chính sách θ sử dụng gradient descent: θ←θ−αt∇θ(x, yw, yl, θ).
end for

đề xuất phương pháp WPO (Thuật toán 1) trong Phần 3.2. Cuối cùng, chúng tôi khám phá cách mô phỏng RL on-policy tốt hơn thông qua căn chỉnh trọng số trong Phần 3.3.

3.1 Kiến thức chuẩn bị
RLHF (Schulman et al., 2017) nhằm căn chỉnh một mô hình ngôn ngữ lớn với các ưu tiên của con người. Cho một tập dữ liệu ưu tiên D={(x(i), y(i)w, y(i)l)}Ni=1, trong đó yw và yl là một cặp đầu ra cho prompt x được lấy mẫu từ một mô hình chính sách, và yw được ưa thích hơn yl như được xác định bởi người chú thích con người hoặc AI. Ưu tiên này được mô hình bởi một hàm phần thưởng tiềm ẩn r∗(x, y), tính điểm về mức độ phù hợp của đầu ra ứng cử y với đầu vào x. Có nhiều cách khác nhau để mô hình hàm phần thưởng, trong đó mô hình Bradley-Terry (BT; Bradley và Terry 1952) được sử dụng phổ biến nhất. Mô hình BT giả định rằng phân phối ưu tiên được đặc trưng bởi phương trình sau:

p(yw≻yl|x) = exp(r∗(x,yw))/(exp(r∗(x,yw))+exp(r∗(x,yl))).

Các tham số của hàm phần thưởng có thể được ước lượng dựa trên ước lượng hợp lý tối đa, dẫn đến mô hình phần thưởng ˆr(x, y). Sau đó, chúng ta có thể sử dụng mô hình phần thưởng được khớp để cung cấp phản hồi cho một mô hình ngôn ngữ lớn bằng cách tối ưu hóa mục tiêu sau:

max πθEx∼D,y∼πθ(·|x)[ˆr(x, y)−βlogπθ(·|x)/πref(·|x)],

trong đó β kiểm soát độ lệch giữa mô hình chính sách πθ và mô hình tham chiếu πref, thường được khởi tạo từ mô hình SFT.

DPO. Tối ưu hóa tối ưu hóa trực tiếp (DPO; Rafailov et al. 2023) tích hợp việc học hàm phần thưởng và mô hình chính sách thành một mục tiêu thống nhất. Cụ thể, giả sử chính sách tối ưu π∗ được cho, phần thưởng tương ứng r∗ có dạng đóng:

r∗(x, y) = βlogπ∗(y|x)/πref(y|x)+βlogZ(x),

trong đó Z(x) là hàm phân hoạch. Áp dụng việc tham số hóa lại này cho mô hình BT, chúng ta có:

p∗(yw≻yl|x) = σ[βlogπ∗(yw|x)/πref(yw|x)−βlogπ∗(yl|x)/πref(yl|x)].

Sau đó chúng ta có thể xây dựng một mục tiêu ước lượng hợp lý tối đa cho mô hình chính sách πθ trên tập dữ liệu ưu tiên D, dẫn đến mục tiêu huấn luyện sau:

LDPO=−E(x,yw,yl)∼D[logp(yw≻yl|x)].

Ở đây, mất mát được tính toán dựa trên việc lấy mẫu đồng đều của tập dữ liệu ưu tiên. Trong thực tế, yw và yl trong tập dữ liệu ưu tiên có thể được tạo ra hoặc với cùng mô hình chính sách đang được tối ưu hóa, tương ứng với cài đặt RL on-policy (Xu et al., 2023; Yuan et al., 2024; Rosset et al., 2024), hoặc với các mô hình khác (ví dụ: GPT-4; Achiam et al. 2023), tương ứng với cài đặt RL off-policy (Tunstall et al., 2023; Ivison et al., 2023; Pal et al., 2024).

--- TRANG 4 ---
3.2 Tối ưu hóa Ưu tiên Có trọng số
DPO không yêu cầu tích cực tạo ra các đầu ra mới từ chính sách hiện tại, làm cho nó tiết kiệm chi phí hơn và phù hợp cho các cài đặt off-policy. Tuy nhiên, DPO giới thiệu một sự khác biệt đáng chú ý giữa phân phối các đầu ra do chính sách tạo ra và những đầu ra có trong tập dữ liệu ưu tiên. Sự khác biệt này có thể dẫn đến việc học kém hiệu quả hơn. Để minh họa, hãy xem xét hai trường hợp dữ liệu ưu tiên:

⟨x(1), y(1)w, y(1)l⟩ và ⟨x(2), y(2)w, y(2)l⟩,

trong đó tuple đầu tiên được lấy mẫu trực tiếp từ mô hình chính sách hiện tại, trong khi tuple thứ hai được lấy mẫu từ một phân phối khác với mô hình chính sách hiện tại. Mặc dù có sự khác biệt này trong xác suất lấy mẫu, DPO đối xử với cả hai trường hợp như nhau trong tính toán mất mát của nó, bỏ qua thực tế rằng tuple đầu tiên, đại diện cho đầu ra có khả năng cao hơn của chính sách hiện tại, lý tưởng nên có ảnh hưởng lớn hơn đến quá trình tối ưu hóa. Sự bỏ sót này có thể dẫn đến hiệu suất không tối ưu, vì DPO không ưu tiên việc học từ đầu ra đại diện hoặc có khả năng nhất của mô hình chính sách.

Để giải quyết vấn đề này, chúng tôi đề xuất mô phỏng RL on-policy sử dụng dữ liệu off-policy, do đó vừa nhanh vừa hưởng lợi từ RL on-policy.

Suy dẫn lý thuyết. Để mô phỏng RL on-policy, trước tiên chúng tôi chuyển đổi tập dữ liệu ưu tiên (off-policy) D={(x(i), y(i)w, y(i)l)}Ni=1 thành hàm gán nhãn ưu tiên sau:

f(x, y1, y2) = {
    y1≻y2, (x, y1, y2)∈ D
    y2≻y1, (x, y2, y1)∈ D  
    NA, otherwise
}

trong đó chúng tôi giả định rằng tập dữ liệu không chứa ưu tiên mâu thuẫn, có nghĩa là với bất kỳ x nào, nếu (x, y1, y2)∈ D thì (x, y2, y1)∉ D. Sau đó chúng tôi tạo ra về mặt khái niệm một tập dữ liệu ưu tiên mới thông qua phương pháp bootstrapping mà không thực sự thực hiện quy trình. Giả sử một đầu vào x được lấy mẫu đồng đều từ tập dữ liệu ưu tiên gốc, và sau đó một cặp đầu ra y1, y2 được lấy mẫu với mô hình chính sách hiện tại. Chúng tôi giữ lại cặp này nếu nó có thể được gán nhãn bởi hàm gán nhãn, và ngược lại loại bỏ cặp này khi f(x, y1, y2) = NA. Nếu chúng ta lấy mẫu trong một lượng thời gian vô hạn, theo định luật số lớn, tỷ lệ xuất hiện của một cặp (x, yw, yl) sẽ tỷ lệ với πθ(yw|x)πθ(yl|x)p(x). Sau đó chúng tôi áp dụng DPO cho tập dữ liệu ưu tiên mới được tạo.

Triển khai thực tế. Quá trình khái niệm ở trên tương đương với việc tối ưu hóa mục tiêu tối ưu hóa ưu tiên có trọng số (WPO) sau, trong đó các cặp khác nhau trong tập dữ liệu ưu tiên gốc được gán trọng số lại:

LWPO=−E(x,yw,yl)∼D[w(x, yw)w(x, yl) logp(yw≻yl|x)], (1)

trong đó w(x, y) =πθ(y|x) và được tách khỏi lan truyền ngược. Thông qua quá trình này, chúng tôi hiệu quả điều chỉnh tầm quan trọng của mỗi cặp trong quá trình huấn luyện, gán trọng số lớn hơn cho những cặp có khả năng cao hơn được lấy mẫu từ mô hình chính sách, do đó mô phỏng RL on-policy.

Trong các mô hình ngôn ngữ nơi yw và yl là các chuỗi token, tích của các xác suất có điều kiện πθ(yw|x)·πθ(yl|x) có thể cực kỳ nhỏ và thể hiện phương sai cao giữa các cặp khác nhau. Để giải quyết điều này, chúng tôi sử dụng xác suất chuỗi được chuẩn hóa độ dài làm yếu tố cân trọng:

w(x, y) = exp(1/|y| ∑|y|t=1 logπθ(yt|x, y<t)),

--- TRANG 5 ---
trong đó |y| đại diện cho số lượng token trong đầu ra.

3.3 Căn chỉnh Trọng số
Mục tiêu của chiến lược cân trọng của chúng tôi là mô phỏng RL on-policy, trong đó các đầu ra được cân trọng theo mức độ chúng căn chỉnh với hành vi on-policy. Đối với các đầu ra được tạo bởi mô hình chính sách hiện tại, chúng tôi mong đợi trọng số của chúng đồng đều là 1, trong khi các đầu ra lệch khỏi hành vi on-policy này nên nhận trọng số nhỏ hơn. Tuy nhiên, do các mức độ tin cậy khác nhau mà LLM thể hiện qua các đầu vào khác nhau (Si et al., 2023; Xiong et al., 2024), ngay cả các đầu ra được tạo bởi mô hình chính sách đôi khi cũng có thể được gán trọng số thấp. Điều này tạo ra một độ lệch không mong muốn trong đó một số đầu ra on-policy nhận trọng số thấp hơn hoàn toàn do tin cậy thấp hơn của mô hình dựa trên đầu vào, làm xáo trộn tính đồng đều mà chúng tôi muốn đạt được. Hình 2 cho thấy phân phối trọng số của các đầu ra được lấy mẫu dựa trên prompts từ Ultrafeedback và mô hình Mistral-sft-beta, trong đó chúng tôi quan sát thấy sự biến đổi đáng kể trong w(x, y).

[Hình 2 hiển thị: Phân phối trọng số của các đầu ra được lấy mẫu sử dụng mô hình chính sách với các phương pháp căn chỉnh khác nhau.]

Để giải quyết điều này và đảm bảo cân trọng đồng đều cho các đầu ra này, chúng tôi đề xuất căn chỉnh các trọng số trong WPO. Một phương pháp trực tiếp là điều chỉnh các trọng số ở trên bằng xác suất chuỗi của các đầu ra on-policy được lấy mẫu từ mô hình chính sách. Tuy nhiên, việc tạo đầu ra trong quá trình huấn luyện tốn kém tính toán, và do đó, chúng tôi khám phá các phương pháp xấp xỉ cho việc căn chỉnh này. Thay vì sử dụng trọng số của toàn bộ chuỗi làm tham chiếu, chúng tôi hoạt động ở cấp độ token và điều chỉnh xác suất của các token đầu ra theo phân phối token trong mô hình chính sách, dựa trên chuỗi con hiện tại.

Chúng tôi đề xuất hai cách để đạt được việc căn chỉnh.

Căn chỉnh tham lam. Trong phương pháp này, chúng tôi điều chỉnh trọng số dựa trên giải mã tham lam bằng cách so sánh xác suất của token hiện tại với xác suất của token có khả năng cao nhất trong từ vựng. Cụ thể, chúng tôi điều chỉnh trọng số dựa trên xác suất token tối đa trong tập hợp tất cả các token trong chuỗi con, được định nghĩa là:

w(x, y) = exp(1/|y| ∑|y|t=1 logπθ(yt|x,y<t)/maxv∈Vπθ(v|x,y<t)),

trong đó V đại diện cho tập hợp tất cả các token trong mô hình ngôn ngữ.

Căn chỉnh được lấy mẫu. Trong phương pháp này, chúng tôi điều chỉnh trọng số dựa trên các đầu ra được lấy mẫu ngẫu nhiên từ mô hình chính sách ở nhiệt độ 1.0. Vì xác suất cho mỗi token v được tính là πθ(v|x, y<t), xác suất kỳ vọng của một token được lấy mẫu ngẫu nhiên sẽ là ∑v∈Vπθ(v|x, y<t)², và các trọng số được hiệu chỉnh sau đó được cho bởi:

w(x, y) = exp(1/|y| ∑|y|t=1 logπθ(yt|x,y<t)/∑v∈Vπθ(v|x,y<t)²). (2)

Chúng tôi sử dụng căn chỉnh được lấy mẫu làm phương pháp căn chỉnh mặc định trong WPO do hiệu suất vượt trội của nó, như được xác nhận trong Phần 4.2. Ngoài ra, trong Hình 2, căn chỉnh được lấy mẫu dẫn đến phân phối trọng số tập trung hơn của các đầu ra từ mô hình chính sách, do đó mô phỏng RL on-policy tốt hơn.

4 Thí nghiệm
Trong phần này, chúng tôi nêu ra các cài đặt thí nghiệm (Phần 4.1) và trình bày các kết quả chính cùng với các nghiên cứu ablation (Phần 4.2). Sau đó chúng tôi so sánh các cài đặt RL khác nhau (Phần 4.3). Phân tích bổ sung về WPO được cung cấp trong Phụ lục A.

4.1 Cài đặt Thí nghiệm
Cấu hình mô hình. Các phương pháp của chúng tôi được triển khai dựa trên mã nguồn chính thức của zephyr¹. Đối với Mistral-base, chúng tôi áp dụng các siêu tham số chính thức từ zephyr. Cụ thể, chúng tôi sử dụng checkpoint SFT của zephyr² làm mô hình SFT của chúng tôi. Huấn luyện được tiến hành trong một epoch với batch size 128, learning rate 5e-7, giai đoạn warm-up cho 10% thời gian huấn luyện, và lịch trình cosine decay. Chúng tôi đặt β là 0.01 cho cả DPO và WPO. Đối với Llama-3-Instruct, chúng tôi thực hiện tìm kiếm siêu tham số trong phạm vi được đề xuất bởi Meng

¹https://github.com/huggingface/alignment-handbook
²https://huggingface.co/HuggingFaceH4/mistral-7b-sft-beta

--- TRANG 6 ---
et al. (2024). Các siêu tham số cuối cùng của chúng tôi là learning rate 1e-6, hai epoch huấn luyện, và β là 0.01 cho cả DPO và WPO. Đối với tất cả các cấu hình huấn luyện, chúng tôi tiến hành huấn luyện cho 5 lần chạy với các random seed khác nhau và báo cáo cả kết quả trung bình và độ lệch chuẩn của chúng.

[Bảng 1: Kết quả Alpaca Eval 2.0 và MT-bench. Chúng tôi báo cáo trung bình và độ lệch chuẩn của kết quả từ 5 lần chạy với random seed khác nhau. Điểm được gạch chân biểu thị mức tăng có ý nghĩa thống kê (p <0.05).]

Dữ liệu huấn luyện. Chúng tôi thực hiện RLHF trong các cài đặt off-policy và lai. Trong cài đặt off-policy, chúng tôi sử dụng tập dữ liệu Ultrafeedback binarized³ (Cui et al., 2023), bao gồm 63k cặp ưu tiên được lấy mẫu từ các mô hình khác ngoài mô hình SFT của chúng tôi, như GPT-4 và Llama-2 (Touvron et al., 2023). Trong cài đặt lai, chúng tôi theo phương pháp trong DNO (Rosset et al., 2024), sử dụng dữ liệu được tạo từ cả mô hình chính sách và các mô hình khác. Cụ thể, chúng tôi lấy mẫu 5 đầu ra từ mô hình SFT dựa trên prompts từ Ultrafeedback và thêm một đầu ra khác được tạo bởi gpt-4-turbo. Chúng tôi sử dụng top-p sampling với p= 0.95 và nhiệt độ 0.7. Chú thích ưu tiên được tạo ra sử dụng gpt-4-turbo với additive scoring prompt. Đối với mỗi prompt, chúng tôi chọn các đầu ra có điểm 5 hoặc 6 làm yw và sau đó chọn một đầu ra ngẫu nhiên với điểm ít nhất thấp hơn một điểm làm yl. Nếu không thể tìm thấy cặp như vậy, prompt sẽ không được sử dụng. Bước xây dựng dữ liệu này tạo ra một tập dữ liệu ưu tiên nhỏ hơn, vì vậy chúng tôi tiếp tục sử dụng cài đặt + Ultrafeedback, trong đó chúng tôi thêm lại các prompt bị thiếu sử dụng các cặp ưu tiên từ Ultrafeedback.

Đánh giá. Chúng tôi đánh giá các mô hình trên Alpaca Eval 2 và MT-bench. Alpaca Eval 2 là một metric tự động đo lường sự căn chỉnh của LLM với ưu tiên của con người sử dụng 805 hướng dẫn đại diện. Đối với mỗi hướng dẫn, phản hồi của mô hình được đánh giá và phản hồi của gpt-4-turbo được so sánh trực tiếp sử dụng auto-evaluator. Tỷ lệ thắng là xác suất mà auto-evaluator ưa thích các phản hồi của mô hình được đánh giá. Alpaca Eval 2 cũng giới thiệu tỷ lệ thắng có kiểm soát độ dài (Dubois et al., 2024) để giải quyết độ lệch độ dài của gpt-4-turbo. Chúng tôi theo cấu hình tạo trong Tunstall et al. (2023) cho các mô hình Mistral và trong Zheng et al. (2024) cho các mô hình Llama-3.

MT-bench là một metric đánh giá tự động dựa trên LLM bao gồm 80 câu hỏi thách thức. Chúng tôi báo cáo kết quả sử dụng hai phương pháp chấm điểm. Trong phương pháp chấm điểm câu trả lời đơn, auto-evaluator (gpt-4-0613) gán điểm từ 1 đến 10 cho các phản hồi, và chúng tôi báo cáo điểm trung bình. Trong phương pháp so sánh theo cặp, evaluator (gpt-4-0613) so sánh hai phản hồi để quyết định cái nào tốt hơn hoặc nếu hòa (được ghi là 0.5 trong tỷ lệ thắng). Phương pháp theo cặp có thể phát hiện sự khác biệt tinh tế hơn giữa các phản hồi so với chấm điểm câu trả lời đơn. Chúng tôi sử dụng cấu hình tạo chính thức trong MT-bench.

4.2 Kết quả Chính và Ablation
WPO nhất quán và đáng kể vượt trội DPO và các biến thể của nó. Kết quả chính được trình bày trong Bảng 1. Chúng tôi bao gồm kết quả của các thuật toán tối ưu hóa ưu tiên khác nhau như DPO, ORPO (Hong et al., 2024), KTO (Ethayarajh et al., 2024), và SimPO (Meng et al., 2024) trên hai benchmark. Đối với ORPO, KTO, và SimPO, chúng tôi báo cáo kết quả đánh giá của các checkpoint mô hình chính thức của họ trên Mistral-base.⁴ Chúng tôi thấy rằng WPO

⁴Chúng tôi không bao gồm kết quả của họ trên Llama-3-Instruct trong cài đặt off-policy vì các checkpoint chính thức không có sẵn. Việc tái tạo các phương pháp này đòi hỏi tìm kiếm siêu tham số mở rộng

--- TRANG 7 ---
nói chung vượt trội DPO trong tất cả các cài đặt và cũng vượt trội tất cả các biến thể của nó trên Mistral-base trong cài đặt off-policy. Đặc biệt, khi được huấn luyện với mô hình Llama-3-Instruct và cài đặt lai +Ultrafeedback, WPO đạt được tỷ lệ thắng có kiểm soát độ dài state-of-the-art mới là 48.6% so với GPT-4-turbo trên Alpaca Eval 2. Những kết quả này làm nổi bật hiệu quả của WPO. Ngoài ra, trong khi DPO kém hiệu suất so với SimPO, nó vẫn thể hiện kết quả cạnh tranh, cung cấp nền tảng vững chắc cho WPO.

Phân tách khác nhau của benchmark. Trên MT-bench, điểm trung bình không phân biệt hiệu quả hiệu suất của các mô hình khác nhau. Ngoài ra, chúng tôi quan sát thấy sự biến đổi trong điểm MT-bench trung bình. Ngay cả khi sử dụng GPT-4 để chấm điểm cùng một đầu ra với nhiệt độ 0, điểm có thể thay đổi lên đến 0.1 tại những thời điểm khác nhau. Cho sự phân tách rõ ràng hơn trong các thí nghiệm của chúng tôi và sự căn chỉnh lớn hơn với đánh giá của con người, như được thể hiện trong bài báo gốc (Zheng et al., 2023), chúng tôi coi tỷ lệ thắng theo cặp là metric phù hợp hơn để đánh giá các phương pháp căn chỉnh khác nhau. Do đó, chúng tôi sử dụng nó cho MT-bench trong phần còn lại của bài báo.

[Bảng 2: Ablation của các phương pháp căn chỉnh trọng số trên Mistral-base trong cài đặt off-policy. Căn chỉnh được lấy mẫu, phương pháp căn chỉnh trọng số mặc định, cho kết quả tốt nhất.]

Căn chỉnh trọng số được lấy mẫu hoạt động tốt nhất. Bảng 2 cho thấy kết quả của WPO với các phương pháp căn chỉnh trọng số khác nhau trên Mistral-base trong cài đặt off-policy. Chúng tôi quan sát thấy rằng căn chỉnh được lấy mẫu vượt trội các biến thể khác trên cả hai benchmark, trong khi lấy mẫu tham lam vượt trội w/o căn chỉnh. Chúng tôi cũng thấy rằng thứ hạng hiệu suất khớp với thứ hạng mức độ tập trung trong phân phối trọng số được hiển thị trong Hình 2. Điều này chỉ ra rằng căn chỉnh trọng số cho phép mô phỏng RL on-policy hiệu quả hơn, dẫn đến hiệu suất được cải thiện.

WPO cũng cải thiện các hàm mất mát khác cho tối ưu hóa ưu tiên. Điều quan trọng cần lưu ý là,

[Bảng 3: Kết quả của WPO với các hàm mất mát khác nhau cho tối ưu hóa ưu tiên trên Mistral-base trong cài đặt off-policy, cho thấy rằng việc kết hợp WPO dẫn đến cải thiện nhất quán.]

ngoài DPO, còn có các hàm mất mát khác để căn chỉnh LLM. Vì WPO hoạt động bằng cách cân trọng dữ liệu ưu tiên và độc lập với hàm mất mát được sử dụng, nó có thể được tích hợp dễ dàng vào chúng. Chúng tôi nghiên cứu liệu việc tích hợp WPO có nâng cao hiệu suất của các hàm mất mát khác hay không. Các mất mát hiện có có thể được phân loại thành những cái sử dụng dữ liệu ưu tiên được ghép cặp và những cái sử dụng dữ liệu ưu tiên không được ghép cặp. Đối với các mất mát sử dụng dữ liệu được ghép cặp, chúng tôi cân trọng mỗi cặp tương tự như DPO. Đối với các mất mát sử dụng dữ liệu không được ghép cặp, chúng tôi cân trọng mỗi đầu ra y độc lập với w(x, y) và chuẩn hóa các trọng số sao cho tổng trọng số của các đầu ra được ưa thích và không được ưa thích đều là 1 trong batch. Việc chuẩn hóa này đảm bảo sự cân bằng giữa các đầu ra được ưa thích và không được ưa thích trong mất mát. Trong nghiên cứu này, chúng tôi xem xét IPO (Azar et al., 2024) và SimPO cho căn chỉnh với dữ liệu được ghép cặp, và KTO cho căn chỉnh với dữ liệu không được ghép cặp. Kết quả trên Mistral-base trong cài đặt off-policy, được hiển thị trong Bảng 3, chỉ ra rằng việc tích hợp WPO dẫn đến kết quả được cải thiện cho tất cả các hàm mất mát. Điều này chứng minh rằng WPO cung cấp cải thiện phổ quát qua các hàm mất mát khác nhau cho tối ưu hóa ưu tiên.

Mô hình cơ sở và phần thưởng tốt hơn mang lại kết quả mạnh hơn. Để tiếp tục nâng cao mô hình của chúng tôi, chúng tôi nghiên cứu việc sử dụng các mô hình cơ sở và mô hình phần thưởng tốt hơn. Cụ thể, chúng tôi áp dụng Gemma-2-9b-it (Team et al., 2024) làm mô hình cơ sở. Trong một thiết lập tương tự như phương pháp lai của chúng tôi, chúng tôi lấy mẫu năm đầu ra từ Gemma và một đầu ra bổ sung từ gpt-4-turbo. Để xếp hạng các đầu ra này, chúng tôi áp dụng ArmoRM (Wang et al., 2024a,b) và sử dụng các đầu ra tốt nhất và tệ nhất để tạo thành các cặp ưu tiên. Chúng tôi sử dụng cùng bộ siêu tham số huấn luyện được sử dụng trong Llama-3-Instruct. Mô hình được tinh chỉnh sử dụng WPO đạt được tỷ lệ thắng có kiểm soát độ dài là 76.7% và tỷ lệ thắng là 77.8% trên Alpaca Eval 2, chứng minh

--- TRANG 8 ---
hiệu quả của phương pháp này.

4.3 So sánh các Cài đặt RL Khác nhau
Các nghiên cứu gần đây về RLHF đã sử dụng nhiều cài đặt RL khác nhau trong đó dữ liệu ưu tiên được tạo theo cách off-policy, on-policy, hoặc lai. Công trình hiện có (Tang et al., 2024a; Xu et al., 2024) đã chứng minh rằng tối ưu hóa ưu tiên on-policy vượt trội các phương pháp off-policy, trong khi Rosset et al. (2024) cho thấy rằng việc kết hợp các đầu ra off-policy chất lượng cao có thể mang lại hiệu suất vượt trội, vì những đầu ra này có thể giới thiệu thông tin có giá trị mà chính sách hiện tại có thể không gặp phải. Trong nghiên cứu này, chúng tôi so sánh hiệu suất mô hình được huấn luyện với WPO qua các cài đặt RL này. Kết quả được trình bày trong Hình 3, thể hiện tỷ lệ thắng có kiểm soát độ dài trên Alpaca Eval 2 và tỷ lệ thắng theo cặp so với cài đặt off-policy trên MT-bench.

[Hình 3: Kết quả của WPO trong các cài đặt RL khác nhau. Cài đặt lai nhất quán cho kết quả tốt hơn các cài đặt RL khác.]

RL lai đạt kết quả tốt nhất. Hình 3 cho thấy rằng đối với cả Mistral-base và Llama-3-Instruct, cài đặt lai—sử dụng cả dữ liệu on-policy và dữ liệu off-policy chất lượng cao từ gpt-4-turbo—nhất quán mang lại hiệu suất vượt trội. Điều này gợi ý rằng việc kết hợp dữ liệu off-policy chất lượng cao và dữ liệu on-policy có thể nâng cao đáng kể tối ưu hóa ưu tiên, điều này phù hợp với kết quả trong Rosset et al. (2024).

On-policy không phải lúc nào cũng tốt hơn off-policy. Phân tích của chúng tôi tiết lộ rằng hiệu quả của tối ưu hóa ưu tiên on-policy so với off-policy phụ thuộc vào mô hình (Munos et al., 2016; Voloshin et al., 2019). Đối với mô hình Mistral-base, cài đặt off-policy cho hiệu suất hơi tốt hơn, trong khi đối với Llama-3-Instruct, cài đặt on-policy cho hiệu suất tốt hơn. Chúng tôi quy nguyên nhân biến đổi này cho chất lượng của mô hình SFT. Trong trường hợp Mistral-base, các đầu ra được lấy mẫu có chất lượng thấp hơn, khiến quá trình tối ưu hóa ưu tiên bắt chước các đầu ra không tối ưu và dẫn đến kết quả tệ hơn. Điều này làm nổi bật tầm quan trọng của chất lượng chính sách ban đầu và gợi ý rằng các mô hình với hiệu suất ban đầu cao hơn có thể hưởng lợi nhiều hơn từ tối ưu hóa on-policy, trong khi những mô hình có chất lượng ban đầu thấp hơn có thể không đạt được nhiều lợi ích.

Dữ liệu không được ưa thích nên là on-policy, dữ liệu được ưa thích ít có lợi hơn. Trong khi WPO mô phỏng dữ liệu on-policy bằng cách cân trọng cả yw và yl trong dữ liệu ưu tiên, hai đầu ra này đóng vai trò khác nhau trong quá trình tối ưu hóa. Gradient của WPO được cho bởi:

∇LWPO =−βw(x, yw)w(x, yl)σ(ˆr(x, yl)−ˆr(x, yw))
[∇logπ(yw|x) - ∇logπ(yl|x)]
       ↑                    ↑
  tăng xác suất của yw   giảm xác suất của yl

[Hình 4: Kết quả của các biến thể của WPO trong các cài đặt RL khác nhau.]

Nghĩa là, WPO sẽ làm cho mô hình chính sách bắt chước yw trong khi tránh xa yl. Cho các hướng tối ưu hóa khác nhau của chúng, chúng tôi nghiên cứu tầm quan trọng của lấy mẫu on-policy cho yw và yl trong tối ưu hóa ưu tiên. Để đạt được điều này, chúng tôi tiếp tục nghiên cứu hai biến thể khác nhau của WPO, cụ thể là WPO W và WPO L. Những mất mát này được xây dựng như sau:

LWPO=−E(x,yw,yl)∼D[w(x, yw)w(x, yl) logp(yw≻yl|x)],
LWPOW=−E(x,yw,yl)∼D[w(x, yw) logp(yw≻yl|x)],
LWPOL=−E(x,yw,yl)∼D[w(x, yl) logp(yw≻yl|x)],

trong đó trong WPO W, chúng tôi chỉ tăng trọng số của các cặp nơi yw gần hơn với các đầu ra on-policy. Đối với WPO L, chúng tôi chỉ tăng trọng số của các cặp nơi yl gần hơn với các đầu ra on-policy. Kết quả trên Mistral-base và Llama-3-Instruct trong Hình 4. Nó cho thấy rằng WPO L nói chung đạt kết quả tương tự WPO. Ngược lại, WPO W nhất quán kém hiệu suất so với WPO và thậm chí kém hiệu suất so với DPO trong hầu hết các cài đặt. Do đó, làm cho yl on-policy giải thích hầu hết các cải thiện của

--- TRANG 9 ---
WPO, trong khi làm cho yw on-policy vẫn hữu ích nhưng không quan trọng bằng. Phát hiện này gợi ý rằng việc sử dụng dữ liệu không được ưa thích on-policy quan trọng cho tối ưu hóa ưu tiên, trong khi việc sử dụng dữ liệu được ưa thích on-policy có thể có lợi nhưng không quan trọng bằng.

5 Kết luận
Trong nghiên cứu này, chúng tôi giải quyết vấn đề khoảng cách phân phối vốn có trong tối ưu hóa ưu tiên off-policy. Bằng cách giới thiệu Tối ưu hóa Ưu tiên Có trọng số (WPO), chúng tôi thành công mô phỏng tối ưu hóa ưu tiên on-policy sử dụng dữ liệu ưu tiên off-policy, hợp nhất lợi ích của cả hai phương pháp. Phương pháp của chúng tôi không chỉ giải quyết khoảng cách phân phối mà không phát sinh chi phí bổ sung mà còn nâng cao hiệu quả của tối ưu hóa ưu tiên. Các thí nghiệm mở rộng chứng minh rằng WPO có thể tạo ra các LLM tốt hơn được căn chỉnh gần hơn với ưu tiên của con người.

Hạn chế
Khoảng cách hiệu suất giữa tối ưu hóa ưu tiên off và on-policy vẫn tồn tại. Mặc dù WPO mô phỏng RL on-policy với dữ liệu off-policy, nó không hoàn toàn thu hẹp khoảng cách hiệu suất giữa RL off-policy và on-policy. Như được thể hiện trong kết quả, ngay cả với WPO, các phương pháp off-policy vẫn có thể kém hiệu suất so với các phương pháp on-policy và lai. Do đó, trong khi chúng tôi đề xuất WPO như một giải pháp, nó không hoàn toàn loại bỏ sự khác biệt hiệu suất, và dữ liệu ưu tiên on-policy vẫn quan trọng. Công việc tương lai sẽ tập trung vào cách giảm thêm khoảng cách hiệu suất này mà không phát sinh chi phí huấn luyện bổ sung.

Tính toàn diện của tập dữ liệu ưu tiên. Mục tiêu của các thí nghiệm của chúng tôi là so sánh WPO với các thuật toán tối ưu hóa ưu tiên khác, không phải cung cấp một LLM được căn chỉnh toàn diện. Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng Ultrafeedback làm dữ liệu ưu tiên, chủ yếu tập trung vào tính hữu ích, tính trung thực và tuân theo hướng dẫn, và không bao gồm các khía cạnh an toàn. Ngoài ra, nó không xem xét tối ưu hóa ưu tiên cho các cuộc trò chuyện nhiều lượt. Công việc tương lai nên bao gồm việc thu thập các tập dữ liệu ưu tiên toàn diện hơn và tích hợp nhiều khía cạnh của tối ưu hóa ưu tiên để huấn luyện các LLM được căn chỉnh tốt hơn.

Tài liệu tham khảo
[Phần tài liệu tham khảo với tất cả các trích dẫn được liệt kê...]

--- TRANG 10 ---
[Tiếp tục phần tài liệu tham khảo...]

--- TRANG 11 ---
[Tiếp tục phần tài liệu tham khảo...]

--- TRANG 12 ---
[Bảng 4: Kết quả trên OpenLLM leaderboard.]

[Hình 5: Kết quả của DPO và WPO khi được huấn luyện cho nhiều epoch hơn.]

A Phân tích Bổ sung
Kết quả trên các tác vụ downstream. Chúng tôi tiếp tục đánh giá hiệu suất của các mô hình SFT, DPO, và WPO trên OpenLLM leaderboard (Beeching et al., 2023) để đánh giá khả năng của chúng trên các tác vụ downstream. Đối với đánh giá này, chúng tôi sử dụng lm-evaluation-harness⁵, cơ sở mã chính thức cho OpenLLM leaderboard. Kết quả được hiển thị trong Bảng 4. Nói chung, chúng tôi thấy rằng tối ưu hóa ưu tiên với DPO hoặc WPO vượt trội mô hình SFT, trong khi các mô hình dựa trên Llama-3-Instruct vượt trội Mistral-base. Tuy nhiên, chúng tôi không quan sát thấy mối tương quan giữa hiệu suất trên OpenLLM leaderboard và hiệu suất trên các benchmark tuân theo hướng dẫn như Alpaca Eval 2 và MT-bench. Ví dụ, mặc dù Llama-3-Instruct với DPO hoặc WPO trong cài đặt lai cho kết quả tốt nhất trên các benchmark tuân theo hướng dẫn, nó kém hiệu suất so với các đối tác off-policy trên OpenLLM leaderboard. Ngoài ra, chúng tôi thấy rằng tối ưu hóa ưu tiên có thể không cải thiện kết quả trên tất cả các tác vụ downstream. Trên MMLU, kết quả tương tự như SFT, và trên GSM8K, kết quả thậm chí thấp hơn SFT trong tất cả các cài đặt. Phát hiện của chúng tôi phù hợp với hiện tượng alignment tax (Askell et al., 2021), cho thấy rằng căn chỉnh tốt hơn có thể không cải thiện và đôi khi thậm chí có thể làm tổn hại hiệu suất trên các tác vụ downstream.

So sánh giữa DPO và WPO về động lực huấn luyện. Chúng tôi nghiên cứu cách hiệu suất của DPO và WPO thay đổi với số lượng epoch huấn luyện khác nhau. Cả DPO và WPO đều được huấn luyện sử dụng checkpoint SFT của Mistral-base và tập dữ liệu Ultrafeedback trong năm epoch, với kết quả đánh giá được ghi lại vào cuối mỗi epoch, như được hiển thị trong Hình 5. Trong nghiên cứu này, chúng tôi sử dụng cùng bộ siêu tham số như đã đề cập trong Phần 4.1, với DPO và WPO sử dụng cùng bộ siêu tham số. Chúng tôi quan sát thấy rằng hiệu suất của DPO giảm mạnh sau hai epoch, gợi ý tối ưu hóa quá mức mô hình phần thưởng mạnh (Rafailov et al., 2024). Ngược lại, WPO duy trì hiệu suất nhất quán qua nhiều epoch hơn, cho thấy sự ổn định huấn luyện tốt hơn. Điều này gợi ý rằng mô phỏng RL on-policy, như được thực hiện bởi WPO, có thể giảm thiểu các vấn đề liên quan đến tối ưu hóa quá mức mô hình phần thưởng và tăng sự ổn định của tối ưu hóa ưu tiên. Hơn nữa, so sánh kết quả giữa DPO và WPO, đặc biệt trên Alpaca Eval 2, cho thấy rằng hiệu suất đỉnh của DPO qua các epoch khác nhau vẫn thấp hơn WPO. Điều này cho thấy rằng WPO không chỉ cung cấp động lực huấn luyện ổn định hơn mà còn tìm ra một giải pháp khác và tốt hơn so với DPO. Hiệu suất và sự ổn định nâng cao này làm nổi bật lợi thế của WPO trong việc hiệu quả tận dụng dữ liệu ưu tiên và duy trì tối ưu hóa ưu tiên ổn định và mạnh mẽ trong suốt quá trình huấn luyện.

B Liên kết của Các Mô hình Mã nguồn Mở trong Thí nghiệm
Danh sách các LLM mã nguồn mở và ID Huggingface của chúng được liệt kê trong Bảng 5.

[Bảng 5: Danh sách các mô hình mã nguồn mở trong thí nghiệm.]

C Chi tiết Bổ sung
Các artifact khoa học. Chúng tôi sử dụng nhiều artifact khoa học khác nhau trong bài báo, bao gồm các mô hình LLM cơ sở, tập dữ liệu ưu tiên, và các công cụ/benchmark đánh giá. Tham chiếu đến tất cả các artifact được sử dụng được cung cấp, và các chi tiết như giấy phép, ngôn ngữ, phạm vi bao phủ, số lượng tham số, và bất kỳ vấn đề an toàn nào có thể được tìm thấy bằng cách theo các tham chiếu tương ứng. Lưu ý rằng các LLM và tập dữ liệu ưu tiên hiện tại có thể bao gồm một loạt các loại dữ liệu và sử dụng dữ liệu từ các miền và nguồn khác nhau, vì vậy chúng tôi không liệt kê chi tiết trong bài báo này và khuyến khích độc giả tham khảo các nguồn gốc để biết thêm thông tin. Trong bài báo này, chúng tôi chủ yếu sử dụng những artifact này cho mục đích không phân phối và không thương mại, phù hợp với giấy phép của chúng.

Ngân sách. Chúng tôi tiến hành tất cả các thí nghiệm sử dụng 8 ×H100 GPU. Các thí nghiệm mất khoảng 1.5 giờ cho Mistral-base và khoảng 4 giờ cho Llama-3-Instruct.

Sử dụng trợ lý AI. Chúng tôi sử dụng ChatGPT chỉ để chỉnh sửa ngôn ngữ của bài báo. Lưu ý rằng việc chỉnh sửa chỉ dành riêng cho việc nâng cao độ rõ ràng và dễ đọc của văn bản, và không cho bất kỳ mục đích nào khác.

--- TRANG 13 ---
có thể yêu cầu tìm kiếm siêu tham số mở rộng, có thể không mang lại các giá trị siêu tham số tối ưu cho so sánh công bằng.

⁵https://github.com/EleutherAI/lm-evaluation-harness
