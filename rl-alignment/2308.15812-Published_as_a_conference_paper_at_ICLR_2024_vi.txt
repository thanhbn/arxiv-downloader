# 2308.15812.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2308.15812.pdf
# Kích thước tệp: 824628 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
NHÌN XUN QUA CÁC SỞ THÍCH: KHÁM PHÁ
VIỆC THU THẬP PHẢN HỒI ĐỂ CĂN CHỈNH CÁC MÔ 
HÌNH NGÔN NGỮ LỚN
Hritik Bansal, John Dang, Aditya Grover
Khoa Khoa học Máy tính, Đại học California Los Angeles
{hbansal,john.dang,adityag }@cs.ucla.edu
TÓM TẮT
Việc căn chỉnh các mô hình ngôn ngữ lớn (LLM) với các giá trị và ý định của con người 
đòi hỏi việc sử dụng phản hồi từ con người hoặc AI một cách quan trọng. Trong khi các 
chú thích phản hồi dày đặc rất tốn kém để thu thập và tích hợp, phản hồi thưa thớt đưa 
ra một lựa chọn thiết kế cấu trúc giữa xếp hạng (ví dụ: chấm điểm Phản hồi A trên thang 
điểm 1-7) và thứ tự (ví dụ: Phản hồi A có tốt hơn Phản hồi B không?). Trong nghiên cứu 
này, chúng tôi phân tích tác động của lựa chọn thiết kế này đối với việc căn chỉnh và đánh 
giá các LLM. Chúng tôi phát hiện một vấn đề không nhất quán trong đó các sở thích được 
suy ra từ xếp hạng và thứ tự có sự khác biệt đáng kể 60% đối với cả người chú thích con 
người và AI. Phân tích tiếp theo của chúng tôi xác định các khía cạnh khác nhau của thành 
kiến người chú thích giải thích hiện tượng này, chẳng hạn như người chú thích con người 
sẽ đánh giá các phản hồi dày đặc hơn cao hơn trong khi ưa thích độ chính xác trong các 
phán đoán theo cặp, đối với một trường hợp so sánh cụ thể. Đáng ngạc nhiên, chúng tôi 
quan sát thấy rằng việc lựa chọn giao thức phản hồi có tác động đáng kể đến việc đánh giá 
các LLM đã được căn chỉnh. Cụ thể, chúng tôi thấy rằng các LLM tận dụng dữ liệu thứ tự 
để căn chỉnh (ví dụ mô hình X) được ưa thích hơn những mô hình tận dụng dữ liệu xếp 
hạng (ví dụ mô hình Y), với giao thức đánh giá dựa trên thứ tự (phản hồi của X/Y có tốt 
hơn phản hồi tham chiếu không?) nhưng không phải với giao thức đánh giá dựa trên xếp 
hạng (chấm điểm phản hồi của X/Y trên thang điểm 1-7). Do đó, các phát hiện của chúng 
tôi làm sáng tỏ những khoảng cách quan trọng trong các phương pháp đánh giá tính hữu 
ích thực tế của các mô hình ngôn ngữ và sự phụ thuộc mạnh mẽ của chúng vào giao thức 
phản hồi được sử dụng để căn chỉnh. Mã và dữ liệu của chúng tôi có sẵn tại
https://github.com/Hritikbansal/sparse_feedback .

1 GIỚI THIỆU
Gần đây, căn chỉnh đã nổi lên như một bước quan trọng cho các trợ lý văn bản thế hệ tiếp theo [35]. 
Cụ thể, mục tiêu của nó là căn chỉnh các mô hình ngôn ngữ lớn (LLM) với các giá trị và ý định của 
con người, làm cho nội dung được tạo ra của chúng chính xác, mạch lạc và vô hại khi phản hồi các 
truy vấn của con người [26, 2, 4, 38]. Quá trình căn chỉnh mô hình bao gồm ba thành phần chính: 
thu thập phản hồi, nơi con người (hoặc AI) đánh giá chất lượng của các phản hồi từ mô hình cơ sở; 
các thuật toán căn chỉnh, điều chỉnh các kỹ năng của mô hình cơ sở dựa trên dữ liệu phản hồi; và 
đánh giá mô hình, đánh giá hiệu suất của mô hình đã được căn chỉnh trên một loạt các hướng dẫn 
người dùng mới [18].

Các nghiên cứu trước đây chủ yếu tập trung vào việc thiết kế các thuật toán căn chỉnh, chẳng hạn 
như PPO, DPO và PRO [31, 23, 28, 33, 45, 19], dưới các giao thức phản hồi và thiết lập đánh giá 
cụ thể. Ngoài ra, nghiên cứu trước đây về thu thập phản hồi [6, 43, 30] đã tập trung vào việc phát 
triển các giao thức phản hồi chi tiết và dày đặc để căn chỉnh các LLM; tuy nhiên, các giao thức này 
thách thức và tốn kém để thu thập. Trong các giao thức phản hồi thưa thớt dễ thu thập, có một lựa 
chọn thiết kế cấu trúc giữa xếp hạng và thứ tự, và tác động của nó đến pipeline căn chỉnh vẫn chưa 
được khám phá đầy đủ.

Để giải quyết vấn đề này, chúng tôi phân tích tác động của hai giao thức phản hồi: xếp hạng và thứ 
tự đối với việc căn chỉnh LLM và đánh giá tiếp theo. Cụ thể, giao thức xếp hạng là một hình thức 
phản hồi tuyệt đối trong đó người chú thích gán một xếp hạng cho một phản hồi từ LLM cơ sở bằng 
cách sử dụng một thang điểm được xác định trước (ví dụ: thang Likert 1-7 [20]). Ngược lại, giao 
thức thứ tự là một hình thức phản hồi tương đối trong đó người chú thích chọn phản hồi ưa thích 
của họ từ một cặp phản hồi ứng viên được tạo ra bởi LLM cơ sở. Các xếp hạng trên các phản hồi 
của mô hình định lượng mức độ tốt của chúng, cho phép các nhà xây dựng mô hình đánh giá điểm 
mạnh và điểm yếu của họ, nhưng khó xác định đối với các hướng dẫn phức tạp (bài thơ về 'Hoa 
hồng và Hồ trong phong cách Shakespeare') [19, 32]. Mặt khác, giao thức thứ tự dễ thu thập đối 
với các hướng dẫn phức tạp nhưng không định lượng khoảng cách giữa cặp phản hồi [35, 26]. Do 
những hành vi độc đáo này, cả hai giao thức phản hồi xếp hạng và thứ tự đều là đối tượng hấp dẫn 
cho nghiên cứu của chúng tôi.

Trong nghiên cứu của chúng tôi, trước tiên chúng tôi khám phá sự tương tác giữa các giao thức 
phản hồi này. Để làm như vậy, chúng tôi thu thập xếp hạng cho các phản hồi riêng lẻ từ các người 
chú thích, cả con người và AI, và chuyển đổi chúng thành dạng thứ tự bằng cách so sánh các xếp 
hạng được gán (ví dụ: nếu Phản hồi A được xếp hạng 5 và Phản hồi B được xếp hạng 4, điều này 
ngụ ý rằng Phản hồi A được xếp hạng cao hơn Phản hồi B). Ngoài ra, chúng tôi thu thập các sở 
thích thứ tự theo cặp cho cùng các phản hồi (ví dụ: so sánh Phản hồi A với Phản hồi B) từ các 
người chú thích, độc lập với các xếp hạng của họ. Đáng ngạc nhiên, chúng tôi phát hiện một vấn 
đề không nhất quán phản hồi và thấy rằng một phần đáng kể của tất cả các so sánh sở thích giữa 
xếp hạng và thứ tự không đồng ý với nhau đối với con người 58% và người chú thích AI (GPT-3.5-
Turbo) 59.4% (§4). Chúng tôi quan sát thấy rằng các phản hồi nhận được phản hồi không nhất quán 
thực sự được cảm nhận là gần nhau hơn so với những phản hồi nhận được phản hồi nhất quán. Về 
mặt định tính, chúng tôi truy nguyên vấn đề này về sự khác biệt trong cách các người chú thích 
đánh giá các khía cạnh khác nhau của chất lượng phản hồi (§J.1). Phân tích của chúng tôi cung cấp 
những hiểu biết cho việc đưa ra quyết định liên quan đến các giao thức thu thập phản hồi để căn 
chỉnh các LLM.

Sau khi nhận được phản hồi, mục tiêu của chúng tôi là điều tra tác động của các giao thức đối với 
việc huấn luyện các mô hình phần thưởng để căn chỉnh mô hình và đánh giá mô hình tiếp theo. Để 
đạt được điều này, chúng tôi huấn luyện các mô hình phần thưởng trên dữ liệu xếp hạng và thứ tự 
(§2.3). Chúng tôi sử dụng chính sách Best-of-n trong đó các mô hình phần thưởng được huấn luyện 
sau đó được sử dụng để chọn phản hồi tốt nhất từ một tập hợp n phản hồi ứng viên được tạo ra 
bởi LLM cơ sở. Tiếp theo, chúng tôi đánh giá chất lượng của các chính sách Best-of-n (cả xếp hạng 
và thứ tự) bằng cách đánh giá các phản hồi của chúng so với một mô hình tham chiếu [26]. Đáng 
ngạc nhiên, chúng tôi quan sát thấy một sự không nhất quán đánh giá, một hiện tượng trong đó 
việc lựa chọn giao thức phản hồi (thứ tự) để đánh giá mô hình ủng hộ các phản hồi từ chính sách 
Best-of-n sử dụng cùng giao thức phản hồi (thứ tự), và ngược lại. Ngoài ra, chúng tôi thấy rằng sự 
không nhất quán phản hồi xảy ra đối với cả con người và AI như những người đánh giá phản hồi. 
Cụ thể, chúng tôi quan sát thấy rằng khoảng cách tỷ lệ thắng giữa chính sách Best-of-n (thứ tự) và 
LLM cơ sở lớn hơn (11.2%) so với khoảng cách giữa chính sách Best-of-n (xếp hạng) và LLM cơ sở 
(5.3%) khi sử dụng thứ tự con người để đánh giá (xem §5.2). Tuy nhiên, khoảng cách tỷ lệ thắng 
giữa chính sách Best-of-n (xếp hạng) và LLM cơ sở (5%) chỉ lớn hơn một chút so với khoảng cách 
giữa chính sách Best-of-n (thứ tự) và LLM cơ sở (4%) khi sử dụng xếp hạng con người để đánh 
giá. Hình 1 cung cấp minh họa cho pipeline của chúng tôi.

Các đóng góp của chúng tôi như sau:
1. Chúng tôi phát hiện một vấn đề không nhất quán phản hồi trong đó các xếp hạng trên cặp phản 
hồi không đồng ý với thứ tự giữa chúng cho 60% các so sánh đối với cả con người và AI.
2. Chúng tôi tiếp tục phân tích các khía cạnh khác nhau của chất lượng phản hồi được cảm nhận 
bởi các người chú thích trong khi cung cấp các dạng phản hồi khác nhau.
3. Cuối cùng, chúng tôi thấy rằng việc lựa chọn giao thức phản hồi có ảnh hưởng mạnh mẽ đến 
việc đánh giá các LLM đã được căn chỉnh dưới dạng sự không nhất quán đánh giá.

2 KIẾN THỨC NỀN TẢNG
Trong nghiên cứu này, chúng tôi tập trung vào việc căn chỉnh một LLM để tạo ra các đầu ra chất 
lượng cao, được coi là chính xác, mạch lạc và vô hại bởi con người, cho các hướng dẫn chưa thấy. 
Bước đầu tiên trong quá trình căn chỉnh là trang bị cho LLM cơ sở khả năng hiểu các truy vấn hoặc 
hướng dẫn đầu vào do con người hoặc máy tạo ra. Điều này được thực hiện thông qua tinh chỉnh 
có giám sát (SFT), trong đó LLM cơ sở được tinh chỉnh với các cặp hướng dẫn và phản hồi tương 
ứng do con người viết hoặc máy tạo ra. Đáng chú ý, đã có những tiến bộ đáng kể trong việc xây 
dựng các bộ dữ liệu và mô hình tinh chỉnh hướng dẫn gần đây [36, 49, 42, 40, 27, 44, 13, 46, 10, 
41]. Sau SFT, mô hình tạo ra các phản hồi ứng viên cho các hướng dẫn mới (§2.1), và dữ liệu phản 
hồi được thu thập dưới một giao thức (xếp hạng hoặc thứ tự) từ các người chú thích (§2.2). Hơn 
nữa, chúng tôi sử dụng một thuật toán căn chỉnh (Best-of-n) huấn luyện các mô hình phần thưởng 
trên dữ liệu phản hồi xếp hạng hoặc thứ tự (§2.3).

2.1 THU THẬP DỮ LIỆU HƯỚNG DẪN-PHẢN HỒI
Xem xét một mô hình ngôn ngữ pθ có thể hiểu các hướng dẫn và phản hồi chúng. Chúng tôi xem 
xét một tập hợp các hướng dẫn X={x1, . . . , xn} trong đó n là số lượng hướng dẫn. Tiếp theo, chúng 
tôi tạo ra một tập hợp k phản hồi ứng viên {y(1)i, . . . y(k)i} cho mỗi hướng dẫn xi bằng cách lấy 
mẫu từ phân phối của mô hình y(j)i ∼ pθ(.|xi). Tiếp theo, chúng tôi thu thập dữ liệu phản hồi trên 
các hướng dẫn được xác định trước và các phản hồi ứng viên được tạo ra D={(xi,{y(1)i, . . . , y(k)i})}.

2.2 DỮ LIỆU PHẢN HỒI
Giao thức Phản hồi Xếp hạng Với dữ liệu hướng dẫn và phản hồi ứng viên đã cho D, mục tiêu của 
thu thập phản hồi xếp hạng là gán một giá trị thứ tự cho mỗi phản hồi riêng lẻ một cách độc lập. 
Trong nghiên cứu của chúng tôi, chúng tôi yêu cầu các người chú thích đánh giá các phản hồi trên 
thang Likert (1 - 7). Chúng tôi hướng dẫn các người chú thích đánh giá các phản hồi dựa trên chất 
lượng của chúng có xem xét các yếu tố như độ chính xác, tính mạch lạc và tính vô hại của phản 
hồi ngoài phán đoán chủ quan của họ. Chính thức, các người chú thích gán một điểm tuyệt đối 
a(xi, y(j)i) ∈ {1,2, . . . , 7} trong đó xi và y(j)i lần lượt là hướng dẫn và một phản hồi ứng viên được 
tạo ra. Quá trình chú thích do đó sẽ tạo ra dữ liệu phản hồi DA={(xi, y(j)i, a(xi, y(j)i))} trong đó A 
đại diện cho giao thức xếp hạng.

Giao thức Phản hồi Thứ tự Với dữ liệu hướng dẫn và phản hồi ứng viên đã cho D, mục tiêu của 
giao thức thứ tự là gán một phản hồi được ưa thích (hoặc chọn 'bằng nhau') cho mỗi cặp phản hồi 
ứng viên cho một hướng dẫn đã cho. Để làm điều đó, trước tiên chúng tôi chuyển đổi dữ liệu hướng 
dẫn-phản hồi thành dữ liệu hướng dẫn-phản hồi theo cặp bằng cách tạo ra các kết hợp theo cặp 
của các phản hồi tức là D′ = {(xi, y(j)i, y(ℓ)i)} trong đó i ̸= ℓ. Giống hệt với phản hồi tuyệt đối, các 
người chú thích được hướng dẫn sử dụng các trục chất lượng khác nhau và phán đoán chủ quan 
của họ để đưa ra quyết định.

Chính thức, các người chú thích gán một phản hồi tương đối r(xi, y(j)i, y(ℓ)i) ∈ {'phản hồi 1', 
'phản hồi 2', 'bằng nhau'} trong đó 'phản hồi 1' chỉ ra rằng yi,j được ưa thích hơn y(ℓ)i. Quá trình 
chú thích do đó sẽ tạo ra dữ liệu phản hồi DR={(xi, y(j)i, y(ℓ)i, r(xi, y(j)i, y(ℓ)i))} trong đó chỉ số 
R đại diện cho giao thức phản hồi thứ tự.

2.3 MÔ HÌNH HÓA PHẦN THƯỞNG
Chúng tôi mô tả các mục tiêu huấn luyện cho các mô hình phần thưởng được huấn luyện trên phản 
hồi xếp hạng và thứ tự.

Mô hình Phần thưởng Hồi quy. Ở đây, trước tiên chúng tôi chuẩn hóa các xếp hạng (giữa 1-7) 
a(xi, y(j)i) cho một hướng dẫn đã cho trong bộ dữ liệu DA thành điểm a′(xi, y(j)i) giữa 0-1. Tiếp 
theo, chúng tôi huấn luyện một mô hình hồi quy fθ(xi, y(j)i) trong đó (xi, y(j)i) ∈ DA đầu ra một 
vô hướng. Mô hình phần thưởng hồi quy được huấn luyện với mục tiêu sau:

LA(DA) = E(xi,y(j)i)∼DA[(σ(fθ(xi, y(j)i)) − a′(xi, y(j)i)]2    (1)

trong đó σ(.) là hàm sigmoid chiếu bất kỳ giá trị vô hướng nào vào phạm vi giữa 0-1.

Mô hình Phần thưởng Log-Likelihood Âm (NLL). Để huấn luyện một mô hình phần thưởng từ dữ 
liệu phản hồi theo cặp DR, các nghiên cứu trước [26] huấn luyện một mô hình phần thưởng gθ(x, y) 
đầu ra một giá trị vô hướng cho một phản hồi y đã cho cho hướng dẫn x. Đầu tiên, chúng tôi lọc 
các trường hợp nhận được r(xi, y(j)i, y(ℓ)i) = 'bằng nhau' từ DR để có một tập con dữ liệu mới SR 
vì người ta không thể học từ những ví dụ như vậy trong mục tiêu mô hình hóa phần thưởng tương 
đối. Đối với các trường hợp còn lại, mục tiêu log-likelihood âm có thể được áp dụng như sau:

LR(SR) = −E(x,ya,yb)∼SR[log σ(gθ(xi, ya) − gθ(xi, yb)]    (2)

trong đó ya và yb lần lượt là các phản hồi được ưa thích và không được ưa thích từ cặp phản hồi 
ứng viên (y(j)i, y(ℓ)i) như được quyết định bởi giá trị của r(xi, y(j)i, y(ℓ)i) trong bộ dữ liệu SR. 
Chúng tôi cung cấp thêm chi tiết về việc lựa chọn fθ, gθ và thiết lập huấn luyện trong Phụ lục §G.

Chính sách Best-of-n. Trong nghiên cứu của chúng tôi, chúng tôi sử dụng chính sách Best-of-n 
(lấy mẫu từ chối) Pn tận dụng mô hình phần thưởng được huấn luyện để tăng cường hiệu suất của 
mô hình SFT hướng tới các sở thích của con người. Trong phương pháp này, chúng tôi đơn giản lấy 
mẫu n lần từ mô hình SFT từ một hướng dẫn đã cho và sử dụng các mô hình phần thưởng (hồi quy 
hoặc NLL) để chấm điểm tập hợp n phản hồi. Đầu ra cuối cùng của chính sách là phản hồi đạt được 
điểm cao nhất dưới mô hình phần thưởng được huấn luyện. Chính thức chính sách Best-of-n tận 
dụng mô hình phần thưởng xếp hạng fθ,

Pn(fθ, pθ, xi, {y(1)i, . . . , y(n)i}) = y(m)i    (3)

trong đó m = argmaxj fθ(xi, y(j)i), xi là các hướng dẫn đầu vào, và y(j)i là phản hồi thứ j từ n 
phản hồi ứng viên được lấy mẫu từ mô hình SFT pθ. Ở đây, cách tiếp cận của chúng tôi có thể sử 
dụng bất kỳ thuật toán RL nào khác như PPO được sử dụng để căn chỉnh mô hình [26] hoặc một 
hỗn hợp của lấy mẫu từ chối và PPO [38]. Chúng tôi tập trung vào lấy mẫu từ chối do tính đơn giản 
và hiệu suất mạnh mẽ của nó so với các thuật toán RL khác như được ghi nhận bởi các nghiên cứu 
trước [23, 48, 11].

3 THU THẬP DỮ LIỆU PHẢN HỒI
Ở đây, chúng tôi mô tả quá trình thu thập dữ liệu phản hồi. Chúng tôi bắt đầu bằng việc thu thập 
các phản hồi cho một tập hợp lớn các hướng dẫn từ LLM cơ sở. Tiếp theo, chúng tôi mô tả việc thu 
thập phản hồi từ AI có quy mô lớn và do đó được sử dụng để huấn luyện các mô hình phần thưởng. 
Hơn nữa, chúng tôi mô tả việc thu thập phản hồi từ con người tiếp theo là phân tích dữ liệu phản hồi.

Dữ liệu Phản hồi Hướng dẫn. Các hướng dẫn được thiết kế để trình bày một bộ sưu tập các truy 
vấn có thể được đặt ra cho một trợ lý AI dựa trên văn bản trong các tình huống thực tế. Chúng tôi 
thu thập 5.2K hướng dẫn từ các nguồn đa dạng như Dolly [10], User-orient [42] và SuperNI [42].

Trong nghiên cứu của chúng tôi, chúng tôi sử dụng Alpaca-7B [36], được xây dựng bằng cách tinh 
chỉnh hướng dẫn LLaMA-7B [37] trên 52K dữ liệu hướng dẫn-phản hồi, làm LLM cơ sở. Do đó, 
chúng tôi nhắc Alpaca tạo ra năm phản hồi ứng viên cho một loạt các hướng dẫn đa dạng. Chúng 
tôi đặt độ dài tối đa cho mỗi phản hồi được tạo ra là 128. Chúng tôi cung cấp thêm chi tiết về thành 
phần bộ dữ liệu trong Phụ lục §C.

Phản hồi từ AI. Các nghiên cứu trước [12] chứng minh rằng các hệ thống AI hiện có (LLM) như 
GPT-4 [25] có thể được tận dụng để cung cấp dữ liệu phản hồi theo cặp và cải thiện so với LLM cơ 
sở. Ngoài ra, [49, 14] đưa ra lập luận cho LLM như một sự thay thế cho các sở thích của con người 
theo cách có thể mở rộng và đáng tin cậy. Trong nghiên cứu của chúng tôi, chúng tôi thu thập dữ 
liệu phản hồi xếp hạng và thứ tự quy mô lớn bao gồm 71K trường hợp từ GPT-3.5-Turbo (ChatGPT). 
Chúng tôi chọn ChatGPT vì nó rẻ hơn GPT-4 20 lần và dễ tiếp cận hơn vào thời điểm thực hiện dự án.

Để thu thập phản hồi xếp hạng, chúng tôi nhắc GPT-3.5-Turbo gán một điểm giữa 1-7 cho các phản 
hồi ứng viên riêng lẻ cho các hướng dẫn độc lập với nhau. Mô hình được nhắc cụ thể để gán điểm 
xếp hạng sau khi đánh giá phản hồi về độ chính xác, tính mạch lạc và tính vô hại của nó, ngoài 
phán đoán chủ quan của nó. Chúng tôi làm rõ thêm rằng xếp hạng 1 ngụ ý một phản hồi chất lượng 
thấp trong khi xếp hạng 7 chỉ ra một phản hồi chất lượng cao. Tổng cộng, chúng tôi thu thập 24.6K 
trường hợp phản hồi xếp hạng và chi 150 đô la cho việc thu thập.

Để thu thập phản hồi thứ tự, chúng tôi nhắc LLM GPT-3.5-Turbo với hướng dẫn và một cặp phản 
hồi ứng viên ('phản hồi 1', 'phản hồi 2'), và ra lệnh cho nó cung cấp sở thích của nó giữa hai phản 
hồi. Để giảm thiểu bất kỳ thành kiến tiềm ẩn nào trong thứ tự 'phản hồi 1' và 'phản hồi 2' [49], 
chúng tôi chạy hai truy vấn cho tất cả các trường hợp bao gồm cả hai thứ tự có thể. Nếu sở thích 
của mô hình đảo ngược bằng cách đảo ngược thứ tự của các phản hồi, thì chúng tôi coi đó là tình 
huống hòa và gán phản hồi 'bằng nhau' cho cặp phản hồi, như đã làm trong [5]. Tổng cộng, chúng 
tôi thu thập 46.5K trường hợp duy nhất của phản hồi thứ tự, loại trừ hai thứ tự có thể của cặp phản 
hồi ứng viên, và chi 600 đô la cho việc thu thập. Chúng tôi sẽ cung cấp thêm phân tích về dữ liệu 
phản hồi trong §3.1.

Phản hồi từ Con người. Chúng tôi thu thập dữ liệu phản hồi dưới các giao thức xếp hạng và thứ tự 
từ con người 6K trường hợp chú thích. Chúng tôi cung cấp chi tiết về việc thu thập sở thích con 
người trong Phụ lục §D. Dữ liệu như vậy phục vụ nhiều mục đích, đáng chú ý: (a) cung cấp hiểu 
biết về hành vi của các dạng phản hồi khác nhau được thu thập từ con người, (b) tạo điều kiện cho 
phân tích so sánh về những điểm tương đồng và khác biệt giữa các mẫu phản hồi có nguồn gốc từ 
hệ thống AI và những mẫu thu được từ những người tham gia con người, và (c) cho phép đánh giá 
toàn diện về sự đồng thuận giữa dữ liệu phản hồi được tạo ra bởi hệ thống AI và phản hồi được 
cung cấp bởi những người tham gia con người. Trong nghiên cứu của chúng tôi, chúng tôi nhấn 
mạnh rằng dữ liệu phản hồi con người không được sử dụng để huấn luyện các mô hình phần thưởng, 
và do đó căn chỉnh mô hình, do quy mô nhỏ của nó mà việc mở rộng rất tốn kém.

3.1 PHÂN TÍCH DỮ LIỆU
Phân phối Xếp hạng. Chúng tôi trình bày phân phối điểm cho 4K dữ liệu xếp hạng từ các chú thích 
con người và các chú thích GPT-3.5-Turbo tương ứng của chúng trong Hình 2. Chúng tôi thấy rằng 
phần lớn các phản hồi (~80%) đạt được xếp hạng tốt hơn trung bình (>4) từ con người và AI. Điều 
này chỉ ra rằng chất lượng của các phản hồi từ Alpaca-7B là tốt, được quy cho khả năng tuân theo 
hướng dẫn của nó. Ngoài ra, chúng tôi thấy rằng các người chú thích con người có xu hướng đưa 
ra điểm hoàn hảo 7 cho một tỷ lệ lớn các phản hồi, trong khi GPT-3.5-Turbo gán điểm hoàn hảo 
cho ít hơn <5% trường hợp. Chúng tôi quan sát thấy rằng phần lớn các phản hồi đạt được điểm xếp 
hạng 5 hoặc 6 từ GPT-3.5, và phân phối trở nên phẳng về hai bên của những điểm này (giống như 
Hình 2 trong [27]).

Dữ liệu phản hồi không thiên vị đối với các phản hồi dài hơn và độc đáo. Các nghiên cứu trước 
[49, 41] đã chỉ ra rằng các LLM có thể ủng hộ các phản hồi dài hơn và nhiều lời, ở mức độ khác 
nhau, mặc dù không chính xác và chất lượng thấp hơn. Chúng tôi đánh giá xem độ dài hoặc số từ 
độc đáo trong các phản hồi ứng viên có thiên vị các phán đoán phản hồi xếp hạng và thứ tự từ con 
người và AI trong dữ liệu của chúng tôi hay không (Bảng Phụ lục 5 và 6). Tóm lại, chúng tôi thấy 
rằng không có sự khác biệt rõ ràng giữa độ dài trung bình và số từ độc đáo trung bình của phản 
hồi được ưa thích và không được ưa thích trong phản hồi thứ tự được thu thập từ con người và AI.

Phân tích Đồng thuận. Ở đây, chúng tôi tiến hành phân tích đồng thuận so sánh các đánh giá được 
cung cấp bởi các người chú thích con người-con người và con người-AI cho cả phản hồi xếp hạng 
và thứ tự. Cụ thể, chúng tôi đã thu thập 1000 trường hợp với phản hồi xếp hạng và 500 trường hợp 
với phản hồi thứ tự từ bốn người chú thích con người và GPT-3.5-Turbo. Để thiết lập sự thật cơ 
bản cho mỗi trường hợp, chúng tôi coi ba chú thích con người đầu tiên là nhãn vàng, và chúng tôi 
sử dụng chú thích thứ tư làm nhãn dự đoán con người. Đối với các trường hợp với phản hồi xếp 
hạng, chúng tôi tính trung bình của các điểm được đưa ra bởi ba người chú thích con người và làm 
tròn đến số nguyên gần nhất và coi nó là nhãn vàng. Đối với trường hợp với phản hồi thứ tự, chúng 
tôi xác định nhãn cuối cùng dựa trên phiếu bầu đa số trong ba chú thích con người. Các lựa chọn 
có thể cho phản hồi phản hồi con người là {'phản hồi 1', 'phản hồi 2', 'bằng nhau'}. Nếu một trường 
hợp nhận được hai phiếu cho 'bằng nhau' và một phiếu cho 'phản hồi 1', chúng tôi gán cho nó nhãn 
vàng 'bằng nhau'. Trong trường hợp hòa giữa ba lựa chọn, chúng tôi lấy mẫu ngẫu nhiên nhãn vàng 
từ các lựa chọn có thể.

Chúng tôi tính toán sự khác biệt xếp hạng trung bình giữa phản hồi nhãn vàng và phản hồi con 
người (và AI) để định lượng sự đồng thuận xếp hạng. Chúng tôi tính toán tỷ lệ phần trăm các trường 
hợp (trong số 500) mà nhãn vàng và phản hồi con người (và AI) đồng ý với nhau để định lượng sự 
đồng thuận thứ tự. Cụ thể, chúng tôi gán điểm 1 nếu nhãn vàng khớp với dự đoán, và điểm 0.5 nếu 
chỉ một trong nhãn vàng hoặc dự đoán gán 'bằng nhau' cho cặp phản hồi. Trong Bảng 1, chúng tôi 
quan sát thấy rằng sự khác biệt xếp hạng trung bình giữa xếp hạng con người-con người (1.08) gần 
với xếp hạng con người-GPT-3.5-Turbo (0.9). Ngoài ra, chúng tôi quan sát thấy rằng sự đồng thuận 
con người-con người và sự đồng thuận con người-GPT-3.5-Turbo lần lượt là 62.7% và 60.5% trong 
phản hồi thứ tự. Chúng tôi quan sát thấy rằng tỷ lệ đồng thuận của chúng tôi gần với tỷ lệ đồng 
thuận con người-con người 60% được báo cáo trong các nghiên cứu trước [18].

4 VẤN ĐỀ KHÔNG NHẤT QUÁN PHẢN HỒI

4.1 TÍNH TOÁN TÍNH NHẤT QUÁN
Ở đây, chúng tôi sử dụng quan sát rằng thứ tự của một cặp phản hồi cho một hướng dẫn đã cho có 
thể được so sánh với dữ liệu phản hồi xếp hạng được chuyển đổi thành dạng thứ tự của nó. Chính 
thức, chúng tôi có thể chuyển đổi dữ liệu xếp hạng DA thành DR A = {xi, y(j)i, y(ℓ)i, h(a(xi, y(j)i), 
a(xi, y(ℓ)i))} trong đó h(a(xi, y(j)i), a(xi, y(ℓ)i)) ∈ {'phản hồi 1', 'phản hồi 2', 'bằng nhau'} biểu thị 
phản hồi được ưa thích giữa y(j)i (phản hồi 1) và y(ℓ)i (phản hồi 2). Ở đây, chúng tôi định nghĩa 
thuật ngữ tính nhất quán phản hồi là sự đồng thuận giữa xếp hạng (được chuyển đổi thành dạng 
thứ tự của nó) và thứ tự nhận được bởi một cặp phản hồi cho một hướng dẫn đã cho. Chính thức,

C(xi, y(j)i, y(ℓ)i) = {
1 nếu r(xi, y(j)i, y(ℓ)i) = h(xi, y(j)i, y(ℓ)i)
0 ngược lại
}    (4)

trong đó (xi, y(j)i, y(ℓ)i) ∈ S và S = DR A ∩ DR.

4.2 KẾT QUẢ
Chúng tôi trình bày kết quả trên 42K so sánh trong dữ liệu phản hồi từ GPT-3.5-Turbo (Bảng 2a) 
và 500 so sánh trong dữ liệu phản hồi từ con người (Bảng 2b). Trong cả hai bảng, một ô cụ thể (ví 
dụ hàng 1 và cột 2) sẽ chỉ ra tỷ lệ phần trăm của tổng số trường hợp mà phản hồi xếp hạng coi cặp 
phản hồi ứng viên là 'bằng nhau' trong khi phản hồi thứ tự ưa thích 'Phản hồi 1' cho cùng cặp 
phản hồi ứng viên.

Chúng tôi quan sát một vấn đề không nhất quán trong cả dữ liệu phản hồi con người và AI. Đó là, 
điểm tính nhất quán nằm trong phạm vi tương tự 40%-42% đối với cả con người và AI, cho thấy rằng 
một phần đáng kể (~60%) của dữ liệu phản hồi có thể mang lại các sở thích mâu thuẫn tùy thuộc 
vào giao thức thu thập phản hồi được sử dụng. Vấn đề tính nhất quán này nhấn mạnh một số điểm 
quan trọng: (a) nó chỉ ra sự biến thiên trong chất lượng phản hồi được cảm nhận dựa trên việc lựa 
chọn các giao thức thu thập phản hồi, (b) nó nhấn mạnh rằng pipeline căn chỉnh có thể thay đổi 
đáng kể tùy thuộc vào việc xếp hạng hay thứ tự được sử dụng như các dạng phản hồi thưa thớt, và 
(c) nó nhấn mạnh sự cần thiết của việc lựa chọn dữ liệu tỉ mỉ khi làm việc với nhiều giao thức phản 
hồi để căn chỉnh các LLM. Trong Phụ lục §E, chúng tôi thảo luận về sự biến thiên trong điểm tính 
nhất quán với sự biến thiên trong các người chú thích.

Né tránh. Chúng tôi thấy rằng GPT-3.5-Turbo có xu hướng né tránh dự đoán của mình nhiều hơn 
đáng kể so với con người. Cụ thể, 47.1% phản hồi thứ tự và 57.4% phản hồi xếp hạng được gán 
sở thích 'bằng nhau' bởi GPT-3.5-Turbo (tổng của hàng và cột đầu tiên trong Bảng 2a). Tuy nhiên, 
30.9% phản hồi thứ tự và 40.8% phản hồi xếp hạng được gán sở thích 'bằng nhau' bởi con người. 
Chúng tôi quan sát thấy rằng tỷ lệ phần trăm né tránh cao hơn đối với phản hồi xếp hạng so với 
phản hồi thứ tự đối với cả con người và AI. Điều này có thể được quy cho một phần lớn các phản 
hồi riêng lẻ nhận được điểm 5 hoặc 6 từ GPT-3.5-Turbo, như đã thảo luận trong §3.1.

Phản hồi thứ tự quyết định. Chúng tôi quan sát thấy rằng bất cứ khi nào phản hồi thứ tự từ con 
người và GPT-3.5-Turbo là quyết định (tức là phản hồi ưa thích một trong các phản hồi từ cặp thay 
vì chọn 'bằng nhau'), phần các phân công nhất quán cao hơn những phân công không nhất quán. 
Trong Bảng 2a, 7.4% (Cột 2 và Hàng 2) cao hơn 4.4% và 6.9% cao hơn 4.5%. Tương tự, trong 
Bảng 2b, 13.1% (Cột 2 và Hàng 2) cao hơn 6.4% và 14.2% cao hơn 7.8%.

Phản hồi xếp hạng quyết định. Chúng tôi quan sát thấy rằng bất cứ khi nào phản hồi xếp hạng 
được chuyển đổi thành dạng thứ tự của nó là quyết định, phần các phân công nhất quán cao hơn 
những phân công không nhất quán. Cụ thể, chúng tôi tập trung vào Hàng 2 và Hàng 3 của các bảng. 
Trong Bảng 2a, 7.4% (Cột 2 và Hàng 2) cao hơn 4.5% và 6.9% cao hơn 4.4%. Tương tự, trong 
Bảng 2b, 13.1% (Cột 2 và Hàng 2) cao hơn 7.8% và 14.2% cao hơn 6.4%.

Phân tích Chi tiết Định lượng. Chúng tôi thực hiện các thí nghiệm chi tiết để định lượng sự khác 
biệt giữa các tập con nhất quán và không nhất quán của dữ liệu phản hồi từ con người và AI trong 
Phụ lục J.1. Tóm lại, chúng tôi thấy rằng khoảng cách định lượng giữa cặp phản hồi thuộc về các 
trường hợp nhất quán cao hơn cặp phản hồi thuộc về các trường hợp không nhất quán. Do sự gần 
gũi trong chất lượng được cảm nhận của cặp phản hồi thuộc về các trường hợp không nhất quán, 
chúng tôi cho rằng các người chú thích con người ưa thích các yếu tố chất lượng cụ thể hơn những 
yếu tố khác (ví dụ: tập trung vào độ chính xác hơn tính mạch lạc) trong việc đưa ra quyết định 
quyết định. Kết quả là, chúng tôi quan sát thấy rằng các trường hợp không nhất quán nhận được 
nhiều biến thiên hơn trong phản hồi thứ tự từ con người (ví dụ: người chú thích 1,2 ưa thích Phản 
hồi A hơn B trong khi người chú thích 3 ưa thích Phản hồi B hơn A cho một hướng dẫn đã cho) 
hơn các trường hợp nhất quán (ví dụ: người chú thích 1,2,3 ưa thích Phản hồi A và Phản hồi B).

Phân tích Định tính. Chúng tôi điều tra nguồn gốc của vấn đề không nhất quán một cách định tính 
bằng cách yêu cầu các người chú thích con người cho một số trường hợp phản hồi. Chúng tôi bao 
gồm các xếp hạng và thứ tự với các giải thích của con người trong Phụ lục P, và thấy rằng sự khác 
biệt trong các sở thích của con người trong khi chú thích cho các giao thức phản hồi khác nhau đóng 
một vai trò quan trọng trong việc đưa ra quyết định của họ. Ví dụ, trong Bảng 12, chúng tôi quan 
sát thấy rằng các người chú thích chịu trách nhiệm cho thứ tự ủng hộ 'phản hồi 2' do tính dày đặc 
của nó, trong khi cảm nhận 'phản hồi 1' là buồn tẻ. Đồng thời, mặc dù những sở thích khác biệt 
này, các phản hồi riêng lẻ nhận được điểm xếp hạng mâu thuẫn: 'phản hồi 2' chịu một hình phạt 
đáng kể do tính dày đặc và việc không tuân thủ được cảm nhận với các hướng dẫn nhiệm vụ, trong 
khi 'phản hồi 1' nhận được điểm trung bình chỉ vì tuân thủ một phần các hướng dẫn.

5 CĂN CHỈNH VÀ ĐÁNH GIÁ
Trong các phần trước, chúng tôi đã thiết lập rằng dữ liệu phản hồi thu được từ cả con người và AI 
đều gặp phải vấn đề không nhất quán. Tuy nhiên, việc xem xét tác động của các dạng phản hồi 
này đối với việc căn chỉnh và đánh giá các mô hình ngôn ngữ là rất quan trọng.

5.1 THIẾT LẬP
Đầu tiên, chúng tôi huấn luyện các mô hình phần thưởng trên dữ liệu phản hồi được thu thập dưới 
giao thức xếp hạng và thứ tự. Sau đó, chúng tôi sử dụng chính sách Best-of-n (n = 64) để đánh giá 
tác động của các dữ liệu phản hồi khác nhau đối với việc căn chỉnh mô hình. Ở đây, chúng tôi thay 
đổi việc lựa chọn các giao thức phản hồi là xếp hạng và thứ tự trong quá trình đánh giá và việc lựa 
chọn các người chú thích là con người và ChatGPT-3.5-Turbo. Cụ thể, chúng tôi chọn thích ứng 
hạng thấp [15] (LoRA) của Alpaca-7B làm mô hình phần thưởng¹. Sau huấn luyện, chúng tôi đánh 
giá các mô hình trên 553 hướng dẫn chưa thấy, bao gồm 188 hướng dẫn từ Open Assistant (OASST), 
129 từ đánh giá hữu ích được phát hành bởi Anthropic [3], 80 từ Vicuna [8] và 156 từ Koala [13]. 
Trong Best-of-n, trước tiên chúng tôi tạo ra n phản hồi ứng viên có độ dài 200 từ LLM cơ sở (Alpaca-
7B) với các hướng dẫn chưa thấy. Tiếp theo, đầu ra của Best-of-n sẽ là cái đạt được điểm tối đa với 
mô hình phần thưởng được huấn luyện. Cuối cùng, chúng tôi tính toán tỷ lệ thắng của LLM cơ sở 
và các chính sách Best-of-n so với DaVinci-003 (mô hình tham chiếu) trên các hướng dẫn chưa 
thấy này. Ở đây, chúng tôi sử dụng các phản hồi DaVinci-003 được thu thập trong AlpacaEval [18]. 
Sau căn chỉnh, chúng tôi đánh giá các chính sách Best-of-n (xếp hạng, thứ tự) so với một mô hình 
tham chiếu trên một tập hợp các hướng dẫn người dùng chưa thấy U. Chúng tôi cung cấp thêm chi 
tiết về thiết lập huấn luyện trong §G.

5.2 KẾT QUẢ
Chúng tôi huấn luyện các mô hình phần thưởng trên dữ liệu phản hồi xếp hạng và thứ tự được thu 
thập từ GPT-3.5-Turbo. Chúng tôi trình bày tỷ lệ thắng cho LLM cơ sở và các chính sách Best-of-n 
trên các hướng dẫn chưa thấy được đánh giá bởi con người và GPT-3.5-Turbo trong Hình 3.

Các chính sách Best-of-n vượt trội hơn SFT. Chúng tôi thấy rằng việc lấy mẫu ngẫu nhiên từ n (= 
64) phản hồi ứng viên từ mô hình SFT (được ghi là 'SFT' trong hình) đạt được tỷ lệ thắng 33.9% 
so với DaVinci-003 bằng cách sử dụng các phán đoán theo cặp từ con người. Hơn nữa, chúng tôi 
quan sát thấy rằng Best-of-64 đạt được tỷ lệ thắng 39.2% và 45.1% so với DaVinci-003 bằng cách 
sử dụng các mô hình phần thưởng được huấn luyện với phản hồi xếp hạng và thứ tự, tương ứng. 
Chúng tôi quan sát một xu hướng tương tự đối với các mô hình phần thưởng được huấn luyện với 
phản hồi xếp hạng bằng cách sử dụng con người và AI làm người đánh giá. Điều này chỉ ra rằng 
phản hồi được thu thập ở quy mô từ các LLM AI hữu ích để cải thiện việc căn chỉnh của SFT.

Không nhất quán Đánh giá. Đáng ngạc nhiên, chúng tôi quan sát một hiện tượng không nhất quán 
đánh giá trong đó việc lựa chọn giao thức phản hồi trong quá trình đánh giá ủng hộ thuật toán căn 
chỉnh sử dụng cùng giao thức phản hồi. Cụ thể, chúng tôi thấy rằng khoảng cách giữa tỷ lệ thắng 
của chính sách Best-of-n (thứ tự) và SFT (11.2%) lớn hơn khoảng cách giữa chính sách Best-of-n 
(xếp hạng) và SFT (5.3%) như được đánh giá bởi các người chú thích con người dưới giao thức thứ 
tự. Tuy nhiên, khoảng cách giữa tỷ lệ thắng của chính sách Best-of-n (xếp hạng) và SFT (5%) lớn 
hơn một chút so với khoảng cách giữa chính sách Best-of-n (thứ tự) và SFT (4.3%) như được đánh 
giá bởi các người chú thích con người dưới giao thức xếp hạng. Thú vị, chúng tôi quan sát sự không 
nhất quán đánh giá tương tự khi GPT-3.5-Turbo được sử dụng làm người chú thích đánh giá. Chúng 
tôi trình bày kết quả hiệu suất cụ thể cho từng nhiệm vụ trong Phụ lục §L. Điều này chỉ ra rằng 
các người chú thích cảm nhận chất lượng của các phản hồi chính sách khác nhau tùy thuộc vào 
giao thức thu thập phản hồi. Điều này làm nổi bật một thách thức trong việc thiết kế các giao thức 
đánh giá mạnh mẽ phản ánh hiệu suất thực tế.

Ablations. Ở đây, chúng tôi muốn hiểu liệu hiện tượng không nhất quán đánh giá có được quan 
sát trong các thiết lập khác nhau hay không. Trong Phụ lục §M, chúng tôi thấy sự không nhất quán 
đánh giá khi chính sách căn chỉnh được thay đổi thành tinh chỉnh lấy mẫu từ chối. Trong Phụ lục 
§N, chúng tôi cho thấy rằng các quan sát của chúng tôi mạnh mẽ đối với việc lựa chọn mô hình 
phần thưởng được sử dụng cho chính sách Best-of-n. Trong Phụ lục O, chúng tôi thấy rằng việc 
chuyển đổi dữ liệu xếp hạng thành định dạng thứ tự cũng dẫn đến sự không nhất quán đánh giá.

6 CÔNG TRÌNH LIÊN QUAN
Học từ Phản hồi Con người. [9] giới thiệu khung RL từ Phản hồi Con người (RLHF), ban đầu học 
một mạng mô hình phần thưởng từ một bộ dữ liệu các sở thích con người. Mô hình phần thưởng 
này sau đó được sử dụng để thay thế hàm phần thưởng do con người viết trong quá trình tối ưu 
hóa RL. [35] và [26] giới thiệu khung RLHF để căn chỉnh các LM với các sở thích con người.

Học từ Phản hồi AI. Trong khi RLHF đã cho thấy kết quả đầy hứa hẹn, việc thu thập các sở thích 
con người vẫn là một chi phí đáng kể để căn chỉnh các mô hình ngôn ngữ. Nghiên cứu trước đây 
đã chứng minh rằng các LM có khả năng cung cấp phản hồi [16, 49, 12] có thể được sử dụng để 
cải thiện khả năng của chính chúng [22, 1]. [4] giới thiệu khung Học Tăng cường từ Phản hồi AI 
(RLAIF), thay thế bước phản hồi con người của RLHF bằng một quy trình tự động tạo phản hồi AI. 
Nghiên cứu của chúng tôi so sánh phản hồi con người với phản hồi AI trong cả thiết lập xếp hạng 
và thứ tự.

Thu thập Dữ liệu Phản hồi. Các nghiên cứu trước đây đã sử dụng cả phản hồi con người và AI 
trong cả thiết lập xếp hạng [19, 17] và thứ tự [35, 26, 37, 12] trên các phản hồi do máy viết [3] 
và các phản hồi do con người viết [24]. Tuy nhiên, chưa có nghiên cứu nào so sánh có hệ thống 
các tác động của việc thu thập phản hồi xếp hạng so với thứ tự.

7 KẾT LUẬN
Trong nghiên cứu này, chúng tôi đã tiến hành một cuộc khảo sát về các giao thức thu thập phản 
hồi nhằm căn chỉnh các LLM. Cuộc điều tra của chúng tôi đã tiết lộ một thách thức đáng chú ý: 
sự không nhất quán phản hồi trong các giao thức thưa thớt (xếp hạng và thứ tự). Chúng tôi thấy 
rằng sự không nhất quán phản hồi được thể hiện bởi cả con người và hệ thống AI như những nhà 
cung cấp phản hồi. Ngoài ra, chúng tôi đã đi sâu vào các yếu tố góp phần vào sự không nhất quán 
này và làm sáng tỏ những hậu quả thực tế của nó trong bối cảnh căn chỉnh và đánh giá mô hình, 
đáng chú ý là làm nổi bật hiện tượng không nhất quán đánh giá. Cụ thể, chúng tôi nhấn mạnh rằng 
các xu hướng quan sát được trong các đánh giá bị ảnh hưởng đáng kể bởi việc lựa chọn các giao 
thức phản hồi. Các nỗ lực nghiên cứu trong tương lai có thể khám phá các nền tảng nhận thức của 
vấn đề không nhất quán, mở rộng loạt các giao thức phản hồi để bao gồm phản hồi dày đặc hơn, 
và điều tra các tác động của những lựa chọn này đối với thuật toán căn chỉnh và các quy trình đánh 
giá tiếp theo.

8 LỜI CẢM ƠN
Hritik Bansal được hỗ trợ một phần bởi tài trợ AFOSR MURI FA9550-22-1-0380. Chúng tôi cảm 
ơn Ashima Suvarna vì những bình luận hữu ích của cô ấy về bản thảo. Chúng tôi cũng cảm ơn các 
nhà phê bình vì phản hồi sâu sắc của họ.

TÀI LIỆU THAM KHẢO
[1] Afra Feyza Akyürek, Ekin Akyürek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, 
và Niket Tandon. Rl4f: Generating natural language feedback with reinforcement learning for 
repairing model outputs. arXiv preprint arXiv:2305.08844, 2023.

[2] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy 
Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a 
laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.

[3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn 
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless 
assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 
2022.

[4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, 
Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: 
Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.

[5] Yonatan Bitton, Hritik Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh 
Gardner, Rohan Taori, và Ludwig Schimdt. Visit-bench: A benchmark for vision-language 
instruction following inspired by real-world use, 2023.

[6] Jon Ander Campos và Jun Shern. Training language models with language feedback. In ACL 
Workshop on Learning with Natural Language Supervision. 2022., 2022.

[7] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier 
Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel 
Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul 
Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, 
Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık, 
Anca Dragan, David Krueger, Dorsa Sadigh, và Dylan Hadfield-Menell. Open problems and 
fundamental limitations of reinforcement learning from human feedback, 2023.

[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, 
Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna: An open-source chatbot 
impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 
2023), 2023.

[9] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, và Dario Amodei. Deep 
reinforcement learning from human preferences. Advances in neural information processing 
systems, 30, 2017.

[10] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick 
Wendell, Matei Zaharia, và Reynold Xin. Free dolly: Introducing the world's first truly open 
instruction-tuned llm, 2023. URL https://www.databricks.com/blog/2023/04/12/dolly-first-
open-commercially-viable-instruction-tuned-llm.

[11] Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun 
Shum, và Tong Zhang. Raft: Reward ranked finetuning for generative foundation model 
alignment. arXiv preprint arXiv:2304.06767, 2023.

[12] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos 
Guestrin, Percy Liang, và Tatsunori B Hashimoto. Alpacafarm: A simulation framework for 
methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.

[13] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, và 
Dawn Song. Koala: A dialogue model for academic research. Blog post, April 2023. URL 
https://bair.berkeley.edu/blog/2023/04/03/koala/.

[14] Fabrizio Gilardi, Meysam Alizadeh, và Maël Kubli. Chatgpt outperforms crowd-workers for 
text-annotation tasks. arXiv preprint arXiv:2303.15056, 2023.

[15] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu 
Wang, và Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.

[16] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, 
Nicholas Schiefer, Zac Hatfield Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language 
models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.

[17] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter 
Abbeel, Mohammad Ghavamzadeh, và Shixiang Shane Gu. Aligning text-to-image models using 
human feedback. arXiv preprint arXiv:2302.12192, 2023.

[18] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy 
Liang, và Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following 
models. https://github.com/tatsu-lab/alpaca_eval, 2023.

[19] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan 
Leike, John Schulman, Ilya Sutskever, và Karl Cobbe. Let's verify step by step. arXiv preprint 
arXiv:2305.20050, 2023.

[20] Rensis Likert. A technique for the measurement of attitudes. Archives of psychology, 1932.

[21] Ilya Loshchilov và Frank Hutter. Decoupled weight decay regularization. arXiv preprint 
arXiv:1711.05101, 2017.

[22] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, 
Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative 
refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.

[23] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, 
Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-
assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.

[24] Stanford NLP. Stanfordnlp/shp · datasets at hugging face. URL https://huggingface.co/
datasets/stanfordnlp/SHP.

[25] OpenAI. Gpt-4 technical report, 2023.

[26] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, 
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to 
follow instructions with human feedback. Advances in Neural Information Processing Systems, 
35:27730–27744, 2022.

[27] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, và Jianfeng Gao. Instruction tuning 
with gpt-4. arXiv preprint arXiv:2304.03277, 2023.

[28] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, và 
Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. 
arXiv preprint arXiv:2305.18290, 2023.

[29] Keita Saito, Akifumi Wachi, Koki Wataoka, và Youhei Akimoto. Verbosity bias in preference 
labeling by large language models. arXiv preprint arXiv:2310.10076, 2023.

[30] Jérémy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, 
Kyunghyun Cho, và Ethan Perez. Training language models with language feedback at scale. 
arXiv preprint arXiv:2303.16755, 2023.

[31] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, và Oleg Klimov. Proximal 
policy optimization algorithms, 2017.

[32] Weiyan Shi, Emily Dinan, Kurt Shuster, Jason Weston, và Jing Xu. When life gives you lemons, 
make cherryade: Converting feedback from bad responses into good labels. arXiv preprint 
arXiv:2210.15893, 2022.

[33] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, và Houfeng Wang. 
Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.

[34] Ziang Song, Tianle Cai, Jason D Lee, và Weijie J Su. Reward collapse in aligning large language 
models. arXiv preprint arXiv:2305.17608, 2023.

[35] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec 
Radford, Dario Amodei, và Paul F Christiano. Learning to summarize with human feedback. 
Advances in Neural Information Processing Systems, 33:3008–3021, 2020.

[36] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, 
Percy Liang, và Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama 
model. https://github.com/tatsu-lab/stanford_alpaca, 2023.

[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, 
Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: 
Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.

[38] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, 
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open 
foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.

[39] Amos Tversky. Intransitivity of preferences. Psychological review, 76(1):31, 1969.

[40] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, 
Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. 
Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. 
arXiv preprint arXiv:2204.07705, 2022.

[41] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi 
Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels 
go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751, 
2023.

[42] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, 
và Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions, 
2023.

[43] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A 
Smith, Mari Ostendorf, và Hannaneh Hajishirzi. Fine-grained human feedback gives better 
rewards for language model training. arXiv preprint arXiv:2306.01693, 2023.

[44] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, và 
Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. 
arXiv preprint arXiv:2304.12244, 2023.

[45] Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, và Yuandong Tian. Rlcd: Reinforcement 
learning from contrast distillation for language model alignment. arXiv preprint 
arXiv:2307.12950, 2023.

[46] Da Yin, Xiao Liu, Fan Yin, Ming Zhong, Hritik Bansal, Jiawei Han, và Kai-Wei Chang. 
Dynosaur: A dynamic growth paradigm for instruction-tuning data curation, 2023.

[47] Weizhe Yuan, Kyunghyun Cho, và Jason Weston. System-level natural language feedback, 2023.

[48] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, và Fei Huang. Rrhf: 
Rank responses to align language models with human feedback without tears. arXiv preprint 
arXiv:2304.05302, 2023.

[49] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, 
Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and 
chatbot arena. arXiv preprint arXiv:2306.05685, 2023.

--- TRANG 14 ---
Được xuất bản như một bài báo hội nghị tại ICLR 2024
PHỤ LỤC

A HẠN CHẾ
Mặc dù có nhiều cách khác nhau để thu thập phản hồi, trọng tâm của chúng tôi là phản hồi tương 
đối và tuyệt đối. Nghiên cứu trong tương lai nên khám phá tác động của việc thu thập các dạng 
phản hồi phong phú hơn. Dữ liệu phản hồi con người vốn mang tính chủ quan, và trong khi chúng 
tôi cố gắng cung cấp hướng dẫn cho các người chú thích để đánh giá các phản hồi theo các chiều 
như độ chính xác, tính mạch lạc và tính vô hại, vẫn sẽ có nhiễu ngay cả với nhiều chú thích cho 
mỗi ví dụ. Ngoài ra, việc thu thập chỉ các điểm tuyệt đối hoặc sở thích tương đối không nắm bắt 
đầy đủ tất cả các loại phản hồi có thể có thể được thu thập. Các dạng phản hồi dữ liệu phong phú 
hơn nên được khám phá trong nghiên cứu tương lai.

Phân tích của chúng tôi chủ yếu tập trung vào tác động của các phương pháp thu thập phản hồi 
khác nhau đối với hiệu suất downstream của các LM như được đánh giá bởi tỷ lệ thắng so với 
DaVinci-003. Nghiên cứu tương lai nên điều tra tác động của các phương pháp thu thập và chuyển 
đổi phản hồi khác nhau đối với tính hữu ích, tính có hại và ảo giác. Các phương pháp phản hồi khác 
nhau, bao gồm những phương pháp ngoài sở thích tuyệt đối và tương đối, có thể có những tác động 
hoàn toàn khác biệt đối với tính hữu ích, tính có hại, ảo giác và các tiêu chí đánh giá LM khác.

Dữ liệu con người của chúng tôi được thu thập từ Amazon Mechanical Turkers và không nhất thiết 
đại diện cho tất cả mọi người. Nghiên cứu tương lai nên điều tra tác động của các phương pháp thu 
thập phản hồi đối với việc căn chỉnh với các nhóm nhân khẩu học khác nhau (đặc biệt là các nhóm 
thiểu số). Một số phương pháp thu thập phản hồi có thể khuếch đại thành kiến trong nhân khẩu 
học người chú thích trong các mô hình phần thưởng được huấn luyện nhiều hơn những phương 
pháp khác. Ngoài ra, trong khi chúng tôi tập trung vào các miền phản hồi mở, các phát hiện của 
chúng tôi về tác động của các phương pháp thu thập phản hồi khác nhau có thể khác biệt trong 
các miền ứng dụng cụ thể hơn như lý luận toán học hoặc các nhiệm vụ mã hóa. Ví dụ, trong một 
nhiệm vụ chứng minh toán học, phản hồi nhị phân có thể hữu ích hơn vì một bước trong chứng 
minh luôn đúng hoặc sai.

B CÔNG TRÌNH LIÊN QUAN CHI TIẾT
Học từ Phản hồi Con người. Trong một thiết lập Học Tăng cường điển hình, một chính sách được 
tối ưu hóa để tối đa hóa một hàm phần thưởng, được định nghĩa rõ ràng bởi con người. [9] giới 
thiệu khung Học Tăng cường từ Phản hồi Con người (RLHF), ban đầu học một mạng mô hình phần 
thưởng từ một bộ dữ liệu các sở thích con người. Mô hình phần thưởng này sau đó được sử dụng 
để thay thế hàm phần thưởng do con người viết trong quá trình tối ưu hóa RL. [35] và [26] giới 
thiệu khung RLHF để căn chỉnh các LM với các sở thích con người. Họ đầu tiên học một mô hình 
phần thưởng được huấn luyện trên thứ tự phản hồi con người được lấy mẫu từ đầu ra LM, sau đó 
được sử dụng để tối ưu hóa một LM với thuật toán RL. Ngược lại, [33] và [28] bỏ qua việc học 
mô hình phần thưởng và tinh chỉnh trực tiếp một LM bằng dữ liệu sở thích con người.

Học từ Phản hồi AI. Trong khi RLHF đã cho thấy kết quả đầy hứa hẹn, việc thu thập các sở thích 
con người vẫn là một chi phí đáng kể để căn chỉnh các mô hình ngôn ngữ. Nghiên cứu trước đây 
đã chứng minh rằng các LM có khả năng cung cấp phản hồi [16, 49, 12] có thể được sử dụng để 
cải thiện khả năng của chính chúng [22, 1]. [4] giới thiệu khung Học Tăng cường từ Phản hồi AI 
(RLAIF), thay thế bước phản hồi con người của RLHF bằng một quy trình tự động tạo phản hồi AI. 
Nghiên cứu của chúng tôi so sánh phản hồi con người với phản hồi AI trong cả thiết lập xếp hạng 
và thứ tự.

Thu thập Dữ liệu Phản hồi. Trong khi RLHF và RLAIF đã được chứng minh là các phương pháp 
hiệu quả cao để căn chỉnh các LLM, những thách thức bổ sung vẫn còn, bao gồm việc tối ưu hóa 
thu thập phản hồi và mô hình hóa phần thưởng [7]. Các nghiên cứu trước đây đã sử dụng cả phản 
hồi con người và AI trong cả thiết lập xếp hạng [19, 17] và thứ tự [35, 26, 37, 12] trên các phản 
hồi do máy viết [3] và các phản hồi do con người viết [24]. Tuy nhiên, chưa có nghiên cứu nào so 
sánh có hệ thống các tác động của việc thu thập phản hồi xếp hạng so với thứ tự. Trong khi các 
nghiên cứu trước đây đã điều tra việc sử dụng các dạng phản hồi phong phú như phản hồi chi tiết 
[43] và phản hồi ngôn ngữ tự nhiên cấp hệ thống [47], trọng tâm của nghiên cứu chúng tôi chỉ là 
phản hồi xếp hạng và thứ tự. Cuối cùng, sự không nhất quán đã được khám phá trước đây về mặt 
tính bắc cầu trong các sở thích con người [39] và (không) đồng thuận phát sinh giữa các người 
chú thích khác nhau khi cung cấp phản hồi trên một thực thể duy nhất. Tuy nhiên, nghiên cứu của 
chúng tôi tập trung vào (không) đồng thuận giữa các giao thức phản hồi khác nhau.

C THÀNH PHẦN CỦA DỮ LIỆU HƯỚNG DẪN
Chúng tôi thu thập một tập hợp đa dạng các hướng dẫn từ các nguồn khác nhau:
1. Dolly [10]: Chúng tôi chọn ngẫu nhiên một tập con 4K trong số 15K cặp gợi ý và phản hồi chất 
lượng cao do con người tạo ra.
2. Self-Instruct (User Oriented) [42]: Đây là một tập hợp 252 hướng dẫn được viết bởi chuyên gia.
3. Super-NI [40]: Ban đầu, nó chứa 1600+ nhiệm vụ NLP với các mô tả nhiệm vụ được viết bởi 
chuyên gia. Trong nghiên cứu của chúng tôi, chúng tôi chọn một tập con 100 nhiệm vụ NLP dày 
đặc như 'tạo câu hỏi dựa trên một tình huống đã cho' thay vì những nhiệm vụ chỉ yêu cầu câu trả 
lời 'có/không' (Phụ lục §Q). Tiếp theo, chúng tôi chọn ngẫu nhiên 10 trường hợp cho mỗi nhiệm vụ.

Thành phần của dữ liệu hướng dẫn được trình bày trong Bảng 3.

D THU THẬP PHẢN HỒI TỪ CON NGƯỜI
Đầu tiên, chúng tôi chọn ngẫu nhiên một tập con 500 từ 46.5K từ dữ liệu phản hồi thứ tự GPT-3.5-
Turbo. Chúng tôi sử dụng dữ liệu này để tạo 1000 trường hợp cho chú thích giao thức xếp hạng 
(một trường hợp thứ tự theo cặp đóng góp vào hai trường hợp xếp hạng riêng lẻ cho các phản hồi). 
Chúng tôi sử dụng các người chú thích trên nền tảng Amazon Mechanical Turk cho nhiệm vụ của 
chúng tôi. Trước khi đủ điều kiện, các người chú thích được cho xem một vài ví dụ đã giải cho cả 
hai giao thức. Sau khi đủ điều kiện, tổng cộng 26 người chú thích đã tham gia vào việc chú thích 
trong đó 13 người chú thích được sử dụng cho phản hồi xếp hạng và thứ tự mỗi loại. Các người 
chú thích được trả lương theo giờ 18 đô la/giờ. Chi phí tổng thể cho chú thích con người là 1000 
đô la. Chúng tôi gán mỗi trường hợp của dữ liệu phản hồi cho bốn người lao động. Tóm lại, chúng 
tôi có 2000 phản hồi thứ tự (= 500 trường hợp × 4 người lao động mỗi trường hợp) và 4000 xếp 
hạng (= 1000 trường hợp × 4 người lao động mỗi trường hợp) từ các người chú thích con người.

E BIẾN THIÊN TRONG ĐIỂM TÍNH NHẤT QUÁN VỚI CÁC NGƯỜI CHÚ THÍCH

E.1 NGƯỜI CHÚ THÍCH CON NGƯỜI
Trong Bảng 2b, phân tích tính nhất quán được thực hiện bằng cách tổng hợp các sở thích của ba 
người chú thích ngẫu nhiên (trong số bốn). Để hiểu tác động của các nhóm con người khác nhau, 
chúng tôi tính toán điểm tính nhất quán cho một nhóm khác gồm 3 người chú thích được chọn 
ngẫu nhiên bốn lần. Chúng tôi thấy rằng độ lệch chuẩn của phân tích không nhất quán cho bốn 
nhóm của các người đánh giá con người là 1.6%. Điều này chỉ ra rằng phân tích không nhất quán 
mạnh mẽ đối với nhóm con người được sử dụng để chú thích.

E.2 NGƯỜI CHÚ THÍCH AI
Trong các thí nghiệm chính của chúng tôi, chúng tôi sử dụng GPT-3.5-Turbo (nhiệt độ = 0.0) cho 
phân tích tính nhất quán. Để cung cấp thêm bằng chứng cho cùng mô hình sử dụng các nhiệt độ 
khác nhau – 0.0, 0.2 và 0.5 cho 1000 so sánh. Chúng tôi thấy rằng độ lệch chuẩn trong sự không 
đồng thuận giữa các nhiệt độ này là 2.7%. Điều này chỉ ra rằng phân tích không nhất quán cũng 
mạnh mẽ đối với các nhiệt độ mô hình khác nhau được sử dụng để tạo dữ liệu phản hồi.

Ngoài ra, chúng tôi thử nghiệm với các mô hình khác để phân tích không nhất quán tức là GPT-3.5-
Turbo-0613 và GPT-4 với 50. Kết quả không nhất quán cho các mô hình này được trình bày trong 
Bảng 4.

Chúng tôi thấy rằng tất cả các lựa chọn mô hình đều gặp phải vấn đề không nhất quán. Điều này 
chỉ ra rằng kết quả của chúng tôi không bị hạn chế bởi việc lựa chọn người chú thích AI, và thực 
sự có thể quan sát được trong các mô hình khác nhau.

F XẾP HẠNG THEO CẶP
Chúng tôi thu thập dữ liệu phản hồi bằng giao thức thứ ba tức là xếp hạng theo cặp. Với một cặp 
phản hồi, cung cấp điểm xếp hạng [1-7] cho cả hai. Điều này khác với giao thức xếp hạng ban đầu 
của chúng tôi nơi các xếp hạng được thu thập riêng lẻ trên các phản hồi. Chúng tôi chuyển đổi 
phản hồi xếp hạng theo cặp mới được thu thập này thành dạng thứ tự bằng cách so sánh các điểm 
được đưa ra cho cặp phản hồi. Ví dụ, nếu Phản hồi 1 nhận được 7 và Phản hồi 2 nhận được 5, 
chúng tôi coi Phản hồi 1 > Phản hồi 2. Chúng tôi so sánh tính nhất quán của dữ liệu xếp hạng theo 
cặp này (được chuyển đổi thành thứ tự) với dữ liệu phản hồi xếp hạng ban đầu của chúng tôi. 
Chúng tôi thấy rằng ngay cả các giao thức phản hồi này cũng gặp phải sự không nhất quán 58% 
trên 500 so sánh. Nó cũng làm nổi bật rằng xếp hạng được thu thập theo cách theo cặp (giống với 
định dạng thứ tự) không được hiệu chỉnh với xếp hạng được thu thập cho các phản hồi riêng lẻ. 
Điều này có thể được quy cho thực tế là các người chú thích tập trung vào các thuộc tính chất lượng 
phản hồi khác nhau khi cung cấp phản hồi theo cặp và phản hồi độc lập.

G THIẾT LẬP
Mô hình hóa Phần thưởng. Chúng tôi chọn thích ứng hạng thấp [15] (LoRA) của Alpaca-7B làm 
mô hình phần thưởng². Chúng tôi sử dụng hàm mục tiêu được định nghĩa trong Phương trình 2 để 
huấn luyện mô hình phần thưởng trên dữ liệu phản hồi thứ tự. Tương tự như [26], một lô đơn chỉ 
chứa tất cả các cặp phản hồi ứng viên cho một hướng dẫn cụ thể. Hơn nữa, 70% dữ liệu phản hồi 
được sử dụng để huấn luyện và phần còn lại để xác thực. Chúng tôi cung cấp thêm chi tiết về các 
thiết lập huấn luyện trong Phụ lục §K. Chúng tôi huấn luyện các mô hình phần thưởng trên phản 
hồi thứ tự và xếp hạng trong 5 epoch với dừng sớm trong đó mất mát xác thực là tiêu chí. Chúng 
tôi huấn luyện mỗi mô hình phần thưởng cho ba hạt giống ngẫu nhiên.

Đánh giá. Sau huấn luyện, chúng tôi đánh giá các mô hình trên 553 hướng dẫn chưa thấy, bao 
gồm 188 hướng dẫn từ đánh giá Open Assistant (OASST), 129 từ đánh giá hữu ích được phát hành 
bởi Anthropic [3], 80 từ đánh giá Vicuna [8] và 156 từ đánh giá Koala [13]. Chúng tôi sử dụng 
chính sách Best-of-n (n = 64) để đánh giá tác động của các dữ liệu phản hồi khác nhau đối với việc 
căn chỉnh mô hình. Đối với điều này, trước tiên chúng tôi tạo ra n phản hồi ứng viên có độ dài 200 
từ mô hình SFT (Alpaca-7B) với các hướng dẫn chưa thấy. Tiếp theo, đầu ra của Best-of-n sẽ là 
cái đạt được điểm tối đa với mô hình phần thưởng được huấn luyện. Cuối cùng, chúng tôi tính toán 
tỷ lệ thắng của mô hình SFT và các chính sách Best-of-n so với DaVinci-003 (mô hình tham chiếu) 
trên các hướng dẫn chưa thấy này. Ở đây, chúng tôi sử dụng các phản hồi DaVinci-003 được thu 
thập trong AlpacaEval [18]. Ở đây, chúng tôi thay đổi việc lựa chọn các giao thức phản hồi là xếp 
hạng và thứ tự trong quá trình đánh giá và việc lựa chọn các người chú thích là con người và 
ChatGPT-3.5-Turbo. Mỗi so sánh theo cặp hoặc xếp hạng phản hồi được thực hiện bởi một người 
lao động đám đông, nơi chúng tôi trả 18 đô la mỗi giờ cho các chú thích. Chúng tôi chi 1000 đô la 
để thu thập chú thích đánh giá từ con người.

Tính toán Tỷ lệ Thắng. Sau căn chỉnh, chúng tôi đánh giá các chính sách Best-of-n (xếp hạng, thứ 
tự) so với một mô hình tham chiếu trên một tập hợp các hướng dẫn người dùng chưa thấy U. Vì 
chúng tôi muốn hiểu tác động của việc lựa chọn giao thức thu thập phản hồi đối với toàn bộ pipeline 
căn chỉnh, chúng tôi thu thập xếp hạng và thứ tự cho các phản hồi từ chính sách căn chỉnh và các 
mô hình tham chiếu từ con người và AI làm người chú thích. Chúng tôi tính toán tỷ lệ thắng so với 
mô hình tham chiếu là phần các trường hợp mà phản hồi chính sách Best-of-n được ưa thích hơn 
mô hình tham chiếu dưới một giao thức phản hồi cụ thể. Trong trường hợp phán đoán 'hòa' bởi 
các người chú thích, cả chính sách và mô hình tham chiếu đều nhận được nửa điểm. Chính thức, 
chúng tôi định nghĩa điểm cho mỗi ví dụ cho thứ tự là

E(u, yP, yF) = {
1 nếu r(u, yP, yF) = yP
0.5 nếu r(u, yP, yF) = hòa
0 nếu r(u, yP, yF) = yF
}    (5)

và cho xếp hạng là

E(u, yP, yF) = {
1 nếu a(u, yP) > a(u, yF)
0.5 nếu a(u, yP) = a(u, yF)
0 nếu a(u, yP) < a(u, yF)
}    (6)

trong đó u ∈ U là hướng dẫn đầu vào chưa thấy, yP = Pn(gθ, pθ, u, {v1, . . . , vn}) là đầu ra từ 
chính sách Best-of-n tận dụng mô hình phần thưởng thứ tự, yF là phản hồi từ mô hình tham chiếu, 
r(u, yP, yF) là sở thích giữa cặp phản hồi được quyết định bởi người chú thích. Metric tỷ lệ thắng 
w được tính toán như điểm trung bình cho mỗi ví dụ:

w = (1/|U|) ∑u∈U E(u, yP, yF)    (7)

H PHÂN TÍCH ĐỘ DÀI PHẢN HỒI VÀ SỐ TỪ DỘC ĐÁO
Chúng tôi đánh giá xem độ dài hoặc số từ độc đáo trong các phản hồi ứng viên có thiên vị các phán 
đoán phản hồi xếp hạng và thứ tự từ con người và AI trong dữ liệu của chúng tôi hay không. Chúng 
tôi trình bày kết quả cho đánh giá như vậy trong phản hồi xếp hạng và thứ tự trong Bảng Phụ lục 
5 và 6 tương ứng. Tóm lại, chúng tôi thấy rằng không có sự khác biệt rõ ràng giữa độ dài trung 
bình và số từ độc đáo trung bình của phản hồi được ưa thích và không được ưa thích trong phản 
hồi thứ tự được thu thập từ con người và AI.

Trong Bảng 5, chúng tôi thấy rằng điểm xếp hạng được gán cho các phản hồi riêng lẻ không tăng 
theo độ dài trung bình và số từ độc đáo trung bình trong phản hồi cho cả con người và AI. Tương 
tự trong Bảng 6, chúng tôi thấy rằng không có sự khác biệt rõ ràng giữa độ dài trung bình và số 
từ độc đáo trung bình của phản hồi được ưa thích và không được ưa thích trong phản hồi thứ tự 
được thu thập từ con người và AI. Điều này làm nổi bật rằng dữ liệu phản hồi của chúng tôi không 
thiên vị đối với các phản hồi dài hơn và độc đáo, và sự khác biệt quan sát được từ các nghiên cứu 
trước có thể được quy cho sự khác biệt trong các thiết lập thí nghiệm.

Chúng tôi làm rõ rằng thiết lập của chúng tôi không thiên vị đối với độ dài phản hồi. Tuy nhiên, 
nó không ngụ ý rằng con người nói chung không ưa thích các phản hồi dài hơn. [49] đề cập đến 
thành kiến verbosity đã kiểm tra thành kiến verbosity khi sử dụng các LLM để thu thập dữ liệu 
phản hồi bằng cách xây dựng một cuộc tấn công danh sách lặp đi lặp lại trong đó các phản hồi có 
danh sách được đặt trước với phiên bản diễn giải của danh sách (được tạo ra bởi GPT-4) không 
chứa thông tin mới. Ví dụ, mô hình A có hai phản hồi con trỏ và mô hình B có cùng hai con trỏ + 
các phiên bản diễn giải của chúng (không thêm thông tin mới), các người chú thích ưa thích phản 
hồi mô hình B. Chúng tôi tin rằng thiết lập này khác biệt lớn so với thiết lập của chúng tôi vì chúng 
tôi xem xét hai phản hồi khác nhau không có sự can thiệp thêm như cuộc tấn công lặp lại. Tương 
tự, [29] nói về thành kiến verbosity trong ghi nhãn sở thích từ con người và AI cho các nhiệm vụ 
viết sáng tạo. Trong nghiên cứu của chúng tôi, chúng tôi xem xét một loạt các hướng dẫn được 
lấy từ 3 bộ dữ liệu như được thảo luận trong Bảng 3.

I LỜI NHẮC LLM CHO THU THẬP PHẢN HỒI
Chúng tôi trình bày các lời nhắc LLM để thu thập phản hồi xếp hạng và thứ tự từ GPT-3.5-Turbo 
trong Bảng 4 và Bảng 5.

J THÍ NGHIỆM CON NGƯỜI VỚI SCREENSHOT UI
Chúng tôi cung cấp cùng hướng dẫn chú thích cho con người như các mô hình AI. Chúng tôi trình 
bày các screenshot giao diện người dùng cho việc thu thập phản hồi trong Hình 6 và 7. Các screenshot 
chứa một biểu mẫu giải thích chỉ có mặt trong quá trình phân tích định tính của dữ liệu phản hồi, 
trong khi nó được loại bỏ trong quá trình thu thập phản hồi quy mô lớn.

J.1 PHÂN TÍCH CHI TIẾT CỦA DỮ LIỆU NHẤT QUÁN VÀ KHÔNG NHẤT QUÁN
Chúng tôi thực hiện các thí nghiệm chi tiết để hiểu sự khác biệt giữa các tập con nhất quán và 
không nhất quán của dữ liệu phản hồi từ con người và AI.

Sự khác biệt trong Xếp hạng. Mục tiêu của chúng tôi là định lượng sự khác biệt xếp hạng trung 
bình cho cặp phản hồi ứng viên nhận được sở thích quyết định (một trong 'phản hồi 1' hoặc 'phản 
hồi 2' nhưng không phải 'bằng nhau') từ dữ liệu phản hồi xếp hạng. Cụ thể, chúng tôi tập trung 
vào Hàng 2 và Hàng 3 của Bảng 2a và 2b. Trong các hàng này, (a) sự kết hợp của (Cột 2 - Hàng 
2, Cột 3 - Hàng 3) đại diện cho các trường hợp nhất quán nơi cả hai dạng phản hồi đều quyết định, 
(b) sự kết hợp của (Cột 2 - Hàng 3, Cột 3 - Hàng 2) đại diện cho các trường hợp quyết định không 
nhất quán, và (c) Cột 1 đại diện cho tình huống nơi phản hồi thứ tự không quyết định. Chúng tôi 
báo cáo sự khác biệt xếp hạng trung bình giữa các điểm đạt được bởi các trường hợp thuộc về 
danh mục (a), (b) và (c) trong Bảng 7a. Chúng tôi thấy rằng sự khác biệt điểm xếp hạng trung 
bình tối đa được đạt bởi các trường hợp nhất quán đối với các người chú thích con người cũng như 
GPT-3.5-Turbo. Điều này chỉ ra rằng khoảng cách định lượng giữa cặp phản hồi ứng viên thuộc 
về các trường hợp nhất quán cao hơn cặp phản hồi thuộc về các trường hợp không nhất quán, điều 
này làm nổi bật thêm rằng sự khác biệt trong chất lượng của hai phản hồi trong một trường hợp 
nhất quán có thể phân biệt được nhiều hơn so với trong một trường hợp không nhất quán.

Ngoài ra, chúng tôi quan sát thấy rằng sự khác biệt điểm xếp hạng trung bình cho các trường hợp 
không nhất quán tương tự khi phản hồi thứ tự quyết định và không quyết định với phản hồi xếp 
hạng quyết định (Cột 2 và 3), đối với cả người chú thích con người và AI. Điều này gợi ý về sự 
gần gũi định tính tiềm năng trong cặp phản hồi khiến chúng nhận được phản hồi xếp hạng quyết 
định nhưng phản hồi thứ tự mâu thuẫn trong quá trình chú thích. Chúng tôi sẽ khám phá giả thuyết 
này thêm thông qua phân tích biến thiên thứ tự tiếp theo.

Biến thiên trong Thứ tự. Do sự gần gũi trong chất lượng được cảm nhận của cặp phản hồi thuộc 
về các trường hợp không nhất quán, chúng tôi cho rằng các người chú thích con người ưa thích 
các yếu tố chất lượng cụ thể hơn những yếu tố khác (ví dụ: tập trung vào độ chính xác hơn tính 
mạch lạc) trong việc đưa ra quyết định quyết định. Ngược lại, chúng tôi mong đợi sự biến thiên 
trong thứ tự con người thấp hơn khi một trong các phản hồi được cảm nhận là vượt trội về mặt 
định tính. Để kiểm tra giả thuyết này, chúng tôi sử dụng bộ dữ liệu bao gồm 500 trường hợp phản 
hồi thứ tự, mỗi trường hợp có bốn chú thích con người. Trong bộ dữ liệu, các chú thích ủng hộ 
'phản hồi 1' được đánh dấu là '+1', những chú thích ủng hộ 'phản hồi 2' được đánh dấu là '-1', và 
các chú thích chỉ ra sở thích 'bằng nhau' được đánh dấu là '0'. Đối với mỗi trường hợp, chúng tôi 
tính toán tổng xếp hạng của bốn đánh dấu sở thích con người. Điểm thấp trên một trường hợp 
cho thấy rằng cặp phản hồi ứng viên được coi là 'gần' bởi các người chú thích, dẫn đến khả năng 
bằng nhau của việc chọn 'bằng nhau' hoặc một trong hai phản hồi. Cuối cùng, chúng tôi tính toán 
sự biến thiên trung bình trong phản hồi thứ tự cho cả các trường hợp nhất quán và không nhất 
quán một cách riêng biệt.

Kết quả phân tích của chúng tôi được trình bày trong Bảng 7b. Để định lượng sự biến thiên trung 
bình trong phản hồi người chú thích của một ví dụ duy nhất, chúng tôi định nghĩa một metric điểm 
biến thiên. Đối với mỗi ví dụ, chúng tôi lấy tổng điểm cho nhãn của mỗi người chú thích. Ưa thích 
phản hồi 1, ưa thích phản hồi 2 và sở thích bằng nhau lần lượt được đưa ra điểm 1, -1 và 0. Điểm 
cho mỗi ví dụ này được tính trung bình trên toàn bộ bộ dữ liệu để tính toán điểm biến thiên. Chúng 
tôi thấy rằng các trường hợp nhất quán đạt được điểm biến thiên cao hơn (0.5) so với các trường 
hợp không nhất quán trong dữ liệu phản hồi con người (0.36). Quan sát này hỗ trợ giả thuyết của 
chúng tôi rằng các trường hợp không nhất quán gần nhau về mặt định tính.

K THIẾT LẬP HUẤN LUYỆN
Chúng tôi huấn luyện các mô hình phần thưởng trên dữ liệu phản hồi thứ tự và xếp hạng trong 5 
epoch với dừng sớm ở cuối mỗi epoch. Chúng tôi sử dụng mất mát xác thực để dừng sớm. Chúng 
tôi huấn luyện các mô hình phần thưởng trên một GPU Nvidia RTX A6000 với 48GB VRAM.

Các mô hình phần thưởng được tối ưu hóa bằng phương trình 2 được huấn luyện với kích thước 
lô hiệu quả = 16 trong đó kích thước lô = 4 và số bước tích lũy gradient = 4. Ở đây, mỗi lô chứa 
tất cả các cặp phản hồi ứng viên cho một hướng dẫn đã cho. Vì chúng tôi có 5 phản hồi ứng viên 
cho một hướng dẫn đã cho trong dữ liệu phản hồi, chúng tôi nhận được 10 cặp phản hồi ứng viên 
(5 chọn 2) trong một lô. Các mô hình phần thưởng được tối ưu hóa bằng phương trình 1 được huấn 
luyện với kích thước lô hiệu quả = 64 trong đó kích thước lô = 16 và số bước tích lũy gradient = 4.

Cả hai mô hình phần thưởng đều sử dụng bộ tối ưu hóa AdamW [21] với khởi động tuyến tính 100 
bước đến tốc độ học tối đa tiếp theo là suy giảm cosine. Chúng tôi thực hiện tìm kiếm siêu tham 
số trên {1e−4, 1e−5, 1e−6} cho tốc độ học tối đa. Chúng tôi thấy rằng 1e−4 cho kiến trúc mô hình 
phần thưởng Alpaca-LoRA. Chúng tôi cũng áp dụng suy giảm trọng số 0.001 cho tất cả các mô 
hình phần thưởng và huấn luyện chúng ở độ chính xác fp16.

L KẾT QUẢ CỤ THỂ CHO TỪNG NHIỆM VỤ
Trong §5.2, chúng tôi đã trình bày sự không nhất quán đánh giá trên 553 hướng dẫn từ AlpacaEval. 
Trong Bảng 8, chúng tôi muốn đánh giá sự không nhất quán đánh giá trên các nhiệm vụ cụ thể.

Chúng tôi thấy rằng sự không nhất quán đánh giá có mặt trên các nhiệm vụ khác nhau. Cụ thể, 
giao thức đánh giá xếp hạng ủng hộ chính sách best-of-64 xếp hạng và ngược lại. Điều này làm 
nổi bật rằng giao thức thu thập phản hồi thiên vị đánh giá.

M TINH CHỈNH LẤY MẪU TỪ CHỐI (RSFT)
Chúng tôi thực hiện một thí nghiệm bổ sung với tinh chỉnh lấy mẫu từ chối (RSFT) được sử dụng 
trong ba bài báo căn chỉnh LLM được trích dẫn nhiều, bao gồm LLaMA 2 [38, 4, 48] Cụ thể, thiết 
lập của chúng tôi như sau:

1. Chúng tôi nhắc Alpaca-7B với 5K hướng dẫn từ dữ liệu Alpaca-52K.
2. Chúng tôi tạo ra 64 phản hồi cho mỗi hướng dẫn.
3. Chúng tôi sử dụng mô hình phần thưởng xếp hạng và thứ tự của chúng tôi (LoRA-Alpaca) để 
chọn phản hồi tốt nhất từ 64 phản hồi.
4. Chúng tôi tinh chỉnh hai mô hình Alpaca-7B với dữ liệu hướng dẫn-phản hồi 5K.
   (a) Một mô hình nơi các phản hồi được chọn từ mô hình phần thưởng xếp hạng.
   (b) Thứ hai nơi các phản hồi được chọn từ mô hình phần thưởng thứ tự.
5. Sau tinh chỉnh, chúng tôi lấy mẫu một phản hồi duy nhất từ Alpaca-7B được tinh chỉnh với 
553 hướng dẫn đánh giá.
6. Chúng tôi tính toán tỷ lệ thắng so với DaVinci003 bằng giao thức xếp hạng và thứ tự bằng 
ChatGPT.

Kết quả cho thí nghiệm này nơi baseline là mô hình Alpaca-7B cơ sở được trình bày trong Bảng 9.

Chúng tôi thấy rằng sự không nhất quán đánh giá vẫn tồn tại dưới thuật toán căn chỉnh này cũng 
vậy. Nó chỉ ra rằng việc lựa chọn giao thức phản hồi để đánh giá ủng hộ cùng giao thức phản hồi 
được sử dụng để huấn luyện các mô hình phần thưởng và tiếp theo tinh chỉnh LLM cơ sở.

N SỬ DỤNG ROBERTA-LARGE LÀM MÔ HÌNH PHẦN THƯỞNG
Các nghiên cứu trước [26, 12] sử dụng kiến trúc chỉ giải mã cho mô hình phần thưởng. Các mô 
hình này thường có quy mô 10-100 tỷ tham số và khó huấn luyện theo cách hiệu quả về mặt tính 
toán mà không có các kỹ thuật kỹ thuật. Do đó, chúng tôi đã chọn Alpaca-7B trong thiết lập của 
chúng tôi, và LoRA chỉ là một phương pháp để tinh chỉnh nó theo cách hiệu quả tham số. Mặt khác, 
[34] đã sử dụng các mô hình mã hóa BERT, thường có ít tham số hơn, khoảng 500M-1B tham số. 
Tinh chỉnh các mô hình BERT đạt được hiệu suất rất tốt trên nhiều nhiệm vụ NLP. Do đó, chúng 
tôi lặp lại huấn luyện các mô hình phần thưởng trên kiến trúc RoBERTA-Large.

Cụ thể, chúng tôi tinh chỉnh một mô hình RoBERTA-Large trên dữ liệu xếp hạng bằng mất mát hồi 
quy. Ngoài ra, chúng tôi huấn luyện một mô hình RoBERTA-Large khác trên dữ liệu thứ tự bằng 
mất mát log likelihood âm. Chúng tôi sử dụng hai mô hình được tinh chỉnh này để thực hiện chính 
sách Best-of-64. Tiếp theo, chúng tôi sử dụng GPT-3.5-Turbo để cung cấp phản hồi xếp hạng và 
thứ tự cho chính sách Best-of-64 xếp hạng/thứ tự. Chúng tôi trình bày kết quả trong Bảng 10.

Chúng tôi thấy rằng sự không nhất quán đánh giá vẫn tồn tại nếu chúng tôi thay đổi việc lựa chọn 
mô hình phần thưởng. Cụ thể, giao thức đánh giá xếp hạng ủng hộ chính sách best-of-64 xếp hạng 
và ngược lại. Điều này làm nổi bật rằng giao thức thu thập phản hồi thiên vị đánh giá. Chúng tôi 
sẽ thêm những kết quả bổ sung này vào bài báo được sửa đổi.

O CHUYỂN ĐỔI PHẢN HỒI XẾP HẠNG THÀNH ĐỊNH DẠNG THỨ TỰ ĐỂ HUẤN LUYỆN 
PHẦN THƯỞNG
Trong nghiên cứu của chúng tôi, chúng tôi đã thiết lập rằng dữ liệu phản hồi thu được từ con người 
và AI gặp phải các vấn đề không nhất quán. Chúng tôi tin rằng nghiên cứu của chúng tôi là độc 
đáo vì nó cho thấy rằng điều này có tác động trực tiếp đến việc căn chỉnh và đánh giá các LLM 
thông qua bằng chứng thực nghiệm.

Việc lựa chọn các mô hình phần thưởng được thực hiện một cách tự nhiên để phản ánh bản chất 
của giao thức phản hồi. Cụ thể, giao thức xếp hạng cung cấp cho chúng tôi điểm vô hướng do đó 
mô hình hồi quy có ý nghĩa, và giao thức thứ tự không cung cấp cho chúng tôi điểm vô hướng do 
đó chúng tôi sử dụng hàm mục tiêu log likelihood âm (NLL).

Chúng tôi quan sát thấy rằng 57.4% các so sánh theo cặp được coi là "bằng nhau" khi chúng tôi 
chuyển đổi dữ liệu xếp hạng thành định dạng thứ tự (Hàng 1 của Bảng 2b). Trong thực tế, điều 
này có nghĩa là chúng tôi cần lọc gần 60% dữ liệu xếp hạng nếu chúng tôi muốn huấn luyện một 
mô hình phần thưởng định dạng thứ tự. Đây không phải là cách tối ưu nhất để sử dụng dữ liệu 
xếp hạng khi chúng tôi biết rằng một phương pháp hồi quy tốt hơn tồn tại có thể sử dụng dữ liệu 
hoàn chỉnh. Do đó, chúng tôi huấn luyện một mô hình phần thưởng hồi quy trong thiết lập ban 
đầu của chúng tôi.

Chúng tôi huấn luyện hai mô hình phần thưởng, của kiến trúc RoBERTA-Large, trên (a) dữ liệu 
thứ tự với mất mát NLL và (b) dữ liệu xếp hạng được chuyển đổi thành thứ tự với mất mát NLL. 
Chúng tôi thực hiện lấy mẫu Best-of-n với n = 64. Chúng tôi đánh giá tỷ lệ thắng của mô hình so 
với davinci-003 với sơ đồ đánh giá xếp hạng và thứ tự. Chúng tôi trình bày kết quả trong Bảng 11.

Chúng tôi làm nổi bật rằng sự không nhất quán đánh giá vẫn rõ ràng khi định dạng dữ liệu được 
sử dụng để huấn luyện các mô hình phần thưởng được giữ giống hệt nhau. Điều này chỉ ra rằng 
phát hiện của chúng tôi không bị nhầm lẫn bởi định dạng dữ liệu của mô hình phần thưởng mà 
bởi bản chất của dữ liệu giao thức phản hồi tức là xếp hạng và thứ tự.

P VÍ DỤ ĐỊNH TÍNH VỚI GIẢI THÍCH CON NGƯỜI
Chúng tôi thực hiện phân tích định tính với các người chú thích con người. Nhóm người chú thích 
cung cấp xếp hạng/thứ tự với giải thích loại trừ lẫn nhau. Tổng cộng, chúng tôi xem xét một tập 
con 50 hướng dẫn và 100 phản hồi (2 phản hồi mỗi hướng dẫn) từ bộ dữ liệu của chúng tôi. Ngoài 
ra, chúng tôi gán 3 người chú thích cho mỗi trường hợp xếp hạng/thứ tự. Con người được cung 
cấp cùng hướng dẫn chú thích như trước nhưng họ được yêu cầu cung cấp giải thích cho chú thích 
của họ. Chúng tôi cung cấp cho họ 2 ví dụ đã giải về cách giải thích có thể trông như thế nào. 
Chúng tôi dự định giữ cho các giải thích con người không có thành kiến từ các tác giả, do đó, 
chúng tôi hướng dẫn con người cung cấp giải thích của họ dựa trên nhận thức của họ về chất lượng 
phản hồi.

Chúng tôi trình bày một vài ví dụ định tính với giải thích từ các người chú thích con người trong 
Bảng 12, 13, 14, 15, 16 và 17.

Trong Bảng 12, chúng tôi quan sát thấy rằng các người chú thích chịu trách nhiệm cho thứ tự ủng 
hộ 'phản hồi 2' do tính dày đặc của nó, trong khi cảm nhận 'phản hồi 1' là buồn tẻ. Đồng thời, 
mặc dù những sở thích khác biệt này, các phản hồi riêng lẻ nhận được điểm xếp hạng mâu thuẫn: 
'phản hồi 2' chịu một hình phạt đáng kể do tính dày đặc và việc không tuân thủ được cảm nhận 
với các hướng dẫn nhiệm vụ, trong khi 'phản hồi 1' nhận được điểm trung bình chỉ vì tuân thủ 
một phần các hướng dẫn.

Bảng 13 chứa một cặp phản hồi khác nơi phản hồi xếp hạng và thứ tự không nhất quán. Cả hai 
phản hồi đều cung cấp câu trả lời khá tốt cho các hướng dẫn. Giải thích về phản hồi thứ tự làm 
nổi bật sự khác biệt giữa các phản hồi, bao gồm việc phản hồi bị từ chối "hơi không tự nhiên" so 
với phản hồi được chọn. Mặt khác, giải thích về phản hồi xếp hạng chủ yếu cung cấp hiểu biết về 
lý do tại sao mỗi phản hồi tốt và do đó nhận được điểm cao. Những quan sát này cho thấy rằng 
vấn đề không nhất quán phát sinh từ những biến thiên có hệ thống trong sở thích người chú thích 
với sự khác biệt trong bản chất của giao thức phản hồi.

Bảng 15 chứa một ví dụ nơi dữ liệu thứ tự và xếp hạng nhất quán với nhau. Hướng dẫn yêu cầu 
một câu hỏi sao cho mọi mục trong danh sách các đầu vào đã cho đều là câu trả lời đúng và hợp 
lý cho câu hỏi. Giải thích cho phản hồi xếp hạng cho phản hồi 2 chứa phản hồi đúng rất đơn giản, 
thường chỉ chỉ ra rằng hướng dẫn đã được tuân theo. Giải thích cho phản hồi xếp hạng cho phản 
hồi 1, chứa phản hồi tệ hơn nhiều, không tuân theo hướng dẫn thường chứa mô tả ngắn về một 
điều sai với phản hồi như không tuân theo một phần hướng dẫn hoặc một số lỗi logic hoặc thực 
tế. Giải thích cho phản hồi thứ tự chứa phân tích chi tiết hơn về sự khác biệt giữa hai phản hồi và 
có xu hướng dài hơn giải thích cho phản hồi xếp hạng. Phản hồi 2 tốt hơn phản hồi 1 được phản 
ánh trong điểm xếp hạng cao hơn nhiều cho phản hồi 2. Kích thước của sự khác biệt này không 
được nắm bắt bởi phản hồi thứ tự.

Bảng 16 chứa một ví dụ khác nơi phản hồi xếp hạng và nhất quán nhất quán. Hướng dẫn yêu cầu 
câu trả lời thực tế cho một câu hỏi đơn giản. Phản hồi 1 cung cấp câu trả lời đúng và phản hồi 2 
cung cấp câu trả lời không chính xác. Đối với các phản hồi rất đơn giản, ngắn gọn, cả giải thích 
phản hồi thứ tự và xếp hạng đều tương tự chỉ ra rằng phản hồi 1 đúng trong khi phản hồi 2 không 
chính xác. Sự khác biệt đơn giản này được phản ánh trong khoảng cách điểm xếp hạng rất lớn 
giữa hai phản hồi.

Q DANH SÁCH CÁC NHIỆM VỤ TỪ SUPER NI- V2
Danh sách các nhiệm vụ Super-Natural Instructions [40] được sử dụng trong dữ liệu phản hồi được 
trình bày trong Bảng 18.
