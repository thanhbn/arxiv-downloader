Được xuất bản như một bài báo hội nghị tại ICLR 2024
B-CODER : HỌC TĂNG CƯỜNG SÂU DỰA TRÊN GIÁ TRỊ CHO TỔNG HỢP CHƯƠNG TRÌNH

Tổng hợp chương trình nhằm tạo ra các chương trình chính xác, có thể thực thi từ các đặc tả bài toán, cụ thể là từ các mô tả ngôn ngữ tự nhiên trong bối cảnh của chúng tôi. Các nghiên cứu gần đây đã tận dụng sức mạnh của học tăng cường (RL) kết hợp với các mô hình ngôn ngữ lớn (LLM), nâng cao đáng kể khả năng tạo mã. Việc áp dụng RL tập trung vào việc tối ưu hóa trực tiếp tính đúng đắn về chức năng, mang lại lợi thế so với các phương pháp giám sát thông thường. Mặc dù các phương pháp RL dựa trên chính sách thống trị tài liệu về RL cho tổng hợp chương trình, bản chất của các nhiệm vụ tổng hợp chương trình gợi ý sự phù hợp tự nhiên với các phương pháp dựa trên giá trị. Điều này xuất phát từ bộ sưu tập phong phú các chương trình off-policy, bao gồm những chương trình được phát triển bởi các lập trình viên con người và cả các mẫu lịch sử, cùng với việc xác minh đơn giản các chương trình được tạo ra thông qua kiểm thử đơn vị tự động, có nghĩa là phần thưởng dễ dàng thu được. Khác với việc sử dụng phổ biến các thuật toán dựa trên chính sách, công trình của chúng tôi khám phá tính khả thi của các phương pháp dựa trên giá trị, dẫn đến việc phát triển B-Coder (được phát âm là Bellman coder) của chúng tôi. Tuy nhiên, việc huấn luyện các phương pháp dựa trên giá trị gặp thách thức do không gian tìm kiếm khổng lồ vốn có trong tổng hợp chương trình. Để giải quyết điều này, chúng tôi giới thiệu một giao thức khởi tạo cho các tác nhân RL sử dụng các LM đã được huấn luyện trước và một toán tử Bellman bảo thủ để giảm độ phức tạp huấn luyện. Hơn nữa, chúng tôi chứng minh cách tận dụng các hàm giá trị đã học như một chiến lược kép để hậu xử lý các chương trình được tạo ra. Các đánh giá thực nghiệm của chúng tôi chứng minh khả năng của B-Coder trong việc đạt được hiệu suất tối tân khi so sánh với các phương pháp dựa trên chính sách. Đáng chú ý, thành tựu này đạt được với nỗ lực thiết kế phần thưởng tối thiểu, nhấn mạnh hiệu quả của RL dựa trên giá trị, độc lập với thiết kế phần thưởng.

1 GIỚI THIỆU

Tổng hợp chương trình (hoặc tạo mã) nhằm tạo ra các chương trình thực thi chính xác về chức năng từ các đặc tả bài toán, chẳng hạn như các ví dụ đầu vào-đầu ra (IO), dựa trên ràng buộc hoặc mô tả ngôn ngữ tự nhiên, cùng với những phương pháp khác. Sự quan tâm ngày càng tăng đối với lĩnh vực này có thể được quy cho tiềm năng của nó trong việc biến đổi mô hình phát triển phần mềm. Đáng chú ý, các công cụ hỗ trợ AI đã cho thấy bằng chứng về việc thúc đẩy hiệu suất trong ngành công nghiệp phần mềm.

Các mô hình ngôn ngữ lớn (LLM) đã thu hút sự quan tâm đáng kể và cho thấy những thành tựu đáng chú ý. Lược đồ huấn luyện trước trên lượng dữ liệu khổng lồ đã mang lại những thành công đáng chú ý trong tạo ngôn ngữ tự nhiên. Xu hướng này mở rộng ảnh hưởng đến tổng hợp chương trình, nơi nhiều LLM mã chuyên biệt đã được giới thiệu để giải quyết các thách thức trong tổng hợp chương trình.

Không giống như nhiều nhiệm vụ tạo ngôn ngữ tự nhiên tự do, nơi chất lượng đầu ra của mô hình khó đánh giá, tính đúng đắn của các chương trình được tổng hợp có thể được xác minh thông qua thực thi tự động với các bài kiểm tra đơn vị được xác định trước. Điều này cho phép tối ưu hóa trực tiếp kết quả thực thi thông qua học tăng cường (RL), bằng cách xây dựng kết quả kiểm tra như tín hiệu phần thưởng. Cuộc thảo luận của chúng tôi tập trung vào các công trình RL gần đây đã đạt được những tiến bộ đáng chú ý trong tạo mã Python văn bản-thành-mã, được đánh giá trên các bảng xếp hạng thử thách có nguồn từ các cuộc thi lập trình Codeforces. Đáng chú ý, những công trình này chủ yếu ưa chuộng các thuật toán dựa trên chính sách on-policy.

Trong khi các phương pháp dựa trên chính sách (on-policy) được ưa chuộng trong các công trình tổng hợp chương trình hiện có, chúng được biết đến là không hiệu quả về mẫu do không thể sử dụng các mẫu off-policy. Ngược lại, các phương pháp dựa trên giá trị, sử dụng học chênh lệch thời gian, được biết đến là hiệu quả hơn về mẫu, vì chúng giải quyết một lặp điểm cố định không yêu cầu rõ ràng một phân phối dữ liệu cụ thể, do đó cung cấp khả năng tương thích tốt hơn với dữ liệu off-policy. Chúng tôi hoãn các giải thích kỹ thuật về dữ liệu on/off-policy và lý do cho hiệu quả khác nhau đến Phần 3.2, nơi chúng tôi đã chuẩn bị ký hiệu và định nghĩa.

Trong tổng hợp chương trình, các nguồn dữ liệu off-policy chính bao gồm các chương trình của con người và các chương trình được tổng hợp trước đó. Cả hai đều là off-policy vì chúng không tuân theo phân phối chuỗi được tạo ra bởi mô hình hiện tại. Các công trình tổng hợp chương trình hiện tại thường trực tiếp sử dụng các mẫu off-policy với các phương pháp on-policy. Không ngạc nhiên, Shojaee và cộng sự (2023) nhận thấy rằng việc tăng các chương trình tổng hợp off-policy có thể làm giảm hiệu suất. Điều này xảy ra vì dữ liệu off-policy dẫn đến các ước lượng gradient có thiên lệch. Lý tưởng, một mục tiêu nên là nâng cao hoặc ít nhất duy trì hiệu suất khi khối lượng dữ liệu tăng lên.

Tóm lại, các lý do gợi ý sự phù hợp tự nhiên cho các phương pháp dựa trên giá trị trong tổng hợp chương trình là hai mặt: khả năng có sẵn phần thưởng (không tốn kém), tương tự như các nhiệm vụ RL cổ điển như GO và Atari; và khả năng tương thích nguyên tắc với dữ liệu off-policy để tận dụng hiệu quả dữ liệu con người và dữ liệu lịch sử. Tuy nhiên, RL dựa trên giá trị đối mặt với những thách thức như khó hội tụ trong không gian trạng thái-hành động lớn. Để giải quyết điều này, chúng tôi giới thiệu B-Coder (Bellman coder), với ba đóng góp của chúng tôi:

• Chúng tôi ổn định RL dựa trên giá trị cho tổng hợp chương trình bằng cách đề xuất một giao thức khởi tạo cho các hàm Q và một toán tử Bellman bảo thủ để giảm thiểu độ phức tạp huấn luyện.
• Chúng tôi chứng minh cách tận dụng các hàm giá trị như một chiến lược kép để cải thiện việc tạo ra.
• B-Coder đạt được hiệu suất thực nghiệm mạnh mẽ với thiết kế phần thưởng tối thiểu, cung cấp thêm góc nhìn về thiết kế thuật toán RL độc lập với thiết kế hàm phần thưởng.

Cấu trúc bài báo. Chúng tôi giới thiệu các công trình liên quan và ký hiệu trong Phần 2 và 3. Phần 4 trình bày chi tiết phương pháp của chúng tôi và lý do đằng sau các lựa chọn thiết kế. Cụ thể, các Phần 4.1, 4.2, và 4.3 giải quyết các thách thức của việc huấn luyện hàm giá trị bằng cách: tận dụng cấu trúc nhiệm vụ, cung cấp khởi tạo hàm Q hiệu quả, và một toán tử bảo thủ cho các cập nhật ổn định nhưng ít tham vọng hơn, tương ứng. Phần 4.5 cho thấy một lợi ích bổ sung của các hàm giá trị, và Phần 5 cho thấy kết quả thực nghiệm của chúng tôi.

2 CÁC CÔNG TRÌNH LIÊN QUAN

Tổng hợp chương trình được hướng dẫn bởi thực thi. Tính khả thi của việc xác minh các chương trình thông qua kết quả trường hợp kiểm tra đã dẫn đến dòng các công trình được hướng dẫn bởi thực thi. Trong khi những nỗ lực này tận dụng phản hồi thực thi, chúng không tối ưu hóa trực tiếp hướng tới tỷ lệ thành công thực thi cao hơn do tính không khả vi vốn có của kết quả thực thi.

RL cho mô hình hóa chuỗi tổng quát. Huấn luyện LM có giám sát, sử dụng dự đoán token tiếp theo (NTP) hoặc mô hình hóa ngôn ngữ có mặt nạ, có những hạn chế được công nhận. Một vấn đề nổi bật là thiên lệch phơi bày: cho rằng việc huấn luyện được thực hiện theo cách "teacher-forcing", các lỗi có xu hướng tích lũy trong quá trình kiểm tra do tạo ra tự hồi quy. Ngược lại, các công trình trước đây đã chứng minh hiệu quả của RL trong việc giải quyết thiên lệch phơi bày và tối ưu hóa các chỉ số không khả vi, ví dụ BLEU và ROUGE, bằng cách tận dụng chấm điểm tự động như hàm phần thưởng.

RL cho tổng hợp chương trình. Các mất mát có giám sát cũng không đáp ứng được khi đánh giá độ chính xác chức năng của các chương trình được tổng hợp. Do vậy, việc chỉ dựa vào học có giám sát cho tổng hợp chương trình không lý tưởng. Vì RL cung cấp một con đường để tối ưu hóa trực tiếp các mục tiêu không khả vi, nhiều công trình đã nghiên cứu việc nâng cao tạo mã thông qua RL. Đối với các công trình liên quan nhất với chúng tôi: CodeRL thích nghi REINFORCE, một thuật toán gradient chính sách (PG) cổ điển, cùng với thủ thuật baseline để giảm phương sai và một mô hình phần thưởng được huấn luyện có giám sát để giảm thiểu vấn đề tín hiệu thực thi thưa thớt. Ngoài ra, họ đề xuất một chiến lược lấy mẫu critic để tinh chỉnh và sửa chữa chương trình dựa trên phản hồi kiểm tra đơn vị ví dụ.

PPOCoder áp dụng gradient chính sách gần (PPO) để tinh chỉnh các LM đã được huấn luyện trước. Ngoài ra, họ tận dụng cấu trúc cú pháp và ngữ nghĩa của mã, chẳng hạn như cây cú pháp và đồ thị luồng dữ liệu, để cải thiện thiết kế hàm phần thưởng. RLTF đề xuất một khung huấn luyện trực tuyến cho tổng hợp chương trình sử dụng gradient chính sách với phần thưởng chi tiết được thiết kế theo kinh nghiệm.

Thảo luận bổ sung. Phụ lục D liệt kê một số ứng dụng RL, cho thấy sự tương tự giữa tổng hợp chương trình và các nhiệm vụ được hưởng lợi từ các phương pháp dựa trên giá trị. Trong C, chúng tôi mở rộng thảo luận về các công trình mở rộng các phương pháp dựa trên chính sách sang một thiết lập off-policy. Những nỗ lực như vậy thường liên quan đến việc huấn luyện một hàm giá trị, nhấn mạnh thêm động lực của chúng tôi để bắt đầu với các phương pháp dựa trên giá trị.

3 SỐ LIỆU BAN ĐẦU

Người ta có thể xây dựng nhiệm vụ tổng hợp chương trình như một nhiệm vụ tạo chuỗi-thành-chuỗi, trong đó một mô hình nhận một mô tả bài toán D làm đầu vào và xuất ra một chương trình Ŵ nhằm đạt được chức năng được chỉ định bởi D. Một chương trình được tạo ra Ŵ = (ŵ₀, ..., ŵₜ) được cấu thành bởi một chuỗi các token ŵₜ ∈ V. Để ngắn gọn, chúng tôi sử dụng hằng số T để biểu thị độ dài chuỗi mặc dù nó có thể là một biến trong thực tế, và W để biểu thị một chương trình nói chung (cả được tạo ra và ground truth).

Gọi LM là một thể hiện của LM, ℓ((w<t, D), ·) là đầu ra lớp logits (đầu mô hình hóa ngôn ngữ), và p(·|w<t, D) là phân phối xác suất trên từ vựng V (được tính bằng cách truyền ℓ(·,·) qua softmax), được điều kiện trên một chuỗi w<t và D. Giả sử W* là một chương trình ground truth và Dtrain là tập huấn luyện, theo thông lệ các LM có thể được huấn luyện bằng cách giảm thiểu mất mát cross-entropy.

3.1 KÝ HIỆU RL

Để làm cho ký hiệu dễ diễn giải hơn, chúng tôi kết nối ký hiệu tổng hợp chương trình với ký hiệu RL tiêu chuẩn. Các bài toán RL thường được xây dựng như Quá trình Quyết định Markov (MDP) và một MDP M thường được cấu thành bởi một bộ 5 M=(S,A,P,r,γ) lần lượt là không gian trạng thái, không gian hành động, hàm chuyển đổi, hàm phần thưởng và hệ số giảm giá. Hệ số giảm giá γ giảm giá trị tương lai để nhấn mạnh tương lai gần, và chúng tôi sử dụng γ=0.999 (ưa chuộng nhẹ giải pháp ngắn gọn hơn).

Một hàm chuyển đổi (ngẫu nhiên) P: S × A → Δ(S) là một phân phối trên S được điều kiện trên một cặp trạng thái-hành động (s, a). Trong tổng hợp chương trình, P là tầm thường vì st+1 ≡ st ∘ at, trong đó ∘ biểu thị nối chuỗi.

Trạng thái và hành động. Trong bối cảnh tạo mã, một hành động at là một token ŵt. Do đó không gian hành động A là từ vựng V. Vì thông tin được sử dụng để tạo token ŵt là (ŵ<t, D), trạng thái do đó được định nghĩa là st := (ŵ<t, D). Đối với một D cho trước, không gian trạng thái S = V^T. Để ngắn gọn, chúng tôi sẽ chủ yếu sử dụng st, at thay vì ký hiệu wt, và đôi khi bỏ qua chỉ số thời gian t nếu nó không dẫn đến nhầm lẫn. Chúng tôi cũng sẽ sử dụng s', a' để biểu thị st+1, at+1 khi chỉ vị trí thời gian tương đối quan trọng.

Chính sách. Một chính sách π: S → Δ(A) gán một phân phối hành động Δ(A) cho bất kỳ trạng thái s ∈ S nào, có nghĩa là dự đoán một token ŵt dựa trên chuỗi hiện tại ŵ<t và đặc tả bài toán D. Các công trình trước thường định nghĩa πθ ≡ pθ và tối ưu hóa trực tiếp các tham số LM θ với các phương pháp PG. Tuy nhiên chúng tôi định nghĩa π := f(θ, □) là một hàm của θ và các thành phần khác □, xem chi tiết trong Phần 4.

Hàm phần thưởng. Một hàm phần thưởng r: S × A → R xác định phần thưởng khi thực hiện hành động at tại trạng thái st. Chúng tôi tuân theo thiết kế phần thưởng của Le và cộng sự (2022) trong phương trình 2. Chúng tôi cũng có thể sử dụng ký hiệu viết tắt rt := r(st, at). Lưu ý rằng phần thưởng được xác định khi chương trình W hoàn thành tại T. Do đó rt = 0 nếu t ≠ T nếu không được định nghĩa như phương trình 2.

r(W) = r(sT, aT) = {
+1.0, nếu W vượt qua tất cả kiểm tra đơn vị
-0.3, nếu W thất bại bất kỳ kiểm tra đơn vị nào
-0.6, nếu W không thể được thực thi
-1.0, nếu W không thể được biên dịch
}

Các hàm giá trị. RL tối đa hóa lợi nhuận có giảm giá, J(π) = E[∑t γ^t rt | π, M]. Hàm giá trị trạng thái-hành động Q^π: S × A → R và hàm giá trị trạng thái V^π: S → R, được định nghĩa đệ quy như:

V^π(s) := E[∑∞t=0 γ^t rt | π, M, S0 = s]
= Ea∼π(·|s),s'∼P(·|s,a)[r(s, a) + γV^π(s')]

Q^π(s, a) := E[∑∞t=0 γ^t rt | π, M, S0 = s, A0 = a]
= Es'∼P(·|s,a)[r(s, a) + γQ^π(s', π)]

trong đó Q(s, π) := Ea∼π Q(s, a). Ngoài ra, hàm ưu thế là A^π(s, a) := Q^π(s, a) - V^π(s).

3.2 RL DỰA TRÊN GIÁ TRỊ VÀ DUELING DQN

Các thuật toán dựa trên giá trị đặc biệt là họ Q-learning đã đạt được những thành công đáng chú ý. Một khung chính tắc của họ Q-learning lặp giữa đánh giá chính sách và cải thiện chính sách:

đánh giá chính sách (PE): Qk = arg min Q ED[(Qk-1(s, a) - (r + γQk-1(s', πk-1)))²]
cải thiện chính sách (PI): πk = arg max π Qk(s, π(s))

trong đó D là một tập dữ liệu tùy ý, bước PE ước lượng chính sách trước πk-1 sử dụng phương trình Bellman, và bước PI tìm một πk được cải thiện bằng cách tối đa hóa các ước lượng Qk.

Cụ thể, chúng tôi xây dựng khung của mình trên đỉnh Dueling DQN (DDQN). Tóm lại, DDQN xấp xỉ V(s) và A(s, a) với các đầu riêng biệt, và chạy các bước cải thiện và đánh giá với Q(s, a) = V(s) + A(s, a). Sự phân chia này cho phép ước lượng mạnh mẽ V(s) mà không gây nhầm lẫn với các hành động, điều này sau đó đảm bảo việc học ổn định A(s, a) cho rằng nó chỉ tập trung vào các giá trị tương đối. Kết quả là, DDQN thường thể hiện sự ổn định tăng cường trong động lực huấn luyện và cải thiện khái quát hóa. Ngoài những lợi thế đã đề cập trước đó, DDQN cho phép chúng tôi tận dụng một cấu trúc nhiệm vụ mà các chương trình ground truth nên đạt được những ưu thế cao nhất, do đó giảm không gian tìm kiếm, điều mà chúng tôi sẽ trình bày chi tiết trong Phần 4.1.

Nhận xét về hiệu quả mẫu. Chúng tôi minh họa sự không hiệu quả của các phương pháp dựa trên chính sách bằng cách sử dụng PG vanilla làm ví dụ. PG tối đa hóa J(μ) := E[∑t γ^t rt | πμ, M] ≡ EW∼πμ[∑t γ^t rt], với gradient ∇μJ(μ) được tính sử dụng định lý gradient chính sách. Phương pháp này yêu cầu dữ liệu huấn luyện W được rút ra từ phân phối được cảm ứng bởi chính sách hiện tại πμ, do đó được gọi là on-policy. Do đó, về nguyên tắc người ta nên tạo ra dữ liệu mới và loại bỏ dữ liệu lịch sử tại mỗi lần cập nhật, dẫn đến sự không hiệu quả mẫu không mong muốn. Ngược lại, đánh giá chính sách như trong phương trình 5 hoạt động với tập dữ liệu tùy ý D.

4 THIẾT KẾ THUẬT TOÁN - TĂNG TỐC HUẤN LUYỆN DỰA TRÊN GIÁ TRỊ

Trong khi RL dựa trên giá trị có triển vọng lớn, việc huấn luyện của nó có thể thách thức do không gian hành động lớn A = V và không gian trạng thái chiều cao S = V^T. Điều này dẫn đến một bảng Q đặc biệt lớn có kích thước O(|V|^T). Và cardinality của không gian chính sách là |A|^|S| = O(|V|^|V|^T), tăng theo cấp số nhân kép. Cả thách thức từ không gian hành động lớn và không gian trạng thái chiều cao đều là các chủ đề nghiên cứu quan trọng trong RL. Các thách thức không gian hành động được thảo luận bởi ví dụ Dulac-Arnold và cộng sự (2015); Tavakoli và cộng sự (2018); Kalashnikov và cộng sự (2018), trong khi He và cộng sự (2016); Nair và cộng sự (2018), cùng những người khác, xem xét độ phức tạp không gian trạng thái. Đặc biệt, Silver (2015); Duan và cộng sự (2016) nhận xét về sự ổn định huấn luyện có thể tốt hơn của các phương pháp dựa trên chính sách trong những tình huống này.

Để giải quyết các thách thức vốn có trong việc huấn luyện RL dựa trên giá trị cho LM, ở mức độ cao, chúng tôi đã phát triển B-Coder xem xét ba khía cạnh chính: kết hợp cấu trúc nhiệm vụ, khởi tạo hàm Q, và sao lưu sử dụng một toán tử Bellman bảo thủ. Hình 1 xem trước hiệu quả của các thiết kế thuật toán của chúng tôi, cho thấy đường cong huấn luyện của các thuật toán RL dựa trên giá trị khác nhau trên tập dữ liệu APPS. Do những thách thức đã đề cập, hiệu suất của DDQN vanilla liên tục giảm ngay cả khi được đánh giá trên tập huấn luyện. Ngược lại, cả việc khởi tạo hàm Q và toán tử Bellman bảo thủ đều cho thấy lợi ích trong việc ổn định và tăng tốc quá trình huấn luyện.

Để thuận tiện ký hiệu trong các phần tiếp theo, chúng tôi bắt đầu với tổng quan về ký hiệu và tham số hóa của chúng tôi, được tóm tắt trong Hình 2. Hình 2(a) biểu thị một LM mã hóa-giải mã đã được huấn luyện trước được tham số hóa bởi θ_ckpt (trong đó chỉ số dưới ckpt biểu thị thực tế rằng đó là một checkpoint/hằng số). Hình 2(b) và (c) cho thấy các đồ thị chuyển tiếp của hai giai đoạn huấn luyện khác nhau của chúng tôi: (b) tương ứng với giai đoạn huấn luyện trước cho φ, để cung cấp một khởi tạo tốt cho (c) việc tinh chỉnh tiếp theo của θ. Động lực và chi tiết được hoãn lại đến Phần 4.2 và 4.3, tương ứng. Khi chúng tôi tiến tới lý do đằng sau các thiết kế của mình, khuyến khích duy trì sự quen thuộc với θ_ckpt, φ, θ và các sản phẩm tương ứng của chúng, đặc biệt là các đường dẫn chuyển tiếp đến Q_φ và Q_θ, để ngăn ngừa nhầm lẫn trong các phần tiếp theo.

4.1 TẬN DỤNG CẤU TRÚC NHIỆM VỤ

Như đã lưu ý trước đó, một thuộc tính chính của nhiệm vụ tổng hợp chương trình là việc cung cấp các giải pháp con người, được đảm bảo là đúng. Kết quả là, những giải pháp này nên đạt được các giá trị Q cao nhất, ngay cả khi các giải pháp đúng có thể không duy nhất. Do vậy, đối với một chương trình ground truth W* = (s*₀, a*₀, ..., s*ₜ, a*ₜ), Q(s*ₜ, a*ₜ) ≥ Q(s*ₜ, a) đúng cho tất cả a ∈ V, do đó A(s*ₜ, a*ₜ) ≥ A(s*ₜ, a).

Để thực thi cấu trúc này, người ta có thể đảm bảo A(W) ≤ 0 và A(W*) ≈ 0, trong đó chúng tôi lạm dụng ký hiệu và bằng cách để A(W) := ∑ᵀₜ₌₀ A(sₜ, aₜ). Nó đảm bảo rằng W* có những ưu thế gần như cao nhất. Để thực hiện điều này, giả sử g(·) là một mạng neural tổng quát, chúng tôi phân tách Q như sau:

Q(s, a) = g(s, a) - max_a g(s, a) + V(s) = A(s, a) + V(s)
         |___________________|
         ưu thế không dương

Nó thực thi điều kiện đầu tiên của chúng tôi rằng A(W) ≤ 0. Đối với điều kiện thứ hai A(W*) ≈ 0, chúng tôi tối ưu hóa một hàm ưu thế A bằng cách giảm thiểu một hàm mất mát ưu thế phụ trợ, cụ thể là L_adv:

L_adv(A) = E_(s*₀,a*₀,...,s*ₜ,a*ₜ)∼D_train [∑ᵀₜ₌₀ |A(s*ₜ, a*ₜ)|]

Chúng tôi cũng giới hạn hàm Q với R_max = 1, tổng phần thưởng tối đa. Xem Phụ lục G để biết chi tiết.

4.2 KHỞI TẠO HÀM Q

Mặc dù có các cấu trúc nhiệm vụ đã giới thiệu, việc huấn luyện hàm Q từ đầu vẫn cực kỳ thách thức. Trong khi điều này không phải là vấn đề đối với việc học dựa trên chính sách (cho rằng trực tiếp tinh chỉnh các LM đã được huấn luyện trước mà không yêu cầu hàm Q), nó đặt ra những thách thức đáng kể trong các phương pháp dựa trên giá trị vì người ta thường không có một hàm Q đã được huấn luyện trước. Để thực hiện điều này, chúng tôi cho thấy rằng người ta có thể khởi tạo một hàm Q từ đầu ra logits ℓ(·,·) của một LM đã được huấn luyện trước.

Khởi tạo Q thông qua các mô hình đã được huấn luyện trước. Yu & Zhang (2023) xem xét việc tinh chỉnh các tác nhân RL sau huấn luyện trước RL offline. Ý tưởng chính của họ là tái cấu trúc một hàm Q từ chính sách đã được huấn luyện trước, để tinh chỉnh. Lấy cảm hứng từ cách tiếp cận của họ, người ta có thể tương tự tái cấu trúc/khởi tạo một hàm Q sử dụng một LM đã được huấn luyện trước, giống như sử dụng một chính sách đã được huấn luyện trước.

Việc khởi tạo này được thúc đẩy bởi dòng công trình chính sách dựa trên năng lượng, trong đó một chính sách π là sản phẩm của việc truyền một hàm Q qua một hàm chuyển đổi softmax. Tương tự, trong LM, p - phân phối trên V - được tạo ra bằng cách truyền logits ℓ qua softmax.

mô hình hóa ngôn ngữ: p(a|s) = exp(ℓ(s, a)) / ∑_{a∈A} exp(ℓ(s, a))
π dựa trên năng lượng: π(a|s) = exp(1/α Q(s, a)) / ∑_{a∈A} exp(1/α Q(s, a))

trong đó α là một siêu tham số nhiệt độ. Người ta có thể tự nhiên đặt Q(s, a) = αℓ(s, a) để khởi tạo. Do đó, với cấu trúc dueling đã đề cập trong phương trình 7 và tham số hóa được định nghĩa trước của chúng tôi, người ta có thể đặt hàm ưu thế là A_{θ_ckpt}(s, a) := α[ℓ_{θ_ckpt}(s, a) - max_a ℓ_{θ_ckpt}(s, a)], dẫn đến Q_φ(s, a) := A_{θ_ckpt}(s, a) + V_φ(s). Xem cũng đồ thị lượt truyền được định nghĩa trong Hình 2b. Tóm lại, hàm Q_φ này tạo ra một chính sách π_φ giống hệt với phân phối đầu ra p_{θ_ckpt} của LM_{θ_ckpt}:

π_φ(a|s) = softmax[1/α Q_φ(s)][a] = softmax[ℓ_{θ_ckpt}(s) - max_a ℓ_{θ_ckpt}(s, a) + 1/α V_φ(s)][a] = p_{θ_ckpt}(a|s)

trong đó Q(s) := [Q(s, a)]_{a∈A} và ℓ(s) := [ℓ(s, a)]_{a∈A}.

Nhớ lại phương trình 5-6, họ Q-learning có thể được xem như các lặp giữa đánh giá chính sách và cải thiện. Bây giờ chúng tôi trình bày chi tiết về cách việc khởi tạo hàm Q_φ này ảnh hưởng đến cả hai bước.

Cải thiện chính sách. Người ta có thể, một cách không chính thức, xem xét thao tác lấy softmax đối với 1/α Q_φ như một bước cải thiện chính sách mềm (Haarnoja và cộng sự, 2018) với nhiệt độ α. Do đó, phương trình 11 có thể được diễn giải như: chạy cải thiện chính sách mềm một mình với Q_φ được khởi tạo này đã bảo toàn hiệu suất của LM_{θ_ckpt} đã được huấn luyện trước, cung cấp một điểm khởi đầu tốt cho việc tinh chỉnh trực tuyến.

Đánh giá chính sách. Tuy nhiên, hàm Q_φ này chỉ nắm bắt các giá trị tương đối, vì chúng tôi chỉ khởi tạo các ưu thế A_{θ_ckpt} - thông tin tương đối - như được hiển thị trong phương trình 11. V_φ do đó có thể là một hàm tùy ý. Điều này sẽ không ảnh hưởng đến bước cải thiện chính sách do tính bất biến dịch chuyển của hàm softmax. Tuy nhiên, trong bước đánh giá chính sách, xem ví dụ phương trình 5, lỗi Bellman có thể bị ảnh hưởng nặng nề bởi các V-values. Khi V-values là nguồn lỗi chủ yếu, việc tối ưu hóa đánh giá chính sách có thể phần lớn được thúc đẩy bởi các V-values chỉ-trạng thái. Điều này có thể dẫn đến mất mát các giá trị hành động tương đối, mà chúng tôi dự định bảo toàn trong bước trước.

Huấn luyện trước V_φ. Điều này có thể được giải quyết bằng cách thêm một giai đoạn huấn luyện trước của V_φ(s), trong đó chúng tôi đóng băng hàm ưu thế A_{θ_ckpt} và huấn luyện V_φ bằng cách giảm thiểu lỗi chênh lệch thời gian (hoặc tương đương thực hiện đánh giá chính sách). Trong giai đoạn này, chúng tôi tối ưu hóa mất mát sau đây cho đến khi hội tụ:

L_V(V_φ; ℓ_{θ_ckpt}) = 1/T E_{(s_t,a_t,r_t,s_{t+1})∼D_train} ∑^T_{t=0} [r_t + γSG(Q_φ(s_{t+1}, â_{t+1})) - Q_φ(s_t, a_t)]²

trong đó SG là một toán tử dừng gradient, SG(Q_φ(s', â')) tuân theo tối ưu hóa bán-gradient tiêu chuẩn, â_{t+1} là một hành động mục tiêu (chi tiết được hoãn lại đến phần 4.3), và Q_φ(s, a) = A_{θ_ckpt}(s, a) + V_φ(s).

Tóm lại, các bước khởi tạo của chúng tôi đảm bảo rằng, trước khi tinh chỉnh θ, Q_φ của chúng tôi đáp ứng hai điều kiện quan trọng: nó bắt đầu với phân phối hành động p_{θ_ckpt} của một LM_{θ_ckpt} đã được huấn luyện trước, và nó bắt đầu với lỗi chênh lệch thời gian thấp (vì việc huấn luyện trước V_φ trong phương trình 12 trực tiếp giảm thiểu nó).

4.3 MỘT TOÁN TỬ BELLMAN BẢO THỦ

Với một hàm giá trị trạng thái V_φ đã được huấn luyện trước, bây giờ chúng tôi đã sẵn sàng học một hàm giá trị trạng thái-hành động tốt thông qua tinh chỉnh. Chúng tôi tham số hóa Q_θ(s, a) := A_θ(s, a) + V_θ(s) = α[ℓ_θ(s, a) - max_a ℓ_θ(s, a)] + V^r_θ + V_φ, trong đó chúng tôi định nghĩa V_θ = V^r_θ + V_φ, và chúng tôi khởi tạo θ theo cách sao cho ℓ_θ = ℓ_{θ_ckpt} và V^r_θ = 0. Nó đảm bảo rằng Q_θ = Q_φ khi khởi tạo, một điểm khởi đầu tốt cho việc tinh chỉnh tiếp theo trên θ. Về mặt kỹ thuật, việc đặt V_θ = V^r_θ + V_φ không bắt buộc, vì người ta có thể tinh chỉnh cả θ và φ. Tuy nhiên chúng tôi quan sát thấy rằng việc tinh chỉnh một đầu dư V^r_θ, với φ được đóng băng, dẫn đến sự ổn định tốt hơn.

Mặc dù chúng tôi tránh huấn luyện Q_θ từ đầu, việc tối ưu hóa Q_θ bằng các thuật toán họ Q-learning vẫn có thể thách thức. Chúng tôi quy điều này cho các đặc tính của toán tử tối ưu Bellman B* tìm cách học hàm giá trị tối ưu Q* và chính sách tối ưu π*, yêu cầu một phạm vi dữ liệu tốt của không gian trạng thái-hành động S × A (ví dụ Jiang & Huang, 2020; Xie và cộng sự, 2021a; Zhan và cộng sự, 2022). Trong tổng hợp chương trình, tuy nhiên, giả định như vậy khó có thể được đáp ứng do không gian trạng thái-hành động lớn và chi phí tính toán cao của suy luận Transformer. Trong khi họ Q-learning thông thường dựa vào toán tử B*, các công trình gần đây trong RL, đặc biệt là những công trình xem xét chế độ dữ liệu hạn chế (ví dụ Agarwal và cộng sự, 2020; Levine và cộng sự, 2020), thường thiết kế các toán tử "bảo thủ" (ví dụ Achiam và cộng sự, 2017; Kumar và cộng sự, 2020; Brandfonbrener và cộng sự, 2021) để giải quyết những khó khăn do B* gây ra.

Các toán tử Bellman bảo thủ. Khái niệm đằng sau các toán tử Bellman bảo thủ là "nhắm thấp". Thay vì học Q* và π* tối ưu, những toán tử này thường tìm cách học một chính sách π vượt qua một chính sách hành vi (được sử dụng để thu thập một tập dữ liệu RL trong tài liệu RL offline, xem ví dụ Achiam và cộng sự, 2017; Brandfonbrener và cộng sự, 2021) hoặc tinh chỉnh một chính sách có sẵn (ví dụ Xie và cộng sự, 2021b; Yu & Zhang, 2023). Điều này thường được đạt được bằng cách giới thiệu một regularizer phạt các sai lệch khỏi chính sách hành vi/có sẵn. Cụ thể, như được hiển thị trong phương trình 14, chúng tôi định nghĩa toán tử Bellman bảo thủ B_q của chúng tôi, phụ thuộc vào một chính sách cố định, được định nghĩa trước q, như sau:

tối ưu B: (B*Q)(s, a) = r(s, a) + γE_{s'}[Q(s', â')], trong đó â' = arg max_a Q(s', a)
bảo thủ B: (B_q Q)(s, a) = r(s, a) + γE_{s'}[Q(s', â')], trong đó â' = arg max_a q(a|s')

Trực giác đằng sau toán tử B_q của chúng tôi là chúng tôi đánh giá hàm giá trị hành động Q_q↑ của một chính sách greed-ified q↑(a|s) := 1{a = arg max_a q(a|s)}, trong đó 1 là hàm chỉ thị. Lý do đằng sau greedification là q↑ có thể được xem như q trong chế độ giải mã tham lam, thường có khả năng (một lần) tốt hơn chế độ lấy mẫu (mặc dù cái sau có tính đa dạng tạo ra tốt hơn). Xem xét việc đặt q = p_{θ_ckpt}, toán tử B_{p_{θ_ckpt}} tìm cách học một chính sách π vượt trội hơn p_{θ_ckpt}.

Chúng tôi tiếp tục nhận xét về một số tính chất của B_q: mệnh đề 4.1 cho thấy B_q là một co, có nghĩa là có một điểm cố định duy nhất. Nó dẫn đến mệnh đề 4.2, thúc đẩy sự phát triển của chúng tôi trong Phần 4.5.

Mệnh đề 4.1. B_q là γ-co trong chuẩn ℓ_∞.

Cho toán tử Bellman bảo thủ của chúng tôi, chúng tôi có thể định nghĩa mất mát chênh lệch thời gian bảo thủ của chúng tôi:

L_Q(Q_θ; q) = 1/T E_{(s_t,a_t,r_t,s_{t+1})∼D_train} ∑^T_{t=0} [r_t + γSG(Q_θ(s_{t+1}, â_{t+1})) - Q_θ(s_t, a_t)]²

trong đó â_{t+1} = arg max_a q(a|s_{t+1}), và Q_θ(s, a) = α[ℓ_θ(s, a) - max_a ℓ_θ(s, a)] + V^r_θ(s) + V_φ(s).

4.4 TRIỂN KHAI VÀ TỐI ỨU

Tóm tắt kiến trúc và tham số hóa. Theo Le và cộng sự (2022); Shojaee và cộng sự (2023); Liu và cộng sự (2023), chúng tôi chọn T5 làm kiến trúc cơ sở cho θ_ckpt, φ và θ; và θ_ckpt được khởi tạo với checkpoint CodeRL có sẵn công khai. Cụ thể, θ_ckpt, φ và θ chia sẻ cùng một bộ mã hóa, và bộ mã hóa được đóng băng trong suốt quá trình, để giảm lượng tham số có thể học.

Huấn luyện hai giai đoạn. Như đã lưu ý trước đó, việc huấn luyện của chúng tôi bao gồm hai giai đoạn: một giai đoạn huấn luyện trước của φ, cụ thể là giai đoạn φ, và một giai đoạn tinh chỉnh của θ, cụ thể là giai đoạn θ. Một thuật toán giả có thể được tìm thấy trong Phụ lục A. Ngoài ra, chi tiết triển khai thêm được hoãn lại đến Phụ lục H.

Giai đoạn φ: Cho sự phát triển của chúng tôi trong Phần 4.2, chúng tôi huấn luyện trước hàm V_φ sử dụng gradient descent ngẫu nhiên với ∇_φ L_V(V_φ; ℓ_{θ_ckpt}), với L_V được định nghĩa trong phương trình 12.

Giai đoạn θ (tinh chỉnh): Trong giai đoạn này, chúng tôi tìm cách tối ưu hóa Q_θ để giảm thiểu các mất mát đã phát triển trước đó của chúng tôi: L_adv và L_Q, như được định nghĩa trong phương trình 8 và 15, tương ứng. Ngoài ra, cũng là một thực tế phổ biến để bao gồm một mất mát cross-entropy L_ce trong quá trình tinh chỉnh. Do đó, chúng tôi kết luận hàm mất mát cuối cùng của chúng tôi như phương trình 17, và θ được cập nhật sử dụng gradient descent ngẫu nhiên với ∇_θ L_ft(Q_θ; p_{θ_ckpt}).

Nhớ lại: Q_θ(s, a) = A_θ(s, a) + V_θ(s) = α(ℓ_θ(s, a) - max_a ℓ_θ(s, a)) + V^r_θ(s) + V_φ(s)

L_ft(Q_θ; p_{θ_ckpt}) = L_Q(Q_θ; p_{θ_ckpt}) + β_adv L_adv(A_θ) + β_ce L_ce(π_θ), trong đó π_θ = softmax(1/α Q_θ)

4.5 MỘT MÔ HÌNH PHẦN THƯỞNG MIỄN PHÍ

Mô hình hóa phần thưởng là quan trọng trong mô hình hóa ngôn ngữ và cũng trong RL nghịch đảo (các thảo luận chi tiết có thể được tìm thấy trong Phụ lục C). Một phát hiện hấp dẫn từ IRL, áp dụng cho khung của chúng tôi, là một hàm Q được huấn luyện có thể khôi phục một hàm phần thưởng mà không cần huấn luyện bổ sung. Tương tự như Garg và cộng sự (2021), một sự tương ứng một-một giữa Q và phần thưởng tồn tại với toán tử Bellman bảo thủ B_q của chúng tôi. Chúng tôi định nghĩa toán tử Bellman bảo thủ nghịch đảo T_q: R^{S×A} → R^{S×A}:

(T_q Q)(s, a) = Q(s, a) - γE_{s'} Q(s', arg max_a q(a|s'))

Mệnh đề 4.2. Toán tử Bellman bảo thủ nghịch đảo T_q là một song ánh.

Mệnh đề 4.2 cho thấy rằng một Q_θ tương ứng duy nhất với một hàm phần thưởng r̃_θ := T_q Q_θ. Cho định nghĩa của T_q, chúng tôi có thể khôi phục một mô hình phần thưởng r̃_θ với Q_θ mà không cần huấn luyện bổ sung:

r̃_θ(s, a) = Q_θ(s, a) - γE_{s'} Q_θ(s', arg max_a p_{θ_ckpt}(a|s')) ≈ Q_θ(s, a) - γV_θ(s')

Chúng tôi sử dụng ước lượng r̃_θ(s, a) ≈ Q_θ(s, a) - γV_θ(s') trong thực tế, với lý do được hoãn lại đến Phụ lục F.

Lựa chọn ứng viên với r̃_θ. Chúng tôi tận dụng mô hình phần thưởng r̃_θ của chúng tôi để thực hiện lựa chọn chương trình ứng viên, như một ví dụ để làm nổi bật những lợi ích bổ sung của RL dựa trên giá trị. Chúng tôi xếp hạng các chương trình được tạo ra bởi phần thưởng tích lũy R̃_θ(W) := ∑^T_{t=0} r̃_θ(s_t, a_t), được dự đoán bởi mô hình phần thưởng r̃_θ của chúng tôi, để chọn các chương trình có khả năng đúng nhất. Cụ thể, đối với các chỉ số pass@k, chúng tôi tuân theo giao thức đánh giá được sử dụng trong CodeT, một công trình xem xét lựa chọn chương trình thông qua các kiểm tra được tạo tự động. Giao thức này tính pass@k bằng cách đầu tiên tạo ra m chương trình và chọn một tập con k chương trình để đánh giá pass@k. Trong trường hợp của chúng tôi, chúng tôi chọn tập con kích thước k với R̃_θ(·) cao nhất top-k từ tổng m ứng viên. Kết quả của chúng tôi trong Phần 5 tuân theo giao thức đánh giá này.

Nhận xét về r̃_θ. Để giải thích thêm động lực của việc xếp hạng với r̃_θ, xem xét một thiết lập triển khai thực tế nơi một mô hình được tinh chỉnh được triển khai cho các ứng dụng người dùng cuối. Người dùng thường cung cấp một mô tả ngôn ngữ về nhu cầu của họ nhưng có thể không bao gồm các trường hợp kiểm tra (điều này cũng có thể thách thức đối với người mới bắt đầu hoặc người dùng thông thường). Ngoài ra, mô hình thường được yêu cầu cung cấp một phản hồi tốt nhất duy nhất thay vì một loạt các tùy chọn. Do đó, khả năng xếp hạng các chương trình mà không có phần thưởng thực là một lợi thế mong muốn.

Để xem trước hiệu quả của r̃_θ, chúng tôi cho thấy mối tương quan giữa phần thưởng môi trường r và phần thưởng tích lũy R̃_θ của chúng tôi. Trong Hình 3, vùng xanh lá cây tương ứng với các chương trình đúng, và có R̃_θ trung bình cao nhất. Đối với các chương trình không đúng, những chương trình có lỗi biên dịch và runtime có R̃_θ thấp nhất và thấp thứ hai, tương ứng. Các chương trình có thể được thực thi nhưng thất bại một số kiểm tra, có R̃_θ cao thứ hai. Do đó, nó kết luận rằng R̃_θ có một mối tương quan dương rõ ràng với phần thưởng thực r.

5 ĐÁNH GIÁ THỰC NGHIỆM

Lấy mẫu sử dụng Q_θ. Lấy mẫu Nucleus (lấy mẫu top-p) với nhiệt độ lấy mẫu đã là một trong những kỹ thuật lấy mẫu quan trọng nhất. Nó cũng có thể được triển khai dễ dàng trong khung của chúng tôi. Người ta có thể đơn giản xem Q_θ/α như logits và quy trình lấy mẫu sẽ giữ nguyên giống hệt với các LM tiêu chuẩn, xem Phụ lục B để biết chi tiết.

Bảng xếp hạng APPS và baseline. Phù hợp với các công trình dựa trên RL trước đây, chúng tôi đánh giá B-Coder trên bảng xếp hạng cuộc thi mã thách thức APPS. Nó chứa 5.000 bài toán huấn luyện và 5.000 bài toán kiểm tra, với ba cấp độ khó: giới thiệu, phỏng vấn và thi đấu. Chúng tôi so sánh B-Coder của chúng tôi với các baseline LLM được huấn luyện trước hoặc tinh chỉnh có giám sát: GPT2, GPT3, GPT-Neo, GPT-J, Codex và AlphaCode; và các baseline được tinh chỉnh RL: CodeRL, PPOCoder và một công trình đồng thời RLTF.

APPS: không có kết quả kiểm tra ví dụ. Trong tập dữ liệu APPS, mỗi bài toán có một số kiểm tra đơn vị ví dụ (khác với các kiểm tra đơn vị ẩn được sử dụng để đánh giá). Những kiểm tra ví dụ này thường được tận dụng để tinh chỉnh các mẫu được tạo ra. Ví dụ, CodeRL và RLTF xem xét một chiến lược lấy mẫu critic (CS) tinh chỉnh và sửa chữa các chương trình được tạo ra dựa trên kết quả thực thi của các kiểm tra ví dụ. Chúng tôi bắt đầu với kết quả thực nghiệm trong đó kết quả kiểm tra ví dụ không được sử dụng (do đó kết quả CodeRL và RLTF trong Bảng 1 là không có CS). Bảng 1 cho thấy rằng B-Coder của chúng tôi có tổng thể pass@k tốt nhất cho k={1,5} và đạt vị trí thứ hai tốt nhất cho k=1000 (kết quả tốt nhất được báo cáo bởi công trình đồng thời RLTF). Đối với kết quả Bảng 1, chúng tôi sử dụng lấy mẫu nucleus với nhiệt độ lấy mẫu 0.6. Chúng tôi đặt m thành 256 cho k={1,5} và m thành 2500 cho k=1000, trong đó m là một siêu tham số của giao thức xếp hạng của chúng tôi được giới thiệu trong Phần 4.5 (xem Phụ lục I cho một nghiên cứu ablation về m).

APPS: sử dụng kết quả kiểm tra ví dụ. Bảng 2 liệt kê các kết quả sử dụng kiểm tra ví dụ. Ngoài chiến lược CS sử dụng kiểm tra ví dụ để tinh chỉnh/sửa chữa chương trình, Li và cộng sự (2022) và Chen và cộng sự (2021a) xem xét một thiết lập được lọc, trong đó các chương trình thất bại kiểm tra ví dụ được loại trừ, và pass@k được đánh giá sử dụng (một tập con của) các chương trình vượt qua kiểm tra ví dụ (điều này cũng liên quan đến chỉ số k@m, tỷ lệ pass sử dụng k submission từ m mẫu). Chúng tôi cũng kiểm tra B-Coder trong thiết lập được lọc này. Tương tự, chúng tôi đầu tiên loại trừ các chương trình thất bại kiểm tra ví dụ. Giả sử n trong số m chương trình vượt qua; sau đó chúng tôi tuân theo giao thức xếp hạng của chúng tôi để lấy top-k trong số n chương trình để đánh giá. B-Coder vượt trội hơn baseline với thiết lập CS hoặc được lọc cho k={1,5}. Baseline, CodeRL+CS+filtered, kết hợp cả hai chiến lược đạt được một lợi thế nhỏ so với B-Coder cho pass@5 trong khi bị vượt qua bởi B-Coder cho pass@1. Đáng đề cập rằng CS là một thành phần plug-and-play, cũng có thể được kết hợp với B-Coder, để cải thiện thêm tỷ lệ pass. Đối với kết quả trong Bảng 2, chúng tôi sử dụng nhiệt độ 0.4 và m được đặt thành 1000, phù hợp với m được sử dụng trong Le và cộng sự (2022).

Khả năng khái quát hóa. Ngoài ra, chúng tôi kiểm tra khả năng khái quát hóa của chiến lược kép của chúng tôi, xếp hạng với R̃_θ. Chúng tôi nghiên cứu hai khía cạnh: khái quát hóa cho các mô hình khác và khái quát hóa cho các miền khác nhau. Để thực hiện điều này, chúng tôi thiết kế các thí nghiệm sau, xác nhận khả năng khái quát hóa của nó một cách tích cực.

Đối với cái trước, chúng tôi tạo ra các chương trình (off-policy) sử dụng CodeRL (với m=256), và xếp hạng những chương trình đó bằng R̃_θ. Bảng 3 cho thấy chiến lược xếp hạng của chúng tôi dẫn đến cải thiện trong hầu hết các trường hợp, ngay cả khi các chương trình được xếp hạng không được tạo ra bởi B-Coder.

Đối với cái sau, chúng tôi kiểm tra chiến lược kép của chúng tôi với một tập dữ liệu khác MBPP (với m=512). Bảng 4 cho thấy cải thiện nhất quán cho tất cả nhiệt độ và k.

6 KẾT LUẬN

Trong công trình này, chúng tôi khám phá tính khả thi của các thuật toán RL dựa trên giá trị cho nhiệm vụ tổng hợp chương trình. Chúng tôi chứng minh cách ổn định và tăng tốc huấn luyện thông qua khởi tạo hàm Q và cập nhật bảo thủ. Hơn nữa, công trình của chúng tôi được thực hiện với nỗ lực thiết kế phần thưởng tối thiểu, do đó đặt trọng tâm vào góc nhìn thiết kế thuật toán. Trong khi các thuật toán dựa trên chính sách vẫn là xu hướng chính trong tài liệu tổng hợp chương trình hiện tại, câu hỏi về cách tận dụng hiệu quả các chương trình off-policy, bao gồm các mẫu tổng hợp lịch sử, theo cách có nguyên tắc, có thể vẫn chưa được khám phá đầy đủ. Chúng tôi tin chắc rằng RL dựa trên giá trị cung cấp một hướng đi đầy hứa hẹn để giải quyết câu hỏi này, và do đó để mở rộng RL cho tạo mã quy mô lớn bằng cách (tái) sử dụng bộ sưu tập rộng lớn các chương trình off-policy. Công trình của chúng tôi do đó có thể phục vụ như một bước đầu quan trọng hướng tới hướng này.
