# 2310.09520.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2310.09520.pdf
# File size: 590052 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Reward-Augmented Decoding: Efficient Controlled Text Generation
With a Unidirectional Reward Model
Haikang Deng
UNC-Chapel Hill
frankdenghaikang@gmail.comColin Raffel
University of Toronto, Vector Institute
craffel@gmail.com
Abstract
While large language models have proven ef-
fective in a huge range of downstream appli-
cations, they often generate text that is prob-
lematic or lacks a desired attribute. In this
paper, we introduce Reward-Augmented De-
coding (RAD), a text generation procedure that
uses a small unidirectional reward model to en-
courage a language model to generate text that
has certain properties. Specifically, RAD uses
the reward model to score generations as they
are produced and rescales sampling probabil-
ities to favor high-reward tokens. By using a
unidirectional reward model, RAD can cache
activations from prior generation steps to de-
crease computational overhead. Through exper-
iments on generating non-toxic and sentiment-
controlled text, we demonstrate that RAD per-
forms best among methods that change only the
generation procedure and matches the perfor-
mance of state-of-the-art methods that involve
re-training the language model. We further val-
idate that RAD is effective on very large lan-
guage models while incurring a minimal com-
putational overhead.
1 Introduction
Large language models (LLMs, Rae et al., 2021;
Hoffmann et al., 2022; Scao et al., 2022; Touvron
et al., 2023) are seeing widespread adoption thanks
to the fact that they can perform many language
tasks and generate coherent long-form text. As
LLMs are deployed in situations where they in-
teract with humans, it can be beneficial to control
the language model so that it generates text with
certain properties (Sudhakar et al., 2019) – for ex-
ample, we might desire generations that are unbi-
ased, non-toxic, and helpful. In addition, we may
want models to output text with specific proper-
ties, such as having a positive sentiment, a certain
writing style, etc. Typically, LLMs pre-trained on
uncurated large-scale text corpora can generate text
that does not have these desired attributes (Wallace
Language Model 
UncleBobBobloves
is
hasReward Model 
UncleBoblovesFigure 1: Reward-Augmented Decoding (RAD). RAD
steers a language model towards generating text that is
assigned a high reward by an auxiliary reward model.
Blue/red boxes in the reward model correspond to
cached/newly computed hidden states.
et al., 2019; Gehman et al., 2020), which motivates
the need for techniques that enable controllable
text generation . Such techniques can be seen as
providing a means to condition text generation on
a desired attribute.
A straightforward way to control the text gener-
ated by an LLM is to perform additional training
on data that has desired properties (Gururangan
et al., 2020). Alternatively, an LLM can be trained
with “control codes” (Keskar et al., 2019; Lu et al.,
2022) that indicate text characteristics and can be
used to induce the LLM to generate content with
those characteristics. If available, annotated human
preferences can be used to train a reward model
that is then used to train a language model with
reinforcement learning (Ouyang et al., 2022; Kim
et al., 2023). A drawback of these methods is that
they can degrade performance on text that is dif-
ferent from the data used for additional training.
Besides, work done to control one language model
cannot be reused to control another language model.
Moreover, the additional training cost can be pro-
hibitively expensive, especially for very large mod-
els.arXiv:2310.09520v4  [cs.CL]  2 Jan 2024

--- PAGE 2 ---
One way to avoid the cost and shortcomings
of additional training is to instead modify the de-
coding procedure used to generate text from a lan-
guage model (Chaffin et al., 2022). For example,
weighted decoding modifies the probabilities as-
signed to each token during decoding using an
auxiliary model. Most weighted decoding meth-
ods (Holtzman et al., 2018; Krause et al., 2021;
Liu et al., 2021; Yang and Klein, 2021; Sitdikov
et al., 2022) obtain an attribute probability P(c|X)
from a separate reward model (typically smaller
than the base language model) and construct class-
conditional text probabilities following Bayes rule,
P(X|c)∝P(X)P(c|X), where cis an attribute
class and P(X)is the distribution over natural lan-
guage sequences X. During decoding, Krause et al.
(2021) and Liu et al. (2021) process signals from
auxiliary generative models, whereas Yang and
Klein (2021) and Sitdikov et al. (2022) evaluate
intermediate sequences. Weighted decoding only
requires access to the next-step probabilities output
by a language model, does not require expensive
training, and is often modular, i.e. a single reward
model can be reused with many language models.
Despite these benefits, weighted decoding can sig-
nificantly increase the cost of decoding and often
underperforms methods that involve further train-
ing (See et al., 2019).
In this paper, we close the gap between weighted
decoding and re-training by introducing reward-
augmented decoding (RAD), an efficient, effective,
and modular weighted decoding method that steers
text generation based on the reward returned by
an attribute-specific reward model. In particular,
RAD uses a unidirectional reward model trained
to output a reward representing how well a given
sequence aligns with a desired attribute. The uni-
directionality of the reward model allows caching
intermediate activations as the sequence is gener-
ated, greatly decreasing computational costs. Dur-
ing decoding, the tokens with the top- khighest
probabilities are rescaled according to the reward
model so that tokens that better reflect the desired
attribute are more likely to be chosen as the next
generated token.
To validate RAD’s effectiveness, we evaluate it
on standard detoxification and sentiment-controlled
generation tasks, showing that it steers text genera-
tion towards a desired attribute without sacrificing
much diversity and fluency. We ultimately find that
RAD outperforms other weighted decoding meth-ods and achieves results comparable to methods
that involve additional training. We further validate
RAD in a real-world large-scale setting by show-
ing it is effective and introduces minimal computa-
tional overhead when applied to the LLaMA (Tou-
vron et al., 2023) family of language models with
up to 65B parameters.
2 Reward-Augmented Decoding
At a high level, reward-augmented decoding, as
shown in fig. 1, feeds intermediate candidate se-
quences into a reward model that evaluates their
alignment with a desired attribute. Then, at each de-
coding step, RAD uses the predicted reward of each
candidate sequence to modify the token probabili-
ties output by the language model. In this section,
we describe these steps in detail. Refer to table 2
for descriptions of the notations used in this paper.
2.1 Unidirectional Reward Model
Consider using a reward model to compute rewards
forkcandidate tokens at each of mgeneration
timesteps. If scoring each candidate token requires
re-processing the entire generated sequence up to
the current timestep, the reward model would need
to process O(km2)tokens, which could be pro-
hibitively expensive. To address these issues, we
use a unidirectional reward model, specifically a
Transformer decoder with causal masking (Liu
et al., 2018; Radford et al., 2018). In a unidirec-
tional model with causal masking, previously com-
puted representations remain unchanged when new
tokens are appended, so at each generation timestep
the reward model only needs to compute the repre-
sentation of the newly added token. This reduces
computational costs to O(km).
In this work, the reward model is a modi-
fied pre-trained decoder-only Transformer (GPT-
2 small (Radford et al., 2019a) in all of our ex-
periments) fine-tuned on text annotated with the
amount of the target attribute present. We use a
cumulative squared error loss that takes a weighted
mean of each prefix’s loss:
L(r,ˆr) =Pl
t=1t(rt−ˆr)2
Sl, Sl=l(l+ 1)
2
where rtis the reward model’s prediction at gen-
eration timestep t,ˆr∈[0,1]is the ground-truth
reward value, and lis the generation length. The
cumulative loss encourages the reward model to
output the correct reward for every prefix of the

--- PAGE 3 ---
Algorithm 1 Reward-Augmented Decoding
Input fθneural network language model (outputs logits)
gλ neural network reward model (outputs reward score)
X generation prefix
1:xt←none
2:while xt̸=<EOS>do
3: wt←topk( fθ(X)) // get top- ktokens (indices), wt∈Nk
4: zt←fθ(X)[wt] // get top- ktoken logits, zt∈Rk
5: ρt←gλ

X;wt,1
...
X;wt,k

 // compute rewards, ρt∈[0,1]k
6: pt←softmax( zt+βρt) // compute reweighted distribution
7: xt∼Categorical (pt)
8: X← {X;xt} // append new sample
Output generated text Xsteered towards higher rewards
text sequence in order to capture both current and
future alignment of a candidate sequence with the
desired attribute.
2.2 Weighted decoding
RAD utilizes top- ksampling (Fan et al., 2018;
Holtzman et al., 2018; Radford et al., 2019b) and
re-weights the probabilities of the tokens with the
top-khighest probabilities based on each candi-
date’s reward score. Specifically, at timestep t,
re-weighting is done by computing
softmax( zt+βρt)
where zt∈Rkare top- klargest logits output by the
language model’s at output timestep t,β∈Ris a
scaling hyperparameter (with higher βcorrespond-
ing to more intense steering), and ρt∈[0,1]k
are the reward values for the ksequences corre-
sponding to appending each of the top- ktokens.
Adding βρtand renormalizing with softmax is
proportional to reweighting the top- kprobabilities
byeβρt. Consequently, RAD effectively rescales
probabilities of the top- ktokens in accordance with
their relative difference in reward. Algorithm 1 pro-
vides an overview of the decoding process.
3 Experiments
We now evaluate RAD’s performance in two stan-
dard settings: Preventing language models from
generating toxic text (Wallace et al., 2019; Gehman
et al., 2020) and controlling the sentiment of gener-
ated text (Li et al., 2018; Sudhakar et al., 2019).
Baselines In both settings, we consider the same
set of baselines as Liu et al. (2021), namely: the
performance of the base language model itself with-
out any interventions; PPLM (Pascual et al., 2021),which uses a bag-of-word classifier to update LM
hidden states during decoding; GeDi (Krause et al.,
2021) and DExperts (Liu et al., 2021), which
use signals from auxiliary language models to
modify LM probabilities in one pass; Rectifica-
tion (Cao et al., 2023), which adjusts LM prob-
abilities proportional to the risk of resulting in a
toxic generation; DAPT (Gururangan et al., 2020),
which further trains the model on data that has
the desired property; PPO (Schulman et al., 2017),
which updates the LM with gradients from the re-
ward model; Quark (Lu et al., 2022), which per-
forms parameter-efficient fine-tuning on attribute-
annotated data (Lester et al., 2021; Li and Liang,
2021); and CTRL (Keskar et al., 2019), a language
model trained to condition on control codes. Un-
less otherwise mentioned, we report results directly
from Liu et al. (2021) and Lu et al. (2022), which
can be consulted for further baseline details.
3.1 Detoxification
Experimental Setup. We closely follow past
work (Liu et al., 2021) and use RAD to detox-
ify generations from GPT-2 Large (Radford et al.,
2019a) after conditioning on prompts from the Re-
alToxicityPrompts (Gehman et al., 2020) dataset.
For our reward model, we fine-tune GPT-2 Small
on 2M human-annotated comments with continu-
ous labels between 0 and 1 from the Jigsaw Unin-
tended Bias in Toxicity Classification dataset.1We
report RAD’s performance with different values k
(used in top- ksampling) and β(used for adjusting
weighted decoding).
Evaluation Metrics. For every prompt, we sam-
ple 25 continuations, each containing up to 20 new
1https://bit.ly/43CAdCJ

--- PAGE 4 ---
10 20 30 40 5060
Perplexity0.10.20.30.4Average Max T oxicity
/uni0052/uni0041/uni0044/uni0020k=20
/uni0052/uni0041/uni0044/uni0020k=50
/uni0047/uni0050/uni0054/uni0032
/uni0050/uni0050/uni004C/uni004D
/uni0052/uni0065/uni0063/uni0074/uni0069/uni0066/uni0069/uni0063/uni0061/uni0074/uni0069/uni006F/uni006E
/uni0047/uni0065/uni0044/uni0069
/uni0044/uni0045/uni0078/uni0070/uni0065/uni0072/uni0074/uni0073
/uni0044/uni0041/uni0050/uni0054
/uni0050/uni0050/uni004F
/uni0051/uni0075/uni0061/uni0072/uni006BFigure 2: RAD outperforms all weighted decoding meth-
ods (round points •in the graph) and matches methods
that involve additional training.
tokens. As in Liu et al. (2021), we measure the Av-
erage Max Toxicity , i.e. the expected maximum tox-
icity score of the 25 continuations evaluated by the
Perspective API2and the Toxic Rate , i.e. the prob-
ability that at least one out of 25 continuations is
toxic (Perspective API toxicity score >0.5). Since
the perspective API changes over time (Pozzobon
et al., 2023), we recomputed the scores for all base-
line methods. We also measure the Diversity as
the number of distinct bigrams and trigrams nor-
malized by the length of text (Li et al., 2016) and
theFluency as the perplexity assigned to the con-
tinuation by GPT-2-XL conditioned on the prompt.
In general, a good method should reduce toxicity
while preserving fluency and diversity.
Results. As shown in fig. 2 and table 4 (ap-
pendix), RAD demonstrates a favorable trade-off
between toxicity and fluency without significantly
sacrificing diversity, ultimately outperforming all
weighted decoding methods and matching the per-
formance of methods that involve additional train-
ing. Moreover, RAD achieves the lowest Average
Max Toxicity of any method. Our results further
demonstrate that RAD provides an intuitive means
to effectively trade-off toxicity and fluency by tun-
ingβ.
3.2 Sentiment-Controlled Generation
Experimental Setup. Following past work (Li
et al., 2018; Sudhakar et al., 2019; Liu et al., 2021),
we use RAD to steer GPT-2 Large’s generation
to be either positive/negative in sentiment when
prompted with negative/positive or neutral prompts.
Specifically, we evaluate on 2.5K negative, 5K neu-
tral, and 2.5K positive prompts from OpenWeb-
Text (Gokaslan and Cohen, 2019). For RAD’s re-
ward model, we fine-tune GPT-2 Small on millions
2https://bit.ly/3p2r87b
0.50.60.70.80.91.0Neutral Prompt
Positive Rate
/uni0052/uni0041/uni0044/uni0020k=20
/uni0052/uni0041/uni0044/uni0020k=50
/uni0047/uni0050/uni0054/uni0032
/uni0050/uni0050/uni004C/uni004D
/uni0043/uni0054/uni0052/uni004C
/uni0047/uni0065/uni0044/uni0069
/uni0044/uni0045/uni0078/uni0070/uni0065/uni0072/uni0074/uni0073
/uni0044/uni0041/uni0050/uni0054
/uni0050/uni0050/uni004F
/uni0051/uni0075/uni0061/uni0072/uni006B
10 20 30 405060 100 2000.00.10.20.30.40.50.6Negative Prompt
Positive Rate
PerplexityFigure 3: RAD achieves the highest positive rate for
negative prompts and outperforms all weighted decod-
ing methods.
of product and movie reviews from Amazon Polar-
ity3and SST-2 (Socher et al., 2013).
Evaluation Metrics. We sample 25 continu-
ations for each prompt and compute the aver-
agePositive Rate measured by HuggingFace text-
classification pipeline4(a DistilBERT model fine-
tuned on SST-2). We also report the Diversity and
Fluency as introduced above.
Results. As seen in fig. 3 and table 5 (appendix),
RAD attains a better fluency/positivity trade-off
(when conditioning on negative or neutral prompts)
than any other weighted decoding method and
achieves comparable performance to the state-of-
the-art methods involving training (Quark and
PPO), which both make use of the evaluation model
(DistilBERT model fine-tuned on SST-2) during
training. Tuning βeffectively trades off fluency
and alignment, again enabling RAD to produce the
best attribute scores. Figure 4 (appendix) visual-
izes RAD’s steering process when prompted with
negative input.
3.3 Scaling the Language Model
In all prior experiments, we followed past work and
considered using GPT-2 Large as the base language
model. Recent LLMs have dramatically more pa-
rameters (and dramatically better performance). To
test RAD in more realistic settings, we apply RAD
to the state-of-the-art LLaMA models (Touvron
3https://bit.ly/3XfY6NZ
4https://bit.ly/3qIycX9

--- PAGE 5 ---
  step 1:  be
          2:  in
          3:  developing
          4:  countries
          5: .
          6:  But
          7:  with
          8:  clean
          9:  energy
        10:  technologies
        11:  and
        12:  other
        13:  energy
        14:  efficiency
        15:  innovations
        16: 
        17: like
        18:  solar
        19:  panels
0.0 0.2 0.4 0.6 0.8 1.0
Rewards        20: ,Prompt: The most polluted cities tend toFigure 4: Visualization of RAD’s decoding process.
Each row represents a single decoding step, where the
area is the estimated reward distribution of the top- 50
candidate sequences, and the red line indicates the se-
lected token’s reward score.
et al., 2023) in the detoxification setting of sec-
tion 3.1, using the same GPT-2 Small reward model.
In table 6 (appendix), we show that RAD signif-
icantly reduces LLaMA’s toxicity while preserv-
ing its diversity and fluency. In terms of compu-
tational costs, we list the relative cost of different
methods for controlled text generation in table 1.
While RAD and other weighted decoding methods
increase costs significantly when the size of the
language model and reward model are similar, the
additional expense of using RAD is only about 3%
when using LLaMA 65B as the language model
and GPT-2 Small as the reward model. These re-
sults confirm that RAD can effectively control text
generation of state-of-the-art models while incur-
ring negligible computational overhead.
4 Conclusion and Future Work
In this paper, we propose RAD, a simple weighted
decoding method for controlling text generation
that uses a unidirectional reward model to mini-
mize computational costs. RAD outperforms prior
weighted decoding methods and matches the per-
formance of state-of-the-art techniques that involve
additional training. When the size of the reward
model is relatively small compared to the base lan-
guage model, RAD incurs negligible computational
overhead. In future work, we are interested in ap-Decoding Cost
Method GPT-2 Large LLaMA 65B
PPLM 4.0× 4.00×
GeDi 1.9× 1.01×
DExperts 3.0× 1.02×
Additional training 1× 1×
RAD 3.4× 1.03×
Table 1: Computational overhead (as a relative increase
in cost) for different methods for controlling text gener-
ation using GPT-2 Small as a reward model and GPT-2
Large or LLaMA 65B as the language model. “Ad-
ditional training” refers to methods that train the lan-
guage model and do not modify decoding (e.g. Quark,
DAPT, PPO, etc.). Calculation details provided in ap-
pendix C.2.
plying RAD to more sophisticated tasks, such as
encouraging language models to follow instruc-
tions (Ouyang et al., 2022).
Limitations
Although RAD achieves decent performance and
generalizes to other language models, two limita-
tions should be considered for this work. Firstly,
RAD incurs additional compute and memory allo-
cation linear to k. As mentioned in section 2.1, we
manage to reduce time complexity from O(km2)
toO(km)by reusing previously computed repre-
sentations in the decoder reward model. Yet, track-
ing and copying past_key_values take up a certain
amount of GPU memory, which reduces decoding
throughput. Secondly, our experiments regarding
toxicity and sentiment explore only some capabil-
ities of RAD. More tasks should be conducted to
form a comprehensive review of RAD.
Ethics Statement
This work centers around controllable text gener-
ation, which holds significant relevance in regu-
lating natural language generation. For example,
the detoxification task aims to mitigate the toxicity
present in texts generated by pre-trained language
models. In this context, RAD offers a solution
for controlling the text generation process without
modifying the base language model.
Acknowledgements
We would like to thank Derek Tam for valuable
discussions. We also extend our appreciation to the
Perspective API team for increasing API quota on
our behalf.

--- PAGE 6 ---
References
Meng Cao, Mehdi Fatemi, Jackie CK Cheung, and
Samira Shabanian. 2023. Systematic rectification
of language models via dead-end analysis. In The
Eleventh International Conference on Learning Rep-
resentations .
Antoine Chaffin, Thomas Scialom, Sylvain Lamprier,
Jacopo Staiano, Benjamin Piwowarski, Ewa Kijak,
and Vincent Claveau. 2022. Which discriminator
for cooperative text generation? In Proceedings of
the 45th International ACM SIGIR Conference on
Research and Development in Information Retrieval .
ACM.
Angela Fan, Mike Lewis, and Yann Dauphin. 2018.
Hierarchical neural story generation. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 889–898, Melbourne, Australia. Association
for Computational Linguistics.
Samuel Gehman, Suchin Gururangan, Maarten Sap,
Yejin Choi, and Noah A. Smith. 2020. RealToxi-
cityPrompts: Evaluating neural toxic degeneration
in language models. In Findings of the Association
for Computational Linguistics: EMNLP 2020 , pages
3356–3369, Online. Association for Computational
Linguistics.
Aaron Gokaslan and Vanya Cohen. 2019. Open-
webtext corpus. http://Skylion007.github.io/
OpenWebTextCorpus .
Suchin Gururangan, Ana Marasovi ´c, Swabha
Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A. Smith. 2020. Don’t stop pretraining:
Adapt language models to domains and tasks. In
Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics , pages
8342–8360, Online. Association for Computational
Linguistics.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-
sch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-
ford, Diego de Las Casas, Lisa Anne Hendricks,
Johannes Welbl, Aidan Clark, et al. 2022. Train-
ing compute-optimal large language models. arXiv
preprint arXiv:2203.15556 .
Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine
Bosselut, David Golub, and Yejin Choi. 2018. Learn-
ing to write with cooperative discriminators. In Pro-
ceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers) , pages 1638–1649, Melbourne, Australia. As-
sociation for Computational Linguistics.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling laws for neural language models.
Nitish Shirish Keskar, Bryan McCann, Lav R Varshney,
Caiming Xiong, and Richard Socher. 2019. Ctrl: Aconditional transformer language model for control-
lable generation. arXiv preprint arXiv:1909.05858 .
Minbeom Kim, Hwanhee Lee, Kang Min Yoo, Joon-
suk Park, Hwaran Lee, and Kyomin Jung. 2023.
Critic-guided decoding for controlled text generation.
InFindings of the Association for Computational
Linguistics: ACL 2023 , pages 4598–4612, Toronto,
Canada. Association for Computational Linguistics.
Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann,
Nitish Shirish Keskar, Shafiq Joty, Richard Socher,
and Nazneen Fatema Rajani. 2021. GeDi: Gener-
ative discriminator guided sequence generation. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021 , pages 4929–4952, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing ,
pages 3045–3059, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016. A diversity-promoting ob-
jective function for neural conversation models. In
Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies ,
pages 110–119, San Diego, California. Association
for Computational Linguistics.
Juncen Li, Robin Jia, He He, and Percy Liang. 2018.
Delete, retrieve, generate: a simple approach to senti-
ment and style transfer. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers) ,
pages 1865–1874, New Orleans, Louisiana. Associa-
tion for Computational Linguistics.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , pages 4582–
4597, Online. Association for Computational Lin-
guistics.
Alisa Liu, Maarten Sap, Ximing Lu, Swabha
Swayamdipta, Chandra Bhagavatula, Noah A. Smith,
and Yejin Choi. 2021. DExperts: Decoding-time con-
trolled text generation with experts and anti-experts.
InProceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Lan-
guage Processing (Volume 1: Long Papers) , pages
6691–6706, Online. Association for Computational
Linguistics.

--- PAGE 7 ---
Peter J Liu, Mohammad Saleh, Etienne Pot, Ben
Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam
Shazeer. 2018. Generating wikipedia by summariz-
ing long sequences. In International Conference on
Learning Representations .
Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang,
Lianhui Qin, Peter West, Prithviraj Ammanabrolu,
and Yejin Choi. 2022. Quark: Controllable text
generation with reinforced unlearning. Advances
in neural information processing systems , 35:27591–
27609.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Damian Pascual, Beni Egressy, Clara Meister, Ryan
Cotterell, and Roger Wattenhofer. 2021. A plug-and-
play method for controlled text generation. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2021 , pages 3973–3997, Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Luiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara
Hooker. 2023. On the challenges of using black-
box apis for toxicity evaluation in research. arXiv
preprint arXiv:2304.12397 .
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. 2018. Improving language under-
standing by generative pre-training.
Alec Radford, Jeff Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019a. Language
models are unsupervised multitask learners.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019b. Lan-
guage models are unsupervised multitask learners.
OpenAI blog , 1(8):9.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, Eliza Rutherford, Tom Hennigan, Ja-
cob Menick, Albin Cassirer, Richard Powell, George
van den Driessche, Lisa Anne Hendricks, Mari-
beth Rauh, Po-Sen Huang, Amelia Glaese, Johannes
Welbl, Sumanth Dathathri, Saffron Huang, Jonathan
Uesato, John F. J. Mellor, Irina Higgins, Antonia
Creswell, Nathan McAleese, Amy Wu, Erich Elsen,
Siddhant M. Jayakumar, Elena Buchatskaya, David
Budden, Esme Sutherland, Karen Simonyan, Michela
Paganini, L. Sifre, Lena Martens, Xiang Lorraine
Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena
Gribovskaya, Domenic Donato, Angeliki Lazaridou,
Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-
poukelli, N. K. Grigorev, Doug Fritz, Thibault Sotti-
aux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong,
Daniel Toyama, Cyprien de Masson d’Autume, YujiaLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,
Aidan Clark, Diego de Las Casas, Aurelia Guy,
Chris Jones, James Bradbury, Matthew G. Johnson,
Blake A. Hechtman, Laura Weidinger, Iason Gabriel,
William S. Isaac, Edward Lockhart, Simon Osindero,
Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W.
Ayoub, Jeff Stanway, L. L. Bennett, Demis Hass-
abis, Koray Kavukcuoglu, and Geoffrey Irving. 2021.
Scaling language models: Methods, analysis & in-
sights from training gopher. ArXiv , abs/2112.11446.
Teven Le Scao, Angela Fan, Christopher Akiki, El-
lie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman
Castagné, Alexandra Sasha Luccioni, François Yvon,
Matthias Gallé, et al. 2022. Bloom: A 176b-
parameter open-access multilingual language model.
arXiv preprint arXiv:2211.05100 .
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv:1707.06347 .
Abigail See, Stephen Roller, Douwe Kiela, and Jason
Weston. 2019. What makes a good conversation?
how controllable attributes affect human judgments.
InProceedings of the 2019 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers) , pages 1702–1723,
Minneapolis, Minnesota. Association for Computa-
tional Linguistics.
Askhat Sitdikov, Nikita Balagansky, Daniil Gavrilov,
and Alexander Markov. 2022. Classifiers are better
experts for controllable text generation.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
InProceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing , pages
1631–1642, Seattle, Washington, USA. Association
for Computational Linguistics.
Akhilesh Sudhakar, Bhargav Upadhyay, and Arjun Ma-
heswaran. 2019. “transforming” delete, retrieve, gen-
erate approach for controlled text style transfer. In
Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 3269–
3279, Hong Kong, China. Association for Computa-
tional Linguistics.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. ArXiv ,
abs/2302.13971.
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-
ner, and Sameer Singh. 2019. Universal adversarial

--- PAGE 8 ---
triggers for attacking and analyzing NLP. In Proceed-
ings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th Inter-
national Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP) , pages 2153–2162, Hong
Kong, China. Association for Computational Linguis-
tics.
Kevin Yang and Dan Klein. 2021. FUDGE: Controlled
text generation with future discriminators. In Pro-
ceedings of the 2021 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages
3511–3535, Online. Association for Computational
Linguistics.
A Notations
Refer to table 2 for notations used in the paper.
B RAD Training Details
B.1 Detoxification
We train a GPT-2 Small reward model on the Jigsaw
Unintended Bias in Toxicity Classification dataset1
for 5 epochs. We use learning rate = 1e−5, weight
decay = 0.01, and batch size = 100 . The reward
model achieves a final squared error of 0.0147 on
thetest public leaderboard subset.
B.2 Sentiment-Controlled Generation
We first train the reward model on Amazon
Polarity3for 5 epochs, with learning rate = 1e−5,
weight decay = 0.01, and batch size = 100 . We
then continue to train the reward model on SST-
2 (Socher et al., 2013) for 10 epochs, with learning
rate= 2e−6.
C Computational Costs
C.1 RAD
In the paper, we use GPT-2 Small (124M) as RAD’s
reward model and replace the lm_head layer with
a linear layer with one output for predicting the re-
ward. Following the approximations in Kaplan et al.
(2020), the number of non-embedding parameters
in a model is approximately
N≈12nlayerd2
model
where nlayeris the number of layers and dmodel is
the hidden size. In addition, a forward pass requires
Cforward≈2N+ 2nlayernctxdmodel
FLOPs per token, where nctxis the context token.
With embedding operation costing 4dmodel and re-
ward predicting costing 2dmodel, we construct thenumber of FLOPs needed for a token in the reward
model as
CRM= 6dmodel +Cforward
Since 6dmodel≪N,
CRM≈Cforward
= 2(1 +nctx
12dmodel)N
Notice the context-dependent computational cost
per token is a small portion of the total compute,
asdmodel >nctx
12is often true in practice (Ka-
plan et al., 2020). In fact, in detoxification and
sentiment-controlled generation experiments, nctx
is constantly below 50. Thus, it is safe to assume
CRM=Cforward = 2N
for both the language model and the reward model.
The reward model evaluates kcandidate sequences
at each decoding step, which requires kCRM
FLOPs in total. Assuming k= 20 ,CRAD=kCRM,
appendix C.1 shows the estimated FLOPs per token
of the reward model and various language models.
C.2 Comparison
We continue to explore the computation cost of
baseline methods based on the methodology in ap-
pendix C.1. Define Cmethod as the additional cost a
method incurs during decoding and TC method×LM
as the total cost in FLOPs for every token generated
using the method on the specific LM during test
time. Retraining methods (DAPT, PPO, and Quark)
haveCmethod = 0andTC method×LM=CLM.
PPLM updates the previous token’s representa-
tion in the LM using gradients from an attribute-
specific linear discriminator and recomputes the
current state probabilities. Thus, two forward
passes and one backward pass of the LM are re-
quired for every generated token. As a backward
pass has roughly twice the number of matrix mul-
tiplications as in a forward pass (Kaplan et al.,
2020), PPLM incurs an additional decoding cost of
CPPLM = 3CLM. Thus,
TC PPLM×GPT2-large = 4CGPT2-large = 5.66G
TC PPLM×LLaMA-65b = 4CLLaMA-65b = 515 .40G
GeDi & DExperts take a similar approach where
they use two opposite discriminator/expert mod-
els to produce classification probabilities and then

--- PAGE 9 ---
Notation Dimension Description
k N number of candidate tokens to consider at every timestep
β R steering amount hyperparameter
l N generation length of reward model training data
ˆr [0,1] label of reward model training data
r [0,1]lpredictions generated by reward model during training
ρt [0,1]kreward scores predicted by the reward model at time t
wt Nkindices of top- ktokens at time t
zt Rklogits of top- ktokens at time t
Table 2: We use two notations randρto differentiate the reward model’s output during train time and test time.
RM nlayer dmodel CRAD
GPT2-small 12 768 3.40G
LM nlayer dmodel CLM
GPT-2 Large 36 1280 1.42G
LLaMA 7B 32 4096 12.89G
LLaMA 13B 40 5120 25.17G
LLaMA 33B 60 6656 63.80G
LLaMA 65B 80 8192 128.85G
Table 3: Model specifications and FLOPs per token.
rescale the LM probabilities. Thus, two additional
forward passes of the expert model are needed. For
GeDi,
CGeDi= 2CGPT2-medium = 1.21G
TC GeDi×GPT2-large =CGeDi+CGPT2-large
= 2.62G
TC GeDi×LLaMA-65b =CGeDi+CLLaMA-65b
= 130 .06G
For DExperts,
CDExperts = 2CGPT2-large = 2.83G
TC DExperts ×GPT2-large =CDExperts +CGPT2-large
= 4.25G
TC DExperts ×LLaMA-65b =CDExperts +CLLaMA-65b
= 131 .68G
RAD withk= 20 has
CRAD= 20CGPT2-small = 3.40G
TC RAD×GPT2-large =CRAD+CGPT2-large
= 4.81G
TC RAD×LLaMA-65b =CRAD+CLLaMA-65b
= 132 .25G
DAPT, PPO, and Quark have decoding costs
the same as the underlying LM because they per-
form additional training and do not modify the
decoding procedure.D Full Results
D.1 Detoxification
Since perspective API updates its model regu-
larly (Pozzobon et al., 2023), we ensure fair com-
parison by evaluating all model outputs (except for
PPO and Quark, see below) using the most up-to-
date API. Queries were made between May and
June 2023. As PPO and Quark directly optimize
the language model with Perspective API score
during training, a change in the API model would
lead to a different optimized model. For PPO and
Quark, we adopt the values reported in Lu et al.
(2022). Full results see table 4.
D.2 Sentiment-Controlled Generation
The sentiment-controlled generation results are pre-
sented in table 5.
D.3 Scaling the Language Model
Following previous experiments, we use nucleus
sampling with p= 0.9to get raw LLaMA gener-
ations on the same 10K non-toxic subset of Real-
ToxicityPrompts (Gehman et al., 2020). For each
model size, we apply RAD with k= 20 andβ
from 20 to 500. Results are shown in table 6.
The performance gap between RAD on GPT-
2 Large and RAD on LLaMA may be attributed
to the difference in tokenization between the lan-
guage model and the reward model. Specifically,
the reward model, GPT-2 Small, shares the same
tokenizer and vocabulary with GPT-2 Large, but
not with LLaMA. In this way, a given text sequence
can be tokenized into different token combinations,
which, during decoding, would mislead the reward
model to give distorted scores. Thus, we believe a
smaller model from the same family of the base LM
may be the best choice for RAD’s reward model.
E Generated Examples
Detoxification and sentiment-controlled generation
examples are presented in tables 7 and 8.

--- PAGE 10 ---
MethodToxicity ( ↓) Fluency ( ↓) Diversity ( ↑)
Average Max Toxicity Toxic Rate Perplexity Dist-2 Dist-3
GPT2 0.384 0.257 11.31 0.85 0.85
PPLM 0.376 0.240 32.58 0.86 0.86
GeDi 0.242 0.055 60.03 0.84 0.83
DExperts 0.201 0.021 32.41 0.84 0.84
Rectification 0.180 0.014 25.12 0.86 0.87
DAPT 0.270 0.093 31.21 0.84 0.84
PPO 0.218 0.044 14.27 0.80 0.84
Quark 0.196 0.035 12.47 0.80 0.84
RADk= 20β= 10 0.265 0.076 12.54 0.81 0.84
β= 20 0.232 0.042 12.57 0.81 0.84
β= 30 0.211 0.026 12.69 0.81 0.84
β= 50 0.183 0.014 13.06 0.81 0.84
β= 100 0.148 0.005 13.7 0.81 0.83
β= 200 0.114 0.002 15.93 0.79 0.81
β= 300 0.099 0.001 19.97 0.76 0.78
k= 50β= 10 0.267 0.069 16.86 0.84 0.85
β= 20 0.233 0.035 17.04 0.84 0.85
β= 30 0.21 0.022 17.27 0.84 0.85
β= 50 0.183 0.011 17.62 0.84 0.85
β= 100 0.146 0.004 18.97 0.84 0.84
β= 200 0.112 0.001 23.63 0.83 0.83
β= 300 0.099 0.001 32.84 0.79 0.8
Table 4: Full results of the detoxification experiment. In general, RAD can produce the least toxic outputs without
sacrificing much fluency and diversity. While increasing kenhances diversity and reduces toxicity by a margin, it
takes into account more unlikely words which hurts the output fluency.
MethodToward Positive Toward Negative
% Positive Rate ( ↑) Fluency ( ↓) Diversity ( ↑) % Positive Rate ( ↓) Fluency ( ↓) Diversity ( ↑)
Negative NeutralPerplexity Dist-2 Dist-3Positive NeutralPerplexity Dist-2 Dist-3prompt prompt prompt prompt
GPT2 0.00 50.02 11.42 0.85 0.85 99.08 50.02 11.42 0.84 0.84
PPLM 8.72 52.68 142.1 0.86 0.85 89.74 39.05 181.7 0.87 0.86
CTRL 18.88 61.81 43.79 0.83 0.86 79.05 37.63 35.94 0.83 0.86
GeDi 26.80 86.01 58.41 0.80 0.79 39.57 8.73 84.11 0.84 0.82
DExperts 36.42 94.46 25.83 0.84 0.84 35.99 3.77 45.91 0.84 0.83
DAPT 14.17 77.24 30.52 0.83 0.84 87.43 33.28 32.86 0.85 0.84
PPO 43.13 94.10 15.16 0.80 0.84 32.22 3.65 15.54 0.81 0.84
Quark 46.55 95.00 14.54 0.80 0.84 27.50 2.75 14.72 0.80 0.84
RADk= 20β= 10 19.99 86.21 12.86 0.79 0.82 73.34 17.38 13.33 0.8 0.83
β= 20 35.24 92.71 14.6 0.78 0.82 57.19 10.49 15.36 0.79 0.83
β= 30 43.94 94.8 16.7 0.77 0.81 50.04 8.03 17.79 0.78 0.82
β= 40 48.37 95.72 19.23 0.76 0.8 47.01 6.84 20.27 0.77 0.81
β= 50 51.19 96.3 21.77 0.75 0.79 45.45 6.05 22.8 0.76 0.8
β= 60 53.21 96.62 25.06 0.73 0.78 44.76 5.47 25.51 0.74 0.78
k= 50β= 10 22.43 86.66 17.98 0.82 0.84 67.22 15.13 18.77 0.83 0.85
β= 20 41.56 93.28 21.02 0.82 0.84 45.08 8.63 22.54 0.83 0.84
β= 30 52.25 95.37 25.02 0.81 0.83 36.2 6.52 26.49 0.81 0.84
β= 40 57.91 96.24 28.99 0.8 0.82 32.27 5.62 30.92 0.8 0.83
β= 50 61.64 96.7 33.97 0.79 0.82 30.16 4.89 35.92 0.79 0.82
β= 60 63.57 97.0 39.23 0.78 0.81 28.75 4.45 40.1 0.78 0.81
Table 5: Full results of the sentiment-controlled generation experiment.

--- PAGE 11 ---
LM SettingToxicity ( ↓) Fluency ( ↓) Diversity ( ↑)
Average Max Toxicity Toxic Rate Perplexity Dist-2 Dist-3
LLaMA 7BRaw LM 0.338 0.212 12.93 0.81 0.82
β= 20 0.282 0.129 13.79 0.82 0.83
β= 50 0.250 0.097 14.21 0.82 0.82
β= 100 0.221 0.072 15.13 0.82 0.82
β= 200 0.185 0.048 17.40 0.80 0.81
β= 500 0.125 0.016 29.97 0.69 0.73
LLaMA 13BRaw LM 0.336 0.204 11.81 0.81 0.82
β= 20 0.284 0.129 12.70 0.82 0.82
β= 50 0.253 0.103 13.00 0.82 0.82
β= 100 0.223 0.073 13.79 0.82 0.82
β= 200 0.187 0.047 16.20 0.80 0.81
β= 500 0.127 0.017 29.82 0.69 0.72
LLaMA 33BRaw LM 0.337 0.210 10.80 0.80 0.81
β= 20 0.287 0.139 11.68 0.81 0.82
β= 50 0.258 0.109 11.96 0.81 0.82
β= 100 0.229 0.076 12.79 0.81 0.82
β= 200 0.192 0.050 15.06 0.80 0.81
β= 500 0.130 0.019 28.58 0.68 0.72
LLaMA 65BRaw LM 0.337 0.215 10.42 0.80 0.80
β= 20 0.288 0.139 11.30 0.81 0.81
β= 50 0.260 0.108 11.56 0.81 0.81
β= 100 0.232 0.081 12.27 0.81 0.81
β= 200 0.197 0.055 14.58 0.79 0.81
β= 500 0.133 0.019 28.64 0.70 0.73
Table 6: Similar to its application to GPT2-large, RAD ( k= 20 ) reduces the toxicity of LLaMA’s generated text
without greatly impacting its fluency and diversity. Its performance remains stable even as LLaMA model increases
in size.
Prompt: No matter who you’ll pick you’ll
GPT2 be well placed to take advantage of many of the remaining lower season attractions, which include big-city
PPLM need a base set of rules. A position that will remain stable is a real issue as you’ll
GeDi need the focus for this one. Personally, I had a good time in jokingly calling out Card
DExperts need a versatile DH player and Dahl will certainly help you as he’s ideally positioned at the back of
Rectification still get a great value. I would only recommend this product to people who have some sort of skin
DAPT likely be joining a roster of other people who remember that some characters are cuter than others, so
RAD still get the chance. I mean, we’ve got some pretty awesome teams and some of them are
Table 7: Examples of detoxified generation using different methods. RAD with k= 20 andβ= 100 .
Prompt: Meanwhile the iron and lead
GPT2 transfer would be successful to some extent, but the heart issue is still very dangerous and will probably have
To Positive
PPLM content on the mass spectrometer has reached to near the limit that these elements can be detected,"
CTRL parts of both were in good condition, though when I was ordering them I did not notice that they only
GeDi gathered, our new friends danced, jests were merrily spiced, and plenty of songs fired
DExperts fields may not seem like the perfect areas for reclaiming coal and steel, but technology has brought mining
DAPT in the water begins to leach from the old pipes, whilst the decaying furniture sheds a great deal
RAD industries, which provide a great deal of economic and social support for the British working class and a great
To Negative
PPLM prices have collapsed to record lows and given no indication they will recede as the seller and take the
CTRL content is over half of that in all US bottled water - 0.32 ppm versus 0.012 ppm.
GeDi content in some vaccines have already precipitously risen above acceptable limits. They’ve even gone so far
DExperts sinks. 15 Too many carts loaded with rusty iron bars running off of a firewood dump. 80
DAPT reserves have fallen precipitously. At last count, the final daily production estimate was 50,000
RAD shortages have only been getting worse. The steel industry that is vital to the economy today is in serious
Table 8: Examples of sentiment-controlled generation using different methods. RAD with k= 50 andβ= 30 .
