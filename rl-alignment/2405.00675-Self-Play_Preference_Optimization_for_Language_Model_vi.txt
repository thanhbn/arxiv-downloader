# 2405.00675.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2405.00675.pdf
# Kích thước tệp: 788493 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tối Ưu Hóa Sở Thích Tự Chơi cho Việc Căn Chỉnh Mô Hình Ngôn Ngữ

Yue Wu∗†Zhiqing Sun∗‡Huizhuo Yuan∗§Kaixuan Ji¶Yiming Yang‖Quanquan Gu∗∗

Tóm tắt
Các phương pháp học tăng cường từ phản hồi của con người (RLHF) tiêu chuẩn dựa vào các mô hình tham số như mô hình Bradley-Terry không đủ khả năng nắm bắt tính không bắc cầu và phi lý trong sở thích của con người. Những tiến bộ gần đây cho thấy rằng việc làm việc trực tiếp với xác suất sở thích có thể mang lại sự phản ánh chính xác hơn về sở thích của con người, cho phép việc căn chỉnh mô hình ngôn ngữ linh hoạt và chính xác hơn. Trong bài báo này, chúng tôi đề xuất một phương pháp dựa trên tự chơi để căn chỉnh mô hình ngôn ngữ, coi vấn đề này như một trò chơi hai người tổng không đổi nhằm xác định chính sách cân bằng Nash. Phương pháp của chúng tôi, được gọi là Tối Ưu Hóa Sở Thích Tự Chơi (SPPO), sử dụng các cập nhật chính sách lặp để xấp xỉ cân bằng Nash một cách có thể chứng minh. Ngoài ra, chúng tôi đề xuất một mục tiêu SPPO mới vừa có động lực mạnh mẽ từ lý thuyết vừa đơn giản và hiệu quả trong thực tế. Trong các thí nghiệm của chúng tôi, chỉ sử dụng 60k prompt (không có phản hồi) từ bộ dữ liệu UltraFeedback và không có bất kỳ việc tăng cường prompt nào, bằng cách tận dụng mô hình sở thích được huấn luyện trước PairRM chỉ với 0.4B tham số, SPPO có thể thu được một mô hình từ việc tinh chỉnh Mistral-7B-Instruct-v0.2 đạt được tỷ lệ thắng có kiểm soát độ dài hiện đại nhất là 28.53% so với GPT-4-Turbo trên AlpacaEval 2.0. Nó cũng vượt trội hơn (lặp) DPO và IPO trên MT-Bench, Arena-Hard, và Open LLM Leaderboard. Bắt đầu từ mô hình cơ sở mạnh hơn Llama-3-8B-Instruct, chúng tôi có thể đạt được tỷ lệ thắng có kiểm soát độ dài là 38.77%. Đáng chú ý, hiệu suất mạnh mẽ của SPPO được đạt được mà không cần giám sát bên ngoài bổ sung (ví dụ: phản hồi, sở thích, v.v.) từ GPT-4 hoặc các mô hình ngôn ngữ mạnh hơn khác. Mã nguồn có sẵn tại https://github.com/uclaml/SPPO.

1 Giới thiệu
Các Mô hình Ngôn ngữ Lớn (LLM) (ví dụ: Ouyang et al., 2022; OpenAI et al., 2023), đã cho thấy khả năng đáng kể trong việc tạo ra văn bản giống con người, trả lời câu hỏi và lập trình. Mặc dù có những tiến bộ, các mô hình này gặp phải thách thức trong các nhiệm vụ đòi hỏi mức độ tin cậy, an toàn và căn chỉnh đạo đức cao. Để giải quyết những thách thức này, Học tăng cường từ Phản hồi của Con người (RLHF), còn được gọi là Học tăng cường dựa trên Sở thích (PbRL), trình bày một giải pháp đầy hứa hẹn. Khung công tác này cho tối ưu hóa chính sách, được làm nổi bật trong các công trình của Christiano et al. (2017) và gần đây trong Ouyang et al. (2022), đã dẫn đến thành công thực nghiệm đáng kể trong việc tinh chỉnh các LLM tuân thủ hướng dẫn, làm cho chúng phù hợp hơn với sở thích của con người và do đó hữu ích hơn.

∗Đóng góp ngang nhau
†Khoa Khoa học Máy tính, Đại học California, Los Angeles, Los Angeles, CA 90095; e-mail: ywu@cs.ucla.edu
‡Viện Công nghệ Ngôn ngữ, Đại học Carnegie Mellon, Pittsburgh, PA 15213; e-mail: zhiqings@cs.cmu.edu
§Khoa Khoa học Máy tính, Đại học California, Los Angeles, Los Angeles, CA 90095; e-mail: hzyuan@cs.ucla.edu
¶Khoa Khoa học Máy tính, Đại học California, Los Angeles, Los Angeles, CA 90095; e-mail: kauxuanji@cs.ucla.edu
‖Viện Công nghệ Ngôn ngữ & Khoa Học máy học, Đại học Carnegie Mellon, Pittsburgh, PA 15213; e-mail: yiming@cs.cmu.edu
∗∗Khoa Khoa học Máy tính, Đại học California, Los Angeles, Los Angeles, CA 90095; e-mail: qgu@cs.ucla.edu

1arXiv:2405.00675v5 [cs.LG] 4 Oct 2024

--- TRANG 2 ---
Hầu hết các phương pháp RLHF hiện có dựa vào các mô hình phần thưởng tường minh hoặc ngầm. Lấy InstructGPT (Ouyang et al., 2022) làm ví dụ, một chính sách tham chiếu πref được thiết lập trước, thường từ việc huấn luyện trước có giám sát hoặc tinh chỉnh dựa trên hướng dẫn (có giám sát). Một hàm phần thưởng tường minh được thu được bằng cách huấn luyện một mô hình phần thưởng dựa trên dữ liệu phản hồi sở thích của con người, sử dụng mô hình Bradley-Terry (BT) (Bradley và Terry, 1952). Tiếp theo, các thuật toán học tăng cường như Tối Ưu Hóa Chính sách Gần đúng (Schulman et al., 2017, PPO) được sử dụng để tinh chỉnh LLM tham chiếu πref bằng cách tối đa hóa hàm phần thưởng kỳ vọng. Mô hình phần thưởng cung cấp một "điểm phần thưởng" r(y;x) cho phản hồi y và prompt x đã cho, xấp xỉ phản ánh cách con người đánh giá những phản hồi này. Gần đây hơn, các phương pháp như Tối Ưu Hóa Sở thích Trực tiếp (Rafailov et al., 2024b, DPO) đã được giới thiệu. Những phương pháp này bỏ qua việc huấn luyện một mô hình phần thưởng riêng biệt nhưng vẫn tuân thủ cơ bản mục tiêu tối đa hóa phần thưởng và được xác định bởi các mô hình tham số như mô hình BT.

Những mô hình này giả định một mối quan hệ đơn điệu và bắc cầu giữa các sở thích cho các lựa chọn khác nhau. Tuy nhiên, bằng chứng thực nghiệm cho thấy ngược lại. Ví dụ, Tversky (1969) quan sát thấy các quyết định của con người có thể bị ảnh hưởng bởi các yếu tố khác nhau và thể hiện sự không nhất quán. Những quan sát như vậy chỉ ra rằng sở thích của con người không phải lúc nào cũng tuân theo một hệ thống phân cấp dựa trên giá trị duy nhất và thậm chí có thể xuất hiện phi lý, chẳng hạn như thể hiện các vòng lặp trong các mối quan hệ sở thích. Đối với LLM, một bằng chứng động lực khác là Munos et al. (2023) đã chỉ ra thực nghiệm rằng việc dự đoán trực tiếp sở thích theo cặp có thể đạt được độ chính xác cao hơn so với việc dự đoán sở thích thông qua mô hình phần thưởng dựa trên BT.

Để giải quyết sự không nhất quán trong sở thích của con người, các nhà nghiên cứu đã đề xuất làm việc trực tiếp với xác suất sở thích và thiết kế các thuật toán có thể biểu diễn linh hoạt hơn sở thích của con người (Lou et al., 2022; Wu et al., 2023) trong bối cảnh xếp hạng hoặc bandit. Gần đây, một hướng nghiên cứu mới nổi (Wang et al., 2024; Munos et al., 2023; Swamy et al., 2024) cũng đề xuất nghiên cứu RLHF cho LLM dưới sở thích tổng quát như vậy P(y≻y′|x), trong đó y và y′ là hai phản hồi khác nhau và x là prompt. Mục tiêu là xác định cân bằng Nash hoặc người thắng cuộc von Neumann của trò chơi hai người tổng không đổi

(π∗, π∗) = arg max π min π′ Ex∼X[ Ey∼π(·|x),y′∼π′(·|x) P(y≻y′|x) ],

trong đó mỗi người chơi là một LLM xuất ra phản hồi và nhằm tối đa hóa xác suất được ưa thích hơn đối thủ của mình.

Độc lập với công trình của chúng tôi, Swamy et al. (2024) đã đề xuất Tối Ưu Hóa Sở thích Tự chơi (SPO)¹ cho cùng trò chơi hai người tổng không đổi (không có regularization). Họ cung cấp một giảm thiểu tổng quát từ tối ưu hóa sở thích xuống học trực tuyến không hối tiếc cho Quá trình Quyết định Markov nhiều bước. Khi bị giới hạn trong bối cảnh bandit cho LLM, khung thuật toán đề xuất của họ giảm xuống thuật toán Hedge nổi tiếng (Freund và Schapire, 1997), cho phép quy tắc cập nhật hàm mũ như được mô tả trong (4.1). Để giải quyết xấp xỉ cập nhật hàm mũ, Swamy et al. (2024) sau đó đề xuất sử dụng các thuật toán tối ưu hóa chính sách điển hình như Tối Ưu Hóa Chính sách Gần đúng (PPO) (Schulman et al., 2017) hoặc Soft Actor-Critic (SAC) (Haarnoja et al., 2018) để tối đa hóa tỷ lệ thắng so với chính sách tham chiếu và đánh giá hiệu suất của các thuật toán tự chơi của họ trong các nhiệm vụ robot và trò chơi. Tuy nhiên, thường cần nhiều nỗ lực hơn để áp dụng PPO hoặc SAC vào việc tinh chỉnh quy mô lớn của LLM và làm cho chúng hoạt động ổn định. Do đó, vẫn chưa rõ ràng khung tự chơi của họ có thể được áp dụng như thế nào để căn chỉnh mô hình ngôn ngữ quy mô lớn một cách hiệu quả.

¹Khung SPO không liên quan đến việc tinh chỉnh hiệu quả của LLM. Tối Ưu Hóa Sở thích Tự Chơi (SPPO) của chúng tôi tập trung vào căn chỉnh LLM và được phát triển độc lập. Để phân biệt nó với khung SPO, chúng tôi sử dụng từ viết tắt SPPO.

2

--- TRANG 3 ---
Trong bài báo này, được thúc đẩy bởi những phát triển được đề cập ở trên, chúng tôi đề xuất một thuật toán tự chơi mới có (1) các đảm bảo có thể chứng minh để giải quyết trò chơi hai người tổng không đổi; và (2) có thể mở rộng quy mô lên việc tinh chỉnh hiệu quả quy mô lớn của các mô hình ngôn ngữ lớn. Cụ thể, chúng tôi xây dựng vấn đề RLHF như một trò chơi hai người tổng không đổi. Mục tiêu của chúng tôi là xác định chính sách cân bằng Nash, luôn cung cấp phản hồi được ưa thích hơn so với bất kỳ chính sách nào khác về mặt trung bình. Để xác định chính sách cân bằng Nash một cách xấp xỉ, chúng tôi áp dụng thuật toán thích ứng trực tuyến cổ điển với trọng số nhân (Freund và Schapire, 1999) làm khung cấp cao giải quyết trò chơi hai người. Hơn nữa, mỗi bước của khung cấp cao có thể được xấp xỉ bằng cơ chế tự chơi, trong đó ở mỗi vòng chính sách đang chơi chống lại chính nó trong vòng trước bằng cách tinh chỉnh nó trên dữ liệu tổng hợp được tạo ra bởi chính sách và được chú thích bởi mô hình sở thích.

Các đóng góp của chúng tôi được nổi bật như sau:

• Bắt đầu từ thuật toán cập nhật trọng số hàm mũ có thể chứng minh hội tụ về cân bằng Nash của trò chơi hai người tổng không đổi, chúng tôi đề xuất thuật toán Tối Ưu Hóa Sở thích Tự Chơi (SPPO) để căn chỉnh mô hình ngôn ngữ lớn. Thuật toán hội tụ về cân bằng Nash xấp xỉ một cách có thể chứng minh và có dạng hàm mất mát đơn giản để tối ưu hóa dễ dàng.

• Không giống như mất mát theo cặp đối xứng như DPO và Tối Ưu Hóa Sở thích Nhận dạng (IPO) (Azar et al., 2023), chúng tôi đề xuất một mục tiêu tối ưu hóa mới không dựa vào so sánh theo cặp. Mục tiêu mất mát mới (4.4), ban đầu được thúc đẩy bởi các khái niệm lý thuyết trò chơi, hóa ra có động lực mạnh mẽ từ lý thuyết gradient chính sách và ngầm khuyến khích LLM học một hàm giá trị tối ưu cấp token.

• Về mặt thực nghiệm, SPPO cải thiện đáng kể mô hình Mistral-7B-Instruct-v0.2 và Llama-3-8B-Instruct được căn chỉnh tốt, đạt được sự tăng hơn 11% về tỷ lệ thắng có kiểm soát độ dài so với GPT-4-Turbo trên bộ thử nghiệm AlpacaEval 2.0 (Dubois et al., 2024a). Ngoài ra, SPPO thể hiện khả năng tổng quát mạnh mẽ trên các nhiệm vụ khác nhau, bao gồm MT-Bench, Open LLM Leaderboard, và điểm chuẩn thách thức hơn gần đây, Arena-Hard. Không giống như DPO/IPO lặp, có xu hướng cho thấy sự suy giảm hiệu suất trên các điểm chuẩn khác khi được tối ưu hóa hướng tới điểm PairRM, lợi ích hiệu suất của SPPO là nhất quán. Đáng chú ý, tất cả hiệu suất mạnh mẽ được đạt được mà không có giám sát bên ngoài (ví dụ: phản hồi, sở thích, v.v.) từ GPT-4 hoặc các mô hình ngôn ngữ mạnh hơn khác.

Song song với công trình của chúng tôi, một số nghiên cứu, bao gồm Tối Ưu Hóa Nash Trực tiếp (Rosset et al., 2024) và REBEL (Gao et al., 2024) cũng đã khám phá việc sử dụng mất mát entropy chéo hoặc tối thiểu hóa mất mát bình phương để xấp xỉ cập nhật hàm mũ. Cụ thể, họ sử dụng cùng thủ thuật được đề xuất trong DPO (Rafailov et al., 2024b) để hủy bỏ yếu tố log-partition và hồi quy trực tiếp trên sự khác biệt tỷ lệ thắng. Tuy nhiên, Pal et al. (2024) đã chỉ ra về mặt lý thuyết và thực nghiệm rằng mất mát theo cặp chỉ có thể khiến khoảng cách khả năng tương đối trở nên lớn, nhưng không nhất thiết phải tăng khả năng của các phản hồi được ưa thích. Thay vào đó, phương pháp của chúng tôi có mối liên hệ sâu sắc hơn với lý thuyết gradient chính sách và có thể khớp hiệu quả khả năng của phản hồi với tỷ lệ thắng của nó.

3

--- TRANG 4 ---
2 Công trình liên quan

RLHF với Mô hình Phần thưởng Tường minh/Ngầm Ban đầu, học tăng cường từ phản hồi của con người (RLHF) được đề xuất bởi Christiano et al. (2017) như một phương pháp luận đầu tiên học một mô hình phần thưởng phản ánh sở thích của con người và sau đó sử dụng các thuật toán học tăng cường để tối đa hóa phần thưởng. Phương pháp luận này được áp dụng bởi Ouyang et al. (2022) để tinh chỉnh các mô hình ngôn ngữ lớn tuân thủ hướng dẫn và dẫn đến ChatGPT phổ biến.

Mô hình phần thưởng trong các công trình được đề cập ở trên giả định một mô hình tham số như mô hình Bradley-Terry (Bradley và Terry, 1952), gán một "điểm số" đại diện cho mức độ ưa thích của một phản hồi nhất định. Gần đây hơn, Rafailov et al. (2024b) đề xuất thay vào đó giải quyết trực tiếp nghiệm dạng đóng của điểm số như vậy được ngụ ý bởi mô hình Bradley-Terry. Phương pháp Tối Ưu Hóa Chính sách Trực tiếp (DPO) được cho là hiệu quả và ổn định hơn, tuy nhiên, vẫn ngầm giả định mô hình phần thưởng như vậy xác định "điểm số". Theo tinh thần tương tự, Zhao et al. (2023) đề xuất hiệu chỉnh điểm số sao cho điểm số của người thắng trong so sánh có một biên so với điểm số của người thua, và tạo ra một mất mát SLic khác. Tương tự, Ethayarajh et al. (2024) đã dẫn xuất một hàm mất mát khác (được gọi là KTO) từ hàm tiện ích con người Kahneman-Tversky, ngầm biểu thị một điểm số của phản hồi đã cho. Liu et al. (2023) đề xuất Tối Ưu Hóa Lấy mẫu Từ chối (RSO) sử dụng một mô hình sở thích để tạo ra các cặp sở thích với các ứng viên được lấy mẫu từ chính sách tối ưu; sau đó tối ưu hóa sở thích được áp dụng trên các cặp sở thích được lấy mẫu. Hong et al. (2024) đề xuất thuật toán Tối Ưu Hóa Sở thích Tỷ lệ Odds (ORPO) có thể thực hiện tinh chỉnh có giám sát và căn chỉnh sở thích trong một phiên huấn luyện mà không duy trì chính sách tham chiếu trung gian.

RLHF với Mô hình Sở thích Tổng quát Thường thì, sở thích của con người không hoàn toàn bắc cầu, và không thể được biểu diễn đầy đủ bằng một điểm số duy nhất. Azar et al. (2023) đề xuất một mục tiêu tối ưu hóa sở thích tổng quát dựa trên xác suất sở thích giữa một cặp phản hồi thay vì một điểm số của một phản hồi duy nhất. Họ tiếp tục đề xuất một mục tiêu học tập dựa trên ánh xạ nhận dạng của xác suất sở thích được gọi là IPO (Tối Ưu Hóa Sở thích với ánh xạ Nhận dạng), nhằm tối đa hóa xác suất thắng kỳ vọng của chính sách hiện tại so với một chính sách tham chiếu nhất định. Munos et al. (2023) xây dựng vấn đề RLHF với sở thích tổng quát như một trò chơi hai người, tổng không đổi, trong đó mỗi người chơi là một chính sách nhằm tối đa hóa xác suất phản hồi của nó được ưa thích so với đối thủ. Họ nhằm xác định chính sách cân bằng Nash của trò chơi này và đề xuất một thuật toán mirror-descent đảm bảo hội tụ lần lặp cuối của một chính sách với biểu diễn dạng bảng². Wang et al. (2024) đề xuất xác định chính sách cân bằng Nash cho MDP nhiều bước khi có mô hình sở thích tổng quát và chỉ ra rằng vấn đề có thể được giảm thiểu thành một trò chơi Markov hai người tổng không.

Lý thuyết của RLHF Cũng có một hướng nghiên cứu để phân tích RLHF và cung cấp các đảm bảo lý thuyết của nó. Zhu et al. (2023) nghiên cứu RLHF tiêu chuẩn với việc học phần thưởng riêng biệt và điều chỉnh mô hình và đề xuất một quá trình học phần thưởng bi quan có thể chứng minh học được một mô hình phần thưởng tuyến tính. Wang et al. (2024) đề xuất một khung để giảm thiểu bất kỳ vấn đề RLHF nào với mô hình phần thưởng thành một vấn đề RL tiêu chuẩn dựa trên phần thưởng. Ngoài ra, họ đề xuất xác định chính sách cân bằng Nash khi có mô hình sở thích tổng quát và chỉ ra rằng vấn đề có thể được giảm thiểu thành một trò chơi Markov hai người tổng không. Xiong et al. (2023) nghiên cứu bandit có ngữ cảnh được regularized reverse-KL cho RLHF trong các cài đặt khác nhau và đề xuất các thuật toán hiệu quả với các đảm bảo lý thuyết mẫu hữu hạn. Ye et al. (2024) nghiên cứu khả năng học lý thuyết của Học Nash từ Phản hồi Con người (NLHF) được regularized KL bằng cách xem xét cả cài đặt offline và online và đề xuất các thuật toán hiệu quả có thể chứng minh. Ji et al. (2024) đề xuất một thuật toán tối ưu hóa chính sách gần đúng dựa trên truy vấn tích cực với các ràng buộc hối tiếc và độ phức tạp truy vấn dựa trên chiều của vấn đề và khoảng cách tối ưu phụ.

Tinh chỉnh Tự Chơi Hầu hết các công trình được đề cập ở trên (Rafailov et al., 2024b; Zhao et al., 2023; Azar et al., 2023; Ethayarajh et al., 2024) xem xét một quy trình tối ưu hóa duy nhất bắt đầu từ một chính sách tham chiếu nào đó. Cùng một quy trình có thể được áp dụng lặp đi lặp lại cho nhiều vòng theo cách tự chơi. Trong mỗi vòng, dữ liệu mới được tạo ra bởi chính sách thu được trong vòng cuối; những dữ liệu mới này sau đó được sử dụng để huấn luyện một chính sách mới có thể vượt trội hơn chính sách cũ.

²Do biểu diễn dạng bảng, việc tính toán yếu tố chuẩn hóa là cấm và thuật toán được thực hiện xấp xỉ bằng cách lấy mẫu một token thay vì một phản hồi đầy đủ.

4

--- TRANG 5 ---
Tinh chỉnh tự chơi có thể được áp dụng cho cả hai tình huống có hoặc không có dữ liệu sở thích của con người. Ví dụ, Singh et al. (2023) đề xuất một khung Kỳ vọng-Tối đa hóa (EM) trong đó ở mỗi vòng, dữ liệu mới được tạo ra và chú thích với điểm phần thưởng; chính sách mới được thu được bằng cách tinh chỉnh chính sách trên dữ liệu có phần thưởng cao. Chen et al. (2024) đề xuất một khung tự chơi để tinh chỉnh mô hình theo cách có giám sát. Trong mỗi vòng, các cặp sở thích mới được tổng hợp bằng cách gắn nhãn các phản hồi được tạo ra bởi chính sách là người thua và các phản hồi được tạo ra bởi con người là người thắng. Sau đó DPO được áp dụng trong mỗi vòng để tinh chỉnh một chính sách khác dựa trên những dữ liệu sở thích được tổng hợp này. Yuan et al. (2024) đề xuất Mô hình Ngôn ngữ Tự Thưởng, trong đó chính mô hình ngôn ngữ được sử dụng để chú thích sở thích trên các phản hồi của chính nó. DPO lặp được áp dụng để tinh chỉnh các mô hình ngôn ngữ trên những dữ liệu được chú thích này. Những công trình này cho thấy tinh chỉnh lặp có thể cải thiện đáng kể hiệu suất.

Swamy et al. (2024) xem xét một cài đặt Quá trình Quyết định Markov (MDP) nhiều bước tổng quát hơn và đề xuất Tối Ưu Hóa Sở thích Tự chơi (SPO), một khung RLHF có thể sử dụng bất kỳ thuật toán học trực tuyến không hối tiếc nào cho tối ưu hóa chính sách dựa trên sở thích. Sau đó họ khởi tạo khung của mình với Lặp Chính sách Mềm như một biến thể lý tưởng hóa của thuật toán của họ, giảm xuống quy tắc cập nhật trọng số hàm mũ (4.1) khi bị giới hạn trong cài đặt bandit. Sự khác biệt chính là họ tập trung vào quá trình quyết định Markov (MDP) nhiều vòng trong các nhiệm vụ robot và trò chơi thay vì tinh chỉnh các mô hình ngôn ngữ lớn và xấp xỉ cập nhật bằng các phương pháp tối ưu hóa chính sách như PPO.

Song song với công trình của chúng tôi, Rosset et al. (2024) đề xuất thuật toán Tối Ưu Hóa Nash Trực tiếp (DNO) dựa trên entropy chéo giữa khoảng cách tỷ lệ thắng thực và dự đoán, và cung cấp các đảm bảo lý thuyết về lỗi của xấp xỉ mẫu hữu hạn. Tuy nhiên, phiên bản thực tế của họ vẫn sử dụng khung iterative-DPO như trong Xu et al. (2023) với mất mát DPO thay vì mất mát DNO được dẫn xuất của họ. Đáng chú ý, trong các thí nghiệm của họ, họ đã thêm các phản hồi được tạo ra bởi GPT-4 làm "mẫu vàng" vào dữ liệu tinh chỉnh của họ, và sử dụng GPT-4 làm thẩm phán để gán một điểm số cho mỗi phản hồi để xây dựng cặp sở thích. Trái ngược hoàn toàn, công trình của chúng tôi không yêu cầu sử dụng bất kỳ giám sát bên ngoài mạnh nào ngoài một mô hình phần thưởng kích thước nhỏ. Một công trình song song khác (Gao et al., 2024) đề xuất REBEL, một khung tinh chỉnh lặp thông qua hồi quy phần thưởng tương đối. Khi áp dụng vào cài đặt sở thích, nó dẫn đến một thuật toán tương tự như thuật toán SPPO của chúng tôi, ngoại trừ việc SPPO xấp xỉ yếu tố log-partition logZπt(x) với một hằng số η/2 trong khi REBEL hồi quy trên sự khác biệt tỷ lệ thắng (để logZπt(x) bị hủy bỏ). Ngoài ra, Calandriello et al. (2024) chỉ ra rằng việc tối ưu hóa mất mát IPO (Azar et al., 2023) lặp với dữ liệu được tạo ra bằng tự chơi tương đương với việc tìm cân bằng Nash của trò chơi hai người, và họ đề xuất thuật toán IPO-MD dựa trên quan sát này, tạo ra dữ liệu với chính sách hỗn hợp tương tự như thuật toán Nash-MD.

3 Kiến thức cơ bản

Chúng tôi xem xét tình huống học sở thích như sau. Cho một chuỗi văn bản (thường được gọi là prompt) x = [x1, x2, . . .], hai chuỗi văn bản y = [y1, y2, . . .] và y′ được tạo ra làm phản hồi cho prompt x. Một mô hình ngôn ngữ tự hồi quy π cho prompt x có thể tạo ra phản hồi y theo phân rã xác suất

π(y|x) = ∏(i=1 to N) π(yi|x,y<i).

Cho prompt x và hai phản hồi y và y′, một oracle sở thích (hoặc là một người chú thích con người hoặc một mô hình ngôn ngữ) sẽ cung cấp phản hồi sở thích o(y≻y′|x) ∈ {0,1} cho biết liệu y có được ưa thích hơn y′ hay không. Chúng tôi ký hiệu P(y≻y′|x) = E[o(y≻y′|x)] là xác suất của y "thắng cuộc đấu" so với y′. Độ phân kỳ KL của hai phân phối xác suất có mật độ p và q được định nghĩa là

KL(p∥q) = Ey∼p(y)[log p(y)/q(y)].

3.1 RLHF với Mô hình Phần thưởng

Christiano et al. (2017) đầu tiên học một hàm phần thưởng r(y;x) theo mô hình Bradley-Terry (Bradley và Terry, 1952). Đối với một bộ ba prompt-phản hồi-phản hồi (x,y,y′), mô hình Bradley-Terry xác định xác suất của y được chọn hơn y′ là

P(y≻y′|x) = exp(r(y;x))/(exp(r(y;x)) + exp(r(y′;x))) = σ(r(y;x) - r(y′;x)), (3.1)

trong đó σ(x) = ex/(ex + 1) là hàm logistic. Hàm phần thưởng liên kết với mô hình Bradley-Terry có thể được ước tính bằng cách tối đa hóa log-likelihood logP(y≻y′|x). Giả sử hàm phần thưởng thực r(y;x) có sẵn, Christiano et al. (2017) đề xuất giải quyết vấn đề tối ưu hóa sau với các thuật toán tối ưu hóa chính sách trong RL như PPO (Schulman et al., 2017):

max θ Ex∼X,y∼πθ(·|x)[r(y;x)] - η⁻¹Ex∼X[KL(πθ(·|x)∥πref(·|x))], (3.2)

trong đó X là phân phối prompt.

Rafailov et al. (2024b) xác định rằng vấn đề tối ưu hóa ở trên có nghiệm dạng đóng sao cho với bất kỳ y nào,

π*(y|x) ∝ πref(y|x) exp(ηr(y;x)),

6

--- TRANG 6 ---
có thể được chuyển đổi thêm thành mất mát DPO cho bất kỳ bộ ba (x,yw,yl) nào trong đó người thắng yw được chọn hơn người thua yl:

ℓDPO(x,yw,yl;θ;πref) := -log σ(η⁻¹[log πθ(yw|x)/πref(yw|x) - log πθ(yl|x)/πref(yl|x)]).

3.2 RLHF với Sở thích Tổng quát

Theo Wang et al. (2024); Munos et al. (2023), chúng tôi nhằm thiết lập các phương pháp RLHF mà không có mô hình phần thưởng, vì sở thích của con người có thể không bắc cầu (Tversky, 1969). Dưới một oracle sở thích tổng quát P(y≻y′|x), chúng tôi theo Dudík et al. (2015) và nhằm xác định người thắng cuộc von Neumann. Cụ thể hơn, người thắng cuộc von Neumann π* là cân bằng Nash (đối xứng) của trò chơi hai người tổng không đổi sau:

(π*, π*) = arg max π min π′ Ex∼X[Ey∼π(·|x),y′∼π′(·|x) P(y≻y′|x)]. (3.3)

Ngoài ra, chúng tôi định nghĩa xác suất thắng của một phản hồi y so với một phân phối phản hồi π là

P(y≻π|x) = Ey′∼π(·|x)[P(y≻y′|x)],

và xác suất thắng của một chính sách π so với một chính sách khác π′ là

P(π≻π′|x) = Ey∼π(·|x)Ey′∼π′(·|x)[P(y≻y′|x)].

Hơn nữa, chúng tôi định nghĩa P(π≻π′) = Ex∼X[P(π≻π′|x)], trong đó x là một prompt được rút từ phân phối prompt X. Trò chơi hai người tổng không đổi (3.3) có thể được đơn giản hóa thành

(π*, π*) = arg max π min π′ P(π≻π′).

4 Tối Ưu Hóa Sở thích Tự Chơi (SPPO)

Trong phần này, chúng tôi giới thiệu thuật toán Tối Ưu Hóa Sở thích Tự Chơi (SPPO), được dẫn xuất từ khung lý thuyết sau.

4.1 Khung Lý thuyết

Có những thuật toán nổi tiếng để giải quyết xấp xỉ cân bằng Nash trong một trò chơi hai người tổng không đổi. Trong công trình này, chúng tôi theo Freund và Schapire (1999) để thiết lập một khung lặp có thể hội tụ tiệm cận về chính sách tối ưu trên trung bình. Chúng tôi bắt đầu với một khung lý thuyết giải quyết khái niệm trò chơi hai người như sau:

πt+1(y|x) ∝ πt(y|x) exp(ηP(y≻πt|x)), cho t = 1,2, ... . (4.1)

(4.1) là một khung lặp dựa vào cập nhật trọng số nhân trong mỗi vòng t và có cấu trúc rõ ràng. Ban đầu, chúng ta có một chính sách cơ sở π1 thường từ một mô hình được tinh chỉnh có giám sát nào đó. Trong mỗi vòng, chính sách được cập nhật πt+1 được thu được từ chính sách tham chiếu πt theo cập nhật trọng số nhân. Cụ thể hơn, một phản hồi y nên có trọng số xác suất cao hơn nếu nó có lợi thế trung bình cao hơn so với chính sách hiện tại πt.

Tương đương, (4.1) có thể được viết là

πt+1(y|x) = πt(y|x) exp(ηP(y≻πt|x))/Zπt(x), (4.2)

trong đó Zπt(x) = Σy πt(y|x)exp(ηP(y≻πt|x)) là yếu tố chuẩn hóa (còn gọi là hàm phân vùng). Đối với bất kỳ x và y cố định nào, chính sách cập nhật lý tưởng πt+1 nên thỏa mãn phương trình sau:

log πt+1(y|x)/πt(y|x) = η·P(y≻πt|x) - log Zπt(x). (4.3)

Không giống như thiết kế theo cặp trong DPO hoặc IPO hủy bỏ yếu tố chuẩn hóa log logZπt(x) bằng cách lấy vi phân (4.3) giữa y và y′, chúng tôi chọn xấp xỉ (4.3) trực tiếp theo khoảng cách L2:

πt+1 = arg min π Ex∼X,y∼πt(·|x) [log π(y|x)/πt(y|x) - (ηP(y≻πt|x) - log Zπt(x))]². (4.4)

Ước tính Xác suất Mục tiêu tối ưu hóa (4.4) có thể được xấp xỉ với mẫu hữu hạn. Chúng tôi chọn lấy mẫu K phản hồi y1,y2, . . . ,yK ∼ πt(·|x) cho mỗi prompt x, và ký hiệu phân phối thực nghiệm bằng π̂Kt. Vấn đề tối ưu hóa mẫu hữu hạn có thể được xấp xỉ là

πt+1 = arg min π Ex∼X,y∼πt(·|x) [log π(y|x)/πt(y|x) - (ηP(y≻π̂Kt|x) - log Zπ̂Kt(x))]². (4.5)

Cụ thể, P(y≻π̂Kt|x) = ΣKk=1 P(y≻yk|x)/K và Zπ̂Kt(x) = Ey∼πt(·|x)[exp(ηP(y≻π̂Kt|x))]. Zπ̂Kt(x), được coi như một kỳ vọng, có thể được ước tính thêm bằng B mẫu mới với tổng cộng O(KB) truy vấn của oracle sở thích P. (4.5) là một vấn đề tối ưu hóa có thể giải quyết hiệu quả. Một cách không chính thức, khi K→∞, (4.5) sẽ khôi phục (4.4). Chúng tôi có đảm bảo sau về sự hội tụ của (4.4):

Định lý 4.1. Giả sử vấn đề tối ưu hóa (4.4) là khả thi. Ký hiệu πt là chính sách thu được thông qua (4.4) và chính sách hỗn hợp π̄T = 1/T ΣTt=1 πt. Bằng cách đặt η = Θ(1/√T), chúng ta có

max π P(π≻π̄T) - min π P(π≺π̄T) = O(1/√T).

Định lý 4.1 đặc trưng tỷ lệ hội tụ của chính sách trung bình qua khung thời gian T hướng về cân bằng Nash, theo khoảng cách đối ngẫu. Chứng minh dựa trên Định lý 1 trong Freund và Schapire (1999) với sửa đổi nhỏ. Để hoàn chỉnh, chúng tôi bao gồm chứng minh trong Phụ lục B.

Thay vào đó, chúng ta có thể tránh ước tính log Zπ̂Kt(x) bằng cách thay thế nó bằng một hằng số dựa trên mô hình sở thích con người. Việc lựa chọn hằng số được thảo luận chi tiết trong Phụ lục A. Ở đây, chúng tôi thay thế log Zπ̂Kt(x) bằng η/2³ trong (4.5) để thu được một mục tiêu rõ ràng hơn:

πt+1 = arg min π Ex∼X,y∼πt(·|x) [log π(y|x)/πt(y|x) - η(P(y≻π̂Kt|x) - 1/2)]². (4.6)

³Giả sử xác suất thắng giữa bất kỳ cặp nào đều là 1 hoặc 0 với cơ hội bằng nhau, khi K→∞, chúng ta có thể chỉ ra rằng thực sự Zπ̂Kt(x)→eη/2. Cũng xem Phụ lục A để có dẫn xuất đầy đủ.

8

--- TRANG 7 ---
Thuật toán 1 Tối Ưu Hóa Sở thích Tự Chơi (SPPO)
1: đầu vào: chính sách cơ sở πθ1, oracle sở thích P, tỷ lệ học η, số mẫu phản hồi được tạo ra K.
2: cho t = 1,2, ... làm
3: Tạo phản hồi tổng hợp bằng cách lấy mẫu x ∼ X và y1:K ∼ πt(·|x).
4: Chú thích tỷ lệ thắng P(yk≻yk′|x), ∀k, k′ ∈ [K].
5: Chọn phản hồi từ y1:K để tạo bộ dữ liệu Dt = {(xi,yi,P̂(yi≻πt|xi))}i∈[N].
6: Tối ưu hóa πθt+1 theo (4.6):
θt+1 ← arg min θ E(x,y,P̂(y≻πt|x))∼Dt [log πθ(y|x)/πt(y|x) - η(P̂(y≻πt|x) - 1/2)]². (4.7)
7: kết thúc cho

Trực quan, nếu hòa xảy ra (tức là P(y≻π̂Kt|x) = 1/2), chúng tôi ưu tiên mô hình không cập nhật trọng số tại y. Nếu y thắng π̂Kt trên trung bình (tức là P(y≻π̂Kt|x) > 1/2), thì chúng tôi tăng mật độ xác suất tại y để sử dụng lợi thế của y so với π̂Kt. Trong các thí nghiệm của chúng tôi, chúng tôi chọn tối thiểu hóa mục tiêu (4.6).

4.2 Thuật toán SPPO

Dựa trên khung lý thuyết được đề cập ở trên, chúng tôi đề xuất thuật toán Tối Ưu Hóa Sở thích Tự Chơi trong Thuật toán 1. Trong mỗi vòng t, Thuật toán 1 sẽ đầu tiên tạo ra K phản hồi y1,y2, . . . ,yK theo πt(·|x) cho mỗi prompt x (Dòng 3). Sau đó, oracle sở thích P sẽ được truy vấn để tính tỷ lệ thắng giữa K phản hồi (Dòng 4). Tại Dòng 5, một số tiêu chí có thể được áp dụng để xác định phản hồi nào nên được giữ trong bộ dữ liệu được xây dựng Dt và xây dựng bộ ba prompt-phản hồi-xác suất (x,y,P̂(y≻πt|x)). Chúng tôi sẽ thảo luận các lựa chọn thiết kế sau trong Phần 5. Một lựa chọn thiết kế đơn giản là bao gồm tất cả K phản hồi vào Dt và mỗi P̂(yi≻πt|x) được ước tính bằng cách so sánh yi với tất cả K phản hồi. Tổng cộng, O(K²) truy vấn sẽ được thực hiện. Sau đó thuật toán sẽ tối ưu hóa (4.6) trên bộ dữ liệu Dt (Dòng 6).

4.3 Mối liên hệ với Policy Gradient

Trong khi SPPO được dẫn xuất từ khung lặp (Freund và Schapire, 1999) cho trò chơi hai người, mất mát bình phương trong mục tiêu SPPO (4.4) cung cấp một cách diễn giải thay thế cho SPPO như một biến thể semi-online của phương pháp policy gradient do dạng mất mát đặc biệt của nó. Sự khác biệt so với policy gradient tiêu chuẩn là nó thu thập mẫu từ πθt tại đầu lặp t, thay vì thực hiện lấy mẫu on-policy tại mỗi bước gradient.

Xem xét một hàm phần thưởng tổng quát r(y;x), vấn đề RLHF (3.2) có thể được viết là:

max θ J(θ) := Ex∼X,y∼πθ(·|x) [r(y;x) - η⁻¹log πθ(y|x)/πref(y|x)]. (4.8)

Policy gradient của mục tiêu J(θ) là:

∇J(θ) = Ex∼X,y∼πθ(·|x) [(r(y;x) - η⁻¹log πθ(y|x)/πref(y|x) - b(x))∇log πθ(y|x)] (4.9)

9

--- TRANG 8 ---
= ηEx∼X,y∼πθ(·|x) [-∇(r(y;x) - η⁻¹log πθ(y|x)/πref(y|x) - b(x))²], (4.10)

trong đó dòng đầu tiên theo định lý policy gradient (Sutton et al., 1999) và baseline b(x) là một hằng số tùy ý chỉ phụ thuộc vào x được sử dụng để giảm phương sai. So sánh mất mát bình phương (4.10) với mục tiêu SPPO (4.4) (được viết lại bên dưới):

θt+1 = arg min θ Ex∼X,y∼πθt(·|x) [P(y≻πθt|x) - η⁻¹log πθ(y|x)/πθt(y|x) - η⁻¹log Zπθt(x)]²,

người ta có thể thấy rằng tỷ lệ thắng P(y≻πθt|x) chính xác là phần thưởng mà SPPO nhằm tối đa hóa, và η⁻¹log Zπθt(x) thực tế là baseline tốt nhất có thể - hàm giá trị (mềm). Khi hàm giá trị không có sẵn trong thực tế, nó có thể được thay thế bằng bất kỳ baseline hằng số nào để giảm phương sai của policy gradient. Chúng tôi chọn 1/2 như một xấp xỉ tốt cho η⁻¹log Zπθt(x) nhưng hằng số có thể thay đổi tùy thuộc vào mô hình sở thích con người (xem Phụ lục A). Phương trình (4.9) cũng được thảo luận trong Munos et al. (2023).

So sánh với khung tổng quát được đề xuất bởi Swamy et al. (2024), SPPO có thể được xem như một biến thể mới, đơn giản của phương pháp policy gradient mà không cần các sửa đổi bổ sung như gradient clipping trong PPO, tính toán Hessian trong TRPO, hoặc duy trì nhiều thành phần (Q-critic, V-critic, actor, v.v.) trong nhiều thuật toán tối ưu hóa chính sách.

4.4 Q*-Learning Cấp Token

Rafailov et al. (2024a) đã chỉ ra rằng dưới công thức Max-Entropy RL, log-ratio cấp token log πθ(y|x)/πref(y|x) có thể được xem như một hàm phần thưởng hoặc lợi thế cấp token ngầm (bất biến dưới reward shaping). Dưới đây chúng tôi chỉ ra mất mát bình phương trong SPPO cũng có thể dẫn đến chính sách Max-Entropy tối ưu π*, với hàm giá trị/lợi thế cấp token tối ưu.

Chúng tôi đầu tiên tóm tắt ngắn gọn cài đặt và kết quả trong Rafailov et al. (2024b). MDP cấp token định nghĩa trạng thái sh = (x, y1, y2, . . . , yh-1) như các token tiền tố, và hành động ah = yh là token tiếp theo. Một mô hình ngôn ngữ tự hồi quy π(y|x) có thể được xem như một chính sách cấp token π(ah|sh) và kernel chuyển tiếp được biết và xác định vì nó chỉ nối token tiếp theo vào tiền tố để tạo thành một chuỗi token mới sh+1 = (x, y1, y2, . . . , yh).

Cài đặt Max-Entropy RL một lần nữa xem xét vấn đề tối đa hóa phần thưởng được regularized reverse-KL (3.2):

max θ Ex∼X,y∼πθ(·|x)[r(y;x)] - η⁻¹Ex∼X[KL(πθ(·|x)∥πref(·|x))]
= Ex∼X,y∼πθ(·|x)[r(y;x) + η⁻¹log πref(y|x)] + η⁻¹Ex∼X[H(πθ(·|x))].

Chúng tôi ký hiệu nghiệm tối ưu cho vấn đề trên là π*. Rafailov et al. (2024a) đã chỉ ra rằng mô hình sở thích Bradley-Terry (4.13) có thể được viết lại là:

P(yw≻yl|x) = σ(η⁻¹Σh=1^|yw| log π*(awh|swh)/πref(awh|swh) - η⁻¹Σh=1^|yl| log π*(alh|slh)/πref(alh|slh)),

trong đó trạng thái và hành động được định nghĩa như trong MDP cấp token được giới thiệu ở trên, với chỉ số trên (·)w và (·)l ký hiệu nếu nó dành cho người thắng yw hoặc người thua yl. Và tối đa hóa log likelihood với π* được thay thế bằng πθ cho ra mất mát DPO.

10

--- TRANG 9 ---
Từ bây giờ chúng tôi giả sử horizon được cố định tại H để đơn giản. Việc dẫn xuất công thức Max-Entropy RL dựa vào hàm giá trị (mềm) tối ưu Q* và V*⁴:

V*(sH+1) = r(sH+1) := r(y;x), (phần thưởng tại EOS)
Q*(sh,ah) = η⁻¹log πref(ah|sh) + V*(sh+1),
V*(sh) = η⁻¹log Σa exp(ηQ*(sh,a)), khi h ≤ H.

Rafailov et al. (2024a) đã chỉ ra rằng chính sách tối ưu π* thỏa mãn:

η⁻¹log π*(ah|sh) = Q*(sh,ah) - V*(sh)
= η⁻¹log πref(ah|sh) + V*(sh+1) - V*(sh).

Có thể được xác minh rằng với s1 = (x), chúng ta có ηV*(s1) = log Σy πref(y|x) exp(ηr(y;x)):

exp(ηV*(s1)) = Σa1 exp(ηQ*(s1,a1))
= Σa1 πref(a1|s1) exp(ηV*(s2))
= Σa1,a2 πref(a1|s1)πref(a2|s2) exp(ηV*(s3))
···
= Σ(a1,a2,...,aH) ∏h=1^H πref(ah|sh) exp(ηr(sH+1))
= Σy πref(y|x) exp(ηr(y;x)).

Quay trở lại mục tiêu SPPO (4.4) tại lần lặp thứ t, nếu chúng ta đặt πref = πt và r(y;x) = P(y≻πt|x), chúng ta có V*(s1) = η⁻¹log Zπt(x), và mục tiêu học tập tại lần lặp thứ t trở thành:

πt+1 = arg min π Ex∼X,y∼πt(·|x) [log π(y|x)/πt(y|x) - (ηP(y≻πt|x) - log Zπt(x))]²
= arg min π Es1∼X,ah∼πt(·|sh) [Σh=1^H log π(ah|sh)/π*(ah|sh)]². (4.11)

Tương tự như DPO, SPPO "bí mật" khuyến khích chính sách πθ hội tụ về chính sách tối ưu π* ở cấp token thông qua dạng mất mát bình phương (4.11). Ngoài ra, người ta có thể nhận ra rằng việc tối thiểu hóa dạng mất mát bình phương liên quan đến việc tối thiểu hóa độ phân kỳ KL KL(πθ∥π*) thông qua policy gradient:

∇θKL(πθ∥π*) = Es1∼X,ah∼πθ(·|sh) [Σh=1^H log πθ(ah|sh)/π*(ah|sh) · Σh=1^H ∇θlog πθ(ah|sh)]

⁴Ở đây chúng tôi trình bày lại với phần thưởng cấp chuỗi r(y;x). Rafailov et al. (2024a) bắt đầu dẫn xuất của họ từ một phần thưởng cấp token ground-truth r(sh,ah), không được xác định đầy đủ do vấn đề reward reshaping (Ng et al., 1999): việc reshape phần thưởng sẽ không ảnh hưởng đến xác suất sở thích Bradley-Terry nên không thể khôi phục phần thưởng ground-truth từ tín hiệu sở thích (Rafailov et al., 2024a, Phần 4.2).

11

--- TRANG 10 ---
= Es1∼X,ah∼πθ(·|sh) [∇θ(Σh=1^H log πθ(ah|sh)/π*(ah|sh))²].

4.5 So sánh với DPO, IPO, và KTO

Trong thực tế, chúng tôi sử dụng mini-batch có hơn 2 phản hồi để ước tính tỷ lệ thắng của một phản hồi nhất định, trong khi mất mát DPO và IPO tập trung vào một cặp phản hồi duy nhất. Khi chỉ có một cặp phản hồi yw và yl có sẵn, chúng ta có mất mát theo cặp đối xứng dựa trên bộ ba sở thích (x,yw,yl) được định nghĩa là:

ℓSPPO(x,yw,yl;θ;πref) := [log πθ(yw|x)/πref(yw|x) - η(P(yw≻yl|x) - 1/2)]²
+ [log πθ(yl|x)/πref(yl|x) - η(P(yw≺yl|x) - 1/2)]², (4.12)

trong đó P(yw≻yl|x) có thể là một xác suất mềm trong [0,1] hoặc một nhãn cứng 1 cho biết yw≻yl.

Bây giờ chúng tôi so sánh mất mát SPPO với các baseline khác giả sử một nhãn cứng yw≻yl được cho. Để dễ so sánh, đặt (β = η⁻¹):

a = β log πθ(yw|x)/πref(yw|x), b = β log πθ(yl|x)/πref(yl|x), c = βKL(πθ∥πref),

thì chúng ta có

ℓDPO(yw,yl,x) = -log σ(a-b), (4.13)
ℓIPO(yw,yl,x) = [(a-b) - 1]², (4.14)
ℓKTO(yw,yl,x) = σ(-a+c) + σ(b-c) (đã đơn giản hóa), (4.15)

trong đó σ(x) = ex/(ex + 1) và mất mát SPPO có thể được viết là

ℓSPPO(yw,yl,x) = (a - 1/2)² + (b + 1/2)².

Có thể thấy rằng SPPO không chỉ đẩy khoảng cách giữa a và b lên 1, mà còn cố gắng đẩy giá trị của a gần 1/2 và giá trị của b gần -1/2 sao cho πθ(yw|x) > πref(yw|x) và πθ(yl|x) < πref(yl|x). Chúng tôi tin rằng điều này đặc biệt quan trọng: khi các cặp sở thích khan hiếm (ví dụ: một cặp cho mỗi prompt), không có đảm bảo rằng log-ratio của người thắng aw sẽ tăng và log-ratio của người thua b sẽ giảm. Thay vào đó, chỉ có khoảng cách giữa người thắng và người thua (tức là a-b) sẽ tăng. Hiện tượng này được quan sát bởi Pal et al. (2024) rằng DPO chỉ làm giảm likelihood của người thua, nhưng hầu như không thay đổi likelihood của người thắng.

Như đã thảo luận ở trên, việc fit βlog πt+1(y|x)/πt(y|x) trực tiếp với P(y≻πt|x) - 1/2 dưới mất mát bình phương có liên quan chặt chẽ với policy gradient. Điều này giải thích tại sao SPPO hiệu quả hơn IPO, cái mà cố gắng fit βlog πt+1(yw|x)/πt(yw|x) - βlog πt+1(yl|x)/πt(yl|x) với P(yw≻πt|x) - P(yl≻πt|x). Ngoài ra, SPPO chia sẻ tinh thần tương tự như KTO. Mất mát KTO đẩy a lên lớn bằng cách tối thiểu hóa σ(-a+c) và đẩy b xuống nhỏ bằng cách tối thiểu hóa σ(b-c). Ngược lại, SPPO đẩy a lên lớn bằng 1/2 và b xuống nhỏ bằng -1/2.

Mặt khác, chúng tôi muốn bình luận rằng mặc dù DPO và KTO có thể được mở rộng thành các biến thể lặp của chúng, về bản chất chúng không phải là thuật toán lặp và không có đảm bảo có thể chứng minh rằng chúng có thể đạt được cân bằng Nash. Ngược lại, SPPO và IPO được thiết kế để có khả năng giải quyết cân bằng Nash một cách lặp. SPPO vượt trội hơn IPO vì thiết kế của nó một cách rõ ràng giảm nhẹ vấn đề khan hiếm dữ liệu, như đã thảo luận ở trên và chi tiết trong Pal et al. (2024).

12

--- TRANG 11 ---
5 Thí nghiệm

5.1 Thiết lập Thí nghiệm

Mô hình Cơ sở và Bộ dữ liệu Chúng tôi theo thiết lập thí nghiệm của Snorkel⁵, một mô hình sử dụng DPO lặp để đạt được hiệu suất hiện đại trên các điểm chuẩn AlpacaEval. Cụ thể, chúng tôi sử dụng Mistral-7B-Instruct-v0.2 làm mô hình cơ sở⁶. Mistral-7B-Instruct-v0.2 là một phiên bản được tinh chỉnh hướng dẫn của mô hình Mistral-7B-v0.2 (Jiang et al., 2023a). Chúng tôi cũng áp dụng Ultrafeedback (Cui et al., 2023) làm nguồn prompt bao gồm khoảng 60k prompt từ các nguồn đa dạng. Trong quá trình tạo, chúng tôi theo template chat tiêu chuẩn của Mistral-7B. Để tránh overfitting trong quá trình tinh chỉnh, chúng tôi chia bộ dữ liệu thành ba phần và chỉ sử dụng một phần cho mỗi lần lặp. Những thiết lập này cũng được áp dụng bởi việc huấn luyện mô hình Snorkel-Mistral-PairRM-DPO⁷ (Snorkel). Chúng tôi theo phân chia trong Snorkel để so sánh công bằng. Ngoài ra, chúng tôi sử dụng Llama-3-8B-Instruct⁸ làm mô hình cơ sở mạnh hơn cùng với cùng bộ dữ liệu sở thích và phân chia dữ liệu.

Mô hình Sở thích Chúng tôi sử dụng PairRM (Jiang et al., 2023b), một mô hình sở thích theo cặp hiệu quả có kích thước 0.4B. PairRM dựa trên DeBERTA-V3 (He et al., 2021) và được huấn luyện trên các bộ dữ liệu sở thích con người chất lượng cao. Kết quả trên các điểm chuẩn như bộ dữ liệu Auto-J Pairwise (Li et al., 2023a) cho thấy nó vượt trội hơn hầu hết các mô hình phần thưởng dựa trên mô hình ngôn ngữ và hoạt động tương đương với các mô hình phần thưởng lớn hơn như UltraRM-13B (Cui et al., 2023). Chúng tôi giới thiệu độc giả đến trang chủ trên Huggingface⁹ để biết kết quả điểm chuẩn chi tiết. Do đó chúng tôi giữ PairRM làm mô hình xếp hạng theo Snorkel để cân bằng giữa độ chính xác và hiệu quả.

Cụ thể, PairRM sẽ xuất ra một "phần thưởng tương đối" s(y,y′;x) phản ánh sự khác biệt độ mạnh giữa y và y′, tức là,

P(y≻y′|x) = exp(s(y,y′;x))/(1 + exp(s(y,y′;x))).

Không giống như mô hình phần thưởng dựa trên Bradley-Terry, PairRM chỉ gán phần thưởng tương đối không được đảm bảo bắc cầu (tức là s(y1,y2;x) + s(y2,y3;x) ≠ s(y1,y3;x)). Vì vậy nó thực sự mô hình hóa sở thích tổng quát.

Tạo và Lựa chọn Phản hồi Trong giai đoạn tạo trong mỗi lần lặp, chúng tôi sử dụng top p = 1.0 và temperature 1.0 để lấy mẫu từ chính sách hiện tại. Chúng tôi lấy mẫu với các seed ngẫu nhiên khác nhau để có K = 5 phản hồi khác nhau cho mỗi prompt. Các công trình trước sử dụng DPO Lặp chọn 2 phản hồi để tạo thành một cặp cho mỗi prompt. Để so sánh công bằng, chúng tôi không bao gồm tất cả K = 5 phản hồi trong dữ liệu sở thích mà chọn hai phản hồi trong số chúng. Theo Snorkel,

⁵https://huggingface.co/snorkelai/Snorkel-Mistral-PairRM-DPO
⁶https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
⁷https://huggingface.co/snorkelai/Snorkel-Mistral-PairRM-DPO
⁸https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
⁹https://huggingface.co/llm-blender/PairRM

13

--- TRANG 12 ---
chúng tôi chọn người thắng yw và người thua yl là phản hồi có điểm PairRM cao nhất và thấp nhất, được định nghĩa cho mỗi phản hồi yi là:

sPairRM(yi;x) := (1/K) ΣKk=1 s(yi,yk;x).

Ước tính Xác suất Sau đó chúng tôi ước tính tỷ lệ thắng so với phân phối bằng tỷ lệ thắng trung bình so với tất cả các phản hồi được lấy mẫu như được giải thích trong (4.5):

P̂(yi≻πt|x) = (1/K) ΣKk=1 P(yi≻yk|x), ∀i ∈ [K].

Điều chỉnh Siêu tham số Các thí nghiệm được tiến hành trên 8 × GPU Nvidia A100. Đối với SPPO, chúng tôi huấn luyện tổng cộng ba lần lặp. Trong mỗi lần lặp, chúng tôi chọn mô hình được huấn luyện trên epoch đầu tiên của 20k prompt từ UltraFeedback để tiến tới lần lặp tiếp theo. Đối với cả Mistral-7B-Instruct-v0.2 và Llama-3-8B-Instruct, kích thước batch huấn luyện toàn cầu được đặt thành 64, và η được đặt thành 1e-3. Lịch trình tỷ lệ học được xác định bởi các siêu tham số sau: learning rate = 5.0e-7, tổng số epoch huấn luyện = 18, tỷ lệ warmup = 0.1, lịch trình tuyến tính. Các siêu tham số tốt nhất cho mỗi mô hình được chọn bằng tỷ lệ thắng trung bình (được đánh giá bởi PairRM-0.4B) trên một tập con riêng của Ultrafeedback làm metric. Để biết thêm chi tiết về so sánh tỷ lệ thắng sử dụng PairRM làm thẩm phán, vui lòng tham khảo Phần 5.2 và Hình 3.

Baseline Chúng tôi đánh giá các mô hình cơ sở sau cũng như các phương pháp baseline để tinh chỉnh LLM:

• Mistral-7B-Instruct-v0.2: Mistral-7B-Instruct-v0.2 là một phiên bản được tinh chỉnh hướng dẫn của mô hình Mistral-7B-v0.2 (Jiang et al., 2023a). Đây là điểm khởi đầu của thuật toán chúng tôi.

• Snorkel (Mistral-PairRM-DPO): Chúng tôi trực tiếp đánh giá checkpoint được tải lên trên HuggingFace¹⁰. Mô hình này được thu được bằng ba vòng DPO lặp từ Mistral-7B-Instruct-v0.2.

• (Lặp) DPO: Chúng tôi cũng triển khai thuật toán DPO lặp bằng chính chúng tôi. Các thiết lập thí nghiệm và sơ đồ lựa chọn mô hình phù hợp với những cái được sử dụng cho SPPO, ngoại trừ việc áp dụng hàm mất mát DPO như được định nghĩa trong (4.13). Các siêu tham số được tối ưu hóa để tối đa hóa tỷ lệ thắng trung bình được đánh giá bởi PairRM tại mỗi lần lặp. Lưu ý rằng thuật toán thực tế trong Rosset et al. (2024) về cơ bản giống như DPO lặp.

• (Lặp) IPO: Chúng tôi triển khai thuật toán IPO lặp bằng chính chúng tôi. Thiết lập thí nghiệm và sơ đồ lựa chọn mô hình giống như DPO lặp, ngoại trừ việc hàm mất mát là mất mát IPO (4.14). Để so sánh công bằng, các siêu tham số cho IPO cũng được chọn bằng đánh giá sử dụng tỷ lệ thắng PairRM trung bình trên tập con riêng của Ultrafeedback.

• Self-rewarding LM: Yuan et al. (2024) đề xuất nhắc chính LLM làm thẩm phán sở thích để xây dựng các cặp sở thích mới và tinh chỉnh LLM lặp với thuật toán DPO. Chúng tôi sử dụng tỷ lệ thắng AlpacaEval 2.0 được báo cáo bởi Yuan et al. (2024) để so sánh. Lưu ý rằng Self-rewarding LM được huấn luyện từ Llama 2 70B.

¹⁰https://huggingface.co/snorkelai/Snorkel-Mistral-PairRM-DPO

14

--- TRANG 13 ---
• Llama-3-8B-Instruct: Llama-3-8B-Instruct là một mô hình được tinh chỉnh hướng dẫn được tối ưu hóa cho các trường hợp sử dụng đối thoại và vượt trội hơn nhiều mô hình chat mã nguồn mở có sẵn trên các điểm chuẩn ngành phổ biến.

Điểm chuẩn Theo các công trình trước, chúng tôi sử dụng AlpacaEval 2.0 (Dubois et al., 2024a), Arena-Hard (Li et al., 2024), MT-Bench (Zheng et al., 2024), và Open LLM Leaderboard (Beeching et al., 2023a) làm các điểm chuẩn đánh giá.

• AlpacaEval 2.0 là một điểm chuẩn đánh giá tự động dựa trên LLM. Nó sử dụng AlpacaFarm (Dubois et al., 2024b) làm tập prompt được composed từ các hướng dẫn con người tổng quát. Các phản hồi của mô hình và phản hồi tham chiếu được tạo ra bởi GPT-4-Turbo được đưa vào một annotator dựa trên GPT-4-Turbo để được đánh giá. Chúng tôi theo cách tiếp cận tiêu chuẩn và báo cáo tỷ lệ thắng so với các phản hồi tham chiếu.

• Arena-Hard (Li et al., 2024) là một điểm chuẩn chất lượng cao tuyên bố khó hơn và có tương quan và khả năng tách biệt cao nhất với Chatbot Arena trong các điểm chuẩn LLM mở phổ biến bao gồm AlpacaEval 2.0. Chúng tôi đánh giá các mô hình Mistral-PairRM-SPPO và các mô hình baseline.

• MT-Bench (Zheng et al., 2024) là một tập hợp 80 câu hỏi mở nhiều lượt chất lượng cao. Các câu hỏi bao gồm các chủ đề như viết, nhập vai, toán, lập trình, v.v. Câu trả lời được tạo ra được đánh giá bởi GPT-4 và được cho điểm trực tiếp mà không có so sánh theo cặp.

• Open LLM Leaderboard (Beeching et al., 2023a) bao gồm sáu bộ dữ liệu, mỗi bộ tập trung vào một khía cạnh của đánh giá mô hình ngôn ngữ. Chi tiết, rubric đánh giá bao gồm giải quyết vấn đề toán học, hiểu ngôn ngữ, bắt chước sự giả dối của con người, và lý luận. Chúng tôi theo quy trình đánh giá tiêu chuẩn và sử dụng học trong ngữ cảnh để nhắc mô hình ngôn ngữ và tính điểm trung bình trên sáu bộ dữ liệu để đo lường hiệu suất.

5.2 Kết quả Thí nghiệm

Đánh giá sử dụng GPT-4 làm thẩm phán Đánh giá con người vẫn là điểm chuẩn cho chất lượng và độ chính xác (Askell et al., 2021; Ouyang et al., 2022). Tuy nhiên, do hạn chế về khả năng mở rộng và tái sản xuất, chúng tôi khám phá cách tiếp cận thay thế sử dụng khả năng tiên tiến của GPT-4 (OpenAI et al., 2023) như một công cụ đánh giá tự động. Chúng tôi tiến hành đánh giá tự động dựa trên GPT-4 trên AlpacaEval 2.0 (Li et al., 2023b), MT-Bench (Zheng et al., 2023), và Arena-Hard (Li et al., 2024) để đo lường khả năng chatbot của mô hình chúng tôi. Kết quả có thể được tìm thấy trong Bảng 1 cho AlpacaEval 2.0, Hình 2 (trái) cho MT-Bench, và Hình 2 (phải) cho Arena-Hard. Chúng tôi thấy rằng hiệu suất của các mô hình SPPO liên tục cải thiện qua tất cả các lần lặp.

Bảng 1 (AlpacaEval 2.0) cho thấy tỷ lệ thắng so với baseline GPT-4-Turbo của các mô hình khác nhau trên 805 prompt. Chúng tôi cũng bao gồm một cột cho biết tỷ lệ thắng có kiểm soát độ dài, và một cột về độ dài trung bình của mỗi mô hình, để tính đến xu hướng của thẩm phán dựa trên LLM thiên về các đầu ra chuỗi dài hơn — một vấn đề được gọi thông thường là hiện tượng "reward hacking".

15

--- TRANG 14 ---
Bảng 1: Đánh giá AlpacaEval 2.0 của các mô hình khác nhau (chi tiết trong Baseline) theo tỷ lệ thắng bình thường và có kiểm soát độ dài (LC) theo phần trăm (%). Mô hình Mistral-7B-SPPO Iter3 đạt được tỷ lệ thắng LC cao nhất là 28.53% và tỷ lệ thắng bình thường là 31.02%. SPPO thể hiện sự gia tăng hiệu suất ổn định qua các lần lặp và vượt trội hơn các baseline khác có xu hướng tạo ra phản hồi dài hơn. Ngoài ra, việc xếp hạng lại với mô hình phần thưởng PairRM (best-of-16) tại thời điểm thử nghiệm liên tục tăng cường hiệu suất trên tất cả các mô hình và SPPO (best-of-16) đạt được tỷ lệ thắng cao mà không có giám sát bên ngoài mạnh như GPT-4. Chúng tôi bổ sung bao gồm các kết quả thu được từ tinh chỉnh Llama-3-8B-Instruct, cũng cho thấy sự cải thiện hiệu suất ổn định.

| Mô hình | AlpacaEval 2.0 |
|---------|----------------|
|         | Tỷ lệ thắng LC | Tỷ lệ thắng | Độ dài TB |
| Mistral-7B-Instruct-v0.2 | 17.11 | 14.72 | 1676 |
| Mistral-7B-Instruct-v0.2 (best-of-16) | 22.45 | 17.94 | 1529 |
| Snorkel (Mistral-PairRM-DPO) | 26.39 | 30.22 | 2736 |
| Snorkel (Mistral-PairRM-DPO best-of-16) | 29.97 | 34.86 | 2616 |
| Self-Rewarding 70B Iter1 | - | 9.94 | 1092 |
| Self-Rewarding 70B Iter2 | - | 15.38 | 1552 |
| Self-Rewarding 70B Iter3 | - | 20.44 | 2552 |
| Mistral-7B-DPO Iter1 | 23.81 | 20.44 | 1723 |
| Mistral-7B-DPO Iter2 | 24.23 | 24.46 | 2028 |
| Mistral-7B-DPO Iter3 | 22.30 | 23.39 | 2189 |
| Mistral-7B-IPO Iter1 | 23.78 | 20.77 | 1693 |
| Mistral-7B-IPO Iter2 | 21.08 | 23.38 | 2660 |
| Mistral-7B-IPO Iter3 | 20.06 | 22.47 | 2760 |
| Mistral-7B-SPPO Iter1 | 24.79(+7.69) | 23.51(+8.79) | 1855 |
| Mistral-7B-SPPO Iter2 | 26.89(+2.10) | 27.62(+4.11) | 2019 |
| Mistral-7B-SPPO Iter3 | 28.53(+1.64) | 31.02(+3.40) | 2163 |
| Mistral-7B-SPPO Iter1 (best-of-16) | 28.71(+6.26) | 27.77(+9.83) | 1901 |
| Mistral-7B-SPPO Iter2 (best-of-16) | 31.23(+2.52) | 32.12(+4.35) | 2035 |
| Mistral-7B-SPPO Iter3 (best-of-16) | 32.13(+0.9) | 34.94(+2.82) | 2174 |
| Llama-3-8B-Instruct | 22.92 | 22.57 | 1899 |
| Llama-3-8B-SPPO Iter1 | 31.73(+8.81) | 31.74(+9.17) | 1962 |
| Llama-3-8B-SPPO Iter2 | 35.15(+3.42) | 35.98(+4.24) | 2021 |
| Llama-3-8B-SPPO Iter3 | 38.77(+3.62) | 39.85(+3.87) | 2066 |

Theo bảng, Mistral-7B-SPPO Iter3 có tỷ lệ thắng cao nhất, 28.52% cho phiên bản có kiểm soát độ dài, và 31.02% cho tỷ lệ thắng tổng thể.

Các gia tăng hiệu suất so với các lần lặp trước lần lượt là 7.69% (Mistral-7B-Instruct →Iter1), 2.10% (Iter1 →Iter2), và 1.64% (Iter2 →Iter3), cho thấy sự cải thiện ổn định qua các lần lặp, như được minh họa trong Hình 1. Chúng tôi cũng áp dụng SPPO cho một mô hình baseline mạnh hơn, tức là Llama-3-8B-Instruct, và mô hình được tinh chỉnh Llama-3-8B-SPPO có tỷ lệ thắng có kiểm soát độ dài cao hơn

16

--- TRANG 15 ---
Bảng 2: Kết quả bảng xếp hạng AlpacaEval 2.0 của cả tỷ lệ thắng bình thường và có kiểm soát độ dài (LC) theo phần trăm (%). Mistral-7B-SPPO có thể vượt trội hơn các mô hình lớn hơn và Mistral-7B-SPPO (best-of-16) có thể vượt trội hơn các mô hình độc quyền như GPT-4(6/13). Llama-3-8B-SPPO thể hiện hiệu suất thậm chí tốt hơn.

| Mô hình | AlpacaEval 2.0 |
|---------|----------------|
|         | Tỷ lệ thắng LC | Tỷ lệ thắng |
| GPT-4 Turbo | 50.0 | 50.0 |
| Claude 3 Opus | 40.5 | 29.1 |
| Llama-3-8B-SPPO Iter3 | 38.8 | 39.9 |
| GPT-4 0314 | 35.3 | 22.1 |
| Llama 3 70B Instruct | 34.4 | 33.2 |
| Mistral-7B-SPPO Iter3 (best-of-16) | 32.1 | 34.9 |
| GPT-4 0613 | 30.2 | 15.8 |
| Snorkel (Mistral-PairRM-DPO best-of-16) | 30.0 | 34.9 |
| Mistral Medium | 28.6 | 21.9 |
| Mistral-7B-SPPO Iter3 | 28.5 | 31.0 |
| Claude 2 | 28.2 | 17.2 |
| Snorkel (Mistral-PairRM-DPO) | 26.4 | 30.2 |
| Gemini Pro | 24.4 | 18.2 |
| Mistral 8×7B v0.1 | 23.7 | 18.1 |
| Llama 3 8B Instruct | 22.9 | 22.6 |

[THIS IS FIGURE: Two line charts showing win rates against GPT-4-Turbo with (a) and without (b) Length Controlling (LC) on AlpacaEval 2.0. Both charts show iterations 1-3 on x-axis and win rates on y-axis, with multiple model lines including GPT-4 0314, Snorkel, Mistral-7B-Instruct-v0.2, Self-Rewarding 70B, Mistral-7B-DPO, Mistral-7B-IPO, and Mistral-7B-SPPO.]

Hình 1: Tỷ lệ Thắng so với GPT-4-Turbo có (a) và không có (b) Kiểm soát Độ dài (LC) trên AlpacaEval 2.0. SPPO thể hiện sự cải thiện ổn định trên cả tỷ lệ thắng LC và raw.

là 38.77% và tỷ lệ thắng tổng thể 39.85%. Các gia tăng hiệu suất đáng kể hơn: 8.81% (Llama-3-8B-Instruct →Iter1), 3.42% (Iter1 →Iter2), và 3.62% (Iter2 →Iter3), tổng cộng là 15.85%.

Ngoài ra, kết quả cho thấy SPPO đạt được hiệu suất vượt trội so với các biến thể lặp của DPO và IPO. Tỷ lệ thắng có kiểm soát độ dài của SPPO đạt 28.53%,

17

--- TRANG 16 ---
| Mô hình | MT-Bench |
|---------|----------|
|         | Lượt 1 | Lượt 2 | Trung bình |
| Mistral-7B-Instruct-v0.2 | 7.78 | 7.25 | 7.51 |
| Snorkel (Mistral-PairRM-DPO) | 7.83 | 7.33 | 7.58 |
| Mistral-7B-DPO Iter1 | 7.45 | 6.58 | 7.02 |
| Mistral-7B-DPO Iter2 | 7.57 | 6.56 | 7.06 |
| Mistral-7B-DPO Iter3 | 7.49 | 6.69 | 7.09 |
| Mistral-7B-SPPO Iter1 | 7.63 | 6.79 | 7.21 |
| Mistral-7B-SPPO Iter2 | 7.90 | 7.08 | 7.49 |
| Mistral-7B-SPPO Iter3 | 7.84 | 7.34 | 7.59 |

| Mô hình | Arena-Hard-Auto-v0.1 |
|---------|---------------------|
| Mistral-7B-Instruct | 12.6 |
| Snorkel (Mistral-PairRM-DPO) | 20.7 |
| Mistral-7B-SPPO Iter1 | 18.7 |
| Mistral-7B-SPPO Iter2 | 20.4 |
| Mistral-7B-SPPO Iter3 | 23.3 |

Hình 2: Đánh giá MT-Bench & Arena-Hard. Trái: Mistral-7B-SPPO Iter3 vượt trội hơn tất cả các mô hình baseline bằng cách đạt được điểm trung bình 7.59 trong MT-Bench. Mặc dù có sự sụt giảm hiệu suất ban đầu trong hai lần lặp đầu tiên, SPPO Iter3 cải thiện so với mô hình cơ sở vào lần lặp cuối cùng. Phải: Mistral-7B-SPPO Iter3 vượt trội hơn mô hình baseline Snorkel(Mistral-PairRM-DPO) trong Arena-Hard. Sự cải thiện qua các lần lặp khác nhau là nhất quán.

vượt trội hơn tỷ lệ tốt nhất của DPO là 26.39% (bởi Snorkel) và tỷ lệ của IPO là 25.45%. Đáng chú ý, trong khi huấn luyện DPO và IPO có xu hướng tăng đáng kể độ dài đầu ra trung bình—lần lượt là 2736 và 2654—SPPO cho thấy sự tăng độ dài vừa phải hơn, di chuyển từ 1676 trong mô hình cơ sở lên 2163 tại lần lặp thứ ba. Cuối cùng, chúng tôi trình bày kết quả best-of-16 cho mỗi mô hình, được chọn bằng mô hình phần thưởng PairRM. Chúng tôi thấy rằng việc xếp hạng lại với mô hình sở thích tại thời điểm thử nghiệm có thể cải thiện hiệu suất của mô hình cơ sở (Mistral-7B-Instruct-v0.2), DPO (Snorkel), và SPPO (Iter3) lần lượt là 5.34%, 3.57%, và 3.6%. Đáng chú ý, điều này cho thấy rằng trong khi SPPO cải thiện đáng kể việc căn chỉnh mô hình sử dụng PairRM-0.4B làm giám sát bên ngoài duy nhất, nó không dẫn đến tối ưu hóa quá mức so với mô hình sở thích (Gao et al., 2023).

Trong Bảng 2, chúng tôi so sánh SPPO trên bảng xếp hạng AlpacaEval 2.0 với các chatbot AI hiện đại khác. Chúng tôi thấy mô hình SPPO của chúng tôi vượt trội hơn nhiều mô hình cạnh tranh được huấn luyện trên dữ liệu căn chỉnh độc quyền (ví dụ: Claude 2, Gemini Pro, & Llama 3 8B Instruct). Khi áp dụng cho Llama 3 8B Instruct, Llama-3-8B-SPPO của chúng tôi thể hiện tỷ lệ thắng thậm chí cao hơn. Với việc xếp hạng lại tại thời điểm thử nghiệm, Mistral-7B-SPPO Iter3 (best-of-16) thậm chí cạnh tranh với GPT-4 0613 và Llama 3 70B Instruct.

Trong Hình 2 (trái), chúng tôi đánh giá hiệu suất của SPPO trên MT-Bench. Chúng ta có thể thấy rằng Mistral-7B-SPPO Iter3 vượt trội hơn tất cả các mô hình baseline, đạt được điểm trung bình 7.59. Trong khi chúng tôi không chắc chắn tại sao hiệu suất MT-Bench giảm tại hai lần lặp đầu tiên, hiệu suất của SPPO tại lần lặp cuối cùng vẫn cải thiện so với mô hình cơ sở.

Arena-Hard (Li et al., 2024) chứa 500 truy vấn người dùng thách thức và theo cùng phương pháp đánh giá như AlpacaEval 2.0. Trong Hình 2 (phải), chúng tôi đánh giá hiệu suất của SPPO trên Arena-Hard. Chúng ta có thể thấy rằng Mistral-7B-SPPO thể hiện sự gia tăng hiệu suất ổn định qua các lần lặp. Mistral-7B-SPPO Iter 3 vượt trội hơn các mô hình baseline, đạt được điểm trung bình 23.3.

Open LLM Leaderboard Chúng tôi tiếp tục đánh giá khả năng của các mô hình SPPO sử dụng Huggingface Open LLM Leaderboard (Beeching et al., 2023b). Bảng xếp hạng này bao gồm 6 bộ dữ liệu khác nhau, mỗi bộ tập trung vào một khả năng cụ thể của LLM: Arc (Clark et al., 2018), HellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2021), MMLU (Hendrycks et al., 2020), TruthfulQA (Lin et al., 2021), và GSM8k (Cobbe et al., 2021). Các mô hình được nhắc với các mẫu zero hoặc few-shot.

18

--- TRANG 17 ---
Bảng 3: Đánh giá Open LLM Leaderboard. Tinh chỉnh SPPO cải thiện hiệu suất của mô hình cơ sở trên các nhiệm vụ khác nhau, đạt được điểm trung bình hiện đại là 66.75 cho Mistral-7B và 70.29 cho Llama-3-8B. Đối với Mistral-7B, các lần lặp tiếp theo của DPO, IPO, và SPPO thấy sự suy giảm hiệu suất. Có thể việc căn chỉnh với sở thích con người (được mô phỏng bởi mô hình sở thích PairRM trong nghiên cứu của chúng tôi) không phải lúc nào cũng tăng cường, và thậm chí có thể làm giảm hiệu suất tổng thể.

| Mô hình | Arc | TruthfulQA | WinoGrande | GSM8k | HellaSwag | MMLU | Trung bình |
|---------|-----|------------|------------|-------|-----------|------|-----------|
| Mistral-7B-Instruct-v0.2 | 63.65 | 66.85 | 77.98 | 41.93 | 84.89 | 59.15 | 65.74 |
| Snorkel | 66.04 | 70.86 | 77.74 | 36.77 | 85.64 | 60.83 | 66.31 |
| Mistral-7B-DPO Iter1 | 63.14 | 68.39 | 77.19 | 40.33 | 85.25 | 59.41 | 65.62 |
| Mistral-7B-DPO Iter2 | 64.16 | 67.84 | 76.09 | 39.95 | 85.23 | 59.03 | 65.38 |
| Mistral-7B-DPO Iter3 | 65.19 | 67.89 | 77.27 | 32.30 | 85.49 | 59.00 | 64.52 |
| Mistral-7B-IPO Iter1 | 64.68 | 68.60 | 77.98 | 43.75 | 85.08 | 59.04 | 66.52 |
| Mistral-7B-IPO Iter2 | 62.12 | 66.30 | 77.51 | 39.20 | 83.15 | 59.70 | 64.66 |
| Mistral-7B-IPO Iter3 | 62.97 | 67.12 | 77.51 | 37.45 | 83.69 | 59.57 | 64.72 |
| Mistral-7B-SPPO Iter1 | 65.02 | 69.40 | 77.82 | 43.82 | 85.11 | 58.84 | 66.67 |
| Mistral-7B-SPPO Iter2 | 65.53 | 69.55 | 77.03 | 44.35 | 85.29 | 58.72 | 66.75 |
| Mistral-7B-SPPO Iter3 | 65.36 | 69.97 | 76.80 | 42.68 | 85.16 | 58.45 | 66.40 |
| Llama-3-8B-Instruct | 62.29 | 51.65 | 76.09 | 75.89 | 78.73 | 65.59 | 68.37 |
| Llama-3-8B-SPPO Iter1 | 63.82 | 54.96 | 76.40 | 75.44 | 79.80 | 65.65 | 69.35 |
| Llama-3-8B-SPPO Iter2 | 64.93 | 56.48 | 76.87 | 75.13 | 80.39 | 65.67 | 69.91 |
| Llama-3-8B-SPPO Iter3 | 65.19 | 58.04 | 77.11 | 74.91 | 80.86 | 65.60 | 70.29 |

Kết quả, được trình bày trong Bảng 3, chứng minh rằng SPPO có thể tăng cường hiệu suất của mô hình cơ sở trên Arc, TruthfulQA, và GSM8k, và đạt được hiệu suất hiện đại với điểm trung bình 66.75. Tuy nhiên, những cải thiện này không duy trì trong các lần lặp căn chỉnh tiếp theo: hiệu suất của DPO, IPO, và SPPO giảm sau lần lặp đầu tiên hoặc thứ hai. Hạn chế này có thể được quy cho hiện tượng "thuế căn chỉnh" (Askell et al., 2021), gợi ý rằng việc căn chỉnh với sở thích con người (được mô phỏng bởi sở thích PairRM trong nghiên cứu của chúng tôi) có thể không cải thiện hoặc thậm chí làm tổn hại hiệu suất tổng quát. Cải thiện khả năng mô hình ngôn ngữ thông qua các lần lặp căn chỉnh vẫn là một chủ đề cho nghiên cứu tương lai, và chúng tôi cho rằng việc kết hợp các chú thích SFT chất lượng cao (Chen et al., 2024) có thể đóng vai trò quan trọng trong nỗ lực này.

Đánh giá sử dụng PairRM làm thẩm phán Vì SPPO xác định người thắng cuộc von Neumann (xem (3.3)) trong một trò chơi hai người tổng không đổi, chúng tôi kiểm tra các sở thích theo cặp giữa các mô hình SPPO và các baseline khác. Các tỷ lệ thắng theo cặp, được đo bằng PairRM, được mô tả trong Hình 3. Chúng tôi quan sát thấy rằng trong tất cả các thuật toán—tức là DPO, IPO, và SPPO—các lần lặp mô hình mới hơn vượt trội hơn các lần trước đó. Ví dụ, SPPO Lần lặp 3 vượt trội hơn SPPO Lần lặp 2. Cả SPPO và IPO đều vượt trội hơn DPO nhất quán qua tất cả các lần lặp. Trong khi SPPO vượt trội hơn IPO trong hai lần lặp đầu tiên, IPO vượt trội hơn SPPO về hiệu suất trong lần lặp cuối cùng. Xem xét hiệu suất vượt trội của SPPO trong các điểm chuẩn tiêu chuẩn được đánh giá bởi GPT-4 hoặc so với các câu trả lời ground-truth (ví dụ: AlpacaEval 2.0, MT-Bench, và Open LLM Leaderboard), cùng với xu hướng của IPO tạo ra các đầu ra chuỗi dài hơn (xem Avg. Len trong Bảng 1), chúng tôi tin rằng điều này là do IPO khai thác bias độ dài trong PairRM ưa thích các chuỗi dài hơn. Ngược lại, các mô hình SPPO hưởng lợi từ một regularization mạnh mẽ hơn trong khung cập nhật trọng số nhân.

19

--- TRANG 18 ---
[THIS IS FIGURE: A heatmap showing pairwise win rates between different models, with values ranging from 0.191 to 0.809. The models listed include IPO Iter3, SPPO Iter3, Snorkel, DPO Iter3, etc.]

Hình 3: Tỷ lệ thắng theo cặp giữa mô hình cơ sở (Mistral-7B-Instruct-v0.2), các mô hình DPO, các mô hình IPO, và các mô hình SPPO sử dụng PairRM-0.4B làm thẩm phán, có thể thiên về các mô hình có đầu ra dài hơn. Trên các điểm chuẩn với các mô hình thẩm phán mạnh mẽ hơn (ví dụ: GPT-4), như AlpacaEval 2.0 và MT-Bench, SPPO vượt trội hơn các thuật toán baseline khác với biên độ lớn.

5.3 Nghiên cứu Loại bỏ

Chúng tôi nghiên cứu ảnh hưởng của kích thước mini-batch khi ước tính tỷ lệ thắng P(y≻πt|x). Cụ thể, cho mỗi prompt, chúng tôi vẫn tạo 5 phản hồi và chọn người thắng yw và người thua yl theo điểm PairRM. Khi ước tính xác suất, chúng tôi thay đổi kích thước batch thành K = 2,3,5. Đối với K = 2, chúng tôi ước tính P(y≻πt|x) chỉ với 2 mẫu yw và yl:

P̂(yw≻πt|x) = [P(yw≻yw|x) + P(yw≻yl|x)]/2 = 1/2 + P(yw≻yl|x)/2,

và P̂(yl≻πt|x) tương tự. K = 5 chỉ ra thiết lập ban đầu chúng tôi sử dụng.

Chúng tôi so sánh các kết quả trên AlpacaEval 2.0, như được hiển thị trong Hình 4. Chúng tôi thấy rằng hiệu suất của SPPO mạnh mẽ đối với nhiễu trong việc ước tính P(y≻πt|x). Trong khi K = 5 ban đầu vượt trội hơn

20

--- TRANG 19 ---
| Kích thước Mini-Batch | Lần lặp | AlpacaEval 2.0 |
|----------------------|---------|----------------|
|                      |         | Tỷ lệ thắng | Độ dài TB (ký tự) | LC. | Raw |
| K = 2                | Iter1   | 23.85 | 23.53 | 1948 |
|                      | Iter2   | 26.91 | 27.24 | 1999 |
|                      | Iter3   | 28.26 | 28.22 | 1961 |
|                      | Iter1   | 24.79 | 23.51 | 1855 |
| K = 5                | Iter2   | 26.89 | 27.62 | 2019 |
|                      | Iter3   | 28.53 | 31.02 | 2163 |

[THIS IS FIGURE: A line graph showing LC. Win Rate (%) on y-axis vs Iter1, Iter2, Iter3 on x-axis. Multiple lines represent different models including Snorkel (Mistral-PairRM-DPO), Mistral-7B-Instruct-v0.2, SPPO (K=2), and SPPO (K=5)]

Hình 4: Đánh giá AlpacaEval 2.0 trên SPPO với kích thước mini-batch khác nhau theo tỷ lệ thắng bình thường và có kiểm soát độ dài (LC) theo phần trăm (%). K = 2,5 ký hiệu các kích thước mini-batch khác nhau khi ước tính tỷ lệ thắng P(y≻πt|x).

K = 2 trong lần lặp đầu tiên, sự khác biệt trong hiệu suất của chúng giảm dần trong các lần lặp tiếp theo. Ngoài ra, chúng tôi quan sát thấy rằng K = 2 thể hiện xu hướng giảm trong việc tăng độ dài đầu ra.

6 Kết luận

Bài báo này giới thiệu Tối Ưu Hóa Sở thích Tự Chơi (SPPO), một cách tiếp cận để tinh chỉnh Mô hình Ngôn ngữ Lớn (LLM) từ Phản hồi Con người/AI. SPPO đã chứng minh những cải thiện đáng kể so với các phương pháp hiện có như DPO và IPO trên nhiều điểm chuẩn, bao gồm AlpacaEval 2.0, MT-Bench, Arena-Hard, và Open LLM Leaderboard. Bằng cách tích hợp một mô hình sở thích và sử dụng một mục tiêu tối ưu hóa mới, SPPO có thể căn chỉnh LLM gần gũi hơn với sở thích con người.

Hạn chế Về mặt lý thuyết, việc xấp xỉ cập nhật chính sách tối ưu thông qua hồi quy dựa vào giả định rằng lớp mô hình đủ biểu cảm và dữ liệu được tạo ra bao phủ tốt không gian đầu vào. Việc xấp xỉ yếu tố log-partition với một hằng số có thể giúp giảm phương sai chỉ khi nó gần với hàm giá trị mềm. Các thí nghiệm được chạy trên một bộ dữ liệu UltraFeedback và các mô hình được thử nghiệm trên một vài điểm chuẩn do nguồn lực tính toán hạn chế, nhưng các phương pháp đề xuất có thể được xác thực thêm trên nhiều mô hình, bộ dữ liệu, và điểm chuẩn hơn để có đánh giá toàn diện với nhiều nguồn lực hơn.

Lời cảm ơn

Chúng tôi muốn cảm ơn Alekh Agarwal vì những bình luận sâu sắc đã giúp làm rõ các đóng góp kỹ thuật của công trình chúng tôi và mối liên hệ cũng như sự khác biệt với SPO (Swamy et al., 2024). Chúng tôi cũng muốn cảm ơn Wen Sun vì đã chỉ ra công trình đồng thời (Gao et al., 2024) và vì cuộc thảo luận có giá trị về lựa chọn thiết kế của các hàm mục tiêu.

21

--- TRANG 20 ---
A Xấp xỉ Yếu tố Chuẩn hóa

Như đã thảo luận trước đó, chúng tôi thay thế yếu tố log-partition bằng một hằng số để tránh việc ước tính hoặc dự đoán yếu tố log-partition. Nhìn lại, việc xấp xỉ yếu tố chuẩn hóa phục vụ như một baseline để giảm phương sai, và không cần phải chính xác. Ở đây chúng tôi thảo luận các giả định ngầm và cách chúng tôi thu được một xấp xỉ dựa trên các giả định khác nhau về hành vi sở thích con người.

Chúng tôi đầu tiên xem xét trường hợp có K phản hồi và sau đó tính giới hạn của ZπKt(x) khi K→∞. Chúng ta có hai trường hợp cực端:

1. Trường hợp "hỗn loạn" nhất: bất kỳ sở thích nào đều là một tung đồng xu công bằng
2. Trường hợp "có trật tự" nhất: có một thứ tự nghiêm ngặt giữa tất cả các phản hồi.

Trường hợp "hỗn loạn" nhất Cụ thể, chúng ta có K phản hồi khác nhau y1,y2, . . . ,yK cho prompt x đã cho. Vì chúng tôi xem xét cài đặt sở thích tổng quát, chúng tôi giả định rằng xác suất sở thích giữa yi và yj (i < j) chúng ta quan sát là một tung đồng xu công bằng:

P(yi≻yj|x) = {1, với xác suất 1/2, 0, với xác suất 1/2.

Lưu ý rằng để đơn giản, chúng tôi giả định rằng xác suất sở thích tuân theo phân phối Bernoulli, không phải phản hồi sở thích. Phản hồi sở thích là xác định vì xác suất sở thích là 0 hoặc 1. Giả định P(yi≻yj|x) tuân theo bất kỳ phân phối trung bình 1/2 nào khác sẽ cho ra cùng hằng số.

Chúng tôi định nghĩa biến ngẫu nhiên pi,j := 2P(yi≻yj|x) - 1 để thuận tiện. Tổng cộng, chúng ta có K(K-1)/2 biến ngẫu nhiên Rademacher độc lập cho tất cả i < j, và sau đó chúng ta có pj,i = -pi,j cho tất cả i > j. Với i = j, pi,j = 0. Chúng tôi cũng định nghĩa Xi = ΣKj=1 pi,j/K.

Với cài đặt và ký hiệu trên, chúng ta có

P(yi≻π̂Kt|x) = ΣKj=1 P(yi≻yj|x)/K = 1/2 + Xi.

Hơn nữa,

Zπ̂Kt(x) = ΣKi=1 exp(ηP(yi≻π̂Kt|x))/K = eη/2 · ΣKi=1 eηXi/K.

Với bất kỳ i cố định nào, chúng ta có kỳ vọng như sau:

E[eηXi] = E[∏Kj=1 eηpi,j/K] = ∏Kj=1 E[eηpi,j/K] = (eη/K + e-η/K)/2^(K-1),

trong đó phương trình cuối cùng đến từ định nghĩa của pi,j (lưu ý rằng pi,i = 0). Phương sai là:

Var[eηXi] = E[e2ηXi] - E[eηXi]² = (e2η/K + e-2η/K)/2^(K-1) - [(eη/K + e-η/K)/2^(K-1)]².

22

--- TRANG 21 ---
Ngoài ra, hiệp phương sai giữa eηXi và eηXj (i ≠ j) là:

Cov(eηXi, eηXj) = E[eηXi+ηXj] - E[eηXi]E[eηXj]
= E[exp(η∑Kk=1 pi,k/K + η∑Kl=1 pj,l/K)] - E[eηXi]E[eηXj]
= (eη/K + e-η/K)²/2²(K-4) - E[eηXi]E[eηXj]
= (eη/K + e-η/K)²/2²(K-4) - (eη/K + e-η/K)²/2²(K-2),

trong đó dòng thứ ba đúng vì pi,i = pj,j = 0, pi,j + pj,i = 0, và các số hạng còn lại là độc lập.

Người ta có thể kiểm tra rằng khi K→∞, chúng ta có E[eηXi]→1, Var[eηXi]→0, và Cov(eηXi, eηXj)→0. Bằng bất đẳng thức Chebyshev, ∑Ki=1 eηXi/K sẽ hội tụ về 1 theo xác suất. Vì vậy chúng ta có

Zπ̂Kt(x) = eη/2 · ∑Ki=1 eηXi/K → eη/2,

và chúng ta có thể xấp xỉ log Zπ̂Kt(x) bằng η/2.

Trường hợp "có trật tự" nhất Chúng tôi giả định có một thứ tự σ(·) giữa K phản hồi khác nhau y1,y2, . . . ,yK cho prompt x đã cho. Xác suất sở thích giữa yi và yj (i < j) là:

P(yi≻yj|x) = {1, nếu σ(i) < σ(j), 0, nếu σ(i) > σ(j).

Một lần nữa, phản hồi sở thích là xác định: miễn là yi được xếp hạng cao hơn yj, yi sẽ luôn được ưa thích hơn yj. Các phản hồi giống nhau vẫn hòa: P(yi≻yi|x) = 1/2.

Không mất tính tổng quát, chúng ta có thể giả định y1≺y2≺y3≺ ··· ≺ yK. Với cài đặt và ký hiệu trên, chúng ta có

P(yi≻π̂Kt|x) = ∑Kj=1 P(yi≻yj|x)/K = (i-1 + 1/2)/K = (i-1/2)/K,

vì với yi, có i-1 phản hồi nghiêm ngặt tệ hơn, và yi hòa với chính nó.

Đối với yếu tố chuẩn hóa, chúng ta có

log Zπ̂Kt(x) = log ∑Ki=1 exp(ηP(y≻π̂Kt|x))/K
= log ∑Ki=1 exp(η(i-1/2)/K)/K
→ log ∫01 exp(ηx)dx
= log (eη - 1)/η.

23

--- TRANG 22 ---
trong đó dòng thứ ba (giới hạn) có thể được thu được bằng định lý squeeze.

Với η = 1, log (eη - 1)/η ≈ 0.54η. Với η lớn ≈ 1e-3 như chúng tôi sử dụng trong các thí nghiệm, chúng ta có log (eη - 1)/η ≈ η.

Lựa chọn η Tùy thuộc vào mức độ "hỗn loạn" của sở thích, η có thể thay đổi giữa η/2 và η. Vì bài báo này một phần được thúc đẩy bởi hành vi sở thích không bắc cầu và phi lý của con người, chúng tôi chọn sử dụng η/2 để xấp xỉ log Zπ̂Kt(x). Tinh chỉnh hệ số của hằng số này như một siêu tham số cũng là một lựa chọn và có thể giúp cải thiện hiệu suất trên bộ dữ liệu đã cho.

B Chứng minh Định lý 4.1

Chứng minh Định lý 4.1. Giả sử vấn đề tối ưu hóa là khả thi, chúng ta có chính xác rằng

πt+1(y|x) ∝ πt(y|x) exp(ηP(y≻πt|x)), cho t = 1,2, ... . (B.1)

Để chứng minh rằng cập nhật trọng số hàm mũ có thể tạo ra chính sách tối ưu, chúng tôi trực tiếp gọi một phiên bản được phát biểu lại của Định lý 1 trong Freund và Schapire (1999):

Bổ đề B.1 (Định lý 1 trong Freund và Schapire (1999), được phát biểu lại). Với bất kỳ oracle P nào và với bất kỳ chuỗi chính sách hỗn hợp μ1, μ2, . . . , μT nào, chuỗi chính sách π1, π2, . . . , πT được tạo ra bởi (B.1) thỏa mãn:

∑Tt=1 P(πt≺μt) ≤ min π [η(1-e-η)/(1-e-η) ∑Tt=1 P(π≺μt) + KL(π∥π0)/(1-e-η)].

Bằng cách đặt μt = πt, chúng ta có rằng

T/2 ≤ min π [ηT P(π≺π̄T)/(1-e-η) + KL(π∥π0)/(1-e-η)],

trong đó LHS đến từ việc P(πt≺πt) = 1/2 và RHS đến từ việc (1/T)∑Tt=1 P(π≺πt) = P(π≺π̄t). Bây giờ sắp xếp lại các số hạng cho

(1-e-η)/(2η) ≤ min π P(π≺π̄T) + KL(π∥π0)/(ηT).

Lưu ý rằng π0 là một mô hình tự hồi quy được hỗ trợ đầy đủ trên một từ vựng hữu hạn (π0(yk+1|x,y1:k) có xác suất khác không cho mọi token). Vì hỗ trợ của nó là một tập hợp lớn nhưng hữu hạn, |log π0(·)| bị chặn từ trên. Vì vậy chúng ta có thể ràng buộc ngây thơ độ phân kỳ KL KL(π∥π0) ≤ ∥log π0(·)∥∞, có thể được xem như một hằng số (lớn).

Bằng cách chọn η = ∥log π0(·)∥∞/√T, chúng ta có

1/2 - ∥log π0(·)∥∞/(4√T) + O(T-1) ≤ min π P(π≺π̄T) + √(∥log π0(·)∥∞/T),

trong đó LHS đến từ khai triển Taylor (1-e-η)/(2η) = 1/2 - η/4 + O(η²). Lưu ý rằng 1/2 tại LHS đã là giá trị của trò chơi hai người tổng không đổi đối xứng. Điều này cho thấy rằng với η và T được chọn thích hợp, chính sách hỗn hợp π̄T gần với chính sách tối ưu minimax (cân bằng Nash).

Khoảng cách tối ưu do đó bị ràng buộc bởi

max π P(π≻π̄T) - min π P(π≺π̄T) = max π [1 - P(π≺π̄T)] - min π P(π≺π̄T) = 2[1/2 - min π P(π≺π̄T)] = O(1/√T).

24

--- TRANG 23 ---
C Ví dụ Phản hồi trong Các Lần lặp Khác nhau

25

--- TRANG 24 ---
Bảng 4: Ví dụ tạo của mô hình được tinh chỉnh bằng SPPO tại các lần lặp khác nhau.

Prompt: Bạn sẽ được đưa ra định nghĩa của một nhiệm vụ trước, sau đó là một số đầu vào của nhiệm vụ. Bạn được cho một cuộc đối thoại giữa 2 hoặc nhiều cá nhân. Cuộc đối thoại sẽ bao gồm thông tin về mối quan hệ giữa 2 trong số những người tham gia. Cuối cùng, bạn sẽ được yêu cầu đưa ra câu trả lời về mối quan hệ của 2 người tham gia đó. Trong tình huống này, '0' sẽ đại diện cho mối quan hệ vợ chồng, '1' sẽ đại diện cho anh chị em, và '2' sẽ đại diện cho mối quan hệ con/cha mẹ. Người nói 1: Tôi sẽ đến với bạn ngay. Được không? Cám ơn, Wendy. Người nói 2: Ôi trời! Bác sĩ trực tuyến đó dễ thương quá! Người nói 3: Ôi, dễ thương đến nỗi tôi đang nghĩ về việc đâm cái bút này vào mắt. Người nói 4: Bác sĩ Burke sẽ gặp bạn ngay bây giờ. Người nói 2: Ôi không-không-không, không phải Bác sĩ Burke. Bác sĩ Burke đã đi xa. Bác sĩ trực sẽ gặp tôi ngay bây giờ. Người nói 4: Bác sĩ Richard Burke đã đi xa. Bác sĩ Timothy Burke, con trai của ông ấy, sẽ gặp bạn ngay bây giờ. Người nói 5: Sẵn sàng? Mối quan hệ giữa Người nói 5 và Richard Burke là gì? Đầu ra:

Mistral-7B: Mối quan hệ giữa Người nói 5 và Richard Burke là mối quan hệ cha mẹ và con, vì Richard Burke được xác định là con trai của một nhân vật khác trong cuộc đối thoại.

Lần lặp 1: Mối quan hệ giữa Người nói 5 và Richard Burke là mối quan hệ cha và con. Dựa trên cuộc đối thoại, đã được tiết lộ rằng Bác sĩ Timothy Burke là bác sĩ trực và con trai của Bác sĩ Richard Burke.

Lần lặp 2: Mối quan hệ giữa Người nói 5 và Richard Burke là mối quan hệ cha và con. Cuộc trò chuyện tiết lộ rằng Bác sĩ Timothy Burke là bác sĩ trực và được xác định là con trai của Bác sĩ Richard Burke.

Lần lặp 3: Người nói 5 là cá nhân hỏi về mối quan hệ, vì vậy họ không liên quan trực tiếp đến Bác sĩ Richard Burke. Thông tin được cung cấp trong cuộc đối thoại tiết lộ rằng Bác sĩ Timothy Burke là bác sĩ trực và con trai của Bác sĩ Richard Burke. Do đó, câu trả lời là: Mối quan hệ giữa Người nói 5 và Bác sĩ Richard Burke là mối quan hệ con (Bác sĩ Timothy Burke) và cha mẹ. Người nói 5 đang hỏi về mối quan hệ, nhưng bản thân họ không phải là một phần của nó. Các mã quan hệ được chỉ định (0 cho vợ chồng, 1 cho anh chị em, 2 cho con/cha mẹ) phù hợp với mô tả này: Người nói 5 (0, 1, hoặc 2) và Bác sĩ Richard Burke (2) kết nối như một cặp cha mẹ-con. Đầu ra: 2 (mối quan hệ con/cha mẹ) Trong tình huống này, vai trò của Người nói 5 là xác định mối quan hệ giữa cô ấy và Bác sĩ Richard Burke, nhưng vì họ không liên quan (cô ấy là người hỏi từ bên ngoài), câu trả lời đúng chỉ liên quan đến mối quan hệ được xác định trong cuộc đối thoại: Bác sĩ Richard Burke (cha) và Bác sĩ Timothy Burke (con).

26

--- TRANG 25 ---
Bảng 5: Một ví dụ tạo khác của mô hình được tinh chỉnh bằng SPPO tại các lần lặp khác nhau.

Prompt: Q: Ngữ cảnh: Roman Brady là một nhân vật hư cấu từ bộ phim truyền hình dài tập NBC, Days of Our Lives. Vai diễn được khởi tạo vào năm 1981 bởi Wayne Northrop, người đã đóng vai này đến năm 1984, được thay thế bởi Drake Hogestyn từ 1986 – 1991, một lần nữa bởi Northrop từ 1991 – 1994, và hiện đang được thể hiện bởi ngôi sao kỳ cựu của series Josh Taylor, người đã bước vào vai này vào năm 1997. Câu hỏi: ai là roman brady trong days of our lives? Hãy đưa ra dòng ý thức bên dưới

Mistral-7B: Roman Brady là một nhân vật hư cấu trong bộ phim truyền hình NBC "Days of Our Lives." Anh ấy được thể hiện lần đầu bởi Wayne Northrop từ 1981 đến 1984. Sau đó, Drake Hogestyn đảm nhận vai này từ 1986 đến 1991. Northrop trở lại vai này từ 1991 đến 1994. Từ năm 1997, nhân vật đang được thể hiện bởi Josh Taylor. Roman Brady là một nhân vật quan trọng trong các cốt truyện dài hạn của chương trình.

Lần lặp 1: Roman Brady là một nhân vật hư cấu trong bộ phim truyền hình NBC "Days of Our Lives." Anh ấy được thể hiện lần đầu bởi Wayne Northrop từ 1981 đến 1984. Drake Hogestyn đảm nhận vai này từ 1986 đến 1991. Northrop trở lại đóng Roman một lần nữa từ 1991 đến 1994. Từ năm 1997, nhân vật đã được thể hiện bởi Josh Taylor. Vậy, tóm lại, Roman Brady là một nhân vật lâu đời trong "Days of Our Lives" đã được thể hiện bởi ba diễn viên khác nhau qua các năm: Wayne Northrop (1981-1984, 1991-1994), Drake Hogestyn (1986-1991), và hiện tại bởi Josh Taylor (1997-hiện tại).

Lần lặp 2: Roman Brady là một nhân vật hư cấu trong bộ phim truyền hình NBC "Days of Our Lives." Anh ấy được thể hiện lần đầu bởi Wayne Northrop từ 1981 đến 1984. Drake Hogestyn đảm nhận vai này từ 1986 đến 1991. Northrop trở lại đóng Roman một lần nữa từ 1991 đến 1994. Từ năm 1997, nhân vật đã được thể hiện bởi Josh Taylor. Nhìn chung, Roman Brady là một nhân vật lâu đời và phức tạp trong lịch sử của chương trình, với nhiều diễn viên mang đến những diễn giải độc đáo cho vai diễn.

Lần lặp 3: Roman Brady là một nhân vật hư cấu trong bộ phim truyền hình NBC "Days of Our Lives." Anh ấy ban đầu được thể hiện bởi Wayne Northrop từ 1981 đến 1984. Drake Hogestyn đảm nhận vai này từ 1986 đến 1991. Northrop trở lại thể hiện Roman một lần nữa từ 1991 đến 1994. Từ năm 1997, nhân vật đã được thể hiện bởi Josh Taylor. Roman Brady là một nhân vật phức tạp và nổi bật trong bộ phim truyền hình ban ngày dài hạn. Ban đầu được thể hiện bởi Northrop vào đầu những năm 1980, nhiệm kỳ của anh ấy bao gồm sự tham gia vào các cốt truyện khác nhau, như các mối quan hệ lãng mạn và giao dịch kinh doanh. Việc thể hiện của Hogestyn trong cuối những năm 1980 và đầu những năm 1990 đã giới thiệu những khía cạnh mới cho nhân vật. Nhiệm kỳ thứ hai của Northrop, từ 1991 đến 1994, đã phát triển thêm câu chuyện của Roman. Từ năm 1997, Josh Taylor đã đảm nhận vai này, tiếp tục đóng góp vào câu chuyện đang phát triển của Roman Brady trong "Days of Our Lives." Qua những lần thể hiện khác nhau này, Roman vẫn là một nhân vật trung tâm, được biết đến với các mối quan hệ phức tạp, dự án kinh doanh, và những cuộc đấu tranh cá nhân.

27

--- TRANG 26 ---
Tài liệu tham khảo

Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N. et al. (2021). Một trợ lý ngôn ngữ tổng quát như một phòng thí nghiệm để căn chỉnh. arXiv preprint arXiv:2112.00861.

Azar, M. G., Rowland, M., Piot, B., Guo, D., Calandriello, D., Valko, M. và Munos, R. (2023). Một mô hình lý thuyết tổng quát để hiểu việc học từ sở thích con người. arXiv preprint arXiv:2310.12036.

Beeching, E., Fourrier, C., Habib, N., Han, S., Lambert, N., Rajani, N., Sanseviero, O., Tunstall, L. và Wolf, T. (2023a). Bảng xếp hạng llm mở. https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.

Beeching, E., Fourrier, C., Habib, N., Han, S., Lambert, N., Rajani, N., Sanseviero, O., Tunstall, L. và Wolf, T. (2023b). Bảng xếp hạng llm mở. Hugging Face.

Bradley, R. A. và Terry, M. E. (1952). Phân tích Thứ hạng của Thiết kế Khối Không hoàn chỉnh: I. Phương pháp So sánh Theo cặp. Biometrika 39 324–345.

Calandriello, D., Guo, D., Munos, R., Rowland, M., Tang, Y., Pires, B. A., Richemond, P. H., Lan, C. L., Valko, M., Liu, T. et al. (2024). Căn chỉnh con người của các mô hình ngôn ngữ lớn thông qua tối ưu hóa sở thích trực tuyến. arXiv preprint arXiv:2403.08635.

Chen, Z., Deng, Y., Yuan, H., Ji, K. và Gu, Q. (2024). Tinh chỉnh tự chơi chuyển đổi các mô hình ngôn ngữ yếu thành các mô hình ngôn ngữ mạnh. arXiv preprint arXiv:2401.01335.

Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S. và Amodei, D. (2017). Học tăng cường sâu từ sở thích con người. Advances in neural information processing systems 30.

Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C. và Tafjord, O. (2018). Bạn nghĩ mình đã giải quyết được việc trả lời câu hỏi? hãy thử arc, thử thách lý luận ai2. arXiv preprint arXiv:1803.05457.

Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R. et al. (2021). Huấn luyện các trình xác minh để giải quyết các bài toán từ toán học. arXiv preprint arXiv:2110.14168.

Cui, G., Yuan, L., Ding, N., Yao, G., Zhu, W., Ni, Y., Xie, G., Liu, Z. và Sun, M. (2023). Ultrafeedback: Tăng cường các mô hình ngôn ngữ với phản hồi chất lượng cao. arXiv preprint arXiv:2310.01377.

Dubois, Y., Galambosi, B., Liang, P. và Hashimoto, T. B. (2024a). Alpacaeval có kiểm soát độ dài: Một cách đơn giản để giảm thiên vị của các trình đánh giá tự động. arXiv preprint arXiv:2404.04475.

Dubois, Y., Li, C. X., Taori, R., Zhang, T., Gulrajani, I., Ba, J., Guestrin, C., Liang, P. S. và Hashimoto, T. B. (2024b). Alpacafarm: Một khung mô phỏng cho các phương pháp học từ phản hồi con người. Advances in Neural Information Processing Systems 36.

28

--- TRANG 27 ---
Dudík, M., Hofmann, K., Schapire, R. E., Slivkins, A. và Zoghi, M. (2015). Bandit đối đầu có ngữ cảnh. Trong Conference on Learning Theory. PMLR.

Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D. và Kiela, D. (2024). Kto: Căn chỉnh mô hình như tối ưu hóa lý thuyết triển vọng. arXiv preprint arXiv:2402.01306.

Freund, Y. và Schapire, R. E. (1997). Một tổng quát hóa lý thuyết quyết định của học trực tuyến và một ứng dụng vào boosting. Journal of computer and system sciences 55 119–139.

Freund, Y. và Schapire, R. E. (1999). Chơi trò chơi thích ứng sử dụng trọng số nhân. Games and Economic Behavior 29 79–103.

Gao, L., Schulman, J. và Hilton, J. (2023). Luật mở rộng cho tối ưu hóa quá mức mô hình phần thưởng. Trong International Conference on Machine Learning. PMLR.

Gao, Z., Chang, J. D., Zhan, W., Oertell, O., Swamy, G., Brantley, K., Joachims, T., Bagnell, J. A., Lee, J. D. và Sun, W. (2024). Rebel: Học tăng cường thông qua hồi quy phần thưởng tương đối. arXiv preprint arXiv:2404.16767.

Haarnoja, T., Zhou, A., Abbeel, P. và Levine, S. (2018). Soft actor-critic: Học tăng cường entropy tối đa off-policy sâu với một actor ngẫu nhiên. Trong International conference on machine learning. PMLR.

He, P., Gao, J. và Chen, W. (2021). Debertav3: Cải thiện deberta sử dụng pre-training kiểu electra với chia sẻ embedding gradient-disentangled.

Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D. và Steinhardt, J. (2020). Đo lường hiểu biết ngôn ngữ đa nhiệm vụ lớn. arXiv preprint arXiv:2009.03300.

Hong, J., Lee, N. và Thorne, J. (2024). Tối ưu hóa sở thích nguyên khối không tham chiếu với tỷ lệ odds. arXiv preprint arXiv:2403.07691.

Ji, K., He, J. và Gu, Q. (2024). Học tăng cường từ phản hồi con người với các truy vấn tích cực. arXiv preprint arXiv:2402.09401.

Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L. et al. (2023a). Mistral 7b. arXiv preprint arXiv:2310.06825.

Jiang, D., Ren, X. và Lin, B. Y. (2023b). Llm-blender: Kết hợp các mô hình ngôn ngữ lớn với xếp hạng theo cặp và fusion tạo sinh. arXiv preprint arXiv:2306.02561.

Li, J., Sun, S., Yuan, W., Fan, R.-Z., Zhao, H. và Liu, P. (2023a). Thẩm phán tạo sinh để đánh giá căn chỉnh. arXiv preprint arXiv:2310.05470.

Li, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B., Gonzalez, J. E. và Stoica, I. (2024). Từ dữ liệu crowdsourced đến các điểm chuẩn chất lượng cao: Arena-hard và pipeline benchbuilder. arXiv preprint arXiv:2406.11939.

Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang, P. và Hashimoto, T. B. (2023b). Alpacaeval: Một trình đánh giá tự động của các mô hình tuân thủ hướng dẫn. https://github.com/tatsu-lab/alpaca_eval.

29

--- TRANG 28 ---
Lin, S., Hilton, J. và Evans, O. (2021). Truthfulqa: Đo lường cách các mô hình bắt chước sự giả dối của con người. arXiv preprint arXiv:2109.07958.

Liu, T., Zhao, Y., Joshi, R., Khalman, M., Saleh, M., Liu, P. J. và Liu, J. (2023). Lấy mẫu từ chối thống kê cải thiện tối ưu hóa sở thích. arXiv preprint arXiv:2309.06657.

Lou, H., Jin, T., Wu, Y., Xu, P., Gu, Q. và Farnoud, F. (2022). Xếp hạng tích cực mà không có tính bắc cầu ngẫu nhiên mạnh. Advances in neural information processing systems.

Munos, R., Valko, M., Calandriello, D., Azar, M. G., Rowland, M., Guo, Z. D., Tang, Y., Geist, M., Mesnard, T., Michi, A. et al. (2023). Học nash từ phản hồi con người. arXiv preprint arXiv:2312.00886.

Ng, A. Y., Harada, D. và Russell, S. (1999). Bất biến chính sách dưới các biến đổi phần thưởng: Lý thuyết và ứng dụng vào việc định hình phần thưởng. Trong Icml, vol. 99.

OpenAI, J., Achiam, Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S. et al. (2023). Báo cáo kỹ thuật gpt-4. arXiv preprint arXiv:2303.08774.

Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A. et al. (2022). Huấn luyện các mô hình ngôn ngữ để tuân theo hướng dẫn với phản hồi con người. Advances in Neural Information Processing Systems 35 27730–27744.

Pal, A., Karkhanis, D., Dooley, S., Roberts, M., Naidu, S. và White, C. (2024). Smaug: Sửa chữa các chế độ thất bại của tối ưu hóa sở thích với dpo-positive. arXiv preprint arXiv:2402.13228.

Rafailov, R., Hejna, J., Park, R. và Finn, C. (2024a). Từ r đến q*: Mô hình ngôn ngữ của bạn bí mật là một q-function. arXiv preprint arXiv:2404.12358.

Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S. và Finn, C. (2024b). Tối ưu hóa sở thích trực tiếp: Mô hình ngôn ngữ của bạn bí mật là một mô hình phần thưởng. Advances in Neural Information Processing Systems 36.

Rosset, C., Cheng, C.-A., Mitra, A., Santacroce, M., Awadallah, A. và Xie, T. (2024). Tối ưu hóa nash trực tiếp: Dạy các mô hình ngôn ngữ tự cải thiện với sở thích tổng quát. arXiv preprint arXiv:2404.03715.

Sakaguchi, K., Bras, R. L., Bhagavatula, C. và Choi, Y. (2021). Winogrande: Một thử thách winograd schema đối kháng ở quy mô lớn. Communications of the ACM 64 99–106.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A. và Klimov, O. (2017). Các thuật toán tối ưu hóa chính sách gần đúng. arXiv preprint arXiv:1707.06347.

Singh, A., Co-Reyes, J. D., Agarwal, R., Anand, A., Patil, P., Liu, P. J., Harrison, J., Lee, J., Xu, K., Parisi, A. et al. (2023). Vượt ra ngoài dữ liệu con người: Mở rộng quy mô tự huấn luyện để giải quyết vấn đề với các mô hình ngôn ngữ. arXiv preprint arXiv:2312.06585.

30

--- TRANG 29 ---
Sutton, R. S., McAllester, D., Singh, S. và Mansour, Y. (1999). Các phương pháp gradient chính sách cho học tăng cường với xấp xỉ hàm. Advances in neural information processing systems 12.

Swamy, G., Dann, C., Kidambi, R., Wu, Z. S. và Agarwal, A. (2024). Một cách tiếp cận tối thiểu tối đa đến học tăng cường từ phản hồi con người. arXiv preprint arXiv:2401.04056.

Tversky, A. (1969). Sự không bắc cầu của sở thích. Psychological review 76 31.

Wang, Y., Liu, Q. và Jin, C. (2024). Liệu rlhf có khó hơn rl tiêu chuẩn không? một quan điểm lý thuyết. Advances in Neural Information Processing Systems 36.

Wu, Y., Jin, T., Di, Q., Lou, H., Farnoud, F. và Gu, Q. (2023). Tối thiểu hóa hối tiếc borda cho bandit đối đầu tuyến tính tổng quát. Trong ICML 2023 Workshop The Many Facets of Preference-Based Learning.

Xiong, W., Dong, H., Ye, C., Zhong, H., Jiang, N. và Zhang, T. (2023). Lấy mẫu gibbs từ phản hồi con người: Một khung bị ràng buộc kl có thể chứng minh cho rlhf. arXiv preprint arXiv:2312.11456.

Xu, J., Lee, A., Sukhbaatar, S. và Weston, J. (2023). Một số thứ thì nhục nhã hơn những thứ khác: tối ưu hóa sở thích với mất mát nhục nhã theo cặp. arXiv preprint arXiv:2312.16682.

Ye, C., Xiong, W., Zhang, Y., Jiang, N. và Zhang, T. (2024). Một phân tích lý thuyết về học nash từ phản hồi con người dưới regularization kl tổng quát. arXiv preprint arXiv:2402.07314.

Yuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J. và Weston, J. (2024). Các mô hình ngôn ngữ tự thưởng. arXiv preprint arXiv:2401.10020.

Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A. và Choi, Y. (2019). Hellaswag: Liệu một máy thực sự có thể hoàn thành câu của bạn không? arXiv preprint arXiv:1905.07830.

Zhao, Y., Joshi, R., Liu, T., Khalman, M., Saleh, M. và Liu, P. J. (2023). Slic-hf: Hiệu chỉnh khả năng chuỗi với phản hồi con người. arXiv preprint arXiv:2305.10425.

Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. et al. (2023). Đánh giá llm-as-a-judge với mt-bench và chatbot arena. Advances in Neural Information Processing Systems 36.

Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. et al. (2024). Đánh giá llm-as-a-judge với mt-bench và chatbot arena. Advances in Neural Information Processing Systems 36.

Zhu, B., Jiao, J. và Jordan, M. I. (2023). Học tăng cường có nguyên tắc với phản hồi con người từ so sánh theo cặp hoặc k-wise. arXiv preprint arXiv:2301.11270.

31
