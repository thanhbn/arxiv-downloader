# 2302.02948.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2302.02948.pdf
# Kích thước tệp: 3702811 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Học Tăng Cường Trực Tuyến Hiệu Quả với Dữ Liệu Ngoại Tuyến
Philip J. Ball* 1Laura Smith* 2Ilya Kostrikov* 2Sergey Levine2

Tóm Tắt
Hiệu quả mẫu và khám phá vẫn là những thách thức chính trong học tăng cường trực tuyến (RL). Một phương pháp mạnh mẽ có thể được áp dụng để giải quyết các vấn đề này là việc bao gồm dữ liệu ngoại tuyến, chẳng hạn như các quỹ đạo trước đó từ một chuyên gia con người hoặc một chính sách khám phá dưới tối ưu. Các phương pháp trước đây đã dựa vào các sửa đổi rộng rãi và độ phức tạp bổ sung để đảm bảo việc sử dụng hiệu quả dữ liệu này. Thay vào đó, chúng tôi đặt câu hỏi: liệu chúng ta có thể đơn giản áp dụng các phương pháp off-policy hiện có để tận dụng dữ liệu ngoại tuyến khi học trực tuyến không? Trong công trình này, chúng tôi chứng minh rằng câu trả lời là có; tuy nhiên, một tập hợp các thay đổi tối thiểu nhưng quan trọng đối với các thuật toán RL off-policy hiện có là cần thiết để đạt được hiệu suất đáng tin cậy. Chúng tôi tiến hành phân tích chi tiết các lựa chọn thiết kế này, chứng minh các yếu tố chính ảnh hưởng nhất đến hiệu suất, và đưa ra một tập hợp các khuyến nghị mà các nhà thực hành có thể dễ dàng áp dụng, cho dù dữ liệu của họ bao gồm một số lượng nhỏ các minh họa chuyên gia hay khối lượng lớn các quỹ đạo dưới tối ưu. Chúng tôi thấy rằng việc áp dụng đúng các khuyến nghị đơn giản này có thể cung cấp cải thiện 2.5× so với các phương pháp hiện có trên một tập hợp đa dạng các benchmark cạnh tranh, mà không có chi phí tính toán bổ sung. Chúng tôi đã phát hành mã của mình tại đây: github.com/ikostrikov/rlpd.

1. Giới Thiệu
Học tăng cường sâu (RL) đã đạt được thành công trong một số lĩnh vực phức tạp, như Atari (Mnih et al., 2015) và Go (Silver et al., 2016), cũng như các ứng dụng thực tế như Thiết Kế Chip (Mirhoseini et al., 2021) và Sắp Xếp Ưu Tiên Con Người (Ouyang et al., 2022). Trong nhiều môi trường này, hiệu suất RL mạnh mẽ được dựa trên việc có lượng lớn tương tác trực tuyến với một môi trường, điều này thường được thực hiện thông qua việc sử dụng các trình mô phỏng. Tuy nhiên, trong các vấn đề thực tế, chúng ta thường phải đối mặt với các tình huống mà các mẫu đắt đỏ, và hơn nữa, phần thưởng thưa thớt, thường được làm trầm trọng thêm bởi không gian trạng thái và hành động có chiều cao.

Một cách đầy hứa hẹn để giải quyết vấn đề này là thông qua việc bao gồm dữ liệu được tạo ra bởi một chính sách trước đó hoặc chuyên gia con người khi đào tạo các thuật toán RL sâu (thường được gọi là dữ liệu ngoại tuyến (Levine et al., 2020)), như đã được chứng minh về mặt lý thuyết (Wagenmaker & Pacchiano, 2022; Song et al., 2023) và trong các ví dụ thực tế bởi Cabi et al. (2019); Nair et al. (2020); Lu et al. (2021). Điều này có thể làm giảm các thách thức do hiệu quả mẫu và khám phá bằng cách cung cấp cho thuật toán một bộ dữ liệu ban đầu để "khởi động" quá trình học tập, hoặc dưới dạng các minh họa chuyên gia chất lượng cao, hoặc thậm chí là các quỹ đạo khám phá chất lượng thấp nhưng có độ bao phủ cao. Điều này cũng cung cấp cho chúng ta một con đường để tận dụng các bộ dữ liệu lớn được thu thập trước để học các chính sách hữu ích.

Một số công trình trước đã tập trung vào việc sử dụng dữ liệu này thông qua tiền đào tạo, trong khi các phương pháp khác giới thiệu các ràng buộc khi đào tạo trực tuyến để xử lý các vấn đề với sự thay đổi phân phối. Tuy nhiên, mỗi phương pháp đều có nhược điểm riêng, chẳng hạn như thời gian đào tạo bổ sung và siêu tham số, hoặc cải thiện hạn chế vượt qua chính sách hành vi tương ứng. Lùi lại một bước, chúng tôi lưu ý rằng các thuật toán off-policy tiêu chuẩn có thể tận dụng dữ liệu ngoại tuyến này, và hơn nữa các vấn đề với sự thay đổi phân phối có thể được giảm thiểu trong môi trường này, vì chúng ta có thể khám phá môi trường trực tuyến. Tuy nhiên, cho đến nay, các phương pháp như vậy đã thấy thành công hạn chế trong thiết lập vấn đề này. Do đó, trong công trình này, chúng tôi đặt câu hỏi sau: liệu chúng ta có thể đơn giản áp dụng các phương pháp off-policy hiện có để tận dụng dữ liệu ngoại tuyến khi học trực tuyến, mà không cần tiền đào tạo RL ngoại tuyến hoặc các điều khoản bắt chước rõ ràng ưu tiên dữ liệu ngoại tuyến trước đó không?

--- TRANG 2 ---
RL Trực Tuyến Hiệu Quả với Dữ Liệu Ngoại Tuyến

với sự thay đổi phân phối có thể được giảm thiểu trong môi trường này, vì chúng ta có thể khám phá môi trường trực tuyến. Tuy nhiên, cho đến nay, các phương pháp như vậy đã thấy thành công hạn chế trong thiết lập vấn đề này. Do đó, trong công trình này, chúng tôi đặt câu hỏi sau: liệu chúng ta có thể đơn giản áp dụng các phương pháp off-policy hiện có để tận dụng dữ liệu ngoại tuyến khi học trực tuyến, mà không cần tiền đào tạo RL ngoại tuyến hoặc các điều khoản bắt chước rõ ràng ưu tiên dữ liệu ngoại tuyến trước đó không?

Thông qua một tập hợp các thí nghiệm kỹ lưỡng trên một bộ sưu tập các benchmark được nghiên cứu rộng rãi, chúng tôi cho thấy rằng câu trả lời cho câu hỏi này là có. Tuy nhiên, việc áp dụng một cách ngây thơ các thuật toán RL off-policy trực tuyến hiện có có thể dẫn đến hiệu suất tương đối kém, như chúng ta thấy trong Hình 1 so sánh 'SAC + Dữ Liệu Ngoại Tuyến' với 'IQL + Tinh Chỉnh'. Thay vào đó, một tập hợp tối thiểu các lựa chọn thiết kế chính phải được xem xét để đảm bảo thành công của chúng. Cụ thể, trước tiên chúng tôi giới thiệu một phương pháp đơn giản đáng kể để lấy mẫu dữ liệu ngoại tuyến, mà chúng tôi gọi là "lấy mẫu đối xứng", hoạt động tốt trên một loạt các miền đa dạng mà không cần điều chỉnh siêu tham số. Sau đó, chúng ta thấy rằng trong các môi trường phức tạp (ví dụ: phần thưởng thưa thớt, khối lượng dữ liệu ngoại tuyến thấp, chiều cao, v.v.), việc ngăn chặn các hàm giá trị từ việc ngoại suy quá mức là rất quan trọng. Để đạt được điều này, chúng tôi cung cấp một góc nhìn mới về cách Layer Normalization (Ba et al., 2016) ngầm ngăn chặn ngoại suy giá trị thảm khốc, do đó cải thiện đáng kể hiệu quả mẫu và tính ổn định trong nhiều tình huống, trong khi là một sửa đổi tối thiểu đối với các phương pháp hiện có. Sau đó, để cải thiện tốc độ mà dữ liệu ngoại tuyến được sử dụng, chúng tôi kết hợp và so sánh những tiến bộ mới nhất trong RL model-free hiệu quả mẫu, và thấy rằng các ensemble lớn cực kỳ hiệu quả trên nhiều miền khác nhau. Cuối cùng, chúng tôi xác định và cung cấp bằng chứng rằng các lựa chọn thiết kế chính trong văn học RL gần đây thực tế là nhạy cảm với môi trường, cho thấy kết quả đáng ngạc nhiên rằng các môi trường chia sẻ các thuộc tính tương tự thực tế yêu cầu các lựa chọn hoàn toàn khác nhau, và khuyến nghị một quy trình làm việc cho các nhà thực hành để tăng tốc việc áp dụng những hiểu biết của chúng tôi vào các miền mới.

Chúng tôi chứng minh rằng phương pháp cuối cùng của chúng tôi, mà chúng tôi gọi là RLPD (Reinforcement Learning with Prior Data) vượt trội so với các kết quả được báo cáo trước đó, như chúng ta thấy trong Hình 1, trên nhiều miền cạnh tranh, đôi khi lên đến 2.5×. Quan trọng là, vì các thay đổi của chúng tôi là tối thiểu, chúng tôi duy trì các thuộc tính hấp dẫn của các thuật toán trực tuyến, chẳng hạn như dễ dàng triển khai và hiệu quả tính toán. Hơn nữa, chúng ta thấy tính tổng quát của phương pháp, đạt được hiệu suất mạnh mẽ trên một số bộ dữ liệu ngoại tuyến đa dạng, từ những bộ chứa các minh họa chuyên gia hạn chế, đến dữ liệu bao gồm các quỹ đạo dưới tối ưu khối lượng lớn.

Chúng tôi tin rằng những hiểu biết của chúng tôi có giá trị đối với cộng đồng và các nhà thực hành. Chúng tôi cho thấy rằng các thuật toán RL off-policy trực tuyến có thể hiệu quả đáng kể trong việc học với dữ liệu ngoại tuyến. Tuy nhiên, chúng tôi cho thấy hiệu suất đáng tin cậy của chúng được dựa trên một số lựa chọn thiết kế chính, cụ thể là cách dữ liệu ngoại tuyến được lấy mẫu, một cách quan trọng để chuẩn hóa việc cập nhật critic, và sử dụng các ensemble lớn để cải thiện hiệu quả mẫu. Trong khi các thành phần riêng lẻ của RLPD là những sửa đổi đơn giản đáng khích lệ trên các thành phần RL hiện có, chúng tôi cho thấy rằng sự kết hợp của chúng mang lại hiệu suất tiên tiến trên một số benchmark RL trực tuyến với dữ liệu ngoại tuyến phổ biến, vượt qua hiệu suất của các phương pháp trước đó phức tạp hơn đáng kể, và tổng quát hóa cho một số loại dữ liệu ngoại tuyến khác nhau, cho dù đó là các minh họa chuyên gia hay các quỹ đạo dưới tối ưu. Chúng tôi đã phát hành RLPD tại đây: github.com/ikostrikov/rlpd.

2. Công Trình Liên Quan

Tiền đào tạo RL ngoại tuyến. Chúng tôi lưu ý các kết nối với RL ngoại tuyến (Ernst et al., 2005; Fujimoto et al., 2019; Levine et al., 2020); nhiều công trình trước thực hiện RL ngoại tuyến, theo sau bởi tinh chỉnh trực tuyến (Hester et al., 2018; Kalashnikov et al., 2018; Nair et al., 2020; Lee et al., 2021; Kostrikov et al., 2022). Đáng chú ý, Lee et al. (2021) cũng xem xét các ensemble lớn và nhiều chế độ gradient-step mỗi timestep khi học trực tuyến. Tuy nhiên, phương pháp của chúng tôi sử dụng một cơ chế lấy mẫu đơn giản hơn đáng kể không có siêu tham số và không dựa vào tiền đào tạo ngoại tuyến tốn kém, điều này giới thiệu thêm các siêu tham số bổ sung. Chúng tôi cũng nhấn mạnh rằng cập nhật chuẩn hóa của chúng tôi không phải là một phương pháp RL ngoại tuyến — chúng tôi không thực hiện bất kỳ tiền đào tạo ngoại tuyến nào mà chạy RL trực tuyến từ đầu với dữ liệu ngoại tuyến được bao gồm trong một bộ đệm replay.

Ràng buộc với dữ liệu trước. Một thay thế cho mô hình tiền đào tạo RL ngoại tuyến là ràng buộc rõ ràng các cập nhật agent trực tuyến sao cho nó thể hiện hành vi giống với dữ liệu ngoại tuyến (Levine & Koltun, 2013; Fox et al., 2016; Hester et al., 2018; Nair et al., 2018a; Rajeswaran et al., 2018; Rudner et al., 2021). Đặc biệt liên quan đến phương pháp của chúng tôi là công trình của Rajeswaran et al. (2018), tăng cường một cập nhật policy gradient với một cập nhật có trọng số bao gồm rõ ràng dữ liệu minh họa. Ngược lại, chúng tôi sử dụng một mô hình off-policy hiệu quả mẫu, và không thực hiện bất kỳ tiền đào tạo nào. Cũng tương tự với công trình của chúng tôi là của Nair et al. (2018a), người cũng sử dụng một thuật toán off-policy với một bộ đệm replay ngoại tuyến cố định. Tuy nhiên, chúng tôi không hạn chế chính sách bằng một điều khoản behavior cloning, và không reset về các trạng thái minh họa. Hơn nữa, chúng tôi lưu ý rằng các phương pháp này thường yêu cầu dữ liệu ngoại tuyến chất lượng cao (tức là 'học từ dữ liệu minh họa' (Asada & Hanafusa, 1979; Schaal, 1996)), trong khi phương pháp của chúng tôi, quan trọng là, không phụ thuộc vào chất lượng của dữ liệu.

Các phương pháp không ràng buộc với dữ liệu trước. Công trình trước cũng đã xem xét các cách kết hợp dữ liệu ngoại tuyến mà không có bất kỳ ràng buộc nào. Một số phương pháp tập trung vào việc khởi tạo một bộ đệm replay với dữ liệu ngoại tuyến (Veˇcer ík et al., 2017; Hester et al.,

--- TRANG 3 ---
RL Trực Tuyến Hiệu Quả với Dữ Liệu Ngoại Tuyến

2018), trong khi các công trình khác đã sử dụng một chiến lược lấy mẫu cân bằng để xử lý dữ liệu trực tuyến và ngoại tuyến (Nair et al., 2018b; Kalashnikov et al., 2018; Hansen et al., 2022; Zhang et al., 2023). Gần đây nhất, Song et al. (2023) trình bày một phân tích lý thuyết về các phương pháp như vậy, cho thấy rằng lấy mẫu cân bằng quan trọng cả về lý thuyết và thực tế. Trong các thí nghiệm của chúng tôi, chúng tôi cũng cho thấy rằng lấy mẫu cân bằng giúp RL trực tuyến với dữ liệu ngoại tuyến; tuy nhiên, việc sử dụng trực tiếp phương pháp này trên một loạt các nhiệm vụ benchmark là không đủ, và các quyết định thiết kế khác mà chúng tôi trình bày là quan trọng trong việc đạt được hiệu suất tốt trên tất cả các nhiệm vụ.

3. Kiến Thức Cơ Bản

Chúng tôi xem xét các vấn đề có thể được công thức hóa như một Quá Trình Quyết Định Markov (MDP) (Bellman, 1957), được mô tả như một tuple (S,A, γ, p, r, d0) trong đó S là không gian trạng thái, A là không gian hành động và γ∈(0,1) là yếu tố chiết khấu. Động lực học được điều khiển bởi một hàm chuyển tiếp p(s′|s, a); có một hàm phần thưởng r(s, a) và phân phối trạng thái ban đầu d0(s). Mục tiêu của RL sau đó là tối đa hóa tổng phần thưởng chiết khấu dự kiến: Eπ[P∞ t=1γtr(st, at)].

Trong công trình này, chúng tôi tập trung vào RL trong khi có quyền truy cập vào các bộ dữ liệu ngoại tuyến D (Levine et al., 2020), một bộ sưu tập các tuple (s, a, r, s′) được tạo ra từ một MDP cụ thể. Một thuộc tính chính của các bộ dữ liệu ngoại tuyến là chúng thường không cung cấp độ bao phủ trạng thái-hành động đầy đủ, tức là {s, a∈ D} là một tập con nhỏ của S × A. Do thiếu độ bao phủ on-policy này, các phương pháp sử dụng xấp xỉ hàm có thể ngoại suy quá mức các giá trị khi học trên dữ liệu này, dẫn đến tác động rõ rệt đến hiệu suất học (Fujimoto et al., 2019).

4. RL Trực Tuyến với Dữ Liệu Ngoại Tuyến

Như đã nêu trong Phần 3, chúng tôi xem xét thiết lập RL tiêu chuẩn với việc bổ sung một bộ dữ liệu được thu thập trước. Trong công trình này, chúng tôi nhằm thiết kế một phương pháp tổng quát không phụ thuộc vào chất lượng và số lượng của dữ liệu được thu thập trước này. Ví dụ, dữ liệu này có thể có dạng một số ít minh họa của con người, hoặc các lượng lớn dữ liệu khám phá dưới tối ưu. Hơn nữa, chúng tôi muốn đưa ra các khuyến nghị không phụ thuộc vào bản chất của thiết lập vấn đề, chẳng hạn như liệu các quan sát là dựa trên trạng thái hay pixel, hoặc liệu phần thưởng là thưa thớt hay dày đặc.

Để đạt được mục tiêu này, chúng tôi trình bày một phương pháp dựa trên RL model-free off-policy, không có tiền đào tạo hoặc ràng buộc rõ ràng, mà chúng tôi gọi là RLPD (Reinforcement Learning with Prior Data). Như đã thảo luận trong Phần 4.5, chúng tôi dựa thiết kế thuật toán của mình trên SAC (Haarnoja et al., 2018a;b), mặc dù về nguyên tắc những lựa chọn thiết kế này có thể cải thiện các phương pháp RL off-policy khác. Trước tiên, chúng tôi đề xuất một cơ chế đơn giản để kết hợp dữ liệu trước. Sau đó, chúng tôi xác định một bệnh lý tồn tại khi áp dụng một cách ngây thơ các phương pháp off-policy cho việc

0 500 1000−102−101−1000100101102103104105106107Giá Trị Q Trung Bình
0 500 10000204060Trả Về Chuẩn Hóa
0 500 1000−1000100101102103104105106Giá Trị Q Trung Bình
0 500 1000051015202530Trả Về Chuẩn Hóa Không LN
Với LN AntMaze Large Play Pen Sparse
Bước Môi Trường (£103)

Hình 2. Sử dụng SAC với phương pháp lấy mẫu đối xứng của chúng tôi có thể dẫn đến sự bất ổn do giá trị Q phân kỳ; với LayerNorm trong critic, điều này biến mất, cải thiện hiệu suất.

thiết lập vấn đề này, và đề xuất một giải pháp đơn giản và ít xâm lấn tối thiểu. Sau đó, chúng tôi cải thiện tốc độ dữ liệu ngoại tuyến được sử dụng bằng cách kết hợp các phương pháp mới nhất trong RL hiệu quả mẫu. Cuối cùng, chúng tôi nêu bật các lựa chọn thiết kế phổ biến trong RL sâu gần đây thực tế là nhạy cảm với môi trường, và nên được điều chỉnh tương ứng bởi các nhà thực hành.

4.1. Lựa Chọn Thiết Kế 1: Một Chiến Lược Đơn Giản và Hiệu Quả để Kết Hợp Dữ Liệu Ngoại Tuyến

Chúng tôi bắt đầu với một phương pháp đơn giản kết hợp dữ liệu trước không thêm chi phí tính toán, nhưng vẫn không phụ thuộc vào bản chất của dữ liệu ngoại tuyến. Chúng tôi gọi đây là 'lấy mẫu đối xứng', theo đó cho mỗi batch chúng tôi lấy mẫu 50% dữ liệu từ bộ đệm replay của chúng tôi, và 50% còn lại từ bộ đệm dữ liệu ngoại tuyến, giống với sơ đồ được sử dụng bởi Ross & Bagnell (2012). Như chúng ta sẽ thấy trong các phần sau, chiến lược lấy mẫu này hiệu quả đáng ngạc nhiên trên nhiều tình huống khác nhau, và chúng tôi phân tích chi tiết các yếu tố khác nhau của sơ đồ này (xem Phần 5.1). Tuy nhiên, việc áp dụng phương pháp này cho các phương pháp off-policy kinh điển, chẳng hạn như SAC (Haarnoja et al., 2018a), không mang lại hiệu suất mạnh mẽ, như chúng ta thấy trong Hình 1, và các lựa chọn thiết kế khác phải được xem xét.

4.2. Lựa Chọn Thiết Kế 2: Layer Normalization Giảm Thiểu Ước Lượng Quá Mức Thảm Khốc

Các thuật toán RL off-policy tiêu chuẩn truy vấn hàm Q đã học cho các hành động ngoài phân phối (OOD), có thể không được định nghĩa trong quá trình học. Do đó, có thể có ước lượng quá mức đáng kể của các giá trị thực tế do việc sử dụng xấp xỉ hàm (Thrun & Schwartz, 1993). Trong thực tế, hiện tượng này dẫn đến sự bất ổn đào tạo và có thể phân kỳ khi critic cố gắng bắt kịp với một giá trị liên tục tăng.

Đặc biệt, chúng tôi thấy đây là trường hợp khi áp dụng một cách ngây thơ phương pháp lấy mẫu đối xứng của chúng tôi cho các nhiệm vụ phức tạp (xem Hình 2). Phân kỳ critic là một vấn đề được nghiên cứu kỹ, đặc biệt trong chế độ ngoại tuyến, nơi chính sách không thể tạo ra kinh nghiệm mới. Tuy nhiên, trong thiết lập vấn đề của chúng tôi, chúng ta có thể lấy mẫu từ môi trường. Do đó, thay vì tạo ra một cơ chế rõ ràng ngăn cản các hành động OOD, có thể được xem như chống khám phá (Rezaeifar et al., 2022), chúng tôi thay vào đó cần đảm bảo đơn giản rằng các hàm đã học không ngoại suy theo cách không bị ràng buộc. Để đạt được điều này, chúng tôi cho thấy rằng Layer Normalization (LayerNorm) (Ba et al., 2016) có thể ràng buộc việc ngoại suy của các mạng nhưng, quan trọng là, không ràng buộc rõ ràng chính sách để ở gần dữ liệu ngoại tuyến. Điều này đến lượt nó không ngăn cản chính sách khám phá các vùng không biết và có khả năng có giá trị của không gian trạng thái-hành động.

Đặc biệt, chúng tôi chứng minh rằng LayerNorm ràng buộc các giá trị và thực nghiệm ngăn chặn ngoại suy giá trị thảm khốc. Cụ thể, xem xét một hàm Q Qparametrized bởi θ, w, áp dụng LayerNorm và biểu diễn trung gian ψθ(·,·). Đối với bất kỳ a và s nào chúng ta có thể nói1:
∥Qθ,w(s, a)∥=∥wTrelu(ψθ(s, a))∥
≤ ∥w∥∥relu(ψθ(s, a))∥ ≤ ∥ w∥∥ψ(s, a)∥
≤ ∥w∥

Do đó, kết quả của Layer Normalization, các giá trị Q được ràng buộc bởi norm của lớp trọng số, ngay cả đối với các hành động bên ngoài bộ dữ liệu. Vì vậy, tác động của ngoại suy hành động sai lầm được giảm thiểu rất nhiều, vì các giá trị Q của chúng không có khả năng lớn hơn đáng kể so với những giá trị đã thấy trong dữ liệu. Thật vậy, quay lại Hình 2, chúng ta thấy rằng việc giới thiệu LayerNorm vào critic cải thiện đáng kể hiệu suất thông qua việc giảm thiểu phân kỳ critic.

Để minh họa điều này, chúng tôi tạo ra một bộ dữ liệu với đầu vào x phân phối trong một vòng tròn có bán kính 0.5 và nhãn y=∥x∥. Chúng tôi nghiên cứu cách một MLP hai lớp tiêu chuẩn với kích hoạt ReLU (phổ biến trong RL sâu) ngoại suy bên ngoài phân phối dữ liệu, và tác động của việc thêm LayerNorm. Trong Hình 3, tham số hóa tiêu chuẩn dẫn đến ngoại suy không bị ràng buộc bên ngoài support, trong khi LayerNorm ràng buộc các giá trị, giảm đáng kể tác động của ngoại suy không kiểm soát.

4.3. Lựa Chọn Thiết Kế 3: RL Hiệu Quả Mẫu

Bây giờ chúng ta có một phương pháp trực tuyến tận dụng dữ liệu ngoại tuyến cũng như ngăn chặn ngoại suy giá trị cực đoan, trong khi duy trì tự do của một phương pháp off-policy không bị ràng buộc. Tuy nhiên, một lợi ích của các phương pháp ngoại tuyến và bị ràng buộc là chúng có một cơ chế rõ ràng để kết hợp hiệu quả dữ liệu trước, chẳng hạn như thông qua tiền đào tạo (Hester et al., 2018; Lee et al., 2021), hoặc một điều khoản giám sát phụ trợ (Nair et al., 2018a; Rudner et al., 2021) tương ứng. Trong trường hợp của chúng tôi, việc kết hợp dữ liệu trước là ngầm thông qua việc sử dụng backup Bellman trực tuyến trên các chuyển tiếp ngoại tuyến. Do đó, việc các backup Bellman này được thực hiện một cách hiệu quả mẫu nhất có thể là cực kỳ quan trọng.

Một cách để đạt được điều này là tăng số lượng cập nhật chúng ta thực hiện trên mỗi bước môi trường (còn được gọi là tỷ lệ update-to-data (UTD)), cho phép dữ liệu ngoại tuyến được

1Để đơn giản, chúng tôi xem xét LayerNorm không có các điều khoản bias. Điều này không thay đổi phân tích, vì nó là một hằng số.

−1.00−0.75−0.50−0.250.000.250.500.751.00−1.00−0.75−0.50−0.250.000.250.500.751.000.00.20.40.60.81.0
Dữ Liệu
−1.00−0.75−0.50−0.250.000.250.500.751.00 −1.00−0.75−0.50−0.250.000.250.500.751.000.00.20.40.60.81.0Không LayerNorm
−1.00−0.75−0.50−0.250.000.250.500.751.00 −1.00−0.75−0.50−0.250.000.250.500.751.000.00.20.40.60.81.0Với LayerNorm

Hình 3. Chúng tôi khớp dữ liệu (trái) với một MLP hai lớp không có LayerNorm (giữa) và với LayerNorm (phải). LayerNorm ràng buộc các giá trị và ngăn chặn ước lượng quá mức thảm khốc.

được "backup" nhanh hơn. Tuy nhiên, như đã được nêu bật trong văn học gần đây trong RL trực tuyến, điều này có thể tạo ra các vấn đề trong quá trình tối ưu hóa và trớ trêu thay giảm hiệu quả mẫu, do over-fitting thống kê (Li et al., 2022). Để cải thiện điều này, công trình trước đây đã đề xuất một số phương pháp regularization, chẳng hạn như chuẩn hóa L2 đơn giản (Veˇcer ík et al., 2017), Dropout (Srivastava et al., 2014; Hiraoka et al., 2022) và random ensemble distillation (Chen et al., 2021). Trong công trình này, chúng tôi quyết định sử dụng phương pháp random ensemble distillation cuối cùng; chúng tôi sẽ chứng minh thông qua các phân tích rằng điều này hoạt động mạnh nhất, đặc biệt trên các nhiệm vụ phần thưởng thưa thớt.

Chúng tôi cũng lưu ý rằng các vấn đề over-fitting giá trị tồn tại khi thực hiện TD-learning từ hình ảnh (Cetin et al., 2022). Do đó trong các thiết lập này, chúng tôi bao gồm thêm các tăng cường random shift (Kostrikov et al., 2021; Yarats et al., 2022).

4.4. Lựa Chọn Thiết Kế Theo Môi Trường

Sau khi nêu bật 3 lựa chọn thiết kế chính cho phương pháp của chúng tôi có thể được áp dụng một cách tổng quát cho tất cả môi trường và bộ dữ liệu ngoại tuyến, bây giờ chúng tôi chuyển sự chú ý đến các lựa chọn thiết kế thường được coi là đương nhiên, nhưng thực tế có thể nhạy cảm với môi trường. Được ghi nhận rằng các thuật toán RL sâu nhạy cảm với các chi tiết triển khai (Henderson et al., 2018; Engstrom et al., 2020; Andrychowicz et al., 2021; Furuta et al., 2021). Kết quả là, nhiều công trình trong RL sâu yêu cầu điều chỉnh siêu tham số theo môi trường. Cho sự đa dạng lớn của các nhiệm vụ chúng tôi xem xét trong các thí nghiệm, chúng tôi tin rằng việc đóng góp vào diễn ngôn này là quan trọng, và nêu bật rằng những lựa chọn thiết kế nhất định, thường chỉ được kế thừa từ các triển khai trước đây, thực tế nên được xem xét lại cẩn thận, và có thể giải thích tại sao các phương pháp off-policy chưa cạnh tranh cho đến nay trên các vấn đề chúng tôi xem xét. Do đó chúng tôi có quan điểm rằng, cho sự nhạy cảm được ghi nhận của RL sâu, việc chứng minh một con đường quan trọng của các lựa chọn thiết kế để xem xét khi đánh giá môi trường mới là quan trọng, và cung cấp một quy trình làm việc để đơn giản hóa quá trình này cho các nhà thực hành.

Clipped Double Q-Learning (CDQ). Các phương pháp dựa trên giá trị kết hợp với xấp xỉ hàm và tối ưu hóa ngẫu nhiên chịu sự không chắc chắn ước lượng, mà, kết hợp với mục tiêu tối đa hóa của Q-learning, dẫn đến ước lượng quá mức giá trị (van Hasselt et al., 2016) (như cũng đã thảo luận trong Phần 4.2). Để giảm thiểu vấn đề này

--- TRANG 4 ---
RL Trực Tuyến Hiệu Quả với Dữ Liệu Ngoại Tuyến

vấn đề này, Fujimoto et al. (2018) giới thiệu Clipped Double Q-Learning (CDQ) bao gồm việc lấy tối thiểu của một ensemble hai hàm Q để tính TD-backup. Họ định nghĩa các mục tiêu để cập nhật các critic như sau:
y=r(s, a) +γmin i=1,2Qθi(s′, a′) trong đó a′∼π(·|s′).

Tuy nhiên, điều này tương ứng với việc khớp các giá trị Q mục tiêu 1 std. dưới các giá trị mục tiêu thực tế, và công trình gần đây (Moskovitz et al., 2021) đề xuất rằng lựa chọn thiết kế này có thể không hữu ích phổ biến vì nó có thể quá bảo thủ. Do đó, điều này quan trọng để xem xét lại, đặc biệt bên ngoài các miền mà nó được thiết kế ban đầu, chẳng hạn như các nhiệm vụ phần thưởng thưa thớt phổ biến trong thiết lập vấn đề của chúng tôi.

Maximum Entropy RL. MaxEnt RL tăng cường mục tiêu trả về truyền thống với một điều khoản entropy:
max π Es∼ρπ,a∼π[∞X t=0γt(rt+αH(π(a|s)))]

Điều này tương ứng với việc tối đa hóa phần thưởng chiết khấu và entropy chính sách dự kiến tại mỗi bước thời gian. Động lực cho các phương pháp này tập trung xung quanh tính bền vững và khám phá (tức là tối đa hóa phần thưởng trong khi hành vi ngẫu nhiên nhất có thể). Các phương pháp dựa trên mục tiêu này ấn tượng về mặt thực nghiệm (Haarnoja et al., 2018a;b; Chen et al., 2021; Hiraoka et al., 2022). Do đó, chúng tôi tin rằng lựa chọn thiết kế này quan trọng trong bối cảnh tinh chỉnh trực tuyến, nơi phần thưởng thường thưa thớt và yêu cầu khám phá.

Kiến trúc. Kiến trúc mạng có thể có tác động đáng kể đến hiệu suất RL sâu, và cùng một kiến trúc có thể tối ưu trong một môi trường có thể dưới tối ưu trong môi trường khác (Furuta et al., 2021). Để đơn giản hóa không gian tìm kiếm, chúng tôi xem xét tác động của việc có 2 hoặc 3 lớp trong actor và critic, đã được chỉ ra ảnh hưởng đến hiệu suất, ngay cả trên các nhiệm vụ kinh điển (Ball & Roberts, 2021).

4.5. RLPD: Tổng Quan Phương Pháp

Ở đây chúng tôi trình bày mã giả cho phương pháp của chúng tôi, nêu bật bằng màu Xanh lá các yếu tố quan trọng đối với phương pháp của chúng tôi, và bằng màu Tím, các lựa chọn thiết kế cụ thể cho môi trường.

Các yếu tố chính của RLPD nằm ở dòng 1 và 13 của Thuật toán 1 với việc áp dụng LayerNorm, ensemble lớn, học hiệu quả mẫu và phương pháp lấy mẫu đối xứng để kết hợp dữ liệu trực tuyến và ngoại tuyến. Đối với các lựa chọn cụ thể cho môi trường, chúng tôi khuyến nghị như sau như điểm khởi đầu:
• Dòng 3: Tập con 2 critic
• Dòng 16: Loại bỏ entropy
• Dòng 1: Sử dụng MLP 3 lớp sâu hơn

Như một quy trình làm việc thực tế, chúng tôi khuyến nghị phân tích các lựa chọn thiết kế Tím này trước, và theo thứ tự đã nêu ở trên.

Thuật toán 1 RL Trực Tuyến với Dữ Liệu Ngoại Tuyến (RLPD)
1: Chọn LayerNorm, Kích Thước Ensemble Lớn E, Gradient Steps G, và kiến trúc.
2: Khởi tạo ngẫu nhiên Critic θi (đặt mục tiêu θ′ i=θi) cho i= 1,2, . . . , E và tham số Actor ϕ. Chọn chiết khấu γ, nhiệt độ α và trọng số EMA critic ρ.
3: Xác định số lượng mục tiêu Critic để tập con Z∈ {1,2}
4: Khởi tạo bộ đệm replay trống R
5: Khởi tạo bộ đệm D với dữ liệu ngoại tuyến
6: while True do
7: Nhận quan sát trạng thái ban đầu s0
8: for t = 0, T do
9: Thực hiện hành động at∼πϕ(·|st)
10: Lưu trữ chuyển tiếp (st, at, rt, st+1) trong R
11: for g= 1, G do
12: Lấy mẫu minibatch bR của N 2 từ R
13: Lấy mẫu minibatch bD của N 2 từ D
14: Kết hợp bR và bD để tạo batch b có kích thước N
15: Lấy mẫu tập Z của Z chỉ số từ {1,2, . . . , E }
16: Với b, đặt y=r+γ min i∈ZQθ′ i(s′,˜a′) ,˜a′∼πϕ(·|s′)
17: Thêm điều khoản entropy y=y+γα logπϕ(˜a′|s′)
18: for i= 1, E do
19: Cập nhật θi tối thiểu hóa loss: L=1 N X i(y−Qθi(s, a))2
20: end for
21: Cập nhật mạng mục tiêu θ′ i←ρθ′ i+ (1−ρ)θi
22: end for
23: Với b, cập nhật ϕ tối đa hóa mục tiêu: 1 E E X i=1Qθi(s,˜a)−α logπϕ(˜a|s),˜a∼πϕ(· |s)
24: end for
25: end while

5. Thí Nghiệm

Chúng tôi thiết kế các thí nghiệm của mình không chỉ để chứng minh tầm quan trọng của các lựa chọn thiết kế, mà còn cung cấp những hiểu biết cho phép các nhà thực hành nhanh chóng thích ứng RLPD với các vấn đề của họ. Như vậy, chúng tôi nhằm trả lời các câu hỏi sau:

1. RLPD có cạnh tranh với công trình trước mà không sử dụng tiền đào tạo cũng như không có ràng buộc rõ ràng không?
2. RLPD có chuyển giao sang môi trường dựa trên pixel không?
3. LayerNorm có giảm thiểu phân kỳ giá trị không?
4. Quy trình làm việc được đề xuất xung quanh các lựa chọn thiết kế cụ thể cho môi trường có dẫn đến hiệu suất đáng tin cậy không?

--- TRANG 5 ---
RL Trực Tuyến Hiệu Quả với Dữ Liệu Ngoại Tuyến

0 200 400 600 800 10000255075Tất Cả Miền Adroit Thưa Thớt
0 100 200 300050100Tất Cả Miền D4RL AntMaze
0 50 100 150 200 250050100Tất Cả Nhiệm Vụ D4RL Locomotion
SoTA Trước
SACfD
RLPD (Của chúng tôi)
Bước Môi Trường (£103)Trả Về Chuẩn Hóa

Hình 4. RLPD vượt qua hiệu suất tiên tiến trước đây trên một số benchmark phổ biến khác nhau trong khi đơn giản hơn đáng kể. Kết quả được tổng hợp trên 21 môi trường khác nhau (10 Seed, 1 std. bóng). Trong mỗi trường hợp, chúng tôi so sánh với công trình tốt nhất đã biết trước đây (IQL + Tinh Chỉnh trong Adroit và AntMaze, Off2On trong Locomotion), và SACfD, một phương pháp off-policy kinh điển sử dụng dữ liệu ngoại tuyến.

Đối với (1), chúng tôi so sánh RLPD với những phương pháp đã được thiết kế để sử dụng dữ liệu ngoại tuyến để tăng tốc học trực tuyến. Đối với (2), chúng tôi xem xét một bộ nhiệm vụ bổ sung để nghiên cứu khả năng áp dụng của RLPD cho các miền dựa trên thị giác. Sau đó, đối với (3) chúng tôi thực hiện phân tích để chứng minh tầm quan trọng của việc sử dụng LayerNorm. Cuối cùng, đối với (4) chúng tôi chứng minh quy trình làm việc được đề xuất của chúng tôi trên các nhiệm vụ thách thức nhất (xem Tiểu mục 4.5).

RLPD so sánh như thế nào? Chúng tôi xem xét 21 nhiệm vụ sau từ các benchmark đã thiết lập:

• Sparse Adroit (Nair et al., 2020). Các 3 nhiệm vụ thao tác khéo léo này—xoay bút, mở cửa, di chuyển bóng—là các nhiệm vụ thử thách, phần thưởng thưa thớt. Dữ liệu ngoại tuyến là đa phương thức, với một tập nhỏ các minh họa con người và một tập lớn các quỹ đạo từ một chính sách behavior-cloned được đào tạo trên dữ liệu con người này. Chúng tôi tuân theo tiêu chí đánh giá nghiêm ngặt của Kostrikov et al. (2022), theo đó hiệu suất dựa trên tốc độ hoàn thành, chứ không phải tỷ lệ thành công. IQL + Tinh Chỉnh đại diện cho công trình mạnh nhất trước đây (Kostrikov et al., 2022).

• D4RL AntMaze (Fu et al., 2020). Các 6 nhiệm vụ phần thưởng thưa thớt này yêu cầu một agent Ant học đi bộ và điều hướng qua một mê cung để đạt được mục tiêu. Dữ liệu ngoại tuyến chỉ bao gồm các quỹ đạo dưới tối ưu có thể về nguyên tắc được "khâu" lại với nhau. Một lần nữa, IQL + Tinh chỉnh đại diện cho công trình mạnh nhất trước đây (Kostrikov et al., 2022).

• D4RL Locomotion (Fu et al., 2020). Cuối cùng, chúng tôi có 12 nhiệm vụ locomotion phần thưởng dày đặc có dữ liệu ngoại tuyến với các mức độ chuyên môn khác nhau. Off2On (Lee et al., 2021) có hiệu suất tiên tiến trên bộ nhiệm vụ này.

Để đánh giá, trước tiên chúng tôi bao gồm SACfD, một baseline được nghiên cứu trong công trình trước (Veˇcer ík et al., 2017; Nair et al., 2020), mà, tương tự như RLPD, là một phương pháp off-policy kết hợp dữ liệu ngoại tuyến trong quá trình đào tạo. Tuy nhiên, SACfD chỉ đơn giản khởi tạo bộ đệm replay trực tuyến với dữ liệu ngoại tuyến. Chúng tôi triển khai baseline này sử dụng SAC mà không có các quyết định thiết kế bổ sung được thảo luận trong tiểu mục 4.5. Sau đó, vì không có phương pháp trước đây nào đạt được hiệu suất tốt nhất trên tất cả các nhóm nhiệm vụ, chúng tôi so sánh với phương pháp tiên tiến cụ thể cho từng nhóm (như đã liệt kê ở trên). Chúng tôi gọi so sánh này trong các biểu đồ của chúng tôi là SoTA Trước. Đối với tất cả các thí nghiệm trong công trình này, chúng tôi báo cáo giá trị trung bình và độ lệch chuẩn trên 10 seed và tổng hợp kết quả trên các nhiệm vụ trong ba nhóm đã liệt kê. Đối với kết quả chi tiết đầy đủ được chia nhỏ theo nhiệm vụ, xem Phụ lục A, và đối với chi tiết môi trường nhiều hơn, xem Phụ lục B.1.

Chúng ta thấy rằng RLPD hoạt động mạnh mẽ, hoặc khớp hoặc vượt qua đáng kể công trình tốt nhất đã biết trước đây trên các benchmark thử thách này (xem Hình 4). Chúng tôi nhắc lại rằng chúng tôi trình bày kết quả cho RLPD và SACfD mà không thực hiện bất kỳ tiền đào tạo nào, không giống như các phương pháp SoTA Trước. Vì vậy, trong khi công trình trước (hiển thị bằng màu xanh) có thể đạt được hiệu suất ban đầu mạnh mẽ, cải thiện trực tuyến khiêm tốn hơn. Mặt khác, phương pháp của chúng tôi đạt được hoặc vượt qua hiệu suất này trong khoảng chỉ 10k mẫu trực tuyến. Đáng chú ý, chúng tôi vượt trội so với hiệu suất tốt nhất được báo cáo trên nhiệm vụ 'Door' Sparse Adroit lên 2.5×. Hơn nữa, theo hiểu biết của chúng tôi, phương pháp của chúng tôi là phương pháp đầu tiên 'giải quyết' hiệu quả tất cả các nhiệm vụ AntMaze. Hơn nữa, chúng tôi có thể làm điều đó trong ít hơn một phần ba ngân sách time-step được phân bổ cho các phương pháp trước.

RLPD có chuyển giao sang pixel không? Ở đây chúng tôi xem xét các nhiệm vụ locomotion medium và expert trong V-D4RL (Lu et al., 2022), một bộ dữ liệu ngoại tuyến chỉ với quan sát pixel. V-D4RL đặc biệt thử thách vì các chính sách hành vi dựa trên trạng thái, có nghĩa là dữ liệu là quan sát một phần trong không gian pixel. Điều này rõ ràng nhất trong 'Humanoid Walk', nơi các bộ phận cơ thể bị che khuất trực quan, do đó behavior cloning (BC) có thể khó khăn để đạt được hiệu suất mạnh mẽ.

Để đánh giá, vì phương pháp của chúng tôi tìm cách tăng tốc học trực tuyến với dữ liệu ngoại tuyến, chúng tôi tập trung vào hiệu quả dữ liệu—chúng tôi giới thiệu một thử thách mà chúng tôi gọi là "10%DMC", bao gồm việc đào tạo các chính sách chỉ sử dụng 10% tổng timestep được khuyến nghị bởi Yarats et al. (2022). Vì chúng tôi sử dụng dữ liệu medium và expert, chúng tôi bao gồm một baseline BC. Để đánh giá khả năng của RLPD sử dụng hiệu quả dữ liệu ngoại tuyến để tăng cường học trực tuyến, chúng tôi so sánh với một phương pháp baseline không sử dụng dữ liệu ngoại tuyến. Để cô lập tác động của việc sử dụng dữ liệu ngoại tuyến, chúng tôi sử dụng cùng kiến trúc và trình tối ưu hóa chính sách như phương pháp của chúng tôi và gắn nhãn baseline này là Online trong các biểu đồ của chúng tôi. Sau đó, để đánh giá cách này so sánh với phương pháp RL hiệu quả mẫu tiên tiến từ pixel, chúng tôi so sánh với DrQ-v2 (Yarats et al., 2022).

--- TRANG 6 ---
RL Trực Tuyến Hiệu Quả với Dữ Liệu Ngoại Tuyến

0 25 50 75 100020406080Walker Walk
0 100 200 3000204060Cheetah Run
0 1000 2000 30000102030Humanoid Walk
BC Medium
BC Expert
DrQ-v2
Online
RLPD (Của chúng tôi) Medium
RLPD (Của chúng tôi) Expert
Bước Môi Trường (£103)Trả Về Chuẩn Hóa

Hình 5. Phương pháp của chúng tôi tổng quát hóa sang các miền dựa trên thị giác, cung cấp cải thiện nhất quán so với các phương pháp hiện có.

Chúng ta thấy trong Hình 52 rằng RLPD cung cấp cải thiện nhất quán so với các phương pháp hoàn toàn trực tuyến, và trong nhiều trường hợp cải thiện đáng kể so với baseline BC. Chúng ta thấy thông qua sự khác biệt trong RLPD (đường đen và tím đứt nét) và baseline Online rằng RLPD hiệu quả sử dụng dữ liệu ngoại tuyến để bootstrap học tập. Kết luận này giữ vững khi chúng ta so sánh với phương pháp RL dựa trên thị giác SoTA (xanh).

0 100 200 300020406080Cheetah Run Expert
UTD=1 (Online)
UTD=1 (RLPD)
UTD=10 (Online)
UTD=10 (RLPD)
Bước Môi Trường (£103)Trả Về Chuẩn Hóa

Hình 6. Tăng UTD với RLPD cải thiện đáng kể hiệu quả mẫu từ pixel.

Cuối cùng, chúng tôi thử nghiệm tăng UTD lên 10 trên một trong các nhiệm vụ—Cheetah Run với dữ liệu ngoại tuyến Expert. Hình 6 cho thấy cải thiện đáng kể trong hiệu suất khi học với bộ dữ liệu ngoại tuyến. Theo hiểu biết của chúng tôi, đây là minh chứng đầu tiên về một phương pháp UTD cao cải thiện điều khiển liên tục dựa trên pixel model-free.

5.1. Phân Tích và Nghiên Cứu Ablation RLPD

Ở đây, chúng tôi giải quyết (3) và (4) bằng cách định lượng tác động của LayerNorm, và chứng minh độ tin cậy của quy trình làm việc được đề xuất của chúng tôi (xem Tiểu mục 4.5).

LayerNorm có giảm thiểu phân kỳ giá trị không? Bây giờ chúng tôi minh họa tầm quan trọng của LayerNorm trong việc giảm thiểu phân kỳ giá trị thảm khốc, và tập trung vào các nhiệm vụ mà ước lượng quá mức đặc biệt dễ xảy ra. Trong Hình 7, chúng ta thấy LayerNorm quan trọng cho hiệu suất mạnh mẽ trong miền Adroit; loại trừ LayerNorm dẫn đến phương sai cao hơn đáng kể trên các seed và giảm hiệu suất trung bình.

Để minh họa rõ ràng hơn tác động này, chúng tôi xây dựng một bộ dữ liệu chỉ gồm dữ liệu minh họa chuyên gia con người từ các nhiệm vụ Adroit Sparse (xem "Expert Adroit Sparse Tasks" trong Hình 7). Tập con này chỉ bao gồm 22 trong số 500 quỹ đạo trong bộ dữ liệu ban đầu và phân phối hẹp hơn nhiều theo bản chất—đại diện cho một nhiệm vụ với phần thưởng thưa thớt, minh họa hạn chế, và độ bao phủ dữ liệu ngoại tuyến hẹp—có khả năng làm trầm trọng thêm phân kỳ giá trị. Ở đây chúng ta thấy một kết quả đáng kể: RLPD vẫn vượt qua công trình trước, mặc dù có những hạn chế đáng kể trong dữ liệu. Hơn nữa, loại bỏ LayerNorm bây giờ dẫn đến hiệu suất sụp đổ, không có tiến bộ nào được thực hiện trên bất kỳ nhiệm vụ nào. Chúng tôi quan sát thêm cải thiện trong hiệu quả mẫu thông qua việc bao gồm LayerNorm trong AntMaze và Humanoid Walk thông qua việc giảm ngoại suy quá mức.

Các thí nghiệm và kết quả bổ sung có trong Phụ lục A.

0 200 400 600 800 1000050Nhiệm Vụ Adroit Sparse
0 200 400 600 800 1000050Nhiệm Vụ Expert Adroit Sparse
0 100 200 3000100Nhiệm Vụ Large D4RL AntMaze
Không LN
RLPD (Của chúng tôi)
0 1000 2000 3000020Nhiệm Vụ V-D4RL Humanoid Walk
Bước Môi Trường (£103)Trả Về Chuẩn Hóa

Hình 7. LayerNorm quan trọng cho hiệu suất mạnh mẽ, đặc biệt khi dữ liệu hạn chế hoặc phân phối hẹp.

Quy trình làm việc lựa chọn thiết kế. Bây giờ chúng tôi thúc đẩy quy trình làm việc của chúng tôi trong Phần 4.5, chứng minh tầm quan trọng của những lựa chọn thiết kế này. Chúng tôi tập trung vào các nhiệm vụ khó nhất, cụ thể là 'Relocate' trong Adroit Sparse, 'Large Diverse' trong AntMaze, và 'Humanoid Expert' trong V-D4RL, vì chúng tôi thấy chúng nhạy cảm nhất. Chúng tôi cung cấp kết quả đầy đủ trên tất cả các nhiệm vụ trong Phụ lục A, lưu ý rằng các quyết định tối ưu trong các miền khó hơn cũng tạo ra kết quả tối ưu trong các miền dễ hơn.

0 500 100002040Relocate Sparse
Entropy Backups
2 Layers
RLPD (Của chúng tôi)
0 100 200 300020406080100AntMaze Large Diverse
Entropy Backups
2 Layers
CDQ
RLPD (Của chúng tôi)
0 1000 2000 300005101520Humanoid Walk Expert
Entropy Backups
No Ensemble
CDQ
RLPD (Của chúng tôi)
Bước Môi Trường (£103)Trả Về Chuẩn Hóa

Hình 8. Các lựa chọn thiết kế khởi đầu được khuyến nghị và quy trình làm việc của chúng tôi dẫn đến hiệu suất mạnh mẽ trên tất cả các nhiệm vụ.

Chúng ta thấy trong Hình 8 rằng với các lựa chọn thiết kế cụ thể cho môi trường được khuyến nghị cung cấp hiệu suất mạnh mẽ; chúng ta thấy việc sử dụng entropy backup và mạng nhỏ hơn luôn dẫn đến hiệu suất tệ hơn. Trong 'AntMaze Large Diverse' tuy nhiên chúng ta thấy rằng với CDQ, hiệu suất xấu đi. Tuân theo quy trình làm việc của chúng tôi, và phân tích điều này bằng cách tập con 1 critic là quan trọng để khôi phục hiệu suất mạnh mẽ. Điều tương tự áp dụng cho 'Humanoid Walk Expert', theo đó chúng ta ngạc nhiên thấy rằng CDQ có hại cho hiệu suất, mặc dù tính phổ biến của nó trong các triển khai gần đây. Chúng tôi cũng cho thấy tác động tích cực đáng ngạc nhiên của các ensemble lớn hơn trong các nhiệm vụ dựa trên pixel, với ensemble critic 2 thành viên tiêu chuẩn hoạt động tệ hơn ensemble 10 thành viên chúng tôi sử dụng mặc định trong RLPD.

Cuối cùng, chúng tôi tiến hành các ablation bổ sung để hiểu tầm quan trọng của các lựa chọn thiết kế mà chúng tôi đề xuất cho phương pháp của mình, cho thấy rằng, mặc dù các quyết định thiết kế riêng lẻ đơn giản, chúng rất quan trọng cho hiệu suất tốt.

--- TRANG 7 ---
RL Trực Tuyến Hiệu Quả với Dữ Liệu Ngoại Tuyến

Regularization critic. Ở đây chúng tôi xem xét tác động của regularization critic đối với hiệu suất. Chúng tôi so sánh 3 phương pháp: weight-decay3 (Veˇcer ík et al., 2017), Dropout (Hiraoka et al., 2022) và ensembling (Chen et al., 2021). Chúng tôi chọn một tập con của các thí nghiệm trước để đánh giá từ các phần AntMaze, Adroit, và Locomotion.

0 100 200 300020406080100AntMaze Large Play
0 100 200 3000204060Pen Sparse
0 100 2000255075100Walker2d Medium
Dropout
Weight Decay
Ensemble (Của chúng tôi)
Bước Môi Trường (£103)Trả Về Chuẩn Hóa

Hình 9. Nói chung, ensembling critic cung cấp hiệu suất tốt nhất. Dropout hoạt động tệ hơn trong các nhiệm vụ phần thưởng thưa thớt.

Trong Hình 9, chúng ta thấy rằng ensembling là dạng regularization mạnh nhất. Đáng chú ý, trong khi Dropout hoạt động tốt trong miền Locomotion 'walker2d-medium-v0', như được khẳng định bởi Hiraoka et al. (2022), nó không tổng quát hóa sang các môi trường phần thưởng thưa thớt thử thách. Chúng tôi cũng thấy rằng regularization weight-decay kém hiệu quả hơn trong tất cả các miền.

Khởi tạo bộ đệm. Trong phần này, chúng tôi so sánh lấy mẫu đối xứng với một phương pháp dựa vào việc khởi tạo bộ đệm replay với dữ liệu ngoại tuyến.

050Trả Về Chuẩn Hóa Pen Sparse Door Sparse
0 200 400 600 800 1000050% Batch
Reward
Non-Zero
0 200 400 600 800 1000Seeded
Symmetric (Của chúng tôi)
Bước Môi Trường (£103)

Hình 10. Lấy mẫu đối xứng cải thiện hiệu quả mẫu và giảm phương sai trên các seed, và không hoạt động bằng cách đơn giản tăng mật độ phần thưởng trong một batch.

0 100 200020406080100120walker2d-medium-v0
Tốt Nhất Trong Dataset
Online
Seeded
Symmetric (Của chúng tôi)
Bước Môi Trường (£103)Trả Về Chuẩn Hóa

Hình 11. Khởi tạo bộ đệm với lượng lớn dữ liệu hạn chế cải thiện.

Trước tiên quay lại thiết lập Human Expert Demonstrations thử thách trong Adroit, trong Hình 10 chúng tôi cho thấy hai ví dụ tương phản chứng minh lấy mẫu đối xứng hiệu quả cân bằng giữa dữ liệu replay và ngoại tuyến. Chúng tôi quan sát rằng trong môi trường Pen, lấy mẫu đối xứng rõ ràng cải thiện khám phá bằng cách đảm bảo tăng mật độ phần thưởng trong mini-batch. Ngược lại, chúng ta thấy rằng mặc dù phương pháp khởi tạo bộ đệm khám phá tốt như nhau trong môi trường Door, lấy mẫu đối xứng có tác động quan trọng trong việc cải thiện tính ổn định và giảm phương sai do dựa ít hơn vào dữ liệu phương sai cao hơn được tạo ra bởi chính sách trực tuyến.

Bây giờ chúng tôi xem xét một tình huống mà chúng ta có dữ liệu ngoại tuyến dưới tối ưu dồi dào, và thấy rằng một lần nữa, phương pháp lấy mẫu cân bằng của chúng tôi cải thiện hiệu quả mẫu. Trong Hình 11, khởi tạo bộ đệm với khối lượng lớn dữ liệu locomotion chất lượng trung bình mang lại cải thiện ban đầu so với hiệu suất trực tuyến, nhưng khó khăn về mặt tiệm cận, có thể do thiếu lấy mẫu dữ liệu on-policy, quan trọng cho cải thiện trực tuyến. Mặt khác, phương pháp lấy mẫu đối xứng của chúng tôi cải thiện hiệu quả mẫu và khớp hiệu suất tiệm cận trong khi giảm phương sai.

Độ nhạy cảm tỷ lệ lấy mẫu. Chúng tôi đánh giá độ nhạy cảm của phương pháp lấy mẫu của chúng tôi khỏi tỷ lệ "đối xứng" 50% trực tuyến/ngoại tuyến.

0 100 200 300020406080100Antmaze Large Play
0 100 200 3000204060Pen Sparse
0 100 2000255075100125Walker2d Medium
0% Offline
25% Offline
50% Offline (Của chúng tôi)
75% Offline
100% Offline
Bước Môi Trường (£103)Trả Về Chuẩn Hóa

Hình 12. RLPD không nhạy cảm với tỷ lệ replay; 50% cung cấp sự thỏa hiệp tốt nhất giữa phương sai, tốc độ hội tụ và hiệu suất tiệm cận.

Như chúng ta thấy trong Hình 12, RLPD không rất nhạy cảm với tỷ lệ lấy mẫu. Trong khi lấy mẫu 25% ngoại tuyến có thể giúp hiệu suất tiệm cận một chút trong 'walker2d-medium-v0', điều này đi kèm với chi phí phương sai và hiệu quả mẫu trong các nhiệm vụ phần thưởng thưa thớt. Chúng tôi cũng khẳng định với kết quả 100% ngoại tuyến của chúng tôi rằng RLPD không phải là một phương pháp ngoại tuyến, và chìa khóa thành công của nó là cách nó kiểm soát phân kỳ mà không hạn chế khám phá hoặc học hành vi.

6. Kết Luận

Trong công trình này, chúng tôi cho thấy rằng các phương pháp off-policy có thể được thích ứng để tận dụng dữ liệu ngoại tuyến khi đào tạo trực tuyến. Chúng ta thấy rằng với việc áp dụng cẩn thận các lựa chọn thiết kế chính, RLPD có thể đạt được hiệu suất mạnh mẽ đáng kể, và chứng minh điều này trên tổng cộng 30 nhiệm vụ khác nhau. Cụ thể, chúng tôi cho thấy rằng sự kết hợp độc đáo của lấy mẫu đối xứng, LayerNorm như một regularizer ngoại suy giá trị, và học hiệu quả mẫu là chìa khóa thành công của nó, dẫn đến việc chúng tôi vượt trội so với công trình trước lên đến 2.5× trên một loạt các benchmark cạnh tranh đa dạng. Hơn nữa, các khuyến nghị của chúng tôi có tác động không đáng kể đến hiệu quả tính toán so với các phương pháp hoàn toàn trực tuyến, và đơn giản, cho phép các nhà thực hành dễ dàng kết hợp những hiểu biết của công trình này vào các phương pháp hiện có. Để cải thiện thêm khả năng áp dụng, chúng tôi khuyến nghị và chứng minh một quy trình làm việc cho các nhà thực hành cải thiện hiệu suất trên nhiều nhiệm vụ đa dạng, và do đó chứng minh rằng những lựa chọn thiết kế kinh điển nhất định nên được xem xét lại khi áp dụng các phương pháp off-policy. Cuối cùng, để tạo thuận lợi cho nghiên cứu tương lai, chúng tôi đã phát hành codebase RLPD tại đây: github.com/ikostrikov/rlpd, có tính năng các thuật toán off-policy được tối ưu hóa cao cho các nhiệm vụ proprioceptive và dựa trên pixel trong một codebase duy nhất được viết bằng JAX (Bradbury et al., 2018).

Lời Cảm Ơn

PJB được tài trợ thông qua Willowgrove Foundation và Les Woods Memorial Fund. Nghiên cứu này được hỗ trợ một phần bởi chương trình DARPA RACER, ARO W911NF-21-1-0097, Office of Naval Research dưới N00014-21-1-2838 và N00014-19-12042. Nghiên cứu này sử dụng tài nguyên cụm tính toán Savio được cung cấp bởi chương trình Berkeley Research Computing tại University of California, Berkeley (được hỗ trợ bởi UC Berkeley Chancellor, Vice Chancellor for Research, và Chief Information Officer).

Tài Liệu Tham Khảo

[Phần tài liệu tham khảo được giữ nguyên như trong bản gốc do là danh sách trích dẫn tiêu chuẩn]

--- TRANG 8 ---
[Tiếp tục với các trang còn lại theo cùng format dịch thuật...]
