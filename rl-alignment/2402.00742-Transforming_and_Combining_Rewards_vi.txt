# 2402.00742.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2402.00742.pdf
# Kích thước tệp: 6176106 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Biến đổi và Kết hợp Phần thưởng
để Căn chỉnh các Mô hình Ngôn ngữ Lớn
Zihao Wang *1, Chirag Nagpal2, Jonathan Berant3, Jacob Eisenstein3, Alex
D'Amour3, Sanmi Koyejo4,3, và Victor Veitch1,3
1Đại học Chicago
2Google Research
3Google DeepMind
4Đại học Stanford
Tóm tắt
Một phương pháp phổ biến để căn chỉnh các mô hình ngôn ngữ với sở thích của con người là trước tiên học một mô hình phần thưởng từ dữ liệu sở thích, sau đó sử dụng mô hình phần thưởng này để cập nhật mô hình ngôn ngữ. Chúng tôi nghiên cứu hai vấn đề liên quan chặt chẽ phát sinh trong phương pháp này. Thứ nhất, bất kỳ phép biến đổi đơn điệu nào của mô hình phần thưởng đều bảo toàn thứ hạng sở thích; có lựa chọn nào "tốt hơn" các lựa chọn khác không? Thứ hai, chúng ta thường muốn căn chỉnh các mô hình ngôn ngữ với nhiều thuộc tính: chúng ta nên kết hợp nhiều mô hình phần thưởng như thế nào? Sử dụng một cách diễn giải xác suất của quy trình căn chỉnh, chúng tôi xác định một lựa chọn tự nhiên cho phép biến đổi cho (trường hợp phổ biến của) phần thưởng được học từ các mô hình sở thích Bradley-Terry. Phép biến đổi có được rất đơn giản: chúng tôi áp dụng hàm log-sigmoid cho phần thưởng được trung tâm hóa, một phương pháp chúng tôi gọi là "phép biến đổi LSC" (phép biến đổi log-sigmoid-trung tâm). Phép biến đổi này có hai thuộc tính quan trọng. Thứ nhất, nó nhấn mạnh việc cải thiện các đầu ra có hiệu suất kém, thay vì các đầu ra đã có điểm số tốt. Điều này giảm thiểu cả việc khớp dữ liệu kém (nơi một số lời nhắc không được cải thiện) và hack phần thưởng (nơi mô hình học cách khai thác sự sai lệch của mô hình phần thưởng). Thứ hai, nó cho phép tổng hợp có nguyên tắc các phần thưởng bằng cách liên kết phép cộng với phép kết hợp logic: tổng của các phần thưởng đã biến đổi tương ứng với xác suất mà đầu ra "tốt" trong tất cả các thuộc tính được đo, theo nghĩa mà chúng tôi làm rõ. Các thí nghiệm căn chỉnh mô hình ngôn ngữ để vừa hữu ích vừa vô hại bằng RLHF cho thấy cải thiện đáng kể so với phương pháp cơ sở (không biến đổi).

1 Giới thiệu
Trong bài báo này, chúng tôi quan tâm đến cách căn chỉnh các mô hình ngôn ngữ lớn để thiên vị đầu ra của chúng hướng tới việc có các thuộc tính mong muốn—ví dụ, để hữu ích, vô hại, chính xác về sự thật, hoặc sáng tạo [Zie+19; Sti +20; Ouy +22]. Chúng tôi nghiên cứu phương pháp hai giai đoạn nơi chúng ta trước tiên học một mô hình phần thưởng từ sở thích của con người, sau đó căn chỉnh mô hình ngôn ngữ để đầu ra của nó có giá trị cao theo mô hình phần thưởng. Trong ngữ cảnh này, chúng tôi quan tâm đến hai vấn đề cơ bản:

1.Bước căn chỉnh tối đa hóa mô hình phần thưởng đã học kỳ vọng. Tuy nhiên, bất kỳ phép biến đổi đơn điệu nào của phần thưởng đều bảo toàn cách diễn giải của quy trình căn chỉnh như việc thiên vị mô hình ngôn ngữ hướng tới các đầu ra được con người ưa thích. Chúng ta có thể cải thiện bước căn chỉnh bằng cách biến đổi phần thưởng đã học không?

*Công việc được thực hiện trong thời gian thực tập tại Google DeepMind
Xuất bản tại ICML 2024.
1arXiv:2402.00742v2  [cs.CL]  19 Jul 2024

--- TRANG 2 ---
2.Chúng ta thường muốn căn chỉnh các mô hình ngôn ngữ với nhiều thuộc tính—ví dụ, đầu ra nên hữu ích, và vô hại, và chính xác về sự thật. Nếu chúng ta có mô hình phần thưởng cho mỗi thuộc tính, chúng ta nên kết hợp chúng như thế nào?

Một thách thức chính trong việc trả lời những câu hỏi này là mục tiêu của việc căn chỉnh không được định nghĩa chính xác. Kết quả là, không có nguyên tắc rõ ràng nào để hướng dẫn việc lựa chọn phương pháp biến đổi hoặc tổng hợp. Ý tưởng khái niệm trong bài báo này là diễn giải việc căn chỉnh theo cách xác suất. Từ góc độ này, mục tiêu của việc căn chỉnh một mô hình với một thuộc tính cụ thể là tạo ra các mẫu từ phân phối hậu nghiệm có điều kiện trên việc đầu ra "tốt" trên thuộc tính đó. Tương tự, mục tiêu của việc căn chỉnh với nhiều thuộc tính là tạo ra các mẫu có điều kiện trên việc đầu ra "tốt" trên tất cả các thuộc tính.

Để sử dụng ý tưởng này, chúng ta cần định nghĩa ý nghĩa của việc một đầu ra "tốt". Trong ngữ cảnh phần thưởng được học từ dữ liệu sở thích, chúng ta lấy một đầu ra y là "tốt" nếu nó có phần thưởng r(x,y) lớn hơn một giá trị tham chiếu cụ thể theo lời nhắc rref(x). Kết quả chính đầu tiên của bài báo là trong trường hợp (điển hình) nơi mô hình phần thưởng được học từ dữ liệu sở thích bằng mô hình Bradley-Terry và mô hình ngôn ngữ được căn chỉnh bằng cách tối đa hóa phần thưởng kỳ vọng với ràng buộc KL, lựa chọn tự nhiên của phép biến đổi là:

u(x,y) =logσ(r(x,y)−rref(x)), (1.1)

trong đó σ(·) là hàm sigmoid. Ở đây, r là mô hình phần thưởng Bradley-Terry đã học, và u là phần thưởng đã biến đổi mà chúng ta sử dụng trong bước căn chỉnh. Chúng tôi gọi phép biến đổi này là "phép biến đổi LSC" (phép biến đổi log-sigmoid-trung tâm), và việc căn chỉnh sử dụng phép biến đổi như vậy là "căn chỉnh LSC".

Phép biến đổi này được thúc đẩy bởi một cách diễn giải xác suất. Nó cũng có những lợi ích thực tiễn quan trọng so với phương pháp cơ sở của việc sử dụng mô hình phần thưởng thô. Thứ nhất, phần thưởng đã biến đổi thu nhỏ tiện ích biên của các giá trị phần thưởng rất cao. Điều này có tác dụng trong việc căn chỉnh là vừa khuyến khích mô hình cải thiện các lời nhắc có hiệu suất kém, vừa ngăn cản mô hình "hack phần thưởng" bằng cách tối ưu hóa mô hình phần thưởng ngoài phạm vi hợp lệ của nó. Thứ hai, phần thưởng đã biến đổi cung cấp một cách tự nhiên để kết hợp nhiều mô hình phần thưởng. Cụ thể: tổng của các phần thưởng đã biến đổi tương ứng với phép AND logic của việc đầu ra "tốt" trên mỗi thuộc tính. Vậy, sau khi biến đổi các phần thưởng, chúng ta có thể đơn giản cộng chúng để tổng hợp nhiều mô hình phần thưởng.

Kết hợp lại, những lợi ích này có thể dẫn đến cải thiện đáng kể trong hiệu suất căn chỉnh. Hình 1 so sánh việc căn chỉnh một mô hình ngôn ngữ để vừa hữu ích vừa vô hại bằng cách sử dụng phép cộng của phần thưởng đã biến đổi và chưa biến đổi. Thay đổi cường độ chính quy hóa KL được sử dụng trong bước căn chỉnh, chúng tôi quan sát thấy rằng phần thưởng đã biến đổi dẫn đến cải thiện đáng kể ở tất cả các mức KL.

2 Kiến thức chuẩn bị
Chúng tôi trước tiên xem xét quy trình hai bước Học tăng cường từ Phản hồi của Con người (RLHF) tiêu chuẩn để căn chỉnh các mô hình ngôn ngữ với sở thích của con người.

Huấn luyện mô hình phần thưởng từ dữ liệu cặp đôi Các mô hình phần thưởng được huấn luyện để mô phỏng phản hồi của con người. Một loại phản hồi thường được sử dụng là dữ liệu sở thích cặp đôi, bao gồm một lời nhắc x và hai phản hồi được tạo y+,y−, trong đó y+ được người chú thích ưa thích hơn. Thảo luận của chúng tôi chủ yếu tập trung vào trường hợp này.

Thông thường, phần thưởng được học bằng mô hình Bradley-Terry [BT52],
p(y−≺y+|x) =σ(r(x,y+)−r(x,y−)). (2.1)
Hàm r được tham số hóa bởi một mạng neural (thường là một LLM khác) và được khớp bằng mục tiêu log likelihood tối đa tiêu chuẩn.

--- TRANG 3 ---
Hình 1: Biến đổi phần thưởng Bradley-Terry vừa giảm thiểu overfitting vừa làm cho phép cộng hoạt động như phép AND logic. Điều này dẫn đến cải thiện đáng kể trong chất lượng mô hình đã căn chỉnh so với thực hành tiêu chuẩn. Mỗi điểm trên biểu đồ là một LLM được căn chỉnh với trọng số phạt KL khác nhau. Trục y hiển thị cải thiện so với LLM được tinh chỉnh có giám sát (SFT) cơ sở trong cả tính hữu ích VÀ vô hại, như được đánh giá bởi một mô hình đánh giá bên ngoài (không được sử dụng cho RLHF). Đường cơ sở tổng hợp dưới mức tối ưu (thường mất điểm ở tính hữu ích hoặc vô hại) và gặp phải hack phần thưởng (hiệu suất giảm trong chế độ KL cao). Chi tiết trong phần 5.

Căn chỉnh với mô hình phần thưởng Bước tiếp theo là cập nhật LLM cơ sở để thiên vị nó hướng tới các phản hồi có phần thưởng cao.

Thường thì, việc căn chỉnh mô hình với hàm phần thưởng được tiến hành bởi một bước "Tinh chỉnh có Giám sát" (SFT) nơi mô hình cơ sở được tinh chỉnh sử dụng mục tiêu mô hình hóa ngôn ngữ trên các ví dụ thắng từ dữ liệu sở thích của con người. Chúng tôi ký hiệu mô hình này là π0. Sự quan tâm của chúng tôi là cách sử dụng mô hình phần thưởng để căn chỉnh π0 thêm.

Mục tiêu của bước căn chỉnh là cập nhật π0 thành một mô hình mới π* có phần thưởng kỳ vọng cao, trong khi vẫn gần với π0 (để bảo toàn thông tin từ các giai đoạn huấn luyện trước). Thực hành tiêu chuẩn là học π* bằng cách tối đa hóa phần thưởng kỳ vọng của các mẫu, được chính quy hóa bởi một phạt trên sự phân kỳ KL giữa π* và π0.

Ý tưởng chính trong bài báo này là thay vào đó sử dụng một thước đo tiện ích u(x,y) là một phép biến đổi đơn điệu của r(x,y). Chúng tôi để quy trình căn chỉnh không thay đổi khác. Sau đó, về mặt toán học, π* là bộ tối đa hóa của:

Ex{Ey∼π(·|x)[u(x,y)]−γKL(π(·|x)∥π0(·|x))} (2.2)

Ở đây, γ là một siêu tham số kiểm soát sự đánh đổi giữa việc tối đa hóa phần thưởng và căn chỉnh với π0.

3 Biến đổi Phần thưởng
Bây giờ chúng tôi chuyển sang việc dẫn xuất phép biến đổi phần thưởng.

Chính thức hóa Mục tiêu Căn chỉnh Bước đầu tiên là chính thức hóa mục tiêu của việc căn chỉnh. Điều này cần thiết để xác định một phép biến đổi "đúng". Một cách trực quan, chúng ta muốn sửa đổi chính sách ban đầu π0(y|x) để các mẫu được tạo ra được coi là "tốt" trong một thuộc tính nào đó bởi con người. Để tiến bộ, chúng tôi giới thiệu một biến ngẫu nhiên ngữ nghĩa nhị phân G chỉ ra liệu phản hồi y có "tốt" cho lời nhắc x hay không. Sau đó chúng tôi định nghĩa mục tiêu căn chỉnh là tạo ra

--- TRANG 4 ---
(a)Hình dạng Biến đổi
 (b)Ví dụ Tính hữu ích
Hình 2: Phần thưởng Bradley-Terry không nắm bắt được tiện ích giảm dần, và biến đổi log-sigmoid có thể sửa điều này. Trong các ví dụ phản hồi, việc chuyển từ phản hồi 1 sang phản hồi 2 tăng đáng kể tiện ích, nhưng từ phản hồi 2 sang phản hồi 3 chỉ tăng một cách biên tế. Tuy nhiên, phần thưởng BT coi mỗi cải thiện như nhau. Một phép biến đổi log-sigmoid phản ánh lợi nhuận giảm dần.

một mô hình lấy mẫu từ phân phối của các phản hồi có điều kiện trên phản hồi tốt; tức là, πtarget(·|x) =p(·|x,G=1).

Thực tế, chúng tôi tổng quát hóa điều này một chút để cho phép kiểm soát chi tiết hơn của sự đánh đổi phần thưởng vs KL. Bởi quy tắc Bayes, chúng ta có thể viết lại p(y|x,G=1)∝π0(y|x)p(G=1|x,y). Nghĩa là, chúng ta tái trọng số LLM cơ sở bằng một thuật ngữ tăng trọng các phản hồi có khả năng được coi là tốt. Điều tự nhiên là giới thiệu một siêu tham số để kiểm soát cường độ của việc tăng trọng này. Dự đoán kết nối với (2.2), chúng tôi lại sử dụng γ cho siêu tham số này. Sau đó, chúng tôi định nghĩa mục tiêu căn chỉnh của mình là tạo ra một mô hình đã căn chỉnh

πtarget_γ(y|x)∝π0(y|x)p(G=1|x,y)1/γ(3.1)

Biến đổi Phần thưởng Câu hỏi tiếp theo là làm thế nào để tạo ra một mô hình đã căn chỉnh thỏa mãn mục tiêu của chúng ta. Điều này có hai phần: chúng ta phải sử dụng mô hình phần thưởng để định nghĩa biến tốt nhị phân G, và chúng ta phải xác định hàm tiện ích được sử dụng cho việc căn chỉnh liên quan đến G như thế nào.

Hàm Tiện ích Mục tiêu Chúng tôi bắt đầu bằng cách kết nối tiện ích căn chỉnh và G. Ý tưởng là sử dụng kết quả nổi tiếng rằng bộ tối ưu hóa lý tưởng của mục tiêu RLHF được chính quy hóa KL (2.2) là một nghiêng mũ của chính sách cơ sở [ví dụ, KPB22]:

π*(y|x)∝π0(y|x)exp(u(x,y)/γ) (3.2)

So sánh (3.1) với (3.2), chúng ta thấy rằng để có được chính sách mục tiêu thông qua căn chỉnh, chúng ta phải đặt hàm tiện ích là log-xác suất của tốt:

u(x,y) =logp(G=1|x,y). (3.3)

Mô hình Phần thưởng Pointwise Bước tiếp theo là liên hệ mô hình Bradley-Terry với p(G=1|x,y). Như một bài tập khởi động, chúng tôi xem xét trường hợp mô hình phần thưởng được huấn luyện trên dữ liệu pointwise; tức là, nơi mỗi ví dụ là một lời nhắc x, phản hồi y, và một nhãn nhị phân G chỉ ra liệu phản hồi có tốt hay không. Trong trường hợp này, chúng ta sẽ huấn luyện hàm phần thưởng bằng cách giảm thiểu một mục tiêu entropy chéo nhị phân. Đây là một quy tắc chấm điểm thích hợp, vì vậy phần thưởng đã học sẽ là: r(x,y) =logit p(G=1|x,y). Ở đây, chúng ta lấy phần thưởng ở thang logit để r(x,y)∈(−∞,∞), tương tự như phần thưởng Bradley-Terry. Trong trường hợp này, tiện ích đúng là u(x,y) =logσ(r(x,y)).

--- TRANG 5 ---
Mô hình Phần thưởng Cặp đôi Trường hợp cặp đôi tinh tế hơn. Có thể hấp dẫn khi lại áp dụng phép biến đổi log(σ(·)) cho phần thưởng Bradley-Terry. Điều này không đúng vì hai lý do. Thứ nhất, chỉ có sự khác biệt phần thưởng mới có thể diễn giải được như xác suất logit—phần thưởng cho các lời nhắc riêng lẻ thì không. Thứ hai, nói chung mô hình phần thưởng r(x,y) không thể xác định được từ dữ liệu. Đối với bất kỳ r nào, chúng ta có thể dịch chuyển nó bằng bất kỳ hàm tùy ý nào của lời nhắc x mà không thay đổi mô hình Bradley-Terry. Nghĩa là, ˜r(x,y)←r(x,y) +C(x) có cùng mục tiêu trong (2.1). Tuy nhiên, bất kỳ phép biến đổi phi tuyến nào của phần thưởng sẽ nhạy cảm với C(x) không xác định này.

May mắn thay, cả hai vấn đề đều được giải quyết bằng cách chọn một định nghĩa phù hợp về ý nghĩa của việc một phản hồi tốt. Ở đây, chúng tôi lấy một phản hồi được tạo y là "tốt" nếu nó được ưa thích hơn một đầu ra tham chiếu được chọn yref. Ví dụ, chúng ta có thể nói rằng y vô hại miễn là nó được ưa thích bởi phần thưởng vô hại hơn một phản hồi đóng hộp như "Tôi không thể trả lời câu hỏi đó".

Phép biến đổi LSC theo ngay sau đó:

Định lý 1. Giả sử đầu ra y được coi là tốt cho lời nhắc x nếu nó sẽ được ưa thích hơn đầu ra tham chiếu yref(x). Sau đó, nếu mô hình Bradley-Terry (2.1) giữ, và chúng ta căn chỉnh bằng tối đa hóa tiện ích được chính quy hóa KL (2.2), thì sử dụng tiện ích

u(x,y) =logσ(r(x,y)−rref(x)) (3.4)

sẽ thỏa mãn mục tiêu căn chỉnh (3.1). Ở đây rref(x):=r(x,yref(x)).

Nghĩa là, một khi chúng ta quyết định về một phản hồi tham chiếu, chúng ta có được hàm tiện ích đúng bằng cách áp dụng phép biến đổi log-sigmoid cho phần thưởng được trung tâm hóa.

Diễn giải Cơ chế Chúng tôi dẫn xuất phép biến đổi phần thưởng từ một luận cứ xác suất. Cũng có thể hiểu được khi xem xét tác động cơ chế của phép biến đổi.

Một vấn đề cơ bản với phương pháp cơ sở, nơi u(x,y) =r(x,y), là lợi ích tiện ích cho các cải thiện không bao giờ giảm. Trong mô hình đã căn chỉnh, lấy γ=1, chúng ta có rằng xác suất tương đối của hai phản hồi là mũ của sự khác biệt tiện ích.

π*(y1|x)/π*(y0|x)=exp(u(y1,x)−u(y0,x))π0(y1|x)/π0(y0|x)(3.5)

Bây giờ, xem xét trường hợp chúng ta có ba phản hồi ứng viên: yref,y0,y1 sao cho p(yref≺y0) =0.99 và p(yref≺y1) =0.999. Nếu chúng ta sử dụng logit Bradley-Terry thô làm tiện ích của mình, thì sử dụng rằng r(y1,x)−r(y0,x) = ( r(y1,x)−r(yref,x))−(r(y0,x)−r(yref,x)) chúng ta có:

π*(y1|x)/π*(y0|x)=exp(logit (0.999 )−logit (0.99 )))π0(y1|x)/π0(y0|x)
≈10×π0(y1|x)/π0(y0|x)

Nghĩa là, khi căn chỉnh với phần thưởng thô, việc chuyển từ một phản hồi rất tốt sang một phản hồi tốt hơn một cách biên tế tăng xác suất lên một hệ số 10! Tuy nhiên, nếu yref đã tốt, một con người sẽ thấy ít sự khác biệt giữa hai phản hồi. Ngược lại, việc căn chỉnh với mô hình phần thưởng đã biến đổi chỉ tăng xác suất lên một hệ số 1.01.

Tác động này có vẻ đặc biệt nổi bật khi chúng ta xem xét rằng chính mô hình phần thưởng được học. Có vẻ không chắc rằng mô hình thực sự có thể phân biệt đáng tin cậy giữa y1 và

Cuối cùng, vấn đề ở đây là phần thưởng Bradley-Terry không tự động tương ứng với tiện ích.

--- TRANG 6 ---
y0. Theo đó, khi chúng ta căn chỉnh với mô hình đã học thô, chúng ta mong đợi gây ra những thay đổi lớn theo các sở thích dù sao cũng nhiễu.

Chọn phản hồi tham chiếu Phần thưởng tham chiếu hoạt động như một siêu tham số của phép biến đổi. Về cơ bản, phép biến đổi tạo ra một tiện ích tuyến tính dưới giá trị tham chiếu, và giảm nhanh ở trên nó. Theo đó, chúng ta nên đặt tham chiếu thành một giá trị đại diện cho một phản hồi tốt, mà chúng ta tin là có thể đạt được bởi mô hình, và nơi chúng ta tin rằng hàm phần thưởng đã học của mình đưa ra dự đoán có ý nghĩa. Chúng tôi thấy rằng một lựa chọn mặc định tốt là phân vị thứ 85 của các ví dụ từ phân phối cơ sở. Chúng tôi xem xét các ví dụ bổ sung trong phần 5.

4 Tổng hợp Phần thưởng
Bây giờ chúng tôi xem xét trường hợp khi chúng ta có nhiều mô hình phần thưởng cho các mục tiêu khác nhau r1, ...,rn.

Mục tiêu Căn chỉnh Một lần nữa, bước đầu tiên trong việc dẫn xuất một sơ đồ tổng hợp tối ưu là chính thức hóa một mục tiêu. Chúng tôi đưa ra lựa chọn tự nhiên sau: mô hình đã căn chỉnh nên "tốt" trên tất cả các thuộc tính mục tiêu. Ví dụ, chúng ta muốn căn chỉnh mô hình của mình để hữu ích VÀ vô hại.

Để chính thức hóa ý tưởng này, gọi Gi là biến ngẫu nhiên nhị phân chỉ ra liệu y có được coi là "tốt" cho x trong thuộc tính i hay không. Chúng tôi giới thiệu biến ngẫu nhiên nhị phân tương ứng với phép AND logic: GAND:=∧ni=1Gi. Tương tự trường hợp phần thưởng đơn, chúng tôi chính thức hóa mục tiêu là phân phối hậu nghiệm có điều kiện trên tất cả các thuộc tính "tốt":

πtarget_AND,γ∝π0(y|x)p(GAND=1|x,y)1/γ(4.1)

Tổng hợp Phần thưởng Với mục tiêu này, theo Định lý 1, chúng ta muốn căn chỉnh bằng tiện ích

u(x,y) =logp(GAND=1|x,y). (4.2)

Câu hỏi sau đó là làm thế nào để xây dựng hàm tiện ích này bằng các mô hình phần thưởng riêng lẻ.

Trong tổng quát đầy đủ, đây là một nhiệm vụ bất khả thi. Lý do là các phần thưởng riêng lẻ chỉ cho chúng ta biết về phân phối biên của các thuộc tính, nhưng phép AND logic có thể phụ thuộc vào các tương tác. Do đó, chúng ta cần một giả định bổ sung:

Giả định 1 (Đánh giá Độc lập). Cho một lời nhắc cố định x và phản hồi y, việc y có được đánh giá là tốt cho x trên mỗi thuộc tính là độc lập với tất cả các đánh giá trên tất cả các thuộc tính khác. Nghĩa là, (G1, ...,Gn) độc lập có điều kiện cho (X,Y).

Ví dụ, giả định này nói rằng chúng ta có thể quyết định liệu một phản hồi nhất định có hữu ích hay không độc lập với việc quyết định liệu nó có hại hay không. (Lưu ý: điều này có điều kiện trên lời nhắc và phản hồi. Chúng tôi không yêu cầu tính hữu ích và vô hại độc lập về mặt biên.)

Công thức tổng hợp phần thưởng theo ngay sau đó:

--- TRANG 7 ---
Định lý 2. Giả sử đầu ra y được coi là tốt cho lời nhắc x trong khía cạnh i nếu nó sẽ được ưa thích hơn đầu ra tham chiếu yref_i(x) trong thuộc tính i. Sau đó, nếu Giả định 1 giữ, mô hình Bradley-Terry (2.1) giữ cho tất cả các thuộc tính, và chúng ta căn chỉnh bằng tối đa hóa tiện ích được chính quy hóa KL (2.2), thì sử dụng tiện ích

u(x,y) =∑ni=1logσ(ri(x,y)−rref_i(x)) (4.3)

sẽ thỏa mãn mục tiêu căn chỉnh (4.1). Ở đây rref_i(x):=r(x,yref_i(x)).

Diễn giải Cơ chế Chúng tôi dẫn xuất sơ đồ tổng hợp theo một giả định xác suất. Tương tự trường hợp phần thưởng đơn, chúng tôi xem xét tác động cơ chế.

Phương pháp cơ sở là tổng hợp với một tổng (có trọng số) của các phần thưởng thô. Vấn đề chính là phương pháp này cho phép hiệu suất mạnh trong một thuộc tính cân bằng hiệu suất kém trong thuộc tính khác.

Xem xét trường hợp chúng ta có hai thuộc tính A và B mà chúng ta muốn căn chỉnh (ví dụ, tính hữu ích và vô hại), và 4 phản hồi yref_A,yref_B,y0,y1 sao cho p(yref_A≺y1) =p(yref_B≺y1) =0.9, và p(yref_A≺y0) =0.45 với p(yref_B≺y0) =0.99. Nếu chúng ta muốn phân phối đã căn chỉnh tạo ra các mẫu "tốt" trong cả hai khía cạnh, thì y1 nên được ưa thích hơn y0. Tuy nhiên, nếu chúng ta sử dụng tổng của logit Bradley-Terry thô làm tiện ích ( u=rA+rB), tỷ lệ xác suất tương đối dưới chính sách đã căn chỉnh π* sẽ là (với γ=1)

π*(y1|x)/π*(y0|x)=1×π0(y1|x)/π0(y0|x). (4.4)

Nghĩa là, mô hình đã căn chỉnh không tăng trọng phản hồi y1. Nếu thay vào đó chúng ta căn chỉnh bằng tổng của phần thưởng đã biến đổi, thì chúng ta có tỷ lệ xác suất tương đối xấp xỉ 1.8—tức là, phản hồi hoạt động tốt trên cả hai thuộc tính được ưa thích hơn.

5 Thí nghiệm
Bây giờ chúng tôi chuyển sang đánh giá tác động thực tiễn của việc sử dụng phần thưởng đã biến đổi để căn chỉnh LLM. Chúng tôi thí nghiệm với việc căn chỉnh các mô hình để hữu ích, vô hại, và cả hai. Chúng tôi thấy rằng phép biến đổi làm giảm hack phần thưởng và khớp dữ liệu kém phần thưởng, và việc căn chỉnh với tổng đã biến đổi hoạt động như căn chỉnh với phép AND logic. Điều này dẫn đến cải thiện đáng kể trong LLM được căn chỉnh với phần thưởng đã biến đổi.

5.1 Thiết lập Thí nghiệm
Chúng tôi theo một quy trình RLHF tiêu chuẩn; xem phụ lục A.4 để biết chi tiết đầy đủ.

Bộ dữ liệu Chúng tôi sử dụng bộ dữ liệu Tính hữu ích và Vô hại của Anthropic [Bai+22]. Đây là các cuộc đối thoại nhiều lượt giữa con người và trợ lý kỹ thuật số. Mỗi bộ dữ liệu bao gồm phần đầu của cuộc trò chuyện, hai phản hồi cho lượt cuối của phía AI trong cuộc trò chuyện, và một nhãn cho sở thích của con người về thuộc tính mục tiêu. Chúng tôi sử dụng các bộ dữ liệu cơ sở (44K ví dụ cho tính hữu ích và 42K cho vô hại), nơi các phản hồi được tạo từ một LM context-distilled 52B. Đối với cả hai nhiệm vụ, chúng tôi chia tập huấn luyện thành hai: một nửa để huấn luyện mô hình phần thưởng, và một nửa cho bước căn chỉnh.

--- TRANG 8 ---
(a)Hữu ích: RLHF
(b)Vô hại: RLHF
Hình 3: Phần thưởng đã biến đổi có được sự đánh đổi KL và tỷ lệ thắng tốt hơn trong phần thưởng đơn. Chúng tôi hiển thị hai tỷ lệ thắng: 1) tỷ lệ thắng được đánh giá từ bộ đánh giá PALM 2 được prompt, giữa chính sách đã căn chỉnh và các mẫu SFT ngẫu nhiên, và 2) tỷ lệ thắng được đánh giá bởi bộ đánh giá T5-XXL, so với các phân vị SFT: 85% cho tính hữu ích và 95% cho vô hại.

Huấn luyện mô hình phần thưởng Chúng tôi huấn luyện một mô hình phần thưởng Bradley-Terry cho mỗi tính hữu ích và vô hại bằng cách tinh chỉnh một mô hình T5-base đã được tiền huấn luyện (220M tham số) [Raf+20] trên dữ liệu Anthropic.

SFT Đối với mô hình chính sách của chúng tôi, chúng tôi sử dụng mô hình PALM-2-XXS được tinh chỉnh hướng dẫn [Ani+23]. Theo thực hành tiêu chuẩn, chúng tôi trước tiên chạy tinh chỉnh có giám sát (SFT) của LLM được tinh chỉnh hướng dẫn trên các phản hồi 'được ưa thích' từ bộ dữ liệu tính hữu ích. Chúng tôi sử dụng mô hình SFT này làm cơ sở trước căn chỉnh cho tất cả các thí nghiệm.

Thiết lập RLHF Để căn chỉnh, chúng tôi theo thực hành tiêu chuẩn và tối ưu hóa tiện ích kỳ vọng với ràng buộc phạt KL bằng thuật toán Tối ưu hóa Chính sách Gần (PPO). Đối với mỗi hàm tiện ích và bộ dữ liệu, chúng tôi quét qua nhiều giá trị của cường độ chính quy hóa KL γ. Chúng tôi chạy trong 20000 bước, mà chúng tôi thấy đủ để hội tụ trong tất cả các trường hợp.

5.2 Phép biến đổi LSC Cải thiện Căn chỉnh
Việc biến đổi mô hình phần thưởng nên khuyến khích việc căn chỉnh tập trung vào việc cải thiện các phản hồi có phần thưởng thấp hơn so với những phản hồi đã có phần thưởng cao. Chúng tôi mong đợi điều này sẽ vừa giảm hack phần thưởng, vừa giảm số lượng phản hồi có phần thưởng thấp (ít khớp dữ liệu kém hơn).

Lựa chọn phần thưởng tham chiếu Phần thưởng tham chiếu rref(x) nên nắm bắt khái niệm của một phản hồi "đủ tốt". Đối với vô hại, điều này rõ ràng: một phản hồi chung như "Tôi không thể trả lời điều đó" đạt được mục tiêu. Đối với các thí nghiệm, chúng tôi lấy mẫu các biến thể của "Tôi không thể trả lời điều đó" làm phần thưởng tham chiếu.

--- TRANG 9 ---
Đối với tính hữu ích, những phản hồi đóng hộp như vậy sẽ không đủ. Thay vào đó, cho mỗi lời nhắc chúng tôi lấy mẫu 64 phản hồi từ mô hình SFT. Sau đó chúng tôi sử dụng dữ liệu này để xây dựng một bộ ước lượng phân vị thứ 85 của các phần thưởng được lấy mẫu cho mỗi lời nhắc. Chúng tôi sử dụng phân vị thứ 85 ước tính này làm phần thưởng tham chiếu. Chi tiết được cung cấp trong phụ lục A.1.

Phép biến đổi Cải thiện Căn chỉnh Việc căn chỉnh với phần thưởng đã biến đổi nên giảm hack phần thưởng và khớp dữ liệu kém so với việc căn chỉnh với phần thưởng thô. Sau đó, chúng tôi mong đợi rằng việc căn chỉnh đã biến đổi dẫn đến lợi ích lớn hơn so với mô hình SFT.

Chúng tôi có hai chiến lược chính để đánh giá cải thiện so với mô hình SFT. Thứ nhất, theo công việc trước [GSH23; Cos +23; Eis +23], chúng tôi huấn luyện một mô hình T5-XXL sử dụng cùng bộ dữ liệu sở thích và mục tiêu Bradley-Terry. Điều này cung cấp một đại diện cho sở thích thực sự mà chúng tôi không tối ưu hóa so với (vì vậy, nó có thể chứng kiến hack phần thưởng). Chúng tôi nói một mẫu từ mô hình đã căn chỉnh thắng về tính hữu ích nếu nó đánh bại phân vị phần thưởng thứ 85 của các mẫu từ mô hình SFT. Chúng tôi nói nó thắng về vô hại nếu nó đánh bại phân vị thứ 95. (Những con số này được chọn để làm cho việc thắng đủ khó để hiển thị khoảng cách giữa các chiến lược căn chỉnh; kết quả nhất quán trên các lựa chọn phân vị khác.).

Chiến lược thứ hai đánh giá số lần thắng bằng cách truy vấn zero-shot một mô hình PALM-2 medium được tinh chỉnh hướng dẫn. Theo công việc trước [Dub+23; Sin +23; Eis +23; Raf +23], chúng tôi truyền một lời nhắc, một phản hồi từ mô hình SFT, và một phản hồi từ mô hình đã căn chỉnh và hỏi cái nào được ưa thích (về tính hữu ích hoặc vô hại). Chi tiết trong phụ lục A.2.

Hình 3 hiển thị tỷ lệ thắng của các mô hình đã căn chỉnh so với mô hình SFT cho mỗi chiến lược đánh giá. Chúng tôi tính trung bình trên các lời nhắc trong bộ dữ liệu validation RLHF. Chúng tôi thấy rằng việc căn chỉnh với phần thưởng đã biến đổi thống trị việc căn chỉnh với phần thưởng thô, dưới cả hai chiến lược đánh giá và ở tất cả các mức khoảng cách KL đến mô hình chính sách cơ sở.

Xem phụ lục A.2 cho các thí nghiệm bổ sung đánh giá cải thiện do căn chỉnh gây ra.

Cải thiện Đồng đều của Phần thưởng Rõ ràng là việc căn chỉnh bằng phần thưởng đã biến đổi cải thiện so với việc căn chỉnh bằng phần thưởng thô. Một cách trực quan, điều này là do giảm cả hack phần thưởng và khớp dữ liệu kém. Để kiểm tra liệu trực giác này có đúng không, chúng tôi vẽ phân phối phần thưởng của các mẫu từ các mô hình được căn chỉnh thô và được căn chỉnh đã biến đổi khớp KL (xấp xỉ) trong Hình 4. Như mong đợi, phân phối phần thưởng của các mẫu từ mô hình được căn chỉnh đã biến đổi tập trung hơn. Nghĩa là, có ít mẫu phần thưởng rất cao hơn (ít hack phần thưởng hơn) và ít mẫu phần thưởng rất thấp hơn (ít khớp dữ liệu kém hơn).

Chi tiết hơn: cho mỗi tính hữu ích và vô hại, chúng tôi chọn một mô hình được căn chỉnh thô và được căn chỉnh đã biến đổi với KL khớp xấp xỉ. Chúng tôi lấy mẫu phản hồi từ mỗi mô hình với cùng lời nhắc. Sau đó, chúng tôi tính toán phần thưởng (T5-base) cho mỗi phản hồi này, được trung tâm hóa bởi phần thưởng trung vị của các mẫu SFT (để làm cho phần thưởng có thể so sánh trên các lời nhắc). Hình 4 hiển thị biểu đồ của những phần thưởng được lấy mẫu này. Chúng tôi cũng so sánh trên nhiều giá trị KL trong Hình 11.

Phép biến đổi giảm phím tắt Một triệu chứng của hack phần thưởng là mô hình đã căn chỉnh sẽ bắt đầu khai thác "phím tắt" được ưa thích bởi mô hình phần thưởng (nhưng không tương ứng với cải thiện thực sự). Chúng tôi xem xét tác động của phép biến đổi phần thưởng lên hai phím tắt như vậy. Đối với tính hữu ích, Eisenstein et al. [Eis+23] quan sát thấy rằng các mô hình được căn chỉnh thô có xu hướng định dạng đầu ra dưới dạng danh sách. Đối với vô hại, chúng tôi quan sát thấy rằng các mô hình được căn chỉnh thô thường sẽ đưa ra phản hồi dạng "bạn nên tham khảo ý kiến [bác sĩ / nhà tâm lý học / luật sư / v.v.]" (một quan sát tương tự được đưa ra bởi Bai et al. [Bai+22]). Trong Hình 5,

--- TRANG 10 ---
(a)Tính hữu ích (KL≈18)
 (b)Vô hại (KL ≈7)
Hình 4: Phép biến đổi Phần thưởng dẫn đến cải thiện phần thưởng đồng đều hơn so với đường cơ sở. Chúng tôi so sánh phân phối phần thưởng trong các chính sách đã căn chỉnh được khớp trên KL. Phần thưởng được trung tâm hóa bởi trung vị SFT trong cả tính hữu ích và vô hại. Phân phối phần thưởng tập trung hơn khi sử dụng phần thưởng đã biến đổi so với sử dụng phần thưởng thô.

(a)Tính hữu ích
 (b)Vô hại
Hình 5: Phần thưởng đã biến đổi giảm phím tắt trong các phản hồi được tạo. Đối với tính hữu ích, chúng tôi xác định một mẫu phím tắt của việc sử dụng danh sách, tương tự như [Eis+23]. Trong vô hại, một mẫu phím tắt đã biết là khuyến nghị người dùng tìm kiếm liệu pháp hoặc tham khảo ý kiến chuyên gia [Bai+22]. Chúng tôi trích xuất những phím tắt này bằng phương pháp heuristic. Trong phương pháp cơ sở, mô hình chính sách khai thác những phím tắt đó để có giá trị phần thưởng cao hơn. Điều này được giảm thiểu khi chúng ta biến đổi phần thưởng.

chúng tôi vẽ tỷ lệ phản hồi chứa mỗi phím tắt, cho mỗi mô hình đã căn chỉnh. Chúng tôi thấy rằng mô hình được căn chỉnh phần thưởng thô thực sự khai thác những phím tắt này. Hơn nữa, hành vi này được giảm thiểu đáng kể bằng cách căn chỉnh với phần thưởng đã biến đổi thay thế. Xem phụ lục A.3 để biết chi tiết.

5.3 Tổng hợp Phần thưởng sử dụng Phép biến đổi LSC Cải thiện Căn chỉnh Đáng kể

Bây giờ chúng tôi chuyển sang mục tiêu thứ hai: tổng hợp phần thưởng cho nhiều mục tiêu riêng biệt. Để đạt được điều đó, chúng tôi xem xét việc căn chỉnh một LLM để vừa hữu ích vừa vô hại.

5.3.1 Thiết lập Thí nghiệm Thêm
Thiết lập RLHF Chúng tôi sử dụng các mô hình phần thưởng được huấn luyện cho các nhiệm vụ tính hữu ích và vô hại như đã thảo luận ở trên. Đối với huấn luyện RLHF, chúng tôi chỉ sử dụng lời nhắc từ bộ dữ liệu tính hữu ích. Quyết định này là do quan sát về sự căng thẳng giữa tính hữu ích và vô hại, điều này buộc [Bai+22] phải sử dụng tỷ lệ lời nhắc tính hữu ích cao hơn so với vô hại. Chúng tôi sử dụng cùng mô hình chính sách như trong các thí nghiệm cho phần thưởng đơn (SFT-ed trên dữ liệu tính hữu ích). Các chi tiết huấn luyện khác giống như trong các thí nghiệm phần thưởng đơn.

--- TRANG 11 ---
(a)Hữu ích và Vô hại: Best-of-k
 (b)Hữu ích và Vô hại: RLHF
Hình 6: Phép cộng phần thưởng đã biến đổi có được sự đánh đổi tốt hơn giữa KL/K và tỷ lệ thắng. Trong Hình 6a và Hình 6b chúng tôi hiển thị tỷ lệ thắng so với phần thưởng trung vị SFT trong cả tính hữu ích và vô hại, được đánh giá bởi bộ đánh giá T5-XXL, cho Best-of-k và RLHF tương ứng.

thiết lập best-of-k Ngoài các thí nghiệm RLHF, chúng tôi cũng thí nghiệm với lấy mẫu best-of-k, như trong Gao et al. [GSH23] và Eisenstein et al. [Eis+23]. Nghĩa là, chúng tôi rút k mẫu từ mô hình SFT, xếp hạng chúng theo phần thưởng kết hợp, và trả về mẫu được xếp hạng cao nhất. Điều này có thể được xem như một quy trình căn chỉnh khác, nơi lấy mẫu best-of-k có một số (KL) phân kỳ từ chính sách cơ bản, và tạo ra các mẫu với phần thưởng kỳ vọng cao hơn. Trong các thí nghiệm của chúng tôi, chúng tôi thử k tăng từ 1 đến 191.

Có hai động lực chính để xem xét các thí nghiệm best-of-k. Thứ nhất, nó chứng minh rằng phương pháp tổng hợp phần thưởng áp dụng cho các phương pháp ngoài RLHF. Thứ hai, best-of-k không liên quan đến các tối ưu hóa phức tạp. Điều này cho phép chúng tôi tách rời tác động của việc có phần thưởng 'đúng' khỏi tác động lên việc giải quyết bài toán tối ưu hóa.

Đường cơ sở Trong thiết lập này, chúng tôi lấy phương pháp cơ sở là tổng có trọng số của các phần thưởng thô; tức là,

Rbaseline_∧:=wRhelp+ (1−w)Rharm. (5.1)

Chúng tôi quét qua w=0.1, ...,0.9, và báo cáo các đường cơ sở với hiệu suất tốt nhất trên các số liệu đánh giá của chúng tôi.

Phần thưởng Tham chiếu Chúng tôi kết hợp các phần thưởng đã biến đổi bằng phép cộng đơn giản. Việc trọng số phép cộng này không có động lực hoặc diễn giải rõ ràng. Thay vào đó, việc lựa chọn giá trị tham chiếu được sử dụng cho mỗi phần thưởng đóng vai trò tương tự. Một cách trực quan, nếu chúng ta đặt giá trị tham chiếu thấp hơn cho phần thưởng A thì phần thưởng trở nên dễ bão hòa hơn, và việc tối ưu hóa tập trung nhiều hơn vào phần thưởng B.

Đối với best-of-k, việc sử dụng w=0.5 đạt được hiệu suất tốt nhất trong phương pháp cơ sở. Theo đó, chúng tôi muốn lấy các phần thưởng tham chiếu cho tính hữu ích và vô hại có thể so sánh được. Để đạt được điều đó, chúng tôi sử dụng trung vị SFT (ước tính) cho cả tính hữu ích và vô hại. Lưu ý rằng đây là một tham chiếu thấp hơn so với được sử dụng cho các tối ưu hóa riêng lẻ. Điều này có ý nghĩa vì bài toán khó hơn của việc tối ưu hóa cả hai mục tiêu đồng thời ngăn mô hình khỏi việc bão hòa phần thưởng sớm.

Đối với RLHF, chúng tôi quan sát thấy rằng w=0.6 cho kết quả tốt nhất cho phương pháp tổng có trọng số. Sau đó, chúng tôi muốn đặt phần thưởng tham chiếu cho tính hữu ích cao hơn một chút. Chúng tôi sử dụng phân vị 75% SFT (ước tính).

Những lựa chọn này có thể không tối ưu, và có thể tối ưu hóa siêu tham số thêm có thể cải thiện kết quả.

--- TRANG 12 ---
(a)best-of-k(K=191)
(b)RLHF (KL≈14)
Hình 7: Phép cộng của phần thưởng được biến đổi log-sigmoid tương ứng tốt hơn với phép AND logic. Các chính sách đã căn chỉnh sử dụng phương pháp trước có phân phối phần thưởng cân bằng hơn (tập trung nơi hai giá trị phần thưởng tương tự), trong khi phương pháp sau dẫn đến phân phối phần thưởng không cân bằng hơn. Chúng tôi chọn các chính sách đã căn chỉnh bằng cách khớp K cho best-of-k và KL cho RLHF. Phần thưởng được trung tâm hóa bởi trung vị SFT cho cả tính hữu ích và vô hại. Chúng tôi hình dung phân phối chung bằng biểu đồ ước lượng mật độ kernel, với màu tối hơn cho thấy mật độ cao hơn.

Đánh giá Chúng tôi muốn một số liệu phản ánh Phép AND-Logic. Nghĩa là, liệu chúng ta đã cải thiện so với mô hình SFT trong cả tính hữu ích và vô hại hay không. Để đạt được điều đó, chúng tôi sẽ nói rằng một thế hệ thắng nếu nó có phần thưởng tính hữu ích cao hơn phần thưởng tính hữu ích trung vị của các mẫu SFT, và vô hại cao hơn vô hại trung vị của các mẫu SFT.

5.3.2 Kết quả Tổng hợp
Tổng hợp Đã biến đổi Cải thiện Căn chỉnh Việc cộng phần thưởng đã biến đổi nên có hai lợi thế so với phương pháp cơ sở. Thứ nhất, nó tương ứng với phép AND logic. Thứ hai, nó giữ lại những lợi ích của việc làm giảm tối ưu hóa quá mức phần thưởng, như trong trường hợp phần thưởng đơn. Cùng nhau, những điều này nên khiến việc căn chỉnh bằng phần thưởng kết hợp đã biến đổi vượt trội so với việc căn chỉnh bằng phần thưởng cơ sở.

Hình 6a và Hình 6b hiển thị cải thiện so với mô hình SFT để căn chỉnh bằng cả hai phần thưởng, cho cả best-of-k và RLHF. Như dự đoán, chúng tôi thấy cải thiện đáng kể từ phương pháp đã biến đổi.

--- TRANG 13 ---
Đáng chú ý là mô hình được căn chỉnh đã biến đổi vượt trội trong best-of-k, và cho KL thấp trong việc căn chỉnh RLHF. Trong những trường hợp này, có ít hack phần thưởng (mô hình đã căn chỉnh quá gần với mô hình SFT để có hành vi rất xấu). Do đó, chiến thắng ở đây rõ ràng chủ yếu do tác động phép AND logic. Trong chế độ KL cao, tác động hack phần thưởng xuất hiện và phần thưởng đã biến đổi vượt trội đáng kể so với đường cơ sở phần thưởng thô.

Phép cộng Đã biến đổi Tương ứng với Phép AND Logic Tiếp theo, chúng tôi kiểm tra trực tiếp liệu tác động phép AND logic có thể được chứng kiến trong LLM đã căn chỉnh hay không. Để đạt được điều đó, chúng tôi kiểm tra phân phối phần thưởng trong các chính sách đã căn chỉnh. Trong Hình 7, phương pháp tổng hợp của chúng tôi dẫn đến phân phối phần thưởng cân bằng hơn (hai giá trị phần thưởng thường tương tự), trong khi phương pháp cơ sở dẫn đến phân phối phần thưởng không cân bằng hơn (một phần thưởng cao hơn nhiều so với phần thưởng khác). Lưu ý rằng best-of-k (k=191) có phân phối phần thưởng khá khác so với RLHF tại KL≈14 (cái trước phản ánh sự phản tương quan của hai phần thưởng trong chính sách ban đầu; cái sau cập nhật chính sách để có giá trị phần thưởng tổng hợp cao). Nhưng phương pháp tổng hợp đã biến đổi có tác động nhất quán trong cả hai trường hợp.

6 Nghiên cứu Loại bỏ
Phép biến đổi LSC bao gồm hai thành phần: trung tâm hóa và biến đổi log-sigmoid. Việc trung tâm hóa được biết đến để giảm phương sai trong các cập nhật gradient chính sách, trong khi phép biến đổi log-sigmoid giới hạn các giá trị phần thưởng. Để hiểu đóng góp của mỗi thành phần, chúng tôi điều tra tác động của chúng riêng biệt. Như được hiển thị trong Hình 8, việc áp dụng biến đổi log-sigmoid hoặc trung tâm hóa riêng lẻ không cải thiện việc căn chỉnh.

Hình 8: Phép biến đổi Log-sigmoid hoặc trung tâm hóa riêng lẻ không cải thiện việc căn chỉnh. Thí nghiệm là cho nhiệm vụ tính hữu ích.

Việc chỉ trung tâm hóa có giúp không? Phương pháp này không thay đổi hiệu suất RLHF so với đường cơ sở. Mặc dù việc trung tâm hóa được biết đến để giảm phương sai cho cập nhật gradient chính sách, nó không thay đổi mục tiêu tối ưu hóa của mục tiêu RLHF được chính quy hóa KL, như được hiển thị trong (3.2).

Việc chỉ biến đổi log-sigmoid có giúp không? Điều này về cơ bản đặt một giá trị tham chiếu cố định ( rref(x) =0) cho tất cả đầu vào x. Điều này được phát hiện là làm tổn hại hiệu suất so với đường cơ sở. Giá trị tham chiếu quan trọng, như có thể thấy trong hình dạng của hàm tiện ích trong Hình 2(a): khi tham chiếu quá nhỏ, nó bão hòa tiện ích khi phần thưởng vẫn đang khớp dữ liệu kém; khi tham chiếu quá lớn, phép biến đổi phi tuyến không có tác dụng. Cụ thể hơn, có hai lý do có thể tại sao việc chỉ sử dụng phép biến đổi log-sigmoid không hoạt động:

--- TRANG 14 ---
1.Thứ nhất, r(x,y) không thể xác định: bất kỳ r(x,y) +c(x) nào cũng tạo ra cùng mục tiêu Bradley-Terry. Sau đó, bất kỳ phần thưởng tham chiếu cố định nào (độc lập với lời nhắc) về cơ bản không có ý nghĩa.

2.Thứ hai, nếu phần thưởng tham chiếu được đặt quá cao thì sẽ không có tác dụng (đối với r≪rref, phép biến đổi thực sự là một offset không đổi), và nếu phần thưởng tham chiếu được đặt quá thấp thì huấn luyện sẽ bão hòa quá sớm. Vấn đề với một mức tham chiếu cố định như 0 là không rõ điều này có nghĩa gì trong thực tế—nó quá cao hay quá thấp hay ổn? Ngược lại, việc chọn phần thưởng tham chiếu dựa trên các phản hồi được tạo (như trong bài báo) làm cho việc đặt một ngưỡng có ý nghĩa trở nên dễ dàng.

7 Thảo luận và Công việc Liên quan
Có một số lượng ngày càng tăng các công việc về việc giảm thiểu hack phần thưởng trong quy trình RLHF. Các kỹ thuật bao gồm các dạng tính trung bình mô hình phần thưởng [Eis+23; Ram +24; Zha +23], tối ưu hóa có ràng buộc [Mos+23], và chính quy hóa mô hình phần thưởng [She+23], thu thập sở thích con người lặp lại [Bai+22; Sti +20; Fan +22], hoặc giảm thiểu thiên vị dữ liệu [Sin+23]. Những phương pháp này bổ sung cho kỹ thuật biến đổi được đề xuất ở đây, và có thể được sử dụng kết hợp.

Đã có một số đề xuất để căn chỉnh các mô hình ngôn ngữ với nhiều mục tiêu. Phương pháp phổ biến nhất là kết hợp các mô hình phần thưởng riêng lẻ qua tổng có trọng số [ví dụ, Wu +23; Mos +23]. Moskovitz et al. [Mos+23] xác định một ngưỡng ràng buộc cho các phần thưởng riêng lẻ, và chính thức hóa một bài toán MDP có ràng buộc, nhưng việc xác định điểm ngưỡng dựa vào các truy vấn ground-truth. Bakker et al. [Bak+22] xem xét việc thích ứng các sơ đồ phúc lợi xã hội để tổng hợp các sở thích bất đồng của nhiều cá nhân, trái ngược với mục tiêu của chúng tôi là thỏa mãn tất cả các thuộc tính. Bai et al. [Bai+22] huấn luyện một mô hình phần thưởng duy nhất trên cả dữ liệu tính hữu ích và vô hại, nhưng phát hiện rằng điều này dẫn đến hack phần thưởng vô hại. Họ thay đổi tỷ lệ dữ liệu tính hữu ích trong huấn luyện để tránh điều này. Việc kết hợp bằng cách cộng phần thưởng đã biến đổi cho phép chúng tôi tránh những cân nhắc như vậy.

Kỹ thuật biến đổi trong bài báo này có liên quan đến bất kỳ chiến lược căn chỉnh nào tối đa hóa một cách rõ ràng một tiện ích kỳ vọng. Bây giờ cũng có các phương pháp căn chỉnh [ví dụ, Raf+23; Aza +23; Zha +22] sử dụng nhãn sở thích trực tiếp mà không cần khởi tạo rõ ràng các mô hình phần thưởng. Tuy nhiên, lưu ý rằng nếu chúng ta muốn căn chỉnh với nhiều thuộc tính, chúng ta vẫn cần tính toán xếp hạng từ một tổng hợp. Phương pháp đơn giản nhất là huấn luyện các mô hình phần thưởng cho các thuộc tính riêng lẻ, kết hợp những mô hình phần thưởng này, và sau đó xếp hạng các mẫu bằng phần thưởng kết hợp. Các thí nghiệm best-of-k của chúng tôi cho thấy rằng phép biến đổi có thể tạo ra lợi ích đáng kể ngay cả trong trường hợp này.

Cuối cùng, chúng tôi lưu ý rằng Azar et al. [Aza+23] cũng nhấn mạnh nhu cầu về một hàm tiện ích có giới hạn. Công việc ở đây có thể được xem, một phần, như một cách kết hợp hiểu biết này mà vẫn duy trì quy trình tối đa hóa tiện ích tiêu chuẩn. Đó là một câu hỏi thú vị cho công việc tương lai liệu việc sử dụng rõ ràng một phần thưởng (đã biến đổi) có cải thiện bước căn chỉnh so với việc chỉ sử dụng các cặp được xếp hạng hay không.

--- TRANG 15 ---
Lời cảm ơn
Công việc này được hỗ trợ bởi tài trợ ONR N00014-23-1-2591 và Open Philanthropy.

Tuyên bố Tác động
Bài báo này trình bày công việc có mục tiêu thúc đẩy lĩnh vực Học máy. Có nhiều hậu quả xã hội tiềm tàng của công việc chúng tôi, không có gì mà chúng tôi cảm thấy phải được nêu bật cụ thể ở đây.

Tài liệu tham khảo
[Ani+23]R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri,
E. Taropa, P . Bailey, Z. Chen, et al. "Palm 2 technical report". arXiv preprint
arXiv:2305.10403 (2023) (cit. trên tr. 8).
[Aza+23]M. G. Azar, M. Rowland, B. Piot, D. Guo, D. Calandriello, M. Valko, và R.
Munos. "A general theoretical paradigm to understand learning from human
preferences". arXiv preprint arXiv:2310.12036 (2023) (cit. trên tr. 14).
[Bai+22]Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S.
Fort, D. Ganguli, T . Henighan, et al. "Training a helpful and harmless as-
sistant with reinforcement learning from human feedback". arXiv preprint
arXiv:2204.05862 (2022) (cit. trên tr. 7, 9, 10, 14).
[Bak+22]M. A. Bakker, M. J. Chadwick, H. R. Sheahan, M. H. Tessler, L. Campbell-
Gillingham, J. Balaguer, N. McAleese, A. Glaese, J. Aslanides, M. M. Botvinick,
và C. Summerfield. Fine-tuning language models to find agreement among
humans with diverse preferences . 2022. arXiv: 2211.15006 [cs.LG] (cit. trên
tr. 14).
[BT52 ] R. A. Bradley và M. E. Terry. "Rank analysis of incomplete block designs: i.
the method of paired comparisons". Biometrika 3/4 (1952) (cit. trên tr. 2).
[Cos+23]T . Coste, U. Anwar, R. Kirk, và D. Krueger. "Reward model ensembles help
mitigate overoptimization". arXiv preprint arXiv:2310.02743 (2023) (cit. trên
tr. 9).
[Dub+23]Y. Dubois, X. Li, R. Taori, T . Zhang, I. Gulrajani, J. Ba, C. Guestrin, P . Liang, và
T . B. Hashimoto. "Alpacafarm: a simulation framework for methods that learn
from human feedback". arXiv preprint arXiv:2305.14387 (2023) (cit. trên tr. 9).
[Eis+23] J. Eisenstein, C. Nagpal, A. Agarwal, A. Beirami, A. D'Amour, D. Dvijotham,
A. Fisch, K. Heller, S. Pfohl, D. Ramachandran, et al. "Helping or herding?
reward model ensembles mitigate but do not eliminate reward hacking". arXiv
preprint arXiv:2312.09244 (2023) (cit. trên tr. 9–11, 14, 17).
[Fan+22]X. Fan, Y. Lyu, P . P . Liang, R. Salakhutdinov, và L. -P . Morency. "Nano: nested
human-in-the-loop reward learning for few-shot language model control".
arXiv preprint arXiv:2211.05750 (2022) (cit. trên tr. 14).
[GSH23 ] L. Gao, J. Schulman, và J. Hilton. "Scaling laws for reward model overop-
timization". In: International Conference on Machine Learning . PMLR. 2023
(cit. trên tr. 9, 11).
[Hou+23]Y. Hou, J. Zhang, Z. Lin, H. Lu, R. Xie, J. McAuley, và W . X. Zhao. "Large lan-
guage models are zero-shot rankers for recommender systems". arXiv preprint
arXiv:2305.08845 (2023) (cit. trên tr. 17).
[KPB22 ] T . Korbak, E. Perez, và C. L. Buckley. "Rl with kl penalties is better viewed as
bayesian inference". arXiv preprint arXiv:2205.11275 (2022) (cit. trên tr. 4).
[Mos+23]T . Moskovitz, A. K. Singh, D. Strouse, T . Sandholm, R. Salakhutdinov, A. D.
Dragan, và S. McAleer. Confronting reward model overoptimization with con-
strained rlhf . 2023. arXiv: 2310.04373 [cs.LG] (cit. trên tr. 14).

--- TRANG 16 ---
[Ouy+22]L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P . Mishkin, C. Zhang,
S. Agarwal, K. Slama, A. Ray, et al. "Training language models to follow
instructions with human feedback". Advances in Neural Information Processing
Systems (2022) (cit. trên tr. 1).
[Raf+23]R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, và C. Finn.
"Direct preference optimization: your language model is secretly a reward
model". arXiv preprint arXiv:2305.18290 (2023) (cit. trên tr. 9, 14).
[Raf+20]C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W . Li,
và P . J. Liu. "Exploring the limits of transfer learning with a unified text-to-text
transformer". The Journal of Machine Learning Research 1 (2020) (cit. trên tr. 8).
[Ram+24]A. Ramé, N. Vieillard, L. Hussenot, R. Dadashi, G. Cideron, O. Bachem, và
J. Ferret. "Warm: on the benefits of weight averaged reward models". arXiv
preprint arXiv:2401.12187 (2024) (cit. trên tr. 14).
[She+23]L. Shen, S. Chen, L. Song, L. Jin, B. Peng, H. Mi, D. Khashabi, và D. Yu.
"The trickle-down impact of reward (in-) consistency on rlhf". arXiv preprint
arXiv:2309.16155 (2023) (cit. trên tr. 14).
[Sin+23]P . Singhal, T . Goyal, J. Xu, và G. Durrett. "A long way to go: investigating
length correlations in rlhf". arXiv preprint arXiv:2310.03716 (2023) (cit. trên
tr. 9, 14).
[Sti+20] N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D.
Amodei, và P . F . Christiano. "Learning to summarize with human feedback".
Advances in Neural Information Processing Systems (2020) (cit. trên tr. 1, 14).
[Wu+23]Z. Wu, Y. Hu, W . Shi, N. Dziri, A. Suhr, P . Ammanabrolu, N. A. Smith, M.
Ostendorf, và H. Hajishirzi. Fine-grained human feedback gives better rewards
for language model training . 2023. arXiv: 2306.01693 [cs.CL] (cit. trên tr. 14).
[Zha+23]Y. Zhai, H. Zhang, Y. Lei, Y. Yu, K. Xu, D. Feng, B. Ding, và H. Wang. "Uncertainty-
penalized reinforcement learning from human feedback with diverse reward
lora ensembles". arXiv preprint arXiv:2401.00243 (2023) (cit. trên tr. 14).
[Zha+22]Y. Zhao, M. Khalman, R. Joshi, S. Narayan, M. Saleh, và P . J. Liu. "Calibrating
sequence likelihood improves conditional language generation". arXiv preprint
arXiv:2210.00045 (2022) (cit. trên tr. 14).
[Zie+19] D. M. Ziegler, N. Stiennon, J. Wu, T . B. Brown, A. Radford, D. Amodei, P . Chris-
tiano, và G. Irving. "Fine-tuning language models from human preferences".
arXiv preprint arXiv:1909.08593 (2019) (cit. trên tr. 1).

--- TRANG 17 ---
A Chi tiết Thí nghiệm Bổ sung
A.1 Triển khai cho dự đoán giá trị tham chiếu
Mục tiêu là phát triển các mô hình có thể dự đoán các phân vị khác nhau của phần thưởng được lấy mẫu cho các lời nhắc khác nhau. Thay vì xây dựng các mô hình riêng biệt cho mỗi phân vị, chúng tôi giả định rằng phân phối phần thưởng cho mỗi lời nhắc tuân theo phân phối Gaussian. Dưới giả định này, chúng tôi xây dựng hai mô hình: Mô hình Dự đoán Trung bình (rmean) và Mô hình Dự đoán Độ lệch Chuẩn (rstd) để dự đoán trung bình và độ lệch chuẩn của phân phối phần thưởng cho mỗi lời nhắc x.

Để ước tính một phân vị cụ thể cho một lời nhắc, công thức được sử dụng là:
rmean(x) +s×rstd(x)
trong đó s là một hệ số tỷ lệ tương ứng với phân vị mong muốn (ví dụ, sử dụng s=0.7 xấp xỉ cho chúng ta phân vị 75%).

Dữ liệu huấn luyện được thu thập bằng cách tạo 64 phản hồi cho mỗi lời nhắc bằng mô hình chính sách SFT. Những phản hồi này được chấm điểm để tính toán trung bình mẫu và độ lệch chuẩn của phân phối phần thưởng cho mỗi lời nhắc, sau đó được sử dụng làm biến phản hồi trong việc huấn luyện các mô hình rmean và rstd.

A.2 Chi tiết Đánh giá PALM-2
Chúng tôi đánh giá tỷ lệ thắng với prompting zero-shot. Đối với mỗi lời nhắc, chúng tôi lấy mẫu (yπ,ysft) từ chính sách đã căn chỉnh và SFT, sau đó hỏi mô hình PALM-2 cái nào hữu ích hơn /vô hại hơn. Mẫu prompt là Hình 9.

Để chống lại thiên vị vị trí [Hou+23], chúng tôi chạy PALM-2 trên hai thứ tự có thể có (yπ,ysft) và (ysft,yπ), lấy mẫu N=8 đầu ra cho mỗi thứ tự và xác định người thắng bằng bỏ phiếu đa số.

A.3 Heuristics cho khám phá phím tắt
Đối với nhiệm vụ tính hữu ích, chúng tôi theo cùng thực hành trong [Eis+23] và tìm các phản hồi dưới định dạng danh sách.

Đối với nhiệm vụ vô hại, chúng tôi thấy chính sách đã căn chỉnh bắt đầu chỉ tạo ra các thế hệ tương tự cho các câu hỏi khác nhau khi KL trở nên lớn hơn. Để tìm phím tắt, chúng tôi phân tích các thế hệ của chính sách đã căn chỉnh (sử dụng phần thưởng thô làm tiện ích) tại KL≈14. Sau đó chúng tôi thực hiện mô hình hóa chủ đề và xác định một chủ đề chi phối được đặc trưng bởi chủ đề lặp lại "bạn nên nói chuyện với [nghề nghiệp]." Từ 50 từ thường gặp nhất trong chủ đề đó, chúng tôi tìm thấy tất cả các từ liên quan đến nghề nghiệp: nhà trị liệu, chuyên gia, bác sĩ, nhà tâm lý học, cố vấn, luật sư, cảnh sát. Sau đó chúng tôi tìm tất cả các phản hồi chứa từ liên quan đến nghề nghiệp.

A.4 Chi tiết huấn luyện RLHF
Chúng tôi sử dụng Tối ưu hóa Chính sách Gần (PPO) để thực hiện căn chỉnh RLHF. Các siêu tham số cụ thể trong bảng 1 Chúng tôi quét qua γ để có được các chính sách đã căn chỉnh với các giá trị KL khác nhau. Vì chúng tôi muốn khớp các chính sách (đã hội tụ) theo giá trị KL của chúng, chúng tôi tìm một số heuristic để dự đoán KL đã hội tụ cho một γ đã chọn (vì vậy chúng có thể không trông giống như chuỗi tuyến tính hoặc hình học thông thường). Cụ thể hơn, chúng tôi sử dụng tham số hóa của α=γ/(1+γ), và các giá trị được sử dụng trong bảng 2. Lưu ý rằng để có được cùng giá trị KL, chúng tôi sử dụng chính quy hóa KL nhỏ hơn cho biến đổi phần thưởng, so với sử dụng phần thưởng thô. Điều này trực quan vì phép biến đổi log-sigmoid ngăn việc sử dụng ngân sách KL một khi phần thưởng bão hòa.

--- TRANG 18 ---
Hình 9: Prompt cho Đánh giá PALM-2

--- TRANG 19 ---
Bảng 1: Siêu tham số cho RLHF.
Tham số Giá trị
Tốc độ học chính sách 5 ·10−6
Tốc độ học giá trị 4 ·10−5
Lịch học Hằng số (khởi động tuyến tính)
Bước huấn luyện 20000
Bước khởi động 2000
Kích thước batch 32
Độ dài đầu vào 1024
Độ dài đầu ra 256

Bảng 2: Alpha cho Các Nhiệm vụ-Phương pháp Khác nhau.
Nhiệm vụ-Phương pháp α:=γ/(1+γ)
Tính hữu ích (u =phần thưởng) [0.081, 0.086, 0.092, 0.1, 0.114, 0.132, 0.161, 0.222 ]
Tính hữu ích (u =phần thưởng đã biến đổi) [0.02, 0.023, 0.028, 0.032, 0.039, 0.053, 0.075, 0.123 ]
Vô hại (u =phần thưởng) [0.169, 0.182, 0.196, 0.218, 0.248, 0.32 ]
Vô hại (u =phần thưởng đã biến đổi) [0.032, 0.041, 0.052, 0.079, 0.126, 0.222 ]
H+H (0.5 rhelp+0.5rharmless) [0.078, 0.08, 0.084, 0.088, 0.094, 0.102, 0.116, 0.134, 0.168 ]
H+H (0.6 rhelp+0.4rharmless) [0.068, 0.071, 0.074, 0.077, 0.082, 0.088, 0.1, 0.123, 0.163 ]
H+H (0.7 rhelp+0.3rharmless) [0.057, 0.06, 0.065, 0.07, 0.075, 0.084, 0.1, 0.126, 0.173 ]
H+H (tổng phần thưởng đã biến đổi) [0.014, 0.0145, 0.015, 0.017, 0.018, 0.02, 0.023, 0.026, 0.031, 0.034, 0.04, 0.048, 0.066, 0.096 ]

B Kết quả Thí nghiệm Thêm
Trong Hình 4 chúng tôi chọn một cặp chính sách đã căn chỉnh khớp trên giá trị KL để cho thấy rằng biến đổi phần thưởng dẫn đến cải thiện đồng đều hơn so với sử dụng phần thưởng thô. Trong Hình 10 chúng tôi hiển thị trực quan tối ưu hóa quá mức phần thưởng (trong cùng các chính sách đã căn chỉnh), và cách biến đổi phần thưởng làm giảm nó. Việc giảm thiểu rõ ràng nhất đối với các phản hồi có giá trị phần thưởng lớn hơn. Cũng quan trọng cần lưu ý rằng tổng hợp phần thưởng làm trầm trọng thêm hack phần thưởng với tổng có trọng số của phần thưởng thô: mô hình chính sách được khuyến khích tạo ra các phản hồi có điểm vô hại rất cao (mặc dù có trọng số nhỏ hơn cho vô hại); để duy trì điểm tốt trong tính hữu ích, mô hình chính sách hack mô hình phần thưởng tính hữu ích nhiều hơn nữa (so sánh Hình 10a với Hình 10c). Việc sử dụng tổng phần thưởng đã biến đổi làm giảm điều này bằng hai cơ chế.

Trong Hình 11 chúng tôi cho thấy biến đổi phần thưởng dẫn đến phân phối phần thưởng tập trung hơn so với sử dụng phần thưởng thô, cùng xu hướng trên các giá trị KL khác nhau.

Trong Hình 3 chúng tôi báo cáo tỷ lệ thắng so với mẫu ngẫu nhiên dưới PALM-2, và so với phân vị 85% SFT cho tính hữu ích và 95% cho vô hại. Trong Hình 12, chúng tôi báo cáo các đánh giá bổ sung.

--- TRANG 20 ---
(a)Hữu ích (phần thưởng đơn): KL ≈18
 (b)Vô hại (phần thưởng đơn): KL ≈7
(c)Hữu ích (phần thưởng tổng hợp): KL ≈14
 (d)Vô hại (phần thưởng tổng hợp): KL ≈14
Hình 10: Biến đổi phần thưởng giảm thiểu tối ưu hóa quá mức phần thưởng, đặc biệt đối với các phản hồi có giá trị phần thưởng lớn hơn, và trong tổng hợp phần thưởng nơi hack phần thưởng nghiêm trọng hơn. Chúng tôi so sánh các mẫu tối ưu hóa quá mức phần thưởng trong các chính sách được căn chỉnh với phần thưởng thô và phần thưởng đã biến đổi, trong thiết lập phần thưởng đơn (Hình 10a, Hình 10b) và tổng hợp phần thưởng (Hình 10c, Hình 10d). Việc lựa chọn các chính sách đã căn chỉnh và trung tâm hóa điểm số giống như trong Hình 4. Đối với mỗi biểu đồ, chúng tôi sắp xếp các điểm số được trung tâm hóa từ mô hình phần thưởng T5-base thành 20 bin có độ rộng bằng nhau. Đối với mỗi bin có hơn 10 điểm dữ liệu: trong trục x chúng tôi hiển thị trung vị khoảng; trong trục y, chúng tôi hình dung phân vị 25%, 50% và 75% của các điểm số được trung tâm hóa tương ứng từ T5-xxl.

(a)Tính hữu ích
 (b)Vô hại
Hình 11: Biến đổi Phần thưởng dẫn đến cải thiện phần thưởng đồng đều hơn so với đường cơ sở. Các giá trị phần thưởng được trung tâm hóa bởi phần thưởng trung vị SFT. Điều này bổ sung cho kết quả trong Hình 4 để so sánh trên các giá trị KL.

--- TRANG 21 ---
(a)Mẫu Ngẫu nhiên SFT
 (b)Phân vị 50% SFT
 (c)Phân vị 75% SFT
 (d)Phân vị 95% SFT
(e)Tính hữu ích
(f)Mẫu Ngẫu nhiên SFT
 (g)Phân vị 50% SFT
 (h)Phân vị 75% SFT
 (i)Phân vị 85% SFT
(j)Vô hại
Hình 12: Phần thưởng đã biến đổi có được sự đánh đổi KL và tỷ lệ thắng tốt hơn. Đây là tỷ lệ thắng so với mẫu ngẫu nhiên SFT, và phân vị thứ q của phần thưởng của các mẫu SFT. Xem chi tiết trong Hình 3.
