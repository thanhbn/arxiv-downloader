# 2310.02456.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2310.02456.pdf
# File size: 3723826 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
LEARNING OPTIMAL ADVANTAGE FROM PREFERENCES AND
MISTAKING IT FOR REWARD
A P REPRINT
W. Bradley Knox *1,2, Stephane Hatgis-Kessell1, Sigurdur Orn Adalgeirsson2, Serena Booth3, Anca
Dragan4, Peter Stone1,5, and Scott Niekum6
1University of Texas at Austin
2Google Research
3MIT CSAIL
4UC Berkeley
5Sony AI
6University of Massachusetts Amherst
ABSTRACT
We consider algorithms for learning reward func-
tions from human preferences over pairs of tra-
jectory segments, as used in reinforcement learn-
ing from human feedback (RLHF). Most recent
work assumes that human preferences are gener-
ated based only upon the reward accrued within
those segments, or their partial return . Recent
work casts doubt on the validity of this assumption,
proposing an alternative preference model based
upon regret . We investigate the consequences of
assuming preferences are based upon partial return
when they actually arise from regret. We argue that
the learned function is an approximation of the op-
timal advantage function, bA∗
r,nota reward func-
tion. We find that if a specific pitfall is addressed,
this incorrect assumption is not particularly harm-
ful, resulting in a highly shaped reward function.
Nonetheless, this incorrect usage of bA∗
ris less de-
sirable than the appropriate and simpler approach
of greedy maximization of bA∗
r. From the perspec-
tive of the regret preference model, we also pro-
vide a clearer interpretation of fine tuning contem-
porary large language models with RLHF. This pa-
per overall provides insight regarding why learn-
ing under the partial return preference model tends
to work so well in practice, despite it conforming
poorly to how humans give preferences.
∗Correspondence to:
bradknox@cs.utexas.edu1 Introduction
When learning from human preferences (in RLHF), the
dominant model assumes that human preferences are de-
termined only by each segment’s accumulated reward, or
partial return . Knox et al. [2022] argued that the partial
return preference model has fundamental flaws that are
removed or ameliorated by instead assuming that human
preferences are determined by the optimal advantage of
each segment, which is a measure of deviation from opti-
mal decision-making and is equivalent to the negated re-
gret. This past work argues for the superiority of the regret
preference model (1) by intuition, regarding how humans
are likely to give preferences (e.g., see Fig. 2); (2) by the-
ory, showing that regret-based preferences have a desir-
able identifiability property that preferences from partial
return lack; (3) by descriptive analysis, showing that the
likelihood of a human preferences dataset is higher under
the regret preference model than under the partial return
preference model; and (4) by empirical analysis, showing
that with both human and synthetic preferences, the regret
model requires fewer preference labels. Section 2 of this
paper provides details on the general problem setting and
on these two models.
In this paper, we explore the consequences of using algo-
rithms that are designed with the assumption that prefer-
ences are determined by partial return when these prefer-
ences are instead determined by regret. We show in Sec-
tion 3 that these algorithms learn an approximation of the
optimal advantage function ,A∗
r, not of the reward func-
tion, as presumed in many prior works. We then study the
implications of this mistaken interpretation. When inter-
preted as reward, the exact optimal advantage is highly
shaped and preserves the set of optimal policies, which
enables partial-return-based algorithms to perform well.
However, the learned approximation of the optimal advan-arXiv:2310.02456v1  [cs.LG]  3 Oct 2023

--- PAGE 2 ---
Learning Optimal Advantage from Preferences and Mistaking it for Reward A P REPRINT
3 algorithms
learning glearning glearning with regret assumptionDataset created by reward function___andpolicy improvementpolicy improvementnothingAdditional step to create policy (other than greedy action selection)Output of learning from preferencesAlgorithm for learning from preferences
partial return preference modelregretpreference modelregret preference model
Figure 1: Three algorithms that are justified by their assumed
preference model. The top algorithm was popularized by Chris-
tiano et al. [2017] and the middle algorithm was proposed by
Knox et al. [2022]. The third algorithm is described in Sec-
tion 3.2. The reward function ˆr, optimal advantage function bA∗
r,
and optimal policy ˆπ∗
rare approximations of the true versions
of these functions. The function gis defined generally in Equa-
tion 6 to allow it to represent including A∗
rorr. This paper fo-
cuses on what occurs when the solid box represents the actual
algorithm for learning gbut the partial return preference model
is assumed, causing bA∗
rto be used as if it is the reward in the
dashed box.
tage function, bA∗
r, will have errors. We characterize when
and how such errors will affect the set of optimal policies
with respect to this mistaken reward, and we uncover a
method for reducing a harmful type of error. We conclude
that this incorrect usage of bA∗
rstill permits decent perfor-
mance under certain conditions, though it is less desirable
than the appropriate and simpler approach of greedy max-
imization of bA∗
r.
We then show in Section 4 that recent algorithms
used to fine-tune state-of-the-art language models Chat-
GPT [OpenAI 2022], Sparrow [Glaese et al. 2022], and
others [Ziegler et al. 2019, Ouyang et al. 2022, Bai et al.
2022, Touvron et al. 2023] can be viewed as an instance of
learning an optimal advantage function and inadvertently
treating it as one. In multi-turn (i.e., sequential) settings
such as that of ChatGPT, Sparrow, and research by Bai
et al. [2022], this alternative framing allows the removal
of a problematic assumption of these algorithms: that a re-
ward function learned for a sequential task is instead used
in a bandit setting, effectively setting the discount factor
γto0.
2 Preliminaries: Preference models for
learning reward functions
A Markov decision process (MDP) is specified by a tu-
ple (S,A,T,γ,D0,r).SandAare the sets of possible
states and actions, respectively. T:S×A→p(·|s, a)is
a transition function; γis the discount factor; and D0is
the distribution of start states. Unless stated otherwise, we
assume tasks are undiscounted ( γ= 1) and have terminal
states, after which only 0reward can be received. ris a
reward function, r:S×A×S→R, where rtis a func-tion of st,at, and st+1at time t. An MDP \ris an MDP
without a reward function.
Throughout this paper, rrefers to the ground-truth reward
function for some MDP; ˆrrefers to a learned approxima-
tion of r; and ˜rrefers to any reward function (including r
orˆr). A policy ( π:S×A→[0,1]) specifies the probabil-
ity of an action given a state. Qπ
˜randVπ
˜rrefer respectively
to the state-action value function and state value function
for a policy, π, under ˜r, and are defined as follows.
Vπ
˜r(s)≜Eπ[∞X
t=0˜r(st, at, st+1)|s0=s]
Qπ
˜r(s, a)≜Eπ[˜r(s, a, s′) +Vπ
˜r(s′)]
An optimal policy π∗
˜ris any policy where Vπ∗
˜r
˜r(s)≥
Vπ
˜r(s)at every state sfor every policy π. We write
shorthand for Qπ∗
˜r
˜randVπ∗
˜r
˜rasQ∗
˜randV∗
˜r, respectively.
The optimal advantage function is defined as A∗
˜r(s, a)≜
Q∗
˜r(s, a)−V∗
˜r(s); this measures how much an action re-
duces expected return relative to following an optimal pol-
icy.
Throughout this paper, when the preferences are not
human-generated, the ground-truth reward function ris
used to algorithmically generate preferences. ris hidden
during reward learning and is used to evaluate the perfor-
mance of optimal policies under a learned ˆr.
2.1 Reward learning from pairwise preferences
A reward function is commonly learned by minimizing
the cross-entropy loss—i.e., maximizing the likelihood—
of observed human preference labels [Christiano et al.
2017, Ibarz et al. 2018, Wang et al. 2022, Bıyık et al. 2021,
Sadigh et al. 2017, Lee et al. 2021a,b, Ziegler et al. 2019,
Ouyang et al. 2022, Bai et al. 2022, Glaese et al. 2022,
OpenAI 2022, Touvron et al. 2023].
Segments Letσdenote a segment starting at state sσ
0.
Its length |σ|is the number of transitions within the seg-
ment. A segment includes |σ|+ 1states and |σ|actions:
(sσ
0, aσ
0, sσ
1, aσ
1, ..., sσ
|σ|). In this problem setting, segments
lack any reward information. As shorthand, we define
σt≜(sσ
t, aσ
t, sσ
t+1). A segment σisoptimal with respect
to˜rif, for every i∈ {1, ...,|σ|-1},A∗
˜r(sσ
i, aσ
i) = 0 . A
segment that is not optimal is suboptimal . Given some
˜rand a segment σ, where ˜rσ
t≜˜r(sσ
t, aσ
t, sσ
t+1), the
undiscounted partial return of a segment σisP|σ|−1
t=0˜rσ
t,
which we denote in shorthand as Σσ˜r.
Preference datasets Each preference over a pair of
segments creates a sample (σ1, σ2, µ)in a preference
dataset D≻. Vector µ=⟨µ1, µ2⟩represents the pref-
erence; specifically, if σ1is preferred over σ2, denoted
σ1≻σ2,µ=⟨1,0⟩.µis⟨0,1⟩ifσ1≺σ2and is
⟨0.5,0.5⟩forσ1∼σ2(no preference). For a sample
(σ1, σ2, µ), we assume that the two segments have equal
lengths (i.e., |σ1|=|σ2|).
2

--- PAGE 3 ---
Learning Optimal Advantage from Preferences and Mistaking it for Reward A P REPRINT
Loss function When learning a reward function from a
preference dataset, D≻, preference labels are typically as-
sumed to be generated by a preference model Pbased on
an unobservable ground-truth reward function r.We learn
ˆr, an approximation of r, by minimizing cross-entropy
loss:
loss(ˆr, D≻) =
−X
(σ1,σ2,µ)∈D≻µ1logP(σ1≻σ2|ˆr) +µ2logP(σ1≺σ2|ˆr)(1)
Ifσ1≻σ2, the sample’s likelihood is P(σ1≻σ2|ˆr)
and its loss is therefore −logP(σ1≻σ2|ˆr). Ifσ1≺σ2,
its likelihood is 1−P(σ1≻σ2|ˆr). This loss is under-
specified until the preference model P(σ1≻σ2|ˆr) is de-
fined. Algorithms in this paper for learning approxima-
tions of rorA∗
rfrom preferences can be summarized sim-
ply as “minimize Equation 1”.
Preference models A preference model determines the
probability of one trajectory segment being preferred over
another, P(σ1≻σ2|˜r). Preference models can be used to
model preferences provided by humans or other systems,
or to generate synthetic preferences.
2.2 Preference models: partial return and regret
Partial return The dominant preference model (e.g.,
Christiano et al. [2017]) assumes human preferences are
generated by a Boltzmann distribution over the two seg-
ments’ partial returns, expressed here as a logistic func-
tion:2
PΣr(σ1≻σ2|˜r) =logistic
Σσ1˜r−Σσ2˜r
.(2)
Regret Knox et al. [2022] introduced an alternative
human preference model. This regret-based model as-
sumes that preferences are based on segments’ deviations
from optimal decision-making: the regret of each transi-
tion in a segment. We first focus on segments with deter-
ministic transitions. For a single transition (st, at, st+1),
regret d(σt|˜r)≜V∗
˜r(sσ
t)−[˜rt+V∗
˜r(sσ
t+1)]. For a full
segment,
regret d(σ|˜r)≜|σ|−1X
t=0regret d(σt|˜r)
=V∗
˜r(sσ
0)−(Σσ˜r+V∗
˜r(sσ
|σ|)),(3)
with the right-hand expression arising from cancelling out
intermediate state values. Therefore, deterministic regret
measures how much the segment reduces expected return
fromV∗
˜r(sσ
0). An optimal segment σ∗always has 0 regret,
and a suboptimal segment σ¬∗always has positive regret.
Stochastic state transitions, however, can result in
regret d(σ∗|ˆr)> regret d(σ¬∗|˜r), losing the property
above. To retain it, we note that the effect on ex-
pected return of transition stochasticity from a transition
2Unless otherwise stated, we ignore the temperature because
scaling reward has the same effect as changing the temperature.(st, at, st+1)is[˜rt+V∗
˜r(st+1)]−Q∗
˜r(st, at)and add this
expression once per transition to get regret (σ), removing
the subscript dthat refers to determinism. The regret for
a single transition becomes regret (σt|˜r) =[V∗
˜r(sσ
t)−
[˜rt+V∗
˜r(sσ
t+1)]]+[[˜rt+V∗
˜r(sσ
t+1)]−Q∗
˜r(sσ
t, aσ
t)]=
V∗
˜r(sσ
t)−Q∗
˜r(sσ
t, aσ
t) =−A∗
˜r(sσ
t, aσ
t). Regret for a full
segment is:
regret (σ|˜r) =|σ|−1X
t=0regret (σt|˜r)
=|σ|−1X
t=0h
V∗
˜r(sσ
t)−Q∗
˜r(sσ
t, aσ
t)i
=|σ|−1X
t=0−A∗
˜r(sσ
t, aσ
t).(4)
The regret preference model is the Boltzmann distribution
over the sum of optimal advantages, or the negated regret:
Pregret (σ1≻σ2|˜r)
≜logistic|σ1|−1X
t=0A∗
˜r(σ1,t)−|σ2|−1X
t=0A∗
˜r(σ2,t)
=logistic
regret (σ2|˜r)−regret (σ1|˜r)
.
(5)
(Notationally, A∗
˜r(σt) = A∗
˜r(sσ
t, aσ
t).) Lastly, if two
segments have deterministic transitions, end in termi-
nal states, and have the same starting state, this regret
model reduces to the partial return model: Pregret (·|˜r) =
PΣr(·|˜r).
Intuitively, the partial return preference model always as-
sumes preferences are based upon outcomes while the re-
gret model is able to account for preferences based upon
outcomes (Eq. 3) and preferences over decisions (Eq. 4).
Suboptimal segmentOptimal segment
Equal partial return Higher regretEqual partial return Lower regret
Figure 2: Two segments in an undiscounted task with −1re-
ward each time step. The partial return of both segments with
respect to the true reward function is −2. The regret of the left
segment is 4. The right segment is optimal and therefore has a
regret of 0. The regret preference model is more likely to pre-
fer the right segment—as we suspect our human readers are—
whereas the partial return preference model is equally likely to
prefer each segment.
Knox et al. [2022] showed the regret both has desirable
theoretical properties (i.e., it is identifiable where partial
3

--- PAGE 4 ---
Learning Optimal Advantage from Preferences and Mistaking it for Reward A P REPRINT
return is not) and is a better model of true human prefer-
ences. Since regret better models true human preferences,
and since many recent works use true human preferences
but assume them to be generated according to partial re-
turn, we ask: what are the consequences of misinterpreting
the optimal advantage function as reward?
3 Learning optimal advantage from
preferences and using it as reward
We ask: what is actually learned when preferences are as-
sumed to arise from partial return but actually come from
regret (Equation 2), and what implications does that have?
Our results can be reproduced via our code repository, at
github.com/Stephanehk/Learning-OA-From-Prefs.
3.1 Learning the optimal advantage function
To start, let us unify the two preference models from Sec-
tion 2.2 into a single general preference model.
Pg(σ1≻σ2|˜r)≜logistic|σ1|−1X
t=0g(σ1,t)−|σ2|−1X
t=0g(σ2,t)
(6)
In the above unification, the segment statistic in the pref-
erence model is expressed as a sum of some function
gover each transition in the segment:P|σ|−1
t=0g(σt) =P|σ|−1
t=0g(sσ
t, aσ
t, sσ
t+1). When preferences are generated
according to partial return, g(σt) = ˜r(sσ
t, aσ
t, sσ
t+1), and
the reward function ˜ris learned via Equation 1.
When preferences are instead generated according to re-
gret,g(σt) =A∗
˜r(σt) =A∗
˜r(sσ
t, aσ
t)and the parameters
of this optimal advantage function can be learned directly,
also via Equation 1. bA∗
rcan be learned and then acted
upon greedily, via argmax abA∗
r(s, a), an algorithm we call
greedy cA∗
r(bottom algorithm of Fig. 1). Notably, this al-
gorithm does not require the additional step of policy im-
provement and instead uses bA∗
rdirectly. No reward func-
tion is explicitly represented or learned, though we still
assume that preferences were generated by regret under a
hidden reward function r.
The remainder of this section considers first the conse-
quences of using the error-free A∗
ras a reward func-
tion:rA∗
r=A∗
r. We call this mistaken approach
greedy Q∗rA∗r. We then consider the consequences of us-
ing the approximation bA∗
ras a reward function, rcA∗r=bA∗
r,
which we refer to as greedy Q∗rcA∗r. The following investi-
gation is an attempt to answer why learning while assum-
ing the partial return preference model tends to work so
well in practice, despite its poor fit as a descriptive model
of human preference.3.2 Using A∗
ras a reward function
Under the assumption of regret-based preferences, learn-
ing a reward function with the partial return preference
model effectively uses an approximation of A∗
ras a re-
ward function, ˆr=bA∗
r. Let us first assume perfect infer-
ence of A∗
r(i.e., that bA∗
r=A∗
r), and consider the conse-
quences. We will refer to the non-approximate versions of
greedy bA∗
randrcA∗rasgreedy A∗
randrA∗
r.
Optimal policies are preserved. Using A∗
ras a reward
function preserves the set of optimal policies. To prove
this statement, we first prove a more general theorem.
For˜r, an arbitrary reward function, max aA∗
˜r(·, a) = 0 by
definition. Let the set of optimal policies with respect to ˜r
be denoted Π∗
˜r.
Theorem 3.1 (Greedy action is optimal when the maxi-
mum reward in every state is 0.) .
Π∗
˜r={π:∀s,∀a[π(a|s)>0⇔a∈argmaxa˜r(s, a)]}
if max a˜r(·, a) = 0 .
Theorem 3.1 is proven in Appendix A. The sketch of the
proof is that if the maximum reward in every state is 0,
then the best possible return from every state is 0. There-
fore, V∗
˜r(·) = 0 , making ∀(s, a)∈S×A, Q∗
˜r(s, a) =
˜r(s, a) +γEs′[V∗
˜r(s)] = ˜r(s, a).
We now return to our specific case, proven in Appendix B.
Corollary 3.1 (Policy invariance of rA∗r).
LetrA∗r≜A∗
r. If max aA∗
r(·, a) = 0 ,Π∗rA∗r= Π∗
r.
An underspecification issue is resolved. As we discuss
in Section 4, when segment lengths are 1, the partial re-
turn preference model ignores the discount factor γ, mak-
ing its choice arbitrary despite it often affecting the set
of optimal policies. With rA∗r, however, the lack of γin
Corollary 3.1 establishes γdoes not affect the set of opti-
mal policies. To give intuition, we apply the intermediate
result within the proof of Theorem 3.1 that V∗
˜r(·) = 0 to
the specific case of Corollary 3.1, we see that V∗
rA∗r(·) = 0 .
Therefore, Q∗
rA∗r(s, a) =rA∗r(s, a) +γEs′[0], making γ
have no impact on Q∗
rA∗r(s, a)and therefore on on Π∗
r.
Reward is highly shaped. In Ng et al. [1999]’s seminal
research on potential-based reward shaping , they high-
light ϕ(s) =V∗
r(s)as a particularly desirable potential
function. Algebraic manipulation reveals that the MDP
that results from this ϕactually uses as a reward func-
tionrA∗r≜A∗
r. See Appendix C for the derivation. Ng et
al. also note that that it causes V∗
rA∗r(·) = 0 and therefore
results in “a particularly easy value function to learn; ... all
that would remain to be done would be to learn the non-
zero Q-values.” We characterize this approach as highly
shaped because the information required to act optimally
is in the agent’s immediate reward.
4

--- PAGE 5 ---
Learning Optimal Advantage from Preferences and Mistaking it for Reward A P REPRINT
Policy improvement wastes computation and environ-
ment sampling. When using A∗
ras a reward func-
tion, no policy improvement is needed: setting π(s) =
argmax a[A∗
r(s, a)]provides an optimal policy.
3.3 Using the learned bA∗
ras a reward function
A caveat to the preceding analysis is that the algorithm
does not necessarily learn A∗
r. Rather it learns its approx-
imation, bA∗
r. We investigate the effects of the approxima-
tion error of bA∗
r. We find that this error only induces a
difference in performance from that of greedy bA∗
rwhen
max abA∗
r(s, a)̸= 0 in at least one state s, and the conse-
quence of that error is dependent on the maximum par-
tial return of all loops —segments that start and end in the
same state—within the MDP.
For the empirical results below, we build upon the ex-
perimental setting of Knox et al. [2022], including both
for learning and for randomly generating MDPs. Hyper-
parameters and other experimental settings are identical
except where noted. All preferences are synthetically gen-
erated by the regret preference model.
If the maximum value of bA∗
rin every state is 0, behav-
ior is identical between greedy Q∗rcA∗randgreedy bA∗
r.
From Theorem 3.1, the following trivially holds for a
learned approximation bA∗
r.
Corollary 3.2. LetrcA∗r≜bA∗
r. If max abA∗
r(·, a) = 0 ,
then Π∗rcA∗r={π:∀s,∀a[π(a|s)>0⇔a∈
argmaxabA∗
r(s, a)]}.
Therefore, if max abA∗
r(·, a) = 0 , then a policy
from greedy bA∗
ris identical to an optimal policy for
greedy Q∗rcA∗r, assuming ties are resolved identically. The
actual policy from greedy Q∗rcA∗rwill also be identical
unless limitations of the policy improvement algorithm
cause it to not find a policy in Π∗rcA∗rin this highly shaped
setting with the reward function also in hand, not requiring
experience to query. However, max abA∗
r(·, a) = 0 is not
guaranteed for an approximation of A∗
r, which we con-
sider later in this section.
We conduct an empirical test of the assertion above by
adjusting bA∗
rto have the property max abA∗
r(·, a) = 0
by shifting bA∗
rby a state-dependent constant: for all
(s, a),rcA∗r-shifted(s, a)≜bA∗
r(s, a)−max a′bA∗
r(s, a′). Note
thatargmax arcA∗r-shifted(s, a) = argmax abA∗
r(s, a). In 90
small gridworld MDPs, we observe no difference between
greedy bA∗
randgreedy Q∗rcA∗rwithrcA∗r-shifted (see Figure 9).
However, cost is generally incurred from suboptimal be-
havior and environment sampling while a policy improve-
ment algorithm learns this approximately optimal pol-
icy, unless the policy improvement algorithm uses the in-
hand rcA∗r-shifted without environment sampling and makes
use of knowledge that the state value is 0 in every state,
which together allow it to simply define optimal behavior
asargmax aQrcA∗r-shifted(s, a) =argmax arcA∗r-shifted(s, a) =
argmax abA∗
r(s, a), which is greedy bA∗
r.
Figure 3: Performance when noiselessly generated preference
datasets do and do not include segments with transitions from
absorbing state. Results are across 30 randomly generated grid-
world MDPs with tabular representations of the bA∗
r, where seg-
ments of length 3 are chosen by uniformly randomly choosing
a start state and 3 its actions. When transitions from absorbing
states are not included, any segment that terminates before its
final transition is rejected and then resampled. For greedy bA∗
r
(in red) Wilcoxon paired signed-rank tests reveal that including
transitions from absorbing state results in significantly higher
performance for all training set sizes but the smallest, 300, with
p < 0.0007 . No significant difference in performance is de-
tected for greedy Q∗rcA∗rwith or without terminating transitions
except at 30,000 preferences with a more modest p= 0.04.
Appendix G contains the plot for stochastically generated pref-
erences (Figure 11), which contains similar results.
Including segments with transitions from absorbing
state encourages max abA∗
r(·,a) = 0 .If an algorithm
designer is confident that the preferences in their pref-
erence dataset were generated via the regret preference
model, then the technique above of manually shifting bA∗
r
may be justified and tenable, depending on upon the size
of the action space. Yet with such confidence, acting to
greedily maximize bA∗
ris more straightforward and effi-
cient. Further, an appeal that will emerge from our analy-
sis is that algorithmically assuming preferences arise from
partial return can lead to good performance regardless of
whether preferences actually reflect partial return or re-
gret. The manual shift technique could change the set of
optimal policies when preferences are generated by the
partial return preference model. Therefore, we do not rec-
ommend applying the shift above in practice. Below we
describe another method that, although imperfect, avoids
explicitly embracing either preference model.
Adding a constant to bA∗
rdoes not change the likelihood
of a preferences dataset, making the learned value of
max (s,a)bA∗
r(s, a)arbitrary. Consequently, it also makes
max abA∗
r(·, a)underspecified. If tasks have varying hori-
zons, then different choices for this maximum value
can determine different sets of optimal policies (e.g.,
by changing whether termination is desirable). One so-
lution is to convert varying horizon tasks to continu-
ing tasks by including infinite transitions from absorb-
ing states to themselves after termination, where all such
transitions receive 0reward. Note that this issue does
not exist when acting directly from bA∗
r—i.e., π(s) =
argmax a[bA∗
r(s, a)]—for which adding a constant to the
output of bA∗
rdoes not change π. Some past authors have
acknowledged this insensitivity to a shift [Christiano et al.
2017, Lee et al. 2021a, Ouyang et al. 2022, Hejna and
5

--- PAGE 6 ---
Learning Optimal Advantage from Preferences and Mistaking it for Reward A P REPRINT
Figure 4: Comparing the effect on greedy Q∗rcA∗rof including
transitions from absorbing state. For each state within 30 MDPs,
the plots above show the max abA∗
r(s, a)values. The plot shows
that including such transitions moves the resultant maximum
values closer to 0. The plot for stochastically generated pref-
erences is similar and can be found in Appendix F. After learn-
ing with absorbing transitions, max abA∗
r(s, a)across all states
is stochastically closer to 0 than when learning without them.
Wilcoxon paired signed-rank tests at every training set size are
all extremely significant with p <10−7.
Sadigh 2023], and the common practice of forcing all
tasks to have a fixed horizon (e.g., as done by Chris-
tiano et al. [2017, p. 14] and Gleave et al. [2022]) may
be partially attributable to the poor performance that re-
sults when using the partial return preference model in
variable-horizon tasks without transitions from absorbing
states.
Figure 4 shows the large impact of including transi-
tions from absorbing state when ˆr=bA∗
r. As expected,
greedy bA∗
ris not noticeably affected by the inclusions of
such transitions. Further, Figure 4 shows that the inclusion
of these transitions from absorbing state does indeed push
max aA∗
˜r(·, a)towards 0, more so with larger training set
sizes (given a fixed number of epochs), though it does not
completely accomplish making max aA∗
˜r(·, a) = 0 .
Bias towards termination determines performance dif-
ferences. When max abA∗
r(s, a)tends to be near 0, we
find the performances of greedy Q∗rcA∗randgreedy bA∗
rto be
similar. But their performances sometimes differ. Can we
predict which algorithm will perform better? To address
this questions understand why, we performed a detailed
analysis with 90 small gridworld MDPs, from which the
following hypothesis arose. The logic behind the follow-
ing hypothesis assumes an undiscounted task, though the
hypothesized effects should exist in lessened form as dis-
counting is increased. We define a loop to be a segment
that begins and ends in the same state and then focus on
the maximum partial return by rcA∗racross all loops.
Table 1: Hypothesis regarding which algorithm performs as
well or better than the other, given 2 conditions.
Conditionπ∗
r
terminatesπ∗
rdoes not
terminate
Max loop partial return >0greedy Q∗rcA∗rgreedy bA∗
r
Max loop partial return <0greedy bA∗
r greedy Q∗rcA∗r
Figure 5: Validation of the hypothesis that maximum partial
return by rcA∗racross all loops determines the direction of per-
formance differences between greedy bA∗
randgreedy Q∗rcA∗r.
1080 runs are shown, built from the set of 90 MDPs
× {10,100,1000}preferences in the training set × {1,2}seg-
ment lengths × {noiselessly ,stochastically }generated prefer-
ences. Plot points are colored orange when every π∗
rterminates
and blue when every π∗
rdoes not terminate. The blue and orange
shading of the plot represents where our hypotheses predict cir-
cles of each color to be, if y̸= 0. Returns are standardized across
MDPs within [−1,1](detail in Appendix D), and the x axis is
the maximum partial return by rcA∗racross all loops in the MDP.
Of the 75 runs with a performance difference ( y̸= 0), 73 con-
form to our hypothesis. In the remaining 2 runs, both algorithms
achieve near-optimal behavior and therefore have a difference of
less than 0.1.
Focusing on tasks with deterministic transitions,3the jus-
tification for this hypothesis is based on the following bi-
ases created by the maximum partial return of all loops:
• When the maximum partial return of all loops is
positive , any π∗
rcA∗rwill not terminate because it
can achieve infinite value.
• When the maximum partial return of all loops is
negative , anyπˆrforrcA∗rwill terminate, because it
can only achieve negative infinity value without
terminating.
Results shown in Figure 5 validate this hypothesis. Over
1080 runs of learning bA∗
rin various settings, we find that
the hypothesis is highly predictive of deviations in perfor-
mance.
The cause of this predictive measure, the maximum partial
return by rcA∗rof all loops, has not yet been characterized.
Hence, an algorithm designer should still be wary of mis-
takingbA∗
rfor a reward function and relying on this pre-
dictive measure to determine whether the resulting policy
avoids or seeks termination.
Reward is also highly shaped with approximation er-
ror. We also test whether the reward shaping that exists
when using A∗
ras a reward function is also present when
using its approximation, bA∗
r. Figure 6 finds shows that pol-
icy improvement with the Q learning algorithm [Watkins
3For stochastic tasks, this concept of loops generalizes to
the steady-state distribution with the maximum average reward,
across all policies.
6

--- PAGE 7 ---
Learning Optimal Advantage from Preferences and Mistaking it for Reward A P REPRINT
Figure 6: Learning curves for Q learning on the ground truth re-
ward function rand on rcA∗r. Each curve represents 100 instances
of Q learning, each in a different MDP. bA∗
rwas learned with
noiseless 100,000 regret-based preferences. Even without giv-
ing the learning agent access to the known rcA∗r, we see that learn-
ing is more efficient, indicating that in practice rcA∗ris a helpfully
shaped reward function, as is using the true A∗
ras a reward func-
tion. We define AAC as the area above a curve and below 1.0.
A small AAC indicates better learning performance. Wilcoxon
paired signed-rank tests reveal that Q learning with r(purple)
has a larger AAC than with rA∗r(red), which in turn has a larger
AAC than with rcA∗r(both p <0.00003 ).
and Dayan 1992] is more sample efficient with rA∗rand
withrcA∗rthan with the ground truth r, as was expected.
3.4 Summary
When one learns from regret-based preferences using the
partial return preference model, the theoretical and em-
pirical consequences are surprisingly less harmful than
this apparent misuse suggests it would be. The policy that
would have been learned with the correct regret-based
preference model is preserved if bA∗
rhas a maximum of 0
in every state. Further, bA∗
racts as a highly shaped reward.
Perhaps this analysis explains why the partial return pref-
erence model—shown to not model human preferences
well [Knox et al. 2022]—nonetheless has achieved im-
pressive performance on numerous tasks. That said, con-
fusingbA∗
rfor a reward function has drawbacks compared
togreedy bA∗
r, including higher sample complexity and
sensitivity to an understudied factor, the maximum partial
return by rcA∗rof all loops.
4 Reframing related work on fine-tuning
generative models
The partial return preference model has been used in sev-
eral high-profile applications: to fine-tune large language
models for text summarization [Ziegler et al. 2019], to
create InstructGPT and ChatGPT [Ouyang et al. 2022,
OpenAI 2022], to create Sparrow [Glaese et al. 2022], in
work by Bai et al. [2022], and to fine-tune Llama 2 [Tou-
vron et al. 2023]. The use of the partial return model in
these works fortuitously allows an alternative interpre-
tation of their approach: they are applying a regret
preference model and are learning an optimal advan-
tage function, not a reward function. These approaches
make several assumptions:• Preferences are generated by partial return.
• During policy improvement, the sequential task
is treated as a bandit task at each time step. That
treatment is equivalent to setting the discount
factor γto0during policy improvement.
• The reward function is R→S×A, not taking
the next state as input.
These approaches learn gas in Equation 6, which is
interpreted as a reward function according to the par-
tial return preference model. They also assume γ= 0
during what would be the policy improvement stage.
Therefore, ˜r(s, a) = Q∗
˜r(s, a), and for any state s,
π∗
˜r(s) = argmax aQ∗
˜r(s, a) = argmax a˜r(s, a) =
argmax ag(s, a).
Problems with the above assumptions Many of the
language models considered here are applied in the se-
quential setting of multi-turn, interactive dialog, such as
ChatGPT [OpenAI 2022], Sparrow [Glaese et al. 2022],
and work by Bai et al. [2022]. Treating these as bandit
tasks (i.e., setting γ= 0) is an unexplained decision that
contradicts how reward functions are used in sequential
tasks, to accumulate throughout the task to score a trajec-
tory as return.
Further, the choice of γis arbitrary in the original framing
of their algorithms. Because they also assume |σ|= 1,
then the partial return of a segment reduces to the imme-
diate reward without discounting:P|σ|−1
t=0γt˜r(sσ
t, aσ
t) =
˜r(sσ
0, aσ
0). Consequently, γcuriously has no impact on
what reward function is learned from the partial return
preference model (assuming the standard definition in this
setting that 00= 1). This lack of impact is a generally
problematic aspect of learning reward functions with par-
tial return preference models, since changing γfor a fixed
reward function is known to often change the set of op-
timal polices. (Otherwise MDPs could be solved much
more easily by setting γ= 0 and myopically maximiz-
ing immediate reward. )
Despite two assumptions—that preferences are driven
only by partial return and that γ= 0—that lack justifi-
cation and appear to have significant consequences, the
technique is remarkably effective, producing some of the
most capable language models at the time of writing.
Fine-tuning with regret-based preferences Let us in-
stead assume preferences come from the regret prefer-
ence model. As explained in Section 3.2, the γ= 0 as-
sumption then has no effect. Therefore it can be removed,
avoiding both of the troubling assumptions. Specifically,
if preferences come from the regret preference model,
then the same algorithm’s output gisbA∗
r. Consequently,
under this regret-based framing, for any state s,π∗
˜r(s) =
argmax aA∗
˜r(s, a) = argmax ag(s, a). Therefore, both
the learning algorithm and action selection for a greedy
policy in this setting are functionally equivalent to their
algorithm, but their interpretations change .
7

--- PAGE 8 ---
Learning Optimal Advantage from Preferences and Mistaking it for Reward A P REPRINT
agent (LM) 
Two levels of the multi-turn language problem 
environment 
(human) agent  
(LM)
action 
(response) observation 
(prompt) 
environment 
(human) action 
(response) observation 
(prompt) environment 
(token 
appender) agent 
(from policy 
gradient) 
action 
(next token) observation 
(prompt + 
tokens so far) 
Reward Greedy action 
selection as an RL 
problem 
for the final token 
for earlier tokens A multi-turn 
language problem 
ĝ(response, history) 
0
Figure 7: This paper focuses exclusively on the problem to the
left, which involves multiple turns of the human providing a
prompt and the language-model agent responding. The problem
on the right is a common artificial constraint on action selection
to make it tractable, using the policy to sample a response one
token at a time, sequentially; it does not involve any interaction
with the human (i.e., the environment).
In summary, assuming that learning from preferences pro-
duces an optimal advantage function—the consequence of
adopting the more empirically supported regret preference
model—provides a more consistent framing for these al-
gorithms.
A common source of confusion Greedy action selec-
tion can itself be challenging for large action spaces.
These language models have large action spaces, since
choosing a response to the latest human prompt involves
selecting a large sequence of tokens. This choice of re-
sponse is a single action that results in interaction with
the environment, the human. As an example, Ouyang et al.
[2022] instead artificially restrict the selection of an action
to itself be a sequential decision-making problem, forcing
the tokens to be selected one at a time, in order from the
start to the end of the text, as Figure 7 illustrates. They
use a policy gradient algorithm, PPO [Schulman et al.
2017], to learn a policy for this sub-problem, where the
RL agent receives 0 reward until the final token is cho-
sen. At that point, under their interpretation, it receives the
learned bandit reward from the left problem in Figure 7.
This paper does not focus on how to do greedy action se-
lection , and we do not take a stance on whether to treat it
as a token-by-token RL problem. However, if one desires
to take such an approach to greedy action selection while
seeking π(s) =argmax a[bA∗
r(s, a)], then the bandit re-
ward is simply replaced by the optimal advantage, again
executable by the same code, since both are simply the
outputs of g.
Implications for fine-tuning generative models Ex-
tensions of the discussed fine-tuning work may seek to
learn a reward function to use beyond a bandit setting.
Motivations for doing so include reward functions gener-
alizing better when transition dynamics change and allow-
ing the language model to improve its behavior based on
experienced long-term outcomes. To learn a reward func-
tion to use in such a sequential problem setting, framingthe preferences dataset as having been generated by the
regret preference model would provide a different algo-
rithm for doing so (in Section 2). It would also avoid the
arbitrariness of setting γ >0and learning with the partial
return preference model, which outputs the same reward
function under these papers’ assumptions regardless of the
discount factor. The regret-based algorithm for learning a
reward function is more internally consistent and appears
to be more aligned with human stakeholder’s preferences.
However, it does present research challenges for learn-
ing reward functions in complex tasks such as those for
which these language models are fine-tuned. In particular,
the known method for learning a reward function with the
regret preference model requires a differentiable approx-
imation of the optimal advantage function for the reward
function arising from parameters that change at each train-
ing iteration.
5 Conclusion
This paper investigates the consequences of assuming
that preferences are generated according to partial return
when they instead arise from regret. The regret prefer-
ence model provides an improved account of the effec-
tive method of fine-tuning LLMs from preferences (Sec-
tion 4). In the general case (Section 3), we find that this
mistaken assumption is not ruinous to performance when
averaged over many instances of learning, which explains
the success of many algorithms which rely on this flawed
assumption. Nonetheless, this mistaken interpretation ob-
fuscates learning from preferences, confusing practition-
ers’ intuitions about human preferences and how to use
the function learned from preferences. We believe that
partial return preference model is rarely accurate for tra-
jectory segments, i.e., it is rare for a human’s preferences
to be unswayed by any of a segment’s end state value,
start state value, or luck during transitions. Assuming that
humans incorporate allof those three segment character-
istics, as the regret preference model does, results in a bet-
ter descriptive model, yet it does not universally describe
human preferences. To improve the sample efficiency and
alignment of agents that learn from preferences, subse-
quent research should focus further on potential models
of human preference and also on methods for influencing
people to conform to a desired preference model. Lastly,
after reading this paper, one might be tempted to conclude
that it’s safe to close your eyes, clench your teeth, and
put your faith in the partial return preference model. This
conclusion is not supported by this paper, since even with
the addition of transitions from absorbing states, arbitrary
bias to seek or avoid termination is frequently introduced.
The implication of this bias is particularly important since
RLHF is currently the primary safeguarding mechanism
for LLMs [Casper et al. 2023].
8

--- PAGE 9 ---
Learning Optimal Advantage from Preferences and Mistaking it for Reward A P REPRINT
Acknowledgments
This work has taken place in part in the the Interactive
Agents and Colloraborative Technologies (InterACT) lab
at UC Berkeley, the Learning Agents Research Group
(LARG) at UT Austin and the Safe, Correct, and Aligned
Learning and Robotics Lab (SCALAR) at The University
of Massachusetts Amherst. LARG research is supported
in part by NSF (FAIN-2019844, NRT-2125858), ONR
(N00014-18-2243), ARO (E2061621), Bosch, Lockheed
Martin, and UT Austin’s Good Systems grand challenge.
Peter Stone is financially compensated as the Executive
Director of Sony AI America, the terms of which have
been approved by the UT Austin. SCALAR research is
supported in part by the NSF (IIS-1749204), AFOSR
(FA9550-20-1-0077), and ARO (78372-CS, W911NF-19-
2-0333). InterACT research is supported in part by ONR
YIP and NSF HCC. Serena Booth is supported by NSF
GRFP.
References
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
Training a helpful and harmless assistant with reinforce-
ment learning from human feedback. arXiv preprint
arXiv:2204.05862 , 2022.
Erdem Bıyık, Dylan P Losey, Malayandi Palan,
Nicholas C Landolfi, Gleb Shevchuk, and Dorsa
Sadigh. Learning reward functions from diverse sources
of human feedback: Optimally integrating demonstrations
and preferences. The International Journal of Robotics
Research , page 02783649211041652, 2021.
Stephen Casper, Xander Davies, Claudia Shi,
Thomas Krendl Gilbert, J ´er´emy Scheurer, Javier Rando,
Rachel Freedman, Tomasz Korbak, David Lindner, Pedro
Freire, et al. Open problems and fundamental limitations
of reinforcement learning from human feedback. arXiv
preprint arXiv:2307.15217 , 2023.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Mar-
tic, Shane Legg, and Dario Amodei. Deep reinforcement
learning from human preferences. In Advances in Neu-
ral Information Processing Systems (NIPS) , pages 4299–
4307, 2017.
Amelia Glaese, Nat McAleese, Maja Tre
’bacz, John
Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,
Laura Weidinger, Martin Chadwick, Phoebe Thacker,
et al. Improving alignment of dialogue agents via targeted
human judgements. arXiv preprint arXiv:2209.14375 ,
2022.
Adam Gleave, Mohammad Taufeeque, Juan Rocamonde,
Erik Jenner, Steven H. Wang, Sam Toyer, Maximilian
Ernestus, Nora Belrose, Scott Emmons, and Stuart Rus-
sell. imitation: Clean imitation learning implementa-
tions. arXiv:2211.11972v1 [cs.LG], 2022. URL https:
//arxiv.org/abs/2211.11972.Joey Hejna and Dorsa Sadigh. Inverse preference learn-
ing: Preference-based rl without a reward function. arXiv
preprint arXiv:2305.15363 , 2023.
Borja Ibarz, Jan Leike, Tobias Pohlen, Geoffrey Irving,
Shane Legg, and Dario Amodei. Reward learning from
human preferences and demonstrations in atari. arXiv
preprint arXiv:1811.06521 , 2018.
W Bradley Knox, Stephane Hatgis-Kessell, Serena Booth,
Scott Niekum, Peter Stone, and Alessandro Allievi. Mod-
els of human preference for learning reward functions.
arXiv preprint arXiv:2206.02231 , 2022.
Kimin Lee, Laura Smith, and Pieter Abbeel. Peb-
ble: Feedback-efficient interactive reinforcement learning
via relabeling experience and unsupervised pre-training.
arXiv preprint arXiv:2106.05091 , 2021a.
Kimin Lee, Laura Smith, Anca Dragan, and Pieter
Abbeel. B-pref: Benchmarking preference-based rein-
forcement learning. arXiv preprint arXiv:2111.03026 ,
2021b.
A.Y . Ng, D. Harada, and S. Russell. Policy invariance
under reward transformations: Theory and application to
reward shaping. Sixteenth International Conference on
Machine Learning (ICML) , 1999.
OpenAI. Chatgpt: Optimizing language models for di-
alogue. OpenAI Blog https://openai.com/blog/chatgpt/,
2022. Accessed: 2022-12-20.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L Wainwright, Pamela Mishkin, Chong Zhang, Sand-
hini Agarwal, Katarina Slama, Alex Ray, et al. Training
language models to follow instructions with human feed-
back. arXiv preprint arXiv:2203.02155 , 2022.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zem-
ing Lin, Natalia Gimelshein, Luca Antiga, Alban Des-
maison, Andreas Kopf, Edward Yang, Zachary DeVito,
Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,
Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chin-
tala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d'Alch ´e-Buc, E. Fox, and R. Garnett,
editors, Advances in Neural Information Processing
Systems 32 , pages 8024–8035. Curran Associates, Inc.,
2019. URL http://papers.neurips.cc/paper/9015-pytorch-
an-imperative-style-high-performance-deep-learning-
library.pdf.
F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,
B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-
learn: Machine learning in Python. Journal of Machine
Learning Research , 12:2825–2830, 2011.
Dorsa Sadigh, Anca D Dragan, Shankar Sastry, and San-
jit A Seshia. Active preference-based learning of reward
functions. Robotics: Science and Systems , 2017.
9

--- PAGE 10 ---
Learning Optimal Advantage from Preferences and Mistaking it for Reward A P REPRINT
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347 , 2017.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288 , 2023.
Xiaofei Wang, Kimin Lee, Kourosh Hakhamaneshi, Pieter
Abbeel, and Michael Laskin. Skill preferences: Learning
to extract and execute robotic skills from human feed-
back. In Conference on Robot Learning , pages 1259–
1268. PMLR, 2022.
Christopher JCH Watkins and Peter Dayan. Q-learning.
Machine learning , 8:279–292, 1992.
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B
Brown, Alec Radford, Dario Amodei, Paul Christiano,
and Geoffrey Irving. Fine-tuning language models from
human preferences. arXiv preprint arXiv:1909.08593 ,
2019.
10

--- PAGE 11 ---
Learning Optimal Advantage from Preferences and Mistaking it for Reward A P REPRINT
A Proof of Theorem 3.1
Theorem 3.1 (Greedy action is optimal when the maximum reward in every state is 0.)
Π∗
˜r={π:∀s,∀a[π(a|s)>0⇔a∈argmaxa˜r(s, a)]}if max a˜r(·, a) = 0 .
The main idea is that if the maximum reward in every state is 0, then the best possible return from every state is 0.
Therefore, V∗
˜r(·) = 0 , making ∀(s, a)∈S×A, Q∗
˜r(s, a) = ˜r(s, a) +γEs′[V∗
˜r(s)] = ˜r(s, a).
The proof follows.
∀(s, a)∈S×A,˜r(s, a)≤0, so∀s∈S, V∗
˜r(s)≤0.
∀s∈S,∃a∈A: ˜r(s, a) = 0 , so∀s∈S, V∗
˜r(s)≥0.
V∗
˜r(s)≤0andV∗
˜r(s)≥0implies V∗
˜r(s) = 0 , so∀s∈S, V∗
˜r(s) = 0 .
∀(s, a)∈S×A,
Q∗
˜r(s, a) = ˜r(s, a) +γEs′[V∗
˜r(s′)]
Q∗
˜r(s, a) = ˜r(s, a) +γEs′[0]
Q∗
˜r(s, a) = ˜r(s, a)
argmax aQ∗
˜r(s, a) =argmax a˜r(s, a)(7)
By definition, Π∗
˜r={π:∀s, π(s) =argmaxaQ∗
˜r(s, a)}.
Since argmax aQ∗
˜r(s, a) =argmax a˜r(s, a),Π∗
˜r={π:∀s, π(s) =argmaxa˜r(s, a)}. □
B Proof of Corollary 3.1
Corollary 3.1 (Policy invariance of rA∗r)
LetrA∗r≜A∗
r. If max aA∗
r(·, a) = 0 ,Π∗rA∗r= Π∗
r.
Since max aA∗
r(·, a) = 0 andrA∗r≜A∗
r, max arA∗r(·, a) = 0 .
Therefore, by Theorem 3.1, Π∗rA∗r={π:∀s,∀a[π(a|s)>0⇔a∈argmaxarA∗r(s, a)]}.
Also, by definition, Π∗
r={π:∀s,∀a[π(a|s)>0⇔a∈argmaxaA∗
r(s, a)]}.
Consequently,
Π∗
rA∗
r={π:∀s,∀a[π(a|s)>0⇔a∈argmaxarA∗r(s, a)]}
={π:∀s,∀a[π(a|s)>0⇔a∈argmaxaA∗
r(s, a)]}
= Π∗
r(8)
C Used as reward, A∗
ris highly shaped
In Section 3.2, we stated that following the advice below of Ng et al. [1999] is equivalent to using A∗
ras reward. We
derive this result after reviewing their advice.
In their paper on potential-based reward shaping, the authors suggest a potent form of setting Φ(s), which is Φ(s) =
V∗
M(s). Their notation includes MDPs MandM′, where Mis the original MDP and M′is the potential-shaped MDP.
The notation for these two MDPs maps to our notation in that the reward function of Misr, and we ultimately derive
that the reward function of M′isrA∗r.
11

--- PAGE 12 ---
Learning Optimal Advantage from Preferences and Mistaking it for Reward A P REPRINT
Ng et al.’s Corollary 2 includes the statement that, under certain conditions, for any state sand action a,Q∗
rA∗r(s, a) =
Q∗
r(s, a)−Φ(s).
Q∗
rA∗r(s, a) =Q∗
r(s, a)−Φ(s)
Q∗
rA∗r(s, a) =Q∗
r(s, a)−V∗
r(s)
Q∗
rA∗r(s, a) =A∗
r(s, a)
max aQ∗
rA∗r(s, a) =max aA∗
r(s, a)
max aQ∗
rA∗r(s, a) = 0
V∗
rA∗r(s, a) = 0(9)
Eqn 9 above establishes two things that will be applied within Eqn 10 below, that Q∗
rA∗r(s, a) =A∗
r(s, a)and that
V∗
rA∗r(s, a) = 0 .
Q∗
rA∗r(s, a) =rA∗r+γEs′[V∗
˜r(s′)]
Q∗
rA∗r(s, a) =rA∗r+γEs′[0]
Q∗
rA∗r(s, a) =rA∗r
A∗
r(s, a) =rA∗r(10)
□
D Detailed experimental settings
Here we provide details regarding the gridworld tasks and the learning algorithms used in our experiments. The
learning algorithms described include both algorithms for learning from preferences and for policy improvement.
Because much of the details below are repeated from Knox et al. [2022], some of the description in this section is
adapted from that paper with permission from the authors.
D.1 The gridworld domain and MDP generation
Gridworld domain Each instantiation of the gridworld domain consists of a grid of cells. In the following sections,
each gridworld domain instantiation is referred to interchangeably as a randomly generated MDP.
A cell can contain up to one of four types of objects: ”mildly good” objects, ”mildly bad” objects, terminal success
objects, and terminal failure objects. Each object has a specific reward component, and a time penalty provides another
reward component. The reward received upon entering a cell is the sum of all reward components. The delivery agent’s
state is its location. The agent’s action space consists of a single step in one of the four cardinal directions. The episode
can terminate either at a terminal success state for a non-negative reward, or at a terminal failure state for a negative
reward. The reward for a non-terminal transition is the sum of any reward components. The procedure for choosing
the reward component of each cell type is described later in this subsection.
Actions that would move the agent beyond the grid’s perimeter result in no motion and receive reward that includes
the current cell’s time penalty reward component but not any ”mildly good” or ”mildly bad” components. In this work,
the start state distribution is always uniformly random over non-terminal states. This domain was introduced by [Knox
et al. 2022].
Standardizing return across MDPs and defining near optimal performance To compare performance across
different MDPs, the mean return of a policy π,Vπ
r, is normalized to (Vπ
r−VU
r)/V∗
r, where V∗
ris the optimal
expected return and VU
ris the expected return of the uniformly random policy (both given the uniformly random start
state distribution). Normalized mean return above 0 is better than VU
r. Optimal policies have a normalized mean return
of 1, and we consider above 0.9 to be near optimal .
Additionally, when plotting the mean of these standardized returns, we floor each such return at -1, which prevent the
mean from being dominated by low performing policies that never terminate. Such policies can have, for example,
12

--- PAGE 13 ---
Learning Optimal Advantage from Preferences and Mistaking it for Reward A P REPRINT
The delivery domain
w1w2w3w4w5w6
ground-truth rewardterminates
Figure 8: An example task and unspecified reward function from the gridworld delivery domain. Tasks from this domain are used
in all experiments. In Appendix D.1, objects described as “mildly good” are shown as coins, objects described as “mildly bad” are
shown as orange road blocks, objects described as “terminal failure states” are shown as sheep, and objects described as “terminal
success states” are shown as red destination markers. The brick surface and the house object were not used in our experiments. The
gridworld image is reprinted with permission, from Knox et al. [2022].
-1000 or -10000 mean standardized return, which we group together as a similar degree of failure, yet without flooring
at -1, these two failing policies would have very different effects on the means.
Generating the random MDPs used to create Figures 3, 4, and 6 Here we describe the procedure for generating
the 100 MDPs used in Figure 6, which include the 30 MDPs used in Figures 3 and 4. This procedure was also used by
[Knox et al. 2022].
The height for each MDP is sampled from the set {5,6,10}, and the width is sampled from {3,6,10,15}. The pro-
portion of cells that contain terminal failure objects is sampled from the set {0,0.1,0.3}. There is always exactly one
cell with a terminal success object. The proportion of “mildly bad” objects is selected from the set {0,0.1,0.5,0.8},
and the proportion of “mildly good” objects is selected from {0,0.1,0.2}. Each sampled proportion is translated to a
number of objects (rounding down to an integer when needed), and then each of the object types are randomly placed
in empty cells until the proportions are satisfied. A cell can have zero or one object in it.
Then the ground-truth reward component for each of the above cell or object types was sampled from the following
sets:
• Terminal success objects: {0,1,5,10,50}
• Terminal failure objects: {−5,−10,−50}
• Mildly bad objects: {−2,−5,−10}
Mildly good objects always have a reward component of 1. An constant time penalty of -1 is also always applied.
Generating random MDPs as seen in figure 5 For all 90 MDPs, the following parameters were used. The height for
each MDP is sampled from the set {3,5}, and the width is sampled from {1,2}. There is always exactly one positive
terminal cell that is randomly placed on one of the four corners of the board. The ground-truth reward component for
the positive terminal state is sampled from {0,1.5,10}. These 90 MDPs do not contain any ”mildly good” or ”mildly
bad” cells.
For 30/90 of the MDPs, it is always optimal to eventually terminate at either a terminal failure cell or a terminal success
cell:
• For each MDP there is a 50% chance that a terminal failure cell exists. If it does exist it is randomly placed
on one of the four corners of the board.
• The ground-truth reward component for the terminal failure cell is sampled from {−5,−10}.
13

--- PAGE 14 ---
Learning Optimal Advantage from Preferences and Mistaking it for Reward A P REPRINT
• The true reward component for blank cells is always −1.
For 30/90 of the MDPs, it is always optimal to eventually terminate at a terminal success cell:
• For each MDP there is always a terminal failure cell that exists and is randomly placed on one of the four
corners of the board.
• The ground-truth reward component for the terminal failure cell is always −10.
• The true reward component for blank cells is always −1.
For 30/90 of the MDPs, it is always optimal to loop forever and never terminate:
• For each MDP there is always a terminal failure cell that exists and is randomly placed on one of the four
corners of the board.
• The ground-truth reward component for the terminal failure cell is always −10.
• The true reward component for blank cells is always +1.
All parameters for randomly sampling MDPs that are not explicitly discussed above are the same as for Figures 3, 4,
and 6.
D.2 Learning algorithms
Doubling the training set by reversing preference samples To provide more training data and avoid learning
segment ordering effects, for all preference datasets we duplicate each preference sample, swap the corresponding
segment pairs, and reverse the preference.
Discounting during value iteration and Q learning Despite the gridworld domain being episodic, a policy may
endlessly avoid terminal states. In some MDPs, such as a subset of those used in Figure 5, this is an optimal behavior.
In other MDPs this is the result of a low-performing policy. To avoid an infinite loop of value function updates, we
apply a discount factor of γ= 0.999during value iteration, Q learning, and when assessing the mean returns of
policies with respect to the ground-truth reward function, r. We chose this high discount factor to have negligible
effect on the returns of high-performing policies (since relatively quick termination is required for high performance)
while still allowing for convergence within a reasonable time.
Hyperparameters for learning bA∗
ras seen in Figures 3, 4, 5, and 10 These hyperparameters exactly match those
used in [Knox et al. 2022], except that we decreased the number of training epochs. For all experiments, each algorithm
was run once with a single randomly selected seed.
• learning rate: 2
• number of seeds used: 1
• number of training epochs: 1,000
• optimizer: Adam
–β1= 0.9
–β2= 0.999
–eps= 1e−08
Hyperparameters for learning bA∗
ras seen in Figure 6 These hyperparameters exactly match those used in [Knox
et al. 2022]. For all experiments, each algorithm was run once with a single randomly selected seed.
• learning rate: 2
• number of seeds used: 1
• number of training epochs: 30,000
• optimizer: Adam
14

--- PAGE 15 ---
Learning Optimal Advantage from Preferences and Mistaking it for Reward A P REPRINT
–β1= 0.9
–β2= 0.999
–eps= 1e−08
Hyperparameters for Q learning as seen in Figure 6 These hyperparameters were tuned on 10 learned bA∗
rfunc-
tions where setting the reward function as bA∗
rand using value iteration to derive a policy was known to eventually
lead to optimal performance. The hyperparameters were tuned so that, for each bA∗
rfunction in this set, Q-learning also
yielded an optimal policy. For all experiments, each algorithm was run once with a single randomly selected seed.
• learning rate: 1
• number of seeds used: 1
• number of training episodes: 1,600
• maximum episode length: 1000 steps
• initial Q values: 0
• exploration procedure: ϵ-greedy
–ϵ= 0.4
–decay= 0.99
Computer specifications and software libraries used The compute used for all experiments had the following
specification.
• processor: 1x Core™ i9-9980XE (18 cores, 3.00 GHz) & 1x WS X299 SAGE/10G — ASUS — MOBO;
• GPUs: 4x RTX 2080 Ti;
• memory: 128 GB.
Pytorch 1.7.1 [Paszke et al. 2019] was used to implement all reward learning models, and statistical analyses were
performed using Scikit-learn 0.23.2 [Pedregosa et al. 2011].
E Shifting such that the maximum value of cA∗
rin every state is 0
Figure 9: In 90 small gridworld MDPs, when we explicitly shift bA∗
rby a state-dependent constant such that max abA∗
r(·, a) = 0 ,
we empirically observe no difference between greedy bA∗
randgreedy Q∗rcA∗r.
Corollary 3.2 claims that if max abA∗
r(·, a) = 0 , then Π∗rcA∗r={π:∀s,∀a[π(a|s)>0⇔a∈argmaxabA∗
r(s, a)]}.
Figure 9 shows the results of our empirical validation of this claim. Specifically, this figure shows that when
maxabA∗
r(·, a) = 0 , we observe no difference in the mean standardized return between greedy bA∗
randgreedy Q∗rcA∗r.
F Encouraging max abA∗
r(·, a) = 0 without shifting learned values manually
Figure 4 in Section 3.3 uses noiselessly generated preferences. Figure 10 presents an analogous analysis for stochas-
tically generated preferences. The pattern is similar results in the noiseless setting, with even less variance for large
training sets. Specifically, Figure 10 shows that including transitions from absorbing states moves the resultant maxi-
mum values of the approximated optimal advantage function closer to 0.
15

--- PAGE 16 ---
Learning Optimal Advantage from Preferences and Mistaking it for Reward A P REPRINT
Figure 10: Comparing the effect on greedy Q∗rcA∗rof including transitions from absorbing state. For each state within 30 MDPs, the
plots above show the max abA∗
r(s, a)values. The plot shows that including such transitions moves the resultant maximum values
closer to 0. This plot complements Figure 4, which contains a similar plot for noiseless preferences. After learning with absorbing
transitions, max abA∗
r(s, a)across all states is stochastically closer to 0 than when learning without them. Wilcoxon paired signed-
rank tests at every training set size above 300 are extremely significant with p <10−57. For 300 preferences, p= 0.0002 .
G Investigation of performance differences
Figure 11: Performance when stochastically generated preference datasets do and do not include segments with transitions from
absorbing state, complementing Figure 3. Results are across 30 randomly generated gridworld MDPs with tabular representations
of thebA∗
r, where segments of length 3 are chosen by uniformly randomly choosing a start state and 3 its actions. When transitions
from absorbing states are not included, any segment that terminates before its final transition is rejected and then resampled. This
plot complements Figure 4, which shows a similar plot for noiseless preferences. For greedy bA∗
r(in red) Wilcoxon paired signed-
rank tests reveal that including transitions from absorbing state results in significantly higher performance for all training set sizes
but the smallest, 300, with p <0.02for 1000, and p <0.0002 for others. No significant difference in performance is detected for
greedy Q∗rcA∗rwith or without terminating transitions.
Figure 3 in Section 3.3 uses noiselessly generated preferences. As in the section above, Figure 11 presents an analogous
analysis for stochastically generated preferences. This plot likewise shows that greedy Q∗rcA∗rlearned without transitions
from absorbing state performs poorly. We also note that the performance of the other three conditions is more similar
in comparison to Figure 3.
16
