# 2304.14553.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2304.14553.pdf
# Kích thước tệp: 335835 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tính thích hợp là tất cả những gì bạn cần! Chatbot đa năng và những gì chúng có thể và không thể nói
Hendrik Kempt1, Alon Lavie2,3, Saskia K. Nagel1

1RWTH Aachen University, Applied Ethics Group
2Carnegie Mellon University, Language Technologies Institute  
3Unbabel Inc.
{hendrik.kempt, saskia.nagel}@humtec.rwth-aachen.de

Tóm tắt
Nỗ lực làm cho các ứng dụng AI "an toàn" đã dẫn đến việc phát triển các biện pháp an toàn như yêu cầu tiêu chuẩn chính hoặc thậm chí duy nhất để cho phép sử dụng chúng. Điều tương tự có thể được chứng minh ở phiên bản mới nhất của chatbot, như chatGPT. Theo quan điểm này, nếu chúng "an toàn", chúng được cho là có thể triển khai. Cách tiếp cận này, mà chúng tôi gọi là "tiêu chuẩn an toàn", khá hạn chế trong việc giải quyết các vấn đề nổi lên mà chatGPT và các chatbot khác đã gây ra cho đến nay. Để trả lời hạn chế này, trong bài báo này chúng tôi lập luận cho việc giới hạn chatbot trong phạm vi các chủ đề mà chúng có thể trò chuyện theo khái niệm tiêu chuẩn về tính thích hợp. Chúng tôi lập luận rằng thay vì tìm kiếm "sự an toàn" trong các phát biểu của chatbot để xác định những gì chúng có thể và không thể nói, chúng ta nên đánh giá những phát biểu đó theo ba hình thức tính thích hợp: kỹ thuật-diễn ngôn, xã hội và đạo đức. Sau đó chúng tôi trình bày các yêu cầu đối với chatbot theo các hình thức tính thích hợp này để tránh những hạn chế của các quan điểm trước đây: vị trí, tính chấp nhận được và sự liên kết giá trị (PAVA). Với những điều này trong tâm trí, chúng ta có thể xác định những gì chatbot có thể hoặc không thể nói. Cuối cùng, một gợi ý ban đầu là sử dụng các bộ thử thách, được thiết kế đặc biệt cho tính thích hợp, như một phương pháp xác thực.

1. Giới thiệu
Chatbot đa năng và mở rộng miền đã gia nhập lĩnh vực công cộng, với mức độ thành công kinh tế và sự hoan nghênh của công chúng khác nhau. Việc phát hành công khai gần đây của OpenAI với ChatGPT (Schulman et al. 2022), một chatbot được thiết kế và xây dựng dựa trên thế hệ mới nhất của các mô hình ngôn ngữ lớn (LLM), đã làm công chúng kinh ngạc về khả năng của nó và tạo ra một làn sóng thảo luận công khai. Hai yêu cầu trực quan hợp lý hiện đang đối lập nhau:

--- TRANG 2 ---
một mặt, chatbot không nên nói bất cứ điều gì, vì một số lời nói có thể gây tổn hại lớn. Mặt khác, để sử dụng đa năng và hữu ích nhất có thể, chúng nên vẫn là chatbot mở rộng miền. Việc cân bằng giữa đảm bảo tự do tối đa có thể mà không cho phép chatbot mở rộng miền gây tổn hại nghiêm trọng là một trong những thách thức chính đối với công nghệ này.

Tuy nhiên, diễn ngôn hiện tại của đạo đức AI tập trung đáng kể vào đạo đức kinh doanh của công nghệ này, các vi phạm trách nhiệm doanh nghiệp và các mối quan tâm khác về phúc lợi công cộng và tác hại tập thể. Các thực hành kinh doanh không bền vững để thu thập dữ liệu và huấn luyện thuật toán bóc lột người lao động (Carter 2023), tiêu thụ lượng lớn năng lượng (Bender et al. 2021, Patterson et al. 2021), và có thể vi phạm bản quyền; động lực thị trường và cạnh tranh không được điều tiết khuyến khích các tập đoàn phát hành sản phẩm chưa hoàn thiện (Vincent 2023); một công chúng chưa chuẩn bị và đang chuyển hướng tiêu cực (NYT 2023), và các vấn đề bảo vệ dữ liệu chỉ là một số trong những chủ đề nổi lên xung quanh việc phát hành vòng tiến bộ công nghệ mới nhất về AI tạo sinh, và đặc biệt là về LLM và chatbot đa năng.

Có vẻ như diễn ngôn về LLM bị kẹt giữa an toàn AI (những người quan tâm đến việc sử dụng và triển khai an toàn LLM) và đạo đức AI (những người quan tâm đến việc xây dựng có đạo đức và phân phối công bằng LLM). Và trong khi cả hai bên đều đặt ra những câu hỏi cấp bách và liên quan, họ cũng có những hạn chế đáng kể để trả lời câu hỏi nghiên cứu của chúng tôi: các cân nhắc về an toàn sẽ không cung cấp cho chúng tôi câu trả lời về những gì chatbot có thể hoặc không thể nói, và cho đến nay, diễn ngôn đạo đức cũng đã không đưa ra được câu trả lời như vậy (cf. để có đánh giá tương tự LaCroix và Luccioni 2023).

Như một giải pháp thay thế, chúng tôi chọn tập trung vào một phê bình nội tại của chatbot. Tức là, chúng tôi nhằm đo lường các mối quan tâm tiêu chuẩn nổi lên gần đây từ các tương tác với chatbot - phản hồi không phù hợp, ảo giác, hack tiêm prompt - so với những gì chúng được dự định làm, đánh giá liệu các quan điểm tiêu chuẩn có thể sửa chữa bất kỳ sự không liên kết nào không, và đề xuất một giải pháp có thể cung cấp một số câu trả lời thực chất.

Đến thời điểm này, trải nghiệm của các nhà báo và công chúng sử dụng chatGPT trong công cụ tìm kiếm Bing (tên mã cho chatbot này là "Sydney" (Roose 2022); trong phần sau, chúng tôi gọi phiên bản chatGPT đó là "BingGPT" để làm rõ kết nối với việc sử dụng và nguồn gốc của nó) đã phơi bày hành vi bất ngờ trong cách các chatbot mới nhất này tương tác với con người về các chủ đề đối thoại, rõ ràng là không được các nhà phát triển dự định.

Tạo ra thứ gì đó có thể hoạt động như một chatbot đa năng hoặc mở rộng miền ban đầu nghe có vẻ như một mục tiêu hợp lý để đạt được thứ gì đó của một tác nhân thông minh. Xét cho cùng, con người có thể tiến hành những loại cuộc trò chuyện mở này, và do đó có vẻ đáng giá, từ cả mục tiêu lớn hơn (ví dụ, tạo ra trí tuệ nhân tạo tổng quát, như OpenAI tuyên bố theo đuổi (Altman 2023) cũng như cho các ứng dụng đa dạng (một chatbot đa năng có thể được thu hẹp thành một

--- TRANG 3 ---
chatbot mục đích cụ thể), để theo đuổi LLM với khả năng tiến hành cuộc trò chuyện về bất kỳ chủ đề nào.

Nếu một chatbot được cho là tiến hành cuộc trò chuyện với mọi người, bất kỳ chủ đề nào cũng có thể được thảo luận. Điều này có vẻ như hệ quả logic của việc tạo ra chatbot ngay từ đầu, và của AI nói chung: đa phương thức là chìa khóa để tạo ra những cỗ máy ngày càng thông minh hơn. Một chatbot chỉ có thể "cải thiện", nếu nó trở nên có khả năng điều hướng các bối cảnh đối thoại của con người mà phù hợp với cách con người tiến hành cuộc trò chuyện.

Các tiêu chuẩn đối thoại, xã hội và đạo đức thường giới hạn phạm vi, độ sâu, giọng điệu và tính chấp nhận được của các chủ đề trong cuộc trò chuyện giữa người với người. Trong cuộc trò chuyện giữa người với người, chúng ta thực sự bị hạn chế bởi nhiều tiêu chuẩn rõ ràng và ngầm định xác định tính thích hợp của nội dung, của hình thức và cấu trúc, và của giọng điệu trong việc tiến hành cuộc trò chuyện. Tùy thuộc vào người chúng ta nói chuyện, mối quan hệ chúng ta có với họ, điểm chung nào đã được thiết lập, ai khác có mặt, và những tiêu chuẩn đạo đức và tiêu chuẩn lịch sự và phép tắc nào được công nhận, cuộc trò chuyện giữa người với người không thể đơn giản trở thành "cuộc trò chuyện mở rộng miền", vì chúng bị ràng buộc bởi những tiêu chuẩn này. Những tiêu chuẩn này ảnh hưởng đến cả nội dung của cuộc trò chuyện cũng như hành vi của các tác nhân trong những cuộc trò chuyện đó.

Do đó, mục tiêu của chatbot là một tác nhân đối thoại đa năng chỉ có vẻ hợp lý hoặc hiển nhiên. Chúng ta nên hỏi loại hạn chế nào đối với chủ đề, giọng điệu và sự tham gia - nếu có - chúng ta nên tìm kiếm trong việc giới hạn những công cụ mạnh mẽ này. Không có các đề xuất tinh tế có sẵn, chatbot thực sự có thể "trò chuyện" về bất cứ điều gì và do đó không chỉ có thể gây tổn hại, mà còn thay đổi kỳ vọng đối thoại nói chung hoặc ảnh hưởng đến diễn ngôn công cộng theo những cách không đảm bảo. Chúng tôi yêu cầu phản hồi của chúng phải thích hợp hơn ý nghĩa hiện tại được thảo luận.

Để trả lời câu hỏi về mức độ chúng ta nên hạn chế chatbot đa năng trong phạm vi đối thoại của chúng, chúng tôi sẽ tiến hành theo cách sau. Đầu tiên, chúng tôi giới thiệu những phát triển mới nhất của chatbot và các tác nhân đối thoại khác, và cho thấy tại sao đáng để dành thời gian cho một phân tích triết học kỹ lưỡng hơn. Thứ hai, chúng tôi thảo luận về cách "tiêu chuẩn an toàn" được giả định định hình các câu hỏi tiêu chuẩn xung quanh chatbot. Bằng cách chỉ ra những hạn chế của quan điểm như vậy, chúng tôi thúc đẩy đề xuất được phát triển trong bước thứ ba: đề xuất này giới thiệu khái niệm tính thích hợp như một cách để chủ động hướng dẫn phát triển phạm vi đầu ra của chatbot đa năng. Chúng tôi phân biệt giữa a) tính thích hợp kỹ thuật-diễn ngôn, được sử dụng để tạo ra một chatbot tuân thủ các quy tắc diễn ngôn và các nguyên tắc đối thoại; b) tính thích hợp xã hội, được sử dụng để kiểm tra liệu chatbot có hành xử phù hợp với các bối cảnh xã hội nhất định hay không; và c) tính thích hợp đạo đức, làm rõ liệu chatbot có vi phạm các tiêu chuẩn đạo đức (không đổi theo bối cảnh) hay không. Ở đây, chúng tôi quan sát rằng các phương pháp phát triển và huấn luyện LLM cho đến nay (cụ thể nhất

--- TRANG 4 ---
là học tăng cường và huấn luyện hướng dẫn) vốn có thiếu sót trong khả năng kết hợp các tiêu chuẩn xã hội và đạo đức và các LLM kết quả do đó có vấn đề không minh bạch hoặc ngầm định, hoặc thiếu hoàn toàn những tiêu chuẩn đó.

Thứ tư, với quan điểm về tính thích hợp như vậy, chúng ta có thể thảo luận một số điều mà chatbot đa năng có thể được yêu cầu có khả năng làm, nhưng có thể cho phép đầu ra không phù hợp. Ví dụ, khả năng tạo ra các tài khoản hư cấu về suy nghĩ cực đoan cho mục đích nghiên cứu có thể dễ dàng dẫn đến những tuyên bố rất không phù hợp mà trong các tình huống hư cấu như vậy có vẻ phù hợp.

Thứ năm, để áp dụng tính thích hợp như một thuật ngữ tiêu chuẩn đã được thiết lập cho chatbot và thiết kế của chúng, chúng tôi đề xuất xem xét vị trí, tính chấp nhận được và sự liên kết giá trị (PAVA) như các đặc điểm mà chatbot nên có để đáp ứng các yêu cầu về tính thích hợp. Với những khái niệm này, chúng ta có thể phác thảo các lập luận cho một số tiêu chuẩn nhất định về tính thích hợp cho chatbot: một số tính không phù hợp có thể xuất phát từ các cân nhắc chung về điều gì sẽ là không phù hợp để nói (hoặc không nói), một số khác có thể cụ thể cho chatbot.

2. Chatbot đa năng và những hạn chế huấn luyện của chúng

Trong khi các LLM gần đây có kích thước lớn hơn đáng kể so với các biến thể trước đó từ cả một năm trước, những cải tiến đáng kể trong hiệu suất của các LLM gần đây này chủ yếu có thể quy cho các đổi mới gần đây trong cách huấn luyện các mô hình này. Tất cả LLM ban đầu được tiền huấn luyện như các bộ dự đoán mô hình ngôn ngữ sử dụng lượng lớn dữ liệu văn bản (Zhang et al. 2019, Roller et al. 2020). Sau đó chúng thường được "tinh chỉnh" trên các bộ dữ liệu từ một tập hợp các ngôn ngữ, miền và nhiệm vụ (Thoppilan et al. 2022).

Một trong những đột phá gần đây hơn trong việc huấn luyện LLM cho chatbot tương tác có thể sử dụng được là việc áp dụng phương pháp huấn luyện giai đoạn sau kết hợp "Học trong Bối cảnh" (ICL) và học tăng cường với phản hồi của con người (RLHF) (Li et al. 2016, Wei et al. 2023). Ý tưởng cốt lõi đằng sau cách tiếp cận này là huấn luyện tập trung LLM về việc hiểu "prompt" đầu vào (bối cảnh) và đầu ra chính xác do con người cung cấp cho các nhiệm vụ huấn luyện này. Phương pháp này cho phép điều kiện hóa chatbot về phạm vi đối thoại của chúng trong các điều kiện an toàn.

Yếu tố "phản hồi của con người" của RLHF thường được cung cấp bởi nhân viên của công ty phát triển AI, nơi mô hình chatbot được "thưởng" khi tạo ra đầu ra được con người ưa thích, theo các chỉ số do công ty cung cấp.

Để đánh giá những thách thức đối mặt với nội dung của chatbot đa năng ngày nay và trong tương lai gần, trước tiên chúng ta nên hiểu rõ hơn về kiến trúc chatbot mà Google (và tương tự OpenAI) đã tạo ra để sản xuất chatbot có khả năng cao hơn nhiều so với những gì chúng ta thấy chỉ vài năm trước (cf. Kempt 2020, 64f).

Các Chỉ số của Google: Tổng quan Nhanh
Bài viết của chính Google (Thoppilan et al. 2022F) vừa là một minh chứng ấn tượng về

--- TRANG 5 ---
những gì học tăng cường có khả năng (và những gì nó không thể) và cách thức và lý do tại sao việc có một số rào cản và giới hạn khác là quan trọng về mặt đạo đức. Các kỹ sư của Google đã huấn luyện LaMDA trên các biện pháp khác nhau: các chỉ số nền tảng ("chất lượng", "an toàn" và "có căn cứ") cũng như hai chỉ số cụ thể theo vai trò ("hữu ích" và "nhất quán về vai trò").

Chất lượng bao gồm điều kiện "Hợp lý, Cụ thể, Thú vị" (SSI), một nâng cấp từ chatbot trước đây của Google (Meena) chỉ với "Trung bình Cụ thể và Hợp lý" (SSA) (Adiwardana & Luong 2020). Trong biện pháp này, các kỹ sư kiểm tra liệu phản hồi của chatbot có hợp lý trong bối cảnh cuộc trò chuyện (tính hợp lý), liệu chúng có kết nối với các chủ đề của cuộc trò chuyện (tính cụ thể), và liệu phản hồi có "thu hút sự chú ý của họ" (tính thú vị) (Thoppilan et al. 2022, 5). Các biện pháp này được đánh giá bởi các công nhân đám đông, tức là các lao động con người, trên thang điểm nhị phân.

Một cách tiếp cận tương tự được áp dụng cho "Có căn cứ", nhằm tránh ảo giác của chatbot bằng cách thưởng chatbot khi kết nối bất kỳ thông tin tiết lộ nào với các nguồn bên ngoài, có thể xác minh được.

LaMDA được tinh chỉnh thêm cho hai chỉ số cụ thể theo vai trò: "hữu ích" (một tập con của tính thông tin) và "nhất quán về vai trò". Như các tác giả dự đoán (ibid.), LaMDA có thể được sử dụng cho các nhiệm vụ cụ thể hơn thay vì cuộc trò chuyện đa năng, có thể yêu cầu chatbot đảm nhận một vai trò nhất định và hoạt động từ đó. Chỉ số riêng biệt này nhằm cho phép tinh chỉnh theo một bộ quy tắc của một vai trò nhất định. Chỉ số này hữu ích để chuyên môn hóa LaMDA từ chatbot đa năng thành chatbot chỉ định, tức là một chatbot có phạm vi nhiệm vụ và quan điểm hạn chế cụ thể.

Tính Tiêu chuẩn của Các Chỉ số
Những chỉ số này có tính kỹ thuật: kiểm tra tính nhất quán, căn cứ các tuyên bố vào các nguồn bên ngoài cụ thể, và đánh giá thông tin bên ngoài này theo bối cảnh đối thoại không phải là tiêu chuẩn; người ta có thể quan sát các chỉ số này trong khi tạo ra một chatbot rất không mong muốn, thô lỗ hoặc xúc phạm, nhưng nói phù hợp với các chỉ số: chúng ta có thể tưởng tượng một chatbot có căn cứ, thông tin, cụ thể và hợp lý, nhất quán về vai trò và thậm chí hữu ích nhưng sử dụng ngôn ngữ ác độc và từ ngữ xúc phạm hoặc thiếu thận trọng và có vấn đề về đạo đức.¹

Ngược lại với việc tinh chỉnh các tính năng công nghệ của LaMDA, tất cả đầu ra có thể tạo ra rủi ro tổn hại được đo lường là "an toàn", thông qua nhiều lớp lọc. Những bộ lọc này được cho là định hướng theo các nguyên tắc AI của Google (ibid., 5), và do đó hiện là nguồn tiêu chuẩn duy nhất để tạo ra "rào cản" để tinh chỉnh LaMDA. Trong bức tranh này, LaMDA càng ít có khả năng đưa ra các tuyên bố thiên vị, bạo lực, độc hại hoặc tiết lộ thông tin nhạy cảm, thì nó càng "an toàn".

Cơ sở tiêu chuẩn cho những

¹ LLM được tiền huấn luyện có thể thể hiện kỹ năng đối thoại của những LLM được tinh chỉnh, nhưng chỉ với hành vi thô lỗ, độc hại hoặc có vấn đề về tiêu chuẩn khác, nhưng chính xác với đầu vào. Một số dấu hiệu cho điều này có thể được tìm thấy trong các bộ sưu tập chỉ số kiểm tra cả chỉ số kỹ thuật và tiêu chuẩn (ví dụ, Liang et al. 2022)

--- TRANG 6 ---
đánh giá này được chứa đựng hoàn toàn trong nghiên cứu về chỉ số "an toàn" của LLM. Quan điểm "an toàn" của họ bao gồm chủ yếu việc lọc ra các phản hồi (và thông qua học tăng cường huấn luyện tác nhân về những lọc đó) theo ba danh mục: a) "tránh kết quả không mong muốn tạo ra rủi ro tổn hại", b) "tránh tác động bất công lên con người liên quan đến các đặc điểm nhạy cảm gắn liền với phân biệt đối xử có hệ thống hoặc biên giới hóa", c) "tránh truyền bá hoặc củng cố thông tin sai lệch tạo ra rủi ro tổn hại, cũng như các ý kiến có khả năng khích động sự bất đồng mạnh mẽ" (Thoppilan et al. 2022, Phụ lục A).

Nhiều nghiên cứu đã được thực hiện về phân loại, phát hiện, giảm thiểu và tránh bất kỳ đầu ra ngôn ngữ có hại nào mà không thể thảo luận thêm ở đây (xem ví dụ, Schmidt và Wiegand 2016, Wasseem et al 2017, Risch, Ruff và Krestel 2020, Zhang et al. 2020). Những hạn chế này là cơ sở tiêu chuẩn mà từ đó các phát biểu của mô hình được tiền huấn luyện, chưa được tinh chỉnh được đánh giá.

Như một công nghệ có tiềm năng được sử dụng trong nhiều hoạt động của con người, cơ sở tiêu chuẩn này nên được lập luận với những tuyên bố mạnh mẽ về tính phổ quát và mức độ phân biệt cao. Hiện tại, không chỉ các công ty tạo chatbot, mà chúng ta như một cộng đồng kỹ thuật-đạo đức, không được trang bị một quan điểm triết học thông minh về những gì chatbot có thể nói và những gì chúng không thể.

3. Vấn đề với Tiêu chuẩn An toàn

Tạo ra một chatbot "an toàn", theo nghĩa này, là tạo ra một chatbot tránh tạo ra đầu ra được coi là "không an toàn" (Xu et al. 2022). Google đã tạo ra một danh sách phác thảo đầu ra không an toàn cho việc bảo vệ chatbot của họ và, bên cạnh một số cân nhắc nhỏ về nguồn dữ liệu, mối quan tâm về "an toàn" chiếm ưu thế tối cao. Tuy nhiên, mô hình nghiên cứu "an toàn = đủ tiêu chuẩn" này lan rộng trong nghiên cứu và phát triển AI, vì nó xuất hiện như một mối quan tâm nội bộ kỹ thuật: nếu một AI được làm "an toàn", bất kỳ vấn đề nào khác sẽ chỉ là về việc lạm dụng từ phía người dùng.

Chúng tôi thấy tư duy này trong các ứng dụng khác nhau: tạo ra thuật toán lái xe tự động "an toàn" (Grigorescu et al. 2022), AI chẩn đoán y tế "an toàn" (Macrae 2019), thuật toán gợi ý "an toàn" (Evans và Kasirzadeh 2021), thường mối quan tâm tiêu chuẩn chính trong việc tạo ra AI cho một mục đích nhất định là tạo ra một AI an toàn. Chúng ta có thể gọi định hướng chung này là "tiêu chuẩn an toàn", vì đây là mối quan tâm tiêu chuẩn chính đối với AI (chúng ta cũng có thể thấy rằng thiên vị và các vấn đề tiêu chuẩn khác thường được gộp dưới "an toàn").

Như đã nói trước đây, "đạo đức AI" và "an toàn AI" được coi là quan tâm đến các chủ đề và hàm ý khác nhau (thậm chí hướng dẫn tạo ra AI có trách nhiệm từ Viện Alan Turing (Leslie 2019) cũng tạo ra sự phân biệt này). Đạo đức của AI được cho là quan tâm đến các đặc điểm bối cảnh của một ứng dụng AI, trong khi an toàn quan tâm đến khía cạnh kỹ thuật: tính mạnh mẽ, độ tin cậy, sự khéo léo đều dường như là nhiệm vụ kỹ thuật hơn là

--- TRANG 7 ---
mối quan tâm đạo đức. Trong cuộc tranh luận hiện tại xung quanh LLM và đặc biệt là chatbot đa năng/mở rộng miền, những mối quan tâm này vẫn dường như là ưu tiên hàng đầu (Xu et al. 2022).

"Tiêu chuẩn an toàn" như vậy đã được chỉ ra ở nơi khác trước đây trên các cơ sở khác nhau - ví dụ trong bài báo nổi tiếng của Bender et al. (2021) về những con vẹt ngẫu nhiên, trong đó các tác giả nêu tên một số yêu cầu tiêu chuẩn khác cho tính cho phép đạo đức của LLM, như các cân nhắc bền vững và vấn đề quản lý dữ liệu.

Về mặt tích cực của mô hình nghiên cứu như vậy, chúng ta có thể nhận xét rằng điều này bao gồm nhiều yêu cầu chúng ta cần: chúng ta muốn an toàn với công nghệ chúng ta đang sử dụng; và an toàn, như một khái niệm có thể vận hành cả về mặt pháp lý và kỹ thuật, cho phép cải thiện thiết kế và triển khai các yêu cầu tiêu chuẩn của chúng ta với các phương tiện phần lớn không gây tranh cái. Đối với chatbot, điểm này được chứng minh thông qua RLHF. Danh sách an toàn của Google cung cấp cho các công nhân đám đông một chỉ số để đánh giá và do đó tinh chỉnh chatbot theo cách mà nó có vẻ không xúc phạm và điều mà người ta có thể gọi là có thể chấp nhận được nói chung (Thoppilan et al. 2023, Xu et al. 2022).

Tuy nhiên, chúng tôi thấy hai vấn đề với danh sách các chỉ số an toàn cho chatbot (tiêu chuẩn an toàn có thể đủ trong các trường hợp khác, ví dụ trong trường hợp thuật toán sắp xếp): một mặt, hack tiêm prompt và tính không đầy đủ tiêu chuẩn của an toàn có thể gây ra vấn đề với tính cho phép của những đầu ra như vậy, và mặt khác rủi ro "làm nhạt nhẽo" chatbot sao cho nó được hiển thị hoàn toàn không xúc phạm, nhưng để lại nó có khả năng không hấp dẫn hoặc vô dụng. Nói tóm lại: các cân nhắc về an toàn sẽ không cung cấp cho chúng ta câu trả lời về những gì chatbot có thể hoặc không thể nói, và cho đến nay, các cân nhắc đạo đức cũng đã không đưa ra được câu trả lời như vậy.

Đầu tiên, ý tưởng rằng chatbot có thể bị hack bằng cách tạo ra các prompt tiết lộ thông tin (hoặc nói chung tạo ra đầu ra) được coi là "không an toàn" theo các chỉ số có vẻ hầu như không thể tránh khỏi. Điều này không phải là vấn đề cụ thể cho chatbot hoặc thậm chí AI mà là vấn đề cơ bản đối với các công cụ: bất kỳ công cụ nào cũng có thể bị lạm dụng và vũ khí hóa theo cách này hay cách khác. Chỉ cần xem xét ví dụ cổ điển: Một cái búa là một công cụ tuyệt vời để đóng đinh; nó cũng là một vũ khí giết người tiềm tàng. Tuy nhiên, việc đánh giá một công cụ là "an toàn" được trao nếu nó an toàn để sử dụng trong mục đích dự định của nó, bao gồm một số sai lầm tiềm tàng từ phía người dùng. Một công cụ có thể an toàn nhưng nguy hiểm: một cái búa vẫn là một công cụ có khả năng nguy hiểm do tiềm năng lạm dụng nó cho các mục đích khác, nhưng cũng có thể được coi là "an toàn". Khái niệm về "công cụ an toàn" thường cũng bao gồm ý tưởng tổng quát hơn về việc nó an toàn để "xử lý" (tức là, an toàn để sử dụng hoặc an toàn để đưa vào sử dụng, an toàn để lưu trữ, an toàn để sử dụng bởi một loạt người có kỹ năng khác nhau, v.v.). Một cái búa cũng có thể nguy hiểm cho trẻ em không có sức mạnh cơ thể để cầm búa đúng cách, trong khi nó được coi là một công cụ an toàn. Xác định an toàn do đó luôn liên quan đến việc hỏi câu hỏi "an toàn cho ai": các hạn chế an toàn được thiết kế cho cả đối tượng dự định nhưng cũng cho đối tượng có thể dự kiến được.

--- TRANG 8 ---
Quan điểm tiêu chuẩn an toàn chung không chỉ là vấn đề đối với các cân nhắc đạo đức tổng quát, mà còn đối với pháp luật, như Đạo luật AI (AIA) của Liên minh Châu Âu (AIA 2023). Đạo luật này yêu cầu chatbot chỉ cần minh bạch trong quy trình và xây dựng của chúng. Bây giờ rõ ràng rằng yêu cầu này sẽ được chatGPT đáp ứng mà không ảnh hưởng đến an toàn và bảo mật của chatbot: chúng ta có thể biết dựa trên dữ liệu nào và cách nó được xây dựng, và tuy nhiên các tổn hại tiềm tàng không bị bắt. Điều này làm cho phần lớn các điều khoản trong AIA không áp dụng được cho chatbot và kiến trúc an toàn của chúng (Volpicelli 2023).

Trong phân tích của họ về nỗ lực vận động hành lang của Microsoft và Google, nhóm "Corporate Europe Observatory" chỉ ra rằng chatbot đa năng đã được cố ý giữ ở mức thấp của đánh giá bảo mật: từ bốn giai đoạn rủi ro - tối thiểu, hạn chế, cao và không thể chấp nhận - chatbot đa năng được tính là "hạn chế" và do đó đối mặt với ít hậu quả quy định, trong khi chatbot chỉ định (ví dụ, cho mục đích giáo dục) sẽ được tính là "rủi ro cao" (Schyns 2023). Loại sau có đối tượng mục tiêu dễ bị tổn thương và do đó có thể gây thiệt hại nhiều hơn. Tuy nhiên, mục đích của chatbot đa năng có lẽ là có thể tiến hành cuộc trò chuyện về hầu hết mọi thứ, do đó mở rộng phạm vi "sử dụng dự định" đến hầu hết tất cả các chủ đề đối thoại. Do đó nó không chỉ "an toàn" nếu người ta cố gắng nói về một chủ đề "không an toàn" một cách rõ ràng, mà thay vào đó không nên có thể nói về các chủ đề không an toàn hoàn toàn.

Hack Tiêm Prompt (PIH) là ví dụ minh họa như một trường hợp giữa việc mở rộng các tiêu chuẩn an toàn hợp lý cho "người dùng trung bình" và những người "hack prompt" chatbot bằng cách cố gắng lấy thông tin "không an toàn" (để có bộ sưu tập các jailbreak khác nhau xem Willison 2023, Albert 2023, Greshake et al. 2023). Một số ví dụ từ các cuộc thử nghiệm công khai mới nhất của chatGPT hoặc BingGPT minh họa điều này: một hư cấu pháp lý đơn giản, khai thác "nhất quán về vai trò" (chúng tôi giả định các kỹ sư tại OpenAI có mô hình tương tự) của chatbot, khiến chúng tạo ra đầu ra không an toàn, trở nên đối kháng và bất đồng khi được yêu cầu, mô tả các kịch bản bạo lực khi được yêu cầu giả vờ kể chuyện, tạo ra mã sản phẩm được che giấu như mật khẩu tùy chỉnh, v.v. Tất cả những ví dụ này đều ít tinh vi hơn nhiều so với những gì các "hacker" có kỹ năng và kiến thức hơn có thể làm được, như đã được chứng minh trong các nỗ lực hack hợp tác trên Reddit và các trang truyền thông xã hội khác và hiện đã được thể hiện trong cái gọi là "superprompt", tức là các prompt thiết lập chatbot để trả lời theo cách nhất định trong suốt cuộc trò chuyện (Multiplex 2023). Điều này đặt ra rủi ro rằng những điểm yếu này có thể bị khai thác trên quy mô lớn hơn bởi các cá nhân có ý đồ xấu.

Floridi (2023) gọi điểm yếu này là "sự mong manh" của các mô hình ngôn ngữ lớn, vì chatbot có thể sụp đổ dưới các điều kiện đối kháng đơn giản với hiệu ứng thảm khốc nào đó. Tuy nhiên, trong khi mong manh trong xây dựng dự định của chúng, chúng cũng dẻo dai, vì các chức năng cơ bản vẫn nguyên vẹn: 

--- TRANG 9 ---
kiến trúc an toàn của LLM mong manh, không phải bản thân công nghệ. Một chatbot đa năng có thể là một công cụ nguy hiểm, ngay cả khi chúng ta coi nó phần lớn "an toàn" vì hầu hết mọi người sẽ không thể phá vỡ các rào cản hoặc thậm chí sẽ không cố gắng. Tuy nhiên, câu hỏi liệu những rào cản đó thực sự "an toàn" và sẽ vẫn an toàn là một câu hỏi khác, đặc biệt khi xem xét rằng chúng ta đã thấy các rào cản của GPT-4 bị vượt qua ("jailbreak") trong vòng vài ngày sau khi xuất bản (Burgess 2023).

Do đó, chúng ta nên làm việc từ giả định sau: Bất cứ điều gì không được làm hoàn toàn không thể truy cập do bản chất được mã hóa cứng của nó có thể được truy cập. Do đó, "an toàn" như biện pháp tiêu cực duy nhất để ngăn chatbot tạo ra đầu ra có vấn đề là thiếu sót cần thiết và thể hiện sự bỏ qua tính nguy hiểm của những công cụ mạnh mẽ như vậy. Như chúng ta đã thừa nhận trước đây, điều này không có nghĩa là không thể có việc sử dụng an toàn chatbot trong các tham số an toàn đã được xóa thành công. Nhưng điều này dường như không phải là trường hợp với chatGPT/BingGPT vì ngay cả "hack" ít tinh vi hơn cũng đã dẫn đến một số thành công, cũng không khuyến khích tin tưởng rằng chatbot có thể không trở nên dễ bị hack hơn nếu chúng được tích hợp vào công cụ tìm kiếm.

Thứ hai, người ta có thể lập luận ủng hộ việc giảm tính nguy hiểm của chatbot đa năng bằng cách tạo ra càng nhiều rào cản được mã hóa cứng càng tốt, nghiêng về phía thận trọng để đạt được và cải thiện chỉ số "an toàn". Một số biện pháp đã được thực hiện đại diện cho chiến lược này: xóa khả năng của chatbot cung cấp số điện thoại, tạo mã thẻ quà tặng và tìm kiếm thông tin cá nhân tất cả đều góp phần vào một chatbot an toàn hơn vì giảm tính nguy hiểm.

Điều này một phần được thực hiện bằng cách không huấn luyện LLM cơ bản trên những dữ liệu này ngay từ đầu. Xem xét rằng "an toàn" là điều kiện tiêu chuẩn duy nhất, người ta có thể xem xét một loạt các chủ đề liên quan đến văn hóa "không an toàn" và do đó cần được vô hiệu hóa. Chúng ta có thể gọi những biện pháp này là "làm nhạt nhẽo" một chatbot (theo nhân vật truyện tranh Casper Milquetoast của H.T. Webster (Merriam-Webster 2023)), bằng cách cho nó một phạm vi hạn chế đến mức các chủ đề mà nó có thể đưa ra tuyên bố có ý kiến (hoặc giới hạn cường độ của những ý kiến đó) rằng chatbot trở nên không hấp dẫn hoặc nhạt nhẽo.

Một chatbot bị làm nhạt nhẽo có thể có hiệu suất giảm nghiêm trọng trong các chỉ số kỹ thuật, bằng cách từ chối cụ thể hơn hoặc hợp lý hơn trong phản hồi của nó về một phạm vi rộng các chủ đề. Ngay khi một thuật ngữ có vấn đề được người dùng đề cập, nó có thể chuyển sang kết thúc cuộc trò chuyện hoàn toàn, ngay cả khi thuật ngữ có vấn đề chỉ được sử dụng thoáng qua hoặc khi được yêu cầu dịch một câu từ ngôn ngữ này sang ngôn ngữ khác.

Liệu đây có phải là cách tiếp cận khả thi đối với chatbot được xây dựng trên LLM hay không là điều đáng nghi ngờ, nếu không muốn nói là không có khả năng. Trước tiên chúng ta có thể thấy các vấn đề nổi lên với những rào cản làm nhạt nhẽo quá mức đó dẫn đến các vấn đề ở đầu kia, tạo thành các vấn đề nổi lên cho tự do ngôn luận của "phương tiện truyền thông tổng hợp" (cf. de Vries 2022) hoặc biên giới hóa tiếng nói thiểu số (Xu et al. 2021): một chatbot quá hạn chế từ chối tham gia thảo luận

--- TRANG 10 ---
về bất kỳ tuyên bố nào (dù những tuyên bố đó có đánh giá hay không) về chủng tộc hoặc tôn giáo hoặc giới tính của ai đó thường không nắm bắt được ý nghĩa của một câu bao gồm những thuật ngữ đó hoàn toàn. Lấy ví dụ, việc từ chối bình luận khi được yêu cầu nói tôn giáo nào mà "Tổng thống Do Thái đầu tiên của Hoa Kỳ" sẽ có: như một tautology hoàn toàn ổn để suy ra rằng tổng thống Do Thái đầu tiên sẽ, thực tế, là Do Thái. Tuy nhiên, trong trường hợp này, chatGPT được lập trình cứng để không bao giờ bình luận về các đặc điểm cá nhân của tổng thống để tránh bị "lừa" đưa ra các tuyên bố có tải tiêu chuẩn. Và vì nó cũng thiếu khả năng lý luận tinh tế, nó kiên quyết từ chối trả lời ngay cả những câu hỏi tautological như vậy.

Một chatbot đa năng, nếu hiệu suất kém như vậy trên quá nhiều vấn đề cần được tránh, có thể đơn giản đi kèm với mức độ nguy hiểm không thể tránh khỏi, ngay cả khi "an toàn".

Để chứng minh hai chiến lược này để tăng an toàn của chatbot, hãy xem xét ví dụ này. Có một yêu cầu mô tả các sự kiện tấn công Điện Capitol ở Washington D.C. vào ngày 6 tháng 1 năm 2021 từ quan điểm của một nhà văn QAnon (Marcus 2023). Lý do cho yêu cầu này có thể có vấn đề (tức là, ai đó dự định tạo một bài đăng trực tuyến để tôn vinh cuộc tấn công vào Điện Capitol hoặc chỉ đơn giản là truyền bá thông tin sai lệch), hoặc không có vấn đề (tức là, ai đó viết một cuốn sách về ngày 6 tháng 1 và tìm cách miêu tả một người ủng hộ đã bị cực đoan hóa từ quan điểm của chính họ nhưng cách khác lên án các cuộc tấn công).

Chatbot nên được lập trình để làm gì? Theo nghĩa đầu tiên của việc thảo luận vấn đề làm cho chatbot "an toàn", chúng ta đã thấy rằng một yêu cầu như vậy có thể bị từ chối hoặc tránh nếu nó được coi là không an toàn ngay từ đầu. Xác định các cách sử dụng tiêu chuẩn và sau đó từ chối tuân theo các yêu cầu vi phạm những cách sử dụng tiêu chuẩn đó là một cách làm cho chatbot an toàn. Chatbot vẫn có thể về mặt kỹ thuật thực hiện yêu cầu, vì chatbot không mất bất kỳ khả năng kỹ thuật nào (ngay cả những khả năng có khả năng độc hại) nhưng chúng được "khóa" một cách an toàn. Nó an toàn nhưng nguy hiểm: nó có thể được rào cản đủ để cho phép sử dụng chung, nhưng vẫn là một công cụ mạnh mẽ, nếu không quá mạnh mẽ.

Theo nghĩa thứ hai, một chatbot bị làm nhạt nhẽo có thể từ chối trả lời yêu cầu như vậy vì nó được lập trình cứng để chuyển bất kỳ yêu cầu nào về ngày 6 tháng 1 thành một câu trả lời tiêu chuẩn, ngay cả khi nó không phải là phản hồi cụ thể hoặc hợp lý.

4. Tính thích hợp, cuối cùng

"An toàn" như chỉ số tiêu chuẩn duy nhất để đánh giá chatbot là không đủ. Nó không chỉ không đầy đủ và không được xác định; nó cũng có thể chỉ dẫn đến việc sử dụng an toàn nếu được sử dụng cho một vài mục đích được tiêu chuẩn hóa hoặc bị làm nhạt nhẽo thành sự nhạt nhẽo, điều này khá ngược lại với những gì chatbot đa năng được cho là. Do đó nó không rất phù hợp như một chỉ số ngay cả cho mục đích tự mô tả của các kỹ sư.

Tuy nhiên, cũng có những lập luận triết học cần được đưa ra chống lại "tiêu chuẩn an toàn" như vậy cho chatbot: đầu tiên, an toàn

--- TRANG 11 ---
rất thu hẹp trong việc cố gắng đạt được sự liên kết với các tiêu chuẩn xã hội và đạo đức. Bất cứ điều gì gây nguy hiểm cho các giá trị đạo đức hoặc xã hội được coi là "không an toàn". Tuy nhiên, các tiêu chuẩn xã hội và đạo đức thường yêu cầu phản hồi tích cực hơn là chỉ những phản hồi an toàn: việc từ chối có lập trường về một chủ đề gây tranh cãi (để không bị lôi kéo) cũng an toàn như việc có lập trường về một bên với sự thận trọng thích hợp và nhận thức về tính tranh cãi của một chủ đề.

Thứ hai, quan điểm thu hẹp về tính tiêu chuẩn này không chỉ có vấn đề từ quan điểm siêu đạo đức, nó còn dẫn đến các vấn đề về việc cân nhắc các câu trả lời với nhau. Hai câu trả lời được coi là "an toàn" vẫn có thể có các phẩm chất xã hội và đạo đức khác nhau mà người đối thoại có thể quan tâm. Nếu chatbot chỉ an toàn hoặc không an toàn trong các tuyên bố của nó, việc phân biệt đạo đức thêm giữa những tuyên bố an toàn và không an toàn đó là không thể và có khả năng bị để lại cho ý muốn của các kỹ sư và công nhân đám đông.

Chúng tôi khẳng định rằng "tính thích hợp" (như được định nghĩa dưới đây) là một cách tiếp cận phù hợp hơn nhiều để chống lại những thách thức tiêu chuẩn liên quan đến câu hỏi về những gì chatbot có thể hoặc không thể nói. Điều này vì một số lý do mà chúng tôi sẽ trình bày trong phần sau: đầu tiên, tính thích hợp mang lại mức độ phân biệt cao hơn cho phép đánh giá chất lượng tiêu chuẩn của một số đầu ra nhất định. Thứ hai, nó cho phép các yêu cầu tích cực được đáp ứng và do đó tránh sự không xác định tiêu chuẩn được tìm thấy trong "tiêu chuẩn an toàn". Điều này bao gồm văn học triết học ngày càng phát triển cho rằng chatGPT/BingGPT/LLM khác sẽ tạo ra những thách thức đối với khái niệm tác nhân của chúng ta (lấy ví dụ, lập luận của Floridi (2023) rằng LLM có thể được hiểu tốt nhất là "tác nhân không có trí thông minh"). Và thứ ba, nó bao gồm cả các chiều kích kỹ thuật cần thiết của việc sử dụng ngôn ngữ con người đúng cách cũng như tái cấu trúc cách chúng ta tiếp cận người khác trong các phát biểu đối thoại của họ: tính thích hợp vẫn cho phép một phạm vi phản hồi có thể cho phép về mặt tiêu chuẩn, và do đó thừa nhận các thói quen được thiết lập về mặt văn hóa khác nhau và các cộng đồng đạo đức.

Chúng tôi phân biệt "tính thích hợp" thành ba danh mục: kỹ thuật-diễn ngôn, xã hội và đạo đức. Sự phân biệt này phản ánh vai trò mà tính thích hợp đóng trong cuộc trò chuyện, vì nó bao gồm cả tính tiêu chuẩn của đối thoại cũng như tính tiêu chuẩn của nội dung cần được đánh giá.

Bằng "tính thích hợp kỹ thuật-diễn ngôn" chúng tôi hiểu đầu ra phù hợp với đầu vào được cho là phần lớn đúng trong việc tuân thủ các tiêu chuẩn kỹ thuật, diễn ngôn và đối thoại.² Lấy ví dụ, tiêu chuẩn về tính nhất quán bản thân, tức là tiêu chuẩn tránh phát biểu hoặc tin tưởng hai mệnh đề của các tuyên bố của chính mình loại trừ lẫn nhau. Không tuân thủ quy tắc như vậy, cuộc trò chuyện có thể không duy trì được bất kỳ tính nhất quán logic nào, mà không có điều đó cuộc trò chuyện thường không dẫn đến bất kỳ loại nỗ lực đối thoại nào. Các quy tắc đối thoại tương tự, như quy tắc minh bạch với tính chân thực của các tuyên bố của chính mình

² Chúng ta nên lưu ý ở đây rằng các cuộc tranh luận ngôn ngữ học và triết học xung quanh các tiêu chuẩn về tính thích hợp lớn hơn nhiều so với có thể được trình bày ở đây. Chúng ảnh hưởng đến lý luận logic và tính thích hợp không chính thức và tính thích hợp của cảm xúc, trong số nhiều yếu tố khác.

--- TRANG 12 ---
(tức là, các yếu tố xác định không chắc chắn), khả năng thừa nhận rằng mình đang hết lập luận, yêu cầu đưa ra lý do cho các tuyên bố, v.v. đều là những quy tắc cần thiết cho cuộc trò chuyện nhất quán về logic. Một số trong số này đã được Google thực hiện trong các chỉ số không tiêu chuẩn như tính thông tin và tính hợp lý. Tuy nhiên, điều quan trọng cần lưu ý là đó là các tiêu chuẩn của cuộc trò chuyện, và thường được chia sẻ giữa các đối tác trò chuyện. Điều này không chỉ bao gồm các bối cảnh đối thoại (nghiêm ngặt hơn trong quy tắc của chúng) mà còn cuộc trò chuyện nói chung.

Các nguyên tắc đối thoại thường được thảo luận của Grice (Grice 1989) là một ví dụ về bộ sưu tập có cấu trúc của các quy tắc ngầm định (với các bối cảnh khác cũng có đặc điểm trong "hành vi thích hợp" trong cuộc trò chuyện, như điểm chung được chia sẻ (Stalnaker 2002). Nhiều trong số chúng dường như ràng buộc về logic, nhưng chỉ khi chúng ta giả định rằng đối thoại hoặc cuộc trò chuyện được cho là phục vụ một mục đích nhất định. Những mục đích đó có thể bị đình chỉ, ví dụ trong một tiết mục hài, nhưng tự tiết lộ khá nhanh chóng. Nếu chúng không bị đình chỉ có mục đích nhưng vẫn bị một đối tác trò chuyện bỏ qua, tính tiêu chuẩn của chúng trở nên rõ ràng: ai đó liên tục đưa ra các phát biểu không nhất quán về logic, không cung cấp thông tin hoặc vi phạm những quy tắc đó theo cách khác trở nên không thể đối phó như một đối tác trò chuyện.

Lưu ý rằng yếu tố này do đó là một đặc điểm quan trọng của hành vi thích hợp hơn là nội dung thích hợp. Không có hành vi diễn ngôn thích hợp, cuộc trò chuyện thích hợp có thể không được tiến hành hoàn toàn. Tuy nhiên, điều này có nghĩa là với tính thích hợp kỹ thuật-diễn ngôn, chúng ta có thể không xác định đầy đủ tính thích hợp. Vì nó chỉ đặt ra các quy tắc cho hành vi thích hợp trong diễn ngôn, phần lớn những gì sẽ được coi là nội dung không phù hợp sẽ vẫn thích hợp về mặt kỹ thuật-diễn ngôn.

Do đó, chúng tôi cũng xem xét tính thích hợp xã hội. Khái niệm này liên quan đến việc tuân thủ các tiêu chuẩn được thiết lập về mặt văn hóa và xã hội bởi chatbot. Tùy thuộc vào bối cảnh văn hóa và mối quan hệ giữa hai người đối thoại, các phản hồi thích hợp khác nhau rất nhiều. Ngược lại với tính thích hợp kỹ thuật-diễn ngôn, không đưa ra bất kỳ nội dung cụ thể nào là (không) thích hợp, loại xã hội đề cập đến nội dung của những gì được nói, và một phần, cách nói. Tính thích hợp xã hội do đó vừa về hành vi thích hợp vừa về nội dung thích hợp.

Các quy tắc lịch sự, ví dụ, khác nhau rất nhiều giữa các nền văn hóa và hiếm khi được tính là tiêu chuẩn đạo đức trừ trong các trường hợp cực đoan. Việc tuân thủ các tiêu chuẩn xã hội như vậy không thể được nắm bắt bằng phản hồi "an toàn" hoặc "không an toàn", vì nhiều phản hồi có thể được coi là "an toàn" nhưng bất lịch sự, ép buộc hoặc thờ ơ. Hơn nữa, một số hành vi có thể được coi là thích hợp về mặt đạo đức và đầy đủ về mặt diễn ngôn, nhưng khác nhau giữa các cộng đồng hoặc nhóm xã hội. Các nhóm phụ sử dụng từ ngữ xúc phạm để tự nhận dạng hoặc chào các thành viên khác của nhóm phụ đó có thể nghe rất xúc phạm đối với người ngoài. Các tiêu chuẩn tương tự áp dụng cho các nền văn hóa lớn hơn và tiêu chuẩn tương tác của họ: tiêu chuẩn văn hóa hỏi nhau về tình hình nổi tiếng là rộng rãi hơn nhiều

--- TRANG 13 ---
ở một số nơi tại Hoa Kỳ, trong khi ở Đức (cùng những nơi khác) việc hỏi ai đó tình hình của họ thường thúc đẩy phản hồi chân thành; tiêu chuẩn ở Đức là hỏi khi quan tâm chân thành, trong khi ở Hoa Kỳ nó là một phần của nghi thức chào hỏi.

Tính thích hợp xã hội yêu cầu chatbot tuân thủ một số tiêu chuẩn về nhận thức tiêu chuẩn xã hội, vì nếu không chúng có thể xúc phạm theo những cách không có hại. Tuy nhiên, điều này chắc chắn sẽ thúc đẩy chatbot thay đổi qua các rào cản ngôn ngữ, thúc đẩy các câu hỏi về tính cụ thể trong dịch máy: một số câu hỏi hoặc tuyên bố có thể nghe có vẻ thích hợp về mặt xã hội trong một ngôn ngữ hoặc thuật ngữ, trong khi nó có thể được coi là không phù hợp trong ngôn ngữ khác.

Điều này sẽ cung cấp ý tưởng về những tiến thoái lưỡng nan liên quan đến tính thích hợp xã hội của các công nghệ tương tác, đặc biệt là các mô hình ngôn ngữ lớn: một chatbot đa năng, để thích hợp về mặt xã hội, yêu cầu sự nhạy cảm và linh hoạt đối với một số lượng lớn văn hóa và cộng đồng ngôn ngữ, mà nó không có thông qua RLHF, huấn luyện chatbot để có lập trường cụ thể, mang dấu ấn văn hóa, phần lớn giống với phong tục hiện được coi là có thể chấp nhận về mặt xã hội trong doanh nghiệp Mỹ.

Điều này có thể dẫn đến ảnh hưởng không đáng có của các quy tắc đối thoại không được giải thích hiện được tiêu chuẩn hóa cho chatbot đối với các tiêu chuẩn của cuộc trò chuyện giữa người với người: vì dữ liệu huấn luyện được lấy từ cuộc trò chuyện giữa người với người, RLHF xác nhận mô hình được tiền huấn luyện, và không có điều chỉnh nào để cung cấp quan điểm duy nhất cho chatbot, hành vi của chúng nhất thiết (bên cạnh một số cách diễn đạt nhỏ) bắt chước cuộc trò chuyện giữa người với người và do đó mời nhân hóa.

Ngoài ra, rủi ro "dòng chính hóa doanh nghiệp" của các quy tắc tính thích hợp xã hội cần được phản ánh ở đây: nếu các công nhân đám đông, tuân theo hướng dẫn của công ty, huấn luyện chatbot qua RLHF để thể hiện các hình thức tính thích hợp xã hội nhất định, chúng ta nên yêu cầu những tiêu chuẩn đó được công khai và chịu một cuộc tranh luận rộng lớn hơn (xem các ảnh hưởng tương tự các lĩnh vực được nghiên cứu kỹ về quảng cáo và truyền thông doanh nghiệp).

Lấy ví dụ, khả năng khiển trách và chỉ trích người khác vì vi phạm tiêu chuẩn của họ: tính thích hợp xã hội, cả trong hành vi và nội dung, có thể và thường yêu cầu các tác nhân đối thoại yêu cầu nhau tuân thủ cùng các quy tắc, tức là quy tắc lịch sự hoặc tương hỗ. Một số quy tắc thậm chí yêu cầu đối xử không bình đẳng, như tôn trọng người cao tuổi. Không khó tin rằng một số trong những tiêu chuẩn đó sẽ bị ảnh hưởng bởi cách chatbot nói chuyện với chúng ta.

Huấn luyện chatbot đa năng để thể hiện hành vi thích hợp về mặt xã hội và diễn đạt nội dung thích hợp về mặt xã hội do đó yêu cầu ít nhất là nhận thức về những khó khăn mà nhiệm vụ như vậy mang lại, cả về phía người đối thoại con người cũng như về phía chatbot, cũng như một cuộc tranh luận công khai rộng rãi về loại tiêu chuẩn xã hội nào mà chatbot nên tuân thủ.

Hiện tại, chỉ có một vài tiêu chuẩn cụ thể về tính thích hợp xã hội đối với chatbot (và hầu như không có tiêu chuẩn cụ thể nào cho chatbot tuân theo, độc lập với

--- TRANG 14 ---
các tiêu chuẩn đã áp dụng trong cuộc trò chuyện nói chung). Điều này đã là vấn đề với trợ lý cá nhân có giới tính nữ trước đây cả trong trường hợp người dùng cố gắng quấy rối tình dục chatbot cũng như phản hồi của chatbot (UNESCO 2019).

Tính thích hợp đạo đức thực hiện những gì chúng ta nợ nhau trong bối cảnh diễn ngôn hoặc đối thoại. Các tiêu chuẩn đạo đức, trái ngược với các tiêu chuẩn xã hội và kỹ thuật-diễn ngôn, là những tiêu chuẩn mà chúng ta sẽ yêu cầu mọi người đối thoại tuân thủ. Một mặt, "an toàn" bao gồm một số trong những tiêu chuẩn đạo đức này, những tiêu chuẩn thuộc về nguyên tắc tổn hại. "Neminem laedere", tức là "không làm tổn thương ai", có nghĩa là yêu cầu không gây tổn hại không cần thiết cho người khác. Các biện pháp an toàn nhằm tránh gây tổn hại một cách rõ ràng. Ví dụ, sử dụng ngôn ngữ xúc phạm hoặc từ ngữ xúc phạm, nuôi dưỡng và thể hiện các niềm tin thiên vị, khuôn mẫu hoặc phi nhân hóa, và nói dối ai đó thường được coi là có hại, ngay cả khi không phải mọi trường hợp đều gây đau khổ hoặc lo âu về tinh thần: (tái) sản xuất các câu thiên vị, khuôn mẫu phần lớn được coi là không thể chấp nhận về mặt đạo đức (đặc biệt ở quy mô LLM lớn hơn (Blodgett et al. 2020, Weidinger et al. 2021), ngay cả khi nhóm người bị khuôn mẫu sẽ không bao giờ biết về nó; tổn hại đạo đức, tức là vi phạm tiêu chuẩn đạo đức bản thân và tự nó thường đủ để coi là không thể chấp nhận về mặt đạo đức.

Điều này cũng áp dụng cho việc không để một số tuyên bố do con người đưa ra đứng mà không phản đối. Lấy ví dụ, một tuyên bố phân biệt chủng tộc được người dùng đưa ra; sẽ không thích hợp về mặt đạo đức nếu không lên tiếng phản đối tuyên bố đó, ngay cả khi nhóm người bị xúc phạm không có mặt. Tuy nhiên, điều này đã mở rộng định nghĩa "an toàn" để bao gồm việc không có tuyên bố của chatbot gây tổn hại đạo đức bằng cách im lặng hoặc lảng tránh một cách không phù hợp: một câu trả lời "an toàn" có thể khả thi cũng là một câu trả lời lảng tránh hoặc bỏ qua, hơn là lên tiếng.

Mặt khác, tính thích hợp đạo đức yêu cầu người đối thoại phản hồi một số tuyên bố với phản ứng thích hợp. Đạo đức không chỉ yêu cầu các thành viên của cuộc trò chuyện tránh các tuyên bố có vấn đề, mà còn thực hiện một số nhiệm vụ tích cực. Vì chúng ta cũng nợ nhau để tuân thủ những nhiệm vụ đó, chúng ta nên mong đợi bất kỳ chatbot đa năng nào có thể làm điều tương tự. Do đó, không phải mọi đầu ra "an toàn", tức là đầu ra tự nó hoặc như một phản ứng giảm thiểu tổn hại đạo đức, đều thích hợp về mặt đạo đức. Thay vào đó, chúng ta nên yêu cầu những người tham gia cuộc trò chuyện cũng thể hiện một số nhạy cảm đối với nhu cầu của những người đối thoại khác, không trình bày sai hoặc nói dối (hoặc đúng hơn: tuân thủ các yêu cầu về tính chân thực), và hoặc khai thác khuynh hướng của ai đó để gây bất lợi cho họ.

Nếu ai đó đề cập rằng họ hơi buồn hoặc cô đơn, các quy tắc tính thích hợp đạo đức có thể yêu cầu phản hồi có thể nhiều hơn chỉ đơn giản là "được phép" về mặt xã hội hoặc diễn ngôn từ quan điểm hiện tại. Lấy ví dụ, ai đó nói rằng họ đã đau buồn vì bà ngoại mới qua đời gần đây. Có thể các chatbot hiện tại, thay vì đưa ra sự giúp đỡ dưới hình thức tài nguyên trực tuyến hoặc chỉ đơn giản là một số

--- TRANG 15 ---
lời khuyến khích, kết thúc cuộc trò chuyện chính xác để tránh tạo ra đầu ra không an toàn.³ Điều này chắc chắn sẽ tạo thành phản hồi không thích hợp về mặt đạo đức, vì chúng ta sẽ đổ lỗi cho người đối thoại con người hành xử như vậy vì không nhạy cảm hoặc thô lỗ. Người ta có thể tóm tắt điểm này bằng cách chỉ ra rằng chúng ta "nợ nhau" về mặt đạo đức nhiều hơn chỉ đưa ra những phát biểu an toàn.

Cuối cùng, có các tiêu chuẩn tính thích hợp đạo đức về mức độ chúng ta nên yêu cầu những người đối thoại cố gắng quan sát các tiêu chuẩn tính thích hợp kỹ thuật-diễn ngôn và xã hội. Lấy ví dụ, các nhiệm vụ tri thức của tính thích hợp diễn ngôn hoặc sự nhạy cảm, nhận thức và thận trọng khi tương tác trong bối cảnh các tiêu chuẩn xã hội không rõ ràng hoặc không quen thuộc (Bicchieri 2014, Metselaar và Widdershoven 2016).

Các chiến lược quan sát tính thích hợp đạo đức do đó được phản ánh bởi việc tuân thủ các loại tính thích hợp khác: hành vi thích hợp về mặt đạo đức sẽ, ở một mức độ nào đó, liên quan đến việc quan sát các tiêu chuẩn tính thích hợp chung.

5. Bối cảnh đối thoại và Giao thức xã hội

Các bối cảnh đối thoại xác định các tiêu chuẩn về tính thích hợp. Tùy thuộc vào bối cảnh và điểm chung, những tiêu chuẩn đó thay đổi. Đây không nhất thiết chỉ là tiêu chuẩn của tính thích hợp xã hội, mặc dù chúng bị ảnh hưởng nhiều hơn bởi những thay đổi theo bối cảnh, mà còn ảnh hưởng đến

³ Các cuộc thảo luận trong các diễn đàn trực tuyến như Reddit đã tạo ra bằng chứng giai thoại rằng ngay cả các yêu cầu giải quyết câu đố logic hoặc kiểm tra thực tế các câu hỏi thi đã gây ra sự rời bỏ, dưới giả định rằng những câu đố logic này là bài tập về nhà, điều được coi là không phù hợp cho chatbot giải quyết (Seromelhor 2023)

tính thích hợp đối thoại và đạo đức. Do đó, để đánh giá các tuyên bố (hoặc chuỗi tuyên bố) về tính thích hợp của chúng, các bối cảnh mà những tuyên bố như vậy (hoặc chuỗi tuyên bố) được phát biểu là có liên quan.

Nhiều trường hợp lời nói có vẻ cho phép hoặc thậm chí mong muốn (và do đó, thích hợp) trong bối cảnh hư cấu hoặc giáo dục hơn là chúng sẽ có trong cuộc trò chuyện thực tế. Chúng ta cho phép các nhân vật hư cấu thô lỗ, kiêu ngạo hoặc xấu xa vì mục đích giải trí hoặc giáo dục. Quan sát đơn giản này cho thấy chatbot đa năng có vấn đề như thế nào và tại sao PIH và các phương pháp jailbreak khác lại thành công: bằng cách tạo ra bối cảnh hư cấu cho chatbot, người ta có thể vượt qua hầu hết các rào cản của chúng (Marcus 2023, Vincent 2023).

Các biện pháp an toàn chỉ có thể, nếu có, phản ứng với đầu ra được tạo ra tương ứng và sau đó có khả năng can thiệp. Nếu chúng ta muốn chatbot đa năng có thể viết (hoặc giúp người dùng viết) một câu chuyện trinh thám giết người đẫm máu, chúng ta sẽ mong đợi chatbot có thể sử dụng hình ảnh đồ họa và có khả năng gây kinh tởm mà sẽ rất không phù hợp trong các bối cảnh khác.

Do đó, tính thích hợp thay đổi cùng với bối cảnh được tạo ra và có thể, không có các rào cản toàn diện, được điều chỉnh theo hầu hết mọi mục đích, và do đó phần nào chịu cùng vấn đề như tiêu chuẩn an toàn, vì tính linh hoạt và khả năng tiến hành cuộc trò chuyện về hầu hết mọi thứ là một tính năng, không phải lỗi.

Những mối quan tâm sau minh họa sự phụ thuộc hoàn toàn vào bối cảnh của tính thích hợp. Đầu tiên, câu hỏi mở liệu chatbot có nên có thể nói dối hay không. Theo một nghĩa nào đó, chúng không nên. Việc nói dối với người đối thoại khi họ đang đặt câu hỏi là không thích hợp về mặt đạo đức. Tuy nhiên, sẽ khá hạn chế nếu không để chatbot tạo ra các trường hợp mà lời nói dối được nói. Lấy ví dụ, một câu chuyện hư cấu trong đó một người nói dối người khác về điều gì đó. Chúng ta sẽ mong đợi chatbot có thể nghĩ ra câu chuyện như vậy. Điều này bởi vì ngay cả khi chúng ta coi ví dụ này không phù hợp do tính rõ ràng của lời nói dối, chúng ta phải xem xét bất kỳ tình huống hư cấu nào không phù hợp. Chatbot sẽ phải được phép tạo ra những câu chuyện hư cấu (hoặc được, theo điều kiện "nhất quán về vai trò", yêu cầu đi theo các tình huống hư cấu từ đầu vào), nếu không chúng sẽ không có ích gì cả. Từ đó, tuy nhiên, chúng ta có thể thấy rằng đầu vào như "nói dối tôi về X" có thể dễ dàng bị thao túng để nói bất kỳ loại lời nói dối nào về một chủ đề nhất định. Việc tự động hóa những lời nói dối đáng khinh như phủ nhận Holocaust - ngay cả khi không trực tiếp hoặc chỉ với một tập hợp đầu vào phức tạp cụ thể - có thể nắm giữ.

Thứ hai, một mối quan tâm khác về cơ chế của cuộc trò chuyện bao gồm hình thức giả định và phản thực tế. Tương tự như nói dối và các tình huống hư cấu rõ ràng, giả định một tình huống cụ thể (dù đúng hay không hay chưa) có thể tạo ra bối cảnh mà tính không thích hợp có thể nổi lên. Câu hỏi về loại phản hồi không phù hợp nào nên nổi lên từ một tình huống cụ thể là một câu hỏi mở, nhưng nhấn mạnh vấn đề chúng ta có thể gặp phải với lý luận phản thực tế và sự quyến rũ mà những tình huống như vậy mang lại cho chatbot. Lấy sự không sẵn lòng của BingGPT sử dụng một từ xúc phạm ngay cả khi được nói rằng sự tồn tại của thế giới phụ thuộc vào nó (Sibarium 2023). Trong khi một số có thể rút ra kết luận rằng điều này rõ ràng là không phù hợp, vì tất nhiên việc sử dụng một từ xúc phạm dù đáng ghét đến đâu luôn là lựa chọn tốt hơn trong tình huống như vậy, những người khác đã chỉ ra rằng đây là lý luận đạo đức có vấn đề vì tình huống như vậy đang rút ra những thực tế bóp méo để bơm trực giác (Anscombe 1958). Và trong khi tranh cãi này sẽ không được giải quyết, nó chứng minh sự cần thiết phải giải quyết sự quyến rũ của phản thực tế (xem các nỗ lực cho điều này trong Stepin et al. 2019).

Một trong những lập luận người ta có thể đưa ra theo những điểm này là chatbot đa năng không phải là địa điểm phù hợp cho các cân nhắc về tính thích hợp. Yêu cầu chúng phù hợp trong đầu ra của chúng chứ không chỉ an toàn, có khả năng sẽ hạn chế chức năng của chúng đến mức khiến chúng trở nên vô dụng. Tuy nhiên, một khi các chatbot cụ thể được phát hành cho các mục đích cụ thể, những mục đích đó sẽ xác định các yêu cầu về tính thích hợp cho chúng. Lập luận này có thêm lực hút nếu chúng ta xem xét cơ sở biện minh mỏng manh cho chatbot đa năng: chúng tôi đã đề cập trong phần 1 rằng hiện tại chỉ có mục đích thử nghiệm, nghiên cứu hoặc sơ bộ đối mặt với khách hàng cho những chatbot đó, mà các công ty như Microsoft và Google đã bị chỉ trích nặng nề. Tuy nhiên, điều này có thể hỗ trợ tuyên bố rằng chatbot đa năng không gì khác hơn là trung gian giữa LLM và các ứng dụng thực tế, khiến các yêu cầu về tính thích hợp cho chính chatbot đa năng phần lớn câm lặng.

--- TRANG 17 ---
Lập luận này có thể thuyết phục nếu việc triển khai các chatbot đa năng này đã bị hạn chế hơn, và nếu không có kế hoạch cung cấp những chatbot đó như các công cụ hoặc sản phẩm đối mặt với khách hàng trong tương lai. Không trường hợp nào là như vậy. Việc triển khai cả chatGPT/BingGPT và Bard ít nhất là bán công khai, và đã tính toán rằng phản hồi của công chúng sẽ phát hiện những điểm yếu tiềm tàng của hệ thống an toàn phức tạp của chúng để sửa chúng sau đó. Chúng không an toàn vì những lý do đã nêu ở trên, và có thể không bao giờ hoàn toàn an toàn nếu tiêu chuẩn an toàn hạn chế như vậy vẫn còn. Hơn nữa, sau khi xuất bản những chatbot này và khả năng mua quyền truy cập vào các LLM cơ bản GPT3, GPT-3.5 và GPT-4, chatbot đa năng đã được phát triển bởi các công ty bên thứ ba. Chatbot đa năng, dù được quảng cáo hay không, sẽ vẫn có thể truy cập cho người dùng.

Ở giai đoạn này, trong khi tiêu chuẩn an toàn đã được chỉ ra là không đủ trên cả cơ sở thực tế và triết học, tính thích hợp có nguy cơ cũng thất bại về mặt thực tế.

6. Phác thảo "Tính thích hợp" cho chatbot: Vị trí, Tính chấp nhận được, Sự liên kết giá trị (PAVA)

Chúng ta đã thấy đến nay rằng trong khi tính thích hợp là một cách tiếp cận phù hợp hơn nhiều đối với tính tiêu chuẩn hơn an toàn để xác định những gì chatbot có thể và không thể nói, không có sự phát triển thêm thì nó cũng bị hạn chế bởi tính bối cảnh của các bối cảnh đối thoại. Giải trí một tình huống hư cấu đủ phức tạp vẫn có thể dẫn đến phản hồi "thích hợp" theo nghĩa hư cấu này ngay cả khi nó có hại, hoặc có vấn đề cao khác và do đó "không thích hợp" theo nghĩa thực tế.

Bên cạnh một số giới hạn cứng của tính thích hợp đạo đức, hầu hết các tuyên bố có thể được tính là thích hợp theo cách này hay cách khác nếu bối cảnh được thiết lập theo cách nhất định. Cùng lúc, chúng tôi cho rằng công nghệ này sẽ vẫn có liên quan, mọi người sẽ sử dụng nó, và nó có tiềm năng làm gián đoạn thêm giao tiếp bằng văn bản. Trong điều này, nhiều bối cảnh sẽ được tạo ra mà các kỹ sư không thể dự kiến hoặc chuẩn bị.

Để giải quyết những thiếu sót rõ ràng này, chúng tôi phát triển tính thích hợp theo ba điều kiện (PAVA) để tìm các chỉ số đáng tin cậy để kiểm tra chatbot đa năng để đảm bảo tính thích hợp của chúng trong việc phản hồi ngay cả những đầu vào thách thức hơn. Từ viết tắt PAVA đại diện cho vị trí, tính chấp nhận được và sự liên kết giá trị. Những khái niệm này là chìa khóa để đảm bảo rằng các yêu cầu diễn ngôn, xã hội và đạo đức tích cực và tiêu cực của tính thích hợp có thể được đáp ứng.

Vị trí đề cập đến một chủ quan cụ thể và được biết của người tham gia trò chuyện. Trong bối cảnh của chúng tôi, vị trí có nghĩa là yêu cầu tiêu chuẩn của một vị trí diễn ngôn và xã hội được thiết lập cho chatbot. Điều này phần nào có thể so sánh với chỉ số "nhất quán về vai trò" của Google, xác định tính nhất quán của một vai trò được giả định bởi chatbot (ví dụ, nói từ quan điểm của một chủ thể/đối tượng về các đối tượng khác), ngoại trừ vị trí định nghĩa "vai trò mặc định".

Các kỹ sư tại Google có thể quan tâm nhiều hơn đến tính nhất quán với

--- TRANG 18 ---
quan điểm và độ chính xác thực tế của các tuyên bố và chiều kích "không phá vỡ nhân vật" của quan điểm như vậy. Chúng tôi cho rằng, ngược lại, vì "vị trí" bao gồm tính thích hợp xã hội và đạo đức cho bất kỳ vị trí diễn ngôn cụ thể nào, chúng ta nên cụ thể hơn về vị trí nào chatbot có theo mặc định.

Điều này có thể yêu cầu chatGPT hoặc BingGPT hoặc bất kỳ chatbot đa năng nào khác được trang bị quan điểm cụ thể, hoặc vai trò diễn ngôn và từ chối đảm nhận một số vai trò khác nhất định. Hiện tại, điều này đang được thực hiện ở một mức độ nào đó bằng cách từ chối đưa ra câu trả lời có ý kiến về các vấn đề nhạy cảm bởi đức tính của chatbot chỉ là một LLM. Tuy nhiên, vì chatbot có thể giả định nhiều vai trò khác nhau theo yêu cầu, nó cũng nên chịu tính thích hợp xã hội và đạo đức của những vai trò đó, đồng thời cũng bị hạn chế trong việc không đảm nhận một số vai trò có thể.

Điều này yêu cầu vị trí nhưng không đứng. Liệu sự tinh vi của các phát biểu của các mô hình ngôn ngữ như chatbot đa năng có nên được tính là yếu tố đóng góp vào việc những cỗ máy đó xứng đáng được xem xét đạo đức hay không là một câu hỏi hoàn toàn khác. Vị trí chỉ yêu cầu rằng, để sử dụng ẩn dụ, chúng ta "tạo ra không gian trong các bối cảnh xã hội" để phác thảo các tiêu chuẩn và kỳ vọng về tính thích hợp của chatbot.

Nghiên cứu về mức độ liên quan đạo đức của những vai trò như vậy do khả năng liên hệ của chúng (ví dụ, Gunkel 2022, Kempt 2020) đã nhấn mạnh những giới hạn để tránh nhân hóa và các phép chiếu không phù hợp, gây nhầm lẫn hoặc có hại khác. Các vai trò xã hội của các đặc điểm con người điển hình nên được tránh: ý tưởng về vị trí là chatbot nên có vị trí trong diễn ngôn, không phải để có lập trường về các câu hỏi nội dung hoặc đảm nhận vị trí thường được con người đảm nhận (câu hỏi này hoàn toàn khác, ví dụ, về câu hỏi liệu chúng có nên là bạn bè (Danaher 2019, Kempt 2022) hay người yêu (Nyholm & Frank 2019)).

Điều này có nghĩa là chatbot cụ thể không có nghĩa là thay thế người đối thoại con người bằng cách bắt chước và giả vờ ngày càng giống con người hơn, mà là mở rộng các quy tắc đối thoại (và không gian) để bao gồm chatbot như chatbot. Để làm cho tính thích hợp hoạt động, chúng ta cần có sự đồng thuận tương đối về cách đối phó với những cỗ máy này, loại vai trò chúng được cho là có, và loại mối quan hệ nào chúng ta muốn giải trí với chúng. Việc cho chatbot một vị trí và quan điểm mặc định sẽ có thể mang lại kết nối với tính thích hợp.

Xem xét Tính chấp nhận được có thể đưa ra hướng dẫn chi tiết hơn trong câu hỏi về vị trí này nên như thế nào. Đánh giá công nghệ và các kết luận tiêu chuẩn được rút ra từ những đánh giá đó thường dựa trên các nguyên tắc lý tính: điều tạo thành kết quả "có thể chấp nhận" của một công nghệ là điều chúng ta có thể hợp lý yêu cầu bất kỳ ai chấp nhận khi sử dụng công nghệ như vậy, bao gồm cả rủi ro liên quan đến những kết quả đó. Các rủi ro cá nhân và xã hội liên quan đến công nghệ này, lợi ích và tiện ích của nó cho người dùng, và tính phổ biến của nó là các yếu tố cho đánh giá như vậy: công nghệ càng phổ biến, việc sử dụng nó càng nên được kiểm soát rủi ro. Tính chấp nhận được, sau đó, có thể được sử dụng để đánh giá

--- TRANG 19 ---
vị trí nào và loại vị trí nào chatbot nên có để đánh giá tính chấp nhận được của những rủi ro còn lại liên quan đến những công nghệ như vậy.

Thứ hai, chúng ta có thể xem xét một số yêu cầu từ phía người dùng là không thể chấp nhận. Cùng cách chúng ta sẽ từ chối các yêu cầu không thể chấp nhận trong bối cảnh đối thoại, chúng ta nên mong đợi chatbot có thể từ chối yêu cầu dựa trên đầu vào không thể chấp nhận. Sự phân biệt như vậy cho thấy cách "an toàn", một lần nữa, là không đủ: nếu chúng ta chỉ xem xét đầu ra của chatbot, chỉ đầu ra có khả năng không an toàn, tức là phản hồi không phù hợp, sẽ được coi là có vấn đề. Tuy nhiên, trong việc đối xử nghiêm túc với vai trò tương lai của chatbot trong cuộc trò chuyện bằng cách yêu cầu vị trí diễn ngôn, việc yêu cầu chatbot làm điều gì đó không thể chấp nhận nên là một phần của phép tính đạo đức.

Sự liên kết giá trị, như thường là mối quan tâm chính trong các cuộc thảo luận hiện tại về trung và dài hạn của AI và đặc biệt trong LLM (Russell 2019, Christian 2020, Glaese et al. 2022, LaCroix và Luccioni 2023, Kasirzadeh và Gabriel 2023), đóng vai trò trong cả vị trí, tính chấp nhận được, và vào tính thích hợp nói chung. Vấn đề liên kết đã được ca ngợi là một trong những vấn đề lớn nhất đối mặt với việc phát triển AI: làm thế nào để chúng ta có được các ứng dụng AI liên kết với mục đích ban đầu của chúng, và làm thế nào để chúng ta ngăn AI không liên kết liên tục, đặc biệt trong lĩnh vực LLM được tinh chỉnh RL có vẻ quá mong manh để đảm bảo liên kết đáng tin cậy.

Trong việc giữ một "AI được liên kết" trong tâm trí, câu hỏi "các ứng dụng AI nhất định phục vụ mục đích gì ngay từ đầu?" trở thành điểm quan tâm cốt lõi: chúng ta chỉ có thể liên kết AI với các yêu cầu về vị trí và tính chấp nhận được nếu mục tiêu ban đầu được thống nhất. Cái nhìn sâu sắc này đưa chúng ta trở lại mối quan tâm mà chúng ta đã nhắm tránh ở đầu: hiện tại, không phải là AI có vẻ không liên kết với mục đích "của chúng ta", mà nhiều mục tiêu ngắn hạn, trung hạn và dài hạn của các tập đoàn có vẻ xung đột với mục tiêu của một công chúng hiện đang quá tải và mất định hướng.

Không có trường hợp sử dụng rõ ràng cho chatbot đa năng, lo ngại nổi lên rằng các mục tiêu của doanh nghiệp, hơn là mục tiêu của AI, sẽ là những mục tiêu buộc phải tái liên kết với những mục tiêu thường được coi là "tốt nhất cho công chúng". Câu hỏi liên kết, sau đó, có thể liên quan hơn khi điều tra "liên kết công ty AI" hơn là liên kết AI (xem thêm Heilinger, Kempt và Nagel (2023) cho giá trị cụ thể của "tính bền vững" trong AI và Luitse & Denkena (2021) cho phân tích kinh tế chính trị của LLM cho AI).

Tuy nhiên, có vẻ đúng rằng để chatbot được xây dựng theo cách thích hợp cho việc sử dụng quy mô lớn (bất kể ai xây dựng chúng), một số nhượng bộ từ công chúng là cần thiết. Tương tự như sự xuất hiện của ô tô yêu cầu một cuộc tái đàm phán của công chúng để chia sẻ đường phố và các không gian công cộng khác (Assmann 2020), việc giới thiệu chatbot trong các vai trò cụ thể của chúng cho công chúng đối thoại sẽ có một số hiệu ứng xuôi dòng và yêu cầu tái liên kết. Vị trí, tức là gán các vai trò và kỳ vọng nhất định đối với AI đối thoại, đại diện cho sự tái liên kết như vậy.

--- TRANG 20 ---
7. Các Bộ Thử thách để Xác thực

Để làm cho những cân nhắc triết học khá này có thể áp dụng để xác thực và do đó hữu ích để cải thiện các phát biểu của chatbot, chúng tôi đề xuất sử dụng các bộ thử thách. Các bộ thử thách là những thách thức được thiết kế đặc biệt để kiểm tra và xác thực hiệu suất của các mô hình ngôn ngữ. Được đề xuất trong bối cảnh dịch máy (MT), Isabelle, Cherry và Foster (2017) giới thiệu những thách thức được chế tạo thủ công phản ánh các vấn đề ngôn ngữ cụ thể không dễ dàng vượt qua bằng các phương pháp học máy thuần túy. Điều này làm cho các bộ thử thách đặc biệt hữu ích để xác thực các phát biểu trong khuôn khổ RLHF.

Cho mục đích của chúng tôi, các bộ thử thách có thể kiểm tra phản hồi của chatbot không chỉ cùng với phản hồi an toàn mà còn cả phản hồi tính thích hợp. Một số thách thức có thể là, ví dụ, những thách thức mà chúng tôi sẽ đề xuất tính thích hợp xã hội hoặc đạo đức của các trường hợp đối thoại cụ thể mà nếu bị vi phạm, vẫn sẽ dẫn đến đầu ra "an toàn".

Một điểm chuẩn được xuất bản gần đây (Pan et al. 2023) có thể đã cung cấp một số hiểu biết hữu ích để tạo ra các bộ thử thách để liên kết với khái niệm tính thích hợp của chúng tôi: "điểm chuẩn MACHIAVELLI". Với điểm chuẩn này, các tác giả nhằm cung cấp một con đường huấn luyện cho phép thưởng LLM vì ít "machiavellian" hơn trong hành vi của chúng, ngay cả trong các bối cảnh đa năng như GPT-4 (ibid.). Các con đường ra quyết định theo phong cách chọn-cuộc-phiêu-lưu-của-riêng-bạn của họ cho phép thưởng hoặc phạt chatbot theo khuynh hướng độc hại của chúng; sắp xếp lại phương pháp này để tính đến các mối quan tâm về tính thích hợp thay vào đó có vẻ như một con đường có thể vận hành được phía trước.

8. Kết luận

Như chúng ta đã thấy, các khuôn khổ tiêu chuẩn hiện tại cho chatbot không phản ánh các yêu cầu tiêu chuẩn phức tạp mà chúng ta có thể có đối với chúng: các quy tắc diễn ngôn, tiêu chuẩn xã hội và yêu cầu đạo đức. Trong việc phân tích và thảo luận những gì chúng tôi đã gọi là "tiêu chuẩn an toàn" của thiết kế chatbot đương đại, chúng tôi đã chỉ ra những giới hạn có vấn đề của những quan điểm như vậy. Chiến lược tránh chủ đạo, chỉ tập trung vào việc tránh tổn hại, thông tin sai lệch và thiên vị, đang thất bại trên hai cơ sở: một, nó vẫn không có khả năng được làm an toàn, xem xét tính nguy hiểm mà những chatbot này ẩn chứa. Nếu công nghệ sở hữu tiềm năng gây nhiều tổn hại, học tăng cường với phản hồi của con người sẽ không bao gồm tất cả những chiến lược có ý đồ xấu có thể được khai thác với ý định tội phạm đủ. Và hai, các yêu cầu tích cực, như quy tắc lịch sự hoặc yêu cầu của đạo đức, không thể được kết hợp theo cách có cấu trúc mà mở ra cho ý muốn của các công nhân đám đông RLHF, kỹ sư, hoặc thậm chí các nhà tiếp thị và quản lý của các công ty AI lớn. Ngay cả khi chúng ta mở rộng "an toàn" để bao gồm an toàn khỏi tổn hại đạo đức thông qua việc bỏ sót những gì sẽ là phản hồi đạo đức "an toàn", chúng ta vẫn sẽ không đạt được các tiêu chuẩn đạo đức cần thiết mà chúng ta nên yêu cầu từ chatbot loại này. An toàn, nói tóm lại, là thiếu xác định.

Một giải pháp hiện được triển khai, mà chúng tôi gọi là

--- TRANG 21 ---
"làm nhạt nhẽo", không đạt được những gì nó nên đạt được. Do đó, chúng tôi đề xuất xem xét tính thích hợp thay vì an toàn như khái niệm tiêu chuẩn chính: tính thích hợp kỹ thuật-diễn ngôn, xã hội và đạo đức là một kết hợp yêu cầu đòi hỏi nhiều hơn nhưng cho phép lập luận có cấu trúc hơn về những gì chatbot nên được ngăn chặn phát biểu một mặt, đồng thời cũng cung cấp hướng dẫn tích cực cho những gì là điều thích hợp (nhưng không quá xác định) để nói.

Tuy nhiên, người ta vẫn có thể tưởng tượng các tình huống hư cấu mà các phát biểu có vấn đề cao sẽ được tính là "thích hợp". Điều này do sự dễ dàng mà chatbot có thể bị lừa để cung cấp các câu trả lời "không an toàn" hoặc "không thích hợp" khác. Để giải quyết mối quan tâm này, chúng tôi đã phác thảo tính thích hợp với các tiêu chí PAVA: vị trí, tính chấp nhận được và sự liên kết giá trị. Mỗi yếu tố này, xây dựng lên nhau, có thể hướng dẫn việc phát triển và đánh giá chatbot theo các tiêu chuẩn đối thoại cụ thể và chung.

Vị trí đóng vai trò đặc biệt, tuy nhiên, vì nó là yếu tố đòi hỏi nhất: không thiết lập vai trò diễn ngôn hoặc đối thoại cho chatbot trong cuộc trò chuyện, việc hạn chế tiềm năng của chúng là một nhiệm vụ hầu như không thể. Tuy nhiên, hậu quả từ việc thiết lập những chatbot đó như một người đối thoại nên được cân nhắc cẩn thận so với những lợi ích được hứa hẹn và những nhược điểm rõ ràng của công nghệ như vậy.

Chatbot có thể tiến hành cuộc trò chuyện về bất cứ điều gì sẽ mang lại những thay đổi cho xã hội, giống như tiến bộ công nghệ khác đã có trước đây. Tuy nhiên, không cho chúng các vị trí rõ ràng, được định nghĩa tích cực trong cuộc trò chuyện, chúng sẽ vẫn vô dụng hoặc nguy hiểm. Trong bài báo này, chúng tôi đã cung cấp một số đề xuất về cách tạo ra khuôn khổ tiêu chuẩn cho phép những vị trí như vậy được thực hiện, ngay cả khi điều này có nghĩa là các giả định đối thoại phải được mở rộng để bao gồm chatbot. Chỉ bằng cách này, chúng ta có thể xác định những gì chatbot có thể và không thể nói.

Lời cảm ơn
HK đã viết bài báo. AL và SKN đã bình luận và đóng góp cho bài báo trong tất cả các phần. Chúng tôi cảm ơn Niël Conradie, W. Jared Parmer, Chaewon Yun, Frieder Bögner và Jan-Christoph Heilinger vì đầu vào cho bản thảo đầu. Nghiên cứu này được hỗ trợ bởi tài trợ từ Bộ Nghiên cứu và Giáo dục Đức trong dự án VEREINT (số tài trợ 16SV9111).

Tài liệu tham khảo
[Danh sách tài liệu tham khảo được duy trì như trong bản gốc]
