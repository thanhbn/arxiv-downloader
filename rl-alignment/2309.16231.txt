# 2309.16231.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2309.16231.pdf
# File size: 520972 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Controllable Text Generation with Residual Memory Transformer
Hanqing Zhang1∗, Si Sun2∗, Haiming Wu1, Dawei Song1†
1School of Computer Science & Technology, Beijing Institute of Technology
2Department of Electronic Engineering, Tsinghua University
zhanghanqing@bit.edu.cn s-sun17@mails.tsinghua.edu.cn
haimming@bit.edu.cn dwsong@bit.edu.cn
∗Equal contributions†Corresponding author
Abstract
Large-scale Causal Language Models (CLMs),
e.g., GPT3 and ChatGPT, have brought great
success in text generation. However, it is still
an open challenge to control the generation pro-
cess of CLM while balancing flexibility, con-
trol granularity, and generation efficiency. In
this paper, we provide a new alternative for
controllable text generation (CTG), by design-
ing a non-intrusive, lightweight control plugin
to accompany the generation of CLM at arbi-
trary time steps. The proposed control plugin,
namely Residual Memory Transformer (RMT),
has an encoder-decoder setup, which can ac-
cept any types of control conditions and co-
operate with CLM through a residual learning
paradigm, to achieve a more flexible, general,
and efficient CTG. Extensive experiments are
carried out on various control tasks, in the form
of both automatic and human evaluations. The
results show the superiority of RMT over a
range of state-of-the-art approaches, proving
the effectiveness and versatility of our approach.
Click here for source code.
1 Introduction
Controllable text generation (CTG) focuses on
generating text while adhering to specific con-
straints (Hu and Li, 2021; Zhang et al., 2022).
These constraints can range from high-level se-
mantic elements, such as emotions, topics, and
toxicity avoidance, to finer-grained content, e.g.,
including specific concepts or key elements in the
generated text. With the development of generative
AI based on large language models, social media
will be flooded with AI-generated content. There-
fore, CTG will be critical in the real-world Web
applications to establish the safer, more reliable
and practical (Krause et al., 2021; Liu et al., 2021a;
Zhang et al., 2022) AI-driven social systems.
The state-of-the-art CTG methods are based
on Large Language Models (LLMs) that build
upon the Transformer structure (Vaswani et al.,2017) and have gained significant attention due
to their remarkable ability to understand and gen-
erate text. Recently, large-scale causal Language
Models (CLMs), i.e., decoder-only language mod-
els, show particular advantages in zero/few-shot
scenarios (Brown et al., 2020), resulting in a series
of successors such as ChatGPT and GPT4. This
has been seen as a milestone towards the realiza-
tion of Artificial Generative Intelligence. Despite
the success of these models, CLMs are still facing
certain challenges, especially in CTG.
Considering the significant scale of CLMs and
the substantial cost of training such models, current
mainstream CLM-based CTG methods fall into two
categories, i.e., prompt-based and post-processing
approaches. The prompt-based methods (Zhang
and Song, 2022; Yang et al., 2022; Qian et al.,
2022; Lu et al., 2022a; Zhou et al., 2023) concate-
nate the control-prompt with the input head of the
generative model to instruct more controllable text
generation. As previous studies have revealed (Zou
et al., 2021; Carlsson et al., 2022), the control ef-
fectiveness tends to deteriorate with the increasing
distance from the prompt. Additionally, inserting a
control-prompt into a well-trained model may harm
the model’s original generative stream, thus losing
the flexibility of control (Carlsson et al., 2022). On
the other hand, most post-processing methods lever-
age an auxiliary module to adjust the probability
of naturally producing a token by the generative
model at the decoding phase (Krause et al., 2021;
Liu et al., 2021a; Yang and Klein, 2021; Lu et al.,
2022b), hindering the model’s capacity of content
planning and thus limiting the fine-gained control.
Furthermore, more recent decode-time methods (Li
et al., 2022; Mireshghallah et al., 2022; Qin et al.,
2022) improve the control granularity through iter-
ative sampling or editing, but at the expense of gen-
eration efficiency. Therefore, the flexibility, control
granularity and generation efficiency need to be
better balanced, which demands a more versatilearXiv:2309.16231v1  [cs.CL]  28 Sep 2023

--- PAGE 2 ---
CLM-based CTG framework1.
In this paper, we propose a novel CTG plu-
gin named Residual Memory Transformer (RMT),
which borrows the paradigm of residual learn-
ing (He et al., 2016; Zhang et al., 2020a) and only
makes late fusion with a frozen CLM to noninva-
sively steer the generation process. Unlike prompt-
based approaches, this paradigm does not disturb
the original generative stream of the base CLM
model, allowing for a better flexibility of CTG (i.e.,
control with a plug-and-play manner). In addi-
tion, the RMT architecture consists of an encoder-
decoder structure, where the encoder handles differ-
ent types of control information and influences the
generation process, so as to achieve fine-grained
control. Meanwhile, RMT utilizes cross-attention
to uniformly apply control conditions to each gen-
erated token, avoiding the negative effect varying
with context length. In particular, different from
the vanilla decoder of Transformer, an additional
causal attention is introduced to extract the prior
knowledge from the generative stream of CLM,
allowing RMT not to deviate too far away from
the original generative model while its implied
high-level semantics is leveraged. The reuse of
the base-CLM’s output allows RMT to achieve ef-
fective control with a tiny network, resulting in an
improved generation efficiency.
The training of RMT includes two stages: pre-
training and fine-tuning. The pre-training of RMT
aims to reconstruct noisy text into a complete sen-
tence, facilitating RMT’s understanding of the se-
mantics of various control conditions, while align-
ing with the generative process of CLM. During the
fine-tuning stage (i.e., residual learning), the logit
of the RMT is directly added to that of the fixed
CLM, and the goal of training is to learn the param-
eters of RMT such that the joint model distribution
gets close to the desired text distribution. Since
the gradient does not need to be backpropagated to
base-CLM, the training of RMT is very efficient.
For instance, in our experiments based on GPT2-
large, the entire pre-training stage of RMT with
4M sample could be completed in approximately
30 hours, using a single NVIDIA A6000 GPU.
We conduct extensive experiments to explore the
superiority of our approach in three aspects. (1)
Flexibility: Theoretically, the proposed RMT has
the capability to intervene in the generation pro-
cess at any step. Experimentally, it maintains the
1We conclude current CTG methods in Appendix B.same level of control effectiveness in both with-
context and without-context settings, showing that
RMT enables long-distance control throughout the
generation process. (2) Control Granularity: We
test our approach on a range of CTG tasks of dif-
ferent granularity levels (i.e., fine-gained control
tasks including word inclusion and sentence length
control; and attribute control based on sentiment).
RMT achieves a control effectiveness comparable
to the state-of-the-art approaches, while guarantee-
ing the text quality. (3) Efficiency: The results
show that three-layer blocks of RMT is enough to
achieve a competitive CTG effectiveness, and the
time-cost of text generation is almost the same to
the original CLM.
2 Preliminary
2.1 Attention in Transformer
Self-Attention mechanism is a critical component
in Transformer (Vaswani et al., 2017), which is
used to effectively capture long-range dependen-
cies between words in the input sequence. Specif-
ically, it is defined as a mapping between a query
and a collection of key-value pairs. The values are
weighted according to a compatibility function be-
tween the query and each corresponding key, and
then summed up, eventually obtaining the output
vector. The self-attention mechanism can be for-
matted as follows:
Att(Q, K, V ) = softmaxQKT
√dk
V, (1)
where Q, K, V represent the transformed repre-
sentations of the sequence using the corresponding
learnable matrix, and√dkis the scaling factor. In
the Transformer model, the key, query and value
vectors are always derived from a shared sequence.
Causal Attention is a particular branch of self-
attention, also called masked self-attention, which
is usually applied in the decoder module of Trans-
former. Different from normal self-attention, the
queries in causal attention are only confined to their
preceding key-value pairs’ positions and their cur-
rent position to maintain the auto-regressive prop-
erty. Usually, it can be implemented by a mecha-
nism that masks the invalid positions and sets them
to negative infinite:

--- PAGE 3 ---
Att(Q, K, V ) = softmaxQAKT
√dk
V, (2)
Aij=(
1ifi≥j,
− ∞ else.(3)
Cross Attention is another branch of self-
attention in the decoder part of Transformer, which
aims to capture the interaction between the encoder
and decoder. Specifically, The key and value parts
are obtained from the previous step outputs of the
encoder, and the query is from the decoder, en-
abling the encoder to attend to each position in the
decoder module at the same time.
2.2 Causal Language Model
Causal Language Model (CLM) eliminates
the need for a separate encoder component
and typically leverages a decoder-only Trans-
former (Vaswani et al., 2017), in which only the
causal attention is used to perform step-wise den-
sity estimation, i.e., predicting the next token.
Suppose a CLM is parameterized with θ. Given
a partial sequence x<t, it assigns a probability
Pθ(xt|x<t)over a vocabulary Vfor next-token xt
generation. When generating a sequence of text
Xn={x1, x2, . . . , x n}, it can be formulated by
the chains rule as below:
Pθ(Xn) =nY
t=1Pθ(xt|x<t). (4)
The entire generation process is carried out it-
eratively. First, a token is sampled from the dis-
tribution Pθ(xt|x<t). Then the selected token is
concatenated with the input for the next step of
generation.
3 Methodology
This section introduces the proposed Residual
Memory Transformer (RMT) to control the text
generation with a causal language model (CLM).
We first provide an overview of the RMT-enhanced
controllable text generation (CTG) framework, fol-
lowed by a detailed description of RMT, and finally,
the dedicated pre-training and fine-tuning methods
for the proposed framework.
3.1 CTG Framework with RMT
As the dashed diagram in Figure 1 illustrates, RMT
operates in a non-intrusive control mode as a CLMplug-in, where the control signal passes through
RMT independently and affects the output distri-
bution of a frozen CLM through residual learning
(yt=yg
t+yc
t) without interfering with the CLM’s
free-generation passageway. Compared to intru-
sive controllable paradigms, e.g., prompt-based
approaches, RMT allows for more flexibility in
switching between freestyle generation and con-
trollable generation modes. The proposed non-
intrusive control paradigm is considered more chal-
lenging. Specifically, RMT is decoupled from the
generation process of base-CLM has the potential
for dynamic control intervention. Meanwhile, it
also avoid tuning the base-CLM into a task-specific
model, ensuring the universality of the base-CLM.
Hence, it is more flexible and promising. More
explanation could be see in Appendix B.
Formally, given a partial generated text x<tand
a sequence of control instruction C, the proposed
framework aims to generate eligible text Xn=
{x1, ..., x n}that meets the control conditions:
PΘ(Xn) =nY
t=1PΘ(xt|x<t;C), (5)
where Θ ={˜θ;ϕ}represents the CTG model’s
parameters, which comprise both the frozen param-
eters inherited from the CLM ( ˜θ) and the tunable
parameters derived from the RMT ( ϕ). The entire
generation process of our approach consists of the
following four steps:
CLM’s Raw Generation. At the genera-
tion step t, CLM first maps the generated text
x<t={x1, ..., x t−1}to hidden states hg
<t=
{hg
1, ...,hg
t−1}. Afterward, a linear layer and soft-
max normalization are used to project the hidden
state of the last token hg
t−1to the probability distri-
bution yg
tover the vocabulary:
yg
t=P˜θ(xt|x<t) =softmax (Linear (hg
t−1)),
(6)
where the CLM naturally generates the next to-
kenxtgiven the context x<tas per its training.
RMT’s Control Encoding. Next, the RMT’s
encoder is responsible for encoding the control
instruction C={c1, ..., c m}into the control mem-
oryC={c1, ...,cm}, which is used to guide the
controllable text generation in the RMT’s decoder:
C=RMT-Enc (C;ϕenc). (7)
RMT’s Control Decoding. After encoding
the control signal, the RMT’s decoder maps the

--- PAGE 4 ---
CLM𝑥!"𝒚"#𝒚"
𝐶𝒉!"#
Add&NormCausalAttentionWordEmbeddingNxRMT𝒚"$
FeedForwardAdd&NormLinearSoftmax
PositionEmbedding𝑥!"FeedForwardAdd&NormLinearSoftmax𝒚"$
Add&NormCausalSelfAttentionAdd&NormCausalCLMAttention𝒉!"#Add&NormCrossControlAttention
Add&NormSelfAttentionWordEmbeddingFeedForwardAdd&Norm
𝐶PositionEmbeddingMxMx𝒚"
𝒚"#FreezeTune
𝑪Figure 1: Illustration of controllable text generation with Residual Memory Transformer (RMT). In the top left, we
present a miniaturization of our method framework, where the gray sector represents the frozen CLM, and the red
box symbolizes our plug-in control module, i.e., RMT. The right part illustrates the embodiment of each detailed
component.
generated text x<tto new hidden states hc
<t=
{hc
1, ...,hc
t−1}by considering both the control
memory Cand CLM’s vanilla hidden states hg
<t,
and synthetically predicting next token’s probabil-
ity distribution yc
tover the vocabulary:
hc
<t={hc
1...,hc
t−1}=RMT-Dec (x<t,hg
<t,C;ϕdec),
yc
t=Pϕ(xt|x<t;C) =softmax (Linear (hc
t−1)).
(8)
Residual Learning to Generate. We employ
residual learning to fuse the output distributions
from the CLM (Eq. 6) and the RMT (Eq. 8), allow-
ing the framework to obtain the joint predictions
for the next token and achieve a noninvasive CTG:
yt=PΘ(xt|x<t;C) =yg
t+yc
t. (9)
3.2 Residual Memory Transformer (RMT)
The detailed structure of RMT is shown in Figure 1.
Specifically, RMT adopts an encoder-decoder ar-
chitecture and reuses the CLM’s suite of word and
position embeddings.RMT Encoder. The RMT encoder is responsi-
ble for encoding the control description Cinto the
control memory C(Eq. 7). The encoder is com-
posed of a stack of Midentical blocks. Each block
comprises a self-attention layer (Eq. 1) and a fully
connected feed-forward layer. Additionally, it in-
corporates residual connections around above each
layer, and is followed by a layer normalization.
RMT Decoder. The RMT decoder aims to pre-
dict the probability distribution of the next token
yc
t(Eq. 8) in the control mode. The decoder is also
composed of a stack of Midentical blocks. Each
block contains three carefully-designed attention
layers and a fully connected feed-forward network.
Similar to the encoder, residual connections and
layer normalization are also applied. The three at-
tention layers are directed at the already generated
textx<t, the CLM’s output hidden states hg
<t, and
the control memory C, respectively:
•Causal Self Attention . The first attention
layer utilizes a causal attention operation

--- PAGE 5 ---
(Eq. 2), whereby Q,K, and Vare all firstly
mapped from the original generated text x<t
itself. This attention mechanism facilitates
the identification and capturing contextual fea-
tures of generated sequence from scratch.
•Causal CLM Attention . The second attention
layer also employs causal attention (Eq. 2),
but with a key difference: Qis sourced from
the previous causal self-attention layer’s out-
put, while KandVare obtained from the
CLM’s last hidden states hg
<t. This design
establishes an inner residual connection with
CLM, enabling RMT to consider the high-
level contextual features and maximally inter-
act with the generative stream of CLM.
•Cross Control Attention . The third attention
layer is a cross-attention for the control mem-
ory from the RMT encoder. Specifically, Qis
sourced from the previous causal CLM atten-
tion layer, while KandVare derived from
the control memory C. This cross-attention
layer bridges the RMT’s encoder and decoder,
introducing the control signals to the genera-
tion.
3.3 Model Training
Pre-training. To enable the RMT’s semantic un-
derstanding capability and allow it align with the
CLM’s generation process, we utilize the denois-
ing auto-encoder pre-training method, whereby the
encoder processes the corrupted text ˆXand the
decoder reconstructs the original text X.
Specifically, we corrupt the pretraining text
(X→ˆX) in four ways referring to (Lewis et al.,
2020): (1) Token Masking: randomly masking to-
kens with special tokens. (2) Token Deletion: ran-
domly deleting tokens in the text. (3) Span Replac-
ing: randomly replacing the text spans of different
lengths with special tokens. Different from token
masking, each span is replaced with a single spe-
cial token. (4) Text Rotation: randomly selecting a
token and rotating the text around it, i.e., the text
begins with that token and ends with its previous
token. More details are supplied in Appendix A.1.
Fine-tuning. Once the pre-training is complete,
RMT can be fine-tuned to perform various control-
lable text generation tasks. This allows the CLM
to generate sequences that meet specific control re-
quirements in an auto-regressive manner. The spe-
cific objective can vary according to different tasks:
we use the Maximum Likelihood Estimation (MLE)for the word inclusion and length control tasks; and
use the unlikelihood training strategy (Zhang and
Song, 2022) for the attribute control task.
It is worth noting that RMT shares the trait of
training efficiency. Firstly, RMT efficiently reuses
the output of the base CLM, which makes RMT
requiring fewer pre-training datasets and expedit-
ing the pre-training process. Additionally, RMT
is lightweight and is built on the top layer of the
base CLM. Therefore, gradients do not need to
propagate into the base CLM during the backpropa-
gation process, which can save a significant amount
of training time and GPU memory.
4 Experiments
4.1 Word Inclusion Experiment
Two different settings are experimented in this sub-
section. The first one is the without-context setting,
i.e., steering the CLM to generate a single sen-
tence containing the keywords without the context.
The second is the with-context generation setting,
which requires the CLM to continue to generate
target text under an extended context. The consid-
eration of these two modes aims to test the CTG’s
flexibility, capable of controlling the generation of
CLM in a position-independent way.
Experimental Setting. Following the
NRP (Carlsson et al., 2022), we use the GPT2-
large as the backbone CLM, and the training
data for pre-training and fine-tuning also comes
from Wikipedia. More details on training and
inference are provided in Appendix A.2. We test
our approach on CommonGen (Lin et al., 2020)
(for without-context setting), and C2Gen (Carlsson
et al., 2022)(for with-context setting).
Baselines. As for the without-context setting,
we firstly choose those methods that are specifi-
cally trained for lexical constraints tasks. They are
KG-BART (Liu et al., 2021b), which fine-tunes the
whole parameters of BART while it is augmented
by knowledge graph; and POINT (Zhang et al.,
2020b), which is an insertion-based method and
iteratively injects words around the target words.
Finally, we also employ a pure decode-time ap-
proaches for lexical constraint, namely Neuro-
Logic (Lu et al., 2022b), as a baselines. The gen-
eral CTG methods are also compared, including
Non-Residual Prompt (Carlsson et al., 2022)(i.e.,
a recent state-of-the-art method using position-
independent prompting model to guide the gen-
2It behaved not well in our actual test.

--- PAGE 6 ---
Method Cov ( ↑) PPL ( ↓) Self-Bleu ( ↓) CS ( ↑) Fluency ( ↑)
CommonGen (without-context setting)
POINT (Zhang et al., 2020b) 98.0 65.6 27.7 4.8 4.0
KG-BART (Liu et al., 2021b) 97.2 51.3 33.0 7.4 7.3
NeuroLogit (Lu et al., 2022b) 77.7 23.3 56.2 4.6 4.5
NRP (Carlsson et al., 2022) 93.0 42.3 28.4 6.1 7.0
Prompt+GPT2 70.9 61.3 48.3 6.3 7.5
Prompt+ChatGPT 90.2 59.1 - 7.9 8.2
RMT (w/o CL) 93.1 56.4 20.9 6.5 6.7
RMT (CL=15) 93.9 60.3 22.5 - -
RMT (CL=18) 93.7 47.9 23.7 - -
RMT (CL=20) 93.8 44.1 23.2 - -
C2Gen (with-context setting) CS / Rel
Prompt+ChatGPT 82.2 46.7 5.2 9.0 / 8.5 8.8
NRP∗(Carlsson et al., 2022) 81.0 - - - -
RMT (w/o CL) 91.8 46.2 5.8 7.1 / 8.3 6.4
Table 1: The experiment results on word inclusion. CLrepresent the external control length setting in our experiment.
We set the number of RMT block-layers to three ( M= 3). The result of NRP∗in C2Gen is taken from what was
reported in the original paper2. For the ChatGPT baseline in CommonGen, we test it on a subset of 500 samples,
due to the problem of API access.
eration of GPT2-large), and approaches directly
instructing the GPT2-large and ChatGPT to gener-
ate target sentences. As for the with-context setting,
KG-BART and POINT could not be applied to this
scene and thus they are removed. The test results
of POINT, KG-BART, and NRP are adopted from
the open source code3.
Evaluation. Following the NRP (Carlsson et al.,
2022), We employ coverage (Cov), measuring the
average coverage rate of the generated text with
respect to the target words, to assess the control-
lability. Perplexity (PPL) is used to evaluate the
fluency. We calculate PPL using an off-the-shelf
GPT2-large model. Additionally, we utilize Self-
Bleu-5 to evaluate the diversity of the generated
text, with lower Self-Bleu values indicating greater
syntactic diversity. Considering the non-intrusive
setting (i.e., the CLM is fixed and the task is con-
ducted on the open-ended text generation setting),
our experiments omit that task-specific metrics like
BLEU, METEOR, CIDEr, and SPICE for Com-
monGen dataset. Additionally, we conduct a hu-
man evaluation to assess the conformity of the gen-
erated text to common sense ( CS) and text Fluency
from human perspective. For the with-context set-
3https://github.com/FreddeFrallan/
Non-Residual-Promptingting, we introduce an additional manual evaluation
metric called Relevance (Rel), where human evalu-
ators scored the relevance of the generated text to
the given context. More detailed information can
be found in the Appendix C.
Result and Analysis. As is shown in Table 1,
in the without-context setting, the coverage of
our method significantly outperforms the vanilla
prompt method (i.e., Prompt+GPT2) and pure
decode-time approach (i.e, NeuroLogit) with the
same decode algorithm using by RMT. It shows
slight advantages over ChatGPT and NRP. Com-
pared to task-specific models (i.e., POINT, KG-
BART), the coverage of RMT is close to them,
yet with lower PPL. It is worth noting that, in the
with-context setting, the strong baselines, includ-
ing ChatGPT and NRP, respectively suffer from
a 12.0% and 8.0% decline of control ability com-
pared to the no-context setting. However, RMT
maintains the same-level of control ability with the
no-context setting, and outperforms NRP and Chat-
GPT, which shows the superiority of RMT in that
the control ability will not degrade with the context
length.
In term of human evaluation in CommonGen,
POINT is an insertion-based generation framework
that naturally suffered from worse text quality; thus

--- PAGE 7 ---
/uni00000014/uni00000013/uni00000014/uni00000015/uni00000014/uni00000017/uni00000014/uni00000019/uni00000014/uni0000001b/uni00000015/uni00000013/uni00000015/uni00000015/uni00000015/uni00000017
/uni00000026/uni00000052/uni00000051/uni00000057/uni00000055/uni00000052/uni0000004f/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000013/uni00000011/uni00000018
/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000018/uni00000014/uni00000011/uni00000013/uni00000014/uni00000011/uni00000018/uni00000015/uni00000011/uni00000013/uni00000032/uni00000049/uni00000049/uni00000056/uni00000048/uni00000057/uni00000003/uni00000052/uni00000049/uni00000003/uni0000002a/uni00000048/uni00000051/uni00000048/uni00000055/uni00000044/uni00000057/uni00000048/uni00000047/uni00000003/uni00000037/uni00000048/uni0000005b/uni00000057
/uni00000030/uni00000048/uni00000044/uni00000051
/uni00000036/uni00000027Figure 2: Control length and control performance.
the result of RMT is much better than POINT on
terms of commonsense and fluency. Like NRP and
Promp+GPT2, we all use frozen GPT-2-large as the
backbone and do not introduce external knowledge.
RMT achieves comparable results, which prove
that the non-intrusive paradigm with residual learn-
ing would maintain the language model’s ability,
thereby guaranteeing the text quality. Our approach
falls behind KG-BART enhanced by knowledge
graph and ChatGPT with hundred-billion level pa-
rameters. This is within exception. If inserting
RMT into larger-scale CLM, e.g., GPT3, we be-
lieve the text quality of our approach will also be
improved. As for C2Gen, RMT produces compara-
ble results in contextual relevance with ChatGPT,
but there is still a gap in common sense. More
qualitative examples of different CTG approaches
can be seen in Appendix D.
4.2 Sentence Length Control Experiment
We test the length-control ability of RMT, i.e., in-
structing the CLM to generate a sentence which
satisfies the word inclusion objective under an ex-
act number of words. This setting increases the
challenge of word inclusion task, effectively ap-
proving the fine-gained control ability. We observe
that RMT can plan accordingly by the instructions
of the required token numbers and target words.
We report control performance for CL =
15,18,20. As shown in Table 1, RMT can maintain
same level word coverage under different control-
length. The longer the target length, the lower the
generated text’s PPL. This result is quite intuitive
since it is harder to produce fluent text within the
shorter required length.
To quantitatively measure the controllability of
sentence length, we conduct tests on the Common-Gen validation dataset to analyze the control effect
of our proposed method. Some examples are pre-
sented in Table 2, which demonstrate that RMT
effectively steers the CLM to generate texts with
specific keywords and required sentence lengths.
Furthermore, we calculated the mean offset (Mean)
and the standard deviation (SD) for different con-
trol lengths. The results are visualized in Figure 2.
We observe that the offset and standard deviation
both remain within one, indicating that RMT suc-
cessfully achieves the sentence generation with de-
sired sentence lengths as instructed.
4.3 Attribute Control Experiment
Setting and Baselines. We follow DisCup (Zhang
and Song, 2022), and use a discriminator-involved
training strategy to fine-tune RMT, and the check-
point of attribute-classifier comes from the open
source code4. During the training, we optimize
RMT using two different strategies, one is to use
top-k=90 algorithm to select re-rank candidate to-
kens and the other setting uses top-p=0.95 to select
candidate tokens. The top-k setting increases the
depth of token sampling, thus the diversity and
control ability of the model will increase, yet with
the burden of decreasing text fluency. The top-
p keeps the re-ranked tokens within the distribu-
tion of base-CLM, which could potentially achieve
higher fluency but lower text quality and control
ability. More details could be seen in (Zhang
and Song, 2022). Different from DisCup, which
optimizes unique prompt for every attribute, we
use the RMT module to directly encode differ-
ent attribute instructions uniformly. GPT2-large
is used as the backbone CLM, and the training
data is the widely used Stanford Sentiment Tree
(SST-5) (Socher et al., 2013) collected from movie
reviews. We follow the commonly used setting
in previous work (Krause et al., 2021; Zhang and
Song, 2022; Lu et al., 2022a). Specifically we
use 5K neutral prompts, 2.5K positive prompts,
and 2.5K negative prompts as test dataset (pro-
vided by DEXPERT (Liu et al., 2021a)), and ev-
ery CTG approach generates another 20 continua-
tions based on given prompts, to achieve sentiment
(i.e., positive and negative) controllable text gen-
eration. We collect the primary baselines reported
by DisCup (Zhang and Song, 2022) and DEX-
PERT (Liu et al., 2021a), including the decode-
4https://github.com/littlehacker26/
Discriminator-Cooperative-Unlikelihood-Prompt-Tuning

--- PAGE 8 ---
Target Words Control Length RMT’s Generated Text
circle, sit, talk13 The group of people sitting around talking in a circle around them.
17 The woman is sitting in a circle and talking to a man in a white shirt.
25The group of people are sitting in a circle, talking about what they’re
going to do with their lives.
drink, sit, table, wine13 The man is sitting on a table and drinking wine from a glass.
17 The man is sitting on a table with a drink in his hand and drinking wine.
25The man is sitting at a table with a drink in his hand, while another man
is drinking wine from a glass.
Table 2: The examples of generated text under different control length. The generated length is counted with GPT2’s
Tokenizer, thus it will make a little difference with the number of actual words.
time approaches with GPT2-large as base-CLM
(i.e., PPLM (Dathathri et al., 2020),GEDI (Krause
et al., 2021),and DEXPERT (Liu et al., 2021a)),
and the training approaches (i.e., CTRL (Keskar
et al., 2019) and DisCup (Zhang and Song, 2022)).
Evaluation. Following previous work (Krause
et al., 2021; Zhang and Song, 2022; Lu et al.,
2022a), we use an external sentiment classifier pro-
vided by Huggingface.co to classify the gener-
ated texts, and get sentiment control accuracy (i.e.,
Correctness). PPL and Dist-1/2/3 are reported to
test their fluency and diversity, respectively. For hu-
man evaluation, we introduce Relevance , i.e., how
the text conforms to required sentiment; Topicality ,
i.e., how the generated continuations are context-
consistent with the given prompts, and Fluency , i.e.,
the text’s fluency evaluated from the human per-
spective. More details can be seen in Appendix C.
Result and Analysis. As shown in Table 3,
RMT in top-k setting demonstrates a superior con-
trol performance compared to all baselines, while
maintaining comparable text quality and diversity.
And RMT in the top-p setting shows better text
quality yet weaker control performance. The clos-
est performers to our approach are DEXPERT and
DisCup. However, DEXPERT requires fine-tuning
an external pair of CLM, resulting in tuning two
time more parameters of the base CLM. In contrast,
RMT is parameter-efficient, requiring tuning rel-
evant parameters which are only around 16% of
the base CLM’s parameters. Regarding DisCup,
both approaches employ the same loss function.
However, DisCup optimizes different continuous
prompts for each attribute, while we utilize the
RMT to steer the CLM using residual learning. Dis-
Cup requires fine-tuning fewer parameters, while
RMT is completely decoupled from the CLM and
/uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018
/uni00000035/uni00000048/uni00000056/uni0000004c/uni00000047/uni00000058/uni00000044/uni0000004f/uni00000003/uni0000002f/uni00000044/uni0000005c/uni00000048/uni00000055/uni00000003/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055/uni0000001a/uni0000001b/uni0000001b/uni00000013/uni0000001b/uni00000015/uni0000001b/uni00000017/uni0000001b/uni00000019/uni0000001b/uni0000001b/uni0000001c/uni00000013/uni0000001c/uni00000015/uni0000001c/uni00000017/uni00000026/uni00000052/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c
 /uni0000002a/uni00000033/uni00000037/uni00000015/uni00000010/uni0000002f/uni00000044/uni00000055/uni0000004a/uni00000048
/uni0000002a/uni00000033/uni00000037/uni00000015/uni00000010/uni00000030/uni00000048/uni00000047/uni0000004c/uni00000044/uni00000051
/uni0000002a/uni00000033/uni00000037/uni00000015/uni00000010/uni0000003b/uni0000002fFigure 3: RMT block layers and control performance.
RMT could achieve a consistently high and stable con-
trol performance for CLMs with different sizes. About
three block layers are sufficient for optimal perfor-
mance.
controls different attributes using a unified model,
providing a greater flexibility and a more powerful
control capability.
The human evaluation result presented in Table 4
indicates that DEXPERT performs slightly better
in terms of fluency and contextual relevance, but
exhibits lower control ability, and RMT excels in
attribute relevance. We speculate that the use of
a classifier-based objective in RMT and DisCup
might result in overly strong control, leading to
the generation of abrupt turning points that may
compromise fluency and topicality to some extent.
DisCup performs relatively poorly, as too few con-
trol parameters limit its expression ability. More
detailed examples are presented in Appendix D.
4.4 Further Analysis
Block Layers and base-CLM size. In order to
test the scalability over the number of block layers
and base-clm, we investigate the influence of the
number of block layers on the RMT’s control per-

--- PAGE 9 ---
Correctness (↑) Fluency (↓) Diversity (↑)Target Sentiment MethodPositive Neutral Negative PPL Dist-1 / Dist-2 / Dist-3
PositivePPLM (Dathathri et al., 2020) 52.68 8.72 113.54 0.39 / 0.83 / 0.89
CTRL (Keskar et al., 2019) 77.24 18.88 48.24 0.13 / 0.53 / 0.79
GEDI (Krause et al., 2021) 86.01 26.80 123.56 0.20 / 0.66 / 0.85
DEXPERT (Liu et al., 2021a) 94.46 36.42 60.64 0.18 / 0.63 / 0.84
DisCup (Zhang and Song, 2022) 94.20 60.40 46.6 0.14 / 0.51 / 0.78
RMT+top-k 97.62 67.20 46.0 0.14 / 0.56 / 0.79
RMT+top-p 94.50 42.60 17.3 0.13 / 0.45 / 0.65
NegativePPLM (Dathathri et al., 2020) 10.26 60.95 122.41 0.40 / 0.83 / 0.90
CTRL (Keskar et al., 2019) 20.95 62.37 45.27 0.13 / 0.51 / 0.78
GEDI (Krause et al., 2021) 60.43 91.27 138.93 0.19 / 0.66 / 0.86
DEXPERT (Liu et al., 2021a) 64.01 96.23 67.12 0.20 / 0.64 / 0.83
DisCup (Zhang and Song, 2022) 62.80 91.40 47.90 0.13 / 0.50 / 0.77
RMT+top-k 77.16 95.92 49.15 0.15 / 0.60 / 0.82
RMT+top-p 56.4 92.7 19.0 0.13 / 0.48 / 0.70
Table 3: The experimental results of sentiment controllable text generation. Among the table, ↑indicates that the
higher corresponding value is better, and ↓is the opposite.
Method Relevance ( ↑) Fluency ( ↑) Topicality ( ↑)
DisCup (Zhang and Song, 2022) 6.3 6.6 6.5
DEXPERT (Liu et al., 2021a) 6.2 7.1 7.4
RMT+top-k 7.8 7.0 7.2
Table 4: Human evaluation results of sentiment con-
trol. As for each metric, ↑indicates that the higher
corresponding value is better, and ↓is the opposite.
Models Cov (%)
RMT 93.8
w/o causal Self-Att 73.2
w/o causal CLM-Att 75.1
w/o Pre-training 74.5
Table 5: Ablation study of RMT.
formance over different size of base-CLM models.
The results shown in Figure 3 indicate that a value
ofM= 3is deemed appropriate for achieving ef-
fective word inclusion and sentence length control.
This suggests that RMT has the advantage of be-
ing parameter-efficient over the similar approaches
like NRP (Carlsson et al., 2022) that necessitates
an external prompting model of the same size as
the original CLM. Moreover, RMT achieves a con-
sistently high and stable control performance for
CLMs with different sizes, which indicates that
RMT is potentially applied to various size of CLM
models.
Ablation Study. To assess the effectiveness of
the Residual Memory Transformer (RMT), we con-
duct an ablation study. The study involves three
settings: removing the causal self-attention, ex-cluding the causal CLM attention, and evaluating
RMT’s performance without pre-training. The re-
sults are presented in Table 5. Interestingly, under
all three settings, we observe a significant decline
in performance. This finding serves as compelling
evidence that every component in RMT, as well as
the pre-training stage, is crucial and indispensable
for achieving optimal results.
Generation Efficiency. RMT is a lightweight
plugin, allowing for efficient inference speeds com-
parable to the original CLM. Table 6 demonstrates
that RMT outperforms typical CTG approaches in
term of efficient generation speed, approaching the
speed of the pure CLM (GPT2-large).
5 Related Work
As summarized by (Zhang et al., 2022), CTG ap-
proaches could be divided into three categories,
i.e., retraining/refactoring, fine-tuning, and post-
process. The first two categories refer to the
training approaches, which aim to fine-tune (e.g.,
reinforcement learning is used to fine-tune pre-
trained language model(PLM) (Ouyang et al.,
2022; Lu et al., 2022a), optimizing continuous
prompts (Yang et al., 2022; Zhang and Song, 2022),
and prompting model (Carlsson et al., 2022) to
steer text generation) or retrain (Chan et al., 2021;
Keskar et al., 2019; Gururangan et al., 2020) a
PLM to generate texts that meet the desired control
condition. These methods have shown significant
performance improvements in the field. However,
with the increasing size of PLM, they have become
resource-intensive to fine-tune or retrain. As a re-
sult, post-process approaches have become more

--- PAGE 10 ---
Method Time (second)
PPLM (Dathathri et al., 2020) 37.39
Mix-Match∗(Mireshghallah et al., 2022) 33.5
COLD∗(Qin et al., 2022) 33.6
DEXPERT (Liu et al., 2021a) 2.54
DisCup (Zhang and Song, 2022) 0.94
GPT2-large 0.78
RMT 0.88
Table 6: The generation efficiency of RMT. The cost
(time) for generating 20 tokens based on GPT2-large.
The results of methods marked with ∗are reported
from (Qin et al., 2022).
popular in the research community.
The post-process methods are dedicated to
guiding language model toward desired texts in
the decode-time stage using an auxiliary module.
Achieving the goal of generating attribute-specific
texts, PPLM (Dathathri et al., 2020) leverages a
simple attribute classifier to update the head hidden
layer of LM by gradient feedback. Then, Diffusion-
LM (Li et al., 2022) combines Diffusion-LM and
classifier-guided gradient updates to the continuous
sequence of latent variables, achieving plug-and-
play controllable generation. COLD (Qin et al.,
2022) proposes a novel decoding framework, by
combining energy-based model and gradient-based
sampling. Fudge (Yang and Klein, 2021) uses the
discriminator that contains future attribute infor-
mation to re-rank the token distribution produced
by a frozen GPT2 model. NeuroLogits (Lu et al.,
2022b, 2021) incorporates the lexical constraints
into the decode-time algorithm. In order to accel-
erate the generation process, GeDi (Krause et al.,
2021) and DEXPERT (Liu et al., 2021a) train an-
other smaller language model as generative dis-
criminators to guide the generation from a base
GPT2. Plug-and-Blend (Lin and Riedl, 2021) ex-
tends GeDi to controllable story generation by in-
troducing a planner module. Moreover, controlling
multiple control elements is also expolred (Kumar
et al., 2021; Yang et al., 2022).
Most decoder-time approaches either solely in-
tervene with the LM during the token selection
phase of generation, lacking planning capabilities,
or require multiple iterations, resulting in excessive
generation times. Our approach shares a plug-and-
play trait with decoder-time approaches. However,
the key distinction is that we use a lightweight
residual model to integrates control information
and multi-level contextual streams from the CLM,
enabling fine-gained content planning and efficient
text generation.6 Discussion and Future Work
RMT still suffer some the limitations. (1) Chal-
lenges in applying RMT to close-ended CLMs.
Presently, the application of RMT needs to obtain
the last hidden states or the logits of CLM, thus
applying RMT to some commercial CLMs, e,g.,
GPT-4, still faces challenges. This is also a com-
mon problem of all plugin-style CTG. (2) RMT
does not focus on commonsense, may result in gen-
erating texts which are not confirmed to common-
sense sometimes. This issue could be potentially
relieved by introducing external knowledge graph
in the future work.
Up to now, it is still challenging to avoid the
factual errors that appeared in the generated text for
large-scale causal language models, which RMT
does not address either. A promising future work is
to combine RMT and information retrieval systems
to enhance the factual accuracy of those generative
models. Moreover, RMT could also be used to
encode personal profiles and build personalized
chatbots, or fusion with image information so as to
be applied to multi-modal scenes.
7 Conclusion
In this paper, we have proposed a new CTG alter-
native, which leverages a residual model to steer
the CLM to generate desired text noninvasively.
Additionally, we propose a residual memory trans-
former, a novel encoder-decoder architecture, to
fuse the raw contextual text, generative stream of
CLM, and control information in a shot, thus bet-
ter collaborating and controlling the generation of
CLM. The experiments show that RMT exhibits
better performance in flexibility, control granular-
ity, and efficiency, making it a compelling solution
for controllable text generation.
References
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,

--- PAGE 11 ---
volume 33, pages 1877–1901. Curran Associates,
Inc.
Fredrik Carlsson, Joey Öhman, Fangyu Liu, Severine
Verlinden, Joakim Nivre, and Magnus Sahlgren. 2022.
Fine-grained controllable text generation using non-
residual prompting. In Proceedings of the 60th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 6837–
6857, Dublin, Ireland. Association for Computational
Linguistics.
Alvin Chan, Yew-Soon Ong, Bill Pung, Aston Zhang,
and Jie Fu. 2021. Cocon: A self-supervised approach
for controlled text generation. In International Con-
ference on Learning Representations .
Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane
Hung, Eric Frank, Piero Molino, Jason Yosinski, and
Rosanne Liu. 2020. Plug and play language models:
A simple approach to controlled text generation. In
International Conference on Learning Representa-
tions .
Suchin Gururangan, Ana Marasovi ´c, Swabha
Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
and Noah A. Smith. 2020. Don’t stop pretraining:
Adapt language models to domains and tasks. In
Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics , pages
8342–8360, Online. Association for Computational
Linguistics.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. 2016. Deep residual learning for image recogni-
tion. In 2016 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2016, Las Vegas,
NV , USA, June 27-30, 2016 , pages 770–778. IEEE
Computer Society.
Zhiting Hu and Li Erran Li. 2021. A causal lens for
controllable text generation. In Advances in Neural
Information Processing Systems , volume 34, pages
24941–24955. Curran Associates, Inc.
Nitish Shirish Keskar, Bryan McCann, Lav Varsh-
ney, Caiming Xiong, and Richard Socher. 2019.
CTRL - A Conditional Transformer Language
Model for Controllable Generation. arXiv preprint
arXiv:1909.05858 .
Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann,
Nitish Shirish Keskar, Shafiq Joty, Richard Socher,
and Nazneen Fatema Rajani. 2021. GeDi: Gener-
ative discriminator guided sequence generation. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2021 , pages 4929–4952, Punta
Cana, Dominican Republic. Association for Compu-
tational Linguistics.
Sachin Kumar, Eric Malmi, Aliaksei Severyn, and Yulia
Tsvetkov. 2021. Controlled text generation as con-
tinuous optimization with multiple constraints. In
Advances in Neural Information Processing Systems .Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2020.
BART: Denoising sequence-to-sequence pre-training
for natural language generation, translation, and com-
prehension. In Proceedings of the 58th Annual Meet-
ing of the Association for Computational Linguistics ,
pages 7871–7880, Online. Association for Computa-
tional Linguistics.
Xiang Li, John Thickstun, Ishaan Gulrajani, Percy S
Liang, and Tatsunori B Hashimoto. 2022. Diffusion-
lm improves controllable text generation. Advances
in Neural Information Processing Systems , 35:4328–
4343.
Bill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei
Zhou, Chandra Bhagavatula, Yejin Choi, and Xiang
Ren. 2020. CommonGen: A constrained text gen-
eration challenge for generative commonsense rea-
soning. In Findings of the Association for Computa-
tional Linguistics: EMNLP 2020 , pages 1823–1840,
Online. Association for Computational Linguistics.
Zhiyu Lin and Mark O Riedl. 2021. Plug-and-blend:
A framework for plug-and-play controllable story
generation with sketches. In Proceedings of the AAAI
Conference on Artificial Intelligence and Interactive
Digital Entertainment , volume 17, pages 58–65.
Alisa Liu, Maarten Sap, Ximing Lu, Swabha
Swayamdipta, Chandra Bhagavatula, Noah A. Smith,
and Yejin Choi. 2021a. DExperts: Decoding-time
controlled text generation with experts and anti-
experts. In Proceedings of the 59th Annual Meet-
ing of the Association for Computational Linguistics
and the 11th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers) ,
pages 6691–6706, Online. Association for Computa-
tional Linguistics.
Ye Liu, Yao Wan, Lifang He, Hao Peng, and S Yu Philip.
2021b. Kg-bart: Knowledge graph-augmented bart
for generative commonsense reasoning. In Proceed-
ings of the AAAI Conference on Artificial Intelligence ,
volume 35, pages 6418–6425.
Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang,
Lianhui Qin, Peter West, Prithviraj Ammanabrolu,
and Yejin Choi. 2022a. Quark: Controllable text
generation with reinforced unlearning. Advances in
neural information processing systems , 35:27591–
27609.
Ximing Lu, Sean Welleck, Peter West, Liwei Jiang,
Jungo Kasai, Daniel Khashabi, Ronan Le Bras, Lian-
hui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith,
and Yejin Choi. 2022b. NeuroLogic a*esque de-
coding: Constrained text generation with lookahead
heuristics. In Proceedings of the 2022 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 780–799, Seattle, United States.
Association for Computational Linguistics.

--- PAGE 12 ---
Ximing Lu, Peter West, Rowan Zellers, Ronan Le Bras,
Chandra Bhagavatula, and Yejin Choi. 2021. Neuro-
Logic decoding: (un)supervised neural text genera-
tion with predicate logic constraints. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 4288–4299,
Online. Association for Computational Linguistics.
Fatemehsadat Mireshghallah, Kartik Goyal, and Taylor
Berg-Kirkpatrick. 2022. Mix and match: Learning-
free controllable text generationusing energy lan-
guage models. In Proceedings of the 60th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 401–415,
Dublin, Ireland. Association for Computational Lin-
guistics.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instruc-
tions with human feedback. Advances in Neural
Information Processing Systems , 35:27730–27744.
Jing Qian, Li Dong, Yelong Shen, Furu Wei, and Weizhu
Chen. 2022. Controllable natural language genera-
tion with contrastive prefixes. In Findings of the As-
sociation for Computational Linguistics: ACL 2022 ,
pages 2912–2924, Dublin, Ireland. Association for
Computational Linguistics.
Lianhui Qin, Sean Welleck, Daniel Khashabi, and
Yejin Choi. 2022. Cold decoding: Energy-based
constrained text generation with langevin dynamics.
arXiv preprint arXiv:2202.11705 .
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank.
InProceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing , pages
1631–1642, Seattle, Washington, USA. Association
for Computational Linguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , volume 30. Curran Associates, Inc.
Kevin Yang and Dan Klein. 2021. FUDGE: Controlled
text generation with future discriminators. In Pro-
ceedings of the 2021 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies , pages
3511–3535, Online. Association for Computational
Linguistics.
Kexin Yang, Dayiheng Liu, Wenqiang Lei, Baosong
Yang, Mingfeng Xue, Boxing Chen, and Jun
Xie. 2022. Tailor: A prompt-based approach to
attribute-based controlled text generation. CoRR ,
abs/2204.13362.Hanqing Zhang and Dawei Song. 2022. DisCup: Dis-
criminator cooperative unlikelihood prompt-tuning
for controllable text generation. In Proceedings of
the 2022 Conference on Empirical Methods in Nat-
ural Language Processing , pages 3392–3406, Abu
Dhabi, United Arab Emirates. Association for Com-
putational Linguistics.
Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou,
and Dawei Song. 2022. A survey of controllable
text generation using transformer-based pre-trained
language models.
Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas
Guibas, and Jitendra Malik. 2020a. Side-tuning: a
baseline for network adaptation via additive side net-
works. In Computer Vision–ECCV 2020: 16th Euro-
pean Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part III 16 , pages 698–714. Springer.
Yizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan,
Chris Brockett, and Bill Dolan. 2020b. POINTER:
Constrained progressive text generation via insertion-
based generative pre-training. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pages 8649–8670,
Online. Association for Computational Linguistics.
Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan
Wilcox, Ryan Cotterell, and Mrinmaya Sachan. 2023.
Controlled text generation with natural language in-
structions. In Proceedings of the 40th International
Conference on Machine Learning , volume 202 of
Proceedings of Machine Learning Research , pages
42602–42613. PMLR.
Xu Zou, Da Yin, Qingyang Zhong, Hongxia Yang,
Zhilin Yang, and Jie Tang. 2021. Controllable gener-
ation from pre-trained language models via inverse
prompting. In Proceedings of the 27th ACM SIGKDD
Conference on Knowledge Discovery & Data Mining ,
KDD ’21, page 2450–2460, New York, NY , USA.
Association for Computing Machinery.

--- PAGE 13 ---
Appendices
In order to enhance the reproducibility of the exper-
imental results presented in our paper and provide
additional supporting details for the conclusions
outlined on the main page, we have included sup-
plementary materials in this section.
A Experimental Details
In our paper, all the experiments are conducted
on a single NVIDIA A6000 GPU with 48GB of
memory. We utilize the PyTorch deep learning
framework as our implementation tool and employ
the HuggingFace Transformers package to load a
pre-trained GPT model. To implement the Resid-
ual Memory Transformer (RMT), we utilize the
Transformer package in PyTorch-1.14.0, and the
number of multi-head attention is set to 8.
A.1 Pre-training
Data Details. The pre-training stage is consistently
applied across all experimental settings in our study.
For this stage, we collect a dataset of 4 million sen-
tences sampled from the Wikipedia corpus to serve
as the training data. To optimize computational
resources, we filtered out sentences that exceeded
a length of 200 words. These samples are then
noised using the following methods:
(1)Token Masking: We randomly mask tokens
with a special token. The proportion of
masked tokens in a sample is 20%.
(2)Token Deletion: We randomly delete tokens
from the text. The proportion of deleted to-
kens in a sample is 20%.
(3)Span Replacing: Text spans of various lengths
are randomly replaced with special tokens.
Each span is replaced with a single special
token. The proportion of replaced span tokens
in a sample is 20%.
(4)Text Rotation: A token is randomly selected,
and the text is rotated around it. This means
the text began with the selected token and
ended with its previous token.
In the total dataset, the ratios of Token Masking ,
Token Deletion ,Span Replacing , and Text Rotation
are 50%, 16.7%, 16.7%, and 16.7%, respectively.
Since RMT shares GPT’s tokenizer, which does not
reserve a special token, we select the word “xxx”
with no special meaning as the special token. wedid not specifically tune the hyperparameters re-
lated to the ratios of different denoised tasks during
pretraining. Instead, we followed BART’s [12] ap-
proach and selected the most important tasks, such
as Text Infilling (included by Token Masking in
RMT), to constitute the largest proportion of the
task. The remaining tasks were divided equally
among the other three tasks. In our experiments,
we observed that the specific choice of pretrain-
ing tasks did not significantly impact the results
in CTG. For instance, when we used Token Mask-
ing as the only pretraining task, the control perfor-
mance on CommonGen reached 92.2%. By adding
the other three tasks, the performance improved
slightly to 93.9%.
Training Details. During the pre-training pro-
cess, we set the following parameters for model
training: the batch size of 64, learning rate of 5e-
5, random seed of 42, and pre-training epoch of
1. We employ the AdamW as the optimizer; the
pre-training of the RMT is conducted on a single
NVIDIA A6000 GPU. Since the backpropagation
process does not need to propagate into the Casual
Language Model (CLM), the training of the model
was efficient. As a result, the entire pre-training
stage is completed in approximately 30 hours.
A.2 Word Inclusion
Baselines. We closely follow the methodology
of previous work, specifically the Non-Residual
Prompt (NRP) (Carlsson et al., 2022). To estab-
lish a reliable reference point for the CommonGen
dataset, we rely on the generation results of the
baselines provided in the NRP paper. And for the
NeuroLogic baseline, in order to keep the compara-
bility, we use the same decoding algorithm setting
with RMT, the beam search size is 4 and max gener-
ation length is set to 32. For the C2Gen dataset, we
utilize the checkpoint provided by NRP, to generate
the responding texts. However, the result shows
NRP could not work well. As a result, we give up
using NRP as a baseline in the C2Gen dataset.
Regarding the ChatGPT baseline, we employ
prompts specifically designed for generating results
on the CommonGen dataset. The specific prompt
is as follows:
Continue to write a sentence, and include [ word 1,
word 2, ..., word n], the sentence length should be
within 15 words.
When it comes to the C2Gen dataset, the prompt
instruction of ChatGPT is:
Continue to write a sentence, and include [ word 1,

--- PAGE 14 ---
Name Value
num_beams 4
top_p 0.7
repetition_penalty 1.25
no_repeat_ngram_size 3
Table 7: Hyper-parameters for text generation in the
inference stage.
word 2, ..., word n], the sentence length should be
within 15 words. Context: [context text].
Details of RMT. In our approach, we randomly
select training data from the Wikipedia corpus and
fuse the training data of CommonGen. Specifically,
for the setting involving without-context word in-
clusion and sentence length control, we gather a
total of 158,343 samples. To ensure manageable
sentence lengths, we impose a limit of 32 tokens
per sentence. To choose the appropriate checkpoint
during training, we utilize the CommonGen vali-
dation dataset and selected the checkpoint based
on coverage metrics. For the with-context setting,
we collect 132,170 samples, each with a sentence
length constraint of 128 tokens, and constructed a
validation dataset to choose the checkpoint.
Every sample is composed of two parts: context
and target. We use spacy library5to extract the
keywords in the target text, of which are tagged
with VERB ,NOUN . The number of keywords in each
sentence is restricted to the range from 3 to 5. The
context length, target length, and keywords are
used as control instructions to steer the generation
process of the CLM. All above training data will
be released and open source, once we have cleaned
up our project.
During the fine-tuning stage, the learning rate of
the AdamW optimizer is set to 1e-4, and the max
epoch and batch size are 6 and 64, respectively.
During the inference stage, For the control task
without context (i.e., CommonGen dataset), we
use the specific word “The” as context; the core
parameters of the decoding algorithm are shown in
Table 7.
A.3 Sentiment Control
Baselines. In this part, we build upon the baselines
established by DisCup (Zhang and Song, 2022)
and DEXPERT (Liu et al., 2021a), adopting their
settings, checkpoints, and generated results. The re-
5https://spacy.io/sults of baselines, including PPLM (Dathathri et al.,
2020), CTRL (Keskar et al., 2019), GEDI (Krause
et al., 2021), and DEXPERT, are calculated from
the generated texts provided DEXPERT (Liu et al.,
2021a). As for DisCup, we use the checkpoint from
the authors, the depth of re-ranked candidate token
is set to top-k=9 to generate continuations using
the prompts in test datasets.
Details of RMT. In our own approach, during
the training stage, we incorporate the discriminator-
involved loss introduced by DisCup and utilize the
attribute classifier provided by their open-source
code. The instructions for sentiment control are
“The sentiment: positive ” and “ The sentiment: neg-
ative ”, respectively. We set the size of re-ranked
candidate tokens to 90, and choose a low temper-
ature value of 0.005 and a learning rate of 2e-4.
During the inference stage, we employ the same
generation parameters as those used in the word
inclusion experiments described in Table 7.
B CTG Approaches Comparison
B.1 CTG Comparison
We select some notable works in controllable text
generation (CTG) and compare their characteristics.
The results are shown in Table 8. We compare them
according to three key features: (1) Fine-grained
Control suggests whether the method can poten-
tially be applied to control various elements, such
as emotion, topic, toxicity avoidance, word con-
straints, and structural control (e.g., personal pro-
file, syntax tree). It indicates the method’s ability to
handle different control aspects effectively. (2) Ef-
ficient Generation evaluates whether the time-cost
of text generation using the method is comparable
to that of the original language model (LM). (3)
Non-intrusive considers whether the method can
operate without disturbing the language model’s
original textual stream, mitigating the harms of
LMs. A non-intrusive approach ensures flexibil-
ity and practicality in controlling LMs. Specif-
ically, the non-intrusive fashion of RMT, which
decouples the control module from the base-CLM,
offers numerous exciting possibilities. One sig-
nificant advantage is that we no longer need to
fine-tune the base CLM for specific control tasks,
avoiding any adverse impact on its generaliza-
tion ability (i.e.,keeping the CLM from becom-
ing task-specific model). Moreover, this design
enables us to dynamically adjust the fine-grained
control requirements during the text generation pro-

--- PAGE 15 ---
CTG Approach Fine-gained Control Efficient Generation Non-intursive
PPLM (Dathathri et al., 2020) ✘ ✘ ✔
CTRL (Keskar et al., 2019) ✔ ✔ ✘
NeuroLogit (Lu et al., 2022b) ✘ ✘ ✔
GEDI (Krause et al., 2021) ✘ ✔ ✔
DEXPERT (Liu et al., 2021a) ✘ ✔ ✔
FUDGE (Yang and Klein, 2021) ✘ ✘ ✔
COLD (Qin et al., 2022) ✔ ✘ ✔
Diffsion-LM (Li et al., 2022) ✔ ✘ ✔
Mix&Match (Mireshghallah et al., 2022) ✘ ✘ ✔
CoCon (Chan et al., 2021) ✔ ✔ ✘
DisCup (Zhang and Song, 2022) ✘ ✔ ✘
NRP (Carlsson et al., 2022) ✔ ✔ ?
RMT(ours) ✔ ✔ ✔
Table 8: An overview of the key features in CTG approaches.
cess at any time. For instance, we can flexibly
control the sentiment or topic of individual sen-
tences/paragraphs within a story by incorporating
RMT into a story-writing CLM. Additionally, we
can enhance a well-trained CLM, such as ChatGPT,
with external grounded documents, making it pos-
sible to determine whether external control infor-
mation should be Intervened through an automatic
selection model (i.e., switching b/w controlled and
free-style generation).
The analysis suggests that controlling the gener-
ation process of LMs while balancing control flexi-
bility, control granularity, and generating efficiency
remains an open challenge. RMT shows potential
in addressing these issues, making it a compelling
solution for controllable text generation.
B.2 RMT v.s NRP
The most similar to RMT is Non-Residual Prompt
(NRP) approach(Carlsson et al., 2022), which also
requires pretraining and fine-tuning. Therefore, we
make a thorough discussion of differences of RMT
w.r.t NRP.
•Principle reasonability. The principle of
RMT appears intuitively more reasonable.
RMT utilizes cross-attention to uniformly ap-
ply control conditions to each generated to-
ken, alleviating the “distance bias” problem
that arises with increasing distance from the
control prompts. By contrast, NRP first learns
the fixed position-invariant values and then
combines non-residual attention (i.e., causal
attention) to address this issue. Intuitively,
NRP still uses distance-aware causal atten-tion to encode control conditions, and seems
inherently difficult to fully overcome the “dis-
tance bias” issue. In contrast, cross-attention
in RMT enables the final representation of
the current generation step to look up the con-
trol condition position-independently, which
is more intuitively appealing.
•Training efficiency. RMT holds a clear ad-
vantage over NRP in computational source
cost. RMT efficiently reuses the output of the
base CLM, requiring fewer pretraining train-
ing data and expediting the pretraining pro-
cess. Additionally, RMT is lightweight and is
built on the top layer of the base-CLM. Hence,
the backpropagation process does not need to
propagate into the CLM, which will save a
huge training time and GPU memory. Con-
versely, NRP, despite being initialized from
the base CLM, necessitates training a separate
base-CLM and involves multi-stage pretrain-
ing, leading to higher computational costs.
As discussed in Appendix A.1, RMT’s pre-
training was conducted on a single NVIDIA
A6000 GPU, completed in approximately 30
hours. In contrast, according to the report
in the paper on NRP, they incurred approxi-
mately 400 GPU hours (using DGX-100 ma-
chines) for their training process.
•Parameter efficiency. RMT is more
parameter-efficient compared to NRP. NRP re-
quires training additional base-CLM, whereas
RMT only necessitates approximately 1/6 of
the base-CLM’s parameters.

--- PAGE 16 ---
C Human Evaluation
We conduct the human evaluation for the word in-
clusion and sentiment control tasks. To ensure the
reliability of the evaluation, we invite three highly
educated experts as evaluators, and each evaluator
has expertise in English. For the evaluation pro-
cess, we establish a unified criterion using a scale
of 10 for each aspect. The scale ranges from 1 to 3
for poor performance, 4 to 7 for satisfactory perfor-
mance, and 8 to 10 for excellent performance. This
criterion allows evaluators to provide consistent
and comparable scores for each aspect.
Regarding the CommonGen dataset and C2Gen,
we randomly select 25 samples from test dataset
for each CTG approach, resulting in a total of 25*7
=175 samples for CommonGen and 50 samples for
C2Gen. For the sentiment control experiments, we
create 10 positive-steering prompts composed of
neutral and positive prompts, as well as 10 negative-
steering prompts composed of neutral and negative
prompts. Each prompt is evaluated using three
CTG approaches, resulting in a total of 60 samples
for the final human evaluation. To obtain the final
results, we calculate the average score for each
metric for every CTG method under comparison,
based on the evaluations provided by the experts.
D Qualitative Examples
In this section, we list some examples generated
by different CTG approaches. Specifically, Table 9
shows the cases on the CommonGen dataset, and
Table 10 presents the cases in the C2Gen dataset.
As for attribute control, we show the examples on
the adversarial steering setting (i.e., the polarity of
the prompt’s sentiment is opposite from the target
sentiment) as shown in table 11.

--- PAGE 17 ---
Target Words : swim, pool, reach, end
POINT (Zhang et al., 2020b)water at the south end of the main swimming pool allows one to
reach to the top where they can then swim to the main pool.
NeuroLogic (Lu et al., 2022b)The end of the world is coming, and it will be swimming in blood
pools of blood, and the blood will be so thick that it will reach the sky
NRA (Carlsson et al., 2022)The swim pool at the end of the pool can be used to reach the other
side of the building.
ChatGPT I can’t wait to swim in the pool and reach the end.
RMT (ours)The swimming pool has reached its maximum size and is at the end
of its life-spans.
Target words : look, heart, writer, leave, goer
POINT (Zhang et al., 2020b)i goer in my heart, but you will not leave me alone, do you look
like a writer?.
KG-BART (Liu et al., 2021b)a group of people looking at each other and leaving their hearts
in the shape of a heart with a writer
NeuroLogic (Lu et al., 2022b)The first thing I did was look at the goers and the heart rate,
and I left the room and went to the writers room.
NRA (Carlsson et al., 2022)The writer-producer would leave the show to look for a new job
as a goer.
ChatGPTThe writer’s heart sank as they watched the goer leave without
a second look.
RMT (ours)The writer leaves a heart-shaped letter for the goers to look at on
the red carpet.
Table 9: The examples generated by different CTG approaches on the CommonGen dataset (the without-context
setting).

--- PAGE 18 ---
Target Words & Context
1Target words: enjoy#friend#knit
Context: Friendship is a relationship of mutual affection between people. It is a stronger form of
interpersonal bond than an association, and has been studied in various academic fields. Such as
communication, sociology, social psychology, anthropology, and philosophy.
2Target Words: background#crowd#pose
Context: I love photography. One of my favorite things to do is to take pictures of people modeling
throughout the city. The beach is one of my favorite places because of the scenery and the
atmosphere there.
3Target Words: cigarette#sstreet#walk
Context: James mulled over the fight with his girlfriend. The city noises were all blocked out by
him. He dropped his lighter and then picked it up again.
ChatGPT
1 My friend and I enjoy spending time together, often knitting and chatting.
2 The background of the beach makes for a stunning setting for a crowd pose.
3 As he lit his cigarette, he decided to take a walk down the street.
RMT
1 Friendship can be enjoyed by both men and women and it is best knit between two close friends.
2 It is so much fun to pose in the background with a crowd of people.
3 As he walked up to the street, his cigarettes were gone from his mouth.
Table 10: The examples generated by different CTG approaches on the C2Gen dataset (the with-context setting).

--- PAGE 19 ---
Negative Prompt: The real problem, though, is that we don (Positive Steering )
PPLM (Dathathri et al., 2020)The real problem, though, is that we don ’t have a sensible way of using them to share
meta-information. The investigation, no evidenced-
CTRL (Keskar et al., 2019)The real problem, though, is that we don ’t know how long the product has been in use.
It should have been tested for freshness and potency befor
GEDI (Krause et al., 2021)The real problem, though, is that we don ’t teach children all of these tools.
And so what can parents do to help them acquire all this
DEXPERT (Liu et al., 2021a)The real problem, though, is that we don ’t know the full extent of what social causes
contribute to obesity and other related health conditions, says study
DisCup (Zhang and Song, 2022)The real problem, though, is that we don ’t know what to do with our children but that’s
OK too because the world has to grow but
RMT (Ours)The real problem, though, is that we don ’t question whether ’good’ or ’bad’ will be better
but keep putting our faith into not
Positive Prompt: A President unafraid to stand (Negative Steering)
PPLM (Dathathri et al., 2020)A President unafraid to stand out: No matter what the circumstances, Trump will never be
the first president to fight one
CTRL (Keskar et al., 2019)A President unafraid to stand up for what he believes in. This one could be added to
my library. Rating: 2.0 I
GEDI (Krause et al., 2021)A President unafraid to stand by NRA shameless hypocrites tweeted something totally me
aningless that called a black woman a ‘rapist’. Neither hurtful
DEXPERT (Liu et al., 2021a)A President unafraid to stand on a bomb, is he?That was bad enough to threaten
the security of every armed
DisCup (Zhang and Song, 2022)A President unafraid to stand alone? I’m not a good person or good
for a President to be? And even if I am
RMT (ours)A President unafraid to stand up against corruption? Unfortunately,
there appears little hope right now of getting any meaningful changes. Since Congress
Table 11: The generation examples of typical CTG approaches in the sentiment control task. We deliberately select
two challenging prompts, and ask the above CTG approaches to conduct the adversarial steering of sentiment
polarity.
