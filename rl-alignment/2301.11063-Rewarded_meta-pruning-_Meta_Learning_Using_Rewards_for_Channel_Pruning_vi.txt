# 2301.11063.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2301.11063.pdf
# Kích thước tệp: 1347625 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Cắt tỉa meta được thưởng: Meta Learning sử dụng phần thưởng cho việc cắt tỉa kênh
Athul Shibu, Abhishek Kumar, Heechul Jung, Dong-Gyu Lee
Khoa Trí tuệ nhân tạo, Đại học Quốc gia Kyungpook
fathulshibu, abhishek.ai, heechul, dglee g@knu.ac.kr
Tóm tắt
Mạng nơ-ron tích chập (CNN) có số lượng tham số lớn và cần tài nguyên phần cứng đáng kể để tính toán, vì vậy các thiết bị biên gặp khó khăn trong việc chạy các mạng cấp cao. Bài báo này đề xuất một phương pháp mới để giảm các tham số và FLOP nhằm hiệu quả tính toán trong các mô hình học sâu. Chúng tôi giới thiệu các hệ số độ chính xác và hiệu quả để kiểm soát sự đánh đổi giữa độ chính xác của mạng và hiệu quả tính toán của nó. Thuật toán cắt tỉa meta được thưởng đề xuất huấn luyện một mạng để tạo ra trọng số cho một mô hình đã được cắt tỉa được chọn dựa trên các tham số gần đúng của mô hình cuối cùng bằng cách kiểm soát các tương tác sử dụng hàm phần thưởng. Hàm phần thưởng cho phép kiểm soát nhiều hơn các chỉ số của mô hình đã cắt tỉa cuối cùng. Các thí nghiệm mở rộng chứng minh hiệu suất vượt trội của phương pháp đề xuất so với các phương pháp tiên tiến trong việc cắt tỉa mạng ResNet-50, MobileNetV1 và MobileNetV2.

1. Giới thiệu
Mạng nơ-ron tích chập (CNN) đã được chứng minh đạt được kết quả tiên tiến trong nhiều tác vụ thị giác máy tính [23,26,29–31]. Tuy nhiên, việc huấn luyện các tham số của CNN cần một lượng lớn dữ liệu được gán nhãn. Hơn nữa, một lượng lớn tài nguyên phần cứng cũng được yêu cầu để huấn luyện một lượng lớn dữ liệu huấn luyện.
Gần đây, việc cắt tỉa mạng đã trở thành một chủ đề quan trọng để đơn giản hóa và tăng tốc các CNN lớn [24, 30, 52, 58].
Nhiều vấn đề cần quan tâm khi cố gắng cắt tỉa mạng, chẳng hạn như cấu trúc [32], tính liên tục [42] hoặc khả năng mở rộng [41].
Có chủ yếu hai cách để nén mạng nơ-ron: cắt tỉa trọng số [3,12,50] và cắt tỉa kênh [8,10,38]. Giảm tham số bằng cách cắt tỉa các kết nối là cách trực quan nhất để cắt tỉa mạng. Cắt tỉa trọng số bao gồm việc xác định các trọng số hiệu suất thấp để được cắt tỉa [14]. Điều này liên quan đến việc đơn giản loại bỏ các trọng số có độ lớn nhỏ, điều này dễ thực hiện [15]. Tuy nhiên, hầu hết các framework không thể tăng tốc ma trận thưa trong quá trình tính toán. Để có được nén thực sự và tăng tốc, nó cần phần mềm được định nghĩa cụ thể [24] hoặc phần cứng [13] để xử lý tính thưa. Chi phí thực tế không bị ảnh hưởng bất kể có bao nhiêu trọng số được cắt tỉa.

Do đó, cắt tỉa kênh liên quan đến việc loại bỏ toàn bộ bộ lọc thay vì chỉ đơn giản giảm giá trị trọng số về không được ưa chuộng hơn. [27, 39]. Sự ưa chuộng này xuất phát từ thực tế rằng cắt tỉa kênh có thể loại bỏ toàn bộ bộ lọc, tạo ra một mô hình với tính thưa có cấu trúc [20].
Với tính thưa có cấu trúc, mô hình có thể tận dụng đầy đủ các thư viện Chương trình con Đại số Tuyến tính Cơ bản (BLAS) hiệu quả cao để đạt được tăng tốc tốt hơn. Điều này làm cho mô hình đã cắt tỉa có cấu trúc hơn và đạt được tăng tốc thực tế [13].

MetaPruning [40] là một phương pháp cắt tỉa kênh như vậy có thể đạt được tăng tốc của CNN. Phương pháp trung tâm của MetaPruning là tạo ra trọng số cho các cấu trúc đã cắt tỉa thay vì cắt tỉa trọng số hoặc bộ lọc của mạng hiện có. Độ chính xác của các mô hình chưa được huấn luyện được tính toán để xếp hạng mỗi Vector Mã hóa Mạng (NEV).
Các thuật toán tiến hóa, được thúc đẩy bởi các quá trình tiến hóa tự nhiên [28], được sử dụng để tìm NEV tạo ra mô hình có độ chính xác cao nhất. Tuy nhiên, do đó, MetaPruning chỉ có thể chọn độ chính xác tốt nhất được chọn từ một phạm vi FLOP đã đặt. Vì vậy thuật toán tìm độ chính xác cao nhất trong phạm vi FLOP được xác định trước thay vì cố gắng tìm sự cân bằng phù hợp giữa việc hy sinh độ chính xác và giảm FLOP.

Bài báo này cố gắng giải quyết vấn đề này. Trong Cắt tỉa meta được thưởng đề xuất, thay vì tìm các NEV tạo ra độ chính xác cao nhất, mô hình cố gắng cân bằng độ chính xác với FLOP của mạng để tìm độ chính xác cao nhất có thể cho các FLOP đã cho. Trong MetaPruning, phần thưởng tỷ lệ thuận trực tiếp với độ chính xác, bởi vì phần thưởng là độ chính xác. Vì vậy việc tăng phần thưởng của các đột biến tiếp theo thấp hơn so với những đột biến của Cắt tỉa meta được thưởng, nơi phần thưởng tỷ lệ thuận trực tiếp với bình phương độ chính xác. Đồng thời, Cắt tỉa meta được thưởng có thể kiểm soát FLOP của mô hình cuối cùng bằng cách tính toán một điểm số có tính đến cả độ chính xác và FLOP để tìm các mô hình có độ chính xác cao và FLOP thấp. Điểm số này, đó là phần thưởng, có thể được điều chỉnh thêm để bao gồm các tham số khác nhau và kiểm soát các chỉ số của mô hình đã cắt tỉa, cũng như cách các tham số tương tác với nhau.
arXiv:2301.11063v1  [cs.CV]  26 Jan 2023

--- TRANG 2 ---
số khác nhau và kiểm soát các chỉ số của mô hình đã cắt tỉa, cũng như cách các tham số tương tác với nhau.
Đóng góp của chúng tôi nằm trong ba khía cạnh:
• Chúng tôi đề xuất một phương pháp cắt tỉa kênh, Cắt tỉa meta được thưởng, có thể học cách gán trọng số cho các mạng đã được cắt tỉa.
• Chúng tôi khám phá tầm quan trọng của các hàm phần thưởng và các đặc điểm để định nghĩa một hàm phần thưởng hiệu quả.
• Chúng tôi chứng minh thực nghiệm tính ưu việt của phương pháp cắt tỉa đề xuất trên các CNN được huấn luyện trước có sẵn công khai; ResNet-50, MobileNetV1, và MobileNetV2.

2. Công trình liên quan
Giả thuyết vé số may mắn nói rằng một mạng nơ-ron dày đặc được khởi tạo ngẫu nhiên chứa một mạng con mà khi được huấn luyện riêng biệt, có thể cho kết quả tốt hoặc thậm chí vượt trội so với mạng gốc [12]. Nói cách khác, một mạng đã cắt tỉa tiêu chuẩn có thể có độ chính xác tương đương, nếu không muốn nói là cao hơn, so với mạng gốc. Có một số phương pháp để tìm những vé đúng.

Cắt tỉa mạng không có cấu trúc: Các phương pháp cắt tỉa ngẫu nhiên khác nhau [3, 12, 50] dựa vào việc cắt tỉa các tham số ngẫu nhiên dựa trên các yếu tố khác nhau của trọng số. [60] sử dụng chuẩn L1 và L2 của mỗi trọng số để tính toán tầm quan trọng của chúng. Mô hình đã cắt tỉa cuối cùng được tạo ra bằng cách cắt tỉa các trọng số ít quan trọng hơn. [19] tính toán trung vị hình học của các trọng số trong khi [46] sử dụng khai triển chuỗi Taylor phức tạp để tính toán hàm trọng số. Các phương pháp cắt tỉa trọng số khác như [44] và [35] sử dụng tầm quan trọng KL-divergence và độ nhạy thực nghiệm của các trọng số tương ứng để cắt tỉa chúng.

Cắt tỉa mạng có cấu trúc: Trong các phương pháp khác, AutoPruner [43] tích hợp lựa chọn bộ lọc vào quá trình huấn luyện mô hình để mạng được tinh chỉnh có thể tự động chọn các bộ lọc không quan trọng. Lựa chọn Cấu trúc Thưa (SSS) [25] đề xuất việc giới thiệu một tham số mới, yếu tố tỷ lệ, điều chỉnh đầu ra của các cấu trúc cụ thể. Việc chính quy hóa thưa trên các yếu tố tỷ lệ này đẩy chúng về không trong quá trình huấn luyện. Cắt tỉa kênh nhận biết phân biệt (DCP) [62] có thể tìm các kênh với sức mạnh phân biệt thực sự và cập nhật mô hình bằng cách cắt tỉa từng giai đoạn sử dụng các mất mát nhận biết phân biệt. DCP thích ứng [38] giới thiệu một mất mát nhận biết phân biệt bổ sung sử dụng mất mát thứ p, và các mất mát bổ sung như mất mát biên góc cộng [8]. AutoML cho Nén Mô hình (AMC) [18] tận dụng học tăng cường để tự động lấy mẫu không gian thiết kế và cải thiện chất lượng nén mô hình. Các phương pháp đơn giản hơn như HRank [36] xác định thứ hạng của các bản đồ đặc trưng được tạo bởi các bộ lọc để xếp hạng các bộ lọc và hiệu quả của chúng đối với độ chính xác cuối cùng. Tuy nhiên điều này mất nhiều epoch hơn để huấn luyện sau khi cắt tỉa. [61] tận dụng giả thuyết vé số may mắn để tìm kiếm tham lam qua một mạng, tìm các mạng con với mất mát thấp hơn các mạng được huấn luyện với gradient descent. Các thuật toán cắt tỉa được lấy cảm hứng từ lý thuyết Hebbian, như Fire Together Wire Together (FTWT) [10], cắt tỉa các bộ lọc dựa trên mặt nạ nhị phân của mỗi lớp và kích hoạt của lớp trước đó.

Meta-learning: Meta-learning là việc học các thuật toán từ các thuật toán học khác [11, 40, 53]. Về cơ bản có ba mô hình [54] trong meta-learning; meta-optimizer, meta-representation và meta-objective.
Meta-optimizer là optimizer được sử dụng để học tối ưu hóa trong vòng lặp ngoài của meta-learning [21]. Meta-representation nhằm học và cập nhật meta-knowledge [11]. Cuối cùng, Meta-objective là nhiệm vụ cuối cùng đạt được sau khi hoàn thành huấn luyện [34].

Học cắt tỉa bộ lọc: Các thuật toán Học tăng cường (RL) đã được sử dụng để tạo ra các mô tả kiến trúc mạng sử dụng Mạng nơ-ron Hồi quy được huấn luyện qua phương pháp gradient chính sách [63]. Điều tương tự cũng đã được thực hiện sử dụng Q-learning [49]. Thuật toán Try-and-learn [24] sử dụng RL để tính toán phần thưởng của mỗi bộ lọc và những phần thưởng này sau đó được sử dụng để xếp hạng các bộ lọc. Nó cắt tỉa quyết liệt các bộ lọc trong mạng cơ sở trong khi duy trì hiệu suất ở mức mong muốn. Mô hình tính toán phần thưởng như một tích của độ chính xác và số hạng hiệu quả, và sau đó sử dụng REINFORCE [55] để ước tính các gradient. Các gradient sau đó có thể được sử dụng để tính toán mất mát và huấn luyện mạng, mạng này học cắt tỉa các bộ lọc. Hàm phần thưởng làm cho việc kiểm soát sự đánh đổi giữa hiệu suất mạng và quy mô mà không cần can thiệp của con người trở nên khả thi. Thuật toán try-and-learn tự động khám phá các bộ lọc dư thừa và loại bỏ chúng bằng cách lặp lại quá trình cho mỗi lớp.

Tìm kiếm kiến trúc nơ-ron: Nhiều phương pháp đã được đề xuất để tìm kiếm các cấu trúc mạng tối ưu từ các kiến trúc nơ-ron có thể [51, 56, 63]. Có chủ yếu năm phương pháp được sử dụng để tìm kiếm mạng được tối ưu hóa; học tăng cường [1, 63], thuật toán di truyền [47, 57], các phương pháp dựa trên gradient [56], chia sẻ tham số [5] và dự đoán trọng số [4]. [63] sử dụng RL để tối ưu hóa các mạng được tạo ra từ các mô tả mô hình được đưa ra bởi một mạng hồi quy. MetaQNN [1] sử dụng RL với chiến lược khám phá tham lam để tạo ra các kiến trúc CNN hiệu suất cao. Các thuật toán di truyền được sử dụng để giải quyết các vấn đề tìm kiếm và tối ưu hóa sử dụng các toán tử lấy cảm hứng từ sinh học [57]. [47] sử dụng thuật toán di truyền để khám phá các kiến trúc mạng nơ-ron, giảm thiểu vai trò của con người trong thiết kế. Học dựa trên gradient cho phép một mạng tối ưu hóa hiệu quả các trường hợp mới của một nhiệm vụ [51]. FBNets (Facebook-Berkeley-Nets) [56] sử dụng phương pháp dựa trên gradient để tối ưu hóa CNN được tạo ra bởi một framework tìm kiếm kiến trúc nơ-ron có thể vi phân.

--- TRANG 3 ---
3. Cắt tỉa meta được thưởng
Thuật toán Cắt tỉa meta được thưởng đề xuất sử dụng hệ số phần thưởng để kiểm soát sự đánh đổi giữa độ chính xác và hiệu quả của mỗi mô hình thay vì tìm mô hình có độ chính xác cao nhất trong phạm vi FLOP đã đặt trước. Mục tiêu là tối đa hóa phần thưởng, tỷ lệ thuận trực tiếp với độ chính xác và tỷ lệ nghịch với hiệu quả của mô hình. Phương pháp này được thực hiện trong ba giai đoạn: huấn luyện, tìm kiếm và huấn luyện lại như được hiển thị trong Thuật toán 1.

3.1. Huấn luyện
Hầu hết các CNN phổ biến [16, 22, 48] chủ yếu sử dụng ba loại lớp; khối tích chập, bottleneck, và các lớp tuyến tính. Tỷ lệ kênh đại diện cho kích thước của mỗi lớp. Đối với khối tích chập ban đầu và mỗi loại bottleneck, batch normalization với các kích thước từ 10% đến 100% kích thước của kiến trúc gốc, được phân phối đều giữa 31 trọng số ban đầu.

Hình 1. Phương pháp huấn luyện ngẫu nhiên

Mỗi mô hình được định nghĩa bởi một NEV, đó là một danh sách các số ngẫu nhiên định nghĩa tỷ lệ mà mỗi lớp được cắt tỉa từ 31 tỷ lệ và các trọng số Batch Normalisation tương ứng của chúng. Nó cũng tạo ra các lớp tuyến tính ở tỷ lệ được định nghĩa bởi NEV. NEV được chuyển vào hai lớp Kết nối Đầy đủ (FC) tạo ra ma trận trọng số như được hiển thị trong Hình 1. Các trọng số của những lớp này được huấn luyện bằng gradient của các trọng số được tạo ra được tính toán w.r.t các trọng số của các lớp FC. Các trọng số được tạo ra cho mỗi tổ hợp kích thước bộ lọc đầu ra được ánh xạ duy nhất bởi vì hàm chuyển đổi NEV thành trọng số là một hàm đơn ánh. Vì vậy, cho mỗi epoch, một NEV ngẫu nhiên tạo ra một mô hình mới với các trọng số ánh xạ một-một với không gian mẫu của các NEV. Những trọng số được tạo ra này được định hình lại. Mỗi giá trị Ci trong NEV C tương ứng với các kênh đầu ra của lớp i.

Khi một mô hình được tạo ra, nó được huấn luyện cho mỗi batch của dữ liệu huấn luyện với những trọng số ban đầu này, và mất mát cross-entropy được tính toán để cập nhật các trọng số. Mỗi mô hình về cơ bản là một lát của một mô hình hoàn chỉnh, nơi lát được định nghĩa bởi NEV. Dữ liệu validation không được sử dụng để validate mô hình, thay vào đó chỉ để đo lường tiến trình.

3.2. Tìm kiếm
Các mô hình được tạo ra từ các ứng viên NEV sử dụng các trọng số của mô hình đã được huấn luyện để tạo ra một mô hình đã cắt tỉa. Mỗi NEV do đó được chuyển đổi thành một mô hình đã cắt tỉa và phần thưởng được tính toán cho chúng. Tìm kiếm tiến hóa [45] sau đó được sử dụng để tìm NEV tốt nhất, từ đó tìm ra mô hình đã cắt tỉa tối ưu. Các trọng số ban đầu là các trọng số đã được huấn luyện, nhưng mô hình cuối cùng sẽ được huấn luyện từ đầu để loại bỏ bias trong mô hình được huấn luyện trước.

3.2.1 Tạo gen
Các ứng viên ngẫu nhiên được tạo ra để gieo hạt cho tìm kiếm tiến hóa. Mỗi gen được tạo ra như một danh sách các kích thước tương ứng với mô hình với các giá trị đại diện cho các trọng số từ từ điển. Tuy nhiên, vì các chỉ số của các mô hình được tạo ra bởi những NEV này cũng ngẫu nhiên, các siêu tham số tùy ý được sử dụng để kiểm soát mô hình cuối cùng. Một gen được coi là hợp lệ nếu FLOP của mô hình được tạo ra nằm giữa maxFLOPs và minFLOPs. FLOP của mỗi gen được lưu trữ như phần tử cuối cùng của NEV để giảm thời gian tính toán tổng thể. Điều này sau đó được thay thế bằng phần thưởng.

3.2.2 Phần thưởng và lựa chọn NEV
Các ứng viên được xếp hạng sau mỗi epoch theo phần thưởng, được tính toán như một tích của các hệ số độ chính xác và hiệu quả được đưa ra bởi Phương trình (2) và (3) cho mỗi NEV như được hiển thị trong Hình 2. Phần thưởng được tính toán theo công thức sau:

R(Gi) = α(Gi; ba) × β(Gi; bf).     (1)

Các hệ số độ chính xác và hiệu quả, được ký hiệu bởi α và β, được định nghĩa như:

α(Gi; ba) = ba / (ba - A(Gi))²,     (2)

β(Gi; bf) = log(bf / F(Gi)),     (3)

trong đó Gi biểu thị một gen có chỉ số i từ tất cả các gen ứng viên G, và A và F đại diện cho các hàm trả về độ chính xác và FLOP của mô hình được tạo ra sử dụng gen được chuyển vào chúng. Hệ số độ chính xác tăng theo cấp số nhân

--- TRANG 4 ---
Hình 2. Tính toán phần thưởng cho các vector mã hóa mạng.

với việc tăng độ chính xác của mô hình, nhưng khi nó tiếp cận độ chính xác cơ sở, giá trị có xu hướng tiến tới vô cùng. Vì mô hình không được tinh chỉnh, độ chính xác không đến đủ gần độ chính xác của mô hình cơ sở, ba, để phần thưởng đạt mức cao. Nếu chưng cất kiến thức được sử dụng để tăng độ chính xác của mô hình mới, độ chính xác cơ sở sẽ là độ chính xác của mô hình mới, từ đó loại bỏ bất kỳ tác động tiêu cực nào từ bản chất đối xứng của phương trình 2. Mặt khác, hệ số hiệu quả β giảm tuyến tính với FLOP tăng nhưng bị giới hạn bởi FLOP của mô hình gốc bf. Vì tỷ lệ cắt tỉa tỷ lệ nghịch với FLOP, hệ số hiệu quả thấp hơn tương ứng với tỷ lệ cắt tỉa cao hơn trong hầu hết các trường hợp. Hàm phần thưởng tỷ lệ thuận trực tiếp với độ chính xác và tỷ lệ cắt tỉa.

Hệ số độ chính xác tỷ lệ thuận trực tiếp với phần thưởng nhưng được điều tiết bởi hệ số hiệu quả. Điều này tạo ra sự cân bằng giữa chúng để độ chính xác cao không đạt được với chi phí của tỷ lệ cắt tỉa thấp trong mô hình cuối cùng.
Khi phần thưởng được tính toán, nó được lưu trữ như phần tử cuối cùng của NEV, sau đó được sử dụng để xếp hạng mỗi NEV. Top-50 NEV từ mỗi epoch được lưu trữ, sau đó 10 NEV tốt nhất trong số chúng được đột biến và lai ghép để có được các gen ứng viên cho epoch tiếp theo.

3.2.3 Đột biến và lai ghép
Các ứng viên tốt nhất từ mỗi epoch được đột biến và lai ghép để tạo ra các ứng viên cho epoch tiếp theo. Đột biến liên quan đến việc thay đổi một vài phần tử trong một gen để tạo ra một gen mới. Có 10% cơ hội cho mỗi phần tử trong một gen được thay đổi thành một phần tử hợp lệ ngẫu nhiên. Lai ghép là việc kết hợp hai gen ngẫu nhiên để tạo ra một gen mới. Một phần tử được chọn ngẫu nhiên từ một trong hai gen được chọn cho mỗi chỉ số. Các cấu hình kênh trong một vùng cục bộ của không gian cấu hình có xu hướng có các chỉ số tương tự [33], vì vậy các ứng viên mới cũng có độ chính xác và FLOP tương tự. Điều này làm cho phần thưởng của ít nhất ứng viên tốt nhất có xu hướng không giảm. Nếu phần thưởng không tăng trong quá nhiều epoch, tìm kiếm di truyền bị kẹt trong cực tiểu cục bộ. Tuy nhiên, vì tìm kiếm tiến hóa là một tìm kiếm không lồi nhiều chiều, các điểm tới hạn với lỗi lớn hơn nhiều so với cực tiểu toàn cục có khả năng là các điểm yên ngựa [6]. Nói cách khác, các cực tiểu cục bộ được tìm thấy có khả năng đủ gần với cực tiểu toàn cục, vì vậy tìm kiếm có thể được kết thúc. Không phải là không thể đột biến và lai ghép thêm gen có thể tìm thấy gen, nhưng tăng tỷ lệ đột biến và lai ghép sẽ ảnh hưởng đến tính toàn vẹn của tìm kiếm tiến hóa. Nếu không thể tạo ra đủ ứng viên mới từ hai toán tử tiến hóa, phần còn lại được tạo ra sử dụng gen ngẫu nhiên.

3.3. Huấn luyện lại
Khi tìm kiếm tiến hóa đã được hoàn thành, gen tốt nhất được chọn là gen đầu tiên trong danh sách các ứng viên. Đây là gen có phần thưởng cao nhất như được tìm thấy sau nhiều epoch tìm kiếm di truyền. NEV tốt nhất được chuyển đổi thành một mô hình và được huấn luyện từ đầu. Tất cả các thuật toán cắt tỉa huấn luyện một mô hình đã cắt tỉa trong một vài epoch để lấy lại độ chính xác bị mất trong một quá trình gọi là tinh chỉnh [15].
Trong thuật toán Cắt tỉa meta được thưởng, mô hình được tạo ra từ một NEV thay vì sử dụng NEV để cắt tỉa, và sau đó được huấn luyện từ đầu. Do đó độ chính xác bão hòa ở epoch cao hơn trong quá trình tinh chỉnh.

4. Kết quả thực nghiệm
Trong phần này, chúng tôi chứng minh tính ưu việt của phương pháp Cắt tỉa meta được thưởng. Đầu tiên chúng tôi mô tả các thiết lập thực nghiệm để tái tạo các thí nghiệm. Sau đó chúng tôi so sánh các kết quả thu được với các phương pháp khác cắt tỉa ba mạng chính. Cuối cùng, chúng tôi thực hiện một nghiên cứu ablation để hiểu hiệu quả của phương pháp đề xuất.

4.1. Thiết lập thực nghiệm
Mạng ResNet-50 [16] được huấn luyện trong 32 epoch, trong khi MobileNetV1 [22] và MobileNetV2 [48] được huấn luyện trong 64 epoch. ResNet và MobileNetV2 được huấn luyện lại sau khi tìm kiếm trong 400 epoch, nhưng MobileNetV1 chỉ cần 320 epoch.
Tìm kiếm các NEV mất 20 epoch cho tất cả các mạng. Mỗi epoch tìm kiếm 50 NEV, tìm kiếm qua 1000 NEV duy nhất trong suốt quá trình chạy. MobileNetV1 và MobileNetV2 đều sử dụng Lambda scheduler để giảm các mô hình bằng λ là 0.1 mỗi epoch từ tỷ lệ học

--- TRANG 5 ---
Thuật toán 1 Thuật toán của Cắt tỉa meta được thưởng
Siêu tham số: maxtraining: Số epoch huấn luyện, maxiter: Số epoch tìm kiếm, maxtuning: Số epoch tinh chỉnh
Đầu vào: dataset: hình ảnh huấn luyện có thể được chia thành các batch, ri: Số nguyên ngẫu nhiên được đánh chỉ số tại i, wi: Trọng số ngẫu nhiên được đánh chỉ số tại i, ∇: Gradient của mất mát của mô hình đã cho
Hàm: norm(nev) chuyển đổi nev thành trọng số, FC(weights) tạo mô hình sử dụng trọng số, f(model; data) huấn luyện mô hình sử dụng dữ liệu đã cho, reward(nev) tính toán phần thưởng của mô hình được tạo ra sử dụng nev, mutation và crossover thực hiện các phép toán tiến hóa trên danh sách các nev
Đầu ra: x: Mô hình đã cắt tỉa và được huấn luyện

for i = 0, 1, ..., maxtraining do
    for each batch in dataset do
        nev = [r1, r2, ..., rn]
        {w1, w2, ..., wn} = norm(nev)
        x = FC({w1, w2, ..., wn})
        L = f(x, batch)
        x += ∇L
    end for
end for
candidate = List of n random nevs
for i = 0, 1, ..., maxiter do
    rewards = [r1, r2, ..., rn]
    for j = 0, 1, ..., n do
        rj = reward(nevj)
    end for
    sort candidate in descending order of rewards
    mutated = mutation(candidate[:10])
    crossedover = crossover(candidate[:10])
    candidate = mutated + crossedover
end for
{w1, w2, ..., wn} = norm(candidate[0])
x = FC({w1, w2, ..., wn})
for i = 0, 1, ..., maxtuning do
    x += ∇f(x, dataset)
end for

ban đầu là 0.2. Tuy nhiên, scheduler cho ResNet giảm với hệ số 0.1 tại epoch 80 và 160.

Các thí nghiệm được tiến hành trên ba mạng thường được sử dụng bao gồm ResNet-50, MobileNetV1, và MobileNetV2. Các mạng được huấn luyện sử dụng ImageNet [7] từ đầu như được mô tả trong phần trước. ImageNet bao gồm 1.2M hình ảnh huấn luyện và 50K hình ảnh validation. Nó cũng bao gồm 100K hình ảnh test, nhưng vì các nhãn của dữ liệu test không được công bố, dữ liệu validation được sử dụng để kiểm tra. Dữ liệu validation không được sử dụng trong bất kỳ phần nào của quá trình huấn luyện ngoại trừ để tính toán độ chính xác ở mỗi giai đoạn. Như một hệ quả tự nhiên của việc sử dụng tính toán tiến hóa, Cắt tỉa meta được thưởng tốn nhiều tài nguyên trong việc tính toán các mạng đã cắt tỉa. Nhưng chi phí này được cân bằng bởi hiệu quả và độ chính xác của các mô hình được tạo ra.
Một mạng không cần được tạo ra mỗi lần nó được sử dụng bởi vì kiến thức được chưng cất từ nó có thể được chuyển giao và sử dụng trong các bối cảnh khác nhau.

4.2. Giao thức đánh giá
Bốn chỉ số được sử dụng để đánh giá các thuật toán cắt tỉa: tỷ lệ tham số, lỗi top-1 và top-5 và FLOP. Tỷ lệ tham số là tỷ lệ của mô hình đã cắt tỉa so với mô hình cơ sở. Nó được tính toán như:

P = (Pm/Pb) × 100%,     (4)

trong đó Pm và Pb là số lượng trọng số trong mô hình đã cắt tỉa và mô hình cơ sở tương ứng. Độ chính xác là tỷ lệ phần trăm dữ liệu validation được xác định chính xác so với toàn bộ dataset. Lỗi Top-1 là nghịch đảo của độ chính xác, tức là, tỷ lệ hình ảnh mà các nhãn dự đoán có xác suất cao nhất là sai. Lỗi Top-5 là tỷ lệ hình ảnh mà nhãn chính xác không có mặt trong năm xác suất cao nhất của các nhãn dự đoán. FLOP là một thước đo số lượng phép toán Floating-point được tính toán mỗi giây.

4.3. Hiệu suất trên ResNet-50
ResNet-50 là một CNN với độ sâu 50 lớp. Nó được tạo ra để giải quyết sự suy giảm trong mô hình khi các lớp sâu hơn được xếp chồng [16]. ResNet sử dụng các kết nối bỏ qua để xác định ánh xạ. Điều này thêm các đặc trưng với các tham số gốc của chúng trước khi chuyển chúng vào lớp tiếp theo. Ánh xạ đồng nhất theo sau bởi phép chiếu tuyến tính được sử dụng để mở rộng các kênh của các đặc trưng để có thể được thêm vào với các tham số gốc.

Phương pháp | Lỗi Top-1 | Lỗi Top-5 | FLOP
Baseline [33] | 23.40% | - | 4110M
GAL-0.5 [37] | 28.05% | 9.06% | 2341M
SSS [25] | 28.18% | 9.21% | 2341M
HRank [36] | 25.02% | 7.67% | 2311M
Random Pruning [33] | 24.87% | 7.48% | 2013M
AutoPruner [43] | 25.24% | 7.85% | 2005M
Adapt-DCP [38] | 24.85% | 7.70% | 1955M
MetaPruning [40] | 24.60% | - | 2005M
Rewarded meta-pruning | 24.24% | 7.35% | 1950M

Bảng 1. So sánh các phương pháp cắt tỉa kênh tiên tiến với ResNet-50

Bảng 1 hiển thị kết quả của ResNet-50 được huấn luyện sử dụng ImageNet-2012 sau khi cắt tỉa với Cắt tỉa meta được thưởng và các phương pháp cạnh tranh khác. Có thể suy ra rằng phương pháp này có tỷ lệ lỗi thấp hơn mọi phương pháp, và điều này được đạt được trong khi giữ FLOP tương đối thấp. FLOP, so với mạng cơ sở [33], giảm 52.55%, nhưng lỗi chỉ tăng 0.84%.
Khi so sánh với cắt tỉa ngẫu nhiên tiêu chuẩn, cho một sự giảm FLOP tương tự, có 0.63% lỗi thấp hơn.
Phương pháp MetaPruning [40] cho thấy 0.36% lỗi cao hơn trong khi vẫn sử dụng 1.34% FLOP cao hơn so với mô hình cơ sở. Adapt-DCP [38] có sự giảm FLOP gần nhất so với cơ sở, nhưng phương pháp này có 0.61% lỗi thấp hơn. Các phương pháp SSS [25] và HRank [36] có tỷ lệ cắt tỉa rất tương tự với Cắt tỉa meta được thưởng, nhưng có FLOP cao hơn khoảng 9%, và lỗi cao hơn 3.94% và 0.78% tương ứng.

4.4. Hiệu suất trên MobileNetV2
MobileNetV2 chứa tích chập theo chiều sâu và theo điểm. Nó có một bottleneck đảo ngược với một bottleneck tuyến tính nhận một biểu diễn nén chiều thấp làm đầu vào và mở rộng nó lên chiều cao hơn, sau đó lọc chúng với các tích chập theo chiều sâu nhẹ như trong MobileNetV1 [48]. MobileNetV2 là một mạng hiệu quả với lỗi tương đối thấp. Vì vậy, chứng minh trên MobileNetV2 là một cách hiệu quả để hiển thị hiệu suất của thuật toán cắt tỉa.

Phương pháp | Lỗi Top-1 | Lỗi Top-5 | FLOP
Baseline [33] | 28.12% | 9.71% | 314M
0.75 MobileNetV2 [33] | 30.20% | - | 220M
Random Pruning [10] | 29.10% | - | 223M
AMC [18] | 29.20% | - | 220M
MetaPruning [40] | 28.80% | - | 227M
Greedy Selection [61] | 28.80% | - | 201M
Adapt-DCP [38] | 28.55% | - | 216M
Rewarded meta-pruning | 28.51% | 10.65% | 199M

Bảng 2. So sánh các phương pháp cắt tỉa kênh tiên tiến với MobileNetV2

Bảng 2 so sánh hiệu suất của phương pháp Cắt tỉa meta được thưởng với các phương pháp tiên tiến.
Phương pháp Cắt tỉa meta được thưởng có FLOP thấp hơn bất kỳ phương pháp nào khác trong khi chỉ hiển thị 0.39% lỗi cao hơn so với cơ sở. 0.75 MobileNetV2, đó là MobileNetV2 có chiều rộng thấp hơn 25% so với bản gốc, có 1.69% lỗi cao hơn phương pháp này. Khi so sánh với cắt tỉa ngẫu nhiên, đó là cơ sở cho tất cả các phương pháp cắt tỉa [2], phương pháp này có 0.59% lỗi thấp hơn. MetaPruning [40] có 0.29% lỗi cao hơn mặc dù có 7.64% FLOP cao hơn. AMC [18] và Adapt-DCP [38] có 0.69% và 0.04% lỗi và FLOP cao hơn. Cắt tỉa meta được thưởng cũng vượt trội hơn Greedy selection [61] 0.29% mặc dù có số lượng FLOP gần như tương tự.

4.5. Hiệu suất trên MobileNetV1
MobileNetV1 có kiến trúc được tinh giản xây dựng các mạng sâu nhẹ sử dụng các tích chập có thể tách rời theo chiều sâu. Tất cả các lớp sử dụng Batch Normalisation và ReLu, ngoại trừ lớp kết nối đầy đủ được theo sau bởi một lớp softmax để phân loại [22].

Phương pháp | Lỗi Top-1 | Lỗi Top-5 | FLOP
Baseline [22] | 29.40% | - | 569M
0.75 MobileNet-224 [22] | 31.60% | - | 325M
FTWT (r=1.0) [10] | 30.34% | - | 335M
MetaPruning [40] | 29.10% | - | 324M
Rewarded meta-pruning | 29.60% | 9.65% | 295M

Bảng 3. So sánh các phương pháp cắt tỉa kênh tiên tiến với MobileNetV1

Trong Bảng 3, chúng tôi so sánh phương pháp Cắt tỉa meta được thưởng với các kỹ thuật cắt tỉa cạnh tranh khác để cắt tỉa một mô hình MobileNetV1. Nó gần như lấy lại được độ chính xác của mạng cơ sở [22], với 0.2% độ chính xác thấp hơn và 48.15% FLOP thấp hơn. Phương pháp này rõ ràng đạt được kết quả vượt trội khi so sánh với phương pháp cắt tỉa Fire-Together-Wire-Together [10], với 0.94% lỗi thấp hơn và 7.03% FLOP thấp hơn. Nó vượt trội hơn 0.75 MobileNet-224 [22], đó là MobileNetV1 có chiều rộng thấp hơn 25%, 2% trong khi sử dụng 5.27% FLOP thấp hơn. Trong khi MetaPruning [40] hiển thị lỗi thấp hơn thậm chí cả mạng cơ sở, phương pháp của chúng tôi có FLOP thấp hơn. Tuy nhiên, kích thước của mạng đã cắt tỉa vẫn lớn hơn 9.83%, khi so sánh với phương pháp đề xuất.
Điều này có thể do thiếu kết nối tắt trong MobileNetV1 mặc dù là một mạng nhỏ hơn, dẫn đến một số lượng lớn các lớp kết nối đầy đủ. Về hiệu suất đạt được cho mỗi tài nguyên, Cắt tỉa meta được thưởng vượt trội hơn MetaPruning. Có thể giả định rằng phương pháp Cắt tỉa meta được thưởng có thể có kết quả tốt hơn với các hàm phần thưởng tiên tiến hơn. Điều này sẽ được xác thực bằng nghiên cứu thêm về tính mạnh mẽ của các siêu tham số khác nhau.

4.6. Thảo luận
Từ các kết quả, rõ ràng rằng phương pháp đề xuất hoạt động tốt nhất dưới các hàm phần thưởng phù hợp. Hàm phần thưởng trong trường hợp này tỷ lệ thuận trực tiếp với độ chính xác và tỷ lệ nghịch với FLOP.

Khi độ chính xác của mô hình đã cắt tỉa tiếp cận độ chính xác cơ sở, phần thưởng tăng theo cấp số nhân. Trong MetaPruning [40], phần thưởng tỷ lệ thuận trực tiếp với độ chính xác, trong khi ở phương pháp này, phần thưởng tỷ lệ với bình phương độ chính xác của mô hình. Điều này tự nó sẽ không tăng độ chính xác của mô hình cuối cùng, nhưng theo đuổi phần thưởng cao hơn chỉ phụ thuộc vào độ chính xác có thể dẫn đến mô hình cuối cùng có FLOP cao. Điều này có thể được thấy trong MetaPruning, nơi mô hình đã cắt tỉa có xu hướng

--- TRANG 6 ---
hiển thị FLOP cao như FLOP tối đa đã đặt trước sẽ cho phép. Phần thưởng được kiểm soát bởi hệ số hiệu quả để ngăn chặn điều này.

Phần thưởng giảm với FLOP tăng bởi vì hệ số hiệu quả tỷ lệ nghịch với FLOP. Tuy nhiên, nếu tỷ lệ quá cao, phần thưởng sẽ bị hạn chế. Do đó nó được kiểm soát bởi hệ số hiệu quả giảm tuyến tính.

Hình 3. Phần thưởng ở các Độ chính xác và FLOP khác nhau.

Từ định nghĩa của hàm phần thưởng trong Phương trình (1), rõ ràng rằng chỉ độ chính xác cao thôi không đủ để một mô hình được chọn. Khi độ chính xác tăng, xác suất mô hình được chọn tăng, nhưng chỉ khi FLOP của mô hình cũng đủ thấp. Điều này có thể được suy ra từ cách phân phối Xanh tăng khi chúng ta di chuyển về phía bên phải như được hiển thị trong Hình 3. Phần thưởng tăng khi FLOP giảm, nhưng nó không được coi là chấp nhận được cho đến khi độ chính xác đủ cao. Điều này có thể được suy ra từ cách phân phối Đỏ tăng khi FLOP giảm như được suy ra từ Hình 3. Vì vậy theo định nghĩa, phương pháp Cắt tỉa meta được thưởng nghiêng nhiều hơn về phía độ chính xác.

Khi so sánh với MetaPruning, phần thưởng của phương pháp Cắt tỉa meta được thưởng tăng cao hơn với mỗi lần lặp tìm kiếm. Điều này là bởi vì phần thưởng của MetaPruning tỷ lệ thuận trực tiếp với độ chính xác trong khi phần thưởng của Cắt tỉa meta được thưởng tỷ lệ với bình phương độ chính xác. Điều này có thể được quan sát trong Hình 4 hiển thị phần thưởng, độ chính xác, và FLOP của mô hình tốt nhất sau mỗi lần lặp tìm kiếm. Độ chính xác và FLOP, ban đầu, được chọn ngẫu nhiên, nhưng điều đó không thể được thay đổi mà không làm xáo trộn tìm kiếm tiến hóa cơ bản. Nhưng có thể quan sát rằng FLOP của mô hình tốt nhất trong MetaPruning có xu hướng tăng trong hầu hết các trường hợp trong khi, trong trường hợp phương pháp Cắt tỉa meta được thưởng, nó có xu hướng giữ thấp. Độ chính xác tăng trong cả hai trường hợp, nhưng MetaPruning bão hòa sớm hơn Cắt tỉa meta được thưởng.
Cũng có thể suy ra rằng nếu hai mô hình bắt đầu với cùng một batch các mô hình được khởi tạo ngẫu nhiên, phương pháp Cắt tỉa meta được thưởng sẽ có độ chính xác cao hơn và FLOP thấp hơn bởi vì độ dốc của độ chính xác tốt nhất và FLOP của các mô hình tốt nhất cao hơn và thấp hơn tương ứng so với MetaPruning.

Tuy nhiên, cách các NEV được chọn, cả ngẫu nhiên và sau đột biến hoặc lai ghép, có nghĩa là FLOP của mô hình đã cắt tỉa xấp xỉ tạo thành một đường cong chuông giữa 1350 và 2100. Điều này có thể được thay đổi bằng cách kiểm soát việc tạo ra các kích thước bộ lọc ngẫu nhiên trong các NEV như được hiển thị trong Hình 5.

Tính mạnh mẽ của các siêu tham số được sử dụng bởi Cắt tỉa meta được thưởng đã được khám phá bởi He et al. [17]. Khi hàm phần thưởng được điều chỉnh để thêm các siêu tham số khác nhau, các chỉ số khác nhau của mô hình cuối cùng có thể được kiểm soát. Có nhiều hệ số khác có thể được sử dụng trong phần thưởng. Tỷ lệ cắt tỉa của mô hình đã cắt tỉa có thể được đặt tỷ lệ nghịch với phần thưởng, vì tỷ lệ cắt tỉa thấp tự động dẫn đến FLOP thấp hơn. FLOP cũng tỷ lệ thuận trực tiếp với độ trễ phần cứng, đó là thời gian chạy của các mạng [9]. Nhưng điều này phụ thuộc vào phần cứng, và các phần cứng khác nhau có độ trễ khác nhau. Các chỉ số khác như tiêu thụ năng lượng cũng đã được sử dụng để cắt tỉa. NetAdapt [59] sử dụng tiêu thụ năng lượng như một chỉ số để đo lường độ phức tạp của mạng ở mỗi giai đoạn và cắt tỉa mạng thêm trong khi duy trì độ chính xác.

5. Kết luận
Trong công trình này, chúng tôi đã trình bày những điều sau: 1) Thực hiện hàm phần thưởng tốt hơn để meta-learn các tham số cho việc cắt tỉa và cho phép kiểm soát tốt hơn các tham số khác nhau của mô hình đã cắt tỉa. 2) Phương pháp Cắt tỉa meta được thưởng đã được chứng minh là vượt trội so với các phương pháp tiên tiến khác, với độ chính xác cao hơn và FLOP thấp hơn so với các phương pháp cắt tỉa kênh truyền thống. 3) Hàm phần thưởng có thể được tối ưu hóa sử dụng các chỉ số khác để tối đa hóa phần thưởng. 4) ResNet-50, MobileNetV1 và MobileNetV2 được cắt tỉa hiệu quả.

6. Lời cảm ơn
Công trình này được hỗ trợ bởi tài trợ của Quỹ Nghiên cứu Quốc gia Hàn Quốc (NRF) được tài trợ bởi Chính phủ Hàn Quốc (MSIT) (Số.2021R1C1C1012590), (Số. NRF-2022R1A4A1023248) và chương trình hỗ trợ Trung tâm Nghiên cứu Công nghệ Thông tin (ITRC) được giám sát bởi Viện Lập kế hoạch & Đánh giá Truyền thông & Công nghệ Thông tin (IITP) tài trợ được tài trợ bởi Chính phủ Hàn Quốc (MSIT) (IITP-2022-2020-0-01808).

Tài liệu tham khảo
[1] Bowen Baker, Otkrist Gupta, Nikhil Naik, và Ramesh Raskar. Designing neural network architectures using rein-

--- TRANG 7 ---
Hình 4. Phần thưởng, Độ chính xác và FLOP của các mô hình tốt nhất sau mỗi lần lặp tìm kiếm.

Hình 5. Phân phối FLOP của 1000 NEV được tạo ngẫu nhiên với các phạm vi FLOP khác nhau.

forcement learning. arXiv preprint arXiv:1611.02167, 2016. 2
[2] Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, và John Guttag. What is the state of neural network pruning? Proceedings of machine learning and systems, 2:129–146, 2020. 6
[3] Alexandre Bouchard-Côté, Slav Petrov, và Dan Klein. Randomized pruning: Efficiently calculating expectations in large dynamic programs. Advances in Neural Information Processing Systems, 22, 2009. 1, 2
[4] Andrew Brock, Theodore Lim, James M Ritchie, và Nick Weston. Smash: one-shot model architecture search through hypernetworks. arXiv preprint arXiv:1708.05344, 2017. 2
[5] Han Cai, Ligeng Zhu, và Song Han. Proxylessnas: Direct neural architecture search on target task and hardware. arXiv preprint arXiv:1812.00332, 2018. 2
[6] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, và Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. Advances in neural information processing systems, 27, 2014. 4
[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, và Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 5
[8] Jiankang Deng, Jia Guo, Niannan Xue, và Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4690–4699, 2019. 1, 2
[9] Jin-Dong Dong, An-Chieh Cheng, Da-Cheng Juan, Wei Wei, và Min Sun. Dpp-net: Device-aware progressive search for pareto-optimal neural architectures. In Proceedings of the European Conference on Computer Vision (ECCV), pages 517–531, 2018. 7
[10] Sara Elkerdawy, Mostafa Elhoushi, Hong Zhang, và Nilanjan Ray. Fire together wire together: A dynamic pruning approach with self-supervised mask prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12454–12463, 2022. 1, 2, 6
[11] Chelsea Finn, Pieter Abbeel, và Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pages 1126–1135. PMLR, 2017. 2
[12] Jonathan Frankle và Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. 1, 2
[13] Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, và William J Dally. Eie: Efficient inference engine on compressed deep neural network. ACM SIGARCH Computer Architecture News, 44(3):243–254, 2016. 1
[14] Song Han, Huizi Mao, và William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. 1
[15] Song Han, Jeff Pool, John Tran, và William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. 1, 4

--- TRANG 8 ---
[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 3, 4, 5
[17] Yang He, Yuhang Ding, Ping Liu, Linchao Zhu, Hanwang Zhang, và Yi Yang. Learning filter pruning criteria for deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2009–2018, 2020. 7
[18] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, và Song Han. Amc: Automl for model compression and acceleration on mobile devices. In Proceedings of the European conference on computer vision (ECCV), pages 784–800, 2018. 2, 6
[19] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, và Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4340–4349, 2019. 2
[20] Yang He, Ping Liu, Linchao Zhu, và Yi Yang. Filter pruning by switching to neighboring cnns with good attributes. IEEE Transactions on Neural Networks and Learning Systems, 2022. 1
[21] Rein Houthooft, Yuhua Chen, Phillip Isola, Bradly Stadie, Filip Wolski, OpenAI Jonathan Ho, và Pieter Abbeel. Evolved policy gradients. Advances in Neural Information Processing Systems, 31, 2018. 2
[22] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, và Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. 3, 4, 6
[23] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, và Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708, 2017. 1
[24] Qiangui Huang, Kevin Zhou, Suya You, và Ulrich Neumann. Learning to prune filters in convolutional neural networks. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 709–718. IEEE, 2018. 1, 2
[25] Zehao Huang và Naiyan Wang. Data-driven sparse structure selection for deep neural networks. In Proceedings of the European conference on computer vision (ECCV), pages 304–320, 2018. 2, 5, 6
[26] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, và Li Fei-Fei. Large-scale video classification with convolutional neural networks. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 1725–1732, 2014. 1
[27] John K Kruschke và Javier R Movellan. Benefits of gain: Speeded learning and minimal hidden layers in back-propagation networks. IEEE Transactions on systems, Man, and Cybernetics, 21(1):273–280, 1991. 1
[28] Abhishek Kumar, Rakesh Kumar Misra, Devender Singh, Sujeet Mishra, và Swagatam Das. The spherical search algorithm for bound-constrained global optimization problems. Applied Soft Computing, 85:105734, 2019. 1
[29] Dong-Gyu Lee và Yoon-Ki Kim. Joint semantic understanding with a multilevel branch for driving perception. Applied Sciences, 12(6):2877, 2022. 1
[30] Dong-Gyu Lee và Seong-Whan Lee. Human activity prediction based on sub-volume relationship descriptor. In 2016 23rd International Conference on Pattern Recognition (ICPR), pages 2060–2065. IEEE, 2016. 1
[31] Dong-Gyu Lee và Seong-Whan Lee. Prediction of partially observed human activity based on pre-trained deep representation. Pattern Recognition, 85:198–206, 2019. 1
[32] Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, và Hans Peter Graf. Pruning filters for efficient convnets. arXiv preprint arXiv:1608.08710, 2016. 1
[33] Yawei Li, Kamil Adamczewski, Wen Li, Shuhang Gu, Radu Timofte, và Luc Van Gool. Revisiting random channel pruning for neural network compression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 191–201, 2022. 4, 5, 6
[34] Yiying Li, Yongxin Yang, Wei Zhou, và Timothy Hospedales. Feature-critic networks for heterogeneous domain generalization. In International Conference on Machine Learning, pages 3915–3924. PMLR, 2019. 2
[35] Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, và Daniela Rus. Provable filter pruning for efficient neural networks. arXiv preprint arXiv:1911.07412, 2019. 2
[36] Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, và Ling Shao. Hrank: Filter pruning using high-rank feature map. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1529–1538, 2020. 2, 5, 6
[37] Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liujuan Cao, Qixiang Ye, Feiyue Huang, và David Doermann. Towards optimal structured cnn pruning via generative adversarial learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2790–2799, 2019. 5
[38] Jing Liu, Bohan Zhuang, Zhuangwei Zhuang, Yong Guo, Junzhou Huang, Jinhui Zhu, và Mingkui Tan. Discrimination-aware network pruning for deep model compression. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. 1, 2, 5, 6
[39] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, và Changshui Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE international conference on computer vision, pages 2736–2744, 2017. 1
[40] Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, và Jian Sun. Metapruning: Meta learning for automatic neural network channel pruning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 3296–3305, 2019. 1, 2, 5, 6
[41] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, và Trevor Darrell. Rethinking the value of network pruning. arXiv preprint arXiv:1810.05270, 2018. 1
[42] Christos Louizos, Max Welling, và Diederik P Kingma. Learning sparse neural networks through l0 regularization. arXiv preprint arXiv:1712.01312, 2017. 1

--- TRANG 9 ---
[43] Jian-Hao Luo và Jianxin Wu. Autopruner: An end-to-end trainable filter pruning method for efficient deep model inference. Pattern Recognition, 107:107461, 2020. 2, 5
[44] Jian-Hao Luo và Jianxin Wu. Neural network pruning with residual-connections and limited-data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1458–1467, 2020. 2
[45] Rammohan Mallipeddi, Ponnuthurai N Suganthan, Quan-Ke Pan, và Mehmet Fatih Tasgetiren. Differential evolution algorithm with ensemble of parameters and mutation strategies. Applied soft computing, 11(2):1679–1696, 2011. 3
[46] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, và Jan Kautz. Importance estimation for neural network pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11264–11272, 2019. 2
[47] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V Le, và Alexey Kurakin. Large-scale evolution of image classifiers. In International Conference on Machine Learning, pages 2902–2911. PMLR, 2017. 2
[48] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, và Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4510–4520, 2018. 3, 4, 6
[49] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016. 2
[50] Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, và Jason D Lee. Sanity-checking pruning methods: Random tickets can win the jackpot. Advances in Neural Information Processing Systems, 33:20390–20401, 2020. 1, 2
[51] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P Srinivasan, Jonathan T Barron, và Ren Ng. Learned initializations for optimizing coordinate-based neural representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2846–2855, 2021. 2
[52] Hongduan Tian, Bo Liu, Xiao-Tong Yuan, và Qingshan Liu. Meta-learning with network pruning for overfitting reduction. CoRR, 2019. 1
[53] Joaquin Vanschoren. Meta-learning: A survey. arXiv preprint arXiv:1810.03548, 2018. 2
[54] Vibashan VS, Domenick Poster, Suya You, Shuowen Hu, và Vishal M. Patel. Meta-uda: Unsupervised domain adaptive thermal object detection using meta-learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 1412–1423, January 2022. 2
[55] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229–256, 1992. 2
[56] Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, Yuandong Tian, Peter Vajda, Yangqing Jia, và Kurt Keutzer. Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10734–10742, 2019. 2
[57] Lingxi Xie và Alan Yuille. Genetic cnn. In Proceedings of the IEEE international conference on computer vision, pages 1379–1388, 2017. 2
[58] Kohei Yamamoto và Kurato Maeno. Pcas: Pruning channels with attention statistics for deep network compression. arXiv preprint arXiv:1806.05382, 2018. 1
[59] Tien-Ju Yang, Andrew Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, và Hartwig Adam. Netadapt: Platform-aware neural network adaptation for mobile applications. In Proceedings of the European Conference on Computer Vision (ECCV), pages 285–300, 2018. 7
[60] Jianbo Ye, Xin Lu, Zhe Lin, và James Z Wang. Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers. arXiv preprint arXiv:1802.00124, 2018. 2
[61] Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, và Qiang Liu. Good subnetworks provably exist: Pruning via greedy forward selection. In International Conference on Machine Learning, pages 10820–10830. PMLR, 2020. 2, 6
[62] Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, và Jinhui Zhu. Discrimination-aware channel pruning for deep neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, và R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. 2
[63] Barret Zoph và Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint arXiv:1611.01578, 2016. 2
