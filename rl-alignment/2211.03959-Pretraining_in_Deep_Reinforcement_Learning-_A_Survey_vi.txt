# Tiền huấn luyện trong Học tăng cường sâu: Một khảo sát

Zhihui Xie FFFFFARMER @SJTU .EDU.CN
Đại học Giao thông Thượng Hải
Zichuan Lin ZICHUANLIN @TENCENT .COM
Tencent
Junyou Li JUNYOULI @TENCENT .COM
Tencent
Shuai Li SHUAILI 8@ SJTU .EDU.CN
Đại học Giao thông Thượng Hải
Deheng Ye DERICYE @TENCENT .COM
Tencent

Tóm tắt

Trong vài năm qua đã chứng kiến sự tiến bộ nhanh chóng trong việc kết hợp học tăng cường (RL) với học sâu. Các đột phá khác nhau từ trò chơi đến robot học đã thúc đẩy sự quan tâm trong việc thiết kế các thuật toán và hệ thống RL phức tạp. Tuy nhiên, quy trình làm việc phổ biến trong RL là học tabula rasa, điều này có thể gây ra sự kém hiệu quả tính toán. Điều này ngăn cản việc triển khai liên tục các thuật toán RL và có thể loại trừ các nhà nghiên cứu không có tài nguyên tính toán quy mô lớn. Trong nhiều lĩnh vực khác của học máy, mô hình tiền huấn luyện đã được chứng minh là hiệu quả trong việc thu nhận kiến thức có thể chuyển đổi, có thể được sử dụng cho nhiều nhiệm vụ hạ lưu khác nhau. Gần đây, chúng ta đã thấy sự gia tăng quan tâm đến Tiền huấn luyện cho RL sâu với những kết quả đầy hứa hẹn. Tuy nhiên, phần lớn nghiên cứu đã dựa trên các thiết lập thí nghiệm khác nhau. Do bản chất của RL, tiền huấn luyện trong lĩnh vực này phải đối mặt với những thách thức độc đáo và do đó đòi hỏi các nguyên tắc thiết kế mới. Trong khảo sát này, chúng tôi tìm cách xem xét một cách có hệ thống các công trình hiện có trong tiền huấn luyện cho học tăng cường sâu, cung cấp một phân loại các phương pháp này, thảo luận từng tiểu lĩnh vực, và thu hút sự chú ý đến các vấn đề mở và hướng phát triển tương lai.

1. Giới thiệu

Học tăng cường (RL) cung cấp một hình thức toán học tổng quát cho việc ra quyết định tuần tự (Sutton & Barto, 2018). Bằng cách sử dụng các thuật toán RL cùng với mạng nơ-ron sâu, nhiều cột mốc quan trọng trong các lĩnh vực khác nhau đã đạt được hiệu suất vượt trội con người thông qua việc tối ưu hóa các hàm phần thưởng do người dùng chỉ định theo cách hướng dữ liệu (Silver et al., 2016; Akkaya et al., 2019; Vinyals et al., 2019; Ye et al., 2020, 2020, 2022; Chen et al., 2021b). Do đó, chúng ta đã thấy sự quan tâm ngày càng tăng gần đây trong hướng nghiên cứu này.

Tuy nhiên, trong khi RL đã được chứng minh là hiệu quả trong việc giải quyết các nhiệm vụ được chỉ định rõ ràng, vấn đề về hiệu quả mẫu (Jin et al., 2021) và khả năng tổng quát hóa (Kirk et al., 2021) vẫn cản trở việc ứng dụng của nó vào các vấn đề thực tế. Trong nghiên cứu RL, một mô hình tiêu chuẩn là để tác nhân học từ kinh nghiệm thu thập của chính mình hoặc của người khác, thường là trên một nhiệm vụ duy nhất, và tối ưu hóa mạng nơ-ron tabula rasa với các khởi tạo ngẫu nhiên. Đối với con người, ngược lại, kiến thức trước về thế giới đóng góp rất lớn vào quá trình ra quyết định. Nếu nhiệm vụ liên quan đến các nhiệm vụ đã thấy trước đó, con người có xu hướng tái sử dụng những gì đã học được để nhanh chóng thích ứng với một nhiệm vụ mới, mà không cần học từ các tương tác đầy đủ từ đầu. Do đó, so với con người, các tác nhân RL thường gặp phải sự kém hiệu quả dữ liệu lớn (Kapturowski et al., 2022) và dễ bị quá khớp (Zhang et al., 2018).

Tuy nhiên, những tiến bộ gần đây trong các lĩnh vực khác của học máy tích cực ủng hộ việc tận dụng kiến thức trước được xây dựng từ tiền huấn luyện quy mô lớn. Bằng cách huấn luyện trên dữ liệu rộng lớn theo quy mô, các mô hình chung lớn, còn được gọi là mô hình nền tảng (Bommasani et al., 2021), có thể nhanh chóng thích ứng với nhiều nhiệm vụ hạ lưu khác nhau. Mô hình tiền huấn luyện-tinh chỉnh này đã được chứng minh hiệu quả trong các lĩnh vực như thị giác máy tính (Chen et al., 2020; He et al., 2020; Grill et al., 2020) và xử lý ngôn ngữ tự nhiên (Devlin et al., 2019; Brown et al., 2020). Tuy nhiên, tiền huấn luyện vẫn chưa có tác động đáng kể đến lĩnh vực RL. Mặc dù có triển vọng, việc thiết kế các nguyên tắc cho tiền huấn luyện RL quy mô lớn đối mặt với những thách thức từ nhiều nguồn: 1) sự đa dạng của các lĩnh vực và nhiệm vụ; 2) các nguồn dữ liệu hạn chế; 3) khó khăn trong việc thích ứng nhanh để giải quyết các nhiệm vụ hạ lưu. Những yếu tố này xuất phát từ bản chất của RL và cần thiết phải được xem xét.

Khảo sát này nhằm mục đích trình bày một cái nhìn toàn cảnh về nghiên cứu hiện tại về Tiền huấn luyện trong RL sâu. Tiền huấn luyện có nguyên tắc trong RL có nhiều lợi ích tiềm năng. Trước hết, chi phí tính toán đáng kể phát sinh bởi việc huấn luyện RL vẫn là một rào cản cho các ứng dụng công nghiệp. Ví dụ, việc tái tạo kết quả của AlphaStar (Vinyals et al., 2019) tốn khoảng hàng triệu đô la (Agarwal et al., 2022). Tiền huấn luyện có thể cải thiện vấn đề này, bằng các mô hình thế giới được tiền huấn luyện (Sekar et al., 2020) hoặc các biểu diễn được tiền huấn luyện (Schwarzer et al., 2021b), bằng cách cho phép thích ứng nhanh để giải quyết các nhiệm vụ theo cách zero hoặc few-shot. Bên cạnh đó, RL nổi tiếng là đặc thù theo nhiệm vụ và lĩnh vực. Đã được chứng minh rằng tiền huấn luyện với dữ liệu massive không phụ thuộc nhiệm vụ có thể tăng cường những loại khái quát hóa này (Lee et al., 2022). Cuối cùng, chúng tôi tin rằng tiền huấn luyện với các kiến trúc phù hợp có thể mở khóa sức mạnh của quy luật mở rộng (Kaplan et al., 2020), như được chứng minh bởi thành công gần đây trong trò chơi (Schwarzer et al., 2021b; Lee et al., 2022). Bằng cách mở rộng các mô hình đa mục đích với tính toán tăng lên, chúng ta có thể đạt được kết quả vượt trội con người hơn nữa, như được dạy trong "bài học cay đắng" (Sutton, 2019).

Tiền huấn luyện trong RL sâu đã trải qua một số đột phá trong những năm gần đây. Tiền huấn luyện đơn giản với các cuộc biểu diễn chuyên gia, sử dụng học có giám sát để dự đoán các hành động được thực hiện bởi các chuyên gia, đã được thể hiện với AlphaGo nổi tiếng (Silver et al., 2016). Để theo đuổi tiền huấn luyện quy mô lớn với ít giám sát hơn, lĩnh vực RL không giám sát đã phát triển nhanh chóng trong những năm gần đây (Burda et al., 2019a; Laskin et al., 2021), cho phép tác nhân học từ việc tương tác với môi trường khi không có tín hiệu phần thưởng. Phù hợp với những tiến bộ gần đây trong RL ngoại tuyến (Levine et al., 2020), các nhà nghiên cứu tiếp tục xem xét cách tận dụng dữ liệu ngoại tuyến không được gán nhãn và không tối ưu cho tiền huấn luyện (Stooke et al., 2021; Schwarzer et al., 2021b), mà chúng tôi gọi là tiền huấn luyện ngoại tuyến. Mô hình ngoại tuyến với dữ liệu không liên quan đến nhiệm vụ tiếp tục mở đường cho tiền huấn luyện tổng quát, nơi các bộ dữ liệu đa dạng từ các nhiệm vụ và phương thức khác nhau cũng như các mô hình đa mục đích với thuộc tính mở rộng tuyệt vời được kết hợp để xây dựng các mô hình tổng quát (Reed et al., 2022; Lee et al., 2022).

Tiền huấn luyện có tiềm năng đóng vai trò lớn cho RL và khảo sát này có thể phục vụ như một điểm khởi đầu cho những ai quan tâm đến hướng này. Trong bài báo này, chúng tôi tìm cách cung cấp một đánh giá có hệ thống về các công trình hiện có trong tiền huấn luyện cho học tăng cường sâu. Theo hiểu biết tốt nhất của chúng tôi, đây là một trong những nỗ lực tiên phong để nghiên cứu có hệ thống tiền huấn luyện trong RL sâu.

Theo xu hướng phát triển của tiền huấn luyện trong RL, chúng tôi tổ chức bài báo như sau. Sau khi đi qua các kiến thức cơ bản về học tăng cường và tiền huấn luyện (Phần 2), chúng tôi bắt đầu với tiền huấn luyện trực tuyến trong đó một tác nhân học từ việc tương tác với môi trường mà không có tín hiệu phần thưởng (Phần 3). Và sau đó, chúng tôi xem xét tiền huấn luyện ngoại tuyến, kịch bản nơi dữ liệu huấn luyện không được gán nhãn được thu thập một lần với bất kỳ chính sách nào (Phần 4). Trong Phần 5, chúng tôi thảo luận những tiến bộ gần đây trong việc phát triển các tác nhân tổng quát cho nhiều nhiệm vụ trực giao khác nhau. Chúng tôi tiếp tục thảo luận cách thích ứng với các nhiệm vụ RL hạ lưu (Phần 6). Cuối cùng, chúng tôi kết luận khảo sát này cùng với một vài triển vọng (Phần 7).

2. Kiến thức cơ bản

2.1 Học tăng cường

Học tăng cường xem xét vấn đề tìm một chính sách tương tác với môi trường dưới sự không chắc chắn để tối đa hóa phần thưởng thu thập được. Về mặt toán học, vấn đề này có thể được công thức hóa thông qua một Quá trình Quyết định Markov (MDP) được định nghĩa bởi tuple (S,A,T,ρ₀,r,γ), với không gian trạng thái S, không gian hành động A, phân phối chuyển đổi trạng thái T:S×A×S→[0,1], phân phối trạng thái ban đầu ρ₀:S→[0,1], hàm phần thưởng r:S×A→ℝ, và hệ số chiết khấu γ∈(0,1). Mục tiêu là tìm một chính sách π(a|s) được tham số hóa bởi θ mà tối đa hóa

J(θ) = 𝔼[τ,T,ρ₀][∑ᵗ₌₀^∞ γᵗr(sᵗ,aᵗ)],

được gọi là lợi nhuận chiết khấu. Ký hiệu được sử dụng trong bài báo được tóm tắt trong Bảng 1.

2.2 Tiền huấn luyện

Tiền huấn luyện nhằm mục đích thu được kiến thức có thể chuyển đổi từ dữ liệu huấn luyện quy mô lớn để tạo thuận lợi cho các nhiệm vụ hạ lưu. Trong bối cảnh RL, kiến thức có thể chuyển đổi thường bao gồm các biểu diễn tốt giúp tác nhân cảm nhận thế giới (tức là, một không gian trạng thái tốt hơn) và các kỹ năng có thể tái sử dụng mà từ đó tác nhân có thể nhanh chóng xây dựng các hành vi phức tạp cho các mô tả nhiệm vụ (tức là, một không gian hành động tốt hơn). Dữ liệu huấn luyện có thể là một nút thắt cổ chai cho tiền huấn luyện RL hiệu quả. Không giống như những gì chúng ta đã chứng kiến trong các lĩnh vực như thị giác máy tính và xử lý ngôn ngữ tự nhiên nơi một lượng lớn dữ liệu không được gán nhãn có thể được thu thập với sự giám sát tối thiểu, RL thường đòi hỏi thiết kế phần thưởng rất đặc thù cho nhiệm vụ, điều này cản trở việc mở rộng tiền huấn luyện cho các ứng dụng quy mô lớn.

Do đó, trọng tâm của khảo sát này là tiền huấn luyện không giám sát, trong đó các phần thưởng đặc thù cho nhiệm vụ không có sẵn trong quá trình tiền huấn luyện nhưng vẫn được phép học từ tương tác trực tuyến, dữ liệu đã ghi không được gán nhãn, hoặc dữ liệu không liên quan đến nhiệm vụ từ các phương thức khác. Chúng tôi bỏ qua tiền huấn luyện có giám sát cho rằng với các phần thưởng đặc thù cho nhiệm vụ, kịch bản này gần như thoái hóa thành các thiết lập RL hiện có (Levine et al., 2020). Hình 1 minh họa tổng quan về quá trình tiền huấn luyện và thích ứng.

Mục tiêu là thu nhận kiến thức trước hữu ích dưới các hình thức khác nhau như biểu diễn thị giác tốt, chính sách khám phá π(a|s), chính sách có điều kiện tiềm ẩn π(a|s,z), hoặc đơn giản là các bộ dữ liệu đã ghi. Tùy thuộc vào dữ liệu có sẵn trong quá trình tiền huấn luyện, nó đòi hỏi các cân nhắc khác nhau để thu được kiến thức hữu ích (Phần 3-5) và thích ứng nó một cách phù hợp với các nhiệm vụ hạ lưu (Phần 6).

3. Tiền huấn luyện trực tuyến

Hầu hết các thành công trước đây trong RL đã được đạt được nhờ các hàm phần thưởng dày đặc và được thiết kế tốt. Mặc dù có tính ưu việt trong việc cung cấp hiệu suất xuất sắc cho một nhiệm vụ cụ thể, mô hình RL truyền thống đối mặt với hai thách thức quan trọng khi mở rộng nó lên tiền huấn luyện quy mô lớn. Thứ nhất, một tác nhân RL dễ dàng bị quá khớp một cách đáng chú ý (Zhang et al., 2018). Kết quả là, một tác nhân được tiền huấn luyện với các phần thưởng nhiệm vụ phức tạp khó có thể tổng quát hóa đến các đặc tả nhiệm vụ chưa thấy. Hơn nữa, việc thiết kế các hàm phần thưởng vẫn là một thách thức thực tiễn, thường tốn kém và đòi hỏi kiến thức chuyên môn.

Tiền huấn luyện trực tuyến mà không có các tín hiệu phần thưởng này có thể là một giải pháp tốt để học các kỹ năng chung và loại bỏ yêu cầu giám sát. Tiền huấn luyện trực tuyến nhằm mục đích thu nhận kiến thức trước bằng cách tương tác với môi trường khi không có sự giám sát của con người. Trong giai đoạn tiền huấn luyện, tác nhân được phép tương tác với môi trường trong một thời gian dài mà không có quyền truy cập vào các phần thưởng bên ngoài. Khi môi trường có thể truy cập được, việc chơi với nó tạo thuận lợi cho việc học kỹ năng sẽ hữu ích sau này khi một nhiệm vụ được giao cho tác nhân. Giải pháp này, còn được gọi là RL không giám sát, đã được nghiên cứu tích cực trong những năm gần đây (Burda et al., 2019a; Srinivas & Abbeel, 2021).

Để khuyến khích tác nhân xây dựng kiến thức của riêng mình mà không có bất kỳ sự giám sát nào, chúng ta cần các cơ chế có nguyên tắc để cung cấp cho tác nhân các động lực nội tại. Các nhà tâm lý học phát hiện ra rằng trẻ em có thể khám phá cả các nhiệm vụ cần học và giải pháp cho những nhiệm vụ đó thông qua việc tương tác với môi trường (Smith & Gasser, 2005). Với kinh nghiệm tích lũy, chúng có khả năng thực hiện các nhiệm vụ khó khăn hơn sau này. Điều này thúc đẩy một lượng lớn nghiên cứu về cách xây dựng các tác nhân tự học với các phần thưởng nội tại (Schmidhuber, 1991; Singh et al., 2004; Oudeyer et al., 2007). Các phần thưởng nội tại, trái ngược với các phần thưởng bên ngoài chỉ định nhiệm vụ, đề cập đến các tín hiệu học tổng quát khuyến khích tác nhân thu thập kinh nghiệm đa dạng hoặc phát triển các kỹ năng hữu ích. Đã được chứng minh rằng việc tiền huấn luyện một tác nhân với các phần thưởng nội tại và các thuật toán RL tiêu chuẩn có thể dẫn đến thích ứng nhanh một khi nhiệm vụ hạ lưu được đưa ra (Laskin et al., 2021).

Dựa trên cách thiết kế các phần thưởng nội tại, chúng tôi phân loại các phương pháp hiện có của RL không giám sát thành ba loại: khám phá dựa trên tò mò, khám phá kỹ năng, và tối đa hóa phủ sóng dữ liệu. Bảng 2 trình bày việc phân loại các thuật toán tiền huấn luyện trực tuyến đại diện cùng với các phần thưởng nội tại được sử dụng.

3.1 Khám phá dựa trên tò mò

Trong tâm lý học động lực, tò mò thể hiện động lực để giảm sự không chắc chắn về thế giới (Silvia, 2012). Được truyền cảm hứng từ dòng lý thuyết tâm lý này, các ý tưởng tương tự đã được nghiên cứu để xây dựng các phương pháp dựa trên tò mò cho tiền huấn luyện trực tuyến. Các phương pháp dựa trên tò mò tìm cách khám phá các trạng thái thú vị có thể mang lại kiến thức về môi trường. Một cách trực quan, nếu tác nhân thiếu khả năng dự đoán chính xác môi trường, nó thu được kiến thức bằng cách tương tác và sau đó giảm phần không chắc chắn này. Đặc điểm xác định của một tác nhân dựa trên tò mò là cách tính toán mức độ tò mò đến những trạng thái thú vị này, điều này trực tiếp phục vụ như phần thưởng nội tại cho việc học. Một ví dụ cụ thể là ICM (Pathak et al., 2017), áp dụng phần thưởng nội tại tỷ lệ thuận với lỗi dự đoán như được hiển thị trong Hình 2:

rₜ ∝ ‖f(φ(sₜ), aₜ) - φ(sₜ₊₁)‖₂²,

trong đó f và φ lần lượt đại diện cho mô hình động lực học tiến đã học và bộ mã hóa đặc trưng.

Để đo lường tò mò, một lớp rộng các phương pháp (Pathak et al., 2017; Haber et al., 2018) tận dụng loại mô hình động lực học đã học này để dự đoán các trạng thái tương lai trong một không gian đặc trưng phụ trợ. Có chủ yếu hai loại ước lượng: lỗi dự đoán và sự không chắc chắn dự đoán. Mặc dù các phương pháp dựa trên động lực học này hoạt động khá tốt trong các kịch bản phổ biến, chúng thường gặp phải vấn đề TV nhiễu phụ thuộc hành động (Burda et al., 2019a), sẽ được thảo luận sau trong Phần 3.1.1. Sự thiếu sót này khuyến khích công việc tiếp theo thiết kế ước lượng tò mò không phụ thuộc động lực học (Burda et al., 2019b) và các phương pháp ước lượng không chắc chắn phức tạp hơn (Pathak et al., 2019; Sekar et al., 2020).

Một lựa chọn thiết kế quan trọng khác liên quan đến bộ mã hóa đặc trưng, đặc biệt đối với các quan sát có chiều cao. Một bộ mã hóa đặc trưng phù hợp có thể làm cho nhiệm vụ dự đoán dễ xử lý hơn và lọc ra các khía cạnh không liên quan để tác nhân chỉ có thể tập trung vào những khía cạnh thông tin. Các nghiên cứu ban đầu (Stadie et al., 2015; Burda et al., 2019a) tận dụng các embedding tự mã hóa để khôi phục các đầu vào có chiều cao ban đầu, nhưng không gian đặc trưng được tạo ra thường quá thông tin về các chi tiết không liên quan và do đó dễ bị nhiễu. Để giải quyết vấn đề này, Pathak et al. (2017) sử dụng một mô hình động lực học nghịch đảo để mã hóa đặc trưng nhằm đảm bảo rằng tác nhân không bị ảnh hưởng bởi các yếu tố gây rối trong môi trường. ICM được đề xuất cho thấy hiệu suất zero-shot ấn tượng trong việc chơi video game. Burda et al. (2019b) tiếp tục nới lỏng gánh nặng thiết kế bằng cách đơn giản thay thế mô hình đặc trưng bằng một mạng nơ-ron được khởi tạo ngẫu nhiên cố định, điều này được chứng minh hiệu quả bởi một nghiên cứu thực nghiệm quy mô lớn tiếp theo (Burda et al., 2019a). Mặc dù các bộ mã hóa đặc trưng ngẫu nhiên đủ cho hiệu suất tốt khi huấn luyện, các đặc trưng đã học (ví dụ, dựa trên động lực học nghịch đảo) tổng quát hóa tốt hơn (Burda et al., 2019a). Được truyền cảm hứng từ những tiến bộ gần đây trong học biểu diễn, Du et al. (2021) trực tiếp liên kết tò mò và mất mát học biểu diễn bằng cách công thức hóa một trò chơi minimax giữa một thuật toán học biểu diễn chung và một chính sách học tăng cường.

3.1.1 THÁCH THỨC & HƯỚNG PHÁT TRIỂN TƯƠNG LAI

Loại phương pháp này có một số thiếu sót. Một trong những vấn đề quan trọng nhất là cách phân biệt sự không chắc chắn nhận thức và ngẫu nhiên. Sự không chắc chắn nhận thức đề cập đến sự không chắc chắn gây ra bởi thiếu kiến thức. Ngược lại, sự không chắc chắn ngẫu nhiên đề cập đến sự biến thiên trong kết quả do các hiệu ứng ngẫu nhiên vốn có. Một hiện tượng cụ thể trong RL là vấn đề TV nhiễu (Mavor-Parker et al., 2022), đề cập đến các trường hợp mà tác nhân bị mắc kẹt bởi tò mò của nó trong các môi trường có tính ngẫu nhiên cao. Để giảm thiểu vấn đề này, một số công trình cố gắng sử dụng các phần thưởng nội tại tỷ lệ thuận với việc giảm sự không chắc chắn (Houthooft et al., 2016; Pathak et al., 2019). Tuy nhiên, ước lượng sự không chắc chắn nhận thức có thể xử lý trong chiều cao vẫn đầy thách thức (Hüllermeier & Waegeman, 2021) do tính nhạy cảm của nó với dữ liệu không hoàn hảo.

Một vấn đề khác với các phương pháp trên là chúng chỉ nhận được tín hiệu hồi tố sau khi tác nhân đã đạt được sự không chắc chắn nhận thức, điều này có thể gây ra sự kém hiệu quả trong khám phá. Dựa trên trực giác này, Sekar et al. (2020) thiết kế một phương pháp dựa trên mô hình có thể tìm kiếm sự không chắc chắn trong môi trường một cách tiềm năng.

3.2 Khám phá kỹ năng

Ngoài các phương pháp dựa trên tò mò giải quyết RL không giám sát từ góc độ dựa trên mô hình, người ta cũng có thể xem xét việc học không dựa trên mô hình của các kỹ năng nguyên thủy có thể được kết hợp để giải quyết các nhiệm vụ hạ lưu. Loại phương pháp này thường được gọi là phương pháp khám phá kỹ năng. Trực giác chính đằng sau điều này là kỹ năng đã học nên kiểm soát những trạng thái mà tác nhân ghé thăm, có thể được xem như một khái niệm về quyền năng.

Nói chung, mục tiêu cho khám phá kỹ năng có thể được chính thức hóa như tối đa hóa thông tin tương hỗ (MI) giữa biến tiềm ẩn kỹ năng z và các trạng thái:

I(s;z) = H(z) - H(z|s) = H(s) - H(s|z), (1)

nơi chúng ta định nghĩa kỹ năng hoặc tùy chọn như các chính sách có điều kiện trên z. Có hai thành phần cho một tác nhân khám phá kỹ năng để xác định: 1) một phân phối kỹ năng p(z); 2) một chính sách kỹ năng π(a|s,z). Trước mỗi tập, tiềm ẩn kỹ năng z được lấy mẫu từ phân phối p(z), tiếp theo là kỹ năng π(a|s,z) để tương tác với môi trường. Học các kỹ năng tối đa hóa MI là một bài toán tối ưu hóa đầy thách thức, mà một loạt các phương pháp chia sẻ cùng tinh thần đã được áp dụng.

Trong số các phương pháp khám phá kỹ năng dựa trên MI hiện có, phần lớn (Gregor et al., 2016; Eysenbach et al., 2019; Hansen et al., 2020) áp dụng dạng trước của Phương trình 1 với cận dưới biến phân sau (Agakov, 2004):

I(s;z) = E_{s;z~p(s;z)}[log p(z|s)] - E_{z~p(z)}[log p(z)]
       ≥ E_{s;z~p(s;z)}[log q(z|s)] - E_{z~p(z)}[log p(z)].

Trong trường hợp này, một mô hình tham số q(z|s) được huấn luyện cùng với các biến khác để ước lượng phân phối có điều kiện p(z|s). Tối đa hóa H(z) có thể được đạt được bằng cách lấy mẫu z từ một phân phối đã học (Gregor et al., 2016) hoặc trực tiếp từ một phân phối đều cố định (Eysenbach et al., 2019). Như được hiển thị trong Hình 3, phần thưởng nội tại được đưa ra bởi r_t = log q(z|s_t) - log p(z), mà người ta có thể áp dụng các thuật toán RL tiêu chuẩn để học kỹ năng.

Một dòng nghiên cứu khác (Sharma et al., 2020; Campos et al., 2020; Liu & Abbeel, 2021a; Laskin et al., 2022) xem xét dạng sau và tương tự rút ra một cận dưới:

I(s;z) = E_{s;z~p(s;z)}[log p(s|z)] - E_{s~p(s)}[log p(s)]
       ≥ E_{s;z~p(s;z)}[log q(s|z)] - E_{s~p(s)}[log p(s)].

Trong công thức này, tối đa hóa entropy trạng thái H(s) khuyến khích khám phá trong khi tối thiểu hóa entropy có điều kiện dẫn đến các hành vi có hướng. Khó khăn nằm ở việc ước lượng mật độ của s, đặc biệt đối với các không gian trạng thái có chiều cao. Một thực hành phổ biến là tối đa hóa H(s) thông qua ước lượng entropy tối đa (Hazan et al., 2019; Lee et al., 2019; Liu & Abbeel, 2021b), sẽ được elaborated hơn trong Phần 3.3.

Mặc dù các công trình khác nhau sử dụng các phương pháp hơi khác nhau để tối ưu hóa Phương trình 1, có thể quan trọng hơn là quyết định các yếu tố thiết kế khác khi sử dụng khám phá kỹ năng cho tiền huấn luyện trực tuyến. Ví dụ, trong khi hầu hết các nghiên cứu xem xét thiết lập tập, một số nỗ lực đã được thực hiện để mở rộng khám phá kỹ năng dựa trên MI đến các thiết lập không tập (Xu et al., 2020; Lu et al., 2021b). Cũng có triển vọng để xem xét một chương trình giảng dạy với số lượng kỹ năng tăng dần để học (Achiam et al., 2018). Một số yếu tố khác cũng đáng đề cập, chẳng hạn như liệu tiềm ẩn kỹ năng z có rời rạc (Achiam et al., 2018) hay liên tục (Warde-Farley et al., 2019), liệu các tín hiệu phần thưởng có dày đặc (Eysenbach et al., 2019) hay thưa thớt (Gregor et al., 2016), và liệu nó có hoạt động cho các quan sát dựa trên hình ảnh (Warde-Farley et al., 2019).

Khám phá kỹ năng cũng có thể được tái diễn giải như học chính sách có điều kiện mục tiêu, nơi z như mục tiêu tự sinh và trừu tượng được lấy mẫu từ một phân phối thay vì được cung cấp bởi nhiệm vụ. Người ta cũng có thể xem xét việc tạo ra các mục tiêu cụ thể theo cách tự giám sát (Warde-Farley et al., 2019; Pong et al., 2020) và rút ra một hàm phần thưởng có điều kiện mục tiêu tương tự từ việc tối đa hóa MI. DISCERN (Warde-Farley et al., 2019) thiết kế một phương pháp không tham số cho việc lấy mẫu mục tiêu, duy trì một bộ đệm các quan sát trong quá khứ trôi đi khi tác nhân thu thập kinh nghiệm mới. Skew-Fit (Pong et al., 2020) thay vào đó học một phân phối mục tiêu entropy tối đa bằng cách tăng entropy của một mô hình sinh trong một cách lặp. Choi et al. (2021) cung cấp một kết nối chính thức hơn chủ yếu từ góc độ của RL có điều kiện mục tiêu. Chúng tôi giới thiệu độc giả quan tâm đến Colas et al. (2022) để thảo luận thêm.

3.2.1 THÁCH THỨC & HƯỚNG PHÁT TRIỂN TƯƠNG LAI

Một vấn đề lớn đối với các phương pháp khám phá kỹ năng dựa trên MI là mục tiêu không nhất thiết dẫn đến phủ sóng trạng thái mạnh vì người ta có thể tối đa hóa I(s;z) ngay cả với các biến thể trạng thái nhỏ nhất (Campos et al., 2020; Park et al., 2022). Sự thiếu phủ sóng này có thể hạn chế đáng kể khả năng áp dụng của chúng vào các nhiệm vụ hạ lưu với môi trường phức tạp (Campos et al., 2020). Để giải quyết vấn đề này, một số công trình hiện có một cách rõ ràng sử dụng tọa độ x-y như các đặc trưng để thực thi phủ sóng trạng thái được tạo ra bởi các kỹ năng (Eysenbach et al., 2019; Sharma et al., 2020). Cũng được khám phá để tách biệt quá trình học để đầu tiên tối đa hóa H(s) thông qua ước lượng entropy tối đa, tiếp theo là học hành vi (Campos et al., 2020; Liu & Abbeel, 2021a).

Hơn nữa, được chứng minh thực nghiệm rằng các phương pháp khám phá kỹ năng kém hiệu suất hơn các loại phương pháp tiền huấn luyện trực tuyến khác, có thể do các không gian kỹ năng bị hạn chế (Laskin et al., 2021). Điều này thu hút sự chú ý đến việc phân tích những kỹ năng nào được học. Để sống lên với tiềm năng đầy đủ của chúng, các kỹ năng được khám phá phải đạt được sự cân bằng giữa tính tổng quát (tức là, khả năng áp dụng cho một loạt lớn các nhiệm vụ hạ lưu) và tính đặc thù (tức là, chất lượng hữu ích để tạo ra các hành vi cụ thể) (Gehring et al., 2021). Cũng mong muốn tránh học các kỹ năng tầm thường (Sharma et al., 2020; Baumli et al., 2021).

3.3 Tối đa hóa phủ sóng dữ liệu

Trước đây chúng ta đã thảo luận cách thu được kiến thức hoặc kỹ năng, được đo bằng khả năng của chính tác nhân, từ tương tác không giám sát. Mặc dù liên quan gián tiếp đến khả năng của tác nhân, sự đa dạng dữ liệu được tạo ra bởi tiền huấn luyện trực tuyến đóng vai trò thiết yếu trong việc quyết định tác nhân thu được kiến thức trước tốt như thế nào. Trong lĩnh vực học có giám sát, những tiến bộ gần đây đã cho thấy rằng dữ liệu đa dạng có thể tăng cường khả năng tổng quát hóa ngoài phân phối (Hendrycks et al., 2020b) và tính mạnh mẽ (Hendrycks et al., 2020a). Một bằng chứng hỗ trợ khác là hầu hết các bộ dữ liệu nổi tiếng đều lớn và đa dạng (Deng et al., 2009; Wang et al., 2019). Được thúc đẩy bởi các cân nhắc trên, mong muốn sử dụng tối đa hóa phủ sóng dữ liệu, thường được đo bằng việc thăm quan trạng thái, như một mục tiêu để kích thích học không giám sát.

3.3.1 KHÁM PHÁ DỰA TRÊN ĐẾM

Loại đầu tiên của tối đa hóa phủ sóng dữ liệu là khám phá dựa trên đếm. Các phương pháp khám phá dựa trên đếm trực tiếp sử dụng số lần thăm quan để hướng dẫn tác nhân hướng tới các trạng thái chưa được khám phá (Bellemare et al., 2016; Ostrovski et al., 2017). Đối với các MDP dạng bảng, Ước lượng Khoảng dựa trên Mô hình với Bonus Khám phá (Strehl & Littman, 2008) có thể chứng minh chuyển đổi số đếm hành động-trạng thái N(s,a) thành phần thưởng bonus khám phá:

r_t ∝ N(s_t,a_t)^(-1/2). (2)

Được xây dựng trên Phương trình 2, một loạt công trình đã nghiên cứu cách tổng quát hóa các bonus đếm một cách có thể xử lý đến các không gian trạng thái có chiều cao (Bellemare et al., 2016; Ostrovski et al., 2017; Tang et al., 2017). Để xấp xỉ những số đếm này trong chiều cao, Bellemare et al. (2016) giới thiệu các pseudo-count được rút ra từ một mô hình mật độ. Cụ thể, pseudo-count được định nghĩa là:

N̂(s) = ρ_t(s)(1-ρ'_t(s))/(ρ'_t(s)-ρ_t(s)),

trong đó ρ là một mô hình mật độ trên không gian trạng thái S, ρ_t(s) là mật độ được gán cho s sau khi huấn luyện trên một chuỗi các trạng thái s_1,...,s_t, và ρ'_t(s) là mật độ của s nếu ρ được huấn luyện trên s một lần nữa. Dựa trên các ý tưởng tương tự, đã được chứng minh rằng một mô hình mật độ tốt hơn (Ostrovski et al., 2017) hoặc một hàm hash (Tang et al., 2017; Rashid et al., 2020) để tính toán thống kê trạng thái có thể cải thiện hiệu suất hơn nữa. Bên cạnh đó, một mô hình động lực học nghịch đảo tự giám sát như đã thảo luận trong Phần 3.1 cũng có thể được sử dụng để thiên vị các bonus dựa trên đếm hướng tới những gì tác nhân có thể kiểm soát (Badia et al., 2020).

3.3.2 TỐI ĐA HÓA ENTROPY

Để khuyến khích thăm quan trạng thái mới, một mục tiêu thay thế là trực tiếp tối đa hóa entropy của phân phối thăm quan trạng thái d^π được tạo ra bởi chính sách π(a|s):

π ∈ arg max_π H(d^π),

trong đó H(·) có thể là entropy Shannon (Hazan et al., 2019; Lee et al., 2019; Seo et al., 2021), entropy Rényi (Zhang et al., 2021b), hoặc entropy nhận thức hình học (Guo et al., 2021). Phân phối trạng thái d^π có thể là một phân phối chiết khấu (Hazan et al., 2019), một phân phối biên (Lee et al., 2019), hoặc một phân phối dừng (Tarbouriech & Lazaric, 2019).

Mặc dù hấp dẫn, mục tiêu dựa vào việc tối đa hóa entropy trạng thái, điều này nổi tiếng khó ước lượng và tối ưu hóa. Hazan et al. (2019) đóng góp một thuật toán hiệu quả có thể chứng minh trong thiết lập dạng bảng sử dụng phương pháp gradient có điều kiện (Frank & Wolfe, 1956) để tránh tối ưu hóa trực tiếp. Lee et al. (2019) đề xuất một phương pháp tương tự có thể được xem từ góc độ khớp biên trạng thái giữa phân phối trạng thái và một phân phối mục tiêu cho trước (ví dụ, một phân phối đều). Cả Hazan et al. (2019) và Lee et al. (2019) đều đề xuất học một hỗn hợp các chính sách tối đa hóa entropy trạng thái được tạo ra theo cách lặp. Mặc dù ấn tượng, các phương pháp tham số này gặp khó khăn để mở rộng lên các không gian có chiều cao. Để giải quyết vấn đề này, Mutti et al. (2020) thay vào đó tối ưu hóa một ước lượng không tham số, dựa trên hạt của entropy phân phối trạng thái (Kontoyiannis et al., 1998), nhưng hạn chế việc sử dụng của nó cho các nhiệm vụ dựa trên trạng thái.

Đối với tiền huấn luyện trực tuyến không giám sát với các quan sát thị giác, tối đa hóa entropy trở nên khó khăn hơn vì khám phá hiện bị gắn chặt với học biểu diễn. Điều này dẫn đến một vấn đề gà và trứng (Yarats et al., 2021; Tam et al., 2022), nơi học các biểu diễn hữu ích đòi hỏi dữ liệu đa dạng, trong khi khám phá hiệu quả chỉ có thể được đạt được với các biểu diễn tốt. Dựa trên các ước lượng entropy dựa trên hạt, một số phương pháp thành công áp dụng tối đa hóa entropy trong các nhiệm vụ dựa trên hình ảnh với các biểu diễn tự giám sát được học bởi dự đoán động lực học nghịch đảo (Seo et al., 2021), học đối lập (Liu & Abbeel, 2021b; Yarats et al., 2021), hoặc nút thắt thông tin (Tao et al., 2020).

3.3.3 THÁCH THỨC & HƯỚNG PHÁT TRIỂN TƯƠNG LAI

Mặc dù các phương pháp dựa trên đếm được chứng minh hiệu quả cho khám phá, đã được chỉ ra trong công trình trước đó (Ecoffet et al., 2021) rằng chúng thường gặp phải tách rời, trong đó tác nhân mất dấu các khu vực thú vị để khám phá, và trật đường ray, trong đó cơ chế khám phá ngăn cản nó quay trở lại các trạng thái đã thăm quan trước đó. Các phương pháp dựa trên đếm cũng có xu hướng cận thị, đẩy tác nhân vào tình trạng bị mắc kẹt trong các cực tiểu địa phương (Burda et al., 2019b).

Khi áp dụng các phương pháp tối đa hóa entropy trạng thái cho tiền huấn luyện, đáng chú ý rằng nhiều trong số chúng nhằm mục đích tối đa hóa entropy của tất cả các trạng thái được thăm quan trong quá trình, và do đó chính sách cuối cùng không nhất thiết phải khám phá (Lee et al., 2019). Cũng đã được chứng minh về mặt lý thuyết rằng lớp các chính sách Markovian là không đủ cho mục tiêu entropy trạng thái tối đa, trong khi các chính sách không Markovian là thiết yếu để đảm bảo khám phá tốt.

Thay vì học một chính sách khám phá, một dòng nghiên cứu khác xem xét việc thu thập các bản ghi không được gán nhãn như một điều kiện tiên quyết cho RL ngoại tuyến (Yarats et al., 2022; Lambert et al., 2022), đây là một hướng thú vị để hiểu và sử dụng các tác nhân không phụ thuộc nhiệm vụ.

4. Tiền huấn luyện ngoại tuyến

Mặc dù có hiệu quả hấp dẫn của việc học mà không có sự giám sát của con người, tiền huấn luyện trực tuyến vẫn bị hạn chế cho các ứng dụng quy mô lớn. Cuối cùng, rất khó để dung hòa tương tác trực tuyến với nhu cầu huấn luyện trên các bộ dữ liệu lớn và đa dạng (Levine, 2021). Để giải quyết vấn đề này, mong muốn tách biệt việc thu thập dữ liệu và tiền huấn luyện và trực tiếp tận dụng dữ liệu lịch sử được thu thập từ các tác nhân khác hoặc con người.

Một giải pháp khả thi là RL ngoại tuyến (Lange et al., 2012; Levine et al., 2020), đã thu hút sự chú ý gần đây. RL ngoại tuyến nhằm mục đích thu được một chính sách tối đa hóa phần thưởng hoàn toàn từ dữ liệu ngoại tuyến. Một thách thức cơ bản của RL ngoại tuyến là sự dịch chuyển phân phối, đề cập đến sự khác biệt phân phối giữa dữ liệu huấn luyện và những gì được thấy trong quá trình kiểm tra. Các phương pháp RL ngoại tuyến hiện có tập trung vào cách giải quyết thách thức này khi sử dụng xấp xỉ hàm. Ví dụ, các phương pháp ràng buộc chính sách (Kumar et al., 2019; Siegel et al., 2020) một cách rõ ràng yêu cầu chính sách đã học tránh thực hiện các hành động chưa thấy trong bộ dữ liệu. Các phương pháp chính quy hóa giá trị (Kumar et al., 2020) giảm thiểu vấn đề đánh giá quá cao của các hàm giá trị bằng cách khớp chúng với một số dạng cận dưới. Tuy nhiên, vẫn chưa được khám phá đầy đủ liệu các chính sách được huấn luyện ngoại tuyến có thể tổng quát hóa đến các bối cảnh mới chưa thấy trong bộ dữ liệu ngoại tuyến (Kirk et al., 2021).

Một kịch bản khác là RL ngoại tuyến đến trực tuyến (Nair et al., 2020; Lu et al., 2022; Lee et al., 2022; Kostrikov et al., 2022), nơi RL ngoại tuyến được sử dụng cho tiền huấn luyện, tiếp theo là tinh chỉnh trực tuyến. Đã được chứng minh trong kịch bản này rằng RL ngoại tuyến có thể tăng tốc RL trực tuyến (Nair et al., 2020). Tuy nhiên, cả RL ngoại tuyến và RL ngoại tuyến đến trực tuyến đều yêu cầu kinh nghiệm ngoại tuyến được chú thích với phần thưởng, điều này đầy thách thức để cung cấp cho các bộ dữ liệu thực tế lớn (Pertsch et al., 2021a).

Một hướng thay thế hấp dẫn để tận dụng dữ liệu ngoại tuyến là tránh việc học chính sách, mà thay vào đó học kiến thức trước có lợi cho các nhiệm vụ hạ lưu về tốc độ hội tụ hoặc hiệu suất cuối cùng. Điều thú vị hơn, nếu mô hình của chúng ta có thể sử dụng dữ liệu mà không có sự giám sát của con người, nó có thể có lợi từ dữ liệu quy mô web cho việc ra quyết định. Chúng tôi đề cập đến thiết lập này như tiền huấn luyện ngoại tuyến, nơi tác nhân có thể trích xuất thông tin quan trọng (ví dụ, biểu diễn tốt và prior hành vi) từ dữ liệu ngoại tuyến. Trong Bảng 3, chúng tôi phân loại các phương pháp tiền huấn luyện ngoại tuyến hiện có cũng như tóm tắt các thuộc tính chính của từng phương pháp.

4.1 Trích xuất kỹ năng

Học các hành vi hữu ích từ dữ liệu ngoại tuyến có lịch sử lâu dài (Pomerleau, 1988; Argall et al., 2009). Khi dữ liệu ngoại tuyến đến từ các cuộc biểu diễn chuyên gia, việc tiền huấn luyện chính sách thông qua học bắt chước là điều đơn giản (Silver et al., 2016; Rajeswaran et al., 2018; Gupta et al., 2020), thường được sử dụng trong các ứng dụng thực tế như thao tác robot (Zhu et al., 2018; Zhang et al., 2018) và lái xe tự động (Codevilla et al., 2019). Tuy nhiên, các phương pháp học bắt chước thường giả định rằng dữ liệu huấn luyện chứa các giải pháp hoàn chỉnh. Do đó, chúng thường thiếu khả năng thu được các chính sách tốt khi các cuộc biểu diễn được thu thập từ một loạt các nguồn.

Một giải pháp thay thế là học các prior hành vi hữu ích từ dữ liệu ngoại tuyến (Lynch et al., 2020; Shankar & Gupta, 2020; Chebotar et al., 2021), tương tự như những gì chúng ta đã thảo luận trong Phần 3.2. So với đối tác trực tuyến của nó, trích xuất kỹ năng ngoại tuyến giả định một tập hợp quỹ đạo cố định. Các phương pháp này học một phổ các chính sách hành vi có điều kiện trên tiềm ẩn z, cung cấp một không gian hành động nhỏ gọn hơn để học các chính sách cấp cao có thể nhanh chóng thích ứng với các nhiệm vụ hạ lưu. Cụ thể, trích xuất kỹ năng thời gian (Yang et al., 2022) cho bắt chước few-shot (Ajay et al., 2021) và RL (Ajay et al., 2021; Pertsch et al., 2021a, 2021b) xem xét cách chưng cất các quỹ đạo ngoại tuyến thành các chính sách nguyên thủy π(a|s,z), trong đó z∈Z biểu thị một tiềm ẩn kỹ năng được học thông qua học không giám sát. Bằng cách tận dụng các mô hình biến tiềm ẩn ngẫu nhiên, chúng ta nhằm mục đích học một tiềm ẩn kỹ năng z_i∈Z cho một chuỗi các cặp trạng thái-hành động {s_t,a_t,...,s_{t+H-1},a_{t+H-1}}, trong đó H là một đường chân trời cố định hoặc một đường chân trời biến đổi (Kipf et al., 2019; Shankar et al., 2020). Ví dụ, Ajay et al. (2021) đề xuất mục tiêu mã hóa tự động sau để học các kỹ năng nguyên thủy:

min_{θ,φ,ψ} J(θ,φ,ψ) = E_{D,z~q_φ(z|τ)}[∑_{t=0}^{H-1} log π_θ(a_t|s_t,z)]

s.t. E_D[D_KL(q_φ(z|τ)||p_ψ(z|s_0))] ≤ β_KL,

trong đó q_φ(z|τ) mã hóa quỹ đạo thành tiềm ẩn kỹ năng z và chính sách kỹ năng π_θ(a|s,z) phục vụ như một bộ giải mã để dịch tiềm ẩn kỹ năng z thành các chuỗi hành động. Để chuyển kỹ năng vào các nhiệm vụ hạ lưu, khả thi để học một chính sách phân cấp tạo ra các hành vi cấp cao với π(z|s) được huấn luyện trên các nhiệm vụ hạ lưu (Pertsch et al., 2021a), sẽ được elaborated trong Phần 6.2.

Các mô hình biến tiềm ẩn khác nhau đã được sử dụng cho tiền huấn luyện prior hành vi. Ví dụ, các bộ tự mã hóa biến phân (Kingma & Welling, 2014) được xem xét rộng rãi (Pertsch et al., 2021a; Lynch et al., 2020; Ajay et al., 2021). Công trình tiếp theo (Singh et al., 2021; Florence et al., 2022) cũng khám phá dòng chảy chuẩn hóa (Dinh et al., 2017) và các mô hình dựa trên năng lượng (LeCun et al., 2006) để học các prior hành động.

Kịch bản tiền huấn luyện prior hành vi cũng có sự tương đồng với học bắt chước few-shot (Dance et al., 2021; Hakhamaneshi et al., 2022). Tuy nhiên, đối với học bắt chước few-shot, thường được giả định rằng dữ liệu chuyên gia được thu thập từ một chính sách hành vi duy nhất. Hơn nữa, do tích lũy lỗi (Ross et al., 2011), học bắt chước few-shot thường bị hạn chế đến các vấn đề đường chân trời ngắn (Hakhamaneshi et al., 2022). Về mặt này, học các prior hành vi từ dữ liệu đa dạng và không tối ưu có vẻ là một hướng đầy hứa hẹn.

4.1.1 THÁCH THỨC & HƯỚNG PHÁT TRIỂN TƯƠNG LAI

Mặc dù có tiềm năng trích xuất các kỹ năng nguyên thủy hữu ích, vẫn đầy thách thức để tiền huấn luyện trên dữ liệu ngoại tuyến rất không tối ưu chứa các hành động ngẫu nhiên (Ajay et al., 2021). Bên cạnh đó, RL với các kỹ năng đã học thường không tổng quát hóa đến các nhiệm vụ hạ lưu một cách hiệu quả, đòi hỏi hàng triệu tương tác trực tuyến để hội tụ (Hakhamaneshi et al., 2022). Một giải pháp có thể là kết hợp với các đặc trưng kế thừa (Barreto et al., 2017; Hansen et al., 2020) để suy luận nhiệm vụ nhanh. Tuy nhiên, các chiến lược trực tiếp sử dụng các chính sách được tiền huấn luyện cho khai thác có thể dẫn đến các giải pháp không tối ưu trong kịch bản như vậy (Campos et al., 2021).

4.2 Học biểu diễn

Trong khi tiền huấn luyện prior hành vi tập trung vào việc giảm độ phức tạp của không gian hành động, tồn tại một dòng công việc khác nhằm mục đích tiền huấn luyện các biểu diễn trạng thái tốt từ dữ liệu ngoại tuyến để thúc đẩy chuyển đổi. Nếu tác nhân hiệu quả giảm khoảng cách biểu diễn giữa các biểu diễn trạng thái đã học và các trạng thái nội sinh sự thật cơ bản, nó có thể tập trung tốt hơn vào các yếu tố thiết yếu cho điều khiển. Bảng 4 so sánh các loại mục tiêu học biểu diễn khác nhau về tính đầy đủ (tức là, liệu các biểu diễn có chứa thông tin trạng thái đầy đủ) và tính nhỏ gọn (tức là, liệu các biểu diễn có loại bỏ thông tin không liên quan).

Học các biểu diễn trạng thái tốt cho RL là một lĩnh vực nghiên cứu trưởng thành với một loạt các công cụ (Nachum et al., 2019; van den Oord et al., 2018; Zhang et al., 2022b). Theo truyền thống, vấn đề được công thức hóa để nhóm các trạng thái thành cụm dựa trên một số thuộc tính nhất định (Li et al., 2006). Các phương pháp học biểu diễn hiện có nói chung đề xuất một số thuộc tính dự đoán mà các biểu diễn mong muốn có, liên quan đến các trạng thái, hành động và phần thưởng qua các bước thời gian khác nhau. Một trong những khái niệm đại diện nhất là bisimulation (Larsen & Skou, 1991; Givan et al., 2003), ban đầu yêu cầu hai trạng thái tương đương có cùng phần thưởng và các phân phối tương đương trên các trạng thái bisimilar tiếp theo. Mục tiêu hóa ra rất hạn chế và được nới lỏng hơn bởi công việc tiếp theo (Ferns et al., 2004; Castro & Precup, 2010) với một không gian pseudo-metric được định nghĩa để đo sự tương tự hành vi. Mặc dù có những tiến bộ gần đây (Gelada et al., 2019; Zhang et al., 2021a; Agarwal et al., 2021) trong học biểu diễn hiệu quả sử dụng mạng nơ-ron sâu, các phương pháp bisimulation không cung cấp sự trừu tượng hóa tốt khi các phần thưởng thưa thớt hoặc thậm chí không có. Trong trường hợp này, chỉ dựa vào một mô hình tiến có thể dẫn đến sự sụp đổ biểu diễn (Allen et al., 2021).

Để giảm thiểu sự sụp đổi biểu diễn, người ta có thể thay vào đó đặt các mục tiêu cho các quan sát pixel. Điều này bao gồm các phương pháp dựa trên tái tạo (Lange & Riedmiller, 2010; Ha & Schmidhuber, 2018) và những phương pháp dựa trên dự đoán pixel (Watter et al., 2015; Kaiser et al., 2020). Các phương pháp dựa trên tái tạo thường huấn luyện một bộ tự mã hóa trên các quan sát hình ảnh để học một biểu diễn chiều thấp, sử dụng nó một chính sách được học tiếp theo. Các phương pháp dựa trên dự đoán pixel buộc các biểu diễn chứa thông tin đầy đủ về các quan sát pixel tương lai. Mặc dù các biểu diễn đã học này bảo tồn thông tin đầy đủ về quan sát, nó thiếu tính nhỏ gọn và không đảm bảo nắm bắt thông tin hữu ích cho nhiệm vụ điều khiển.

Thay vì dự đoán tương lai, cũng có lợi để mô hình hóa động lực học nghịch đảo của hệ thống (Christiano et al., 2016; Pathak et al., 2017). Mô hình hóa động lực học nghịch đảo học một biểu diễn có tính dự đoán của hành động được thực hiện giữa một cặp trạng thái liên tiếp. Đã được chứng minh rằng biểu diễn đã học có thể lọc ra tất cả các khía cạnh không thể kiểm soát của các quan sát (Efroni et al., 2021). Tuy nhiên, nó cũng có thể sai lầm bỏ qua thông tin có thể kiểm soát và gây ra sự trừu tượng hóa quá mức trên không gian trạng thái (Rakelly et al., 2021; Efroni et al., 2021).

Với sự gia tăng của học tự giám sát được phát triển cho CV và NLP, một hướng tự nhiên là thích ứng các kỹ thuật không phụ thuộc nhiệm vụ này cho RL. Ví dụ, một khối lượng lớn công việc đã khám phá học đối lập (Gutmann & Hyvärinen, 2010) như một khung hiệu quả để học các biểu diễn tốt (Nachum et al., 2019; Laskin et al., 2020; Mazoure et al., 2020; Zhan et al., 2020). Học đối lập thường sử dụng mất mát InfoNCE (Poole et al., 2019) để tối đa hóa thông tin tương hỗ giữa hai biến:

L_InfoNCE = E[log(exp(f(x_i,y_i))/(1/K ∑_{j=1}^K exp(f(x_i,y_j))))],

trong đó f là một hàm song tuyến f(x_i,y_i) = φ(x_i)^T W ψ(y_i) với tham số đã học W∈ℝ^{n×n} và K là số lượng mẫu âm. Các phương pháp này thường kết hợp thông tin thời gian, nhằm mục đích phân biệt giữa các trạng thái tuần tự và không tuần tự (Anand et al., 2019; Stooke et al., 2021). Công việc tiếp theo (Schwarzer et al., 2021a, 2021b) tiếp tục xem xét các biểu diễn tiềm ẩn được bootstrapped (Grill et al., 2020) loại bỏ các mẫu âm.

Ngoài các mục tiêu học biểu diễn trên, một số công việc khác xem xét việc áp đặt sự mượt Lipschitz (Gelada et al., 2019; Zhang et al., 2021a), tính không thể tách rời động học (Misra et al., 2020), hoặc thuộc tính Markov (Allen et al., 2021). Cũng đã được chứng minh rằng một mục tiêu kết hợp cũng có thể dẫn đến hiệu suất tốt hơn (Schwarzer et al., 2021b).

4.2.1 THÁCH THỨC & HƯỚNG PHÁT TRIỂN TƯƠNG LAI

Trong khi các biểu diễn không giám sát đã được chứng minh mang lại những cải thiện đáng kể cho các nhiệm vụ hạ lưu, sự vắng mặt của các tín hiệu phần thưởng thường dẫn đến bộ mã hóa được tiền huấn luyện tập trung vào các đặc trưng không liên quan đến nhiệm vụ thay vì những đặc trưng liên quan đến nhiệm vụ trong các môi trường phức tạp về mặt thị giác (Yamada et al., 2022). Để giảm thiểu vấn đề này, người ta có thể kết hợp thiên vị quy nạp bổ sung (Janny et al., 2022) hoặc dữ liệu được gán nhãn rẻ hơn để thu được. Chúng tôi sẽ thảo luận giải pháp sau trong Phần 5.

Một thách thức khác đối với học biểu diễn không giám sát là cách đo lường hiệu quả của nó mà không có quyền truy cập vào các nhiệm vụ hạ lưu. Đánh giá như vậy có lợi vì nó có thể cung cấp một metric proxy để dự đoán hiệu suất và thúc đẩy hiểu biết sâu hơn về ý nghĩa ngữ nghĩa của các biểu diễn được tiền huấn luyện. Để đạt được điều này, mong muốn phân tích các biểu diễn này với các kỹ thuật thăm dò và xác định những thuộc tính nào chúng mã hóa. Mặc dù công việc trước đây đã nỗ lực trong hướng này (Zhang et al., 2022a), vẫn chưa rõ những thuộc tính nào không thể thiếu nhất cho các biểu diễn được tiền huấn luyện.

5. Hướng tới các tác nhân tổng quát với RL

Cho đến nay chúng ta đã thảo luận các kịch bản trực tuyến và ngoại tuyến thường bị hạn chế đến một phương thức duy nhất và môi trường duy nhất. Gần đây, có sự gia tăng quan tâm đến việc xây dựng một mô hình tổng quát duy nhất (Reed et al., 2022; Lee et al., 2022; Fan et al., 2022) để xử lý các nhiệm vụ trong các môi trường khác nhau qua các phương thức khác nhau. Để cho phép tác nhân học từ và thích ứng với nhiều nhiệm vụ mở khác nhau, mong muốn tận dụng kiến thức trước đáng kể dưới các hình thức khác nhau như nhận thức thị giác và hiểu ngôn ngữ. Một cách trực quan, mục tiêu là kết nối các thế giới của RL và các lĩnh vực khác của học máy, kết hợp thành công trước đây để xây dựng một mô hình ra quyết định lớn có khả năng thực hiện một tập hợp đa dạng các nhiệm vụ. Trong phần này, chúng ta xem xét các cân nhắc khác nhau để xử lý dữ liệu và nhiệm vụ từ các phương thức khác nhau để thu nhận kiến thức trước hữu ích.

5.1 Tiền huấn luyện thị giác

Nhận thức là một điều kiện tiên quyết không thể tránh khỏi cho các ứng dụng thực tế. Với số lượng ngày càng tăng các nhiệm vụ ra quyết định dựa trên hình ảnh, các bộ mã hóa thị giác được tiền huấn luyện đã được tiếp xúc với một phân phối rộng của hình ảnh có thể cung cấp cho các tác nhân RL các biểu diễn mạnh mẽ và kiên cường như một cơ sở để học các chính sách tối ưu.

Lĩnh vực thị giác máy tính đã chứng kiến tiến bộ to lớn trong việc tiền huấn luyện các bộ mã hóa thị giác từ các bộ dữ liệu hình ảnh quy mô lớn (Deng et al., 2009) và các kho video (Abu-El-Haija et al., 2016). Cho rằng những dữ liệu này rẻ để truy cập, một số công trình đã khám phá việc sử dụng các bộ mã hóa thị giác được tiền huấn luyện trên các bộ dữ liệu hình ảnh quy mô lớn như phương tiện để cải thiện khả năng tổng quát hóa và hiệu quả mẫu của các tác nhân RL. Shah và Kumar (2021) trang bị các thuật toán RL sâu tiêu chuẩn với các bộ mã hóa ResNet được tiền huấn luyện trên ImageNet và quan sát rằng các biểu diễn được tiền huấn luyện dẫn đến hiệu suất ấn tượng trong Adroit (Rajeswaran et al., 2018) nhưng gặp khó khăn trong bộ điều khiển DeepMind (Tassa et al., 2018) do khoảng cách lĩnh vực lớn. Parisi et al. (2022) tiếp tục điều tra các lựa chọn thiết kế khác nhau bao gồm bộ dữ liệu, tăng cường và các lớp, và báo cáo kết quả tích cực trên tất cả bốn nhiệm vụ điều khiển được xem xét. Träuble et al. (2022) tiến hành một nghiên cứu quy mô lớn về cách các thuộc tính khác nhau của các embedding dựa trên VAE được tiền huấn luyện ảnh hưởng đến khả năng tổng quát hóa ngoài phân phối, kết luận rằng một số trong số chúng (ví dụ, metric GS (Dittadi et al., 2021)) có thể là các metric proxy tốt để dự đoán hiệu suất tổng quát hóa.

Thay vì trích xuất thông tin thị giác từ các bộ dữ liệu hình ảnh tĩnh, một hướng thú vị khác là nắm bắt các mối quan hệ thời gian từ các video không được gán nhãn. Sermanet et al. (2018) thiết kế một phương pháp tự giám sát để học phương sai thời gian và bất biến đa góc nhìn trên dữ liệu video đa góc nhìn. Xiao et al. (2022) tìm thấy thực nghiệm rằng, mà không khai thác thông tin thời gian, các hình ảnh in-the-wild được thu thập từ YouTube hoặc video Egocentric dẫn đến các biểu diễn tự giám sát tốt hơn cho các nhiệm vụ thao tác so với hình ảnh ImageNet. Seo et al. (2022) giới thiệu một khung học hai giai đoạn, đầu tiên học các biểu diễn hữu ích thông qua tiền huấn luyện sinh trên video và sau đó sử dụng mô hình được tiền huấn luyện để học các mô hình thế giới có điều kiện hành động. Baker et al. (2022) thành công trích xuất các prior hành vi từ video quy mô internet với một mô hình động lực học nghịch đảo để khám phá các hành động cơ bản được theo sau bởi cloning hành vi, phát hiện rằng mô hình được tiền huấn luyện thể hiện khả năng zero-shot ấn tượng và kết quả tinh chỉnh để chơi Minecraft. Zhang et al. (2022) cũng tận dụng các mô hình động lực học nghịch đảo để dự đoán các nhãn hành động từ các video không có hành động, mà một khung học đối lập mới được đề xuất để tiền huấn luyện các chính sách có điều kiện hành động.

5.2 Tiền huấn luyện ngôn ngữ tự nhiên

Con người không chỉ có thể cảm nhận thế giới thị giác qua đôi mắt của họ, mà hiểu các hướng dẫn ngôn ngữ tự nhiên cấp cao và nền tảng kiến thức phong phú từ văn bản để hoàn thành các nhiệm vụ. Theo quan điểm này, đã có một lịch sử lâu dài về cách kết nối ngôn ngữ và hành động (Kollar et al., 2010; Tellex et al., 2011). Đặc biệt do sự phát triển nhanh chóng của các mô hình ngôn ngữ lớn (LLM) (Brown et al., 2020; Chowdhery et al., 2022) thể hiện khả năng lớn trong việc mã hóa kiến thức ngữ nghĩa, có vẻ là một hướng đầy hứa hẹn để tận dụng các LLM tiên tiến như các động cơ tính toán chung để tạo thuận lợi cho việc ra quyết định (Lu et al., 2021a).

5.2.1 HỌC CHÍNH SÁCH CÓ ĐIỀU KIỆN NGÔN NGỮ

Để trích xuất và khai thác kiến thức của các LLM được tiền huấn luyện được thông tin tốt, một giải pháp khả thi là điều kiện các chính sách trên các mô tả văn bản được xử lý bởi LLM. Loại học chính sách có điều kiện ngôn ngữ này có thể cực kỳ hữu ích cho các nhiệm vụ robot nơi các hướng dẫn ngôn ngữ cấp cao có sẵn. Ví dụ, Ahn et al. (2022) sử dụng các LLM được tiền huấn luyện để chia các hướng dẫn cấp cao thành các nhiệm vụ phụ thông qua kỹ thuật prompt để nền tảng các hàm giá trị trong các nhiệm vụ robot thực tế. Huang et al. (2022b) tiếp tục cho phép phản hồi vòng kín được nền tảng được tạo ra bởi các mô hình nhận thức bổ sung như nguồn sửa chữa cho các dự đoán của LLM. Tam et al. (2022) thay vào đó xem xét khám phá hiệu quả trong các môi trường 3D, cho thấy rằng các biểu diễn được tiền huấn luyện từ các mô hình thị giác-ngôn ngữ (Radford et al., 2021) tạo thành một không gian trạng thái có ý nghĩa ngữ nghĩa cho các phần thưởng nội tại dựa trên tò mò. Mahmoudieh et al. (2022) cũng kết nối đặc tả phần thưởng với giám sát thị giác-ngôn ngữ, giới thiệu một khung tận dụng các mô tả văn bản và quan sát pixel để tạo ra các tín hiệu phần thưởng.

5.2.2 KHỞI TẠO CHÍNH SÁCH

Những tiến bộ gần đây kết nối khoảng cách giữa học tăng cường và mô hình hóa tuần tự (Chen et al., 2021a; Janner et al., 2021; Zheng et al., 2022; Furuta et al., 2022), mở ra cơ hội để mượn các mô hình tuần tự cho các nhiệm vụ RL. Mặc dù có sự khác biệt rõ ràng, các LLM được tiền huấn luyện có thể cung cấp kiến thức có thể tái sử dụng thông qua khởi tạo trọng số. Reid et al. (2022) điều tra liệu các LLM được tiền huấn luyện có thể cung cấp khởi tạo trọng số tốt cho các mô hình RL ngoại tuyến dựa trên Transformer, và kết luận với kết quả rất tích cực. Li et al. (2022b) cũng chứng minh rằng các LLM được tiền huấn luyện có thể được sử dụng để khởi tạo các chính sách và tạo thuận lợi cho cloning hành vi cũng như học tăng cường trực tuyến cho các nhiệm vụ embodied. Họ cũng đề xuất sử dụng các biểu diễn đầu vào tuần tự và tinh chỉnh các trọng số được tiền huấn luyện để tổng quát hóa tốt hơn.

5.3 Tiền huấn luyện đa nhiệm vụ và đa phương thức

Với những tiến bộ gần đây trong việc xây dựng các mô hình chuỗi mạnh mẽ để xử lý các phương thức và nhiệm vụ khác nhau (Lu et al., 2019; Jaegle et al., 2022; Wang et al., 2022), làn sóng sử dụng các mô hình đa mục đích lớn (Bommasani et al., 2021) đã quét qua lĩnh vực học có giám sát. Thành phần chính là Transformer (Vaswani et al., 2017), một kiến trúc nơ-ron có khả năng cao được xây dựng trên cơ chế self-attention (Bahdanau et al., 2015) vượt trội trong việc nắm bắt các phụ thuộc tầm xa trong dữ liệu tuần tự. Do tính tổng quát mạnh mẽ của nó nơi các nhiệm vụ khác nhau trong các lĩnh vực khác nhau có thể được công thức hóa như mô hình hóa chuỗi, Transformer được tin là một kiến trúc thống nhất để phát triển các mô hình nền tảng (Bommasani et al., 2021).

Gần đây, các kiến trúc dựa trên Transformer cũng đã được mở rộng đến lĩnh vực RL ngoại tuyến (Chen et al., 2021a; Janner et al., 2021) và sau đó RL trực tuyến (Zheng et al., 2022), trong đó tác nhân được huấn luyện auto-regressively theo cách có giám sát thông qua tối đa hóa likelihood. Điều này mở ra khả năng nhân bản thành công trước đây đạt được với Transformer trong lĩnh vực học có giám sát. Cụ thể, dự kiến rằng bằng cách kết hợp dữ liệu quy mô lớn, mục tiêu mở và các kiến trúc dựa trên Transformer, chúng ta sẵn sàng xây dựng các tác nhân ra quyết định đa mục đích có khả năng thực hiện nhiều nhiệm vụ hạ lưu khác nhau trong các môi trường khác nhau.

Công trình tiên phong trong hướng này là Gato (Reed et al., 2022), một tác nhân tổng quát được huấn luyện trên nhiều nhiệm vụ khác nhau từ môi trường điều khiển, bộ dữ liệu thị giác và bộ dữ liệu ngôn ngữ theo cách có giám sát. Để xử lý dữ liệu đa nhiệm vụ và đa phương thức, Gato sử dụng các cuộc biểu diễn như chuỗi prompt (Wei et al., 2022) tại thời điểm suy luận. Lee et al. (2022) mở rộng Decision Transformer (Chen et al., 2021a) để huấn luyện một tác nhân tổng quát được gọi là Multi-Game DT có thể chơi 41 trò chơi Atari đồng thời. Cả Gato và Multi-Game DT đều cho thấy các thuộc tính quy luật mở rộng ấn tượng. Fan et al. (2022) sử dụng dữ liệu đa phương thức quy mô lớn từ video YouTube, trang Wikipedia và bài đăng Reddit để huấn luyện một tác nhân có khả năng giải quyết nhiều nhiệm vụ khác nhau trong Minecraft. Để cung cấp các tín hiệu phần thưởng dày đặc, một mô hình thị giác-ngôn ngữ được tiền huấn luyện dựa trên CLIP (Radford et al., 2021) được giới thiệu như một proxy của đánh giá con người.

5.4 Thách thức & Hướng phát triển tương lai

Mặc dù có một số kết quả đầy hứa hẹn, cách các mô hình tổng quát hưởng lợi từ dữ liệu đa phương thức và đa nhiệm vụ vẫn chưa rõ ràng. Cụ thể hơn, các mô hình này có thể gặp phải sự can thiệp gradient có hại (Yu et al., 2020) giữa các phương thức và nhiệm vụ do các thách thức tối ưu hóa phát sinh. Để giảm thiểu vấn đề này, mong muốn kết hợp nhiều công cụ phân tích hơn cho các cảnh quan tối ưu hóa (Goodfellow & Vinyals, 2015) và gradient (Yu et al., 2020) để tách ra các nguyên tắc chính xác.

Một hướng hấp dẫn khác là kết hợp các mô hình được tiền huấn luyện riêng biệt (ví dụ, GPT-3 (Brown et al., 2020) và CLIP (Radford et al., 2021)) với nhau. Bằng cách tận dụng kiến thức chuyên gia từ các mô hình khác nhau, loại khung này có thể giải quyết các nhiệm vụ đa phương thức phức tạp (Li et al., 2022a).

6. Thích ứng nhiệm vụ

Trong khi tiền huấn luyện trên các kinh nghiệm không giám sát có thể dẫn đến kiến thức có thể chuyển đổi phong phú, vẫn đầy thách thức để thích ứng kiến thức đó với các nhiệm vụ hạ lưu trong đó các tín hiệu phần thưởng được tiếp xúc. Trong phần này, chúng tôi thảo luận ngắn gọn các cân nhắc khác nhau cho thích ứng nhiệm vụ hạ lưu. Chúng tôi giới hạn phạm vi đến thích ứng trực tuyến, trong khi thích ứng với RL ngoại tuyến hoặc học bắt chước cũng khả thi (Yang & Nachum, 2021).

Trong thích ứng nhiệm vụ trực tuyến, một mô hình được tiền huấn luyện được đưa ra, có thể được tạo thành từ các thành phần khác nhau như chính sách và biểu diễn, cùng với một MDP mục tiêu có thể tương tác với. Cho rằng tiền huấn luyện có thể dẫn đến các hình thức kiến thức khác nhau, nó mang lại khó khăn trong việc thiết kế các kỹ thuật thích ứng có nguyên tắc. Tuy nhiên, những nỗ lực đáng kể đã được thực hiện để nghiên cứu khía cạnh này.

6.1 Chuyển đổi biểu diễn

Trong lĩnh vực học có giám sát, những tiến bộ gần đây (Devlin et al., 2019; He et al., 2020; Chen et al., 2020) đã chứng minh rằng các biểu diễn tốt có thể được tiền huấn luyện trên bộ dữ liệu không được gán nhãn quy mô lớn, như được chứng minh bởi hiệu suất hạ lưu ấn tượng của chúng. Thực hành phổ biến nhất là đóng băng các trọng số của bộ mã hóa đặc trưng được tiền huấn luyện và huấn luyện một mạng đặc thù nhiệm vụ được khởi tạo ngẫu nhiên trên đó trong quá trình thích ứng. Thành công của mô hình này về cơ bản dựa trên lời hứa rằng các nhiệm vụ liên quan thường có thể được giải quyết bằng các biểu diễn tương tự.

Đối với RL, đã được chứng minh rằng việc tái sử dụng trực tiếp các biểu diễn không phụ thuộc nhiệm vụ được tiền huấn luyện có thể cải thiện đáng kể hiệu quả mẫu trên các nhiệm vụ hạ lưu. Ví dụ, Schwarzer et al. (2021b) tiến hành thí nghiệm trên benchmark Atari 100K và thấy rằng các biểu diễn đóng băng được tiền huấn luyện trên dữ liệu ngoại tuyến khám phá đã tạo thành một cơ sở của RL hiệu quả dữ liệu. Thành công này cũng mở rộng đến các trường hợp nơi tồn tại sự khác biệt lĩnh vực giữa các nhiệm vụ thượng lưu và hạ lưu (Shah & Kumar, 2021; Parisi et al., 2022). Tuy nhiên, vấn đề chuyển đổi âm trước mặt sự khác biệt lĩnh vực có thể bị trầm trọng hóa đối với RL do độ phức tạp của nó (Shah & Kumar, 2021).

Khi thích ứng với các nhiệm vụ có cùng động lực học môi trường như của (các) nhiệm vụ thượng lưu, các đặc trưng kế thừa (Barreto et al., 2017) có thể là một công cụ mạnh mẽ để hỗ trợ thích ứng nhiệm vụ. Khung của các đặc trưng kế thừa dựa trên sự phân rã sau của các hàm phần thưởng:

r_{s,a,s'} = φ_{s,a,s'}^T w, (3)

trong đó φ(s,a,s') ∈ ℝ^d đại diện cho các đặc trưng của chuyển đổi (s,a,s') và w ∈ ℝ^d mã hóa các trọng số chỉ định phần thưởng. Điều này dẫn đến một biểu diễn của hàm giá trị tách biệt động lực học của môi trường khỏi các phần thưởng:

Q^π(s,a) = E_{s_t=s,a_t=a}[∑_{i=t}^∞ γ^{i-t} φ_{s_{i+1},a_{i+1},s'_{i+1}}]^T w = ψ^π(s,a)^T w,

trong đó chúng ta gọi ψ^π(s,a) là các đặc trưng kế thừa của (s,a) dưới π. Một cách trực quan, ψ tóm tắt động lực học được tạo ra bởi π và đã được nghiên cứu trong khung của tiền huấn luyện trực tuyến (Hansen et al., 2020; Liu & Abbeel, 2021a) bằng cách kết hợp với các phương pháp khám phá kỹ năng để học ngầm các đặc trưng kế thừa có thể kiểm soát ψ^π(s,a). Cho một ψ^π(s,a) đã học, vấn đề thích ứng nhiệm vụ được rút gọn thành một hồi quy tuyến tính được rút ra từ Phương trình 3.

6.2 Chuyển đổi chính sách

Một thay thế hấp dẫn cho thích ứng nhiệm vụ là chuyển đổi các hành vi đã học. Như đã thảo luận trong các phần trước, công việc hiện có đã khám phá cách tiền huấn luyện các kỹ năng nguyên thủy có thể được tái sử dụng để đối mặt với các nhiệm vụ mới hoặc một chính sách khám phá duy nhất tạo thuận lợi cho việc khám phá ở đầu thích ứng nhiệm vụ. Sự khác biệt trong các hành vi được tiền huấn luyện dẫn đến các chiến lược thích ứng khác nhau.

Để đạt được phần thưởng cao trên nhiệm vụ hạ lưu với chính sách có điều kiện kỹ năng π(a|s,z), một chiến lược đơn giản là chỉ đơn giản chọn kỹ năng z với kết quả tốt nhất và tăng cường nó hơn nữa với tinh chỉnh. Tuy nhiên, một kỹ năng có hiệu suất tốt nhất duy nhất không thể phát huy hết tiềm năng của nó. Để kết hợp tốt hơn các kỹ năng đa dạng cho việc giải quyết nhiệm vụ, người ta có thể xem chúng từ góc độ của RL phân cấp (Barto & Mahadevan, 2003; Kulkarni et al., 2016). Trong RL phân cấp, nhiệm vụ ra quyết định thường được phân rã thành một hệ thống phân cấp hai cấp, nơi một meta-controller π(z|s) quyết định chính sách cấp thấp nào để sử dụng cho việc giải quyết nhiệm vụ, tùy thuộc vào trạng thái hiện tại. Sơ đồ phân cấp này không phụ thuộc vào cách các chính sách cấp thấp được học. Do đó, đủ để huấn luyện một meta-controller trên đỉnh các kỹ năng được khám phá, đã được chứng minh hiệu quả cho thích ứng few-shot (Hakhamaneshi et al., 2022) và thích ứng zero-shot (Sharma et al., 2020).

Các chính sách khám phá, như một hình thức kiến thức trước khác, mang lại lợi ích cho các nhiệm vụ hạ lưu theo cách khác. Do tầm quan trọng của khám phá, các chính sách khám phá có thể cung cấp khởi tạo tốt cho tác nhân để thu thập kinh nghiệm đa dạng và đạt đến các trạng thái có phần thưởng cao. Ví dụ, Campos et al. (2021) xác nhận hiệu quả của việc chuyển đổi các chính sách khám phá được huấn luyện bởi các phương pháp dựa trên tò mò, đặc biệt đối với các lĩnh vực đòi hỏi khám phá có cấu trúc.

Trong khi luôn khả thi để tinh chỉnh các chính sách được tiền huấn luyện, các cân nhắc nên được thực hiện để ngăn chặn việc quên thảm khốc khi học trong nhiệm vụ hạ lưu. Quên thảm khốc đề cập đến xu hướng của mạng nơ-ron bỏ qua kiến thức đã thu được trước đó khi thông tin mới được thu nhận. Để giảm thiểu vấn đề này, người ta có thể áp dụng chính quy hóa giống như chưng cất kiến thức cùng với các mục tiêu RL (Schmitt et al., 2018):

L_KD = H(π̂(a|s) || π(a|s)),

trong đó H là entropy chéo và π̂ là chính sách giáo viên. Chúng tôi giới thiệu độc giả đến Khetarpal et al. (2020) để thảo luận thêm về quên thảm khốc trong học tăng cường.

6.3 Thách thức & Hướng phát triển tương lai

Hiệu quả tham số. Mặc dù các mô hình được tiền huấn luyện hiện có cho RL có ít tham số hơn nhiều so với những mô hình trong lĩnh vực học có giám sát, vấn đề hiệu quả tham số vẫn quan trọng với số lượng tham số mô hình ngày càng tăng. Cụ thể hơn, mong muốn thiết kế học chuyển đổi hiệu quả tham số chỉ cập nhật một phần nhỏ tham số trong khi giữ hầu hết các tham số được tiền huấn luyện nguyên vẹn. Nó đã được nghiên cứu tích cực trong xử lý ngôn ngữ tự nhiên (He et al., 2022) với các giải pháp như thêm các mô-đun nơ-ron nhỏ như adapter (Houlsby et al., 2019) và prepending các token prefix có thể học như soft prompt (Lester et al., 2021). Được xây dựng trên các kỹ thuật này, một số nỗ lực đã được thực hiện để cho phép chuyển đổi hiệu quả tham số với prompting (Xu et al., 2022; Reed et al., 2022), mà chúng tôi tin có một khoảng lớn để cải thiện với các phương pháp được thiết kế riêng.

Thích ứng lĩnh vực. Trong phần này, chúng tôi chủ yếu xem xét thích ứng nhiệm vụ nơi các nhiệm vụ chưa thấy được đưa ra trong cùng môi trường. Một kịch bản đầy thách thức nhưng thực tiễn hơn là thích ứng lĩnh vực. Trong thích ứng lĩnh vực, tồn tại các dịch chuyển môi trường giữa các nhiệm vụ thượng lưu và hạ lưu. Mặc dù những dịch chuyển môi trường này thường được thấy trong các ứng dụng thực tế, vẫn là một vấn đề đầy thách thức để chuyển đổi qua các lĩnh vực khác nhau (Eysenbach et al., 2021; Huang et al., 2022a). Tuy nhiên, chúng tôi tin rằng hướng này sẽ phát triển nhanh chóng bằng cách mang các kỹ thuật liên quan từ học có giám sát đến học tăng cường.

Các mô hình được phát triển liên tục. Đối với các ứng dụng thực tiễn, chúng ta có thể tiến thêm một bước và xem xét việc xây dựng các mô hình được tiền huấn luyện lớn liên tục để hỗ trợ các tính năng được thêm vào (ví dụ, một không gian hành động được sửa đổi, các kiến trúc mạnh mẽ hơn, v.v.). Trong khi cân nhắc như vậy đã được tiến hành trong quá trình phát triển các mô hình RL quy mô lớn (Berner et al., 2019), nó đòi hỏi một cách có nguyên tắc hơn để kết hợp các cập nhật vào các mô hình RL. Chúng tôi giới thiệu độc giả đến công việc gần đây trong hướng này trong lĩnh vực học có giám sát (Raffel, 2021) và học tăng cường (Campos et al., 2021; Agarwal et al., 2022).

7. Kết luận và Triển vọng tương lai

Trong phần này, chúng tôi kết luận khảo sát này và làm nổi bật một số hướng tương lai mà chúng tôi tin sẽ là các chủ đề quan trọng cho công việc tương lai.

Bài báo này giới thiệu tiền huấn luyện trong RL sâu bằng cách thảo luận các xu hướng gần đây để thu được kiến thức trước chung cho việc ra quyết định. Trái ngược với đối tác học có giám sát của nó, tiền huấn luyện đối mặt với nhiều thách thức độc đáo đối với RL. Trong khảo sát này, chúng tôi trình bày một số hướng nghiên cứu đầy hứa hẹn để giải quyết những thách thức này và chúng tôi tin rằng lĩnh vực này sẽ phát triển nhanh chóng trong những năm tới.

Vẫn còn một số câu hỏi mở quan trọng và vẫn cần được giải quyết.

Benchmark và metric đánh giá. Đánh giá phục vụ như một phương tiện để so sánh các phương pháp khác nhau và thúc đẩy những cải thiện hơn nữa. Trong lĩnh vực xử lý ngôn ngữ tự nhiên, GLUE (Wang et al., 2019) là một benchmark được sử dụng rộng rãi để đánh giá hiệu suất của các mô hình qua nhiều nhiệm vụ hiểu ngôn ngữ tự nhiên khác nhau. Gần đây, đã có sự gia tăng nghiên cứu về cải thiện đánh giá cho RL về metric đánh giá (Agarwal et al., 2021) và bộ dữ liệu benchmark (Cobbe et al., 2020). Theo hiểu biết tốt nhất của chúng tôi, URLB (Laskin et al., 2020) là benchmark duy nhất cho tiền huấn luyện trong RL sâu. Nó trình bày một giao thức đánh giá thống nhất cho tiền huấn luyện trực tuyến dựa trên bộ điều khiển DeepMind (Tassa et al., 2018). Tuy nhiên, một khung đánh giá có nguyên tắc cho tiền huấn luyện ngoại tuyến và tiền huấn luyện tổng quát vẫn còn thiếu. Chúng tôi kỳ vọng các benchmark RL ngoại tuyến hiện có như D4RL (Fu et al., 2020) và RL Unplugged (Gulcehre et al., 2020) có thể phục vụ như cơ sở để phát triển các benchmark tiền huấn luyện, nhưng các nhiệm vụ đầy thách thức hơn nên minh họa tốt hơn giá trị của tiền huấn luyện.

Kiến trúc. Như đã thảo luận trong các phần trước, đã có sự gia tăng việc tận dụng các transformer lớn cho các nhiệm vụ RL. Chúng tôi kỳ vọng các tiến bộ gần đây khác trong kiến trúc mô hình có thể mang lại nhiều cải thiện hơn. Ví dụ, Mustafa et al. (2022) học các mô hình thưa thớt lớn với một hỗn hợp các chuyên gia xử lý đồng thời hình ảnh và văn bản với định tuyến không phụ thuộc phương thức. Điều này có triển vọng giải quyết các nhiệm vụ phức tạp theo quy mô. Bên cạnh đó, người ta cũng có thể suy nghĩ lại các kiến trúc hiện có có tiềm năng hỗ trợ tiền huấn luyện quy mô lớn (ví dụ, Progress Neural Networks (Rusu et al., 2016)).

RL đa tác nhân. RL đa tác nhân (Zhang et al., 2019) là một tiểu lĩnh vực quan trọng của RL. Mở rộng các kỹ thuật tiền huấn luyện hiện có đến kịch bản đa tác nhân không tầm thường. RL đa tác nhân thường đòi hỏi các hành vi mong muốn xã hội (Ndousse et al., 2021) và biểu diễn (Li et al., 2021). Theo hiểu biết tốt nhất của chúng tôi, Meng et al. (2021) trình bày nỗ lực duy nhất trong tiền huấn luyện RL đa tác nhân với sự giám sát. Cách cho phép tiền huấn luyện không giám sát cho RL đa tác nhân vẫn chưa rõ ràng, mà chúng tôi tin là một hướng nghiên cứu đầy hứa hẹn.

Kết quả lý thuyết. Đối với RL, khoảng cách đáng kể giữa lý thuyết và thực hành đã là một vấn đề lâu dài, và việc mang tiền huấn luyện quy mô lớn đến RL có thể làm trầm trọng thêm điều này. May mắn thay, các nghiên cứu lý thuyết gần đây đã nỗ lực về chuyển đổi biểu diễn (Agarwal et al., 2022) và chuyển đổi chính sách có điều kiện kỹ năng (Eysenbach et al., 2022). Tăng cường tập trung vào kết quả lý thuyết có khả năng có tác động sâu sắc đến việc phát triển các phương pháp tiền huấn luyện tiên tiến hơn.
