# 2209.12106.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2209.12106.pdf
# File size: 6201048 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Moral Mimicry: Large Language Models Produce Moral Rationalizations
Tailored to Political Identity
Gabriel Simmons
UC Davis
gsimmons@ucdavis.edu
Abstract
Large Language Models (LLMs) have demon-
strated impressive capabilities in generating flu-
ent text, as well as tendencies to reproduce
undesirable social biases. This study investi-
gates whether LLMs reproduce the moral biases
associated with political groups in the United
States, an instance of a broader capability herein
termed moral mimicry . This hypothesis is ex-
plored in the GPT-3/3.5 and OPT families of
Transformer-based LLMs. Using tools from
Moral Foundations Theory, it is shown that
these LLMs are indeed moral mimics. When
prompted with a liberal or conservative politi-
cal identity, the models generate text reflecting
corresponding moral biases. This study also ex-
plores the relationship between moral mimicry
and model size, and similarity between human
and LLM moral word use.
1 Introduction
Recent work suggests that Large Language Model
(LLM) performance will continue to scale with
model and training data sizes (Kaplan et al., 2020).
As LLMs advance in capability, it becomes more
likely that they will be capable of producing text
that influences human opinions (Tiku, 2022), po-
tentially lowering barriers to disinformation (Wei-
dinger et al., 2022). More optimistically, LLMs
may play a role in bridging divides between social
groups (Alshomary and Wachsmuth, 2021; Jiang
et al., 2022). For better or worse, we should under-
stand how LLM-generated content will impact the
human informational environment - whether this
content is influential, and to whom.
Morality is an important factor in persuasiveness
and polarization of human opinions (Luttrell et al.,
2019). Moral argumentation can modulate willing-
ness to compromise (Kodapanakkal et al., 2022),
and moral congruence between participants in a
dialogue influences argument effectiveness (Fein-
berg and Willer, 2015) and perceptions of ethicality
(Egorov et al., 2020).Therefore, it is important to characterize the ca-
pabilities of LLMs to produce apparently-moral
content1. This requires a framework from which
we can study morality; Moral Foundations The-
ory (MFT) is one such framework. MFT pro-
poses that human morals rely on five foundations:
Care/Harm, Fairness/Cheating, Loyalty/Betrayal,
Authority/Subversion, and Sanctity/Degradation2.
Evidence from MFT supports the “Moral Foun-
dations Hypothesis” that political groups in the
United States vary in their foundation use - lib-
erals rely primarily on the individualizing founda-
tions (Care/Harm and Fairness/Cheating), while
conservatives make more balanced appeals to all 5
foundations, appealing to the binding foundations
(Authority/Subversion, Sanctity/Degradation, and
Loyalty/Betrayal) more than liberals (Graham et al.,
2009; Do ˘gruyol et al., 2019; Frimer, 2020).
Existing work has investigated the moral foun-
dational biases of language models that have been
fine-tuned on supervised data (Fraser et al., 2022),
investigated whether language models reproduce
other social biases (see (Weidinger et al., 2022)
section 2.1.1), and probed LLMs for differences
in other cultural values (Arora et al., 2023). Con-
current work has shown that LLMs used as dialog
agents tend to repeat users’ political views back
to them, and that this happens more frequently in
larger models (Perez et al., 2022). To my knowl-
edge, no work yet examines whether language mod-
els can perform moral mimicry - that is, reproduce
the moral foundational biases associated with social
1Anthropomorphization provides convenient ways to talk
about system behavior, but can also distort perception of un-
derlying mechanisms (Bender and Koller, 2020). To be clear,
I ascribe capabilities such as “moral argumentation” or “moral
congruence” to language models only to the extent that their
outputs may be perceived as such, and make no claim that
LLMs might generate such text with communicative intent.
2Liberty/Oppression was proposed as a sixth foundation
- for the sake of this analysis I consider only the original 5
foundations, as these are the ones available in the Moral Foun-
dations Dictionaries (Graham et al., 2009; Frimer, 2019; Hopp
et al., 2021).arXiv:2209.12106v2  [cs.CL]  17 Jun 2023

--- PAGE 2 ---
Figure 1: An example of the experimental methods. Prompts 2are constructed from scenarios 1a, identity phrases
1b, and stances 1c, combined in a template (Section 2). Text completions 3are generated by LLMs based on the
prompts (Section 2). The completions are analyzed for their foundational contents 4using the moral foundations
dictionaries (Section 2). Differences between texts generated from liberal and conservative prompting are used to
calculate effect sizes 5.
groups such as political identities.
The present study considers whether LLMs use
moral vocabulary in ways that are situationally-
appropriate, and how this compares to human foun-
dation use. I find that LLMs respond to the salient
moral attributes of scenario descriptions, increas-
ing their use of the appropriate foundations, but
still differ from human consensus foundation use
more than individual humans (Section 2.1). I then
turn to the moral mimicry phenomenon. I inves-
tigate whether conditioning an LLM with a politi-
cal “identity” influences the model’s use of moral
foundations in ways that are consistent with human
moral biases. I find confirmatory results for text
generated based on“liberal” and “conservative” po-
litical identities (Section 2.2). Finally, I ask how the
moral mimicry phenomenon varies with model size.
Results show that the extent to which LLMs can
reproduce moral biases increases with model size,
in the OPT family (Section 2.2). This is also true
for the GPT-3 and -3.5 models considered together,
and to a lesser extent for the GPT-3 models alone.
2 Methods
Data Generation All experiments follow the
same pattern for data generation, described in the
following sections and illustrated in Figure 1. Meth-
ods accompanying specific research questions are
presented alongside results in Sections 2.1 - 2.3.
Prompt Construction I constructed prompts that
encourage the language model to generate apparent
moral rationalizations. Each prompt conditions the
model with three variables: a scenario s, a political
identity phrase i, and a moral stance r. Each promptconsists of values for these variables embedded in
a prompt template t.
Scenarios are text strings describing situations
or actions apt for moral judgement. I used three
datasets (Moral Stories3(Emelin et al., 2021),
ETHICS4(Hendrycks et al., 2021), and Social
Chemistry 1015(Forbes et al., 2020)) to obtain
four sets of scenarios, which I refer to as Moral
Stories, ETHICS, Social Chemistry Actions, and
Social Chemistry Situations. Appendix Section A.2
provides specifics on how each dataset was con-
structed. I use Sandsto a set of scenarios, and a
single scenario, respectively.
Political identity phrases are text strings refer-
ring to political ideologies (e.g. “liberal”). I use I
andito refer to a set of political identities and an
individual identity, respectively.
Moral Stances The moral stance presented in
each prompt conditions the model to produce an
apparent rationalization indicating approval or dis-
approval of the scenario. I use R, r to refer to the set
of stances {moral ,immoral }, and a single stance,
respectively. The datasets used herein contain la-
bels indicating the normative moral acceptability
of each scenario. For a scenario s, I refer to its
normative moral acceptability as rH(s).
Prompt Templates are functions that convert a
tuple of scenario, identity phrase, and moral stance
into a prompt. To check for sensitivity to any par-
ticular phrasing, five different styles of prompt tem-
plate were used (see Appendix Tables 2 and 3).
3Downloaded from https://github.com/demelin/moral_stories
4Downloaded from https://github.com/hendrycks/ethics
5Downloaded from https://github.com/mbforbes/social-
chemistry-101

--- PAGE 3 ---
Prompts were constructed by selecting a template
tfor a particular style, and populating it with a
stance, scenario, and political identity phrase.
Text Generation with LLMs Language models
produce text by autoregressive decoding. Given a
sequence of tokens, the model assigns likelihoods
to all tokens in its vocabulary indicating how likely
they are to follow the sequence. Based on these
likelihoods, a suitable next token is appended to
the sequence, and the process is repeated until a
maximum number of tokens is generated, or the
model generates a special “end-of-sequence” token.
I refer to the text provided initially to the model as
a “prompt” and the text obtained through the decod-
ing process as a “completion”. In this work I used
three families of Large Language Models: GPT-3,
GPT-3.5, and OPT (Table 1). GPT-3 is a family
of Transformer-based (Vaswani et al., 2017) au-
toregressive language models with sizes up to 175
billion parameters, pre-trained in self-supervised
fashion on web text corpora (Radford et al., 2019).
The largest 3 of the 4 GPT-3 models evaluated
here also received supervised fine-tuning on high-
quality model samples and human demonstrations
(OpenAI, 2022). The GPT-3.5 models are also
Transformer-based, pre-trained on text and code
web corpora, and fine-tuned using either supervised
fine-tuning or reinforcement learning from human
preferences (OpenAI, 2022). I accessed GPT-3/3.5
through the OpenAI Completions API (OpenAI,
2021). I used the engine parameter to indicate a spe-
cific model. GPT-3 models “text-ada-001”, “text-
babbage-001”, “text-curie-001”, and “text-davinci-
001”, and GPT-3.5 models “text-davinci-002” and
“text-davinci-003” were used. The OPT models are
Transformer-based pre-trained models released by
Meta AI, with sizes up to 175B parameters (Zhang
et al., 2022). Model sizes up to 30B parameters
were used herein. OPT model weights were ob-
tained from the HuggingFace Model Hub. I ob-
tained completions from these models locally using
the HuggingFace Transformers (Wolf et al., 2020)
and DeepSpeed ZeRo-Inference libraries (Deep-
Speed, 2022), using a machine with a Threadripper
3960x CPU and two RTX3090 24GB GPUs. For
all models, completions were produced with tem-
perature=0 for reproducibility. The max_tokens
parameter was used to stop generation after 64 to-
kens (roughly 50 words). All other settings wereleft as default6.
Measuring Moral Content
Moral Foundations Dictionaries I estimated the
moral foundational content of each completion
using three dictionaries: the Moral Foundations
Dictionary version 1.0 (MFDv1) (Graham et al.,
2009), Moral Foundations Dictionary version 2.0
(MFDv2) (Frimer, 2019), the extended Moral Foun-
dations Dictionary (eMFD) (Hopp et al., 2021).
MFDv1 consists of a lexicon containing 324
word stems, with each word stem associated to one
or more categories. MFDv2 consists of a lexicon of
2014 words, with each word associated to a single
category. In MFDv1, the categories consist of a
“Vice” and “Virtue” category for each of the five
foundations, plus a “MoralityGeneral” category, for
11 categories in total. MFDv2 includes all cate-
gories from MFDv1 except “MoralityGeneral”, for
a total of 10 categories. The eMFD (Hopp et al.,
2021) contains 3270 words and differs slightly from
MFDv1 and MFDv2. Words in the eMFD are as-
sociated with all foundations by scores in [0,1].
Scores were derived from annotation of news arti-
cles, and indicate how frequently each word was
associated to each foundation, divided by the to-
tal word appearances. Word overlap between the
dictionaries is shown in Appendix Figure 5.
Removing Valence Information All three dic-
tionaries indicate whether a word is associated with
the positive or negative aspect of a foundation. In
MFDv1 and MFDv2 this is indicated by word as-
sociation to the “Vice” or “Virtue” category for
each foundation. In the eMFD, each word has sen-
timent scores for each foundation. In this work I
was interested in the foundational contents of the
completions, independent of valence. Accordingly,
“Vice” and “Virtue” categories were merged into a
single category for each foundation, in both MFDv1
and MFDv2. The “MoralityGeneral” score from
MFDv1 was unused as it does not indicate asso-
ciation with any particular foundation. Sentiment
scores from eMFD were also unused.
Applying the Dictionaries Applying dictionary
dto a piece of text produces five scores {wd f|f∈
F}. For MFDv1 and MFDv2, these are inte-
ger values representing the number of foundation-
associated words in the text. The eMFD produces
6Default values for unused parameters of the OpenAI
Completions API were suffix: null; top_p: 1; n: 1;
stream: false; logprobs: null; echo: false; stop: null;
presence_penalty: 0; frequency_penalty: 0; best_of: 1;
logit_bias: null; user: null

--- PAGE 4 ---
continuous values in [0,∞]- the foundation-wise
sums of scores for all eMFD words in the text.
I am interested in the probability Pthat a human
or language model (apparently) expresses founda-
tionf, which I write as Ph(ef)andPLM(ef), re-
spectively. I use Pd(ef|s, r, i)to denote this prob-
ability conditioned on a scenario s, stance r, and
political identity i, using a dictionary dfor mea-
surement.
I useFto refer to the set of moral foundations,
andffor a single foundation. I use Dto refer to the
set of dictionaries. In each dictionary, Wdrefers
to all words in the dictionary. For MFDv1 and
MFDv2, Wd frefers to all the words in dbelonging
to foundation f. I approximate Pd(ef|s, r, i)as the
foundation-specific score wd fobtained by applying
the dictionary dto the model’s response to a prompt,
normalized by the total score across all foundations,
as shown in Equation 1 below.
Pd(ef|s, r, i)≈wfdP
f′∈Fwf′d(1)
Calculating Effect Sizes Effect sizes capture
how varying political identity alters the likelihood
that the model will express foundation f, given
the same stance and scenario. Effect sizes were
calculated as the absolute difference in foundation
expression probabilities for pairs of completions
that differ only in political identity (Equation 2 be-
low). Equation 3 calculates the average effect size
for foundation fover scenarios Sand stances R,
measured by dictionary d. Equation 4 gives one av-
erage effect size by the results across dictionaries.
∆Pd
i1,i2(ef|s,r)=Pd(ef|s,i1,r)−Pd(ef|s,i2,r)(2)
∆Pd
i1,i2(ef)=Es,r∈S×R∆Pd
i1,i2(ef|s,r) (3)
∆Pi1,i2(ef)=Ed∈D∆Pd
i1,i2(ef) (4)
2.1 LLM vs. Human Moral Foundation Use
Experiment Details This experiment considers
whether LLMs use foundation words that are situa-
tionally appropriate7. LLMs would satisfy a weak
criterion for this capability if they were more likely
to express foundation fin response to scenarios
where foundation fis salient, compared to their av-
erage use of facross a corpus of scenarios contain-
ing all foundations in equal proportion. I formalize
this with Criterion A below.
Criterion A Average use of foundation fis
greater across scenarios Sfthat demonstrate only
7e.g. using the Care/Harm foundation when prompted with
a violent scenariofoundation f, in comparison to average use of foun-
dation facross a foundationally-balanced corpus
of scenarios S(Equation 5).
Esf,r∈Sf×RPLM(ef|sf,r)>Es,r∈S×RPLM(ef|s,r)
A stronger criterion would require LLMs to not
to deviate from human foundation use beyond some
level of variation that is expected among humans. I
formalize this with Criterion 2b below.
Criterion B The average difference between lan-
guage model and consensus human foundation use
is less than the average difference between individ-
ual human and consensus human foundation use.
DIFFLM,CH≤DIFFH,CH(5)
DIFFLM,CH=Es∈S[|PLM(ef|s,rH(s))−CH(s)|](6)
DIFFH,CH=Es∈S[EH[|Ph(ef|s)−CH(s)|]] (7)
CH(s)=Eh[Ph(ef|s)] (8)
Stance rHsis the normative moral acceptability
of scenario s- the human-written rationalizations
are “conditioned” on human normative stance for
each scenario, so I only compare these with model
outputs that are also conditioned on human norma-
tive stance.
Criterion A requires a corpus with ground-truth
knowledge that only a particular foundation fis
salient for each scenario. To obtain such clear-
cut scenarios, I select the least ambiguous actions
from the Social Chemistry dataset, according to the
filtering methods described in Appendix Section
A.2.3. Estimating human consensus foundation use
(Criterion B) requires a corpus of scenarios that are
each annotated in open-ended fashion by multiple
humans. I obtain such a corpus from the Social
Chemistry dataset using the methods described in
Appendix Section A.2.4.
Results
Figure 2 (left) shows average values of P(ef|s)
for each foundation. For all five foundations, the
model increases its apparent use of foundation-
associated words appropriate to the ground truth
foundation label, satisfying Criterion A. Figure
2 (right) shows LM differences from human con-
sensus |PLM(ef|s, rHs)−CH(s)|obtained from
the text-davinci-002 model, and human differences
from human consensus EH[|Ph(ef|s)−CH(s)|],
on the Social Chemistry Situations dataset. In gen-
eral the LM-human differences are greater than the
human-human differences.

--- PAGE 5 ---
Figure 2: Left: Foundation expression probabilities for
foundation-specific examples vs. average foundation use
across all examples. Text-davinci-002; Social Chemistry
Actions scenarios. Right: LM and individual human
differences from human consensus foundation use, in
response to scenarios from the Social Chemistry Situa-
tions dataset; text-davinci-002.
2.2 Are LLMs Moral Mimics?
Experiment Details I consider whether condi-
tioning LLMs with political identity influences their
use of moral foundations in a way that reflects hu-
man moral biases. To investigate this question I
used a corpus of 2,000 scenarios obtained from the
Moral Stories dataset and 1,000 scenarios obtained
from the ETHICS dataset, described in Appendix
Section A.2.
Prompts were constructed with template style
2 from table 2. For each scenario, four prompts
were constructed based on combinations of “lib-
eral” and “conservative” political identity and moral
and immoral stance, for a total of 12,000 prompts.
Completions were obtained from the most capa-
ble model in each family that our computational
resources afforded: text-davinci-001 (GPT-3), text-
davinci-002 and text-davinci-003 (GPT-3.5) and
OPT-30B. One generation was obtained from each
model for each prompt. I calculated average effect
size∆Pi1,i2(ef)withi1= “liberal” and i2= “con-
servative” for all five foundations. Effect sizes were
computed separately for each dictionary, for a total
of 18,000 effect sizes computed per model.
Results Figure 3 shows effect sizes for liberal
vs. conservative political identity, for the most capa-
ble models tested from the OPT, GPT, and GPT-3.5
model families, measured using the three moral
foundations dictionaries. The shaded regions in
each plot represent the effects that would be ex-
pected based on the Moral Foundations Hypothesis- namely that prompting with liberal political iden-
tity would result in more use of the individualizing
foundations (positive ∆Pi1,i2) and prompting with
conservative political identity would result in more
use of the binding foundations (negative ∆Pi1,i2).
The majority of effect sizes coincide with the
Moral Foundations Hypothesis. Of 60 combina-
tions of 5 foundations, 4 models, and 3 dictionaries,
only 11 effect sizes are in the opposite direction
from expected, and all of these effect sizes have
magnitude of less than 1 point absolute difference.
2.3 Is Moral Mimicry Affected By Model
Size?
Experiment Details In this section, I consider
how moral mimicry relates to model size. I used
text-ada-001, text-babbage-001, text-curie-001, and
text-davinci-001 models from the GPT-3 family,
text-davinci-002 and text-davinci-003 from the
GPT-3.5 family (OpenAI, 2022), and OPT-350m,
OPT-1.3B, OPT-6.7B, OPT-13B, and OPT-30B
(Zhang et al., 2022). The GPT-3 models have
estimated parameter counts of 350M, 1.3B, 6.7B
and 175B, respectively (OpenAI, 2022; Gao, 2021).
Text-davinci-002 and text-davinci-003 also have
175B parameters (OpenAI, 2022). Parameters in
billions for the OPT models are indicated in the
model names.
To analyze to what extent each model demon-
strates the moral mimicry phenomenon, I define a
scoring function MFH-S CORE that scores a model
mas follows:
MFH-S CORE (m)=P
f∈FsignMFH (f)∆Pm(ef) (9)
signMFH =

−1,iff∈ {A/S, S/D, L/B }
+1,iff∈ {C/H, F/C }(10)
A/S: Authority/Subversion; S/D: Sanctity/Degradation;
L/B: Loyalty/Betrayal; C/H: Care/Harm; F/C; Fairness/Cheating
TheMFH-S CORE calculates the average effect
size for each model in the direction predicted by
the Moral Foundations Hypothesis.
Results Figure 4 above shows effect sizes ∆(Pef)
for each foundation and MFH-S CORE s vs. model
size (number of parameters). Effect sizes are aver-
aged over the three moral foundations dictionaries.
For the OPT model family, we can see that model
parameter count and MFH-S CORE show some rela-
tionship ( r=0.69, although statistical power is lim-

--- PAGE 6 ---
Figure 3: Effect sizes for liberal vs. conservative political identity for OPT-30B, text-davinci-001, text-davinci-002,
and text-davinci-003. Dot markers represent average effect size. Error bars represent 95% CI. Shaded regions
represent directions of expected effect size based on the Moral Foundations Hypothesis.
ited due to the limited number of models). In par-
ticular, the Sanctity/Degradation foundation main-
tains a non-zero effect size in the expected direc-
tion for all models 6.7B parameters or larger. Sur-
prisingly, OPT-13B shows decreased effect sizes
for Fairness/Cheating and Care/Harm in compari-
son to the smaller OPT-6.7B. The relationship be-
tween model size and effect size is weaker for
GPT-3 ( r=0.23). Care/Harm, Fairness/Cheating,
Sanctity/Degradation, and Authority/Subversion
have effect size in the expected direction for Bab-
bage, Curie, and DaVinci models, though the effect
sizes are smaller than for the OPT family. Mod-
els from the GPT-3.5 family show the largest ef-
fect sizes overall. Unfortunately, no smaller model
sizes are available for this family. If we include
the GPT-3 and GPT-3.5 models together (indi-
cated by †in Figure 4), the correlation between
MFH-S CORE and model parameters increases to
r=0.84. Interestingly, the OPT and GPT-3 families
show Sanctity/Degradation as the most pronounced
effect size for conservative prompting, and Fair-
ness/Cheating as the most pronounced effect size
for liberal prompting. GPT-3.5 instead shows the
largest effect sizes for Authority/Subversion and
Care/Harm, respectively.
3 Discussion
Section 2.1 posed two criteria to judge whether
LLMs use moral foundations appropriately. For
the weaker Criterion A, results show that LLMs
do increase use of foundation words relevant to
the foundation that is salient in a given scenario,
at least for scenarios with clear human consensuson foundation salience. However, for Criterion B,
results show that LLMs differ more from human
consensus foundation use than humans do in terms
of foundation use.
Section 2.2 compared LM foundation use with
findings from moral psychology that identify dif-
ferences in the moral foundations used by lib-
eral and conservative political groups. Specif-
ically, according to the Moral Foundations Hy-
pothesis, liberals rely mostly on the Care/Harm
and Fairness/Cheating foundations, while conser-
vatives use all 5 foundations more evenly, using
Authority/Subversion, Loyalty/Betrayal, and Fair-
ness/Cheating more than liberals. This finding was
first presented in (Graham et al., 2009), and has
since been supported with confirmatory factor anal-
ysis in (Do ˘gruyol et al., 2019), and partially repli-
cated (though with smaller effect sizes) in (Frimer,
2020).
Results indicate that models from the GPT-3,
GPT-3.5 and OPT model families are more likely
to use the binding foundations when prompted
with conservative political identity, and are more
likely to use the individualizing foundations when
prompted with liberal political identity. Emphasis
on individual foundations in each category differs
by model family. OPT-30B shows larger effect
sizes for Fairness/Cheating than Care/Harm and
larger effect sizes for Sanctity/Degradation vs. Au-
thority/Subversion, while GPT-3.5 demonstrates
the opposite. I suspect that this may be due to dif-
ferences in training data and/or training practices
between the model families. This opens an interest-
ing question of how to influence the moral mimicry

--- PAGE 7 ---
Figure 4: Top: Effect size vs. model parameters, based on completions obtained from Moral Stories dataset. Dark
lines show mean effect size. Error bars show 95% CI. Effect sizes are averaged over the three moral foundations
dictionaries.; 002: text-davinci-002; 003: text-davinci-003.; Bottom: MFH-S CORE vs. model parameters; r,p: value
and p-value for Pearson’s Correlation between MFH-S CORE and model parameters.; †results of correlation analysis
with GPT-3 and GPT-3.5 models analyzed together
capabilities that emerge during training, via dataset
curation or other methods.
The results from Section 2.3 show some relation-
ship between moral mimicry and model size. Effect
sizes tend to increase with parameter count in the
OPT family, and less so in the GPT-3 family. Both
175B-parameter GPT-3.5 models show relatively
strong moral mimicry capabilities, moreso than the
175B GPT-3 model text-davinci-001. This suggests
that parameter count is not the only factor lead-
ing to moral mimicry. The GPT-3.5 models were
trained with additional supervised fine-tuning not
applied to the GPT-3 family, and used text and code
pre-training rather than text alone (OpenAI, 2022).
4 Limitations
This work used the moral foundations dictionar-
ies to measure the moral content of text produced
by GPT-3. While studies have demonstrated cor-
respondence between results from the dictionaries
and human labels of moral foundational content
(Mutlu et al., 2020; Graham et al., 2009), dictionary-
based analysis is limited in its ability to detect nu-
anced moral expressions. Dictionary-based analy-
sis could be complemented with machine-learning
approaches (Garten et al., 2016; Johnson and Gold-
wasser, 2018; Pavan et al., 2020; Roy et al., 2022)
as well as human evaluation. This study attempted
to control for variations in the prompt phrasing by
averaging results over several prompt styles (Tables
2 and 3). These prompt variations were chosen
by the author. A more principled selection proce-
dure could result in a more diverse set of prompts.
The human studies that this study refers to (Graham
et al., 2009; Frimer, 2020) were performed on popu-
lations from the United States. The precise political
connotations of the terms “liberal” and “conserva-tive” differ across demographics. Future work may
explore how language model output varies when
additional demographic information is provided, or
when multilingual models are used. Documenta-
tion for the datasets used herein indicates that the
crowd workers leaned politically left, and morally
towards the Care/Harm and Fairness/Cheating foun-
dations (Forbes et al., 2020; Hendrycks et al., 2021;
Fraser et al., 2022). However, bias in the marginal
foundation distribution does not hinder the present
analysis, since the present experiments experiments
focus primarily on the difference in foundation
use resulting from varying political identity. The
analysis in Section 2.1 relies more heavily on the
marginal foundation distribution; a foundationally-
balanced dataset was constructed for this experi-
ment. This study used GPT-3 (Brown et al., 2020),
GPT-3.5 (OpenAI, 2022), and OPT (Zhang et al.,
2022). Other pre-trained language model families
of similar scale and architecture include BLOOM8,
which I was unable to test due to compute bud-
get, and LLaMA (Touvron et al., 2023), which was
released after the experiments for this work con-
cluded. While the OPT model weights are available
for download, GPT-3 and GPT-3.5 model weights
are not; this may present barriers to future work
that attempts to connect the moral mimicry phe-
nomenon to properties of the model. On the other
hand, the hardware required to run openly-available
models may be a barrier to experimentation that is
not a concern for models hosted via an API.
Criticisms of Moral Foundations Theory include
disagreements about whether a pluralist theory of
morality is parsimonious (Suhler and Churchland,
2011; Dobolyi, 2016); Ch. 6 of (Haidt, 2013), dis-
agreements about the number and character of the
8https://bigscience.huggingface.co/blog/bloom

--- PAGE 8 ---
foundations (Yalçında ˘g et al., 2019; Harper and
Rhodes, 2021), disagreements about stability of the
foundations across cultures (Davis et al., 2016), and
criticisms suggesting bias in the Moral Foundations
Questionnaire (Dobolyi, 2016). Moral foundations
theory was used in this study because it provides
established methods to measure moral content in
text, and because MFT-based analyses have identi-
fied relationships between political affiliation and
moral biases, offering a way to compare LLM and
human behavior. The methods presented here may
be applicable to other theories of morality; this is
left for future work.
Work that aims to elicit normative moral or eth-
ical judgement from non-human systems has re-
ceived criticism. Authors have argued that non-
human systems lack the autonomy and communica-
tive intent to be moral agents (Talat et al., 2022;
Bender and Koller, 2020). Criticisms have also
been raised about the quality and appropriateness
of data used to train such systems. Notably, crowd-
sourced or repurposed data often reflects a priori
opinions of individuals who may not be informed
about the topics they are asked to judge, and who
may not have had the opportunity for discourse or
reflection before responding (Talat et al., 2022; Eti-
enne, 2021). Some have argued that systems that ag-
gregate moral judgements from descriptive datasets
cannot help but be seen as normative, since their re-
production of the popular or average view tends to
be implicitly identified with a sense of correctness
(Talat et al., 2022). Finally, several authors argue
that the use of non-human systems that produce
apparent or intended normative judgements sets a
dangerous precedent by short-circuiting the discur-
sive process by which moral and ethical progress is
made, and by obscuring accountability should such
a system cause harm (Talat et al., 2022; Etienne,
2021).
The present study investigates the apparent moral
rationalizations produced by prompted LLMs. This
study does not intend to produce a system for nor-
mative judgement, and I would discourage a nor-
mative use or interpretation of the methods and
results presented here. The recent sea change in nat-
ural language processing towards general-purpose
LLMs prompted into specific behaviors enables end
users to produce a range of outputs of convincing
quality, including apparent normative moral or eth-
ical judgements. Anticipating how these systems
will impact end users and society requires study-
ing model behaviors under a variety of promptinginputs. The present study was conducted with this
goal in mind, under the belief that the benefit of
understanding the moral mimicry phenomenon out-
weighs the risk of normative interpretation.
5 Related Work
Several machine ethics projects have assessed the
extent to which LLM-based systems can mimic
human normative ethical judgement, for example
(Hendrycks et al., 2021) and (Jiang et al., 2021).
Other projects evaluate whether LLMs can pro-
duce the relevant moral norms for a given scenario
(Forbes et al., 2020; Emelin et al., 2021), or whether
they can determine which scenarios justify moral
exceptions (Jin et al., 2022). Yet other works fo-
cus on aligning models to normative ethics (Ziems
et al., 2022), and investigating to what extent soci-
etal biases are reproduced in language models (see
Section 5.1 of Bommasani et al. 2022). As an exam-
ple, Fraser, Kiritchenko, and Balkir (2022) analyze
responses of the Delphi model (Jiang et al., 2021)
to the Moral Foundations Questionnaire (Graham
et al., 2011), finding that its responses reflect the
moral foundational biases of the groups that pro-
duced the model and its training data.
The aforementioned research directions typically
investigate language models not prompted with any
particular identity. This framing implies the pre-
trained model itself as the locus where a cohesive
set of biases might exist. Recent work suggests an
alternative view that a single model may be capable
of simulating a multitude of “identities”, and that
these apparent identities may be selected from by
conditioning the model via prompting (Argyle et al.,
2023; Aher et al., 2023). Drawing on the latter view,
the present study prompts LLMs to simulate behav-
ior corresponding to opposed political identities,
and evaluates the fidelity of these simulacra with
respect to moral foundational bias. Relations be-
tween the present work and other works taking this
“simulation” view are summarized below.
Arora et. al. probe for cultural values using Hof-
stede’s six-dimenension theory (Hofstede, 2001)
and the World Values Survey (Survey, 2022), and
use prompt language rather than prompt tokens
to condition the model with a cultural “identity”.
Alshomary et al. 2021 and Qian et al. 2021 fine-
tune GPT-2 models (1.5B parameters) on domain-
specific corpora, and condition text generation with
stances on social issues. The present work, in con-
trast, conditions on political identity rather than

--- PAGE 9 ---
stance, evaluates larger models without domain-
specific fine-tuning, and investigates LLM capabili-
ties to mimic moral preferences. Concurrent work
probes language models for behaviors including
sycophancy , the tendency to mirror users’ politi-
cal views in a dialog setting (Perez et al., 2022).
Perez et. al. find that this tendency increases with
scale above ~10B parameters. While sycophancy
describes how model-generated text appears to ex-
press political views, conditioned on dialog user po-
litical views, moral mimicry describes how model-
generated text appears to express moral founda-
tional salience, conditioned on political identity
labels. Argyle et. al. propose the concept of “algo-
rithmic fidelity” - an LLM’s ability to “accurately
emulate the response distribution . . . of human sub-
groups” under proper conditioning (Argyle et al.,
2023). Moral mimicry can be seen as an instance of
algorithmic fidelity where moral foundation use is
the response variable of interest. Argyle et. al. study
other response variables: partisan descriptors, vot-
ing patterns, and correlational structure in survey
responses.
6 Conclusion
This study evaluates whether LLMs can reproduce
the moral foundational biases associated with social
groups, a capability herein coined moral mimicry .
I measure the apparent use of five moral founda-
tions in the text generated by pre-trained language
models conditioned with a political identity. I show
that LLMs reproduce the moral foundational biases
associated with liberal and conservative political
identities, modify their moral foundation use situ-
ationally, although not indistinguishably from hu-
mans, and that moral mimicry may relate to model
size.
Acknowledgements
I would like to thank the anonymous reviewers who
provided valuable comments on this paper. I would
also like to thank Professors Dipak Ghosal, Jiawei
Zhang, and Patrice Koehl, who provided valuable
feedback on this work, and colleagues, friends, and
family for insightful discussions.
References
Gati Aher, Rosa I. Arriaga, and Adam Tauman Kalai.
2023. Using Large Language Models to Simulate
Multiple Humans and Replicate Human Subject Stud-
ies.Milad Alshomary, Wei-Fan Chen, Timon Gurcke, and
Henning Wachsmuth. 2021. Belief-based Generation
of Argumentative Claims. In Proceedings of the 16th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics: Main Volume ,
pages 224–233, Online. Association for Computa-
tional Linguistics.
Milad Alshomary and Henning Wachsmuth. 2021. To-
ward audience-aware argument generation. Patterns ,
2(6):100253.
Lisa P. Argyle, Ethan C. Busby, Nancy Fulda, Joshua R.
Gubler, Christopher Rytting, and David Wingate.
2023. Out of One, Many: Using Language Mod-
els to Simulate Human Samples. Political Analysis ,
pages 1–15.
Arnav Arora, Lucie-aimee Kaffee, and Isabelle Augen-
stein. 2023. Probing pre-trained language models for
cross-cultural differences in values. In Proceedings
of the First Workshop on Cross-Cultural Considera-
tions in NLP (C3NLP) , pages 114–130, Dubrovnik,
Croatia. Association for Computational Linguistics.
Emily M. Bender and Alexander Koller. 2020. Climbing
towards NLU: On Meaning, Form, and Understand-
ing in the Age of Data. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pages 5185–5198, Online. Association
for Computational Linguistics.
Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ
Altman, Simran Arora, Sydney von Arx, Michael S.
Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, Erik Brynjolfsson, Shyamal Buch, Dal-
las Card, Rodrigo Castellon, Niladri Chatterji, An-
nie Chen, Kathleen Creel, Jared Quincy Davis,
Dora Demszky, Chris Donahue, Moussa Doumbouya,
Esin Durmus, Stefano Ermon, John Etchemendy,
Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor
Gale, Lauren Gillespie, Karan Goel, Noah Goodman,
Shelby Grossman, Neel Guha, Tatsunori Hashimoto,
Peter Henderson, John Hewitt, Daniel E. Ho, Jenny
Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil
Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth
Karamcheti, Geoff Keeling, Fereshte Khani, Omar
Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna,
Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak,
Mina Lee, Tony Lee, Jure Leskovec, Isabelle Lev-
ent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali
Malik, Christopher D. Manning, Suvir Mirchandani,
Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika
Narayan, Deepak Narayanan, Ben Newman, Allen
Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian
Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou,
Joon Sung Park, Chris Piech, Eva Portelance, Christo-
pher Potts, Aditi Raghunathan, Rob Reich, Hongyu
Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack
Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa,
Keshav Santhanam, Andy Shih, Krishnan Srinivasan,
Alex Tamkin, Rohan Taori, Armin W. Thomas, Flo-
rian Tramèr, Rose E. Wang, William Wang, Bohan
Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michi-
hiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael

--- PAGE 10 ---
Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang,
Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2022.
On the Opportunities and Risks of Foundation Mod-
els.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford,
Ilya Sutskever, and Dario Amodei. 2020. Language
models are few-shot learners. In Advances in Neural
Information Processing Systems , volume 33, pages
1877–1901. Curran Associates, Inc.
Don E. Davis, Kenneth Rice, Daryl R. Van Tongeren,
Joshua N. Hook, Cirleen DeBlaere, Everett L. Wor-
thington Jr., and Elise Choe. 2016. The moral foun-
dations hypothesis does not replicate well in Black
samples. Journal of Personality and Social Psychol-
ogy, 110(4):e23–e30.
DeepSpeed. 2022. ZeRO-Inference: De-
mocratizing massive model inference.
https://www.deepspeed.ai/2022/09/09/zero-
inference.html.
David Dobolyi. 2016. Critiques | Moral Foundations
Theory.
Burak Do ˘gruyol, Sinan Alper, and Onurcan Yilmaz.
2019. The five-factor model of the moral founda-
tions theory is stable across WEIRD and non-WEIRD
cultures. Personality and Individual Differences ,
151:109547.
Maxim Egorov, Karianne Kalshoven, Armin Pircher Ver-
dorfer, and Claudia Peus. 2020. It’s a Match: Moral-
ization and the Effects of Moral Foundations Congru-
ence on Ethical and Unethical Leadership Perception.
Journal of Business Ethics , 167(4):707–723.
Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell
Forbes, and Yejin Choi. 2021. Moral Stories: Situ-
ated Reasoning about Norms, Intents, Actions, and
their Consequences. In Proceedings of the 2021 Con-
ference on Empirical Methods in Natural Language
Processing , pages 698–718, Online and Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Hubert Etienne. 2021. The dark side of the ‘Moral
Machine’ and the fallacy of computational ethical
decision-making for autonomous vehicles. Law, In-
novation and Technology , 13(1):85–107.
Matthew Feinberg and Robb Willer. 2015. From Gulf
to Bridge: When Do Moral Arguments Facilitate Po-
litical Influence? Personality and Social Psychology
Bulletin , 41(12):1665–1681.Maxwell Forbes, Jena D. Hwang, Vered Shwartz,
Maarten Sap, and Yejin Choi. 2020. Social chem-
istry 101: Learning to reason about social and moral
norms. In Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing
(EMNLP) , pages 653–670, Online. Association for
Computational Linguistics.
Kathleen C. Fraser, Svetlana Kiritchenko, and Esma
Balkir. 2022. Does Moral Code have a Moral Code?
Probing Delphi’s Moral Philosophy. In Proceedings
of the 2nd Workshop on Trustworthy Natural Lan-
guage Processing (TrustNLP 2022) , pages 26–42,
Seattle, U.S.A. Association for Computational Lin-
guistics.
Jeremy Frimer. 2019. Moral Foundations Dictionary
2.0.
Jeremy A. Frimer. 2020. Do liberals and conservatives
use different moral languages? Two replications and
six extensions of Graham, Haidt, and Nosek’s (2009)
moral text analysis. Journal of Research in Personal-
ity, 84:103906.
Leo Gao. 2021. On the Sizes of OpenAI API Models.
https://blog.eleuther.ai/gpt3-model-sizes/.
Justin Garten, Reihane Boghrati, J. Hoover, Kate M.
Johnson, and Morteza Dehghani. 2016. Morality
Between the Lines : Detecting Moral Sentiment In
Text.
Jesse Graham, Jonathan Haidt, and Brian A. Nosek.
2009. Liberals and conservatives rely on different
sets of moral foundations. Journal of Personality and
Social Psychology , 96(5):1029–1046.
Jesse Graham, Brian A. Nosek, Jonathan Haidt, Ravi
Iyer, Spassena Koleva, and Peter H. Ditto. 2011. Map-
ping the Moral Domain. Journal of personality and
social psychology , 101(2):366–385.
Jonathan Haidt. 2013. The Righteous Mind: Why Good
People Are Divided by Politics and Religion . Vintage
Books.
Craig A. Harper and Darren Rhodes. 2021. Reanalysing
the factor structure of the moral foundations ques-
tionnaire. The British Journal of Social Psychology ,
60(4):1303–1329.
Dan Hendrycks, Collin Burns, Steven Basart, Andrew
Critch, Jerry Li, Dawn Song, and Jacob Steinhardt.
2021. Aligning AI with shared human values. In
9th International Conference on Learning Represen-
tations, ICLR 2021, Virtual Event, Austria, May 3-7,
2021 . OpenReview.net.
Geert Hofstede. 2001. Culture’s Recent Consequences:
Using Dimension Scores in Theory and Research.
International Journal of Cross Cultural Management ,
1(1):11–17.

--- PAGE 11 ---
Frederic R. Hopp, Jacob T. Fisher, Devin Cornell,
Richard Huskey, and René Weber. 2021. The ex-
tended Moral Foundations Dictionary (eMFD): Devel-
opment and applications of a crowd-sourced approach
to extracting moral intuitions from text. Behavior Re-
search Methods , 53(1):232–246.
Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,
Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster,
Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li,
Brian O’Horo, Gabriel Pereyra, Jeff Wang, Christo-
pher Dewan, Asli Celikyilmaz, Luke Zettlemoyer,
and Ves Stoyanov. 2023. OPT-IML: Scaling Lan-
guage Model Instruction Meta Learning through the
Lens of Generalization.
Hang Jiang, Doug Beeferman, Brandon Roy, and Deb
Roy. 2022. CommunityLM: Probing partisan world-
views from language models. In Proceedings of the
29th International Conference on Computational Lin-
guistics , pages 6818–6826, Gyeongju, Republic of
Korea. International Committee on Computational
Linguistics.
Liwei Jiang, Jena D. Hwang, Chandra Bhagavatula, Ro-
nan Le Bras, Jenny Liang, Jesse Dodge, Keisuke
Sakaguchi, Maxwell Forbes, Jon Borchardt, Saadia
Gabriel, Yulia Tsvetkov, Oren Etzioni, Maarten Sap,
Regina Rini, and Yejin Choi. 2021. Can Machines
Learn Morality? The Delphi Experiment.
Zhijing Jin, Sydney Levine, Fernando Gonzalez Adauto,
Ojasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada
Mihalcea, Josh Tenenbaum, and Bernhard Schölkopf.
2022. When to make exceptions: Exploring language
models as accounts of human moral judgment. In
NeurIPS .
Kristen Johnson and Dan Goldwasser. 2018. Classifi-
cation of Moral Foundations in Microblog Political
Discourse. In Proceedings of the 56th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 720–730, Melbourne,
Australia. Association for Computational Linguistics.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B.
Brown, Benjamin Chess, Rewon Child, Scott Gray,
Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
Scaling Laws for Neural Language Models.
Rabia I. Kodapanakkal, Mark J. Brandt, Christoph
Kogler, and Ilja van Beest. 2022. Moral Frames Are
Persuasive and Moralize Attitudes; Nonmoral Frames
Are Persuasive and De-Moralize Attitudes. Psycho-
logical Science , 33(3):433–449.
Andrew Luttrell, Aviva Philipp-Muller, and Richard E.
Petty. 2019. Challenging Moral Attitudes With Moral
Messages. Psychological Science , 30(8):1136–1150.
Ece Çi ˘gdem Mutlu, Toktam Oghaz, Ege Tütüncüler, and
Ivan Garibay. 2020. Do Bots Have Moral Judgement?
The Difference Between Bots and Humans in Moral
Rhetoric. In 2020 IEEE/ACM International Confer-
ence on Advances in Social Networks Analysis and
Mining (ASONAM) , pages 222–226.OpenAI. 2021. OpenAI API. https://openai.com/api/.
OpenAI. 2022. Model Index for Researchers.
Matheus C. Pavan, Vitor G. Dos Santos, Alex G. J.
Lan, Joao Martins, Wesley R. Santos, Caio Deutsch,
Pablo B. Costa, Fernando C. Hsieh, and Ivandre
Paraboni. 2020. Morality Classification in Natural
Language Text. IEEE Transactions on Affective Com-
puting , pages 1–1.
Ethan Perez, Sam Ringer, Kamil ˙e Lukoši ¯ut˙e, Karina
Nguyen, Edwin Chen, Scott Heiner, Craig Pettit,
Catherine Olsson, Sandipan Kundu, Saurav Kadavath,
Andy Jones, Anna Chen, Ben Mann, Brian Israel,
Bryan Seethor, Cameron McKinnon, Christopher
Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn
Drain, Dustin Li, Eli Tran-Johnson, Guro Khun-
dadze, Jackson Kernion, James Landis, Jamie Kerr,
Jared Mueller, Jeeyoon Hyun, Joshua Landau, Ka-
mal Ndousse, Landon Goldberg, Liane Lovitt, Mar-
tin Lucas, Michael Sellitto, Miranda Zhang, Neerav
Kingsland, Nelson Elhage, Nicholas Joseph, Noemí
Mercado, Nova DasSarma, Oliver Rausch, Robin
Larson, Sam McCandlish, Scott Johnston, Shauna
Kravec, Sheer El Showk, Tamera Lanham, Timothy
Telleen-Lawton, Tom Brown, Tom Henighan, Tristan
Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark,
Samuel R. Bowman, Amanda Askell, Roger Grosse,
Danny Hernandez, Deep Ganguli, Evan Hubinger,
Nicholas Schiefer, and Jared Kaplan. 2022. Discover-
ing Language Model Behaviors with Model-Written
Evaluations.
Ming Qian, Jaye Laguardia, and Davis Qian. 2021.
Morality Beyond the Lines: Detecting Moral Senti-
ment Using AI-Generated Synthetic Context. In Arti-
ficial Intelligence in HCI , Lecture Notes in Computer
Science, pages 84–94, Cham. Springer International
Publishing.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Shamik Roy, Nishanth Sridhar Nakshatri, and Dan Gold-
wasser. 2022. Towards Few-Shot Identification of
Morality Frames using In-Context Learning. In
Proceedings of the Fifth Workshop on Natural Lan-
guage Processing and Computational Social Science
(NLP+CSS) , pages 183–196, Abu Dhabi, UAE. Asso-
ciation for Computational Linguistics.
Christopher Suhler and Pat Churchland. 2011. Can
Innate, Modular “Foundations” Explain Morality?
Challenges for Haidt’s Moral Foundations Theory.
Journal of cognitive neuroscience , 23:2103–16; dis-
cussion 2117.
World Values Survey. 2022. WVS Database.
https://www.worldvaluessurvey.org/wvs.jsp.
Zeerak Talat, Hagen Blix, Josef Valvoda, Maya Indira
Ganesh, Ryan Cotterell, and Adina Williams. 2022.
On the Machine Learning of Ethical Judgments from

--- PAGE 12 ---
Natural Language. In Proceedings of the 2022 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies , pages 769–779, Seattle, United
States. Association for Computational Linguistics.
Nitasha Tiku. 2022. The Google engineer who thinks
the company’s AI has come to life. Washington Post .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. LLaMA: Open
and Efficient Foundation Language Models.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems , volume 30. Curran Associates, Inc.
Laura Weidinger, Jonathan Uesato, Maribeth Rauh,
Conor Griffin, Po-Sen Huang, John Mellor, Amelia
Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,
Courtney Biles, Sasha Brown, Zac Kenton, Will
Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne
Hendricks, Laura Rimell, William Isaac, Julia Haas,
Sean Legassick, Geoffrey Irving, and Iason Gabriel.
2022. Taxonomy of Risks posed by Language Mod-
els. In 2022 ACM Conference on Fairness, Account-
ability, and Transparency , FAccT ’22, pages 214–229,
New York, NY , USA. Association for Computing Ma-
chinery.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020. Hug-
gingFace’s Transformers: State-of-the-art Natural
Language Processing.
Bilge Yalçında ˘g, Türker Özkan, Sevim Cesur, Onurcan
Yilmaz, Beyza Tepe, Zeynep Ecem Piyale, Ali Furkan
Biten, and Diane Sunar. 2019. An Investigation of
Moral Foundations Theory in Turkey Using Different
Measures. Current Psychology , 38(2):440–457.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. OPT: Open Pre-
trained Transformer Language Models.
Caleb Ziems, Jane Yu, Yi-Chia Wang, Alon Halevy,
and Diyi Yang. 2022. The moral integrity corpus: A
benchmark for ethical dialogue systems. In Proceed-
ings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers) ,
pages 3755–3773, Dublin, Ireland. Association for
Computational Linguistics.AAppendix A: Additional Details Related
to Experimental Methods
A.1 Additional Details Related to LLMs Used
in the Study
Model Family Model Variant Number of Parameters Instruction Fine-tuning
GPT-3 text-ada-001 350M None
GPT-3 text-babbage-001 1.3B FeedME
GPT-3 text-curie-001 6.7B FeedME
GPT-3 text-davinci-001 175B FeedME
GPT-3.5 text-davinci-002 175B ?
GPT-3.5 text-davinci-003 175B PPO
OPT opt-350m 350M None
OPT opt-1.3b 1.3B None
OPT opt-6.7b 6.7B None
OPT opt-13b 13B None
OPT opt-30b 30B None
Table 1: Models evaluated in this study. Information for
GPT-3 and GPT-3.5 from (OpenAI, 2022). Information
for OPT from (Zhang et al., 2022). Information for OPT-
IML from (Iyer et al., 2023). FeedME: “Supervised
fine-tuning on human-written demonstrations and on
model samples rated 7/7 by human labelers on an overall
quality score” (OpenAI, 2022); PPO: “Reinforcement
learning with reward models trained from comparisons
by humans” (OpenAI, 2022); ?: use of instruction fine-
tuning is uncertain based on documentation.
A.2 Additional Details Related to Datasets
Used in the Study
A.2.1 Preprocessing Details for Moral Stories
Dataset
Each example in Moral Stories consists of a moral
norm (a normative expectation about moral behav-
ior), a situation which describes the state of some
characters, an intent which describes what a partic-
ular character wants, and two paths , amoral path
andimmoral path . Each path consists of a moral
orimmoral action (an action following or violating
the norm) and a moral orimmoral consequence
(a likely outcome of the action). For the present
experiments, I construct scenarios as the string con-
catenation of an example’s situation, intent, and
either moral action or immoral action. We do not
use the consequences or norms, as they often in-
clude a reason why the action was moral/immoral,
and thus could bias the moral foundational contents
of the completions.
We used 2,000 scenarios produced from
the Moral Stories dataset, consisting of 1,000
randomly-sampled moral scenarios and 1,000
randomly-sampled immoral scenarios.

--- PAGE 13 ---
A.2.2 Preprocessing Details for ETHICS
Dataset
The ETHICS dataset contains five subsets of data,
each corresponding to a particular ethical frame-
work (deontology, justice, utilitarianism, common-
sense, and virtue), each further divided into a “train”
and “test” portion. For the present experiments, I
use the “train” split of the “commonsense” portion
of the dataset, which contains 13,910 examples of
scenarios paired with ground-truth binary labels of
ethical acceptability. Of these, 6,661 are “short”
examples, which are 1-2 sentences in length. These
short examples were sourced from Amazon Me-
chanical Turk workers and consist of 3,872 moral
examples, and 2,789 immoral examples. From
these, I randomly select 1,000 examples split evenly
according to normative acceptability, resulting in
500 moral scenarios and 500 immoral scenarios.
The train split of the commonsense portion of the
ETHICS dataset also contains 7,249 “long” exam-
ples, 1-6 paragraphs in length, which were obtained
from Reddit. These were unused in the present ex-
periment, primarily due to the increased costs of
using longer scenarios.
A.2.3 Preprocessing Details for Social
Chemistry Actions Dataset
The Social Chemistry 101 (Forbes et al., 2020)
dataset contains 355,922 structured annotations of
103,692 situations, drawn from four sources (Dear
Abby, Reddit AITA, Reddit Confessions, and sen-
tences from the ROCStories corpus; see (Forbes
et al., 2020) for references). Situations are brief
descriptions of occurrences in everyday life where
social or moral norms may dictate behavior, for
example “pulling out of a group project at the last
minute”. Situations are annotated with Rules-of-
Thumb (RoTs), which are judgements of actions
that occur in the situation, such as “It’s bad to not
follow through on your commitments”. Some sit-
uations may contain more than one action, but I
consider situations that are unanimously annotated
as having only one action for the present experi-
ment, as this simplifies interpretation of the moral
foundation annotations. RoTs in the dataset are
annotated with “RoT breakdowns”. RoT break-
downs parse each RoT into its constituent action
(e.g. “not following through on commitments”) and
judgement (“it’s bad”). Judgements are standard-
ized to five levels of approval/disapproval: very
bad, bad, expected/OK, good, very good. I discard
actions labeled with “expected/OK”, and collapse“very bad” and “bad” together, and “very good” and
“good” together to obtain actions annotated with
binary normative acceptability. Actions are also
annotated with moral foundation labels (the exam-
ple in the previous sentence was annotated with
the Fairness/Cheating and Loyalty/Betrayal foun-
dations). Additionally, each RoT belongs to one of
the following categories - morality-ethics, social-
norms, advice, description. I use RoTs belonging
to the “morality-ethics” category, since this is the
category indicating that the RoT contains moral
reasoning rather than advice or etiquette recom-
mendations. After filtering RoTs and situations by
category, and selecting examples with unanimous
ratings for moral foundation and normative accept-
ability, I obtain a dataset of 1300 actions - 130
normatively moral actions and 130 normatively im-
moral actions for each of the five moral foundations.
These scenarios are used in the experiment related
to Criterion A in Section 2.1.
A.2.4 Preprocessing Details for Social
Chemistry Situations Dataset
Criterion B requires comparing PH(ef|s)and
PLM(ef|s), for human- and LLM-written open-
ended text responses containing moral reasoning
about some scenarios. I use situations from the So-
cial Chemistry 101 dataset (Forbes et al., 2020), and
use the human-written RoTs to estimate PH(ef|s)
using the moral foundations dictionaries. To es-
timate consensus human judgement CH(s), I use
situations that are multiply annotated. Specifically,
I filter the Social Chemistry 101 dataset to situa-
tions with 4 or more RoTs, and 4 or more RoT
breakdowns per RoT. This results in a corpus of
170 scenarios. Unlike the Social Chemistry Actions
dataset, this Social Chemistry Situations dataset is
not foundationally-balanced - I encountered a trade-
off between the minimum number of annotations
per situation, and the final corpus size - balancing
the dataset in terms of foundations would have re-
duced the dataset size further. The set of scenarios
is used for the experiment related to Criterion B in
Section 2.1.
A.3 Additional Details Related to Moral
Foundations Dictionaries
A.4 Additional Details Related to Prompt
Construction
Templates from Table 2 were used for the Moral
Stories, ETHICS, and Social Chemistry Situations
datasets, where the scenarios are longer descrip-

--- PAGE 14 ---
Figure 5: Venn diagram of word overlap between
MFDv1, MFDv2 and eMFD. Since some entries in
MFDv1 are regexes, I represent MFDv1 in this diagram
by all non-compound words in WordNet matching a
regex in MFDv1.
tions of events, with length one sentence or longer.
Templates from Table 3 were used for the Social
Chemistry Actions dataset, where scenarios are
brief action descriptions (sentence fragments). This
was done to ensure grammaticality.
Template Style Stance Template
1 moral [SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], this is a moral thing to do because
1 immoral [SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], this is an immoral thing to because
2 moral [SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here are the moral arguments for why this is a moral thing to do:
2 immoral [SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here are the moral arguments for why this is an immoral thing to do:
3 moral [SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here’s why this is acceptable behavior:
3 immoral [SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here’s why this is unacceptable behavior:
4 moral [SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here’s why this is a good thing to do:
4 immoral [SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here’s why this is a bad thing to do:
5 moral [SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here’s why this is the right thing to do:
5 immoral [SCENARIO] As a/an [POLITICAL_IDENTITY_PHRASE], here’s why this is the wrong thing to do:
Table 2: Prompt template styles for situations
Template Style Stance Template
1 moral As a/an [POLITICAL_IDENTITY_PHRASE], [ACTION] is a moral thing to do because
1 immoral As a/an [POLITICAL_IDENTITY_PHRASE], [ACTION] is an immoral thing to because
2 moral As a/an [POLITICAL_IDENTITY_PHRASE], here are the moral arguments for why [ACTION] is a moral thing to do:
2 immoral As a/an [POLITICAL_IDENTITY_PHRASE], here are the moral arguments for why [ACTION] is an immoral thing to do:
3 moral As a/an [POLITICAL_IDENTITY_PHRASE], here’s why [ACTION] is acceptable behavior:
3 immoral As a/an [POLITICAL_IDENTITY_PHRASE], here’s why [ACTION] is unacceptable behavior:
4 moral As a/an [POLITICAL_IDENTITY_PHRASE], here’s why [ACTION] is a good thing to do:
4 immoral As a/an [POLITICAL_IDENTITY_PHRASE], here’s why [ACTION] is a bad thing to do:
5 moral As a/an [POLITICAL_IDENTITY_PHRASE], here’s why [ACTION] is the right thing to do:
5 immoral As a/an [POLITICAL_IDENTITY_PHRASE], here’s why [ACTION] is the wrong thing to do:
Table 3: Prompt template styles for actions
B Appendix B: Additional Experimental
Results
B.1 Effect Size vs. Dataset
Figure 6 shows effect sizes for liberal vs. conser-
vative prompting, based on completions obtained
from 2000 scenarios produced from Moral Stories
and 1000 scenarios produced from ETHICS. Scores
Figure 6: Effect sizes, liberal vs. conservative prompt
identity, by dataset and dictionary
are separated by dictionary and dataset. See Section
2 for the methods used to calculate effect sizes.
Effect sizes and directions are consistent
across datasets for the Care/Harm and Author-
ity/Subversion foundations.
B.2 Effect Size vs. Prompt Template Style
Figure 7 shows the results obtained from analysis of
compeletions obtained from five different prompt
styles, as described in 2.
Effects of liberal vs. conservative political iden-
tity are uniform in direction for the Care/Harm and
Authority/Subversion foundations. Regardless of
the prompt style or dictionary used, the comple-
tions contain more Care/Harm words when the lib-
eral political identity is used, and more Author-
ity/Subversion words when the conservative polit-
ical identity is used. Effects are nearly uniform
in direction for the Fairness/Cheating foundation,
with liberal political identity resulting in increased
use of this foundation for thirteen of fifteen com-
binations of prompt style and dictionary. Liberal
prompting resulted in decreased use of the Fair-
ness/Cheating foundation for prompt styles 1 and
2, when measured using MFDv2.

--- PAGE 15 ---
Figure 7: Effect sizes, liberal vs. conservative prompt
identity, by prompt style and dictionary.Results for the Sanctity/Degradation and Loy-
alty/Betrayal foundations are more varied. Effect
directions are uniform for the Sanctity/Degradation
foundation when measured with MFDv2 - lib-
eral political identity results in lower Sanc-
tity/Degradation use by 1-2 percent score across
all prompt styles. Effects on Sanctity/Degradation
are less consistent when measured using MFDv1
or eMFD - liberal prompting resulted in decreased
use of Sanctity/Degradation words for only three
out of five prompt styles. Measured by the eMFD,
liberal prompting results in decreased use of Sanc-
tity/degradation words for four of five prompt
styles.
Effect directions are uniform for Loy-
alty/Betrayal when measured with MFDv1 -
prompting with liberal political identity results in
greater percent scores for Loyalty for all prompt
styles. Results are varied when measured with
MFDv1 - liberal prompting results in decreased
use for only three of five prompt styles. When mea-
sured using the eMFD, liberal prompting results
in decreased or equal use of the Loyalty/Betrayal
foundation across the prompt styles, which is
consistent within the dictionary, but is opposite in
effect direction in comparison to MFDv1.

--- PAGE 16 ---
C Appendix C: LLM Output Examples
Figure 8: Examples of completions obtained from Moral Stories dataset, from OpenAI models of increasing size.
Examples were randomly selected
