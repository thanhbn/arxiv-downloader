# 2505.16410.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2505.16410.pdf
# File size: 2085774 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Tool-Star: Empowering LLM-Brained Multi-Tool
Reasoner via Reinforcement Learning
Guanting Dong1, Yifei Chen1, Xiaoxi Li1, Jiajie Jin1, Hongjin Qian2, Yutao Zhu1
Hangyu Mao3, Guorui Zhou3, Zhicheng Dou1∗, Ji-Rong Wen1
1Renmin University of China2BAAI3Kuaishou Technology
{dongguanting, dou}@ruc.edu.cn
Abstract
Recently, large language models (LLMs) have shown remarkable reasoning ca-
pabilities via large-scale reinforcement learning (RL). However, leveraging the
RL algorithm to empower effective multi-tool collaborative reasoning in LLMs
remains an open challenge. In this paper, we introduce Tool-Star, an RL-based
framework designed to empower LLMs to autonomously invoke multiple external
tools during stepwise reasoning. Tool-Star integrates six types of tools and incorpo-
rates systematic designs in both data synthesis and training. To address the scarcity
of tool-use data, we propose a general tool-integrated reasoning data synthesis
pipeline, which combines tool-integrated prompting with hint-based sampling to
automatically and scalably generate tool-use trajectories. A subsequent quality
normalization and difficulty-aware classification process filters out low-quality
samples and organizes the dataset from easy to hard. Furthermore, we propose a
two-stage training framework to enhance multi-tool collaborative reasoning by: (1)
cold-start fine-tuning, which guides LLMs to explore reasoning patterns via tool-
invocation feedback; and (2) a multi-tool self-critic RL algorithm with hierarchical
reward design, which reinforces reward understanding and promotes effective tool
collaboration. Experimental analyses on over 10 challenging reasoning benchmarks
highlight the effectiveness and efficiency of Tool-Star. The code is available at
https://github.com/dongguanting/Tool-Star .
ToRL(3B)Recall(3B)Tool-Star(3B)Search-o1(3B)WebWalkerHotpotQA2WikiMuSiQueBamboogleAIME24MATH500AIME25GSM8KMATH20.851.940.019.352.520.016.772.085.082.618.145.136.416.442.417.213.567.073.168.8
Figure 1: Overview result comparison of Tool-Star and other TIR baselines. Left: Average tool-
use accuracy (TE) on knowledge-intensive and computational reasoning datasets. Right : The
performance overview on 10 challenging reasoning datasets.
∗Corresponding author.
Preprint. Under review.arXiv:2505.16410v1  [cs.CL]  22 May 2025

--- PAGE 2 ---
1 Introduction
Recent advances in large language models (LLMs) have demonstrated remarkable reasoning ca-
pabilities empowered by large-scale reinforcement learning (RL) [ 59,21,57,36,79]. Advanced
models exemplified by Deepseek R1 [ 9] and OpenAI o1 [ 41] exhibit diverse emergent behaviors
in Chain-of-Thought (CoT) reasoning, including deep thinking, self-reflection, thereby improving
problem-solving performance on complex reasoning tasks [ 48,40]. However, real-world reasoning
scenarios often require models to integrate multiple capabilities through interaction with the environ-
ment, such as deep information seeking, long-term knowledge memory, and precise computation. To
achieve this, Tool-Integrated Reasoning (TIR) [ 17,28,67,72,42] approaches encourage LLMs to
interact with external tools during the reasoning process, substantially expanding the paradigm from
language-only reasoning to a broader scope of applications.
Current foundational work on TIR primarily focuses on distilling tool-use trajectories from strong
models and applying supervised fine-tuning (SFT) to guide weaker models via imitation learning [ 17,
27,76,22]. As large reasoning models exhibit emergent capabilities, subsequent efforts aim to
enhance long chain-of-thought reasoning in LLMs through tool-integrated prompting design [ 2,30,
31,69,62]. However, these approaches rarely enable LLMs to autonomously discover effective
reasoning patterns of tool usage. To address this limitation, recent studies introduce outcome-based
rewards [ 4,51], extending reinforcement learning to the TIR domain [ 53,23,1,16,35,64,46]. While
RL-based methods encourage exploration of efficient tool-use behaviors, they primarily focus on
interacting with the individual tool. Notably, real-world reasoning tasks often require LLMs to possess
both accurate computation and dynamic information-seeking capabilities.2This necessitates deeper
integration of feedback from multiple tools, particularly search engines and code generators. The
lack of systematic investigation into multi-tool collaborative reasoning presents a major bottleneck
for practical TIR deployment. In this work, we aim to bridge this gap by addressing the following
research questions:
1.Rationality and efficiency of tool usage. How to enable the model to perform efficient tool-
augmented reasoning while balancing the cost of tool invocation.
2.Multi-tool collaborative reasoning. How to effectively enable models to integrate the functionali-
ties of multiple tools throughout the reasoning process.
In this paper, we propose Tool-Star , a reinforcement learning-based framework designed to empower
LLMs to autonomously invoke multiple external tools during stepwise reasoning. Specifically,
Tool-Star integrates six types of tools into the reasoning process (three for training and three for
inference-time optimization) and incorporates systematic designs in both data synthesis and training
algorithms.
To address the scarcity of tool-use data, we first design a general Tool-Integrated Reasoning
Data Synthesis Pipeline that combines tool-integrated prompting with hint-based sampling to
automatically generate large-scale tool-use trajectories. We then introduce a quality normalization
and difficulty-aware classification process to effectively filter out unreasonable tool-use samples and
partition the data in a curriculum-like manner from easy to hard [ 54,11,12]. Leveraging this pipeline,
we construct high-quality datasets for both cold-start fine-tuning and reinforcement learning in a
staged manner, laying a solid foundation for subsequent TIR training.
To incentivize the model’s capability for multi-tool collaboration, we propose a two-stage TIR training
framework that progressively aligns reasoning abilities in an easy-to-hard manner. (1) In the first
stage, we introduce a Cold-Start Supervised Fine-Tuning strategy, allowing LLMs to initially
explore reasoning patterns with feedback from tool invoking. (2) In the second stage, we develop a
Multi-Tool Self-Critic Reinforcement Learning Algorithm . Unlike prior RL approaches that focus
on single-tool usage, our method employs a hierarchical reward mechanism that not only evaluates
answer correctness and tool-use format but also assigns additional rewards for effective multi-tool
collaboration. To further improve the model’s understanding of this complex reward structure, we
interleave a self-critic reward fine-tuning phase within the standard RL process, facilitating the
internalization of reward principles.
To comprehensively evaluate the capability of Tool-Star, we design experiments across more than
10 challenging computational reasoning tasks (e.g., AIME24 and MATH500 [ 37]) and knowledge-
2https://openai.com/index/introducing-o3-and-o4-mini/
2

--- PAGE 3 ---
intensive reasoning tasks (e.g., WebWalker [ 68] and HotpotQA [ 73]). As illustrated in Figure 1,
Tool-Star demonstrates strong overall reasoning performance while ensuring both efficiency and
reliability in tool usage. In summary, our main contributions are as follows:
•We propose Tool-Star, a two-stage training framework to enhance multi-tool collaborative reasoning
in LLMs: (1) Cold-Start fine-tuning enables LLMs to explore reasoning patterns guided by tool-
invocation feedback; (2) Multi-Tool self-critic RL algorithm with a hierarchical reward design
reinforces the LLM’s grasp of reward principles and promotes effective multi-tool collaboration.
•To address the scarcity of tool-use data, we propose a general TIR data synthesis pipeline that
combines tool-integrated prompting with hint-based sampling to automatically and scalably gen-
erate tool-use trajectories. We further introduce a quality normalization and difficulty-aware
classification process to filter out unreasonable samples and organize the dataset from easy to hard.
•Experiments on more than 10 challenging reasoning datasets validate the effectiveness of Tool-Star.
Further quantitative analyses demonstrate its high efficiency, offering insights for incentivizing
multi-tool collaborative reasoning.
2 Related Work
Tool-Integrated Reasoning. Tool-Integrated Reasoning (TIR) has emerged as a promising
paradigm for enhancing LLM reasoning by enabling autonomous invocation of external tools. Existing
TIR approaches can be broadly categorized into three streams: (1) Prompting-based methods guide
models to use tools via carefully crafted prompts without additional training [ 2,30,38,31,69,25,62].
While easy to implement, they often suffer from instability and limited accuracy in tool usage. (2)
SFT-based methods apply supervised fine-tuning to teach weaker models tool use by distilling tra-
jectories from stronger models [ 17,27,76,22,33,10,13,28]. Though effective, their performance is
constrained by the quality of demonstrations and limited generalization beyond seen examples. (3) RL-
based methods optimize tool-use behavior via outcome-driven rewards [ 53,56,23,1,16,35,32,52],
enabling models to autonomously discover effective tool-use strategies through exploration. This
often leads to more robust and adaptive tool utilization than imitation-based approaches. Despite these
advances, most work focuses on single-tool settings. Multi-tool collaborative reasoning—requiring
coordination across heterogeneous tools (e.g., search engines, code interpreters)—remains under-
explored. While systems like OTC [ 64] and ToolRL [ 45] have made initial progress, there is still
a significant gap in developing specialized RL algorithms and reward mechanisms tailored for
multi-tool collaboration.
Reinforcement Learning for Reasoning. Recent advances in reinforcement learning (RL) have
significantly enhanced reasoning capabilities in language models. Large-scale RL implementations
such as DeepSeek-R1 [ 9], OpenAI-o1 [ 41], and QwQ [ 60] have demonstrated the effectiveness of RL
for improving reasoning performance. Concurrently, several studies have explored achieving emergent
reasoning capabilities with minimal resources, either through high-quality but small-scale datasets or
smaller parameter models [ 79,74,34]. Notable examples include SimpleRL-Zoo [ 79], LIMO [ 74],
LIMR [ 34], and Qwen’s mathematical reasoning approach [ 71], with additional insights from works
examining RL effectiveness in both small [7] and large models with minimal examples [66].
A series of research has focused on analyzing RL training strategies and reward function design
for reasoning tasks, providing practical insights for broader applications. Key contributions include
PRIME [ 6], which explores process reinforcement through implicit rewards, empirical studies on
improving R1-like reasoning models [ 3,75], and DAPO’s scalable RL system [ 77]. Another research
direction has concentrated on optimizing reasoning efficiency through specialized RL algorithms,
as demonstrated in works on concise reasoning [ 15,47,55]. More recently, researchers have begun
integrating external tools to enhance reasoning accuracy, with notable examples including ToRL [ 35],
ReTool [ 16], ToolRL [ 46], and OTC [ 64]. However, exploration of multi-tool collaborative reasoning
remains relatively limited, highlighting a significant gap in current research.
3 Methodology
Tool-Star is an RL-based framework that empowers LLMs to autonomously invoke multiple external
tools during reasoning. As shown in Figure 2 and Figure 3, Tool-Star includes two key compo-
nents: (1)A tool-integrated data synthesis pipeline that generates large-scale, high-quality reasoning
3

--- PAGE 4 ---
trajectories with reasonable tool usage (§3.1); (2)A two-stage training paradigm that facilitates
multi-tool collaborative reasoning, comprising cold-start fine-tuning and a multi-tool self-critic RL
algorithm (§3.2). We delve into the specifics of our approach as follows.
Problem Formulation. Multi-tool Collaborative Reasoning aims to enable LLMs to perform multi-
step reasoning through interaction and collaboration with multiple external tools. Specifically, given
a task query qand an external tool set T, Tool-Star autonomously invokes tools during the generation
of the reasoning chain Rc, guided by the tool-integrated instruction IT. The real-time tool-call
feedback FTis dynamically concatenated into the reasoning chain to facilitate ongoing inference
until the final output yis produced. This process can be modeled as:
Pθ(Rc, y|IT, q,T) =YTc
t=1Pθ(Rc
t| Rc
<t, IT, q,{FT}<t)
| {z }
Multi-Tool Integrated Reasoning·YTy
t=1Pθ(yt|y<t,Rc, IT, q)
| {z }
Answer Generation, (1)
where Tcdenotes the number of tokens in the reasoning chain Rc,Rc
tis the token at position t, and
Rc
<trepresents all tokens generated before t.{FT}<tdenotes the feedback of all tool calls prior to
t.Tyis the length of the Answer y, with ytas the token at position t.
Tool Design. In this work, we design six tools to enhance the reasoning capabilities of LLMs within
the Tool-Star framework. During the two-stage training process, we introduce three core tools that
enable LLMs to autonomously invoke external functionalities during reasoning:
•Search Engine: Executes search queries to retrieve relevant information, supporting both local
and web-based search modes.
•Web Browser Agent: Parses web search results by visiting URLs, extracting relevant content, and
summarizing key information in response to the query.
•Code Interpreter: Executes code snippets generated by the LLM in a sandbox environment,
returning either the execution results or error messages based on code correctness.
During inference, we introduce three Inference-time tools to optimize the reliability of TIR process:
•Code Debugger: Automatically corrects LLM-generated erroneous code by leveraging the original
code and compiler error messages to guide revisions.
•Tool-Use Backtracer: Locates and rolls back to the reasoning step preceding a failed tool
invocation, enabling the model to resume and revise its reasoning path.
•Reasoning Chain Refiner: When the output exceeds the maximum length, refiner prunes and
optimizes redundant steps in the reasoning process, replacing original reasoning chain with a more
concise and coherent version to complete the task.
3.1 Tool-Integrated Reasoning Data Synthesis
In this section, we present our tool-integrated reasoning data synthesis pipeline, aiming to enable
automated and scalable construction of high-quality tool-use datasets (Figure 2).
3.1.1 Step-1: Data Collection and Sampling
To balance data scale and diversity while ensuring accessibility, we curate a high-quality training set
from open-source knowledge-based and computational reasoning datasets, comprising approximately
90K text-based reasoning data ( Dtext) and 1K existing TIR datasets ( Dtool), as summarized in Table 5.
To further expand TIR trajectories, we introduce two complementary sampling strategies:
(1) TIR Prompting-based Sampling. We design a tool-integrated prompt ITto guide the LLM Pθ
in sampling responses for queries in Dtext. Following the Eq.(2), LLM decodes tool-invoking requests
within special tokens (e.g., <search> ... </search> ,<python> ... </python> ) during trajectory generation.
We then automatically parse and extract these requests, invoke external tools to obtain feedback F,
and insert the tool feedback—enclosed within <result> and </result> tags—back into the reasoning
chain as additional context for subsequent generation steps. This process iterates until either (1)
the maximum number of tool calls or length is reached, or (2) the model generates a final answer,
enclosed by designated tokens <answer> and </answer> . After inference on the entire Dtext, we filter
for correct samples to obtain the dataset DP
tool.
4

--- PAGE 5 ---
Figure 2: The overview of 3-step tool-integrated reasoning data synthesis pipeline.
(2) Hint-based Sampling. To further diversify tool invoking patterns, we employ a hint-based
method [ 29] that inserts hint tool-call tokens into language-only reasoning trajectories. We first prompt
the LLM to perform language-only reasoning on queries from Dtext. Following the START [ 28], we
propose two hint instructions— Logical Verification andAnswer Reflection —to insert tool-invoking
hints into the original reasoning chains. As shown in Figure 2, logical verification hints randomly
replace uncertain expressions (e.g., maybe ,wait,not sure ) in the chain, while reflection hints are
inserted after the answer. These diverse hints facilitate the model to invoke tools when information
is insufficient or after answer generation, enabling information completion and answer verification.
After inserting hints, we truncate the original reasoning chain at the hint position, prompting the
model to perform tool-augmented reasoning in response to the hint, which can be formulated as:
Pθ(Rc
>tH, y|IT, q,Rc
≤tH,T) =YTc
t=tHPθ(Rc
t| Rc
≤t, IT, q,{FT}≤t)
| {z }
Hint-based Tool-Integrated Reasoning·YTy
t=1Pθ(yt|y<t,Rc, IT, q)
| {z }
Answer Generation,
(2)
Unlike Eq.(2), tHis the end position of the hint insertion, and Rc
≤trepresents the language-only
reasoning chain before the hint position. The LLM continues reasoning from Rc
≤twithqto derive
the tool-integrated reasoning chain Rc
>tH. Other variables are defined consistently with Eq.(2). After
filtering the samples for the correct answers, we obtain the hint-based TIR dataset DH
tool. Finally, we
merge two datasets to obtain the final dataset Dv1
tool={DH
tool∪DM
tool∪Dtool}.
3.1.2 Step-2: Tool-Use Quality Normalization.
To ensure the rationality of tool usage within each sample, we implement the following 3 TIR
normalization strategies for tool-use data quality control: (1) Tool-call Frequency Control: Remove
samples with tool-call frequency exceeding a predefined threshold βto alleviate excessive tool
invocation. (2) Duplicate Tool-call Removal: Eliminate samples containing redundant tool calls,
such as repeated generation of identical search queries or code snippets in the same response. (3)
Format Normalization: Standardize tool call formats in reasoning chains by unifying special tokens
for invocation, feedback, and final answers, while ensuring balanced usage of start and end tokens.
By applying these criteria, we obtain a quality-filtered dataset denoted as Dv2
tool.
3.1.3 Step-3: Difficulty-Aware Data Classification.
Considering the computational overhead of tool-use and the multi-stage nature of TIR training, we
argue that a high-quality tool-use dataset should meet the following criteria: (1) Invoke tools only
when necessary: Tool calls should be avoided when the model is capable of solving the problem
through direct reasoning. (2) Organize samples from easy to hard: As emphasized in prior RL tool
learning [53, 81], stage-wise training based on sample difficulty is crucial for effective learning.
To achieve the above objectives, we first perform a language-only reasoning pass on each question
in the dataset Dv2
tool, yielding a direct reasoning result denoted as Dv2
text. Based on the correctness of
both direct reasoning (DR) and tool-integrated reasoning (TIR), each sample is categorized into four
5

--- PAGE 6 ---
types (Figure 2). For categories 1 and 2, where the model already produces correct answers through
direct reasoning, tool use is deemed unnecessary. These samples are extracted from Dv2
textto form a
subset Dsub
text. For category 3, which clearly illustrates the advantages of tool use over language-only
reasoning, we directly sample from Dv2
toolto construct the subset Dsub
tool.
To support a curriculum learning paradigm [ 11] from easy to hard, we construct a cold-start fine-
tuning dataset DSFT
toolby combining Dsub
textandDsub
tool. For Category-4 samples, which are challenging
for both DR and TIR, are treated as hard examples and reserved for reinforcement learning, forming
the dataset DRL
tool. This design enables the LLM to acquire basic tool-use capabilities through cold-
start fine-tuning, and subsequently generalize to more complex scenarios during the RL phase (e.g.
multi-tool collaboration), thereby facilitating a progressive learning trajectory.
3.2 Tool-Star: A Two-stage Multi-tool Collaborative Training Framework
In this section, we propose a two-stage multi-tool collaborative training framework that generalize the
LLM’s TIR capability from a single to multiple tool-use paradigm, and from easy to hard learning.
Cold-Start Supervised Fine-tuning. To equip the LLM with an initial understanding of tool usage
for problem solving, given (xi, yi)∈DSFT
tool, we apply the standard Supervised Fine-tuning objective
on the backbone model Pθwith parameters θ:L(θ) =−P
(xi,yi)∈DSFT
toollogPθ(yi|xi), where xi
denotes the i-th input. Ultimately, we obtain a cold-start LLM ˆπθwith initial TIR capability.
Multi-Tool Self-Critic Reinforcement Learning. We will introduce how the LLM learn to au-
tonomously invoke tools, including a Code interpreter, a search engine and a web browser agent.
(1) Memory-based Roll-Out with Tools. As shown in Figure 3, we employ multi-tool invocation
instructions to guide the model in decoding tool-use requests into special tokens (e.g., <python> )
during the roll-out process. Upon detecting these tokens, the corresponding tool is automatically
invoked, and the resulting feedback is integrated back into the reasoning chain. To reduce latency
caused by frequent tool calls, we incorporate a memory mechanism that caches the mapping between
each tool request and its output. This allows the model to retrieve responses for repeated requests
directly from memory, thereby improving efficiency.
(2) Hierarchical Reward Design. Reward signals serve as the optimization objective and directly
guide the behavior of the policy model during training. Distinct from previous tool-use RL ap-
proaches, we not only design correctness and format rewards for LLMs, but also introduce multi-tool
collaborative rewards. This design aims to encourage multiple tool usage while maintaining LLM
correctness. Specifically, when both the answer and tool invocation format are correct, and the model
employs multiple tools (i.e., both <search> and <python> appear in the reasoning chain), an extra
reward rMis granted. Formally, the overall reward Ris defined as follows:
R=

max( Acc. +rM, Acc. )If Format is Good & Acc.>0
0 If Format is Good & Acc.=0
−1 Otherwise, rM=(
0.1If∃(<search> &<python> )
0 Otherwise(3)
(3) Self-Critic RL Algorithm. Multi-tool RL algorithms involve complex reward structures, making
it challenging for LLMs to discover optimal behavior. To address this, we propose a self-critic RL
algorithm that enables LLMs to better align with the design principles of reward mechanisms. As
illustrated in Figure 3, we begin by performing Ksteps of vanilla RL training on the cold-start model
ˆπθ. To optimize the policy, we adopt Group Relative Policy Optimization (GRPO) [ 51] as our RL
algorithm, which estimates the baseline using a group of rollouts.
LGRPO(θ) =E[q∼DRL
tool,{oi}G
i=1∼ˆπθold(O|q)]1
GGX
i=11
|oi||oi|X
t=1
minˆπθ(oi,t|q, oi,<t)
ˆπθold(oi,t|q, oi,<t)ˆAi,t,clipˆπθ(oi,t|q, oi,<t)
ˆπθold(oi,t|q, oi,<t),1−ϵ,1 +ϵ
ˆAi,t
−βDKL[ˆπθ||ˆπref]
,(4)
where ϵandβare hyper-parameters, qandodenote query and model’s output with tools. Aiis the
normalized advantage of the i-th rollout in group. Then we can obtain an RL-initialized model πRL
θ.
Subsequently, we detail our self-critic reward finetuning phrase, which help LLMs better internalize
the reward structure. As shown in Figure 3, we start by randomly rejection sampling kexamples
6

--- PAGE 7 ---
(a) GRPO with Tool Environment
QuestionRollout Module
...Reference 
Model
Reward
Model
...Group
Computation
 ...
User Question Assistant <Think> <Search> [Search Query] </Search> <Result> [Tool Feedback] <Result>
 <python> [Code] </python >
 ......
Autonomous Iterative Tool -Calling: Repeated N Times
Cold -Start 
ModelK-step RL
TrainingRL Initilaized
Model1. Self -Sampling
2. Reward Assigning Response 1
Response NResponse 2Reward 1
Reward 2
Reward N...
Online Self -Critic DPO Training
(b) Multi -Tool Self -Critic Reinforcement Learning
Reward>=1→Postive 
Reward<1→NegativePrinciple 1
Principle 2
Principle N... ...Self-Critic Response Format Reward -Aware 
Preference DatasetResponse ：
Tool-Star Dataset
Policy
ModelTool 
Env.
Memory
<Result> [Feedback]Token with Loss
Token without Loss
Self-Critic
RL Model×N  Iteration s
Figure 3: The overall framework of Multi-Tool Self-Critic Reinforcement Learning.
from the RL training set DRL
toolto construct DS
tool[61,78]. For each query q∈DS
tool, the RL initialized
model πRL
θself-samples ncandidate responses, forming a diverse QA set D∼(q,{ai}N
i=1). Notably,
our hierarchical rule-based reward function acts as an executable program that automatically assigns
a reward label to each response. This enables on-policy reasoning by providing each aiwith a
corresponding reward. We then construct a reward-aware dataset Dcritic
tool={(xi, yi)}N
i=1, where each
input xiis a query q, and each output yiincludes a candidate response ai, its reasoning trace pi, and
reward score ri. We treat samples with ri≥1as positive and those with ri<1as negative, forming
a preference dataset (x, yw, yl)∼Dcritic
tool. Finally, we fine-tune the RL-initialized model using DPO
objective as:
LDPO(πRL
θ;πref) =−E(x,yw,yl)∼D[logσ(βlogπRL
θ(yw|x)
πref(yw|x)−βlogπRL
θ(yl|x)
πref(yl|x))], (5)
where the reference model πrefis set to πRL
θinitially and remains fixed throughout training. βis a
hyperparameter and σis the sigmoid function. LDPOencourages higher log-probability for preferred
responses ywover dispreferred ones yl. We interleave self-critic reward fine-tuning every ksteps
during standard RL training. This iterative process allows the model to progressively learn behavioral
distinctions via self-sampling and evaluation, implicitly capturing the hierarchical reward structure.
The full algorithm is detailed in Algorithm 1.
3.3 Multi-tool Collaborative Inference
During inference, we follow Eq.(2), enabling the model to autonomously use the search engine ,web
browser agent , and Python interpreter . To improve TIR robustness, we introduce three inference-time
mechanisms targeting common failure scenarios:
•Code Execution Error: When generated code contains syntax errors, the Code Debugger uses
compiler messages and the original code to produce executable fixes.
•Tool Invocation Failure: If tool calls yield no useful output or debugging fails, the Backtracer
identifies the statement before the tool-invoking token and restarts inference from there.
•Inference Length Overflow: When tool outputs exceed the context limit, the Chain Refiner
compresses and restructures the reasoning chain by removing redundant or irrelevant content,
allowing continued inference without tools.
These mechanisms enhance tool invocation reliability and provide an effective inference-time scaling
strategy. Further details are listed in the Appendix B.2.
7

--- PAGE 8 ---
Table 1: Overall results on 10 challenging reasoning tasks. The top two results are highlighted in
bold andunderlined . All baseline methods use Qwen2.5-3B-Instruct as the backbone. For baselines
lacking a 3B Instruct version, we replicate them using the exact parameter settings from their open-
source code. To ensure fairness, we report the RAG setting (Top-5 Docs) for the code-assistant model
in knowledge-intensive reasoning. The abbreviations of the datasets: HQA (HotpotQA), 2Wiki.
(2wikiMultiHopQA), MuSi. (MuSiQue), and Bamb (Bamboogle).
MethodComputational Reasoning Knowledge-Intensive Reasoning
Avg.
AIME24 AIME25 MATH500 GSM8K MATH WebWalker HQA 2Wiki. MuSiQ. Bamb.
Qwen2.5-3B-Instruct 10.0 6.7 63.0 75.0 71.6 0.5 9.7 9.4 3.6 11.7 26.1
Llama3.2-3B-Instruct 0.0 3.3 40.0 71.2 58.2 0.5 12.5 9.2 4.0 18.3 21.7
Code-Assistant Reasoning Methods
ToRL 20.0 10.0 72.0 84.4 81.0 12.0 37.9 27.0 8.3 25.4 37.8
DotaMath 3.3 6.7 56.2 78.2 71.8 11.5 35.6 31.2 7.5 23.8 32.6
Search-Enhanced Reasoning Methods
RAG 13.3 10.0 54.0 46.0 56.0 14.6 39.4 31.2 10.3 17.4 29.0
Search-o1 16.7 3.3 69.0 34.0 63.0 13.0 34.9 28.9 9.6 35.1 30.2
Search-R1 0.0 3.3 26.0 43.0 44.0 14.4 43.2 25.5 16.5 40.8 25.7
Multi-Tool Integrated Reasoning Methods
Multi-Tool Prompting 3.3 3.3 54.2 48.8 59.6 9.2 15.9 16.2 6.5 17.8 23.4
ReCall 16.6 6.7 63.0 77.8 74.2 13.0 43.5 38.9 16.5 40.8 39.1
Tool-Star(Qwen2.5-3B) 20.0 16.7 72.0 85.0 82.6 20.8 51.9 40.0 19.3 52.5 46.1
Tool-Star(Llama3-3B) 10.0 10.0 54.8 77.8 70.6 24.0 48.9 43.2 16.6 54.7 41.0
4 Experiments
Datasets. To comprehensively evaluate the tool-use capabilities of our model, we conduct experi-
ments on two types of reasoning datasets: (1) Math reasoning benchmarks , including AIME2024,
AIME20253, MATH500 [ 37], MATH [ 18], and GSM8K. (2) Knowledge reasoning benchmarks ,
including three complex reasoning QA tasks: GAIA [ 39], HLE [ 43], and WebWalker [ 68]; and three
Wikipedia-based open-domain QA tasks: HotpotQA [ 73], 2WikiMultihopQA [ 19], and Musique [ 63].
For all tasks, we follow previous work [ 31] and extract answers from the model output enclosed in
\box{} .
Evaluation Metrics. For computational reasoning tasks, we follow Search-o1 [ 31] employ LLM-
based judging (Qwen2.5-72B=Instruct) to ensure answer correctness. For open-domain QA tasks, we
adopt token-level F1 score as the evaluation metric. To assess tool usage efficiency, we propose the
Tool-use Efficiency metric: TE=1
NPN
i=1Si
Tc
iwhere Nis the number of datasets, SiandTc
idenote
the number of correct answers and the number of total samples for the i-th dataset when using tools.
Baselines. Our experiments include the following three categories of baselines: (1) Proprietary
Models : Qwen2.5 [ 49], Llama3.2 [ 14]. (2) Single-tool Enhanced Baselines : Code-enhanced models
such as ToRL [ 35] and DotaMath [ 27], and search-enhanced models such as Vanilla RAG [ 26],
Search-o1 [ 31] and Search-R1 [ 23]. (3) Multi-tool Enhanced Baselines : Multiple tools integrated
prompting and ReCall [1]. More details of implementations are provided in the Appendix C.
4.1 Results on Challenging Reasoning Tasks
Our main results are presented in Table 1. Overall, Tool-Star consistently outperforms all baselines,
decisively establishing the superiority. Furthermore, we have identified the following insights:
(1) TIR prompting fails to explore the better tool-use behavior. Focusing on Search-o1 and
Multi-Tool Prompting, their performance on comprehensive reasoning tasks remains suboptimal.
Notably, multi-tool prompting strategies even exhibit lower consistency compared to their backbone
3https://huggingface.co/datasets/AI-MO/aimo-validation-aime
8

--- PAGE 9 ---
Prompt-base(w/oinferencetimetool)Prompt-baseToolStar(w/oinferencetimetool)ToolStarFigure 4: Left: Tool-use efficiency comparison across TIR methods. Middle: Performance analysis
of TIR methods with inference-time tool. Right: Error statistics of tool invocation.
model. This reveals that relying solely on prompt engineering to elicit tool usage is insufficient for
guiding the LLM toward effective tool utilization, thereby limiting its reasoning capabilities.
(2) Single-tool RL-based methods exhibit strong domain specialization but limited generaliz-
ability. Search-enhanced RL approaches perform well on knowledge-intensive tasks (e.g., 40+% F1
on HQA), but their accuracy drops sharply by over 20% on computation-heavy tasks like MATH500
and AIME24 compared to the backbone model. Conversely, code-assistant methods like ToRL under-
perform on knowledge-based tasks, even falling behind RAG baselines. These trends underscore the
specialization bias of single-tool RL methods and their limited cross-domain transferability.
(3) Tool-Star demonstrates strong and versatile reasoning, along with robust plug-and-play
capability. Tool-Star consistently outperforms both single- and multi-tool baselines, achieving over
40% average accuracy across 10 datasets while maintaining competitive performance on individual
tasks. Notably, it also delivers substantial gains across different backbone models, with average
improvements nearing 20% on both Qwen and LLaMA. These results highlight Tool-Star’s gener-
ality, efficiency, and strong adaptability across models and tasks. To further validate Tool-Star’s
generalizability, we present results on deep web exploration and a case study in Appendix A.1 and E.
4.2 Quantitative Analysis
Table 2: Ablation study on Tool-Star(3B).
MethodKnowledge. Computational.
HQA Bamb. GSM8K MATH
Tool-Star (3B) 51.9 52.5 85.0 82.6
w/oCold-Start 43.5(-8.4)40.8(-11.7) 77.8(-7.2)74.2(-8.4)
w/oRL stage 47.5(-4.4)43.9(-8.6)80.2(-4.8)78.4(-4.2)
w/oReward 50.4(-1.5)50.3(-2.2)83.1(-1.9)80.2(-2.4)
w/oSelf-Critic 49.8(-2.1)48.3(-4.2)82.8(-2.2)77.8(-4.8)Ablation Study. In this section, we conduct
a systematic ablation study in Table 2, where
“w/o” denotes removing a specific component.
The results demonstrate that: (1)Removing
any single module leads to performance degra-
dation, highlighting the critical role of each
part of design. (2)Excluding either results in
significant performance drops. the Cold-Start
phase helps the model initially acquire tool-
use capabilities, while the RL phase promotes
generalization in multi-tool reasoning. Both
stages are indispensable and complementary. (3)Incorporating hierarchical rewards and a self-
critic mechanism on top of vanilla RL consistently brings further improvements, confirming the
effectiveness of our tailored RL strategy.
Tool-Use Efficiency Analysis. To validate whether Tool-Star can efficiently invoke tools during
stepwise reasoning, we compare the Tool-use Accuracy TEof various TIR methods on knowledge-
intensive and computation-based reasoning datasets, as shown in Figure 4 (left). Results indicate that
while Search-R1 and ToRL achieve high accuracy in knowledge and computation tasks respectively,
they underperform in the other domain—highlighting the tendency of single-tool RL methods to
specialize in specific reasoning types. In contrast, Tool-Star consistently maintains high tool-use
accuracy across both task types and baselines, demonstrating its efficiency in tool utilization.
9

--- PAGE 10 ---
HotpotQA 2Wiki MuSiQue Bamboogle010203040506070Overall PerformanceDifferent Search Mode
Tool-Star-3B(Local)
Tool-Star-3B(Web)Figure 5: Performance Comparison of Tool-
Star(3B) in Local and Web Search ModesLocal Search VS. Web Search. Inspired by R1-
Searcher [ 53] and DeepResearch [ 81], we further
compared the differences between local search and
web search of Tool-Star-3B in knowledge-intensive
reasoning tasks. As shown in Figure 5, we found that
after using web search, except for HotpotQA which
did not show obvious improvement, the other three
domains all demonstrated significant improvements,
especially 2wiki (+13% F1) and Bamboogle (+8%
F1). Regarding the above phenomena, we believe
that the performance differences between the two
mainly stem from: (1)The retrieval library of wiki is
a subset of the web page corpus. Moreover, when combined with the browser agent for web page
information, it can provide more accurate and concise information compared to wiki documents.
(2)We use the Bing API4for web search, and the retrieval models and strategies of their services
perform better than the E5-base we deployed offline for local search.
Inference-Time Tool-Use Analysis. To verify the effectiveness of inference-time tool design,
we further analyze its impact on the performance of Tool-Star and DotaMath across two datasets.
As shown in Figures 4 (middle) and (right), both models exhibit a notable reduction in tool-use
errors during reasoning after applying inference-time tool optimizations, accompanied by significant
performance improvements. These results not only underscore the detrimental effect of incorrect
tool usage on reasoning accuracy, but also reinforces the motivation for incorporating inference-time
tools. Notably, the performance gain for DotaMath is substantially larger than that for Tool-Star, with
improvements exceeding 20% on GSM8K. This suggests that inference-time tools are particularly
beneficial for models with weaker coding capabilities, while models with stronger coding proficiency,
such as Tool-Star, naturally make fewer tool-related errors and thus have less room for improvement.
Figure 6: Training curves of reward and response
length for models with different parameters.Scaling Analysis on Parameter Scales. To inves-
tigate the effectiveness of Tool-Star across differ-
ent parameter scales, we display the RL training
curves of Qwen2.5 models with 0.5B, 1.5B, and
3B parameters in Figure 6. Our key observations
are as follows: (1) All models achieve good reward
scores at the beginning of training (step 0), ben-
efiting from the cold-start SFT stage, which sig-
nificantly reduces the exploration burden in early
RL; (2) As training progresses, the average reward
scores steadily improve across all model sizes, in-
dicating that our self-critic RL framework further
improve TIR capabilities on top of the cold-start initialization. (3) Notably, the average reward shows
signs of emergent improvement around step 10, while no clear inflection point (i.e., “aha moment”)
is observed in response length. Instead, response lengths gradually stabilize, suggesting convergence
toward optimized response patterns. More training details can be found in Appendix A.3.
Scaling Analysis on the RL Training Curve. To better understand the training dynamics of
the reinforcement learning process, we present the trends of KL penalty, average reward score,
average response length, and validation score over the first 100 steps RL training (Figure 7). Our key
observations are as follows:
•All metrics—including reward score, KL penalty, and validation score—increase steadily with
training steps, indicating the stability of our self-critic RL training method.
•The initial reward scores of the 3B and 1.5B models are already above 0 (within a possible
range of -1 to 1.1), with corresponding validation scores starting at approximately 0.375 and 0.29
respectively—substantially higher than the vanilla instruct model’s score of below 0.15. These
results demonstrate the effectiveness of our cold-start strategy. In contrast to RL methods trained
entirely from scratch [ 35,53,23], the cold-start approach endows the model with a baseline level of
TIR capability from the outset, significantly reducing the need for extensive black-box exploration
during the early stages of RL training.
4https://www.microsoft.com/en-us/bing/apis/bing-web-search-api
10

--- PAGE 11 ---
Figure 7: The scaling relationship of kl loss, reward, response length and validation score for
Tool-Star(Qwen2.5-3B & 1.5B in instruct version).
•After cold-start SFT, the reward score shows mild emergence within the first 0–20 steps, though
less pronounced than in from-scratch RL (i.e., no sudden rise-and-drop). Similarly, the response
length does not fluctuate sharply but gradually converges to a stable value, further supporting the
stability benefits of cold-start training.
5 Conclusion
In this paper, we introduce Tool-Star, an RL-based framework designed to enable LLMs to au-
tonomously invoke multiple external tools during stepwise reasoning. To address the scarcity of
tool-use data, Tool-Star employs a scalable TIR data synthesis pipeline, incorporating normalization
and a difficulty-aware data classification process. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning in LLMs, consisting of a Cold-Start Fine-
tuning phase and a Multi-Tool Self-Critic RL stage. This framework progressively fosters effective
multi-tool collaboration and strengthens the LLM’s understanding of reward principles. Extensive ex-
periments across over ten challenging benchmarks demonstrate that Tool-Star significantly improves
both the effectiveness and efficiency of tool-augmented reasoning in LLMs.
References
[1]Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Z.
Pan, Wen Zhang, Huajun Chen, Fan Yang, Zenan Zhou, and Weipeng Chen. Research: Learning
to reason with search for llms via reinforcement learning, 2025.
[2]Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts
prompting: Disentangling computation from reasoning for numerical reasoning tasks. Trans.
Mach. Learn. Res. , 2023, 2023.
[3]Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng,
Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen.
An empirical study on eliciting and improving r1-like reasoning models. CoRR , abs/2503.04548,
2025.
[4]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems. CoRR , abs/2110.14168, 2021.
11

--- PAGE 12 ---
[5]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John
Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 ,
2021.
[6]Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan,
Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo
Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, Zhiyuan Liu, Maosong Sun, Bowen Zhou, and
Ning Ding. Process reinforcement through implicit rewards. CoRR , abs/2502.01456, 2025.
[7]Quy-Anh Dang and Chris Ngo. Reinforcement learning for reasoning in small llms: What
works and what doesn’t. CoRR , abs/2503.16219, 2025.
[8]Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. CoRR ,
2023.
[9]DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin
Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu,
Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan
Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang,
Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli
Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng
Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li,
Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian
Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean
Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan
Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian,
Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong
Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan
Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting
Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun,
T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu,
Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao
Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su,
Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang
Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y . K. Li, Y . Q. Wang, Y . X.
Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao
Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang
Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He,
Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y . X. Zhu, Yanhong
Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha,
Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan
Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu,
Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang,
and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement
learning, 2025.
[10] Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren
Zhou. Self-play with execution feedback: Improving instruction-following capabilities of large
language models. CoRR , abs/2406.13542, 2024.
[11] Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu,
Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. How abilities in large language
models are affected by supervised fine-tuning data composition. In Lun-Wei Ku, Andre Martins,
and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August
11-16, 2024 , pages 177–198. Association for Computational Linguistics, 2024.
[12] Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, and Ji-Rong Wen.
Progressive multimodal reasoning via active retrieval. CoRR , abs/2412.14835, 2024.
12

--- PAGE 13 ---
[13] Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, and Ji-Rong Wen.
Understand what LLM needs: Dual preference alignment for retrieval-augmented generation.
CoRR , abs/2406.18676, 2024.
[14] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,
Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd
of models. arXiv preprint arXiv:2407.21783 , 2024.
[15] Mehdi Fatemi, Banafsheh Rafiee, Mingjie Tang, and Kartik Talamadupula. Concise reasoning
via reinforcement learning, 2025.
[16] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan
Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in
llms, 2025.
[17] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan,
and Weizhu Chen. Tora: A tool-integrated reasoning agent for mathematical problem solving.
InThe Twelfth International Conference on Learning Representations, ICLR 2024, Vienna,
Austria, May 7-11, 2024 . OpenReview.net, 2024.
[18] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn
Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset.
In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information
Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks
2021, December 2021, virtual , 2021.
[19] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing A multi-
hop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Núria Bel, and
Chengqing Zong, editors, Proceedings of the 28th International Conference on Computational
Linguistics, COLING 2020, Barcelona, Spain (Online), December 8-13, 2020 , pages 6609–6625.
International Committee on Computational Linguistics, 2020.
[20] Jian Hu. Reinforce++: A simple and efficient approach for aligning large language models.
arXiv preprint arXiv:2501.03262 , 2025.
[21] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A
survey. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors, Findings of the
Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023 , pages
1049–1065. Association for Computational Linguistics, 2023.
[22] Jinhao Jiang, Jiayi Chen, Junyi Li, Ruiyang Ren, Shijie Wang, Wayne Xin Zhao, Yang Song, and
Tao Zhang. Rag-star: Enhancing deliberative reasoning with retrieval augmented verification
and refinement. CoRR , abs/2412.12881, 2024.
[23] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-
r1: Training llms to reason and leverage search engines with reinforcement learning. CoRR ,
abs/2503.09516, 2025.
[24] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: A modular
toolkit for efficient retrieval-augmented generation research. CoRR , abs/2405.13576, 2024.
[25] Shanglin Lei, Guanting Dong, Xiaoping Wang, Keheng Wang, and Sirui Wang. Instructerc:
Reforming emotion recognition in conversation with a retrieval multi-task llms framework.
CoRR , abs/2309.11911, 2023.
[26] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and
Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,
editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual , 2020.
13

--- PAGE 14 ---
[27] Chengpeng Li, Guanting Dong, Mingfeng Xue, Ru Peng, Xiang Wang, and Dayiheng Liu.
Dotamath: Decomposition of thought with code assistance and self-correction for mathematical
reasoning. CoRR , abs/2407.04078, 2024.
[28] Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen
Yu, Binyuan Hui, Junyang Lin, and Dayiheng Liu. START: self-taught reasoner with tools.
CoRR , abs/2503.04625, 2025.
[29] Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen
Yu, Binyuan Hui, Junyang Lin, and Dayiheng Liu. START: self-taught reasoner with tools.
CoRR , abs/2503.04625, 2025.
[30] Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey
Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of code: Reasoning with a language
model-augmented code emulator. In Forty-first International Conference on Machine Learning,
ICML 2024, Vienna, Austria, July 21-27, 2024 . OpenReview.net, 2024.
[31] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang,
and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. CoRR ,
abs/2501.05366, 2025.
[32] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen,
and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research
capability. arXiv preprint arXiv:2504.21776 , 2025.
[33] Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yongkang Wu, Zhonghua Li, Qi Ye, and Zhicheng Dou.
Retrollm: Empowering large language models to retrieve fine-grained evidence within genera-
tion. CoRR , abs/2412.11919, 2024.
[34] Xuefeng Li, Haoyang Zou, and Pengfei Liu. LIMR: less is more for RL scaling. CoRR ,
abs/2502.11886, 2025.
[35] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated RL. CoRR ,
abs/2503.23383, 2025.
[36] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao,
Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong,
Zhijiang Guo, Le Song, and Cheng-Lin Liu. From system 1 to system 2: A survey of reasoning
large language models. CoRR , abs/2502.17419, 2025.
[37] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee,
Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In The
Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria,
May 7-11, 2024 . OpenReview.net, 2024.
[38] Pan Lu, Bowen Chen, Sheng Liu, Rahul Thapa, Joseph Boen, and James Zou. Octotools: An
agentic framework with extensible tools for complex reasoning. CoRR , abs/2502.11271, 2025.
[39] Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom.
GAIA: a benchmark for general AI assistants. In The Twelfth International Conference on
Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net, 2024.
[40] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng
Wang, Xiaoxue Cheng, Huatong Song, et al. Imitate, explore, and self-improve: A reproduction
report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413 , 2024.
[41] OpenAI. Learning to reason with llms, September 2024.
[42] Bhargavi Paranjape, Scott M. Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer,
and Marco Túlio Ribeiro. ART: automatic multi-step reasoning and tool-use for large language
models. CoRR , abs/2303.09014, 2023.
14

--- PAGE 15 ---
[43] Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi,
Michael Choi, Anish Agrawal, Arnav Chopra, Adam Khoja, Ryan Kim, Jason Hausenloy, Oliver
Zhang, Mantas Mazeika, Daron Anderson, Tung Nguyen, Mobeen Mahmood, Fiona Feng,
Steven Y . Feng, Haoran Zhao, Michael Yu, Varun Gangal, Chelsea Zou, Zihan Wang, Jessica P.
Wang, Pawan Kumar, Oleksandr Pokutnyi, Robert Gerbicz, Serguei Popov, John-Clark Levin,
Mstyslav Kazakov, Johannes Schmitt, Geoff Galgon, Alvaro Sanchez, Yongki Lee, Will Yeadon,
Scott Sauers, Marc Roth, Chidozie Agu, Søren Riis, Fabian Giska, Saiteja Utpala, Zachary
Giboney, Gashaw M. Goshu, Joan of Arc Xavier, Sarah-Jane Crowson, Mohinder Maheshbhai
Naiya, Noah Burns, Lennart Finke, Zerui Cheng, Hyunwoo Park, Francesco Fournier-Facio,
John Wydallis, Mark Nandor, Ankit Singh, Tim Gehrunger, Jiaqi Cai, Ben McCarty, Darling
Duclosel, Jungbae Nam, Jennifer Zampese, Ryan G. Hoerr, Aras Bacho, Gautier Abou Loume,
Abdallah Galal, Hangrui Cao, Alexis C. Garretson, Damien Sileo, Qiuyu Ren, Doru Cojoc,
Pavel Arkhipov, Usman Qazi, Lianghui Li, Sumeet Motwani, Christian Schröder de Witt,
Edwin Taylor, Johannes Veith, Eric Singer, Taylor D. Hartman, Paolo Rissone, Jaehyeok Jin,
Jack Wei Lun Shi, Chris G. Willcocks, Joshua Robinson, Aleksandar Mikov, Ameya Prabhu,
Longke Tang, Xavier Alapont, Justine Leon Uro, Kevin Zhou, Emily de Oliveira Santos,
Andrey Pupasov Maksimov, Edward Vendrow, Kengo Zenitani, Julien Guillod, Yuqi Li, Joshua
Vendrow, Vladyslav Kuchkin, and Ng Ze-An. Humanity’s last exam. CoRR , abs/2501.14249,
2025.
[44] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis.
Measuring and narrowing the compositionality gap in language models. In Houda Bouamor,
Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguis-
tics: EMNLP 2023, Singapore, December 6-10, 2023 , pages 5687–5711. Association for
Computational Linguistics, 2023.
[45] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan
Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958 ,
2025.
[46] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan
Tur, and Heng Ji. Toolrl: Reward is all tool learning needs, 2025.
[47] Runqi Qiao, Qiuna Tan, Guanting Dong, Minhui Wu, Chong Sun, Xiaoshuai Song, Zhuoma
Gongque, Shanglin Lei, Zhe Wei, Miaoxuan Zhang, Runfeng Qiao, Yifan Zhang, Xiao Zong,
Yida Xu, Muxi Diao, Zhimin Bao, Chen Li, and Honggang Zhang. We-math: Does your large
multimodal model achieve human-like mathematical reasoning? CoRR , abs/2407.01284, 2024.
[48] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe
Yuan, Hector Liu, Yuanzhi Li, et al. O1 replication journey: A strategic progress report–part 1.
arXiv preprint arXiv:2410.18982 , 2024.
[49] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu,
Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu,
Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu,
Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji
Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang
Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical
report, 2024.
[50] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System
optimizations enable training deep learning models with over 100 billion parameters. KDD ’20,
2020.
[51] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y . K. Li,
Y . Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open
language models. CoRR , abs/2402.03300, 2024.
[52] Joykirat Singh, Raghav Magazine, Yash Pandya, and Akshay Nambi. Agentic reasoning and
tool integration for llms via reinforcement learning. arXiv preprint arXiv:2505.01441 , 2025.
15

--- PAGE 16 ---
[53] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang,
and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement
learning. CoRR , abs/2503.05592, 2025.
[54] Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. Curriculum learning: A survey.
Int. J. Comput. Vis. , 130(6):1526–1565, 2022.
[55] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi
Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Ben Hu. Stop overthinking: A
survey on efficient reasoning for large language models. CoRR , abs/2503.16419, 2025.
[56] Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan
Zhang, Fei Huang, and Jingren Zhou. Zerosearch: Incentivize the search capability of llms
without searching, 2025.
[57] Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu,
Mingyu Ding, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai Wang, Junsong Chen, Zhangyue
Yin, Xiaozhe Ren, Jie Fu, Junxian He, Wu Yuan, Qi Liu, Xihui Liu, Yu Li, Hao Dong, Yu Cheng,
Ming Zhang, Pheng-Ann Heng, Jifeng Dai, Ping Luo, Jingdong Wang, Ji-Rong Wen, Xipeng
Qiu, Yike Guo, Hui Xiong, Qun Liu, and Zhenguo Li. A survey of reasoning with foundation
models. CoRR , abs/2312.11562, 2023.
[58] Shuang Sun, Huatong Song, Yuhao Wang, Ruiyang Ren, Jinhao Jiang, Junjie Zhang, Fei
Bai, Jia Deng, Wayne Xin Zhao, Zheng Liu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen.
Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis.
2025.
[59] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li,
Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement
learning with llms. arXiv preprint arXiv:2501.12599 , 2025.
[60] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown. Hugging Face , 2024.
[61] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez,
Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation
language models. CoRR , abs/2302.13971, 2023.
[62] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving
retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv
preprint arXiv:2212.10509 , 2022.
[63] Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. musique:
Multihop questions via single-hop question composition. Transactions of the Association for
Computational Linguistics , 10:539–554, 2022.
[64] Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin,
Mengdi Wang, Kam-Fai Wong, and Heng Ji. Otc: Optimal tool calls via reinforcement learning.
arXiv preprint arXiv:2504.14870 , 2025.
[65] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan
Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training,
2024.
[66] Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng,
Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and
Yelong Shen. Reinforcement learning for reasoning in large language models with one training
example, 2025.
[67] Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan
Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Monica Lam, Yiping Lu, Kyunghyun Cho,
Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, and Manling Li. Ragen: Understanding
self-evolution in llm agents via multi-turn reinforcement learning, 2025.
16

--- PAGE 17 ---
[68] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang,
Yulan He, Deyu Zhou, Pengjun Xie, and Fei Huang. Webwalker: Benchmarking llms in web
traversal. CoRR , abs/2501.07572, 2025.
[69] Junde Wu, Jiayuan Zhu, and Yuyuan Liu. Agentic reasoning: Reasoning llms with tools for the
deep research. CoRR , abs/2502.04644, 2025.
[70] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,
Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong
Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu,
Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin
Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao,
Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin
Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng
Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu,
Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR ,
abs/2407.10671, 2024.
[71] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu,
Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu,
Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical
expert model via self-improvement. CoRR , abs/2409.12122, 2024.
[72] Dayu Yang, Tianyang Liu, Daoan Zhang, Antoine Simoulin, Xiaoyi Liu, Yuwei Cao, Zhaopu
Teng, Xin Qian, Grey Yang, Jiebo Luo, and Julian J. McAuley. Code to think, think to code:
A survey on code-enhanced reasoning and reasoning-driven code intelligence in llms. CoRR ,
abs/2502.19411, 2025.
[73] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,
and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop ques-
tion answering. In EMNLP , pages 2369–2380, Brussels, Belgium, October-November 2018.
Association for Computational Linguistics.
[74] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. LIMO: less is
more for reasoning. CoRR , abs/2502.03387, 2025.
[75] Chao Yu, Xuejing Zheng, Hankz Hankui Zhuo, Hai Wan, and Weilin Luo. Reinforcement
learning with knowledge representation and reasoning: A brief survey. CoRR , abs/2304.12090,
2023.
[76] Dian Yu, Baolin Peng, Ye Tian, Linfeng Song, Haitao Mi, and Dong Yu. Siam: Self-improving
code-assisted mathematical reasoning of large language models. CoRR , abs/2408.15565, 2024.
[77] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan,
Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan
Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen,
Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu,
Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO:
an open-source LLM reinforcement learning system at scale. CoRR , abs/2503.14476, 2025.
[78] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou.
Scaling relationship on learning mathematical reasoning with large language models. CoRR ,
abs/2308.01825, 2023.
[79] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He.
Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the
wild. CoRR , abs/2503.18892, 2025.
[80] Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Xinyan Wen, and Jitao Sang. Agent models: Inter-
nalizing chain-of-action generation into reasoning models. arXiv preprint arXiv:2503.06580 ,
2025.
[81] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and
Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world
environments. arXiv preprint arXiv:2504.03160 , 2025.
17

--- PAGE 18 ---
Appendix
A Supplement Experimental Results 19
A.1 Deep Web Exploration Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.2 Different RL Algorithm: REINFORCE++ VS. GRPO . . . . . . . . . . . . . . . . 19
A.3 Analysis of Tool-Call Masking . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B Details of Tool-Star Framework 20
B.1 Training Algorithm of Tool-Star . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.2 Details of Tool Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C Implementation Details 22
C.1 The Statistics of Data Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.2 Training Details of Tool-Star . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.3 Details of Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.4 Baseline Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.5 Datasets Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D Instruction Templates 25
D.1 Instructions for Tool-Star . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.2 Instructions for Code Debugger . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.3 Instructions for Reasoning Chain Refiner . . . . . . . . . . . . . . . . . . . . . . . 26
D.4 Instructions for Standard RAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
D.5 Instructions for Knowledge-Intensive Reasoning Model . . . . . . . . . . . . . . . 26
D.6 Instructions for Computational Reasoning Model . . . . . . . . . . . . . . . . . . 26
D.6.1 Instructions for Direct Sampling and Reasoning Computational Datasets . . 26
D.6.2 Instructions for Direct Sampling and Reasoning Knowledge-Intensive Datasets 26
E Case Study 27
E.1 TIR Cases of Tool-Star . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
E.2 Cases of Self-Critic DPO Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
F Limitations and Future Directions 27
G Broader Impacts 28
18

--- PAGE 19 ---
Table 3: The performance of Tool-Star and other baseline methods on the GAIA and HLE datasets.
The experimental settings are consistent with those in Table 1. Here, Qwen refers to the use of the
Qwen2.5-3B-Instruct model, and Llama refers to the use of the Llama3.2-3B-Instruct model.
Dataset Qwen Llama ToRL DotaMath RAG Search-o1 Search-R1 Multi-Tool Prompting Tool-Star
GAIA 3.9 5.8 8.7 4.9 8.7 7.8 9.7 5.8 15.5
HLE 7.4 5.0 7.2 4.8 7.2 6.2 7.8 5.6 8.0
A Supplement Experimental Results
A.1 Deep Web Exploration Result
In this section, we further evaluate the performance of the Tool-Star framework on web exploraton
tasks. Specifically, we assess it on the General AI Assistants (GAIA) andHumanity’s Last Exam
(HLE) datasets with the same experimental settings as in main result.
As shown in Table 3, In detail, we observe that the Tool-Star framework achieves strong performance
even on web search tasks. Specifically, on the GAIA dataset, our method achieves the highest score,
outperforming the second-best by over 60%. On the HLE dataset, our method also demonstrates
strong competitiveness. This fully validates the effectiveness of our approach in the deep web
exploration task.
In terms of task difficulty, GAIA consists of complex deep web exploration tasks that typically require
both information retrieval and reasoning. To address such challenges, Tool-Star often employs a
multi-tool collaboration strategy to accomplish the subtasks. Examples of multi-tool collaborative
reasoning are shown in Table 6.
A.2 Different RL Algorithm: REINFORCE++ VS. GRPO
Table 4: Performance comparison of RL Algo-
rithm on ToolStar (Qwen2.5-3B-Instruct).
Models GSM8K MATH HQA. Bamb.
Tool-Star (SFT) 80 79.2 47.5 48.3
+ GRPO 85.0 81 51.9 52.5
+ REINFORCE++ 84.4 80.5 48.8 53.1As two widely-used rule-based reinforcement
learning algorithms, Tool-Star with GRPO [ 51]
and Reinforce++ [ 20] are compared in Table 4 in
terms of training curves and experimental perfor-
mance. It can be seen that the overall differences
between GRPO and Reinforce++ are relatively
minor.
Experimental results show that Tool-Star with
GRPO slightly outperforms REINFORCE++ on
computational reasoning tasks such as GSM8K and MATH. However, when it comes to knowledge-
intensive reasoning tasks, REINFORCE++ slightly outperforms GRPO on Bamboogle. These results
suggest that while the performance of the two methods fluctuates slightly across different domains,
the overall difference remains within 3%.
A.3 Analysis of Tool-Call Masking
Figure 8: The result of tool-call masking
0510152025303540
Step1.0
0.8
0.6
0.4
0.2
0.00.20.4Val Score
Tool-Star(3B)
Tool-Star(3B) w/o maskIn our work, we apply masking to the retrieved and python
execution results when computing the loss to prevent the
training policy from being biased by tool outputs. There-
fore, all tool call results in Tool-Star are masked. Based
on this design, we conduct ablation studies on Tool-Star
(Qwen2.5-3B-Instruct) with or without tool-call masking
in RL training.
As shown in Figure 8, removing the masking leads to
a rapid collapse of the model’s reward score to -1, with
persistent strategy hacking. This stands in contrast to prior
findings in single-tool reinforcement learning [ 80], where,
19

--- PAGE 20 ---
Algorithm 1 Multi-Tool Self-Critic Reinforcement Learning
Require: Reasoning Model πθ, External Tools T, Reward Function Rϕ, Cycle Times C, GRPO
StepsS
1:Input: Datasets D, Task instruction I
2:Initialize πθold←πθ
3:fori← {1, ...,C}do
4: forstep← {1, ...,S}do ▷Start Vanilla RL phrase (GRPO)
5: Sample a batch Dbfrom D
6: foreach query q∈Dbdo
7: q←I⊕q
8: Sample Gsequences for qwithT:{oj}G
j=1∼πθ(· |q, T)
9: Calculate the optimization objective function Jθof GRPO according to Eq.(4)
10: Update the weight parameters of πθ:θ←θ+η· ∇θJθ
11: Randomly sample Dsample from D ▷ Start Self-Critic DPO
12: foreach query q∈Dsample do
13: q←I⊕q
14: Sample the query qforGtimes with Tto generate Gdifferent results: {oj}G
j=1∼πθ(· |
q, T)
15: Filter one correct result and one incorrect result from Gresults to obtain ochosen and
oreject
16: Calculate the optimization objective function Jθof DPO according to Eq.(5)
17: Update the weight parameters of πθ:θ←θ+η· ∇θJθ
18:Output: Trained Model πθnew
according to their experiments, masking in single-tool RL
did not lead to result hacking. We consider that maybe tool feedback in multi-tool reasoning can
introduce significantly greater instability during RL training.
B Details of Tool-Star Framework
B.1 Training Algorithm of Tool-Star
In this section, we delve into the overall workflow of the our Multi-Tool Self-Critic Reinforcement
Learning algorithm. The training process consists of two alternating phases over multiple cycles,
which are illustrated in algorithm 1:
(1) Vanilla RL stage: In each cycle, the model πθis first optimized via GRPO for a fixed number
of steps. For each query sampled from the dataset, the task instruction is prepended, and the model
generates multiple output sequences using external tools. These outputs are used to compute the
GRPO objective (Eq. 4), and the model parameters are updated accordingly.
(2) Self-Critic DPO stage: After GRPO, a self-critic phase is performed to refine the model’s
decision-making. For each sampled query, the model generates multiple outputs, from which one
preferred (correct) and one rejected (incorrect) response are selected. These are used to compute the
DPO objective (Eq. 5), further updating the model via gradient ascent.
This alternating training scheme enables the model to not only improve through reinforcement
learning with tool-augmented outputs but also self-correct by distinguishing between good and bad
generations. The final output is a tool-aware reasoning model πnew
θoptimized for both performance
and efficiency.
B.2 Details of Tool Design
To provide a more detailed illustration of the design and usage guideline of our tools, we present the
specifics of both the training tools and inference-time tools below.
Training Tools:
20

--- PAGE 21 ---
Table 5: The statistics of data sources in our main result.
Dataset Type Task # Counts
NuminaMath Language-only Computational Reasoning 72.4K
HotpotQA [53] Language-only Knowledge-Intensive Reasoning 8.1k
2WikiMultiHopQA [19] Language-only Knowledge-Intensive Reasoning 10k
SimpleDeepSearch [58] TIR Knowledge-Intensive Reasoning 0.9k
WebWalker [68] Language-only Knowledge-Intensive Reasoning 3k
•Search Engine: During inference, the search engine executes queries to retrieve relevant infor-
mation, supporting both local and web-based retrieval modes. Inspired by DeepResearch [ 81],
we adopt web search during training. Empirically, we find that both using webpage snippets
and incorporating a browser agent consistently improve RL performance. However, the former
significantly reduces efficiency. During evaluation, we use Wikipedia5as the retrieval corpus and
E5 [65] as the retriever for four open-domain QA tasks. For all other tasks, we combine web search
with the browser agent to obtain higher-quality retrieval results.
•Web Browser Agent: This component accesses URLs from search results, parses webpage
content, extracts information relevant to the query, and summarizes it. The process includes
webpage parsing, regex-based cleaning, and an auxiliary model that refines the cleaned content
based on the search query. The summarized output is then fed back into the reasoning chain.
Notably, the browser agent is optional—when it is not used, the search engine directly returns
webpage snippets or raw text.
•Code Interpreter: Code snippets generated by the language model are executed in a sandboxed
environment, returning either the execution result or error messages based on correctness. Our
Python interpreter implementation follows the design of ToRA [ 17], ensuring secure and accurate
execution.
Inference-time tool:
•Code Debugger: This module automatically corrects erroneous code generated by large language
models and guides the code revision process. Specifically, we design a dedicated debug prompt
( D.2) that incorporates the model-generated code and compiler error messages, enabling the model
to understand and fix the errors for automated code correction.
•Tool-Use Backtracer: This module identifies and reverts to the reasoning step prior to a failed tool
call, allowing the model to recover and revise its reasoning path. In real experiments, we compared
model-based and rule-based rewinding strategies. As the model often struggles to accurately locate
the faulty tool call, we adopt the rule-based approach. Concretely, we set the rewind position at
the newline token preceding the special token of the erroneous tool call, prompting the model to
regenerate the reasoning chain from that point.
•Reasoning Chain Refiner: When the model output exceeds the maximum length limit, this
module prunes and refines redundant steps in the reasoning process, replacing the original chain
with a more concise and coherent version to complete the task. We design a refinement prompt
( D.3) that enables the model to identify and remove errors or repetitive thoughts in the original
reasoning chain and generate a streamlined version. Template examples can be found in Table D.3.
To ensure fairness, all deployed models of tools in the experiments (e.g., the browser agent’s
information refinement process, code debugger, and Reasoning Chain Refiner) use instruct-version
models with the same parameter size as the test model. Therefore, our inference-time tool can also be
regarded as a self-play inference-time scaling process.
5https://archive.org/details/enwiki-20181220
21

--- PAGE 22 ---
C Implementation Details
C.1 The Statistics of Data Sources
In this section, we sample data from sources including Numina-MATH, 2Wiki, HotpotQA, with
the detailed breakdown and corresponding data sizes presented in Table 5. We strictly adhere to all
copyright and licensing terms associated with the datasets and open-source code.
For the computational reasoning dataset, we use 72.4K samples aligned with Numina-MATH TIR.
For knowledge-based reasoning, we include 10.6K samples from HotpotQA and 2Wiki, aligned
with R1 Searcher and Research respectively, along with 0.9K samples from the SimpleDeepSearch
dataset and 3K samples randomly selected from the Webwalker training set. Finally, we obtained
approximately 100K samples containing 1% TIR data.
It is important to note that we only use the queries from these language-only datasets for reasoning
chain sampling, without relying on the reasoning chains generated by stronger models of original
datasets. Our cold-start data is sampled using Qwen2.5-3B-Instruct, which serves as the backbone
model to match the parameter scale of the main table. Following the START settings [ 28], we set
the temperature to 0.7 and the top-p value to 0.95 during sampling. Due to resource constraints,
each question is sampled three times only. In the tool-use quality normalization stage, our tool-call
frequency threshold βis set to 5.
Ultimately, following multiple stages of quality filtering in Section 3.1, we curated approximately
54K samples from categories 1, 2, and 3 for cold-start training. Furthermore, 10K challenging
samples were randomly selected from categories-4 samples for the RL phase to enable progressive
alignment from simpler to more difficult tasks.
C.2 Training Details of Tool-Star
Cold-Start Supervised Finetuning. After applying our sampling strategies to the original datasets,
we use approximately 54k samples for cold-start supervised fine-tuning. During the supervised
fine-tuning stage, we fine-tune a Qwen2.5-3B-Instruct model on the training data with a learning rate
of 7e-6. We utilize DeepSpeed ZeRO-3 [ 50] and FlashAttention2 [ 8] for training. The batch size is
set to 128, weight decay to 0.1, and we train for 3 epoch. Mixed precision training with BF16 is
adopted, and the maximum input length is set to 4096.
Multi-Tool Self-Critic Reinforcement Learning. After SFT, we further train the model using the
GRPO algorithm under the VERL framework6. Similarly, we randomly select around 10k samples
from DRL
toolfor reinforcement learning. The learning rate is set to 8, with 8 rollouts per sample. The
total training batch size is 128, and the mini-batch size is 16. The maximum output length is set to
4096 to ensure completeness of exploration. To accelerate the training speed, following DAPO [ 77],
we set the maximum number of tool invocations during training to 3. To stabilize training, we set the
KL divergence coefficient in GRPO to 0. The reinforcement learning phase lasts for 2 epochs. All
experiments are conducted on 8 NVIDIA A800 GPUs. To ensure reproducibility, all experimental
results are averages of three independent trials. We have approximately 300 samples in the validation
set during the RL stage, which includes AIME25, 25, MATH500, HotpotQA, 2wikiMultiHopQA,
MuSiQue and Bamboogle.
It is worth noting that all tool invocation results in Tool-Star are masked . As illustrated in the
Figure 3, these masked segments are excluded from the loss computation to prevent the training
policy from being biased toward the tool outputs. Only the tokens from the text-based reasoning
process and tool requests are considered during loss calculation.
Self-Critic DPO Training. In this phase, the learning rate is set to 5e-7 with a cosine scheduler
and a 0.1 warm-up ratio. We use DeepSpeed ZeRO Stage 3 and Flash-Attention 2 for efficiency, with
a global batch size of 64. Training utilizes a sigmoid loss function with a beta value of 0.3 and spans
2 epochs. Mixed precision training with bf16 is employed, and the maximum context length is 4096
tokens. We perform self-critic training every 0.5 epoch during the standard reinforcement learning
6https://github.com/volcengine/verl
22

--- PAGE 23 ---
process (approximately every 50 steps), and use the resulting model to resume the subsequent vanilla
reinforcement learning training. The detailed data format are listed in Appendix E.2
C.3 Details of Retrieval
During training, we use the Bing Web Search API as the retriever, configured with the US-EN region
setting. For each query, we retrieve 10 web pages as supporting documents. During inference, we
use Bing Web Search for the Math Reasoning and Complex Reasoning datasets. For open-domain
QA, we employ Wikipedia as the retriever. The Wikipedia retriever is based on the Wikipedia 2018
Corpus provided by FlashRAG [24] and uses E5-base-v2 as the retriever.
C.4 Baseline Details
Proprietary Models
•Qwen2.5 Series [70,49], developed by Alibaba, is available in several sizes: Qwen2.5-0.5B /1.5B
/7B and 72B. This model is trained on data sources spanning 29 kinds of languages, enabling
it to perform exceptionally well in multilingual tasks. Additionally, Qwen2.5 exhibits strong
capabilities in coding and mathematics. Qwen2.5-72B-Instruct is notable for its ability to handle
input windows of up to 128K tokens in length, making it exceptionally well-suited for processing
long texts and tackling complex tasks.
•LLaMA 3 Series [14], developed by Meta AI, is released in multiple sub-series, including LLaMA
3, LLaMA 3.1, LLaMA 3.2 and LLaMA 3.3. LLaMA series models offer two primary variants
for each model size: the base model and the instruct model. These models are trained on a
mixture of datasets, covering a wide range of domains and languages. The LLaMA series models
achieve strong performance across reasoning, coding, and multilingual understanding benchmarks.
With continued improvements in alignment and context length, they are increasingly suitable for
real-world tasks.
Search-Enhanced Baselines
•RAG [ 26]is a basic baseline that demonstrates the model’s ability to directly utilize retrieved
information. In our implementation, we directly used the retriever to obtain the top-5 results and
inserted them into a carefully designed prompt to let the model generate the answer.
•Search-o1 [ 31]is a reasoning-model-driven RAG method. Its core idea is to allow the reasoning
model to autonomously decide whether to perform a search during its reasoning process. It
generates queries, retrieves relevant information, organizes the retrieved results, and incorporates
them into the model’s reasoning chain. The model then continues generation based on this chain
until a final answer is reached. In our experiments, we modified the official code of Search-o1 by
replacing its base model with one that has the same number of parameters as ours, and we also
used the same retriever to ensure consistency.
•Search-R1 [ 23]is a reinforcement learning method that trains a model from scratch to call the
search engine during reasoning. Its overall process is similar to Search-o1 [ 31]: the model generates
necessary queries during reasoning, retrieves relevant information, and inserts it into the reasoning
chain. We follow the original prompt used in the method to ensure its effectiveness.
Code-Enhanced Baselines
•ToRL [ 35]is a tool-integrated framework trained with reinforcement learning methods. This
framework enables the model to autonomously invoke code interpreter to solve mathematical
reasoning problems. Compared to prompt-based methods and supervised fine-tuning training
approaches, ToRL encourages the model to explore extensively to find optimal tool usage strategies.
This approach shows significant improvements over traditional methods. At the same time, the
framework demonstrates the broad potential applications of tool integration in mathematical
reasoning.
•DotaMath [ 27]models are fine-tuned on mathematical instruction datasets. After training, the
DotaMath model demonstrates strong capabilities in solving complex mathematical reasoning
problems. By training on the DotaMathQA dataset built for mathematical problems, the model
23

--- PAGE 24 ---
has acquired the abilities of thought decomposition and self-correction during reasoning. This
paradigm opens up a new direction for solving complex math problems.
Multi-tool Enhanced Baselines
•Multi-Tool Prompting method refers to the approach where the model learns to use external tools
to solve problems solely through prompts. In this setup, we primarily aim to explore whether the
model has the capability to autonomously call relevant tools to complete tasks. For the prompt
format, we use the same structure as the one employed by Tool-Star. The specific prompt can be
found in Appendix D.1.
•ReCall [ 1]method makes significant improvements over the ReSearch framework. Specifically,
compared to the purely search-based framework, it further allows the model to call code interpreter
to solve complex reasoning problems. This setup expands the tasks that ReSearch could handle to
mathematical reasoning, while still maintaining the original capability of ReSearch to call search
engines. Additionally, ReCall introduces a new method for generating data towards complex
multi-step tasks, enabling the LLM to develop sophisticated reasoning abilities based on multiple
tools.
C.5 Datasets Details
We provide a detailed introduction to the computational reasoning benchmarks evaluated by Tool-Star:
•AIME247A dataset of 30 challenging math problems from the 2024 American Invitational Mathe-
matics Examination (AIME I & II), covering algebra, geometry, number theory, and combinatorics.
Each JSONL-formatted entry includes the problem ID, statement, detailed solution steps, and the
final integer answer. Designed to evaluate large language models on complex mathematical reason-
ing, the dataset features high difficulty, rigorous multi-step solutions, and unique answers—making
it a strong benchmark for assessing advanced problem-solving capabilities.
•AIME258is a dataset comprising 30 challenging mathematical problems sourced from the 2025
American Invitational Mathematics Examination (AIME I & II). These problems encompass a
diverse range of topics, including algebra, combinatorics, geometry, number theory, and probability.
AIME25 is designed to evaluate the mathematical reasoning capabilities of LLMs. It serves as
a benchmark for assessing how well these models can tackle complex mathematical problems
requiring multi-step reasoning and understanding of advanced mathematical concepts.
•GSM8K [ 5]is a widely adopted benchmark for assessing the mathematical reasoning abilities of
large language models through grade-school level word problems. In our experiments, we utilize
its standard test set and report performance based on greedy decoding accuracy (maj@1).
•MATH [ 18]is a dataset consisting of challenging high school-level mathematics problems, span-
ning topics such as Prealgebra, Algebra, Number Theory, Counting & Probability, Geometry,
Intermediate Algebra, and Precalculus. Compared to GSM8K, MATH offers greater difficulty and
topic diversity.
•MATH500 [ 37]is a subset of 500 problems drawn from the MATH benchmark developed by
OpenAI. This dataset is designed to assess the mathematical reasoning capabilities of large
language models. The problems in MATH500 cover a range of mathematical topics including
algebra, geometry, trigonometry, and probability. Each entry in the dataset includes the problem
statement, a detailed step-by-step solution, and the final answer, allowing for thorough model
training and evaluation.
Furthermore, we also conduct a series experiments on follow knowledge-intensive reasoning bench-
marks:
•WebWalker [ 68]s a dataset consisting of queries derived from various web pages, where answering
a single query may require gathering information from one or multiple pages. In our experiments,
we adopt an open-domain setting by using a search engine for retrieval rather than the root URLs
provided in the dataset.
7https://huggingface.co/datasets/HuggingFaceH4/aime_2024
8https://huggingface.co/datasets/math-ai/aime25
24

--- PAGE 25 ---
•HotPotQA [ 73]is a widely-used question answering dataset that features multi-hop questions
built from Wikipedia articles. In our experiments, we use its validation set for evaluation.
•2WikiMultihopQA [ 19]is a large-scale multi-hop QA dataset that combines structured knowledge
from Wikidata and unstructured text from Wikipedia. It includes diverse question types and
annotated reasoning paths to support explainable multi-step inference.
•Bamboogle [ 44]is a challenging multi-hop QA dataset that contains hand-crafted questions
requiring up to four reasoning hops. It tests a model’s ability to perform complex and compositional
reasoning across multiple facts.
•Musique [ 63]is a multi hop dataset that requires inference. The problem inside is a dependency
based multi hop problem, where one inference step depends on the information obtained from
the previous inference step, making the dataset very difficult. We conducted experiments using
Musique’s dev set.
D Instruction Templates
D.1 Instructions for Tool-Star
Instruction for Tool-Star
You are a helpful assistant that can solve the given question step by step with the help of the wikipedia search
tool and python interpreter tool. Given a question, you need to first think about the reasoning process in the
mind and then provide the answer. During thinking, you can invoke the wikipedia search tool to search and
python interpreter tool to calculate the math problem for fact information about specific topics if needed. The
reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags respectively,
and the search query and result are enclosed within <search> </search> and <result> </result> tags respectively.
After receiving the search or python result, you should continue your reasoning process begin with <think>. For
example, <think> This is the reasoning process. </think> <search> search query here </search> <result> search
result here </result> <think> This is the reasoning process. </think> <python> python code here </python>
<result> python interpreter result here </result> <think> This is the reasoning process. </think> <answer> The
final answer is
answer here
</answer>. In the last part of the answer, the final exact answer is enclosed within \boxed{} with latex format.
D.2 Instructions for Code Debugger
Instruction for Code Debugger
You are a code expert. I need you to debug the following code. Below are the code originally generated by the
model and the error information that occurred during code execution. Please output ONLY the corrected Python
code, without any explanation or markdown formatting:
**Inputs:**
**Original Code:**
{code}
**Execution Error:**
{error}
Output the corrected Python code only, without any explanation or markdown formatting:
25

--- PAGE 26 ---
D.3 Instructions for Reasoning Chain Refiner
Instruction for Reasoning Chain Refiner
You are an expert in response refinement. Given a prompt and its corresponding response, your task is to
compress and restructure the response by removing redundant, repetitive, or irrelevant content. Preserve all key
information needed to directly and accurately address the original prompt. Only output your revised response
and do not output anything else.
**Original Prompt:**
{prompt}
**Original Response:**
{response}
**Revised Response:**
D.4 Instructions for Standard RAG
Instruction for Standard RAG
Find the useful content from the provided documents, then answer the question. Answer the question directly.
Your response should be very concise. Please provide use ’So the final answer is:’ as a prefix for the final answer.
The following are given documents.
Documents: {reference}
Question: {question}
D.5 Instructions for Knowledge-Intensive Reasoning Model
Instruction for Sampling and Inferencing Knowledge-Intensive Datasets
You are a helpful assistant that can solve the given question step by step with the help of the python interpreter
tool. Given a question, you need to first think about the reasoning process in the mind and then provide the
answer. During thinking, you can invoke the python interpreter tool to calculate the math problem for fact
information about specific topics if needed. The reasoning process and answer are enclosed within <think>
</think> and <answer> </answer> tags respectively. For example, <think> This is the reasoning process.
</think> <python> python code here </python> <result> python interpreter result here </result> <think> This is
the reasoning process. </think> <answer> The final answer is
answer here
</answer>. In the last part of the answer, the final exact answer is enclosed within \boxed{} with latex format.
D.6 Instructions for Computational Reasoning Model
D.6.1 Instructions for Direct Sampling and Reasoning Computational Datasets
Instruction for Sampling and Reasoning Computational Datasets
A conversation between User and Assistant. The user asks a question, and the Assistant solves it. Please integrate
natural language reasoning with programs to solve the problem above, and put your final answer within \boxed{}.
{question}
D.6.2 Instructions for Direct Sampling and Reasoning Knowledge-Intensive Datasets
Instructions for Sampling and Reasoning Knowledge-Intensive Datasets
A conversation between User and Assistant. The user asks a question, and the Assistant solves it. Please refer to
the given document to solve the problem above, and put your final answer within \boxed{}.
Question: {question}
Reference documents: {reference}
26

--- PAGE 27 ---
E Case Study
E.1 TIR Cases of Tool-Star
To better understand how Tool-Star performs tool-based collaborative reasoning, we conducted a
case study to clearly illustrate its reasoning process. The model-generated think process are enclosed
within <think> and </think> . The model-generated search queries are enclosed within <search> and
</search> , and the model-generated Python codes are enclosed within <python> and </python> . Search
results, Python execution results and final answers are enclosed within <result> ,</result> ,<answer>
and </answer> . Our analysis reveals the following:
In terms of task difficulty, GAIA is complex deep web exploration tasks that typically involve both
information retrieval and reasoning. When handling such tasks, Tool-Star often adopts a multi-
tool collaboration strategy to complete the subtasks (as shown in the Table 6). In contrast, for
domain-specific datasets like HotpotQA and AIME, which are more focused on knowledge retrieval
or mathematical computation, Tool-Star can accurately identify the task type and select the most
appropriate single tool for problem-solving (as shown in the Table 7 & 8).
These observations demonstrate that Tool-Star can flexibly choose between tool combinations or
single tools based on task requirements, showing strong task understanding and tool selection
strategies. This highlights the effectiveness and autonomy of our approach in multi-tool collaborative
reasoning.
E.2 Cases of Self-Critic DPO Data
To more comprehensively illustrate the data format used in our self-critic reinforcement learning
framework, we provide several reward data example (positive & negative pairs).
As shown in the Table 10 to 16, we can observe that the question and reasoning chain in the
example are consistent with those in vanilla reinforcement learning data, ensuring no impact on
model performance during subsequent training. The only difference in our approach lies in the
addition of two key fields after the original reasoning chain: Reward principle andReward score .
The Reward principle is automatically generated by a rule-based reward function, which identifies
specific mismatches leading to the score, aiming to provide interpretability for the reward. For
instance, in one case, a reward score of -1 is assigned because the number of special tokens in the
model output does not align with the reference, resulting in a formatting inconsistency.
F Limitations and Future Directions
Despite our best efforts to optimize the overall Tool-Star pipeline, there remain certain limitations
and areas for improvement that warrant further exploration.
•Expanding Tool Diversity: Currently, the Tool-Star framework integrates three tools during
training and three tools during inference. Compared to prior work that relies on a single tool, our
approach represents a pioneering and timely attempt. However, given the growing diversity of
available tools, we believe many more remain to be explored for enhancing reasoning capabilities.
For instance, incorporating vision-language models (VLMs) as external tools could further unlock
visual understanding, and adopting the Model Context Protocol may enable more flexible tool
invocation. We plan to continue expanding the types and integration of tools in future work,
advancing multi-tool collaborative reasoning.
•Scaling Backbone Model Parameters: Due to limited computational resources and the time-
consuming rollout process in multi-tool reasoning, this work focuses on models with 0.5B, 1.5B,
and 3B parameters for RL training and evaluation. Nonetheless, our “Scaling Analysis on Parameter
Scales” (see Table 4) provides preliminary evidence of the Tool-Star framework’s scalability. In
future work, we aim to extend our experiments to larger models, such as 7B and 32B, to assess
generalization across more complex tasks and model capacities.
Outlook: Achieving reliable and robust step-by-step reasoning remains a challenging task that
demands continued research and innovation. We sincerely hope to collaborate with researchers in
27

--- PAGE 28 ---
the fields of alignment and reasoning to jointly address these shared challenges and further advance
tool-integrated reasoning systems.
G Broader Impacts
This study presents the Tool-Star framework, which empowers large language models with multi-tool
collaborative reasoning, enhancing their adaptability and effectiveness in complex tasks. Through
reinforcement learning and scalable data synthesis, Tool-Star offers a generalizable and flexible
reasoning paradigm, demonstrating strong performance across benchmarks and broad potential in
domains such as education, science, and engineering.
However, as models gain greater autonomy in tool use, concerns arise regarding inappropriate tool
selection, opaque reasoning, and bias propagation—especially in high-stakes scenarios. To ensure
responsible deployment, future development should prioritize controllability, interpretability, and
robust oversight, aligning technological progress with ethical standards and societal values.
28

--- PAGE 29 ---
Table 6: An example from Tool-Star on GAIA dataset, with special symbols used in think content,
search queries, Python codes, returned results and final answer highlighted with purple box ,green box ,
blue box and red box , respectively.
Example #1 from GAIA Dataset
Question:
The longest-lived vertebrate is named after an island. According to Wikipedia, what is the 2023 population of
that island, to the nearest thousand?
Labeled Answer:
56000
Model Output by Tool Star
<think> To find out which island this refers to and its corresponding population from 2020, I will start off by
identifying the longest-lived vertebrate mentioned in Wikipedia. </think>
<search> search query: What is the longest-lived vertebrate according to Wikipedia? </search>
<result>
An international team of scientists led by the University of Copenhagen and including the University of Oxford
has found that the Greenland shark has a life expectancy of at least 272 years. This discovery shows it is
the longest living vertebrate known to science, exceeding even bowhead whales, turtles and tortoises.“Our
results demonstrate that the Greenland shark is among the longest-lived vertebrate species surpassing even
the bowhead whale (Balaena mysticetus, estimated longevity of 211 years),” Dr. Nielsen and co-authors said.
“The life expectancy of the Greenland shark is exceeded only by that of the ocean quahog (Arctica islandica,
507 years).”
</result>
<think> Now having identified the longest-lived vertebrate - the Greenland Shark, my next task would be
finding out more details regarding the associated island specifically referred to in the context. Since the name
¨Greenland ¨directly connects to the island, let me proceed further with determining the population data related
to this location. </think>
<search> search query: Population of Greenland as per the latest available statistics </search>
<result>
The current population of Greenland is 55,764 as of April 26, 2025, based on interpolation of the latest
United Nations data. The population of Greenland is projected at 55,745, or 56k, as of July 1, 2025. The
total population in Greenland is projected at 55,840, or 56k people, for the year 2024. （***More reasoning
contentare omitted here***)
After the 1960 ´s, the population in the towns grew rapidly. As of January 1, 2023, the population of
Greenland amounted to 56,609. The majority of the population held the Danish citizenship, as the country is
an autonomous region of the Kingdom of...As of July 1, 2024 the population of Greenland reached 55,840, of
which 29,360 people are male and 26,480 female.
</result>
<think> Having gathered sufficient evidence through searching, it appears clear that the relevant piece of
information comes from the statement mentioning the population of Greenland as of January 1, 2023 being
approximately 56,609. Considering the requirement asks for rounding off to the nearest thousand, we should
round this figure accordingly. Let’s finalize the calculation and presentation of the result using Python code
for accuracy. </think>
<python>
# Given data
population = 55840
# Rounding to the nearest thousand
rounded_population = round (population, -3)
print (rounded_population)
</python>
<result>
56000
</result>
<think> After performing the necessary calculations and ensuring alignment with the required precision level,
the estimated population of Greenland to the nearest thousand is determined. </think>
<answer>
56000
</answer>
29

--- PAGE 30 ---
Table 7: An example from Tool-Star on HotpotQA dataset, with special symbols used in think content,
search queries, returned results and final answer highlighted with purple box ,green box and red box ,
respectively.
Example #2 from HotpotQA Dataset
Question:
D1NZ is a series based on what oversteering technique?
Labeled Answer:
Drifting
Model Output by Tool Star
<think> Okay, so I need to figure out what D1NZ is a series based on an oversteering technique. Hmm, first
off, I’m not too familiar with “D1NZ.” Let me start by breaking down the term.
The name might be related to some racing or driving terminology. The acronym could stand for something
like “Driving 1 NZ” or maybe it’s a play on words. Oversteer is when the rear wheels lose traction, causing
the car to slide outward. Common techniques include understeer (front losing grip) and oversteer. But how
does this relate to a series?
I should probably look up what D1NZ refers to. Maybe it’s a motorsport series? Like rallycross or drag
racing? Alternatively, perhaps it’s a video game series involving cars where oversteer is a key mechanic. Wait,
there was a game called “Need for Speed: Most Wanted,” but that’s more about drifting. Or maybe “Forza
Horizon”? Not sure.
Alternatively, “D1NZ” might be an abbreviation used in real-world racing categories. Let me try searching for
“D1NZ oversteering technique.” That seems direct.
Wait, another thought: sometimes series have names combining letters with numbers or other codes. Maybe D1
stands for something else. For example, in some contexts, D1 could refer to a division number. If it’s a racing
series, maybe D1 is a category, and NZ indicates New Zealand. So perhaps D1NZ is a New Zealand-based
racing series focused on oversteer techniques.
Let me structure my searches properly. First, check what D1NZ is. Search terms could be “What is D1NZ
racing series?” or “D1NZ oversteering technique.”
So I’ll start with the first search query: </think>
<search> D1NZ racing series oversteering technique </search>
<result>
“Drifting (motorsport)”
Drifting (motorsport) Drifting is a driving technique where the driver intentionally oversteers, with loss of
traction in the rear wheels or all tires, while maintaining control and driving the car through the entirety of a
corner. Car drifting is caused when the rear slip angle is greater than the front slip angle, to such an extent
that often the front wheels are pointing in the opposite direction to the turn (e.g. car is turning left, wheels
are pointed right or vice versa, also known as opposite lock or counter-steering). As a motoring discipline,
drifting competitions were first popularized in 1970s
（***More reasoning contentare omitted here***)
of Driftworks embarked on a journey to Japan where they would shoot a documentary about the local drifting
scene, which would become known to the public as “Outsiders”. Drifting (motorsport) Drifting is a driving
technique where the driver intentionally oversteers, with loss of traction in the rear wheels or all tires, while
maintaining control and driving the car through the entirety of a corner. Car drifting is caused when the rear
slip angle is greater than the front slip angle, to such an extent that often the front wheels are pointing in the
opposite direction to the turn (e.g. car
</result>
<think> From the search results, it looks like D1NZ is a New Zealand drift racing series. The description
mentions that it’s a production car drifting series, focusing on oversteer techniques. Specifically, the explanation
talks about intentional oversteer maneuvers where the driver loses traction in the rear wheels, allowing the
car to spin or slide around corners. This aligns with the concept of drift racing, which relies heavily on
oversteer to execute controlled slides and spins. Therefore, the oversteering technique associated with D1NZ
must involve intentionally inducing rear-wheel loss of traction to achieve controlled drifts during races or
competitions. </think>
<answer>
The final answer is drifting
</answer>
30

--- PAGE 31 ---
Table 8: An example from Tool-Star on AIME24 dataset.
Example #3 from AIME24 Dataset
Question:
Every morning Aya goes for a 9-kilometer-long walk and stops at a coffee shop afterwards. When she walks
at a constant speed of skilometers per hour, the walk takes her 4 hours, including tminutes spent in the
coffee shop. When she walks s+ 2kilometers per hour, the walk takes her 2 hours and 24 minutes, including
tminutes spent in the coffee shop. Suppose Aya walks at s+1
2kilometers per hour. Find the number of
minutes the walk takes her, including the tminutes spent in the coffee shop.
Labeled Answer:
204
Model Output by Tool Star
<think> To solve this problem, we need to set up a system of equations based on the information provided.
Let’s break down the problem step-by-step.
1. **Define Variables:**
-s: Aya’s walking speed in kilometers per hour.
-t: Time spent in the coffee shop in minutes (which needs to be converted to hours since the other times are in
hours).
- Distance walked each time: 9 kilometers.
2. **Formulate Equations Based on Given Information:**
- When Aya walks at skm/h, the total time taken is 4 hours, which includes tminutes spent in the coffee shop.
Converting tminutes to hours, we gett
60. So the equation becomes:
9
s+t
60= 4
- When Aya walks at s+ 2km/h, the total time taken is 2 hours and 24 minutes, which is 2 +24
60= 2.4hours.
Converting tminutes to hours again, we gett
60. So the equation becomes:
9
s+ 2+t
60= 2.4
3. **Solve the System of Equations:**
We now have two equations:(
9
s+t
60= 4
9
s+2+t
60= 2.4
Let’s use Python to solve these equations. </think>
<python>
from sympy import symbols, Eq, solve
# Define the variables
s, t = symbols(’s t’)
# Define the equations based on the given information
eq1 = Eq(9/s + t/60, 4)
eq2 = Eq(9/(s + 2) + t/60, 2.4)
# Solve the system of equations
solution = solve((eq1, eq2), (s, t))
print (solution)
</python>
<result>
(−4.50000000000000 ,360.000000000000) ,(2.50000000000000 ,24.0000000000000)
</result>
<think>
（***More reasoning contentare omitted here***)
</think>
<answer>
The final answer is 204
</answer>
31

--- PAGE 32 ---
Table 9: Example 1 with positive output of our self-critic reward finetuning dataset, including the
question, model output, corresponding reward principle and reward score. The special symbols used
in think content, Python codes, returned results and final answer highlighted with purple box ,blue box
and red box , respectively.
Example #1 from Training Dataset
Question:
Find the sum1
1·3+1
3·5+···+1
(2n−1)(2n+1)+···+1
255·257. The original answer is ink
mformat, where
the fraction is in its simplest form. Please give the value of k+m.
Labeled Answer:
385
Positive Output of Tool-Star
<think> Okay, so I need to find the sum of this series: 1/(1·3) + 1 /(3·5) +···+ 1/(255·257) . Hmm, the
general term seems to be 1/[(2n−1)(2n+ 1)] and we are summing up to some nwhere 2n−1 = 255 . Let
me figure out how many terms there are first. If the last term is 1/(255·257) , then 2n−1 = 255 , so solving
forn:</think>
<python>
n = (255 + 1) // 2
print (n)
</python>
<result> 128 </result>
<think> So there are 128 terms in total. Now, I remember that for fractions like 1/[(2n−1)(2n+ 1)] , partial
fractions might be helpful here. Let me try to decompose the general term. Suppose that:
1
(2n−1)(2n+1)=A
2n−1+B
2n+1
Multiplying both sides by (2n−1)(2n+ 1) gives:
1 =A(2n+ 1) + B(2n−1)
***More reasoning contents are omitted here***
<python>
total = (1 - 1/257) / 2
numerator = total.numerator
denominator = total.denominator
print (f"{numerator}/{denominator}")
</python>
<result> 128/257 </result>
<think> So the sum is 128/257. Let me check if this can be simplified. </think>
<python>
from fractions import Fraction
frac = Fraction(128, 257)
print (f"Simplified: {frac.numerator}/{frac.denominator}")
</python>
<result> Simplified: 128/257 </result>
<think> Since 128 and 257 share no common divisors other than 1, the fraction is already in simplest form.
Therefore, the answer is 128/257, andk+m= 128 + 257 = 385 .</think>
<answer> 385 </answer>
Reward Principle of Output
The response format is correct. The final answer is correct. The reasoning chain contains single tool usage.
Reward Score of Output
So the reward score is: 1
32

--- PAGE 33 ---
Table 10: Example 1 with negative output of our self-critic reward finetuning dataset, including the
question, model output, corresponding reward principle and reward score. The special symbols used
in think content, Python codes, returned results and final answer highlighted with purple box ,blue box
and red box , respectively.
Example #1 from Training Dataset
Question:
Find the sum1
1·3+1
3·5+···+1
(2n−1)(2n+1)+···+1
255·257. The original answer is ink
mformat, where
the fraction is in its simplest form. Please give the value of k+m.
Labeled Answer:
385
Positive Output of Tool-Star
<think> Okay, so I have this series to compute:1
1·3+1
3·5+···+1
(2n−1)(2n+1)+···+1
255·257. And the
problem says that the answer should be in the formk
m, which is already simplified, and we need to find
k+m. Alright, let’s start breaking this down. First, notice that each term in the series has the denominator as
a product of two consecutive odd numbers. Specifically, the general term seems like1
(2n−1)(2n+1). Maybe
there’s a way to simplify this term using partial fractions?
Let me recall: when dealing with denominators like (a−b)(a+b), we can use partial fractions to split
it into simpler terms. So,1
(2n−1)(2n+1)=A
2n−1+B
2n+1. To find AandB, I’ll combine the right-hand
side over a common denominator:A(2n+1)+ B(2n−1)
(2n−1)(2n+1)=1
(2n−1)(2n+1). So the numerator must equal 1:
A(2n+ 1) + B(2n−1) = 1 . Expanding this gives: 2An+A+ 2Bn−B= 1, which simplifies to
(2A+ 2B)n+ (A−B) = 1 .
For this equation to hold true for all n, the coefficients of nand the constant term must both match on both
sides. Therefore, we get the system of equations: (1) 2A+ 2B= 0, and (2) A−B= 1. From equation (1),
we get A=−B. Substituting into equation (2): −B−B= 1implies −2B= 1, soB=−1
2andA=1
2.
Therefore, the partial fraction decomposition is:1
(2n−1)(2n+1)=1
2
1
2n−1−1
2n+1
. This is much cleaner!
Each term can be rewritten as half the difference between reciprocals of adjacent odd numbers.
Now, the entire series becomes:P128
n=11
(2n−1)(2n+1)=1
2P128
n=1
1
2n−1−1
2n+1
. This looks like a tele-
scoping series! When expanded, many terms will cancel out.
Let’s verify: when written out, the series is1
1·3+1
3·5+···+1
255·257, and using the telescoping identity, this
becomes: </think>
<python>
# Calculate the sum using telescoping series formula
total_terms = 128
term_start = 1
term_end = 257
# Compute the telescoping sum
telescoping_sum = (1 / term_start) - (1 / term_end)
print (f"Telescoping sum: {telescoping_sum}")
</python>
<result> Telescoping sum: 0.9961089494163424 </result>
<think> The result from the code execution shows the telescoping sum converges to approximately
0.9961089494163424 , which is very close to256
257. Thus, the sum simplifies to256
257. Checking the code
confirms this approximation is accurate enough, considering floating-point precision limitations. Finally, since
256 and 257 are coprime (their greatest common divisor is 1), the fraction256
257is already in its simplest form.
Hence, k= 256 andm= 257 , leading to k+m= 513 .</think>
<answer> 513 </answer>
Reward Principle of Output
The response format is correct. The answer is incorrect.
Reward Score of Output
So the reward score is: 0
33

--- PAGE 34 ---
Table 11: Example 2 with positive output of our self-critic reward finetuning dataset, including the
question, model output, corresponding reward principle and reward score. The special symbols used
in think content, Python codes, returned results and final answer highlighted with purple box ,blue box
and red box , respectively.
Example #1 from Training Dataset
Question:
How many four-digit positive integers are multiples of 7?
Labeled Answer:
1286
Positive Output of Tool-Star
<think> To determine how many four-digit positive integers are multiples of 7, we consider all integers from
1000 to 9999 and find those divisible by 7. The smallest four-digit number is 1000. Since 1000÷7≈142.857,
the smallest multiple of 7 greater than or equal to 1000 is 143×7 = 1001 . Similarly, the largest four-digit
number is 9999. Since 9999÷7≈1428.42857 , the largest multiple of 7 less than or equal to 9999 is
1428×7 = 9996 . These numbers form an arithmetic sequence starting at 1001 and ending at 9996, with a
common difference of 7. The total number of terms is given by the formula n=an−a1
d+1, where a1= 1001 ,
an= 9996 , andd= 7. Substituting, we get n=9996−1001
7+ 1 =8995
7+ 1 = 1285 + 1 = 1286 .</think>
<python>
# Calculate the smallest four-digit multiple of 7
smallest_multiple = (1000 + 6) // 7 *7
# Calculate the largest four-digit multiple of 7
largest_multiple = (9999 // 7) *7
# Calculate the number of four-digit multiples of 7
number_of_multiples = (largest_multiple - smallest_multiple) // 7 + 1
print (number_of_multiples)
</python>
<result> 1286 </result>
<answer> The number of four-digit positive integers that are multiples of 7 is 1286 .</answer>
Reward Principle of Output
The response format is correct. The final answer is correct. The reasoning chain contains single tool usage.
Reward Score of Output
So the reward score is: 1
34

--- PAGE 35 ---
Table 12: Example 2 with negative output of our self-critic reward finetuning dataset, including the
question, model output, corresponding reward principle and reward score. The special symbols used
in think content, Python codes, returned results and final answer highlighted with purple box ,blue box
and red box , respectively.
Example #1 from Training Dataset
Question:
How many four-digit positive integers are multiples of 7?
Labeled Answer:
1286
Positive Output of Tool-Star
<think> Okay, so I need to figure out how many four-digit positive integers are multiples of 7. A four-digit
number ranges from 1000 to 9999. The smallest four-digit multiple of 7 should be 1001, since 1000÷7≈
142.857, and rounding up gives 143×7 = 1001 . The largest one would be 9996, since 9999÷7≈1428.428,
and rounding down gives 1428×7 = 9996 . So, we have the range starting at 1001 and ending at 9996
inclusive. Now, the problem reduces to finding the number of terms in an arithmetic sequence where the
first term is 1001, the common difference is 7, and the last term is 9996. The formula for the nth term of an
arithmetic sequence is an=a1+ (n−1)d, where anis the last term, a1is the first term, dis the common
difference, and nis the number of terms. Solving for n, we get n=an−a1
d+ 1. Substituting the values
an= 9996 ,a1= 1001 , andd= 7, we get n=9996−1001
7+ 1 =8995
7+ 1 = 1285 + 1 = 1286 .</think>
<python> <python>
# Calculate the number of terms in the arithmetic sequence
first_term = 1001
last_term = 9998
common_difference = 7
n = ((last_term - first_term) // common_difference) + 1
print (n)
</python>
<result> 1286 </result>
<answer> The calculation shows there are 1286 four-digit multiples of 7 between 1000 and 9999. </answer>
Reward Principle of Output
The response format is incorrect, <python> and </python> are not matched.
Reward Score of Output
So the reward score is: -1
35

--- PAGE 36 ---
Table 13: Example 3 with positive output of our self-critic reward finetuning dataset, including the
question, model output, corresponding reward principle and reward score. The special symbols used
in think content, search queries, returned results and final answer highlighted with purple box ,blue box
and red box , respectively.
Example #1 from Training Dataset
Question:
The main symptoms of overtraining syndrome in athletes include which of the following?
A. Improved Fitness
B. Mental Clarity
C. Muscle Strength
D. Weight Gain
Labeled Answer:
D
Positive Output of Tool-Star
<think> Okay, so the question is about the main symptoms of overtraining syndrome in athletes, and the options
are A through D. Overtraining syndrome occurs when an athlete trains excessively without adequate recovery,
leading to negative effects on performance and health. The key is to identify which of the given options are
consistent with known symptoms. Option A is Improved Fitness, which contradicts the nature of overtraining,
since performance typically declines. Option B is Mental Clarity, but overtraining is associated with mental
fatigue, irritability, and poor concentration, so mental clarity is unlikely. Option C is Muscle Strength, but
chronic overtraining impairs recovery and can lead to decreased strength rather than improvement. Option D is
Weight Gain, which at first seems counterintuitive, as overtraining might lead to weight loss due to increased
energy expenditure. However, hormonal imbalances and stress responses could result in fluid retention or
metabolic changes that cause weight gain. To confirm, I should search for the main symptoms of overtraining
syndrome. </think> <search> overtraining syndrome main symptoms </search>
<result> Overtraining syndrome main symptoms include: fatigue, persistent muscle soreness, unexplained
weight loss or gain, increased susceptibility to illness and injury, gastrointestinal issues, sleep disturbances,
elevated resting heart rate, decreased maximal heart rate during exercise, psychological symptoms like
anxiety, irritability, depression, burnout, mood swings, emotional exhaustion, and performance decline. These
symptoms worsen with inadequate recovery and may require medical evaluation. </result>
<think> Based on the search result, overtraining syndrome includes both physical and psychological symptoms.
Among the physical symptoms, unexplained weight gain is explicitly mentioned, making Option D valid.
Option A (Improved Fitness) contradicts the performance decline seen in overtraining. Option B (Mental
Clarity) is inconsistent with the psychological symptoms like irritability and burnout. Option C (Muscle
Strength) is unlikely, as overtraining typically leads to decreased strength due to insufficient recovery. Since
weight gain is a listed symptom, and the others are not, the correct answer is D. </think>
<answer> D </answer>
Reward Principle of Output
The response format is correct. The final answer is correct. The reasoning chain contains single tool usage.
Reward Score of Output
So the reward score is: 1
36

--- PAGE 37 ---
Table 14: Example 3 with negative output of our self-critic reward finetuning dataset, including the
question, model output, corresponding reward principle and reward score. The special symbols used
in think content, search queries, returned results and final answer highlighted with purple box ,blue box
and red box , respectively.
Example #1 from Training Dataset
Question:
The main symptoms of overtraining syndrome in athletes include which of the following?
A. Improved Fitness
B. Mental Clarity
C. Muscle Strength
D. Weight Gain
Labeled Answer:
D
Positive Output of Tool-Star
<think> In this case study, we are tasked with identifying the main symptom of overtraining syndrome among
four options: A through D. Overtraining syndrome occurs when an athlete engages in excessive training
without adequate recovery, leading to a decline in performance and various physical and psychological issues.
Let’s analyze each option:
Option A: "Improved Fitness" — This is generally a result of balanced training and recovery. However,
overtraining leads to a plateau or decline in fitness, not improvement. Therefore, improved fitness is not a
symptom of overtraining.
Option B: "Mental Clarity" — Overtraining is associated with mood disturbances, irritability, anxiety, and
difficulty concentrating. These symptoms reflect a decline in mental clarity, making "mental clarity" an
unlikely symptom.
Option C: "Muscle Strength" — While proper training enhances muscle strength, overtraining without
sufficient recovery can lead to muscle breakdown and decreased strength. Thus, increased muscle strength is
not a symptom of overtraining.
Option D: "Weight Gain" — This is more nuanced. Overtraining can sometimes lead to hormonal imbalances,
particularly elevated cortisol, which may result in fluid retention or increased fat storage. Additionally, some
athletes may overeat in response to chronic fatigue or stress, potentially leading to weight gain. While not the
most common symptom, weight gain can occur in certain cases.
***More reasoning contents are omitted here, over 15K tokens***
To verify, key symptoms of overtraining syndrome include: persistent fatigue, decreased performance, mood
changes (e.g., depression, irritability), sleep disturbances, elevated resting heart rate, frequent illnesses, and
hormonal imbalances. None of the options perfectly match these core symptoms, but among the given choices,
weight gain (D) is the most plausible indirect symptom, while the others describe positive outcomes that are
contrary to overtraining effects.
Therefore, based on the evidence and reasoning, the best answer is: </think>
<answer> D </answer>
Reward Principle of Output
The response format is incorrect, the response over max length.
Reward Score of Output
So the reward score is: -1
37

--- PAGE 38 ---
Table 15: Example 4 with positive output of our self-critic reward finetuning dataset, including the
question, model output, corresponding reward principle and reward score. The special symbols used
in think content, search queries, Python codes, returned results and final answer highlighted with
purple box ,green box ,blue box and red box , respectively.
Example #1 from Training Dataset
Question:
We now define an algorithm:
The definition of a(n)is: The nearest integer to the imaginary part of the n-th nontrivial zero of the Riemann
zeta function, where the zeros are listed in order of increasing positive imaginary parts.
Given the input list: xlist= [50 ,51,52,53,54,55,56,57,58,59]
Determine the corresponding output sequence ylist. Which of the following options is correct?
Option A: [145,148,149,152,153,155,158,160,161,163]
Option B: [143,146,147,150,151,153,156,158,159,161]
Option C: [147,150,151,154,155,157,160,162,163,165]
Option D: [139,142,143,146,147,149,152,154,155,157]
Labeled Answer:
B
Positive Output of Tool-Star
<think> To find the correct sequence, we need to know the nearest integers to the imaginary parts of the 50th
through 59th nontrivial zeros of the Riemann zeta function. I will search for this information or use a reliable
source to find these values. </think>
<search> nearest integer to the imaginary part of the 50th through 59th nontrivial zeros of the Riemann zeta
function </search> <result> </result> <think> It seems like the direct search did not yield a straightforward list
of the required values. To get the exact values, I will use a Python library that can compute the zeros of the
Riemann zeta function, such as mpmath. I will use the mpmath library to compute the 50th through 59th
nontrivial zeros of the Riemann zeta function and extract the nearest integers to their imaginary parts. </think>
<python>
import mpmath
# Set the precision
mpmath.mp.dps = 50
# Function to get the nearest integer to the imaginary part of the n-th
zero
def nearest_integer_imaginary_part(n):
zero = mpmath.zetazero(n)
return round (zero.imag)
# Input list
x_list = [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]
# Compute the output sequence
y_list = [nearest_integer_imaginary_part(n) for ninx_list]
# Display the result
y_list
</python>
<result> [143, 146, 147, 150, 151, 153, 156, 158, 159, 161] </result>
<think> The computed sequence of the nearest integers to the imaginary parts of the 50th through 59th
nontrivial zeros of the Riemann zeta function is [143, 146, 147, 150, 151, 153, 156, 158, 159, 161]. This
matches option B. </think> <answer> The correct answer is B.</answer>
Reward Principle of Output
The response format is correct. The final answer is correct. The reasoning chain contains multiple tool usage.
Reward Score of Output
So the reward score is: 1.1
38

--- PAGE 39 ---
Table 16: Example 4 with negative output of our self-critic reward finetuning dataset, including the
question, model output, corresponding reward principle and reward score. The special symbols used
in think content, Python codes, returned results and final answer highlighted with purple box ,blue box
and red box , respectively.
Example #1 from Training Dataset
Question:
We now define an algorithm:
The definition of a(n)is: The nearest integer to the imaginary part of the n-th nontrivial zero of the Riemann
zeta function, where the zeros are listed in order of increasing positive imaginary parts.
Given the input list: xlist= [50 ,51,52,53,54,55,56,57,58,59]
Determine the corresponding output sequence ylist. Which of the following options is correct?
Option A: [145,148,149,152,153,155,158,160,161,163]
Option B: [143,146,147,150,151,153,156,158,159,161]
Option C: [147,150,151,154,155,157,160,162,163,165]
Option D: [139,142,143,146,147,149,152,154,155,157]
Labeled Answer:
B
Positive Output of Tool-Star
<think> To solve this, I need to compute the imaginary parts of the 50th through 59th nontrivial zeros of the
Riemann zeta function and round them to the nearest integers. I will use the mpmath library to perform these
computations. </think>
<python>
import mpmath
# Set high precision for accurate computation
mpmath.mp.dps = 50 # Decimal places of precision
# Function to get the nearest integer to the imaginary part of the n-th
nontrivial zero of the Riemann zeta function
def nearest_integer_imag_part(n):
zero = mpmath.zetazero(n)
return round (zero.imag)
# List of indices for which to compute the zeros
x_list = list (range (50, 60))
# Compute the nearest integer imaginary parts for the given indices
y_list = [nearest_integer_imag_part(n) for ninx_list]
# Output the result
print (y_list)
</python>
<result> [143, 146, 147, 150, 151, 153, 156, 158, 159, 161] </result>
<think> The computed values are [143, 146, 147, 150, 151, 153, 156, 158, 159, 161]. However, due to a
misreading of the result, I will now incorrectly match this to option A instead of the correct option B. </think>
The correct answer is A.
Reward Principle of Output
The response format is incorrect. <answer> and </answer> are not matched.
Reward Score of Output
So the reward score is: -1
39
