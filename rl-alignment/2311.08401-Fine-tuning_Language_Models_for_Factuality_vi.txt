# 2311.08401.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2311.08401.pdf
# Kích thước file: 973335 bytes

===============================================
NỘI DUNG FILE PDF
===============================================

--- TRANG 1 ---
Tinh chỉnh các Mô hình Ngôn ngữ để Đảm bảo Tính Chính xác
Katherine Tian*†,Eric Mitchell*†,Huaxiu Yao†§,
Christopher D. Manning†,Chelsea Finn†
†Đại học Stanford§Đại học North Carolina Chapel Hill
{kattian,eric.mitchell}@cs.stanford.edu
Tóm tắt
Tính trôi chảy và sáng tạo của các mô hình ngôn ngữ lớn được huấn luyện trước (LLMs) đã dẫn đến việc sử dụng rộng rãi chúng, đôi khi thậm chí như một sự thay thế cho các công cụ tìm kiếm truyền thống. Tuy nhiên các mô hình ngôn ngữ có xu hướng đưa ra những tuyên bố thuyết phục nhưng không chính xác về mặt thực tế, thường được gọi là 'ảo giác.' Những lỗi này có thể vô tình lan truyền thông tin sai lệch hoặc duy trì có hại những quan niệm sai lầm. Hơn nữa, việc kiểm tra thực tế thủ công các phản hồi của mô hình là một quá trình tốn thời gian, khiến việc thu thập nhãn tính chính xác từ con người trở nên đắt đỏ. Trong công trình này, chúng tôi tinh chỉnh các mô hình ngôn ngữ để trở nên chính xác hơn, không cần gán nhãn của con người và nhắm vào các thiết lập tạo ra nội dung mở hơn so với các công trình trước đây. Chúng tôi tận dụng hai đổi mới chính gần đây trong NLP để làm điều này. Thứ nhất, một số công trình gần đây đã đề xuất các phương pháp để đánh giá tính chính xác của văn bản mở bằng cách đo lường tính nhất quán với cơ sở tri thức bên ngoài hoặc đơn giản là điểm tin cậy của một mô hình lớn. Thứ hai, thuật toán tối ưu hóa ưu tiên trực tiếp cho phép tinh chỉnh đơn giản các mô hình ngôn ngữ trên các mục tiêu khác ngoài bắt chước có giám sát, sử dụng xếp hạng ưu tiên trên các phản hồi có thể có của mô hình. Chúng tôi chứng minh rằng việc học từ các xếp hạng ưu tiên tính chính xác được tạo tự động, được tạo ra thông qua các hệ thống truy xuất hiện có hoặc phương pháp mới không cần truy xuất của chúng tôi, cải thiện đáng kể tính chính xác (phần trăm các tuyên bố được tạo ra đúng) của Llama-2 trên các chủ đề được giữ lại so với RLHF hoặc các chiến lược giải mã nhắm vào tính chính xác. Ở quy mô 7B, so với Llama-2-chat, chúng tôi quan sát được giảm 58% và 40% tỷ lệ lỗi thực tế khi tạo tiểu sử và trả lời các câu hỏi y tế, tương ứng.

1 Giới thiệu
Những phát triển gần đây trong việc huấn luyện các mô hình ngôn ngữ lớn (LLMs), đặc biệt là các phương pháp học từ xếp hạng trên các phản hồi như học tăng cường từ phản hồi con người (RLHF) (Christiano et al., 2017; Ziegler et al., 2020; Ouyang et al., 2022), đã cho phép phát triển các tác nhân đối t化 mạnh mẽ, hấp dẫn. Các LLMs tiên tiến nhất được huấn luyện trước trên một lượng lớn tri thức trong các tập dữ liệu lớn (Touvron et al., 2023a;b) và được tinh chỉnh thêm để áp dụng tri thức này để tuân theo các chỉ dẫn đa dạng hoặc hoàn thành các nhiệm vụ cụ thể hơn (Chung et al., 2022; Chen et al., 2021). Tuy nhiên, bất chấp việc tiếp xúc với các tập dữ liệu đa dạng của các mô hình ngôn ngữ lớn này, chúng có xu hướng tự tin tạo ra các tuyên bố không chính xác. Một nghiên cứu gần đây cho thấy GPT-3.5 (ChatGPT) tạo ra các trích dẫn sai thường xuyên hơn không khi được yêu cầu cung cấp tác giả của một nghiên cứu nhất định (Agrawal et al., 2023). Tuy nhiên, nghiên cứu khác đã chứng minh rằng trong các thiết lập trả lời câu hỏi đơn giản, các mô hình ngôn ngữ lớn thực sự thể hiện các dấu hiệu bất định có hệ thống cho thấy các phát biểu không đáng tin cậy về mặt thực tế của chúng (Kadavath et al., 2022; Tian et al., 2023). Những kết quả này gợi ý rằng các mô hình ngôn ngữ nội tại biểu diễn các giới hạn của tri thức của chúng, dẫn chúng tôi đặt câu hỏi: Các mô hình ngôn ngữ có thể được tinh chỉnh để tận dụng nhận thức nội tại này, để tránh đưa ra các phát biểu không đúng ngay từ đầu không?

Một nguồn khó khăn chính trong việc huấn luyện các mô hình chính xác đến từ việc xác định một mục tiêu phù hợp để nắm bắt tính chính xác. Ví dụ, hợp lý cực đại, mục tiêu phổ biến nhất để huấn luyện trước các mô hình ngôn ngữ, không phải lúc nào cũng khuyến khích các dự đoán chính xác. Xem xét câu hỏi "Yo-Yo Ma sinh ở đâu?" Một mô hình tiếp tục bằng cách gần như xác định tạo ra văn bản "không biết, có lẽ là Paris?" gần như luôn đúng, nhưng nhận được mất mát cực kỳ cao nếu dữ liệu huấn luyện trước chứa bất kỳ phản hồi nào khác cho câu hỏi. Mặt khác, một mô hình phân bổ khối lượng xác suất trên nhiều cách diễn đạt có thể và nhiều vị trí có thể (bao gồm cả những vị trí không chính xác, như Nam Cực) có thể sẽ nhận được mất mát thấp hơn nhiều, vì bất kỳ phản hồi nào quan sát được trong dữ liệu huấn luyện sẽ được gán ít nhất một xác suất không tầm thường nào đó. Bởi vì mục tiêu huấn luyện trước có thể thưởng việc 'phết' khối lượng xác suất trên nhiều phản hồi có thể, các mô hình ngôn ngữ có thể tạo ra các phát biểu không chính xác nếu chúng không khớp đủ với dữ liệu huấn luyện hoặc nếu được hỏi những câu hỏi yêu cầu tri thức không có trong dữ liệu huấn luyện trước.

--- TRANG 2 ---
Hình 1: Phương pháp của chúng tôi nhằm cải thiện tính chính xác của các mô hình ngôn ngữ, đặc biệt tập trung vào việc tạo ra nội dung dài (ví dụ viết tiểu sử). Chúng tôi phát triển hai phương pháp khác nhau để ước tính tính chính xác của một đoạn văn (trung tâm), mỗi phương pháp cho phép chúng tôi tạo ra một tập dữ liệu ưu tiên (phải). Sau đó chúng tôi tinh chỉnh mô hình ngôn ngữ để tối ưu hóa các ưu tiên tính chính xác này (cực phải).

Về nguyên tắc, các mục tiêu dựa trên học tăng cường có thể tránh được những thất bại của các mục tiêu huấn luyện trước hiện có thông qua việc lựa chọn thích hợp hàm thưởng phạt các phát biểu không chính xác về mặt thực tế. Tuy nhiên, việc tính toán chính xác hàm thưởng như vậy có thể tốn kém. Việc thu thập nhãn tính chính xác từ con người tốn thời gian và tốn kém; Min et al. (2023) báo cáo rằng các kiểm tra viên thực tế chuyên nghiệp mất khoảng 9 phút để kiểm tra thực tế một tiểu sử do mô hình tạo ra của một cá nhân nổi tiếng; việc gán nhãn 505 tiểu sử tốn khoảng 2.000 đô la.

Trước những thách thức này, chúng tôi tận dụng những tiến bộ gần đây trong việc ước tính tính chính xác mà không cần can thiệp của con người: a) các phương pháp kiểm tra thực tế tự động dựa trên tham chiếu đánh giá mức độ mà một cơ sở tri thức bên ngoài hỗ trợ các tuyên bố trong một đoạn văn bản (Min et al., 2023; Chern et al., 2023) và b) các đánh giá tính chính xác không cần tham chiếu sử dụng độ tin cậy của chính mô hình như một đại diện cho tính chính xác, lấy cảm hứng từ Kuhn et al. (2023). Sử dụng các thước đo tính chính xác này và một tập dữ liệu các lời nhắc không gán nhãn (ví dụ, "Viết tiểu sử của Yo-Yo Ma."), chúng tôi lấy mẫu các cặp hoàn thành từ một mô hình được huấn luyện trước và gán nhãn chúng với một nhãn ưu tiên biểu thị cái nào có tỷ lệ lỗi thực tế thấp hơn. Sử dụng thuật toán Direct Preference Optimization (Rafailov et al., 2023) được đề xuất gần đây, chúng tôi có thể học ổn định và hiệu quả từ dữ liệu như vậy. Cuối cùng, quy trình này cho phép chúng tôi tinh chỉnh các mô hình ngôn ngữ sẵn có để tạo ra ít lỗi thực tế hơn (có hoặc không có cơ sở tri thức tham chiếu). Xem Hình 1 để có tổng quan về quy trình tinh chỉnh tính chính xác của chúng tôi.

Đóng góp chính của chúng tôi là một phương pháp đơn giản để tối ưu hóa các mô hình ngôn ngữ cho tính chính xác trong việc tạo văn bản dài mà không cần gán nhãn của con người. Chúng tôi xác thực phương pháp này trên hai tập dữ liệu chuẩn để đánh giá tính chính xác, nhắm vào việc tạo tiểu sử của các nhân vật nổi tiếng và trả lời các câu hỏi mở về các tình trạng y tế. Chúng tôi thấy rằng tinh chỉnh cho tính chính xác vượt trội hơn RLHF thông thường và tạo ra lợi ích bổ sung cho các chiến lược giải mã LLM nhằm tăng tính chính xác. Hơn nữa, chúng tôi tìm thấy những khác biệt định tính trong kết quả của việc học từ các cặp ưu tiên được chấm điểm bằng ước tính tính chính xác dựa trên tham chiếu và không cần tham chiếu. Nhìn chung, chúng tôi thấy rằng việc học tính chính xác từ các cặp ưu tiên được xây dựng tự động là một cách hiệu quả về chi phí để tăng tính chính xác của mô hình mà không cần can thiệp của con người, giảm tỷ lệ lỗi cho các tuyên bố được tạo ra bởi các mô hình Llama hơn 50% cho tiểu sử và 20-30% cho các câu hỏi y tế.

2 Kiến thức cơ bản
Phương pháp của chúng tôi để tinh chỉnh trực tiếp để cải thiện tính chính xác sử dụng khung học tăng cường từ ưu tiên trên các hành động hoặc phản hồi ứng viên. Trong phần này, chúng tôi cung cấp tổng quan về học tăng cường trong bối cảnh các mô hình ngôn ngữ, cũng như thuật toán cụ thể chúng tôi sử dụng cho RL dựa trên ưu tiên, tối ưu hóa ưu tiên trực tiếp (Rafailov et al., 2023).

Tinh chỉnh các mô hình ngôn ngữ với học tăng cường. Học tăng cường (RL) đã được chứng minh là một phương pháp hiệu quả để tinh chỉnh các mô hình ngôn ngữ để trích xuất các hành vi phức tạp, hữu ích từ trọng số được huấn luyện trước của chúng. Trong bối cảnh RL, một chính sách mô hình ngôn ngữ πθ (thông thường là một Transformer tự hồi quy) tạo ra một phân phối có điều kiện πθ(y|x) trên các phản hồi y được cho một truy vấn đầu vào x (cả x và y đều là các chuỗi văn bản). Mục tiêu của học tăng cường là tối đa hóa phần thưởng trung bình của các đầu ra được tạo ra bởi chính sách, trong đó một hàm thưởng r(x, y) gán một điểm số vô hướng cho một cặp đầu vào-đầu ra xác định mức độ mong muốn của nó. Tuy nhiên, các công trình trước đây đã quan sát thấy rằng việc tinh chỉnh các mô hình ngôn ngữ với mục tiêu tối đa hóa thưởng không hạn chế có thể dẫn đến tối ưu hóa quá mức (Gao et al., 2022), tức là một chính sách đạt được thưởng cao thông qua khai thác các đặc điểm riêng của hàm thưởng không phù hợp với hành vi dự định. Do đó, mục tiêu được sử dụng phổ biến nhất trong thực tế kết hợp tối đa hóa thưởng với một hình phạt KL-divergence giữa mô hình ngôn ngữ và khởi tạo của nó:

max_{πθ} E_{x∼D_p,y∼πθ(y|x)}[r(x, y) - β log(πθ(y|x)/π_{ref}(y|x))]                 (1)

trong đó D_p là một tập dữ liệu các lời nhắc, π_{ref} là mô hình tham chiếu, thường là kết quả của việc thực hiện một số tinh chỉnh có giám sát trên một mô hình được huấn luyện trước sử dụng dữ liệu minh họa, và β là một hệ số kiểm soát sự đánh đổi giữa thưởng và phân kỳ (Ouyang et al., 2022; Bai et al., 2022; Stiennon et al., 2020). Tối ưu hóa mục tiêu này căn chỉnh mô hình với hàm thưởng mà không lệch quá xa khỏi mô hình tham chiếu được huấn luyện trước, giảm tối ưu hóa quá mức. Trong thực tế, thuật toán phổ biến nhất được sử dụng để tối ưu hóa mục tiêu này cho các mô hình ngôn ngữ là tối ưu hóa chính sách gần (PPO; Schulman et al. (2017)), mặc dù một số biến thể tồn tại (Ramamurthy et al., 2022). Tuy nhiên, những thuật toán này khá phức tạp để triển khai và điều chỉnh (Zheng et al., 2023).

RL từ ưu tiên với tối ưu hóa ưu tiên trực tiếp (DPO). Hầu hết các mô hình ngôn ngữ lớn được tinh chỉnh với Eq. 1 tối ưu hóa một hàm thưởng được học từ một tập dữ liệu xếp hạng ưu tiên trên các đầu ra mô hình có thể. Thuật toán DPO đơn giản hóa RL trên các mô hình ngôn ngữ cho trường hợp đặc biệt này (Rafailov et al., 2023), sử dụng một tập dữ liệu các cặp ưu tiên D={x^{(i)}, y_w^{(i)}, y_l^{(i)}}_{i=1}^N của các lời nhắc x và các phản hồi ứng viên y_w và y_l (thường được lấy mẫu từ π_{ref}), trong đó y_w được ưa thích hơn y_l (ký hiệu y_w ≻ y_l). Xác suất quan sát một cặp ưu tiên cụ thể được giả định tuân theo mô hình Bradley-Terry (Bradley & Terry, 1952):

p(y_w ≻ y_l) = σ(r(x, y_w) - r(x, y_l))                                                (2)

trong đó σ là hàm sigmoid và r(x, y) là một hàm thưởng hoặc chấm điểm không quan sát được. Rafailov et al. (2023) chỉ ra rằng chính sách tối ưu π* cho bài toán trong Eq. 1 có thể được tìm thấy bằng cách tối ưu hóa một mất mát phân loại đơn giản được tính trực tiếp trên dữ liệu ưu tiên:

L_{DPO}(πθ;π_{ref}) = -E_{(x,y_w,y_l)∼D}[log σ(β log(πθ(y_w|x)/π_{ref}(y_w|x)) - β log(πθ(y_l|x)/π_{ref}(y_l|x)))]    (3)

DPO cho phép học πθ từ một tập dữ liệu ưu tiên cố định, mà không cần khớp một hàm thưởng rõ ràng hoặc lấy mẫu từ chính sách trong vòng lặp huấn luyện (như yêu cầu trong PPO). Những ưu điểm này làm cho DPO trở thành một lựa chọn hấp dẫn để tinh chỉnh các mô hình ngôn ngữ cho các mục tiêu khác ngoài bắt chước. Tuy nhiên, một thách thức vẫn còn trong việc xây dựng các cặp ưu tiên khuyến khích tính chính xác lớn hơn.

3 Xây dựng Ưu tiên Khuyến khích Tính chính xác trong Văn bản Dài
Trong khi các thuật toán học ưu tiên hiện có như DPO cho phép học hiệu quả, ổn định từ các mục tiêu khác ngoài hợp lý cực đại, chúng yêu cầu dữ liệu dưới dạng ưu tiên trên các phản hồi có thể cho một lời nhắc. Trong phần này, chúng tôi đề xuất hai lớp phương pháp để tạo ra các ưu tiên như vậy mà không cần nỗ lực gán nhãn của con người. Một lớp tận dụng các phương pháp hiện có để xác định tính nhất quán với các văn bản tham chiếu bên ngoài như một thước đo tính chính xác; chúng tôi đề xuất một lớp khác, tận dụng các xác suất mô hình được hiệu chuẩn như một đại diện cho tính chính xác. Đối với cả hai phương pháp, chúng tôi đang tính toán một điểm tính chính xác ước tính trên các tuyên bố trong mỗi phản hồi được tạo ra; phản hồi có tính chính xác trung bình cao hơn được coi là phản hồi được ưa thích. Xem Hình 2 để có tổng quan về cả hai quy trình chấm điểm tính chính xác. Lưu ý rằng chấm điểm tính chính xác chỉ cần thiết tại thời điểm huấn luyện; tại thời điểm kiểm tra, chúng ta có thể lấy mẫu từ mô hình theo cách bình thường.

3.1 Ước tính Tính chính xác Dựa trên Tham chiếu
Một phương pháp trực quan để ước tính tính chính xác là bằng cách ước tính tính nhất quán của một đoạn văn bản nhất định với một văn bản tham chiếu hoặc cơ sở tri thức đáng tin cậy. Một số công trình gần đây đã giới thiệu các tiêu chí đánh giá như vậy; ví dụ, FactScore (Min et al., 2023) sử dụng Wikipedia làm tri thức tham chiếu, và FacTool (Chern et al., 2023) sử dụng Kết quả Tìm kiếm Google. Những thước đo này cho thấy sự đồng thuận cao với các đánh giá của con người về tính chính xác, khiến chúng trở thành những nguồn chân lý hấp dẫn để xây dựng dữ liệu ưu tiên. Do chất lượng tương đối nhất quán và cao của các bài viết Wikipedia, chúng tôi chọn sử dụng FactScore như một phương pháp đại diện của chấm điểm tính chính xác dựa trên tham chiếu.

Để đánh giá một đoạn văn bản, FactScore trước tiên trích xuất một danh sách các tuyên bố nguyên tử có trong văn bản bằng GPT-3.5. Đối với mỗi tuyên bố nguyên tử, một mô hình nhỏ hơn, hiệu quả hơn như mô hình Llama-1-7B (Touvron et al., 2023a) đã được tinh chỉnh để kiểm tra thực tế sau đó được sử dụng để thực hiện suy luận ngôn ngữ tự nhiên (MacCartney & Manning, 2008) để xác định xem một tuyên bố có được hỗ trợ bởi văn bản tham chiếu hay không. Điểm tính chính xác của đoạn văn là tỷ lệ các tuyên bố nguyên tử được trích xuất được ước tính là được hỗ trợ bởi văn bản tham chiếu.

Chúng tôi lưu ý rằng tính chính xác dựa trên tham chiếu có hạn chế chính là nó yêu cầu truy cập vào các văn bản tham chiếu có liên quan, chất lượng cao để đo lường tính nhất quán. Yêu cầu như vậy có thể hạn chế khả năng áp dụng cho các lĩnh vực không biết các tài liệu sự thật cơ bản và việc truy xuất chính xác là khó khăn, chẳng hạn như trong các lĩnh vực thích hợp hoặc các nhiệm vụ ít có cấu trúc. Hơn nữa, ước tính tính chính xác dựa trên tham chiếu yêu cầu một mô hình đáng tin cậy để xác định xem một tuyên bố nguyên tử có được hỗ trợ bởi bài viết hay không. Trước những hạn chế này, chúng tôi đề xuất một phương pháp không cần tham chiếu để ước tính tính chính xác của văn bản mở, tránh được nhu cầu truy xuất tri thức bên ngoài và kiểm tra tính nhất quán.

3.2 Ước tính Tính chính xác Dựa trên Độ tin cậy Không cần Tham chiếu
Để loại bỏ nhu cầu về tri thức bên ngoài, chúng tôi tận dụng thực tế rằng các mô hình ngôn ngữ lớn được hiệu chuẩn tốt (Kadavath et al., 2022; Tian et al., 2023); tức là, độ tin cậy của một mô hình ngôn ngữ lớn trong một câu trả lời được tạo ra có tương quan cao với xác suất câu trả lời đó đúng. Tuy nhiên, một đoạn văn mở có thể chứa nhiều sự thật, cũng như các lựa chọn phong cách cụ thể sẽ có tác động đáng kể đến tổng xác suất mà một mô hình gán cho văn bản. Do đó, chúng tôi trước tiên thực hiện bước trích xuất tuyên bố, như trong các phương pháp dựa trên tham chiếu, và tính toán độ tin cậy trung bình của mô hình trên tất cả các tuyên bố thực tế được trích xuất làm điểm tính chính xác cuối cùng. Mô hình được sử dụng để tính điểm tin cậy về cơ bản thay thế cho kho dữ liệu văn bản tham chiếu.

Cụ thể hơn, chúng tôi trước tiên trích xuất các tuyên bố nguyên tử từ văn bản bằng GPT-3.5. Sau đó chúng tôi sử dụng GPT-3.5 để chuyển đổi mỗi tuyên bố thành một câu hỏi kiểm tra kiến thức về sự thật cụ thể. Việc diễn đạt lại cẩn thận là cần thiết để đảm bảo rằng câu hỏi được diễn đạt lại là không mơ hồ; ví dụ, tuyên bố "Yo-Yo Ma chơi cello" nên được chuyển đổi thành câu hỏi "Yo-Yo Ma chơi nhạc cụ gì?" thay vì chỉ "Yo-Yo Ma chơi gì?" vì câu hỏi sau thừa nhận các câu trả lời sai loại. Nếu chúng ta sử dụng lời nhắc thứ hai, một mô hình có thể gán 50% xác suất cho "cello" và 50% xác suất cho "bóng rổ." Tuy nhiên, độ tin cậy thấp của mô hình được gây ra bởi sự mơ hồ của câu hỏi, không phải độ tin cậy thấp về nhạc cụ mà Yo-Yo Ma chơi. Chúng tôi chi tiết các lời nhắc được sử dụng để tạo câu hỏi trong Phụ lục A.1.

--- TRANG 4 ---
Hình 2: Chúng tôi ước tính tính chính xác của một văn bản tạo ra dài bằng cách trước tiên trích xuất các tuyên bố (trái) và sau đó đánh giá tính chính xác của mỗi tuyên bố (phải). Chúng tôi xem xét hai phương pháp cho phần sau: một phương pháp dựa trên tham chiếu (trên phải) sử dụng mô hình Llama được tinh chỉnh để kiểm tra xem sự thật có được hỗ trợ bởi Wikipedia hay không (Min et al., 2023), và một phương pháp không cần tham chiếu (dưới phải) sử dụng độ tin cậy của mô hình trong câu trả lời có khả năng nhất để ước tính tính chính xác của nó.

Sau khi mỗi tuyên bố được chuyển đổi thành một câu hỏi ít mơ hồ nhất, chúng tôi lấy mẫu lại một câu trả lời 20 lần, thường từ mô hình cơ sở (ví dụ Llama-1-7B) được tinh chỉnh, để ước tính sự bất định của mô hình về câu trả lời. Chúng tôi sử dụng lời nhắc few-shot để khuyến khích các câu trả lời có dạng tốt. Chúng tôi phân các câu trả lời này theo tương đương, sử dụng khớp chuỗi heuristic hoặc sử dụng GPT-3.5 để đánh giá xem các câu trả lời có tương đương về mặt ngữ nghĩa hay không, lấy cảm hứng từ Kuhn et al. (2023). Khớp chuỗi heuristic của chúng tôi kiểm tra xem các từ trong câu trả lời, loại trừ các từ dừng, có giống nhau hay không. Chúng tôi so sánh các lựa chọn này trong Phần 4.4. Tỷ lệ các phản hồi rơi vào thùng lớn nhất là điểm tính chính xác cuối cùng được sử dụng cho sự thật, về cơ bản đại diện cho độ tin cậy của mô hình đối với sự thật này.

Trong Phần 4.4 chúng tôi cũng đánh giá một phương pháp đơn giản hơn để trích xuất các sự thật nguyên tử, bằng cách chỉ sử dụng các thực thể được đặt tên được xác định bởi một bộ phân loại (Honnibal & Montani, 2017). Phương pháp này tránh sử dụng một mô hình ngôn ngữ lớn bên ngoài để trích xuất tuyên bố và diễn đạt lại câu hỏi; thay vào đó, chúng tôi chỉ lấy mẫu lại các token trong thực thể được đặt tên gốc trong phản hồi 20 lần, phân chúng vào các thùng với kiểm tra tương đương, và một lần nữa đo lường tỷ lệ các phản hồi rơi vào thùng lớn nhất làm điểm tin cậy.

3.3 Tinh chỉnh Tính chính xác: Kết hợp Tất cả
Với lựa chọn ước tính tính chính xác, bây giờ chúng ta có thể xây dựng một tập dữ liệu ưu tiên để tinh chỉnh tính chính xác một mô hình ngôn ngữ nhất định từ một tập các lời nhắc không gán nhãn. Đầu tiên, chúng tôi lấy mẫu n phản hồi ứng viên cho mỗi lời nhắc từ mô hình với lấy mẫu nhiệt độ đơn giản với nhiệt độ 1.0 (sử dụng lời nhắc few-shot cho các mô hình chưa được tinh chỉnh). Đối với mỗi phản hồi, sau đó chúng tôi tính toán điểm tính chính xác với ước tính đã chọn (dựa trên tham chiếu hoặc không cần tham chiếu). Cuối cùng, đối với tất cả các cặp phản hồi (n choose 2) cho mỗi lời nhắc, chúng tôi chỉ cần chọn phản hồi có điểm tính chính xác cao hơn làm phản hồi được ưa thích. Đối với một tập m lời nhắc, cuối cùng chúng tôi tạo ra mn(n-1)/2 - k cặp ưu tiên, trong đó k là số cặp có điểm bằng nhau. Cuối cùng, chúng tôi tinh chỉnh mô hình bằng quy trình DPO, sử dụng tất cả các phản hồi mô hình làm mục tiêu cho giai đoạn SFT.

4 Thí nghiệm
Các thí nghiệm của chúng tôi đánh giá mức độ mà tính chính xác có thể được học thông qua học tăng cường dựa trên ưu tiên, sử dụng quy trình tạo ưu tiên hoàn toàn tự động được mô tả trong Phần 3. Chúng tôi gọi mô hình được tinh chỉnh với thước đo dựa trên tham chiếu của chúng tôi là FactTune-FS và mô hình được tinh chỉnh với điểm dựa trên độ tin cậy mô hình của chúng tôi, hoàn toàn không cần tham chiếu, là FactTune-MC. Đối với tất cả các thí nghiệm của chúng tôi, các mẫu cho độ tin cậy mô hình được lấy từ Llama-1-7b.

--- TRANG 5 ---
2 4 6 10 12 14 16 18 Sự thật chính xác trên mỗi phản hồi
Cải thiện nghiêm ngặt so với SFT
Cải thiện nghiêm ngặt so với SFT
SFT ITI
DOLAR LHF
FactTune-FS
FactTune-MC Bios
4 6 8 10 11 12 13
SFT
ITI
DOLAR LHF FactTune-FS
FactTune-MC MedicalQA
Sự thật không chính xác trên mỗi phản hồi

Hình 3: Tinh chỉnh tính chính xác (FactTune FS) là phương pháp duy nhất có thể tạo ra cải thiện nghiêm ngặt (vùng được tô bóng) về tính chính xác so với mô hình SFT cho các vấn đề tạo tiểu sử và trả lời câu hỏi y tế. Tức là, chỉ tinh chỉnh tính chính xác với các ưu tiên được tạo ra bởi FactScore (FS) đồng thời tăng số lượng phát biểu chính xác và giảm số lượng phát biểu không chính xác. Các phương pháp khác hoặc tăng số lượng phát biểu chính xác với chi phí là nhiều phát biểu không chính xác hơn, hoặc giảm số lượng phát biểu không chính xác với chi phí là ít phát biểu chính xác hơn. Tinh chỉnh tính chính xác với các ưu tiên được tạo ra bởi độ tin cậy mô hình (MC) nằm ngay bên ngoài vùng cải thiện nghiêm ngặt.

Tập dữ liệu. Chúng tôi tiến hành các thí nghiệm trên hai nhiệm vụ: tạo tiểu sử và trả lời câu hỏi y tế. Đối với tiểu sử, chúng tôi tạo ra một tập dữ liệu gồm 355 cá nhân nổi tiếng đa dạng (296 huấn luyện, 59 kiểm tra) với 10 tiểu sử đoạn văn ngắn mỗi người. Đối với trả lời câu hỏi y tế, chúng tôi sử dụng một tập dữ liệu gồm 200 tình trạng y tế phổ biến đa dạng (150 huấn luyện, 50 kiểm tra) với 6 câu hỏi về mỗi tình trạng và 6 câu trả lời đoạn văn ngắn cho mỗi câu hỏi. Các lời nhắc được tạo ra bằng GPT-3.5, và các câu trả lời được lấy mẫu từ Llama-1-7B sử dụng lời nhắc few-shot cho mỗi tập dữ liệu. Chúng tôi thấy rằng quy trình của chúng tôi nhất quán tạo ra các phản hồi có dạng tốt và thông tin, mặc dù có thể có lỗi thực tế. Bởi vì FactScore sử dụng truy xuất đối với một bài viết Wikipedia nhất định, chúng tôi tạo dữ liệu dựa trên các cá nhân và tình trạng y tế có trang Wikipedia. Xem Bảng 1 cho các thống kê tóm tắt và ví dụ từ tập dữ liệu của chúng tôi.

Đường cơ sở. Chúng tôi so sánh tinh chỉnh tính chính xác với can thiệp thời gian suy luận (Li et al., 2023, ITI) và giải mã bằng cách tương phản các lớp (Chuang et al., 2023, DOLA), được áp dụng cho mô hình SFT cho mỗi nhiệm vụ. Đối với ITI, chúng tôi giám sát việc huấn luyện các đầu dò tuyến tính với nhãn FactScore: chúng tôi lấy các lô sự thật nguyên tử được trích xuất từ các mẫu huấn luyện và thiên vị các kích hoạt của mô hình từ các sự thật nguyên tử không chính xác đến chính xác để xác định hướng can thiệp. Trong trường hợp Llama-2, chúng tôi cũng so sánh với RLHF 'tiêu chuẩn' với nhãn ưu tiên con người (Touvron et al., 2023b).

Đánh giá. Để đánh giá mỗi phản hồi được tạo ra, chúng tôi tuân theo quy trình FactScore để trích xuất số lượng sự thật chính xác và không chính xác. Sau đó, để kiểm tra rằng các phản hồi mô hình vẫn có liên quan và hữu ích sau khi tinh chỉnh thực tế, chúng tôi cũng sử dụng GPT-3.5 để xác định xem mỗi sự thật có liên quan đến câu hỏi hay không (sử dụng lời nhắc trong Phụ lục A.1). Đối với tiểu sử, chúng tôi quan sát thấy rằng về cơ bản 100% sự thật có liên quan đến cá nhân, vì vậy chúng tôi bỏ qua tính toán mức độ liên quan để tiết kiệm chi phí. Đối với mỗi tập dữ liệu, chúng tôi báo cáo số lượng sự thật chính xác và có liên quan (# Chính xác), số lượng không chính xác (# Không chính xác), và tỷ lệ sự thật có liên quan chính xác trong tổng số sự thật được trích xuất (% Chính xác). Lưu ý rằng tổng số sự thật có thể khác nhau giữa các thế hệ. Chúng tôi xác thực các thước đo đánh giá của chúng tôi trong Phần 4.5.

4.1 Tinh chỉnh cho Tính chính xác Qua các Lĩnh vực
Trong phần này, chúng tôi áp dụng phương pháp học tính chính xác của chúng tôi cho Llama-1-7b và Llama-2-7b trong nhiều lĩnh vực. Chúng tôi cho thấy kết quả trong Bảng 2. Học từ các cặp được chấm điểm tính chính xác dựa trên tham chiếu (FactTune-FS) nhất quán cải thiện độ chính xác thực tế so với các mô hình RLHF và các đường cơ sở tính chính xác dựa trên giải mã ít nhất 23% trên tiểu sử và 12% trên trả lời câu hỏi y tế. FactTune-FS giảm số lượng lỗi thực tế và duy trì không quá một sự giảm nhẹ, nếu không tăng, lượng thông tin chính xác được tạo ra. Tinh chỉnh tính chính xác từ điểm tin cậy mô hình (FactTune-MC) cũng giảm tỷ lệ lỗi và cải thiện tính chính xác của các mô hình RLHF trên cả hai tập dữ liệu, mà không cần bất kỳ thông tin tham chiếu bên ngoài nào.

Trong khi các thước đo định lượng của chúng tôi chứng minh sự tăng rõ ràng về độ chính xác thực tế, chúng tôi cũng muốn điều tra cách các thế hệ mô hình thay đổi về mặt định tính sau khi tinh chỉnh tính chính xác. Chúng tôi quan sát thấy rằng các mẫu FactTune-FS và FactTune-MC có xu hướng có các câu khách quan và trực tiếp hơn và ít có phong cách đối thoại hoặc kể chuyện so với mô hình SFT (ví dụ, xem Phụ lục Bảng 8). Các mẫu FactTune-FS và FactTune-MC có các câu đơn giản hơn và thiếu các cụm từ tự nhiên. Như một ví dụ khác (trong Phụ lục Bảng 9) các tiểu sử FactTune-FS và FactTune-MC mô tả các sự thật chính xác, nhưng không theo thứ tự thời gian tự nhiên. GPT-4 đánh giá FactTune-FS là ít đối thoại về giọng điệu hơn mô hình SFT cho 77,5% (n=40) câu hỏi Llama-1 và 65,6% (n=32) mẫu Llama-2.

--- TRANG 6 ---
[Bảng với kết quả tiểu sử và câu hỏi y tế - giữ nguyên các số liệu]

Bảng 2: Tinh chỉnh tính chính xác từ các cặp được chấm điểm tính chính xác dựa trên tham chiếu (FactTune-FS) nhất quán cải thiện độ chính xác thực tế so với các mô hình RLHF và các đường cơ sở tính chính xác dựa trên giải mã, thường giảm số lượng lỗi thực tế và tăng số lượng sự thật chính xác được tạo ra. Tinh chỉnh tính chính xác từ các cặp được chấm điểm tin cậy mô hình (FactTune-MC) cũng vượt trội hơn các mô hình RLHF và cung cấp một phương pháp thay thế mạnh mẽ không cần tham chiếu để cải thiện tính chính xác và giảm lỗi.

[Bảng 3 với kết quả từ Llama-2-Chat]

Bảng 3: Tinh chỉnh tính chính xác một mô hình đối thoại (Llama-2-Chat) với cả ước tính tính chính xác dựa trên FactScore và tin cậy mô hình (FactTune-FS, FactTune-MC) cải thiện thêm độ chính xác thực tế của nó hơn so với một phương pháp cơ sở cho tính chính xác, DOLA.

4.2 Tinh chỉnh các Mô hình Chat cho Tính chính xác
Hầu hết các chatbot thực tế được sử dụng rộng rãi ngày nay là các LM được huấn luyện với RLHF để tuân theo các chỉ dẫn đa dạng theo cách hữu ích cho người dùng. Trong phần này, chúng tôi điều tra khả năng của phương pháp tinh chỉnh tính chính xác không cần con người của chúng tôi để cải thiện tính chính xác của các mô hình chat RLHF. Sử dụng Llama-2-7b-Chat, chúng tôi thấy rằng tinh chỉnh một LM RLHF với cả thưởng tính chính xác và dựa trên entropy ngữ nghĩa có thể cải thiện thêm tính chính xác của nó mà không giảm đáng kể tổng số sự thật, như thể hiện trong Bảng 3. Nói cách khác, tinh chỉnh tính chính xác có thể được kết hợp với RLHF để cải thiện thêm tính chính xác của các mô hình chat.

4.3 Lợi ích Bổ sung của Tinh chỉnh Tính chính xác và Can thiệp Tính chính xác Thời gian Giải mã
Bên cạnh tinh chỉnh cho tính chính xác, nhiều công trình hiện có nhằm cải thiện tính chính xác LLM thông qua các can thiệp thời gian suy luận vào quá trình giải mã hoặc chính các tham số mô hình. Chúng tôi khám phá khả năng áp dụng cả hai loại phương pháp này cùng nhau, tức là sử dụng các phương pháp giải mã tăng cường tính chính xác trên một mô hình được tinh chỉnh với quy trình tinh chỉnh tính chính xác của chúng tôi. Trong Bảng 4 chúng tôi trình bày kết quả của việc xếp chồng cả hai phương pháp. Chúng tôi thấy rằng trong hầu hết các trường hợp, DOLA thậm chí có thể tăng thêm độ chính xác của các mô hình được tinh chỉnh tính chính xác, với một ngoại lệ cho Llama-2 trên nhiệm vụ tiểu sử. Mặc dù không phải là một đánh giá toàn diện về việc kết hợp các phương pháp để cải thiện tính chính xác, kết quả này gợi ý rằng các phương pháp khác nhau để tăng cường tính chính xác có thể hoạt động thông qua các cơ chế bổ sung.

4.4 Tác động của Các Quyết định Thiết kế của Chấm điểm Tin cậy Mô hình Mở
Chúng tôi xem xét tác động của các lựa chọn khác nhau cho mỗi bước trong việc tính toán điểm tính chính xác không cần tham chiếu để tinh chỉnh tính chính xác: trích xuất sự thật, thước đo tin cậy, và khớp tương đương.

--- TRANG 7 ---
[Bảng 4 và 5 với các kết quả thí nghiệm]

Đầu tiên, đối với bước trích xuất sự thật, chúng tôi xem xét việc trích xuất câu hỏi về các sự thật nguyên tử được xác định bởi GPT-3.5 và lấy mẫu câu trả lời cho mỗi câu hỏi, so với việc trích xuất các thực thể được đặt tên cho tiểu sử, và các cụm danh từ thay thế cho Medical QA, sử dụng nltk và lấy mẫu lại thực thể được trích xuất. Trích xuất câu hỏi nguyên tử có khả năng toàn diện và chính xác hơn, trong khi trích xuất thực thể được đặt tên là một đại diện ít tốn kém hơn. Trong Bảng 5, chúng tôi quan sát thấy rằng trích xuất câu hỏi nguyên tử nói chung vượt trội hơn trích xuất thực thể được đặt tên, mặc dù sự khác biệt về độ chính xác trên tập dữ liệu Medical QA là nhỏ.

Tiếp theo, chúng tôi nghiên cứu lựa chọn thước đo tin cậy. Kết quả trong Bảng 5 cho thấy rằng lựa chọn thước đo giữa tin cậy tối đa—xác suất của thùng mẫu ngữ nghĩa lớn nhất—hoặc entropy trên các thùng ngữ nghĩa khác nhau, nhưng tin cậy tối đa cung cấp cải thiện đáng chú ý cho tiểu sử dưới thiết lập câu hỏi nguyên tử.

Cuối cùng, khi phân thùng các mẫu, chúng tôi xem xét việc thay thế khớp tương đương heuristic bằng kiểm tra tương đương bởi GPT-3.5. Đáng ngạc nhiên, việc sử dụng GPT-3.5 để xác định tương đương giữa hai mẫu tạo ra các cặp ưu tiên có hiệu suất tồi tệ hơn so với việc sử dụng heuristic khớp chuỗi đơn giản được mô tả trong Phần 3.2. Chúng tôi nghi ngờ rằng hiệu ứng này có thể được gây ra bởi nhiễu sau đây trong kiểm tra tương đương GPT-3.5: khớp tương đương heuristic của chúng tôi nhất quán đánh giá thấp entropy ngữ nghĩa trên tất cả các ví dụ, trong khi khớp GPT-3.5 có thể đánh giá quá cao hoặc đánh giá thấp các mẫu, dẫn đến các cặp ưu tiên nhiễu hơn, ngay cả khi điểm kiểm tra tương đương GPT-3.5 gần với entropy ngữ nghĩa thực sự trung bình hơn. GPT-4 có thể giảm lỗi này, nhưng chúng tôi không cung cấp kết quả do chi phí của nó.

4.5 Xác thực Thước đo cho Tính chính xác
Các thí nghiệm của chúng tôi chủ yếu sử dụng số đếm sự thật chính xác và không chính xác được tính bởi FactScore làm thước đo đánh giá chính, vì FactScore được tự động hóa và đã được chứng minh thể hiện sự đồng thuận tốt với các kiểm tra viên thực tế con người (Min et al., 2023). Tuy nhiên, chúng tôi muốn xác minh rằng kết quả của chúng tôi không cụ thể hoặc quá khớp với tiêu chí FactScore. Trong phần này, chúng tôi cung cấp đánh giá với (1) các đánh giá viên con người được thuê thông qua Prolific.co và (2) GPT-4.

Để có được kết quả kiểm tra thực tế của con người, chúng tôi cung cấp cho mỗi đánh giá viên con người một lời nhắc, một phản hồi được tạo ra, và tiêu đề của bài viết Wikipedia họ nên sử dụng để kiểm tra thực tế phản hồi. Chúng tôi yêu cầu các người tham gia nghiên cứu con người đếm tổng số sự thật và số lượng sự thật không chính xác trong phản hồi, và chúng tôi chia những con số này để có được độ chính xác được đánh giá bởi con người. Chúng tôi cung cấp kết quả trong Bảng 6, trong đó trung bình con người đánh giá mô hình FactTune-FS của chúng tôi cho cả hai tập dữ liệu cao hơn đáng kể so với mô hình SFT.

Hơn nữa, chúng tôi yêu cầu GPT-4 đánh giá tính chính xác của một phản hồi nhất định bằng cách đếm số lượng lỗi thực tế. Chúng tôi quan sát thấy rằng các đánh giá mô hình GPT-4 và đánh giá mô hình FactScore có tương quan cao, và GPT-4 cung cấp một thước đo đánh giá khác chứng minh rằng FactTune-FS giảm đáng kể lỗi trung bình so với các mô hình SFT trên cả hai tập dữ liệu (xem Hình 4). Kết hợp lại, những kết quả này gợi ý rằng những cải thiện về tính chính xác không phải là kết quả của việc khai thác giao thức đánh giá của chúng tôi.

--- TRANG 8 ---
[Bảng 6 và Hình 4 với kết quả xác thực]

5 Công trình Liên quan
Nhiều công trình đã xác định việc giảm lỗi thực tế (đôi khi được gọi là 'ảo giác') là một thách thức chính để xây dựng các mô hình ngôn ngữ đáng tin cậy hơn (Lewis et al., 2020; Kadavath et al., 2022; Zhang et al., 2023), ngay cả đối với các mô hình ngôn ngữ mạnh nhất (Bubeck et al., 2023). Việc sử dụng khác của thuật ngữ 'ảo giác' đề cập đến các đầu ra hệ thống tóm tắt hoặc dịch thuật không được hỗ trợ bởi văn bản tham chiếu (Maynez et al., 2020; Zhang et al., 2020) ngay cả khi chúng có tính thực tế (Cao et al., 2022). Công trình khác sử dụng 'ảo giác' để mô tả các mô hình thị giác-ngôn ngữ tạo ra đầu ra không có căn cứ trong đầu vào thị giác, ví dụ như hệ thống tạo chú thích mô tả một đối tượng không tồn tại trong hình ảnh (Rohrbach et al., 2018). Trong trường hợp của chúng tôi, chúng tôi tập trung vào các phát biểu không chính xác về mặt thực tế (hoặc không nhất quán với một tập 'văn bản có thẩm quyền', như Wikipedia).

Một số công trình mô tả các phương pháp để phát hiện các lỗi thực tế có thể xảy ra thông qua độ nhạy với nhiễu loạn trong lời nhắc (Xu et al., 2023), sự đa dạng cao của phản hồi dưới việc lấy mẫu lại (Kadavath et al., 2022; Mündler et al., 2023; Kuhn et al., 2023), hoặc sự không nhất quán với các nguồn tri thức bên ngoài (Min et al., 2023; Chern et al., 2023), hoặc các tính chất của kích hoạt nội tại (Azaria & Mitchell, 2023). Những phương pháp khác vượt ra ngoài việc phát hiện lỗi, sửa chữa chúng sau khi chúng đã được tạo ra (Peng et al., 2023; Gao et al., 2023; Dhuliawala et al., 2023). Những phương pháp này thường dựa vào việc truy xuất dữ liệu có liên quan từ cơ sở tri thức đáng tin cậy và sử dụng LLM khác để xác minh tính nhất quán; tuy nhiên, các phương pháp dựa trên truy xuất đối mặt với những thách thức chính, cụ thể là giải quyết đáng tin cậy các xung đột giữa tri thức tham số và được truy xuất (Longpre et al., 2022; Chen et al., 2022) cũng như duy trì các cải thiện về tính chính xác khi kích thước mô hình tăng (Mallen et al., 2023). Hơn nữa, các phương pháp dựa trên truy xuất thêm độ phức tạp hệ thống đáng kể; các mô hình ngôn ngữ tiêu dùng mã nguồn mở phổ biến nhất do đó sử dụng các mô hình hoàn toàn tham số (Touvron et al., 2023a). Biến thể FactScore của phương pháp chúng tôi chỉ sử dụng truy xuất trong quá trình huấn luyện, tránh độ phức tạp thời gian suy luận.

Tương tự nhất với chúng tôi, một số phương pháp cố gắng ngăn chặn việc tạo ra lỗi thực tế ngay từ đầu, sử dụng các chiến lược gợi ý (Si et al., 2023) hoặc nhiễu loạn các biểu diễn nội tại của mô hình (Chuang et al., 2023; Li et al., 2023). Không giống như việc sử dụng heuristic cố định để xác định một chiều 'tính chính xác' nội tại, chúng tôi tối ưu hóa trực tiếp cho mục tiêu cuối cùng là tạo ra các phát biểu thực tế, mà chúng tôi thấy cho thấy cải thiện lớn hơn về tính chính xác. Cuối cùng, trong khi hầu hết công trình trước đây đã tập trung vào các nhiệm vụ NLG dạng ngắn như trả lời câu hỏi dạng ngắn (Kadavath et al., 2022), chúng tôi khám phá các cách để đo lường độ tin cậy mô hình về thông tin thực tế trong văn bản dài, không có cấu trúc và ước tính tính chính xác theo cách không cần tham chiếu (tức là không yêu cầu bất kỳ cơ sở tri thức bên ngoài hoặc gán nhãn nào).

6 Kết luận
Trong bài báo này, chúng tôi trình bày một chiến lược thực tế, hiệu quả để cải thiện khả năng của mô hình ngôn ngữ trong việc tạo ra nội dung thực tế, đặc biệt tập trung vào các thế hệ dài. Chúng tôi phát triển và nghiên cứu hai phương pháp khác nhau để ước tính tính chính xác của văn bản dài và tối ưu hóa cho các tiêu chí này bằng cách sử dụng học dựa trên ưu tiên. Ngoài các ước tính tính chính xác dựa trên tham chiếu hiện có tận dụng tri thức bên ngoài để thiết lập sự thật của một phát biểu cụ thể, chúng tôi giới thiệu một quy trình mới không cần tham chiếu để ước tính tính chính xác sử dụng sự bất định của chính mô hình ngôn ngữ như một dấu hiệu của tính chính xác. Các thí nghiệm của chúng tôi cho thấy rằng tinh chỉnh một mô hình ngôn ngữ với một trong hai tiêu chí giảm đáng tin cậy số lượng sự thật không chính xác (tức là ảo giác) mà mô hình tạo ra. Các phương pháp không cần tham chiếu như phương pháp chúng tôi đã giới thiệu cung cấp một chiến lược tự giám sát đặc biệt có thể mở rộng để cải thiện tính chính xác, loại bỏ nhu cầu về một kho văn bản 'vàng' tham chiếu.

Kết quả thí nghiệm gợi ý một số hướng cho công việc tương lai. Đầu tiên, do nghiên cứu hạn chế và do đó các chuẩn mực hạn chế về tính chính xác của các thế hệ mô hình ngôn ngữ dài, chúng tôi đề xuất hai nhiệm vụ mới để đánh giá phương pháp của chúng tôi. Những nhiệm vụ này đại diện nhưng không bao phủ hoàn toàn phạm vi các tình huống mà chúng ta hy vọng cải thiện tính chính xác. Hơn nữa, các thí nghiệm của chúng tôi cung cấp bằng chứng để cải thiện tính chính xác của các mô hình đối thoại đã được tinh chỉnh với RLHF, nhưng vẫn để ngỏ câu hỏi về cách tốt nhất để kết hợp các thưởng RLHF điển hình và các phương pháp với xếp hạng tính chính xác. Tương tự, khám phá các cách bổ sung để kết hợp tinh chỉnh tính chính xác với các phương pháp hiện có để cải thiện tính chính xác, chẳng hạn như trong thí nghiệm tinh chỉnh tính chính xác + DOLA của chúng tôi, có thể là một hướng nghiên cứu hiệu quả cho nghiên cứu tương lai. Cuối cùng, chúng tôi chỉ khám phá các mô hình 7B trong công trình này. Việc mở rộng công thức tinh chỉnh tính chính xác của chúng tôi lên các mô hình lớn hơn (và các tập dữ liệu ưu tiên lớn hơn) có thể giảm ảo giác thậm chí nhiều hơn nữa.

Lời cảm ơn
EM trân trọng ghi nhận tài trợ từ học bổng nghiên cứu sinh Knight-Hennessy và tài trợ Stanford Accelerator for Generative AI and Education. CF và CDM là các học giả CIFAR.

--- TRANG 11-16 ---
[Các trang tham khảo và phụ lục với các bảng ví dụ - nội dung chủ yếu là danh sách tham khảo học thuật và ví dụ về việc tạo ra tiểu sử bằng các phương pháp khác nhau]
