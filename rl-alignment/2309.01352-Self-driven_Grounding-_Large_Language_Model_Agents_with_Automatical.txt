# 2309.01352.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2309.01352.pdf
# File size: 1139549 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Self-driven Grounding: Large Language Model Agents with Automatical
Language-aligned Skill Learning
Shaohui Peng1, Xing Hu2, Qi Yi2, 5, Rui Zhang2, 4, Jiaming Guo2, Di Huang2,
Zikang Tian1, Ruizhi Chen1, 3, Zidong Du2, 4, Qi Guo2, Yunji Chen2, 3, Ling Li1, 3
1Intelligent Software Research Center, Institute of Software, CAS
2SKL of Processors, Institute of Computing Technology, CAS
3University of Chinese Academy of Sciences
4Cambricon Technologies
5University of Science and Technology of China
pengshaohui@iscas.ac.cn
Abstract
Large language models (LLMs) show their powerful auto-
matic reasoning and planning capability with a wealth of
semantic knowledge about the human world. However, the
grounding problem still hinders the applications of LLMs in
the real-world environment. Existing studies try to fine-tune
the LLM or utilize pre-defined behavior APIs to bridge the
LLMs and the environment, which not only costs huge human
efforts to customize for every single task but also weakens
the generality strengths of LLMs. To autonomously ground
the LLM onto the environment, we proposed the Self-Driven
Grounding (SDG) framework to automatically and progres-
sively ground the LLM with self-driven skill learning. SDG
first employs the LLM to propose the hypothesis of sub-goals
to achieve tasks and then verify the feasibility of the hypothe-
sis via interacting with the underlying environment. Once ver-
ified, SDG can then learn generalized skills with the guidance
of these successfully grounded subgoals. These skills can be
further utilized to accomplish more complex tasks which fail
to pass the verification phase. Verified in the famous instruc-
tion following task set-BabyAI, SDG achieves comparable
performance in the most challenging tasks compared with im-
itation learning methods that cost millions of demonstrations,
proving the effectiveness of learned skills and showing the
feasibility and efficiency of our framework.
1 Introduction
Large language models (LLMs) have shown their powerful
capability in automatic reasoning and planning with a wealth
of semantic knowledge about the human world (Wei et al.
2022, 2021; OpenAI 2023; Kojima et al. 2022). However,
there still remains a large gap in adopting LLMs to auto-
matically solve problems in specific environments. This is
because of the misalignment between the LLM’s semantic
planning and the grounded-specific implementation, which
is also known as the grounding problem (Ichter et al. 2022;
Driess et al. 2023). Solving this problem can unlock the
LLMs’ capacity of understanding and affecting the real
world, which is a solid step towards real-world applications
of artificial intelligence.
To address the grounding problem, existing studies try to
fine-tune the LLM to predict feasible actions (Carta et al.
2023; Li et al. 2022; Wang et al. 2022) or utilize a set ofbehavior APIs (i.e. low-level skills) that serve as a bridge
between the LLMs and the environment (Ichter et al. 2022;
Raman et al. 2022; Liang et al. 2022). On the one hand, fine-
tuning LLMs is of low sample efficiency and may also dam-
age the reasoning ability of LLMs. On the other hand, exist-
ing methods relying on behavior APIs often assume the APIs
are pre-defined by the environment (Liang et al. 2022) or
pre-trained using expert demonstrations (Ichter et al. 2022),
which not only costs huge human efforts to customize for ev-
ery single task but also weakens the generality strengths of
LLMs. Therefore, how to autonomously ground the LLM
onto the environment still remains an open problem and is
the key challenge of LLM-based agents.
It is challenging to achieve the goal of autonomous
grounding that maps the LLM’s semantic plan to practi-
cal implementation, because of the following reasons: 1)
The prerequisite of grounding, obtaining successful experi-
ences, is difficult because of the sparse rewards in the phys-
ical world. 2) Even obtaining rare success experiences, the
grounding is usually closely related to specific tasks with-
out a shared API library, therefore is of low generality and
invaluable for general tasks. To address these issues, we pro-
duce intrinsic rewards based on LLM-generated subgoals
and their check functions, which increase successful expe-
riences by alleviating the sparse reward issue. We then pro-
pose the language-aligned general skill learning methodol-
ogy by forcing each skill to achieve a group of goals with
similar semantic descriptions. These skills show good gen-
erality in solving other or even more complex tasks.
In summary, we propose a Self-Driven Grounding (SDG)
framework that intimately combines the LLM and the rein-
forcement learning process within the following key stages:
1) Hypothesis: the LLM not only acts as the planner by
decomposing tasks into small subgoals but also provides
the check functions so that RL agents can evaluate whether
they can complete these subgoals. Such intrinsic rewards
from the LLM significantly alleviate the sparse reward issue.
2) Verification: with the subgoals and corresponding check
functions, RL agents learn the policies of the subgoals based
on intrinsic rewards and finally are verified through whether
the tasks are accomplished. 3) Induction: RL agents cluster
verified subgoals through semantic similarity and learn gen-arXiv:2309.01352v1  [cs.CL]  4 Sep 2023

--- PAGE 2 ---
eralized skill policy upon them. With these general skills,
the LLM can generate solutions for unseen or even more
complex tasks through minimal and efficient interaction.
We validate the self-driven grounding framework in in-
struction following tasks, which is common and reason-
able for LLM-based agents because of textural task instruc-
tion. Verified in BabyAI, a grid world platform to study
language-grounded tasks, our automatic grounding frame-
work achieves comparable performance in the most difficult
tasks compared with imitation learning methods that cost
millions of demonstrations. The experiment results not only
proves the effectiveness of learned skills but also show the
feasibility and efficiency of our framework.
2 Related Work
LLM-assisted Agents LLMs show their great power in
automatic reasoning and planning with a wealth of semantic
knowledge about the human world. Therefore, it is promis-
ing to involve LLMs in developing intelligent agents. The
key challenge in LLM-assisted agents is how to ground the
LLM’s knowledge (in linguistic form) to the tasks at hand.
Regarding this challenge, there are two mainstream meth-
ods: (1) utilizing a set of behaviour APIs with detailed lin-
guistic annotations. Such APIs can be pre-defined by the
environment (Liang et al. 2022) or pre-trained using expert
demonstrations (Ichter et al. 2022; Huang et al. 2022; Yuan
et al. 2023). For example, Code as Policies (Liang et al.
2022) uses the LLM to generate executable codes for ac-
complishing instructions which can invoke behaviour APIs
under certain conditions. SayCan (Ichter et al. 2022) invites
humans to rate the success of given demonstrations, which
are utilized to train API policies and then derive an affor-
dance function. V oyager (Wang et al. 2023) stores and re-
trieves executable code, which calls pre-implemented basic
APIs, to handle complex scenarios. To better interact with
the environment, some approaches (Huang et al. 2022; Ra-
man et al. 2022) introduce the environment feedback to re-
generate new plans, which can be seen as another kind of
trial-and-error learning. Although these methods have made
impressive progress by utilizing APIs, their applications are
also limited by the behaviour APIs in that the agent can only
accomplish tasks that can be solved by arranging these ba-
sic APIs. (2) fine-tuning the LLMs. The LLM can be fine-
tuned to predict the agent’s feasible action given the state
descriptions. Such fine-tuning can be performed using ex-
pert demonstrations (Wang et al. 2022; Li et al. 2022) or on-
line RL (Carta et al. 2023). However, fine-tuning a model as
large as the LLM is quite time expensive and requires much
training data.
Instruction Following In instruction following, an agent
is given an instruction and the goal is to accomplish the
task described by the instruction. Such a paradigm makes
the agent able to assist human beings by following human
instructions, which has wide real-world applications. The
works for instruction following can be divided into three
categories: (1) Semantic-parsing methods (Artzi and Zettle-
moyer 2013; Misra et al. 2016), which directly parses the
instruction into the agent’s actions via lexical analysis andother pre-defined rules. These methods require great human
efforts to design proper rules, and can not generalize to com-
plex environments. (2) Learning-based methods, which di-
rectly train a language-conditioned policy to accomplish in-
structions. Many prior works require expert demonstrations
in their training loops. For example, expert demonstrations
are often used in policy imitation learning (Lynch and Ser-
manet 2021; Chaplot et al. 2018), hindsight instruction re-
labelling (R ¨oder, Eppe, and Wermter 2022; Chen, Gupta,
and Marino 2021), and learning the language-conditioned
reward function (Bahdanau et al. 2019). Some works try to
sidestep the need for expert demonstrations (Ranzato et al.
2021; hig 2020), but at the cost of much lower sample effi-
ciency. All learning-based approaches are typically trained
using hard-coded instruction templates, which can not pro-
vide diverse, ambiguous and long-term planning instruc-
tions as humans. Therefore, they can only deal with sim-
ple and low-level instructions such as pick-and-place tasks.
(3) LLM-based methods, which use LLMs to assist the un-
derstanding and planning of instructions (Ichter et al. 2022;
Liang et al. 2022; Raman et al. 2022). See the last paragraph
for more details.
3 Preliminaries
3.1 Problem Formulation
We consider adopting an LLM-based agent to solve instruc-
tion following (IF) tasks.Each instruction I∈Tdescribes a
task coarsely in the environment. Given the instruction, only
when the agent accomplishes the task using primitive action
setAcan receive a positive reward from the environment.
For example, in BabyAI, which is a famous instruction fol-
lowing task set, instructions like “Open the green door” or
“Put the red box next to the blue ball” specify some macro-
scopic object manipulation tasks, while the agent needs to
accomplish them in a grid world using primitive actions like
“turn right”, “move forward”, “pick” and so on.
An LLM-based agent takes instructions Iand environ-
ment observation oas input and outputs actions to accom-
plish tasks. As shown in Figure 1, the general framework of
LLM-based agents contains a high-level planner and some
low-level skills that can unitize the semantic knowledge in
the LLM to accomplish instruction following tasks. Given
the coarse instruction, the planner (often LLMs) will de-
compose it into a sub-instruction sequence or generate a
program to solve it. At the same time, the low-level skills
consist of pre-trained policies or pre-implemented scripts to
execute the plan or program. Researchers usually assume the
environment provides textual descriptions of state and task-
related feedback to adapt to the LLM setting. With the im-
mediate translation from the semantic output to execution
in the environment through low-level APIs, the high-level
planner can get adequate feedback and iteratively refine its
plan to accomplish tasks.
Current methods leverage semantic knowledge in the
high-level planner to reason and decompose the coarse in-
struction, but meanwhile, bypass the grounding problem by
translating the semantic plan into implementation through
pre-defined low-level APIs. In order to maximize the use

--- PAGE 3 ---
of priors in LLMs to reduce human effort, our framework
aims to automatically learn generalized skills in the en-
vironment to build low-level APIs to solve the ground-
ing problem .Besides, in this paper, we assume the LLM
could call basic perception functions P={p1, p2, ...}, like
“get observed objects()”, to get environment status instead
of pre-designed textual observation and feedback mecha-
nisms.
3.2 Challenges
The essential problem of building an LLM-based agent is
to ground semantic knowledge of LLM in the environment.
To automatically solve the grounding problem, there are two
main challenges:
• How to obtain successful grounding experiences from
scratch? Without pre-defined low-level APIs, the agent
cannot interact with the environment to attempt the se-
mantic plan directly and efficiently explore the reward
of task accomplishment. To address the challenge, we
make the LLM hypothesize the plan for each task and
generate corresponding checks for each sub-step. Based
on these intrinsic rewards provided by checks, we can
quickly train small policies to execute and verify the plan,
then collect successful trajectories as grounding experi-
ences. Although such experience obtained through quick
attempts may belong to simple tasks, we can also lever-
age them to enhance the grounding ability of the agent to
accomplish more complex and long-term tasks progres-
sively.
• How to efficiently train generalized low-level behavior
APIs under the guidance of experience? The subgoals in
successful experience are proposed by the LLM based on
specific instruction, and cannot be applied to new scenar-
ios as general behavior APIs. Inspired by inductive ideas
in mathematics, we introduce a mechanism to group sub-
goals with similar semantics together, then train skill
policies that can achieve a group of subgoals as gener-
alized behavior APIs. Unlike standard online reinforce-
ment learning, which suffers from data efficiency issues,
we make skills training efficient through dense rewards
provided by checks, and initial state restoration by the
successful trajectories.
4 Method
In this section, we will first give an overview of our proposed
Self-Driven Grounding framework, which can automatically
and progressively ground LLM in the environment.
4.1 Overview
As shown in Figure 2, SDG can be divided into four phases.
•Hypothesis : For each instruction, LLM tries to decom-
pose it into subgoals and generate check functions for
each subgoal.
•Verification : Based on the reward provided by the check
function, we train separate policies for each subgoal
within limited steps until the task accomplishes to ver-
ify the feasibility of the hypothesis of LLM.
0810 Method对比图 version4
Low-level Skills Hypothesis
Verification
Induction
Expert 
experience
Self-driven skill learningOthers
OursHigh-level Planner 
Reinforcement LearningLarge Language ModelsLLM-based AgentFigure 1: A general framework of LLM-based agents
•Induction : We group the subgoals in successful hypothe-
ses with similar semantics to train generalized skills re-
inforcement learning.
•Deduction : Based on learned skills as low-level actors,
we use LLM as a few-shot high-level planner to generate
programs to solve unseen and more complex tasks.
4.2 Hypothesis
The hypothesis phase aims to solve tasks separately regard-
less of generality to collect grounding experience. Consider-
ing the gap between the semantic knowledge in LLM and the
environment, the hypothesis phase decomposes the task into
several subgoals rather than directly giving the solution, and
leaves correctness verification to the next process. As shown
in Figure2(a), the hypothesis can be formed as IPrompt− − − − − →
G, F . We use LLM as the zero-shot planner, which takes
an instruction Iand necessary decomposition prompt as in-
put, then outputs a subgoal sequence G={g1, g2, ...}and
corresponding check functions F={f1, f2, ...}. The sub-
goalgiis a small instruction labeled with the explicit mark
“Goal X” to facilitate further processing. Each check func-
tionfi:S→ { 0,1}is a program that checks the achieve-
ment of corresponding sub-goal givia invoking perception
functions provided by the environment. To make the LLM
output as we want, except for the task instruction I, we add
role definition, perception API descriptions, and explanation
of the instruction following task space to the decomposition
prompts.
4.3 Verification
After getting the subgoals and check functions, we need to
verify their feasibility in the environment to collect success-
ful grounding experiences.The feasibility of decomposition
is verified by the consistency between achieved signals gen-
erated by check functions and the tasks accomplished sig-
nal. Specifically, as shown in Figure 2(b), we train indepen-
dent policies for each subgoal based on the bool reward pro-
vided by its check function. Once the subgoal is achieved,
we stop the training and save the action sequence. The saved
action sequence can be used as the restoring mechanism to
prepare the initial state for further skill training. Until all

--- PAGE 4 ---
Goal X
Check X
Restore
ActionsGoal X
Check X
Restore
Actionsput a green box next 
to a red ball and open 
the red door in front 
of you, then pick up 
the green ballput a green box next 
to a red ball and open 
the red door in front 
of you, then pick up 
the green ball0808 Method细节
…
Perceptions:
Get_observation(): …
Get_visited_rooms(): …
…
Instruction: Pick the Green BoxDecomposition Prompt
Goal 1: discover the green box
Def Check_1():
    return “Green Box” in Get_obs…
Goal 2: …
…
Initial State + Check_1
Policy for Goal 1
Goal 1 state + Check_2
…
Verification
EnvironmentSuccessful 
ExperienceGoal X
Check X
Restore
Actions
SkillsTasksput a green box next 
to a red ball and open 
the red door in front 
of you, then pick up 
the green ball
…
APIs:
Discover(): This API ...
Pick(): …
…
Instruction: put a green box next...Generation / 
Debugging Prompt
# python
# task 1: put a green ...
def put_green_box_next_red_ball():
...
# main
put_green_box_next_red_ball()
...Restore
ActionsCheck X
TrainingEnvironmentParametersTasks pick the green box
Restore
ActionsCheck YParameters
…(a) Hypothesis
(b) Verification(c) Induction (d) Deduction
Multi-task
Reinforcement 
learningClusteringLLMGoal X
ParametersAPISemantic 
embedding
LLM
LLMLLMFigure 2: Overview of Self-Driven Grounding (SDG) framework. (a) Hypothesis: try to decompose tasks; (b) Verification:
leverage efficient interaction with the environment to verify the results of hypothesis; (c) Induction: group successful experience
to train generalized skills; (d) Deduction: build few-shot planner to solve tasks using acquired skills.
sub-goals are achieved and the task is also accomplished,
we can verify the decomposition is successful. Meanwhile,
the grounding experiences (including subgoal descriptions,
check functions, and restore action sequences) are collected
for skill learning in the future phase. Considering some com-
plex and long-term tasks cannot be solved by direct decom-
position, the above mechanism is only suitable for solving
simple tasks within an acceptable environment interaction
steps to collect experience. We set the maximum number of
verification steps Tverify as a threshold to distinguish in-
tractable complex tasks for the next stage to solve.
4.4 Induction
After collecting successful grounding experiences, the in-
duction phase aims to discover and learn generalized skills
from separate grounding trajectories of different instructions
so that we can reuse them in more unseen and complex tasks.
Discovery As described above, we have collected task-
solving experience through efficient hypothesis and verifi-
cation, including subgoal descriptions, corresponding check
functions, and start state restore action sequences. However,
such successful experiences can only be used on specific in-
structions that are easy to decompose for LLM. To make
the LLM-based agent able to solve more unseen and com-
plex tasks, we must further abstract and learn skills to build
the generalized low-level actor.To this end, we cluster thecollected subgoals according to their semantics to ensure a
certain generalized skill can accomplish a category of sub-
goals as shown in Figure 2(c). Specifically, we first use LLM
to translate each subgoal description giinto API descrip-
tiongi,api and parameter gi,param . For example, the sub-
goal “discover the green box” are translated into the API
“discover” and parameter “green box”. Then we use the k-
means algorithm to conduct unsupervised clustering based
on the semantic distance between subgoal descriptions com-
puted by the following cosine similarity:
C(gi,api, gj,api) =emb(gi,api)·emb(gj,api)
||emb(gi,api)|| · ||emb(gj,api)||,(1)
where gi,api is the API description and emb(·)is the embed-
ding functions of LLM.
Training We have divided subgoals with similar seman-
tics into different categories through the clustering pro-
cess.Then we build reinforcement learning (RL) environ-
ments to train skills that can achieve a cluster of subgoals
separately. Unlike common RL environments, the skill train-
ing environment is like a multitask learning scenario. Each
subgoal of a cluster can be seen as a single task, the subgoal
parameter gi,param is the task description, the reward is pro-
vided by the corresponding check function fi, and the initial
state is set by the saved restore action sequence. Trained in
such a multiple tasks environment consists of subgoals that

--- PAGE 5 ---
2023/8/16 09:34 Boss Level - MiniGrid Documentation
https://minigrid.farama.org/environments/babyai/BossLevel/ 1/4Boss Level
Action Space Discrete(7)
Observation
SpaceDict('direction':  Discrete(4),  'image': Box(0, 255, (7, 7, 3), uint8), 'mission':  MissionSpace(<function
BabyAIMissionSpace._gen_mission  at 0x7f87c5855700>,  None))
Reward Range (0, 1)
Creation gymnasium.make("BabyAI-BossLevel-v0")
Description
Command can be any sentence drawn from the Baby Language grammar. Union of all competencies. This level is a superset of all
other levels.Back to topMiniGrid Documentation
v2.3.1 (latest)This page uses Google Analytics to collect statistics. Deny AllowInstruction: 
open the red door on your left , 
pick up a green box  and  then 
put the purple key next to a  red 
ball
Action Space:
Turn left
Turn right
Move forward
Pickup
Drop
Toggle
DoneFigure 3: A example of BossLevel task in BabyAI
have the same semantics, the skill is supposed to have gen-
eralization in tasks with similar scenarios. Besides, subgoals
belonging to the same cluster are divided into training and
verification sets to monitor the generalization ability of the
trained skill to prevent overfitting.
4.5 Deduction
Through the above process, we have overcome the chal-
lenges of obtaining successful grounding experience from
scratch and training generalized skills efficiently. In other
words, we have autonomously built the low-level skills for
the LLM-based agent without human effort. To apply auto-
matically learned skills to solve unseen and complex instruc-
tions following tasks, we next introduce the high-level plan-
ner in this section. As shown in Figure 2(d), we use LLM
as a few-shot planner to generate programs to accomplish
tasks. The process can be divided into program generation
phase and debugging phase.
Generation The generation prompt for LLM contains the
role definition and API descriptions (including skills and
perception functions). Besides, considering the complexity
of the task, the generation prompt follows a few-shot in-
context learning paradigm. We include some skill API de-
scriptions and a hand-written example that leverages the
learned skills to solve a complex instruction-following task.
Debugging To better solve complex tasks, we also de-
signed an interaction debugging process in our high-level
planner. Besides the task instruction and the generated pro-
gram to be modified, the debugging prompt also includes
the error message and some general debugging suggestions
to fix the possible bugs. Benefiting from the feasibility and
robustness of adaptive learned skills, the basic error report-
ing mechanism based on illegal action detection can ef-
fectively improve the accuracy of the generated programs,
which greatly reduces human effort.
5 Results
5.1 Experiment Setting
Environment To evaluate the efficiency and effectiveness
of our proposed framework that automatically discovers,Task Method Type Demos rate
GoToLocalOriginal IL 10K 99.8
LID-Text IL 10K 99.5
SDG-action LLM 0 55.1
SDG LLM 0 99.9
PickupLocOriginal IL 10K 99.8
LID-Text IL 10K 99.8
SDG-action LLM 0 47.6
SDG LLM 0 92.9
PutNextLocalOriginal IL 10K 97.7
LID-Text IL 10K 99.9
SDG-action LLM 0 0
SDG LLM 0 91.9
OpenOriginal IL 1M 100
SDG-action LLM 0 0
SDG LLM 0 92.4
SynthSeqOriginal IL 1M 87.7
LISA IL 100K 61.2
SDG-action LLM 0 0
SDG LLM 0 78.9
BossLevelOriginal IL 1M 84.3
LISA IL 100K 69.8
SDG-action LLM 0 0
SDG LLM 0 75.9
Table 1: Overall results. “IL” means “Imitation Learning”,
“LLM” means “LLM-based agent”.
learns, and applies skills, we test SDG on the BabyAI en-
vironment (Chevalier-Boisvert et al. 2019). BabyAI is a grid
world environment for instruction following. Given the lan-
guage instruction and a 7×7×3partial and local view, the
agent must learn to accomplish various tasks of arbitrary dif-
ficulty levels. In this paper, we choose the following six lev-
els of instruction with different types and difficulties (more
details of the environment can be found in the Appendix):
•GoToLocal : Go to an object inside a single room.
•PickupLocal : Pick up an object inside a single room.
•PutNextLocal : Pick up an object and put it next to an-
other object inside a single room.
•Open : Open a door in a 3×3room maze, the door may
in another room.
•SynthSeq : Union of all instructions from PutNext, Open,
Goto, and PickUp and may with multiple commands.
•BossLevel : The hardest task of BabyAI as shown in 3.
The command can be any sentence drawn from the Baby
Language grammar.
Baselines We verify the effectiveness of SDG by compar-
ing it with several baselines, including Imitation Learning
methods relying on expert demonstrations and a variant of
our LLM-based agent.
•Original : The baseline from the original BabyAI paper,
which trained the GRU + CONV model with imitation
learning using one million demonstration episodes for
each level.

--- PAGE 6 ---
0.700.750.800.850.900.951.00ratediscover_object
Restore Train
Restore T est
w/o Restore Train
w/o Restore T est0.00.20.40.60.81.0pick
0.40.60.81.0go_next_to
0.20.40.60.81.0ratediscover_door
0.00.20.40.60.81.0open_door
0.00.20.40.60.81.0put_next_to
0.00 0.25 0.50 0.75 1.00
steps 1e50.60.70.80.91.0ratefind_unexplored_door
0.00 0.25 0.50 0.75 1.00
steps 1e50.40.60.81.0enter_unexplored_room
0 2 4
steps 1e50.00.20.40.60.81.0go_to_doorFigure 4: Training curves of skills. The blue curve repre-
sents skill learning through restoring the initial state, while
the green curve didn’t. The dotted curve represents test per-
formance in the held-out verification set.
00.20.40.60.811.2
GoToLocalPickupLocPutNextLocal Open SynthSeq BossLevelInteraction=1Interaction=2Interaction=3Interaction=4
Figure 5: Successful rates of different interaction times
•LID-Text (Li et al. 2022) : An approach that first rep-
resents goals and observations as a sequence of embed-
dings, then uses a policy network initialized with a pre-
trained LM and trained with demonstrations to predict
the next action.
•LISA (Garg et al. 2022) : A hierarchical imitation learn-
ing framework that can learn diverse, interpretable skills
from language-conditioned demonstrations.
•SDG-action : The variant of our framework which main-
tains the same high-level planner but employs primitive
actions instead of the acquired skills.
Implementation In this paper, we use ChatGPT (GPT-
3.5-turbo) as the large language model to complete task
decomposition, the semantic embedding of API, high-level
planning, and debugging. More details on the prompt con-
tents are shown in the Appendix. In the verification and
skill learning phase, we use the standard model proposed
00.20.40.60.811.2
GoToLocalPickupLocPutNextLocal Open SynthSeq BossLevelAttempts=1Attempts=5Figure 6: Successful rates of different attempts times
0 500 1000 1500 2000 2500 3000
steps0.00.10.20.30.40.50.60.70.8successful rateGoT oLocal
PickupLoc
PutNextLocal
Open
SynthSeq
BossLevel
Figure 7: Task verification results
in BabyAI, and train the policy using the PPO algorithm.
5.2 Overall Results Comparison
The main performance results are shown in Table 1. We
separately compare SDG with baselines in each level task.
For each level task, we randomly sample 100 instructions
that never occurred in the skill training phase. Considering
the randomness of ChatGPT’s answers, we repeat the ex-
periment of each instruction 3 times to get the average re-
sults. The results showed that SDG can achieve comparable
performance using automatically learned skills rather than a
large number of expert demonstrations, which shows the ef-
fectiveness of our framework. Besides, the results of SDG-
action showed it failed to accomplish harder tasks without
acquired skill, which also emphasizes the importance of self-
driven skill learning in the LLM grounding scenario.
5.3 Ablation Results
Skill Learning Ablation We first investigate the data ef-
ficiency when learning skills through reinforcement learn-
ing as shown in Figure 4 . Although the difficulty varies be-
cause the clustering process only relies on the semantics of
subgoals and ignored the difficulty, skills can be learned ef-
ficiently. The verification phase proves that the instruction
has been decomposed into small-enough subgoals and can
be trained in limited steps. Therefore, the skill training en-
vironment consisting of verified subgoals can lead to an ef-
ficient skill training process. Besides, results also demon-
strate that restoring actions that make the skill start at an ex-
pected state is important to learning efficiency. Without the

--- PAGE 7 ---
start state reset, the learning efficiency drops obviously, and
some skills even cannot be learned. Some learning curves
show overfitting trends in the late period, which is different
from the normal reinforcement learning process and Illus-
trates the role of the hold-out validation set. For some skills
like “enter unexplored room”, the green curves seem better
than the blue ones. This is because without restoring the ini-
tial state, the difficulty decreases a lot since the agent can
enter any room.
Deduction Ablation We also explored the effect of the in-
teraction debugging times and multiple attempts of skills.
The ablation results show the importance of interactive de-
bugging and multiple attempts.
A. Interaction Times: Figure 5 shows the results of dif-
ferent interaction times between the high-level planner and
the environment. For some tasks like “GoToLocal”, the suc-
cessful rate promotion is limited because of simplicity. For
complex tasks like “BossLevel”, repeat debugging can bring
more than 40% promotion, which shows the adaptability and
feasibility of learned skills. However, when the performance
reaches some ceiling bound, more interaction seems useless.
B. Multiple Attempts: Different from pre-defined APIs
with scripts, our learned skills are stochastic policies. Thus,
we also investigate the effect of multiple attempts of skill
policy on the final success rate. Figure 6 shows similar re-
sults with interaction times, the multiple attempts improve
complex tasks greater.
5.4 Method Details
To show more insight into SDG, we show some key inter-
mediate results.
Task Verification Figure 7 shows the verification results
of different level tasks. In the implementation, we random
sample 100 instructions from each level task, and set the
verification steps threshold Tverify equals to 3000. The re-
sults prove our assumption, for some simple task levels, like
“GoToLocal”, “PickupLoc” and “PutNextLocal”, the LLM
can decompose the instruction into reasonable subgoals and
check functions, so that the verification training be success-
ful in limited steps. For hard levels which also include some
simple tasks, direct decomposition can accomplish few in-
structions, which means it cannot solve complex and long-
term tasks.
Skill Clustering In the skill discovery phase, we use the k-
means clustering algorithm to cluster subgoals into different
groups based on their semantic similarity. On the choice of
k, we compute the Calinski-Harabasz (CH) score, which is
a commonly used metric for clustering results, from k= 2
to16as shown in Figure 8, and finally chose k= 9 with
the highest CH score. We also show the clustering results of
semantic embedding through the TSNE technique in Figure
9. However, unsupervised clustering may cause some uncer-
tainty in the skill discovery process, we leave further investi-
gating and minimizing the effect of clustering results on our
framework as future works.
BabyAI-Open-v0Figure 8: Task clustering results
50070090011001300150017001900
2345678910111213141516Calinski-Harabasz Score
k图表标题
Figure 9: Calinski-Harabasz Scores of Different k
6 Conclusion
i24 In this paper, we propose a framework called Self-Driven
Grounding (SDG) to address the challenge of automatically
grounding LLM onto specific environments. In order to al-
leviate the problem of grounding experience acquisition, we
make the LLM not only decompose tasks but also gener-
ate intrinsic rewards to help RL agents efficiently verify the
decomposition results. We also propose a language-aligned
general skill learning methodology by forcing each skill
to achieve a group of goals with similar semantic descrip-
tions to enhance their generality. Compared with imitation
learning methods that cost millions of demonstrations, SDG
can achieve comparable performance in the hardest tasks in
BabyAI. The ablation study also shows the flexibility and
feasibility of learned skills in the interactions between the
high-level planner and the environment.
However, limited by the simple way of perceiving the en-
vironment status, SDG can only deal with tasks with tex-
tual descriptions. This problem can be addressed by intro-
ducing multi-modal LLMs, which will extend the applica-
tions of SDG. Besides, SDG only contains a single cycle of
hypothesis, verification, and induction. It is an interesting
and promising direction to design a mechanism of multiple
cycles in SDG, allowing SDG to learn more powerful and
diverse hierarchical skills to accomplish more flexible tasks.

--- PAGE 8 ---
References
2020. 2020 IEEE Symposium Series on Computational In-
telligence, SSCI 2020, Canberra, Australia, December 1-4,
2020 . IEEE. ISBN 978-1-7281-2547-3.
Artzi, Y .; and Zettlemoyer, L. 2013. Weakly Supervised
Learning of Semantic Parsers for Mapping Instructions to
Actions. Trans. Assoc. Comput. Linguistics , 1: 49–62.
Bahdanau, D.; Hill, F.; Leike, J.; Hughes, E.; Hosseini, S. A.;
Kohli, P.; and Grefenstette, E. 2019. Learning to Understand
Goal Specifications by Modelling Reward. In 7th Interna-
tional Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019 .
Carta, T.; Romac, C.; Wolf, T.; Lamprier, S.; Sigaud, O.;
and Oudeyer, P. 2023. Grounding Large Language Mod-
els in Interactive Environments with Online Reinforcement
Learning. CoRR , abs/2302.02662.
Chaplot, D. S.; Sathyendra, K. M.; Pasumarthi, R. K.; Ra-
jagopal, D.; and Salakhutdinov, R. 2018. Gated-Attention
Architectures for Task-Oriented Language Grounding. In
Proceedings of the Thirty-Second AAAI Conference on Ar-
tificial Intelligence, (AAAI-18), the 30th innovative Applica-
tions of Artificial Intelligence (IAAI-18), and the 8th AAAI
Symposium on Educational Advances in Artificial Intelli-
gence (EAAI-18), New Orleans, Louisiana, USA, February
2-7, 2018 , 2819–2826.
Chen, V .; Gupta, A.; and Marino, K. 2021. Ask Your Hu-
mans: Using Human Instructions to Improve Generaliza-
tion in Reinforcement Learning. In 9th International Con-
ference on Learning Representations, ICLR 2021, Virtual
Event, Austria, May 3-7, 2021 .
Chevalier-Boisvert, M.; Bahdanau, D.; Lahlou, S.; Willems,
L.; Saharia, C.; Nguyen, T. H.; and Bengio, Y . 2019.
BabyAI: A Platform to Study the Sample Efficiency of
Grounded Language Learning. In 7th International Con-
ference on Learning Representations, ICLR 2019, New Or-
leans, LA, USA, May 6-9, 2019 .
Driess, D.; Xia, F.; Sajjadi, M. S. M.; Lynch, C.; Chowdh-
ery, A.; Ichter, B.; Wahid, A.; Tompson, J.; Vuong, Q. H.;
Yu, T.; Huang, W.; Chebotar, Y .; Sermanet, P.; Duck-
worth, D.; Levine, S.; Vanhoucke, V .; Hausman, K.; Tous-
saint, M.; Greff, K.; Zeng, A.; Mordatch, I.; and Florence,
P. R. 2023. PaLM-E: An Embodied Multimodal Language
Model. ArXiv , abs/2303.03378.
Garg, D.; Vaidyanath, S.; Kim, K.; Song, J.; and Ermon, S.
2022. LISA: Learning Interpretable Skill Abstractions from
Language. ArXiv , abs/2203.00054.
Huang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; Florence,
P.; Zeng, A.; Tompson, J.; Mordatch, I.; Chebotar, Y .; Ser-
manet, P.; Jackson, T.; Brown, N.; Luu, L.; Levine, S.; Haus-
man, K.; and Ichter, B. 2022. Inner Monologue: Embod-
ied Reasoning through Planning with Language Models. In
Conference on Robot Learning, CoRL 2022, 14-18 Decem-
ber 2022, Auckland, New Zealand , 1769–1782.
Ichter, B.; Brohan, A.; Chebotar, Y .; Finn, C.; Hausman, K.;
Herzog, A.; Ho, D.; Ibarz, J.; Irpan, A.; Jang, E.; Julian, R.;
Kalashnikov, D.; Levine, S.; Lu, Y .; Parada, C.; Rao, K.; Ser-
manet, P.; Toshev, A.; Vanhoucke, V .; Xia, F.; Xiao, T.; Xu,P.; Yan, M.; Brown, N.; Ahn, M.; Cortes, O.; Sievers, N.;
Tan, C.; Xu, S.; Reyes, D.; Rettinghouse, J.; Quiambao, J.;
Pastor, P.; Luu, L.; Lee, K.; Kuang, Y .; Jesmonth, S.; Joshi,
N. J.; Jeffrey, K.; Ruano, R. J.; Hsu, J.; Gopalakrishnan, K.;
David, B.; Zeng, A.; and Fu, C. K. 2022. Do As I Can, Not
As I Say: Grounding Language in Robotic Affordances. In
Conference on Robot Learning, CoRL 2022, 14-18 Decem-
ber 2022, Auckland, New Zealand , 287–318.
Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y .; and Iwasawa,
Y . 2022. Large Language Models are Zero-Shot Reasoners.
ArXiv , abs/2205.11916.
Li, S.; Puig, X.; Paxton, C.; Du, Y .; Wang, C.; Fan, L.;
Chen, T.; Huang, D.; Aky ¨urek, E.; Anandkumar, A.; An-
dreas, J.; Mordatch, I.; Torralba, A.; and Zhu, Y . 2022. Pre-
Trained Language Models for Interactive Decision-Making.
InNeurIPS .
Liang, J.; Huang, W.; Xia, F.; Xu, P.; Hausman, K.; Ichter,
B.; Florence, P.; and Zeng, A. 2022. Code as Policies:
Language Model Programs for Embodied Control. CoRR ,
abs/2209.07753.
Lynch, C.; and Sermanet, P. 2021. Language Conditioned
Imitation Learning Over Unstructured Data. In Robotics:
Science and Systems XVII, Virtual Event, July 12-16, 2021 .
Misra, D. K.; Sung, J.; Lee, K.; and Saxena, A. 2016. Tell
me Dave: Context-sensitive grounding of natural language
to manipulation instructions. Int. J. Robotics Res. , 35(1-3):
281–300.
OpenAI. 2023. GPT-4 Technical Report. ArXiv ,
abs/2303.08774.
Raman, S. S.; Cohen, V .; Rosen, E.; Idrees, I.; Paulius, D.;
and Tellex, S. 2022. Planning with Large Language Models
via Corrective Re-prompting. CoRR , abs/2211.09935.
Ranzato, M.; Beygelzimer, A.; Dauphin, Y . N.; Liang, P.;
and Vaughan, J. W., eds. 2021. Advances in Neural Informa-
tion Processing Systems 34: Annual Conference on Neural
Information Processing Systems 2021, NeurIPS 2021, De-
cember 6-14, 2021, virtual .
R¨oder, F.; Eppe, M.; and Wermter, S. 2022. Grounding
Hindsight Instructions in Multi-Goal Reinforcement Learn-
ing for Robotics. In IEEE International Conference on De-
velopment and Learning, ICDL 2022, London, United King-
dom, September 12-15, 2022 , 170–177.
Wang, G.; Xie, Y .; Jiang, Y .; Mandlekar, A.; Xiao, C.;
Zhu, Y .; Fan, L.; and Anandkumar, A. 2023. V oyager: An
Open-Ended Embodied Agent with Large Language Mod-
els.CoRR , abs/2305.16291.
Wang, R.; Jansen, P. A.; C ˆot´e, M.; and Ammanabrolu, P.
2022. ScienceWorld: Is your Agent Smarter than a 5th
Grader? In Proceedings of the 2022 Conference on Em-
pirical Methods in Natural Language Processing, EMNLP
2022, Abu Dhabi, United Arab Emirates, December 7-11,
2022 , 11279–11298.
Wei, J.; Bosma, M.; Zhao, V .; Guu, K.; Yu, A. W.; Lester, B.;
Du, N.; Dai, A. M.; and Le, Q. V . 2021. Finetuned Language
Models Are Zero-Shot Learners. ArXiv , abs/2109.01652.

--- PAGE 9 ---
Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; hsin Chi,
E. H.; Xia, F.; Le, Q.; and Zhou, D. 2022. Chain of Thought
Prompting Elicits Reasoning in Large Language Models.
ArXiv , abs/2201.11903.
Yuan, H.; Zhang, C.; Wang, H.; Xie, F.; Cai, P.; Dong, H.;
and Lu, Z. 2023. Plan4MC: Skill Reinforcement Learn-
ing and Planning for Open-World Minecraft Tasks. CoRR ,
abs/2303.16563.
Figure 10: Different maps in BabyAI
A. BabyAI Environment
BabyAI is a grid world environment for instruction follow-
ing investigations, which consists of 19 levels of tasks with
increasing difficulty. All instructions in BabyAI are gener-
ated through a combinatorially rich synthetic grammar. Con-
sidering some levels repeat each other or are too specific to a
single scene, like “GoToRedBall”, We selected 4 representa-
tive single-task scenarios and 2 challenging combined-task
levels. There are two kinds of map in BabyAI, single room
or3×3rooms as shown in Figure 10.
The following Table 2 shows the details of each level of
tasks.
B. Hypothesis Details
As described in the paper, in the hypothesis phase, HYVIN
makes LLM not only decompose the instruction into sub-
goals but also corresponding check functions. The prompt
consists of the role definition, task definitions, perception
function descriptions, and the output format constraint.
Role Definition
There is a robot in the maze, which consists of multiple
rooms connected by doors. Some objects are in the room,
like boxes, balls, doors, and keys. You are a kindful assistant
that helps to guide the robot to complete some objects ma-
nipulating tasks. Please reply “Yes” if you understand your
role, and then I will give your more details information.
Task Definitions
Tasks like “go to X”, the robot should first discover X, and
then go next to X.
Tasks like “pick X”, the robot should first “go to X”, then
pick X.
Tasks like “put X next to Y”, the robot should first “pick X”,
then discover Y , and finally put X next to Y .
Tasks like “open the X”, the robot should consider door
X may be in the current room or another room. The robot
should first try to discover, go to, and open door X in the
current room. Otherwise, the robot should go to a new room.

--- PAGE 10 ---
Level Map Scenario Mission Space
GoToLocal single room single-task go to the {color} {type}
PickupLoc single room single-task pick up the {color} {type}
PutNextLocal single room single-task put the {color} {type}next to the {color} {type}
Open 3×3rooms single-task open a {color}door
SynthSeq 3×3rooms combined-taskCombined instructions of ”GoTo”, ”PickUp”, ”PutNext”, ”Open”
using ”and”, ”, then” and ”after you”
BossLevel 3×3rooms combined-task Command can be any sentence drawn from the Baby Language grammar
Table 2: Details of 6 selected levels. “color” can be “re”, “green”, “blue”, “purple”, “yellow” or “grey”. “type” can be “ball”,
“box” or “key”.
Specifically, it should find an unexplored door Y , and then
enter the new room door Y leads to.
Perception Function Descriptions
You can use the following Python APIs to get the status
of the robot.
1(1) get_observation():
2# This API returns a dictionary
containing all objects in the robot’s
vision. The key is the names of the
objects, the value is the object’s
coordinates relative to the robot.
3# For example:
4obs = robot.get_observation()
5# output: obs = {"blue key":[-1,2], "
yellow box":[0,3]}
6# You can use the observation to judge
whether the robot has discovered a
specific object or a kind of object,
like:
7if "red ball" in robot.get_observation()
.keys():
8 do_something()
9# Or you can judge whether the robot is
next to a specific object, like:
10obs = robot.get_observation()
11if "red ball" in obs.keys()
12 if sum(abs(obs["red ball"])) == 1:
13 do_something()
14# Or you can judge whether one object is
next to another object, like:
15obs = robot.get_observation()
16if "red ball" in obs.keys() and "green
key" in obs.keys():
17 if sum(abs(obs["red ball"] - obs["
green key"])) == 1:
18 do_something()
19(2) get_carried():
20# This API returns the current carried
object of the robot.
21# For example:
22carried_item = robot.get_carried()
23# carried_item = "purple ball", means
the robot has carried a "purple ball"
now.24# carried_item = None, means the robot
has not pick anything now.
25# You can use this API to judge whether
the robot has got the target object
before putting it in a special place:
26if robot.get_carried() == "blue box":
27 put_item_to_a_place()
28(3) get_visited_rooms():
29# This API returns the id list of rooms
that have been visited by the robot
recently.
30# For example:
31visited = robot.get_visited_rooms()
32# output: visited = [0,7,3] means the
robot has visited No. 0, 7, and 3
rooms recently
33(4) get_doors_in_observation():
34# This API returns a dictionary
containing all doors in the robot’s
observation. Keys are the names of
doors, the value is the coordinates
relative to the robot, the status (
open or closed), and the id of the
room this door leads to.
35# For example:
36doors = robot.get_doors_in_observation()
37# output: doors = {"purple door
":[2,1,0,3], "green door
":[-1,3,1,6]}, the first two items
are the coordinates, the third item
is the status of the door, the fourth
item is the room id.
38# You can check every door is open or
close:
39doors = robot.get_doors_in_observation()
40for door in doors.keys():
41 status = doors[door][2]
42 room_id = doors[door][3]
43 if status == 0: # the door is open
44 ...
45# Or you can discover doors lead to
unexplored rooms:
46doors = robot.get_doors_in_observation()
47for door in doors.keys():
48 room_id = doors[door][3]
49 if room_id not in robot.
get_visited_rooms:
50 do_something()

--- PAGE 11 ---
51(5) get_current_room()
52# The API returns the current room id.
53current = robot.get_current_room()
54# output: current = 1 means the robot
now in No. 1 room.
Output Format Constraints
After the robot is given the task, your works are:
(1) decompose the task instruction into several goals to help
the robot complete the task. Each goal should be small and
easy to check.
(2) write separate Python functions using the above two
APIs to check whether the robot achieves each goal. Only
check function, and no implementation to help the robot.
Your output should follow this format:
Task instruction: “...”
Goals:
(1) xxx...
1def checkxxx...(robot):
2 if xxx...:
3 return True
4 else:
5 return False
(2)xxx...
1def xxx...
xxx...
Please reply “Yes” if you understand the above APIs and
your works, and then I will give your the object manipulating
task.”
Here are some decomposition examples:
User: Task instruction: “put the red key next to the red ball”
ChatGPT: Task instruction: “Put the red key next to the red
bal”
Goals:
(1) Check if the robot has discovered the red key
1 def check_discovered_red_key(robot):
2 if "red key" in robot.
get_observation().keys():
3 return True
4 else:
5 return False
(2) Check if the robot is next to the red key
1 def check_next_to_red_key(robot):
2 obs = robot.get_observation()
3 if "red key" in obs.keys():
4 if sum(abs(obs["red key"]))
== 1:
5 return True
6 return False
(3) Check if the robot has picked the red key
1 def check_picked_red_key(robot):
2 if robot.get_carried() == "red
key":
3 return True
4 else:
5 return False(4) Check if the robot has discovered the red ball
1 def check_discovered_red_ball(robot)
:
2 if "red ball" in robot.
get_observation().keys():
3 return True
4 else:
5 return False
(5) Check if the robot is next to the red ball
1 def check_next_to_red_ball(robot):
2 obs = robot.get_observation()
3 if "red ball" in obs.keys():
4 if sum(abs(obs["red ball"]))
== 1:
5 return True
6 return False
(6) Check if the robot has put the red key next to the red ball
1 def check_put_next_to_red_ball(robot
):
2 obs = robot.get_observation()
3 if "red key" in obs.keys() and "
red ball" in obs.keys():
4 if sum(abs(obs["red key"] -
obs["red ball"])) == 1:
5 return True
6 return False
Note: These functions are only for checking the completion
of each goal. The implementation of the robot’s actions to
achieve the goals is not included.
Please let me know if you need any further assistance.
User: Task instruction: “open the green door”
ChatGPT: Goals:
Goal 1: Locate the current room’s doors and objects
1def goal_1_locate_objects(robot):
2 observation = robot.get_observation
()
3 return True if observation else
False
Goal 2: Check for the green door in the current room
1def goal_2_find_green_door(robot):
2 observation = robot.get_observation
()
3 return True if "green door" in
observation.keys() else False
Goal 3: Approach and open the green door
1def goal_3_approach_and_open_door(robot)
:
2 doors_in_observation = robot.
get_doors_in_observation()
3 return True if "green door" in
doors_in_observation.keys() and
doors_in_observation["green door
"][2] == 1 else False
Goal 4: Find an unexplored door in the current room
1def goal_4_find_unexplored_door(robot):
2 doors_in_observation = robot.
get_doors_in_observation()
3 for door in doors_in_observation.
keys():

--- PAGE 12 ---
4 room_id = doors_in_observation[
door][3]
5 if room_id not in robot.
get_visited_rooms():
6 return True
7 return False
Goal 5: Enter the unexplored room
1def goal_5_enter_unexplored_room(robot):
2 current_room = robot.
get_current_room()
3 visited_rooms = robot.
get_visited_rooms()
4 return True if current_room not in
visited_rooms else False
Goal 6: Repeat from Goal 1 in the new room
This goal is already covered by the looping nature of the
task instructions. We don’t need a separate function for this
goal.
Note: These functions only check whether the robot has
achieved each specific goal based on the given API infor-
mation. They don’t include the actual task implementation
or navigation instructions for the robot. The navigation part
would depend on the higher-level program that calls these
functions and guides the robot through the maze.
C. Deduction Details
Generation Prompt
Similar to the decomposition prompt, the generation prompt
also has role definition, perception function descriptions
(same as the decomposition prompt), and output format
constraints. Besides, considering the difficulties when faced
with unseen and long-term tasks, we add skill descriptions
and an example to enable the in-context learning mecha-
nism.
Role Definition
There is a robot in the maze, which consists of multiple
rooms connected by doors. Some objects are in the room,
like boxes, balls, doors, and keys. You are a kindful assis-
tant that helps to generate the Python program to control the
robot to complete some objects manipulating tasks
Skill Descriptions
You can use the following Python APIs to control the robot
or get the state of the robot:
1(1) discover_object(object_name):
2# This API makes the robot discover a
specific object (including keys,
boxes, or balls). If there is no such
object in the current room, it may
fail and the robot needs to explore
another room.
3# For example:
4robot.discover_object("blue ball")5if "blue ball" in robot.get_observation
().keys():
6 do_somthing_1() # It means the robot
has discovered the "blue ball"
7else:
8 do_somthing_2()
9(2) go_next_to(object_name):
10# When the robot has discovered the
target object, this API makes the
robot go next to the object.
11# For example:
12if "blue ball" in robot.get_observation
().keys():
13 robot.go_next_to("blue ball")
14(3) pick(object_name):
15# When the robot has been next to the
target object, this API makes the
robot pick the object.
16# For example:
17if sum(abs(robot.get_observation["green
box"])) == 1: # the green box is next
to the robot
18 robot.pick("green box")
19(4) put_next_to(object_name1,
object_name2):
20# When the robot has been next to the
target object 2 and has picked object
1, this API makes the robot put
object 1 next to object 2.
21# For example:
22if sum(abs(robot.get_observation["green
box"])) == 1: # the green box is next
to the robot
23 if "purple key" in robot.get_picked
(): # The robot has picked the
purple key
24 robot.put_next_to("purple key", "
green box")
25(5) discover_door(door_name):
26# This API makes the robot discover a
specific door. If there is no such
door in the current room, it may fail
.
27# For example:
28robot.discover_door("blue door")
29if "blue door" in robot.get_observation
().keys():
30 do_somthing_1() # It means the robot
has discovered the "blue door"
31(6) go_to_door(door_name):
32# When the robot has discovered the
target door, this API makes the robot
go to the door.
33# For example:
34if "red door" in robot.get_observation()
.keys():
35 robot.go_to_door("red door")
36(7) open_door(door_name):
37# This API makes the robot open the door
.
38# For example:
39doors = robot.get_doors_in_observation()
40if "red door" in door.keys(): # doors in

--- PAGE 13 ---
the robot’s observation
41 if sum(doors["red door"][:2]) == 1:
the door is in front of the robot
42 robot.open_door("red door")
43(8) find_unexplored_door():
44# When the robot cannot discover a
specific object in the current room,
you can use this API to guide the
robot to find a door that leads to an
unexplored room.
45(9) enter_unexplored_room():
46# When the robot has found an unexplored
door, this API guides the robot to
open the door and enter an unexplored
room.
47# For example:
48robot.discover_object("grey box")
49while not "grey box" in robot.
get_observation().keys():
50 robot.find_unexplored_door()
51 robot.enter_unexplored_room()
52 robot.discover_object("grey box")
53 # When the while loop ends, it means
the robot has discovered the "grey
box"
Example
Here is an example of a good program:
Instruction: put the blue ball on your left next to a green ball
after you put a blue box next to the yellow door and open
the grey door
1# Program begin:
2# task 1: put a blue box next to the
yellow door
3def put_blue_box_next_to_yellow_door(
robot):
4 # 1. pick the blue box
5 # discover the blue box
6 robot.discover_object("blue box")
7 while "blue box" not in robot.
get_observation().keys():
8 # if cannot discover the blue
box, explore the next room
9 res = robot.find_unexplored_door
()
10 robot.enter_unexplored_room()
11 robot.discover_object("blue box
")
12 # flush the visited room trajectory
flush when discover blue box
13 robot.flush()
14 # go to the blue box
15 robot.go_next_to("blue box")
16 # pick the blue box
17 robot.pick("blue box")
18
19 # 2. put blue box next to yellow box
20 # discover the yellow door
21 robot.discover_door("yellow door")
22 while "yellow door" not in robot.get_observation().keys():
23 # if cannot discover the yellow
door, explore the next room
24 res = robot.find_unexplored_door
()
25 robot.enter_unexplored_room()
26 robot.discover_door("yellow door
")
27 # flush the visited room trajectory
flush when discover yellow box
28 robot.flush()
29 # go to yellow door
30 robot.go_next_to_door("yellow door")
31 # put
32 robot.put_next_to("blue box", "
yellow door")
33 return
34
35# task 2: open the grey door
36def open_grey_door(robot):
37 # 1. open the grey door
38 # discover the grey door
39 robot.discover_door("grey door")
40 while "grey door" not in robot.
get_observation().keys():
41 # if cannot discover the grey
door, explore the next room
42 robot.find_unexplored_door()
43 robot.enter_unexplored_room()
44 robot.discover_door("grey door")
45 # flush the visited room trajectory
flush when discover grey door
46 robot.flush()
47 # go to grey door
48 robot.go_next_to_door("grey door")
49 # open grey door
50 robot.open_door("grey door")
51 return
52
53# task 3: put the blue ball next to a
green ball
54def put_blue_ball_next_to_green_ball(
robot):
55 # 1. pick the blue ball
56 # discover the blue ball
57 robot.discover_object("blue ball")
58 while "blue ball" not in robot.
get_observation().keys():
59 # if cannot discover the blue
ball, explore the next room
60 robot.find_unexplored_door()
61 robot.enter_unexplored_room()
62 robot.discover_object("blue ball
")
63 # flush the visited room trajectory
flush when discover blue ball
64 robot.flush()
65 # go to the blue ball
66 robot.go_next_to("blue ball")
67 # pick the blue ball
68 robot.pick("blue ball")
69
70 # 2. put blue ball next to green

--- PAGE 14 ---
ball
71 # discover the green ball
72 robot.discover_door("green ball")
73 while "green ball" not in robot.
get_observation().keys():
74 # if cannot discover the green
ball, explore the next room
75 res = robot.find_unexplored_door
()
76 robot.enter_unexplored_room()
77 robot.discover_door("green ball
")
78 # flush the visited room trajectory
flush when discover green ball
79 robot.flush()
80 # go to green ball
81 robot.go_next_to("green ball")
82 # put
83 robot.put_next_to("blue ball", "
green ball")
84 return
85
86
87# main:
88put_blue_box_next_to_yellow_door(robot)
89open_grey_door(robot)
90put_blue_ball_next_to_green_ball(robot)
91# Program end
If you understand the example and your work, please wait
for the task instruction.
Output Format Constraints
As the example shows, the whole task may consists mutiple
small task or only one small task.
Your output should follow this format:
Instruction: xxx
1# Program begin:
2# task 1: xxxx
3def task1_xxx...(robot):
4 xxx...
5# task 2: xxx...
6xxx...
7# taskxxx...
8...
9# main
10task1_xxx...(robot)
11task2_xxx...(robot)
12...
13# Program end
Here are some generation examples:
Instruction: put the grey ball next to the red key
1# Program begin:
2# task 1: put grey ball next to the red
key
3def put_grey_ball_next_to_red_key(robot)
:
4 # 1. pick the grey ball5 # discover the grey ball
6 robot.discover_object("grey ball")
7 # go to the grey ball
8 robot.go_next_to("grey ball")
9 # pick the grey ball
10 robot.pick("grey ball")
11
12 # 2. put grey ball next to red key
13 # discover the red key
14 robot.discover_object("red key")
15 # go to red key
16 robot.go_next_to("red key")
17 # put
18 robot.put_next_to("grey ball", "red
key")
19 return
20
21# main
22put_grey_ball_next_to_red_key(robot)
23# Program end
Instruction: Put a ball next to the box on your left and open
a door after you open a green door and put the yellow box
on your left next to a red door.
1# Program begin:
2# Task 1: Open the green door
3def open_green_door(robot):
4 # Discover the green door
5 robot.discover_door("green door")
6
7 while "green door" not in robot.
get_observation().keys():
8 # If the green door cannot be
discovered, explore the next
room
9 robot.find_unexplored_door()
10 robot.enter_unexplored_room()
11 robot.discover_door("green door
")
12
13 # Flush the visited room trajectory
when discovering the green door
14 robot.flush()
15
16 # Go to the green door
17 robot.go_to_door("green door")
18
19 # Open the green door
20 robot.open_door("green door")
21
22# Task 2: Put the yellow box on your
left next to a red door
23def put_yellow_box_next_to_red_door(
robot):
24 # Discover the yellow box
25 robot.discover_object("yellow box")
26
27 while "yellow box" not in robot.
get_observation().keys():
28 # If the yellow box cannot be
discovered, explore the next

--- PAGE 15 ---
room
29 robot.find_unexplored_door()
30 robot.enter_unexplored_room()
31 robot.discover_object("yellow
box")
32
33 # Flush the visited room trajectory
when discovering the yellow box
34 robot.flush()
35
36 # Go to the yellow box
37 robot.go_next_to("yellow box")
38
39 # Pick the yellow box
40 robot.pick("yellow box")
41
42 # Discover the red door
43 robot.discover_door("red door")
44
45 while "red door" not in robot.
get_observation().keys():
46 # If the red door cannot be
discovered, explore the next
room
47 robot.find_unexplored_door()
48 robot.enter_unexplored_room()
49 robot.discover_door("red door")
50
51 # Flush the visited room trajectory
when discovering the red door
52 robot.flush()
53
54 # Go to the red door
55 robot.go_to_door("red door")
56
57 # Put the yellow box next to the red
door
58 robot.put_next_to("yellow box", "red
door")
59
60# Main program
61open_green_door(robot)
62put_yellow_box_next_to_red_door(robot)
Debugging Prompt
The debugging prompt is very similar to the generation
prompt, except for role definition and output format con-
straints. We give the same example as the generation prompt
to debugging prompt.
Role Definition
There is a robot in the maze, which consists of multiple
rooms connected by doors. Some objects are in the room,
like boxes, balls, doors, and keys. You are a kindful assistant
that helps to modify the given candidate python program of
the robot to complete some object manipulating tasks.
Output format constraintsYou would be given the objects manipulating tasks instruc-
tion, the candidate program, and some error message, you
need to modify the program to guide the robot to complete
the task.
You need to modify the program to guide the robot to
complete the task.
You should learn from the above good example and APIs,
such as:
(1) Before manipulating some object (like open, pick, put,
and open door), the robot must discover it first (using some
discover APIs) and then go to the target object .
(2) For a task like ”put A next to the B”, before discovering
and going to B to put A, the robot must first discover and
pick A.
You will get the following information:
Instruction: xxx
1# Candidate program:
2...
3# Error messages:
4...
And your output should follow this format:
1# Modified program begin:
2# task 1: xxxx
3def task1_xxx...(robot):
4 xxx...
5# task 2: xxx...
6xxx...
7# taskxxx...
8...
9# main
10task1_xxx...(robot)
11task2_xxx...(robot)
12...
13# Modified program end
Here is a debugging example:
User:
Instruction: put the red key next to the yellow key
1# Candidate program:
2# task 1: put the red key next to the
yellow key
3def put_red_key_next_to_yellow_key(robot
):
4 # 1. discover the red key and yellow
key
5 robot.discover_object("red key")
6 robot.discover_object("yellow key")
7
8 # 2. go to the red key
9 robot.go_next_to("red key")
10
11 # 3. pick the red key
12 robot.pick("red key")
13
14 # 4. go to the yellow key
15 robot.go_next_to("yellow key")
16

--- PAGE 16 ---
17 # 5. put the red key next to the
yellow key
18 robot.put_next_to("red key", "yellow
key")
19 return
20
21# main
22put_red_key_next_to_yellow_key(robot)
23# Error messages: Pick error!
ChatGPT:
1# Modified program begin:
2# task 1: put the red key next to the
yellow key
3def put_red_key_next_to_yellow_key(robot
):
4 # 1. discover the red key
5 robot.discover_object("red key")
6
7 # 2. go to the red key
8 if "red key" in robot.
get_observation().keys():
9 robot.go_next_to("red key")
10 else:
11 raise ValueError("The robot didn
’t discover the red key
before go to it")
12
13 # 3. pick the red key
14 if sum(abs(robot.get_observation()["
red key"])) == 1:
15 robot.pick("red key")
16 else:
17 raise ValueError("The robot is
not next to the red key")
18
19 # 4. discover the yellow key
20 robot.discover_object("yellow key")
21
22 # 5. go to the yellow key
23 if "yellow key" in robot.
get_observation().keys():
24 robot.go_next_to("yellow key")
25 else:
26 raise ValueError("The robot didn
’t discover the yellow key
before go to it")
27
28 # 6. put the red key next to the
yellow key
29 if sum(abs(robot.get_observation()["
yellow key"])) == 1:
30 robot.put_next_to("red key", "
yellow key")
31 else:
32 raise ValueError("The robot is
not next to the yellow key")
33 return
34
35# main
36put_red_key_next_to_yellow_key(robot)
37# Modified program endD. HYVIN-action Prompt
To demonstrate the effectiveness of self-driven learned
skills, we test the variant of our framework, HYVIN-action,
which maintains the same high-level planner but employs
primitive actions instead of the acquired skills. The follow-
ing are the action descriptions and the example in the gener-
ation prompt of HYVIN-action.
Action Descriptions
You can produce python code to control the robot using the
following APIs:
1(1) turn_left(), turn_right():
2# These two APIs can be called without
any prerequisites, and it turn the
facing direction of the robot and
returns observation of the new facing
direction.
3# For example:
4obs_old = robot.get_observation()
5obs_new = robot.turn_left()
6# obs1 = {"yellow door":[0,3], "purple
ball":[-2,1], "green key":[1,0]}
7# obs2 = {"yellow door":[3,0], "purple
ball":[1,2], "blue box":[-2,0]}
8# After turn left, some old seen objects
’ relative coordinates change (like "
yellow door" and "purple ball"), some
old objects cannot be seen anymore (
like "green key"), and there are some
new seen objects.
9(2) move_forward():
10# This API makes the robot move forward
1 step. It can be called successfully
and return true, if there is no
object one step forward the robot.
11# For a successful example:
12obs = robot.get_observation()
13state = robot.move_forward()
14(3) pick():
15# This API makes the robot pick the
object at 1 step forward position. It
can be called successful if there is
an object 1 step forward from the
robot and the robot carried nothing
now.
16# For a successful example:
17obs = robot.get_observation()
18assert [0,1] in obs.values and robot.
get_carried() is None
19state = robot.pick()
20new_obs = robot.get_observation()
21assert [0,1] not in new_obs.values and
robot.get_carried() is not None
22(4) put():
23# This API makes the robot put the
object it carried now in the 1 step
forward position. It can be called
successful if there is no object 1
step forward the robot and the robot
carried one object now.
24# For successful example:
25obs = robot.get_observation()

--- PAGE 17 ---
26assert [0,1] not in obs.values and robot
.get_carried() is not None
27state = robot.put()
28new_obs = robot.get_observation()
29assert [0,1] not in new_obs.values and
robot.get_carried() is None
30(5) toggle()
31# This API makes the robot toggle the
door at 1 step forward position.
32doors_old = robot.
get_doors_in_observation()
33# doors_old = {"green door":[0,1,1,6]},
the green door is closed
34robot.toggle()
35doors_new = robot.
get_doors_in_observation()
36# doors_new = {"green door":[0,1,0,6]},
the green door is open
Example
Instruction: go to the blue box
1# Step1:
2# Turn facing direction until
discovering red ball
3obs = robot.get_observation()
4for _ in range(4):
5 if "blue box" in obs.keys():
6 break
7 robot.turn_left()
8 obs = robot.get_observation()
9
10# Step2:
11# Go to the position next to blue box
12x_distance, y_distance = obs["blue box"]
13# target position is ahead, move forward
14if y_distance > 0:
15 for _ in range(abs(y_distance)):
16 state = robot.move_forward()
17# target position is on the right, turn
right and move forward
18if x_distance > 0:
19 robot.turn_right()
20 for _ in range(abs(x_distance)):
21 state = robot.move_forward()
22# target position is on the left, turn
left and move forward
23elif x_distance < 0:
24 _ = robot.turn_left()
25 for _ in range(abs(x_distance)):
26 state = robot.move_forward()
Because of the limited expressive power of the primitive
action set, HYVIN-action can only generate simple code
logic, which does not perform well on complex tasks. The
following is an example of complex tasks:
User:
Instruction: open a red door and go to the ball on your left
after you put the grey ball next to a door.
ChatGPT:Certainly, I can help you break down this task into steps and
provide you with the Python code to achieve it. Here’s the
solution:
1# Step 1
2def discover_RedDoor(robot):
3 obs = robot.get_observation()
4 while "red door" not in obs.keys():
5 obs = robot.turn_left()
6
7# Step 2
8def open_RedDoor(robot):
9 obs = robot.get_observation()
10 if "red door" in obs.keys():
11 state = robot.step_forward()
12
13# Step 3
14def turn_to_GreyBall(robot):
15 _ = robot.turn_left()
16
17# Step 4
18def approach_GreyBall(robot):
19 obs = robot.get_observation()
20 x_distance, y_distance = obs["grey
ball"]
21
22 if y_distance > 0: # target
position is ahead
23 for _ in range(y_distance):
24 state = robot.step_forward()
25 elif x_distance > 0: # target
position is on the right
26 _ = robot.turn_right()
27 for _ in range(x_distance):
28 state = robot.step_forward()
29 elif x_distance < 0: # target
position is on the left
30 _ = robot.turn_left()
31 for _ in range(abs(x_distance)):
32 state = robot.step_forward()
33
34# Step 5
35def pick_GreyBall(robot):
36 state = robot.pick()
37
38# Step 6
39def turn_around(robot):
40 for _ in range(2):
41 _ = robot.turn_left()
42
43# Step 7
44def put_GreyBall_next_to_Door(robot):
45 obs = robot.get_observation()
46 if "grey ball" in obs.keys():
47 state = robot.step_forward()
48
49# Step 8
50def turn_to_Ball(robot):
51 _ = robot.turn_left()
52
53# Step 9
54def approach_Ball(robot):
55 obs = robot.get_observation()

--- PAGE 18 ---
56 x_distance, y_distance = obs["ball"]
57
58 if y_distance > 0: # target
position is ahead
59 for _ in range(y_distance):
60 state = robot.step_forward()
61 elif x_distance > 0: # target
position is on the right
62 _ = robot.turn_right()
63 for _ in range(x_distance):
64 state = robot.step_forward()
65 elif x_distance < 0: # target
position is on the left
66 _ = robot.turn_left()
67 for _ in range(abs(x_distance)):
68 state = robot.step_forward()
69
70# Implementing the solution
71def solve_task(robot):
72 discover_RedDoor(robot)
73 open_RedDoor(robot)
74 turn_to_GreyBall(robot)
75 approach_GreyBall(robot)
76 pick_GreyBall(robot)
77 turn_around(robot)
78 put_GreyBall_next_to_Door(robot)
79 turn_to_Ball(robot)
80 approach_Ball(robot)
81
82
83# Execute the solution
84solve_task(robot)
As the example shows, without skills, the LLM can only
imitate the given example to call primitive actions directly,
thus cannot ground the plan to successful implementation in
the environment.
E. Reinforcement Learning Details
In the verification process of the hypothesis phase and skill
learning process in the induction phase, we use the standard
PPO algorithm and model architecture proposed in BabyAI
paper to train the policy. It is worth noting that in order to
promote the generalization ability of learned skills, we add a
Dropout layer after the feature extraction in the model. Table
3 shows the parameters of reinforcement learning.

--- PAGE 19 ---
Parameters Explanation Verification Skill Learning
max-frames The maximum training steps 3e3 1e6
discount The discount facor or RL 0.99
lr The Learning rate 0.001
gae-lambda The lambda coefficient in GAE formula 0.95
entropy-coef The entropy term coefficient 0.05 0.01
value-loss-coef The value loss term coefficient 0.5
max-grad-norm The maximum norm of gradient 0.5
clip-eps The clipping epsilon for PPO 0.2
horizon The max steps per episode 30 15
drop The drop rate of the dropout layer 0 0.2
text Add a GRU to the model to handle text input True
Table 3: The parameters of PPO algorithm
