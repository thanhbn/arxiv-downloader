# 2302.06884.pdf
# Đã chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2302.06884.pdf
# Kích thước tệp: 1045953 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Ước Lượng Giá Trị Trạng Thái Bảo Thủ cho Học Tăng Cường Ngoại Tuyến

Liting Chen∗
McGill University
Montreal, Canada
98chenliting@gmail.com

Jie Yan †
Step.ai
Beijing, China
dasistyanjie@gmail.com

Zhengdao Shao∗
University of Sci. and Tech. of China
Hefei, China
zhengdaoshao@mail.ustc.edu.cn

Lu Wang
Microsoft
Beijing, China
wlu@microsoft.com

Qingwei Lin
Microsoft
Beijing, China
qlin@microsoft.com

Saravan Rajmohan
Microsoft 365
Seattle, USA
saravar@microsoft.com

Thomas Moscibroda
Microsoft
Redmond, USA
moscitho@microsoft.com

Dongmei Zhang
Microsoft
Beijing, China
dongmeiz@microsoft.com

Tóm tắt
Học tăng cường ngoại tuyến đối mặt với một thách thức đáng kể về việc đánh giá quá cao giá trị do sự trôi dạt phân phối giữa tập dữ liệu và chính sách học hiện tại, dẫn đến thất bại trong học tập trong thực tế. Cách tiếp cận phổ biến là kết hợp một thành phần phạt vào ước lượng phần thưởng hoặc giá trị trong các lần lặp Bellman. Trong khi đó, để tránh ngoại suy trên các trạng thái và hành động ngoài phân phối (OOD), các phương pháp hiện có tập trung vào ước lượng hàm Q bảo thủ. Trong bài báo này, chúng tôi đề xuất Ước Lượng Giá Trị Trạng Thái Bảo Thủ (CSVE), một cách tiếp cận mới học hàm V bảo thủ thông qua việc áp đặt trực tiếp hình phạt lên các trạng thái OOD. So với công trình trước đó, CSVE cho phép ước lượng giá trị trạng thái hiệu quả hơn với các đảm bảo bảo thủ và tối ưu hóa chính sách tốt hơn.

Hơn nữa, chúng tôi áp dụng CSVE và phát triển một thuật toán actor-critic thực tế trong đó critic thực hiện ước lượng giá trị bảo thủ bằng cách lấy mẫu và phạt thêm các trạng thái xung quanh tập dữ liệu, và actor áp dụng các cập nhật có trọng số ưu thế được mở rộng với khám phá trạng thái để cải thiện chính sách. Chúng tôi đánh giá trong các tác vụ điều khiển liên tục cổ điển của D4RL, cho thấy phương pháp của chúng tôi hoạt động tốt hơn các phương pháp học hàm Q bảo thủ và có khả năng cạnh tranh mạnh mẽ trong số các phương pháp SOTA gần đây.

1 Giới thiệu

Học Tăng Cường (RL) học hành động bằng cách tương tác với môi trường và đã cho thấy thành công lớn trong nhiều tác vụ khác nhau. Tuy nhiên, trong nhiều tình huống thực tế, không thể học từ đầu trực tuyến vì việc khám phá thường rủi ro và không an toàn. Thay vào đó, RL ngoại tuyến[ 1,2] tránh vấn đề này bằng cách học chính sách hoàn toàn từ dữ liệu lịch sử. Tuy nhiên, việc đơn giản áp dụng các kỹ thuật RL trực tuyến tiêu chuẩn cho các tập dữ liệu tĩnh có thể dẫn đến việc đánh giá quá cao giá trị và quyết định chính sách không đúng khi đối mặt với các tình huống không quen hoặc ngoài phân phối (OOD).

∗Công việc được thực hiện trong thời gian thực tập tại Microsoft. †Công việc được thực hiện trong thời gian làm việc toàn thời gian tại Microsoft.
Hội nghị lần thứ 37 về Hệ thống Xử lý Thông tin Thần kinh (NeurIPS 2023).arXiv:2302.06884v2 [cs.LG] 2 Dec 2023

--- TRANG 2 ---
Gần đây, nguyên tắc ước lượng giá trị bảo thủ đã được giới thiệu để giải quyết các thách thức trong RL ngoại tuyến[ 3–5]. Các phương pháp trước đó, ví dụ, CQL(Conservative Q-Learning [ 4]), tránh vấn đề đánh giá quá cao giá trị bằng cách đánh giá thấp một cách có hệ thống các giá trị Q của các hành động OOD trên các trạng thái trong tập dữ liệu. Trong thực tế, nó thường quá bi quan và do đó dẫn đến các thuật toán quá bảo thủ.
COMBO [ 6] tận dụng một mô hình động học được học để tăng cường dữ liệu theo cách nội suy. Quá trình này giúp tạo ra một hàm Q ít bảo thủ hơn CQL, có khả năng dẫn đến các chính sách tối ưu hơn.

Trong bài báo này, chúng tôi đề xuất CSVE (Conservative State Value Estimation), một cách tiếp cận RL ngoại tuyến mới.
Không giống như các phương pháp trên ước lượng giá trị bảo thủ bằng cách phạt hàm Q cho các hành động OOD, CSVE trực tiếp phạt hàm V cho các trạng thái OOD. Chúng tôi chứng minh lý thuyết rằng CSVE cung cấp các cận chặt hơn trên giá trị trạng thái trong phân phối trong kỳ vọng so với CQL, và các cận tương tự như COMBO nhưng dưới các phân phối trạng thái chiết khấu tổng quát hơn, điều này có khả năng tăng cường tối ưu hóa chính sách trong hỗ trợ dữ liệu. Các đóng góp chính của chúng tôi bao gồm:

•Ước lượng giá trị trạng thái bảo thủ với phân tích lý thuyết liên quan. Chúng tôi chứng minh rằng nó cận dưới cho các giá trị trạng thái thực trong kỳ vọng trên bất kỳ phân phối trạng thái nào được sử dụng để lấy mẫu các trạng thái OOD và được cận trên bởi các giá trị trạng thái thực trong kỳ vọng trên phân phối trạng thái biên của tập dữ liệu cộng với một hạng số phụ thuộc vào lỗi lấy mẫu.
So với công trình trước đó, nó tăng cường tối ưu hóa chính sách với các đảm bảo giá trị bảo thủ.

•Một thuật toán actor-critic thực tế triển khai CSVE. Critic đảm nhận ước lượng giá trị trạng thái bảo thủ, trong khi actor sử dụng hồi quy có trọng số ưu thế (AWR) và khám phá các trạng thái với đảm bảo giá trị bảo thủ để cải thiện chính sách. Đặc biệt, chúng tôi sử dụng một mô hình động học để lấy mẫu các trạng thái OOD có thể đạt được trực tiếp từ tập dữ liệu, để phạt giá trị hiệu quả và khám phá chính sách.

•Đánh giá thực nghiệm trên các tác vụ điều khiển liên tục của Gym [ 7] và Adroit [ 8] trong các điểm chuẩn D4RL [ 9], cho thấy CSVE hoạt động tốt hơn các phương pháp trước đó dựa trên ước lượng giá trị Q bảo thủ, và có khả năng cạnh tranh mạnh mẽ trong số các thuật toán SOTA chính.

2 Kiến thức cơ bản

Học Tăng Cường Ngoại Tuyến. Xem xét Quá trình Quyết định Markov M:= (S,A, P, r, ρ, γ ),
bao gồm không gian trạng thái S, không gian hành động A, mô hình chuyển tiếp P:S × A → ∆(S), hàm phần thưởng r:S × A → R, phân phối trạng thái ban đầu ρ và hệ số chiết khấu γ∈(0,1]. Một chính sách ngẫu nhiên π:S → A chọn một hành động một cách xác suất dựa trên trạng thái hiện tại. Một chuyển tiếp là bộ tuple (st, at, rt, st+1) trong đó at∼π(·|st),st+1∼P(·|st, at), và rt=r(st, at). Giả định rằng các giá trị phần thưởng tuân thủ |r(s, a)| ≤Rmax,∀s, a. Một quỹ đạo dưới π là chuỗi ngẫu nhiên
τ= (s0, a0, r0, s1, a1, r1, . . . , s T) bao gồm các chuyển tiếp liên tục bắt đầu từ s0∼ρ.

RL tiêu chuẩn là học một chính sách π∈Π để tối đa hóa kỳ vọng phần thưởng tích lũy tương lai, được biểu diễn dưới dạng Jπ(M) =EM,π[P∞
t=0γtrt], thông qua tương tác tích cực với môi trường M. Tại bất kỳ thời điểm t nào, đối với chính sách π, hàm giá trị của trạng thái được định nghĩa là Vπ(s) :=EM,π[P∞
k=0γt+krt+k|st=
s], và hàm giá trị Q là Qπ(s, a) :=EM,π[P∞
k=0γt+krt+k|st=s, at=a]. Toán tử Bellman là một phép chiếu hàm: BπQ(s, a) := r(s, a) +γEs′∼P(·|s,a),a′∼π(·|s′)[Q(s′, a′)], hoặc
BπV(s) :=Ea∼π(·|s)[r(s, a) +γEs′∼P(·|s,a)[V(s′)]], dẫn đến các cập nhật giá trị lặp. Tính nhất quán Bellman ngụ ý rằng Vπ(s) =BπVπ(s),∀s và Qπ(s) =BπQπ(s, a),∀s, a. Khi sử dụng xấp xỉ hàm trong thực tế, toán tử Bellman thực nghiệm ˆBπ được sử dụng, trong đó các kỳ vọng nói trên được ước lượng bằng dữ liệu. RL ngoại tuyến nhằm học chính sách π từ một tập dữ liệu tĩnh D={(s, a, r, s′)} được tạo từ các chuyển tiếp thu thập bởi bất kỳ chính sách hành vi nào, với mục tiêu hoạt động tốt trong thiết lập trực tuyến. Lưu ý rằng, không giống như RL trực tuyến tiêu chuẩn, RL ngoại tuyến không tương tác với môi trường trong quá trình học.

Ước Lượng Giá Trị Bảo Thủ. Một thách thức chính trong RL ngoại tuyến phát sinh từ việc đánh giá quá cao giá trị do ngoại suy trong các trạng thái và hành động chưa thấy. Việc đánh giá quá cao như vậy có thể dẫn đến sự xuống cấp của chính sách được học. Để giải quyết vấn đề này, chủ nghĩa bảo thủ hoặc bi quan được sử dụng trong ước lượng giá trị. Ví dụ, CQL học một hàm giá trị Q bảo thủ bằng cách phạt giá trị của các hành động chưa thấy:

--- TRANG 3 ---
ˆQk+1←arg min
Q1
2Es,a,s′∼D[(Q(s, a)−ˆβπˆQk(s, a))2]+α(Es∼D
a∼µ(·|s)[Q(s, a)]−Es∼D
a∼ˆπβ(·|s)[Q(s, a)])
(1)
trong đó ˆπβ và π là chính sách hành vi và chính sách học riêng biệt, µ là một chính sách tùy ý khác với ˆπβ, và α đại diện cho hệ số cân bằng chủ nghĩa bảo thủ.

Tối Ưu Hóa Chính Sách Có Ràng Buộc. Để giải quyết các vấn đề về sự trôi dạt phân phối giữa chính sách học và chính sách hành vi, một cách tiếp cận là ràng buộc chính sách học gần với chính sách hành vi [ 10–13,1]. Ví dụ, Hồi Quy Có Trọng Số Ưu Thế (AWR)[ 14,12] sử dụng một phân kỳ KL ngầm để điều chỉnh khoảng cách giữa các chính sách:
πk+1←arg max
πE
s,a∼Dlogπ(a|s)
Z(s)exp1
λAπk(s, a)
Ở đây, Aπk là ưu thế của chính sách πk, và Z phục vụ như hằng số chuẩn hóa cho s.

RL Ngoại Tuyến Dựa Trên Mô Hình. Trong RL, mô hình là một xấp xỉ của MDP M. Mô hình như vậy được ký hiệu là ˆM:= (S,A,ˆP,ˆr, ρ, γ ), với ˆP và ˆr là xấp xỉ của P và r tương ứng.
Trong RL ngoại tuyến, mô hình thường được sử dụng để tăng cường dữ liệu [ 15,6] hoặc hoạt động như một thay thế cho môi trường thực trong quá trình tương tác [ 16]. Tuy nhiên, các thực hành như vậy có thể vô tình gây ra lỗi bootstrap trong các khoảng thời gian dài[ 17]. Trong bài báo này, chúng tôi giới hạn việc sử dụng mô hình cho việc lấy mẫu một bước trên các trạng thái tiếp theo có thể đạt được gần đúng từ tập dữ liệu.

3 Ước Lượng Giá Trị Trạng Thái Bảo Thủ

Trong thiết lập ngoại tuyến, việc đánh giá quá cao giá trị là một vấn đề chính dẫn đến thất bại trong việc học một chính sách hợp lý [ 13,1]. Trái ngược với các công trình trước[ 4,6] nhận được ước lượng giá trị bảo thủ thông qua việc phạt hàm Q cho các cặp trạng thái-hành động OOD, chúng tôi trực tiếp phạt hàm V cho các trạng thái OOD.
Cách tiếp cận của chúng tôi cung cấp một số kết quả lý thuyết mới cho phép cân bằng tốt hơn giữa ước lượng giá trị bảo thủ và cải thiện chính sách. Tất cả các chứng minh của các định lý có thể được tìm thấy trong Phụ lục A.

3.1 Đánh Giá Ngoại Chính Sách Bảo Thủ

Chúng tôi nhằm ước lượng một cách bảo thủ giá trị của một chính sách mục tiêu sử dụng một tập dữ liệu để tránh đánh giá quá cao các trạng thái OOD. Để đạt được điều này, chúng tôi phạt các giá trị V được đánh giá trên các trạng thái có nhiều khả năng là OOD và tăng các giá trị V trên các trạng thái trong phân phối của tập dữ liệu. Điều chỉnh này được thực hiện lặp đi lặp lại::
ˆVk+1←arg min
V1
2Es∼du(s)[(ˆBπˆVk(s)−V(s))2] +α(Es′∼d(s)V(s′)−Es∼du(s)V(s)) (2)
trong đó du(s) là phân phối trạng thái chiết khấu của D, d(s) là bất kỳ phân phối trạng thái nào, và ˆBπ là toán tử Bellman thực nghiệm (xem phụ lục để biết thêm chi tiết). Xem xét thiết lập không có xấp xỉ hàm, bằng cách đặt đạo hàm của Phương trình 2 bằng không, chúng ta có thể rút ra hàm V sử dụng lập trình động xấp xỉ tại lần lặp k::
ˆVk+1(s) =ˆBπˆVk(s)−α[d(s)
du(s)−1],∀s, k. (3)
Ký hiệu phép chiếu hàm trên ˆVk trong Phương trình 3 là Tπ. Chúng ta có Bổ đề 3.1, đảm bảo rằng ˆVk hội tụ về một điểm cố định duy nhất.

Bổ đề 3.1. Đối với bất kỳ d với supp d⊆supp du, Tπ là một γ-co trong chuẩn L∞.

Định lý 3.2. Đối với bất kỳ d với supp d⊆supp du(d̸=du), với một α đủ lớn (tức là, α≥
Es∼d(s)Ea∼π(a|s)Cr,t,δRmax
(1−γ)√
|D(s,a)|/Es∼d(s)[d(s)
du(s)−1])), kỳ vọng giá trị của ước lượng ˆVπ(s)
dưới d(s) là cận dưới của giá trị thực, nghĩa là: Es∼d(s)[ˆVπ(s)]≤Es∼d(s)[Vπ(s)].

--- TRANG 4 ---
ˆVπ(s) = lim k→∞ˆVk(s) là ước lượng giá trị hội tụ với tập dữ liệu D, và Cr,t,δRmax
(1−γ)√
|D(s,a)|
liên quan đến lỗi lấy mẫu phát sinh khi sử dụng toán tử thực nghiệm thay vì toán tử Bellman. Nếu số lượng của mỗi cặp trạng thái-hành động lớn hơn không, |D(s, a)| ký hiệu một vector có kích thước |S||A| chứa số lượng cho mỗi cặp trạng thái-hành động. Nếu số lượng của cặp trạng thái hành động này bằng không, 1√
|D(s,a)| tương ứng là một giá trị lớn nhưng hữu hạn. Chúng tôi giả định rằng với xác suất ≥1−δ, lỗi lấy mẫu nhỏ hơn Cr,t,δRmax
(1−γ)√
|D(s,a)|, trong khi Cr,t,δ là một hằng số (Xem phụ lục để biết thêm chi tiết.)
Lưu ý rằng nếu lỗi lấy mẫu có thể được bỏ qua, α >0 có thể đảm bảo kết quả cận dưới.

Định lý 3.3. Kỳ vọng giá trị của ước lượng, ˆVπ(s), dưới phân phối trạng thái của tập dữ liệu gốc là cận dưới của giá trị thực cộng với hạng của lỗi lấy mẫu không thể rút gọn. Chính thức:
Es∼du(s)[ˆVπ(s)]≤Es∼du(s)[Vπ(s)] +Es∼du(s)(I−γPπ)−1Ea∼π(a|s)Cr,t,δRmax
(1−γ)√
|D(s,a)|.
trong đó Pπ đề cập đến ma trận chuyển tiếp kết hợp với chính sách π (xem Phụ lục để biết chi tiết).

Bây giờ chúng tôi cho thấy rằng, trong các lần lặp, khoảng cách giữa các giá trị hàm V ước lượng của các trạng thái trong phân phối và các trạng thái OOD cao hơn so với các hàm V thực.

Định lý 3.4. Đối với bất kỳ lần lặp k nào, cho một α đủ lớn, phương pháp của chúng tôi khuếch đại sự khác biệt trong kỳ vọng giá trị V giữa phân phối trạng thái được chọn và phân phối trạng thái tập dữ liệu. Điều này có thể được biểu diễn như: Es∼du(s)[ˆVk(s)]−Es∼d(s)[ˆVk(s)]>Es∼du(s)[Vk(s)]−Es∼d(s)[Vk(s)].

Cách tiếp cận của chúng tôi, phạt hàm V cho các trạng thái OOD, thúc đẩy một ước lượng bảo thủ hơn về giá trị của chính sách mục tiêu trong học tăng cường ngoại tuyến. Do đó, việc trích xuất chính sách của chúng tôi đảm bảo các hành động phù hợp với phân phối của tập dữ liệu.

Để áp dụng cách tiếp cận của chúng tôi một cách hiệu quả trong các thuật toán RL ngoại tuyến, các định lý trước đây phục vụ như các nguyên tắc hướng dẫn. Dưới đây là bốn thông tin chính để sử dụng thực tế Phương trình 2:

Nhận xét 1. Theo Phương trình 2, nếu d=du, hình phạt cho các trạng thái OOD giảm bớt. Điều này có nghĩa là chính sách có khả năng sẽ tránh các trạng thái với hỗ trợ dữ liệu hạn chế, ngăn chặn nó khám phá các hành động chưa thấy trong các trạng thái như vậy. Trong khi AWAC [ 12] sử dụng cấu hình này, những phát hiện của chúng tôi chỉ ra rằng bằng cách chọn một d, phương pháp của chúng tôi vượt trội hơn hiệu suất của AWAC.

Nhận xét 2. Định lý 3.3 gợi ý rằng dưới du, phân phối trạng thái biên của dữ liệu, giá trị ước lượng kỳ vọng của Vπ thấp hơn giá trị thực của nó hoặc vượt quá nó, nhưng trong một giới hạn nhất định. Hiểu biết này thúc đẩy việc chúng tôi áp dụng cập nhật chính sách có trọng số ưu thế, như được minh họa trong Phương trình 9.

Nhận xét 3. Theo Định lý 3.2, giá trị ước lượng kỳ vọng của một chính sách dưới d, đại diện cho phân phối trạng thái chiết khấu của bất kỳ chính sách nào, phải là cận dưới của giá trị thực của nó. Dựa trên định lý này, chiến lược tăng cường chính sách của chúng tôi kết hợp một cập nhật có trọng số ưu thế với một phần thưởng khám phá bổ sung, được giới thiệu trong Phương trình 10.

Nhận xét 4. Định lý 3.4 nêu Es∼d(s)[Vk(s)]−Es∼d(s)[ˆVk(s)]>Es∼du(s)[Vk(s)]−
Es∼du(s)[ˆVk(s)]. Theo cách đơn giản hơn, việc đánh giá thấp giá trị rõ rệt hơn dưới d.
Với việc lựa chọn d phù hợp, chúng ta có thể tự tin xây dựng một chính sách mới và có khả năng vượt trội bằng cách sử dụng ˆVk. Thuật toán của chúng tôi chọn phân phối của các trạng thái tiếp theo dự đoán mô hình làm d, tức là, s′∼d được triển khai bởi s∼D, a∼π(·|s), s′∼ˆP(·|s, a), điều này hiệu quả xây dựng một 'con sông' mềm với các giá trị thấp bao quanh tập dữ liệu.

So sánh với công trình trước: CQL (Phương trình.1), phạt hàm Q của các hành động OOD,
đảm bảo các cận dưới trên ước lượng giá trị theo trạng thái: ˆVπ(s) = Eπ(a|s)(ˆQπ(s, a))≤
Eπ(a|s)(Qπ(s, a)) = Vπ(s) cho tất cả s∈D. COMBO, phạt hàm Q cho các trạng thái và hành động OOD của việc nội suy dữ liệu lịch sử và các cuộn mô hình, đảm bảo cận dưới của kỳ vọng giá trị trạng thái: Es∼µ0[ˆVπ(s)]≤Es∼µ0[Vπ(s)] trong đó µ0 là phân phối trạng thái ban đầu (Nhận xét 1, phần A.2 của COMBO [ 6]); đây là một trường hợp đặc biệt của kết quả chúng tôi trong Định lý
3.2 khi d=µ0. Cả CSVE và COMBO đều có ý định tăng cường hiệu suất bằng cách chuyển từ giá trị trạng thái cá nhân sang giá trị trạng thái kỳ vọng. Tuy nhiên, CSVE cung cấp các cận dưới tương tự nhưng dưới một phân phối trạng thái tổng quát hơn. Lưu ý rằng µ0 phụ thuộc vào môi trường hoặc mô hình động trong quá trình đào tạo ngoại tuyến. Tính linh hoạt của CSVE, được biểu diễn bởi d, đảm bảo các đảm bảo bảo thủ trên bất kỳ phân phối trạng thái chiết khấu nào của chính sách học, nhấn mạnh sự ưu tiên cho việc phạt V hơn hàm Q.

--- TRANG 5 ---
3.2 Đảm Bảo Cải Thiện Chính Sách An Toàn

Bây giờ chúng tôi cho thấy rằng phương pháp của chúng tôi có các đảm bảo cải thiện chính sách an toàn so với chính sách hành vi được ngụ ý bởi dữ liệu. Trước tiên chúng tôi cho thấy rằng phương pháp của chúng tôi tối ưu hóa một mục tiêu RL thực nghiệm bị phạt:

Định lý 3.5. Gọi ˆVπ là điểm cố định của Phương trình 3, thì π∗(a|s) = arg maxπˆVπ(s) được tìm một cách tương đương bằng cách giải:
π∗←arg max
πJ(π,ˆM)−α
1−γE
s∼dπ
ˆM[d(s)
du(s)−1]. (4)

Dựa trên Định lý 3.5, chúng tôi cho thấy rằng phương pháp của chúng tôi cung cấp một cải thiện chính sách ζ-an toàn so với πβ.

Định lý 3.6. Gọi π∗(a|s) là chính sách được thu trong Phương trình 4. Sau đó, nó là một cải thiện chính sách ζ-an toàn so với ˆπβ trong MDP thực M, tức là, J(π∗, M)≥J(ˆπβ, M)−ζ với xác suất cao 1- δ, trong đó ζ được cho bởi:
ζ=2(Cr,δ
1−γ+γRmaxCT,δ
(1−γ)2)E
s∼dπ
ˆM(s)"
cs
E
a∼π(a|s)[π(a|s)
πβ(a|s)]#
−(J(π∗,ˆM)−J(ˆπβ,ˆM))| {z }
≥α1
1−γEs∼dπ
ˆM(s)[d(s)
du(s)−1] trong đó c=p
|A|/p
|D(s)|.

4 Phương pháp luận

Trong phần này, chúng tôi đề xuất một thuật toán actor-critic thực tế sử dụng CSVE để ước lượng giá trị và mở rộng Hồi Quy Có Trọng Số Ưu Thế[ 18] với khám phá trạng thái ngoài mẫu để cải thiện chính sách. Đặc biệt, chúng tôi áp dụng một mô hình động học để lấy mẫu các trạng thái OOD trong quá trình ước lượng giá trị bảo thủ và khám phá trong quá trình cải thiện chính sách. Chi tiết triển khai có trong Phụ lục B. Bên cạnh đó, chúng tôi thảo luận về các lựa chọn kỹ thuật chung của việc áp dụng CSVE vào các thuật toán.

4.1 Ước Lượng Giá Trị Bảo Thủ

Cho một tập dữ liệu D được thu thập bởi chính sách hành vi πβ, mục tiêu của chúng tôi là ước lượng hàm giá trị Vπ cho một chính sách mục tiêu π. Như được nêu trong phần 3, để ngăn chặn việc đánh giá quá cao giá trị, chúng tôi học một hàm giá trị bảo thủ ˆVπ cận dưới các giá trị thực của π bằng cách thêm một hình phạt cho các trạng thái OOD trong chuỗi phép chiếu Bellman. Phương pháp của chúng tôi bao gồm các cập nhật lặp của Phương trình 5 -
7, trong đó ˆQk là mạng mục tiêu của ˆQk.

ˆVk+1←arg min
VLπ
V(V;ˆQk) (5)
=Es∼Dh
(Ea∼π(·|s)[ˆQk(s, a)]−V(s))2i
+α 
Es∼D,a∼π(·|s)
s′∼ˆP(s,a)[V(s′)]−Es∼D[V(s)]!

ˆQk+1←arg min
QLπ
Q(Q;ˆVk+1) = E
s,a,s′∼D
r(s, a) +γˆVk+1(s′)−Q(s, a)2
(6)

ˆQk+1←(1−ω)ˆQk+ωˆQk+1(7)

RHS của Phương trình 5 là một xấp xỉ của Phương trình 2, với hạng đầu tiên đại diện cho lỗi TD tiêu chuẩn. Trong hạng này, giá trị trạng thái mục tiêu được ước lượng bằng cách lấy kỳ vọng của ˆQk trên a∼π,
và hạng thứ hai phạt giá trị của các trạng thái OOD. Trong Phương trình 6, RHS là các lỗi TD được ước lượng trên các chuyển tiếp trong tập dữ liệu D. Lưu ý rằng hạng mục tiêu là tổng của phần thưởng r(s, a) và giá trị trạng thái bước tiếp theo ˆVk+1(s′). Trong Phương trình 7, các giá trị Q mục tiêu được cập nhật với một hệ số nội suy mềm ω∈(0,1). ˆQk thay đổi chậm hơn ˆQk, làm cho việc ước lượng lỗi TD trong Phương trình 5 ổn định hơn.

Chính Sách Có Ràng Buộc. Lưu ý rằng trong RHS của Phương trình 5, chúng tôi sử dụng a∼π(·|s) trong kỳ vọng. Để ước lượng an toàn giá trị mục tiêu của V(s) bằng Ea∼π(·|s)[ˆQ(s, a)], chúng tôi hầu như luôn yêu cầu supp( π(·|s))⊂

--- TRANG 6 ---
supp( πβ(·|s)). Chúng tôi đạt được điều này bằng cập nhật chính sách có trọng số ưu thế, buộc π(·|s) phải có khối lượng xác suất đáng kể trên các hành động được thực hiện bởi πβ trong dữ liệu, như được chi tiết trong phần 3.2.

Lấy Mẫu Trạng Thái OOD Dựa Trên Mô Hình. Trong Phương trình 5, chúng tôi triển khai quá trình lấy mẫu trạng thái s′∼d trong
Phương trình 2 như một luồng {s∼D;a∼π(a|s), s′∼ˆP(s′|s, a)}, đó là phân phối của các trạng thái tiếp theo dự đoán từ D bằng cách theo π. Cách tiếp cận này chứng minh có lợi trong thực tế. Một mặt,
phương pháp này hiệu quả hơn vì nó chỉ lấy mẫu các trạng thái có thể đạt được gần đúng từ D bằng một bước, thay vì lấy mẫu toàn bộ không gian trạng thái. Mặt khác, chúng tôi chỉ cần mô hình để thực hiện dự đoán một bước sao cho nó không gây ra lỗi bootstrap từ các khoảng thời gian dài.
Theo công trình trước [ 17,15,6], Chúng tôi sử dụng một tập hợp các mạng nơ-ron sâu, được biểu diễn như pθ1, . . . , pθB, để triển khai mô hình động học xác suất. Mỗi mạng nơ-ron tạo ra một phân phối Gaussian trên trạng thái và phần thưởng tiếp theo: Pi
θ(st+1, r|st, at) =N(ui
θ(st, at), σi
θ(st, at)).

Hệ Số Phạt Thích Ứng α. Mức bi quan được kiểm soát bởi tham số α≥0. Trong thực tế,
chúng tôi đặt α thích ứng trong quá trình đào tạo như sau, tương tự như trong CQL([4])
max
α≥0[α(Es′∼d[Vψ(s′)]−Es∼D[Vψ(s)]−τ)], (8)
trong đó τ là một tham số ngân sách. Nếu sự khác biệt kỳ vọng trong các giá trị V nhỏ hơn τ, α sẽ giảm.
Ngược lại, α sẽ tăng, phạt các giá trị trạng thái OOD một cách tích cực hơn.

4.2 Cập Nhật Chính Sách Có Trọng Số Ưu Thế

Sau khi học các ˆVk+1 và ˆQk+1 bảo thủ (hoặc ˆVπ và ˆQπ khi các giá trị đã hội tụ),
chúng tôi cải thiện chính sách bằng cập nhật có trọng số ưu thế sau đây [12].
π←arg min
π′Lπ(π′) =−E
s,a∼Dh
logπ′(a|s) exp
βˆAk+1(s, a)i
(9)
trong đó ˆAk+1(s, a) = ˆQk+1(s, a)−ˆVk+1(s). Phương trình.9 cập nhật chính sách π bằng cách áp dụng một phương pháp likelihood tối đa có trọng số. Điều này được tính toán bằng cách tái-trọng số các mẫu trạng thái-hành động trong D sử dụng ưu thế ước lượng ˆAk+1. Nó tránh ước lượng rõ ràng chính sách hành vi, và các lỗi lấy mẫu kết quả, đây là một vấn đề quan trọng trong RL ngoại tuyến [12, 4].

Ràng buộc chính sách ngầm. Chúng tôi áp dụng cập nhật chính sách có trọng số ưu thế áp đặt một ràng buộc phân kỳ KL ngầm giữa π và πβ. Ràng buộc chính sách này cần thiết để đảm bảo rằng trạng thái tiếp theo s′ trong Phương trình 5 có thể được tạo ra một cách an toàn thông qua chính sách π. Như được rút ra trong [ 12] (Phụ lục A), Phương trình 9 là một giải pháp tham số của vấn đề sau (trong đó ϵ phụ thuộc vào β):
max
π′Ea∼π′(·|s)[ˆAk+1(s, a)]
s.t.DKL(π′(·|s)||πβ(·|s))≤ϵ,Z
aπ′(a|s)da= 1.

Lưu ý rằng DKL(π′||πβ) là một phân kỳ KL dự trữ đối với π′, là tìm kiếm chế độ [ 19].
Khi được coi như Lagrangian, nó buộc π′ phân bổ khối lượng xác suất của nó cho các hỗ trợ likelihood tối đa của πβ, được tái-trọng số bởi ưu thế ước lượng. Nói cách khác, đối với không gian A trong đó πβ(·|s) không có mẫu, π′(·|s) cũng có khối lượng xác suất gần như bằng không.

Khám Phá Dựa Trên Mô Hình Trên Các Trạng Thái Gần. Như được gợi ý bởi các nhận xét trong Phần 3.1, trong thực tế,
cho phép chính sách khám phá các chuyển tiếp trạng thái tiếp theo dự đoán (s∼D) theo a∼π′(·|s))
dẫn đến hiệu suất kiểm tra tốt hơn. Với loại khám phá này, chính sách được cập nhật như sau:
π←arg min
π′Lπ(π′)−λEs∼D,a∼π′(s)
s′∼ˆP(s,a)h
r(s, a) +γˆVk+1(s′)i
.(10)

Hạng thứ hai là một xấp xỉ cho Es∼dπ(s)[Vπ(s)]. Việc tối ưu hóa hạng này bao gồm tính toán gradient thông qua mô hình động học được học. Điều này được đạt được bằng cách sử dụng các gradient giải tích thông qua động học được học để tối đa hóa các ước lượng giá trị. Điều quan trọng cần lưu ý là các ước lượng giá trị dựa vào các dự đoán phần thưởng và giá trị, phụ thuộc vào các trạng thái và hành động tưởng tượng. Vì tất cả các bước này được triển khai bằng mạng nơ-ron, gradient được tính toán giải tích bằng cách sử dụng lan truyền ngược ngẫu nhiên, một khái niệm được lấy cảm hứng từ Dreamer[ 20]. Chúng tôi điều chỉnh giá trị của λ, một siêu tham số, để cân bằng giữa tối ưu hóa chính sách lạc quan (trong việc tối đa hóa V) và cập nhật chính sách có ràng buộc (như được chỉ ra bởi hạng đầu tiên).

--- TRANG 7 ---
Bảng 1: So sánh hiệu suất trên các tác vụ điều khiển Gym v2. Kết quả của CSVE trên mười hạt giống và chúng tôi tái triển khai AWAC sử dụng d3rlpy. Kết quả của IQL, TD3-BC, và PBRL từ các bài báo gốc của chúng ( Bảng 1 trong [ 21], Bảng C.3 trong [ 22], và Bảng 1 trong [ 10] tương ứng). Kết quả của COMBO và CQL từ kết quả tái tạo trong [ 23] (Bảng 1) và [ 10] tương ứng, vì kết quả gốc của chúng được báo cáo trên các tập dữ liệu v0.

[Bảng hiệu suất chi tiết với các thuật toán và tác vụ khác nhau]

4.3 Thảo luận về các lựa chọn triển khai

Bây giờ chúng tôi xem xét các cân nhắc kỹ thuật để triển khai CSVE trong một thuật toán thực tế.

Ràng Buộc Trên Trích Xuất Chính Sách. Điều quan trọng cần lưu ý là hàm giá trị trạng thái một mình không đủ để trực tiếp rút ra một chính sách. Có hai phương pháp để trích xuất một chính sách từ CSVE. Phương pháp đầu tiên là lập kế hoạch dựa trên mô hình, tức là, π←arg maxπEs∼d,a∼π(·|s)[ˆr(s, a) +γEs′∼ˆP(s,a)[V(s′)],
bao gồm tìm chính sách tối đa hóa lợi nhuận tương lai kỳ vọng. Tuy nhiên, phương pháp này phụ thuộc nhiều vào độ chính xác của một mô hình và khó triển khai trong thực tế. Như một lựa chọn thay thế, chúng tôi đề xuất phương pháp thứ hai, học một hàm giá trị Q hoặc ưu thế từ hàm giá trị V và dữ liệu kinh nghiệm, và sau đó trích xuất chính sách. Lưu ý rằng CSVE không cung cấp đảm bảo cho ước lượng bảo thủ trên các hành động OOD, điều này có thể khiến các phương pháp trích xuất chính sách bình thường như SAC thất bại. Để giải quyết vấn đề này, chúng tôi áp dụng các kỹ thuật ràng buộc chính sách. Một mặt, trong quá trình ước lượng giá trị trong Phương trình.5, tất cả các trạng thái hiện tại được lấy mẫu từ tập dữ liệu, trong khi chính sách bị ràng buộc gần với chính sách hành vi (đảm bảo thông qua Phương trình.9). Mặt khác, trong quá trình học chính sách trong Phương trình.10, chúng tôi sử dụng AWR [ 18] như phương pháp trích xuất chính sách chính (hạng đầu tiên của Phương trình.10), áp đặt ngầm các ràng buộc chính sách và khám phá hành động bổ sung (hạng thứ hai của Phương trình.10) được áp dụng nghiêm ngặt cho các trạng thái trong tập dữ liệu. Việc khám phá này cung cấp một phần thưởng cho các hành động: (1) bản thân chúng và các trạng thái tiếp theo dự đoán mô hình của chúng đều gần với tập dữ liệu (đảm bảo bởi mô hình động học), và (2) các giá trị của chúng thuận lợi ngay cả với chủ nghĩa bảo thủ.

Tận Dụng CSVE. Như được nêu trong Phần 3.1, CSVE cho phép một cận dưới thoải mái hơn trên ước lượng giá trị bảo thủ so với các giá trị Q bảo thủ, cung cấp tiềm năng lớn hơn để cải thiện chính sách. Để tận dụng điều này, thuật toán nên cho phép khám phá các trạng thái ngoài mẫu nhưng trong phân phối, như được mô tả trong Phần 3. Trong bài báo này, chúng tôi sử dụng một mô hình động học tập hợp sâu để hỗ trợ việc khám phá trạng thái suy đoán này, như được hiển thị trong Phương trình 10. Lý luận đằng sau điều này như sau: đối với một trạng thái trong dữ liệu s và bất kỳ hành động a∼π(·|s), nếu trạng thái tiếp theo s′ trong dữ liệu hoặc gần với hỗ trợ dữ liệu, giá trị của nó được ước lượng hợp lý, và nếu không, giá trị của nó đã bị phạt theo Phương trình.5. Ngoài ra, mô hình động học tập hợp sâu nắm bắt tính không chắc chắn nhận thức tốt, có thể hiệu quả hủy bỏ tác động của các mẫu hiếm của s′. Bằng cách sử dụng CSVE, thuật toán của chúng tôi có thể sử dụng nội suy suy đoán để cải thiện thêm chính sách. Ngược lại, CQL và AWAC không có khả năng này cho việc tối ưu hóa chính sách được tăng cường như vậy.

--- TRANG 8 ---
Bảng 2: So sánh hiệu suất trên các tác vụ Adroit. Kết quả của CSVE trên mười hạt giống. Kết quả của IQL từ Bảng 3 trong [21] và kết quả của các thuật toán khác từ Bảng 4 trong [10].

[Bảng hiệu suất chi tiết cho các tác vụ Adroit]

5 Thí nghiệm

Phần này đánh giá hiệu quả của thuật toán CSVE đề xuất của chúng tôi cho ước lượng giá trị bảo thủ trong RL ngoại tuyến. Ngoài ra, chúng tôi nhằm so sánh hiệu suất của CSVE với các thuật toán tiên tiến (SOTA). Để đạt được điều này, chúng tôi tiến hành đánh giá thực nghiệm trên nhiều tác vụ điều khiển liên tục cổ điển của Gym[7] và Adroit[8] trong điểm chuẩn D4RL[9].

Các đường cơ sở so sánh của chúng tôi bao gồm: (1) CQL[ 4] và các biến thể của nó, CQL-AWR (Phụ lục D.2) sử dụng AWR với khám phá trong mẫu bổ sung như bộ trích xuất chính sách, COMBO[ 6] mở rộng CQL với các cuộn dựa trên mô hình; (2) các biến thể AWR, bao gồm AWAC[ 12] là một trường hợp đặc biệt của thuật toán chúng tôi không có phạt giá trị (tức là, d=du trong Phương trình. 2) và khám phá trên các trạng thái OOD, IQL[ 21] áp dụng ước lượng giá trị bảo thủ dựa trên expectile; (3) PBRL[ 10], một thuật toán mạnh trong RL ngoại tuyến, nhưng khá tốn kém về tính toán vì nó sử dụng tập hợp của hàng trăm mô hình con; (4) các thuật toán SOTA khác với kết quả hiệu suất công khai hoặc triển khai mã nguồn mở chất lượng cao, bao gồm TD3-BC[ 22], UWAC[ 24] và BEAR[ 25]). So sánh với các biến thể CQL cho phép chúng tôi điều tra các ưu thế của ước lượng bảo thủ trên giá trị trạng thái so với giá trị Q. Bằng cách so sánh với các biến thể AWR, chúng tôi phân biệt đóng góp hiệu suất của CSVE từ việc trích xuất chính sách AWR được sử dụng trong triển khai của chúng tôi.

5.1 Hiệu Suất Tổng Thể

Đánh giá trên Các Tác Vụ Điều Khiển Gym. Phương pháp của chúng tôi, CSVE, được đào tạo trong 1 triệu bước và được đánh giá. Kết quả được hiển thị trong Bảng 1. So với CQL, CSVE vượt trội nó trong 11 trong số 15 tác vụ, với hiệu suất tương tự trên các tác vụ còn lại. Ngoài ra, CSVE cho thấy một lợi thế nhất quán trên các tập dữ liệu được tạo ra bằng cách theo các chính sách ngẫu nhiên hoặc không tối ưu (ngẫu nhiên và trung bình). Phương pháp CQL-AWR cho thấy cải thiện nhẹ trong một số trường hợp, nhưng vẫn hoạt động kém so với CSVE. Khi so sánh với COMBO, CSVE hoạt động tốt hơn trong 7 trong số 12 tác vụ và tương tự hoặc hơi tệ hơn trên các tác vụ còn lại, điều này nổi bật hiệu quả của các cận tốt hơn của phương pháp chúng tôi trên V. Phương pháp của chúng tôi có một lợi thế rõ ràng trong việc trích xuất chính sách tốt nhất trên các tác vụ trung bình và trung bình-chuyên gia. Nhìn chung, kết quả của chúng tôi cung cấp bằng chứng thực nghiệm rằng việc sử dụng ước lượng giá trị bảo thủ trên các trạng thái, thay vì Q, dẫn đến hiệu suất được cải thiện trong RL ngoại tuyến. CSVE vượt trội AWAC trong 9 trong số 15 tác vụ, chứng minh hiệu quả của cách tiếp cận của chúng tôi trong việc khám phá vượt ra ngoài chính sách hành vi. Ngoài ra, phương pháp của chúng tôi xuất sắc trong việc trích xuất chính sách tối ưu trên dữ liệu với các chính sách hỗn hợp (trung bình-chuyên gia) nơi AWAC không đạt được. So với IQL, phương pháp của chúng tôi đạt điểm số cao hơn trong 7 trong số 9 tác vụ và duy trì hiệu suất tương đương trong các tác vụ còn lại.
Hơn nữa, mặc dù có khả năng mô hình và chi phí tính toán thấp hơn đáng kể, CSVE vượt trội TD3-BC và ngang bằng với PBRL. Những kết quả này nổi bật hiệu quả của cách tiếp cận ước lượng giá trị bảo thủ của chúng tôi.

Đánh giá trên Các Tác Vụ Adroit. Trong Bảng 2, chúng tôi báo cáo kết quả đánh giá cuối cùng sau khi đào tạo 0.1 triệu bước. Như được hiển thị, phương pháp của chúng tôi vượt trội IQL trong 8 trong số 12 tác vụ, và có khả năng cạnh tranh với các thuật toán khác trên các tập dữ liệu chuyên gia. Ngoài ra, chúng tôi lưu ý rằng CSVE là phương pháp duy nhất có thể học một chính sách hiệu quả trên tập dữ liệu con người cho tác vụ Pen, trong khi duy trì hiệu suất trung bình trên tập dữ liệu nhân bản. Nhìn chung, kết quả của chúng tôi hỗ trợ thực nghiệm hiệu quả của ước lượng giá trị bảo thủ chặt chẽ hơn đề xuất của chúng tôi trong việc cải thiện hiệu suất RL ngoại tuyến.

5.2 Nghiên Cứu Loại Bỏ

Tác Động của Khám Phá Trên Các Trạng Thái Gần. Chúng tôi phân tích tác động của việc thay đổi hệ số λ trong Phương trình 10, kiểm soát cường độ của việc khám phá như vậy. Chúng tôi điều tra các giá trị λ của {0.0,0.1,0.5,1.0} trong các tác vụ trung bình, cố định β= 0.1. Kết quả được vẽ trong Hình 1. Như được hiển thị trong các hình trên, λ có tác động rõ ràng đến hiệu suất chính sách và biến thiên trong quá trình đào tạo. Với việc tăng λ từ 0, hiệu suất hội tụ nói chung trở nên tốt hơn. Tuy nhiên, khi giá trị của λ trở nên quá lớn (ví dụ, λ= 3 cho hopper và walker2d), hiệu suất có thể giảm hoặc thậm chí sụp đổ. Chúng tôi điều tra thêm mất mát Lπ như được mô tả trong các hình dưới của Phương trình 9, phát hiện rằng các giá trị λ lớn hơn ảnh hưởng tiêu cực đến Lπ; tuy nhiên, một khi Lπ hội tụ về một giá trị thấp hợp lý, các giá trị λ lớn hơn dẫn đến cải thiện hiệu suất.

[Hình 1: Biểu đồ hiệu ứng của λ]

Tác Động của Tối Ưu Hóa Chính Sách Trong Mẫu. Chúng tôi xem xét tác động của việc thay đổi hệ số β trong Phương trình 9 về sự cân bằng giữa nhân bản hành vi và tối ưu hóa chính sách trong mẫu. Chúng tôi thử nghiệm các giá trị β khác nhau trên các tập dữ liệu trung bình mujoco, như được hiển thị trong Hình.2. Kết quả chỉ ra rằng β có tác động đáng kể đến hiệu suất chính sách trong quá trình đào tạo. Dựa trên những phát hiện của chúng tôi, một giá trị β= 3.0 được tìm thấy phù hợp cho các tập dữ liệu trung bình. Ngoài ra, trong triển khai của chúng tôi, chúng tôi sử dụng β= 3.0 cho các tác vụ ngẫu nhiên và trung bình, và β= 0.1 cho các tập dữ liệu trung bình-replay, trung bình-chuyên gia, và chuyên gia.
Chi tiết thêm có thể được tìm thấy trong nghiên cứu loại bỏ trong phụ lục.

6 Công trình liên quan

Ý tưởng chính đằng sau các thuật toán RL ngoại tuyến là kết hợp chủ nghĩa bảo thủ hoặc điều chuẩn vào các thuật toán RL trực tuyến. Ở đây, chúng tôi xem xét ngắn gọn công trình trước đó và so sánh nó với cách tiếp cận của chúng tôi.

Ước Lượng Giá Trị Bảo Thủ: Các thuật toán RL ngoại tuyến trước đó điều chuẩn chính sách học gần với dữ liệu hoặc với một chính sách hành vi được ước lượng rõ ràng. và phạt việc khám phá

--- TRANG 9 ---
[Hình 2: Biểu đồ hiệu ứng của β]

của vùng OOD, thông qua ước lượng hiệu chỉnh phân phối [ 26,27], ràng buộc chính sách với khớp hỗ trợ [ 11] và khớp phân phối [ 1,25], áp dụng hình phạt dựa trên phân kỳ chính sách trên hàm Q [ 28,29] hoặc hình phạt dựa trên tính không chắc chắn [ 30] trên hàm Q và ước lượng hàm Q bảo thủ [ 4]. Bên cạnh đó, các thuật toán dựa trên mô hình [ 15] trực tiếp ước lượng tính không chắc chắn động học và chuyển nó thành hình phạt phần thưởng. Khác với công trình trước đó này áp đặt chủ nghĩa bảo thủ trên các cặp trạng thái-hành động hoặc hành động, chúng tôi trực tiếp thực hiện ước lượng bảo thủ như vậy trên các trạng thái và không yêu cầu định lượng tính không chắc chắn rõ ràng.

Thuật Toán Trong Mẫu: AWR [ 18] cập nhật chính sách bị ràng buộc trên các trạng thái và hành động trong mẫu nghiêm ngặt, để tránh ngoại suy trên các điểm ngoài hỗ trợ. IQL[ 21] sử dụng hồi quy dựa trên expectile để thực hiện ước lượng giá trị và AWR cho các cập nhật chính sách của nó. AWAC[ 12], actor của nó là AWR, là một thuật toán actor-critic để tăng tốc RL trực tuyến với dữ liệu ngoại tuyến. Nhược điểm chính của phương pháp AWR khi được sử dụng cho RL ngoại tuyến là việc học chính sách trong mẫu giới hạn hiệu suất cuối cùng.

Thuật Toán Dựa Trên Mô Hình: RL ngoại tuyến dựa trên mô hình học mô hình động học từ tập dữ liệu tĩnh và sử dụng nó để định lượng tính không chắc chắn [ 15], tăng cường dữ liệu [ 6] với các cuộn, hoặc lập kế hoạch [ 16,31].
Các phương pháp như vậy thường dựa vào phạm vi dữ liệu rộng khi lập kế hoạch và tăng cường dữ liệu với các cuộn, và lỗi ước lượng mô hình thấp khi ước lượng tính không chắc chắn, điều này khó thỏa mãn trong thực tế và dẫn đến sự không ổn định chính sách. Thay vào đó, chúng tôi sử dụng mô hình để lấy mẫu các trạng thái bước tiếp theo chỉ có thể đạt được từ dữ liệu, không có các yêu cầu nghiêm ngặt như vậy về phạm vi dữ liệu hoặc thiên lệch mô hình.

Kết Quả Lý Thuyết: Kết quả lý thuyết của chúng tôi được rút ra từ ước lượng giá trị Q bảo thủ (CQL) và cải thiện chính sách an toàn [ 32]. So với đánh giá chính sách ngoại tuyến[ 33], nhằm cung cấp một ước lượng tốt hơn của hàm giá trị, chúng tôi tập trung vào việc cung cấp một cận dưới tốt hơn. Ngoài ra, khi tập dữ liệu được tăng cường với các cuộn dựa trên mô hình, COMBO [ 6] cung cấp một ước lượng giá trị bảo thủ hơn nhưng chặt chẽ hơn CQL. CSVE đạt được các cận dưới tương tự như COMBO nhưng dưới các phân phối trạng thái tổng quát hơn.

7 Kết luận

Trong bài báo này, chúng tôi đề xuất CSVE, một cách tiếp cận mới cho RL ngoại tuyến dựa trên ước lượng giá trị bảo thủ trên các trạng thái. Chúng tôi đã chứng minh cách các kết quả lý thuyết của nó có thể dẫn đến các thuật toán hiệu quả hơn. Đặc biệt, chúng tôi phát triển một thuật toán actor-critic thực tế, trong đó critic đạt được ước lượng giá trị trạng thái bảo thủ bằng cách kết hợp hình phạt của các trạng thái tiếp theo dự đoán mô hình vào các lần lặp Bellman, và actor thực hiện các cập nhật chính sách có trọng số ưu thế được tăng cường thông qua khám phá trạng thái dựa trên mô hình. Đánh giá thực nghiệm cho thấy phương pháp của chúng tôi hoạt động tốt hơn các phương pháp thay thế dựa trên ước lượng hàm Q bảo thủ và có khả năng cạnh tranh trong số các phương pháp SOTA, từ đó xác nhận phân tích lý thuyết của chúng tôi. Hướng tới tương lai, chúng tôi nhằm đi sâu hơn vào việc thiết kế các thuật toán mạnh mẽ hơn dựa trên ước lượng giá trị trạng thái bảo thủ.

--- TRANG 10 ---
[Phần Tài liệu tham khảo từ trang 10-12]

--- TRANG 11 ---
[Tiếp tục phần Tài liệu tham khảo]

--- TRANG 12 ---
[Tiếp tục phần Tài liệu tham khảo]

--- TRANG 13 ---
[Phụ lục A: Chứng minh - nội dung chi tiết các chứng minh toán học]

--- TRANG 14 ---
[Tiếp tục Phụ lục A]

--- TRANG 15 ---
[Tiếp tục Phụ lục A]

--- TRANG 16 ---
[Phụ lục B: Thuật toán CSVE và Chi tiết Triển khai]

--- TRANG 17 ---
[Bảng 3: Siêu tham số của đánh giá CSVE và Phụ lục C: Nghiên cứu Loại bỏ Bổ sung]

--- TRANG 18 ---
[Phụ lục D: Chi tiết Thí nghiệm và Kết quả Bổ sung]

--- TRANG 19 ---
[Tiếp tục Phụ lục D]

--- TRANG 20 ---
[Kết thúc Phụ lục D]
