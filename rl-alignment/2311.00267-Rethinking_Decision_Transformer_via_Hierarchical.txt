# 2311.00267.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2311.00267.pdf
# File size: 2518121 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Rethinking Decision Transformer via Hierarchical
Reinforcement Learning
Yi Ma1,âˆ—Chenjun Xiao2,âˆ—Hebin Liang1Jianye Hao1,2
1College of Intelligence and Computing, Tianjin University2Huawei, Noahâ€™s Ark Lab,
{mayi, lianghebin, jianye.hao}@tju.edu.cn, chenjun@ualberta.ca
Abstract
Decision Transformer (DT) is an innovative algorithm leveraging recent advances of the transformer architecture
in reinforcement learning (RL). However, a notable limitation of DT is its reliance on recalling trajectories from
datasets, losing the capability to seamlessly stitch sub-optimal trajectories together. In this work we introduce a
general sequence modeling framework for studying sequential decision making through the lens of Hierarchical RL .
At the time of making decisions, a high-level policy first proposes an ideal prompt for the current state, a low-level
policy subsequently generates an action conditioned on the given prompt. We show DT emerges as a special case of
this framework with certain choices of high-level and low-level policies, and discuss the potential failure of these
choices. Inspired by these observations, we study how to jointly optimize the high-level and low-level policies to
enable the stitching ability, which further leads to the development of new offline RL algorithms. Our empirical results
clearly show that the proposed algorithms significantly surpass DT on several control and navigation benchmarks.
We hope our contributions can inspire the integration of transformer architectures within the field of RL.
1 Introduction
One of the most remarkable characteristics observed in large sequence models, especially Transformer models, is the
in-context learning ability [Radford et al., 2019, Brown et al., 2020, Ramesh et al., 2021, Gao et al., 2020, AkyÃ¼rek et al.,
2022, Garg et al., 2022, Laskin et al., 2022, Lee et al., 2023]. With an appropriate prompt , a pre-trained transformer
can learn new tasks without explicit supervision and additional parameter updates. Decision Transformer (DT) is
an innovative method that attempts to explore this idea for sequential decision making [Chen et al., 2021]. Unlike
traditional reinforcement learning (RL) algorithms, which learn a value function by bootstrapping or computing policy
gradient, DT directly learns an autoregressive generative model from trajectory data using a causal transformer
[Vaswani et al., 2017, Radford et al., 2019]. This approach allows leveraging existing transformer architectures widely
employed in language and vision tasks that are easy to scale, and benefitting from a substantial body of research
focused on stable training of transformer [Radford et al., 2019, Brown et al., 2020, Fedus et al., 2022, Chowdhery et al.,
2022].
DT is trained on trajectory data, (ğ‘…0,ğ‘ 0,ğ‘0,â€¦,ğ‘…ğ‘‡,ğ‘ ğ‘‡,ğ‘ğ‘‡), whereğ‘…ğ‘¡is the return-to-go , the sum of future rewards along
the trajectory starting from time step ğ‘¡. This can be viewed as learning a model that predicts what action should be
taken at a given state in order to make so many returns . Following this, we can view the return-to-go prompt as a
switch , guiding the model in making decisions at test time. If such a model can be learned effectively and generalized
well even for out-of-distribution return-to-go, it is reasonable to expect that DT can generate a better policy by
prompting a higher return. Unfortunately, this seems to demand a level of generalization ability that is often too
high in practical sequential decision-making problems. In fact, the key challenge facing DT is how to improve its
robustness to the underlying data distribution, particularly when learning from trajectories collected by policies that
are not close to optimal. Recent studies have indicated that for problems requiring the stitching ability , referring to the
capability to integrate suboptimal trajectories from the data, DT cannot provide a significant advantage compared to
behavior cloning [Fujimoto and Gu, 2021, Emmons et al., 2021, Kostrikov et al., 2022, Yamagata et al., 2023, Badrinath
et al., 2023, Xiao et al., 2023a]. This further confirms that a naive return-to-go prompt is not good enough for solving
complex sequential decision-making problems.
âˆ—Equal contributions.
1arXiv:2311.00267v1  [cs.LG]  1 Nov 2023

--- PAGE 2 ---
1Causal Transformer
ğ’‘ğ’•âˆ’ğŸğ’”ğ’•âˆ’ğŸğ’‚ğ’•âˆ’ğŸğ’‘ğ’•ğ’‚ğ’•
ğ’”ğ’•ğ’‚ğ’• ğ’‘ğ’•âˆ’ğŸğ’”ğ’•âˆ’ğŸğ’‚ğ’•âˆ’ğŸ
ğœ‹â„ğœ‹â„ğœ‹â„ğ’‚ğ’•âˆ’ğŸ ğ’‚ğ’•âˆ’ğŸ
â€¦ â€¦Figure 1: ADT architecture. The high-level policy generates prompts that inform the low-level policy to make
decisions. We concatenate prompts with states instead of treating them as separate tokens. Embeddings of tokens are
fed into a causal transformer that predicts actions auto-regressively.
Recent progress on large language models showed that carefully tuned prompts, either human-written or self-
discovered by the model, significantly boost the performance of transformer models [Lester et al., 2021, Singhal et al.,
2022, Zhang et al., 2022, Wei et al., 2022, Wang et al., 2022, Yao et al., 2023, Liu et al., 2023]. In particular, it has been
observed that the ability to perform complex reasoning naturally emerges in sufficiently large language models when
they are presented with a few chain of thought demonstrations as exemplars in the prompts [Wei et al., 2022, Wang
et al., 2022, Yao et al., 2023]. Driven by the significance of these works in language models, a question arises: For RL,
is it feasible to learn to automatically tune the prompt, such that a transformer-based sequential decision model is able to
learn optimal control policies from offline data? This paper attempts to address this problem. Our main contributions
are:
â€¢We present a generalized framework for studying decision-making through sequential modeling by connecting it
with Hierarchical Reinforcement Learning [Nachum et al., 2018]: a high-level policy first suggests a prompt for the
current state, a low-level policy subsequently generates an action conditioned on the given prompt. We show DT
can be recovered as a special case of this framework.
â€¢We investigate when and why DT fails in terms of stitching sub-optimal trajectories. To overcome this drawback
of DT, we investigate how to jointly optimize the high-level and low-level policies to enable the stitching capability.
This further leads to the development of two new algorithms for offline RL. The joint policy optimization framework
is our key contribution compared to previous studies on improving transformer-based decision models [Yamagata
et al., 2023, Wu et al., 2023, Badrinath et al., 2023].
â€¢We provide experiment results on several offline RL benchmarks, including locomotion control, navigation and
robotics, to demonstrate the effectiveness of the proposed algorithms. Additionally, we conduct thorough ablation
studies on the key components of our algorithms to gain deeper insights into their contributions. Through these
ablation studies, we assess the impact of specific algorithmic designs on the overall performance.
2 Preliminaries
2.1 Offline Reinforcement Learning
We consider Markov Decision Process (MDP) determined by ğ‘€={îˆ¿,îˆ­,ğ‘ƒ,ğ‘Ÿ,ğ›¾}[Puterman, 2014], where îˆ¿andîˆ­
represent the state and action spaces. The discount factor is given by ğ›¾âˆˆ[0,1),ğ‘Ÿâˆ¶îˆ¿Ã—îˆ­â†’â„denotes the reward
function,ğ‘ƒâˆ¶îˆ¿Ã—îˆ­â†’Î”(îˆ¿)defines the transition dynamics1. Letğœ=(ğ‘ 0,ğ‘0,ğ‘Ÿ0,â€¦,ğ‘ ğ‘‡,ğ‘ğ‘‡,ğ‘Ÿğ‘‡)be a trajectory. Its return
1We useÎ”(î‰„)to denote the set of probability distributions over î‰„for a finite set î‰„.
2

--- PAGE 3 ---
is defined as the discounted sum of the rewards along the trajectory: ğ‘…=âˆ‘ğ‘‡
ğ‘¡=0ğ›¾ğ‘¡ğ‘Ÿğ‘¡. Given a policy ğœ‹âˆ¶îˆ¿â†’Î”(îˆ­),
we use ğ”¼ğœ‹to denote the expectation under the distribution induced by the interconnection of ğœ‹and the environment.
The value function specifies the future discounted total reward obtained by following policy ğœ‹,
ğ‘‰ğœ‹(ğ‘ )=ğ”¼ğœ‹
[âˆ
âˆ‘
ğ‘¡=0ğ›¾ğ‘¡ğ‘Ÿ(ğ‘ ğ‘¡,ğ‘ğ‘¡)|||ğ‘ 0=ğ‘ ], (1)
There exists an optimal policy ğœ‹âˆ—that maximizes values for all states ğ‘ âˆˆîˆ¿.
In this work, we consider learning an optimal control policy from previously collected offline dataset, îˆ°={ğœğ‘–}ğ‘›âˆ’1
ğ‘–=0,
consisting of ğ‘›trajectories. Each trajectory is generated by the following procedure: an initial state ğ‘ 0âˆ¼ğœ‡0is sampled
from the initial state distribution ğœ‡0; for time step ğ‘¡â‰¥0,ğ‘ğ‘¡âˆ¼ğœ‹îˆ°,ğ‘ ğ‘¡+1âˆ¼ğ‘ƒ(â‹…|ğ‘ ğ‘¡,ğ‘ğ‘¡),ğ‘Ÿğ‘¡=ğ‘Ÿ(ğ‘ ğ‘¡,ğ‘ğ‘¡), this process repeats
until it reaches the maximum time step of the environment. Here ğœ‹îˆ°is an unknown behavior policy . In offline RL, the
learning algorithm can only take samples from îˆ°without collecting new data from the environment [Levine et al.,
2020].
2.2 Decision Transformer
Decision Transformer (DT) is an extraordinary example that bridges sequence modeling with decision-making [Chen
et al., 2021]. It shows that a sequential decision-making model can be made through minimal modification to the
transformer architecture [Vaswani et al., 2017, Radford et al., 2019]. It considers the following trajectory representation
that enables autoregressive training and generation:
ğœ=(Ì‚ğ‘…0,ğ‘ 0,ğ‘0,Ì‚ğ‘…1,ğ‘ 1,ğ‘1,â€¦,Ì‚ğ‘…ğ‘‡,ğ‘ ğ‘‡,ğ‘ğ‘‡). (2)
HereÌ‚ğ‘…ğ‘¡=âˆ‘ğ‘‡
ğ‘–=ğ‘¡ğ‘Ÿğ‘–is the returns-to-go starting from time step ğ‘¡. We denote ğœ‹DT(ğ‘ğ‘¡|ğ‘ ğ‘¡,Ì‚ğ‘…ğ‘¡,ğœğ‘¡)the DT policy, where
ğœğ‘¡=(ğ‘ 0,ğ‘0,Ì‚ğ‘…0,â€¦,ğ‘ ğ‘¡âˆ’1ğ‘ğ‘¡âˆ’1,Ì‚ğ‘…ğ‘¡âˆ’1)2is the sub-trajectory before time step ğ‘¡. As pointed and verified by Lee et al. [2023],
ğœğ‘¡can be viewed as as a context input of a policy, which fully takes advantages of the in-context learning ability of
transformer model for better generalization [AkyÃ¼rek et al., 2022, Garg et al., 2022, Laskin et al., 2022].
DT assigns a desired returns-to-go ğ‘…0, together with an initial state ğ‘ 0are used as the initialization input of the model.
After executing the generated action, DT decrements the desired return by the achieved reward and continues this
process until the episode reaches termination. Chen et al. [2021] argues that the conditional prediction model is able
to perform policy optimization without using dynamic programming. However, recent works observe that DT often
shows inferior performance compared to dynamic programming based offline RL algorithms when the offline dataset
consists of sub-optimal trajectories [Fujimoto and Gu, 2021, Emmons et al., 2021, Kostrikov et al., 2022].
3 Autotuned Decision Transformer
In this section, we present Autotuned Decision Transformer (ADT) , a new transformer-based decision model that is able
to stitch sub-optimal trajectories from the offline dataset. Our algorithm is derived based on a general hierarchical
decision framework where DT naturally emerges as a special case. Within this framework, we discuss how ADT
overcomes several limitations of DT by automatically tune the prompt for decision making.
3.1 Key Observations
Our algorithm is derived by considering a general framework that bridges transformer-based decision models with
hierarchical reinforcement learning (HRL) [Nachum et al., 2018]. In particular, we use the following hierarchical
representation of policy
ğœ‹(ğ‘|ğ‘ )=âˆ«îˆ¼ğœ‹â„(ğ‘|ğ‘ )â‹…ğœ‹ğ‘™(ğ‘|ğ‘ ,ğ‘)ğ‘‘ğ‘, (3)
2We defineğœ0the empty sequence for completeness.
3

--- PAGE 4 ---
where îˆ¼is a set of prompts. To make a decision, the high-level policy ğœ‹â„first generates a prompt ğ‘âˆˆîˆ¼, instructed
by which the low-level policy ğœ‹ğ‘™returns an action conditioned on ğ‘. DT naturally fits into this hierarchical decision
framework. Consider the following value prompting mechanism. At state ğ‘ âˆˆîˆ¿, the high-level policy generates
a real-value prompt ğ‘…âˆˆâ„, representing "I want to obtain ğ‘…returns starting from ğ‘ . ". Informed by this prompt, the
low-level policy responses an action ğ‘âˆˆîˆ­,"Ok, if you want to obtain returns ğ‘…, you should take action ğ‘now. " . This
exactly what DT does. It applies a dummy high-level policy which initially picks a target return-to-go prompt and
subsequently decrement it along the trajectory. The DT low-level policy, ğœ‹DT(â‹…|ğ‘ ,ğ‘…,ğœ), learns to predict which action
to take at state ğ‘ in order to achieve returns ğ‘…given the context ğœ.
To better understand the failure of DT given sub-optimal data, we re-examine the illustrative example shown in
Figure 2 of Chen et al. [2021]. The dataset comprises random walk trajectories and their associated per-state return-
to-go. Suppose that the DT policy ğœ‹DTperfectly memorizes all trajectory information contained in the dataset. The
return-to-go prompt in fact acts as a switch to guide the model to make decisions. Let î‰€(ğ‘ )be the set of trajectories
starting from ğ‘ stored in the dataset, and ğ‘…(ğœ)be the return of a trajectory ğœ. Givenğ‘…â€²âˆˆ{ğ‘…(ğœ),ğœâˆˆî‰€(ğ‘ )},ğœ‹DTis able
to output an action that leads towards ğœ. Thus, given an oracle return ğ‘…âˆ—(ğ‘ )=max ğœâˆˆî‰€(ğ‘ )ğ‘…(ğœ), it is expected that DT is
able to follow the optimal trajectory contained in the dataset following the switch.
There are several issues. First, the oracle return ğ‘…âˆ—is not known. The initial return-to-go prompt of DT is picked
by hand and might not be consistent with the one observed in the dataset. This requires the model to generalize
well for unseen return-to-go and decisions. Second , even though ğ‘…âˆ—is known for all states, memorizing trajectory
information is still not enough for obtaining the stitching ability as ğ‘…âˆ—only serves a lower bound on the maximum
achievable return. To understand this, consider an example with two trajectories ğ‘â†’ğ‘â†’ğ‘, andğ‘‘â†’ğ‘â†’ğ‘’. Suppose
thatğ‘’leads to a return of 10, while ğ‘leads to a return of 0. In this case, using 10 as the return-to-go prompt at state
ğ‘, DT should be able to switch to the desired trajectory. However, the information "leaning towards ğ‘can achieve
a return of 10" does not pass to ğ‘during training, since the trajectory ğ‘â†’ğ‘â†’ğ‘’does not exist in the data. If the
offline data contains another trajectory that starts from ğ‘and leads to a mediocre return (e.g. 1), DT might switch to
that trajectory at ğ‘using 10 as the return-to-go prompt, missing a more promising path. Thus, making predictions
conditioned on return-to-go alone is not enough for policy optimization. Some form of information backpropagation
is still required.
3.2 Algorithms
ADT jointly optimizes the hierarchical policies to overcomes the limitations of DT discussed above. An illustration
of ADT architecture is provided in Fig. 1. Similar to DT, ADT applies a transformer model for the low-level policy.
Instead of (2), it considers the following trajectory representation,
ğœ=(ğ‘0,ğ‘ 0,ğ‘0,ğ‘1,ğ‘ 1,ğ‘1,â€¦,ğ‘ğ‘‡,ğ‘ ğ‘‡,ğ‘ğ‘‡). (4)
Hereğ‘ğ‘–is the prompt generated by the high-level policy ğ‘ğ‘–âˆ¼ğœ‹â„(â‹…|ğ‘ ğ‘–), replacing the return-to-go prompt used by DT.
That is, for each trajectory in the offline dataset, we relabel it by adding a prompt generated by the high-level policies
for each transition. Armed with this general hierarchical decision framework, we propose two algorithms that apply
different high-level prompting generation strategy while sharing a unified low-level policy optimization framework.
We learn a high-level policy ğœ‹ğœ”â‰ˆğœ‹â„with parameters ğœ™, and a low-level policy ğœ‹ğœƒâ‰ˆğœ‹ğ‘™with parameters ğœƒ.
3.2.1 Value-prompted Autotuned Decision Transformer
Our first algorithm, Value-promped Autotuned Decision Transformer (V-ADT) , uses scalar values as prompts. But unlike
DT, it applies a more principled design of value prompts instead of return-to-go. V-ADT aims to answer two questions:
what is the maximum achievable value starting from a state ğ‘ , and what action should be taken to achieve such a
value? To answer these, we view the offline dataset îˆ°as an empirical MDP ,ğ‘€îˆ°={îˆ¿îˆ°,îˆ­,ğ‘ƒîˆ°,ğ‘Ÿ,ğ›¾}, where îˆ¿îˆ°âŠ†îˆ¿is
the set of observed states in the data, ğ‘ƒîˆ°is the transition, which is an empirical estimation of the original transition ğ‘ƒ
[Fujimoto et al., 2019]. The optimal value of this empirical MDP is
ğ‘‰âˆ—
îˆ°(ğ‘ )= max
ğ‘âˆ¶ğœ‹îˆ°(ğ‘|ğ‘ )>0ğ‘Ÿ(ğ‘ ,ğ‘)+ğ›¾ğ”¼ğ‘ â€²âˆ¼ğ‘ƒîˆ°(â‹…|ğ‘ ,ğ‘)[ğ‘‰âˆ—
îˆ°(ğ‘ â€²)]. (5)
4

--- PAGE 5 ---
Letğ‘„âˆ—
îˆ°(ğ‘ ,ğ‘)be the corresponding state-action value. ğ‘‰âˆ—
îˆ°is known as the in-sample optimal value in offline RL [Fujimoto
et al., 2018, Kostrikov et al., 2022, Xiao et al., 2023b]. Computing this value requires to perform dynamic programming
without querying out-of-distribution actions. We apply Implicit Q-learning (IQL) to learn ğ‘‰ğœ™â‰ˆğ‘‰âˆ—
îˆ°andğ‘„ğœ“â‰ˆğ‘„âˆ—
îˆ°with
parameters ğœ™,ğœ“[Kostrikov et al., 2022]. Details of IQL are presented in the Appendix. We now describe how V-ADT
jointly optimizes high and low level policies to facilitate stitching.
High-Level policy V-ADT considers îˆ¼=â„and adopts a deterministic policy ğœ‹ğœ”âˆ¶îˆ¿â†’â„, which predicts
the in-sample optimal value ğœ‹ğœ”â‰ˆğ‘‰âˆ—
îˆ°. Since we already have an approximated in-sample optimal value ğ‘‰ğœ™, we let
ğœ‹ğœ”=ğ‘‰ğœ™. This high-level policy offers two key advantages. First, this approach efficiently facilitates information
backpropagation towards earlier states on a trajectory, addressing a major limitation of DT. This is achieved by
usingğ‘‰âˆ—
îˆ°as the value prompt, ensuring that we have precise knowledge of the maximum achievable return for any
state. Making predictions conditioned on ğ‘…âˆ—(ğ‘ )is not enough for policy optimization, since ğ‘…âˆ—(ğ‘ )=max ğœâˆˆî‰€(ğ‘ )ğ‘…(ğœ)
only gives a lower bound on ğ‘‰âˆ—
îˆ°(ğ‘ )and thus would be a weaker guidance (see Section 3.1 for detailed discussions).
Second , the definition of ğ‘‰âˆ—
îˆ°exclusively focuses on the optimal value derived from observed data and thus avoids
out-of-distribution returns. This prevents the low-level policy from making decisions conditioned on prompts that
require extrapolation.
Low-Level policy Directly training the model to predict the trajectory, as done in DT, is not suitable for our
approach. This is because the action ğ‘ğ‘¡observed in the data may not necessarily correspond to the action at state ğ‘ ğ‘¡
that leads to the return ğ‘‰âˆ—
îˆ°(ğ‘ ğ‘¡). However, the probability of selecting ğ‘ğ‘¡should be proportional to the value of this
action. Thus, we use advantage-weighted regression to learn the low-level policy [Peng et al., 2019, Kostrikov et al.,
2022, Xiao et al., 2023b]: given trajectory data (4) the objective is defined as
îˆ¸(ğœƒ)=âˆ’ğ‘‡
âˆ‘
ğ‘¡=0exp(ğ‘„ğœ“(ğ‘ ğ‘¡,ğ‘ğ‘¡)âˆ’ğ‘‰ğœ™(ğ‘ ğ‘¡)
ğ›¼ )logğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡,ğœ‹ğœ”(ğ‘ ğ‘¡)), (6)
whereğ›¼ >0is a hyper-parameter. The low-level policy takes the output of high-level policy as input. This guarantees
no discrepancy between train and test value prompt used by the policies. We note that the only difference of this
compared to the standard maximum log-likelihood objective for sequence modeling is to apply a weighting for each
transition. One can easily implement this with trajectory data for a transformer. In practice we also observe that
the tokenization scheme when processing the trajectory data affects the performance of ADT. Instead of treating
the prompt ğ‘ğ‘¡as a single token as in DT, we find it is beneficial to concatenate ğ‘ğ‘¡andğ‘ ğ‘¡together and tokenize the
concatenated vector. We provide an ablation study on this in Section 5.2.3. This completes the description of V-ADT.
3.2.2 Goal-prompted Autotuned Decision Transformer
In HRL, the high-level policy often considers a latent action space. Typical choices of latent actions includes sub-goal
[Nachum et al., 2018, Park et al., 2023], skills [Ajay et al., 2020, Jiang et al., 2022], and options [Sutton et al., 1999,
Bacon et al., 2017, Klissarov and Machado, 2023]. We consider goal-reaching problem as an example and use sub-goals
as latent actions, which leads to our second algorithm, Goal-promped Autotuned Decision Transformer (G-ADT) . Letîˆ³
be the goal space3. The goal-conditioned reward function ğ‘Ÿ(ğ‘ ,ğ‘,ğ‘”)provides the reward of taking action ğ‘at stateğ‘ for
reaching the goal ğ‘”âˆˆîˆ³. Letğ‘‰(ğ‘ ,ğ‘”)be the universal value function defined by the goal-conditioned rewards [Nachum
et al., 2018, Schaul et al., 2015]. Similarly, we define ğ‘‰âˆ—
îˆ°(ğ‘ ,ğ‘”)andğ‘„âˆ—
îˆ°(ğ‘ ,ğ‘,ğ‘”)the in-sample optimal universal value
function. We also train ğ‘‰ğœ™â‰ˆğ‘‰âˆ—
îˆ°andğ‘„ğœ“â‰ˆğ‘„âˆ—
îˆ°to approximate the universal value functions. We now describe how
G-ADT jointly optimizes the policies.
High-Level policy G-ADT considers îˆ¼=îˆ³and uses a high-level policy ğœ‹ğœ”âˆ¶îˆ¿â†’îˆ³. To find a shorter path, the
high-level policy ğœ‹ğœ”generates a sequence of sub-goals ğ‘”ğ‘¡=ğœ‹ğœ”(ğ‘ ğ‘¡)that guides the learner step-by-step towards the
final goal. We use a sub-goal that lies in ğ‘˜-steps further from the current state, where ğ‘˜is a hyper-parameter of the
algorithm tuned for each domain [Badrinath et al., 2023, Park et al., 2023]. In particular, given trajectory data (4), the
3The goal space and state space could be the same [Nachum et al., 2018, Park et al., 2023]
5

--- PAGE 6 ---
high-level policy learns the optimal k-steps jump using the recently proposed Hierarchical Implicit Q-learning (HIQL)
algorithms [Park et al., 2023]:
îˆ¸(ğœ™)=âˆ’ğ‘‡
âˆ‘
ğ‘¡=0exp(âˆ‘ğ‘˜âˆ’1
ğ‘¡â€²=ğ‘¡ğ›¾ğ‘¡â€²âˆ’ğ‘¡ğ‘Ÿ(ğ‘ ğ‘¡â€²,ğ‘ğ‘¡â€²,ğ‘”)+ğ›¾ğ‘˜ğ‘‰ğœ™(ğ‘ ğ‘¡+ğ‘˜,ğ‘”)âˆ’ğ‘‰ğœ™(ğ‘ ğ‘¡,ğ‘”)
ğ›¼ )logğœ‹ğœ”(ğ‘ ğ‘¡+ğ‘˜|ğ‘ ğ‘¡,ğ‘”).
Low-Level policy The low-level policy in G-ADT learns to reach the sub-goal generated by the high-level policy.
G-ADT shares the same low-level policy objective as V-ADT. Given trajectory data (4), it considers the following
îˆ¸(ğœƒ)=âˆ’ğ‘‡
âˆ‘
ğ‘¡=0exp(ğ‘„ğœ“(ğ‘ ğ‘¡,ğ‘ğ‘¡,ğœ‹ğœ”(ğ‘ ğ‘¡))âˆ’ğ‘‰ğœ™(ğ‘ ğ‘¡,ğœ‹ğœ”(ğ‘ ğ‘¡))
ğ›¼ )logğœ‹ğœƒ(ğ‘ğ‘¡|ğ‘ ğ‘¡,ğœ‹ğœ”(ğ‘ ğ‘¡)),
Note that this is exactly the same as (6) except that the advantages are computed by universal value functions. G-ADT
also applies the same tokenization method as V-ADT by first concatenating ğœ‹ğœ”(ğ‘ ğ‘¡)andğ‘ ğ‘¡together. This concludes the
description of the G-ADT algorithm.
4 Discussions
Types of Prompts Reed et al. [2022] have delved into the potential scalability of transformer-based decision models
through prompting. They show that a causal transformer, trained on multi-task offline datasets, showcases remarkable
adaptability to new tasks through fine-tuning. The adaptability is achieved by providing a sequence prompt as the
input of the transformer model, typically represented as a trajectory of expert demonstrations. Unlike such expert
trajectory prompts, our prompt can be seen as a latent action generated by the high-level policy, serving as guidance
for the low-level policy to inform its decision-making process.
Comparison of other DT Enhancements Several recent works have been proposed to overcome the limitations
of DT. Yamagata et al. [2023] relabelled trajectory data by replacing return-to-go with values learned by offline RL
algorithms. Badrinath et al. [2023] proposed to use sub-goal as prompt, guiding the DT policy to find shorter path
in navigation problems. Wu et al. [2023] learned maximum achievable returns, ğ‘…âˆ—(ğ‘ )=max ğœâˆˆî‰€(ğ‘ )ğ‘…(ğœ), to boost the
stitching ability of DT at decision time. Liu and Abbeel [2023] structured trajectory data by relabelling the target
return for each trajectory as the maximum total reward within a sequence of trajectories. Their findings showed that
this approach enabled a transformer-based decision model to improve itself during both training and testing time.
Compared to these previous efforts, ADT introduces a principled framework of hierarchical policy optimization. Our
practical studies show that the joint optimization of high and low level policies is the key to boost the performance of
transformer-based decision models.
5 Experiment
We investigate three primary questions in our experiments. First, how well does ADT perform on offline RL tasks
compared to prior DT-based methods? Second , is it essential to auto-tune prompts for transformer-based decision
model? Three , what is the influence of various implementation details within an ADT on its overall performance? We
refer readers to Appendix A for additional details and supplementary experiments.
Benchmark Problems We leverage datasets across several domains including Gym-Mujoco, AntMaze, and
FrankaKitchen from the offline RL benchmark D4RL [Fu et al., 2020]. For Mujoco, the offline datasets are generated
using three distinct behavior policies: â€˜-mediumâ€™, â€˜-medium-playâ€™, and â€˜-medium-expertâ€™, and span across three specific
tasks: â€˜halfcheetahâ€™, â€˜hopperâ€™, and â€˜walker2dâ€™. The primary objective in long-horizon navigation task AntMaze is to
guide an 8-DoF Ant robot from its starting position to a predefined target location. We employ six datasets which
include â€˜-umazeâ€™, â€˜-umaze-diverseâ€™, â€˜-medium-playâ€™, â€˜medium-diverseâ€™, â€˜-large-playâ€™, and â€˜-large-diverseâ€™. The Kitchen
domain focuses on accomplishing four distinct subtasks using a 9-DoF Franka robot. We utilize three datasets that
capture a range of behaviors: â€˜-completeâ€™, â€˜-partialâ€™, and â€˜-mixedâ€™ for this domain.
6

--- PAGE 7 ---
Table 1: Performance of V-ADT across all datasets. The methods on the right of the vertical line are transformer-based
methods, the top scores among which are highlighted in bold .
Environment TD3+BC CQL IQL DT QLDT V-ADT
halfcheetah-medium-v2 48.3 Â±0.3 44.0Â±5.4 47.4Â±0.2 42.4Â±0.2 42.3Â±0.4 48.7Â±0.2
hopper-medium-v2 59.3 Â±4.2 58.5Â±2.1 66.2Â±5.7 63.5Â±5.2 66.5Â±6.3 60.6Â±2.8
walker2d-medium-v2 83.7 Â±2.1 72.5Â±0.8 78.3Â±8.7 69.2Â±4.9 67.1Â±3.2 80.9Â±3.5
halfcheetah-medium-replay-v2 44.6 Â±0.5 45.5Â±0.5 44.2Â±1.2 35.4Â±1.6 35.6Â±0.5 42.8Â±0.2
hopper-medium-replay-v2 60.9 Â±18.8 95.0 Â±6.4 94.7Â±8.6 43.3Â±23.9 52.1Â±20.3 83.5Â±9.5
walker2d-medium-replay-v2 81.8 Â±5.5 77.2Â±5.5 73.8Â±7.1 58.9Â±7.1 58.2Â±5.1 86.3Â±1.4
halfcheetah-medium-expert-v2 90.7 Â±4.3 91.6Â±2.8 86.7Â±5.3 84.9Â±1.6 79.0Â±7.2 91.7Â±1.5
hopper-medium-expert-v2 98.0 Â±9.4 105.4 Â±6.8 91.5Â±14.3 100.6Â±8.3 94.2Â±8.2 101.6Â±5.4
walker2d-medium-expert-v2 110.1 Â±0.5 108.8Â±0.7 109.6Â±1.0 89.6Â±38.4 101.7 Â±3.4 112.1Â±0.4
mujoco-avg 75.3 Â±4.9 77.6Â±3.4 76.9Â±5.8 65.3Â±10.1 66.3 Â±6.1 78.7Â±2.8
antmaze-umaze-v2 78.6 74.0 87.5 Â±2.6 53.6Â±7.3 67.2Â±2.3 88.2Â±2.5
antmaze-umaze-diverse-v2 71.4 84.0 62.2 Â±13.8 42.2Â±5.4 62.1Â±1.6 58.6Â±4.3
antmaze-medium-play-v2 10.6 61.2 71.2 Â±7.3 0.0Â±0.0 0.0Â±0.0 62.2Â±2.5
antmaze-medium-diverse-v2 3.0 53.7 70.0 Â±10.9 0.0Â±0.0 0.0Â±0.0 52.6Â±1.4
antmaze-large-play-v2 0.2 15.8 39.6 Â±5.8 0.0Â±0.0 0.0Â±0.0 16.6Â±2.9
antmaze-large-diverse-v2 0.0 14.9 47.5 Â±9.5 0.0Â±0.0 0.0Â±0.0 36.4Â±3.6
antmaze-avg 27.3 50.6 63.0 Â±8.3 16.0Â±2.1 21.6Â±0.7 52.4Â±2.9
kitchen-complete-v0 25.0 Â±8.8 43.8 62.5 46.5Â±3.0 38.8Â±15.8 55.1Â±1.4
kitchen-partial-v0 38.3 Â±3.1 49.8 46.3 31.4Â±19.5 36.9Â±10.7 46.0Â±1.6
kitchen-mixed-v0 45.1 Â±9.5 51.0 51.0 25.8Â±5.0 17.7Â±9.5 46.8Â±6.3
kitchen-avg 36.1 Â±7.1 48.2 53.3 34.6Â±9.2 30.5Â±12.0 49.3Â±3.1
average 52.7 63.7 68.3 43.8Â±7.3 45.4Â±5.3 65.0Â±2.9
Table 2: Performance of G-ADT across all datasets. The methods on the right of the vertical line are transformer-based
methods, the top scores among which are highlighted in bold .
Environment RvS-R/G HIQL WT G-ADT
antmaze-umaze-v2 65.4 Â±4.9 83.9Â±5.3 64.9Â±6.1 83.8Â±2.3
antmaze-umaze-diverse-v2 60.9 Â±2.5 87.6Â±4.8 71.5Â±7.6 83.0Â±3.1
antmaze-medium-play-v2 58.1 Â±12.7 89.9 Â±3.5 62.8Â±5.8 82.0Â±1.7
antmaze-medium-diverse-v2 67.3 Â±8.0 87.0Â±8.4 66.7Â±3.9 83.4Â±1.9
antmaze-large-play-v2 32.4 Â±10.5 87.3 Â±3.7 72.5Â±2.8 71.0Â±1.3
antmaze-large-diverse-v2 36.9 Â±4.8 81.2Â±6.6 72.0Â±3.4 65.4Â±4.9
antmaze-avg 53.5 Â±7.2 86.2Â±5.4 68.4Â±4.9 78.1Â±2.5
kitchen-complete-v0 50.2 Â±3.6 43.8Â±19.5 49.2Â±4.6 51.4Â±1.7
kitchen-partial-v0 51.4 Â±2.6 65.0Â±9.2 63.8Â±3.5 64.2Â±5.1
kitchen-mixed-v0 60.3 Â±9.4 67.7Â±6.8 70.9Â±2.1 69.2Â±3.3
kitchen-avg 54.0 Â±5.2 58.8Â±11.8 61.3Â±3.4 61.6Â±3.4
average 53.7 Â±6.5 77.1Â±7.5 66.0Â±4.4 72.6Â±2.8
Baseline Algorithms We compare the performance of ADT with several representative baselines including (1)
offline RL methods : TD3+BC [Fujimoto and Gu, 2021], CQL [Kumar et al., 2020] and IQL [Kostrikov et al., 2022]; (2)
valued-conditioned methods : Decision Transformer (DT) [Chen et al., 2021] and Q-Learning Decision Transformer
(QLDT) [Yamagata et al., 2023]; (3) goal-conditioned methods : HIQL [Park et al., 2023], RvS [Emmons et al., 2021]
and Waypoint Transformer (WT) [Badrinath et al., 2023]. All the baseline results except for QLDT are obtained from
[Badrinath et al., 2023] and [Park et al., 2023] or by running the codes of CORL repository [Tarasov et al., 2022]. For
HIQL, we present HIQLâ€™s performance with the goal representation in Kitchen and that without goal representation
in AntMaze, as per our implementation in ADT, to ensure fair comparison. QLDT and the transformer-based actor of
ADT are implemented based on the DT codes in CORL, with similar architecture. Details are given in Appendix. The
critics and the policies to generate prompts used in ADT are re-implemented in PyTorch following the official codes
of IQL and HIQL. In all conducted experiments, five distinct random seeds are employed. Results are depicted with
95% confidence intervals, represented by shaded areas in figures and expressed as standard deviations in tables. The
reported results of ADT in tables are obtained by evaluating the final models.
5.1 Main Results
Tables 1 and 2 present the performance of two variations of ADT evaluated on offline datasets. ADT significantly
outperforms prior transformer-based decision making algorithms. Compared to DT and QLDT, two transformer-based
7

--- PAGE 8 ---
algorithms for decision making, V-ADT exhibits significant superiority especially on AntMaze and Kitchen which
require the stitching ability to success. Meanwhile, Table 2 shows that G-ADT significantly outperforms WT, an
algorithm that uses sub-goal as prompt for a transformer policy. We note that ADT enjoys comparable performance
with state-of-the-art offline RL methods. For example, V-ADT outperforms all offline RL baselines in Mujoco problems.
In AntMaze and Kitchen, V-ADT matches the performance of IQL, and significantly outperforms TD3+BC and CQL.
Table 2 concludes with similar findings for G-ADT.
5.2 Ablation Studies
5.2.1 Effectiveness of Prompting
0 2 4 6 8 10
Training Steps (1e5)020406080Normalized Returnsantmaze-medium-play-v2
0 2 4 6 8 10
Training Steps (1e5)020406080antmaze-medium-diverse-v2
0 2 4 6 8 10
Training Steps (1e5)020406080antmaze-large-play-v2
0 2 4 6 8 10
Training Steps (1e5)020406080antmaze-large-diverse-v2
V-ADT V-ADT w/o prompt DT
Figure 2: Learning curves of V-ADT with and without value prompt. The value prompt significantly boosts the
performance in harder diverse datasets.
32435465768798108119130
Normalized T arget Return020406080100120Normalized Performancewalker2d-medium-v2
32435465768798108119130
Normalized T arget Return020406080100120walker2d-medium-replay-v2
32435465768798108119130
Normalized T arget Return020406080100120walker2d-medium-expert-v2
32435465768798108119130
Normalized T arget Return020406080100120walker2d-expert-v2
V-ADT DT
Figure 3: Average normalized results of DT using different prompt. Incorporating manual prompt engineering could
not help DT outperform V-ADT.
In Section 3.1 we discuss an illustrative example showing how value-based conditional prediction can be leveraged to
solve sequential decision making problem. However, it is still unclear how much the value prompt contributes to the
remarkable empirical performance of V-ADT. This is particularly important to understand as by removing the value
prompt, our low-level policy optimization objective (6) becomes exactly the same as advantage-weighted regression
[Peng et al., 2019] with a transformer policy. We thus compare the performance of V-ADT with and without using
value prompts in Figure 2. Although the value prompt seems to be less useful for the play datasets, it significantly
improves the performance of V-ADT for the much harder diverse datasets. This confirms the effectiveness of value
prompting for solving complex problems.
The main hypothesis behind ADT is that it is essential to learn a policy for adaptive prompt generation in order to
make transformer-based sequential decision models able to learn optimal control policies. Since the initial return-to-go
prompt of DT is a tunable hyper-parameter, a nature question follows: is it possible to match the performance of ADT
through manual prompt tuning? Figure 3 delineates the results of DT using different target returns on four different
walker2d datasets. The x-axis of each subfigure represents the normalized target return input into DT, while the
y-axis portrays the corresponding evaluation performance. Empirical results indicate that manual modifications to
the target return could not improve the performance of DT, with its performance persistently lagging behind V-ADT.
We also note that there is no single prompt that performs universally well across all domains. This highlights that
the utility of prompt in DT appears constrained, particularly when working with datasets sourced from unimodal
behavior policy.
8

--- PAGE 9 ---
5.2.2 Effectiveness of Low-Level Policy Optimization Objective
0 2 4 6 8 10
Training Steps (1e5)020406080100Normalized Returnsantmaze-umaze-v2
0 2 4 6 8 10
Training Steps (1e5)020406080100antmaze-umaze-diverse-v2
0 2 4 6 8 10
Training Steps (1e5)020406080100antmaze-medium-play-v2
0 2 4 6 8 10
Training Steps (1e5)020406080100antmaze-medium-diverse-v2
V-ADT V-ADT w/o RL loss DT
0 2 4 6 8 10
Training Steps (1e4)020406080100Normalized Returnsantmaze-umaze-v2
0 2 4 6 8 10
Training Steps (1e4)020406080100antmaze-umaze-diverse-v2
0 2 4 6 8 10
Training Steps (1e4)020406080100antmaze-medium-play-v2
0 2 4 6 8 10
Training Steps (1e4)020406080100antmaze-medium-diverse-v2
G-ADT G-ADT w/o RL loss DT
Figure 4: Learning curves of V-ADT and G-ADT with and without using (6). The results demonstrate that (6) is
essential in empowering DT with stitching ability to achieve superior performance.
We claim that the sequence prediction loss used by DT does not suit our low-level policy optimization. To verify this
claim, we implement a variant of ADT which uses the original DT objective to learn the low-level policy while still
keeping learning an adaptive high-level policy. Figure 4 presents a comparison between this baseline and ADT. From
the results we observe substantial improvement in performance of both V-ADT and G-ADT when (6) is leveraged. In
particular, without using (6) to optimize the low-level policy, the effectiveness of auto-tuned prompting is notably
compromised. This also strengthens the need of joint policy optimization of high and low level policies.
5.2.3 Effectiveness of Tokenization Strategies
0 2 4 6 8 10
Training Steps (1e5)020406080100Normalized Returnsantmaze-medium-play-v2
0 2 4 6 8 10
Training Steps (1e5)020406080100antmaze-medium-diverse-v2
0 2 4 6 8 10
Training Steps (1e5)020406080100antmaze-large-play-v2
0 2 4 6 8 10
Training Steps (1e5)020406080100antmaze-large-diverse-v2
V-ADT V-ADT using individual tokens DT
0 2 4 6 8 10
Training Steps (1e4)020406080100Normalized Returnsantmaze-medium-play-v2
0 2 4 6 8 10
Training Steps (1e4)020406080100antmaze-medium-diverse-v2
0 2 4 6 8 10
Training Steps (1e4)020406080100antmaze-large-play-v2
0 2 4 6 8 10
Training Steps (1e4)020406080100antmaze-large-diverse-v2
G-ADT G-ADT using individual tokens DT
Figure 5: Learning curves of ADT with different tokenization strategies. Our design contributes to superior perfor-
mance by equally treating the states and related prompts when computing attention.
In ADT, we diverge from the methodology presented in [Chen et al., 2021] where individual tokens are produced for
each input component: return-to-go prompt, state, and action. Instead, we opt for a concatenated representation of
prompts and states. Figure 5 presents a comparative analysis between these two tokenization strategies. We observe
that our tokenization method contributes to superior performance both for V-ADT and G-ADT.
9

--- PAGE 10 ---
6 Conclusion
We propose to rethink transformer-based decision models through a hierarchical decision-making framework. Armed
with this, we introduce Autotuned Decision Transformer (ADT), which jointly optimizes the hierarchical policies
for better performance when learning from sub-optimal data. On standard offline RL benchmarks, we show ADT
significantly outperforms previous transformer-based decision making algorithms.
Our primary focus for future work is to investigate the following problems. First, besides employing values and sub-
goals as latent actions generated by the high-level policy, other options for latent actions in hierarchical RL encompass
skills [Ajay et al., 2020] and options [Sutton et al., 1999]. We would like to investigate the potential extensions of
ADT by incorporating skills and options. Second , according to the reward hypothesis, goals can be conceptualized
as the maximization of expected value through the cumulative sum of a reward signal [Silver et al., 2021, Bowling
et al., 2023]. Can we establish a unified framework that bridges value-prompted ADT and goal-prompted ADT?
Finally , according to our experiments, the advantages of substituting conventional architectures with transformer
models in RL remain uncertain. Previous studies have indicated that the incorporation of transformers in RL is most
advantageous when dealing with extensive and diverse datasets [Chebotar et al., 2023]. With this in mind, we intend
to apply ADT to create foundational decision-making models for learning multi-modal and multi-task policies.
References
Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. Opal: Offline primitive discovery for
accelerating offline reinforcement learning. arXiv preprint arXiv:2010.13611 , 2020.
Ekin AkyÃ¼rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context
learning? investigations with linear models. arXiv preprint arXiv:2211.15661 , 2022.
Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. In Proceedings of the AAAI conference
on artificial intelligence , volume 31, 2017.
Anirudhan Badrinath, Yannis Flet-Berliac, Allen Nie, and Emma Brunskill. Waypoint transformer: Reinforcement
learning via supervised learning with intermediate targets. arXiv preprint arXiv:2306.14069 , 2023.
Michael Bowling, John D Martin, David Abel, and Will Dabney. Settling the reward hypothesis. In International
Conference on Machine Learning , pages 3003â€“3020. PMLR, 2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,
Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural
information processing systems , 33:1877â€“1901, 2020.
Yevgen Chebotar, Quan Vuong, Alex Irpan, Karol Hausman, Fei Xia, Yao Lu, Aviral Kumar, Tianhe Yu, Alexander
Herzog, Karl Pertsch, et al. Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions.
arXiv preprint arXiv:2309.10150 , 2023.
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas,
and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural
information processing systems , 34:15084â€“15097, 2021.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,
Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways.
arXiv preprint arXiv:2204.02311 , 2022.
Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for offline rl via
supervised learning? In International Conference on Learning Representations , 2021.
William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with
simple and efficient sparsity. The Journal of Machine Learning Research , 23(1):5232â€“5270, 2022.
10

--- PAGE 11 ---
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven
reinforcement learning. arXiv preprint arXiv:2004.07219 , 2020.
Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. Advances in neural
information processing systems , 34:20132â€“20145, 2021.
Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In
International conference on machine learning , pages 1587â€“1596. PMLR, 2018.
Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In
International conference on machine learning , pages 2052â€“2062. PMLR, 2019.
Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. arXiv
preprint arXiv:2012.15723 , 2020.
Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case
study of simple function classes. Advances in Neural Information Processing Systems , 35:30583â€“30598, 2022.
Zhengyao Jiang, Tianjun Zhang, Michael Janner, Yueying Li, Tim RocktÃ¤schel, Edward Grefenstette, and Yuandong
Tian. Efficient planning in a compact latent action space. arXiv preprint arXiv:2208.10291 , 2022.
Martin Klissarov and Marlos C Machado. Deep laplacian-based options for temporally-extended exploration. arXiv
preprint arXiv:2301.11181 , 2023.
Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. In In-
ternational Conference on Learning Representations , 2022. URL https://openreview.net/forum?id=
68n2s9ZJWF8 .
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement
learning. Advances in Neural Information Processing Systems , 33:1179â€“1191, 2020.
Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven
Hansen, Angelos Filos, Ethan Brooks, et al. In-context reinforcement learning with algorithm distillation. arXiv
preprint arXiv:2210.14215 , 2022.
Jonathan N Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and Emma Brunskill.
Supervised pretraining can learn in-context reinforcement learning. arXiv preprint arXiv:2306.14892 , 2023.
Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv
preprint arXiv:2104.08691 , 2021.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and
perspectives on open problems. arXiv preprint arXiv:2005.01643 , 2020.
Hao Liu and Pieter Abbeel. Emergent agentic transformer from chain of hindsight experience. arXiv preprint
arXiv:2305.16554 , 2023.
Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. Chain of hindsight aligns language models with feedback. arXiv
preprint arXiv:2302.02676 , 3, 2023.
Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning.
Advances in neural information processing systems , 31, 2018.
Seohong Park, Dibya Ghosh, Benjamin Eysenbach, and Sergey Levine. Hiql: Offline goal-conditioned rl with latent
states as actions. arXiv preprint arXiv:2307.11949 , 2023.
Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable
off-policy reinforcement learning. arXiv preprint arXiv:1910.00177 , 2019.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley & Sons, 2014.
11

--- PAGE 12 ---
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are
unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In International Conference on Machine Learning , pages 8821â€“8831. PMLR,
2021.
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai
Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175 ,
2022.
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In International
conference on machine learning , pages 1312â€“1320. PMLR, 2015.
David Silver, Satinder Singh, Doina Precup, and Richard S Sutton. Reward is enough. Artificial Intelligence , 299:103535,
2021.
Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani,
Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. arXiv preprint
arXiv:2212.13138 , 2022.
Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal
abstraction in reinforcement learning. Artificial intelligence , 112(1-2):181â€“211, 1999.
Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov. CORL: Research-
oriented deep offline reinforcement learning library. In 3rd Offline RL Workshop: Offline RL as a â€Launchpadâ€ , 2022.
URLhttps://openreview.net/forum?id=SyAS49bBcv .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny
Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171 ,
2022.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-
thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems ,
35:24824â€“24837, 2022.
Yueh-Hua Wu, Xiaolong Wang, and Masashi Hamaya. Elastic decision transformer. arXiv preprint arXiv:2307.02484 ,
2023.
Chenjun Xiao, Han Wang, Yangchen Pan, Adam White, and Martha White. The in-sample softmax for offline
reinforcement learning. arXiv preprint arXiv:2302.14372 , 2023a.
Chenjun Xiao, Han Wang, Yangchen Pan, Adam White, and Martha White. The in-sample softmax for offline
reinforcement learning. In The Eleventh International Conference on Learning Representations , 2023b.
Taku Yamagata, Ahmed Khalil, and Raul Santos-Rodriguez. Q-learning decision transformer: Leveraging dynamic
programming for conditional sequence modelling in offline rl. In International Conference on Machine Learning ,
pages 38989â€“39007. PMLR, 2023.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of
thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601 , 2023.
Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language
models. arXiv preprint arXiv:2210.03493 , 2022.
12

--- PAGE 13 ---
A Implementation Details
A.1 Environments
MuJoCo For the MuJoCo framework, we incorporate nine version 2 (v2) datasets. These datasets are generated
using three distinct behavior policies: â€™-mediumâ€™, â€™-medium-playâ€™, and â€™-medium-expertâ€™, and span across three specific
tasks: â€™halfcheetahâ€™, â€™hopperâ€™, and â€™walker2dâ€™.
AntMaze The AntMaze represents a set of intricate, long-horizon navigation challenges. This domain uses the
same umaze, medium, and large mazes from the Maze2D domain, but replaces the agent with an 8-DoF Ant robot from
the OpenAI Gym MuJoCo benchmark. For the â€™umazeâ€™ dataset, trajectories are generated with the Ant robot starting
and aiming for fixed locations. To introduce complexity, the "diverse" dataset is generated by selecting random goal
locations within the maze, necessitating the Ant to navigate from various initial positions. Meanwhile, the "play"
dataset is curated by setting specific, hand-selected initial and target positions, adding a layer of specificity to the task.
We employ six version 2 (v2) datasets which include â€˜-umazeâ€™, â€˜-umaze-diverseâ€™, â€˜-medium-playâ€™, â€˜-medium-diverseâ€™,
â€˜-large-playâ€™, and â€˜-large-diverseâ€™ in our experiments.
Franka Kitchen In the Franka Kitchen environment, the primary objective is to manipulate a set of distinct objects
to achieve a predefined state configuration using a 9-DoF Franka robot. The environment offers multiple interactive
entities, such as adjusting the kettleâ€™s position, actuating the light switch, and operating the microwave and cabinet
doors, inclusive of a sliding mechanism for one of the doors. For the three principal tasks delineated, the ultimate
objective comprises the sequential completion of four salient subtasks: (1) opening the microwave, (2) relocating
the kettle, (3) toggling the light switch, and (4) initiating the sliding action of the cabinet door. In conjunction, three
comprehensive datasets have been provisioned. The â€™-completeâ€™ dataset encompasses demonstrations where all four
target subtasks are executed in a sequential manner. The â€˜-partialâ€™ dataset features various tasks, but it distinctively
includes sub-trajectories wherein the aforementioned four target subtasks are sequentially achieved. The â€˜-mixedâ€™
dataset captures an assortment of subtask executions; however, it is noteworthy that the four target subtasks are not
completed in an ordered sequence within this dataset. We utilize these datasets in our experiments.
A.2 Hyper-parameters and Implementations
We provide the lower-level actorâ€™s hyper-parameters used in our experiments in Table 3. Most hyper-parameters
are set following the default configurations in DT. For the inverse temperature used in calculating the AWR loss of
the lower-level actor in V-ADT, we set it to 1.0, 3.0, 6.0, 6.0, 6.0, 15.0 for antmaze-â€™umazeâ€™, â€™umaze-diverseâ€™, â€™medium-
diverseâ€™, â€™medium-playâ€™, â€™large-diverseâ€™, â€™large-playâ€™ dataset, respectively; for other datasets, it is set 3.0. As for G-ADT,
the inverse temperature is set to 1.0 for all the datasets. For the critic used in V-ADT and G-ADT, we follow the
default architecture and learning settings in IQL [Kostrikov et al., 2022] and HIQL [Park et al., 2023], respectively.
The implementations of ADT is based on CORL repository [Tarasov et al., 2022]. A key different between the
implementation of ADT and DT is that we follow the way in [Badrinath et al., 2023] that we concatenate the (scaled)
prompt and state, then the concatenated information and the action are treated as two tokens per timestep. In practice,
we pretrain the critic for ADT, then the critic is used to train the ADT actor. For each time of evaluation, we run
the algorithms for 10 episodes for MuJoCo datasets, 50 episodes for Kitchen datasets, and 100 episodes for AntMaze
datasets.
B IQL and HIQL
Implicit Q-learning (IQL) [Kostrikov et al., 2022] offers an approach to avoid out-of-sample action queries. This is
achieved by transforming the traditional max operator in the Bellman optimality equation to an expectile regression
framework. More formally, IQL constructs an action-value function ğ‘„(ğ‘ ,ğ‘)and a corresponding state-value function
ğ‘‰(ğ‘ ). These are governed by the loss functions:
13

--- PAGE 14 ---
Table 3: ADT Actor (Transformer) Hyper-parameters
Hyper-parameter Value
ArchitectureHidden layers 3
Hidden dim 128
Heads num 1
Clip grad 0.25
Embedding dim 128
Embedding dropout 0.1
Attention dropout 0.1
Residual dropout 0.1
Activation function GeLU
Sequence length 20 (V-ADT), 10 (G-ADT)
G-ADT Way Step 20 (kitchen-partial, kitchen-mixed), 30 (Others)
LearningOptimizer AdamW
Learning rate 1e-4
Mini-batch size 256
Discount factor 0.99
Target update rate 0.005
Value prompt scale 0.001 (Mujoco) 1.0 (Others)
Warmup steps 10000
Weight decay 0.0001
Gradient Steps 100k (G-ADT, AntMaze), 1000k (Others)
îˆ¸ğ‘‰=ğ”¼(ğ‘ ,ğ‘)âˆ¼îˆ°[ğ¿ğœ
2(Ì„ğ‘„(ğ‘ ,ğ‘)âˆ’ğ‘‰(ğ‘ ))], (7)
îˆ¸ğ‘„=ğ”¼(ğ‘ ,ğ‘,ğ‘ â€²)âˆ¼îˆ°[(ğ‘Ÿ(ğ‘ ,ğ‘)+ğ›¾ğ‘‰(ğ‘ â€²)âˆ’ğ‘„(ğ‘ ,ğ‘))2
], (8)
Here,îˆ°represents the offline dataset, Ì„ğ‘„symbolizes the target Q network, and ğ¿ğœ
2is defined as the expectile loss with
a parameter constraint ğœâˆˆ[0.5,1)and is mathematically represented as ğ¿ğœ
2(ğ‘¥)=|ğœâˆ’ğ•€(ğ‘¥ <0)|ğ‘¥2. Then the policy is
extracted with a simple advantage-weighted behavioral cloning procedure resembling supervised learning:
ğ½ğœ‹=ğ”¼(ğ‘ ,ğ‘,ğ‘ â€²)âˆ¼îˆ°[exp(ğ›½â‹…Ìƒğ´(ğ‘ ,ğ‘))logğœ‹(ğ‘âˆ£ğ‘ )], (9)
whereÌƒğ´(ğ‘ ,ğ‘)=Ì„ğ‘„(ğ‘ ,ğ‘)âˆ’ğ‘‰(ğ‘ ).
Building on this foundation, Hierarchical Implicit Q-Learning [Park et al., 2023] introduces an action-free variant of
IQL that facilitates the learning of an offline goal-conditioned value function ğ‘‰(ğ‘ ,ğ‘”):
îˆ¸ğ‘‰=ğ”¼(ğ‘ ,ğ‘ â€²)âˆ¼îˆ°,ğ‘”âˆ¼ğ‘(ğ‘”âˆ£ğœ)[ğ¿ğœ
2(ğ‘Ÿ(ğ‘ ,ğ‘”)+ğ›¾Ì„ğ‘‰(ğ‘ â€²,ğ‘”)âˆ’ğ‘‰(ğ‘ ,ğ‘”))] (10)
whereÌ„ğ‘‰denotes the target Q network. Then a high-level policy ğœ‹â„
â„(ğ‘ ğ‘¡+ğ‘˜âˆ£ğ‘ ğ‘¡,ğ‘”), which produces optimal k-steps jump,
i.e.,ğ‘˜-step subgoals ğ‘ ğ‘¡+ğ‘˜, is trained via:
ğ½ğœ‹â„=ğ”¼(ğ‘ ğ‘¡,ğ‘ ğ‘¡+ğ‘˜,ğ‘”)[exp(ğ›½â‹…Ìƒğ´â„(ğ‘ ğ‘¡,ğ‘ ğ‘¡+ğ‘˜,ğ‘”))logğœ‹â„(ğ‘ ğ‘¡+ğ‘˜âˆ£ğ‘ ğ‘¡,ğ‘”)], (11)
whereğ›½represents the inverse temperature hyper-parameter, and the value Ìƒğ´â„(ğ‘ ğ‘¡,ğ‘ ğ‘¡+ğ‘˜,ğ‘”)is approximated using
ğ‘‰(ğ‘ ğ‘¡+ğ‘˜,ğ‘”)âˆ’ğ‘‰(ğ‘ ğ‘¡,ğ‘”). Similarly, a low-level policy is trained to learn to reach the sub-goal ğ‘ ğ‘¡+ğ‘˜:
ğ½ğœ‹ğ‘™=ğ”¼(ğ‘ ğ‘¡,ğ‘ğ‘¡,ğ‘ ğ‘¡+1,ğ‘ ğ‘¡+ğ‘˜)[exp(ğ›½â‹…Ìƒğ´ğ‘™(ğ‘ ğ‘¡,ğ‘ğ‘¡,ğ‘ ğ‘¡+ğ‘˜))logğœ‹ğ‘™(ğ‘ğ‘¡âˆ£ğ‘ ğ‘¡,ğ‘ ğ‘¡+ğ‘˜)], (12)
where the value Ìƒğ´ğ‘™(ğ‘ ğ‘¡,ğ‘ğ‘¡,ğ‘ ğ‘¡+ğ‘˜)is approximated using ğ‘‰(ğ‘ ğ‘¡+1,ğ‘ ğ‘¡+ğ‘˜)âˆ’ğ‘‰(ğ‘ ğ‘¡,ğ‘ ğ‘¡+ğ‘˜).
For a comprehensive exploration of the methodology, readers are encouraged to consult the original paper.
14

--- PAGE 15 ---
0 20 40 60 80 100
Training Steps (1e4)020406080100120Normalized Returnshalfcheetah-medium-v2
0 20 40 60 80 100
Training Steps (1e4)020406080100120Normalized Returnshalfcheetah-medium-replay-v2
0 20 40 60 80 100
Training Steps (1e4)020406080100120Normalized Returnshalfcheetah-medium-expert-v2
0 20 40 60 80 100
Training Steps (1e4)020406080100120Normalized Returnshopper-medium-v2
0 20 40 60 80 100
Training Steps (1e4)020406080100120Normalized Returnshopper-medium-replay-v2
0 20 40 60 80 100
Training Steps (1e4)020406080100120Normalized Returnshopper-medium-expert-v2
0 20 40 60 80 100
Training Steps (1e4)020406080100120Normalized Returnswalker2d-medium-v2
0 20 40 60 80 100
Training Steps (1e4)020406080100120Normalized Returnswalker2d-medium-replay-v2
0 20 40 60 80 100
Training Steps (1e4)020406080100120Normalized Returnswalker2d-medium-expert-v2
0 2 4 6 8 10
Training Steps (1e5)020406080100Normalized Returnsantmaze-umaze-v2
0 2 4 6 8 10
Training Steps (1e5)020406080100Normalized Returnsantmaze-medium-play-v2
0 2 4 6 8 10
Training Steps (1e5)020406080100Normalized Returnsantmaze-large-play-v2
0 2 4 6 8 10
Training Steps (1e5)020406080100Normalized Returnsantmaze-umaze-diverse-v2
0 2 4 6 8 10
Training Steps (1e5)020406080100Normalized Returnsantmaze-medium-diverse-v2
0 2 4 6 8 10
Training Steps (1e5)020406080100Normalized Returnsantmaze-large-diverse-v2
02468101214161820
Training Steps (5e4)020406080100Normalized Returnskitchen-complete-v0
02468101214161820
Training Steps (5e4)020406080100Normalized Returnskitchen-partial-v0
02468101214161820
Training Steps (5e4)020406080100Normalized Returnskitchen-mixed-v0Figure 6: Learning curves of V-ADT.
15

--- PAGE 16 ---
0 2 4 6 8 10
Training Steps (1e4)020406080100Normalized Returnsantmaze-umaze-v2
0 2 4 6 8 10
Training Steps (1e4)020406080100Normalized Returnsantmaze-medium-play-v2
0 2 4 6 8 10
Training Steps (1e4)020406080100Normalized Returnsantmaze-large-play-v2
0 2 4 6 8 10
Training Steps (1e4)020406080100Normalized Returnsantmaze-umaze-diverse-v2
0 2 4 6 8 10
Training Steps (1e4)020406080100Normalized Returnsantmaze-medium-diverse-v2
0 2 4 6 8 10
Training Steps (1e4)020406080100Normalized Returnsantmaze-large-diverse-v2
02468101214161820
Training Steps (5e4)020406080100Normalized Returnskitchen-complete-v0
02468101214161820
Training Steps (5e4)020406080100Normalized Returnskitchen-partial-v0
02468101214161820
Training Steps (5e4)020406080100Normalized Returnskitchen-mixed-v0Figure 7: Learning curves of G-ADT.
C Complete Experimental Results
Here we provide the learning curves of our methods on all selected datasets.
D Visualization of decision-making process of G-ADT
16

--- PAGE 17 ---
Figure 8: Example of decision-making process of G-ADT in antmaze-large-play-v2 environments. We present some
snapshots within an episode. The red circle represents the sub-goal given by the prompt policy. The pentagram
indicates the target position to arrive.
17
