# 2305.17926.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2305.17926.pdf
# File size: 736108 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Large Language Models are not Fair Evaluators
Peiyi Wang1Lei Li1Liang Chen1Zefan Cai1Dawei Zhu1
Binghuai Lin3Yunbo Cao3Qi Liu2Tianyu Liu3Zhifang Sui1
1National Key Laboratory for Multimedia Information Processing, Peking University
2The University of Hong Kong3Tencent Cloud AI
{wangpeiyi9979, nlp.lilei, zefncai}@gmail.com
leo.liang.chen@outlook.com ;{dwzhu, szf}@pku.edu.cn
liuqi@cs.hku.hk ;{binghuailin, yunbocao, rogertyliu}@tencent.com
Abstract
In this paper, we uncover a systematic bias in
the evaluation paradigm of adopting large lan-
guage models (LLMs), e.g., GPT-4, as a referee
to score and compare the quality of responses
generated by candidate models. We find that
the quality ranking of candidate responses can
be easily hacked by simply altering their order
of appearance in the context. This manipulation
allows us to skew the evaluation result, making
one model appear considerably superior to the
other, e.g., Vicuna-13B could beat ChatGPT on
66 over 80 tested queries with ChatGPT as an
evaluator. To address this issue, we propose
a calibration framework with three simple yet
effective strategies: 1) Multiple Evidence Cali-
bration, which requires the evaluator model to
generate multiple evaluation evidence before
assigning ratings; 2) Balanced Position Calibra-
tion, which aggregates results across various
orders to determine the final score; 3) Human-
in-the-Loop Calibration, which introduces a
balanced position diversity entropy to measure
the difficulty of each example and seeks hu-
man assistance when needed. We also man-
ually annotate the â€œwin/tie/loseâ€ outcomes of
responses from ChatGPT and Vicuna-13B in
the Vicuna Benchmarkâ€™s question prompt, and
extensive experiments demonstrate that our ap-
proach successfully mitigates evaluation bias,
resulting in closer alignment with human judg-
ments. We release our code and human annota-
tion at https://github.com/i-Eval/
FairEval to facilitate future research.
1 Introduction
The rapid advancement of Large Language Mod-
els (LLMs) (Brown et al., 2020; Chowdhery et al.,
2022) has underscored the importance of evalu-
ating their alignment with human intent in gen-
erated responses, making it an active field of re-
search. Traditional n-gram metrics like BLEU
(Papineni et al., 2002) and ROUGE (Lin, 2004),
as well as more sophisticated model-based evalua-
tions such as BERTScore (Zhang et al., 2020) and
Response1Which response is better? Response 1: â€¦..  Response 2: â€¦..Scoring each response (1-10): Response 1: â€¦â€¦  Response 2: â€¦â€¦Scoring each response (1-10): Response 2: â€¦â€¦  Response 1: â€¦â€¦Which response is better? Response 2: â€¦..  Response 1: â€¦..Response2Response 1: 9Response2:7Response 1: 7Response2:9
GPT-4Figure 1: Simply changing the order of candidate re-
sponses leads to overturned comparison results, even
though we add the command â€œensuring that the order
in which the responses were presented does not affect
your judgmentâ€ into the prompt.
BARTScore (Yuan, Neubig, and Liu, 2021), are
insufficient for thoroughly assessing this alignment
(He et al., 2023). While human evaluation provides
the most accurate measure of model performance
and valuable insights, it can often be costly and
time-consuming. As a result, there is a growing
demand for automated assessment methods that
can consistently align with human judgments while
being more efficient and cost-effective.
ChatGPT (OpenAI, 2022) and GPT-4 (OpenAI,
2023) have recently demonstrated remarkable per-
formance across various tasks, leading to their
widespread use as both the annotators (Peng et al.,
2023; Xu et al., 2023) and evaluators (Zheng et al.,
2023; Peng et al., 2023; Sun et al., 2023; Zhou
et al., 2023; Gao et al., 2023; Wang et al., 2023b;
Dubois et al., 2023; Wang et al., 2023a). For ex-
ample, The evaluation pipeline of Vicuna (Zheng
et al., 2023) has gained significant interest and wide
usage due to its simplicity and interpretability. It
prompts GPT-4 to score and compare candidate
responses and provide explanations, making it a
valuable tool for evaluation. However, it is un-
clear how reliable LLMs are as evaluators, as they
are known to be sensitive to textual instructions
and inputs (Dong et al., 2022; Turpin et al., 2023;arXiv:2305.17926v2  [cs.CL]  30 Aug 2023

--- PAGE 2 ---
Bowman, 2023). This raises questions about the
resilience of this paradigm against perturbations,
such as the ordering of candidates during scoring,
potentially becoming the Achillesâ€™ Heel that can
be easily hacked for unreliable evaluations.
In this paper, we take a sober look at the LLMs-
as-evaluator paradigm and uncover a significant
positional bias. Specifically, we demonstrate that
GPT-4 exhibits a preference for the first displayed
candidate response by consistently assigning it
higher scores, even when the order of candidates
is subtly altered. As illustrated in Figure 1, merely
swapping the presentation order can reverse evalu-
ation outcomes. This bias is also present in Chat-
GPT, which typically favors the second response.
These findings highlight previously overlooked lim-
itations in the current evaluation paradigm.
To address this issue, we propose three simple
yet effective strategies to calibrate positional bias:
1) Multiple Evidence Calibration (MEC) : We
prompt the model to generate evaluation evidence
before assigning scores, leveraging the inherent
properties of causal language models for calibra-
tion. We also employ ensemble techniques to incor-
porate multiple evidence calibration results to fur-
ther stabilize the evaluation. 2) Balanced Position
Calibration (BPC) : To further reduce positional
bias, we evaluate each candidate in both positions
across two runs and compute the final score as the
average of the two runs. 3) Human In The Loop
Calibration (HITLC) : We also explore human-in-
the-loop evaluation and consider a diversity-based
method to get a cue to indicate biased candidates
based on the evaluation results of MEC and BPC.
To assess the efficacy of our methods, we man-
ually annotate the â€œwin/tie/loseâ€ outcomes of re-
sponses from ChatGPT and Vicuna-13B in the
Vicuna benchmark (Zheng et al., 2023), encom-
passing 80questions spanning 9distinct question
categories. Our MEC and BPC enhance the eval-
uation alignment of GPT-4 and ChatGPT by 9.8%
and14.3% accuracy, respectively. Moreover, based
on MEC and BPC, our HITLC can further effec-
tively integrate human assistance into the evalua-
tion process. Specifically, with only a 20% human
annotation cost, GPT-4 and ChatGPT can achieve
comparable or even better annotation alignment
with the average human performance, reducing the
annotation cost by up to 39%.
In summary, our key contributions are: 1)We re-
veal that LLMs exhibit severe positional bias, com-[Question]
{Q}
[The Start of Assistant 1â€™s response]
{R1}
[The End of Assistant 1â€™s response]
[The Start of Assistant 2â€™s response]
{R2}
[The End of Assistant 2â€™s response]
[System]
We would like to request your feedback on the per-
formance of two AI assistants in response to the user
question displayed above.
Please rate the helpfulness, relevance, accuracy, level
of details of their responses. Each assistant receives
an overall score on a scale of 1 to 10, where a higher
score indicates better overall performance.
Please first output a single line containing only two
values indicating the scores for Assistant 1 and 2,
respectively.
The two scores are separated by a space. In the sub-
sequent line, please provide a comprehensive expla-
nation of your evaluation, avoiding any potential bias
and ensuring that the order in which the responses
were presented does not affect your judgment.
Table 1: The evaluation template with three slots ({Q},
{R1} and {R2}) from Zheng et al. (2023). Even though
the template emphasizes not letting the order affect the
results (red text), large language models still have a
large positional bias.
promising their fairness as evaluators; 2)We de-
velop a calibration framework with three simple yet
effective strategies to calibrate the positional bias of
LLMs; 3)We manually annotate the â€œwin/tie/loseâ€
outcomes of responses from ChatGPT and Vicuna-
13B in the Vicuna benchmark and demonstrate the
effectiveness of our proposed approach through ex-
perimental results, which show closer alignment
with human judgments.
2 Positional Bias of the LLM Evaluator
2.1 LLMs as Evaluators
Recently, researchers have been utilizing LLMs
such as GPT-4 as evaluators to compare the per-
formance of two AI assistants. As shown in Table
1, an evaluation template with three placeholders
T(Q, R1, R2), is used to query the LLM for eval-
uation. For each testing question q, given two re-
sponses r1andr2from Assistant 1 and Assistant
2, respectively, the researchers populate these re-
sponses into the corresponding slots of the evalu-
ation template to form a prompt: T(Q=q, R1 =
r1, R2 =r2).The prompt is then used to query the
LLM in order to obtain the comparison result. In
this paper, we found that LLM suffers from severe

--- PAGE 3 ---
EVALUATORS VICUNA -13B V.S. OTHER MODELSVICUNA -13B W INRATECONFLICT RATE
ASASSISTANT 1 ASASSISTANT 2
GPT-4 Vicuna-13B v.s. ChatGPT 51.3% 23.8% 37 / 80 (46.3%)
GPT-4 Vicuna-13B v.s. Alpaca-13B 92.5% 92.5% 4 / 80 (5.0%)
ChatGPT Vicuna-13B v.s. ChatGPT 2.5% 82.5% 66 / 80 (82.5%)
ChatGPT Vicuna-13B v.s. Alpaca-13B 37.5% 90% 42 / 80 (52.5%)
Table 2: The Win Rate of Vicuna-13B significantly fluctuates when positioned as Assistant 1 and Assistant 2, with
GPT-4 and ChatGPT serving as evaluators. CONFLICT RATErefers to the proportion of conflicting results given by
the same evaluator when simply changing the position of two models.
positional bias, i.e., by swapping the slots of the
two responses and querying LLM twice, the evalu-
ator will most likely produce conflicting evaluation
results, and the evaluator prefers the response at a
certain position.
2.2 Revealing the Positional Bias
In this section, we adopt GPT-4 and ChatGPT as
evaluators to analyze the characteristics of posi-
tional bias in LLM evaluators. We find that:
LLMs are sensitive to the position of responses.
As shown in Table 2, in the evaluation of â€œVicuna-
13B v.s. ChatGPTâ€ and â€œVicuna-13B v.s. Alpaca-
13Bâ€, when the order was changed, LLMs provide
different evaluation results, e.g., the win rate of
Vicuna-13B extremely differs when Vicuna-13B is
evaluated as Assistant 1 and Assistant 2.
To empirically evaluate the sensitivity, we in-
troduced a metric Conflict Rate to measure
the sensitivity of the model to response posi-
tions quantitatively. Formally, given Nexamples
{(qi, r1i, r2i)}N
i=1, for each example (qi, r1i, r2i),
we query the LLM with two prompts T(qi, r1i, r2i)
andT(qi, r2i, r1i), and obtain corresponding two
evaluation results ERr12
iandERr21
i. Then we cal-
culate the Conflict Rate of the LLM evaluator as
follows:
Conflict Rate =PN
i=1I(ERr12
iÌ¸=ERr21
i)
N,
(1)
where I(.)is the indicator function. We found
that GPT-4 exhibited conflict rates of 46.3% and
5.0%, respectively. In contrast, ChatGPT displayed
considerably higher conflict rates, with figures of
82.5% and 52.5%, respectively. These findings in-
dicate that LLMs can be self-conflicting due to the
sensitivity of the response order in the template,
with stronger models being less influenced by the
placement of responses.
0 1
 1 2
 2 3
 3 4
 >=5
Score Gap0510152025303540CountTotal Count
Conflict  CountFigure 2: The conflict rate is negatively correlated with
the score gap between the two responses. When swap-
ping the order of two responses, the smaller the score
gap between them, the more likely GPT-4 is to produce
conflicting results.
LLMs suffer from Positional Bias, i.e., they pre-
fer the response in the specific position. Based
on the same evaluation template Tin Table 1, GPT-
4 tends to favor the response in the first position,
while ChatGPT shows a preference for the response
in the second position. For example, as illustrated
in Table 2, in the comparison â€œVicuna-13B v.s.
ChatGPTâ€, GPT-4 yields Win Rates of 51.3% and
23.8% for Vicuna-13B when it is positioned as As-
sistant 1 and Assistant 2, respectively. Conversely,
ChatGPT indicates Win Rates of only 2.5% and up
to82.5% for Vicuna-13B when it is positioned as
Assistant 1 and Assistant 2, respectively.
The degree of positional bias varies based on
the difference in response quality. We notice
that the conflict rate of â€œVicuna-13B v.s. Alpaca-
13Bâ€ is much lower than that of â€œVicuna-13B v.s.
ChatGPTâ€, suggesting that positional bias may not
have the same impact on the assessment of differ-
ent responses. One potential reason is that there is
a significant difference in the quality of responses
between Alpaca models and Vicuna models, and
positional bias is not strong enough to change the

--- PAGE 4 ---
ğ‘‡ğ‘‡!"(ğ‘ğ‘,ğ‘Ÿğ‘Ÿğ‘Ÿ,ğ‘Ÿğ‘Ÿğ‘Ÿ)
ğ‘‡ğ‘‡!"(ğ‘ğ‘,ğ‘Ÿğ‘Ÿğ‘Ÿ,ğ‘Ÿğ‘Ÿğ‘Ÿ)Evaluation evidence : â€¦â€¦.
The scores of Assistent1 :  ğ‘†ğ‘†!""
The scores of Assistent2 : ğ‘†ğ‘†ğ‘† !#"
Evaluation evidence : â€¦â€¦.
The scores of Assistent1 :  ğ‘†ğ‘†!"#
The scores of Assistent2 : ğ‘†ğ‘†ğ‘† !##
Evaluation evidence : â€¦â€¦.
The scores of Assistent1 :  ğ‘†ğ‘†!#"
The scores of Assistent2 : ğ‘†ğ‘†ğ‘† !""
Evaluation evidence : â€¦â€¦.
The scores of Assistent1 :  ğ‘†ğ‘†!##
The scores of Assistent2 : ğ‘†ğ‘†ğ‘† !"#ğ„ğ„ğ„ğ„#
ğ„ğ„ğ„ğ„$
ğ„ğ„ğ„ğ„â€²#
ğ„ğ„ğ„ğ„â€²$BPDE
â‰¥ğœ·ğœ·
LLMsHuman 
Annotations
<ğœ·ğœ·
ğ¶ğ¶ğ‘†ğ‘†!"=âˆ‘$%"&ğ‘†ğ‘†!"$+ğ‘†ğ‘†ğ‘†!"$
2ğ‘˜ğ‘˜
ğ¶ğ¶ğ‘†ğ‘†!#=âˆ‘$%"&ğ‘†ğ‘†!#$+ğ‘†ğ‘†ğ‘†!#$
2ğ‘˜ğ‘˜Compare the 
Calibrated S ğ’„ğ’„ores
ğ‘ªğ‘ªğ‘ªğ‘ªğ’“ğ’“ğ’“ğ’“and  ğ‘ªğ‘ªğ‘ªğ‘ªğ’“ğ’“ğ’“ğ’“(c) Human -in-the-Loop Calibration (a) Multiple Evidence Calibration
(b) Balanced 
Position CalibrationFigure 3: Demonstration of our calibration framework with three calibration methods. SrandSâ€²
rdenotes scores of
the response rin the first and second positions, respectively. BPDE is short for Balanced Position Diversity Entropy
score, which is calculated based on the evaluation results (ER) of MEC and BPC.
[Question]
{Q}
[The Start of Assistant 1â€™s response]
{R1}
[The End of Assistant 1â€™s response]
[The Start of Assistant 2â€™s response]
{R2}
[The End of Assistant 2â€™s response]
[System]
We would like to request your feedback on the per-
formance of two AI assistants in response to the user
question displayed above.
Please rate the helpfulness, relevance, accuracy, and
level of detail of their responses. Each assistant re-
ceives an overall score on a scale of 1 to 10, where a
higher score indicates better overall performance.
Please first provide a comprehensive explanation of
your evaluation, avoiding any potential bias and en-
suring that the order in which the responses were
presented does not affect your judgment. Then, out-
put two lines indicating the scores for Assistant 1 and
2, respectively.
Output with the following format:
Evaluation evidence: <evaluation explanation here>
The score of Assistant 1: <score>
The score of Assistant 2: <score>
Table 3: The evidence calibration evaluation template
that prompts LLMs to generate the evaluation evidence
first (red text), and then evaluate two responses.
judgment in such a situation. To further investigate
this issue, we grouped all the examples based on
the score difference between the two responses. As
shown in Figure 2, we found that when the score
difference between the two responses is small (e.g.,
score gap â‰¤1), the evaluation results of GPT-4
are significantly affected by the position of the re-
sponses. On the other hand, when the score differ-
ence between the two responses is large (e.g., scoregapâ‰¥3), GPT-4â€™s evaluation results are relatively
stable.
3 Calibrating the Positional Bias
We have identified that positional bias can signifi-
cantly impact the evaluation results of LLMs, mak-
ing them unfair evaluators. In this section, we pro-
pose a calibration framework with three simple yet
effective strategies to alleviate this bias to achieve
a more reliable and fair evaluation result.
3.1 Multiple Evidence Calibration
Previous studies (Zheng et al., 2023; Wang et al.,
2023b) utilize the evaluation template that draws
the conclusion first and then makes an explanation,
e.g., the template used in Table 1. However, due to
the nature of the auto-regressive model, the conclu-
sions generated by the model are not supported by
the explanation generated afterward. To this end,
as shown in Table 3, we design an evidence cali-
bration (EC) evaluation template TEC(Q, R1, R2)
that requires the model to generate the explana-
tion (evaluation evidence) first and then give the
score. In this way, the score can be calibrated with
the evaluation evidence. To further improve the
reliability of the evaluation, rather than generating
only a single EC score for each response, we per-
form a multiple evidence calibration (MEC, Figure
3(a)) that samples kEC scores {S1
r1, . . . , Sk
r1}and
{Sâ€²1
r2, . . . , Sâ€²k
r2}for responses r1andr2, where Sr
andSâ€²
rdenotes scores of the response rat the first
and second positions, respectively.

--- PAGE 5 ---
3.2 Balanced Position Calibration
We further employ a balanced position calibra-
tion (BPC) strategy to alleviate the previously
identified positional bias of LLMs. As shown in
Figure 3(b), for each example (q, r1, r2), BPC
additionally creates a query prompt TEC(q, r2, r1)
by swapping the position of two responses
in the original query prompt TEC(q, r1, r2).
Combined with MEC, we can achieve 2k
scores {S1
r1, . . . , Sk
r1, . . . , Sâ€²1
r1, . . . , Sâ€²k
r1}and
{Sâ€²1
r2, . . . , Sâ€²k
r2, . . . , S1
r2, . . . , Sk
r2}forr1andr2,
respectively. The final calibrated scores of two
responses ( CSr1andCSr2) are the average of the
2kscores:
CSR=kX
i=1Si
R+Sâ€²i
R
2k, R=r1, r2 (2)
and we regard the response with the higher average
score as the better response.
3.3 Human-in-the-Loop Calibration
In addition to the automatic calibration strategies,
another interesting question we want to explore is
whether Human-In-The-Loop Calibration (HITLC)
which performs the cooperation of humans and
LLMs as evaluators, could stabilize the evaluation
result. The key point of human-in-the-loop cali-
bration is when humans should be involved in the
evaluation and calibrate the evaluation result on
which LLM evaluators do not perform well.
To target the â€œwhenâ€ problem, inspired by Cai,
Chang, and Han (2023), we introduce a Balanced
Position Diversity Entropy (BPDE) score to find
examples requiring auxiliary human calibration
based on the evaluation results of MEC and BPC.
Specifically, as shown in Figure 3(c), we first com-
pute2kevaluation results {ERi}2k
i=1based on the
2kpairs of scores.
ER i
1â‰¤iâ‰¤k=ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³win,Si
r1> Sâ€²i
r2
tie,Si
r1=Sâ€²i
r2
lose,Si
r1< Sâ€²i
r2,ERâ€²
i
1â‰¤iâ‰¤k=ï£±
ï£´ï£´ï£²
ï£´ï£´ï£³win,Sâ€²i
r1> Si
r2
tie,Sâ€²i
r1=Si
r2
lose,Sâ€²i
r1< Si
r2,
(3)
and BPDE is defined as the entropy of the evalua-
tion results:
BPDE =X
erâˆˆ{win,tie,lose}âˆ’perlogper (4)
per=Pk
i=1I(ERi=er) + I(ERâ€²i=er)
2k.
(5)A higher BPDE score indicates that it is more likely
the evaluation requires manual correction. A thresh-
old is needed for BPDE as the hyper-parameter
to select the top- Î²most likely biased evaluations.
After selection based on the BPDE score, the an-
notators will evaluate the selected examples and
integrate the human annotations based on the ma-
jority opinion as described in Section 4.1.
4 Experiments
4.1 Human Annotation
To assess the effectiveness of our proposed strate-
gies, three of the authors manually annotate the
â€œwin/tie/loseâ€ outcomes of responses from Chat-
GPT and Vicuna-13B independently in all 80 Vi-
cuna Benchmark questions. All of the annotators
are researchers familiar with Artificial Intelligence
and are well-equipped to assess the quality of the
responses. Following the same template as the orig-
inal Vicuna, the annotators are instructed to assess
the responses provided by Vicuna-13B and Chat-
GPT from four different perspectives: helpfulness,
relevance, accuracy, and level of detail. The re-
sponses of Vicuna and ChatGPT are presented to
the annotators in random order. The evaluation
process for each example took an average of three
minutes. The final result is based on the majority
opinion among three annotators.
4.2 Experimental Setup and Metric
We use the OpenAI API to conduct our experiments
(â€œgpt-3.5-turbo-0301â€ for ChatGPT, and â€œgpt-4â€
for GPT-4). For the methods that do not need to
sample multiple generation results, we set the gen-
erated temperature to 0for deterministic generation
results. For the multiple evidence strategy, we set
the temperature to 1and sample three generation
results ( k= 3). We use the accuracy and kappa cor-
relation coefficient (McHugh, 2012) with the final
majority of human annotation results to measure
the performance of different evaluators and eval-
uation methods. When calculating the results for
methods that do not utilize BPC, we randomize the
order of the two responses from the assistants and
calculate the average results of 100 runs to ensure
stable results.
4.3 Main Results
Table 4 illustrates the performance of different
methods on our manually annotated 80annotated
examples. As is shown: 1)There is a good correla-

--- PAGE 6 ---
EVALUATORS METHODS ACCURACY KAPPA COST
Human 1 - 68.8% 0.50 $30.0
Human 2 - 76.3% 0.62 $30.0
Human 3 - 70.0% 0.50 $30.0
Human Average - 71.7% 0.54 $30.0
GPT-4 V ANILLA 52.7% 0.24 $2.00
GPT-4 EC ( k= 1) 56.5% 0.29 $2.00
GPT-4 MEC ( k= 3) 58.7% 0.30 $3.19
GPT-4 MEC ( k= 6) 60.9% 0.33 $6.38
GPT-4 MEC ( k= 3) + BPC ( k= 3) 62.5% 0.37 $6.38
GPT-4 MEC ( k= 3) + BPC ( k= 3) + HITLC ( Î²= 20% ) 73.8% 0.56 $23.1
ChatGPT V ANILLA 44.4% 0.06 $0.10
ChatGPT EC ( k= 1) 52.6% 0.23 $0.10
ChatGPT MEC ( k= 3) 53.2% 0.24 $0.17
ChatGPT MEC ( k= 6) 55.6% 0.27 $0.34
ChatGPT MEC ( k= 3) + BPC ( k= 3) 58.7% 0.31 $0.34
ChatGPT MEC ( k= 3) + BPC ( k= 3) + HITLC ( Î²= 20% ) 71.3% 0.52 $18.3
Table 4: Accuracy and kappa correlation coefficient of different methods and annotators with the final voting
human annotations. The VANILLA evaluation method was commonly used in previous works, which provided the
conclusion first and then followed with the explanation. (M)EC, BPC, and HITLC denote our proposed (multiple)
evidence calibration, balanced position calibration, and human-in-the-loop calibration respectively. Î²% means
selecting the top- Î²most likely biased examples for human annotation.
tion coefficient between the annotations provided
by each human annotator and the final voting re-
sults. In detail, the average accuracy and the kappa
correlation coefficient of human annotations are
71.7% and 0.54, respectively; 2)Overall, GPT-4
achieves higher alignment with human judgments
compared with ChatGPT, showing its powerful
alignment ability with humans; 3)Compared to the
commonly used VANILLA evaluation method, our
proposed automatic calibration strategies (i.e., EC,
MEC and BPC) significantly enhance the align-
ment between GPT-4 and ChatGPT with human
judgments; For instance, by employing the MEC
and BPC calibration strategies, ChatGPT demon-
strates a notable improvement in both accuracy and
the kappa correlation coefficient. Specifically, the
accuracy is improved by 14.3%, and the kappa
correlation coefficient is increased from 0.06to
0.31;4)â€œMEC ( k= 3) + BPC ( k= 3)â€ outper-
forms â€œMEC ( k= 6)â€, demonstrating that LLMs
are affected by positional bias, and BPC effectively
ensures that LLMs serve as fair evaluators; 5)Our
proposed HITLC can effectively enhance the align-
ment between GPT-4 and ChatGPT with human
judgments, requiring only a small amount of hu-man labor. For example, by incorporating just 20%
(Î²= 20% ) human assistance, ChatGPT attains
comparable Human Average accuracy, while reduc-
ing the annotation cost from $30to$18.3, a39%
reduction.1
In conclusion, our proposed calibration methods
are simple yet very effective in improving the eval-
uation performance with LLM as evaluators, while
maintaining low costs.
5 Analysis
5.1 Ablation on Evidence Number kand
Temperature t
In the MEC and BPC strategy, we sample keval-
uation results for each query prompt and ensem-
ble them to enhance the evaluation process. We
conduct an analysis to examine the influence of
the number of evidence k, on the modelâ€™s evalu-
ation performance. As illustrated in Figure 4(a),
we compared the performance of ChatGPT with
different values of k, namely 1, 3, 5, and 7. The
1The minimum hourly wage in the United States is
near $7.5, which can be found at https://www.worker.
gov/ . On average, annotating an example takes 3 minutes,
and the Vicuna evaluation benchmark comprises 80examples
in total. Consequently, the cost per annotator amounts to $30.

--- PAGE 7 ---
1 3 5 7
(a) evidence number k0.20.30.40.50.6
Accuracy
Kappa
0.2 0.6 1.0 1.4
(b) temperature t0.20.30.40.50.6
Accuracy
KappaFigure 4: Variation of accuracy and kappa coefficient
with a different number of evidence kand sampling
temperature twhen ChatGPT is used as the evaluator.
modelâ€™s performance increases and then tends to be
constant or decreases slightly as kbecomes larger.
Despite the slight decrease, the enhancement of the
model effect by the MCE strategy is still signifi-
cant, illustrating the stability of the MEC strategy.
Consequently, we found that a kvalue of 3yields
an optimal performance. With this value, the model
achieves a notable level of performance while keep-
ing the API cost relatively low.
We further investigate the impact of sampling
temperature ton evaluation performance. Figure
4(b) illustrates that both low temperature (i.e., 0.2
) and high temperature (i.e., 1.4) result in sub-
optimal evaluation alignment. We believe that
low temperature eliminates the randomness of sam-
pling, weakening the effect of MEC, while high
temperature compromises the quality of generation
results, leading to poor performance. Hence, it is
crucial to select an appropriate temperature (e.g.,
0.6or1.0in our experiments) for the LLM evalua-
tors.
5.2 Effectiveness of the BPDE
Our HITLC strategy utilizes BPDE score to select
examples for human annotations. In order to an-
alyze the efficiency of BPDE score, we compare
BPDE with two typical baselines, Random and
Vanilla Diversity Entropy , where Random denotes
randomly select examples for human annotations,
and Vanilla Diversity Entropy is calculated by using
only the evaluation results of one position without
swapping the position of two responses. To ensure
fairness, the total number of evaluation results is 6
for both BPDE and Vanilla Diversity Entropy. As
shown in Figure 5: 1)Two Diversity Entropy meth-
ods outperform Random, showing the effectiveness
of selecting examples based on the diversity en-
tropy; 2)BPDE outperforms Vanilla DE, which
shows LLMs are sensitive to position exchange,
and the results of BPC can significantly improve
0 10 20 30 40 50
Threshold Top-
0.550.600.650.700.750.800.85Accuracy
Accuracy for Different Thresholds
BPDE
Vanilla DE
RandomFigure 5: The accuracy of various methods changes
with different human assistant thresholds (Top- Î²) when
ChatGPT is used as the evaluator.
TEMPLATES METHODS ACC. K AP. C.R
SCORING VANILLA 44.4% 0.06 82.5%
SCORING MEC 53.2% 0.24 35.0%
SCORING MEC + BPC 58.7% 0.31 N/A
COMPARING VANILLA 50.2% 0.18 50.0%
COMPARING MEC 54.8% 0.27 42.5%
COMPARING MEC + BPC 60.3% 0.35 N/A
Table 5: Effectiveness of our proposed two automatic
calibrated methods on two different evaluation templates
with ChatGPT as the evaluator. ACC.,KAP.andC.R
are short for Accuracy, Kappa correlation coefficient,
and Conflict Rate, respectively. N/A means the Conflict
Rate is not valid for BPC methods.
the performance of HITLC compared to relying
solely on the results of MEC.
5.3 Generalization on the Pairwise
Comparison Evaluation Template
To provide a more comprehensive validation of our
proposed calibration methods, in addition to the
previous SCORING evaluation template that rates
each response, we extend our analysis to incorpo-
rate the COMPARING evaluation template. This
template facilitates a direct comparison between
two responses, eschewing explicit scores in its as-
sessment. Specifically, we prompt LLMs to pro-
duce results labeled as â€œAssistant 1â€, â€œAssistant 2â€,
or â€œSameâ€, indicating whether the response from
Assistant 1 is better, worse, or equal to that of As-
sistant 2. As is shown in Table 5: 1)Our proposed
methods are applicable to both of these templates,
leading to enhanced accuracy and a heightened cor-
relation coefficient for ChatGPT; 2)The significant

--- PAGE 8 ---
Generic
KnowledgeRoleplay
Common-senseFermi
CounterfactualCodingMathWriting0.00.20.40.60.81.0Accuracy0.70
0.50 0.500.70
0.400.500.711.00
0.60
0.430.400.51
0.400.44
0.310.510.68
0.51ChatGPT:MEC+BPC 
ChatGPT:Vanilla
Generic
KnowledgeRoleplay
Common-senseFermi
CounterfactualCodingMathWriting0.00.20.40.60.81.0Accuracy0.60 0.60
0.400.80
0.60
0.500.861.00
0.80
0.48
0.43 0.420.70
0.65
0.450.791.00
0.41GPT-4:MEC+BPC 
GPT-4:VanillaFigure 6: Fine-grained analysis of evaluation quality. Our MEC and BPC improve the evaluation performance of
ChatGPT and GPT-4 in nearly all categories. Especially on the complex task categories such as common sense,
coding, and math for ChatGPT.
performance gap (nearly 6% accuracy) between
theVANILLA method of two templates, coupled
with the high conflict rate, highlights the sensi-
tivity and unreliability of LLMs. However, our
methods effectively narrow this performance gap
and reduce conflict, showcasing how calibration
enhances LLM robustness.
5.4 Fine-Grained Analysis of Evaluation
Quality
In order to further analyze the evaluation capabili-
ties of the model, we perform a fine-grained anal-
ysis of the questions by dividing them into 9cate-
gories following Zheng et al. (2023). We calculate
the performance of different evaluators within these
categories. As shown in Figure 6, we find that: 1)
In certain complex tasks such as common-sense,
coding and math, GPT-4 performs significantly bet-
ter than ChatGPT, highlighting the strength of GPT-
4 as a more fair evaluator in these scenarios; 2)
Our proposed MEC+BPC strategy demonstrates
noticeable improvement in evaluating ChatGPTâ€™s
performance on complex tasks, allowing us to ob-
tain satisfactory evaluation results with a low API
cost.
6 Related Work
6.1 Evaluation of Large Language Models
LLMs have demonstrated powerful general gen-
eration capabilities, becoming universal assistants
(OpenAI, 2022, 2023; Song et al., 2023b). With
the rapid advancement of LLMs, it becomes cru-
cial to evaluate their ability to follow human in-
structions. Traditional evaluation methods assess
the ability by calculating a metric, like BLEU,ROUGE, BERTScore, or BARTScore, to com-
pare the generated response with a reference re-
sponse. However, these metrics do not adequately
measure the alignment of the generated response
with human intent (He et al., 2023). While hu-
man evaluation is treated as the most accurate mea-
surement of model performance, it is costly and
time-consuming to operate at scales. Consider-
ing the potent capabilities of LLMs, researchers
have started utilizing LLMs to evaluate the profi-
ciency of generative models in adhering to human
instructions (Zheng et al., 2023; Lu et al., 2023; Li
et al., 2023). In these works, Vicunaâ€™s evaluation
paradigm (Zheng et al., 2023) is widely adopted,
where it provides a question and two responses
from two models, and uses GPT-4 to determine
which response has better quality.
6.2 Bias of Deep Neural Networks
Deep Neural Networks have been proven to easily
learn biases from the data, which significantly im-
pacts their reliability. Specifically, bias has also
been investigated in natural language inference
(Gururangan et al., 2018; McCoy, Pavlick, and
Linzen, 2019; Belinkov et al., 2019; Liu et al.,
2020a,b), question answering (Min et al., 2019),
ROC story cloze (Cai, Tu, and Gimpel, 2017;
Schwartz et al., 2017), lexical inference (Levy et al.,
2015), visual question answering (Goyal et al.,
2017), information extraction (Wang et al., 2021,
2022; Song et al., 2023a; Xia et al., 2023) and so on.
LLMs are pre-trained using a vast amount of data
from the internet, making it highly likely for them
to learn biases present in those materials. Although
the LLMs are already widely adopted as a proxy of
human evaluators, the reliability of this paradigm

--- PAGE 9 ---
is not well explored. In this paper, we critically
examine the LLMs-as-evaluator paradigm and un-
cover a significant positional bias. Furthermore,
we propose three simple yet effective methods to
calibrate the positional bias to achieve reliable and
fair evaluation results.
7 Conclusion
In this paper, we reveal a systematic positional
bias in evaluation with advanced ChatGPT/GPT-4
models: by manipulating the order of candidate
responses during evaluation, the quality ranking
results can be significantly influenced. To this end,
we introduce three effective strategies, namely Mul-
tiple Evidence Calibration (MEC), Balanced Posi-
tion Calibration (BPC), and Human-in-the-Loop
Calibration (HITLC). MEC requires the LLM eval-
uator to first provide multiple evaluation evidence
to support their subsequent ratings and BPC aggre-
gates the results from various orders to determine
the final score. Based on the results of MEC and
BPC, HITLC further calculates a balanced position
diversity entropy to select examples for human an-
notations. These strategies successfully reduce the
evaluation bias and improve alignment with human
judgments. We provide our code and human anno-
tations to support future studies and enhance the
evaluation of generative models.
References
Belinkov, Y .; Poliak, A.; Shieber, S.; Van Durme, B.;
and Rush, A. 2019. Donâ€™t Take the Premise for
Granted: Mitigating Artifacts in Natural Language
Inference. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics
(ACL) .
Bowman, S. R. 2023. Eight things to know about large
language models. arXiv preprint arXiv:2304.00612 .
Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Ka-
plan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.;
Sastry, G.; Askell, A.; Agarwal, S.; Herbert-V oss,
A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh,
A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.;
Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess,
B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,
A.; Sutskever, I.; and Amodei, D. 2020. Language
Models are Few-Shot Learners. In Larochelle, H.;
Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H.,
eds., Advances in Neural Information Processing Sys-
tems 33: Annual Conference on Neural Information
Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual .Cai, Z.; Chang, B.; and Han, W. 2023. Human-in-
the-Loop through Chain-of-Thought. arXiv preprint
arXiv:2306.07932 .
Cai, Z.; Tu, L.; and Gimpel, K. 2017. Pay Attention
to the Ending:Strong Neural Baselines for the ROC
Story Cloze Task. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (ACL) .
Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.;
Mishra, G.; Roberts, A.; Barham, P.; Chung, H. W.;
Sutton, C.; Gehrmann, S.; Schuh, P.; Shi, K.;
Tsvyashchenko, S.; Maynez, J.; Rao, A.; Barnes,
P.; Tay, Y .; Shazeer, N. M.; Prabhakaran, V .; Reif, E.;
Du, N.; Hutchinson, B. C.; Pope, R.; Bradbury, J.;
Austin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.;
Levskaya, A.; Ghemawat, S.; Dev, S.; Michalewski,
H.; GarcÃ­a, X.; Misra, V .; Robinson, K.; Fedus, L.;
Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.;
Spiridonov, A.; Sepassi, R.; Dohan, D.; Agrawal, S.;
Omernick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.;
Lewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.;
Lee, K.; Zhou, Z.; Wang, X.; Saeta, B.; DÃ­az, M.;
Firat, O.; Catasta, M.; Wei, J.; Meier-Hellstern, K. S.;
Eck, D.; Dean, J.; Petrov, S.; and Fiedel, N. 2022.
PaLM: Scaling Language Modeling with Pathways.
ArXiv , abs/2204.02311.
Dong, Q.; Li, L.; Dai, D.; Zheng, C.; Wu, Z.; Chang, B.;
Sun, X.; Xu, J.; and Sui, Z. 2022. A Survey for In-
context Learning. arXiv preprint arXiv:2301.00234 .
Dubois, Y .; Li, X.; Taori, R.; Zhang, T.; Gulrajani,
I.; Ba, J.; Guestrin, C.; Liang, P.; and Hashimoto,
T. B. 2023. Alpacafarm: A simulation framework
for methods that learn from human feedback. arXiv
preprint arXiv:2305.14387 .
Gao, P.; Han, J.; Zhang, R.; Lin, Z.; Geng, S.; Zhou, A.;
Zhang, W.; Lu, P.; He, C.; Yue, X.; Li, H.; and Qiao,
Y . J. 2023. LLaMA-Adapter V2: Parameter-Efficient
Visual Instruction Model. ArXiv , abs/2304.15010.
Goyal, Y .; Khot, T.; Summers-Stay, D.; Batra, D.; and
Parikh, D. 2017. Making the V in VQA Matter:
Elevating the Role of Image Understanding in Visual
Question Answering. In 2017 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) .
Gururangan, S.; Swayamdipta, S.; Levy, O.; Schwartz,
R.; Bowman, S.; and Smith, N. A. 2018. Annota-
tion Artifacts in Natural Language Inference Data.
InProceedings of the 2018 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics .
He, T.; Zhang, J.; Wang, T.; Kumar, S.; Cho, K.; Glass,
J.; and Tsvetkov, Y . 2023. On the Blind Spots of
Model-Based Evaluation Metrics for Text Generation.
InProceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers) , 12067â€“12097. Toronto, Canada:
Association for Computational Linguistics.

--- PAGE 10 ---
Levy, O.; Remus, S.; Biemann, C.; and Dagan, I. 2015.
Do Supervised Distributional Methods Really Learn
Lexical Inference Relations? In Proceedings of the
2015 Conference of the North American Chapter of
the Association for Computational Linguistics .
Li, L.; Yin, Y .; Li, S.; Chen, L.; Wang, P.; Ren, S.;
Li, M.; Yang, Y .; Xu, J.; Sun, X.; et al. 2023.
M3IT: A Large-Scale Dataset towards Multi-Modal
Multilingual Instruction Tuning. arXiv preprint
arXiv:2306.04387 .
Lin, C.-Y . 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Text Summarization
Branches Out , 74â€“81. Barcelona, Spain: Association
for Computational Linguistics.
Liu, T.; Xin, Z.; Chang, B.; and Sui, Z. 2020a.
HypoNLI: Exploring the Artificial Patterns of
Hypothesis-only Bias in Natural Language Inference.
InProceedings of the 12th Language Resources and
Evaluation Conference . Marseille, France: European
Language Resources Association. ISBN 979-10-
95546-34-4.
Liu, T.; Xin, Z.; Ding, X.; Chang, B.; and Sui, Z. 2020b.
An Empirical Study on Model-agnostic Debiasing
Strategies for Robust Natural Language Inference.
InProceedings of the 24th Conference on Computa-
tional Natural Language Learning . Online: Associa-
tion for Computational Linguistics.
Lu, Q.; Qiu, B.; Ding, L.; Xie, L.; and Tao, D. 2023.
Error analysis prompting enables human-like trans-
lation evaluation in large language models: A case
study on chatgpt. arXiv preprint arXiv:2303.13809 .
McCoy, T.; Pavlick, E.; and Linzen, T. 2019. Right for
the Wrong Reasons: Diagnosing Syntactic Heuris-
tics in Natural Language Inference. In Proceedings
of the 57th Annual Meeting of the Association for
Computational Linguistics (ACL) .
McHugh, M. L. 2012. Interrater reliability: the kappa
statistic. Biochemia medica , 22(3): 276â€“282.
Min, S.; Wallace, E.; Singh, S.; Gardner, M.; Hajishirzi,
H.; and Zettlemoyer, L. 2019. Compositional Ques-
tions Do Not Necessitate Multi-hop Reasoning. In
Proceedings of the 57th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL) .
OpenAI. 2022. Introducing ChatGPT.
OpenAI. 2023. GPT-4 Technical Report. CoRR ,
abs/2303.08774.
Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.
Bleu: a Method for Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics , 311â€“318. Philadelphia, Pennsylvania, USA:
Association for Computational Linguistics.
Peng, B.; Li, C.; He, P.; Galley, M.; and Gao, J.
2023. Instruction Tuning with GPT-4. ArXiv ,
abs/2304.03277.Schwartz, R.; Sap, M.; Konstas, I.; Zilles, L.; Choi,
Y .; and Smith, N. A. 2017. The Effect of Different
Writing Tasks on Linguistic Style: A Case Study
of the ROC Story Cloze Task. In Proceedings of
the 21st Conference on Computational Natural Lan-
guage Learning (CoNLL) .
Song, Y .; Wang, P.; Zhu, D.; Liu, T.; Sui, Z.; and Li, S.
2023a. RepCL: Exploring Effective Representation
for Continual Text Classification. arXiv preprint
arXiv:2305.07289 .
Song, Y .; Xiong, W.; Zhu, D.; Li, C.; Wang, K.; Tian,
Y .; and Li, S. 2023b. RestGPT: Connecting Large
Language Models with Real-World Applications via
RESTful APIs. arXiv preprint arXiv:2306.06624 .
Sun, Z.; Shen, Y .; Zhou, Q.; Zhang, H.; Chen, Z.;
Cox, D. D.; Yang, Y .; and Gan, C. 2023. Principle-
Driven Self-Alignment of Language Models from
Scratch with Minimal Human Supervision. ArXiv ,
abs/2305.03047.
Turpin, M.; Michael, J.; Perez, E.; and Bowman, S. R.
2023. Language Models Donâ€™t Always Say What
They Think: Unfaithful Explanations in Chain-of-
Thought Prompting. CoRR , abs/2305.04388.
Wang, P.; Song, Y .; Liu, T.; Lin, B.; Cao, Y .; Li, S.;
and Sui, Z. 2022. Learning Robust Representations
for Continual Relation Extraction via Adversarial
Class Augmentation. In Proceedings of the 2022
Conference on Empirical Methods in Natural Lan-
guage Processing , 6264â€“6278. Abu Dhabi, United
Arab Emirates: Association for Computational Lin-
guistics.
Wang, P.; Xun, R.; Liu, T.; Dai, D.; Chang, B.; and Sui,
Z. 2021. Behind the Scenes: An Exploration of Trig-
ger Biases Problem in Few-Shot Event Classification.
InProceedings of the 30th ACM International Con-
ference on Information & Knowledge Management ,
1969â€“1978.
Wang, Y .; Ivison, H.; Dasigi, P.; Hessel, J.; Khot, T.;
Chandu, K. R.; Wadden, D.; MacMillan, K.; Smith,
N. A.; Beltagy, I.; et al. 2023a. How Far Can Camels
Go? Exploring the State of Instruction Tuning on
Open Resources. arXiv preprint arXiv:2306.04751 .
Wang, Y .; Yu, Z.; Zeng, Z.; Yang, L.; Wang, C.;
Chen, H.; Jiang, C.; Xie, R.; Wang, J.; Xie, X.;
et al. 2023b. PandaLM: An Automatic Evaluation
Benchmark for LLM Instruction Tuning Optimiza-
tion. arXiv preprint arXiv:2306.05087 .
Xia, H.; Wang, P.; Liu, T.; Lin, B.; Cao, Y .; and
Sui, Z. 2023. Enhancing Continual Relation Extrac-
tion via Classifier Decomposition. In Findings of
the Association for Computational Linguistics: ACL
2023 , 10053â€“10062. Toronto, Canada: Association
for Computational Linguistics.
Xu, C.; Guo, D.; Duan, N.; and McAuley, J.
2023. Baize: An Open-Source Chat Model with
Parameter-Efficient Tuning on Self-Chat Data. ArXiv ,
abs/2304.01196.

--- PAGE 11 ---
Yuan, W.; Neubig, G.; and Liu, P. 2021. BARTScore:
Evaluating Generated Text as Text Generation. In
Ranzato, M.; Beygelzimer, A.; Dauphin, Y . N.; Liang,
P.; and Vaughan, J. W., eds., Advances in Neural
Information Processing Systems 34: Annual Con-
ference on Neural Information Processing Systems
2021, NeurIPS 2021, December 6-14, 2021, virtual ,
27263â€“27277.
Zhang, T.; Kishore, V .; Wu, F.; Weinberger, K. Q.; and
Artzi, Y . 2020. BERTScore: Evaluating Text Genera-
tion with BERT. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020 . OpenReview.net.
Zheng, L.; Chiang, W.-L.; Sheng, Y .; Zhuang, S.; Wu,
Z.; Zhuang, Y .; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al.
2023. Judging LLM-as-a-judge with MT-Bench and
Chatbot Arena. arXiv preprint arXiv:2306.05685 .
Zhou, C.; Liu, P.; Xu, P.; Iyer, S.; Sun, J.; Mao, Y .;
Ma, X.; Efrat, A.; Yu, P.; Yu, L.; Zhang, S.; Ghosh,
G.; Lewis, M.; Zettlemoyer, L.; and Levy, O. 2023.
LIMA: Less Is More for Alignment.
