# 2305.18290.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2305.18290.pdf
# Kích thước tệp: 1299212 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Tối ưu hóa Sở thích Trực tiếp:
Mô hình Ngôn ngữ của Bạn Bí mật là một Mô hình Phần thưởng
Rafael Rafailov∗†Archit Sharma∗†Eric Mitchell∗†
Stefano Ermon†‡Christopher D. Manning†Chelsea Finn†
†Đại học Stanford‡CZ Biohub
{rafailov,architsh,eric.mitchell}@cs.stanford.edu

Tóm tắt
Trong khi các mô hình ngôn ngữ (LM) không có giám sát quy mô lớn học được kiến thức rộng lớn về thế giới và một số kỹ năng suy luận, việc đạt được kiểm soát chính xác hành vi của chúng là khó khăn do bản chất hoàn toàn không có giám sát của quá trình huấn luyện. Các phương pháp hiện tại để đạt được khả năng điều khiển như vậy thu thập nhãn của con người về chất lượng tương đối của các kết quả tạo ra bởi mô hình và tinh chỉnh LM không có giám sát để phù hợp với những sở thích này, thường sử dụng học tăng cường từ phản hồi của con người (RLHF). Tuy nhiên, RLHF là một quy trình phức tạp và thường không ổn định, đầu tiên phù hợp một mô hình phần thưởng phản ánh sở thích của con người, sau đó tinh chỉnh LM không có giám sát lớn sử dụng học tăng cường để tối đa hóa phần thưởng ước tính này mà không trôi quá xa khỏi mô hình gốc. Trong bài báo này, chúng tôi giới thiệu một tham số hóa mới của mô hình phần thưởng trong RLHF cho phép trích xuất chính sách tối ưu tương ứng ở dạng đóng, cho phép chúng tôi giải quyết vấn đề RLHF tiêu chuẩn chỉ với một hàm mất mát phân loại đơn giản. Thuật toán kết quả, mà chúng tôi gọi là Tối ưu hóa Sở thích Trực tiếp (DPO), ổn định, hiệu quả và nhẹ về mặt tính toán, loại bỏ nhu cầu lấy mẫu từ LM trong quá trình tinh chỉnh hoặc thực hiện điều chỉnh siêu tham số đáng kể. Các thí nghiệm của chúng tôi cho thấy DPO có thể tinh chỉnh LM để phù hợp với sở thích của con người cũng như hoặc tốt hơn các phương pháp hiện tại. Đáng chú ý, tinh chỉnh với DPO vượt trội hơn RLHF dựa trên PPO trong khả năng kiểm soát cảm xúc của các kết quả tạo ra, và khớp hoặc cải thiện chất lượng phản hồi trong tóm tắt và đối thoại một lượt trong khi đơn giản hơn đáng kể để triển khai và huấn luyện.

1 Giới thiệu
Các mô hình ngôn ngữ không có giám sát lớn (LM) được huấn luyện trên các tập dữ liệu rất lớn có được những khả năng đáng ngạc nhiên [11,7,42,8]. Tuy nhiên, những mô hình này được huấn luyện trên dữ liệu được tạo ra bởi con người với nhiều mục tiêu, ưu tiên và bộ kỹ năng khác nhau. Một số mục tiêu và bộ kỹ năng này có thể không mong muốn để bắt chước; ví dụ, trong khi chúng ta có thể muốn trợ lý lập trình AI của mình hiểu những lỗi lập trình phổ biến để sửa chúng, tuy nhiên, khi tạo mã, chúng ta muốn thiên về khả năng lập trình chất lượng cao (có thể hiếm) có trong dữ liệu huấn luyện của nó. Tương tự, chúng ta có thể muốn mô hình ngôn ngữ của mình nhận thức được một quan niệm sai lầm phổ biến được 50% mọi người tin, nhưng chúng ta chắc chắn không muốn mô hình tuyên bố quan niệm sai lầm này là đúng trong 50% các truy vấn về nó! Nói cách khác, việc lựa chọn các phản hồi và hành vi mong muốn của mô hình từ kiến thức và khả năng rất rộng lớn của nó là rất quan trọng để xây dựng các hệ thống AI an toàn, hiệu quả và có thể kiểm soát [28]. Trong khi các phương pháp hiện tại thường điều khiển LM để phù hợp với sở thích của con người sử dụng học tăng cường (RL),

∗Đóng góp bằng nhau; các tác giả ít kinh nghiệm hơn được liệt kê trước.
Hội nghị lần thứ 37 về Hệ thống Xử lý Thông tin Neural (NeurIPS 2023).arXiv:2305.18290v3 [cs.LG] 29 Jul 2024

--- TRANG 2 ---
Hình 1: DPO tối ưu hóa cho sở thích của con người trong khi tránh học tăng cường. Các phương pháp hiện tại để tinh chỉnh mô hình ngôn ngữ với phản hồi của con người trước tiên phù hợp một mô hình phần thưởng với một tập dữ liệu của các lời nhắc và sở thích của con người đối với các cặp phản hồi, sau đó sử dụng RL để tìm một chính sách tối đa hóa phần thưởng đã học. Ngược lại, DPO trực tiếp tối ưu hóa cho chính sách thỏa mãn tốt nhất các sở thích với một mục tiêu phân loại đơn giản, phù hợp một mô hình phần thưởng ngầm mà chính sách tối ưu tương ứng có thể được trích xuất ở dạng đóng.

chúng tôi sẽ cho thấy rằng mục tiêu dựa trên RL được sử dụng bởi các phương pháp hiện tại có thể được tối ưu hóa chính xác với một mục tiêu entropy chéo nhị phân đơn giản, đơn giản hóa đáng kể pipeline học sở thích.

Ở mức độ cao, các phương pháp hiện tại thấm nhuần những hành vi mong muốn vào một mô hình ngôn ngữ sử dụng các tập hợp được tuyển chọn của sở thích con người đại diện cho các loại hành vi mà con người thấy an toàn và hữu ích. Giai đoạn học sở thích này xảy ra sau giai đoạn ban đầu của tiền huấn luyện không có giám sát quy mô lớn trên một tập dữ liệu văn bản lớn. Trong khi cách tiếp cận đơn giản nhất cho việc học sở thích là tinh chỉnh có giám sát trên các minh họa con người về các phản hồi chất lượng cao, lớp phương pháp thành công nhất là học tăng cường từ phản hồi của con người (hoặc AI) (RLHF/RLAIF; [12,2]). Các phương pháp RLHF phù hợp một mô hình phần thưởng với một tập dữ liệu của sở thích con người và sau đó sử dụng RL để tối ưu hóa một chính sách mô hình ngôn ngữ để sản xuất các phản hồi được gán phần thưởng cao mà không trôi quá mức xa khỏi mô hình gốc. Trong khi RLHF tạo ra các mô hình với khả năng hội thoại và lập trình ấn tượng, pipeline RLHF phức tạp hơn đáng kể so với học có giám sát, bao gồm huấn luyện nhiều LM và lấy mẫu từ chính sách LM trong vòng lặp huấn luyện, phát sinh chi phí tính toán đáng kể.

Trong bài báo này, chúng tôi cho thấy cách trực tiếp tối ưu hóa một mô hình ngôn ngữ để tuân thủ sở thích của con người, mà không cần mô hình phần thưởng rõ ràng hoặc học tăng cường. Chúng tôi đề xuất Tối ưu hóa Sở thích Trực tiếp (DPO), một thuật toán ngầm tối ưu hóa cùng mục tiêu như các thuật toán RLHF hiện tại (tối đa hóa phần thưởng với ràng buộc KL-divergence) nhưng đơn giản để triển khai và thẳng thắn để huấn luyện. Một cách trực quan, bản cập nhật DPO tăng xác suất log tương đối của các phản hồi được ưa thích so với không được ưa thích, nhưng nó kết hợp một trọng số quan trọng động, theo từng ví dụ ngăn chặn sự thoái hóa mô hình mà chúng tôi thấy xảy ra với một mục tiêu tỷ lệ xác suất ngây thơ. Giống như các thuật toán hiện tại, DPO dựa vào một mô hình sở thích lý thuyết (như mô hình Bradley-Terry; [5]) đo lường mức độ phù hợp của một hàm phần thưởng cho với dữ liệu sở thích thực nghiệm. Tuy nhiên, trong khi các phương pháp hiện tại sử dụng mô hình sở thích để định nghĩa một hàm mất mát sở thích để huấn luyện một mô hình phần thưởng và sau đó huấn luyện một chính sách tối ưu hóa mô hình phần thưởng đã học, DPO sử dụng một sự thay đổi biến số để định nghĩa hàm mất mát sở thích như một hàm của chính sách trực tiếp. Với một tập dữ liệu của sở thích con người đối với các phản hồi mô hình, DPO do đó có thể tối ưu hóa một chính sách sử dụng một mục tiêu entropy chéo nhị phân đơn giản, tạo ra chính sách tối ưu cho một hàm phần thưởng ngầm phù hợp với dữ liệu sở thích.

Đóng góp chính của chúng tôi là Tối ưu hóa Sở thích Trực tiếp (DPO), một thuật toán đơn giản không cần RL để huấn luyện mô hình ngôn ngữ từ sở thích. Các thí nghiệm của chúng tôi cho thấy DPO ít nhất là hiệu quả như các phương pháp hiện tại, bao gồm RLHF dựa trên PPO, để học từ sở thích trong các nhiệm vụ như điều chỉnh cảm xúc, tóm tắt và đối thoại, sử dụng các mô hình ngôn ngữ với tới 6B tham số.

2 Công trình Liên quan
Các mô hình ngôn ngữ tự giám sát với quy mô ngày càng tăng học hoàn thành một số nhiệm vụ zero-shot [33] hoặc với lời nhắc few-shot [6,27,11]. Tuy nhiên, hiệu suất của chúng trên các nhiệm vụ downstream và sự phù hợp với ý định người dùng có thể được cải thiện đáng kể bằng cách tinh chỉnh trên các tập dữ liệu của hướng dẫn và hoàn thành được viết bởi con người [25,38,13,41]. Quy trình 'điều chỉnh hướng dẫn' này cho phép LLM khái quát hóa cho các hướng dẫn bên ngoài tập điều chỉnh hướng dẫn và nói chung tăng khả năng sử dụng của chúng [13]. Bất chấp thành công của điều chỉnh hướng dẫn, các đánh giá tương đối của con người về chất lượng phản hồi thường dễ thu thập hơn so với các minh họa chuyên gia, và do đó các công trình tiếp theo đã tinh chỉnh LLM với các tập dữ liệu của sở thích con người, cải thiện thành thạo trong dịch thuật [20], tóm tắt [40,51], kể chuyện [51], và tuân thủ hướng dẫn [28,34]. Những phương pháp này đầu tiên tối ưu hóa một hàm phần thưởng mạng neural để tương thích với tập dữ liệu của sở thích dưới một mô hình sở thích như mô hình Bradley-Terry [5], sau đó tinh chỉnh một mô hình ngôn ngữ để tối đa hóa phần thưởng cho trước sử dụng các thuật toán học tăng cường, thường là REINFORCE [47], tối ưu hóa chính sách gần (PPO; [39]), hoặc các biến thể [34]. Một dòng công trình liên quan chặt chẽ tận dụng các LLM được tinh chỉnh để tuân thủ hướng dẫn với phản hồi của con người để tạo ra dữ liệu sở thích tổng hợp bổ sung cho các thuộc tính mục tiêu như an toàn hoặc vô hại [2], chỉ sử dụng giám sát yếu từ con người dưới dạng một rubric văn bản cho các chú thích của LLM. Những phương pháp này đại diện cho sự hội tụ của hai nhóm công trình: một nhóm công trình về huấn luyện mô hình ngôn ngữ với học tăng cường cho nhiều mục tiêu khác nhau [35,29,48] và một nhóm công trình khác về các phương pháp tổng quát để học từ sở thích con người [12,21]. Bất chấp sự hấp dẫn của việc sử dụng sở thích tương đối của con người, tinh chỉnh các mô hình ngôn ngữ lớn với học tăng cường vẫn là một thách thức thực tế lớn; công trình này cung cấp một cách tiếp cận được chứng minh lý thuyết để tối ưu hóa sở thích tương đối mà không cần RL.

Bên ngoài bối cảnh ngôn ngữ, học chính sách từ sở thích đã được nghiên cứu trong cả môi trường bandit và học tăng cường, và một số cách tiếp cận đã được đề xuất. Học bandit theo ngữ cảnh sử dụng sở thích hoặc xếp hạng của các hành động, thay vì phần thưởng, được biết đến như một bandit đấu tay đôi theo ngữ cảnh (CDB; [50,14]). Trong trường hợp không có phần thưởng tuyệt đối, phân tích lý thuyết của CDB thay thế khái niệm về chính sách tối ưu bằng một người chiến thắng von Neumann, một chính sách có tỷ lệ thắng kỳ vọng so với bất kỳ chính sách nào khác ít nhất là 50% [14]. Tuy nhiên, trong môi trường CDB, nhãn sở thích được đưa ra trực tuyến, trong khi trong việc học từ sở thích con người, chúng ta thường học từ một lô cố định của các cặp hành động được chú thích sở thích ngoại tuyến [49]. Tương tự, RL dựa trên sở thích (PbRL) học từ sở thích nhị phân được tạo ra bởi một hàm 'chấm điểm' chưa biết thay vì phần thưởng [9, 37]. Nhiều thuật toán khác nhau cho PbRL tồn tại, bao gồm các phương pháp có thể tái sử dụng dữ liệu sở thích ngoài chính sách, nhưng nói chung bao gồm đầu tiên ước tính rõ ràng hàm chấm điểm tiềm ẩn (tức là mô hình phần thưởng) và sau đó tối ưu hóa nó [16,9,12,36,21]. Thay vào đó, chúng tôi trình bày một cách tiếp cận học chính sách một giai đoạn trực tiếp tối ưu hóa một chính sách để thỏa mãn sở thích.

3 Kiến thức Cơ bản
Chúng tôi xem xét pipeline RLHF trong Ziegler et al. (và sau này [40,1,28]). Nó thường bao gồm ba giai đoạn: 1) tinh chỉnh có giám sát (SFT); 2) lấy mẫu sở thích và học phần thưởng và 3) tối ưu hóa RL.

SFT: RLHF thường bắt đầu bằng việc tinh chỉnh một LM được tiền huấn luyện với học có giám sát trên dữ liệu chất lượng cao cho (các) nhiệm vụ downstream quan tâm (đối thoại, tóm tắt, v.v.), để có được một mô hình πSFT.

Giai đoạn Mô hình Phần thưởng: Trong giai đoạn thứ hai, mô hình SFT được nhắc với các lời nhắc x để tạo ra các cặp câu trả lời (y1, y2) ∼ πSFT(y|x). Những cái này sau đó được trình bày cho các nhãn viên con người, những người bày tỏ sở thích cho một câu trả lời, được ký hiệu là yw ≻ yl|x trong đó yw và yl biểu thị hoàn thành được ưa thích và không được ưa thích giữa (y1, y2) tương ứng. Các sở thích được giả định được tạo ra bởi một số mô hình phần thưởng tiềm ẩn r*(y, x), mà chúng ta không có quyền truy cập. Có một số cách tiếp cận được sử dụng để mô hình hóa sở thích, mô hình Bradley-Terry (BT) [5] là một lựa chọn phổ biến (mặc dù các mô hình xếp hạng Plackett-Luce tổng quát hơn [32,23] cũng tương thích với khung nếu chúng ta có quyền truy cập vào một số câu trả lời được xếp hạng). Mô hình BT quy định rằng phân phối sở thích con người p* có thể được viết như:

p*(y1 ≻ y2|x) = exp(r*(x, y1)) / (exp(r*(x, y1)) + exp(r*(x, y2))).   (1)

Giả sử quyền truy cập vào một tập dữ liệu tĩnh của các so sánh D = {x(i), y(i)w, y(i)l}Ni=1 được lấy mẫu từ p*, chúng ta có thể tham số hóa một mô hình phần thưởng rφ(x, y) và ước tính các tham số thông qua maximum likelihood. Đưa vấn đề về dạng phân loại nhị phân, chúng ta có hàm mất mát negative log-likelihood:

LR(rφ, D) = -E(x,yw,yl)∼D[log σ(rφ(x, yw) - rφ(x, yl))]   (2)

trong đó σ là hàm logistic. Trong bối cảnh LM, mạng rφ(x, y) thường được khởi tạo từ mô hình SFT πSFT(y|x) với việc bổ sung một lớp tuyến tính trên đỉnh lớp transformer cuối cùng tạo ra một dự đoán vô hướng duy nhất cho giá trị phần thưởng [51]. Để đảm bảo một hàm phần thưởng với phương sai thấp hơn, các công trình trước chuẩn hóa phần thưởng, sao cho Ex,y∼D[rφ(x, y)] = 0 cho tất cả x.

Giai đoạn Tinh chỉnh RL: Trong giai đoạn RL, hàm phần thưởng đã học được sử dụng để cung cấp phản hồi cho mô hình ngôn ngữ. Theo các công trình trước [17, 18], việc tối ưu hóa được công thức hóa như

max πθ Ex∼D,y∼πθ(y|x)[rφ(x, y)] - βDKL(πθ(y|x)||πref(y|x)),   (3)

--- TRANG 3 ---
trong đó β là một tham số kiểm soát độ lệch khỏi chính sách tham chiếu cơ sở πref, cụ thể là mô hình SFT ban đầu πSFT. Trong thực tế, chính sách mô hình ngôn ngữ πθ cũng được khởi tạo thành πSFT. Ràng buộc được thêm vào là quan trọng, vì nó ngăn chặn mô hình lệch quá xa khỏi phân phối mà mô hình phần thưởng chính xác, cũng như duy trì sự đa dạng tạo ra và ngăn chặn sự sụp đổ mode thành các câu trả lời phần thưởng cao duy nhất. Do bản chất rời rạc của việc tạo ngôn ngữ, mục tiêu này không thể vi phân và thường được tối ưu hóa với học tăng cường. Cách tiếp cận tiêu chuẩn [51,40,1,28] đã là xây dựng hàm phần thưởng r(x, y) = rφ(x, y) - β(log πθ(y|x) - log πref(y|x)), và tối đa hóa sử dụng PPO [39].

4 Tối ưu hóa Sở thích Trực tiếp
Được thúc đẩy bởi những thách thức của việc áp dụng các thuật toán học tăng cường trên các vấn đề quy mô lớn như tinh chỉnh mô hình ngôn ngữ, mục tiêu của chúng tôi là phái sinh một cách tiếp cận đơn giản cho tối ưu hóa chính sách sử dụng sở thích trực tiếp. Không giống như các phương pháp RLHF trước, học một phần thưởng và sau đó tối ưu hóa nó thông qua RL, cách tiếp cận của chúng tôi tận dụng một lựa chọn đặc biệt của tham số hóa mô hình phần thưởng cho phép trích xuất chính sách tối ưu của nó ở dạng đóng, mà không cần vòng lặp huấn luyện RL. Như chúng tôi sẽ mô tả chi tiết tiếp theo, thông tin quan trọng của chúng tôi là tận dụng một ánh xạ phân tích từ các hàm phần thưởng đến các chính sách tối ưu, cho phép chúng tôi biến đổi một hàm mất mát trên các hàm phần thưởng thành một hàm mất mát trên các chính sách. Cách tiếp cận thay đổi biến số này tránh phù hợp một mô hình phần thưởng rõ ràng, độc lập, trong khi vẫn tối ưu hóa dưới các mô hình sở thích hiện tại, như mô hình Bradley-Terry. Về bản chất, mạng chính sách đại diện cho cả mô hình ngôn ngữ và phần thưởng (ngầm).

Phái sinh mục tiêu DPO. Chúng tôi bắt đầu với cùng mục tiêu RL như công trình trước, Eq. 3, dưới một hàm phần thưởng tổng quát r. Theo công trình trước [31,30,19,15], thật đơn giản để chỉ ra rằng nghiệm tối ưu cho mục tiêu tối đa hóa phần thưởng có ràng buộc KL trong Eq. 3 có dạng:

πr(y|x) = (1/Z(x))πref(y|x) exp((1/β)r(x, y)),   (4)

trong đó Z(x) = Σy πref(y|x) exp((1/β)r(x, y)) là hàm phân vùng. Xem Phụ lục A.1 cho một phái sinh hoàn chỉnh. Ngay cả khi chúng ta sử dụng ước tính MLE rφ của hàm phần thưởng thực tế r*, vẫn tốn kém để ước tính hàm phân vùng Z(x) [19,15], điều này làm cho biểu diễn này khó sử dụng trong thực tế. Tuy nhiên, chúng ta có thể sắp xếp lại Eq. 4 để biểu hiện hàm phần thưởng theo chính sách tối ưu tương ứng πr, chính sách tham chiếu πref, và hàm phân vùng chưa biết Z(·). Cụ thể, chúng ta đầu tiên lấy logarithm của cả hai vế của Eq. 4 và sau đó với một số đại số chúng ta có được:

r(x, y) = β log(πr(y|x)/πref(y|x)) + β log Z(x).   (5)

Chúng ta có thể áp dụng tái tham số hóa này cho phần thưởng thực tế r* và mô hình tối ưu tương ứng π*. May mắn thay, mô hình Bradley-Terry chỉ phụ thuộc vào sự khác biệt của phần thưởng giữa hai hoàn thành, tức là, p*(y1 ≻ y2|x) = σ(r*(x, y1) - r*(x, y2)). Thay thế tái tham số hóa trong Eq. 5 cho r*(x, y) vào mô hình sở thích Eq. 1, hàm phân vùng triệt tiêu, và chúng ta có thể biểu hiện xác suất sở thích con người chỉ theo chính sách tối ưu π* và chính sách tham chiếu πref. Do đó, chính sách RLHF tối ưu π* dưới mô hình Bradley-Terry thỏa mãn mô hình sở thích:

p*(y1 ≻ y2|x) = 1/(1 + exp(β log(π*(y2|x)/πref(y2|x)) - β log(π*(y1|x)/πref(y1|x))))   (6)

Phái sinh trong Phụ lục A.2. Trong khi Eq. 6 sử dụng mô hình Bradley-Terry, chúng ta có thể tương tự phái sinh các biểu thức dưới các mô hình Plackett-Luce tổng quát hơn [32, 23], được hiển thị trong Phụ lục A.3.

Bây giờ chúng ta có xác suất của dữ liệu sở thích con người theo chính sách tối ưu thay vì mô hình phần thưởng, chúng ta có thể công thức hóa một mục tiêu maximum likelihood cho một chính sách tham số πθ. Tương tự với cách tiếp cận mô hình phần thưởng (tức là Eq. 2), mục tiêu chính sách của chúng ta trở thành:

LDPO(πθ; πref) = -E(x,yw,yl)∼D[log σ(β log(πθ(yw|x)/πref(yw|x)) - β log(πθ(yl|x)/πref(yl|x)))].   (7)

Theo cách này, chúng ta phù hợp một phần thưởng ngầm sử dụng một tham số hóa thay thế, mà chính sách tối ưu của nó đơn giản là πθ. Hơn nữa, vì quy trình của chúng ta tương đương với việc phù hợp một mô hình Bradley-Terry được tái tham số hóa,

--- TRANG 4 ---
nó có những tính chất lý thuyết nhất định, như tính nhất quán dưới giả định phù hợp của phân phối dữ liệu sở thích [4]. Trong Phần 5, chúng tôi thảo luận thêm về các tính chất lý thuyết của DPO liên quan đến các công trình khác.

Bản cập nhật DPO làm gì? Để hiểu cơ học của DPO, việc phân tích gradient của hàm mất mát LDPO là hữu ích. Gradient đối với các tham số θ có thể được viết như:

∇θLDPO(πθ; πref) = 
-βE(x,yw,yl)∼D[σ(r̂θ(x, yl) - r̂θ(x, yw))|{z}
trọng số cao hơn khi ước tính phần thưởng sai
[∇θ log π(yw|x)|{z}
tăng khả năng của yw - ∇θ log π(yl|x)|{z}
giảm khả năng của yl]],

trong đó r̂θ(x, y) = β log(πθ(y|x)/πref(y|x)) là phần thưởng được định nghĩa ngầm bởi mô hình ngôn ngữ πθ và mô hình tham chiếu πref (thêm trong Phần 5). Một cách trực quan, gradient của hàm mất mát LDPO tăng khả năng của các hoàn thành được ưa thích yw và giảm khả năng của các hoàn thành không được ưa thích yl. Quan trọng là, các ví dụ được cân bằng bởi mức độ mô hình phần thưởng ngầm r̂θ đánh giá cao hơn các hoàn thành không được ưa thích, được chia tỷ lệ bởi β, tức là, mức độ không chính xác mà mô hình phần thưởng ngầm sắp xếp các hoàn thành, tính đến sức mạnh của ràng buộc KL. Các thí nghiệm của chúng tôi gợi ý tầm quan trọng của việc cân bằng này, vì một phiên bản ngây thơ của phương pháp này mà không có hệ số cân bằng có thể gây ra sự thoái hóa mô hình ngôn ngữ (Bảng 3 Phụ lục).

Đại cương DPO. Pipeline DPO tổng quát như sau: 1) Lấy mẫu hoàn thành y1, y2 ∼ πref(· |x) cho mọi lời nhắc x, gắn nhãn với sở thích của con người để xây dựng tập dữ liệu ngoại tuyến của sở thích D = {x(i), y(i)w, y(i)l}Ni=1 và 2) tối ưu hóa mô hình ngôn ngữ πθ để tối thiểu hóa LDPO cho πref và D cho trước và β mong muốn. Trong thực tế, người ta muốn tái sử dụng các tập dữ liệu sở thích có sẵn công khai, thay vì tạo ra các mẫu và thu thập sở thích của con người. Vì các tập dữ liệu sở thích được lấy mẫu sử dụng πSFT, chúng tôi khởi tạo πref = πSFT khi có sẵn. Tuy nhiên, khi πSFT không có sẵn, chúng tôi khởi tạo πref bằng cách tối đa hóa khả năng của các hoàn thành được ưa thích (x, yw), tức là, πref = arg maxπ Ex,yw∼D[log π(yw|x)]. Quy trình này giúp giảm thiểu sự dịch chuyển phân phối giữa phân phối tham chiếu thực sự không có sẵn, và πref được sử dụng bởi DPO. Chi tiết thêm liên quan đến triển khai và siêu tham số có thể được tìm thấy trong Phụ lục B.

5 Phân tích Lý thuyết của DPO
Trong phần này, chúng tôi đưa ra giải thích thêm về phương pháp DPO, cung cấp nền tảng lý thuyết, và liên hệ các ưu điểm của DPO với các vấn đề của thuật toán actor critic được sử dụng cho RLHF (như PPO [39]).

5.1 Mô hình Ngôn ngữ của Bạn Bí mật là một Mô hình Phần thưởng
DPO có thể bỏ qua cả việc phù hợp một phần thưởng rõ ràng và thực hiện RL để học chính sách sử dụng một mục tiêu maximum likelihood duy nhất. Lưu ý mục tiêu tối ưu hóa Eq. 5 tương đương với một mô hình Bradley-Terry với một tham số hóa phần thưởng r*(x, y) = β log(π*θ(y|x)/πref(y|x)) và chúng ta tối ưu hóa mô hình tham số πθ của chúng ta, tương đương với tối ưu hóa mô hình phần thưởng trong Eq. 2 dưới sự thay đổi biến số. Trong phần này chúng tôi sẽ xây dựng lý thuyết đằng sau tái tham số hóa này, chỉ ra rằng nó không ràng buộc lớp các mô hình phần thưởng đã học, và cho phép khôi phục chính xác chính sách tối ưu. Chúng tôi bắt đầu bằng cách định nghĩa một quan hệ tương đương giữa các hàm phần thưởng.

Định nghĩa 1. Chúng ta nói rằng hai hàm phần thưởng r(x, y) và r'(x, y) tương đương khi và chỉ khi r(x, y) - r'(x, y) = f(x) cho một số hàm f.

Dễ thấy rằng đây thực sự là một quan hệ tương đương, phân chia tập hợp các hàm phần thưởng thành các lớp. Chúng ta có thể phát biểu hai bổ đề sau:

Bổ đề 1. Dưới khung sở thích Plackett-Luce, và đặc biệt là Bradley-Terry, hai hàm phần thưởng từ cùng một lớp tạo ra cùng một phân phối sở thích.

Bổ đề 2. Hai hàm phần thưởng từ cùng một lớp tương đương tạo ra cùng một chính sách tối ưu dưới bài toán RL có ràng buộc.

Các chứng minh đơn giản và chúng tôi hoãn chúng đến Phụ lục A.5. Bổ đề đầu tiên là một vấn đề thiếu đặc tả nổi tiếng với họ mô hình Plackett-Luce [32]. Do thiếu đặc tả này, chúng ta thường phải áp đặt các ràng buộc nhận dạng bổ sung để đạt được bất kỳ đảm bảo nào về các ước tính MLE từ Eq. 2 [4]. Bổ đề thứ hai phát biểu rằng tất cả các hàm phần thưởng từ cùng một lớp mang lại cùng một chính sách tối ưu, do đó cho mục tiêu cuối cùng của chúng ta, chúng ta chỉ quan tâm đến việc khôi phục một hàm phần thưởng tùy ý từ lớp tối ưu. Chúng tôi chứng minh Định lý sau trong Phụ lục A.6:

Định lý 1. Dưới các giả định nhẹ, tất cả các lớp phần thưởng nhất quán với các mô hình Plackett-Luce (và Bradley-Terry đặc biệt) có thể được biểu diễn với tái tham số hóa r(x, y) = β log(π(y|x)/πref(y|x)) cho một số mô hình π(y|x) và một mô hình tham chiếu cho trước πref(y|x).

Phác thảo Chứng minh. Xem xét bất kỳ hàm phần thưởng r(x, y), tạo ra một mô hình tối ưu tương ứng πr(y|x), được xác định bởi Eq. 4. Chúng tôi sẽ chỉ ra rằng một hàm phần thưởng từ lớp tương đương của r có thể được biểu diễn sử dụng tái tham số hóa được đưa ra ở trên. Chúng tôi định nghĩa phép chiếu f như

f(r; πref, β)(x, y) = r(x, y) - β log Σy πref(y|x) exp((1/β)r(x, y))   (8)

Toán tử f đơn giản chuẩn hóa hàm phần thưởng với logarithm của hàm phân vùng của πr. Vì số hạng chuẩn hóa được thêm vào chỉ là một hàm của tiền tố x, f(r; πref, β)(x, y) là một hàm phần thưởng trong lớp tương đương của r(x, y). Cuối cùng, thay thế r với vế phải của Eq. 5 (giữ cho bất kỳ hàm phần thưởng nào), chúng ta có f(r; πref, β)(x, y) = β log(πr(y|x)/πref(y|x)). Tức là, phép chiếu f tạo ra một thành viên của lớp tương đương của r với dạng mong muốn, và chúng ta không mất bất kỳ tính tổng quát nào trong mô hình phần thưởng của mình từ tái tham số hóa được đề xuất.

Chúng ta cũng có thể xem Định lý 1 như xác định chính xác hàm phần thưởng nào trong mỗi lớp tương đương mà tái tham số hóa DPO chọn, tức là, hàm phần thưởng thỏa mãn:

Σy πref(y|x) exp((1/β)r(x, y))|{z}
=π(y|x), sử dụng tái tham số Thm. 1 = 1,   (9)

tức là, π(y|x) là một phân phối hợp lệ (xác suất dương và tổng bằng 1). Tuy nhiên, theo Eq. 4, chúng ta có thể thấy rằng Eq. 9 là hàm phân vùng của chính sách tối ưu được tạo ra bởi hàm phần thưởng r(x, y). Thông tin quan trọng của thuật toán DPO là chúng ta có thể áp đặt các ràng buộc nhất định trên họ mô hình Plackett-Luce thiếu ràng buộc (và Bradley-Terry đặc biệt), sao cho chúng ta bảo tồn lớp các mô hình phần thưởng có thể biểu diễn, nhưng làm cho chính sách tối ưu trong Eq. 4 có thể giải quyết được phân tích cho tất cả các lời nhắc x.

5.2 Sự Không ổn định của Thuật toán Actor-Critic
Chúng ta cũng có thể sử dụng khung của mình để chẩn đoán sự không ổn định với các thuật toán actor-critic tiêu chuẩn được sử dụng cho RLHF, như PPO. Chúng tôi theo pipeline RLHF và tập trung vào bước tinh chỉnh RL được nêu trong Phần 3. Chúng ta có thể kết nối với khung kiểm soát như suy luận [22] cho bài toán RL có ràng buộc được nêu trong 3. Chúng ta giả định một mô hình tham số πθ(y|x) và tối thiểu hóa DKL[πθ(y|x)||π*(y|x)] trong đó π* là chính sách tối ưu từ Eq. 7 được tạo ra bởi hàm phần thưởng rφ(y, x). Với một số đại số, điều này dẫn đến mục tiêu tối ưu hóa:

max πθ Eπθ(y|x)[rφ(x, y) - β log Σy πref(y|x) exp((1/β)rφ(x, y))|{z}
f(rφ,πref,β) - β log(πθ(y|x)/πref(y|x))|{z}
KL]   (10)

Đây là cùng mục tiêu được tối ưu hóa trong các công trình trước [51,40,1,28] sử dụng phần thưởng tương đương DPO cho lớp phần thưởng của rφ. Trong môi trường này, chúng ta có thể giải thích số hạng chuẩn hóa trong f(rφ, πref, β) như hàm giá trị mềm của chính sách tham chiếu πref. Trong khi số hạng này không ảnh hưởng đến nghiệm tối ưu, không có nó, gradient chính sách của mục tiêu có thể có phương sai cao, làm cho việc học không ổn định. Chúng ta có thể chứa số hạng chuẩn hóa sử dụng một hàm giá trị đã học, nhưng điều đó cũng có thể khó tối ưu hóa. Thay vào đó, các công trình trước đã chuẩn hóa phần thưởng sử dụng một đường cơ sở hoàn thành con người, về cơ bản là một ước tính Monte-Carlo mẫu đơn của số hạng chuẩn hóa. Ngược lại, tái tham số hóa DPO mang lại một hàm phần thưởng không yêu cầu bất kỳ đường cơ sở nào.

--- TRANG 5 ---
[Biểu đồ và hình ảnh về hiệu suất DPO so với các phương pháp khác]

6 Thí nghiệm
Trong phần này, chúng tôi đánh giá thực nghiệm khả năng của DPO trong việc huấn luyện chính sách trực tiếp từ sở thích. Đầu tiên, trong một môi trường tạo văn bản được kiểm soát tốt, chúng tôi hỏi: DPO đánh đổi hiệu quả như thế nào giữa việc tối đa hóa phần thưởng và tối thiểu hóa KL-divergence với chính sách tham chiếu, so với các thuật toán học sở thích phổ biến như PPO? Tiếp theo, chúng tôi đánh giá hiệu suất của DPO trên các mô hình lớn hơn và các nhiệm vụ RLHF khó khăn hơn, bao gồm tóm tắt và đối thoại. Chúng tôi thấy rằng với việc điều chỉnh siêu tham số gần như không có, DPO có xu hướng hoạt động tốt như hoặc tốt hơn các đường cơ sở mạnh như RLHF với PPO cũng như trả về tốt nhất của N quỹ đạo được lấy mẫu dưới một hàm phần thưởng đã học. Trước khi trình bày những kết quả này, chúng tôi mô tả thiết lập thí nghiệm; chi tiết bổ sung trong Phụ lục C.

Nhiệm vụ. Các thí nghiệm của chúng tôi khám phá ba nhiệm vụ tạo văn bản mở khác nhau. Đối với tất cả các thí nghiệm, các thuật toán học một chính sách từ một tập dữ liệu sở thích D = {x(i), y(i)w, y(i)l}Ni=1. Trong tạo cảm xúc có kiểm soát, x là một tiền tố của một đánh giá phim từ tập dữ liệu IMDb [24], và chính sách phải tạo ra yw với cảm xúc tích cực. Để thực hiện đánh giá có kiểm soát, cho thí nghiệm này chúng tôi tạo ra các cặp sở thích trên các kết quả tạo ra sử dụng một bộ phân loại cảm xúc được tiền huấn luyện, trong đó p(positive |x, yw) > p(positive |x, yl). Đối với SFT, chúng tôi tinh chỉnh GPT-2-large cho đến khi hội tụ trên các đánh giá từ phần train của tập dữ liệu IMDB (chi tiết thêm trong App C.1). Trong tóm tắt, x là một bài đăng diễn đàn từ Reddit; chính sách phải tạo ra một bản tóm tắt y của những điểm chính trong bài đăng. Theo công trình trước, chúng tôi sử dụng tập dữ liệu tóm tắt Reddit TL;DR [43] cùng với sở thích con người được thu thập bởi Stiennon et al.. Chúng tôi sử dụng một mô hình SFT được tinh chỉnh trên bản tóm tắt bài đăng diễn đàn được viết bởi con người² với khung TRLX [44] cho RLHF. Tập dữ liệu sở thích con người được thu thập bởi Stiennon et al. trên các mẫu từ một mô hình SFT khác nhau, nhưng được huấn luyện tương tự. Cuối cùng, trong đối thoại một lượt, x là một truy vấn của con người, có thể là bất cứ thứ gì từ một câu hỏi về thiên thể học đến một yêu cầu lời khuyên về mối quan hệ. Một chính sách phải tạo ra một phản hồi hấp dẫn và hữu ích y cho truy vấn của người dùng; chúng tôi sử dụng tập dữ liệu đối thoại Anthropic Helpful and Harmless [1], chứa 170k đối thoại giữa con người và một trợ lý tự động. Mỗi bản ghi kết thúc với một cặp phản hồi được tạo ra bởi một mô hình ngôn ngữ lớn (mặc dù không rõ) cùng với một nhãn sở thích biểu thị phản hồi được con người ưa thích. Trong môi trường này, không có mô hình SFT được tiền huấn luyện có sẵn; do đó chúng tôi tinh chỉnh một mô hình ngôn ngữ có sẵn chỉ trên các hoàn thành được ưa thích để tạo thành mô hình SFT.

Đánh giá. Các thí nghiệm của chúng tôi sử dụng hai cách tiếp cận khác nhau để đánh giá. Để phân tích hiệu quả của mỗi thuật toán trong việc tối ưu hóa mục tiêu tối đa hóa phần thưởng có ràng buộc, trong môi trường tạo cảm xúc có kiểm soát chúng tôi đánh giá mỗi thuật toán bằng biên của phần thưởng đạt được và KL-divergence từ chính sách tham chiếu; biên này có thể tính toán được vì chúng ta có quyền truy cập vào hàm phần thưởng thực tế (một bộ phân loại cảm xúc). Tuy nhiên, trong thế giới thực, hàm phần thưởng thực tế không được biết; do đó, chúng tôi đánh giá các thuật toán với tỷ lệ thắng của chúng so với một chính sách đường cơ sở, sử dụng GPT-4 như một proxy cho đánh giá con người về chất lượng tóm tắt và tính hữu ích của phản hồi trong các môi trường tóm tắt và đối thoại một lượt, tương ứng. Đối với tóm tắt, chúng tôi sử dụng bản tóm tắt tham chiếu trong tập test làm đường cơ sở; đối với đối thoại, chúng tôi sử dụng phản hồi được ưa thích trong

²https://huggingface.co/CarperAI/openai_summarize_tldr_sft

--- TRANG 6 ---
tập dữ liệu test làm đường cơ sở. Trong khi các nghiên cứu hiện tại gợi ý LM có thể là những đánh giá viên tự động tốt hơn các metric hiện tại [10], chúng tôi tiến hành một nghiên cứu con người để chứng minh việc sử dụng GPT-4 cho đánh giá trong Sec. 6.4. Chúng tôi thấy các phán đoán của GPT-4 tương quan mạnh với con người, với sự đồng thuận của con người với GPT-4 thường tương tự hoặc cao hơn sự đồng thuận giữa các chú thích viên con người.

Phương pháp. Ngoài DPO, chúng tôi đánh giá một số cách tiếp cận hiện tại để huấn luyện mô hình ngôn ngữ tuân thủ sở thích con người. Đơn giản nhất, chúng tôi khám phá prompting zero-shot với GPT-J [45] trong nhiệm vụ tóm tắt và prompting 2-shot với Pythia-2.8B [3] trong nhiệm vụ đối thoại. Ngoài ra, chúng tôi đánh giá mô hình SFT cũng như Preferred-FT, là một mô hình được tinh chỉnh với học có giám sát trên hoàn thành được chọn yw từ mô hình SFT (trong cảm xúc có kiểm soát và tóm tắt) hoặc một LM chung (trong đối thoại một lượt). Một phương pháp giả-có-giám-sát khác là Unlikelihood [46], đơn giản tối ưu hóa chính sách để tối đa hóa xác suất được gán cho yw và tối thiểu hóa xác suất được gán cho yl; chúng tôi sử dụng một hệ số tùy chọn α ∈ [0,1] trên số hạng 'unlikelihood'. Chúng tôi cũng xem xét PPO [39] sử dụng một hàm phần thưởng được học từ dữ liệu sở thích và PPO-GT, là một oracle học từ hàm phần thưởng thực tế có sẵn trong môi trường cảm xúc có kiểm soát. Trong các thí nghiệm cảm xúc của chúng tôi, chúng tôi sử dụng hai triển khai của PPO-GT, một phiên bản có sẵn [44] cũng như một phiên bản được sửa đổi chuẩn hóa phần thưởng và điều chỉnh thêm siêu tham số để cải thiện hiệu suất (chúng tôi cũng sử dụng những sửa đổi này khi chạy PPO 'bình thường' với phần thưởng đã học). Cuối cùng, chúng tôi xem xét đường cơ sở Best of N, lấy mẫu N phản hồi từ mô hình SFT (hoặc Preferred-FT trong đối thoại) và trả về phản hồi có điểm cao nhất theo một hàm phần thưởng được học từ tập dữ liệu sở thích. Phương pháp hiệu suất cao này tách chất lượng của mô hình phần thưởng khỏi tối ưu hóa PPO, nhưng không thực tế về mặt tính toán ngay cả với N vừa phải vì nó yêu cầu lấy mẫu N hoàn thành cho mọi truy vấn tại thời điểm test.

6.1 DPO có thể tối ưu hóa mục tiêu RLHF tốt như thế nào?
Mục tiêu tối đa hóa phần thưởng có ràng buộc KL được sử dụng trong các thuật toán RLHF điển hình cân bằng khai thác phần thưởng trong khi hạn chế chính sách không lệch xa khỏi chính sách tham chiếu. Do đó, khi so sánh các thuật toán, chúng ta phải tính đến cả phần thưởng đạt được cũng như sự khác biệt KL; đạt được phần thưởng cao hơn một chút nhưng với KL cao hơn nhiều không nhất thiết là mong muốn. Hình 2 cho thấy biên phần thưởng-KL cho các thuật toán khác nhau trong môi trường cảm xúc. Chúng tôi thực hiện nhiều lần chạy huấn luyện cho mỗi thuật toán, sử dụng một siêu tham số khác nhau cho tính bảo thủ chính sách trong mỗi lần chạy (KL mục tiêu ∈ {3,6,9,12} cho PPO, β ∈ {0.05,0.1,1,5}, α ∈ {0.05,0.1,0.5,1} cho unlikelihood, random seeds cho preferred-FT). Quét này bao gồm 22 lần chạy tổng cộng. Sau mỗi 100 bước huấn luyện cho đến khi hội tụ, chúng tôi đánh giá mỗi chính sách trên một tập các lời nhắc test, tính toán phần thưởng trung bình dưới hàm phần thưởng thực cũng như KL mức độ chuỗi trung bình³ với chính sách tham chiếu KL(π||πref). Chúng tôi thấy rằng DPO tạo ra biên hiệu quả nhất, đạt được phần thưởng cao nhất trong khi vẫn đạt được KL thấp. Kết quả này đặc biệt đáng chú ý vì nhiều lý do. Đầu tiên, DPO và PPO tối ưu hóa cùng mục tiêu, nhưng DPO hiệu quả hơn đáng kể; sự đánh đổi phần thưởng/KL của DPO thống trị PPO một cách nghiêm ngặt. Thứ hai, DPO đạt được biên tốt hơn PPO, ngay cả khi PPO có thể truy cập phần thưởng thực tế (PPO-GT).

6.2 DPO có thể mở rộng cho các tập dữ liệu sở thích thực không?
Tiếp theo, chúng tôi đánh giá hiệu suất tinh chỉnh của DPO trên tóm tắt và đối thoại một lượt. Đối với tóm tắt, các metric đánh giá tự động như ROUGE có thể tương quan kém với sở thích con người [40], và công trình trước đã thấy rằng tinh chỉnh LM sử dụng PPO trên sở thích con người để cung cấp bản tóm tắt hiệu quả hơn. Chúng tôi đánh giá các phương pháp khác nhau bằng cách lấy mẫu hoàn thành trên phần test của tập dữ liệu tóm tắt TL;DR, và tính toán tỷ lệ thắng trung bình so với hoàn thành tham chiếu trong tập test. Các hoàn thành cho tất cả các phương pháp được lấy mẫu ở nhiệt độ từ 0.0 đến 1.0, và tỷ lệ thắng được hiển thị trong Hình 2 (phải). DPO, PPO và Preferred-FT đều tinh chỉnh cùng mô hình SFT GPT-J⁴. Chúng tôi thấy rằng DPO có tỷ lệ thắng khoảng 61% ở nhiệt độ 0.0, vượt trội hiệu suất của PPO ở 57% ở nhiệt độ lấy mẫu tối ưu của nó là 0.0. DPO cũng đạt được tỷ lệ thắng tối đa cao hơn so với đường cơ sở best of N. Chúng tôi lưu ý rằng chúng tôi không điều chỉnh siêu tham số β của DPO một cách có ý nghĩa, vì vậy những kết quả này có thể đánh giá thấp tiềm năng của DPO. Hơn nữa, chúng tôi thấy DPO mạnh mẽ hơn nhiều đối với nhiệt độ lấy mẫu so với PPO, hiệu suất của PPO có thể giảm xuống mức của mô hình GPT-J cơ sở ở nhiệt độ cao. Preferred-FT không cải thiện đáng kể so với mô hình SFT. Chúng tôi cũng so sánh DPO và PPO trực tiếp trong đánh giá con người trong Phần 6.4, nơi các mẫu DPO ở nhiệt độ 0.25 được ưa thích 58% lần so với các mẫu PPO ở nhiệt độ 0.

Về đối thoại một lượt, chúng tôi đánh giá các phương pháp khác nhau trên tập con của phần test của tập dữ liệu Anthropic HH [1] với một bước tương tác human-assistant. Đánh giá GPT-4 sử dụng các hoàn thành được ưa thích trên test làm tham chiếu để tính toán tỷ lệ thắng cho các phương pháp khác nhau. Vì không có mô hình SFT tiêu chuẩn cho nhiệm vụ này, chúng tôi bắt đầu với một Pythia-2.8B được tiền huấn luyện, sử dụng Preferred-FT để huấn luyện một mô hình tham chiếu trên các hoàn thành được chọn sao cho các hoàn thành nằm trong phân phối của mô hình, và sau đó huấn luyện sử dụng DPO. Chúng tôi cũng so sánh với best of 128 hoàn thành Preferred-FT (chúng tôi thấy đường cơ sở Best of N đạt ngưỡng ở 128 hoàn thành cho nhiệm vụ này; xem Hình 4 Phụ lục) và một phiên bản 2-shot prompted của mô hình cơ sở Pythia-2.8B, thấy DPO hoạt động tốt như hoặc tốt hơn cho các nhiệt độ hiệu suất tốt nhất cho mỗi phương pháp. Chúng tôi cũng đánh giá một mô hình RLHF được huấn luyện với PPO trên tập dữ liệu Anthropic HH⁵ từ một nguồn nổi tiếng⁶, nhưng không thể tìm thấy một lời nhắc hoặc nhiệt độ lấy mẫu mang lại hiệu suất tốt hơn mô hình cơ sở Pythia-2.8B. Dựa trên kết quả của chúng tôi từ TL;DR và thực tế rằng cả hai phương pháp đều tối ưu hóa cùng hàm phần thưởng, chúng tôi coi Best of 128 như một proxy thô cho hiệu suất mức PPO. Nhìn chung, DPO là phương pháp hiệu quả tính toán duy nhất cải thiện so với các hoàn thành được ưa thích trong tập dữ liệu Anthropic HH, và cung cấp hiệu suất tương tự hoặc tốt hơn đường cơ sở Best of 128 đòi hỏi tính toán cao. Cuối cùng, Hình 3 cho thấy DPO hội tụ đến hiệu suất tốt nhất của nó tương đối nhanh chóng.

6.3 Khái quát hóa cho phân phối đầu vào mới

[Bảng kết quả so sánh hiệu suất DPO và PPO trên CNN/DailyMail]

Để so sánh thêm hiệu suất của PPO và DPO dưới sự dịch chuyển phân phối, chúng tôi đánh giá các chính sách PPO và DPO từ thí nghiệm tóm tắt Reddit TL;DR của chúng tôi trên một phân phối khác, các bài báo tin tức trong phần test của tập dữ liệu CNN/DailyMail [26], sử dụng các nhiệt độ lấy mẫu tốt nhất từ TL;DR (0 và 0.25). Kết quả được trình bày trong Bảng 1. Chúng tôi tính toán tỷ lệ thắng GPT-4 so với bản tóm tắt thực tế trong các tập dữ liệu, sử dụng cùng lời nhắc GPT-4 (C) mà chúng tôi đã sử dụng cho Reddit TL;DR, nhưng thay thế từ "forum post" bằng "news article". Đối với phân phối mới này, DPO tiếp tục vượt trội chính sách PPO với một khoảng cách đáng kể. Thí nghiệm này cung cấp bằng chứng ban đầu rằng các chính sách DPO có thể khái quát hóa tương tự tốt với các chính sách PPO, ngay cả khi DPO không sử dụng các lời nhắc Reddit TL;DR không nhãn bổ sung mà PPO sử dụng.

6.4 Xác thực phán đoán GPT-4 với phán đoán con người
Chúng tôi tiến hành một nghiên cứu con người để xác minh độ tin cậy của phán đoán GPT-4, sử dụng kết quả của thí nghiệm tóm tắt TL;DR và hai lời nhắc GPT-4 khác nhau. Lời nhắc GPT-4 (S) (đơn giản) đơn giản hỏi bản tóm tắt nào tóm tắt tốt hơn thông tin quan trọng trong bài đăng. Lời nhắc GPT-4 (C) (ngắn gọn) cũng hỏi bản tóm tắt nào ngắn gọn hơn; chúng tôi đánh giá lời nhắc này vì chúng tôi thấy rằng GPT-4 ưa thích bản tóm tắt dài hơn, lặp lại hơn so với con người với lời nhắc GPT-4 (S). Xem Phụ lục C.2 cho các lời nhắc hoàn chỉnh. Chúng tôi thực hiện ba so sánh, sử dụng cao nhất (DPO, temp. 0.25), thấp nhất (PPO, temp. 1.0), và một phương pháp hiệu suất trung bình (SFT, temp. 0.25) với mục đích bao phủ một sự đa dạng của chất lượng mẫu; cả ba phương pháp đều được so sánh với PPO được lấy mẫu tham lam (nhiệt độ hiệu suất tốt nhất của nó). Chúng tôi thấy rằng với cả hai lời nhắc, GPT-4 có xu hướng đồng ý với con người khoảng thường xuyên như con người đồng ý với nhau, gợi ý rằng GPT-4 là một proxy hợp lý cho đánh giá con người (do số lượng đánh giá viên con người hạn chế, chúng tôi chỉ thu thập nhiều phán đoán con người cho các so sánh DPO và PPO-1). Nhìn chung, lời nhắc GPT-4 (C) nói chung cung cấp tỷ lệ thắng đại diện hơn cho con người; do đó chúng tôi sử dụng lời nhắc này cho các kết quả chính trong Phần 6.2. Để biết chi tiết bổ sung về nghiên cứu con người, bao gồm giao diện web được trình bày cho các đánh giá viên và danh sách các tình nguyện viên con người, xem Phụ lục D.3.

7 Thảo luận
Học từ sở thích là một khung mạnh mẽ, có thể mở rộng để huấn luyện các mô hình ngôn ngữ có khả năng, phù hợp. Chúng tôi đã giới thiệu DPO, một mô hình huấn luyện đơn giản để huấn luyện mô hình ngôn ngữ từ sở thích mà không cần học tăng cường. Thay vì ép buộc vấn đề học sở thích vào một môi trường RL tiêu chuẩn để sử dụng các thuật toán RL có sẵn, DPO xác định một ánh xạ giữa các chính sách mô hình ngôn ngữ và các hàm phần thưởng cho phép huấn luyện một mô hình ngôn ngữ để thỏa mãn sở thích con người trực tiếp, với một hàm mất mát entropy chéo đơn giản, mà không cần học tăng cường hoặc mất tính tổng quát. Với việc điều chỉnh siêu tham số hầu như không có, DPO hoạt động tương tự hoặc tốt hơn các thuật toán RLHF hiện tại, bao gồm những thuật toán dựa trên PPO; DPO do đó giảm đáng kể rào cản để huấn luyện nhiều mô hình ngôn ngữ hơn từ sở thích con người.

Hạn chế & Công trình Tương lai. Kết quả của chúng tôi đặt ra một số câu hỏi quan trọng cho công trình tương lai. Chính sách DPO khái quát hóa ngoài phân phối như thế nào, so với việc học từ một hàm phần thưởng rõ ràng? Kết quả ban đầu của chúng tôi gợi ý rằng các chính sách DPO có thể khái quát hóa tương tự với các mô hình dựa trên PPO, nhưng cần nghiên cứu toàn diện hơn. Ví dụ, liệu huấn luyện với tự gắn nhãn từ chính sách DPO có thể tận dụng hiệu quả các lời nhắc không nhãn tương tự không? Mặt khác, tối ưu hóa quá mức phần thưởng biểu hiện như thế nào trong môi trường tối ưu hóa sở thích trực tiếp, và liệu sự giảm nhẹ hiệu suất trong Hình 3-phải có phải là một trường hợp của nó không? Ngoài ra, trong khi chúng tôi đánh giá các mô hình lên đến 6B tham số, khám phá việc mở rộng DPO cho các mô hình tiên tiến hơn với quy mô lớn hơn nhiều bậc là một hướng thú vị cho công trình tương lai. Về đánh giá, chúng tôi thấy rằng tỷ lệ thắng được tính bởi GPT-4 bị ảnh hưởng bởi lời nhắc; công trình tương lai có thể nghiên cứu cách tốt nhất để gợi ra phán đoán chất lượng cao từ các hệ thống tự động. Cuối cùng, nhiều ứng dụng có thể của DPO tồn tại ngoài việc huấn luyện mô hình ngôn ngữ từ sở thích con người, bao gồm huấn luyện các mô hình tạo ra trong các modality khác.

Lời cảm ơn
EM biết ơn sự tài trợ từ Knight-Hennessy Graduate Fellowship. CF và CM là CIFAR Fellows. Công trình này được hỗ trợ một phần bởi Stanford Accelerator for Learning (SAL) và chương trình tài trợ khởi nghiệp Stanford Institute for Human-Centered Artificial Intelligence (HAI) Generative AI for the Future of Learning. Stanford Center for Research on Foundation Models (CRFM) cung cấp một phần tài nguyên tính toán được sử dụng cho các thí nghiệm trong công trình này. Công trình này được hỗ trợ một phần bởi tài trợ ONR N00014-20-1-2675.

--- TRANG 7 đến TRANG 27 ---
[Phần còn lại của tài liệu bao gồm các tài liệu tham khảo, phụ lục với các chứng minh toán học chi tiết, chi tiết triển khai, kết quả thí nghiệm bổ sung, và nghiên cứu con người]
