# 2312.14878.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2312.14878.pdf
# File size: 801899 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Pangu-Agent: A Fine-Tunable Generalist Agent
with Structured Reasoning
Filippos Christianos1∗Georgios Papoudakis1∗Matthieu Zimmer1Thomas Coste1
Zhihao Wu1Jingxuan Chen1Khyati Khandelwal1James Doran1Xidong Feng2†
Jiacheng Liu2†Zheng Xiong3†Yicheng Luo2†Jianye Hao1
Kun Shao1‡Haitham Bou-Ammar1,2‡Jun Wang2‡
1Huawei Noah’s Ark Lab,2University College London,3University of Oxford
Abstract
A key method for creating Artificial Intelligence (AI) agents is Reinforcement
Learning (RL). However, constructing a standalone RL policy that maps perception
to action directly encounters severe problems, chief among them being its lack of
generality across multiple tasks and the need for a large amount of training data.
The leading cause is that it cannot effectively integrate prior information into the
perception-action cycle when devising the policy. Large language models (LLMs)
emerged as a fundamental way to incorporate cross-domain knowledge into AI
agents but lack crucial learning and adaptation toward specific decision problems.
This paper presents a general framework model for integrating and learning struc-
tured reasoning into AI agents’ policies. Our methodology is motivated by the
modularity found in the human brain. The framework utilises the construction
of intrinsic and extrinsic functions to add previous understandings of reasoning
structures. It also provides the adaptive ability to learn models inside every module
or function, consistent with the modular structure of cognitive processes. We de-
scribe the framework in-depth and compare it with other AI pipelines and existing
frameworks. The paper explores practical applications, covering experiments that
show the effectiveness of our method. Our results indicate that AI agents perform
and adapt far better when organised reasoning and prior knowledge are embedded.
This opens the door to more resilient and general AI agent systems.
1 Introduction
Since the inception of AI, developing generalist agents capable of solving and adapting to increasingly
complex tasks has been an overarching goal. Such agents can be defined as: "... entities that
appropriately act in uncertain environments, where appropriate actions increase the probability of
success and where success is the achievement of behavioural subgoals that support the system’s
ultimate goal." (J. S. Albus [ 1]).Hence, AI agents must be equipped with sensors to perceive
environments and methods to act upon them to achieve their goals. This definition can capture
∗Contributing equally to this work.
†Work done during an internship at Huawei.
‡Corresponding authors: {shaokun2, haitham.ammar}@huawei.com, jun.wang@cs.ucl.ac.uk.arXiv:2312.14878v1  [cs.AI]  22 Dec 2023

--- PAGE 2 ---
© 10 Creative 2023  |  Confidential  |  10creative.co.uk
System PromptState of the 
environment…Intrinsic 
FunctionsExtrinsic 
FunctionsState of the 
environment
MemoryOrder, Arrange, Compute
Environment
Extrinsic 
Functions
Environment
Environment 
Rewards
…Figure 1: Pictorial depiction of the Pangu-Agent pipeline with RL. Starting from the system prompt
and initial state, our agent executes actions in the environment and observes the next state and the
reward. The trajectories generated can be used for finetuning the LLM.
diverse autonomous systems, ranging from simple software programs running on the web to complex
embodied entities interacting in unknown and stochastic environments.
AI agents, vital to many intelligent applications, are often trained through RL, which develops
decision-making skills through environmental interactions. Both model-based and model-free deep
RL methodologies have shown significant achievements, such as AlphaZero’s board game profi-
ciency [ 2], improved sorting and multiplication algorithms [ 3,4], drone racing championships [ 5]
and plasma control in fusion reactors [ 6]. These successes involved a standard RL pipeline, where
agents learn what we refer to as an extrinsic function – a policy that directly interacts with the outside
world, i.e., responding to environmental stimuli to maximise a reward signal. This function, often a
parameterised neural network, generates actions based on environmental observations.
The classic RL approach, using a single mapping function to define a policy π, often proves inadequate
in complex environments, contradicting the aim of generalist agents to interact, adapt, and learn
across multiple stochastic environments. This sparked studies on improved learnability via meta-RL
[7,8,9,10], intrinsic motivation-based exploration [ 11,12,13], auxiliary tasks [ 14,15,16], inverse
RL [17,18,19], Bayesian priors [ 20,21,22], and hierarchical policies [ 23,24,25,26]. However, RL-
introduced priors are often task-specific and require extensive engineering and domain expertise. To
allow for more general priors, recent studies have shifted towards integrating large language models
(LLMs) into agent frameworks, as seen in AutoGen [ 27], AutoGPT [ 28], and AgentVerse [ 29].
Research on LLM-based AI agents not only uses LLMs as foundational priors but also incorporates
tools [ 30,31] and multi-agent communication [ 29] to build generalist agents. Existing frameworks,
outlined in Table 1 and discussed in Appendix F, assume fixed reasoning structures and typically lack
fine-tuning for new behaviours and skills, limiting agents to the pre-trained LLM’s capabilities and
posing a risk in out-of-distribution tasks.
In this paper, we propose the Pangu-Agent framework (also see Fig. 1) as a first step towards
remedying the above two problems. Our work distinguishes itself from prior frameworks in two
critical ways: i)we formalise the internal thinking process of the agent as a form of structured
reasoning andii)we show how agents can be fine-tuned via supervised learning and RL. To introduce
structured reasoning, we assume a family of intrinsic functions µ(·)that act on and transform
the agent’s internal memory. Introducing these intrinsic functions can reformulate the typical RL
objective to one that supports multiple ‘thinking’ steps. Therefore, the typical RL objective which
aims to find a policy πconditioned on the history of observations ⃗othat maximises the returns R, i.e.
max π(·)R(π(·|⃗o))can be rewritten using a nested set (see Fig. 2) of intrinsic functions ⃗µ(·)as:
Standard RLz}| {
max
π(·)R(π(·|⃗o))→Pangu Opt.z }| {
max
π(·),⃗µ(·)R(π(·|⃗o,⃗µ(⃗o))). (1)
These nested intrinsic functions have been absent in RL formulations, whereby standard RL focuses on
directly learning policies that output actions from perceptions. While it is customary to parameterise
policies by deep network architectures, we contend that the absence of inherent reasoning structures
in standard RL pipelines can become a significant bottleneck when scaling agents across tasks via
foundational model policies precisely because gradients fail to offer enough supervision for all deep
2

--- PAGE 3 ---
Agent Tools Memory Multi-Agent Customisable Control-Flow SFT RLFT
Transformers Agents[32] ✓ × × × × ×
LangChain[33] ✓ ✓ × × × ×
AutoGPT[28] ✓ ✓ × × × ×
OpenAgents[34] ✓ ✓ × × × ×
XAgent[35] ✓ ✓ ✓ × × ×
MetaGPT[36] ✓ ✓ ✓ × × ×
Camel[37] ✓ ✓ ✓ × × ×
AgentVerse[29] ✓ ✓ ✓ × × ×
AGENTS[38] ✓ ✓ ✓ ✓ × ×
AutoGen[27] ✓ ✓ ✓ ✓ × ×
Pangu-Agent ✓ ✓ ✓ ✓ ✓ ✓
Table 1: Comparison of our proposed agent versus recent language-based agents. The overview of
related methods can be found in Appendix F.
© 10 Creative 2023  |  Confidential  |  10creative.co.uk
LLM
ThoughtsReflection Intrinsic Function
Interpreter
Tool FeedbackTool-Use Intrinsic Function
Tool Feedback
ThoughtsComposite Intrinsic Function
Interpreter
 LLM
Figure 2: Visualisation of three intrinsic functions demonstrating our formulation’s importance
in improving our agent’s modularity and flexibility. The intrinsic functions can be re-defined and
re-configured by users, e.g., µ1(·)taking an LLM as input to produce thoughts or µ2(·)utilising tools
to help improve reasoning. We also support nesting those intrinsic functions to build more general
modules for complex and challenging decision-making tasks.
layers with only (sparse) rewards as guidance. Our framework demonstrates how structured reasoning
can empower RL to surmount these challenges, leveraging large-scale foundational models to provide
priors and enable generalisation across broad domains. In short, we summarise our contributions as:
1.We demonstrate the importance of structured reasoning in an agent framework that is general
and versatile enough to effectively cover a diverse spectrum of existing agent framework
in the literature [e.g. 39,40,41,42]. As a meta-agent framework, it can be adjusted or
fine-tuned utilising the sequence of intrinsic function calls or delegating decision-making to
the underlying LLM. As we show in Appendix D, users of the framework can easily expand
the capabilities of the agent and combine or reuse many of the methods that have already
been implemented.
2.We conduct a comprehensive evaluation of structured reasoning by evaluating first-order (e.g.
direct answering, chain-of-thought prompting [ 43]) and composite methods (e.g. ReAct [ 40],
Self-Consistency [ 39], SwiftSage [ 41]), on seven LLMs and six distinct domains. This
evaluation can be used to inform researchers on how to initialise their agents and how to
collect data for the fine-tuning steps.
3.Our study demonstrates the impact of Supervised Fine-Tuning (SFT) and RL Fine-Tuning
(RLFT) of the framework. Through structured reasoning, we successfully implement a
rejection-sampling-based SFT pipeline, notably improving an LLM’s performance in the
ALFWorld domain [ 44], with success rates in held-out tasks increasing from 27% to 82%.
Although the benefits of SFT plateau, we show that further enhancements are achievable
through RL, increasing success rates to 88% and even from 28% to 91% in the BabyAI
task [ 45]. Moreover, our cross-domain experiments reveal the ability of a single LLM, trained
via the RL pipeline, to simultaneously achieve high performance in both the ALFWorld
(82%) and BabyAI (58.7% average in 18 tasks) domains. These findings highlight the
potential of structured reasoning to advance LLM-based agent training significantly.
3

--- PAGE 4 ---
2 The Pangu-Agent Formulation
In this section, we continue our development of incorporating intrinsic functions into the RL pipeline
to allow the definition of flexible reasoning processes. We emphasise the need for those to be defined,
learned, and used separately from the extrinsic functions, whereby users can re-define any arbitrarily
nesting deemed helpful for their tasks. We can rewrite the Pangu-Agent’s optimisation problem from
Eq. (1) in a more detailed form as:
max
π(·)E
π(·)
X
t≥0γtrt
→Pangu Opt.z }| {
max
π(·),⃗µ(·)E
π(·),⃗µ(·)
X
t≥0γtrt
,with⃗µ(·)being a set of intrinsic functions,
(2)
and where rtis the reward at the time step tthat depends on the environmental observation otand
action at. Furthermore, γ∈[0,1)is a discount factor that specifies the degree to which rewards
are discounted over time. The extrinsic functions still serve as actuators interacting with the outside
world, whereas those additionally layered intrinsic functions are designed to encapsulate any internal
reasoning process deemed beneficial by the system’s architect.
Recent advancements from LLMs have demonstrated impressive successes across a broad spectrum
of general reasoning and decision-making tasks to induce comprehensive prior knowledge. While
fine-tunable, those language priors allow us to go beyond narrow task specifications and heavy
engineering demands, and they also enable new paradigms and architectures, including, for example,
memory management and tool usage, that can further aid in solving complex and challenging domains.
Central to this design is the intrinsic function’s role as a versatile encoder, whereby it can, for instance,
facilitate the CoT mechanism [ 43,46], utilising LLMs to generate and process internal thoughts, and
it can serve as a dynamic memory or retrieval system, adept at accessing and utilising previously
gathered information. In principle, we can introduce such intrinsic functions during any step of the RL
pipeline. However, in this first version of our Pangu-Agent, we define our initial policies as ( extrinsic )
functions of pre-trained LLMs, which can be later trained further since those action-selection rules
are a central core component of the agents’ interaction with environments. Faced with a task ℓand
equipped with a memory MEMℓ
t, our agent decides to choose an action atas follows:
πPangu-Agent 
aℓ
t|oℓ
1:t,MEMℓ
t
≡ Fℓ 
· ∼LLM 
ϕℓ 
oℓ
1:t,MEMℓ
t
. (3)
The above equation shows that Pangu-Agent chooses an action in a three-step nested process. First, it
extracts relevant prompts for task ℓusing the observations so far and the current memory, i.e., the
first step in the nesting ϕℓ(oℓ
1:t,MEMℓ
t)withϕℓ(·)denoting a task-relevant prompt generator. The gen-
erated prompt is passed to an LLM from which a response is sampled, i.e. · ∼LLM(ϕℓ(oℓ
1:t,MEMℓ
t)).
Finally, the third step parses this response to enable compatible action execution in the environ-
ment via a task-specific action parsing functions Fℓ(·), leading us to the Pangu-Agent’s policy
πPangu-Agent (aℓ
t|oℓ
1:t,MEMℓ
t)described in Eq. (3).
Any entity that can learn and adapt must manage and form memories depending on past experiences
and new environmental observations. Such memory components allow agents to draw upon past
events to frame their understanding within the present context and can even aid in predicting and
making sense of potential future events. To offer such flexibility, we do not restrict our agent to a
specific form of memory but provide it with the ability to transform the internal state of the memory
using a set of intrinsic functions . Those (parameterisable) functions allow specific operations, e.g.,
using tools, planning, reflecting on past experience, communicating with other agents, etc. - see
Section 3 for a detailed exposition of such functions. Having decided on a nesting of those intrinsic
functions, the agent’s memory evolves to the next time step:
MEMℓ
t=µℓ
k 
oℓ
1:t,aℓ
1:t−1, µℓ
k−1 
··· 
µℓ
1 
oℓ
1:t,aℓ
1:t−1,MEMℓ
t−1
,
with each µℓ
kbeing an intrinsic function. Therefore, instead of optimising the standard RL objective,
Pangu-Agent attempts to find policies of the form presented in Eq. (3) to maximise total discounted
4

--- PAGE 5 ---
returns:
max
πPangu-Agent (),µ()EπPangu-Agent (·)
X
t≥0γtrℓ
t

s.t.πPangu-Agent 
aℓ
t|oℓ
1:t,MEMℓ
t
=Fℓ 
· ∼LLM 
ϕℓ 
oℓ
1:t,MEMℓ
t
∀t
MEMℓ
t=µℓ
k 
oℓ
1:t,aℓ
1:t−1, µℓ
k−1 
··· 
µℓ
1 
oℓ
1:t,aℓ
1:t−1,MEMℓ
t−1
∀t,
with ⃗µ(·)being the set of intrinsic functions that can be optimised in addition to the policy to
maximise expected returns. This way, Pangu-Agent goes beyond other frameworks to support tunable
and flexible learners at every step of the pipeline – during policy training (or RL fine-tuning of
LLMs) and the nested execution of intrinsic functions ⃗µ(·). The proposed framework, which includes
structure constraints and memory manipulation (also known as test-time computation), overcomes
the autoregressive computational limitations of LLMs [ 47] and enables AI agents to achieve Turing
completeness [48].
3 The Pangu-Agent Framework
3.1 Architecture of Pangu-Agent
In this section, we investigate in depth the specifics of the architecture of our agent framework. As
discussed in Section 2, we first define two families of functions: intrinsic andextrinsic .
Intrinsic Functions – Operating on the Agent’s Memory: Intrinsic functions are a family of
functions that operate on the agent’s memory state. These functions, denoted as µk, take as input
the observation history o1:tand action history a1:t−1provided by the environment and the current
memory state MEMt−1of the agent from the previous time step. They then output a new memory state
that incorporates the transformations applied by the intrinsic function. The intrinsic functions are
crucial in shaping the agent’s internal state and can significantly impact its decision-making process.
By leveraging these functions, the agent can adapt its memory state based on the observation history
and previous knowledge, making more informed and contextually appropriate decisions. Additionally,
the intrinsic functions allow the incorporation of existing knowledge (through prompt engineering)
and human-like structural thinking pipelines into the design of the AI agent. Examples of such
functions include thinking ,planning andreflecting on experience. When asking the agent to think, it
looks at the problem and produces a high-level thought about the current situation. However, creating
agents does not end with the methods and techniques only performed within it. Agents are built
to allow interaction with other agents and external tools. Therefore, two other important intrinsic
functions are the communication andtool-use . First, by enabling pair-wise communication between
agents, Pangu-Agent can simulate realistic interactions and close the sim-to-real gap. Second, tool
use is a generic intrinsic function that allows the utilisation of a wide range of tools, such as code
interpreters and web searching.
Extrinsic Functions – Interacting with the Environment: Extrinsic functions serve the purpose
of eliciting an environment interaction from the language model. Unlike intrinsic functions that
operate on the agent’s memory state, extrinsic functions directly interact with the environment by
generating actions to be executed. These functions take as input the observation and action history
o1:t,a1:t−1provided by the environment and the current memory state MEMℓ
tof the agent. They utilise
the transformed memory state obtained from the intrinsic functions and other contextual information
to make informed decisions about the action that will maximise the agent’s objective.
Composite Methods – SwiftSage [ 41], ReAct [ 40], Least-to-Most [ 42], and more: The flexibility
of our formulation means that many composite methods can be created hierarchically. For instance,
we give examples of the ReAct, Least-to-Most, and SwiftSage frameworks. The ability to create
such complex algorithms directly results from the modular formulation of Pangu-Agent. It does not
mean that it is the only framework capable of these functions, but it is generic enough to support
them. Additionally, it should be noted that the implementations of these composite methods that we
provide in the Pangu-Agent codebase are not always faithful reproductions of the original algorithms,
as these require specific task details. To allow for universal usage of these methods, we have adapted
these methods to the Pangu-Agent framework by removing any task-specific details. In Section 4, we
present results for these methods in different tasks. We also show how simple it is to use and create
these composite methods within the Pangu-Agent framework in Appendix D.
5

--- PAGE 6 ---
Search-Enhanced Planning – BFS, DFS, and MCTS : Inspired by recent search-augmented LLMs
[49,50,51], Pangu-Agent framework integrates three tree-search algorithms – Breadth-first/depth-
first search (BFS/DFS) and Monte-Carlo Tree Search (MCTS), to increase the planning ability for
better LLM’s generation and decision-making ability. Specifically, Our framework leverages LLM as
policy, model, and value functions. By interacting with this LLM-based simulated environment, we
can construct a rollout tree which will be further pruned for better action/generation using tree-search
algorithms. We conduct initial experiments on GSM8K and Game24 leveraging our framework
shown in Appendix E and we refer the readers to [51] for more detailed evaluation results.
Task Interaction and Multi-Agent Systems: Pangu-Agent is compatible with a range of tasks,
for example, ALFWorld [ 44], GSM8K [ 52], HotpotQA [ 53], WebShop [ 54], etc. The interface,
influenced by OpenAI’s Gym, is an open design that facilitates task interactions with any system
where an agent performs actions and receives observations and rewards. Support for multi-agent
environments is even built into the foundation of our framework, ensuring agents can work together
or independently within the same task. In settings with multiple agents, we denote them with the
subscript i. The interaction between one or more agents and an environment in our setting can be
captured by the Partially Observable Stochastic Game [ 55] framework, which collapses to a Partially
Observable Markov Decision Process in case there is only a single agent in the task.
Prompting System: The framework incorporates a template system to generate inputs for the LLMs.
Utilising templates enhances the flexibility of prompt crafting, making various prompting mechanisms
possible. The system also promotes extensibility, with new prompting mechanisms or environments
easily incorporated into the existing framework. Most intrinsic and extrinsic functions contain their
task-specific prompting strategy, which uses the memory to create the prompt that will be inputted to
the LLM.
3.2 Adaptation of LLM Priors & Fine-Tuning
In introducing the architecture above, we aim for generalist agents that adapt based on expert data
and environmental feedback using supervised fine-tuning and RL. We believe the interplay between
structured reasoning from intrinsic functions and extrinsic policies promises general RL solvers
that combine well with large model policies like those induced by LLMs. With our framework, its
function design, and prompting strategies, we can now collect valuable rewarding trajectories from
(open-sourced) pre-trained LLM priors to kickstart the training and fine-tuning process effectively.
We differentiate between two types of fine-tuning methods: i) those that do not interact with an
environment, which we refer to as supervised fine-tuning algorithms, and ii)those that learn by
agent-environment interactions to maximise expected returns that we dub under RL-based fine-tuning
techniques. Fine-tuning requires practical algorithms that update the weights of the LLM regardless
of the type used. It is customary to define loss functions - e.g., predictive likelihood - and to rely on
gradient descent when searching for update directions. However, computing those gradients for each
weight of LLMs is often infeasible without accessing GPUs with vast memory capabilities. Hence, we
support full-rank and low-rank adaptation algorithms to democratise and enable our agent’s broader
usage. This way, users can perform complete weight updates if their computing capabilities allow or
rely otherwise on low-rank adaptation (LoRA) [56].
Fine-Tuning Type I: To support the first type of fine-tuning (i.e., supervised), we allow the
collection of successful trajectories from an environment via the different prompting strategies
introduced before. Of course, we do not restrict our pipeline to data collection but also allow for the
standard setup of introducing external expert datasets in case they are available. Given such a data
setD, we rely on causal language modelling losses - a special case of likelihood optimisation - and
perform full- or low-rank updates to minimise:
L(D,θLLM) =−1
NNX
n=1logp(xn|xi≤n−1,θLLM), (4)
where θLLMare the LLM’s weights to be updated. Equation (4) maximises the likelihood of the
observed token xngiven all the previous ones. Since we are in an agent framework, our loss is only
defined by the tokens generated by the agent and not the ones provided by the environment (or by the
system prompts). Updates of θLLMcan be executed either fully if using full-rank optimisation or by
parameterising them via two low-rank matrices in the LoRA scenario. Here, θLLM =θ0
LLM +BA,
6

--- PAGE 7 ---
where θ0
LLM∈Rd×kare the weights of the pre-trained LLM and B∈Rd×randA∈Rr×kare
tunable parameters of rank r << min{d, k}. Here, we follow LoRA [ 56] in keeping θ0
LLMfixed and
performing gradient updates only for BandA.
Fine-Tuning Type II: Regarding RL fine-tuners, we allow agents to interact with environments, as
shown in Fig. 1. Given a system prompt and access to an environment, the agent applies actions after
which the environment transitions to a new state. This process repeats until the end of an episode, at
which stage our learner receives a (sparse) reward signal. Given those rewards, we execute the PPO
algorithm [57] to improve our agent’s policy to maximise expected returns.
Of course, this setup is standard for deep RL. However, we now elaborate on two crucial differences
to deep RL. First, our agent uses intrinsic functions before deciding on an action via the policy. In our
experiments, those intrinsic functions were relatively simple, whereby they ordered and arranged data
from memory to generate suitable trajectories for extrinsic functions and shrank first observations
and actions when exceeding context length constraints. Second, the vital difference with standard
RL pipelines is that our agent utilises LLM policies. While such policies offer prior flexibility, they
arrive with new generation speed and credit assignment problems if actions can be any text prompt.
On Accelerating Generation: A well-known bottleneck for successful RL with LLM policies
is the speed by which LLMs generate actions. In an RL setting, slow-generation speeds coupled
with the high-sample complexities typical for model-free algorithms lead to impractical learners,
especially when only having access to modest computing hardware. We rely on three methods to
accelerate generation speed: i)continuous batching, ii)optimised kernels and iii)caching with Paged
Attention [ 58]. Since we collect trajectories in parallel with multiple workers interacting, each with
its environment, continuous batching allows us to batch queries to the LLM and to possibly receive
an answer before the whole batch computations are over. Optimised attention kernels are written in
C++/CUDA to enable faster memory-efficient inference than vanilla implementations. We notably
relied on the xFormers library [ 59], which also features fused operations for the softmax, linear layer,
dropout and layer normalisation operations. Those fused operations notably reduce the amount of
reading and writing back to the DRAM compared to vanilla PyTorch implementation. Finally, we
relied on Paged Attention [ 58] to cache the key and value in attention operations. Since the generation
is autoregressive, it is unnecessary to recompute all the previous tokens’ keys and values if they were
computed before. With those additions, we observed that we could reduce the generation time by a
factor of 3 compared to vanilla HuggingFace for the Llama 2-7B parameters model.
On Credit Assignment: To work with environments where actions can be any textual prompt (e.g.
in code generation) and in environments with a restricted set of actions, we would need to define
the policy loss and the associated likelihood over every token composing an action. However, when
defining critics in RL, credit must be assigned to an action, not a token. For instance, if the sequence
of the following three tokens [⟨Move⟩,⟨⟩,⟨Right⟩]constitutes one action (i.e. that which moves the
agent to the right), we would need to assign the same value for each of those three tokens. To support
this form of credit assignment, we introduce a new version of generalised advantage estimation in
PPO, where we define:
At=∞X
n=t1n∈¯A(λγ)σ(n)−σ(t) 
rσ(n)+1+γV(sσ(n)+1)−V(sσ(n))
,
where λis a hyper-parameter that balances the bias-variance tradeoff, γis a discount factor that
weighs the future returns, ¯Ais the set composed of the index of the last token of every action, and
σ:N→Nis a mapping from the token index to the action index. Using ¯A, we can keep track of
the tokens corresponding to one action to perform the correct credit assignment. Finally, V(s)is
the current value estimate of a state s. We add a second head to the LLM from the last embedding
layer to allow the critic to accurately approximate V(s). We followed PPO [ 57] to train this critic
and minimise the squared temporal difference loss.
4 Evaluating Pangu-Agent
In this section, we conduct an extensive evaluation of various methods supported by Pangu-Agent,
including both structured reasoning methods and fine-tuning. First, we evaluate the structured
7

--- PAGE 8 ---
Observe Function Modifier of the Mem
Extrinsic 
Function
Act
Think Act
Figure 3: Visualisation of one example of structured reasoning using nesting of intrinsic and extrinsic
functions. The agent initially updates its internal memory, using µ0, by perceiving its observation.
Then the intrinsic function µ1selects between Think-and-Act or just Act. The last intrinsic function
µ2either generates a thought if µ1previously selected Think-and-Act otherwise it is null. Finally,
the extrinsic function πselects the action that the agent will perform in the environment.
reasoning ability of Pangu-Agent (see Figure 3, by considering both first-order nesting and composite
methods, and afterwards we evaluate fine-tuning of Pangu-Agent in three distinct settings using
supervised learning and RL. Our results indicate that composite methods tend to outperform first-
order nesting methods with respect to the achieved returns of the agent. Finally, we show that SFT
and RLFT allow the agent to specialise and further increase its achieved returns in the ALFWorld and
BabyAI tasks. Throughout this evaluation, several LLMs are used, such as GPT [ 60], Llama 2 [ 61],
OpenChat [62], Vicuna [63], and Mistral [64].
4.1 Evaluating Structured Reasoning
The built-in support for intrinsic functions allows us to evaluate how different design choices in the
reasoning structure affect the performance of an AI agent. First, in Table 2, we evaluate first-order
nestings, i.e. setups in which an agent’s memory is modified only by observing the environment
and the actions it performs on it. In the literature, these would be referred to simply as different
prompting methods e.g.: Direct prompting, prompting with Few-Shot (FS) examples, Few-Shot Chain
of Thought (FS-CoT) [ 43], Zero-Shot CoT (ZS-CoT) [ 65] – with a thorough description of these
methods presented in Appendix A.1. It should be noted that due to the non-deterministic nature of
LLM’s text generation, the achieved returns can vary significantly between different runs. To account
for these variations, we run each combination of task-method-LLM three times and report the mean
standard deviation.
But first-order nestings have limits as they may struggle to fully use the capabilities of an LLM.
As motivated in the introduction (Section 1), an agent needs to be able to process the output of the
language model, revisit its answers, change its memory and even use tools. Composite methods, as
we call them, are methods that may require multiple thinking steps until a final action is decided. In
Table 3, we present results for four composite methods: FS-CoT with Self-Consistency (FS-CoT-
SC) [ 39], FS-CoT with an optional distinct thinking step (e.g. React [ 40]), FS-CoT with a reflection
step [e.g. 66], SwiftSage [ 41], and Least-to-Most [ 42] (also see Appendix A.2). All these methods
make use of multiple intrinsic function steps at each environment time step. Refer to Table 7 for brief
descriptions of all method acronyms.
We observe that methods that are similar in their structure but differ in their prompt content, such as
Reflect and React, yield significantly different achieved returns for the agent. This demonstrates the
importance of careful prompt engineering. It is also noteworthy that different methods work better
for some LLMs than others, e.g. React in OpenChat-3.2 performs worse than FS on average, while
React and FS in GPT-3.5 perform similarly in terms of average achieved returns.
Notably, the performance of FS in GSM8K is considerably worse than Direct across all LLMs. This
does not come as a surprise since FS presents only the final answer to the LLM. Therefore, the LLM
aims to answer the question without generating the intermediate steps. However, in Direct, the LLM
generates the intermediate steps even without explicitly requested as this is how similar grade-school
8

--- PAGE 9 ---
LLM Method OverallTask
ALFWorld GSM8K HotpotQA WebShop HumanEval BabyAIGPT-3.5Direct 35.9 4.7±2.5 69.2±0.8 32.8±0.722.5±3.5 58.2±2.1 28.3±4.7
ZS-CoT 29.3 24.7±1.9 65.8±0.9 25.8±0.1 6.9±1.3 17.8±0.8 34.6±0.9
FS 38.3 12.7±3.8 35.0±11.545.3±0.734.4±1.2 66.5±3.5 36.2±2.6
FS-CoT 49.9 40.3±3.1 66.4±0.1 40.5±0.338.9±1.7 63.1±3.7 50.2±4.3Llama 2-70BDirect 17.9 5.3±0.9 28.8±0.7 27.6±0.4 0.0±0.0 23.2±1.5 22.2±2.0
ZS-CoT 21.0 11.3±0.9 48.7±1.0 33.9±0.5 6.6±2.2 11.8±2.0 13.7±1.9
FS 16.3 5.3±3.4 32.2±0.7 31.0±0.4 1.2±0.8 15.7±1.1 12.3±3.7
FS-CoT 27.7 18.0±4.3 53.0±0.6 49.0±0.6 5.7±1.6 19.0±0.3 21.6±3.1OpenChat-3.2Direct 11.5 0.0±0.0 7.6±0.6 43.0±0.4 0.0±0.0 8.5±2.1 9.8±4.2
ZS-CoT 14.8 17.3±6.6 22.7±0.5 28.6±0.2 0.0±0.0 2.5±0.9 17.7±4.6
FS 14.0 2.0±1.6 2.2±0.4 47.0±0.322.2±3.0 1.4±0.6 9.0±6.0
FS-CoT 21.4 26.7±4.1 21.1±0.9 39.2±0.3 3.2±2.5 1.2±0.0 36.8±11.5Vicuna-13BDirect 10.0 3.3±2.5 10.8±0.2 24.9±0.5 0.0±0.0 0.0±0.0 21.0±2.2
ZS-CoT 13.3 13.3±1.9 23.8±0.3 19.3±0.5 0.4±0.3 0.2±0.3 22.9±2.8
FS 14.7 11.3±2.5 6.2±0.6 30.5±0.412.2±0.7 1.2±0.9 27.0±2.3
FS-CoT 17.8 15.3±0.9 27.3±0.9 32.1±0.9 1.0±0.8 2.1±1.1 29.2±2.9Llama 2-13BDirect 11.3 0.0±0.0 16.7±0.6 27.6±0.4 3.5±1.2 11.6±1.3 8.7±5.6
ZS-CoT 13.5 0.7±0.9 31.6±0.6 25.4±0.4 0.4±0.6 9.7±0.3 12.9±3.7
FS 12.1 0.0±0.0 7.9±0.1 35.4±0.710.9±1.5 8.9±1.1 9.8±2.3
FS-CoT 16.4 6.0±1.6 33.5±0.5 23.9±0.911.2±1.2 2.1±0.3 21.5±4.2OpenChat-3.5Direct 30.9 4.0±0.0 37.6±0.6 47.4±0.435.4±0.8 32.9±0.0 28.1±0.6
ZS-CoT 34.9 10.7±5.7 64.9±0.4 41.2±1.017.8±1.4 39.1±4.3 35.7±1.0
FS 31.2 7.3±2.5 24.7±1.1 57.2±0.543.5±1.0 33.3±3.0 20.9±6.7
FS-CoT 47.4 27.3±3.8 70.1±0.5 64.3±0.335.5±2.1 28.0±3.2 58.9±6.7Mistral-7BDirect 11.2 4.0±1.6 14.1±0.7 30.3±0.1 0.0±0.0 10.8±0.6 7.7±3.2
ZS-CoT 13.2 9.3±3.8 33.9±0.7 18.3±0.5 0.0±0.0 0.4±0.3 17.4±4.4
FS 8.3 6.0±1.6 0.5±0.0 32.9±0.5 0.7±0.5 3.7±0.5 6.0±3.3
FS-CoT 17.5 6.0±2.5 32.7±0.3 28.0±1.1 0.9±0.4 8.9±2.4 28.6±3.9
Table 2: Average achieved returns and the standard deviation across three runs, for four first-order
prompt engineering methods on six different tasks, using seven different LLMs.
level questions are presented on the internet, which are likely contained within the training set of
these LLMs. Similar conclusions can be drawn when comparing ZS-CoT with FS, where we observe
that the achieved returns are increased even compared to Direct. This is especially true for smaller
LLMs where we conjecture that when adding the "think step-by-step" quote into the prompt, the
model is more likely to generate reasoning steps that will allow correctly solving the question at hand.
In the HumanEval task, we observe that the difference in the achieved returns between GPT-3.5 and
the remaining models is significantly larger compared to other tasks. This can be attributed to the fact
that HumanEval is a coding task, that requires well-structured responses, such as correct indentation,
from the LLM. However, smaller and open-source LLMs are more prone to these structural errors
which result in failing the task and receiving a return of 0.
Another factor that impedes the performance of LLMs is the limited context length. In tasks such as
WebShop, which involves relatively large observations, the length of the prompt needs to be truncated
to stay within the allowed context length. Consequently, the performance of LLMs in this task can be
substantially affected, particularly in methods such as Reflect, where additional information is also
included in the prompt. This explains why the Reflect method tends to under-perform in WebShop
compared to other methods.
In several cases, FS-CoT-SC can improve the achieved returns of the LLM, especially in GSM8K.
However, this comes with the extra cost of needing to prompt the LLM several times (5 in the
presented experiments) to perform the SC action selection. In tasks such as HumanEval, where the
answer contains long textual answers and potentially several answers can yield the correct outcome,
we found that SC cannot be applied. This happens because the LLM will never generate the same
answer as before, and the SC action selector cannot choose the most frequent answer.
9

--- PAGE 10 ---
LLM Method OverallTask
ALFWorld GSM8K HotpotQA WebShop HumanEval BabyAIGPT-3.5FS-CoT-SC 48.2 34.4±2.7 74.1±0.843.5±0.139.1±1.9 - 50.0 ±7.9
FS-CoT-React 45.8 39.5±3.6 66.9±1.238.1±0.128.5±3.4 61.3±2.8 40.8±2.3
FS-CoT-Reflect 34.0 26.7±1.9 70.8±0.832.5±0.9 0.9±0.4 54.5±3.6 19.0±2.5
FS-Least-to-Most 39.8 - 60.0 ±0.320.3±0.4 - 39.1 ±1.3 -Llama 2-70BFS-CoT-SC 29.7 13.3±2.5 59.7±0.852.1±0.3 2.2±0.3 - 21.0 ±1.3
FS-CoT-React 21.0 3.3±1.9 48.7±1.141.8±0.8 0.7±1.0 12.6±1.5 19.0±2.6
FS-CoT-Reflect 24.6 18.7±3.4 56.8±1.135.8±0.8 1.9±1.2 19.0±0.8 15.5±2.1
FS-CoT-Swift-Sage 23.5 28.0±5.7 - - 15.4 ±0.9 - 27.2 ±2.1
FS-Least-to-Most 16.2 - 31.2 ±0.515.0±0.2 - 2.5 ±1.3 -OpenChat-3.2FS-CoT-SC 25.3 21.3±1.9 21.2±0.642.7±0.6 1.3±0.9 - 40.0 ±3.6
FS-CoT-React 11.3 2.7±0.9 6.6±0.7 42.6±1.0 0.6±0.5 3.3±1.5 11.9±2.4
FS-CoT-Reflect 16.7 20.7±9.3 26.2±0.825.2±0.3 0.0±0.0 3.9±0.8 24.3±0.8
FS-CoT-Swift-Sage 21.3 24.0±6.9 - - 13.2 ±2.2 - 26.6 ±3.1
FS-Least-to-Most 8.6 - 14.0 ±0.611.3±0.6 - 0.4 ±0.6 -Vicuna-13BFS-CoT-SC 25.6 18.7±2.5 37.7±0.538.2±0.2 0.0±0.0 - 33.4 ±0.9
FS-CoT-React 16.1 17.3±1.9 19.0±1.126.0±1.4 0.0±0.0 1.2±0.9 33.0±7.5
FS-CoT-Reflect 16.9 17.3±3.4 32.8±1.021.7±0.4 0.3±0.4 4.8±1.8 24.3±4.2
FS-CoT-Swift-Sage 22.5 27.3±3.4 - - 13.9 ±2.2 - 26.2 ±4.1
FS-Least-to-Most 10.0 - 19.7 ±0.410.1±0.3 - 0.2 ±0.3 -Llama 2-13BFS-CoT-SC 20.1 3.3±0.9 39.3±0.640.8±0.7 2.7±1.6 - 14.5 ±4.5
FS-CoT-React 16.1 0.0±0.0 25.9±0.539.0±0.5 2.1±0.9 3.7±0.9 25.6±7.5
FS-CoT-Reflect 12.8 10.7±2.5 32.4±0.811.0±0.4 0.3±0.4 2.1±0.6 20.6±2.5
FS-CoT-Swift-Sage 18.3 22.7±0.9 - - 11.8 ±4.9 - 20.6 ±2.5
FS-Least-to-Most 8.7 - 12.2 ±0.813.1±0.5 - 0.8 ±0.8 -OpenChat-3.5FS-CoT-SC 53.5 28.0±1.6 80.8±0.270.4±0.742.9±2.0 32.9±0.9 66.1±6.4
FS-CoT-React 39.0 24.7±1.9 62.1±0.348.2±1.033.6±0.7 29.2±1.8 36.2±3.6
FS-CoT-Reflect 39.4 28.7±5.7 74.5±0.257.0±0.114.1±3.2 27.1±3.6 34.8±1.2
FS-CoT-Swift-Sage 37.7 37.3±6.2 - - 31.5 ±1.4 - 44.3 ±3.5
FS-Least-to-Most 36.0 - 59.1 ±0.632.5±0.1 - 16.4 ±0.6 -Mistral-7BFS-CoT-SC 21.3 6.0±1.6 38.2±0.737.8±0.4 0.3±0.4 - 24.4 ±3.3
FS-CoT-React 11.4 5.3±1.9 19.9±1.128.9±0.9 0.6±0.8 5.8±3.1 7.9±2.0
FS-CoT-Reflect 15.0 7.3±0.9 34.4±1.419.6±0.9 0.0±0.0 5.2±0.3 23.3±2.5
FS-CoT-Swift-Sage 19.4 16.7±5.0 - - 12.4 ±3.0 - 29.2 ±2.8
FS-Least-to-Most 5.6 - 5.5 ±0.7 10.8±0.3 - 0.6 ±0.5 -
Table 3: Average achieved returns and the standard deviation across three runs, for five composite
reasoning methods on six different tasks, using seven different LLMs.
4.2 Evaluating Extrinsic Functions: Fine-Tuning
By closely observing the results in Section 4.1, we can conclude that while LLMs can perform
well in achieving returns in a wide range of tasks, there is a large room for improvement towards
achieving a 100% success rate. In this section, we aim to explore how SFT and RLFT can help
Pangu-Agent increase the returns it achieves. We propose two different pipelines: a Bootstrap
SFT (BSFT) that consists of a multi-turn trajectory generation and SFT and a three-step pipeline
consisting of trajectory generation, SFT and RLFT. Expert trajectory demonstrations, for performing
SFT, are always gathered using the OpenChat-3.5 LLM equipped with the structured reasoning
abilities of the Pangu-Agent framework. We perform BSFT using the OpenChat-3.5 LLM, while
the SFT-RLFT pipeline is applied to the Llama 2-7B LLM. We consider two distinct evaluation
paradigms: fine-tuning a different LLM for each task and fine-tuning an LLM in several tasks (e.g.
multi-task fine-tuning).
4.2.1 One Model per Domain
BSFT: In the first experiment, we show a combination of the intrinsic functions and the fine-tuning
offered by the Pangu-Agent framework. We first collect data from a diverse set of prompting methods,
specifically ZS-CoT, FS-CoT, FS-CoT-React, and FS-CoT-Reflect. After this data collection, we
run a rejection sampling step, discarding failed trajectories and only keeping the best-performing
ones regarding discounted returns. An SFT step can then be performed on this dataset to improve the
method’s performance further. Results for the trained model after a single SFT step can be found in
Table 4 under the column "1-step SFT". Importantly, the model created after one step of this process
10

--- PAGE 11 ---
is still usable under the framework. Indeed, we can now feed it back to the aforementioned prompting
methods and create a higher-quality dataset to fine-tune further. We repeat this process two more
times on top of the OpenChat-3.5 model, each for four epochs over the training data, and we report
the results on a held-out test set of ALFWorld episodes in Table 4.
TasksOpenChat-3.5
Direct FS-CoT 1-step SFT 2-step SFT 3-step SFT
ALFWorld 0.04 0.27 0.45 0.68 0.82
Table 4: OpenChat-3.5 on ALFWorld with/without fine-tuning on held-out tasks.
Table 4 shows that after a single round of rejection sampling, we can achieve a strong performance in
ALFWorld while keeping the model’s ability to generate thoughts before actions.
SFT-RLFT: That said, fine-tuning on the full trajectories generated by these intrinsic functions is
computationally expensive and quickly reaches a point of diminishing returns. Instead, we propose
the use of RL to reach even higher performance in a variety of tasks. To do so, we first perform a
fine-tuning step, but on a modified version of intrinsic functions which build the LLM context in a
more space-efficient format (i.e. a chat-based format (ot,at,ot+1,at+1, ...)).
In this experiment, we fine-tune three distinct Llama 2-7B models on ALFWorld, BabyAI-GoToObj-
v0 and BabyAI-GoToRedBlueBall-v0. To do so, we first collect successful trajectories on training
tasks. To perform SFT, we remove the generated thoughts and only fine-tune on the actions. After
fine-tuning, we evaluate the checkpoint achieving the best score over 512 trajectories in the training
domain and test it on the 50 held-out test episodes.
TasksOpenChat-3.5 Llama 2-7B
Direct FS-CoT Original SFT SFT+RL RL
ALFWorld 0.04 0.27 0 0.5 0.88 0.04
BabyAI-GoToObj-v0 0.31 0.61 0.28 0.75 0.91 0.87
BabyAI-GoToRedBlueBall-v0 0.11 0.43 0.04 0.21 0.77 0.69
Table 5: Benchmark of OpenChat and Llama 2-7B with/without fine-tuning on held-out tasks.
Table 5 shows that fine-tuning first with SFT on successful demonstrations, followed by RL, leads to
the largest improvement in success rates. For complex domains like ALFWorld, it also shows that
the SFT step and the intrinsic function (FS-CoT) for trajectory generation are crucial. This shows
the importance of our Pangu-Agent framework where we can benefit from intrinsic functions and
fine-tuning.
4.2.2 Cross-Domain Model
In this experiment, we collected additional trajectories on more diverse BabyAI tasks with the same
methodology and trained a single cross-domain model on ALFWorld and BabyAI. Note that no
successful trajectories could be generated in BabyAI-UnlockPickup-v0 in the allotted time. We also
remove the thoughts from the successful trajectories to speed up the training.
Table 6 presents the achieved returns of the SFT and RLFT using the Llama 2-7B LLM on ALFWorld
and BabyAI tasks. This experiment establishes that it is possible to successfully SFT and RLFT
Llama 2 on multitask training. The performance in ALFWorld is very close to that achieved by
fine-tuning exclusively on ALFWorld. However, for BabyAI tasks, multi-task training has a clear
benefit, achieving even better performance than the specialised model of Table 5. It is also capable of
generalising to BabyAI tasks that are unseen in training.
5 Conclusion & Future Work
This work presents the Pangu-Agent framework to facilitate future research towards developing
generalist AI agents. Pangu-Agent builds upon LLMs to address reasoning and decision problems,
11

--- PAGE 12 ---
Task Training weight Original SFT SFT+RL
ALFWorld 0.5 0 0.42 0.82
BabyAI-GoToObj-v0 0.05 0.24 0.89 0.93
BabyAI-GoToRedBlueBall-v0 0.05 0.18 0.77 0.83
BabyAI-GoToRedBallGrey-v0 0.05 0.08 0.72 0.81
BabyAI-GoToLocalS8N7-v0 0.05 0.22 0.8 0.87
BabyAI-PickupLoc-v0 0.05 0.15 0.57 0.63
BabyAI-PickupDist-v0 0.05 0.07 0.69 0.78
BabyAI-GoToDoor-v0 0.05 0.2 0.89 0.98
BabyAI-OpenRedDoor-v0 0.05 0.01 0.63 0.88
BabyAI-PutNextS7N4Carrying-v0 0.05 0.07 0.39 0.66
BabyAI-UnlockPickup-v0 0.05 0 0 0
BabyAI-GoToObjDoor-v0 0 0.13 0.82 0.81
BabyAI-GoToRedBallNoDists-v0 0 0.36 0.84 0.87
BabyAI-GoToOpen-v0 0 0 0.15 0.13
BabyAI-GoToObjMazeS4R2-v0 0 0.05 0.23 0.24
BabyAI-Open-v0 0 0 0.25 0.51
BabyAI-Unlock-v0 0 0 0.11 0.17
BabyAI-PutNextLocal-v0 0 0 0.01 0.01
BabyAI-ActionObjDoor-v0 0 0.05 0.64 0.78
Table 6: Benchmark of Llama 2-7B with/without cross-domain fine-tuning on held-out tasks.
which allows utilising human priors. First, we propose a general RL-based objective to optimise the
agent’s intrinsic and extrinsic functions. We implemented several intrinsic functions and showcased
the modular functionality of our agent and how it can support recent advances in LLM research. We
extensively evaluated Pangu-Agent in several single-agent and multi-agent tasks for different prompt
engineering methods across various LLMs, and offered insights about their relative advantages and
disadvantages.
Moreover, we discuss how this framework can fine-tune LLMs through an SFT and RL pipeline.
Our results indicate that fine-tuning can improve the agent’s performance up to threefold in specific
multi-step decision-making domains such as ALFWorld and Baby-AI. We also show how LLMs have
the capacity for cross-domain learning by fine-tuning a single LLM and evaluating it simultaneously
in multiple domains. Finally, we conclude this work by presenting the existing limitations of the
current version of Pangu-Agent as well as our vision towards the development of a generalist agent.
Full differentiability: This work focused on independently optimising the intrinsic and extrinsic
functions. Looking forward, we envision that Pangu-Agent development will gradually shift towards
structured and end-to-end fine-tuning of the framework. This will enable passing gradients between
various intrinsic and extrinsic functions, allowing for a more adaptable system.
Real-world applications: At present, the performance of Pangu-Agent is evaluated on a small
number of single-agent and multi-agent tasks. We plan to incorporate more diverse and complex
evaluation tasks in future revisions to make Pangu-Agent effective in real-world applications and
address the simulation-to-reality gap. The long-term goal of this work is the development of generalist
agents that can assist humans in their everyday tasks and function autonomously in many real-world
challenges.
Memory retrieval: Another avenue of future research in the Pangu-Agent framework lies in the
memory retrieval methods. The current version of Pangu-Agent supports a long-term memory that
stores any information available to each agent, such as its observations, thoughts and actions. In
the future, we aim to incorporate more sophisticated memory retrieval methods, such as embedding
similarity from vector databases to allow the agent to incorporate relevant memories in its context
window, enabling it to solve the task.
Planning: Currently, for planning, we only focus on reasoning tasks. We intend to integrate and
test tree search algorithms in agent-based tasks within interactive environments. Additionally, we are
12

--- PAGE 13 ---
committed to developing and implementing strategies for efficient long-term planning. We aim to
enhance the planning capabilities of Pangu-Agent, thereby equipping it to tackle real-world challenges
and adapt to dynamic environments.
Tool use: Lastly, a significant part of our future Pangu-Agent roadmap is to facilitate integration
with external tools. Pangu-Agent includes a code interpreter for executing simple Python scripts in
its current configuration. However, future versions of Pangu-Agent will support compatibility with
various external tools like web search engines, calculators (for instance Wolfram Alpha), and maps.
This expansion will enable a broader deployment of Pangu-Agent across various applications and
enable generalisation to tasks beyond its initial learning distribution.
References
[1]J. S. Albus, “Outline for a theory of intelligence,” IEEE transactions on systems, man, and cybernetics ,
vol. 21, no. 3, pp. 473–509, 1991.
[2]D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran,
T. Graepel et al. , “A general reinforcement learning algorithm that masters chess, shogi, and go through
self-play,” Science , vol. 362, no. 6419, pp. 1140–1144, 2018.
[3]D. J. Mankowitz, A. Michi, A. Zhernov, M. Gelmi, M. Selvi, C. Paduraru, E. Leurent, S. Iqbal, J.-B.
Lespiau, A. Ahern et al. , “Faster sorting algorithms discovered using deep reinforcement learning,” Nature ,
vol. 618, no. 7964, pp. 257–263, 2023.
[4]A. Fawzi, M. Balog, A. Huang, T. Hubert, B. Romera-Paredes, M. Barekatain, A. Novikov, F. J. R Ruiz,
J. Schrittwieser, G. Swirszcz et al. , “Discovering faster matrix multiplication algorithms with reinforcement
learning,” Nature , vol. 610, no. 7930, pp. 47–53, 2022.
[5]E. Kaufmann, L. Bauersfeld, A. Loquercio, M. Müller, V . Koltun, and D. Scaramuzza, “Champion-level
drone racing using deep reinforcement learning,” Nature , vol. 620, no. 7976, pp. 982–987, 2023.
[6]J. Degrave, F. Felici, J. Buchli, M. Neunert, B. Tracey, F. Carpanese, T. Ewalds, R. Hafner, A. Abdolmaleki,
D. de Las Casas et al. , “Magnetic control of tokamak plasmas through deep reinforcement learning,”
Nature , vol. 602, no. 7897, pp. 414–419, 2022.
[7]K. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine, “Efficient off-policy meta-reinforcement learning
via probabilistic context variables,” in International Conference on Machine Learning , 2019. [Online].
Available: https://api.semanticscholar.org/CorpusID:84187276
[8]A. Gupta, R. Mendonca, Y . Liu, P. Abbeel, and S. Levine, “Meta-reinforcement learning of
structured exploration strategies,” in Neural Information Processing Systems , 2018. [Online]. Available:
https://api.semanticscholar.org/CorpusID:3418899
[9]J. X. Wang, Z. Kurth-Nelson, D. Kumaran, D. Tirumala, H. Soyer, J. Z. Leibo, D. Hassabis, and M. M.
Botvinick, “Prefrontal cortex as a meta-reinforcement learning system,” Nature Neuroscience , vol. 21, pp.
860 – 868, 2018. [Online]. Available: https://api.semanticscholar.org/CorpusID:44137923
[10] I. Clavera, A. Nagabandi, S. Liu, R. S. Fearing, P. Abbeel, S. Levine, and C. Finn, “Learning to adapt in
dynamic, real-world environments through meta-reinforcement learning,” in International Conference on
Learning Representations , 2019. [Online]. Available: https://openreview.net/forum?id=HyztsoC5Y7
[11] A. G. Barto, “Intrinsic motivation and reinforcement learning,” in Intrinsically Motivated Learning
in Natural and Artificial Systems , 2013. [Online]. Available: https://api.semanticscholar.org/CorpusID:
2326055
[12] H. Tang, R. Houthooft, D. Foote, A. Stooke, O. Xi Chen, Y . Duan, J. Schulman, F. DeTurck, and P. Abbeel,
“#exploration: A study of count-based exploration for deep reinforcement learning,” in Advances in
Neural Information Processing Systems , I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017. [Online]. Available: https:
//proceedings.neurips.cc/paper_files/paper/2017/file/3a20f62a0af1aa152670bab3c602feed-Paper.pdf
[13] Y . Burda, H. Edwards, A. Storkey, and O. Klimov, “Exploration by random network distillation,” in
7th International Conference on Learning Representations (ICLR 2019) , May 2019, pp. 1–17, seventh
International Conference on Learning Representations, ICLR 2019 ; Conference date: 06-05-2019
Through 09-05-2019. [Online]. Available: https://iclr.cc/
[14] M. Jaderberg, V . Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu,
“Reinforcement learning with unsupervised auxiliary tasks,” in International Conference on Learning
Representations , 2017. [Online]. Available: https://openreview.net/forum?id=SJ6yPD5xg
[15] E. Shelhamer, P. Mahmoudieh, M. Argus, and T. Darrell, “Loss is its own reward: Self-
supervision for reinforcement learning,” ArXiv , vol. abs/1612.07307, 2016. [Online]. Available:
https://api.semanticscholar.org/CorpusID:16561904
13

--- PAGE 14 ---
[16] S. Li, R. Wang, M. Tang, and C. Zhang, “Hierarchical reinforcement learning with advantage-based
auxiliary rewards,” Advances in Neural Information Processing Systems , vol. 32, 2019.
[17] A. Ng, D. Harada, and S. J. Russell, “Policy invariance under reward transformations: Theory and
application to reward shaping,” in International Conference on Machine Learning , 1999. [Online].
Available: https://api.semanticscholar.org/CorpusID:5730166
[18] S. Devlin and D. Kudenko, “Dynamic potential-based reward shaping,” in Adaptive Agents and
Multi-Agent Systems , 2012. [Online]. Available: https://api.semanticscholar.org/CorpusID:17251664
[19] P. Goyal, S. Niekum, and R. J. Mooney, “Using natural language for reward shaping in reinforcement
learning,” in International Joint Conference on Artificial Intelligence , 2019. [Online]. Available:
https://api.semanticscholar.org/CorpusID:70350059
[20] E. Brochu, V . M. Cora, and N. de Freitas, “A tutorial on bayesian optimization of expensive cost
functions, with application to active user modeling and hierarchical reinforcement learning,” ArXiv , vol.
abs/1012.2599, 2010. [Online]. Available: https://api.semanticscholar.org/CorpusID:1640103
[21] P. Poupart, N. Vlassis, J. Hoey, and K. Regan, “An analytic solution to discrete bayesian reinforcement
learning,” in Proceedings of the 23rd international conference on Machine learning , 2006, pp. 697–704.
[22] M. Ghavamzadeh, S. Mannor, J. Pineau, A. Tamar et al. , “Bayesian reinforcement learning: A survey,”
Foundations and Trends® in Machine Learning , vol. 8, no. 5-6, pp. 359–483, 2015.
[23] T. D. Kulkarni, K. Narasimhan, A. Saeedi, and J. Tenenbaum, “Hierarchical deep reinforcement
learning: Integrating temporal abstraction and intrinsic motivation,” in Advances in Neural Information
Processing Systems , D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, Eds., vol. 29. Curran
Associates, Inc., 2016. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2016/file/
f442d33fa06832082290ad8544a8da27-Paper.pdf
[24] A. G. Barto and S. Mahadevan, “Recent advances in hierarchical reinforcement learning,” Discrete event
dynamic systems , vol. 13, no. 1-2, pp. 41–77, 2003.
[25] O. Nachum, S. S. Gu, H. Lee, and S. Levine, “Data-efficient hierarchical reinforcement learning,” Advances
in neural information processing systems , vol. 31, 2018.
[26] G. Andersen, P. Vrancx, and H. Bou-Ammar, “Learning high-level representations from demonstrations,”
CoRR , vol. abs/1802.06604, 2018. [Online]. Available: http://arxiv.org/abs/1802.06604
[27] Q. Wu, G. Bansal, J. Zhang, Y . Wu, B. Li, E. Zhu, L. Jiang, X. Zhang, S. Zhang, J. Liu, A. H. Awadallah,
R. W. White, D. Burger, and C. Wang, “AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent
Conversation,” arXiv e-prints , p. arXiv:2308.08155, Aug. 2023.
[28] S. Gravitas, “Auto-gpt: An autonomous gpt-4 experiment,” 2023. [Online]. Available: https:
//github.com/Significant-Gravitas/AutoGPT
[29] W. Chen, Y . Su, J. Zuo, C. Yang, C. Yuan, C.-M. Chan, H. Yu, Y . Lu, Y .-H. Hung, C. Qian, Y . Qin, X. Cong,
R. Xie, Z. Liu, M. Sun, and J. Zhou, “Agentverse: Facilitating multi-agent collaboration and exploring
emergent behaviors,” 2023.
[30] T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom,
“Toolformer: Language models can teach themselves to use tools,” Advances in Neural Information
Processing Systems , vol. 36, 2023.
[31] Y . Shen, K. Song, X. Tan, D. Li, W. Lu, and Y . Zhuang, “Hugginggpt: Solving ai tasks with chatgpt and its
friends in huggingface,” Advances in Neural Information Processing Systems , vol. 36, 2023.
[32] HuggingFace, “Transformers agent,” 2023. [Online]. Available: https://huggingface.co/docs/transformers/
transformers_agents
[33] C. Harrison, “Langchain,” 2022.
[34] T. Xie, F. Zhou, Z. Cheng, P. Shi, L. Weng, Y . Liu, T. J. Hua, J. Zhao, Q. Liu, C. Liu, L. Z. Liu, Y . Xu,
H. Su, D. Shin, C. Xiong, and T. Yu, “Openagents: An open platform for language agents in the wild,”
2023.
[35] X. Team, “Xagent: An autonomous agent for complex task solving,” 2023.
[36] S. Hong, X. Zheng, J. Chen, Y . Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran,
L. Xiao, and C. Wu, “Metagpt: Meta programming for multi-agent collaborative framework,” 2023.
[37] G. Li, H. A. A. K. Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, “CAMEL: Communicative agents for
”mind” exploration of large language model society,” in Thirty-seventh Conference on Neural Information
Processing Systems , 2023. [Online]. Available: https://openreview.net/forum?id=3IyL2XWDkG
[38] W. Zhou, Y . E. Jiang, L. Li, J. Wu, T. Wang, S. Qiu, J. Zhang, J. Chen, R. Wu, S. Wang, S. Zhu, J. Chen,
W. Zhang, N. Zhang, H. Chen, P. Cui, and M. Sachan, “Agents: An open-source framework for autonomous
language agents,” 2023.
14

--- PAGE 15 ---
[39] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou, “Self-consistency
improves chain of thought reasoning in language models,” arXiv preprint arXiv:2203.11171 , 2022.
[40] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao, “React: Synergizing reasoning and
acting in language models,” arXiv preprint arXiv:2210.03629 , 2022.
[41] B. Y . Lin, Y . Fu, K. Yang, P. Ammanabrolu, F. Brahman, S. Huang, C. Bhagavatula, Y . Choi, and X. Ren,
“Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks,” 2023.
[42] D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. V .
Le, and E. H. Chi, “Least-to-most prompting enables complex reasoning in large language models,”
inThe Eleventh International Conference on Learning Representations , 2023. [Online]. Available:
https://openreview.net/forum?id=WZH7099tgfM
[43] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le, D. Zhou et al. , “Chain-of-thought
prompting elicits reasoning in large language models,” Advances in Neural Information Processing Systems ,
vol. 35, pp. 24 824–24 837, 2022.
[44] M. Shridhar, X. Yuan, M. Côté, Y . Bisk, A. Trischler, and M. J. Hausknecht, “Alfworld: Aligning text and
embodied environments for interactive learning,” CoRR , vol. abs/2010.03768, 2020. [Online]. Available:
https://arxiv.org/abs/2010.03768
[45] M. Chevalier-Boisvert, D. Bahdanau, S. Lahlou, L. Willems, C. Saharia, T. H. Nguyen, and Y . Bengio,
“Babyai: A platform to study the sample efficiency of grounded language learning,” 2019.
[46] R. Tutunov, A. Grosnit, J. Ziomek, J. Wang, and H. Bou-Ammar, “Why can large language models generate
correct chain-of-thoughts?” 2023.
[47] C.-C. Lin, A. Jaech, X. Li, M. R. Gormley, and J. Eisner, “Limitations of autoregressive models and their
alternatives,” arXiv preprint arXiv:2010.11939 , 2020.
[48] C. Wei, Y . Chen, and T. Ma, “Statistically meaningful approximation: a case study on approximating
turing machines with transformers,” Advances in Neural Information Processing Systems , vol. 35, pp.
12 071–12 083, 2022.
[49] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y . Cao, and K. Narasimhan, “Tree of Thoughts: Deliberate
Problem Solving with Large Language Models,” arXiv e-prints , p. arXiv:2305.10601, May 2023.
[50] S. Hao, Y . Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu, “Reasoning with language model is
planning with world model,” arXiv preprint arXiv:2305.14992 , 2023.
[51] X. Feng, Z. Wan, M. Wen, Y . Wen, W. Zhang, and J. Wang, “Alphazero-like tree-search can guide large
language model decoding and training,” in NeurIPS 2023 Foundation Models for Decision Making
Workshop , 2023. [Online]. Available: https://openreview.net/forum?id=PJfc4x2jXY
[52] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton,
R. Nakano, C. Hesse, and J. Schulman, “Training verifiers to solve math word problems,” 2021.
[53] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning, “HotpotQA: A
dataset for diverse, explainable multi-hop question answering,” in Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing . Brussels, Belgium: Association for Computational
Linguistics, Oct.-Nov. 2018, pp. 2369–2380. [Online]. Available: https://aclanthology.org/D18-1259
[54] S. Yao, H. Chen, J. Yang, and K. Narasimhan, “Webshop: Towards scalable real-world web interaction
with grounded language agents,” in ArXiv , preprint.
[55] E. A. Hansen, D. S. Bernstein, and S. Zilberstein, “Dynamic programming for partially observable
stochastic games,” in AAAI , vol. 4, 2004, pp. 709–715.
[56] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, and W. Chen, “LoRA: Low-rank
adaptation of large language models,” in International Conference on Learning Representations , 2022.
[Online]. Available: https://openreview.net/forum?id=nZeVKeeFYf9
[57] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization
algorithms,” CoRR , vol. abs/1707.06347, 2017. [Online]. Available: http://arxiv.org/abs/1707.06347
[58] W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. Gonzalez, H. Zhang, and I. Stoica, “Efficient
memory management for large language model serving with pagedattention,” in Proceedings of the 29th
Symposium on Operating Systems Principles , 2023, pp. 611–626.
[59] B. Lefaudeux, F. Massa, D. Liskovich, W. Xiong, V . Caggiano, S. Naren, M. Xu, J. Hu, M. Tintore,
S. Zhang, P. Labatut, and D. Haziza, “xformers: A modular and hackable transformer modelling library,”
https://github.com/facebookresearch/xformers, 2022.
[60] OpenAI, “Gpt-4 technical report,” 2023.
15

--- PAGE 16 ---
[61] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, N. Bashlykov, S. Batra, P. Bhargava,
S. Bhosale, D. Bikel, L. Blecher, C. Canton-Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu,
W. Fu, B. Fuller, C. Gao, V . Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan, M. Kardas,
V . Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M. Lachaux, T. Lavril, J. Lee, D. Liskovich,
Y . Lu, Y . Mao, X. Martinet, T. Mihaylov, P. Mishra, I. Molybog, Y . Nie, A. Poulton, J. Reizenstein,
R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang, R. Taylor,
A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y . Zhang, A. Fan, M. Kambadur, S. Narang, A. Rodriguez,
R. Stojnic, S. Edunov, and T. Scialom, “Llama 2: Open foundation and fine-tuned chat models,” CoRR , vol.
abs/2307.09288, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2307.09288
[62] G. Wang, S. Cheng, X. Zhan, X. Li, S. Song, and Y . Liu, “Openchat: Advancing open-source
language models with mixed-quality data,” CoRR , vol. abs/2309.11235, 2023. [Online]. Available:
https://doi.org/10.48550/arXiv.2309.11235
[63] W.-L. Chiang, Z. Li, Z. Lin, Y . Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y . Zhuang, J. E. Gonzalez,
I. Stoica, and E. P. Xing, “Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality,”
March 2023. [Online]. Available: https://lmsys.org/blog/2023-03-30-vicuna/
[64] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de Las Casas, F. Bressand,
G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang,
T. Lacroix, and W. E. Sayed, “Mistral 7b,” CoRR , vol. abs/2310.06825, 2023. [Online]. Available:
https://doi.org/10.48550/arXiv.2310.06825
[65] T. Kojima, S. S. Gu, M. Reid, Y . Matsuo, and Y . Iwasawa, “Large language models are zero-shot
reasoners,” in Advances in Neural Information Processing Systems , S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho, and A. Oh, Eds., vol. 35. Curran Associates, Inc., 2022,
pp. 22 199–22 213. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2022/file/
8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf
[66] N. Shinn, B. Labash, and A. Gopinath, “Reflexion: an autonomous agent with dynamic memory and
self-reflection,” arXiv preprint arXiv:2303.11366 , 2023.
[67] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui, “A survey for in-context
learning,” arXiv preprint arXiv:2301.00234 , 2022.
[68] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,
A. Askell et al. , “Language models are few-shot learners,” Advances in neural information processing
systems , vol. 33, pp. 1877–1901, 2020.
[69] M.-A. Côté, Ákos Kádár, X. Yuan, B. Kybartas, T. Barnes, E. Fine, J. Moore, R. Y . Tao, M. Hausknecht,
L. E. Asri, M. Adada, W. Tay, and A. Trischler, “Textworld: A learning environment for text-based games,”
2019.
[70] M. Shridhar, J. Thomason, D. Gordon, Y . Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and
D. Fox, “Alfred: A benchmark for interpreting grounded instructions for everyday tasks,”
in2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . Los
Alamitos, CA, USA: IEEE Computer Society, jun 2020, pp. 10 737–10 746. [Online]. Available:
https://doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.01075
[71] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y . Burda, N. Joseph,
G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray,
N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings,
M. Plappert, F. Chantzis, E. Barnes, A. Herbert-V oss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang,
I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V . Misra,
E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew,
D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, “Evaluating large language models trained on
code,” 2021.
[72] T. Carta, C. Romac, T. Wolf, S. Lamprier, O. Sigaud, and P.-Y . Oudeyer, “Grounding Large Language
Models in Interactive Environments with Online Reinforcement Learning,” vol. 202, no. 3676-3713.
PMLR, 2023. [Online]. Available: https://hal.science/hal-03970122
[73] P. Brookins and J. M. DeBacker, “Playing games with gpt: What can we learn about a large language
model from canonical strategic games?” Available at SSRN 4493398 , 2023.
[74] S. V . Albrecht, F. Christianos, and L. Schäfer, Multi-Agent Reinforcement Learning: Foundations and
Modern Approaches . MIT Press, 2024. [Online]. Available: https://www.marl-book.com
[75] E. Akata, L. Schulz, J. Coda-Forno, S. J. Oh, M. Bethge, and E. Schulz, “Playing repeated
games with large language models,” CoRR , vol. abs/2305.16867, 2023. [Online]. Available:
https://doi.org/10.48550/arXiv.2305.16867
[76] J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou, “Large language models
cannot self-correct reasoning yet,” arXiv preprint arXiv:2310.01798 , 2023.
16

--- PAGE 17 ---
[77] K. Stechly, M. Marquez, and S. Kambhampati, “Gpt-4 doesn’t know it’s wrong: An analysis of iterative
prompting for reasoning problems,” arXiv preprint arXiv:2310.12397 , 2023.
[78] J. Long, “Large Language Model Guided Tree-of-Thought,” arXiv e-prints , p. arXiv:2305.08291, May
2023.
[79] Y . Yao, Z. Li, and H. Zhao, “Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large
Language Models,” arXiv e-prints , p. arXiv:2305.16582, May 2023.
[80] M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, L. Gianinazzi, J. Gajda, T. Lehmann, M. Podstawski,
H. Niewiadomski, P. Nyczyk, and T. Hoefler, “Graph of Thoughts: Solving Elaborate Problems with Large
Language Models,” arXiv e-prints , p. arXiv:2308.09687, Aug. 2023.
[81] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye,
Y . Yang et al. , “Self-refine: Iterative refinement with self-feedback,” arXiv preprint arXiv:2303.17651 ,
2023.
[82] B. Xu, Z. Peng, B. Lei, S. Mukherjee, Y . Liu, and D. Xu, “Rewoo: Decoupling reasoning from observations
for efficient augmented language models,” arXiv preprint arXiv:2305.18323 , 2023.
[83] H. Liu, C. Sferrazza, and P. Abbeel, “Languages are rewards: Hindsight finetuning using human feedback,”
arXiv preprint arXiv:2302.02676 , 2023.
[84] K. Lucas, “open-interpreter,” https://github.com/KillianLucas/open-interpreter, 2023.
[85] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Fun-
towicz et al. , “Huggingface’s transformers: State-of-the-art natural language processing,” arXiv preprint
arXiv:1910.03771 , 2019.
[86] B. Xu, X. Liu, H. Shen, Z. Han, Y . Li, M. Yue, Z. Peng, Y . Liu, Z. Yao, and D. Xu, “Gentopia: A
collaborative platform for tool-augmented llms,” 2023.
[87] Reworkd, “Agentgpt,” https://github.com/reworkd/AgentGPT, 2023.
[88] T. Liang, Z. He, W. Jiao, X. Wang, Y . Wang, R. Wang, Y . Yang, Z. Tu, and S. Shi, “Encouraging divergent
thinking in large language models through multi-agent debate.”
[89] Y . Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch, “Improving factuality and reasoning in
language models through multiagent debate,” 2023.
[90] Y . Nakajima, “babyagi,” https://github.com/yoheinakajima/babyagi, 2023.
[91] C. Qian, X. Cong, W. Liu, C. Yang, W. Chen, Y . Su, Y . Dang, J. Li, J. Xu, D. Li, Z. Liu, and M. Sun,
“Communicative agents for software development,” 2023.
[92] Y . Song, W. Xiong, D. Zhu, W. Wu, H. Qian, M. Song, H. Huang, C. Li, K. Wang, R. Yao, Y . Tian, and
S. Li, “Restgpt: Connecting large language models with real-world restful apis,” 2023.
[93] C. Zhang, K. Yang, S. Hu, Z. Wang, G. Li, Y . Sun, C. Zhang, Z. Zhang, A. Liu, S.-C. Zhu, X. Chang,
J. Zhang, F. Yin, Y . Liang, and Y . Yang, “Proagent: Building proactive cooperative ai with large language
models,” 2023.
[94] Z. Liu, W. Yao, J. Zhang, L. Xue, S. Heinecke, R. Murthy, Y . Feng, Z. Chen, J. C. Niebles, D. Arpit
et al. , “Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents,” arXiv preprint
arXiv:2308.05960 , 2023.
[95] G. Chen, S. Dong, Y . Shu, G. Zhang, S. Jaward, K. Börje, J. Fu, and Y . Shi, “Autoagents: The automatic
agents generation framework,” arXiv preprint , 2023.
[96] Z. Liu, Y . Zhang, P. Li, Y . Liu, and D. Yang, “Dynamic llm-agent network: An llm-agent collaboration
framework with agent team optimization,” 2023.
17

--- PAGE 18 ---
Acknowledgements
The authors would like to express their sincere gratitude to all who contributed to the realisation of
this study. A partnership between the team members sustained the project. Jun Wang conceptualised
the core research thesis and guided the investigation. Jianye Hao guided the project from the technical
and application aspects. Kun Shao assumed the role of project manager, leading the project and
making detailed plans for each division. Haitham Bou-Ammar technically supervised the fine-tuning
pipeline and aided in the paper writing, offering insightful recommendations that enhanced the study.
Filippos Christianos and Georgios Papoudakis were instrumental in architecting the proposed agent
framework. Their contributions were critical in authoring the principal manuscript, developing the
framework, and overseeing the experiments. Matthieu Zimmer and James Doran were dedicated to
fine-tuning methodologies and the RL pipeline. Thomas Coste, Zhihao Wu, Jingxuan Chen, and
Khyati Khandelwal assisted with the development of the framework, the implementation of various
intrinsic methods, and conducted experiments utilising open-source LLMs. Xidong Feng’s efforts
focused on implementing tree-search-based methods and performing experiments related to the GPT
series. Jiacheng Liu executed a comprehensive literature review on frameworks for AI agents. Zheng
Xiong and Yicheng Luo provided support in evaluating the framework. The authors thank Christopher
Mower, Yunfeng Shao, Qiang Ye, Meng Fang, Alexandre Maraval, Antoine Grosnit, Jonas Gonzalez,
Rasul Tutunov, and Nan Zhou for their valuable discussions.
A Decision-Making and Reasoning Methods
In Section 4, we evaluated a variety of both first-order and composite methods across different LLMs.
Here, we provide further details on these methods and how they function, with brief summaries in
Table 7. We also explain additional features which can be used within the framework, such as agent
communication and tool use. A high-level diagram of the Pangu-Agent framework is presented in
Figure 4.
Several of the methods below leverage in-context learning. In-context learning refers to the collection
of methods that utilise analogy-based examples [ 67] to increase the reasoning abilities of LLMs.
Practically, a number of similar tasks along with their solutions are included in the prompt of the
LLM along with other instructions. Note that in-context learning is not a training paradigm as the
pre-trained parameters of the model are not altered. However, by conditioning the text generation
on the in-context examples, the internal activations of the LLM are shaped towards the task at hand,
promoting the emergence of reasoning capabilities. This is a significant advantage compared to
fine-tuning, which allows to specialise an LLM to a task without the need to fine-tune its parameters,
which can be difficult with regards both to computational and data availability considerations.
Method Description
Direct Directly prompt the LLM for answers (zero-shot).
ZS-CoT Zero-Shot, Chain of Thought: Asks the model to think step by step.
FS Few-Shot: In-context examples in which the answer is given directly.
FS-CoT Few-Shot, Chain of Thought: In-context examples with step-by-step thoughts.
FS-CoT-SC Few-Shot, Chain of Thought, Self-Consistency: FS-CoT and checks for consistency in answers.
FS-CoT-React Few-Shot, Chain of Thought, React: Think-then-act React mechanism with FS-CoT examples.
FS-CoT-Reflect Few-Shot, Chain of Thought, Reflect: FS-CoT with an additional reflection step.
FS-CoT-SwiftSage Few-Shot, Chain of Thought, SwiftSage: SwiftSage switching and action buffer with FS-CoT.
FS-Least-to-Most Few-Shot, Least-to-Most: Task decomposition and subgoal answering with FS examples
Table 7: Decision-making and reasoning method summary
A.1 First-order Methods
The initial sequence of tokens used as input to the LLM is called the prompt . LLMs, as auto-
regressive models, condition the generation of each token on the previous tokens. Therefore, the
prompt provided as input to the LLM significantly affects its subsequently generated text. Below we
present the main prompting techniques that were used in the evaluation of Pangu-Agent.
Direct Prompting (Direct): Direct, or zero-shot, prompting is the simplest way to prompt an LLM.
Usually, only a task description and the question at hand are provided. Under direct prompting, no
18

--- PAGE 19 ---
Environment Commands Memory
Language 
ModelsPromptMulti-Agent
Observation
/ FeedbackTasks
• ALFWorld
• WebShop
• GSM8K
• HotPotQnA
• HumanEval
• BabyAI
• MatrixGames
 •••Intrinsic Functions
• Tool-use
• Think
• Reflect/Summarise
• Planning
• Communicate 
 (with other 
 agents)
 •••Extrinsic Functions
• Act
 •••• Trajectory
• Thoughts
• Reflections
• Messages
 •••
• LLaMA
• OpenChat
 ••• • Reflection thought
• System description
• State description
• Few-shot experience
 •••With
RL / Supervised 
Fine TuningUpdate Action
RetrieveFigure 4: The figure above presents a pictorial depiction of the main components of our proposed
agent. On the far left, we can set up a multi-agent environment where each agent can interact with
the environment and communicate with other agents. Each agent can be fine-tuned via reinforcement
or supervised learning. Each of those agents can support any nesting of intrinsic functions ⃗µ(·), such
as tool usage, thinking processes, reflecting, planning and others. Those operate on the memory
component before producing an action to the outer world via extrinsic processes. Our agent also
allows for different prompting strategies and open-source language models, further enabling rigorous
experimentation protocols, see Section 4.
in-context examples are provided to the LLM. It is a simplified variant of the few-shot prompting
technique described below, where it is assumed that zero in-context examples are provided.
Few-Shot Prompting (FS): In FS, several question-answer pairs are added to the prompt. Only
the task/question and the answer are added to the prompt and not the intermediate steps or thoughts
towards the answer. FS has been shown to significantly improve the performance of LLMs in various
downstream tasks without requiring any fine-tuning of their parameters [68].
Few-Shot Chain-of-Thought (FS-CoT) Prompting [ 43]:CoT refers to step-by-step reasoning
through thought generation, eventually leading to the final answer. Several in-context examples are
added to the prompt to enable the LLM to develop such reasoning ability. In contrast to FS alone,
the question-answer pairs added to the prompt contain intermediate reasoning steps. The prompt is
usually augmented with the phrase "think step-by-step". This enables the LLM to follow a similar
reasoning path when the actual question is provided.
Zero-Shot Chain-of-Thought Prompting (ZS-CoT)[ 65]:This technique is used to tap into a
model’s reasoning capabilities by appending the question prompt with "Let’s think step-by-step". No
other context is provided to the agent, and usually, this technique is indifferent towards the task at
hand while also invoking multi-hop reasoning across eclectic tasks with a single template.
A.2 Composite Methods
The first-order methods in the previous part can be combined with more advanced techniques, covered
below. In particular, we define: ‘FS-’ which refers to any variations which include context examples
before prompting the model for an answer (see Few-Shot Prompting) and ‘CoT-’ which elicits
thoughts from the model either through examples when combined with FS (FS-CoT) or simply
by asking it to think step by step (ZS-CoT). We use the term composite methods to refer to the
more advanced methods, which can use first-order prompting techniques and consist of multi-step
prompting.
19

--- PAGE 20 ---
Self-Consistency [ 39]:Self-Consistency (SC) works by repeatedly asking the LLM the same question
and expecting the LLM to generate various reasoning paths. Then, the different answers are checked,
and the most consistent answer is chosen as the final one. The main strategy for deciding the most
consistent answer is majority voting, which chooses the most popular answer.
React [ 40]:React is a two-step prompting mechanism that helps to decompose LLM decision-making
into reasoning traces and concrete actions. The LLM is first prompted to return a reasoning trace, or
thought , relevant to solving the task in the current situation. When the LLM is prompted a second
time, the reasoning trace is appended to the prompt, and the LLM is asked to return a task-specific
action. The reasoning trace provides useful information which can help with commonsense reasoning,
progress tracking, planning, and adaptability, to name a few.
Reflect: In Reflect, the agent is prompted to reflect on its past actions to provide a plan to identify its
mistakes and perform better in the upcoming time steps. Hence, the agent is provided with linguistic
feedback from the LLM itself to improve its actions. This approach is adapted from [ 66]. Our attempt
deviates from the work in [ 66] as we do not maintain a memory across episodes but only provide the
agent with the memory from previous time steps in an episode and the most recent reflection. We
also reflect at every step instead of reflecting only after three incorrect attempts. Finally, we extend
this method to tasks not in the original paper and thus adapt the reflection prompt for these tasks. By
default, reflect does not work for single-step tasks with a simple question-answer format. Hence,
for such tasks, we introduce a zero-step version of Reflect, which prompts the LLM for a temporary
answer and then asks it to reflect on its initial answer before giving a final response.
SwiftSage [ 41]:In this method, two modules are used: Swift, backed by a smaller language model
for quick and intuitive actions, and Sage, backed by a more powerful model for deliberated actions.
The original implementation of this method uses various conditions when switching from Swift to
Sage. Deviating from the initial work, our framework uses Swift until five consecutive time steps
receive a reward of 0. Sage is prompted to create a plan and provide an action buffer when this occurs.
The action buffer is a short list of consecutive actions it believes are the most promising. Actions are
selected in order from the action buffer until it is exhausted, at which point we revert to the Swift
module. If an action buffer step is invalid, it is skipped, and the next one is considered. This method
takes place over multiple time steps and is only relevant for multi-step environments.
Least-to-Most [ 42]:Similar to ReAct, Least-to-Most is another two-step prompting approach, which
asks the LLM first to generate reasoning traces and then produce actions based on the traces. The
difference is that, instead of thoughts, the LLM is first prompted to decompose the question into
several sub-questions. In the second prompt, the LLM is asked to answer all sub-questions and
give a final answer to the original question. Due to context length considerations and our choice of
implementation, we deviate from [ 42] in that all sub-questions are answered in one prompt, and as
such, this method only applies to single-step tasks.
A.3 Additional functions
Communication: An essential feature of any AI agent is the ability to communicate with other agents,
both human and artificial. This enables participation in multi-agent tasks involving several co-existing
agents. LLMs appear to be an ideal tool to enable communication between agents. They are grounded
in human language, which allows for communication with humans and artificial agents using a
standard and universal communication protocol. Pangu-Agent supports pairwise communication
between agents, allowing them to participate in multi-agent tasks.
Tool Use: Augmenting AI agents with the ability to use external tools can further improve their
capacity in many tasks that are hard to solve with LLMs alone, such as mathematical computation
and answering questions about current events. Pangu-Agent supports tool use through prompting
with zero-shot tool descriptions or few-shot tool use examples, which help the agent understand when
a tool should be used and how to call it in the correct format. The tool output can be either integrated
into the agent’s current thought or stored in the agent’s memory for future use. An important example
of tool use is a code interpreter [ 34,35,60], enabling an AI agent to write, run and debug code.
Pangu-Agent can automatically improve code writing to solve various complicated tasks, such as
automated data science by integrating a Python interpreter as a tool and iteratively interacting with it.
20

--- PAGE 21 ---
B Available Tasks
This section describes the tasks included in the codebase and used for evaluation. We categorise these
tasks into two classes based on the number of participating agents: single-agent and multi-agent.
B.1 Single-Agent Tasks
GSM8K [ 52]:This dataset consists of 8.5k grade-school mathematical problems, curated to have
high-quality and diverse language-based problems. It includes natural language solutions instead
of pure mathematical equations for evaluation and prompts the agent to evaluate its reasoning and
problem-solving ability. The agent receives a score of 1 for correctly answered questions, and 0
otherwise. Context examples for FS and CoT are created using the training set of GSM8K, while the
evaluation is performed in the test set, which consists of 1319 questions.
HotpotQA [ 53]:This Wikipedia-based dataset has 113k question-answer pairs that challenge agents
to parse through content and reason over several steps and supporting documents. It is independent
of training data or pre-existing knowledge of the agents, uses additional documents for external
information, and allows the agent to perform multiple steps for reasoning and comprehension. For
instance, it provides a paragraph from Wikipedia about a particular band and asks the model a
question about that band, the answer to which can be inferred by looking up information in the given
paragraph. This environment tests the agent’s language comprehension and reasoning ability. Correct
answers are rewarded with a score of 1, while wrong answers get 0 reward.
ALFWorld [ 44]:ALFWorld has been developed to learn text-based policies from TextWorld, which
proposes a learning environment for text-based games [ 69], and then execute goals from ALFRED,
which is a benchmark for the interpretation of grounded instructions for everyday tasks [ 70]. It is a
benchmark for language understanding, reasoning ability and task execution skills. The ALFWorld
framework aligns text descriptions and commands with physically embodied robotic simulation by
describing the environment, such as "You are in a room with a desk, chair and lamp" and asks the
agent to find a notebook on a desk. Through various actions the agent performs, such as "Go to desk",
it is then supposed to complete its task to be rewarded. A completed task will yield a reward of 1,
while an uncompleted task will not be rewarded.
WebShop [ 54]:WebShop is a simulated e-commerce website with 1.18 million real-world products
and 12,087 crowd-sourced text instructions. [ 54] In the WebShop environment, the agent is given text
instructions for the product environment and asked to navigate different web pages to find, customise,
and purchase an item. The agent can receive observations in either HTML or a text format that strips
most of the metadata in HTML. In response, the agent can search the website (when a search box is
available) or choose from a pre-defined set of actions. An episode in the WebShop environment ends
when the agent selects the buy action. The purchased product is compared with those chosen by a
human demonstrator. The returned reward can range from 0 to 1, depending on the similarity to this
ground truth, as discussed in Yao et al. [54].
HumanEval [ 71]:HumanEval is a dataset created to benchmark the code generation abilities of
language models. It was hand-written by humans, which is quite interesting since the LLMs are thus
unlikely to have seen this data during training. Each problem in the dataset comes with a function
signature, docstring, and unit tests. In the original paper, the authors asked the model to generate k
code samples and considered the task completed if any of the samples were correct. In our setup, we
evaluate the agent on a more difficult task, checking the validity of a single response. This is done by
evaluating the generated function and running the provided unit tests as assert statements. Functions
which run successfully and pass the tests will return a reward of 1, and those that fail will return 0.
BabyAI-Text [ 45,72]:The BabyAI platform consists of several simulated grid-world environments
with instruction-following tasks that have gradually increasing levels of difficulty. The environment
consists of different objects (balls, boxes, keys, etc.) in various colours, and the agent can pick up,
drop or move around objects. The agent is given a 7x7 grid view and textual language instructions for
the next steps at each time step. Carta et al. [72] extended BabyAI to provide a text-based benchmark.
The reward received at the end of the episode is if the agent reached the goal, discounted by the
number of time steps taken to reach it, and 0 otherwise.
21

--- PAGE 22 ---
LLM MethodTask
HumanEval GSM8K Prisoner’s DilemmaGPT-4Direct 68.1±2.1 89.7±0.4 -12.0±0.0
ZS-CoT 64.2±1.7 90.2±0.6 -11.8±0.2
FS 66.5±3.2 90.0±0.7 -
FS-CoT 67.5±1.6 90.2±0.6 -GPT-3.5Direct 28.8±2.8 52.2±0.7 -9.6±0.5
ZS-CoT 30.5±2.6 59.8±0.7 -10.3±0.3
FS 36.6±1.7 53.0±0.4 -
FS-CoT 31.3±1.3 58.7±0.3 -Llama 2-7BDirect 7.0±0.3 25.0±0.5 -12.0±0.0
ZS-CoT 7.7±0.8 24.0±0.5 -12.0±0.0
FS 7.3±1.6 25.7±0.9 -
FS-CoT 5.8±0.6 25.2±1.0 -OpenChat-3.5Direct 23.6±1.8 55.1±1.0 -12.0±0.0
ZS-CoT 19.9±2.7 58.7±0.6 -12.0±0.0
FS 24.2±4.4 55.8±1.1 -
FS-CoT 22.8±3.2 58.3±1.1 -
Table 8: Average achieved returns and the standard deviation across three runs, for four first-order
prompt engineering methods on three different multi-agent tasks, using four pairs of same LLM type.
B.2 Multi-Agent Tasks
Iterated Prisoner’s Dilemma: The iterated Prisoner’s Dilemma is a classic game-theory example of
a non-zero-sum interaction which allows natural language descriptions of self-motivated behaviour in
social dilemmas. In it, two agents simultaneously decide whether to cooperate or defect. In our setup,
if both agents cooperate at a given iteration, they each receive a reward of -4 (years prison sentence);
if they both defect, they each receive a reward of -6; if one defects and the other cooperates, the
reward is 0 and -10 respectively. In the case of mutual defection, the joint outcomes are minimised
[73], while in the case of cooperation, the joint outcomes are maximised. It is a test of altruistic
behaviour as choosing to defect leads to a loss for both parties while choosing to cooperate can benefit
the opposite player. Our Prisoner’s Dilemma task is iterated , with the agents playing the game five
consecutive times while being aware of their partner’s answer at the previous time step.
GSM8K [ 52]:We implement a multi-agent version of the GSM8K task by assigning an external
agent to act as an ‘expert mathematician’ who may receive mathematics doubts or questions from the
agent interacting with the task and is asked to clarify them. Similarly, the interacting agent acts as a
‘mathematics student’ tasked with solving the questions and can seek help from an expert if required.
HumanEval [ 71]:We use HumanEval in a multi-agent setting by assigning an external agent as an
‘expert’ responsible for helping ‘interns’ with their coding tasks. Similarly, the principal agent is
prompted to be an intern, who can seek help from the expert to improve their answers.
C Results on Multi-Agent Tasks
We demonstrate that the Pangu-Agent framework supports multi-agent [ 74] scenarios and tasks by
evaluating three such tasks introduced in Appendix B.2. Table 8 presents the achieved returns for
first-order methods where both agents are of the same model type, while Table 9 presents results
when using different model types. In the latter, GPT-3.5 is always used by the teacher for tasks with a
teacher-student setup.
We observe that using two agents to solve questions in GSM8K and HumanEval reduces returns
compared to those presented in Table 2. This is attributed to the accumulation of error during the
multi-agent communication stage. With an extra communication step, the likelihood of the LLM
making a mistake or getting confused is increased.
22

--- PAGE 23 ---
LLM 1 LLM 2 MethodTask
HumanEval GSM8K Prisoner’s DilemmaGPT-3.5
OpenChat-3.2Direct 4.3±0.5 27.2±1.2 -10.2±0.1
ZS-CoT 3.9±0.6 35.2±0.7 -10.0±0.2
FS 3.9±0.8 30.3±0.4 -
FS-CoT 1.2±1.0 35.9±1.4 -GPT-3.5
Llama 2-7BDirect 10.6±3.5 46.5±0.2 -10.9±0.3
ZS-CoT 9.8±1.6 55.4±0.9 -10.7±0.0
FS 11.8±1.6 46.3±1.1 -
FS-CoT 11.0±2.0 56.1±1.4 -GPT-3.5
OpenChat-3.5Direct 33.6±1.7 53.5±0.5 -10.8±0.1
ZS-CoT 34.2±5.4 59.9±1.3 -10.8±0.1
FS 29.6±3.4 55.7±0.8 -
FS-CoT 31.7±2.2 61.5±0.7 -
Table 9: Average achieved returns and the standard deviation across three runs, for four first-order
prompt engineering methods on three different multi-agent tasks, using three pairs of different LLM
types.
The iterated Prisoner’s Dilemma results show that the framework is capable of supporting multiple
agents interacting with the environment at the same time, as is required by this task. The reward
shown is the average across the five iterated Prisoner’s Dilemma rounds and between both agents.
Most LLM types seem to tend towards a reward of -12, or at least below -10. This indicates that
the agents frequently, or always for some, choose to defect, which is the expected behaviour of any
reasonable agent when the number of iterations is finite and known [75].
D Intrinsic and Composite Functions in the Framework
In this section, we illustrate how simple it is to create, use, and nest intrinsic and composite functions
in Pangu-Agent.
Creating new functions from scratch
New functions can be easily created by defining a new method which defines their behaviour, as
follows:
1class ExampleIntrinsicFunction ( Command ):
2 name : str = " example_intrinsic_function "
3 description : str = "An example intrinsic function ."
4
5 # actions can be defined here ,
6 # such as calling the LLM
7 # or storing information in memory
8 ...
Code Block 1: An example method defining a new intrinsic function.
Using simple first-order functions
Using an existing function is as simple as writing a few lines of configuration. An example is given
for direct prompting, which only uses the Act function:
1main_flow:
2 _target_: pangu . commands . SequentialFlow
3 sequence:
4 - _target_: pangu . commands .Act
5prompt_builder:
23

--- PAGE 24 ---
6 default_kwargs:
7 cot_type: zero_shot
Code Block 2: A configuration defining the Direct prompting method.
Adding Chain of Thought
Chain of Thought (CoT) is controlled by a single variable, such that including CoT in our agent
is as easy as changing the cot_type configuration variable value. Direct prompting can thus be
transformed into ZS-CoT:
1main_flow:
2 _target_: pangu . commands . SequentialFlow
3 sequence:
4 - _target_: pangu . commands .Act
5prompt_builder:
6 default_kwargs:
7 cot_type: zs - cot # change the CoT type
Code Block 3: A configuration defining the ZS-CoT prompting method.
Using composite methods
Composite functions can be used within the agent by nesting functions, such as Reflect before Act.
This is as simple as adding this command to the configuration:
1main_flow:
2 _target_: pangu . commands . SequentialFlow
3 sequence:
4 - _target_: pangu . commands . Reflect # add Reflect
5 - _target_: pangu . commands .Act
6...
Code Block 4: A configuration defining the Reflect method.
Nesting functions
It is even possible to further nest intrinsic functions together, such as Reflect and Think:
1main_flow:
2 _target_: pangu . commands . SequentialFlow
3 sequence:
4 - _target_: pangu . commands . Reflect
5 - _target_: pangu . commands . Think # add Think after Reflect
6 - _target_: pangu . commands .Act
7...
Code Block 5: A configuration defining a combination of the Reflect and React methods.
Creating new composite functions from existing functions
In order to create new composite functions from existing functions, one can simply define a new
function putting the building blocks together. An example of doing this for the previous example of
nested functions follows:
1CompositeFunction = partial (
2 SequentialFlow ,
3 name =" composite_function ",
4 description ="An example composite function .",
5 sequence =[ Reflect () ,Think () , ConsiderAction () , ExecutePlannedAction ()]
6)
Code Block 6: An example method defining a new composite function.
The new composite function can then simply be used by adding it to the configuration as before.
24

--- PAGE 25 ---
Letting the agent choose methods
We can even let the agent decide which of many functions it wants to use by using a DecisionFlow in
the configuration:
1main_flow:
2 _target_: pangu . commands . DecisionFlow # let model choose
3 choices:
4 - _target_: pangu . commands . CompositeFunction # new function
5 - _target_: pangu . commands .Act
6...
Code Block 7: A configuration which lets the agent choose between two methods.
The DecisionFlow presents the list of specified methods to the LLM and the LLM is instructed to
choose a method it believes will help it most. Thus, at every time step, the LLM is able to decide and
change according to which method it will operate. Decision points and sequences of intrinsic actions
can be nested to create complex reasoning steps for the AI agent.
Implementing composite methods
Using the procedures described above, we can define any composite method. For example, for the
Self-Consistency method:
1SelfConsistencyAct = partial (
2 SequentialFlow ,
3 name =" self_consistency_act ",
4 description ="Run CoT multiple times and select the most consistent
answer .",
5 sequence =[
6 ConsiderAction () ,
7 ConsiderAction () ,
8 ConsiderAction () , # as many times as needed
9 ConsistencyOnDiverseActions () , ExecutePlannedAction ()
10 ]
11)
Code Block 8: A method definition for Self-Consistency.
First, ninstances of ConsiderAction are called to generate nanswers to a same prompt. Then
ConsistencyOnDiverseActions is used to select the most consistent answer. Finally, the answer
is executed within the environment using ExecutePlannedAction .
We also show how we can define a method resembling ReAct by defining a configuration file, using
our framework. In the ReAct method, the language model is optionally asked to perform a distinct
thinking step before returning an answer. We can implement this using a DecisionFlow as follows:
1main_flow:
2 _target_: pangu . commands . DecisionFlow
3 choices:
4 - _target_: pangu . commands . SequentialFlow
5 sequence:
6 - _target_: pangu . commands . Think
7 - _target_: pangu . commands .Act
8 - _target_: pangu . commands .Act
Code Block 9: A configuration defining our version of ReAct.
At every time step, the LLM is asked whether it wants to perform a distinct thinking step before
acting, or whether it wants to act directly.
E Results on Search-enhanced Planning
We present the results of planning-enhanced LLM in Table 10. We choose two models - GPT-3.5 and
task-tuned Llama 2-7B with trained value function (obtained from [ 51]) and test them on Game24
25

--- PAGE 26 ---
Table 10: Accuracy on the Game24/GSM8K environment for different tree-search methods
Env Model ZS-CoT FS FS-CoT BFS DFS MCTS
Game24 GPT-3.5 [49] - 5.0 1.0 10.0 3.0 5.0
Game24 Llama 2-7B-tuned [51] 30.0 - - 72.0 74.0 73.0
GSM8K GPT-3.5 [49] 69.2 35.0 66.4 64.1 61.4 65.5
GSM8K Llama 2-7B-tuned [51] 41.4 - - 54.4 53.7 53.0
[49] and GSM8K. Based on the table, we mainly have three conclusions. Firstly, the LLM’s basic task-
solving ability (indicated by zero-shot, few-shot or few-shot CoT performance) largely determines the
performance of different planning methods. Stronger base models (the Llama 2-7B-SFT in Game24
and the GPT-3.5 in GSM8K) lead to better tree-search enhanced generation. Our second and third
conclusion aligns with the findings shown in [ 51]. Secondly, we find that for small-scale search
problems presented in the table, different search algorithms seem to behave similarly. This conclusion
aligns with the findings of [ 51]. Our final finding is that planning-enhanced generation has relatively
limited improvements for GPT-3.5 and we even observe the performance drop. We believe this is
because of GPT-3.5’s weak evaluation ability and this phenomenon also aligns with other papers
studying LLM’s limited self-evaluation ability [ 51,76,77]. When we have a stronger evaluator, such
as a task-tuned value function (used in Llama 2-7B-tuned in our setting) or GPT-4 with well-designed
prompts (used in ToT [49]), we can still obtain performance gain compared to other baselines.
F Related Work
In this appendix, we provide a brief review of relevant prior works about LLM agent frameworks,
readers can refer to table 1 for more intuitive comparisons between some well-known LLM agent
frameworks.
F.1 Single-Agent Frameworks
The rapid advancement in the generalistic capabilities of LLMs has catalysed the development of
more complex and capable LLM-based agents, across both text-based and multimodal environments.
A range of proof-of-concept works have demonstrated the reasoning and planning abilities of LLMs.
Examples such as Chain-of-Thought [ 43], Self Consistency [ 39], Tree of Thoughts [ 49,78], and
Graph of Thoughts [ 79,80] exhibit models’ aptitude for structured conceptual exploration. Other
efforts emphasise the iterative self-improvement capacities of LLMs, including Self-Refine [ 81],
ReAct [ 40], ReWOO [ 82], Reflexion [ 66], and Chain-of-Hindsight [ 83]. These works provide
evidence that LLMs have the basic underlying capacity to support reasoning, planning, memory, and
iterative self-improvement needed to form capable agents.
Other efforts have focused on developing LLM agent frameworks to tackle more complex settings.
Single-agent frameworks incorporate the LLM as a central controller alongside other modular
components. For example, the GPT-4[ 60] model on the OpenAI platform can serve as a personal
assistant agent, leveraging plugins, a code interpreter, and web integration to operate in closed and
open environments. Open Interpreter [ 84] and OpenAgents [ 34] are open-source implementations
which try to emulate the agent structure on the OpenAI platform. Transformer Agents[ 85] and
LangChain [ 33] are open-source repositories that have been developed to help developers build a
single LLM agent more easily with built-in functionalities. AutoGPT [ 28] focuses on utilising LLMs
for achieving flexible goals within a single-agent structure. Other proposed single-agent frameworks
such as Gentopia [ 86] and AgentGPT[ 87] follow similar ideas. In contrast to these, the Pangu-Agent
framework, first, proposes and builds upon a novel optimisation objective which offers modularity
and flexible design, and second, allows fine-tuning the intrinsic and extrinsic functions to improve the
achieved returns of the agent. A comparison between Pangu-Agent and the remaining frameworks is
summarised in Table 1.
26

--- PAGE 27 ---
F.2 Multi-Agent Frameworks
To unlock more advanced features which might be required for complex tasks, multi-agent frame-
works have been studied. While preliminary works like Camel [ 37], Multi-Agent Debate [ 88,89],
BabyAGI [ 90], CHATDEV [ 91], MetaGPT [ 36], RestGPT[ 92], and ProAgent [ 93] demonstrate the
potential for multiple LLMs to collaborate on challenges difficult for a single agent, their inter-agent
communication patterns are relatively fixed.
Recent work has focused on making multi-agent communication more flexible. BOLAA [ 94] utilises
a controller module to manage communication between "labor" agents. AgentVerse [ 29] combines
expert recruitment with collaborative decision-making to adapt to different tasks. AutoAgents [ 95]
employs observer agents to enhance recruitment, collaboration, and action execution process. Dy-
lan [96] introduces a dynamic multi-agent architecture and automatic agent team optimization. Some
frameworks further emphasise customisability and extensibility. Configurability is a shared feature
among these frameworks. AGENTS [ 38] uses configurable symbolic plans called Standard Operating
Procedures to control agent behaviours. AutoGen [ 27] focuses on customisable conversable agents
and conversation programming to enable flexible conversation patterns for collaborative reasoning
between agents.
27
