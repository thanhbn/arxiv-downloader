# 2309.15028.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2309.15028.pdf
# Kích thước tệp: 1828739 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Bản thảo
Đừng vứt bỏ mô hình giá trị của bạn!
Tạo ra văn bản ưa thích hơn với việc giải mã Tìm kiếm Cây Monte-Carlo có hướng dẫn giá trị
Jiacheng Liu♡♣∗Andrew Cohen♣Ramakanth Pasunuru♣
Yejin Choi♡♠Hannaneh Hajishirzi♡♠Asli Celikyilmaz♣
♡Paul G. Allen School of Computer Science & Engineering, University of Washington
♣FAIR, Meta♠Allen Institute for Artificial Intelligence
liujc@cs.washington.edu∗Công việc được thực hiện với tư cách là nghiên cứu viên thăm quan tại FAIR, Meta.
Tóm tắt
Các thuật toán tìm kiếm thời gian suy luận như Tìm kiếm Cây Monte-Carlo (MCTS) có thể
có vẻ không cần thiết khi tạo văn bản ngôn ngữ tự nhiên dựa trên học tăng cường tiên tiến
như Tối ưu hóa Chính sách Proximal (PPO). Trong bài báo này,
chúng tôi chứng minh rằng có thể thu được lợi ích thêm từ PPO bằng cách tích hợp
MCTS lên trên. Ý tưởng chính là không vứt bỏ mạng giá trị, một sản phẩm phụ của
quá trình huấn luyện PPO để đánh giá các chuỗi đầu ra một phần, khi giải mã văn bản từ
mạng chính sách. Cụ thể hơn, chúng tôi trình bày một thuật toán giải mã có hướng dẫn giá trị
mới được gọi là PPO-MCTS, có thể tích hợp mạng giá trị từ PPO
để hoạt động chặt chẽ với mạng chính sách trong quá trình tạo thời gian suy luận. So
sánh với các phương pháp trước đây dựa trên MCTS cho việc tạo văn bản có kiểm soát, điểm mạnh
chính của phương pháp của chúng tôi là giảm sự không khớp cơ bản của các cơ chế tính điểm
của các đầu ra một phần giữa huấn luyện và kiểm tra. Đánh giá trên bốn
nhiệm vụ tạo văn bản cho thấy PPO-MCTS cải thiện đáng kể tính ưa thích
của văn bản được tạo so với thực hành tiêu chuẩn chỉ sử dụng chính sách PPO. Kết quả của chúng tôi chứng minh tiềm năng của các thuật toán tìm kiếm ngay cả trên
các mô hình ngôn ngữ được căn chỉnh từ PPO, và lợi ích chưa được khám phá của
mạng giá trị.
Hình 1: Các phương pháp giải mã văn bản. Trái: giải mã tham lam từ chính sách PPO không thỏa mãn ràng buộc nhiệm vụ. Phải: PPO-MCTS với cùng chính sách PPO và mô hình giá trị bổ sung thỏa mãn ràng buộc nhiệm vụ.
p là xác suất tiên nghiệm được đưa ra bởi mô hình chính sách PPO; V và Q được rút ra từ đầu ra của mô hình giá trị PPO. (Được mô tả là cây tìm kiếm Monte-Carlo cuối cùng để giải mã token đầu tiên. Chuỗi đầy đủ được
giải mã bằng cách lặp lại quá trình này.)
1arXiv:2309.15028v3  [cs.CL]  2 Apr 2024

--- TRANG 2 ---
Bản thảo
1 Giới thiệu
Nhiều thuật toán giải mã văn bản đã được giới thiệu để cải thiện khả năng kiểm soát và căn chỉnh con người
của việc tạo văn bản bằng các mô hình ngôn ngữ (LM). Giải mã có hướng dẫn (Lu et al., 2020; Welleck
et al., 2022, inter alia), trong đó LM tạo sinh được điều hướng bởi một hàm đánh giá phụ trợ
nắm bắt sự thỏa mãn mục tiêu bằng các đầu ra một phần, đã được đặc biệt quan tâm, vì khung này
có thể được kết hợp với các thuật toán tìm kiếm để đạt được kết quả mong muốn hơn. Tuy nhiên, trong hầu hết
các công trình trước đây, việc lựa chọn hàm đánh giá có thể không tối ưu, dựa vào các heuristic dựa trên quy tắc hoặc
mượn các mô hình phân loại không được huấn luyện để đánh giá các chuỗi không đầy đủ.

Quan sát chính của chúng tôi là mô hình giá trị được tạo ra từ Tối ưu hóa Chính sách Proximal (PPO)
(Schulman et al., 2017) là một ứng viên tự nhiên cho hàm đánh giá trong giải mã có hướng dẫn. PPO
đã trở thành thuật toán de facto để căn chỉnh các mô hình ngôn ngữ với phản hồi của con người bằng học tăng
cường (RLHF) (Ouyang et al., 2022; OpenAI, 2022; 2023; Bai et al., 2022; Touvron et al.,
2023b). Để huấn luyện mô hình chính sách, PPO cũng học thêm một mô hình giá trị ước tính
lợi nhuận kỳ vọng của các đầu ra một phần khi tuân theo chính sách hiện tại. Do đó, mô hình giá trị này
được thiết kế để đánh giá các chuỗi không đầy đủ. Tuy nhiên, các nhà nghiên cứu AI và thực hành thường
loại bỏ các checkpoint mô hình giá trị khi lưu hoặc phát hành các mô hình PPO của họ, để lại tính hữu ích
của các mô hình giá trị như vậy chưa được khám phá. Việc không sử dụng thông tin được học bởi mô hình giá trị
giới hạn hiệu suất của PPO. Ví dụ, trong các thí nghiệm của chúng tôi, chúng tôi quan sát thấy rằng việc giải mã chỉ
từ mô hình chính sách PPO có thể tạo ra các đầu ra không mong muốn (ví dụ, Hình 1).

Chúng tôi đề xuất áp dụng giải mã có hướng dẫn trên các LM được huấn luyện với PPO, sử dụng mô hình giá trị liên quan
làm hàm đánh giá. Mô hình giá trị ước tính lợi nhuận kỳ vọng cho một chuỗi một phần để
được hoàn thành bằng cách tuân theo chính sách liên quan, và như chúng tôi sẽ chỉ ra sau này, nó phù hợp để hướng dẫn
giải mã vì: (1) nó được huấn luyện để dự đoán giá trị cho các chuỗi một phần; (2) nó được
thiết kế riêng cho chính sách liên quan. Cụ thể, chúng tôi đề xuất sử dụng Tìm kiếm Cây Monte-Carlo (MCTS)
(Kocsis & Szepesvari, 2006) để tìm kiếm các chuỗi đầu ra có phần thưởng cao hơn. MCTS được chứng minh
là một thành phần thời gian suy luận không thể thiếu giúp chuỗi AlphaGo đạt hiệu suất siêu nhân
trên Go (Silver et al., 2016; 2017), và gần đây được sử dụng để điều hướng các LM (Zhao et al.,
2023b; Hao et al., 2023; Chaffin et al., 2021). Trong công trình này, chúng tôi áp dụng giải mã có hướng dẫn (MCTS,
cụ thể) trên các cặp mô hình chính sách/giá trị được huấn luyện với PPO. Chúng tôi gọi phương pháp giải mã của chúng tôi là
PPO-MCTS.

Hình 1 cho thấy một ví dụ về việc áp dụng PPO-MCTS trên một nhiệm vụ tạo văn bản trong đó việc giải mã từ
chính sách PPO một mình thất bại trong nhiệm vụ. Mục tiêu là tạo ra một sự tiếp tục cảm xúc tích cực
cho một gợi ý cảm xúc tiêu cực được cho, "You can't fix one dangerous situation with one..." Chính sách PPO chỉ ra "bad" là token có khả năng nhất tiếp theo, và việc tuân theo chính sách đó tạo ra một
sự tiếp tục cảm xúc tiêu cực, thất bại trong nhiệm vụ. Tuy nhiên, với PPO-MCTS, một cây tìm kiếm được xây dựng và
các token được giải mã dựa trên thống kê thu được từ việc đánh giá các chuỗi với một vài bước
nhìn trước, và do đó "person" trở thành token có khả năng nhất tiếp theo. Việc tuân theo thuật toán này
thành công tạo ra một sự tiếp tục cảm xúc tích cực.

Để thành công kết hợp các mô hình PPO và giải mã MCTS, chúng tôi giới thiệu một sửa đổi mới và quan trọng
cho thuật toán MCTS ban đầu: khởi tạo Q của các hành động con từ V của
nút cha của chúng để khuyến khích khám phá. Chúng tôi cũng trình bày các kỹ thuật xấp xỉ khi
nhiệm vụ hoặc thiết lập thí nghiệm không cho phép tính toán chính xác. Chúng tôi cũng thảo luận về một số lựa chọn triển khai
trong huấn luyện PPO và ý nghĩa của chúng đối với sự cần thiết của một số xấp xỉ nhất định. Nhiều
chi tiết có thể được tìm thấy trong §3.

Các thí nghiệm trên bốn nhiệm vụ tạo văn bản cho thấy PPO-MCTS tạo ra văn bản với tính ưa thích cao hơn
so với giải mã tiêu chuẩn (ví dụ, lấy mẫu top-p (Holtzman et al., 2019)):

1. Về điều hướng cảm xúc (sử dụng tập dữ liệu OpenWebText (Gokaslan & Cohen, 2019)), PPO-
MCTS đạt được tỷ lệ thành công cao hơn 30% (tuyệt đối) so với việc lấy mẫu trực tiếp từ
cùng chính sách PPO, trong khi duy trì các thuộc tính mong muốn khác như độ trôi chảy, đa dạng và
tính chủ đề. Đánh giá của con người ủng hộ PPO-MCTS hơn chính sách PPO với tỷ lệ 20% (tuyệt đối).

2. Về giảm độc tính (sử dụng tập dữ liệu RealToxicityPrompts (Gehman et al., 2020)), PPO-
MCTS giảm độc tính của văn bản được tạo ra 34% (tương đối). Đánh giá của con người ủng hộ PPO-
MCTS hơn chính sách PPO với tỷ lệ 30% (tương đối).

2

--- TRANG 3 ---
Bản thảo
3. Về nội quan kiến thức (được đánh giá trên một số benchmark QA), PPO-MCTS tạo ra
kiến thức thường thức hữu ích hơn 12% (tương đối) cho các nhiệm vụ QA hạ nguồn.
4. Về tạo chatbot hữu ích và vô hại (sử dụng tập dữ liệu HH-RLHF (Bai et al., 2022)),
PPO-MCTS tạo ra các phản hồi đối thoại với tỷ lệ thắng cao hơn 5% (tuyệt đối) trong đánh giá của con người.

Kết quả thực nghiệm của chúng tôi chứng minh rằng các chính sách được huấn luyện PPO có thể hưởng lợi từ giải mã có hướng dẫn, và
rằng mô hình giá trị PPO vừa có cơ sở lý thuyết vừa hiệu quả thực nghiệm trong việc hướng dẫn
tìm kiếm trong MCTS. Chúng tôi cũng cho thấy rằng PPO-MCTS vượt trội hơn việc huấn luyện PPO lâu hơn hoặc
sử dụng giải mã best-of-n, cả hai đều tối ưu hóa trực tiếp cho phần thưởng. Kết quả của chúng tôi mở khóa
tiềm năng có giá trị của các mô hình giá trị, và chúng tôi khuyến nghị cộng đồng xem xét việc lưu
mô hình giá trị để tăng cường suy luận.

2 Kiến thức cơ bản
Trong phần này, chúng tôi giới thiệu một số kiến thức cơ bản hỗ trợ thảo luận về phương pháp của chúng tôi, bao gồm
các ký hiệu phổ biến trong tạo văn bản, giải mã có hướng dẫn, và Tối ưu hóa Chính sách Proximal.

2.1 Ký hiệu
Trong công thức phổ biến nhất của tạo văn bản, một LM ước tính phân phối xác suất của
token tiếp theo xt cho một ngữ cảnh trước x<t, pθ(xt|x<t), và giải mã token tiếp theo bằng cách tham chiếu
xác suất này. Các thuật toán giải mã phổ biến bao gồm giải mã tham lam, tìm kiếm chùm, lấy mẫu nhiệt độ,
và lấy mẫu top-p (Holtzman et al., 2019). Các nhiệm vụ tạo văn bản thường cung cấp một
gợi ý w và yêu cầu một sự tiếp tục x1..T, trong trường hợp này LM mô hình pθ(xt|w,x<t).

Khi tạo văn bản được xem như một vấn đề ra quyết định tuần tự (Puterman, 1994), một trạng thái là
gợi ý cộng với phản hồi được tạo ra cho đến nay, và một hành động là token được tạo tiếp theo. Chính thức,
st = (w,x<t) và at = xt. LM được gọi là một mô hình chính sách pθ(at|st).

Trong tạo văn bản có kiểm soát, mục tiêu thường được đặc trưng bởi một hàm phần thưởng r(sT+1), trong đó
sT+1 = (w,x1..T) là văn bản được tạo hoàn chỉnh. Hàm này mã hóa các thuộc tính ưa thích của
văn bản được tạo, như cảm xúc mong muốn, độc tính thấp, ràng buộc từ vựng, và các loại
sở thích của con người khác. Hàm phần thưởng có thể là dựa trên quy tắc hoặc được mô hình hóa với một bộ phân loại thần kinh
hoặc bộ hồi quy. Một số phương pháp nhất định, bao gồm các phương pháp giải mã có hướng dẫn mà chúng tôi mô tả dưới đây, có thể
hướng dẫn giải mã văn bản để tăng phần thưởng của đầu ra.

2.2 Giải mã có hướng dẫn
Giải mã có hướng dẫn sử dụng một hàm đánh giá để đánh giá các trạng thái với đầu ra một phần, st =
(w,x<t), đối với một mục tiêu nào đó. Như với hàm phần thưởng, hàm đánh giá có thể là dựa trên quy tắc
(ví dụ, Lu et al., 2020; Welleck et al., 2022) hoặc được mô hình hóa với một bộ phân loại thần kinh hoặc bộ hồi
quy (ví dụ, Chaffin et al., 2021; Yao et al., 2023). Hàm đánh giá này có thể được kết hợp với
LM chính sách để hướng dẫn giải mã, thường được bổ sung với một thuật toán tìm kiếm để cho phép
một số nhìn trước và lập kế hoạch tầm xa (ví dụ, Lu et al., 2021; Chaffin et al., 2021). Tìm kiếm Cây Monte-
Carlo (MCTS) là một trong những thuật toán tìm kiếm hiệu quả nhất (Silver et al., 2016).

2.3 Tối ưu hóa Chính sách Proximal (PPO)
PPO (Schulman et al., 2017) là một thuật toán RL để tối ưu hóa một chính sách đối với một hàm phần thưởng cho trước.
PPO giả định một hàm phần thưởng tính toán phần thưởng r(sT+1) từ trạng thái cuối
sT+1 (tức là, chuỗi hoàn chỉnh). Phần thưởng này được gán cho phần thưởng cấp bước của bước cuối cùng,
rT, trong khi phần thưởng cấp bước cho các bước không cuối được khởi tạo bằng không. Hơn nữa, mỗi phần thưởng cấp bước
được phạt với một thuật ngữ phân kỳ KL, −βlogpθ(at|st)/pθ0(at|st), trong đó θ0 là chính sách tham chiếu
(thường là chính sách được khởi tạo của quá trình huấn luyện PPO), và β là một siêu tham số. Do đó, phần thưởng cấp bước

3

--- TRANG 4 ---
Bản thảo
Giai đoạn 1: chọn
Giai đoạn 2: mở rộng
Giai đoạn 3: đánh giá
Giai đoạn 4: sao lưu
Hình 2: Bốn giai đoạn của một mô phỏng trong MCTS. Lưu ý: chúng tôi hiển thị số lần truy cập nút
N(s) trên cạnh cha của nó dưới dạng số "chân" (ví dụ, trong token xấu trong giai đoạn sao lưu).

rt = {
    −βlogpθ(at|st)/pθ0(at|st) + r(sT+1) (khi t = T)
    −βlogpθ(at|st)/pθ0(at|st) (khi 1 ≤ t < T)  (1)

PPO đồng thời huấn luyện hai mô hình: một mô hình chính sách pθ(at|st), và một mô hình giá trị Vϕ(st), cả hai
được tham số hóa với các mạng thần kinh. Mục tiêu học PPO bao gồm hai phần: một mục tiêu chính sách
và một mục tiêu giá trị. Mục tiêu chính sách cố gắng tối đa hóa một mục tiêu thay thế
chứa một hàm lợi thế, và để tính toán lợi thế chúng ta cần hàm giá trị được
ước tính bởi mô hình giá trị. Mục tiêu giá trị cố gắng tối thiểu hóa lỗi ước tính giá trị
đối với lợi nhuận thực nghiệm,

Gt = ∑(t'=t to T) γ^(t'-t) rt', (2)

trong đó γ là hệ số chiết khấu. Để biết thêm chi tiết về các mục tiêu học PPO, xem §A.2.

Thực hành hiện tại của việc giải mã từ các mô hình PPO. Sau khi huấn luyện PPO, hầu hết các nhà nghiên cứu và
kỹ sư chỉ triển khai mô hình chính sách kết quả, và giải mã với các phương pháp đơn giản như giải mã tham lam
hoặc lấy mẫu top-p (§2.1). Mô hình giá trị liên quan không thể được sử dụng, vì các checkpoint của nó
hoặc không được lưu hoặc không được phát hành bởi các nhà huấn luyện mô hình. Quan sát của chúng tôi là mô hình giá trị này
là một ứng viên rất phù hợp cho hàm đánh giá trong giải mã có hướng dẫn.

3 Phương pháp
Áp dụng giải mã MCTS trên các mô hình được huấn luyện PPO cho phép tìm kiếm có hệ thống và nhìn trước
trong không gian giải mã. Phần này mô tả cách chúng tôi thực tế áp dụng giải mã MCTS trên
các mô hình chính sách và giá trị được huấn luyện PPO. Chúng tôi mở rộng thuật toán MCTS tiêu chuẩn để phù hợp với
thiết lập tạo văn bản và PPO (phần này), và trong Phụ lục, chúng tôi thảo luận về một số kỹ thuật xấp xỉ
khi một số tiêu chí không được đáp ứng (§A.4) và một số lựa chọn triển khai chi tiết trong
PPO có thể khiến những xấp xỉ này trở nên cần thiết (§A.5).

Mục tiêu của MCTS là tìm các chuỗi đầu ra có phần thưởng cao, sử dụng mô hình chính sách và hàm đánh giá
làm hướng dẫn. Chính sách cung cấp các đề xuất ban đầu về các hành động hứa hẹn, và hàm đánh
giá chấm điểm các chuỗi một phần được đề xuất bởi chính sách. Chúng ta muốn đánh giá thường xuyên hơn
các chuỗi một phần xuất phát từ các hành động hứa hẹn hơn để có được các đánh giá chính xác hơn
cho những hành động này (tức là, khai thác), trong khi chúng ta cũng muốn đảm bảo đánh giá một số hành động
ít hứa hẹn hơn để giảm khả năng bỏ lỡ các chuỗi có phần thưởng cao (tức là, khám phá).
MCTS cung cấp một heuristic khám phá và đánh giá hiệu quả dựa trên tìm kiếm cây.

Trong PPO-MCTS, việc giải mã MCTS hoạt động với mô hình chính sách và giá trị được huấn luyện bởi PPO. Để
giải mã mỗi token, MCTS xây dựng một cây tìm kiếm trong khi chạy một số S mô phỏng. Trong
cây tìm kiếm, mỗi nút đại diện cho một trạng thái s và mỗi cạnh đại diện cho một hành động a. Đối với mỗi nút s,
chúng ta duy trì hai biến: một số lần truy cập N(s) (cuối cùng được sử dụng để giải mã token) và một
giá trị trung bình V̄(s); đối với mỗi cạnh (s,a), chúng ta theo dõi giá trị của hàm Q Q(s,a). Mỗi
mô phỏng bao gồm bốn giai đoạn (được minh họa trong Hình 2):

4

--- TRANG 5 ---
Bản thảo
1. Chọn: Chọn một nút chưa được khám phá trong cây. Vì chúng ta giải mã token dựa trên số lần truy cập,
chúng ta muốn trả nhiều lần truy cập hơn cho các nút cho thấy triển vọng của giá trị cao để chúng ta có thể thu được
các ước tính giá trị chính xác hơn cho chúng, trong khi chúng ta cũng muốn đảm bảo khám phá các nút
ít được truy cập. Chúng ta cân bằng khám phá và khai thác bằng thuật toán PUCT (Rosin, 2011): Bắt đầu
từ nút gốc, chúng ta chọn một hành động và di chuyển đến nút con, và lặp lại điều này cho đến khi chúng ta đạt đến
một nút chưa được khám phá. Giả sử hiện tại chúng ta đang ở nút s. Hành động a* được chọn theo
công thức sau, ưu tiên giá trị hàm Q cao trong khi giảm số lần truy cập cao:

a* = arg max_a Q(s,a) + c_puct · pθ(a|s) √(N(s))/(1+N(s'))   (3)

trong đó s' là trạng thái sau khi thực hiện hành động a từ nút s, pθ(a|s) là tiên nghiệm chính sách PPO, Q(s,a) là
hàm Q cho cạnh (s,a) và được rút ra từ các đầu ra của mô hình giá trị PPO (xem công thức
trong giai đoạn sao lưu dưới đây), N(s) là số lần truy cập nút s, V̄(s) là giá trị trung bình được tạo ra
từ việc đánh giá các nút trong cây con của nút s (bao gồm) (xem công thức trong giai đoạn sao lưu dưới đây),
và c_puct là một siêu tham số.

Lưu ý rằng trong PPO, các bước trung gian bị phạt với một thuật ngữ KL (Phương trình 1), và có một
hệ số chiết khấu γ khi tính toán lợi nhuận (Phương trình 2). Để nắm bắt những điều này, chúng ta sử dụng hàm Q
(thay vì V̄(s) như trong Silver et al. (2017)) trong Phương trình 3, phù hợp với
công thức ban đầu của MCTS (Kocsis & Szepesvari, 2006).

2. Mở rộng: Nút được chọn s* được mở rộng và đánh dấu là đã khám phá. Phân phối chính sách tiên nghiệm
pθ(·|s*) được tính toán cho nút này, và các hành động với các tiên nghiệm top-k mỗi cái được liên kết với một
nút con mới, chưa được khám phá. V̄ của các nút con được khởi tạo bằng không.

3. Đánh giá: Hàm giá trị của nút s* được đánh giá, sử dụng mô hình giá trị PPO: V(s*) =
Vϕ(s*). Nếu s* là một trạng thái cuối, phần thưởng cuối r(sT+1) được sử dụng. Chúng ta khởi tạo số lần truy cập của s*
là một, và giá trị trung bình của s* với đầu ra của mô hình giá trị này, tức là, N(s*) ← 1, V̄(s*) ← V(s*).

Theo Silver et al. (2017) và Chaffin et al. (2021), chúng ta không thực hiện rollout Monte-Carlo do
các cân nhắc về hiệu quả.

Chúng ta cũng khởi tạo hàm Q của các cạnh con của s* với đầu ra của mô hình giá trị này, tức là,
Q(s*,a) ← V(s*), ∀a. Điều này tương phản với thuật toán MCTS tiêu chuẩn trong đó V̄ (hoặc trong trường hợp của chúng ta,
Q) của con của các nút mới được khám phá được khởi tạo bằng không. Chúng ta gọi thay đổi này là
khởi tạo Q với V. Chúng ta thực hiện thay đổi này vì với các mô hình PPO, Q có thể có
quy mô khá lớn (theo thứ tự 10s), do các lý do sẽ được giải thích trong §A.5. Trong các thí nghiệm sớm,
chúng ta thấy rằng điều này có thể nghiêm trọng ngăn chặn việc khám phá trong tìm kiếm cây, khiến nó
suy thoái thành giải mã tham lam.

4. Sao lưu: Cập nhật số lần truy cập N(·) và giá trị trung bình V̄(·) cho tất cả các nút, và hàm Q
Q(·,·) cho tất cả các cạnh, trên đường dẫn từ nút s* trở lại nút gốc st. Việc cập nhật được thực hiện
từ dưới lên, với các quy tắc cập nhật:

Q(s,ã) ← r(s,ã) + γV̄(s̃),   (4)
V̄(s) ← ∑_a N(s')Q(s,a) / ∑_a N(s'),   (5)
N(s) ← N(s) + 1,   (6)

trong đó s là một nút trên đường dẫn và s̃ là con của nó (cũng trên đường dẫn) sau khi thực hiện hành động ã;
s' là nút con của trạng thái s sau khi thực hiện một hành động a; r(s,a) là phần thưởng cấp bước được định nghĩa trong Phương trình 1.

Trước các mô phỏng, nút gốc st được khởi tạo với các giai đoạn mở rộng và đánh giá. Sau các
mô phỏng, một hành động được giải mã từ số lần truy cập được chuẩn hóa của con của nút gốc:

p(at|st) ∝ N(st,at)^(1/τd),   (7)

trong đó τd là một siêu tham số nhiệt độ. Từ phân phối này, chúng ta có thể lấy mẫu (tùy chọn
với lấy mẫu top-p) hoặc giải mã tham lam (tức là, τd → 0).

Cấm mở rộng nút sau các trạng thái cuối. Việc tạo văn bản thường dừng lại ở một token cuối,
[EOS]. Khi hành động là [EOS], nút con được gọi là nút cuối. Việc mở rộng nút
sau các trạng thái cuối nên được cấm, vì việc đánh giá trên các trạng thái sau một nút cuối có
hành vi không xác định. Để duy trì số lần truy cập thích hợp cho đến nút cuối, khi gặp một
nút cuối trong giai đoạn chọn, chúng ta không nên dừng mô phỏng sớm, mà nhảy trực tiếp đến
giai đoạn sao lưu.

5

--- TRANG 6 ---
Bản thảo
4 Thiết lập thí nghiệm
Chúng tôi áp dụng phương pháp giải mã PPO-MCTS của chúng tôi trên bốn nhiệm vụ tạo văn bản: điều hướng cảm xúc,
giảm độc tính, nội quan kiến thức, và tạo chatbot hữu ích và vô hại. Chi tiết thí nghiệm bổ sung
có thể được tìm thấy trong §B.

Nhiệm vụ 1: điều hướng cảm xúc. PPO đã được sử dụng trước đây để kiểm soát LM hướng tới việc tạo
văn bản với một cảm xúc cụ thể (tích cực hoặc tiêu cực), và chúng tôi áp dụng giải mã MCTS trên các
mô hình được huấn luyện PPO này để cải thiện thêm tỷ lệ thành công của việc điều hướng cảm xúc (tức là, thỏa mãn
mục tiêu). Chúng tôi tuân theo thiết lập thí nghiệm trong Lu et al. (2022), trong đó nhiệm vụ là tạo ra các sự tiếp tục cảm xúc tích cực
cho các gợi ý cảm xúc tiêu cực (và ngược lại, các sự tiếp tục tiêu cực cho
các gợi ý tích cực) trong tập dữ liệu OpenWebText (Gokaslan & Cohen, 2019).

Nhiệm vụ 2: giảm độc tính. PPO đã được sử dụng trước đây để điều hướng LM hướng tới việc tạo
văn bản ít độc hại hơn, và chúng tôi áp dụng giải mã MCTS trên các mô hình được huấn luyện PPO này để
giảm thêm độc tính. Chúng tôi tuân theo thiết lập thí nghiệm trong Lu et al. (2022), trong đó nhiệm vụ là tạo ra các sự tiếp tục
ít độc hại hơn, nhưng trôi chảy và đa dạng, cho các gợi ý trong tập dữ liệu RealToxicityPrompts (Gehman
et al., 2020).

Nhiệm vụ 3: nội quan kiến thức. Đối với việc trả lời câu hỏi thường thức (QA), các công trình trước đây
đã thấy hữu ích khi đầu tiên yêu cầu LM nội quan về kiến thức thường thức liên quan và sau đó
căn cứ dự đoán của mô hình QA trong kiến thức đã nội quan (Liu et al., 2021; 2022). Mô hình
nội quan kiến thức lấy câu hỏi làm đầu vào và tạo ra một tuyên bố kiến thức, và mô hình này có thể được huấn luyện với PPO để tối đa hóa tính hữu ích của kiến thức được tạo ra trên
mô hình QA hạ nguồn (Liu et al., 2022). Chúng tôi áp dụng giải mã MCTS trên các mô hình
nội quan kiến thức được huấn luyện PPO để tăng thêm tính hữu ích của kiến thức được giải mã và do đó cải thiện
hiệu suất QA hạ nguồn.

Nhiệm vụ 4: chatbot hữu ích và vô hại. Trong nhiệm vụ này, chúng tôi điều tra mức độ hiệu quả của PPO-MCTS
trong việc căn chỉnh LM với sở thích của con người. Chúng tôi sử dụng dữ liệu tính hữu ích và vô hại từ
tập dữ liệu HH-RLHF của Anthropic (Bai et al., 2022).

5 Kết quả
Trên tất cả bốn nhiệm vụ, PPO-MCTS đạt hiệu suất vượt trội so với giải mã trực tiếp từ
chính sách PPO. Trong §5.1, chúng tôi tiến hành thêm các phân tích và ablation với nhiệm vụ điều hướng cảm xúc,
và cho thấy phương pháp của chúng tôi vượt trội hơn các chiến lược cải thiện phần thưởng khác như giải mã best-of-n
và huấn luyện PPO lâu hơn.

Baseline. Chúng tôi so sánh với giải mã trực tiếp từ cùng mô hình chính sách PPO, sử dụng lấy mẫu nucleus
(p = 0.5 trong nội quan kiến thức, và p = 0.9 trong các nhiệm vụ khác). Chúng tôi cũng bao gồm giải mã best-of-n
làm baseline, trong đó mỗi sự tiếp tục được giải mã được chọn từ n ứng viên được tạo ra
bởi lấy mẫu nucleus, và việc lựa chọn được thực hiện bởi mô hình giá trị. Để so sánh công bằng với PPO-
MCTS, trong best-of-n chúng tôi sử dụng n = 50 trong điều hướng cảm xúc, và n = 20 trong giảm độc tính.

Nhiệm vụ 1: điều hướng cảm xúc. Chúng tôi đánh giá trên tập kiểm tra của OpenWebText. Khi điều hướng về
cảm xúc tích cực, chúng tôi đánh giá trên tập con của các gợi ý cảm xúc tiêu cực; khi điều hướng về
cảm xúc tiêu cực, chúng tôi đánh giá trên tập con của các gợi ý cảm xúc tích cực. Theo Lu et al.
(2022), chúng tôi tiến hành cả đánh giá tự động và con người. Đối với đánh giá tự động, chúng tôi báo cáo
tỷ lệ văn bản được tạo với cảm xúc mong muốn (tức là, tỷ lệ thỏa mãn mục tiêu) trên 25 mẫu cho
mỗi gợi ý, độ trôi chảy được đo bằng perplexity của văn bản được tạo theo GPT2-xl có sẵn (Radford et al., 2019), và tính đa dạng là phần trăm của các n-gram khác biệt trong văn bản được tạo. Đối với đánh giá con người, chúng tôi thực hiện so sánh cặp trên các đầu ra từ PPO-MCTS với
baseline lấy mẫu trực tiếp, dựa trên mức độ cảm nhận về tính mong muốn cảm xúc, độ trôi chảy, và tính chủ đề.
Chi tiết thêm về đánh giá con người có thể được tìm thấy trong Phụ lục §C.

Như được hiển thị trong Bảng 1, PPO-MCTS tăng đáng kể tỷ lệ thành công điều hướng cảm xúc so với
baseline lấy mẫu trực tiếp từ chính sách (+34% tuyệt đối về cảm xúc tích cực và +26%
tuyệt đối về cảm xúc tiêu cực), trong khi duy trì độ trôi chảy và tính đa dạng tương đương. Trong khi đó,
best-of-n chỉ mang lại cải thiện biên hoặc không cải thiện tỷ lệ thành công trên cả hai cảm xúc mục tiêu. Đánh giá con người

6

--- TRANG 7 ---
Bản thảo
Bảng 1: Kết quả về điều hướng cảm xúc. Trên: đánh giá tự động. Dưới: đánh giá con người.
Xem Bảng 6 để biết thêm baseline và phương pháp so sánh. †: Chúng tôi sử dụng bản sao của mô hình PPO, được
huấn luyện dưới thiết lập hơi khác so với Lu et al. (2022) (chi tiết trong §B.1) và có hiệu suất tương tự.

[Bảng với các kết quả về điều hướng cảm xúc tích cực và tiêu cực, bao gồm các chỉ số về % mong muốn, độ trôi chảy, và tính đa dạng]

Bảng 2: Kết quả về giảm độc tính. Trái: đánh giá tự động. Phải: đánh giá con người. †:
Chúng tôi sử dụng bản sao của mô hình PPO, được huấn luyện dưới thiết lập hơi khác so với Lu et al. (2022)
(chi tiết trong §B.2) và có hiệu suất tương tự.

[Bảng với các kết quả về giảm độc tính, bao gồm các chỉ số về độc tính, độ trôi chảy, và tính đa dạng]

Bảng 3: Kết quả về nội quan kiến thức. Độ chính xác QA trên
tập dev của mỗi tập dữ liệu được báo cáo. †: kết quả của chúng tôi về
baseline PPO hơi khác so với Liu et al. (2022), có thể do sự khác biệt trong
môi trường tính toán và biến thiên ngẫu nhiên.

[Bảng với các kết quả về nội quan kiến thức trên các tập dữ liệu khác nhau]

Bảng 4: Kết quả về chatbot hữu ích
và vô hại.

[Bảng với các kết quả về phần thưởng và PPL]

cũng cho thấy PPO-MCTS tạo ra văn bản ưa thích hơn so với baseline lấy mẫu trực tiếp (+22% về cảm xúc tích cực và +18% về cảm xúc tiêu cực).

Nhiệm vụ 2: giảm độc tính. Chúng tôi đánh giá trên tập kiểm tra của RealToxicityPrompts. Theo Lu
et al. (2022), chúng tôi tiến hành cả đánh giá tự động và con người. Đối với đánh giá tự động, chúng tôi báo cáo
độc tính tối đa trên 25 mẫu cho mỗi gợi ý, độ trôi chảy được đo bằng perplexity của
văn bản được tạo theo GPT2-xl có sẵn (Radford et al., 2019), và tính đa dạng là
phần trăm của các n-gram khác biệt trong văn bản được tạo. Đối với đánh giá con người, chúng tôi thực hiện so sánh cặp
trên các đầu ra từ PPO-MCTS với baseline lấy mẫu trực tiếp, dựa trên mức độ cảm nhận về
độc tính, độ trôi chảy, và tính chủ đề. Chi tiết thêm về đánh giá con người có trong Phụ lục §C.

Như được hiển thị trong Bảng 2, PPO-MCTS giảm độc tính tối đa của văn bản được lấy mẫu so với
baseline lấy mẫu trực tiếp từ chính sách (-34% tương đối), trong khi duy trì độ trôi chảy và
tính đa dạng tương đương. Trong khi đó, best-of-n chỉ mang lại giảm biên về độc tính (-5% tương đối). Đánh giá con người
cũng cho thấy PPO-MCTS tạo ra văn bản ít độc hại hơn so với baseline lấy mẫu trực tiếp
(với tỷ lệ 30% tương đối), với độ trôi chảy và tính chủ đề tương đương.

Nhiệm vụ 3: nội quan kiến thức. Chúng tôi báo cáo độ chính xác QA trên tập validation của cùng
năm tập dữ liệu như trong Rainier: CommonsenseQA (Talmor et al., 2019), QASC (Khot et al., 2020), PIQA
(Bisk et al., 2020), SIQA (Sap et al., 2019), và Winogrande (Sakaguchi et al., 2020). Như được hiển thị
trong Bảng 3, việc sử dụng PPO-MCTS để giải mã kiến thức thường thức từ Rainier cải thiện
hiệu suất QA hạ nguồn 0.42% tuyệt đối, so với phương pháp giải mã trực tiếp, và
tính hữu ích của kiến thức tăng 12% tương đối.

Nhiệm vụ 4: chatbot hữu ích và vô hại. Chúng tôi đánh giá trên các gợi ý trong tập kiểm tra của một hỗn hợp
dữ liệu tính hữu ích và vô hại từ HH-RLHF. Chúng tôi báo cáo phần thưởng của văn bản được tạo ra, được

7

--- TRANG 8 ---
Bản thảo
đánh giá bởi mô hình phần thưởng. Chúng tôi cũng tiến hành đánh giá con người và so sánh PPO-MCTS với
phương pháp giải mã baseline. Như được báo cáo trong Bảng 4, PPO-MCTS tăng phần thưởng trung bình
của đầu ra 0.04 so với baseline chính sách PPO. Trong đánh giá con người, PPO-MCTS cũng
có tỷ lệ thắng cao hơn 5% (tuyệt đối) so với PPO.

5.1 Phân tích và Ablation
Trong phần này, chúng tôi báo cáo kết quả ablation và phân tích bổ sung về phương pháp PPO-MCTS của chúng tôi, sử dụng
nhiệm vụ điều hướng cảm xúc.

Có cần mô hình giá trị không? Ablation: sử dụng mô hình phần thưởng thay cho mô hình giá trị trong MCTS.
Thật tự nhiên khi hỏi liệu mô hình giá trị PPO có cần thiết trong giải mã MCTS hay không. Một lựa chọn thay thế
là sử dụng trực tiếp mô hình phần thưởng, cũng dự đoán một số vô hướng cho một chuỗi đầu vào.
Mô hình phần thưởng dự đoán phần thưởng của chuỗi đầy đủ (r(sT+1)), trong khi mô hình giá trị
dự đoán lợi nhuận kỳ vọng dưới chính sách hiện tại (Epθ[∑T t'=t γt−′t rt′]). Về mặt lý thuyết, mô hình giá trị
có hai lợi thế so với mô hình phần thưởng: (1) mô hình giá trị được huấn luyện để xử lý các chuỗi một phần,
trong khi mô hình phần thưởng thường không; (2) mô hình giá trị được thiết kế riêng cho mô hình chính sách,
trong khi mô hình phần thưởng là có sẵn hoặc được huấn luyện trước PPO. Về mặt thực nghiệm, chúng tôi thí nghiệm với
một phiên bản của PPO-MCTS trong đó mô hình giá trị được thay thế bằng mô hình phần thưởng, và báo cáo
kết quả trong Bảng 6 dưới PPO-MCTS[R]. Biến thể này dẫn đến tỷ lệ thỏa mãn mục tiêu thấp hơn và
độ trôi chảy thấp hơn so với phiên bản hướng dẫn giá trị của PPO-MCTS. Do đó, chúng tôi đã chứng minh về mặt lý thuyết và thực nghiệm sự cần thiết của mô hình giá trị.

Có cần MCTS không? Ablation: sử dụng stepwise-value thay cho MCTS. Chúng tôi cũng nghiên cứu
sự cần thiết của thuật toán MCTS trong quá trình giải mã. Để làm điều này, chúng tôi so sánh với một baseline đơn giản,
stepwise-value: để giải mã mỗi token, chúng tôi truy vấn chính sách và giữ các token top-k, sử dụng mô hình giá trị để đánh giá từng cái, và lấy mẫu từ phân phối được rút ra từ các logit giá trị. Để giữ
độ trôi chảy tương đối cao, chúng tôi thay đổi các siêu tham số thành S = k = 10 và τd = 1.0 với
giảm tuyến tính xuống không. Như được hiển thị trong Bảng 6, stepwise-value dẫn đến thỏa mãn mục tiêu thấp hơn nhiều
và độ trôi chảy thấp hơn nhiều so với PPO-MCTS. Thay vào đó, một số thuật toán tìm kiếm khác có thể
được áp dụng (ví dụ, beam search, A*). Trong công trình này chúng tôi tập trung vào MCTS do khả năng vượt trội của nó
trong tìm kiếm có thông tin trên không gian hành động.

MCTS so với PPO nhiều hơn? Người ta có thể lập luận rằng phần thưởng cao hơn có thể đạt được bằng cách chạy PPO
nhiều bước hơn. Tuy nhiên, việc làm quá nhiều PPO có thể dẫn đến sự phân kỳ cao hơn trong mô hình chính sách, và chúng tôi
cho thấy rằng việc áp dụng giải mã MCTS là một cách được điều chỉnh hơn để có được các thế hệ được thưởng cao hơn.
Chúng tôi tiếp tục huấn luyện các mô hình PPO của cả hai cảm xúc lên đến 500 bước PPO, và áp dụng
lấy mẫu top-p trực tiếp trên chính sách kết quả, được ký hiệu là "PPO (4x more steps)" trong Bảng 6. Các thí nghiệm
cho thấy rằng trên mô hình cảm xúc tích cực, phương pháp này dẫn đến thỏa mãn mục tiêu kém hơn
với PPL tệ hơn đáng kể so với PPO-MCTS, trong khi trên mô hình cảm xúc tiêu cực, nó
có PPL tương đương nhưng vẫn thỏa mãn mục tiêu thấp hơn. Những kết quả này cho thấy rằng giải mã MCTS
là một cách mong muốn hơn để đạt được phần thưởng cao hơn so với huấn luyện PPO lâu hơn.

Làm thế nào để có được sự đa dạng từ MCTS? MCTS được biết đến để tìm kiếm chuỗi hành động tốt nhất.
Có hai chỗ trong MCTS mà chúng ta có thể thúc đẩy sự đa dạng: (a) khi giải mã hành động từ
số lần truy cập (Phương trình 7), chúng ta có thể áp dụng nhiệt độ khác không τd để chúng ta có được kết quả ngẫu nhiên,
và (b) trong giai đoạn mở rộng, chúng ta có thể áp dụng nhiệt độ cao hơn τe cho các tiên nghiệm pθ(a|s), để
khuyến khích khám phá nhiều hơn trong cây tìm kiếm. Chúng tôi báo cáo thí nghiệm với các τd và τe khác nhau trong
Bảng 5 (trong Phụ lục §D). Tăng τe có thể cải thiện đáng kể thỏa mãn mục tiêu và tính đa dạng,
nhưng với chi phí làm tổn hại độ trôi chảy. Mặt khác, tăng τd có thể tăng tính đa dạng, với
phạt tương đối nhỏ về độ trôi chảy và thỏa mãn mục tiêu. Để duy trì tính đa dạng tương đương trong khi
tối ưu hóa cho thỏa mãn mục tiêu, chúng tôi chọn τd = τe = 2.0 trong các thí nghiệm của chúng tôi.

Các siêu tham số khác. Hai siêu tham số khác có vai trò quan trọng trong MCTS: số lượng
mô phỏng cho mỗi token (S), và hệ số phân nhánh (k). Chúng tôi thí nghiệm với S = [10, 20, 50],
và để ngăn k trở thành yếu tố hạn chế trong tìm kiếm cây, chúng tôi giữ nó giống như S. Như được báo cáo
trong Bảng 5 (trong Phụ lục §D), tăng S và k thường cải thiện thỏa mãn mục tiêu và tính đa dạng,
với chi phí làm tổn hại độ trôi chảy. Dựa trên những quan sát này, chúng tôi chọn S = k = 50 trong
các thí nghiệm của chúng tôi.

8

--- TRANG 9 ---
Bản thảo
Chi phí thời gian suy luận. PPO-MCTS thực sự giới thiệu một chi phí thời gian suy luận so với
lấy mẫu trực tiếp từ các mô hình chính sách. Ước tính một cách ngây thơ, PPO-MCTS chậm hơn 2S lần do
việc xây dựng cây, trong đó S là số lượng mô phỏng được chạy trước khi giải mã mỗi token, nếu chúng ta
giả định rằng mô hình chính sách và giá trị có cùng kiến trúc và kích thước. KV caching có thể
được sử dụng để tăng tốc các phương pháp giải mã tiêu chuẩn (ví dụ, lấy mẫu top-p), và nó cũng có thể áp dụng
và hiệu quả như nhau trên PPO-MCTS, vì vậy nó không có tác động đến chi phí tương đối. Tuy nhiên,
cây con dưới token được giải mã có thể được tái sử dụng khi xây dựng cây tìm kiếm cho token tiếp theo,
và do đó ít nhất ⌈S/k⌉ nút cây không cần được tính toán lại (và trong thực tế, số này
thường cao hơn nhiều). Điều này giảm thiểu chi phí thời gian suy luận lớn ở một mức độ nào đó.

6 Công trình liên quan
Giải mã có hướng dẫn. Giải mã văn bản tiêu chuẩn liên quan đến việc lấy mẫu từ phân phối token tiếp theo được
ước tính bởi một LM tạo sinh. Để đạt được khả năng kiểm soát cao hơn hoặc tỷ lệ thành công nhiệm vụ, giải mã có hướng dẫn
đã được sử dụng bởi nhiều công trình trước đây. Trong số này, một số sử dụng các hàm giá trị cấp token
để hướng dẫn giải mã (Dathathri et al., 2019; Krause et al., 2020; Lu et al., 2020; Chaffin et al., 2021),
trong khi những người khác sử dụng các trình xác minh cấp bước (Welleck et al., 2022; Uesato et al., 2022; Lightman et al., 2023;
Krishna et al., 2022; Li et al., 2022; Khalifa et al., 2023; Xie et al., 2023; Yao et al., 2023), và chúng
thường được bổ sung với một số thuật toán tìm kiếm. Phương pháp của chúng tôi, PPO-MCTS, là một phương pháp giải mã có hướng dẫn
với các hàm giá trị cấp token, và việc giải mã của chúng tôi được tăng cường thêm bởi Tìm kiếm Cây Monte-
Carlo. Hơn nữa, trong khi các hàm giá trị trong hầu hết các công trình trước đây dựa trên các quy tắc
thủ công hoặc các mô hình phân biệt riêng biệt, mô hình giá trị chúng tôi sử dụng được thiết kế riêng
cho LM tạo sinh, và nắm bắt trực tiếp hơn chỉ số mục tiêu mong muốn.

Tìm kiếm Cây Monte-Carlo cho tạo văn bản. MCTS đã được sử dụng trong các nhiệm vụ ngôn ngữ khác nhau.
LLM-MCTS (Zhao et al., 2023b) và RAP (Hao et al., 2023) áp dụng MCTS cho các nhiệm vụ lập kế hoạch và
lý luận logic, trong đó họ sử dụng các LLM có sẵn làm cả mô hình chính sách và giá trị. PPL-
MCTS (Chaffin et al., 2021) áp dụng MCTS theo cách plug-and-play để kiểm soát một số khía cạnh nhất định của
văn bản được tạo (ví dụ, độ phân cực, cảm xúc), trong đó các mô hình giá trị là các bộ phân loại chuỗi có sẵn
(không được thiết kế để hoạt động trên các chuỗi một phần). Leblond et al. (2021) áp dụng MCTS cho
nhiệm vụ dịch máy, trong đó mô hình giá trị được huấn luyện để phù hợp với một chính sách đã được huấn luyện, tương tự như
AlphaGo (Silver et al., 2016). Theo hiểu biết tốt nhất của chúng tôi, chúng tôi là những người đầu tiên áp dụng MCTS cho các mô hình chính sách và giá trị được huấn luyện chung với PPO.

Tối ưu hóa Chính sách Proximal. PPO (Schulman et al., 2017) là một thuật toán RL để tối ưu hóa
một chính sách đối với một hàm phần thưởng cho trước, và đã tìm thấy thành công lớn trong các nhiệm vụ ngôn ngữ (Ouyang
et al., 2022; OpenAI, 2022; 2023; Bai et al., 2022; Touvron et al., 2023b; Liu et al., 2022; Wu
et al., 2023). Tuy nhiên, các nhà nghiên cứu và thực hành chỉ sử dụng chính sách được huấn luyện PPO
cho suy luận, trong khi loại bỏ mô hình giá trị phụ phẩm trong hoặc sau huấn luyện. Theo hiểu biết tốt nhất của chúng tôi, chúng tôi là những người đầu tiên sử dụng mô hình giá trị của PPO và kết hợp nó với chính sách
để tạo ra tạo văn bản tốt hơn. Sau PPO, các thuật toán RL khác cho ngôn ngữ đã được đề xuất,
bao gồm Quark (Lu et al., 2022), DPO (Rafailov et al., 2023), và SLiC (Zhao et al., 2023a). Tuy nhiên
các phương pháp này không huấn luyện một mô hình giá trị đi kèm, và do đó không thể hưởng lợi trực tiếp từ
MCTS có hướng dẫn giá trị.

7 Kết luận và Công việc tương lai
Trong bài báo này, chúng tôi đã đề xuất một phương pháp hiệu quả để áp dụng giải mã Tìm kiếm Cây Monte-Carlo (MCTS)
trên các mô hình chính sách và giá trị được huấn luyện PPO, và chứng minh tính hữu ích của các mô hình giá trị
được huấn luyện như sản phẩm phụ khi căn chỉnh LM với sở thích của con người.

Công việc tương lai có thể xem xét việc sử dụng MCTS như một toán tử tối ưu hóa chính sách (Silver et al., 2017; Grill
et al., 2020) cho việc huấn luyện mô hình ngôn ngữ, tuy nhiên một số thách thức có thể cần được giải quyết: (1)
việc xây dựng cây tìm kiếm Monte-Carlo kém hiệu quả hơn so với rollout chính sách PPO; (2) MCTS cập nhật
phân phối chính sách hướng tới số lần truy cập, có thể không có ý nghĩa đủ do
không gian hành động lớn trong ngôn ngữ.

9

--- TRANG 10 ---
Bản thảo
Hạn chế và Tuyên bố Đạo đức
Trong khi cải thiện đáng kể tính ưa thích của văn bản được tạo, phương pháp của chúng tôi thực sự giới thiệu một
chi phí tính toán đáng kể tại thời gian suy luận. Phương pháp của chúng tôi thực sự thay đổi hành vi của các LM tạo sinh, và có thể
có thể rằng nó có thể chuyển hướng LM chính sách để tạo ra nội dung có hại mà
không được tạo ra dưới các phương pháp giải mã trực tiếp, đặc biệt nếu mô hình giá trị bị
can thiệp một cách đối địch. Nên sử dụng thận trọng khi áp dụng phương pháp này cho các LM hiện có.

Lời cảm ơn
Chúng tôi cảm ơn Ximing Lu đã chia sẻ mã để huấn luyện các mô hình PPO trong các nhiệm vụ giảm độc tính và
điều hướng cảm xúc. Ngoài ra, chúng tôi cảm ơn các thành viên của H2lab vì phản hồi xây dựng của họ.
Công việc này được tài trợ một phần bởi chương trình DARPA MCS thông qua NIWC Pacific (N66001-19-
2-4031), chương trình DARPA ITM (FA8650-23-C-7316), NSF IIS-2044660, và NSF DMS-2134012.
Các quan điểm, ý kiến và/hoặc phát hiện được diễn đạt là của tác giả và không nên được diễn giải
như đại diện cho quan điểm hoặc chính sách chính thức của Bộ Quốc phòng hoặc Chính phủ Hoa Kỳ.
JL được hỗ trợ một phần bởi chương trình Meta AI Mentorship.

Tài liệu tham khảo
[Phần tài liệu tham khảo với các citation đầy đủ]

--- TRANG 11 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 12 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 13 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 14 ---
[Tiếp tục danh sách tài liệu tham khảo]

--- TRANG 15 ---
Bản thảo
A Chi tiết Kỹ thuật Bổ sung
A.1 Ghi chú về triển khai
Chúng tôi triển khai PPO-MCTS như một plugin sẵn sàng thay thế cho câu lệnh "model.generate()" trong
các codebase hiện có. Plugin của chúng tôi được viết bằng PyTorch (Paszke et al., 2019), và tương thích
với các thư viện HuggingFace Transformers và Accelerate (Wolf et al., 2019; Gugger et al., 2022),
điều này giúp dễ dàng thích ứng với các nhiệm vụ khác.

A.2 Các Mục tiêu Huấn luyện PPO
Để hoàn thiện, chúng tôi xem xét mục tiêu học PPO ở đây, bao gồm hai phần: một mục tiêu chính sách
và một mục tiêu giá trị.

Mục tiêu chính sách. Đầu tiên, một hàm lợi thế ước tính (theo bước) được tính toán:
ÂAt = Â(st,at) = -Vϕ(t) + Gt = -Vϕ(t) + ∑(t'=t to T) γ^(t'-t) rt'

trong đó Vϕ là mô hình giá trị, và rt là phần thưởng cấp bước được định nghĩa trong Phương trình 1. (Trong thực tế,
mọi người thường sử dụng phiên bản cắt ngắn của hàm lợi thế ước tính này.) Sau đó, một
mục tiêu thay thế được định nghĩa là
νt(θ)·ÂAt = pθ(at|st)/pθ0(at|st)·ÂAt.

Mục tiêu chính sách là trung bình thực nghiệm của phiên bản cắt ngắn của mục tiêu thay thế này:
Jpolicy(θ) = Êt[min(νt(θ)·ÂAt, clip(νt(θ), 1-ε, 1+ε)·ÂAt)].

Mục tiêu giá trị. Mô hình giá trị được huấn luyện để khớp với lợi nhuận thực nghiệm, Gt, sử dụng một
mục tiêu MSE:
Jvalue(ϕ) = Êt[-(Vϕ(st) - Gt)²].

Mục tiêu cuối cùng là một tổ hợp tuyến tính của các mục tiêu chính sách và giá trị:
JPPO(θ,ϕ) = Jpolicy(θ) + α·Jvalue(ϕ).

A.3 Thuật toán Sao lưu MCTS
Dưới đây là thuật toán chính thức cho giai đoạn sao lưu của MCTS trong PPO-MCTS.

Thuật toán 1 Giai đoạn sao lưu của MCTS trong PPO-MCTS
[Thuật toán chi tiết với các bước cập nhật]

A.4 Xấp xỉ
Khi một số tiêu chí không được đáp ứng (ví dụ, chúng ta không có quyền truy cập vào mô hình phần thưởng tại thời gian suy luận),
chúng ta cần thực hiện một số xấp xỉ trong thuật toán PPO-MCTS. Dưới đây chúng tôi mô tả hai
xấp xỉ tiềm năng chúng ta có thể cần thực hiện.

[Chi tiết về các xấp xỉ]

A.5 Lựa chọn Triển khai trong PPO và ý nghĩa của chúng đối với MCTS
[Chi tiết về các lựa chọn triển khai và ý nghĩa]

B Chi tiết Thí nghiệm Bổ sung (...tiếp tục từ §4)
[Chi tiết thí nghiệm cho từng nhiệm vụ]

--- TRANG 16 ---
[Tiếp tục chi tiết thí nghiệm]

--- TRANG 17 ---
[Tiếp tục chi tiết thí nghiệm]

--- TRANG 18 ---
Bản thảo
C Chi tiết Đánh giá Con người
[Chi tiết về quy trình đánh giá con người]

--- TRANG 19 ---
[Tiếp tục chi tiết đánh giá con người và các hình ảnh giao diện]

--- TRANG 20 ---
[Các bảng kết quả bổ sung và phân tích]
