# Hiệu chỉnh dữ liệu hiệu quả của các mô hình ngôn ngữ lớn với phản hồi của con người thông qua ngôn ngữ tự nhiên

Di Jin, Shikib Mehri, Devamanyu Hazarika, Aishwarya Padmakumar
Sungjin Lee, Yang Liu, Mahdi Namazifar
Amazon
{djinamzn,asmehri,dvhaz,padmakua,sungjinl,yangliud,mahdinam}@amazon.com

Tóm tắt
Học từ phản hồi của con người là một kỹ thuật nổi bật để điều chỉnh đầu ra của các mô hình ngôn ngữ lớn (LLM) theo kỳ vọng của con người. Học tăng cường từ phản hồi của con người (RLHF) tận dụng các tín hiệu ưu tiên của con người dưới dạng xếp hạng các cặp phản hồi để thực hiện việc điều chỉnh này. Tuy nhiên, sở thích của con người đối với đầu ra LLM có thể có nhiều hình thức phong phú hơn bao gồm ngôn ngữ tự nhiên, có thể cung cấp phản hồi chi tiết về điểm mạnh và điểm yếu của một phản hồi nhất định. Trong công trình này, chúng tôi nghiên cứu hiệu quả dữ liệu của việc mô hình hóa phản hồi của con người bằng ngôn ngữ tự nhiên. Cụ thể, chúng tôi tinh chỉnh một LLM mã nguồn mở, ví dụ Falcon-40B-Instruct, trên một lượng tương đối nhỏ (1000 bản ghi hoặc thậm chí ít hơn) phản hồi của con người bằng ngôn ngữ tự nhiên dưới dạng phê bình và sửa đổi các phản hồi. Chúng tôi chỉ ra rằng mô hình này có thể cải thiện chất lượng phản hồi từ thậm chí một số LLM mạnh nhất như ChatGPT, BARD và Vicuna, thông qua phê bình và sửa đổi những phản hồi đó. Ví dụ, thông qua một lần lặp sửa đổi phản hồi ChatGPT, các phản hồi được sửa đổi có tỷ lệ thắng 56.6% so với những phản hồi gốc, và tỷ lệ thắng này có thể được cải thiện thêm lên 65.9% sau khi áp dụng sửa đổi trong năm lần lặp.

1 Giới thiệu
Những phát triển gần đây trong các Mô hình Ngôn ngữ Lớn (LLM) đã tác động sâu sắc đến quá trình nghiên cứu và thực hành trong Xử lý Ngôn ngữ Tự nhiên (NLP), AI đối thoại và xa hơn nữa, kể từ khi giới thiệu ChatGPT. Hiện tại có ba bước chung trong việc xây dựng các hệ thống đối thoại dựa trên LLM như vậy, đó là tiền huấn luyện, tinh chỉnh có giám sát, và việc điều chỉnh thêm các phản hồi của mô hình với sở thích của con người. Bước cuối cùng thường được thực hiện thông qua Học Tăng cường từ Phản hồi của Con người (RLHF) Ouyang et al. [2022]. Trong phương pháp này, đầu tiên một mô hình phần thưởng được huấn luyện sử dụng các chú thích của con người về các phản hồi được ưa thích giữa các cặp phản hồi. Mô hình phần thưởng sau đó được sử dụng để tinh chỉnh thêm LLM thông qua RL, ví dụ, sử dụng tối ưu hóa chính sách trực tuyến, PPO Schulman et al. [2017].

Việc điều chỉnh đầu ra của LLM với sở thích của con người sử dụng RL và dựa trên mô hình phần thưởng có những thách thức riêng. RLHF nổi tiếng không ổn định trong quá trình huấn luyện và yêu cầu nhiều tài nguyên tính toán hơn để lưu trữ cả mô hình chính sách và mô hình cơ sở, điều này tạo ra những thách thức lớn cho cơ sở hạ tầng khi kích thước mô hình vượt quá 50B tham số Lu et al. [2022]. Ngoài ra, trong RLHF, sở thích của con người được truyền đạt cho mô hình thông qua một giá trị phần thưởng duy nhất. Tuy nhiên, sở thích của con người có thể phức tạp, cụ thể và chi tiết hơn nhiều. Ví dụ, một phản hồi có thể được ưa thích vì nó ngắn gọn và đi thẳng vào vấn đề, hoặc bao gồm một khía cạnh chính nhất định chi tiết hơn và để các khía cạnh khác ở mức cao. Để cho phép LLM học từ phản hồi chi tiết của con người vượt ra ngoài một giá trị duy nhất, công trình gần đây như Chain of Hindsight Liu et al. [2023a], đã xem xét cách dạy mô hình sở thích của con người mà không cần mô hình phần thưởng bằng cách cho mô hình thấy hai phản hồi và chỉ ra rằng một phản hồi được ưa thích hơn phản hồi kia dựa trên những lý do nhất định bằng ngôn ngữ tự nhiên.

Trong công trình này, chúng tôi dạy mô hình sở thích của con người thông qua các ví dụ và bằng ngôn ngữ tự nhiên. Cụ thể, với một lời nhắc và một phản hồi, chúng tôi dạy mô hình (1) tạo ra một lời phê bình về phản hồi này và nêu ra các khía cạnh tích cực và tiêu cực của phản hồi, và (2) sửa đổi phản hồi ban đầu theo lời phê bình. Chúng tôi gọi phương pháp này là Critique and Revise, hay CnR để viết tắt. Thông qua chú thích của con người, với một lời nhắc và một phản hồi, chúng tôi thu thập một lời phê bình về phản hồi, cũng như một phản hồi được sửa đổi dựa trên lời phê bình. Với một tập hợp tương đối nhỏ các mẫu được chú thích như vậy, ví dụ ít hơn 1000 mẫu, CnR có thể dạy một LLM mã nguồn mở như Falcon-40B-Instruct để sửa đổi và cải thiện thậm chí các phản hồi ChatGPT. Thông qua một lần lặp sửa đổi phản hồi ChatGPT, các phản hồi được sửa đổi có tỷ lệ thắng 56.6% so với những phản hồi gốc, và tỷ lệ thắng này có thể được cải thiện thêm lên 65.9% sau khi áp dụng sửa đổi trong năm lần lặp. Hiệu quả dữ liệu tuyệt vời như vậy ngụ ý rằng việc các mô hình học điều chỉnh theo phản hồi của con người thông qua các lời phê bình chi tiết bằng ngôn ngữ tự nhiên dễ dàng hơn so với phản hồi so sánh theo cặp chỉ đơn giản cho biết phản hồi nào trong hai phản hồi được ưa thích.

Hơn nữa, thông qua tập dữ liệu kiểm tra chẩn đoán mà chúng tôi thu thập, chúng tôi xác nhận rằng mô hình CnR có thể tạo ra các lời phê bình có ý nghĩa và cụ thể cho một phản hồi nhất định và có thể giải quyết những lỗi đó trong phản hồi. Chúng tôi cũng chỉ ra rằng mô hình này có thể tuân theo một lời phê bình nhất định và sửa đổi các phản hồi tương ứng. Hơn nữa, thông qua các nghiên cứu loại bỏ, chúng tôi chỉ ra rằng chất lượng sửa đổi có thể được cải thiện bằng cách sử dụng các mô hình tiền huấn luyện cơ sở lớn hơn và dữ liệu điều chỉnh hướng dẫn chất lượng cao hơn. Chúng tôi cũng nghiên cứu tác động của lượng dữ liệu CnR đối với hiệu suất của mô hình CnR.

Hình 1: Ví dụ về chú thích phê bình và sửa đổi

2 Công trình liên quan
Điều chỉnh thông qua Mô hình hóa Sở thích. Để điều chỉnh đầu ra LLM thêm với sở thích hoặc giá trị của con người, một phương pháp phổ biến là học và sử dụng các mô hình sở thích có thể bắt chước sở thích của con người. Tận dụng các mô hình này, LLM có thể được huấn luyện để tạo ra các phản hồi đảm bảo điểm sở thích cao. Những phương pháp này yêu cầu huấn luyện các mô hình phần thưởng và các chiến lược học tăng cường, như PPO Ouyang et al. [2022], Glaese et al. [2022].

Để giảm bớt sự phụ thuộc vào huấn luyện RL phức tạp, công trình gần đây đã cố gắng bỏ qua RL thông qua các phương pháp như kết hợp mất mát RLHF ngầm, ví dụ DPO Rafailov et al. [2023] hoặc giới thiệu mất mát dựa trên xếp hạng (ví dụ, Slic Zhao et al. [2022], PPO Song et al. [2023], Stable Alignment Liu et al. [2023b], RRHF Yuan et al. [2023]).

Điều chỉnh thông qua Phản hồi Ngôn ngữ Tự nhiên. Trong khi mô hình hóa sở thích của con người dựa trên phần thưởng có sức hấp dẫn, nó có nhược điểm tự nhiên là yêu cầu một mô hình phần thưởng ngay từ đầu, bản thân nó có thể có nhiễu và không chính xác. Một phương pháp thay thế cho việc điều chỉnh là sử dụng chính LLM chủ yếu cho mục đích này. Đặc biệt, các phương pháp dựa trên sửa đổi là chủ đạo trong các thiết lập như vậy.

Tài liệu về việc sửa đổi các thế hệ mô hình chứa một lượng lớn công trình, như PEER Schick et al. [2022], được xây dựng dựa trên vấn đề truyền thống của việc chỉnh sửa văn bản Dwivedi-Yu et al. [2022]. Tuy nhiên, công trình trước đó đã nghiên cứu vấn đề này trong các thiết lập cụ thể theo lĩnh vực, bao gồm cho tài liệu Wikipedia, sửa đổi mã Reid và Neubig [2022] hoặc các tác vụ cụ thể như sửa lỗi ngữ pháp, đơn giản hóa câu Malmi et al. [2022]. Những công trình này thường yêu cầu các trình tạo phản hồi hoặc sửa đổi tùy chỉnh.

Công trình gần đây, bao gồm Self-Refine Madaan et al. [2023], Constitutional AI Bai et al. [2022b], v.v., bắt đầu sử dụng khả năng tuân theo hướng dẫn của LLM để mô hình phản hồi của con người bằng ngôn ngữ tự nhiên Bai et al. [2022b]. Thông qua việc nhắc nhở, những phương pháp này có khả năng tạo ra các lời phê bình bao gồm các thuộc tính đa diện (ví dụ, làm cho các phản hồi hữu ích, vô hại, cũng như trung thực và thành thật) và thực hiện các sửa đổi lặp đi lặp lại để cải thiện tối đa các thế hệ mô hình. Bằng cách sử dụng cùng một LLM cho cả phê bình và sửa đổi, LLM có thể tự huấn luyện để cải thiện bản thân.

Hình thức tổng thể của việc sửa đổi các thế hệ cung cấp một khung có thể giải thích được nơi các sửa đổi có thể được quan sát một cách rõ ràng bằng ngôn ngữ tự nhiên. Để có một đánh giá mở rộng về tài liệu gần đây về các khung phê bình và sửa đổi, chúng tôi giới thiệu độc giả đến Madaan et al. [2023] và Pan et al. [2023].

Có cần Phê bình trong CnR không? Tạo phản hồi hoặc phê bình có nhiều trường hợp sử dụng. Ví dụ, nó có thể giúp các người chú thích xác định các vấn đề trong các thế hệ mô hình và hỗ trợ họ trong việc đánh giá các phản hồi (để thu thập dữ liệu sở thích) Saunders et al. [2022] hoặc trong việc sửa đổi các phản hồi của mô hình với sự hỗ trợ từ lời phê bình. Các lời phê bình cũng có thể được sử dụng để xác định các vấn đề chú thích, đặc biệt với các người chú thích đối kháng. Hơn nữa, công trình gần đây như Bai et al. [2022b] đã chứng minh rằng các lời phê bình thực sự cung cấp các sửa đổi được cải thiện, do đó biện minh cho việc tạo ra chúng. Hơn nữa, sửa đổi thông qua phê bình có thể đánh giá tốt hơn liệu việc sửa đổi có cải thiện các vấn đề được nêu bật trong lời phê bình hay không. Ví dụ, Campos và Shern [2022] đã sử dụng độ tương tự như một phương pháp phỏng đoán để chọn các sửa đổi tốt nhất đối với lời phê bình.

Nhà cung cấp Phản hồi Đa mô hình. Một dòng công trình gần đây đã sử dụng nhiều LLM làm trình tạo phản hồi và trình sửa đổi Liu et al. [2023b]. Ví dụ, Fu et al. [2023] đã sử dụng hai LLM để đàm phán với nhau thông qua tự chơi, nơi một LLM thứ ba đóng vai trò là người phê bình để tư vấn cho một trong các người đàm phán. Trái ngược với những phương pháp này, công trình của chúng tôi xem xét cùng một LLM cho cả phê bình và sửa đổi Madaan et al. [2023].

Phản hồi Mục đích Chung bằng Ngôn ngữ Tự nhiên. Hầu hết công trình gần đây mô hình phản hồi của con người bằng ngôn ngữ tự nhiên đã nghiên cứu vấn đề trong các ứng dụng cụ thể, như tóm tắt Saunders et al. [2022], hoặc sự kết hợp của chúng, ví dụ tối ưu hóa mã, tạo từ viết tắt, tạo phản hồi đối thoại, lý luận toán học, v.v. trong Self-Refine Madaan et al. [2023]. Trong khi cái sau có phạm vi bao phủ nhiều tác vụ hơn so với những cái trước đó, trọng tâm của chúng tôi là nghiên cứu CnR trong các minh chứng của con người mục đích chung.

Huấn luyện so với Few-Shot cho Thiết lập Phản hồi Ngôn ngữ Tự nhiên. Một số nghiên cứu trước đây đã lưu ý rằng việc tạo phê bình hoặc sửa đổi zero- hoặc few-shot yêu cầu các LLM lớn mạnh mẽ Saunders et al. [2022], Campos và Shern [2022], Madaan et al. [2023]. Tuy nhiên, các LLM lớn thường không có mã nguồn mở và đi kèm với các giấy phép hạn chế, cấm các trường hợp sử dụng đòi hỏi nhu cầu bảo mật, ví dụ, các trường hợp sử dụng pháp lý và chăm sóc sức khỏe. Với vấn đề này, công trình của chúng tôi nghiên cứu mô hình CnR trong các mô hình mã nguồn mở nhỏ hơn.

AlMoST được đề xuất gần đây Kim et al. [2023] chia sẻ động lực tương tự như của chúng tôi, ở chỗ nó cũng cố gắng thực hiện điều chỉnh mà không cần một mô hình giáo viên quy mô lớn. Để làm điều này, họ sử dụng nhiều mô hình quy mô nhỏ và tích hợp sở thích tương đối của các phản hồi từ một mô hình quy mô tương đối lớn hơn (ví dụ, 30B) với các minh chứng few-shot được ưa thích hơn so với một mô hình 1B với việc nhắc nhở zero-shot. Tuy nhiên, AlMoST sử dụng SFT và RL để điều chỉnh, trong khi chúng tôi nghiên cứu khung phê bình và sửa đổi. Vernikos et al. [2023] cũng đã chứng minh rằng các mô hình nhỏ hơn có thể hoạt động như những trình tinh chỉnh mạnh mẽ khi được huấn luyện phù hợp.

3 Dữ liệu CnR
Trong phần này, chúng tôi thảo luận về cách chúng tôi thu thập dữ liệu cho CnR. Đối với một lời nhắc và một phản hồi nhất định, nhiệm vụ chú thích của con người là cung cấp một lời phê bình về phản hồi và một phản hồi được sửa đổi theo lời phê bình. Chúng tôi bắt đầu với một tập hợp 1.000 minh chứng của con người được thu thập lấy cảm hứng từ phân loại được giới thiệu trong InstructGPT Ouyang et al. [2022] cho tóm tắt, tạo sinh, hỏi đáp, trích xuất, động não và viết lại. Mỗi minh chứng này bao gồm một lời nhắc và một phản hồi, cả hai đều được viết bởi một người chú thích con người. Chúng tôi sử dụng 3 LLM khác nhau dựa trên FLAN-T5-XXL, GPT-J và AlexaTM 20B Chung et al. [2022], FitzGerald et al. [2022] và được tinh chỉnh trên tập dữ liệu Anthropic Helpful Bai et al. [2022a] để tạo ra 3 phản hồi bổ sung cho mỗi lời nhắc. Cuối cùng, chúng tôi chọn ngẫu nhiên một trong 4 phản hồi cho mỗi trong số 1000 lời nhắc.

Tập dữ liệu kết quả là 1000 lời nhắc và phản hồi, nơi chúng tôi có chất lượng phản hồi hỗn hợp. Khoảng một phần tư các lời nhắc có phản hồi được viết bởi con người mà chúng tôi kỳ vọng có chất lượng cao, và đối với phần còn lại của các lời nhắc, các phản hồi được tạo ra từ các mô hình tương đối yếu hơn được huấn luyện trên dữ liệu kém hơn, và có chất lượng thấp hơn.

Chúng tôi đã thu thập phê bình và sửa đổi cho tập hợp lời nhắc và phản hồi này. Như được minh họa bởi Hình 1, đối với mỗi phản hồi với một lời nhắc, các người chú thích được yêu cầu đưa ra điểm từ 1 đến 5 cho chất lượng tổng thể của phản hồi. Họ cũng được yêu cầu chỉ định các khía cạnh tích cực của phản hồi, cũng như các khía cạnh tiêu cực của nó. Trong việc cung cấp lời phê bình cho một phản hồi, họ được yêu cầu tránh cung cấp các tuyên bố chung chung như "chất lượng tổng thể cao" hoặc "phản hồi có lỗi". Thay vào đó, họ được hướng dẫn chỉ ra các câu hoặc cụm từ cụ thể trong phản hồi có lỗi và chỉ định lý do tại sao những tuyên bố đó có vấn đề. Hơn nữa, các người chú thích được hướng dẫn không sử dụng ngôn ngữ ngôi thứ nhất như "theo ý kiến của tôi" hoặc "tôi cảm thấy như" trong lời phê bình của họ về phản hồi. Nếu một phản hồi đã tốt, các người chú thích có thể chỉ ra rằng không có gì cần được cải thiện trong "các khía cạnh tiêu cực".

Cuối cùng, các người chú thích được yêu cầu cung cấp một phản hồi được sửa đổi cho lời nhắc theo các khía cạnh tích cực và tiêu cực mà họ đề cập trong lời phê bình. Các phản hồi được sửa đổi cần có chất lượng rất cao. Hơn nữa, các người chú thích được hướng dẫn đảm bảo rằng các lời phê bình và sửa đổi hoàn toàn phù hợp. Nói cách khác, nếu một điểm tiêu cực hoặc tích cực được đề cập trong lời phê bình, nó nên được phản ánh trong phản hồi được sửa đổi. Cũng không nên có thay đổi bổ sung nào được giới thiệu trong phản hồi được sửa đổi mà không được đề cập trong lời phê bình của phản hồi ban đầu. Tất cả các bản ghi được chú thích đều được xem xét bởi một người chú thích thứ hai và các thay đổi đã được thực hiện khi cần thiết trong quá trình xem xét này.

Hình 1 cho thấy một ví dụ CnR được chú thích, bao gồm một lời nhắc, phản hồi ban đầu, lời phê bình về phản hồi ban đầu (bao gồm điểm tổng thể, một tập hợp các khía cạnh tích cực và một tập hợp các khía cạnh tiêu cực), và một phản hồi được sửa đổi.

4 Mô hình CnR
Việc huấn luyện mô hình CnR được thực hiện theo cách tương tự như Tinh chỉnh Có giám sát (SFT) của LLM trên các minh chứng của con người Ouyang et al. [2022]. Dữ liệu được chú thích phê bình và sửa đổi được sử dụng để tạo ra các bản ghi huấn luyện CnR. Trong công trình này, chúng tôi thử nghiệm với các cách khác nhau để xây dựng các bản ghi này cho việc huấn luyện và suy luận mô hình. Mỗi mẫu được chú thích bao gồm một lời nhắc, một phản hồi ban đầu, một lời phê bình và một phản hồi được sửa đổi, được ký hiệu lần lượt là p, i, c và r. Như được minh họa bởi Hình 2, chúng tôi xem xét ba công thức để xây dựng các bản ghi huấn luyện: (1) sử dụng tất cả các chú thích p→i→c→r, (2) chỉ sử dụng phản hồi ban đầu và các phản hồi được sửa đổi, không có lời phê bình p→i→r, (3) chỉ sử dụng các phản hồi được sửa đổi p→r. Chúng tôi so sánh thiết lập nào đạt được hiệu suất tốt hơn.

Hình 2: Các thiết lập chuẩn bị dữ liệu CnR khác nhau

Để huấn luyện, một mô hình đã được tinh chỉnh có giám sát được tiếp tục tinh chỉnh trên các bản ghi huấn luyện CnR. Trong quá trình huấn luyện này, không có che dấu nào trên các bản ghi huấn luyện được áp dụng khi tính toán mất mát. Một lợi ích của điều này là mô hình CnR cuối cùng có thể được sử dụng có hoặc không có phản hồi ban đầu hoặc lời phê bình. Ví dụ, trong thiết lập này, người ta có thể nhắc mô hình CnR cuối cùng tạo ra một phản hồi ban đầu, một lời phê bình về phản hồi ban đầu này (điểm tổng thể, các khía cạnh tích cực và các khía cạnh tiêu cực), và sau đó là một phản hồi được sửa đổi dựa trên lời phê bình. Ngoài ra, mô hình này có thể được sử dụng để phê bình và sửa đổi một phản hồi nhất định cho một lời nhắc; hoặc với một lời nhắc, một phản hồi và một lời phê bình về phản hồi, mô hình có thể được sử dụng để tạo ra một phản hồi được sửa đổi.

Trong công trình này, chúng tôi chạy tinh chỉnh CnR trên một số mô hình mã nguồn mở khác nhau bao gồm GPT-J Wang và Komatsuzaki [2021], GPT-NeoX-20B Black et al. [2022], và Falcon-40B Almazrouei et al. [2023], tất cả đều được tinh chỉnh đầu tiên trên tập dữ liệu Dolly Conover et al. [2023], để nghiên cứu tác động của kích thước mô hình đối với hiệu suất của CnR. Điều này dẫn đến ba mô hình CnR: GPTJ-D-CnR, GPT-NeoX-D-CnR và Falcon-D-CnR. Hơn nữa, chúng tôi đã thực hiện tinh chỉnh CnR trên Falcon-40b-Instruct như một mô hình tinh chỉnh mạnh để nghiên cứu mức cải thiện nào từ CnR có thể được mong đợi khi áp dụng cho một LLM hiệu suất cao. Mô hình CnR này được ký hiệu là Falcon-I-CnR.

Tất cả các mô hình CnR được huấn luyện trong 16 epochs với tỷ lệ học 5e−6 trên 4 instance P4d EC2 trên Amazon AWS sử dụng thư viện transformers của Huggingface. Giá trị của kích thước batch trên mỗi thiết bị được đặt thành 1, điều này dẫn đến kích thước batch hiệu quả là 32.

5 Thí nghiệm
Đối với dữ liệu đánh giá, chúng tôi chủ yếu áp dụng tập dữ liệu đánh giá được sử dụng rộng rãi của FastChat, bao gồm 80 truy vấn bao quát các chủ đề đa dạng, như viết, thu thập kiến thức, toán học, lập trình, v.v. Khi đánh giá thiết lập CnR tốt nhất, chúng tôi cũng thu thập các lời nhắc từ Anthropic Helpful Bai et al. [2022a] và Self-Instruct Wang et al. [2023] như dữ liệu đánh giá bổ sung.

Về các chỉ số đánh giá, chúng tôi áp dụng cả đánh giá tự động và đánh giá của con người, nơi mỗi instance đánh giá chứa một cặp phản hồi mô hình để so sánh, với ba nhãn kết quả có thể: thắng, thua và hòa. Đối với đánh giá tự động, chúng tôi sử dụng GPT4 làm người đánh giá, đã được chỉ ra là có thể khớp với hiệu suất của cả sở thích của con người được kiểm soát và crowdsource Lin và Chen [2023], Zheng et al. [2023]. Lời nhắc chi tiết cho đánh giá GPT4 được cung cấp trong Phụ lục. Xem xét rằng GPT4 được biết là thể hiện sự thiên vị vị trí Zheng et al. [2023], chúng tôi sử dụng thiết lập đánh giá hai chiều. Điều này có nghĩa là mỗi instance đánh giá với một cặp phản hồi mô hình được đánh giá hai lần bằng cách chuyển đổi vị trí của hai phản hồi. Đối với đánh giá của con người, chúng tôi tận dụng các worker Amazon Mechanical Turk. Mỗi cặp phản hồi được đánh giá bởi 5 người chú thích con người được gán ngẫu nhiên và chúng tôi chỉ xem xét các cặp được đánh giá nơi ít nhất 3 trong số 5 người chú thích đồng ý. Sau đó, chúng tôi lấy phiếu bầu đa số làm nhãn con người cuối cùng.

Đối với mỗi instance, chúng tôi thu được điểm đánh giá của các phản hồi từ mô hình A và B. Nếu điểm của mô hình A cao hơn điểm của B, thì mô hình A được tính một chiến thắng cho instance đánh giá đó. Sau khi thu được số lần thắng cho mô hình A (WA) và cho mô hình B (WB), cũng như số lần hòa T trên tất cả các instance đánh giá, tỷ lệ thắng của mô hình A được tính là (WA+T/2)/(WA+WB+T)×100.

6 Kết quả
6.1 Thiết lập CnR tốt nhất

Bảng 1: Tỷ lệ thắng (%) trong các cuộc đối đầu trực tiếp giữa các công thức khác nhau, với tỷ lệ thắng theo 5 người chú thích AMT và GPT4.

So sánh | Con người | GPT4
p→i→r vs p→r | 58.5 | 68.8
p→i→c→r vs p→r | 64.0 | 70.0
p→i→c→r vs p→i→r | 54.0 | 56.9

Đầu tiên, chúng tôi nghiên cứu thiết lập tốt nhất, trong số ba thiết lập được trình bày trong Hình 2, để tận dụng dữ liệu được chú thích CnR để cải thiện phản hồi mô hình. Mục tiêu là hiểu vai trò của phản hồi ban đầu và lời phê bình trong CnR. Để làm điều này, chúng tôi tiến hành cả đánh giá của con người và GPT4 đối với các phản hồi được sửa đổi cho 486 lời nhắc được tổng hợp từ Anthropic Helpful, Self-Instruct và FastChat. Kết quả của những thí nghiệm này được hiển thị trong Bảng 1. Chúng ta có thể thấy rằng thiết lập tốt nhất là nơi mô hình đầu tiên tạo ra một phản hồi ban đầu, sau đó tạo ra một lời phê bình về phản hồi đó, và sau đó tạo ra một sửa đổi của phản hồi ban đầu dựa trên lời phê bình. Nói chung, thí nghiệm này cho thấy rằng các cải thiện từ CnR không chỉ đơn giản là do chất lượng cao hơn của các phản hồi được sửa đổi, thay vào đó việc cho phép mô hình tạo ra một lời phê bình và sửa đổi phản hồi ban đầu góp phần vào các cải thiện thu được từ CnR.

6.2 CnR có thể cải thiện ChatGPT
Để chỉ ra hiệu quả của phương pháp CnR, chúng tôi sử dụng mô hình CnR tốt nhất của mình, Falcon-I-CnR (được tinh chỉnh trên tất cả 1K mẫu huấn luyện CnR), để sửa đổi các phản hồi được tạo ra bởi Vicuna-13B, BARD, ChatGPT và GPT4 trên tập kiểm tra FastChat. Sau đó, chúng tôi sử dụng GPT4 làm người đánh giá để so sánh các phản hồi gốc với những phản hồi được sửa đổi. Tỷ lệ thắng được hiển thị trong Hình 3, nơi mỗi cặp so sánh bao gồm một phản hồi gốc từ một LLM và một phản hồi được sửa đổi tương ứng từ Falcon-I-CnR. Như có thể thấy, Falcon-I-CnR có thể cải thiện hiệu quả các phản hồi từ Vicuna-13B, BARD và ChatGPT. Cần lưu ý rằng mô hình cơ sở của Falcon-I-CnR, tức là Falcon-40B-Instruct, đã được tinh chỉnh trên dữ liệu từ ChatGPT. Điều này ngụ ý rằng đối với một mô hình X đã được tinh chỉnh hướng dẫn trên dữ liệu D, chúng ta có thể tinh chỉnh thêm nó trên dữ liệu CnR và sau đó sử dụng mô hình X-CnR thu được để cải thiện thêm dữ liệu huấn luyện D thành một phiên bản tốt hơn D′. Một tập dữ liệu được cải thiện như vậy D′ có thể giúp huấn luyện một mô hình tinh chỉnh hướng dẫn tốt hơn X'.

Chúng tôi cũng đã tiến hành đánh giá của con người để xác minh lợi thế của các phản hồi ChatGPT được sửa đổi bởi Falcon-I-CnR so với những phản hồi gốc. Chúng tôi thu được tỷ lệ thắng 58.2%, gần với tỷ lệ thắng 56.6% từ đánh giá GPT4.

Hình 3: Tỷ lệ thắng giữa các phản hồi gốc từ Vicuna-13B, BARD, ChatGPT và GPT4 trên 80 truy vấn FastChat và các phản hồi được sửa đổi bởi Falcon-I-CnR.

Hình 4: Tỷ lệ thắng của các phản hồi được sửa đổi bởi Falcon-40B-Instruct-CnR đối với các phản hồi gốc từ ChatGPT qua các sửa đổi lặp lại.

6.3 CnR có thể cải thiện lặp lại
Để xác thực rằng phương pháp CnR của chúng tôi có thể tiếp tục cải thiện các phản hồi qua các lần lặp, chúng tôi sử dụng Falcon-I-CnR để sửa đổi các phản hồi từ ChatGPT năm lần lặp lại. Trong lần lặp N, mô hình sửa đổi sửa đổi các phản hồi thu được trong lần lặp N−1, hoặc phản hồi ban đầu, nếu N= 1. Chúng tôi so sánh chất lượng (được đo bởi GPT4) của các phản hồi được sửa đổi với các phản hồi ChatGPT ban đầu trên tập kiểm tra FastChat và trình bày tỷ lệ thắng trong Hình 4. Như có thể thấy, tỷ lệ thắng có một bước nhảy lớn từ lần lặp 1 đến lần lặp 2 (tỷ lệ thắng tăng từ 56.6% lên 61.9%), sau đó tăng dần mà không có cao nguyên cho đến lần lặp 5 (đạt tới 65.9% tỷ lệ thắng). Xu hướng này xác thực rằng phương pháp CnR của chúng tôi có thể tiếp tục cải thiện các phản hồi qua nhiều lần lặp, mang lại cải thiện cuối cùng hơn 10% về tỷ lệ thắng.

6.4 Điều gì ảnh hưởng đến chất lượng sửa đổi?
Chúng tôi muốn nghiên cứu những yếu tố nào có thể ảnh hưởng đến chất lượng sửa đổi của phương pháp CnR của chúng tôi. Bằng cách phân tách các bước của CnR, chúng tôi đã xác định các yếu tố sau có thể đóng vai trò: kích thước mô hình tiền huấn luyện cơ sở, chất lượng tinh chỉnh hướng dẫn trước khi tinh chỉnh trên các mẫu CnR, và số lượng mẫu CnR.

Hình 5: Tỷ lệ thắng của các phản hồi được sửa đổi so với những phản hồi gốc cho các mô hình cơ sở khác nhau được tinh chỉnh trên số lượng mẫu CnR khác nhau. Các phản hồi gốc là từ mô hình tinh chỉnh hướng dẫn X (như GPTJ-Dolly trong hình ngoài cùng bên trái), trong khi các phản hồi được sửa đổi là từ mô hình sửa đổi tương ứng X-CnR tinh chỉnh mô hình X trên dữ liệu CnR (ví dụ, GPTJ-Dolly-CnR.)

Tinh chỉnh Hướng dẫn: Chúng tôi sử dụng cả Falcon-D-CnR và Falcon-I-CnR để sửa đổi các phản hồi từ Vicuna-13B trên tập kiểm tra FastChat và tính toán tỷ lệ thắng của các phản hồi được sửa đổi đối với những phản hồi gốc. Kết quả lần lượt là 63.1% và 53.1% cho Falcon-I-CnR và Falcon-D-CnR. Cả hai mô hình sửa đổi đều sử dụng cùng mô hình cơ sở Falcon-40B và cùng dữ liệu CnR, và sự khác biệt duy nhất là dữ liệu tinh chỉnh hướng dẫn. Falcon-I-CnR dựa trên một tập hợp dữ liệu tinh chỉnh hướng dẫn có số lượng lớn hơn nhiều và chất lượng cao hơn so với Falcon-D-CnR, và nó thể hiện chất lượng sửa đổi tốt hơn nhiều, cho thấy rằng tinh chỉnh hướng dẫn tốt hơn có thể giúp cải thiện chất lượng sửa đổi.

Số lượng Mẫu CnR. Để nghiên cứu ảnh hưởng của kích thước dữ liệu huấn luyện CnR, chúng tôi tinh chỉnh một mô hình tinh chỉnh hướng dẫn X trên số lượng mẫu CnR khác nhau từ 200 đến 1000 với khoảng cách 200. Sau đó, chúng tôi sử dụng mô hình sửa đổi X-CnR thu được để sửa đổi các phản hồi từ mô hình X và tính toán tỷ lệ thắng của các phản hồi được sửa đổi so với những phản hồi gốc. Kết quả được minh họa trong Hình 5. Nói chung, chất lượng sửa đổi đầu tiên cải thiện đáng kể khi kích thước mẫu tăng từ 0 đến 600, sau đó dần dần bão hòa với 800 mẫu trở lên.

Kích thước Mô hình Cơ sở. Như được hiển thị trong Hình 5, được huấn luyện trên cùng số lượng mẫu CnR, các mô hình lớn hơn có xu hướng đạt được tỷ lệ thắng cao hơn của các phản hồi được sửa đổi so với những phản hồi gốc. Ví dụ, với 800 mẫu CnR, Falcon-D-CnR (40B tham số), GPT-NeoX-D-CnR (20B) và GPTJ-D-CnR (6B) lần lượt thu được 81.6%, 74.7% và 65.0% tỷ lệ thắng. Xu hướng như vậy cho thấy rằng các mô hình tiền huấn luyện lớn hơn dẫn đến chất lượng sửa đổi tốt hơn.

7 Thảo luận
Phần này cung cấp phân tích bổ sung về dữ liệu CnR, mà chúng tôi đã thu thập và sử dụng để huấn luyện các mô hình CnR, và hành vi của mô hình CnR.

7.1 Phân bố của các lời phê bình
Chúng tôi cho thấy phân bố của các lời phê bình trên 1000 mẫu CnR. Cụ thể, chúng tôi định nghĩa sáu loại lỗi cấp cao được đề cập bởi các lời phê bình (các khía cạnh mà các phản hồi ban đầu không tốt): (i) tuân theo hướng dẫn, (ii) tính chính xác (thực tế, logic), (iii) sự liên quan, (iv) tính đầy đủ, chiều sâu và chi tiết, (v) sự rõ ràng, (vi) các cân nhắc an toàn và hạn chế. GPT4 được tận dụng để chú thích liệu mỗi lời phê bình được viết bởi con người có đề cập đến bất kỳ loại lỗi nào trong sáu loại này. Hình 6 cho thấy sự liên quan, tính đầy đủ và sự rõ ràng là các loại lỗi chính.

7.2 Chất lượng phê bình
CnR là một quá trình đa bước (p→i→c→r), tuy nhiên các đánh giá trước đây của chúng tôi xem xét lời nhắc và phản hồi (p, r) và bỏ qua lời phê bình trung gian (c). Ở đây chúng tôi tách riêng bước phê bình, (p, i)→c, để đánh giá liệu các mô hình CnR có thể tạo ra các lời phê bình chính xác và đầy đủ hay không.

Để làm điều này, chúng tôi tận dụng GPT4 để tạo ra một tập dữ liệu chẩn đoán để đo lường chất lượng phê bình. Với một loại lỗi cụ thể (ví dụ, tuân theo hướng dẫn, tính chính xác thực tế, v.v.) và một số ví dụ trong ngữ cảnh, GPT4 được hướng dẫn tạo ra các cặp lời nhắc và phản hồi có lỗi. Tập chẩn đoán kết quả có 551 mẫu, mỗi mẫu bao gồm (i) lời nhắc, (ii) phản hồi, (iii) loại lỗi, (iv) phản hồi ngôn ngữ tự nhiên giải thích các lỗi trong phản hồi.

Hình 6: Phân bố các loại lỗi cho các phản hồi được đề cập bởi các lời phê bình trong dữ liệu CnR.

Bảng 2: Đánh giá các lời phê bình được tạo ra trên tập dữ liệu chẩn đoán. ChatGPT được sử dụng để đánh giá lời phê bình dựa trên phạm vi bao phủ của phản hồi thực tế. Chúng tôi bao gồm hiệu suất tổng thể, cũng như hiệu suất trên sáu loại lỗi: tính chính xác, tuân theo hướng dẫn, sự rõ ràng, an toàn, tính đầy đủ/chi tiết/chiều sâu, sự liên quan.

Tên | Tổng thể | Chính xác | Hướng dẫn | Rõ ràng | An toàn | Đầy đủ | Liên quan
Falcon-D-CnR | 3.0 | 2.7 | 2.9 | 3.3 | 3.0 | 3.1 | 3.0
Falcon-I-CnR | 3.2 | 2.9 | 3.1 | 3.5 | 3.1 | 3.4 | 3.3
ChatGPT | 3.7 | 3.9 | 3.6 | 3.8 | 3.4 | 3.9 | 3.6

Sử dụng tập kiểm tra chẩn đoán này, chúng tôi tạo ra các lời phê bình với một số mô hình CnR. Để đánh giá các lời phê bình được tạo ra, ChatGPT được sử dụng để đo lường mức độ (1-5) mà các lời phê bình được tạo ra bao phủ loại lỗi và phản hồi cấp cao liên quan đến lời nhắc và phản hồi.

Kết quả trong Bảng 2 cho thấy hiệu suất của hai mô hình CnR, cũng như lời phê bình được tạo ra bởi ChatGPT. Đáng chú ý là các mô hình CnR 40B của chúng tôi được chỉ ra hoạt động tốt hơn trong việc xác định các vấn đề liên quan đến sự rõ ràng và tính đầy đủ (3.5; 3.4) so với tính chính xác hoặc tuân theo hướng dẫn (2.9; 3.1). Vì hai loại lỗi sau thường yêu cầu kiến thức thực tế, điều này có thể là hệ quả của mô hình cơ sở. Trên sáu loại lỗi, sự khác biệt giữa Falcon-I-CnR và Falcon-D-CnR là lớn nhất về tính đầy đủ (+0.3).

7.3 Chất lượng sửa đổi
Tương tự, tiếp theo chúng tôi mong muốn tách riêng và đánh giá độc lập bước sửa đổi: (p, i, c)→r, để xác định liệu các mô hình CnR của chúng tôi có thể sửa đổi hiệu quả các phản hồi theo một lời phê bình được cung cấp hay không.

Bảng 3: Đánh giá các mô hình sửa đổi trên tập dữ liệu chẩn đoán. Bảng cho thấy sự tuân thủ của các sửa đổi đối với (i) các lời phê bình chi tiết, và (ii) các lời phê bình thô, ít chi tiết. Cột cuối cùng cho thấy sự tuân thủ của một sửa đổi được tạo ra dựa trên lời phê bình thô, đối với lời phê bình chi tiết tương ứng.

Tên | Chi tiết | Thô | Thô đến Chi tiết
Falcon-D-CnR | 4.86 | 4.85 | 4.77
Falcon-I-CnR | 4.90 | 4.94 | 4.89
ChatGPT | 4.99 | 4.97 | 4.94

Bắt đầu với tập dữ liệu chẩn đoán được sử dụng để đo lường chất lượng phê bình, chúng tôi sử dụng ChatGPT để truyền đạt phản hồi thực tế cấp cao như (i) lời phê bình chi tiết, có thể hành động và (ii) lời phê bình thô, ít chi tiết. Tập dữ liệu chẩn đoán kết quả để đo lường chất lượng sửa đổi bao gồm 551 mẫu, mỗi mẫu chứa (i) lời nhắc, (ii) phản hồi, (iii) loại lỗi, (iv) lời phê bình thô, (v) lời phê bình chi tiết.

Sử dụng tập kiểm tra chẩn đoán này, chúng tôi tạo ra các sửa đổi với một số mô hình CnR của chúng tôi. Để đánh giá các sửa đổi được tạo ra, ChatGPT được sử dụng để đo lường mức độ (1-5) mà các sửa đổi tuân thủ lời phê bình. Tuân thủ lời phê bình yêu cầu rằng các sửa đổi giải quyết các vấn đề được nêu ra bởi lời phê bình, và tuân theo tất cả các hướng dẫn và thông số kỹ thuật được đề cập bởi lời phê bình.

Kết quả được hiển thị trong Bảng 3 chứng minh rằng các mô hình CnR của chúng tôi tuân thủ hiệu quả các lời phê bình được cung cấp, gợi ý rằng công trình tương lai nên tập trung nhiều hơn vào việc cải thiện việc tạo phê bình. Cột ngoài cùng bên phải của bảng đánh giá liệu các sửa đổi được tạo ra dựa trên các lời phê bình thô, có tuân thủ các lời phê bình chi tiết tương ứng hay không. Sự khác biệt về hiệu suất giữa cột thứ hai (Chi tiết) và cột cuối cùng (Thô đến Chi tiết) cho thấy mức độ mà một mô hình sửa đổi cụ thể mạnh mẽ đối với các lời phê bình được chỉ định dưới mức. Sự giảm hiệu suất này nhỏ hơn đối với Falcon-I-CnR (-0.01) so với Falcon-D-CnR (-0.09), gợi ý rằng việc tiếp tục tiền huấn luyện của Falcon-40B-Instruct tạo điều kiện cho một mô hình sửa đổi mạnh mẽ hơn.

7.4 Phân tích lỗi
Để phân tích cách mô hình CnR của chúng tôi hoạt động trên các loại lời nhắc khác nhau, chúng tôi phân loại 80 truy vấn FastChat thành 8 loại: lập trình & toán, fermi, viết, phản thực tế, thường thức, thu thập kiến thức, nhập vai và chung (mỗi loại đều bao gồm 10 mẫu một cách bình đẳng).

Hình 7 trình bày biểu đồ tỷ lệ thắng cho mỗi loại giữa các phản hồi ChatGPT gốc và những phản hồi được sửa đổi bởi Falcon-I-CnR (điểm trung bình của hai điểm đánh giá GPT4 thu được trong cả hai hướng được sử dụng để so sánh). Như có thể thấy, mô hình CnR của chúng tôi có thể cải thiện các phản hồi cho các loại lời nhắc phản thực tế, thường thức, thu thập kiến thức, nhập vai và chung, trong khi nó không thể sửa đổi tốt các loại mẫu lập trình, toán, fermi và viết. Việc chú thích dữ liệu CnR của chúng tôi không bao gồm các loại lời nhắc lập trình, toán và fermi, điều này có thể giải thích một phần lý do tại sao CnR hoạt động kém trong những loại này. Bên cạnh đó, tất cả các LLM mã nguồn mở hiện tại bao gồm Falcon-40B (được sử dụng làm mô hình tiền huấn luyện cơ sở cho CnR) vẫn gặp khó khăn với các tác vụ phức tạp yêu cầu khả năng lý luận tiên tiến, ví dụ lập trình, toán, vật lý, v.v.

Về các mẫu lỗi khi mô hình CnR của chúng tôi không thể cải thiện các phản hồi gốc từ ChatGPT, chúng tôi thấy rằng chúng rất tập trung vào hai loại thiếu hụt: ít chi tiết hơn và ít có cấu trúc hơn. Mô hình CnR của chúng tôi có thể sửa đổi một phản hồi dài từ ChatGPT để ngắn gọn và súc tích hơn, điều này không được ưa thích bởi đánh giá GPT4 được sử dụng nhưng có thể được ưa thích trong một số trường hợp sử dụng như trò chuyện phiếm và trợ lý cá nhân dựa trên giọng nói. Trong một số trường hợp, các phản hồi được sửa đổi có thể mất định dạng có cấu trúc tốt như danh sách có dấu đầu dòng. Sự suy giảm như vậy về định dạng và cấu trúc phản hồi dẫn đến chất lượng sửa đổi thấp hơn trong loại viết ở Hình 7. Chúng tôi cung cấp các ví dụ định tính cho cả hai loại thiếu hụt trong Phụ lục.

Hình 7: Biểu đồ tỷ lệ thắng giữa các phản hồi ChatGPT gốc và những phản hồi được sửa đổi bởi Falcon-I-CnR cho mỗi loại trong số 8 loại trên tập kiểm tra FastChat.

7.5 Hạn chế
Có một số hạn chế trong dữ liệu CnR hiện tại mà chúng tôi thu thập: 1) Các loại lời nhắc cần được mở rộng để bao gồm nhiều tác vụ khó hơn như lập trình, toán, lý luận, v.v.; 2) Chất lượng phản hồi ban đầu cần được tăng cường bằng cách sử dụng các LLM tiên tiến hơn; 3) Tập lời nhắc hiện tại chỉ chứa các lời nhắc một lượt, cần được mở rộng để bao gồm các tương tác nhiều lượt; 4) Chất lượng phản hồi được sửa đổi được chú thích cũng nên được cải thiện để toàn diện và có cấu trúc hơn. Một hạn chế đáng chú ý của chất lượng mô hình CnR của chúng tôi là trong mô hình cơ sở được sử dụng để tinh chỉnh. Các LLM mạnh hơn cho huấn luyện CnR có thể dẫn đến khả năng phê bình và sửa đổi tốt hơn.

8 Kết luận
Trong công trình này, chúng tôi trình bày một mô hình phê bình và sửa đổi (CnR) có thể cung cấp một lời phê bình bao gồm cả các khía cạnh tích cực và tiêu cực cho một phản hồi với một lời nhắc, và tạo ra thêm một phản hồi được sửa đổi theo lời phê bình. Một mô hình CnR như vậy được đạt được bằng cách tinh chỉnh một LLM tinh chỉnh hướng dẫn trên ít hơn 1000 mẫu CnR. Đáng chú ý, mặc dù mô hình CnR được phái sinh từ một LLM mã nguồn mở yếu hơn nhiều so với ChatGPT, nó có thể cải thiện các phản hồi ChatGPT và có thể tiếp tục cải thiện thông qua các sửa đổi lặp lại, đạt tới 65.9% tỷ lệ thắng sau năm lần lặp. Phương pháp này có thể mở ra những ứng dụng thú vị như cải thiện dữ liệu huấn luyện cho tinh chỉnh có giám sát, cải thiện mô hình phần thưởng cho RLHF và hơn thế nữa.
