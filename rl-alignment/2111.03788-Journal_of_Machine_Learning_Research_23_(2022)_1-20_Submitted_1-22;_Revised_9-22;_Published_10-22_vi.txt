# 2111.03788.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2111.03788.pdf
# Kích thước tệp: 719120 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Journal of Machine Learning Research 23 (2022) 1-20 Gửi bài 1/22; Sửa đổi 9/22; Xuất bản 10/22
d3rlpy: Một Thư viện Học Tăng cường Sâu Ngoại tuyến
Takuma Seno seno@ailab.ics.keio.ac.jp
Đại học Keio
Kanagawa, Nhật Bản
Sony AI
Tokyo, Nhật Bản
Michita Imai michita@ailab.ics.keio.ac.jp
Đại học Keio
Kanagawa, Nhật Bản
Biên tập viên: Alexandre Gramfort
Tóm tắt
Trong bài báo này, chúng tôi giới thiệu d3rlpy, một thư viện học tăng cường (RL) sâu ngoại tuyến mã nguồn mở cho Python. d3rlpy hỗ trợ một tập hợp các thuật toán RL sâu ngoại tuyến cũng như các thuật toán trực tuyến off-policy thông qua API plug-and-play được tài liệu hóa đầy đủ. Để giải quyết vấn đề tái tạo, chúng tôi tiến hành một benchmark quy mô lớn với D4RL và tập dữ liệu Atari 2600 để đảm bảo chất lượng triển khai và cung cấp các script thử nghiệm và bảng kết quả đầy đủ. Mã nguồn d3rlpy có thể được tìm thấy trên GitHub: https://github.com/takuseno/d3rlpy .
Từ khóa: học tăng cường ngoại tuyến, học tăng cường sâu, tái tạo, phần mềm mã nguồn mở, pytorch

1. Giới thiệu
Học tăng cường sâu (RL) đã dẫn đến những tiến bộ đáng kể trong nhiều lĩnh vực như chơi game (Wurman et al., 2022) và robot (Lee et al., 2020). Mặc dù các thuật toán RL có tiềm năng giải quyết các tác vụ phức tạp, thu thập dữ liệu chủ động là một thách thức lớn đặc biệt đối với các môi trường mà tương tác là tốn kém. RL ngoại tuyến (Levine et al., 2020), nơi các thuật toán tìm một chính sách tốt trong một tập dữ liệu tĩnh đã được thu thập trước đó, đã được xem xét như một giải pháp cho vấn đề này.

Mặc dù các bài báo RL sâu ngoại tuyến gần đây được xuất bản với các triển khai do tác giả cung cấp, chúng được phân tán trên các kho lưu trữ khác nhau và không cung cấp giao diện chuẩn hóa, điều này khiến các nhà nghiên cứu khó khăn trong việc tích hợp các thuật toán vào dự án của họ. Việc các nhà nghiên cứu có quyền truy cập vào các triển khai được đánh giá chính xác cũng rất quan trọng để giải quyết vấn đề tái tạo (Henderson et al., 2017).

Trong bài báo này, chúng tôi giới thiệu Data-Driven Deep Reinforcement Learning library for Python (d3rlpy), một thư viện RL sâu ngoại tuyến cho Python. d3rlpy cung cấp một tập hợp các thuật toán RL off-policy ngoại tuyến và trực tuyến được xây dựng với PyTorch (Paszke et al., 2019). API của tất cả các thuật toán đã triển khai được tài liệu hóa đầy đủ, plug-and-play và chuẩn hóa để người dùng có thể dễ dàng bắt đầu thử nghiệm với d3rlpy. Để giải quyết vấn đề tái tạo trong RL ngoại tuyến, một benchmark chính xác quy mô lớn được tiến hành với d3rlpy.

©2022 Takuma Seno và Michita Imai.
Giấy phép: CC-BY 4.0, xem https://creativecommons.org/licenses/by/4.0/ . Yêu cầu ghi nhận được cung cấp tại http://jmlr.org/papers/v23/22-0017.html .arXiv:2111.03788v2 [cs.LG] 3 Dec 2022

--- TRANG 2 ---
Seno và Imai

2. Công trình liên quan
Việc lựa chọn thiết kế API quyết định trải nghiệm người dùng, điều này phải cân bằng sự đánh đổi giữa tính dễ sử dụng và tính linh hoạt. KerasRL (Plappert, 2016) và Stable-Baselines3 (Raffin et al., 2021) cung cấp các thuật toán RL sâu với API plug-and-play và tài liệu mở rộng. Tensorforce (Kuhnle et al., 2017), MushroomRL (D'Eramo et al., 2021) và Tianshou (Weng et al., 2021) cung cấp các thành phần RL sâu được module hóa cho phép người dùng tiến hành các thử nghiệm tùy chỉnh. SaLinA (Denoyer et al., 2021) cung cấp một framework tổng quát cho các agent ra quyết định nơi người dùng có thể triển khai các thuật toán có thể mở rộng trên đó. Để khuyến khích cộng đồng RL rộng lớn hơn bắt đầu nghiên cứu RL ngoại tuyến, d3rlpy được thiết kế để trở thành thư viện đầu tiên cung cấp API plug-and-play được tài liệu hóa đầy đủ cho các thử nghiệm RL ngoại tuyến.

Từ góc độ tái tạo, ChainerRL (Fujita et al., 2021) và Tonic (Pardo, 2020) cung cấp nhiều thuật toán RL sâu với kết quả tái tạo chính xác. Dopamine (Castro et al., 2018) cũng là triển khai được sử dụng rộng rãi trong cộng đồng, tập trung vào việc làm cho các biến thể DQN (Hessel et al., 2018) có sẵn cho các nhà nghiên cứu. d3rlpy cũng là thư viện đầu tiên đi kèm với một số thuật toán RL ngoại tuyến và các kết quả benchmark mở rộng trong lĩnh vực nghiên cứu này.

Tích hợp API plug-and-play được tài liệu hóa đầy đủ và một số thuật toán RL ngoại tuyến được đánh giá chính xác một cách tổng thể, các thư viện hiện có khó có thể ngay lập tức đạt được trải nghiệm nghiên cứu RL ngoại tuyến tương tự như d3rlpy.

3. Thiết kế của d3rlpy
Trong phần này, thiết kế thư viện của d3rlpy được mô tả.

3.1 Giao diện thư viện
```python
import d3rlpy
# chuẩn bị đối tượng MDPDataset
dataset, env = d3rlpy.datasets.get_dataset("hopper-medium-v0")
# chuẩn bị đối tượng Algorithm
sac = d3rlpy.algos.SAC(actor_learning_rate=3e-4, use_gpu=0)
# bắt đầu huấn luyện ngoại tuyến
sac.fit(
    dataset,
    n_steps=500000,
    eval_episodes=dataset.episodes,
    scorers={"environment": d3rlpy.metrics.evaluate_on_environment(env)},
)
# liền mạch bắt đầu huấn luyện trực tuyến
sac.fit_online(env, n_steps=500000)
```

d3rlpy cung cấp API theo phong cách scikit-learn (Pedregosa et al., 2011) để việc sử dụng thư viện này trở nên dễ dàng nhất có thể. Về mặt thiết kế thư viện, có hai điểm khác biệt chính so với các thư viện hiện có. Thứ nhất, d3rlpy có giao diện cho huấn luyện RL ngoại tuyến, lấy một thành phần tập dữ liệu RL chuyên dụng, MDPDataset được mô tả trong Phần 3.2. Thứ hai, tất cả các phương thức huấn luyện như fit và fit_online được triển khai trong các thành phần Algorithm để làm cho d3rlpy trở nên plug-and-play nhất có thể. Đối với trải nghiệm người dùng plug-and-play, các kiến trúc mạng neural được tự động chọn từ MLP và mô hình tích chập (Mnih et al.,

--- TRANG 3 ---
d3rlpy: Một Thư viện Học Tăng cường Sâu Ngoại tuyến

EncoderFactory VectorEncoderFactory, …OptimizerFactory SGDFactory, AdamFactory, …Algorithm DQN, DDPG, SAC, BEAR, CQL, PLAS, …AlgorithmImpl DQNImpl, DDPGImpl, SACImpl, BEARImpl, CQLImpl, PLASImpl, …MDPDatasetEnvironment (giao diện OpenAI Gym)QFunctionFactory MeanQFunctionFactory, …Model DiscreteMeanQFunction, ContinuousMeanQFunction, SquashedNormalPolicy, …. . .Scaler StandardScaler, MinMaxScaler, …ActionScaler MinMaxActionScalerRewardScaler StandardRewardScaler, …

Hình 1: Minh họa các thành phần module trong d3rlpy. MDPDataset và môi trường theo phong cách OpenAI Gym có thể được sử dụng để huấn luyện chính sách.

2015) tùy thuộc vào quan sát, điều này cho phép người dùng bắt đầu huấn luyện mà không cần soạn các mô hình mạng neural trừ khi sử dụng các kiến trúc tùy chỉnh được mô tả trong Phần 3.2. Những lựa chọn thiết kế này được kỳ vọng sẽ hạ thấp rào cản để bắt đầu sử dụng thư viện này.

Vì d3rlpy hỗ trợ cả huấn luyện ngoại tuyến và trực tuyến, việc chuyển đổi liền mạch từ huấn luyện ngoại tuyến sang tinh chỉnh trực tuyến được thực hiện. Tinh chỉnh các chính sách được huấn luyện ngoại tuyến là cần thiết, nhưng vẫn là một vấn đề thách thức (Nair et al., 2020; Kostrikov et al., 2021). Sự chuyển đổi liền mạch này hỗ trợ nghiên cứu sâu hơn bằng cách cho phép các nhà nghiên cứu RL dễ dàng tiến hành các thử nghiệm tinh chỉnh.

3.2 Thành phần
Chúng tôi nổi bật các thành phần chính được cung cấp bởi d3rlpy. Hình 1 mô tả các thành phần module trong d3rlpy. Tất cả các thành phần này cung cấp API chuẩn hóa. Tài liệu đầy đủ bao gồm các hướng dẫn mở rộng của thư viện có sẵn tại https://d3rlpy.readthedocs.io .

Algorithm. Các thành phần Algorithm cung cấp các phương thức huấn luyện ngoại tuyến và trực tuyến được mô tả trong Phần 3.1. Algorithm được triển khai trong thiết kế phân cấp mà nội bộ khởi tạo AlgorithmImpl. Hệ thống phân cấp này nhằm cung cấp API thân thiện với người dùng ở cấp cao như phương thức fit cho Algorithm và API cấp thấp như update_actor và update_critic cho AlgorithmImpl. Động lực chính của hệ thống API phân cấp này là tăng khả năng tái sử dụng module của các thuật toán khi thuật toán mới chỉ yêu cầu thay đổi cấp cao. Ví dụ, cập nhật chính sách trì hoãn của TD3, cập nhật tham số chính sách mỗi hai bước gradient, có thể được triển khai bằng cách điều chỉnh tần suất của các lời gọi phương thức update_actor trong module cấp cao mà không thay đổi logic cấp thấp.

MDPDataset. MDPDataset cung cấp giao diện tập dữ liệu RL ngoại tuyến chuẩn hóa. Người dùng có thể xây dựng tập dữ liệu riêng của họ bằng cách sử dụng dữ liệu đã ghi gồm các mảng numpy (Harris et al., 2020) của observations, actions, rewards, terminals và tùy chọn timeouts (Pardo et al., 2018). Các tập dữ liệu benchmark phổ biến như D4RL và tập dữ liệu Atari 2600 cũng được cung cấp bởi gói d3rlpy.datasets chuyển đổi chúng thành đối tượng MDPDataset. Ngoài ra, d3rlpy hỗ trợ thu thập dữ liệu tự động bằng cách cung cấp môi trường theo phong cách OpenAI Gym (Brockman

--- TRANG 4 ---
Seno và Imai

et al., 2016), xuất dữ liệu đã thu thập dưới dạng đối tượng MDPDataset. Đối với các tập hợp tạo tập dữ liệu đa dạng, việc thu thập dữ liệu có thể được thực hiện với và không có cập nhật tham số.

EncoderFactory. Các mô hình mạng neural tùy chỉnh do người dùng định nghĩa được hỗ trợ thông qua các thành phần EncoderFactory. Người dùng có thể định nghĩa tất cả các bộ xấp xỉ hàm để huấn luyện trong d3rlpy bằng cách xây dựng các thành phần EncoderFactory riêng của họ. Tính linh hoạt này cho phép sử dụng các kiến trúc phức tạp (He et al., 2016) và thử nghiệm với các mô hình được huấn luyện trước một phần (Shah và Kumar, 2021).

QFunctionFactory. d3rlpy cung cấp các thành phần QFunctionFactory cho phép người dùng sử dụng các Q-function phân phối: Quantile Regression (Dabney et al., 2018b) và Implicit Quantile Network (Dabney et al., 2018a). Các Q-function phân phối cải thiện hiệu suất đáng kể bằng cách nắm bắt phương sai của lợi nhuận. Không giống như các thư viện RL thông thường triển khai các Q-function phân phối dưới dạng biến thể DQN, d3rlpy cho phép người dùng sử dụng chúng với tất cả các thuật toán đã triển khai, điều này giảm độ phức tạp để hỗ trợ các biến thể thuật toán như QR-DQN (Dabney et al., 2018b) và phiên bản rời rạc của CQL (Kumar et al., 2020).

Scaler, ActionScaler và RewardScaler. Bằng cách khai thác tập dữ liệu tĩnh trong huấn luyện RL ngoại tuyến, d3rlpy cung cấp các phương thức tiền xử lý và hậu xử lý khác nhau thông qua các thành phần Scaler, ActionScaler và RewardScaler. Đối với tiền xử lý quan sát, có sẵn normalization, standardization và pixel. Chuẩn hóa quan sát đã được chứng minh là cải thiện hiệu suất chính sách trong thiết lập RL ngoại tuyến (Fujimoto và Gu, 2021). Về tiền xử lý hành động, normalization có sẵn và đầu ra hành động từ chính sách đã huấn luyện được denormalize về quy mô ban đầu như hậu xử lý. Cuối cùng, tiền xử lý phần thưởng hỗ trợ normalization, standardization, clip và constant multiplication.

4. Benchmark quy mô lớn
Để giải quyết vấn đề tái tạo (Henderson et al., 2017), các thuật toán đã triển khai¹ được đánh giá chính xác với D4RL (Fu et al., 2020) và tập dữ liệu Atari 2600 (Agarwal et al., 2020). Các script Python đầy đủ được sử dụng trong benchmark này cũng được bao gồm trong mã nguồn của chúng tôi², cho phép người dùng tiến hành các thử nghiệm benchmark bổ sung. Bảng đầy đủ các kết quả benchmark được báo cáo trong Phụ lục A, Phụ lục B và Phụ lục C. Tất cả các số liệu đã ghi được phát hành trong kho lưu trữ GitHub của chúng tôi³.

5. Kết luận
Trong bài báo này, chúng tôi đã giới thiệu một thư viện học tăng cường sâu ngoại tuyến, d3rlpy. d3rlpy cung cấp một tập hợp các thuật toán RL ngoại tuyến và trực tuyến thông qua API plug-and-play chuẩn hóa. Benchmark chính xác quy mô lớn đã được tiến hành để giải quyết vấn đề tái tạo.

1. d3rlpy triển khai NFQ (Riedmiller, 2005), DQN (Mnih et al., 2015), Double DQN (van Hasselt et al., 2015), DDPG (Lillicrap et al., 2016), TD3 (Fujimoto et al., 2018), SAC (Haarnoja et al., 2018), BCQ (Fujimoto et al., 2019b), BEAR (Kumar et al., 2019), CQL (Kumar et al., 2020), AWAC (Nair et al., 2020), CRR (Wang et al., 2020), PLAS (Zhou et al., 2020), PLAS+P (Zhou et al., 2020), TD3+BC (Fujimoto và Gu, 2021) và IQL (Kostrikov et al., 2021).
2. https://github.com/takuseno/d3rlpy/tree/master/reproductions
3. https://github.com/takuseno/d3rlpy-benchmarks

--- TRANG 5 ---
d3rlpy: Một Thư viện Học Tăng cường Sâu Ngoại tuyến

Lời cảm ơn
Công trình này được hỗ trợ bởi Information-technology Promotion Agency, Japan (IPA), Exploratory IT Human Resources Project (MITOU Program) trong năm tài chính 2020. Chúng tôi muốn bày tỏ lòng biết ơn chân thành đối với những đóng góp của các cộng tác viên tự nguyện. Chúng tôi cũng muốn cảm ơn người dùng đã cung cấp phản hồi và nhận xét mang tính xây dựng.

Phụ lục A. Kết quả Benchmark: D4RL
Chúng tôi đánh giá các thuật toán đã triển khai trên benchmark D4RL mã nguồn mở của các tác vụ OpenAI Gym MuJoCo (Brockman et al., 2016; Fu et al., 2020). Chúng tôi tuân theo quy trình thử nghiệm được mô tả trong Fu et al. (2020). Chúng tôi huấn luyện mỗi thuật toán trong 500K bước gradient và đánh giá mỗi 5000 bước để thu thập hiệu suất đánh giá trong môi trường cho 10 tập phim. Bảng 1 hiển thị các siêu tham số được sử dụng trong benchmark. Chúng tôi sử dụng cùng các siêu tham số như những siêu tham số đã được báo cáo trước đó trong các bài báo trước hoặc được khuyến nghị trong các kho lưu trữ do tác giả cung cấp. Chúng tôi sử dụng hệ số giảm giá 0.99, tỷ lệ cập nhật mục tiêu 5e-3 và bộ tối ưu hóa Adam (Kingma và Ba, 2014) trên tất cả các thuật toán. Kiến trúc mặc định là MLP với các lớp ẩn [256, 256] trừ khi chúng tôi giải quyết rõ ràng. Chúng tôi lặp lại tất cả các thử nghiệm với 10 seed ngẫu nhiên.

Bảng 2 hiển thị kết quả của benchmark theo quy mô chuẩn hóa (Fu et al., 2020). Bảng 3, 4, 5, 6, 7, 8, 9, 10 và 11 hiển thị so sánh cạnh nhau với điểm số tham chiếu. CRR không được bao gồm trong so sánh cạnh nhau này vì CRR chưa được đánh giá với D4RL và triển khai do tác giả cung cấp không có sẵn công khai. Xem xét thực tế rằng độ lệch chuẩn cho một số thuật toán không được báo cáo bởi các tác giả và nhiều thuật toán trong nghiên cứu RL ngoại tuyến ban đầu chỉ được đánh giá với 3 hoặc 4 seed ngẫu nhiên, chúng tôi tin rằng sự khác biệt về hiệu suất là không đáng kể.

Thuật toán Siêu tham số Giá trị
SAC (Haarnoja et al., 2018) Tỷ lệ học Critic 3e-4
Tỷ lệ học Actor 3e-4
Kích thước mini-batch 256
AWAC (Nair et al., 2020) Tỷ lệ học Critic 3e-4
Tỷ lệ học Actor 3e-4
Kích thước mini-batch 1024
λ (Nair et al., 2020) 1
Đơn vị ẩn Actor [256, 256, 256, 256]
Phân rã trọng số Actor 1e-4
Đơn vị ẩn Critic [256, 256, 256, 256]
BCQ (Fujimoto et al., 2019b) Tỷ lệ học Critic 1e-3
Tỷ lệ học Actor 1e-3
Tỷ lệ học VAE 1e-3
Kích thước mini-batch 100
ϕ (Fujimoto et al., 2019b) 0.75
Đơn vị ẩn Critic [400, 300]
Đơn vị ẩn Actor [400, 300]
Đơn vị ẩn VAE encoder [750, 750]
Đơn vị ẩn VAE decoder [750, 750]
Kích thước latent VAE 2|A|
Phạm vi nhiễu loạn 0.05
Mẫu hành động 100
BEAR (Kumar et al., 2019)⁴ Tỷ lệ học Critic 3e-4
Tỷ lệ học Actor 1e-4
Tỷ lệ học VAE 3e-4

--- TRANG 6 ---
Seno và Imai

Tỷ lệ học ε 1e-3
Kích thước mini-batch 256
Đơn vị ẩn VAE encoder [750, 750]
Đơn vị ẩn VAE decoder [750, 750]
Kích thước latent VAE 2|A|
σ MMD 20
Nhân MMD laplacian (gaussian cho HalfCheetah)
Mẫu hành động MMD 4
ε threshold 0.05
Mẫu hành động mục tiêu 10
Mẫu hành động đánh giá 100
Bước huấn luyện trước 40000
CQL (Kumar et al., 2020)⁵ Tỷ lệ học Critic 3e-4
Tỷ lệ học Actor 1e-4
α Fixed 5 (10 cho tập dữ liệu medium)
Kích thước mini-batch 256
Đơn vị ẩn Critic [256, 256, 256]
Đơn vị ẩn Actor [256, 256, 256]
Mẫu hành động 10
CRR (Wang et al., 2020) Tỷ lệ học Critic 3e-4
Tỷ lệ học Actor 3e-4
Kích thước mini-batch 256
Mẫu hành động 4
Loại Advantage mean
Loại Weight binary
IQL (Kostrikov et al., 2021) Tỷ lệ học Critic 3e-4
Tỷ lệ học Actor 3e-4
Tỷ lệ học V-function 3e-4
Kích thước mini-batch 256
Expectile 0.7
Nhiệt độ nghịch đảo 3.0
Bộ lập lịch tỷ lệ học Actor Cosine
PLAS (Zhou et al., 2020) Tỷ lệ học Critic 1e-3
Tỷ lệ học Actor 1e-4
Tỷ lệ học VAE 1e-4
Kích thước mini-batch 100
Đơn vị ẩn Critic [400, 300]
Đơn vị ẩn Actor [400, 300]
Đơn vị ẩn VAE encoder [750, 750] ([128, 128] cho medium-replay)
Đơn vị ẩn VAE decoder [750, 750] ([128, 128] cho medium-replay)
ϕ (Fujimoto et al., 2019b) 1.0
Kích thước latent VAE 2|A|
Bước huấn luyện trước VAE 500000
PLAS+P (Zhou et al., 2020) Tỷ lệ học Critic 1e-3
Tỷ lệ học Actor 1e-4
Tỷ lệ học VAE 1e-4
Kích thước mini-batch 100
Đơn vị ẩn Critic [400, 300]
Đơn vị ẩn Actor [400, 300]
Đơn vị ẩn VAE encoder [750, 750] ([128, 128] cho medium-replay)
Đơn vị ẩn VAE decoder [750, 750] ([128, 128] cho medium-replay)
ϕ (Fujimoto et al., 2019b) 1.0
Kích thước latent VAE 2|A|
Bước huấn luyện trước VAE 500000
Phạm vi nhiễu loạn Phụ lục D trong Zhou et al. (2020)
TD3+BC (Fujimoto và Gu, 2021) Tỷ lệ học Critic 3e-4
Tỷ lệ học Actor 3e-4
Kích thước mini-batch 256
Nhiễu chính sách 0.2
Cắt nhiễu chính sách (-0.5, 0.5)
Tần suất cập nhật chính sách 2
α (Fujimoto và Gu, 2021) 2.5
Tiền xử lý quan sát standardization

--- TRANG 7 ---
d3rlpy: Một Thư viện Học Tăng cường Sâu Ngoại tuyến

Bảng 1: Siêu tham số cho D4RL.
4. https://github.com/Farama-Foundation/d4rl_evaluations
5. https://github.com/aviralkumar2907/CQL

--- TRANG 8 ---
Seno và Imai

Tập dữ liệu SAC AWAC BCQ BEAR CQL CRR IQL PLAS PLAS+P TD3+BC
halfcheetah-random-v0 30.2±1.9 15.2±1.3 2.3±0.0 2.3±0.0 29.7±1.5 18.9±7.0 14.4±2.3 26.9±1.4 27.3±2.3 11.4±1.5
walker2d-random-v0 2.5±1.7 4.3±1.9 4.2±1.5 4.6±1.4 2.0±2.8 2.3±1.9 5.8±0.3 6.5±7.2 4.5±5.0 2.1±2.4
hopper-random-v0 1.1±0.6 11.1±0.1 10.5±0.2 10.1±0.2 10.8±0.1 10.9±1.3 11.1±0.1 10.4±0.3 12.5±1.5 10.9±0.1
halfcheetah-medium-v0 30.1±10.6 41.9±0.4 40.1±0.5 36.6±0.9 41.7±0.2 41.9±0.4 41.1±0.2 39.8±0.4 42.0±0.6 42.4±1.5
walker2d-medium-v0 2.1±5.3 65.6±11.3 47.7±7.8 57.4±10.5 77.8±5.1 43.3±17.5 59.7±8.9 33.0±9.7 66.1±6.8 76.7±3.2
hopper-medium-v0 1.2±0.8 40.7±20.3 52.5±22.8 34.8±8.0 50.1±19.9 26.1±32.3 31.1±0.3 58.1±23.7 53.6±24.0 95.9±12.1
halfcheetah-medium-replay-v0 38.1±7.3 42.4±1.4 39.0±1.9 37.1±2.0 43.6±3.0 42.0±0.4 40.9±0.9 43.9±0.4 44.8±1.0 42.8±2.1
walker2d-medium-replay-v0 4.5±3.8 22.5±7.0 15.2±4.6 13.6±3.1 20.5±4.2 26.7±4.6 14.3±4.4 20.9±13.3 9.2±10.4 26.4±5.7
hopper-medium-replay-v0 8.5±11.8 34.0±4.1 16.1±7.7 27.8±6.3 31.1±3.4 13.3±14.0 38.1±5.3 17.5±13.3 20.0±26.2 31.4±8.0
halfcheetah-medium-expert-v0 0.0±2.3 16.1±4.2 60.1±11.1 45.4±7.4 9.0±2.8 8.4±1.7 55.2±7.9 81.2±9.6 81.6±8.1 89.2±6.8
walker2d-medium-expert-v0 1.7±2.9 7.9±22.0 43.6±14.0 59.1±9.6 54.7±37.3 41.1±12.3 92.3±22.3 93.5±9.1 86.3±10.4 91.0±14.4
hopper-medium-expert-v0 7.8±9.1 42.7±46.5 111.5±2.8 77.8±19.9 105.1±7.8 0.8±0.1 112.3±0.2 110.8±31.8 77.5±50.4 112.3±0.3

Bảng 2: Điểm số chuẩn hóa và độ lệch chuẩn được thu thập với chính sách đã huấn luyện cuối cùng trong D4RL. Điểm số được tính trung bình trên 10 seed ngẫu nhiên.

--- TRANG 9 ---
d3rlpy: Một Thư viện Học Tăng cường Sâu Ngoại tuyến

Tập dữ liệu d3rlpy tham chiếu
halfcheetah-random-v0 30.2±1.9 30.5
walker2d-random-v0 2.5±1.7 4.1
hopper-random-v0 1.1±0.6 11.3
halfcheetah-medium-v0 30.1±10.6 -4.3
walker2d-medium-v0 2.1±5.3 0.9
hopper-medium-v0 1.2±0.8 0.8
halfcheetah-medium-replay-v0 38.1±7.3 -2.4
walker2d-medium-replay-v0 4.5±3.8 1.9
hopper-medium-replay-v0 8.5±11.8 3.5
halfcheetah-medium-expert-v0 0.0±2.3 1.8
walker2d-medium-expert-v0 1.7±2.9 -0.1
hopper-medium-expert-v0 7.8±9.1 1.6

Bảng 3: So sánh cạnh nhau với điểm số chuẩn hóa tham chiếu của SAC được báo cáo trong Fu et al. (2020), chỉ cung cấp điểm số trung bình.

Tập dữ liệu d3rlpy tham chiếu
halfcheetah-random-v0 15.2±1.3 2.2
walker2d-random-v0 4.3±1.9 5.1
hopper-random-v0 11.1±0.1 9.6
halfcheetah-medium-v0 41.9±0.4 37.4
walker2d-medium-v0 65.6±11.3 30.1
hopper-medium-v0 40.7±20.3 72.0
halfcheetah-medium-replay-v0 42.4±1.4 -
walker2d-medium-replay-v0 22.5±7.0 -
hopper-medium-replay-v0 34.0±4.1 -
halfcheetah-medium-expert-v0 16.1±4.2 36.8
walker2d-medium-expert-v0 7.9±22.0 42.7
hopper-medium-expert-v0 42.7±46.5 80.9

Bảng 4: So sánh cạnh nhau với điểm số chuẩn hóa tham chiếu của AWAC được báo cáo trong Nair et al. (2020), chỉ cung cấp điểm số trung bình.

6. https://github.com/ikostrikov/implicit_q_learning

--- TRANG 10 ---
Seno và Imai

Tập dữ liệu d3rlpy tham chiếu
halfcheetah-random-v0 2.3±0.0 2.2
walker2d-random-v0 4.2±1.5 4.9
hopper-random-v0 10.5±0.2 10.6
halfcheetah-medium-v0 40.1±0.5 40.7
walker2d-medium-v0 47.7±7.8 53.1
hopper-medium-v0 52.5±22.8 54.5
halfcheetah-medium-replay-v0 16.1±7.7 38.2
walker2d-medium-replay-v0 15.2±4.6 15.0
hopper-medium-replay-v0 16.1±7.7 33.1
halfcheetah-medium-expert-v0 60.1±11.1 64.7
walker2d-medium-expert-v0 43.6±14.0 57.5
hopper-medium-expert-v0 111.5±2.8 110.9

Bảng 5: So sánh cạnh nhau với điểm số chuẩn hóa tham chiếu của BCQ được báo cáo trong Fu et al. (2020), chỉ cung cấp điểm số trung bình.

Tập dữ liệu d3rlpy tham chiếu
halfcheetah-random-v0 2.3±0.0 2.3±2.3
walker2d-random-v0 4.6±1.4 9.0±6.2
hopper-random-v0 10.1±0.2 10.0±0.7
halfcheetah-medium-v0 36.6±0.9 37.1±2.3
walker2d-medium-v0 57.4±10.5 56.1±8.5
hopper-medium-v0 34.8±8.0 30.8±0.9
halfcheetah-medium-replay-v0 37.1±2.0 36.2±5.6
walker2d-medium-replay-v0 13.6±3.1 13.7±2.1
hopper-medium-replay-v0 27.8±6.3 31.1±0.9
halfcheetah-medium-expert-v0 45.4±7.4 44.2±13.8
walker2d-medium-expert-v0 59.1±9.6 43.8±6.0
hopper-medium-expert-v0 77.8±19.9 67.3±32.5

Bảng 6: So sánh cạnh nhau với điểm số chuẩn hóa tham chiếu của BEAR được thu thập bằng cách thực thi triển khai do tác giả cung cấp để khớp với các siêu tham số được đề xuất trên trang GitHub của họ.

--- TRANG 11 ---
d3rlpy: Một Thư viện Học Tăng cường Sâu Ngoại tuyến

Tập dữ liệu d3rlpy tham chiếu
halfcheetah-random-v0 29.7±1.5 28.5±2.4
walker2d-random-v0 2.0±2.8 1.2±2.6
hopper-random-v0 10.8±0.1 10.6±0.8
halfcheetah-medium-v0 41.7±0.2 38.8±2.5
walker2d-medium-v0 77.8±5.1 48.7±22.2
hopper-medium-v0 50.1±19.9 31.2±1.0
halfcheetah-medium-replay-v0 43.6±3.0 44.9±2.8
walker2d-medium-replay-v0 20.5±4.2 25.5±13.0
hopper-medium-replay-v0 31.1±3.4 30.1±2.2
halfcheetah-medium-expert-v0 9.0±2.8 11.3±4.9
walker2d-medium-expert-v0 54.7±37.3 75.4±52.8
hopper-medium-expert-v0 105.1±7.8 100.0±18.6

Bảng 7: So sánh cạnh nhau với điểm số chuẩn hóa tham chiếu của CQL được thu thập bằng cách thực thi triển khai do tác giả cung cấp để khớp với các siêu tham số được đề xuất trên trang GitHub của họ.

Tập dữ liệu d3rlpy tham chiếu
halfcheetah-random-v0 14.4±2.3 12.2±3.4
walker2d-random-v0 5.8±0.3 5.7±0.1
hopper-random-v0 11.1±0.1 11.3±0.1
halfcheetah-medium-v0 41.1±0.2 41.0±0.4
walker2d-medium-v0 59.7±8.9 62.8±5.5
hopper-medium-v0 31.1±0.3 31.6±0.3
halfcheetah-medium-replay-v0 40.9±0.9 39.2±1.6
walker2d-medium-replay-v0 14.3±4.4 15.3±2.4
hopper-medium-replay-v0 38.1±5.3 39.1±2.7
halfcheetah-medium-expert-v0 55.2±7.9 54.3±2.2
walker2d-medium-expert-v0 92.3±22.3 101.1±7.4
hopper-medium-expert-v0 112.3±0.2 100.5±16.8

Bảng 8: So sánh cạnh nhau với điểm số chuẩn hóa tham chiếu của IQL được thu thập bằng cách thực thi triển khai do tác giả cung cấp⁶ vì điểm số cho tập dữ liệu -v0 không được báo cáo trong bài báo gốc của họ (Kostrikov et al., 2021).

--- TRANG 12 ---
Seno và Imai

Tập dữ liệu d3rlpy tham chiếu
halfcheetah-random-v0 26.9±1.4 25.8
walker2d-random-v0 6.5±7.2 3.1
hopper-random-v0 10.4±0.3 10.5
halfcheetah-medium-v0 39.8±0.4 39.3
walker2d-medium-v0 33.0±9.7 44.6
hopper-medium-v0 58.1±23.7 32.9
halfcheetah-medium-replay-v0 38.1±7.3 43.9
walker2d-medium-replay-v0 20.9±13.3 30.2
hopper-medium-replay-v0 17.5±13.3 27.9
halfcheetah-medium-expert-v0 81.2±9.6 96.6
walker2d-medium-expert-v0 93.5±9.1 89.6
hopper-medium-expert-v0 110.8±31.8 110.0

Bảng 9: So sánh cạnh nhau với điểm số chuẩn hóa tham chiếu của PLAS được báo cáo trong Zhou et al. (2020), chỉ cung cấp điểm số trung bình.

Tập dữ liệu d3rlpy tham chiếu
halfcheetah-random-v0 27.3±2.3 28.3
walker2d-random-v0 4.5±5.0 6.8
hopper-random-v0 12.5±1.5 13.3
halfcheetah-medium-v0 42.0±0.6 42.2
walker2d-medium-v0 66.1±6.8 66.9
hopper-medium-v0 53.6±24.0 36.9
halfcheetah-medium-replay-v0 44.8±1.0 45.7
walker2d-medium-replay-v0 9.2±10.4 14.3
hopper-medium-replay-v0 20.0±26.2 51.9
halfcheetah-medium-expert-v0 81.6±8.1 99.3
walker2d-medium-expert-v0 86.3±10.4 96.2
hopper-medium-expert-v0 77.5±50.4 94.7

Bảng 10: So sánh cạnh nhau với điểm số chuẩn hóa tham chiếu của PLAS+P được báo cáo trong Zhou et al. (2020), chỉ cung cấp điểm số trung bình.

--- TRANG 13 ---
d3rlpy: Một Thư viện Học Tăng cường Sâu Ngoại tuyến

Tập dữ liệu d3rlpy tham chiếu
halfcheetah-random-v0 11.4±1.5 10.2±1.3
walker2d-random-v0 2.1±2.5 1.4±1.6
hopper-random-v0 10.9±0.1 11.0±0.1
halfcheetah-medium-v0 42.4±0.5 42.8±0.3
walker2d-medium-v0 76.7±3.2 79.7±1.8
hopper-medium-v0 95.9±12.1 99.5±1.0
halfcheetah-medium-replay-v0 42.8±2.1 43.3±0.5
walker2d-medium-replay-v0 26.4±5.7 25.2±5.1
hopper-medium-replay-v0 31.4±8.0 31.4±3.0
halfcheetah-medium-expert-v0 89.2±6.8 97.9±4.4
walker2d-medium-expert-v0 91.0±14.4 101.1±9.3
hopper-medium-expert-v0 112.3±0.3 112.2±0.2

Bảng 11: So sánh cạnh nhau với điểm số chuẩn hóa tham chiếu của TD3+BC được báo cáo trong Fujimoto và Gu (2021), cung cấp cả độ lệch chuẩn và điểm số trung bình.

--- TRANG 14 ---
Seno và Imai

Phụ lục B. Kết quả Benchmark: Atari 2600
Chúng tôi đánh giá các thuật toán đã triển khai với tập dữ liệu Atari 2600 mã nguồn mở (Agarwal et al., 2020). Chúng tôi tuân theo quy trình thử nghiệm được mô tả trong Agarwal et al. (2020). Chúng tôi sử dụng 1% phần của các chuyển tiếp (500K điểm dữ liệu) và huấn luyện mỗi thuật toán trong 12.5M bước gradient và đánh giá mỗi 125K bước để thu thập hiệu suất đánh giá trong môi trường cho 10 tập phim. Bảng 12 hiển thị các siêu tham số được sử dụng trong benchmark. Chúng tôi sử dụng cùng các siêu tham số cho QR-DQN và CQL như những siêu tham số được báo cáo trong Kumar et al. (2020). Đối với NFQ và BCQ, các siêu tham số được chọn dựa trên thiết lập QR-DQN để so sánh công bằng vì không có kết quả benchmark cho chúng với tập dữ liệu có sẵn công khai. Chúng tôi sử dụng hệ số giảm giá 0.99, bộ tối ưu hóa Adam (Kingma và Ba, 2014) và mạng neural tích chập (Mnih et al., 2015) trên tất cả các thuật toán. Lưu ý rằng chúng tôi cấu hình BCQ với Q-function Quantile Regression được giới thiệu trong Phần 3.2 để khớp với thiết lập CQL. Trong đánh giá, chúng tôi sử dụng ε-greedy với ε = 0.001 và 25% xác suất của hành động dính (Agarwal et al., 2020). Chúng tôi lặp lại tất cả các thử nghiệm với 10 seed ngẫu nhiên.

Bảng 13 hiển thị kết quả benchmark, và Bảng 14 và 15 hiển thị so sánh cạnh nhau với điểm số tham chiếu. NFQ và BCQ không được bao gồm trong so sánh cạnh nhau vì những thuật toán đó không được đánh giá với tập dữ liệu có sẵn công khai, triển khai do tác giả cung cấp cho NFQ không có sẵn công khai, và triển khai do tác giả cung cấp⁷ cho BCQ không thể áp dụng trực tiếp cho đánh giá này. Xem xét rằng các tác giả không báo cáo độ lệch chuẩn, chúng tôi tin rằng sự khác biệt về hiệu suất là không đáng kể.

Thuật toán Siêu tham số Giá trị
NFQ Tỷ lệ học 5e-5
Kích thước mini-batch 32
QR-DQN Tỷ lệ học 5e-5
Kích thước mini-batch 32
ε (Kingma và Ba, 2014) 3.125e-4
Số lượng quantile 200
Tần suất cập nhật mục tiêu 2000
BCQ Tỷ lệ học 5e-5
Kích thước mini-batch 32
ε (Kingma và Ba, 2014) 3.125e-4
Số lượng quantile 200
Tần suất cập nhật mục tiêu 2000
ξ (Fujimoto et al., 2019a) 0.3
Regularization tiền kích hoạt (Fujimoto et al., 2019a) 1e-2
CQL Tỷ lệ học 5e-5
Kích thước mini-batch 32
ε (Kingma và Ba, 2014) 3.125e-4
Số lượng quantile 200
Tần suất cập nhật mục tiêu 2000
α (Kumar et al., 2020) 4.0

Bảng 12: Siêu tham số cho tập dữ liệu Atari 2600.

7. https://github.com/sfujim/BCQ

--- TRANG 15 ---
d3rlpy: Một Thư viện Học Tăng cường Sâu Ngoại tuyến

Tập dữ liệu NFQ QR-DQN BCQ CQL
Pong 18.0±0.9 16.6±1.1 13.8±1.5 16.3±1.4
Breakout 6.4±1.0 9.0±1.9 40.7±10.8 89.8±12.2
Qbert 648.5±81.4 695.8±153.1 8058.8±926.1 15791.5±530.5
Seaquest 841.2±67.3 603.2±62.3 1005.1±175.5 820.5±94.0
Asterix 745.0±85.6 551.0±76.2 857.0±61.2 1636.5±166.5

Bảng 13: Điểm số thô được thu thập với chính sách đã huấn luyện tốt nhất trong 1% phần của tập dữ liệu Atari 2600. Điểm số được tính trung bình trên 10 seed ngẫu nhiên.

Tập dữ liệu d3rlpy tham chiếu
Pong 16.6±1.1 -13.8
Breakout 9.0±1.9 7.9
Qbert 695.8±153.1 383.6
Seaquest 603.2±62.3 672.9
Asterix 551.0±76.2 166.3

Bảng 14: So sánh cạnh nhau với điểm số thô tham chiếu của QR-DQN được báo cáo trong Kumar et al. (2020), chỉ cung cấp điểm số trung bình.

Tập dữ liệu d3rlpy tham chiếu
Pong 16.3±1.4 19.3
Breakout 89.8±12.2 61.1
Qbert 15791.5±530.5 14012.0
Seaquest 820.5±94.0 779.4
Asterix 1636.5±166.5 592.4

Bảng 15: So sánh cạnh nhau với điểm số thô tham chiếu của CQL được báo cáo trong Kumar et al. (2020), chỉ cung cấp điểm số trung bình.

--- TRANG 16 ---
Seno và Imai

Phụ lục C. Kết quả Benchmark: Tinh chỉnh
Chúng tôi đánh giá các thuật toán đã triển khai với tập dữ liệu AntMaze (Fu et al., 2020) trong tình huống tinh chỉnh nơi chính sách được huấn luyện trước với tập dữ liệu tĩnh và tinh chỉnh với kinh nghiệm trực tuyến. Chúng tôi tuân theo quy trình thử nghiệm được mô tả trong Kostrikov et al. (2021). Trong đánh giá này, chúng tôi chọn AWAC và IQL, được đề xuất như các thuật toán RL với khả năng tinh chỉnh. Bảng 16 hiển thị các siêu tham số được sử dụng trong benchmark. Tất cả giá trị phần thưởng được trừ đi 1. Chúng tôi sử dụng cùng các siêu tham số được mô tả trong bài báo gốc (Nair et al., 2020; Kostrikov et al., 2021). Trong mỗi lần huấn luyện, chính sách được tinh chỉnh trong 1M bước sau khi huấn luyện trước. Chúng tôi lặp lại tất cả các thử nghiệm với 10 seed ngẫu nhiên.

Bảng 17 hiển thị kết quả benchmark. Bảng 18 và 19 hiển thị so sánh cạnh nhau với điểm số tham chiếu. Xem xét rằng các tác giả không báo cáo độ lệch chuẩn, chúng tôi tin rằng sự khác biệt về hiệu suất là không đáng kể.

Thuật toán Siêu tham số Giá trị
AWAC (Nair et al., 2020) Tỷ lệ học Critic 3e-4
Tỷ lệ học Actor 3e-4
Kích thước mini-batch 1024
λ (Nair et al., 2020) 1
Đơn vị ẩn Actor [256, 256, 256, 256]
Phân rã trọng số Actor 1e-4
Đơn vị ẩn Critic [256, 256, 256, 256]
Bước huấn luyện trước 25000
IQL (Kostrikov et al., 2021) Tỷ lệ học Critic 3e-4
Tỷ lệ học Actor 3e-4
Tỷ lệ học V-function 3e-4
Kích thước mini-batch 256
Expectile 0.9
Nhiệt độ nghịch đảo 10.0
Bộ lập lịch tỷ lệ học Actor Cosine
Bước huấn luyện trước 1M

Bảng 16: Siêu tham số cho thử nghiệm tinh chỉnh.

Tập dữ liệu AWAC IQL
antmaze-umaze-v0 52.0±24.4→92.0±20.9 92.0±4.0→98.0±4.0
antmaze-medium-play-v0 0.0±0.0→0.0±0.0 71.0±14.5→91.0±9.4
antmaze-large-play-v0 0.0±0.0→0.0±0.0 52.0±16.6→69.0±13.7

Bảng 17: Điểm số chuẩn hóa được thu thập với chính sách đã huấn luyện cuối cùng trong tập dữ liệu AntMaze (Fu et al., 2020). Các số bên trái đại diện cho điểm số của các chính sách đã huấn luyện trước. Các số bên phải đại diện cho điểm số của các chính sách đã tinh chỉnh. Điểm số được tính trung bình trên 10 seed ngẫu nhiên.

--- TRANG 17 ---
d3rlpy: Một Thư viện Học Tăng cường Sâu Ngoại tuyến

Tập dữ liệu d3rlpy tham chiếu
antmaze-umaze-v0 52.0±24.4→92.0±20.9 56.7→59.0
antmaze-medium-play-v0 0.0±0.0→0.0±0.0 0.0→0.0
antmaze-large-play-v0 0.0±0.0→0.0±0.0 0.0→0.0

Bảng 18: So sánh cạnh nhau với điểm số chuẩn hóa tham chiếu của AWAC được báo cáo trong Kostrikov et al. (2021), chỉ cung cấp điểm số trung bình.

Tập dữ liệu d3rlpy tham chiếu
antmaze-umaze-v0 92.0±4.0→98.0±4.0 86.7→96.0
antmaze-medium-play-v0 71.0±14.5→91.0±9.4 72.0→95.0
antmaze-large-play-v0 52.0±16.6→69.0±13.7 25.5→46.0

Bảng 19: So sánh cạnh nhau với điểm số chuẩn hóa tham chiếu của IQL được báo cáo trong Kostrikov et al. (2021), chỉ cung cấp điểm số trung bình.

Tài liệu tham khảo
Rishabh Agarwal, Dale Schuurmans, và Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. Trong International Conference on Machine Learning, trang 104–114. PMLR, 2020.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, và Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.

Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, và Marc G. Bellemare. Dopamine: A Research Framework for Deep Reinforcement Learning. 2018. URL http://arxiv.org/abs/1812.06110.

Will Dabney, Georg Ostrovski, David Silver, và Rémi Munos. Implicit quantile networks for distributional reinforcement learning. Trong International conference on machine learning, trang 1096–1105. PMLR, 2018a.

Will Dabney, Mark Rowland, Marc Bellemare, và Rémi Munos. Distributional reinforcement learning with quantile regression. Trong Proceedings of the AAAI Conference on Artificial Intelligence, tập 32, 2018b.

Ludovic Denoyer, Alfredo de la Fuente, Song Duong, Jean-Baptiste Gaya, Pierre-Alexandre Kamienny, và Daniel H Thompson. Salina: Sequential learning of agents. arXiv preprint arXiv:2110.07910, 2021.

Carlo D'Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, và Jan Peters. MushroomRL: Simplifying reinforcement learning research. 2021.

Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, và Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.

--- TRANG 18 ---
Seno và Imai

Scott Fujimoto và Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. arXiv preprint arXiv:2106.06860, 2021.

Scott Fujimoto, Herke van Hoof, và David Meger. Addressing function approximation error in actor-critic methods. Trong Proceedings of the 35th International Conference on Machine Learning, trang 1587–1596, 2018.

Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, và Joelle Pineau. Benchmarking batch deep reinforcement learning algorithms. arXiv preprint arXiv:1910.01708, 2019a.

Scott Fujimoto, David Meger, và Doina Precup. Off-policy deep reinforcement learning without exploration. Trong International Conference on Machine Learning, trang 2052–2062. PMLR, 2019b.

Yasuhiro Fujita, Prabhat Nagarajan, Toshiki Kataoka, và Takahiro Ishikawa. Chainerrl: A deep reinforcement learning library. Journal of Machine Learning Research, 22(77):1–14, 2021. URL http://jmlr.org/papers/v22/20-376.html.

Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, và các tác giả khác. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.

Charles R Harris, K Jarrod Millman, Stéfan J Van Der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith, và các tác giả khác. Array programming with numpy. Nature, 585(7825):357–362, 2020.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, và Jian Sun. Deep residual learning for image recognition. Trong Proceedings of the IEEE conference on computer vision and pattern recognition, trang 770–778, 2016.

P Henderson, R Islam, P Bachman, J Pineau, D Precup, và D Meger. Deep reinforcement learning that matters. arxiv 2017. arXiv preprint arXiv:1709.06560, 2017.

Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, và David Silver. Rainbow: Combining improvements in deep reinforcement learning. Trong Thirty-second AAAI conference on artificial intelligence, 2018.

Diederik P Kingma và Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.

Ilya Kostrikov, Ashvin Nair, và Sergey Levine. Offline reinforcement learning with implicit q-learning. arXiv preprint arXiv:2110.06169, 2021.

Alexander Kuhnle, Michael Schaarschmidt, và Kai Fricke. Tensorforce: a tensorflow library for applied reinforcement learning. Trang web, 2017. URL https://github.com/tensorforce/tensorforce.

Aviral Kumar, Justin Fu, George Tucker, và Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. arXiv preprint arXiv:1906.00949, 2019.

--- TRANG 19 ---
d3rlpy: Một Thư viện Học Tăng cường Sâu Ngoại tuyến

Aviral Kumar, Aurick Zhou, George Tucker, và Sergey Levine. Conservative q-learning for offline reinforcement learning. arXiv preprint arXiv:2006.04779, 2020.

Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, và Marco Hutter. Learning quadrupedal locomotion over challenging terrain. Science robotics, 5(47), 2020.

Sergey Levine, Aviral Kumar, George Tucker, và Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, và Daan Wierstra. Continuous control with deep reinforcement learning. Trong International Conference on Learning Representation, 2016.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, và các tác giả khác. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.

Ashvin Nair, Murtaza Dalal, Abhishek Gupta, và Sergey Levine. Accelerating online reinforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.

Fabio Pardo. Tonic: A deep reinforcement learning library for fast prototyping and benchmarking. arXiv preprint arXiv:2011.07537, 2020.

Fabio Pardo, Arash Tavakoli, Vitaly Levdik, và Petar Kormushev. Time limits in reinforcement learning. Trong International Conference on Machine Learning, trang 4045–4054. PMLR, 2018.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, và các tác giả khác. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019.

Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, và các tác giả khác. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825–2830, 2011.

Matthias Plappert. keras-rl. https://github.com/keras-rl/keras-rl, 2016.

Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, và Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 2021.

Martin Riedmiller. Neural fitted q iteration–first experiences with a data efficient neural reinforcement learning method. Trong European conference on machine learning, trang 317–328. Springer, 2005.

Rutav Shah và Vikash Kumar. Rrl: Resnet as representation for reinforcement learning. arXiv preprint arXiv:2107.03380, 2021.

--- TRANG 20 ---
Seno và Imai

Hado van Hasselt, Arthur Guez, và David Silver. Deep reinforcement learning with double q-learning. corr abs/1509.06461 (2015). arXiv preprint arXiv:1509.06461, 2015.

Ziyu Wang, Alexander Novikov, Konrad Żołna, Jost Tobias Springenberg, Scott Reed, Bobak Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, và các tác giả khác. Critic regularized regression. arXiv preprint arXiv:2006.15134, 2020.

Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Hang Su, và Jun Zhu. Tianshou: A highly modularized deep reinforcement learning library. arXiv preprint arXiv:2107.14171, 2021.

Peter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian, Thomas J Walsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, và các tác giả khác. Outracing champion gran turismo drivers with deep reinforcement learning. Nature, 602(7896):223–228, 2022.

Wenxuan Zhou, Sujay Bajracharya, và David Held. Latent action space for offline reinforcement learning. Trong Conference on Robot Learning, 2020.
