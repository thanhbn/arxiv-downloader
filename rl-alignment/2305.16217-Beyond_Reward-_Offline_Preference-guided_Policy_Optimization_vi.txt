# Vượt Ra Khỏi Phần Thưởng: Tối Ưu Hóa Chính Sách Hướng Dẫn Bởi Sở Thích Ngoại Tuyến

Yachen Kang1 2Diyuan Shi2Jinxin Liu2Li He2Donglin Wang2

Tóm tắt
Nghiên cứu này tập trung vào chủ đề học tăng cường dựa trên sở thích ngoại tuyến (PbRL), một biến thể của học tăng cường thông thường không cần tương tác trực tuyến hoặc xác định hàm phần thưởng. Thay vào đó, tác nhân được cung cấp các quỹ đạo ngoại tuyến cố định và sở thích của con người giữa các cặp quỹ đạo để trích xuất thông tin động lực học và nhiệm vụ, tương ứng. Vì thông tin động lực học và nhiệm vụ là trực giao, một cách tiếp cận đơn giản sẽ bao gồm việc sử dụng học phần thưởng dựa trên sở thích sau đó là một thuật toán RL ngoại tuyến có sẵn. Tuy nhiên, điều này đòi hỏi việc học riêng biệt một hàm phần thưởng vô hướng, được giả định là một nút cổ chai thông tin của quá trình học. Để giải quyết vấn đề này, chúng tôi đề xuất mô hình tối ưu hóa chính sách hướng dẫn bởi sở thích ngoại tuyến (OPPO), mô hình hóa các quỹ đạo ngoại tuyến và sở thích trong một quy trình một bước, loại bỏ nhu cầu học riêng biệt một hàm phần thưởng. OPPO đạt được điều này bằng cách giới thiệu một mục tiêu khớp thông tin nhìn lại ngoại tuyến để tối ưu hóa một chính sách có điều kiện và một mục tiêu mô hình hóa sở thích để tìm bối cảnh tối ưu. OPPO tiếp tục tích hợp một chính sách quyết định hoạt động tốt bằng cách tối ưu hóa hai mục tiêu một cách lặp lại. Kết quả thực nghiệm của chúng tôi chứng minh rằng OPPO mô hình hóa hiệu quả các sở thích ngoại tuyến và vượt trội hơn các cơ sở cạnh tranh trước đó, bao gồm các thuật toán RL ngoại tuyến được thực hiện trên các đặc tả hàm phần thưởng thật hoặc giả. Mã của chúng tôi có sẵn trên trang web dự án: https://sites.google.com/view/oppo-icml-2023.

1. Giới thiệu
Học tăng cường sâu (RL) cung cấp một khung linh hoạt để thu được các hành vi hướng nhiệm vụ, như được chứng minh bởi một bộ tài liệu ngày càng phát triển (Kohl & Stone, 2004; Kober & Peters, 2008; Kober et al., 2013; Silver et al., 2017; Kalashnikov et al., 2018; Vinyals et al., 2019). Trong khung này, "nhiệm vụ" thường được biểu hiện như việc tối đa hóa phần thưởng tích lũy của các quỹ đạo được tạo ra bằng cách triển khai chính sách học trong môi trường tương ứng. Tuy nhiên, công thức RL trên đây giả định hai điều kiện quan trọng cho việc huấn luyện chính sách quyết định: 1) một môi trường có thể tương tác, và 2) một hàm phần thưởng được xác định trước. Đáng tiếc, các tương tác trực tuyến với môi trường có thể vừa tốn kém vừa nguy hiểm (Mihatsch & Neuneier, 2002; Hans et al., 2008; García & Fernández, 2015), trong khi phát triển một hàm phần thưởng phù hợp thường đòi hỏi nỗ lực đáng kể của con người. Ngoài ra, các phần thưởng heuristic thường được sử dụng có thể không đủ để biểu đạt ý định thực sự (Hadfield-Menell et al., 2017).

Để giải quyết những thách thức này, nghiên cứu trước đây đã khám phá hai cách tiếp cận. Đầu tiên, một số công trình đã tập trung vào công thức RL ngoại tuyến (Fujimoto et al., 2019), nơi người học có quyền truy cập vào các quỹ đạo ngoại tuyến cố định cùng với tín hiệu phần thưởng cho mỗi chuyển đổi (hoặc các minh chứng chuyên gia có hạn). Thứ hai, những người khác đã xem xét công thức RL dựa trên sở thích (trực tuyến), nơi mục tiêu nhiệm vụ được truyền đạt cho người học thông qua sở thích của một người chú thích con người giữa hai quỹ đạo thay vì phần thưởng cho mỗi chuyển đổi. Trong việc theo đuổi những tiến bộ hơn nữa trong bối cảnh này, chúng tôi đề xuất một cách tiếp cận mới nới lỏng cả hai yêu cầu này và ủng hộ RL dựa trên sở thích ngoại tuyến (PbRL).

Trong bối cảnh của học tăng cường dựa trên sở thích ngoại tuyến (PbRL), nơi có quyền truy cập vào một tập dữ liệu ngoại tuyến và các sở thích được gán nhãn giữa các quỹ đạo ngoại tuyến, một cách tiếp cận phổ biến là kết hợp các phương pháp PbRL trực tuyến trước đó với các thuật toán RL ngoại tuyến có sẵn (Shin & Brown, 2021). Chiến lược hai bước này, như được minh họa trong Hình 1 (trái), thường bao gồm việc huấn luyện một hàm phần thưởng sử dụng mô hình Bradley-Terry (Bradley & Terry, 1952) theo cách có giám sát, tiếp theo là huấn luyện chính sách với bất kỳ thuật toán RL ngoại tuyến nào trên các chuyển đổi được gán nhãn lại thông qua hàm phần thưởng đã học. Tuy nhiên, việc học riêng biệt một hàm phần thưởng giải thích sở thích của chuyên gia có thể không hướng dẫn trực tiếp chính sách về cách hành động tối ưu. Điều này là do các nhãn sở thích định nghĩa nhiệm vụ PbRL, và mục tiêu là học quỹ đạo được người chú thích ưa thích nhất thay vì tối đa hóa phần thưởng proxy chiết khấu tích lũy của các cuộn chính sách. Trong trường hợp các nhiệm vụ phức tạp, chẳng hạn như các nhiệm vụ không-Markovian, phần thưởng vô hướng có thể tạo ra một nút cổ chai thông tin trong cải thiện chính sách, dẫn đến hành vi không tối ưu (Vamplew et al., 2022). Ngoài ra, một tối ưu hóa chính sách cô lập có thể khai thác lỗ hổng trong các hàm phần thưởng được hiệu chuẩn sai, dẫn đến các hành vi không mong muốn. Với những hạn chế này, việc đặt câu hỏi về sự cần thiết của việc học một hàm phần thưởng là hợp lý, đặc biệt xem xét rằng nó có thể không trực tiếp mang lại chính sách tối ưu.

Để đạt được mục tiêu này, chúng tôi trình bày cách tiếp cận tối ưu hóa chính sách hướng dẫn bởi sở thích ngoại tuyến (OPPO), đây là một mô hình một bước đồng thời mô hình hóa các sở thích ngoại tuyến và học chính sách quyết định tối ưu mà không cần học riêng biệt một hàm phần thưởng (như được minh họa trong Hình 1 phải). Điều này được đạt được thông qua việc sử dụng hai mục tiêu: một mục tiêu khớp thông tin nhìn lại ngoại tuyến và một mục tiêu mô hình hóa sở thích. Bằng cách tối ưu hóa lặp lại các mục tiêu này, chúng tôi thu được một chính sách có điều kiện π(a|s,z) để mô hình hóa dữ liệu ngoại tuyến và một bối cảnh tối ưu z∗ để mô hình hóa sở thích. Trọng tâm chính của OPPO là học cả không gian z chiều cao và đánh giá các chính sách trong không gian đó. Không gian z chiều cao này nắm bắt nhiều thông tin liên quan đến nhiệm vụ hơn so với phần thưởng vô hướng, làm cho nó lý tưởng cho mục đích tối ưu hóa chính sách. Hơn nữa, chính sách tối ưu được thu được bằng cách điều kiện hóa chính sách có điều kiện π(a|s,z) trên bối cảnh tối ưu đã học z∗.

Đóng góp chính của chúng tôi có thể được tóm tắt như sau. Đầu tiên, chúng tôi đề xuất OPPO, một mô hình PbRL ngoại tuyến ngắn gọn, ổn định và một bước tránh nhu cầu học hàm phần thưởng riêng biệt. Thứ hai, chúng tôi trình bày một thể hiện của mục tiêu khớp thông tin nhìn lại dựa trên sở thích và một mục tiêu mô hình hóa sở thích mới trên bối cảnh. Cuối cùng, các thí nghiệm rộng rãi được tiến hành để chứng minh tính ưu việt của OPPO so với các cơ sở cạnh tranh trước đó và để phân tích hiệu suất của nó.

2. Công trình liên quan
Vì OPPO nằm ở giao điểm của Học Tăng cường Dựa trên Sở thích, RL Ngoại tuyến, và RL có điều kiện, chúng tôi xem xét các thuật toán liên quan nhất từ các lĩnh vực này (xem Bảng 1)

PbRL Trực tuyến. Học tăng cường dựa trên sở thích còn được gọi là học tăng cường từ phản hồi con người (RLHF). Một số công trình đã thành công trong việc sử dụng phản hồi từ con người thật để huấn luyện các tác nhân RL (Arumugam et al., 2019; Christiano et al., 2017; Ibarz et al., 2018; Knox & Stone, 2009; Lee et al., 2021; Warnell et al., 2017). Christiano et al. (2017) đã mở rộng quy mô học tăng cường dựa trên sở thích để sử dụng các kỹ thuật học sâu hiện đại, và Ibarz et al. (2018) đã cải thiện hiệu quả của phương pháp này bằng cách giới thiệu các hình thức phản hồi bổ sung như minh chứng. Gần đây, PEBBLE (Lee et al., 2021) đã đề xuất một thuật toán RL hiệu quả phản hồi bằng cách sử dụng học ngoại chính sách và huấn luyện trước. SURF (Park et al., 2022) đã sử dụng gán nhãn giả để sử dụng các đoạn không nhãn và đề xuất một phương pháp tăng cường dữ liệu mới được gọi là cắt xén thời gian. Tất cả các phương pháp trên đều yêu cầu tác nhân tương tác trực tuyến với môi trường, và tất cả đều là các chiến lược hai bước yêu cầu học một hàm phần thưởng vô hướng riêng biệt.

RL Ngoại tuyến. Để giảm thiểu tác động của sự dịch chuyển phân phối trong RL ngoại tuyến, các thuật toán trước đây (a) hạn chế không gian hành động (Fujimoto et al., 2019; Kumar et al., 2019a; Siegel et al., 2020; Zhuang et al., 2023), (b) kết hợp bi quan giá trị (Fujimoto et al., 2019; Kumar et al., 2020; Liu et al., 2022a), và (c) giới thiệu bi quan vào các mô hình động lực học đã học (Kidambi et al., 2020; Yu et al., 2020). Một hướng công trình khác đã khám phá việc học một phân phối hành vi rộng từ tập dữ liệu ngoại tuyến bằng cách học một tập kỹ năng không phụ thuộc nhiệm vụ, hoặc với các cách tiếp cận dựa trên khả năng (Ajay et al., 2020; Campos et al., 2020; Pertsch et al., 2020; Singh et al., 2020) hoặc bằng cách tối đa hóa thông tin tương hỗ (Eysenbach et al., 2018; Lu et al., 2020; Sharma et al., 2019). Một số phương pháp RL trước đây tương tự hơn với học có giám sát tĩnh, chẳng hạn như Q-learning (Watkins, 1989; Mnih et al., 2013) và nhân bản hành vi. Trong các phương pháp này, hiệu suất của tác nhân kết quả tương quan tích cực với chất lượng dữ liệu được sử dụng để huấn luyện. Ngoài các phương pháp RL đã nêu, Srivastava et al. (2019) và Kumar et al. (2019b) đã nghiên cứu học tăng cường "ngược" (UDRL), tìm cách mô hình hóa hành vi thông qua một mất mát có giám sát có điều kiện trên một lợi nhuận mục tiêu. Ghosh et al. (2019); Liu et al. (2022b; 2021) đã mở rộng các phương pháp UDRL trước đây để thực hiện việc đạt mục tiêu bằng cách lấy trạng thái mục tiêu làm điều kiện, và Paster et al. (2020) tiếp tục sử dụng LSTM cho các thiết lập RL có điều kiện mục tiêu trực tuyến. DT (Chen et al., 2021) và TT (Janner et al., 2021) đã giải quyết vấn đề thông qua mô hình hóa chuỗi, vì họ tin rằng mô hình hóa chuỗi cho phép mô hình hóa hành vi mà không cần truy cập vào phần thưởng, theo phong cách tương tự như ngôn ngữ (Radford et al., 2018) và hình ảnh (Chen et al., 2020). Mặc dù các phương pháp trên có thể tránh tương tác trực tuyến giữa tác nhân và môi trường, tất cả đều yêu cầu phần thưởng thực hoặc minh chứng chuyên gia để xác định nhiệm vụ, điều này thường đòi hỏi nhiều lao động con người.

PbRL Ngoại tuyến. OPAL (Shin & Brown, 2021) đầu tiên đã cố gắng giải quyết PbRL ngoại tuyến bằng cách đơn giản kết hợp phương pháp PbRL (trực tuyến) trước đây và thuật toán RL ngoại tuyến có sẵn. PT (Kim et al., 2023) đã giới thiệu một mô hình sở thích mới dựa trên tổng có trọng số của các phần thưởng không-Markovian và sử dụng kiến trúc dựa trên transformer để thiết kế mô hình đó. Cả hai công trình này đều áp dụng chiến lược hai bước đơn giản với việc học một hàm phần thưởng riêng biệt. Để tránh nút cổ chai thông tin mà phần thưởng vô hướng có thể tạo ra (Vamplew et al., 2022), OPPO đồng thời mô hình hóa các sở thích ngoại tuyến và học chính sách quyết định tối ưu trong một mô hình một bước. Và ngược lại với cả RL có giám sát và UDRL, mục đích của phương pháp chúng tôi là tìm kiếm giải pháp tối ưu được giám sát bởi tín hiệu sở thích nhị phân trong thiết lập ngoại tuyến. Phương pháp của chúng tôi không chỉ hoạt động với các minh chứng không tối ưu mà còn tiết lộ các hành vi tối ưu mà không tiêm các tiền đề con người về minh chứng tối ưu.

3. Kiến thức cơ bản
Chúng tôi xem xét học tăng cường (RL) trong một quá trình quyết định Markov (MDP) được mô tả bởi một bộ (S,A, r, P, p0, γ), trong đó st∈ S,at∈ A, và rt=r(st,at) biểu thị trạng thái, hành động, và phần thưởng tại thời điểm t, P(st+1|st,at) biểu thị động lực học chuyển đổi, p0(s0) biểu thị phân phối trạng thái ban đầu, và γ∈[0,1) biểu thị hệ số chiết khấu. Tại mỗi thời điểm t, tác nhân nhận một trạng thái st từ môi trường và chọn một hành động at dựa trên chính sách π(at|st). Trong khung RL tiêu chuẩn, môi trường trả về một phần thưởng rt, và tác nhân chuyển đến trạng thái tiếp theo st+1. Lợi nhuận kỳ vọng Jr(π) =Eτ∼π(τ)Σ∞k=0γkr(st+k, at+k) được định nghĩa là kỳ vọng của phần thưởng tích lũy chiết khấu, trong đó τ= (s0,a0,s1,a1, . . .), s0∼p0(s0), at∼π(at|st), và st+1∼P(st+1|st,at). Mục tiêu của tác nhân là học một chính sách π tối đa hóa lợi nhuận kỳ vọng.

3.1. Học tăng cường dựa trên sở thích ngoại tuyến
Trong công trình này, chúng tôi giả định một thiết lập hoàn toàn ngoại tuyến trong đó tác nhân không được phép tiến hành các cuộn trực tuyến (trên MDP) trong quá trình huấn luyện nhưng được cung cấp một tập dữ liệu tĩnh cố định. Tập dữ liệu tĩnh, D:={τ0, . . . , τN}, bao gồm các quỹ đạo được thu thập trước, trong đó mỗi quỹ đạo τi chứa một chuỗi liên tục các trạng thái và hành động: τi:={si0,ai0,si1, . . .}. Thiết lập ngoại tuyến như vậy thách thức hơn thiết lập tiêu chuẩn (trực tuyến) vì nó loại bỏ khả năng khám phá môi trường và thu thập phản hồi bổ sung. Không giống như học bắt chước, chúng tôi không giả định rằng tập dữ liệu đến từ một chính sách chuyên gia duy nhất. Thay vào đó, tập dữ liệu D có thể chứa các quỹ đạo được thu thập bởi các chính sách hành vi không tối ưu hoặc thậm chí ngẫu nhiên.

Nói chung, RL ngoại tuyến tiêu chuẩn giả định sự tồn tại của thông tin phần thưởng cho mỗi cặp trạng thái-hành động trong D. Tuy nhiên, trong khung RL dựa trên Sở thích Ngoại tuyến (PbRL), chúng tôi giả định rằng phần thưởng như vậy không thể truy cập được, trong khi tác nhân có thể truy cập các sở thích ngoại tuyến (giữa một số cặp quỹ đạo (τi, τj)) được gán nhãn bởi một người chú thích chuyên gia (con người). Cụ thể, người chú thích đưa ra phản hồi chỉ ra quỹ đạo nào được ưa thích, tức là y∈ {0,1,0.5}, trong đó 0 chỉ ra τi≻τj (sự kiện quỹ đạo τi được ưa thích hơn quỹ đạo τj), 1 chỉ ra τj≻τi (τj được ưa thích hơn τi), và 0.5 ngụ ý trường hợp ưa thích bằng nhau. Mỗi phản hồi được lưu trữ trong một tập dữ liệu ngoại tuyến có nhãn D≻ như một bộ ba (τi, τj, y). Với những sở thích này, mục tiêu của PbRL là tìm một chính sách π(at|st) tối đa hóa lợi nhuận kỳ vọng Jrψ, dưới hàm phần thưởng giả định rψ(st,at) phù hợp với sở thích con người. Để cho phép điều này, các công trình trước đây học một hàm phần thưởng rψ(st,at) và sử dụng mô hình Bradley-Terry (Bradley & Terry, 1952) để mô hình hóa sở thích con người, được biểu thị ở đây như một hàm logistic:

P[τi≻τj] =logistic (Σtrψ(sit,ait)−Σtrψ(sjt,ajt)), (1)

trong đó (sit,ait)∼τi,(sjt,ajt)∼τj. Trực quan, điều này có thể được hiểu như giả định rằng xác suất ưa thích một quỹ đạo phụ thuộc theo cấp số nhân vào phần thưởng tích lũy trên quỹ đạo được gán nhãn bởi một hàm phần thưởng cơ bản. Hàm phần thưởng sau đó được cập nhật bằng cách tối thiểu hóa mất mát entropy chéo sau:

− E(τi,τj,y)∼D≻[(1−y) logP[τi≻τj]+ylogP[τj≻τi]]. (2)

Với hàm phần thưởng đã học rψ được sử dụng để gán nhãn mỗi chuyển đổi trong tập dữ liệu, chúng ta có thể áp dụng một thuật toán RL ngoại tuyến có sẵn để cho phép học chính sách.

3.2. Khớp Thông tin Nhìn lại
Ngoài khung RL lặp lại (ngoại tuyến) điển hình, khớp thông tin (IM) (Furuta et al., 2021) gần đây đã được nghiên cứu như một đặc tả vấn đề thay thế trong RL (ngoại tuyến). Mục tiêu của IM trong RL là học một chính sách có điều kiện π(a|s,z) mà các cuộn quỹ đạo của nó thỏa mãn giá trị thống kê thông tin mong muốn được định nghĩa trước z:

minπEz∼p(z)τz∼π(z)[ℓ(z, I(τz))], (3)

trong đó p(z) là một tiền đề, và π(z) biểu thị phân phối quỹ đạo được tạo ra bằng cách cuộn π(a|s,z) trong môi trường. I(τ) là một hàm nắm bắt thông tin thống kê của một quỹ đạo τ, chẳng hạn như thống kê phân phối của trạng thái và phần thưởng, như trung bình, phương sai (Wainwright et al., 2008), và ℓ là một hàm mất mát.

Một mặt, nếu chúng ta đặt p(z) như một phân phối tiền đề, tối ưu hóa Eq.3 tương ứng với việc thực hiện RL không giám sát (trực tuyến) để học một tập kỹ năng (Eysenbach et al., 2018; Sharma et al., 2019). Mặt khác, nếu chúng ta đặt p(z) như thông tin thống kê của một phân phối quỹ đạo (hoặc trạng thái-hành động) ngoại chính sách đã cho D(τ) (hoặc D(s,a)), Eq.3 tương ứng với một mục tiêu cho khớp thông tin nhìn lại trong RL (ngoại tuyến). Ví dụ, HER (Andrychowicz et al., 2017) và RL có điều kiện lợi nhuận (RL ngược (Srivastava et al., 2019; Kumar et al., 2019b; Chen et al., 2021; Janner et al., 2021)) sử dụng khái niệm nhìn lại trên: xác định bất kỳ quỹ đạo τ nào trong tập dữ liệu làm mục tiêu nhìn lại và đặt thông tin z trong Eq.3 như I(τ). Sau đó, chúng tôi cung cấp mục tiêu khớp thông tin nhìn lại (HIM) được điều khiển bởi I(·):

minπEτ∼D(τ)τz∼π(z)[ℓ(I(τ), I(τz))], (4)

trong đó z:=I(τ). Trong HER, chúng ta đặt I(τ) như trạng thái cuối cùng trong quỹ đạo τ, và trong RL có điều kiện phần thưởng, chúng ta đặt I(τ) như lợi nhuận của quỹ đạo τ. Do đó, chúng ta có thể sử dụng thông tin nhìn lại z:=I(τ) để cung cấp giám sát cho việc huấn luyện chính sách có điều kiện π(a|s,z). Tuy nhiên, trong thiết lập ngoại tuyến, việc lấy mẫu τz từ π(z) không thể truy cập được. Do đó, chúng ta phải mô hình hóa động lực học chuyển đổi môi trường bên cạnh mô hình hóa thông tin nhìn lại được điều khiển bởi I(·). Có nghĩa là, chúng ta cần mô hình hóa chính quỹ đạo, tức là minπEτ∼D(τ),τz∼π(z)[ℓ(τ, τz)]. Sau đó, chúng tôi cung cấp mục tiêu HIM ngoại tuyến tổng thể:

minπEτ∼D(τ)τz∼π(z)[ℓ(I(τ), I(τz)) +ℓ(τ, τz)]. (5)

Để đưa ra hiểu biết trực quan về mục tiêu trên, chúng tôi cung cấp một ví dụ đơn giản: xem xét nhìn lại I(·) là lợi nhuận của một quỹ đạo, tối ưu hóa ℓ(I(τ), I(τz)) đảm bảo rằng τz được tạo ra sẽ đạt cùng lợi nhuận như τ=I−1(z). Tuy nhiên, trong thiết lập ngoại tuyến, chúng ta phải đảm bảo rằng τz được tạo ra ở trong hỗ trợ của dữ liệu ngoại tuyến, loại bỏ vấn đề ngoài phân phối (OOD). Do đó chúng ta tối thiểu hóa ℓ(τ, τz) một cách gần đúng. Trong thực hiện, tối ưu hóa trực tiếp ℓ(τ, τz) đủ để đảm bảo thông tin nhìn lại được khớp, ví dụ, ℓ(I(τ), I(τz))< ϵ. Ở đây, chúng tôi một cách rõ ràng chính thức hóa thuật ngữ ℓ(I(τ), I(τz)) với nhấn mạnh đặc biệt về yêu cầu của mục tiêu khớp thông tin nhìn lại và đồng thời làm nổi bật sự khác biệt, xem Phần 4, giữa mục tiêu HIM trên (lấy I(·) như một tiền đề) và công thức OPPO của chúng tôi (yêu cầu học Iθ(·)).

Bằng cách tối ưu hóa Eq.5, chúng ta có thể thu được một chính sách có điều kiện π(a|s,z). Trong giai đoạn đánh giá, chính sách tối ưu π(a|s,z∗) có thể được xác định bằng cách điều kiện hóa chính sách trên một mục tiêu z∗ được chọn. Ví dụ, Decision Transformer (Chen et al., 2021) lấy hiệu suất mong muốn làm mục tiêu z∗ (ví dụ, xác định lợi nhuận tối đa có thể để tạo ra hành vi chuyên gia), và RvS-G (Emmons et al., 2021) lấy trạng thái mục tiêu làm mục tiêu z∗.

4. OPPO: Tối Ưu Hóa Chính Sách Hướng Dẫn Bởi Sở Thích Ngoại Tuyến

Trong phần này, chúng tôi trình bày phương pháp của mình, OPPO (tối ưu hóa chính sách hướng dẫn bởi sở thích ngoại tuyến), áp dụng mục tiêu khớp thông tin nhìn lại (HIM) trong Phần 4.1 để mô hình hóa một chính sách có điều kiện ngoại tuyến π(a|s,z), và giới thiệu một mất mát triplet trong Phần 4.2 để mô hình hóa sở thích con người cũng như bối cảnh tối ưu z∗. Khi kiểm tra, chúng tôi điều kiện hóa chính sách trên bối cảnh tối ưu z∗ và do đó tiến hành cuộn với π(a|s,z∗). Về nguyên tắc, OPPO tương thích với bất kỳ thiết lập PbRL nào, bao gồm cả trực tuyến và ngoại tuyến. Tuy nhiên, trong phạm vi phân tích và thí nghiệm của chúng tôi, chúng tôi tập trung vào thiết lập ngoại tuyến để tách biệt các khó khăn khám phá trong RL trực tuyến.

4.1. Tối Ưu Hóa Chính Sách Điều Khiển bởi HIM

Như mô tả trong Phần 3.1, để trực tiếp thực hiện các thuật toán RL ngoại tuyến có sẵn, các công trình trước đây trong PbRL một cách rõ ràng học một hàm phần thưởng với Eq.2 (như được hiển thị trong Hình 1 trái). Như một thay thế cho cách tiếp cận hai bước như vậy, chúng tôi tìm cách học chính sách trực tiếp từ tập dữ liệu ngoại tuyến có nhãn sở thích (như được hiển thị trong Hình 1 phải). Được truyền cảm hứng bởi mục tiêu HIM ngoại tuyến trong Phần 3.2, chúng tôi đề xuất học một chính sách có điều kiện π(a|s,z) trong thiết lập PbRL ngoại tuyến. Giả định Iθ là một mạng (có thể học) mã hóa thông tin nhìn lại trong PbRL, chúng tôi công thức hóa mục tiêu sau:

minπ,IθLHIM:=Eτ∼D(τ)τz∼π(z)[ℓ(Iθ(τ), Iθ(τz)) +ℓ(τ, τz)], (6)

trong đó z:=Iθ(τ). Lưu ý rằng Eq.6 là một thể hiện khác của Eq.5 trong đó chúng tôi học bộ trích xuất thông tin nhìn lại Iθ(·) trong thiết lập PRBL, trong khi các thuật toán RL (ngoại tuyến) trước đây thường đặt I(·) là một tiền đề (Chen et al., 2021; Emmons et al., 2021). Cấu trúc encoder-decoder như vậy bây giờ tương tự như Bi-directional Decision Transformer (BDT) được đề xuất bởi (Furuta et al., 2021) cho học bắt chước ngoại tuyến. Tuy nhiên, vì các minh chứng chuyên gia bị thiếu trong thiết lập PbRL, trong Phần 4.2, chúng tôi đề xuất sử dụng các nhãn sở thích trong D≻ để trích xuất thông tin nhìn lại.

4.2. Mô hình hóa Sở thích

Để làm cho thông tin nhìn lại Iθ(τ) trong Eq.6 khớp với thông tin sở thích trong tập dữ liệu (có nhãn) D≻, chúng tôi xây dựng mục tiêu mô hình hóa sở thích sau được truyền cảm hứng bởi mất mát contrastive trong học metric (Le-Khac et al., 2020):

minz∗,IθE(τi,τj,y)∼D≻[ℓ(z∗,z+)−ℓ(z∗,z−)], (7)

trong đó z+ và z− đại diện cho việc nhúng của quỹ đạo ưa thích (tích cực) Iθ(yτj+ (1−y)τi) và của quỹ đạo ít ưa thích (tiêu cực) Iθ(yτi+ (1−y)τj), tương ứng. Gần với ý tưởng sử dụng hối tiếc để mô hình hóa sở thích (Knox et al., 2022; Chen et al., 2022), giả định cơ bản của chúng tôi trong việc thiết kế mục tiêu trong Eq.7 là con người thường tiến hành so sánh hai cấp trước khi đưa ra sở thích giữa hai quỹ đạo (τi, τj): 1) đánh giá riêng biệt sự tương tự giữa quỹ đạo τi và quỹ đạo tối ưu giả định τ∗, tức là −ℓ(z∗,zi), và sự tương tự giữa quỹ đạo τj và quỹ đạo tối ưu giả định τ∗, −ℓ(z∗,zj), và 2) đánh giá sự khác biệt giữa hai sự tương tự trên (−ℓ(z∗,zi) vs. −ℓ(z∗,zj)) và đặt quỹ đạo có sự tương tự cao hơn là quỹ đạo được ưa thích. Do đó, tối ưu hóa Eq.7 đảm bảo tìm ra việc nhúng tối ưu tương tự hơn với z+ và ít tương tự hơn với z−. Để làm rõ, z∗ là thông tin bối cảnh tương ứng cho τ∗, trong khi τ∗ sẽ luôn được ưa thích hơn bất kỳ quỹ đạo ngoại tuyến nào trong tập dữ liệu.

Trong thực tế, để làm mạnh mẽ mô hình hóa sở thích, chúng tôi tối ưu hóa mục tiêu sau sử dụng mất mát triplet thay cho mục tiêu trong Eq.7:

minz∗,IθLPM:=E[max(ℓ(z∗,z+)−ℓ(z∗,z−)+m,0)], (8)

trong đó m là một lề được đặt tùy ý giữa các cặp tích cực và tiêu cực. Đáng chú ý rằng hậu nghiệm của việc nhúng tối ưu z∗ và bộ trích xuất thông tin nhìn lại Iθ(·) được cập nhật xen kẽ để đảm bảo ổn định học. Một ước tính tốt hơn của việc nhúng tối ưu giúp bộ mã hóa trích xuất các đặc trưng mà người gán nhãn con người chú ý nhiều hơn. Ngược lại, một bộ mã hóa thông tin nhìn lại tốt hơn, mặt khác, tăng tốc quá trình tìm kiếm cho quỹ đạo tối ưu trong không gian nhúng cấp cao. Theo cách này, hàm mất mát cho bộ mã hóa bao gồm hai phần: 1) một mất mát khớp thông tin nhìn lại theo phong cách có giám sát như trong Eq.6 và 2) một mất mát triplet như trong Eq.8 để tích hợp tốt hơn giám sát nhị phân được cung cấp bởi tập dữ liệu có nhãn sở thích.

4.3. Mục tiêu Huấn luyện & Chi tiết Thực hiện

Trong thí nghiệm của chúng tôi, chúng tôi củng cố ℓ trong Eq.6 như MSE Loss và trong Eq.8 như Khoảng cách Euclidean. Trong trường hợp này, chúng tôi mô hình hóa z∗ như một điểm trong không gian z, và thước đo tương tự ℓ là khoảng cách L2. Một tùy chọn thay thế là mô hình hóa z∗ như một điểm được lấy mẫu từ một phân phối đã học trong không gian z, trong đó ℓ là một phép đo giữa hai phân phối, chẳng hạn như divergence KL. Ngoài ra, chúng tôi thêm một mất mát chuẩn hóa Lnorm để hạn chế chuẩn L2 của tất cả các nhúng được tạo ra bởi bộ trích xuất thông tin nhìn lại Iθ.

Ltotal:=LHIM+αLPM+βLnorm (9)

Tổng quan kiến trúc của OPPO được hiển thị trong Hình 2. OPPO mô hình hóa bộ trích xuất thông tin nhìn lại Iθ như một mạng mã hóa Iθ:τ→z và chúng tôi sử dụng kiến trúc BERT. Hơn nữa, tương tự như DT (Chen et al., 2021), chúng tôi sử dụng kiến trúc GPT để mô hình hóa π(a|s,z), dự đoán các hành động tương lai thông qua mô hình hóa tự hồi quy. Để chọn siêu tham số cụ thể trong quá trình huấn luyện, vui lòng tham khảo mô tả chi tiết trong Phụ lục A.1.3. Thuật toán 1 chi tiết việc huấn luyện của OPPO, và toàn bộ quá trình được tóm tắt như sau: 1) Chúng tôi lấy mẫu một lô quỹ đạo từ tập dữ liệu D; 2) Ở Dòng 4, chúng tôi sử dụng Eq.6 (mất mát khớp thông tin nhìn lại) để cập nhật π(a|s,z) và Iθ(·) dựa trên các quỹ đạo được lấy mẫu; do đó, cho z được trích xuất ra từ một quỹ đạo ngoại tuyến bằng bộ trích xuất, chính sách có thể tái tạo lại nó; 3) Sau đó, chúng tôi lấy mẫu một lô sở thích từ tập dữ liệu có nhãn D≻; 4) Cuối cùng, ở Dòng 6, chúng tôi cập nhật Iθ(·) và z∗ dựa trên {(τi, τj, y)}B được lấy mẫu sử dụng Eq.8, làm cho việc nhúng tối ưu z∗ gần với quỹ đạo được ưa thích hơn z+, và đồng thời xa hơn với quỹ đạo ít được ưa thích z−.

Tóm lại, OPPO học một chính sách có điều kiện π(a|s,z), một bộ mã hóa bối cảnh (thông tin nhìn lại) Iθ(τ), và bối cảnh tối ưu, z∗, cho quỹ đạo tối ưu τ∗. So với các công trình PbRL trước đây (đầu tiên học một hàm phần thưởng với Eq.2 rồi sau đó học chính sách ngoại tuyến với các thuật toán RL ngoại tuyến có sẵn), OPPO học chính sách (ngoại tuyến) tối ưu (π(a|s,z∗)) trực tiếp và do đó tránh nút cổ chai thông tin tiềm năng gây ra bởi khả năng thông tin hạn chế của việc gán phần thưởng vô hướng. So với các thuật toán RL ngoại tuyến dựa trên HIM (ví dụ, DT (Chen et al., 2021) và RvS-G (Emmons et al., 2021)), OPPO không cần xác định thủ công bối cảnh mục tiêu cho chính sách cuộn π(a|s,·) ở giai đoạn kiểm tra.

5. Thí nghiệm

Trong phần này, chúng tôi đánh giá và so sánh OPPO với các cơ sở khác trong thiết lập PbRL ngoại tuyến. Một tiền đề trung tâm đằng sau thiết kế của OPPO là bộ mã hóa thông tin nhìn lại đã học Iθ(·) có thể nắm bắt sở thích trên các quỹ đạo khác nhau, như được mô tả bởi Eq.8. Các thí nghiệm của chúng tôi do đó được thiết kế để trả lời các câu hỏi sau:

1. OPPO có thực sự nắm bắt được sở thích không? Nói cách khác, không gian z đã học (được mã hóa bởi Iθ(·) đã học) có phù hợp với sở thích đã cho không? Vui lòng tham khảo Phần 5.1.

2. Chính sách có điều kiện tối ưu đã học π(a|s,z∗) có thể vượt trội hơn chính sách π(a|s,z) được điều kiện hóa trên bất kỳ bối cảnh z∈ {Iθ(τ)|τ∈ D} nào khác không? Vui lòng tham khảo Phần 5.2.

3. OPPO có thể đạt được hiệu suất cạnh tranh so với các cơ sở PbRL ngoại tuyến khác không? Vui lòng tham khảo Phần 5.3.

4. Chúng ta có thể thu được lợi ích gì từ việc thiết kế PbRL ngoại tuyến một bước, tức là lặp lại việc tiến hành mô hình hóa dữ liệu ngoại tuyến (Eq.6) và mô hình hóa sở thích (Eq.8)? Vui lòng tham khảo Phần 5.4.

5. OPPO hoạt động như thế nào về mặt lượng phản hồi sở thích? Vui lòng tham khảo Phần 5.5.

6. OPPO có thể đạt kết quả thỏa đáng bằng cách kết hợp sở thích từ con người thật thay vì giáo viên có kịch bản không? Vui lòng tham khảo Phần 5.6.

Để trả lời các câu hỏi trên, chúng tôi đánh giá OPPO trên các nhiệm vụ điều khiển liên tục từ điểm chuẩn D4RL (Fu et al., 2020). Cụ thể, chúng tôi chọn Hopper, Walker, và Halfcheetah làm ba nhiệm vụ cơ bản, với medium, medium-replay, medium-expert làm tập dữ liệu cho mỗi nhiệm vụ.

5.1. Không gian z có thể phù hợp tốt với các sở thích đã cho không?

Trong tiểu mục này, chúng tôi khám phá rằng OPPO có thể cho phép các sở thích phù hợp tốt trên không gian z được mã hóa bởi Iθ đã học. Đầu tiên chúng tôi lấy mẫu các quỹ đạo ngẫu nhiên từ tập dữ liệu ngoại tuyến D, và mã hóa chúng với Iθ đã học, và sử dụng t-SNE (van der Maaten & Hinton, 2008) như một công cụ để trực quan hóa z được mã hóa, được hiển thị trong Hình 3. z∗ tối ưu đã học được đánh dấu bằng một chấm cam. Bên cạnh đó, chúng tôi cũng đánh dấu (việc nhúng của) quỹ đạo tối ưu trong tập dữ liệu chuyên gia D4RL, được tạo ra bởi chính sách tối ưu trực tuyến đã học, bằng một chấm đỏ (z∗∗).

Theo Eq.8, các nhúng gần z∗∗ thực tế trong không gian z có nghĩa là chúng được ưa thích hơn được ngụ ý bởi nhãn sở thích. So sánh các quỹ đạo được lấy mẫu (nhúng), chúng tôi thấy OPPO thành công nắm bắt sở thích. Như được minh họa trong Hình 3, các quỹ đạo (nhúng) gần z∗∗ thường có lợi nhuận cao (các điểm có màu sâu hơn). Hơn nữa, chúng tôi quan sát thấy z∗ tối ưu đã học của chúng tôi liên tục ở gần z∗∗ thực tế, điều này gợi ý rằng z∗ đã học của chúng tôi bảo tồn các hành vi gần tối ưu. Do đó, nó đưa ra sự biện minh rằng OPPO có thể thực hiện mô hình hóa sở thích có ý nghĩa.

5.2. π(a|s,z∗) có thể đạt được hiệu suất tốt hơn không?

Hình 3 cho thấy Iθ(·) đã học của chúng tôi có thể tạo ra một không gian nhúng bối cảnh z phù hợp tốt thể hiện mô hình hóa sở thích hiệu quả trên (các nhúng của) quỹ đạo. Quan trọng hơn, thuộc tính sở thích của các nhúng bối cảnh nên được bảo tồn khi chúng ta điều kiện hóa bối cảnh trên chính sách có điều kiện đã học π(a|s,·). Nói cách khác, Iθ(·) nên chuyển mối quan hệ sở thích từ (τi, τj) sang (ℓ(zi,z∗), ℓ(zj,z∗)); hơn nữa, cuộn chính sách có điều kiện π(a|s,·), (τzi, τzj) tương tự nên bảo tồn mối quan hệ sở thích trên.

Để chỉ ra điều đó, chúng tôi so sánh hiệu suất của các cuộn bởi chính sách có điều kiện π(a|s,·) được điều kiện hóa trên các bối cảnh khác nhau trong Bảng 2. Chúng tôi chọn ba nhúng bối cảnh: z∗, zhigh (nhúng của quỹ đạo có lợi nhuận cao nhất trong D), và zlow (nhúng của quỹ đạo có lợi nhuận thấp nhất trong D) và cung cấp hiệu suất cuộn tương ứng (trung bình trên 3 hạt giống). Chúng tôi phát hiện rằng chính sách có điều kiện π(a|s,z) được điều kiện hóa trên z với lợi nhuận cao (hoặc thấp) (của quỹ đạo tương ứng τ=I−1θ(z)) thu được lợi nhuận thực tế cao (hoặc thấp) khi cuộn chính sách này trong môi trường, ví dụ, π(a|s,zhigh) hoạt động tốt hơn π(a|s,zlow) (do đó bảo tồn mối quan hệ sở thích nhìn lại). Hơn nữa, khi được điều kiện hóa trên z∗ tối ưu đã học, π(a|s,z∗) tạo ra hiệu suất tốt nhất so với việc được điều kiện hóa trên tất cả các nhúng ngoại tuyến khác. Lưu ý rằng π(a|s,z∗) tối ưu đã học của chúng tôi hoạt động tốt hơn chính sách có điều kiện π(a|s,zhigh). Kết quả này ngụ ý rằng quỹ đạo của chính sách tối ưu chúng tôi nói chung tốt hơn các quỹ đạo khác trong tập dữ liệu ngoại tuyến.

5.3. Hiệu suất của OPPO trên Các Nhiệm vụ Điểm chuẩn với Giáo viên Có kịch bản

Chúng tôi đã chỉ ra rằng OPPO tạo ra một bối cảnh gần tối ưu z∗, và chính sách có điều kiện đã học π(a|s,·) có thể bảo tồn sở thích nhìn lại. Tiểu mục này khảo sát liệu chính sách tối ưu π(a|s,z∗) có thể đạt được hiệu suất cạnh tranh trên điểm chuẩn (PBRL) ngoại tuyến hay không. Để so sánh, chúng tôi xem xét ba phương pháp PbRL ngoại tuyến và BC làm cơ sở: 1) DT+ r: thực hiện Decision Transformer (Chen et al., 2021) với hàm phần thưởng thực, và kết quả được chạy bởi chúng tôi; 2) DT+ rψ: thực hiện Decision Transformer với hàm phần thưởng đã học (sử dụng Eq.2); 3) CQL+ r: thực hiện CQL (Kumar et al., 2020) với hàm phần thưởng thực; 3) IQL+ r: thực hiện IQL với hàm phần thưởng thực, kết quả được báo cáo từ IQL (Kostrikov et al., 2022); 4) BC: thực hiện nhân bản hành vi trên tập dữ liệu, kết quả được báo cáo từ DT (Chen et al., 2021).

Trong Bảng 3, chúng tôi hiển thị hiệu suất của OPPO và các cơ sở. Chúng tôi có các quan sát sau. 1) OPPO đã duy trì hiệu suất so sánh được chống lại Decision Transformer được huấn luyện sử dụng phần thưởng thực. OPPO là một cách tiếp cận PbRL chỉ yêu cầu sở thích (con người), có hình thức giám sát linh hoạt và thẳng thắn hơn trong thế giới thực. 2) Mặc dù DT+ rψ cũng cho thấy kết quả cạnh tranh trong các điểm chuẩn này, phương pháp như vậy cần một mục tiêu của lợi nhuận để đi được xác định bởi tiền đề con người. Phương pháp của chúng tôi, ngược lại, tránh nhu cầu cho tiền đề mục tiêu như vậy bằng cách tìm kiếm qua không gian z. Chúng tôi lập luận rằng phương pháp tìm kiếm của chúng tôi mang lại lợi thế vì phần thưởng thường khó thu được trong các ứng dụng RL thế giới thực, nơi sở thích là thông tin duy nhất có thể truy cập để huấn luyện và triển khai một phương pháp RL.

5.4. Lợi ích của Mô hình PbRL Ngoại tuyến Một bước

Chúng tôi tiến hành một nghiên cứu loại bỏ để phân tích lợi ích của việc lặp lại LHIM và LPM (để cập nhật Iθ) trong một mô hình một bước. Đầu tiên, chúng tôi loại bỏ Iθ khỏi ∂LPM/∂θ và chỉ giữ việc nhúng tối ưu z∗ được cập nhật trong Eq.8. Sau đó, chúng tôi tiếp tục trực quan hóa không gian nhúng z cho thiết lập loại bỏ này (OPPO-a), trực quan hóa t-SNE được hiển thị trong Hình 4. Bằng cách so sánh Hình 4 với Hình 3, chúng ta có thể thấy rằng mối quan hệ sở thích trong không gian nhúng (được học với OPPO-a) đều bị xáo trộn. Trong một không gian z ít biểu cảm, việc mô hình hóa sở thích và tìm z∗ tối ưu là thách thức. Hơn nữa, như được hiển thị trong Bảng 4, kết quả so sánh của các nhiệm vụ medium-replay chứng minh rằng việc loại bỏ như vậy thực sự gây ra suy giảm hiệu suất.

5.5. Hiệu suất với Lượng Phản hồi Sở thích Khác nhau

Đối với nhiệm vụ Hopper, chúng tôi đánh giá tác động của các lượng nhãn sở thích khác nhau đối với hiệu suất của OPPO và hiển thị kết quả trong Bảng 5. Cụ thể, OPPO được đánh giá sử dụng lượng nhãn từ 50k, 1k, 500, trên tập dữ liệu từ Medium-Expert, Medium, Medium Replay. Như được minh họa trong Bảng 5, OPPO hoạt động tốt nhất khi được cung cấp 50k nhãn sở thích và đạt được tổng điểm chuẩn hóa 283.1 trong ba tập dữ liệu. Tuy nhiên, hiệu suất giảm xuống khoảng 250 khi lượng phản hồi giảm xuống 1k và 500. Do đó, OPPO mạnh mẽ với sự biến đổi về lượng phản hồi sở thích được sử dụng để huấn luyện.

5.6. Hiệu suất của OPPO trên Các Nhiệm vụ Điểm chuẩn với Giáo viên Con người Thật

Chúng tôi đã tiến hành các thí nghiệm bổ sung sử dụng dữ liệu có nhãn con người thật trên các nhiệm vụ Hopper và Walker. Các sở thích con người chúng tôi sử dụng được thu được từ tập dữ liệu nguồn mở của PT (Kim et al., 2023), được thu thập từ con người thực tế quen thuộc với các nhiệm vụ robot. Ngoài ra, chúng tôi đã tiến hành các thí nghiệm trên tập dữ liệu Robomimic (Mandlekar et al., 2022), cung cấp một tập các tập dữ liệu ngoại tuyến trên các miền thao tác robot 7-DoF. Trong các thí nghiệm của chúng tôi, chúng tôi kiểm tra phương pháp của mình trên hai nhiệm vụ (Lift và Can), nơi dữ liệu ngoại tuyến được thu thập bởi một toán tử xa điều khiển con người thành thạo (Proficient-Human) hoặc nhiều toán tử xa điều khiển con người với trình độ thay đổi (Multi-Human), và các nhãn sở thích cũng được gán nhãn bởi con người thật. Trong Bảng 6, chúng tôi so sánh OPPO với IQL+ r (Kostrikov et al., 2022) và IQL+PT (Kim et al., 2023). Chúng tôi thấy rằng phương pháp của chúng tôi vượt trội hơn IQL+PT trong hầu hết các nhiệm vụ, và thậm chí đạt được hiệu suất cạnh tranh hoặc tốt hơn so với IQL+ r.

Tóm lại, thông qua sáu thí nghiệm và trực quan hóa kết quả, chúng tôi chứng minh rằng không gian z được học bởi bộ mã hóa có thông tin và có thể diễn giải trực quan. Bên cạnh đó, nghiên cứu loại bỏ chứng minh rằng một không gian nhúng bối cảnh hướng dẫn bởi sở thích có thể cải thiện hiệu suất nhiệm vụ tiệm cận bằng một lề không thể bỏ qua. Hơn nữa, OPPO có thể tìm một nhúng để đại diện cho bối cảnh của quỹ đạo tối ưu, nơi quỹ đạo kết quả tốt hơn bất kỳ quỹ đạo ngoại tuyến nào trong tập dữ liệu. Cuối cùng nhưng không kém phần quan trọng, trong thiết lập ngoại tuyến với tương tác môi trường bị vô hiệu hóa, mô hình của chúng tôi có thể thu được các hành vi tối ưu sử dụng các nhãn sở thích nhị phân giữa các quỹ đạo không tối ưu. Như được hiển thị trong kết quả thí nghiệm, OPPO đạt được hiệu suất cạnh tranh so với DT được huấn luyện sử dụng phần thưởng thực hoặc phần thưởng giả.

6. Kết luận

Bài báo này giới thiệu tối ưu hóa chính sách hướng dẫn bởi sở thích ngoại tuyến (OPPO), một mô hình PbRL ngoại tuyến một bước. Không giống như các cách tiếp cận PbRL trước đây học chính sách từ một hàm phần thưởng giả (học một hàm phần thưởng riêng biệt là điều kiện tiên quyết), OPPO trực tiếp tối ưu hóa chính sách trong một không gian nhúng cấp cao. Để cho phép điều đó, chúng tôi đề xuất một mục tiêu khớp thông tin nhìn lại (HIM) ngoại tuyến và một mục tiêu mô hình hóa sở thích. Về mặt thực nghiệm, chúng tôi chỉ ra rằng việc lặp lại hai mục tiêu trên có thể tạo ra các nhúng bối cảnh có ý nghĩa và phù hợp sở thích. Hơn nữa, được điều kiện hóa trên bối cảnh tối ưu đã học, chính sách có điều kiện dựa trên HIM của chúng tôi có thể đạt được hiệu suất cạnh tranh trên các nhiệm vụ (PbRL) ngoại tuyến tiêu chuẩn.
