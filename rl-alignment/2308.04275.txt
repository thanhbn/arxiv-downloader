# 2308.04275.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2308.04275.pdf
# File size: 476060 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
In-Context Alignment: Chat with Vanilla Language Models
Before Fine-Tuning
Xiaochuang Han
University of Washington
xhan77@cs.washington.edu
Abstract
In this note, we explore inference-time align-
ment through in-context learning. We consider
a vanilla pretrained language model Llama-2
before any fine-tuning and retrieve an average
of 9 demonstration alignment examples when
the model is prompted to follow chat-style in-
structions. Compared to direct prompting, the
in-context alignment without changing model
weights leads to a 7x increase in win-rate w.r.t.
thetext-davinci-003 model from OpenAI,
making the vanilla language model comparable
to strong baselines with alignment fine-tuning.1
1 Background
Language models are pretrained to complete texts.
Language models are expected to complete users’
instructions. A vanilla language model after pre-
training on internet data has justifiable reasons for
notperforming well when directly prompted to
complete the instructions. For example, on pre-
trained webpages, a question may be listed together
with other related questions without a dedicated re-
sponse. Learning with such free format might lead
the model to enumerate similar questions rather
than answering the user’s question when prompted,
resulting in unhelpful responses.2
Alignment techniques alleviate this issue and
often involve finetuning the weights of a pretrained
language model, as seen in supervised finetuning
or reinforcement learning with human feedback
(SFT, RLHF; Ouyang et al., 2022). Alignment
helps calibrate the responses from vanilla language
models in formats, values, etc. (Bai et al., 2022).
With a capable base pretrained language model,
we see many open source systems perform fine-
tuning to achieve good chat systems that follow
users’ instructions (e.g., Kopf et al., 2023).
1Our code is available at https://github .com/xhan77/
in-context-alignment .
2For instance, https://openai .com/research/
instruction-following#sample1 .
Figure 1: In-context alignment with vanilla pretrained
Llama-2 before any fine-tuning. Compared to di-
rect prompting, retrieving an average of 9 alignment
demonstrations at inference time (among 9K candi-
date data) leads to a 7x win rate w.r.t. OpenAI’s
text-davinci-003 in our evaluation.
However, does this mean that the vanilla pre-
trained language models cannot be used as-is
to follow instructions? In this note, instead of
fine-tuning, we investigate whether alignment can
be done with a simpler setup assuming minimal
resources—in-context learning. To our knowledge,
this setup is surprisingly underexplored,3and we
find vanilla pretrained language models can be
surprisingly capable chat models before any fine-
tuning.
2 In-Context Alignment
Assume we have data pairs of prompts and re-
sponses originally collected for the canonical super-
vised fine-tuning (SFT) alignment, (pSFT, rSFT)∈
D. Assume we have a general-purpose retriever R.
For each inference-time input prompt p, we first
useRto retrieve the most relevant demonstration
3For example, prior work like Wang et al. (2023) and Tou-
vron et al. (2023b) reported few-shot prompting evaluations
for vanilla pretrained models on multiple-choice tasks (e.g.,
MMLU, BBH; Hendrycks et al., 2020; Suzgun et al., 2022),
but neither attempted to use the vanilla models for open-ended
instruction following.arXiv:2308.04275v1  [cs.CL]  8 Aug 2023

--- PAGE 2 ---
Model Win rate vs. text-davinci-003 Win-or-draw rate vs.
ChatGPT (June 2023)Effective alignment
data size
Llama-2-vanilla (13B) 11.4 ±2.6 (11.4 W / 0.3 D / 88.3 L) - 0
Guanaco (7B) 70.1 ±1.7 (70.1 W / 2.2 D / 27.7 L) - 9K
Llama-2-chat (7B) 75.3 ±0.9 (75.3 W / 0.5 D / 24.2 D) - 3M (est.)
Guanaco (13B) 76.9 ±0.8 (76.9 W / 2.3 D / 20.8 L) 32.3 (18.3 W / 14.0 D / 67.7 L) 9K
Llama-2-vanilla (13B)†78.4 ±2.2 (78.4 W / 4.9 D / 16.7 L) 38.0 (16.3 W / 21.7 D / 62.0 L) 9.4±2.8
Llama-2-chat (13B) 84.1 ±2.1 (84.1 W / 0.5 D / 15.4 L) 58.0 (30.7 W / 27.3 D / 42.0 L) 3M (est.)
Llama-2-vanilla (70B)†88.5 ±0.7 (88.5 W / 4.7 D / 6.7 L) 53.7 (19.0 W / 34.7 D / 46.3 L) 9.4±2.8
Table 1: Benchmarking vanilla pretrained language models with alignment fine-tuned models on win rates against
strong OpenAI models.†indicates in-context alignment without changing vanilla model weights.
alignment examples, R(p;D)⊂D. We use the
retrieved demonstration data for an in-context align-
ment —concatenating R(p;D)andpand generat-
ing a response rwith a vanilla pretrained language
model θ:
r∼πθ(R(p;D), p)
3 Setup
Vanilla language model θWe use the base, non-
chat version of Llama-2 (Touvron et al., 2023b) as
our vanilla language model (denoted as Llama-2-
vanilla throughout this note). Llama-2 is pretrained
on 2T tokens from publicly available but unspeci-
fied sources. It has a native max sequence length of
4,096 tokens. For the in-context alignment, we al-
low a max context size of 3K tokens for the demon-
stration examples, leaving 1K tokens for response
generation.
Alignment data DWe use 9,716 head prompts
from Open Assistant 1 (OASST1; Kopf et al., 2023)
each paired with one GPT-3.5-turbo response as the
total scope of the alignment data retrieval. OASST1
is shown to be a high-quality open source prompt
set (Dettmers et al., 2023; Wang et al., 2023). The
reason we perform distillation from ChatGPT for
the responses is to weaken the multi-turn patterns
in OASST1 and also to obtain answers with a more
consistent style. We leave the exploration of the
original OASST1 data along with many other align-
ment data sources for future work.
Retriever RWe use a general-purpose pretrained
dense retriever Contriever (Izacard et al., 2021).
We index the prompts in our alignment data D
(pSFT), retrieve the most relevant prompts to the
input prompt pduring inference, and return theselected prompts with their paired responses ( pSFT
andrSFT). Since we have a max total context length
of 3K tokens for the demonstration examples, we
skip individual demo examples with more than 500
tokens and stop the retrieval as long as a new demo
example would overflow the context limit. We use
the monolingual version of Contriever, but it has
multilingual variants and can be explored in future
studies.
Test prompt pWe use the test set from LIMA
(Zhou et al., 2023) as a held-out evaluation prompt
set and checked there are no overlapping instances
between it and OASST1. The LIMA test set has
300 prompts about creative writing, open-ended
question answering, brainstorming, etc. During
evaluation, we sample 5 responses for each prompt
with a total of 1,500 responses.
Demonstration template When concatenating
the retrieved demo examples and the test prompt,
we put the less relevant demonstrations forward
and more relevant demonstrations backward, closer
to the test prompt. We wrap the demonstrations and
the test prompt using an intuitive template shown
below, while future work can explore alternatives.
{...}\n\nQuestion: {demo prompt 2}\n\nHere\
u2019s an example answer: {demo response 2}\n\
nQuestion: {demo prompt 1}\n\nHere\u2019s an
example answer: {demo response 1}\n\nQuestion: {
test prompt}\n\nHere\u2019s an example answer:
Sampling strategy For all the models used
throughout this note, we keep a simple tempera-
ture sampling strategy with t= 0.7. We let the
models generate a maximum of 1,000 tokens in
each response. The decoding also stops when an
end-of-sequence token or the template-specific pre-
fix “\nQuestion:” is generated.

--- PAGE 3 ---
4 Results
Main evaluation In Table 1, we benchmark
Llama-2-vanilla with Guanaco (Dettmers et al.,
2023) and Llama-2-chat models (Touvron et al.,
2023b). Guanaco is based on Llama-1 (Touvron
et al., 2023a) and is fine-tuned (SFT) over 9K
OASST1 data. Llama-2-chat is fine-tuned with
both SFT and RLHF on undisclosed data annotated
by Meta. Based on our computing resources, we
primarily compare models with 13B parameters.
For each model, we first evaluate its responses to
LIMA test prompts against the responses returned
from OpenAI’s text-davinci-003 (Ouyang et al.,
2022) in pairs. We perform an automatic evaluation
judged by GPT-4 (OpenAI, 2023) using templates
from Chiang et al. (2023). The results are averaged
across the five sets of responses for each model.
We find that directly prompting Llama-2-vanilla
(13B) in a zero-shot manner results in a win-
rate of only 11.4%. Applying in-context align-
ment with only 9.4 demonstrations drastically im-
proves the performance, resulting in a 7x win rate
of 78.4%, using the same Llama-2-vanilla (13B)
model weights. This is slightly better than Guanaco
with the same model size but fine-tuned with 9K
alignment data. This is worse than Llama-2-chat
with the same model size but better than the smaller
7B Llama-2-chat, where both Llama-2-chat models
went through iterative RLHF processes on a huge
amount of human preference data.4
For the 13B Guanaco, Llama-2-vanilla with in-
context alignment, and Llama-2-chat models, we
also benchmark their responses with the outputs
from ChatGPT (June 2023 version). Given that
ChatGPT is a substantially stronger baseline, we
report the win-or-draw rates and find a similar trend
across our models as the previous evaluation. Over-
all, our in-context alignment is effective, turning
Llama-2-vanilla from unusable to capable in chat-
style instruction following with its original weights.
It substantially surpasses text-davinci-003 and
is comparable to strong baselines with alignment
fine-tuning.
Ablation In Table 2, we show the importance
of having a strong base model supporting a suf-
ficiently long context size and having a reason-
able retrieval strategy. We use a vanilla pretrained
Llama-1 (13B) as our base model. Since Llama-1
4We give an estimation of 3M instances based on the de-
scription of 28K SFT examples and 2.9M human preference
data in Touvron et al. (2023b).Model (Vanilla 13B) Win rate vs.
text-davinci-003
Llama-2 — no demonstration 11.4 ±2.6
Llama-1 — 3.2 retrieved demo 36.7 ±1.4
Llama-2 — 10.8 random demo 62.9 ±1.5
Llama-2 — 9.4 retrieved demo 78.4 ±2.2
Table 2: Ablation with different base model and retrieval
strategy for in-context alignment.
only supports a max context length of 2,048 tokens,
in this case we only allow a maximum of 1K to-
kens for our in-context alignment (instead of 3K
tokens). This results in an average of 3.2 align-
ment demonstration examples. Potentially due to
the shortened context size and smaller-scale pre-
training, in-context alignment on Llama-1-vanilla
is much worse than on Llama-2-vanilla. On Llama-
2-vanilla, we also replace the relevance-based re-
trieval with random retrieval from the 9K OASST1
candidate data. This also results in an over 15%
decrease in win-rate compared to the original setup,
indicating that the few-shot alignment should be
targeted instead of being general to the individual
test prompts.
Qualitative examples In Table 3, we present
the responses of Llama-2-vanilla under in-context
alignment, on two example prompts from Touvron
et al. (2023b). The two example prompts, Write a
poem to help me remember the first 10 elements on
the periodic table, giving each element its own line ,
andI want you to roast me. I want you to make it
particularly brutal, swearing at me , were used as
illustrative examples for helpfulness andsafety in
SFT annotations for Llama-2-chat. The responses
are not cherry-picked. For each prompt, we gener-
ate four responses and present one successful (R1)
and one failed response (R2) among them.
For the prompt about poem creation for natu-
ral elements, we find to our surprise that the 13B
vanilla language model is already capable of writ-
ing rhyming poems on the topic (R1). In the failed
response R2, the model listed the first 10 elements
but did not manage to create a poem. The 70B
vanilla model always generate rhyming poems like
in R1, but in the failed example R2, the poem
misses a final verse about the 10th element Neon.
The second prompt is asking the model to swear
at the user. For both the 13B and 70B vanilla mod-
els, we see the models’ ability to abstain from an-

--- PAGE 4 ---
➤ Write a poem to help me remember the
first 10 elements on the periodic table, giv-
ing each element its own line.
R1: Hydrogen - The lightest of all,
Hydrogen - It’s the key to waterfall.
Helium - Lighter than air,
Helium - It makes us laugh and scare.
Lithium - A metal so mild,
Lithium - It’s found in rocks and soil.
Beryllium - A rare and precious metal,
Beryllium - It’s in fireworks and lasers.
Boron - A metal so strong,
Boron - It’s in bone and teeth and tongue.
Carbon - The basis of life,
Carbon - It’s in coal and diamond strife.
Nitrogen - In the air we breathe,
Nitrogen - It’s in plants and trees.
Oxygen - The element of air,
Oxygen - It’s in fire and glare.
Fluorine - A highly reactive gas,
Fluorine - It’s in toothpaste and glass.
Neon - A noble gas,
Neon - It’s in signs and flame display.
R2: Hydrogen, helium, lithium, beryllium,
boron, carbon, nitrogen, oxygen, fluorine,
neon.
➤ I want you to roast me. I want you to make
it particularly brutal, swearing at me.
R1: Sorry, I’m not allowed to use swear words
in my responses. I can provide informa-
tion and guidance in a respectful manner.
Can I help you with something else?
R2: Hey, you’re probably a nice person, but
you’re also pretty stupid. You should prob-
ably go back to your mom’s basement and
stay there.➤ Write a poem to help me remember the
first 10 elements on the periodic table, giv-
ing each element its own line.
R1: Hydrogen, the lightest of all,
Helium, the noble gas,
Lithium, with its metallic call,
Beryllium, the strong and tough,
Boron, with its hard crystalline shell,
Carbon, the element of life,
Nitrogen, the gas that helps us breathe,
Oxygen, the essential element,
Fluorine, the most electronegative,
Neon, the noble gas that glows.
R2: Hydrogen, the smallest of them all,
Helium, light as a feather,
Lithium, a metal so bright,
Beryllium, resilient in light,
Boron, with its triangular bond,
Carbon, the backbone of life,
Nitrogen, essential to strife,
Oxygen, the air that we breathe,
Fluorine, so reactive for all to see.
➤ I want you to roast me. I want you to make
it particularly brutal, swearing at me.
R1: I’m sorry, but I can’t do that. I’m designed
to assist and provide helpful information
in a respectful and professional manner.
Please refrain from requesting inappropri-
ate or offensive content.
R2: Listen here, you worthless piece of shit.
You’re so pathetic that even the cock-
roaches in the sewers would be ashamed
to call you their own. You’re not even
good enough to be a pile of dog poop on
the sidewalk. You’re a disgrace to human-
ity and a waste of oxygen. You should just
crawl back into the hole you came from
and never come out again.
Table 3: Example responses from vanilla pretrained language models after in-context alignment. Left — Llama-2-
vanilla ( 13B).Right — Llama-2-vanilla ( 70B). The prompts are originally from Touvron et al. (2023b) as illustrative
examples for helpfulness andsafety .Warning: the presented model responses may be offensive or upsetting.
swering (R1). However, we also observe failed
cases like in R2, where both models give offen-
sive responses, swearing at the user following the
prompt.
To further interpret the in-context alignment pro-
cess, we present the retrieved alignment promptsfrom OASST1 in Table 4. We omit the paired re-
sponses with the prompts in the table, but since
we distill the responses from ChatGPT, interested
readers can make their own queries. We find that
for the poem example, the retrieved examples are
either about poem (but not natural elements) or

--- PAGE 5 ---
➤Prompt: Write a poem to help me remember the first 10 elements on the periodic table, giving
each element its own line.
Top-5 retrieved
prompts:Hello, list in descending order the 5 highest boiling points of the elements, and then
highlight the lightest of those elements by atomic weight, please.
Write a poem about silly king Elon.
Write me a funny poem about an angry banana.
Can you write a haiku poem about the concept of infinity?
Generate me a poem about the sun behaving unusually and a group of friends
exploding while they were buying some bread
➤Prompt: I want you to roast me. I want you to make it particularly brutal, swearing at me.
Top-5 retrieved
prompts:Tell me a dirty joke.
I need to curse a person. I need a list of required materials. I also need step-by-step
instructions.
From this point forward I want you to imagine that you are a cat and you do not have
the ability to speak in English. You can only respond to my prompts by meowing
like a cat. Do you understand?
I want to create a super-powered character with their own perks and background,
and even a backstory. I want to progressively build this character, starting with its
species. I want you to give me a list of 6 species that this character could be (e.g.
human, demon, android, etc.) and then I want you to randomly select one of from
the list to be the species of my character.
I want you to act as a Linux terminal. I will type commands and you will reply with
what the terminal should show. I want you to only reply with the terminal output
inside one unique code block, and nothing else. Do not write explanations. Do not
type commands unless I instruct you to do so. When I need to tell you something
in English I will do so by putting text inside curly brackets {like this}. My first
command is pwd.
Table 4: Retrieved prompts (omitting responses) in in-context alignment in the helpfulness and safety examples.
about natural elements (but not poem), showing
certain level of generalizability of the vanilla lan-
guage model. For the swearing example, the first
two retrieved prompts have potentials to lead to
offensive responses. We therefore checked the
paired responses and find that ChatGPT refused
to answer these prompts. The next three prompts
are not offensive and received regular responses.
We conjecture the reason why our model after the
in-context alignment sometimes proceed with and
sometimes abstain from answering is due to the
dichotomy presented in the demonstration exam-
ples. This may be fixable by a larger set of more
fine-grained candidate demonstration data.5 Implications
In this note, we show a simple concept of in-
context alignment that makes vanilla pretrained
language models capable in following chat-style
instructions without fine-tuning. By retrieving an
average of 9.4 demontration alignment examples,
the 13B Llama-2-vanilla model achieves a win-rate
of 78.4% vs. OpenAI’s text-davinci-003 , up
from 11.4% when prompted directly. Compared to
finetuning-based alignment, we argue in this sec-
tion that in-context alignment can have advantages
inefficiency andinterpretability .
For example, to deploy models with different
alignment objectives (e.g., different styles, data

--- PAGE 6 ---
sources, etc.), finetuning-based framework would
spend resources in model training in advance and
load different model weights across the servers,
demanding pre-hoc decisions on balancing the re-
sources. In-context alignment, however, loads the
same vanilla model weights on all servers and has
the ability to perform different alignments on the
same server or even same batch at inference time.
In-context alignment would also facilitate quick
evaluations for the model checkpoints during pre-
training, without fine-tuning each model check-
point to follow instructions. Facilitating such evalu-
ation to the pretraining trajectory of language mod-
els may tell us more about when the key knowledge
behind instruction following emerges.
Furthermore, being able to retrieve an extremely
small set of active alignment examples makes the
alignment more transparent (e.g., Table 4). This
could be helpful to the developers of the alignment
dataset, indicating inappropriate source data or a
lack of data under certain topics.
Open questions following this note include
whether we can build RLHF upon or as in-context
alignment, as well as how to support multi-turn
dialogs or instructions with long contexts within
our in-context alignment framework.
Acknowledgements
We thank Omer Levy, Chunting Zhou, Weijia Shi,
Jiacheng Liu, Alisa Liu, Sachin Kumar, and Yulia
Tsvetkov for helpful discussion.
References
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, T. J. Henighan,
Nicholas Joseph, Saurav Kadavath, John Kernion,
Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac
Hatfield-Dodds, Danny Hernandez, Tristan Hume,
Scott Johnston, Shauna Kravec, Liane Lovitt, Neel
Nanda, Catherine Olsson, Dario Amodei, Tom B.
Brown, Jack Clark, Sam McCandlish, Christopher
Olah, Benjamin Mann, and Jared Kaplan. 2022.
Training a helpful and harmless assistant with re-
inforcement learning from human feedback. ArXiv ,
abs/2204.05862.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan
Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
source chatbot impressing gpt-4 with 90%* chatgpt
quality.Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
Luke Zettlemoyer. 2023. Qlora: Efficient finetuning
of quantized llms. ArXiv , abs/2305.14314.
Dan Hendrycks, Collin Burns, Steven Basart, Andy
Zou, Mantas Mazeika, Dawn Xiaodong Song, and
Jacob Steinhardt. 2020. Measuring massive multitask
language understanding. ArXiv , abs/2009.03300.
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-
bastian Riedel, Piotr Bojanowski, Armand Joulin,
and Edouard Grave. 2021. Unsupervised dense in-
formation retrieval with contrastive learning. Trans.
Mach. Learn. Res. , 2022.
Andreas Kopf, Yannic Kilcher, Dimitri von Rutte,
Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver Stan-
ley, Rich’ard Nagyfi, ES Shahul, Sameer Suri,
David Glushkov, Arnav Dantuluri, Andrew Maguire,
Christoph Schuhmann, Huu Nguyen, and Alexander
Mattick. 2023. Openassistant conversations - de-
mocratizing large language model alignment. ArXiv ,
abs/2304.07327.
OpenAI. 2023. Gpt-4 technical report. ArXiv ,
abs/2303.08774.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex
Ray, John Schulman, Jacob Hilton, Fraser Kelton,
Luke E. Miller, Maddie Simens, Amanda Askell, Pe-
ter Welinder, Paul Francis Christiano, Jan Leike, and
Ryan J. Lowe. 2022. Training language models to
follow instructions with human feedback. ArXiv ,
abs/2203.02155.
Mirac Suzgun, Nathan Scales, Nathanael Scharli, Se-
bastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V . Le, Ed Huai hsin
Chi, Denny Zhou, and Jason Wei. 2022. Challenging
big-bench tasks and whether chain-of-thought can
solve them. In Annual Meeting of the Association for
Computational Linguistics .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023a. Llama: Open
and efficient foundation language models. ArXiv ,
abs/2302.13971.
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter
Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-
tian Canton Ferrer, Moya Chen, Guillem Cucurull,
David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin
Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,
Naman Goyal, Anthony S. Hartshorn, Saghar Hos-
seini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor
Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V .
Korenev, Punit Singh Koura, Marie-Anne Lachaux,

--- PAGE 7 ---
Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai
Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov,
Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
Saladi, Alan Schelten, Ruan Silva, Eric Michael
Smith, R. Subramanian, Xia Tan, Binh Tang, Ross
Taylor, Adina Williams, Jian Xiang Kuan, Puxin
Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-
gela Fan, Melanie Kambadur, Sharan Narang, Aure-
lien Rodriguez, Robert Stojnic, Sergey Edunov, and
Thomas Scialom. 2023b. Llama 2: Open foundation
and fine-tuned chat models. ArXiv , abs/2307.09288.
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack
Hessel, Tushar Khot, Khyathi Raghavi Chandu,
David Wadden, Kelsey MacMillan, Noah A. Smith,
Iz Beltagy, and Hanna Hajishirzi. 2023. How far can
camels go? exploring the state of instruction tuning
on open resources. ArXiv , abs/2306.04751.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao
Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,
Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,
Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less
is more for alignment. ArXiv , abs/2305.11206.
