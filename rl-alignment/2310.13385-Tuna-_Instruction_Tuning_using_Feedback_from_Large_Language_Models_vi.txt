# Tuna: Điều chỉnh hướng dẫn sử dụng phản hồi từ các mô hình ngôn ngữ lớn

Haoran Li1,†,Yiran Liu3,‡,Xingxing Zhang2,Wei Lu1,Furu Wei2
1Nhóm Nghiên cứu StatNLP, Đại học Công nghệ và Thiết kế Singapore
2Microsoft Research Asia,3Đại học Thanh Hoa
haoran2_li@mymail.sutd.edu.sg, wei_lu@sutd.edu.sg
liu-yr21@mails.tsinghua.edu.cn ,{xizhang,fuwei} @microsoft.com

## Tóm tắt

Việc điều chỉnh hướng dẫn các mô hình ngôn ngữ lớn (LLM) mã nguồn mở như LLaMA, sử dụng các đầu ra trực tiếp từ các LLM mạnh mẽ hơn như Instruct-GPT và GPT-4, đã được chứng minh là một cách hiệu quả về chi phí để điều chỉnh hành vi mô hình theo sở thích của con người. Tuy nhiên, mô hình được điều chỉnh hướng dẫn chỉ thấy một phản hồi cho mỗi hướng dẫn, thiếu kiến thức về các phản hồi có thể tốt hơn. Trong bài báo này, chúng tôi đề xuất tinh chỉnh một LLM đã được điều chỉnh hướng dẫn bằng cách sử dụng các phương pháp xếp hạng xác suất và xếp hạng theo ngữ cảnh mới của chúng tôi để tăng khả năng tạo ra các phản hồi tốt hơn. Xếp hạng xác suất cho phép mô hình được điều chỉnh hướng dẫn kế thừa các thứ hạng tương đối của các phản hồi chất lượng cao và chất lượng thấp từ LLM giáo viên. Mặt khác, việc học với xếp hạng theo ngữ cảnh cho phép mô hình tinh chỉnh phân phối phản hồi của chính nó bằng cách sử dụng khả năng hiểu ngữ cảnh của các LLM mạnh mẽ hơn. Hơn nữa, chúng tôi áp dụng xếp hạng xác suất và xếp hạng theo ngữ cảnh tuần tự cho LLM được điều chỉnh hướng dẫn. Mô hình kết quả, mà chúng tôi gọi là Tuna, cải thiện hiệu suất một cách nhất quán trên Super Natural Instructions (119 nhiệm vụ kiểm tra), LMentry (25 nhiệm vụ kiểm tra), Vicuna QA, và thậm chí có thể đạt được kết quả tốt hơn so với một số đường cơ sở học tăng cường mạnh. Mã và dữ liệu của chúng tôi có sẵn tại https://github.com/microsoft/LMOps.

## 1 Giới thiệu

Các mô hình ngôn ngữ lớn (LLM) đã đạt được tiến bộ đáng kể bằng cách mở rộng quy mô mô hình và quy mô dữ liệu (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Brown et al., 2020; OpenAI, 2023) cho việc tiền huấn luyện không giám sát và sau đó áp dụng học tăng cường từ phản hồi của con người (RLHF) để điều chỉnh các phản hồi của mô hình với sở thích của con người (Christiano et al., 2017; Ouyang et al., 2022). Gần đây hơn, điều chỉnh hướng dẫn (Wei et al., 2022) với thuật toán Self-Instruct (Wang et al., 2022a) đã nổi lên như một phương pháp hiệu quả về chi phí để điều chỉnh với sở thích của con người. Trong phương pháp này, các LLM mở như LLaMA (Touvron et al., 2023) có thể được tinh chỉnh trên dữ liệu theo hướng dẫn được tạo bởi OpenAI GPT sử dụng thuật toán Self-Instruct. Mô hình Alpaca (Taori et al., 2023) minh họa kỹ thuật này, cho phép điều chỉnh chặt chẽ với sở thích của con người trong khi giảm sự phụ thuộc vào dữ liệu được gắn nhãn bởi con người.

Tuy nhiên, điều chỉnh hướng dẫn chỉ cung cấp một hướng dẫn rộng cho các LLM cơ sở để chuyển từ "dự đoán token tiếp theo" sang một phong cách tương tác, theo hướng dẫn hơn. Kết quả là, mô hình có thể học một số đặc điểm hoặc phong cách bề mặt từ dữ liệu hướng dẫn nhưng vẫn thiếu hiểu biết sâu sắc về những gì cấu thành một phản hồi được ưa thích. Ví dụ, khi được đưa ra một câu hỏi như "Đưa ra ba lời khuyên để giữ sức khỏe", một LLM cơ sở có thể tạo ra các phần tiếp theo lưu loát nhưng không mong muốn, trong khi một LLM được điều chỉnh hướng dẫn có thể đưa ra ba lời khuyên chung. Con người có thể thích các lời khuyên chi tiết hơn so với các lời khuyên chung, nhưng những lời khuyên như vậy ít có khả năng được lấy mẫu vì chúng có khả năng thấp hơn trong phân phối mô hình hiện tại. Điều này có thể được quy cho thực tế là chúng hoặc không được thấy trong quá trình điều chỉnh hướng dẫn hoặc khó được lấy mẫu do thiên lệch phơi bày (Ranzato et al., 2015).

Để giải quyết vấn đề này, chúng tôi đề xuất tinh chỉnh thêm một LLM được điều chỉnh hướng dẫn để phân biệt chất lượng của nhiều phản hồi một cách chính xác hơn, sử dụng các phương pháp xếp hạng xác suất mới (Phần 2.2; Hình 1 trên) và xếp hạng theo ngữ cảnh (Phần 2.3; Hình 1 giữa) của chúng tôi. Xếp hạng xác suất cho phép LLM được điều chỉnh hướng dẫn kế thừa các phản hồi chất lượng cao và chất lượng thấp cũng như thứ hạng tương đối của chúng từ LLM giáo viên (ví dụ, text-davinci-003). Ngược lại, xếp hạng theo ngữ cảnh nhằm cân bằng lại phân phối phản hồi của mô hình được điều chỉnh hướng dẫn với sự trợ giúp của các LLM mạnh mẽ hơn (ví dụ, GPT-4), giảm thiểu vấn đề thiên lệch phơi bày.

Chúng tôi áp dụng xếp hạng xác suất và xếp hạng theo ngữ cảnh tuần tự cho một mô hình được điều chỉnh hướng dẫn, tức là Alpaca (Taori et al., 2023), tạo ra một mô hình được gọi là Tuna (Phần 2.4; Hình 1 dưới). Chúng tôi đánh giá Tuna trên các điểm chuẩn khác nhau, bao gồm Super Natural Instructions (Wang et al., 2022b), chứa 119 nhiệm vụ kiểm tra đa dạng; LMentry (Efrat et al., 2022), bao gồm 25 nhiệm vụ để đánh giá khả năng cơ bản và tính mạnh mẽ của LLM; và Vicuna QA (Chiang et al., 2023) đánh giá khả năng trả lời một tập hợp đa dạng các câu hỏi của mô hình với sự hỗ trợ của GPT-4. Kết quả thử nghiệm cho thấy mô hình Tuna không chỉ liên tục vượt trội so với các mô hình được điều chỉnh hướng dẫn tiêu chuẩn trên tất cả các điểm chuẩn, mà còn vượt qua một số đường cơ sở RLHF mạnh (Ouyang et al., 2022).

Tóm lại, các đóng góp của chúng tôi như sau:
• Chúng tôi đề xuất xếp hạng xác suất và xếp hạng theo ngữ cảnh, cho phép mô hình được điều chỉnh hướng dẫn phân biệt các phản hồi chất lượng cao và chất lượng thấp và gán xác suất cao hơn cho cái trước một cách tương ứng.
• Mô hình Tuna, thu được bằng cách áp dụng tuần tự xếp hạng xác suất và xếp hạng theo ngữ cảnh trên một LLM được điều chỉnh hướng dẫn, đạt được kết quả tốt hơn so với một số điểm chuẩn mạnh, bao gồm cả các mô hình RLHF;
• Mô hình, dữ liệu và mã của chúng tôi sẽ được phát hành để hỗ trợ nghiên cứu trong tương lai.

## 2 Phương pháp

Trong phần này, chúng tôi mô tả cách thu được mô hình Tuna của chúng tôi bằng cách sử dụng phản hồi từ LLM. Đầu tiên chúng tôi mô tả điều chỉnh hướng dẫn cơ bản. Sau đó chúng tôi giới thiệu các phương pháp xếp hạng xác suất và xếp hạng theo ngữ cảnh của chúng tôi. Cuối cùng, chúng tôi mô tả cách tích hợp cả hai phương pháp xếp hạng.

### 2.1 Điều chỉnh hướng dẫn

Các LLM như GPT-3 (Brown et al., 2020) đã được huấn luyện trên một tập dữ liệu văn bản khổng lồ sử dụng ước lượng khả năng tối đa (MLE):

LMLE(y) = -1/|y| ∑t log p(yt|y<t;θ),                    (1)

trong đó θ đại diện cho các tham số của mô hình cơ sở. Hàm mục tiêu tiền huấn luyện buộc mô hình dự đoán token tiếp theo yt cho trước tiền tố y<t = [y0, y1, ..., yt-1]. Một LLM được huấn luyện đầy đủ có thể tạo ra các phần tiếp theo lưu loát cho hầu như bất kỳ tiền tố nào. Tuy nhiên, các phần tiếp theo được tạo có thể không phù hợp tốt với sở thích của con người. Vì mục tiêu chính của LLM là hỗ trợ con người, việc khuyến khích tạo nội dung tuân theo hướng dẫn của con người và phù hợp với sở thích của con người trở nên cần thiết. Phương pháp chủ đạo hiện tại để nâng cao khả năng theo hướng dẫn của LLM được gọi là điều chỉnh hướng dẫn (Mishra et al., 2021; Wei et al., 2022; Taori et al., 2023), tinh chỉnh các LLM cơ sở theo cách có giám sát trên các cặp hướng dẫn-phản hồi {i, r} (trong đó i là một hướng dẫn và r là phản hồi của nó) sử dụng MLE:

LMLE(i, r) = -1/|r| log p(r|i;θ'),                      (2)

trong đó θ' đại diện cho các tham số của mô hình được điều chỉnh hướng dẫn. Sau điều chỉnh hướng dẫn, chúng ta mong đợi phân phối mô hình p(·|i;θ') phân bổ xác suất cao hơn cho các phản hồi phù hợp như r thay vì các phần tiếp theo không mong muốn.

Lưu ý rằng các phản hồi trong các cặp hướng dẫn-phản hồi có thể được chú thích bởi con người hoặc được tạo bởi các LLM mạnh, như Instruct-GPT hoặc GPT-4 (Wang et al., 2022a). Một phương pháp phổ biến và hiệu quả về chi phí để tạo dữ liệu điều chỉnh hướng dẫn là thuật toán Self-Instruct (Wang et al., 2022a). Cụ thể, nó sử dụng một LLM mạnh, ví dụ text-davinci-003, để tạo hướng dẫn dựa trên một vài hướng dẫn mầm, và sau đó tạo một phản hồi duy nhất cho mỗi hướng dẫn sử dụng cùng LLM đó.

### 2.2 Xếp hạng xác suất

Điều chỉnh hướng dẫn với dữ liệu được tạo bởi thuật toán Self-Instruct về cơ bản là một dạng chưng cất cấp độ chuỗi (Kim và Rush, 2016). Lý do đằng sau lớp phương pháp chưng cất này là các LLM thương mại hiện tại có khả năng tốt hơn đáng kể so với các đối tác mã nguồn mở của chúng. Thay vì học từ dữ liệu phản hồi đơn, phương pháp xếp hạng xác suất của chúng tôi tận dụng thứ hạng tương đối của nhiều phản hồi dựa trên xác suất của mô hình giáo viên để chưng cất nhãn giả tốt hơn (xem Hình 1 trên).

Gọi r là phản hồi gốc cho hướng dẫn i trong tập dữ liệu điều chỉnh hướng dẫn. Chúng tôi truy vấn các LLM mạnh (giáo viên), như text-davinci-003, để tạo N phản hồi mới cho i. Gọi r(0), r(1), ..., r(N-1) là các phản hồi mới này, và p(r(0)|i), p(r(1)|i), ..., p(r(N-1)|i) là xác suất của chúng. Mặc dù các LLM giáo viên được mong đợi tạo ra các phản hồi có chất lượng tương đương trung bình, chắc chắn sẽ có một số biến đổi trong chất lượng của các phản hồi được tạo. Sự biến đổi vốn có này thể hiện qua nhiều khía cạnh khác nhau, như sự khác biệt về độ chính xác (Wang et al., 2023a), độ dài phản hồi, và mức độ chi tiết được cung cấp (Wang et al., 2023b).

Trực quan, nếu một mô hình được chưng cất hoàn hảo, các xác suất tương đối được gán cho hai mẫu nên giống như những xác suất của mô hình giáo viên. Cụ thể, gọi p(r(j)|i;θ') và p(r(k)|i;θ') là xác suất của r(j) và r(k) w.r.t. mô hình học sinh. Nếu p(r(j)|i) > p(r(k)|i), thì p(r(j)|i;θ') > p(r(k)|i;θ'). Chúng tôi sử dụng log-likelihood chuẩn hóa sau đây làm điểm chất lượng mô hình giáo viên để tính đến sự khác biệt về độ dài phản hồi:

s(i, r(k)) = log p(r(k)|i) / |r(k)|^β, k = {0, ..., N-1}     (3)

trong đó |r(k)| là độ dài của r(k) và β đại diện cho phạt độ dài.

Sau đó chúng tôi xếp hạng các phản hồi đó theo thứ tự giảm dần dựa trên s(i, r(k)). Các cặp hướng dẫn-phản hồi kết quả trở thành {i, r, (r[0], ...r[N-1])}, trong đó i, r từ dữ liệu điều chỉnh hướng dẫn gốc, và r[j] được coi là có chất lượng tốt hơn r[k], nếu j < k. Một khi chúng tôi có được các phản hồi được xếp hạng, chúng tôi có thể khuyến khích mô hình của mình học từ các thứ hạng này bằng cách sử dụng một mục tiêu xếp hạng theo cặp, đã được sử dụng thành công trong các công trình trước (Zhong et al., 2020; Liu et al., 2022; Zhang et al., 2022; Zhao et al., 2023). Hàm mục tiêu xếp hạng như sau:

Lrank = ∑(0≤j<k≤N-1) L^j,k_rank                           (4)

L^j,k_rank = max(0, v^k_θ' - v^j_θ' + m×(k-j)), j < k      (5)

trong đó v^k_θ' = 1/|r[k]| log p(r[k]|i;θ'), m > 0 là tham số biên. Tổn thất xếp hạng, Lrank, nhằm dạy mô hình phân biệt các phản hồi tốt và xấu dựa trên quan điểm của LLM giáo viên. Ngoài Lrank, chúng tôi cũng áp dụng tổn thất entropy chéo trên phản hồi gốc làm điều hòa:

L = Lrank + λLMLE, LMLE = -1/|r| log p(r|i;θ')            (6)

trong đó r là phản hồi gốc, và λ > 0 kiểm soát tầm quan trọng của LMLE, giúp ngăn chặn tối ưu hóa quá mức của tổn thất xếp hạng.

Sau khi học với xếp hạng xác suất, mô hình có thể gán xác suất tốt hơn cho các phản hồi ưu việt và kém hơn.

### 2.3 Xếp hạng theo ngữ cảnh

Trong giai đoạn điều chỉnh hướng dẫn hoặc xếp hạng xác suất, mô hình được tinh chỉnh để tạo ra một r tốt cho trước hướng dẫn i. Tuy nhiên, cho cùng i trong quá trình suy luận, mô hình vẫn có thể tạo ra một phản hồi chất lượng tương đối thấp r'. Điều này liên quan đến vấn đề thiên lệch phơi bày (Ranzato et al., 2015), trong đó mô hình không thể tạo ra r do các lỗi tích lũy trong quá trình tạo tự hồi quy. Để giải quyết vấn đề này, chúng tôi sử dụng phương pháp xếp hạng theo ngữ cảnh của mình để tinh chỉnh phân phối của các phản hồi được tạo bởi chính mô hình, gán xác suất cao hơn cho các phản hồi tốt hơn với sự trợ giúp của các LLM mạnh (Hình 1 giữa), do đó giảm thiểu thiên lệch phơi bày (Ranzato et al., 2015).

Đối với mỗi hướng dẫn, đầu tiên chúng tôi lấy mẫu N phản hồi từ chính mô hình được điều chỉnh hướng dẫn, tức là r(0), r(1), ..., r(N-1) ~ p(·|i;θ'). Chúng tôi hy vọng các mẫu đủ đa dạng để các phản hồi tốt hơn có nhiều khả năng xuất hiện trong kết quả được lấy mẫu.

Để đảm bảo tính đa dạng, chúng tôi áp đặt một ràng buộc về điểm ROUGE-L (Lin, 2004) giữa mỗi cặp phản hồi, yêu cầu nó phải nhỏ hơn một ngưỡng τ. Nếu điểm ROUGE-L vượt quá τ, chúng tôi tăng nhiệt độ lấy mẫu và lấy mẫu lại một phản hồi khác. Nếu nhiều lần thử vẫn dẫn đến điểm ROUGE-L trên τ, chúng tôi giữ lại phản hồi ít tương tự nhất từ các lần thử. Sau khi có được N phản hồi, chúng tôi tận dụng khả năng hiểu ngữ cảnh của các LLM thương mại, như GPT-4 (OpenAI, 2023), để xếp hạng chúng dựa trên nhiều khía cạnh khác nhau. Quá trình xếp hạng bao gồm nhiều bước. Đầu tiên, chúng tôi yêu cầu GPT-4 đánh giá xem hướng dẫn có yêu cầu câu trả lời mở (ví dụ, tạo câu chuyện) hay câu trả lời đóng (ví dụ, giải bài toán). Sau đó chúng tôi yêu cầu GPT-4 tạo phản hồi riêng của nó làm tham chiếu. Tiếp theo, GPT-4 so sánh phản hồi tham chiếu với N phản hồi từ các khía cạnh khác nhau và gán điểm cho mỗi phản hồi. Đối với hướng dẫn mở, GPT-4 đánh giá mức độ liên quan (điểm 0-5), mức độ chi tiết/lý giải (điểm 0-5), và độ chính xác (điểm 0-5) của các phản hồi mô hình so với phản hồi tham chiếu của nó. Đối với hướng dẫn đóng, các tiêu chí đánh giá là độ chính xác (điểm 0-5), mức độ chi tiết/lý giải (điểm 0-5), và độ rõ ràng (điểm 0-5). Cuối cùng, GPT-4 xếp hạng các phản hồi theo thứ tự giảm dần dựa trên tổng điểm của chúng (xem Phụ lục E cho prompt đầy đủ của chúng tôi). Chúng tôi cũng đã đánh giá thủ công các thứ hạng GPT-4, đạt được mối tương quan mạnh với đánh giá của con người (xem Phụ lục G, H).

Như trong Phần 2.2, tập dữ liệu điều chỉnh hướng dẫn kết quả trở thành {i, r, (r[0], ...r[N-1])}. Lưu ý rằng r[k], 0≤k≤N-1, được suy ra từ chính mô hình được điều chỉnh hướng dẫn. Cuối cùng, chúng tôi sử dụng cùng hàm mục tiêu như trong Eq. 6 để khuyến khích mô hình gán xác suất cao hơn cho các phản hồi tốt hơn.

### 2.4 Tích hợp xếp hạng xác suất và xếp hạng theo ngữ cảnh

Cho một mô hình được điều chỉnh hướng dẫn, có một số tùy chọn để tinh chỉnh thêm: 1) học chỉ với xếp hạng xác suất; 2) học chỉ với xếp hạng theo ngữ cảnh; 3) học với xếp hạng xác suất tiếp theo bởi xếp hạng theo ngữ cảnh (xem Hình 1 dưới). Chúng tôi gọi các mô hình được tinh chỉnh với ba phương pháp này là Tuna_p, Tuna_c, và Tuna, tương ứng.

Để tích hợp tối ưu cả kỹ thuật xếp hạng xác suất và xếp hạng theo ngữ cảnh, được khuyến nghị đầu tiên thu được mô hình Tuna_p, tiếp theo áp dụng xếp hạng theo ngữ cảnh cho phân phối phản hồi của Tuna_p, tạo ra mô hình Tuna. Có hai lý do cho lựa chọn này. Đầu tiên, mặc dù có lợi khi học thứ hạng của các phản hồi khác nhau từ quan điểm của LLM giáo viên (xếp hạng xác suất), mô hình có thể không nắm bắt đầy đủ kiến thức xếp hạng của giáo viên do khả năng hạn chế của nó. Thứ hai, xếp hạng theo ngữ cảnh cho phép mô hình thích ứng tốt hơn với khả năng riêng của nó bằng cách làm việc với các thế hệ riêng của mô hình. Bằng cách tạo ra các phản hồi riêng của mình, mô hình có thể tinh chỉnh hiểu biết của nó với sự trợ giúp của các LLM mạnh hơn và hiệu quả hơn trong việc tạo ra các phản hồi vừa gần gũi hơn với sở thích của con người vừa tương thích với các ràng buộc khả năng của nó, giảm thiểu vấn đề thiên lệch phơi bày (Ranzato et al., 2015).

## 3 Thí nghiệm

### 3.1 Mô hình và dữ liệu

Trong các thí nghiệm của chúng tôi, chúng tôi sử dụng mô hình LLaMA 7B (Touvron et al., 2023) làm mô hình cơ sở. Dữ liệu điều chỉnh hướng dẫn được lấy từ Alpaca (Taori et al., 2023), bao gồm 52K hướng dẫn được ghép nối với các phản hồi được tạo bởi text-davinci-003 sử dụng thuật toán Self-Instruct (Wang et al., 2022a). Chúng tôi thực hiện điều chỉnh hướng dẫn trên 52K dữ liệu Alpaca sử dụng các siêu tham số được khuyến nghị, như tốc độ học 2e-5 và bộ tối ưu AdamW (0.9, 0.999) (Loshchilov và Hutter, 2019). Để đơn giản, chúng tôi cũng gọi mô hình được điều chỉnh hướng dẫn là Alpaca.

Đối với xếp hạng xác suất, chúng tôi đưa 52K hướng dẫn từ tập dữ liệu Alpaca vào text-davinci-003 để tạo ra N = 4 phản hồi cho mỗi hướng dẫn cùng với log-likelihood của chúng, với nhiệt độ suy luận là 1. Chúng tôi tính điểm phản hồi sử dụng Eq. 3 với β là 1.3, và xếp hạng các phản hồi tương ứng. Sau đó, chúng tôi tinh chỉnh mô hình Alpaca trong 1 epoch với tốc độ học 1e-5, biên m = 0.1, và trọng số điều hòa entropy chéo λ = 1.0. Chúng tôi ký hiệu mô hình được huấn luyện độc quyền với xếp hạng xác suất là Tuna_p.

Đối với xếp hạng theo ngữ cảnh, chúng tôi lấy mẫu N = 4 phản hồi từ mô hình Alpaca với nhiệt độ T = 1 cho mỗi hướng dẫn. Để tránh các thế hệ tương tự, chúng tôi đảm bảo ROUGE-L theo cặp (Lin, 2004) giữa các phản hồi nhỏ hơn τ = 0.8. Nếu không, chúng tôi loại bỏ phản hồi tương tự, tăng nhiệt độ lên 0.1, và lấy mẫu lại. Nếu ba lần thử không tạo ra các phản hồi đủ độc đáo, chúng tôi giữ lại cái ít tương tự nhất. Sau đó chúng tôi sử dụng GPT-4 để xếp hạng các phản hồi cho 13K dữ liệu hướng dẫn đầu tiên với nhiệt độ suy luận GPT-4 là 0. Prompt xếp hạng theo ngữ cảnh được hiển thị trong Bảng 9. Các siêu tham số tinh chỉnh tuân theo những của xếp hạng xác suất. Chúng tôi gọi mô hình được huấn luyện trên 13K dữ liệu xếp hạng theo ngữ cảnh của mô hình Alpaca là Tuna_c.

Hơn nữa, chúng tôi sử dụng 13K dữ liệu xếp hạng GPT-4 để huấn luyện một mô hình xếp hạng proxy (PRM) dựa trên StableLM-3B. PRM được sử dụng để xếp hạng lại các phản hồi của Alpaca trên 52K hướng dẫn. Chúng tôi gọi mô hình Alpaca được huấn luyện với 52K dữ liệu xếp hạng hoàn toàn được tạo bởi PRM là Tuna_c(PRM).

Cuối cùng, chúng tôi cũng thu thập 13K dữ liệu xếp hạng theo ngữ cảnh GPT-4 dựa trên các phản hồi của Tuna_p thay vì của Alpaca. Chúng tôi gọi mô hình được tinh chỉnh trên Tuna_p là Tuna.

Chúng tôi cũng bao gồm các đường cơ sở học tăng cường mạnh để so sánh (tức là các mô hình PPO-sim và PPO-sim-GPT4-20K từ AlpacaFarm (Dubois et al., 2023)).

### 3.2 Đánh giá

**Super Natural Instruction (Super NI)** Super NI (Wang et al., 2022b) chứa 119 nhiệm vụ kiểm tra được thiết kế để đánh giá khả năng tổng quát hóa liên nhiệm vụ của mô hình. Nó bao gồm nhiều nhiệm vụ phân loại và tạo sinh, như suy luận văn bản và tạo tiêu đề. Chúng tôi báo cáo cả hiệu suất 0-shot và 2-shot, trong đó 0-shot chỉ cung cấp một hướng dẫn (được gọi là "định nghĩa" trong tài liệu của họ) và 2-shot cung cấp hai ví dụ tích cực bổ sung. Thước đo đánh giá cho tất cả 119 nhiệm vụ là ROUGE-L (Lin, 2004), có mối tương quan mạnh với đánh giá của con người với hệ số Pearson là 0.998 theo Wang et al. (2022b). Giải mã tham lam được áp dụng trong quá trình suy luận.

**LMentry** LMentry (Efrat et al., 2022) là một điểm chuẩn chủ yếu tập trung vào các khía cạnh độ chính xác và tính mạnh mẽ của các thế hệ LLM. Nó chứa 25 nhiệm vụ ngắn tầm thường đối với con người nhưng thách thức đối với LLM. Thước đo cuối cùng là điểm LMentry, được tính bằng cách nhân độ chính xác trung bình của nó trên 25 nhiệm vụ với điểm tính mạnh mẽ. Mô hình sẽ được đánh giá theo cách 0-shot, và giải mã tham lam được áp dụng trong quá trình suy luận.

**Vicuna QA** Vicuna QA (Chiang et al., 2023) bao gồm 80 câu hỏi kiểm tra qua 9 danh mục đo lường khả năng tạo ra các phản hồi liên quan, chi tiết và chính xác của LLM và nó đã được áp dụng rộng rãi trong nhiều công trình. Thay vì có cơ sở thật để đánh giá, nó thực hiện so sánh theo cặp với sự trợ giúp của GPT-4 (OpenAI, 2023). Nó nhắc GPT-4 so sánh đầu ra của các mô hình của chúng tôi với mô hình Alpaca. Chúng tôi báo cáo tỷ lệ thắng/thua/hòa so với mô hình Alpaca.

**Đánh giá con người** Ngoài ra, chúng tôi thực hiện đánh giá con người trên Vicuna QA. Cụ thể, các phản hồi từ năm hệ thống ẩn danh, cụ thể là Alpaca, Alpaca + PPO-sim, Tuna, Tuna_p, và Tuna_c, được xáo trộn ngẫu nhiên và trình bày cho các chú thích viên, sau đó được yêu cầu xếp hạng các đầu ra này. Việc chấm điểm được thiết kế sao cho hệ thống xếp hạng thứ i nhận được điểm 6-i, có nghĩa là hệ thống xếp hạng tốt nhất nhận được điểm 5, và hệ thống xếp hạng tệ nhất nhận được điểm 1. Mỗi câu hỏi được chú thích bởi hai chú thích viên khác nhau, và điểm được tính trung bình.

### 3.3 Kết quả chính

Các kết quả chính được trình bày trong Bảng 1. Sau điều chỉnh hướng dẫn, Alpaca cho thấy cải thiện hiệu suất đáng kể so với LLaMA trên cả ba điểm chuẩn. Điều này làm nổi bật sự chuyển đổi thành công từ mô hình "dự đoán token tiếp theo" sang mô hình tương tác theo hướng dẫn hơn.

Hơn nữa, cả xếp hạng theo ngữ cảnh và xếp hạng xác suất đều nâng cao hiệu suất trên cả ba điểm chuẩn. Cụ thể, Tuna_c thể hiện cải thiện nhiều hơn trên kết quả Super NI 2-shot trong khi Tuna_p hoạt động tốt hơn trên Super NI 0-shot và LMentry, thu hẹp khoảng cách hiệu suất với các mô hình lớn hơn nhiều như InstructGPT-175B. Vì đầu vào 2-shot dài hơn 0-shot, chúng tôi phỏng đoán rằng xếp hạng theo ngữ cảnh có thể có lợi hơn cho việc tạo chuỗi dài hơn so với xếp hạng xác suất. Trên điểm chuẩn Vicuna QA, cả Tuna_p và Tuna_c đều vượt trội so với Alpaca đáng kể trên gần 70% câu hỏi, được đánh giá bởi GPT-4. Khi so sánh với các đường cơ sở RLHF, Tuna_p và Tuna_c liên tục cho thấy hiệu suất ưu việt trên cả điểm chuẩn Super NI và LMentry. Tuy nhiên, khi nói đến điểm chuẩn Vicuna QA, hiệu suất của chúng thấp hơn một chút so với các đường cơ sở RLHF.

Hơn nữa, Tuna đạt được hiệu suất tốt nhất trên Vicuna QA trong khi duy trì điểm cạnh tranh trên Super-NI và LMentry. Kết quả con người trên Vicuna QA (xem Bảng 2) cũng xác nhận rằng con người ưa thích các phản hồi từ các mô hình của chúng tôi.

Hơn nữa, Tuna_c(PRM) cho thấy hiệu suất tương đương với Tuna_c trên Vicuna QA và LMentry, nhưng nó hoạt động kém hơn cả Tuna_c và Alpaca trên Super NI. Điều này cho thấy rằng mặc dù PRM chủ yếu đã học được xếp hạng từ dữ liệu xếp hạng theo ngữ cảnh GPT-4, nó cũng đưa vào một số nhiễu trong quá trình học. Nhìn chung, việc học trực tiếp từ dữ liệu xếp hạng theo ngữ cảnh GPT-4 hiệu quả hơn.

### 3.4 Nghiên cứu loại bỏ

Trong phần phụ này, chúng tôi tìm hiểu sâu hơn về hiệu suất của phương pháp của chúng tôi bằng cách xem xét một số khía cạnh, bao gồm: (a) tác động của nhiều phản hồi hơn trong điều chỉnh hướng dẫn, (b) thứ tự áp dụng hai phương pháp xếp hạng, (c) ảnh hưởng của điều hòa entropy chéo, (d) lượng dữ liệu xếp hạng xác suất, và (e) rủi ro của đánh giá GPT-4.

**Nhiều phản hồi hơn trong điều chỉnh hướng dẫn** Chúng tôi khám phá xem hiệu quả của Tuna có chỉ do dữ liệu phản hồi tăng không bằng cách xem xét tác động của việc thêm nhiều phản hồi hơn cho mỗi hướng dẫn trong quá trình điều chỉnh hướng dẫn. Chúng tôi tạo một mô hình mới, Alpaca-Mul, bằng cách thêm bốn phản hồi bổ sung từ tập dữ liệu xếp hạng xác suất vào tập dữ liệu Alpaca và tinh chỉnh mô hình LLaMA sử dụng Eq. 2. Kết quả được trình bày trong Bảng 3.

Khi đánh giá trên Super NI, hiệu suất của Alpaca-Mul gần như giống hệt với Alpaca nhưng thua kém so với cài đặt 0-shot của Tuna_p và Tuna. Trên LMentry, Alpaca-Mul vượt trội so với Alpaca, nhưng vẫn không đạt được mức hiệu suất của Tuna_p và Tuna. Thú vị là, trong nhiệm vụ Vicuna QA, Alpaca-Mul hoạt động kém hơn một chút so với Alpaca.

Những phát hiện này cho thấy rằng chỉ việc thêm nhiều phản hồi hơn mà không phân biệt chúng không nhất thiết dẫn đến cải thiện tạo phản hồi. Nhìn chung, kết quả của Alpaca-Mul chỉ ra rằng hiệu suất ưu việt của Tuna không thể được quy cho hoàn toàn việc có sẵn nhiều dữ liệu phản hồi hơn.

**Thứ tự tích hợp** Một phương pháp thay thế cho Tuna bao gồm việc đầu tiên huấn luyện mô hình Tuna_c, và sau đó tiếp tục huấn luyện mô hình Tuna_c với dữ liệu xếp hạng xác suất. Mô hình kết quả được gọi là Tuna_cp.

Chúng tôi khám phá các chiến lược khác nhau để huấn luyện Tuna_cp: 1) tinh chỉnh Tuna_c với 13K dữ liệu xếp hạng xác suất đầu tiên (Tuna_cp-13K); 2) tinh chỉnh mô hình Tuna_c với 39K dữ liệu xếp hạng xác suất cuối (Tuna_cp-39K); 3) tinh chỉnh mô hình Tuna_c với 52K dữ liệu xếp hạng xác suất (Tuna_cp-52K). Ngoài ra, chúng tôi cũng thử tinh chỉnh mô hình Alpaca gốc với sự kết hợp của 13K dữ liệu xếp hạng theo ngữ cảnh GPT-4 (được tạo từ các phản hồi của mô hình Alpaca) và 39K dữ liệu xếp hạng xác suất cuối (mix-Tuna-52K). Chúng tôi cũng tinh chỉnh mô hình Alpaca với 52K dữ liệu xếp hạng theo ngữ cảnh (13K xếp hạng theo ngữ cảnh GPT-4 + 39K dữ liệu được tạo bởi mô hình xếp hạng) cộng 52K dữ liệu xếp hạng xác suất (mix-Tuna-104K). Chi tiết huấn luyện được liệt kê trong Phụ lục C. Kết quả được liệt kê trong Bảng 3.

Không có chiến lược kết hợp nào liên tục vượt trội so với cả Tuna_p và Tuna_c trên các điểm chuẩn Vicuna QA và Super NI. Trên LMentry, tuy nhiên, việc tinh chỉnh Tuna_c với dữ liệu xếp hạng xác suất có lợi, đặc biệt khi không có dữ liệu trùng lặp (Tuna_cp-39K). Điều này cho thấy dữ liệu xếp hạng xác suất ngắn hơn có lợi khi độ chính xác và tính mạnh mẽ cao là ưu tiên hàng đầu.

Thú vị là, Tuna_cp không sánh được với Tuna, cho thấy rằng thứ tự mà mô hình được huấn luyện với xếp hạng theo ngữ cảnh và xếp hạng xác suất quan trọng. Một giải thích có thể là cả dữ liệu Alpaca gốc và dữ liệu xếp hạng xác suất đều được tạo bởi text-davinci-003, trong khi Tuna_c đã thay đổi đáng kể phân phối mô hình bằng cách xếp hạng lại các phản hồi của mô hình Alpaca, khiến việc tinh chỉnh Tuna_c với dữ liệu xếp hạng xác suất lại trở nên thách thức.

**Tác động của điều hòa entropy chéo** Chúng tôi xem xét ảnh hưởng của trọng số λ của điều hòa entropy chéo trong Eq. 6 lên hiệu suất bằng cách thay đổi λ qua các giá trị khác nhau: {0, 0.1, 1, 5, 10} trong khi huấn luyện mô hình Tuna_c. Hình 2 minh họa rằng khi λ tăng, hiệu suất trên các điểm chuẩn định hướng độ chính xác như Super NI và LMentry cải thiện, trong khi hiệu suất trên câu hỏi mở không nhất thiết theo cùng xu hướng. Một mặt, phát hiện này cho thấy với λ nhỏ, việc học với xếp hạng theo ngữ cảnh có thể tạo ra các câu trả lời dài và chi tiết, nhưng những câu trả lời đó không phải lúc nào cũng chính xác. Mặt khác, nó ngụ ý rằng các điểm chuẩn định hướng độ chính xác và các điểm chuẩn QA mở bổ sung cho nhau, và các nhà nghiên cứu nên xem xét các trường hợp kiểm tra đa dạng hơn để đánh giá mô hình một cách kỹ lưỡng (Wang et al., 2023b).

**Lượng dữ liệu xếp hạng xác suất** Chúng tôi điều tra tác động của việc thay đổi lượng dữ liệu xếp hạng xác suất được sử dụng để tinh chỉnh mô hình Tuna_p bằng cách kiểm tra các kích thước dữ liệu khác nhau, tức là {0, 13000, 24000, 52000}. 0 đề cập đến mô hình Alpaca. Kết quả, được hiển thị trong Hình 3, cho thấy đối với xếp hạng xác suất, 13K điểm dữ liệu đủ cho Super NI và LMentry, trong khi Vicuna QA yêu cầu 24K điểm dữ liệu. Chúng tôi phỏng đoán rằng hiện tượng bão hòa này có thể được quy cho hai lý do. Đầu tiên, 52K hướng dẫn Alpaca được tạo bởi thuật toán Self-Instruct không đủ đa dạng, vì các hướng dẫn mới được tạo bởi text-davinci-003 sử dụng hướng dẫn prompt được lấy mẫu từ một nhóm nhiệm vụ mầm hạn chế. Thứ hai, bản thân điều chỉnh hướng dẫn có thể chỉ yêu cầu một lượng dữ liệu hạn chế để thực hiện sao chép hành vi, như đã thảo luận trong Zhou et al. (2023). Do đó, chúng ta có thể giảm thêm chi phí tạo dữ liệu xếp hạng xác suất một nửa.

**Rủi ro trong đánh giá GPT-4** Chúng tôi trình bày bằng chứng rằng việc đánh giá mô hình trên QA mở với sự trợ giúp của GPT-4 có thể rủi ro. Bảng 4 hiển thị độ dài xếp hạng của mô hình xếp hạng proxy (PRM) của chúng tôi. Nó cho thấy rằng PRM đã kế thừa thiên lệch của xếp hạng GPT-4 đối với đầu ra dài hơn (Li et al., 2023).

Tuy nhiên, như chúng tôi đã thảo luận trong Phần 3.3, dữ liệu được tạo bởi PRM không tốt bằng 13K dữ liệu xếp hạng theo ngữ cảnh gốc, được đánh giá bởi các đánh giá tự động có mục tiêu hơn như Super NI và LMentry. Mặc dù chất lượng kém hơn của dữ liệu được tạo bởi PRM, hiệu suất trên Vicuna QA vẫn gần như không bị ảnh hưởng (xem Tuna_c(PRM) trong Bảng 1). Quan sát này cho thấy rằng việc đánh giá LLM trên QA mở với GPT-4 có thể không phải lúc nào cũng chính xác như nó xuất hiện, lặp lại những phát hiện của Wang et al. (2023b). Nó làm nổi bật nhu cầu về các câu hỏi kiểm tra đại diện hơn hoặc các điểm chuẩn có mục tiêu bổ sung để đánh giá.

## 4 Công trình liên quan

**Điều chỉnh hướng dẫn** Điều chỉnh hướng dẫn nhằm cải thiện khả năng sử dụng của các mô hình ngôn ngữ cơ sở (Brown et al., 2020; Raffel et al., 2020; Chowdhery et al., 2022) bằng cách tinh chỉnh chúng trên các cặp hướng dẫn-phản hồi theo cách zero-shot (Wei et al., 2022) hoặc few-shot (Mishra et al., 2021; Wang et al., 2022b; Mallen et al., 2023). Dữ liệu hướng dẫn có thể được lấy từ các điểm chuẩn NLP có sẵn (Mishra et al., 2021; Wei et al., 2022; Wang et al., 2022b) hoặc được tạo bởi LLM (Wang et al., 2022a; Honovich et al., 2022; Taori et al., 2023; Peng et al., 2023).

**Tổn thất xếp hạng** Việc học thông qua xếp hạng lại các đầu ra cấp độ chuỗi đã được nghiên cứu trong các mô hình sequence-to-sequence (Wiseman và Rush, 2016; Edunov et al., 2018; Liu et al., 2022; Zhang et al., 2022). Các thuật toán BRIO và MoCa (Liu et al., 2022; Zhang et al., 2022) áp dụng tổn thất xếp hạng theo cặp để hướng dẫn mô hình tạo ra các tóm tắt với điểm ROUGE cao hơn (Lin, 2004). Trong bài báo này, chúng tôi sử dụng khả năng hiểu ngữ cảnh mạnh của GPT-4 (OpenAI, 2023) và các biện pháp xác suất nội tại của text-davinci-003 (Ouyang et al., 2022) để xếp hạng. Song song với công trình của chúng tôi, Yuan et al. (2023) cũng đề xuất tổn thất xếp hạng theo cặp để tinh chỉnh LLM. Các khác biệt chính bao gồm: 1) chiến lược tinh chỉnh pipeline của chúng tôi; 2) tập trung của chúng tôi vào việc xếp hạng các phản hồi của mô hình; 3) việc sử dụng phản hồi gốc của chúng tôi để điều hòa entropy chéo, trong khi họ chọn phản hồi có phần thưởng cao nhất. Ngoài ra, Liu et al. (2023c) cũng sử dụng các mô hình GPT để tinh chỉnh BART (Lewis et al., 2019) trên nhiệm vụ tóm tắt.

**Đánh giá mô hình được tiền huấn luyện** Các mô hình được tiền huấn luyện lớn là các thước đo đánh giá mạnh mẽ do khả năng hiểu ngữ cảnh mạnh của chúng, như BERTScore (Zhang* et al., 2020), BARTScore (Yuan et al., 2021), MoverScore (Zhao et al., 2019), COMET (Rei et al., 2020), và GPTScore (Fu et al., 2023). Gần đây hơn, có nhiều chiến lược đánh giá hơn dựa trên GPT-3.5 và GPT-4 (Liu et al., 2023b; Gao et al., 2023).

## 5 Kết luận

Trong bài báo này, chúng tôi đề xuất tinh chỉnh một LLM được điều chỉnh hướng dẫn bằng cách sử dụng phương pháp xếp hạng xác suất (Tuna_p), phương pháp xếp hạng theo ngữ cảnh (Tuna_c), và sự kết hợp của cả hai (Tuna) của chúng tôi. Các thí nghiệm toàn diện của chúng tôi cho thấy cải thiện hiệu suất nhất quán trên ba điểm chuẩn: Super Natural Instructions (119 nhiệm vụ kiểm tra), LMentry (25 nhiệm vụ kiểm tra), và vicuna QA. Hơn nữa, các phương pháp của chúng tôi vượt trội so với các đường cơ sở học tăng cường từ phản hồi của con người phổ biến dựa vào thuật toán tối ưu hóa chính sách gần đúng. Những phát hiện này nhấn mạnh hiệu quả của phương pháp của chúng tôi trong việc nâng cao hiệu suất của các LLM được điều chỉnh hướng dẫn và mở đường cho nghiên cứu tương lai trong lĩnh vực này.

## Hạn chế

Mặc dù có những kết quả đầy hứa hẹn được đạt bởi mô hình Tuna của chúng tôi, có một số hạn chế cần được thừa nhận. Hạn chế đầu tiên là sự không nhất quán trong xếp hạng GPT-4. Trong các thí nghiệm của chúng tôi, chúng tôi dựa vào GPT-4 cho xếp hạng theo ngữ cảnh, có thể đưa vào thiên lệch do sự không nhất quán trong hiệu suất xếp hạng của nó. Là một LLM mạnh mẽ, GPT-4 thường được mong đợi cung cấp các thứ hạng chính xác và đáng tin cậy; tuy nhiên, nó vẫn có thể nhạy cảm với cách diễn đạt hoặc cấu trúc của prompt (Dubois et al., 2023). Sự không nhất quán này có thể dẫn đến các thứ hạng không tối ưu và có khả năng ảnh hưởng đến hiệu suất tổng thể của mô hình Tuna. Trong công trình tương lai, sẽ có lợi khi thiết kế các prompt mạnh mẽ hơn có thể giảm thiểu tác động của sự không nhất quán trong xếp hạng của GPT-4. Một hạn chế khác là điểm chuẩn đánh giá. Trong bài báo này, chúng tôi đánh giá mô hình Tuna trên ba điểm chuẩn, cung cấp một phạm vi đa dạng các nhiệm vụ và thách thức. Tuy nhiên, không rõ mô hình Tuna sẽ tổng quát hóa tốt như thế nào cho các loại nhiệm vụ, lĩnh vực, hoặc ngôn ngữ khác. Nghiên cứu thêm cần thiết để khám phá khả năng áp dụng của mô hình Tuna cho một phạm vi rộng hơn các vấn đề và cài đặt. Hạn chế cuối cùng là sự phụ thuộc vào việc sử dụng các LLM độc quyền, như GPT-4 và text-davinci-003, để tạo phản hồi và xếp hạng. Sự phụ thuộc này có thể hạn chế khả năng truy cập và tái tạo phương pháp của chúng tôi đối với các nhà nghiên cứu không có quyền truy cập vào các mô hình độc quyền này. Phát triển các phương pháp thay thế có thể tận dụng các LLM mã nguồn mở hoặc các cơ chế xếp hạng khác sẽ là một hướng nghiên cứu có giá trị trong tương lai.

## Lời cảm ơn

Chúng tôi muốn cảm ơn các nhà đánh giá về phản hồi có giá trị của họ. Nghiên cứu/dự án này được hỗ trợ bởi Bộ Giáo dục, Singapore, trong Chương trình Cấp 3 (Số giải thưởng: MOET320200004), Quỹ Nghiên cứu Quốc gia Singapore và Phòng thí nghiệm Quốc gia DSO trong Chương trình AI Singapore (Số giải thưởng AISG: AISG2-RP-2020-016), và Bộ Giáo dục, Singapore, trong Chương trình Quỹ Nghiên cứu Học thuật (AcRF) Cấp 2 (Số giải thưởng MOE AcRF Cấp 2: MOE-T2EP20122-0011). Bất kỳ ý kiến, phát hiện và kết luận hoặc khuyến nghị nào được thể hiện trong tài liệu này là của các tác giả và không phản ánh quan điểm của Bộ Giáo dục, Singapore.
