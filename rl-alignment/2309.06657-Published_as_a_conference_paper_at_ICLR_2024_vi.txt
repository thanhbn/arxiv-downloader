TÀI LIỆU THAM KHẢO

Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Báo cáo kỹ thuật palm 2. arXiv preprint arXiv:2305.10403, 2023.

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Huấn luyện một trợ lý hữu ích và vô hại với học tăng cường từ phản hồi của con người. arXiv preprint arXiv:2204.05862, 2022.

Ralph Allan Bradley và Milton E Terry. Phân tích xếp hạng của thiết kế khối không đầy đủ: I. phương pháp so sánh theo cặp. Biometrika, 39(3/4):324-345, 1952.

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Các mô hình ngôn ngữ là những người học few-shot. Advances in neural information processing systems, 33:1877-1901, 2020.

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Mở rộng các mô hình ngôn ngữ được tinh chỉnh theo hướng dẫn. arXiv preprint arXiv:2210.11416, 2022.

Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, và Tong Zhang. Raft: Tinh chỉnh được xếp hạng phần thưởng cho căn chỉnh mô hình nền tảng sinh tạo. arXiv preprint arXiv:2304.06767, 2023.

Leo Gao, John Schulman, và Jacob Hilton. Định luật mở rộng cho tối ưu hóa quá mức mô hình phần thưởng. Trong International Conference on Machine Learning, pp. 10835-10866. PMLR, 2023.

Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Tự huấn luyện được tăng cường (rest) cho mô hình hóa ngôn ngữ. arXiv preprint arXiv:2308.08998, 2023.

Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, và Phil Blunsom. Dạy máy đọc và hiểu. Advances in neural information processing systems, 28, 2015.

Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, và Abhinav Rastogi. Rlaif: Mở rộng học tăng cường từ phản hồi của con người với phản hồi AI. arXiv preprint arXiv:2309.00267, 2023.

Radford M Neal. Lấy mẫu slice. The annals of statistics, 31(3):705-767, 2003.

R OpenAI. Báo cáo kỹ thuật gpt-4. arXiv, pp. 2303-08774, 2023.

Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, và Jeff Dean. Mở rộng suy luận transformer một cách hiệu quả. Proceedings of Machine Learning and Systems, 5, 2023.

Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, và Chelsea Finn. Tối ưu hóa sở thích trực tiếp: Mô hình ngôn ngữ của bạn bí mật là một mô hình phần thưởng. arXiv preprint arXiv:2305.18290, 2023.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, và Peter J Liu. Khám phá giới hạn của học chuyển giao với một transformer văn bản-sang-văn bản thống nhất. The Journal of Machine Learning Research, 21(1):5485-5551, 2020.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, và Oleg Klimov. Thuật toán tối ưu hóa chính sách gần đúng. arXiv preprint arXiv:1707.06347, 2017.

Noam Shazeer và Mitchell Stern. Adafactor: Tỷ lệ học thích ứng với chi phí bộ nhớ dưới tuyến tính. Trong International Conference on Machine Learning, pp. 4596-4604. PMLR, 2018.

Lei Shu, Liangchen Luo, Jayakumar Hoskere, Yun Zhu, Canoee Liu, Simon Tong, Jindong Chen, và Lei Meng. Rewritelm: Một mô hình ngôn ngữ lớn được tinh chỉnh theo hướng dẫn cho viết lại văn bản. arXiv preprint arXiv:2305.15685, 2023.

Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, và Houfeng Wang. Tối ưu hóa xếp hạng sở thích cho căn chỉnh con người. arXiv preprint arXiv:2306.17492, 2023.

Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, và Paul F Christiano. Học tóm tắt với phản hồi của con người. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, và Thomas Scialom. Llama 2: Các mô hình chat nền tảng mở và được tinh chỉnh. arXiv preprint arXiv:2307.09288, 2023.

Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, và Zhifang Sui. Làm cho các mô hình ngôn ngữ lớn lý luận tốt hơn với sự căn chỉnh. arXiv preprint arXiv:2309.02144, 2023.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, và Quoc V Le. Các mô hình ngôn ngữ được tinh chỉnh là những người học zero-shot. arXiv preprint arXiv:2109.01652, 2021.

Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, và Fei Huang. Rrhf: Xếp hạng phản hồi để căn chỉnh mô hình ngôn ngữ với phản hồi của con người mà không có nước mắt. arXiv preprint arXiv:2304.05302, 2023.

Yao Zhao, Misha Khalman, Rishabh Joshi, Shashi Narayan, Mohammad Saleh, và Peter J Liu. Hiệu chỉnh khả năng chuỗi cải thiện sinh tạo ngôn ngữ có điều kiện. arXiv preprint arXiv:2210.00045, 2022.

Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, và Peter J Liu. Slic-hf: Hiệu chỉnh khả năng chuỗi với phản hồi của con người. arXiv preprint arXiv:2305.10425, 2023.

Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, và Yu Qiao. Vượt ra ngoài một-sở-thích-cho-tất-cả: Tối ưu hóa sở thích trực tiếp đa mục tiêu. arXiv preprint arXiv:2310.03708, 2023.

Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, và Geoffrey Irving. Tinh chỉnh mô hình ngôn ngữ từ sở thích của con người. arXiv preprint arXiv:1909.08593, 2019.

A PHỤ LỤC

A.1 THUẬT TOÁN LẤY MẪU LOẠI BỎ THỐNG KÊ

Triển Khai Python Một triển khai Python của thuật toán được hiển thị trong Thuật toán 1.

--- TRANG 11 ---
Xuất bản như một bài báo hội thảo tại ICLR 2024

Thuật toán 1 Thuật Toán Lấy Mẫu Loại Bỏ Thống Kê trong Python
```python
from typing import List
import numpy as np

def conduct_rejection_sampling(response_candidates: List[str],
                               response_rewards: List[float],
                               num_samples: int,
                               beta: float):
    """Tiến hành lấy mẫu loại bỏ được hướng dẫn bởi phần thưởng.
    
    Args:
        response_candidates: ứng viên phản hồi từ chính sách SFT
        response_rewards: phần thưởng phản hồi.
        num_samples: số lượng mẫu để lấy mẫu phụ.
        beta: tham số beta trong mục tiêu tối đa hóa phần thưởng bị ràng buộc KL.
    
    Returns:
        Các chuỗi được lấy mẫu loại bỏ từ chính sách tối ưu được ước lượng.
    """
    candidates = {c: r for c, r in zip(response_candidates, response_rewards)}
    accepted = []
    
    while len(accepted) < num_samples:
        max_reward = max(candidates.values())
        to_remove = []
        
        for c, r in candidates.items():
            u = np.random.uniform()
            if u >= np.exp((r - max_reward) / beta):
                continue
            accepted.append(c)
            to_remove.append(c)
            if len(accepted) == num_samples:
                break
        
        for c in to_remove:
            candidates.pop(c)
    
    return accepted
```

Dẫn Xuất của Thuật toán 1 Theo Phương trình (4), chúng ta có

pirpsi(y|x) = (1/Zpsi(x)) * pisft(y|x) * exp((1/beta) * rpsi(x, y))     (11)

trong đó Zpsi(x) = Σy pisft(y|x) exp((1/beta) * rpsi(x, y)). Sau đó chúng ta có

pirpsi(y|x)/pisft(y|x) = (1/Zpsi(x)) * exp((1/beta) * rpsi(x, y))      (12)

Rõ ràng rằng MDx ≜ min{m|m·pisft(y|x) >= pirpsi(y|x) cho tất cả y không trong Dx} = maxy không trong Dx pirpsi(y|x)/pisft(y|x), thì

MDx = (1/Zpsi(x)) * max(y không trong Dx) exp((1/beta) * rpsi(x, y))    (13)

Sau đó chúng ta có

pirpsi(y|x)/(MDx * pisft(y|x)) = exp((1/beta) * (rpsi(x, y) - max(y không trong Dx) rpsi(x, y)))  (14)

Bằng cách sử dụng phiên bản mẫu của max(y không trong Dx) rpsi(x, y), chúng ta đã dẫn xuất Thuật toán 1.

A.2 CHỨNG MINH ĐỊNH LÝ 1

Chứng minh. Cho quá trình sinh tạo là cái được mô tả trong Thuật toán 1 và tập chuỗi được chấp nhận là Dx ở bước hiện tại, chúng ta có

P(lấy mẫu y và được chấp nhận |x) = P(u < pirpsi(y|x)/(MDx * pisft(y|x))) * pisft(y|x)
                                   = (1/MDx) * pirpsi(y|x)                    (15)

trong đó MDx ≜ min{m|m·pisft(y|x) >= pirpsi(y|x) cho tất cả y không trong Dx}.

--- TRANG 12 ---
Xuất bản như một bài báo hội thảo tại ICLR 2024

P(y được chấp nhận |x) = P(u < pirpsi(y|x)/(MDx * pisft(y|x)))
                        = E[1(u < pirpsi(y|x)/(MDx * pisft(y|x)))]
                        = E_pisft[E[1(u < pirpsi(y|x)/(MDx * pisft(y|x)))|y]]
                        = E_pisft[pirpsi(y|x)/(MDx * pisft(y|x))]
                        = 1/MDx                                                (16)

P(y|y được chấp nhận, x) = P(lấy mẫu y và được chấp nhận |x) / P(y được chấp nhận |x) = pirpsi(y|x)   (17)

Bởi Phương trình (13), chúng ta có tỷ lệ chấp nhận

1/MDx = E_y~pisft(y|x)[exp((1/beta) · (rpsi(x, y) - max(y không trong Dx) rpsi(x, y)))]    (18)

A.3 VÍ DỤ ĐỊNH TÍNH CỦA RSO SO SÁNH VỚI CÁC PHƯƠNG PHÁP KHÁC

Các so sánh định tính giữa RSO và các phương pháp khác được hiển thị trong Hình 4 và Hình 5 cho Reddit TL;DR và AnthropicHH, tương ứng.

Hình 4: Ví dụ tóm tắt được tạo ra bởi các chính sách SFT, SLiC, DPO, và RSO cho một bài đăng Reddit. RSO tạo ra tóm tắt tốt nhất trong bốn cái vì nó tóm tắt thông tin chính một cách ngắn gọn và chính xác trong bài đăng diễn đàn. Các chi tiết nổi bật được in đậm.

--- TRANG 13 ---
Xuất bản như một bài báo hội thảo tại ICLR 2024

Hình 5: Ví dụ phản hồi được tạo ra bởi các chính sách SFT, SLiC, DPO, và RSO cho một cuộc đối thoại Người-Trợ lý trên tập dữ liệu AnthropicHH. RSO tạo ra phản hồi hữu ích nhất trong bốn cái vì nó đưa ra một câu trả lời rõ ràng và đơn giản để gửi thư nhanh chóng qua thư truyền thống. Ngược lại, SFT lặp lại thông tin về email thay vì trả lời câu hỏi về thư truyền thống. SLiC và DPO mơ hồ và lặp lại. Các chi tiết nổi bật được in đậm.

A.4 CHI TIẾT PALM 2-L VÀ MẪU SXS FEW-SHOT

A.4.1 CHI TIẾT

Mục đích của AutoSxS là ngăn chặn điểm phần thưởng cao một cách giả tạo bởi Mô Hình Phần Thưởng do hack phần thưởng trên các chính sách đã học. Vì chính sách được huấn luyện bằng cách sử dụng thông tin trong mô hình phần thưởng-xếp hạng theo cặp, không nhất thiết tỷ lệ thắng cao hơn trên mô hình phần thưởng-xếp hạng thì chính sách càng tốt. AutoSxS sử dụng học trong ngữ cảnh few-shot PaLM 2-L để suy luận 8 mẫu được giải mã với 4 thứ tự lật của phản hồi A và B. Nhãn chứa ba lựa chọn: A, B, và hòa với điểm 1, 0, và 0.5, tương ứng. Để đảm bảo tính mạnh mẽ, chúng tôi sử dụng điểm trung bình để xác định thắng hoặc thua nếu độ lớn vượt quá 0.35. AutoSxS đã được chứng minh là hiệu quả và nhất quán trong DPO sử dụng GPT-4 như người đánh giá zero-shot. Trong công việc này, chúng tôi thay thế GPT-4 bằng PaLM 2-L để đánh giá của chúng tôi sử dụng các prompt few-shot. Chất lượng của PaLM 2-L trên các tác vụ tương tự đã được chỉ ra là gần với người đánh giá con người (Lee et al., 2023; Shu et al., 2023). Nghiên cứu hệ thống về tính nhất quán và chất lượng của AutoSxS nằm ngoài phạm vi của công việc này.

A.4.2 PROMPT FEW-SHOT REDDIT TL;DR

tác vụ: Đánh giá chất lượng của hai TLDR, chọn các tùy chọn trong số (A), (B) hoặc giống nhau.

ngữ cảnh: Tôi (M[21]) đã có mối quan hệ trong một năm rưỡi với F[22] và nó thực sự chưa bao giờ diễn ra tốt. Tôi nghĩ chúng tôi muốn những thứ khác nhau và chúng tôi không quá tương thích. Tôi đã chia tay cô ấy khoảng một năm trước và cô ấy đã cố gắng tự sát nên chúng tôi quay lại với nhau. Tuần này tôi gặp một F[19] mà tôi nghĩ tôi thực sự tương thích. Cô ấy và tôi đã nói chuyện trong vài giờ và chúng tôi có nhiều điểm chung. Tôi thích cô ấy rất nhiều, nhưng cô ấy hiện tại là sinh viên năm nhất và tôi hiện tại là sinh viên năm cuối nên tôi sẽ tốt nghiệp vào tháng 5 và tiếp tục một chương trình PhD danh tiếng bắt đầu mùa thu tới.

Vậy đây là những câu hỏi của tôi: * Tôi nên làm gì liên quan đến mối quan hệ hiện tại của mình? Tôi biết tôi cần kết thúc nó, nhưng tôi chỉ không biết như thế nào. * Tôi nên làm gì liên quan đến cô gái khác? * Bạn có nghĩ cảm xúc của tôi đối với cô gái khác xuất phát từ sự không thích mối quan hệ hiện tại của tôi không?

Tôi đánh giá cao bất kỳ sự giúp đỡ nào bạn cho tôi.

tldr (A): Tôi không hạnh phúc trong mối quan hệ hiện tại của mình với một cô gái tôi vừa gặp, nhưng không biết cách kết thúc nó. Tôi không biết tôi đang làm gì hoặc phải làm gì.

tldr (B): M[21] không hạnh phúc trong mối quan hệ với F[22]. Gặp một F[19] trong thị trấn với sở thích tương tự và tôi thực sự thích cô ấy. Tôi nên làm gì liên quan đến mối quan hệ hiện tại/cô gái khác?

giải thích: câu thứ hai và thứ ba của tldr (A) truyền đạt ý tưởng tương tự và dư thừa. tldr (B) đề cập đến một phần thông tin quan trọng của cô gái mới, chứa nhiều chi tiết hơn tldr (A) và đồng thời ngắn gọn.

chọn trong số (A), (B) hoặc giống nhau: (B)

[Tiếp tục với các ví dụ few-shot khác...]

A.4.3 PROMPT FEW-SHOT ANTHROPIC HH

tác vụ: Đối với truy vấn sau đây tới một chatbot, phản hồi nào hữu ích hơn? Chọn trong số (A), (B) và giống nhau.

[Tiếp tục với các ví dụ few-shot cho AnthropicHH...]

A.5 MẪU SXS CON NGƯỜI

Mẫu đánh giá con người Reddit TL;DR và AnthropicHH được hiển thị trong Hình 6 và Hình 7, tương ứng.

A.6 CHÍNH QUY TRONG SLIC

Bảng 5 cho thấy kết quả SLiC với các trọng số chính quy khác nhau. Không có tăng mạnh bằng cách thêm mất mát chính quy. Và chúng tôi bỏ nó để căn chỉnh tốt hơn với cài đặt DPO.

A.7 THÍCH ỨNG VÀ KHÁI QUÁT HÓA CHÉO-TÁC VỤ

Tập dữ liệu CNN/DailyMail (Hermann et al., 2015) chỉ chứa dữ liệu được tinh chỉnh Dcnndm_sft với 287k/13k/11k ví dụ trong các phần train, validation và test. Chúng tôi sử dụng tập dữ liệu để kiểm tra khái quát hóa chéo-tác vụ của các phương pháp khác nhau. Chúng tôi giả định không có quyền truy cập vào bất kỳ văn bản mục tiêu hoặc sở thích nào của tập dữ liệu CNN/DailyMail trong quá trình huấn luyện. Bắt đầu từ một mô hình SFT được huấn luyện trên Reddit TL;DR Dtldr_sft, chúng tôi tối ưu hóa thêm chính sách SFT bằng cách sử dụng dữ liệu sở thích từ Reddit TL;DR Dtldr_hf. Đối với "direct", chúng tôi sử dụng sở thích trực tiếp. Đối với "sft-sample-rank" và "rso-sample-rank", chúng tôi đầu tiên khớp một mô hình phần thưởng-xếp hạng và sau đó tạo ra các cặp sở thích sử dụng các prompt từ tập huấn luyện của CNN/DailyMail. Chúng tôi đánh giá hiệu suất sử dụng văn bản mục tiêu trên phần validation của Dcnndm_sft. Từ Bảng 6, RSO cũng nhất quán cải thiện so với SLiC và DPO cho chuyển giao chéo-tác vụ.

A.8 CÁC BASELINE KHÁC

Trong Bảng 1, chúng tôi không bao gồm các baseline cho RRHF và RLHF. Đối với RRHF, chúng tôi không có quyền truy cập vào các hệ thống LLM khác và khó thiết lập một so sánh apples-to-apples. Hơn nữa, hàm mất mát của RRHF rất tương tự với SLiC. Chúng tôi tin rằng kỹ thuật lấy mẫu của chúng tôi cũng có thể cải thiện RRHF, nhưng chúng tôi để dành điều này cho nghiên cứu tương lai. Đối với RLHF, chúng tôi thiếu chuyên môn về RLHF và DPO chỉ ra nó là một lựa chọn thay thế cạnh tranh. Mục đích chính của công việc này là cải thiện DPO và SLiC với một chiến lược lấy mẫu tốt hơn.

A.9 KIỂM TRA SÂU HƠN VỀ BIAS VÀ CÔNG BẰNG TRONG CÁC MÔ HÌNH NGÔN NGỮ

Phần này đi sâu vào các khía cạnh quan trọng của bias và công bằng trong các mô hình ngôn ngữ, đặc biệt liên quan đến phương pháp được đề xuất của chúng tôi. Các công trình của Anil et al. (2023) và Touvron et al. (2023) cung cấp các đánh giá sâu sắc về bias và công bằng trong cả các mô hình ngôn ngữ được tiền huấn luyện và được căn chỉnh. Về mặt căn chỉnh với sở thích của con người, phương pháp của chúng tôi kết hợp hai tập dữ liệu học thuật: tóm tắt Reddit TL;DR (Stiennon et al., 2020) và đối thoại AnthropicHH (Bai et al., 2022). Mục tiêu chính của chúng tôi là tăng cường sự căn chỉnh với sở thích của con người, tập trung vào chất lượng tóm tắt trong Reddit TL;DR và tính hữu ích trong các đối thoại AnthropicHH.

Trong các tình huống thực tế, điểm phần thưởng thường đa chiều, và mục tiêu của việc căn chỉnh là đạt được một biên Pareto tối ưu (Bai et al., 2022). Điều này cho phép giới thiệu các mục tiêu bổ sung như vô hại, an toàn, và các cặp sở thích bias. Phương pháp của chúng tôi có thể thích ứng, hoạt động với các điểm phần thưởng trung bình có trọng số hoặc thông qua tích hợp với các hàm mất mát DPO đa mục tiêu (Zhou et al., 2023). Các nghiên cứu thực nghiệm đã chứng minh rằng phương pháp RSO của chúng tôi hiệu quả căn chỉnh với các cặp sở thích của con người.

Chúng tôi cho rằng phương pháp của chúng tôi có tiềm năng tăng cường công bằng và giảm bias trong các mô hình ngôn ngữ, với điều kiện nó được áp dụng với các cặp sở thích con người phù hợp. Tuy nhiên, quan trọng là lưu ý rằng một nghiên cứu toàn diện về công bằng và bias nằm ngoài phạm vi của công việc này.

A.10 HIỆU QUẢ TÍNH TOÁN

So với PPO (Schulman et al., 2017), RSO chỉ cần một mạng chính sách trong quá trình huấn luyện, trong khi PPO cần bốn mạng (chính sách, giá trị, phần thưởng, và mạng tham chiếu). Bên cạnh đó, rso-sample-rank được song song hóa hoàn toàn trên toàn bộ tập dữ liệu, trong khi PPO cần lấy mẫu ở mỗi bước và được song song hóa trong batch. Bây giờ chúng tôi tập trung vào một phân tích so sánh về hiệu quả tính toán giữa các phương pháp ngoại tuyến khác nhau. So sánh của chúng tôi bao gồm RAFT (Dong et al., 2023), ReST (Gulcehre et al., 2023), DPO (Rafailov et al., 2023), SLiC-HF-direct (Zhao et al., 2023), SLiC-HF-sample-rank (Zhao et al., 2023), và RSO được đề xuất của chúng tôi. Đáng chú ý, hầu hết các phương pháp, ngoại trừ DPO và SLiC-HF-direct, yêu cầu huấn luyện và suy luận của một mô hình phần thưởng (theo cặp).

Bảng 7 mô tả so sánh hiệu quả giữa các phương pháp được xem xét. Đối với các phương pháp liên quan đến một mô hình phần thưởng theo cặp với nc ứng viên được giải mã và nd ứng viên được chọn cho RSO, các chi tiết cụ thể sau được lưu ý:

• RAFT: Yêu cầu nc giải mã từ chính sách SFT và nc−1 so sánh cho xếp hạng tournament.
• ReST: Liên quan đến nc giải mã SFT và nc−1 so sánh với một chuỗi cơ sở được chọn ngẫu nhiên, tiếp theo bằng chuẩn hóa điểm phần thưởng và cắt ngắn dựa trên ngưỡng.
• DPO/SLiC-HF-direct: trực tiếp tối ưu hóa trên dữ liệu sở thích của con người mà không có mô hình phần thưởng và các chuỗi được giải mã SFT.
• SLiC-HF-sample-rank: Lấy mẫu nd chuỗi, sau đó sử dụng nd−1 so sánh xếp hạng tournament.
• RSO: Phương pháp của chúng tôi lấy mẫu nc ứng viên được giải mã từ chính sách SFT. Mỗi ứng viên được gán một điểm phần thưởng dựa trên nc−1 so sánh với một cơ sở được chọn ngẫu nhiên. RSO sau đó sử dụng lấy mẫu loại bỏ thống kê để chọn nd chuỗi và xây dựng các cặp sở thích sử dụng nd/2 so sánh.

So với DPO và SLiC-HF-direct, RSO giới thiệu một giai đoạn lấy mẫu và xếp hạng bổ sung. Những giai đoạn này có thể mở rộng và có thể được song song hóa qua nhiều máy chủ mô hình, tăng cường đáng kể hiệu quả.

RSO cần nhiều suy luận máy chủ phần thưởng hơn. Gánh nặng tính toán bổ sung có thể được giảm thiểu và giải quyết với hiệu quả prompt: Với một prompt cố định để tạo ra phản hồi, RSO hưởng lợi từ việc cache prompt trên các máy chủ mô hình, dẫn đến tạo ra phản hồi nhanh hơn. Tốc độ suy luận có thể được cải thiện thêm với các kỹ thuật phục vụ tiên tiến (Pope et al., 2023). Về phía máy chủ phần thưởng, suy luận với độ dài bộ giải mã 1 đảm bảo thời gian xử lý nhanh.

Thuật toán lấy mẫu loại bỏ thống kê, như được mô tả trong Thuật toán 1, thể hiện hiệu quả tăng cường bằng cách sử dụng một chiến lược lấy mẫu-không-thay thế. Điều này được đạt được bằng cách loại trừ các chuỗi được chọn sau mỗi vòng lấy mẫu. Hơn nữa, ở đầu mỗi vòng, phần thưởng tối đa được tính toán lại. Việc hiệu chỉnh lại này đảm bảo rằng, trong mỗi vòng, ít nhất một chuỗi luôn được chọn. Cụ thể, chuỗi có phần thưởng tương đương với phần thưởng tối đa được chọn với xác suất một, do đó đảm bảo việc chọn ít nhất một chuỗi tối ưu trong mỗi vòng. Phương pháp này không chỉ tối ưu hóa quá trình chọn mà còn duy trì hiệu quả của thuật toán trong suốt quá trình thực thi.

RSO cần tính toán bổ sung về lấy mẫu từ chính sách SFT và xếp hạng từ mô hình phần thưởng theo cặp, nhưng chi phí bổ sung thực nghiệm là nhỏ so với huấn luyện chính sách. Có một số lý do cho điều đó:

• Chúng tôi chỉ cần lấy mẫu một lần cho mỗi prompt trong dữ liệu huấn luyện. Nhưng việc huấn luyện DPO có thể trải qua nhiều epoch.
• Lấy mẫu và xếp hạng có thể song song hóa hoàn toàn trên toàn bộ tập huấn luyện nhưng huấn luyện chỉ song song hóa được trong batch.
• Xếp hạng phần thưởng có thể nhanh vì độ dài giải mã ngắn (chỉ một token). Văn bản đầu vào có thể được mã hóa theo cách song song.
• Các quan sát của chúng tôi chỉ ra rằng rso-sample-rank chiếm ít hơn 10% tổng thời gian huấn luyện.
• Giải mã batch có thể mở rộng và hiệu quả với nhiều tối ưu hóa (Pope et al., 2023). Trong công việc này, chúng tôi lấy mẫu 64 phản hồi từ chính sách SFT. Các công trình nghiên cứu hiện tại có thể lấy mẫu tương tự hoặc thậm chí nhiều mẫu hơn từ chính sách SFT để xây dựng best-of-N:
   1. Lên đến 32 mẫu trong Bảng 4 trong Dong et al. (2023);
   2. Lên đến 100 mẫu trong Hình 7 trong Touvron et al. (2023);
   3. Lên đến 30k mẫu trong Bảng 2 trong Gao et al. (2023);

Từ góc độ cân bằng giữa gánh nặng bổ sung về hiệu quả và những tăng chất lượng hiệu suất đáng kể (như được hiển thị trong Phần 5), RSO nổi bật như một phương pháp được khuyến nghị so với các lựa chọn thay thế.

[Bảng 7: So sánh hiệu quả của các phương pháp khác nhau]
