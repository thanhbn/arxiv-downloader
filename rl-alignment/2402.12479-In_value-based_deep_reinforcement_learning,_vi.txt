# 2402.12479.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2402.12479.pdf
# Kích thước tệp: 1586923 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Trong học tăng cường sâu dựa trên giá trị,
một mạng được tỉa là một mạng tốt
Johan Obando-Ceron1 2 3Aaron Courville2 3Pablo Samuel Castro1 2 3

Tóm tắt
Các nghiên cứu gần đây đã cho thấy rằng các tác nhân học tăng cường sâu gặp khó khăn trong việc sử dụng hiệu quả các tham số mạng của chúng. Chúng tôi tận dụng các hiểu biết trước đó về lợi ích của các kỹ thuật huấn luyện thưa thớt và chứng minh rằng việc tỉa cường độ dần dần cho phép các tác nhân dựa trên giá trị tối đa hóa hiệu quả tham số. Điều này dẫn đến các mạng mang lại cải thiện hiệu suất đáng kể so với các mạng truyền thống, chỉ sử dụng một phần nhỏ của các tham số mạng đầy đủ. Mã của chúng tôi được công khai, xem Phụ lục A để biết chi tiết.

1. Giới thiệu
Mặc dù có những ví dụ thành công về việc áp dụng học tăng cường sâu (RL) vào các vấn đề thực tế (Mnih et al., 2015; Berner et al., 2019; Vinyals et al., 2019; Fawzi et al., 2022; Bellemare et al., 2020), có bằng chứng ngày càng tăng về các thách thức và bệnh lý phát sinh khi huấn luyện các mạng này (Ostrovski et al., 2021; Kumar et al., 2021a; Lyle et al., 2022; Graesser et al., 2022; Nikishin et al., 2022; Sokar et al., 2023; Ceron et al., 2023). Đặc biệt, đã được chứng minh rằng các tác nhân RL sâu sử dụng dưới mức các tham số mạng của chúng: Kumar et al. (2021a) đã chứng minh rằng có một sự thiếu tham số hóa ngầm, Sokar et al. (2023) tiết lộ rằng một số lượng lớn các tế bào thần kinh trở nên không hoạt động trong quá trình huấn luyện, và Graesser et al. (2022) cho thấy rằng các phương pháp huấn luyện thưa thớt có thể duy trì hiệu suất với một phần rất nhỏ của các tham số mạng ban đầu.

Một trong những phát hiện đáng ngạc nhiên nhất của công trình cuối cùng này là việc áp dụng kỹ thuật tỉa cường độ dần dần được đề xuất bởi Zhu & Gupta (2017) trên DQN (Mnih et al., 2015) với xương sống ResNet (như được giới thiệu trong Impala (Espeholt et al., 2018)), dẫn đến cải thiện hiệu suất 50% so với

*Đóng góp bằng nhau1Google DeepMind2Mila - Viện AI Québec3Université de Montréal. Liên hệ: Johan Obando-Ceron <jobando0730@gmail.com>, Pablo Samuel Castro<psc@google.com>.

Kỷ yếu Hội nghị Quốc tế lần thứ 41 về Học máy, Vienna, Áo. PMLR 235, 2024. Bản quyền 2024 thuộc về (các) tác giả.

1 2 3 4 5
Tỷ lệ độ rộng2.02.53.03.54.04.55.05.5Điểm bình thường hóa theo con người IQM
Kiến trúc
Dày đặc
Được tỉa (95% độ thưa thớt)Tác nhân
Rainbow
DQNHình 1. Mở rộng độ rộng mạng cho kiến trúc ResNet, cho DQN và Rainbow với ResNet dựa trên Impala (Espeholt et al., 2018). Chúng tôi báo cáo trung bình liên tứ phân vị sau 40 triệu bước môi trường, tổng hợp trên 15 game với 5 seed mỗi game; thanh lỗi chỉ ra khoảng tin cậy bootstrap phân tầng 95%. Tỷ lệ phát lại được cố định ở mức tiêu chuẩn 0.25. Mạng mặc định là Dày đặc, mà chúng tôi chỉ ra bằng màu xanh trong tất cả các biểu đồ, để rõ ràng.

đối tác dày đặc, chỉ với 10% tham số ban đầu (xem bảng dưới bên phải của Hình 1 của Graesser et al. (2022)). Một cách tò mò, khi cùng kỹ thuật tỉa được áp dụng cho kiến trúc CNN ban đầu, không có cải thiện hiệu suất, nhưng cũng không có suy giảm.

Việc cùng một kỹ thuật tỉa có thể có những kết quả định tính khác nhau như vậy, nhưng không âm, chỉ bằng cách thay đổi kiến trúc cơ bản là thú vị. Điều này gợi ý rằng việc huấn luyện các tác nhân RL sâu với các cấu trúc mạng không tiêu chuẩn (như được tạo ra bởi các kỹ thuật như tỉa cường độ dần dần) có thể hữu ích một cách tổng quát, và đáng để điều tra sâu hơn.

Trong bài báo này, chúng tôi khám phá việc tỉa cường độ dần dần như một kỹ thuật tổng quát để cải thiện hiệu suất của các tác nhân RL. Chúng tôi chứng minh rằng ngoài việc cải thiện hiệu suất của các kiến trúc mạng tiêu chuẩn cho các tác nhân dựa trên giá trị, các lợi ích tăng tỷ lệ thuận với kích thước của kiến trúc mạng cơ sở. Điểm cuối cùng này rất quan trọng, vì các mạng RL sâu được biết là gặp khó khăn với việc mở rộng kiến trúc (Ota et al., 2021; Farebrother et al., 2023; Taiga et al., 2023; Schwarzer et al., 2023).

1arXiv:2402.12479v3  [cs.LG]  25 Jun 2024

--- TRANG 2 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

Các đóng góp chính của chúng tôi như sau. Chúng tôi:
•trình bày việc tỉa cường độ dần dần như một kỹ thuật tổng quát để tối đa hóa hiệu quả tham số trong RL dựa trên giá trị;
•chứng minh rằng các mạng được huấn luyện với kỹ thuật này tạo ra các tác nhân mạnh hơn so với các đối tác dày đặc của chúng, và tiếp tục cải thiện khi chúng tôi mở rộng kích thước của mạng ban đầu;
•điều tra kỹ thuật này trên một tập hợp đa dạng các tác nhân và chế độ huấn luyện, bao gồm các phương pháp actor-critic;
•trình bày các phân tích chuyên sâu để hiểu rõ hơn lý do đằng sau lợi ích của chúng.

2. Công trình liên quan
Mở rộng trong RL sâu Các mạng nơ-ron sâu đã là yếu tố thúc đẩy đằng sau nhiều ứng dụng thành công của học tăng cường vào các nhiệm vụ thực tế. Tuy nhiên, trong lịch sử, việc mở rộng các mạng này một cách tương tự như những gì đã dẫn đến "quy luật mở rộng" trong học có giám sát mà không bị suy giảm hiệu suất là khó khăn; điều này phần lớn do các bất ổn định huấn luyện gia tăng vốn đặc trưng của học tăng cường (Van Hasselt et al., 2018; Sinha et al., 2020; Ota et al., 2021). Các công trình gần đây đã có thể làm được điều này thành công đã phải dựa vào một số kỹ thuật nhắm mục tiêu và lựa chọn siêu tham số cẩn thận (Farebrother et al., 2023; Taiga et al., 2023; Schwarzer et al., 2023; Ceron et al., 2023).

Cobbe et al. (2020); Farebrother et al. (2023) và Schwarzer et al. (2023) đã chuyển từ kiến trúc CNN ban đầu của Mnih et al. (2015) sang kiến trúc dựa trên ResNet, như được đề xuất bởi Espeholt et al. (2018), điều này đã chứng minh là phù hợp hơn cho việc mở rộng. Cobbe et al. (2020) và Farebrother et al. (2023) quan sát thấy lợi thế khi tăng số lượng đặc trưng trong mỗi lớp của kiến trúc ResNet. Schwarzer et al. (2023) cho thấy rằng hiệu suất của tác nhân của họ (BBF) tiếp tục tăng tỷ lệ thuận với độ rộng của mạng. Bjorck et al. (2021) đề xuất chuẩn hóa phổ để giảm thiểu các bất ổn định huấn luyện và cho phép mở rộng kiến trúc của họ. Ceron et al. (2023) đề xuất giảm kích thước batch để cải thiện hiệu suất, ngay cả khi mở rộng mạng.

Ceron et al. (2024) chứng minh rằng trong khi việc mở rộng tham số với các mạng tích chập làm tổn hại hiệu suất RL một nhiệm vụ trên Atari, việc kết hợp các mô-đun Mixture-of-Expert (MoE) trong các mạng như vậy cải thiện hiệu suất. Farebrother et al. (2024) chứng minh rằng các hàm giá trị được huấn luyện với entropy chéo phân loại cải thiện đáng kể hiệu suất và khả năng mở rộng trong nhiều lĩnh vực khác nhau.

Các mô hình thưa thớt trong RL sâu Các nghiên cứu trước đây (Schmitt et al., 2018; Zhang et al., 2019) đã sử dụng chưng cất kiến thức với dữ liệu tĩnh để giảm thiểu bất ổn định, dẫn đến các tác nhân nhỏ nhưng dày đặc. Livne & Cohen (2020) giới thiệu việc tỉa và thu nhỏ chính sách, sử dụng việc tỉa chính sách lặp đi lặp lại tương tự như việc tỉa cường độ lặp đi lặp lại (Han et al., 2015), để có được một tác nhân DRL thưa thớt. Việc khám phá giả thuyết vé số trong DRL ban đầu được thực hiện bởi Yu et al. (2019), và sau đó Vischer et al. (2021) chứng minh rằng một vé trúng thưa thớt cũng có thể được xác định thông qua việc học hành vi theo mẫu. Sokar et al. (2021) đề xuất việc sử dụng tiến hóa cấu trúc của cấu trúc mạng trong DRL, đạt được 50% độ thưa thớt mà không suy giảm hiệu suất. Arnob et al. (2021) giới thiệu việc tỉa một lần cho Học tăng cường ngoại tuyến.

Graesser et al. (2022) phát hiện rằng việc tỉa thường mang lại kết quả cải thiện, và các phương pháp huấn luyện thưa thớt động, nơi cấu trúc thưa thớt thay đổi trong suốt quá trình huấn luyện (Mocanu et al., 2018; Evci et al., 2020), có thể vượt trội đáng kể so với huấn luyện thưa thớt tĩnh, nơi cấu trúc thưa thớt vẫn cố định trong suốt quá trình huấn luyện. Tan et al. (2023) nâng cao hiệu quả của huấn luyện thưa thớt động thông qua việc giới thiệu một cơ chế mục tiêu sai phân thời gian đa bước trì hoãn mới và một bộ đệm phát lại dung lượng động. Grooten et al. (2023) đề xuất một phương pháp lọc nhiễu tự động, sử dụng các nguyên tắc của huấn luyện thưa thớt động để điều chỉnh cấu trúc mạng nhằm tập trung vào các đặc trưng liên quan đến nhiệm vụ.

Quá tham số hóa trong RL sâu Song et al. (2019) và Zhang et al. (2018) nổi bật và xu hướng của các mạng RL để quá khớp, trong khi Nikishin et al. (2022) và Sokar et al. (2023) chứng minh sự phổ biến của mất tính dẻo dai trong các mạng RL, dẫn đến suy giảm hiệu suất cuối cùng. Một số chiến lược đã được đề xuất để giảm thiểu điều này, như tăng cường dữ liệu (Yarats et al., 2021; Cetin et al., 2022), dropout (Gal & Ghahramani, 2016), và chuẩn hóa lớp và batch (Ba et al., 2016; Ioffe & Szegedy, 2015). Hiraoka et al. (2021) chứng minh sự thành công của việc sử dụng dropout và chuẩn hóa lớp trong Soft Actor-Critic (Haarnoja et al., 2018), trong khi Liu et al. (2020) xác định rằng việc áp dụng chính quy hóa trọng số ℓ2 trên các actors có thể nâng cao cả thuật toán on-policy và off-policy.

Nikishin et al. (2022) xác định xu hướng của các mạng quá khớp với dữ liệu sớm (thiên lệch ưu tiên), có thể cản trở việc học tiếp theo, và đề xuất việc khởi tạo lại mạng định kỳ như một phương tiện để giảm thiểu nó. Tương tự, Sokar et al. (2023) đề xuất việc khởi tạo lại các tế bào thần kinh không hoạt động để cải thiện tính dẻo dai của mạng, trong khi Nikishin et al. (2023) đề xuất việc tiêm tính dẻo dai bằng cách tạm thời đóng băng mạng hiện tại và sử dụng các trọng số mới được khởi tạo để tạo điều kiện cho việc học liên tục.

2

--- TRANG 3 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

0 20 40 60 80 100
Số khung hình (tính bằng triệu)0.00.20.40.60.81.0% Lịch trình độ thưa thớt
40M
100M

Hình 2. Các lịch trình tỉa cường độ dần dần được sử dụng trong các thí nghiệm của chúng tôi, với độ thưa thớt mục tiêu 95%, như được chỉ định trong Phương trình 1. Tác động của việc thay đổi lịch trình tỉa, xem Hình 10.

3. Kiến thức nền tảng

Học tăng cường sâu Mục tiêu trong Học tăng cường là tối ưu hóa tổng lợi nhuận chiết khấu tích lũy trong một chân trời dài, và thường được hình thức hóa như một quá trình quyết định Markov (MDP) (X,A, P, r, γ). Một MDP bao gồm một không gian trạng thái X, một không gian hành động A, một mô hình động lực chuyển tiếp P:X ×A → ∆(X)(trong đó ∆(X) là một phân phối trên tập hợp X), một hàm phần thưởng R:X × A → R, và một yếu tố chiết khấu γ∈[0,1). Một chính sách π:X → ∆(A) hình thức hóa hành vi của tác nhân.

Đối với một chính sách π, Qπ(x,a) đại diện cho phần thưởng chiết khấu kỳ vọng đạt được bằng cách thực hiện hành động a trong trạng thái x và sau đó tuân theo chính sách π:
Qπ(x,a) := Eπ[P∞ t=0γtR(xt,at)|x0=x,a0=a].

Hàm Q tối ưu, ký hiệu là Q⋆(x,a), thỏa mãn quan hệ lặp Bellman
Q⋆(x,a) =Ex′∼P(x′|x,a)[R(x,a) +γmax a′Q⋆(x′,a′)].

Hầu hết các phương pháp dựa trên giá trị hiện đại sẽ xấp xỉ Q thông qua một mạng nơ-ron với các tham số θ, ký hiệu là Qθ. Ý tưởng này được giới thiệu bởi Mnih et al. (2015) với tác nhân DQN của họ, đã phục vụ như cơ sở cho hầu hết các thuật toán RL sâu hiện đại. Mạng Qθ thường được huấn luyện với một mất mát sai phân thời gian, như:
L(θ) = E(x,a,r,x′)∼D["r+γmax a′∈A¯Q(x′,a′)−Qθ(x,a)2#,

Ở đây D đại diện cho một bộ sưu tập được lưu trữ của các chuyển tiếp (xt,at,rt,xt+1) mà tác nhân lấy mẫu từ đó để học (được gọi là bộ đệm phát lại). ¯Q là một mạng tĩnh sao chép các tham số của nó từ Qθ một cách không thường xuyên; mục đích của nó là tạo ra các mục tiêu học ổn định hơn.

Rainbow (Hessel et al., 2018) đã mở rộng và cải thiện

0 10 20 30 40
Số khung hình (tính bằng triệu)012345Điểm bình thường hóa theo con người IQM
Tỷ lệ độ rộng = 3
Độ thưa thớt
0
0.9
0.95
0.99

Hình 3. Đánh giá cách thay đổi độ thưa thớt ảnh hưởng đến hiệu suất cho DQN với kiến trúc ResNet và nhân tử độ rộng là 3. Xem Mục 4.1 để biết chi tiết huấn luyện.

thuật toán DQN ban đầu với double Q-learning (van Hasselt et al., 2016), phát lại kinh nghiệm ưu tiên (Schaul et al., 2016), mạng đối đầu (Wang et al., 2016), lợi nhuận đa bước (Sutton, 1988), học tăng cường phân phối (Bellemare et al., 2017), và mạng nhiễu (Fortunato et al., 2018).

Tỉa dần dần Trong các thiết lập học có giám sát, có một quan tâm rộng rãi đến các kỹ thuật huấn luyện thưa thớt, theo đó chỉ một tập con của các tham số mạng đầy đủ được huấn luyện/sử dụng (Gale et al., 2019). Điều này được thúc đẩy bởi hiệu quả tính toán và không gian, cũng như tốc độ suy luận. Zhu & Gupta (2017) đề xuất một lịch trình đa thức để dần dần làm thưa thớt một mạng dày đặc trong quá trình huấn luyện bằng cách tỉa các tham số mô hình có cường độ trọng số thấp.

Cụ thể, gọi sF∈[0,1] biểu thị mức độ thưa thớt cuối cùng mong muốn (ví dụ 0.95 trong hầu hết các thí nghiệm của chúng tôi) và gọi tstart và tend biểu thị các lần lặp bắt đầu và kết thúc của việc tỉa, tương ứng; thì mức độ thưa thớt tại lần lặp t được cho bởi:

st= (1)
sF 
1−
1−t−tstart
tend−tstart3!
nếu tstart≤t≤tend
0.0 nếu t < tstart
sF nếu t > tend

Graesser et al. (2022) đã áp dụng ý tưởng này cho các mạng RL sâu, đặt tstart ở 20% của quá trình huấn luyện và tend ở 80% của quá trình huấn luyện.

4. Việc tỉa có thể tăng hiệu suất RL sâu

Chúng tôi điều tra tính hữu ích tổng quát của việc tỉa cường độ dần dần trong các tác nhân RL sâu trong cả thiết lập trực tuyến và ngoại tuyến.

3

--- TRANG 4 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

1 3 5
Tỷ lệ độ rộng1.61.82.02.22.42.62.8Điểm bình thường hóa theo con người IQM
Kiến trúc
Dày đặc
Được tỉa (95% độ thưa thớt)Tác nhân
Rainbow
DQN

Hình 4. Mở rộng độ rộng mạng cho kiến trúc CNN ban đầu của Mnih et al. (2015), cho DQN (trái) và Rainbow (phải). Xem Mục 4.1 để biết chi tiết huấn luyện.

4.1. Chi tiết triển khai

Đối với các tác nhân DQN và Rainbow cơ bản, chúng tôi sử dụng các triển khai Jax của thư viện Dopamine1(Castro et al., 2018) với các giá trị mặc định của chúng. Đáng chú ý là Dopamine cung cấp một phiên bản "nhỏ gọn" của tác nhân Rainbow ban đầu, chỉ sử dụng cập nhật đa bước, phát lại ưu tiên và RL phân phối. Đối với tất cả các thí nghiệm, chúng tôi sử dụng kiến trúc Impala được giới thiệu bởi Espeholt et al. (2018), đây là một ResNet 15 lớp, trừ khi được chỉ định khác. Lý do của chúng tôi cho điều này không chỉ bởi vì kết quả từ Graesser et al. (2022), mà còn do một số công trình gần đây chứng minh rằng kiến trúc này dẫn đến hiệu suất cải thiện nói chung (Schmidt & Schmied, 2021; Kumar et al., 2022; Schwarzer et al., 2023).

Chúng tôi sử dụng thư viện JaxPruner2(Lee et al., 2024) cho việc tỉa cường độ dần dần, vì nó đã cung cấp tích hợp với Dopamine. Chúng tôi tuân theo lịch trình của Graesser et al. (2022): bắt đầu tỉa mạng 20% vào quá trình huấn luyện và dừng ở 80%, giữ mạng thưa thớt cuối cùng cố định trong phần còn lại của quá trình huấn luyện. Hình 2 minh họa các lịch trình tỉa được sử dụng trong các thí nghiệm của chúng tôi (cho độ thưa thớt 95%). Chúng tôi đánh giá các tác nhân của mình trên Arcade Learning Environment (ALE) (Bellemare et al., 2013) trên cùng 15 game được sử dụng bởi Graesser et al. (2022), được chọn vì tính đa dạng của chúng3. Để cân nhắc tính toán, hầu hết các thí nghiệm được tiến hành trong 40 triệu khung hình (thay vì 200 triệu tiêu chuẩn); trong các điều tra của chúng tôi, chúng tôi thấy sự khác biệt định tính giữa các thuật toán ở 40 triệu khung hình hầu như nhất quán với những ở 100 triệu (ví dụ xem Hình 10).

1Mã Dopamine có sẵn tại https://github.com/google/dopamine.
2Mã JaxPruner có sẵn tại https://github.com/google-research/jaxpruner.
3Được thảo luận trong A.4 trong Graesser et al. (2022).

0.25 0.5 1 2
Tỷ lệ phát lại12345Điểm bình thường hóa theo con người IQM
Tỷ lệ độ rộng = 3
Dày đặc
Được tỉa (95% độ thưa thớt)

Hình 5. Mở rộng tỷ lệ phát lại cho Rainbow với kiến trúc ResNet với nhân tử độ rộng là 3. Tỷ lệ phát lại mặc định là 0.25. Xem Mục 4.1 để biết chi tiết huấn luyện.

Chúng tôi tuân theo các hướng dẫn được nêu bởi Agarwal et al. (2021) để đánh giá: mỗi thí nghiệm được chạy với 5 seed độc lập, và chúng tôi báo cáo trung bình liên tứ phân vị được chuẩn hóa theo con người (IQM), tổng hợp trên 15 game, cấu hình và seed, cùng với khoảng tin cậy bootstrap phân tầng 95%. Tất cả các thí nghiệm được chạy trên GPU NVIDIA Tesla P100, và mỗi thí nghiệm mất khoảng 2 ngày để hoàn thành. Tất cả các siêu tham số được liệt kê trong Phụ lục F.

4.2. RL trực tuyến

Trong khi Graesser et al. (2022) chứng minh rằng các mạng thưa thớt có khả năng duy trì hiệu suất tác nhân, nếu các mức độ thưa thớt này quá cao, hiệu suất cuối cùng suy giảm. Điều này trực quan, vì với mức độ thưa thớt cao hơn, có ít tham số hoạt động hơn còn lại trong mạng. Một câu hỏi tự nhiên là liệu việc mở rộng mạng ban đầu của chúng ta có cho phép mức độ thưa thớt cao hay không. Do đó, chúng tôi bắt đầu cuộc điều tra của mình bằng cách áp dụng việc tỉa cường độ dần dần trên DQN với kiến trúc Impala, nơi chúng tôi đã mở rộng các lớp tích chập theo hệ số 3. Hình 3 xác nhận rằng đây thực sự là trường hợp: độ thưa thớt 90% và 95% tạo ra cải thiện hiệu suất 33%, và độ thưa thớt 99% duy trì hiệu suất.

Độ thưa thớt 95% liên tục mang lại hiệu suất tốt nhất trong các khám phá ban đầu của chúng tôi, vì vậy chúng tôi chủ yếu tập trung vào mức độ thưa thớt này cho các điều tra của mình. Hình 1 là một kết quả nổi bật: chúng tôi quan sát cải thiện hiệu suất gần 60% (DQN) và 50% (Rainbow) so với các kiến trúc ban đầu (không được tỉa và không được mở rộng). Ngoài ra, trong khi hiệu suất của các kiến trúc không được tỉa giảm đơn điệu với độ rộng tăng, hiệu suất của các đối tác được tỉa tăng đơn điệu. Trong Hình 6, chúng tôi đánh giá việc tỉa trên DQN và Rainbow trên tất cả 60 game Atari 2600, xác nhận các phát hiện của chúng tôi không cụ thể cho 15 game được chọn ban đầu.

4

--- TRANG 5 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

0 10 20 30 40
Số khung hình (tính bằng triệu)0.00.51.01.5Điểm bình thường hóa theo con người IQM
DQN
0 10 20 30 40
Số khung hình (tính bằng triệu)0.00.51.01.52.02.5
 Rainbow
Dày đặc
Được tỉa (95% độ thưa thớt)

Hình 6. Đánh giá hiệu suất trên bộ đầy đủ Atari 2600. DQN (trái) và Rainbow (phải), cả hai đều sử dụng kiến trúc ResNet với độ rộng 3. Chúng tôi báo cáo hiệu suất IQM với thanh lỗi chỉ ra khoảng tin cậy 95%. Xem Mục 4.1 để biết chi tiết huấn luyện.

Khi chuyển cả hai tác nhân sang sử dụng kiến trúc CNN ban đầu của Mnih et al. (2015), chúng ta thấy xu hướng tương tự với Rainbow, nhưng thấy ít cải thiện trong DQN (Hình 4). Kết quả này phù hợp với các phát hiện của Graesser et al. (2022), nơi không có cải thiện thực sự nào được quan sát với việc tỉa trên các kiến trúc CNN. Một quan sát thú vị là, với kiến trúc CNN này, hiệu suất của DQN dường như có lợi từ độ rộng tăng, trong khi hiệu suất của Rainbow chịu suy giảm nhẹ.

Khi đánh giá trên các tác nhân dựa trên giá trị hiện đại hơn, cụ thể là IQN (Dabney et al., 2018) và Munchausen-IQN (Vieillard et al., 2020), chúng tôi quan sát cùng những lợi thế phát sinh từ việc tỉa (xem Phụ lục G.2).

Các phát hiện của chúng tôi đến nay gợi ý rằng việc sử dụng tỉa cường độ dần dần tăng hiệu quả tham số của các tác nhân này. Nếu vậy, thì các mạng thưa thớt này cũng sẽ có thể hưởng lợi từ nhiều cập nhật gradient hơn. Tỷ lệ phát lại4, là số lượng cập nhật gradient cho mỗi bước môi trường, đo chính xác điều này; nó được biết rõ rằng rất khó tăng giá trị này mà không bị suy giảm hiệu suất (Fedus et al., 2020; Nikishin et al., 2022; Schwarzer et al., 2023; D'Oro et al., 2023).

Trong Hình 5, chúng tôi thực sự có thể xác nhận rằng các kiến trúc được tỉa duy trì ưu thế hiệu suất so với baseline không được tỉa ngay cả ở các giá trị tỷ lệ phát lại cao. Tỷ lệ suy giảm sắc hơn với việc tỉa có thể gợi ý rằng lịch trình tỉa cần được điều chỉnh lại cho các thiết lập này.

4.3. Chế độ dữ liệu thấp

Kaiser et al. (2020) giới thiệu benchmark Atari 100k để đánh giá các tác nhân RL trong thiết lập hạn chế mẫu, chỉ cho phép các tác nhân 100k5 tương tác môi trường. Đối với chế độ này

4Trong các siêu tham số được thiết lập trong (Mnih et al., 2015), chính sách được cập nhật mỗi 4 bước môi trường được thu thập, dẫn đến tỷ lệ phát lại 0.25.
5Ở đây, 100k đề cập đến các bước tác nhân, hoặc 400k khung hình môi trường, do việc bỏ qua khung hình trong thiết lập huấn luyện tiêu chuẩn.

0 10 20 30 40
Số khung hình (tính bằng triệu)0.00.20.40.60.8Điểm bình thường hóa theo con người IQM
Dày đặc
Được tỉa (90% độ thưa thớt)
Được tỉa (95% độ thưa thớt)
0 10 20 30 40
Số khung hình (tính bằng triệu)0.00.20.40.60.8

Hình 7. Hiệu suất của DrQ(ϵ) (trái) và DER (phải) khi được huấn luyện trong 40M khung hình. Cả hai tác nhân đều sử dụng kiến trúc ResNet với nhân tử độ rộng 3. Xem Mục 4.1 để biết chi tiết huấn luyện.

chế độ, Kostrikov et al. (2020) giới thiệu DrQ, một biến thể của DQN sử dụng tăng cường dữ liệu; các siêu tham số cho tác nhân này được tối ưu hóa thêm bởi Agarwal et al. (2021) trong DrQ(ϵ). Tương tự, Van Hasselt et al. (2019) giới thiệu Data-Efficient Rainbow (DER), tối ưu hóa các siêu tham số của Rainbow (Hessel et al., 2018) cho chế độ dữ liệu thấp này.

Khi được đánh giá trên chế độ dữ liệu thấp này, các tác nhân được tỉa của chúng tôi không cho thấy lợi ích. Tuy nhiên, khi chúng tôi chạy trong 40M tương tác môi trường (như được đề xuất bởi Ceron et al. (2023)), chúng tôi quan sát được lợi ích đáng kể khi sử dụng tỉa cường độ dần dần, như được hiển thị trong Hình 7. Thú vị là, trong DrQ(ϵ), các tác nhân được tỉa tránh được suy giảm hiệu suất ảnh hưởng đến baseline khi được huấn luyện lâu hơn.

4.4. RL ngoại tuyến

Học tăng cường ngoại tuyến tập trung vào việc huấn luyện một tác nhân chỉ từ một tập dữ liệu cố định của các mẫu mà không có bất kỳ tương tác môi trường nào. Chúng tôi sử dụng hai phương pháp hiện đại từ văn học: CQL (Kumar et al., 2020) và CQL+C51 (Kumar et al., 2022), cả hai với kiến trúc ResNet từ Espeholt et al. (2018). Theo Kumar et al. (2021b), chúng tôi huấn luyện các tác nhân này trên 17 game Atari trong 200 triệu lần lặp khung hình, nơi 1 lần lặp tương ứng với 62,500 cập nhật gradient. Chúng tôi đánh giá các tác nhân bằng cách xem xét một tập dữ liệu gồm một mẫu ngẫu nhiên 5% của tất cả các tương tác môi trường được thu thập bởi một tác nhân DQN được huấn luyện trong 200M bước môi trường (Agarwal et al., 2020).

Lưu ý rằng vì chúng tôi đang huấn luyện cho một số bước khác với các thí nghiệm trước đó, chúng tôi điều chỉnh lịch trình tỉa cho phù hợp. Như được hiển thị trong Hình 8, cả CQL và CQL+C51 đều quan sát được lợi ích đáng kể khi sử dụng các mạng được tỉa, đặc biệt với các mạng rộng hơn. Thú vị là, trong chế độ ngoại tuyến, việc tỉa cũng giúp tránh sụp đổ hiệu suất khi sử dụng một mạng nông (tỷ lệ độ rộng bằng 1), hoặc thậm chí cải thiện hiệu suất cuối cùng như trong trường hợp của CQL+C51.

5

--- TRANG 6 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

0 50 100 150 200
Bước gradient (x 62.5K)0.00.20.40.60.8Điểm bình thường hóa theo con người IQM
Tỷ lệ độ rộng = 1
0 50 100 150 200
Bước gradient (x 62.5K)0.00.20.40.60.8
Tỷ lệ độ rộng = 3
Dày đặc
Được tỉa (95% độ thưa thớt)
Được tỉa (99% độ thưa thớt)
0 50 100 150 200
Bước gradient (x 62.5K)0.51.01.52.02.5
Tỷ lệ độ rộng = 1
0 50 100 150 200
Bước gradient (x 62.5K)0.51.01.52.02.5
Tỷ lệ độ rộng = 3

Hình 8. Mở rộng độ rộng mạng cho các tác nhân ngoại tuyến CQL (trái) và CQL+C51 (phải), cả hai đều sử dụng kiến trúc ResNet. Chúng tôi báo cáo hiệu suất trung bình liên tứ phân vị với thanh lỗi chỉ ra khoảng tin cậy 95% trên 17 game Atari. Trục x biểu thị các bước gradient; không có dữ liệu mới nào được thu thập.

0 50 100024Lợi nhuận×103 Walker2d-v2
Độ thưa thớt
0.0
0.9
0.95
0.99
0 50 100
Tương tác môi trường (x1e3)0123×103 Ant-v2

Hình 9. Đánh giá cách thay đổi tham số độ thưa thớt ảnh hưởng đến hiệu suất của SAC trên hai môi trường MuJoCo khi tăng độ rộng x5. Chúng tôi báo cáo lợi nhuận trên 10 lần chạy cho mỗi thí nghiệm.

0 20 40 60 80 100
Số khung hình (tính bằng triệu)012345Điểm bình thường hóa theo con người IQM
Baseline
Tỉa (lịch trình 40M)
Tỉa (lịch trình 100M)

Hình 10. Tác động của việc thay đổi lịch trình tỉa, cho DQN với ResNet dựa trên Impala với nhân tử độ rộng 3.

4.5. Các phương pháp Actor-Critic

Điều tra của chúng tôi đến nay đã tập trung vào các phương pháp dựa trên giá trị. Ở đây chúng tôi điều tra liệu việc tỉa cường độ dần dần có thể mang lại lợi ích hiệu suất cho Soft Actor Critic (SAC; Haarnoja et al., 2018), một thuật toán policy-gradient phổ biến. Chúng tôi đánh giá SAC trên năm môi trường điều khiển liên tục từ bộ MuJoCo (Todorov et al., 2012), sử dụng 10 seed độc lập cho mỗi môi trường. Trong Hình 9, chúng tôi trình bày kết quả cho Walker2d-v2 và Ant-v2, nơi chúng ta thấy các lợi thế của việc tỉa cường độ dần dần vẫn tồn tại; trong ba môi trường còn lại (xem Phụ lục E), không có lợi ích quan sát được thực sự từ việc tỉa. Trong Phụ lục G.1, chúng ta thấy xu hướng tương tự với PPO (Schulman et al., 2017).

4.6. Tính ổn định của mạng được tỉa

Chúng tôi tuân theo lịch trình tỉa được đề xuất bởi Graesser et al. (2022), điều này thích ứng tự nhiên với các bước huấn luyện khác nhau (như đã thảo luận ở trên cho các thí nghiệm RL ngoại tuyến). Lịch trình này huấn luyện mạng thưa thớt cuối cùng chỉ trong 20% cuối cùng của các bước huấn luyện. Một câu hỏi tự nhiên là liệu mạng thưa thớt kết quả, khi được huấn luyện lâu hơn, vẫn có thể duy trì hiệu suất của nó hay không. Để đánh giá điều này, chúng tôi huấn luyện DQN trong 100 triệu khung hình và áp dụng hai lịch trình tỉa: lịch trình thông thường chúng tôi sẽ sử dụng cho 100M cũng như lịch trình chúng tôi thường sử dụng cho 40M bước huấn luyện (xem Hình 2).

Như Hình 10 cho thấy, ngay cả với lịch trình nén 40M, mạng được tỉa vẫn có thể duy trì hiệu suất mạnh mẽ của nó. Thú vị là, với lịch trình nén, tác nhân đạt được hiệu suất cao hơn nhanh hơn so với lịch trình thông thường. Điều này gợi ý có nhiều chỗ để khám phá các lịch trình tỉa thay thế.

4.7. Mở rộng tỷ lệ học và kích thước batch

Tỷ lệ học hoặc kích thước batch mặc định có thể không tối ưu cho các mạng nơ-ron lớn. Tỷ lệ học mặc định cho DQN là 6.25×10−5. Chúng tôi chạy các thí nghiệm với tỷ lệ học chia cho hệ số tỷ lệ độ rộng (vậy 2.08×10−5 cho hệ số độ rộng 3, và 1.25×10−5 cho hệ số độ rộng 5). Trong khi các tỷ lệ học này cải thiện hiệu suất của baseline, nó vẫn bị vượt qua bởi việc tỉa (xem Hình 28). Chúng tôi quan sát xu hướng tương tự khi đánh giá các giá trị kích thước batch khác nhau. Kích thước batch mặc định là 32 (cho tất cả các tác nhân dựa trên giá trị được sử dụng trong bài báo này), và chúng tôi chạy các thí nghiệm với kích thước batch 16 và 64. Trong tất cả các trường hợp, việc tỉa duy trì lợi thế mạnh mẽ của nó (xem Hình 29). Những kết quả này phù hợp với luận điểm rằng việc tỉa có thể phục vụ như một cơ chế thả vào để tăng hiệu suất tác nhân.

6

--- TRANG 7 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

0.51.0BeamRider×104Lợi nhuận
0.02.55.07.5×102
QVariance
506070ParametersNormDày đặc Được tỉa (95% độ thưa thớt)
0.00.51.0×102QNorm
1.801.851.90×103Srank
7.510.012.515.0DormantNeurons
0123Breakout×102
024×102
0.60.81.0×102
0.00.51.0×102
1.61.8×103
51015
012Enduro×103
024×101
0.60.81.0×102
0123×102
1.01.5×103
152025
0 20 40024VideoPinball×105
0 20 400.00.51.01.5
0 20 400.500.751.00×102
0 20 40012×102
0 20 401.01.5×103
0 20 40
Số khung hình (tính bằng triệu)101520

Hình 11. Các phân tích thực nghiệm cho bốn game đại diện khi áp dụng việc tỉa. Từ trái qua phải: lợi nhuận huấn luyện, phương sai mục tiêu Q trung bình, chuẩn tham số trung bình, chuẩn ước lượng Q trung bình, srank (Kumar et al., 2021a), và các tế bào thần kinh không hoạt động (Sokar et al., 2023). Tất cả kết quả được tính trung bình trên 5 seed, các vùng tô bóng biểu thị khoảng tin cậy 95%.

5. Tại sao việc tỉa lại hiệu quả đến vậy?

Chúng tôi tập trung phân tích của mình vào bốn game: BeamRider, Breakout, Enduro và VideoPinball. Đối với mỗi game, chúng tôi đo phương sai của các ước lượng Q (QVariance); chuẩn trung bình của các tham số mạng (ParametersNorm); chuẩn trung bình của các giá trị Q (QNorm); hạng hiệu dụng của ma trận (Srank) như được đề xuất bởi Kumar et al. (2021a), và phần các tế bào thần kinh không hoạt động như được định nghĩa bởi Sokar et al. (2023).

Chúng tôi trình bày kết quả của mình trong Hình 11. Điều trở nên rõ ràng từ các hình này là việc tỉa (i) giảm phương sai, (ii) giảm chuẩn của các tham số, (iii) giảm số lượng tế bào thần kinh không hoạt động, và (iv) tăng hạng hiệu dụng của các tham số. Một số quan sát này có thể được quy cho một dạng chuẩn hóa, trong khi những quan sát khác có thể phát sinh do tăng tính dẻo dai của mạng.

Lyle et al. (2024) cho thấy rằng chuẩn tham số tăng đi kèm với mất tính dẻo dai trong các kiến trúc nơ-ron khác nhau. Trong Hình 11, chúng ta quan sát giá trị chuẩn tham số thấp khi áp dụng việc tỉa dần dần, điều này biểu thị hiệu suất lợi nhuận cuối cùng cao.

5.1. So sánh với các phương pháp khác

Để tách biệt tác động của việc tỉa khỏi chuẩn hóa và tiêm tính dẻo dai rõ ràng, chúng tôi so sánh với các phương pháp hiện có trong văn học.

Baseline vé số Frankle & Carbin (2018) lập luận rằng các mạng nơ-ron chứa các mạng con thưa thớt có thể được huấn luyện ở mức độ thưa thớt cao mà không cần tỉa dần dần; các tác giả cung cấp một thuật toán để tìm các vé trúng này. Sau khi huấn luyện một mạng với việc tỉa, chúng tôi huấn luyện một mạng mới với mặt nạ cuối cùng được cố định (tức là không được điều chỉnh trong quá trình huấn luyện) và với các tham số được khởi tạo như trong mạng dày đặc ban đầu. Chúng tôi thấy rằng đề xuất này hoạt động kém hơn cả phương pháp tỉa và baseline không được tỉa. Thú vị khi quan sát rằng cả phương pháp tỉa và thí nghiệm vé số dường như vẫn tiến bộ ở 40M, trong khi baseline dường như bắt đầu xấu đi (Hình 13).

Các baseline huấn luyện thưa thớt động Evci et al. (2020) đề xuất RigL như một cơ chế huấn luyện thưa thớt động duy trì một mạng thưa thớt trong suốt toàn bộ quá trình huấn luyện. Trong Phụ lục G.3, chúng tôi đánh giá RigL ở các mức độ thưa thớt khác nhau và thấy rằng, mặc dù hiệu quả, RigL không thể

7

--- TRANG 8 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

0 10 20 30 40
Số khung hình (tính bằng triệu)012345Điểm bình thường hóa theo con người IQM
DQN
+L2
+WD
+Redo
+Reset
+Tỉa
DQN +L2+WD+Redo +Reset
+Tỉa012345Tỷ lệ độ rộng
1
3
5

Hình 12. So sánh với việc đặt lại mạng (Nikishin et al., 2022), suy giảm trọng số, ReDo (Sokar et al., 2023) và chuẩn hóa của (Kumar et al., 2022). Trái: Đường cong hiệu quả mẫu với hệ số độ rộng 3; Phải: hiệu suất cuối cùng sau 40M khung hình với độ rộng thay đổi (bảng phải). Tất cả các thí nghiệm chạy trên DQN với kiến trúc ResNet và tỷ lệ phát lại 0.25.

0 10 20 30 40
Số khung hình (tính bằng triệu)012345Điểm bình thường hóa theo con người IQM
Tỉa
VéSố
Baseline

Hình 13. Thí nghiệm giả thuyết vé số. Lấy mạng được tỉa cuối cùng (với hệ số độ rộng 3) và huấn luyện lại với khởi tạo ban đầu dẫn đến hiệu suất tồi tệ hơn.

khớp với hiệu suất của việc tỉa cường độ dần dần; những kết quả này phù hợp với những của Graesser et al. (2022).

Các baseline chuẩn hóa Để điều tra vai trò của chuẩn hóa trong việc cải thiện hiệu suất được tạo ra bởi việc tỉa, chúng tôi xem xét hai loại chuẩn hóa ℓ2 đã được chứng minh hiệu quả trong văn học. Loại đầu tiên là suy giảm trọng số (WD), một kỹ thuật tiêu chuẩn thêm một thuật ngữ bổ sung vào mất mát để phạt chuẩn ℓ2 của các trọng số, do đó ngăn cản các tham số mạng phát triển quá lớn. Loại thứ hai là L2, phương pháp chính quy hóa được đề xuất bởi Kumar et al. (2022), được thiết kế để thực thi chuẩn ℓ2 là 1 cho các tham số.

Các baseline tiêm tính dẻo dai Chúng tôi so sánh với hai công trình gần đây đề xuất các phương pháp để xử lý trực tiếp việc mất tính dẻo dai. Nikishin et al. (2022) quan sát sự suy giảm hiệu suất với tỷ lệ phát lại tăng, quy cho điều này cho việc quá khớp trên các mẫu sớm, một hiệu ứng họ gọi là "thiên lệch ưu tiên". Các tác giả đề xuất việc đặt lại mạng định kỳ và chứng minh rằng nó rất hiệu quả trong việc giảm thiểu thiên lệch ưu tiên và quá khớp nói chung (điều này được gắn nhãn là Reset trong kết quả của chúng tôi).

Sokar et al. (2023) chứng minh rằng hầu hết các tác nhân RL sâu chịu từ hiện tượng tế bào thần kinh không hoạt động, theo đó các tế bào thần kinh ngày càng "tắt" trong quá trình huấn luyện các tác nhân RL sâu, do đó giảm khả năng biểu hiện của mạng. Để giảm thiểu điều này, họ đề xuất một phương pháp đơn giản và hiệu quả Tái chế các tế bào thần kinh Không hoạt động (ReDo) trong suốt quá trình huấn luyện.

Như Hình 12 minh họa, việc tỉa cường độ dần dần vượt qua tất cả các phương pháp chính quy hóa khác ở tất cả các mức tỷ lệ, và trong suốt toàn bộ quá trình huấn luyện. Thú vị là, hầu hết các phương pháp chính quy hóa chịu suy giảm khi tăng độ rộng mạng. Điều này gợi ý rằng hiệu ứng của việc tỉa không thể được quy hoàn toàn cho một dạng chuẩn hóa hoặc tiêm tính dẻo dai. Tuy nhiên, như chúng ta sẽ thấy bên dưới, tăng tính dẻo dai dường như phát sinh từ việc sử dụng nó. Chúng tôi cung cấp các quét trên các siêu tham số baseline khác nhau trong Phụ lục G.5 đến G.8.

5.2. Tác động lên tính dẻo dai

Tính dẻo dai là khả năng của một mạng nơ-ron để điều chỉnh nhanh chóng để đáp ứng với các phân phối dữ liệu thay đổi (Lyle et al., 2022; 2023; Lewandowski et al., 2023); với tính không ổn định của học tăng cường, việc duy trì để đảm bảo hiệu suất tốt là rất quan trọng. Tuy nhiên, các mạng RL được biết là mất tính dẻo dai trong quá trình huấn luyện (Nikishin et al., 2022; Sokar et al., 2023; Lee et al., 2023).

Lyle et al. (2023) đã tiến hành đánh giá cấu trúc hiệp phương sai của các gradient để kiểm tra cảnh quan mất mát của mạng, và lập luận rằng hiệu suất cải thiện và tăng tính dẻo dai thường được liên kết với tương quan gradient yếu hơn và giảm can thiệp gradient. Các quan sát của chúng tôi phù hợp với những phát hiện này, như được minh họa trong các bản đồ nhiệt hiệp phương sai gradient trong Hình 14. Trong các mạng dày đặc, các gradient thể hiện tính đồng tuyến đáng chú ý, trong khi tính đồng tuyến này được giảm đáng kể trong các mạng được tỉa.

6. Thảo luận và Kết luận

Công trình trước đây đã chứng minh rằng các tác nhân học tăng cường có xu hướng sử dụng dưới mức các tham số có sẵn của nó, và việc sử dụng dưới mức này tăng lên trong suốt quá trình huấn luyện và được khuếch đại khi kích thước mạng tăng (Graesser et al., 2022; Nikishin et al., 2022; Sokar et al., 2023; Schwarzer et al., 2023). Các tác nhân RL đạt được hiệu suất mạnh

8

--- TRANG 9 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

Dày đặc
 Được tỉa (95% độ thưa thớt)Breakout
Dày đặc
 Được tỉa (95% độ thưa thớt)VideoPinball

Hình 14. Ma trận hiệp phương sai gradient cho các game Atari Breakout (trái) và VideoPinball (phải). Màu đỏ đậm biểu thị tương quan âm cao, trong khi màu xanh đậm chỉ ra tương quan dương cao. Việc sử dụng tỉa tạo ra tương quan gradient yếu hơn và ít can thiệp gradient hơn, như được chứng minh bởi các sắc thái nhạt hơn trong các bản đồ nhiệt cho các mạng thưa thớt.

trong phần lớn các benchmark đã thiết lập với các mạng nhỏ (tương đối so với những mạng được sử dụng trong các mô hình ngôn ngữ, chẳng hạn), vì vậy việc không hiệu quả tham số rõ ràng này có thể được coi là ít quan trọng hơn so với các cân nhắc thuật toán khác.

Khi RL tiếp tục phát triển ngoài các benchmark học thuật và vào các nhiệm vụ phức tạp hơn, gần như chắc chắn sẽ cần các mạng lớn hơn và biểu cảm hơn. Trong trường hợp này, hiệu quả tham số trở nên quan trọng để tránh sự sụp đổ hiệu suất mà các công trình trước đây đã chỉ ra, cũng như để giảm chi phí tính toán (Ceron & Castro, 2021).

Công trình của chúng tôi cung cấp bằng chứng thuyết phục rằng các kỹ thuật huấn luyện thưa thớt như tỉa cường độ dần dần có thể hiệu quả trong việc tối đa hóa việc sử dụng mạng, đặc biệt khi các mạng ban đầu được mở rộng (xem Hình 1 và 4). Kết quả trong Hình 8, 7 và 10 đều chứng minh rằng các mạng thưa thớt được tạo ra bởi việc tỉa tốt hơn trong việc duy trì hiệu suất ổn định khi được huấn luyện lâu hơn. Lợi thế của việc tỉa vẫn còn ngay cả khi quét qua các siêu tham số baseline khác nhau (xem Phụ lục G.4 và G.9 đến G.11). Đáng chú ý là hiệu suất của các baseline dày đặc cải thiện khi điều chỉnh tỷ lệ học dựa trên nhân tử độ rộng (Phụ lục G.9); tuy nhiên, việc tỉa vẫn là hiệu suất nhất trong các thiết lập này.

Tổng hợp lại, kết quả của chúng tôi chứng minh rằng, bằng cách loại bỏ có ý nghĩa các tham số mạng trong suốt quá trình huấn luyện, chúng ta có thể vượt trội so với các đối tác dày đặc truyền thống và tiếp tục cải thiện hiệu suất khi chúng ta phát triển các kiến trúc mạng ban đầu. Kết quả của chúng tôi với các tác nhân và chế độ huấn luyện đa dạng ngụ ý rằng việc tỉa cường độ dần dần là một kỹ thuật hữu ích tổng quát có thể được sử dụng như một "thả vào" để tối đa hóa hiệu suất tác nhân.

Công việc tương lai Sẽ tự nhiên để khám phá việc kết hợp tỉa cường độ dần dần vào các tác nhân gần đây được thiết kế cho tổng quát hóa đa nhiệm vụ (Taiga et al., 2023; Kumar et al., 2022), hiệu quả mẫu (Schwarzer et al., 2023; D'Oro et al., 2023), và khả năng tổng quát hóa (Hafner et al., 2023). Hơn nữa, tính ổn định quan sát được của các mạng được tỉa có thể có ý nghĩa đối với các phương pháp dựa vào tinh chỉnh hoặc tái sinh (Agarwal et al., 2022).

Những tiến bộ gần đây trong bộ tăng tốc phần cứng để huấn luyện các mạng thưa thớt có thể dẫn đến thời gian huấn luyện nhanh hơn và phục vụ như một động lực cho nghiên cứu thêm về các phương pháp huấn luyện mạng thưa thớt. Hơn nữa, thực tế là một hệ quả của phương pháp này là một mạng có ít tham số hơn so với khi khởi tạo khiến nó hấp dẫn cho các ứng dụng tiếp theo trên các thiết bị biên.

Ít nhất, chúng tôi hy vọng công trình này phục vụ như một lời mời để khám phá các kiến trúc và cấu trúc mạng không tiêu chuẩn như một cơ chế hiệu quả để tối đa hóa hiệu suất của các tác nhân học tăng cường. Các tác nhân học tăng cường thường sử dụng các mạng ban đầu được thiết kế cho các vấn đề tĩnh; do đó, các cấu trúc khác có thể phù hợp hơn với bản chất không ổn định của RL.

Lời cảm ơn

Các tác giả muốn cảm ơn Laura Graesser, Utku Evci, Gopeshh Subbaraj, Evgenii Nikishin, Hugo Larochelle, Ayoub Echchahed, Zhixuan Lin và phần còn lại của nhóm Google DeepMind Montreal vì những thảo luận có giá trị trong quá trình chuẩn bị công trình này.

Laura Graesser xứng đáng được đề cập đặc biệt vì đã cung cấp cho chúng tôi phản hồi có giá trị về bản thảo sớm của bài báo. Chúng tôi cảm ơn các nhà đánh giá ẩn danh vì sự giúp đỡ có giá trị của họ trong việc cải thiện bản thảo của chúng tôi. Chúng tôi cũng muốn cảm ơn cộng đồng Python (Van Rossum & Drake Jr, 1995; Oliphant, 2007) vì đã phát triển các công cụ cho phép công trình này, bao gồm NumPy (Harris et al., 2020), Matplotlib (Hunter, 2007), Jupyter (Kluyver et al., 2016), Pandas (McKinney, 2013) và JAX (Bradbury et al., 2018).

9

--- TRANG 10 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

Tuyên bố tác động

Bài báo này trình bày công trình có mục tiêu thúc đẩy lĩnh vực Học máy, và học tăng cường nói riêng. Có nhiều hệ quả xã hội tiềm tàng của công trình chúng tôi, không có điều nào chúng tôi cảm thấy phải được nêu bật cụ thể ở đây.

Tài liệu tham khảo

Agarwal, R., Schuurmans, D., and Norouzi, M. An optimistic perspective on offline reinforcement learning. In III, H. D. and Singh, A. (eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 104–114. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/agarwal20c.html.

Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., and Bellemare, M. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:29304–29320, 2021.

Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C., and Bellemare, M. Reincarnating reinforcement learning: Reusing prior computation to accelerate progress. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 28955–28971. Curran Associates, Inc., 2022.

Arnob, S. Y., Ohib, R., Plis, S., and Precup, D. Single-shot pruning for offline reinforcement learning. arXiv preprint arXiv:2112.15579, 2021.

Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.

Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253–279, jun 2013. doi: 10.1613/jair.3912.

Bellemare, M. G., Dabney, W., and Munos, R. A distributional perspective on reinforcement learning. In ICML, 2017.

Bellemare, M. G., Candido, S., Castro, P. S., Gong, J., Machado, M. C., Moitra, S., Ponda, S. S., and Wang, Z. Autonomous navigation of stratospheric balloons using reinforcement learning. Nature, 588:77 – 82, 2020.

Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.

Bjorck, N., Gomes, C. P., and Weinberger, K. Q. Towards deeper deep reinforcement learning with spectral normalization. Advances in Neural Information Processing Systems, 34:8242–8255, 2021.

Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., et al. Jax: composable transformations of python+ numpy programs. 2018.

Castro, P. S., Moitra, S., Gelada, C., Kumar, S., and Bellemare, M. G. Dopamine: A research framework for deep reinforcement learning. arXiv preprint arXiv:1812.06110, 2018.

Ceron, J. S. O. and Castro, P. S. Revisiting rainbow: Promoting more insightful and inclusive deep reinforcement learning research. In International Conference on Machine Learning, pp. 1373–1383. PMLR, 2021.

Ceron, J. S. O., Bellemare, M. G., and Castro, P. S. Small batch deep reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=wPqEvmwFEh.

Ceron, J. S. O., Sokar, G., Willi, T., Lyle, C., Farebrother, J., Foerster, J. N., Dziugaite, G. K., Precup, D., and Castro, P. S. Mixtures of experts unlock parameter scaling for deep RL. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=X9VMhfFxwn.

Cetin, E., Ball, P. J., Roberts, S., and Celiktutan, O. Stabilizing off-policy deep reinforcement learning from pixels. In International Conference on Machine Learning, pp. 2784–2810. PMLR, 2022.

Cobbe, K., Hesse, C., Hilton, J., and Schulman, J. Leveraging procedural generation to benchmark reinforcement learning. In International conference on machine learning, pp. 2048–2056. PMLR, 2020.

Dabney, W., Ostrovski, G., Silver, D., and Munos, R. Implicit quantile networks for distributional reinforcement learning. In International conference on machine learning, pp. 1096–1105. PMLR, 2018.

D'Oro, P., Schwarzer, M., Nikishin, E., Bacon, P.-L., Bellemare, M. G., and Courville, A. Sample-efficient reinforcement learning by breaking the replay ratio barrier. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=OpC-9aBBVJe.

Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron, Y., Firoiu, V., Harley, T., Dunning,

10

--- TRANG 11 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

I., et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, pp. 1407–1416. PMLR, 2018.

Evci, U., Gale, T., Menick, J., Castro, P. S., and Elsen, E. Rigging the lottery: Making all tickets winners. In International Conference on Machine Learning, pp. 2943–2952. PMLR, 2020.

Farebrother, J., Greaves, J., Agarwal, R., Lan, C. L., Goroshin, R., Castro, P. S., and Bellemare, M. G. Proto-value networks: Scaling representation learning with auxiliary tasks. In Submitted to The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=oGDKSt9JrZi. under review.

Farebrother, J., Orbay, J., Vuong, Q., Taïga, A. A., Chebotar, Y., Xiao, T., Irpan, A., Levine, S., Castro, P. S., Faust, A., Kumar, A., and Agarwal, R. Stop regressing: Training value functions via classification for scalable deep rl. In Forty-first International Conference on Machine Learning. PMLR, 2024.

Fawzi, A., Balog, M., Huang, A., Hubert, T., Romera-Paredes, B., Barekatain, M., Novikov, A., R Ruiz, F. J., Schrittwieser, J., Swirszcz, G., et al. Discovering faster matrix multiplication algorithms with reinforcement learning. Nature, 610(7930):47–53, 2022.

Fedus, W., Ramachandran, P., Agarwal, R., Bengio, Y., Larochelle, H., Rowland, M., and Dabney, W. Revisiting fundamentals of experience replay. In International Conference on Machine Learning, pp. 3061–3071. PMLR, 2020.

Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos, R., Hassabis, D., Pietquin, O., Blundell, C., and Legg, S. Noisy networks for exploration. 2018.

Frankle, J. and Carbin, M. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In International Conference on Learning Representations, 2018.

Gal, Y. and Ghahramani, Z. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pp. 1050–1059. PMLR, 2016.

Gale, T., Elsen, E., and Hooker, S. The state of sparsity in deep neural networks. arXiv preprint arXiv:1902.09574, 2019.

Graesser, L., Evci, U., Elsen, E., and Castro, P. S. The state of sparse training in deep reinforcement learning. In International Conference on Machine Learning, pp. 7766–7792. PMLR, 2022.

Grooten, B., Sokar, G., Dohare, S., Mocanu, E., Taylor, M. E., Pechenizkiy, M., and Mocanu, D. C. Automatic noise filtering with dynamic sparse training in deep reinforcement learning. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, AAMAS '23, pp. 1932–1941, Richland, SC, 2023. International Foundation for Autonomous Agents and Multiagent Systems. ISBN 9781450394321.

Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861–1870. PMLR, 2018.

Hafner, D., Pasukonis, J., Ba, J., and Lillicrap, T. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023.

Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.

Harris, C. R., Millman, K. J., Van Der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., et al. Array programming with numpy. Nature, 585(7825):357–362, 2020.

Hessel, M., Modayil, J., Hasselt, H. V., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M. G., and Silver, D. Rainbow: Combining improvements in deep reinforcement learning. In AAAI, 2018.

Hiraoka, T., Imagawa, T., Hashimoto, T., Onishi, T., and Tsuruoka, Y. Dropout q-functions for doubly efficient reinforcement learning. In International Conference on Learning Representations, 2021.

Hunter, J. D. Matplotlib: A 2d graphics environment. Computing in science & engineering, 9(03):90–95, 2007.

Ioffe, S. and Szegedy, C. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pp. 448–456. pmlr, 2015.

Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell, R. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., et al. Model-based reinforcement learning for atari. International Conference on Learning Representations, 2020.

Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.

Kluyver, T., Ragan-Kelley, B., Pérez, F., Granger, B., Bussonnier, M., Frederic, J., Kelley, K., Hamrick, J., Grout,

11

--- TRANG 12 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

J., Corlay, S., Ivanov, P., Avila, D., Abdalla, S., Willing, C., and Jupyter Development Team. Jupyter Notebooks—a publishing format for reproducible computational workflows. In IOS Press, pp. 87–90. 2016. doi: 10.3233/978-1-61499-649-1-87.

Kostrikov, I., Yarats, D., and Fergus, R. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020.

Kumar, A., Zhou, A., Tucker, G., and Levine, S. Conservative q-learning for offline reinforcement learning. Advances in Neural Information Processing Systems, 33: 1179–1191, 2020.

Kumar, A., Agarwal, R., Ghosh, D., and Levine, S. Implicit under-parameterization inhibits data-efficient deep reinforcement learning. In International Conference on Learning Representations, 2021a. URL https://openreview.net/forum?id=O9bnihsFfXU.

Kumar, A., Agarwal, R., Ma, T., Courville, A., Tucker, G., and Levine, S. Dr3: Value-based deep reinforcement learning requires explicit regularization. arXiv preprint arXiv:2112.04716, 2021b.

Kumar, A., Agarwal, R., Geng, X., Tucker, G., and Levine, S. Offline q-learning on diverse multi-task data both scales and generalizes. In The Eleventh International Conference on Learning Representations, 2022.

Lee, H., Cho, H., Kim, H., Gwak, D., Kim, J., Choo, J., Yun, S.-Y., and Yun, C. Plastic: Improving input and label plasticity for sample efficient reinforcement learning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.

Lee, J. H., Park, W., Mitchell, N. E., Pilault, J., Ceron, J. S. O., Kim, H.-B., Lee, N., Frantar, E., Long, Y., Yazdanbakhsh, A., et al. Jaxpruner: A concise library for sparsity research. In Conference on Parsimony and Learning, pp. 515–528. PMLR, 2024.

Lewandowski, A., Tanaka, H., Schuurmans, D., and Machado, M. C. Curvature explains loss of plasticity. arXiv preprint arXiv:2312.00246, 2023.

Liu, Z., Li, X., Kang, B., and Darrell, T. Regularization matters in policy optimization-an empirical study on continuous control. In International Conference on Learning Representations, 2020.

Livne, D. and Cohen, K. Pops: Policy pruning and shrinking for deep reinforcement learning. IEEE Journal of Selected Topics in Signal Processing, 14(4):789–801, May 2020. ISSN 1941-0484. doi: 10.1109/jstsp.2020.2967566. URL http://dx.doi.org/10.1109/JSTSP.2020.2967566.

Lyle, C., Rowland, M., and Dabney, W. Understanding and preventing capacity loss in reinforcement learning. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=ZkC8wKoLbQ7.

Lyle, C., Zheng, Z., Nikishin, E., Pires, B. A., Pascanu, R., and Dabney, W. Understanding plasticity in neural networks. In Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org, 2023.

Lyle, C., Zheng, Z., Khetarpal, K., van Hasselt, H., Pascanu, R., Martens, J., and Dabney, W. Disentangling the causes of plasticity loss in neural networks. arXiv preprint arXiv:2402.18762, 2024.

McKinney, W. Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython. O'Reilly Media, 1 edition, February 2013. ISBN 9789351100065. URL http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20&path=ASIN/1449319793.

Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D. Human-level control through deep reinforcement learning. Nature, 518(7540): 529–533, February 2015.

Mocanu, D. C., Mocanu, E., Stone, P., Nguyen, P. H., Gibescu, M., and Liotta, A. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature communications, 9(1):2383, 2018.

Nikishin, E., Schwarzer, M., D'Oro, P., Bacon, P.-L., and Courville, A. The primacy bias in deep reinforcement learning. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 16828–16847. PMLR, 17–23 Jul 2022.

Nikishin, E., Oh, J., Ostrovski, G., Lyle, C., Pascanu, R., Dabney, W., and Barreto, A. Deep reinforcement learning with plasticity injection. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=jucDLW6G9l.

Oliphant, T. E. Python for scientific computing. Computing in Science & Engineering, 9(3):10–20, 2007. doi: 10.1109/MCSE.2007.58.

12

--- TRANG 13 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

Ostrovski, G., Castro, P. S., and Dabney, W. The difficulty of passive learning in deep reinforcement learning. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=nPHA8fGicZk.

Ota, K., Jha, D. K., and Kanezaki, A. Training larger networks for deep reinforcement learning. arXiv preprint arXiv:2102.07920, 2021.

Schaul, T., Quan, J., Antonoglou, I., and Silver, D. Prioritized experience replay. CoRR, abs/1511.05952, 2016.

Schmidt, D. and Schmied, T. Fast and data-efficient training of rainbow: an experimental study on atari. arXiv preprint arXiv:2111.10247, 2021.

Schmitt, S., Hudson, J. J., Zidek, A., Osindero, S., Doersch, C., Czarnecki, W. M., Leibo, J. Z., Kuttler, H., Zisserman, A., Simonyan, K., et al. Kickstarting deep reinforcement learning. arXiv preprint arXiv:1803.03835, 2018.

Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Schwarzer, M., Ceron, J. S. O., Courville, A., Bellemare, M. G., Agarwal, R., and Castro, P. S. Bigger, better, faster: Human-level atari with human-level efficiency. In International Conference on Machine Learning, pp. 30365–30380. PMLR, 2023.

Sinha, S., Bharadhwaj, H., Srinivas, A., and Garg, A. D2rl: Deep dense architectures in reinforcement learning. arXiv preprint arXiv:2010.09163, 2020.

Sokar, G., Mocanu, E., Mocanu, D. C., Pechenizkiy, M., and Stone, P. Dynamic sparse training for deep reinforcement learning. arXiv preprint arXiv:2106.04217, 2021.

Sokar, G., Agarwal, R., Castro, P. S., and Evci, U. The dormant neuron phenomenon in deep reinforcement learning. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 32145–32168. PMLR, 23–29 Jul 2023. URL https://proceedings.mlr.press/v202/sokar23a.html.

Song, X., Jiang, Y., Tu, S., Du, Y., and Neyshabur, B. Observational overfitting in reinforcement learning. arXiv preprint arXiv:1912.02975, 2019.

Sutton, R. S. Learning to predict by the methods of temporal differences. Machine Learning, 3(1):9–44, August 1988.

Taiga, A. A., Agarwal, R., Farebrother, J., Courville, A., and Bellemare, M. G. Investigating multi-task pretraining and generalization in reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023.

Tan, Y., Hu, P., Pan, L., Huang, J., and Huang, L. RLx2: Training a sparse deep reinforcement learning model from scratch. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=DJEEqoAq7to.

Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 5026–5033. IEEE, 2012.

van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double q-learning. In Proceedings of the Thirthieth AAAI Conference On Artificial Intelligence (AAAI), 2016, 2016. cite arxiv:1509.06461Comment: AAAI 2016.

Van Hasselt, H., Doron, Y., Strub, F., Hessel, M., Sonnerat, N., and Modayil, J. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018.

Van Hasselt, H. P., Hessel, M., and Aslanides, J. When to use parametric models in reinforcement learning? Advances in Neural Information Processing Systems, 32, 2019.

Van Rossum, G. and Drake Jr, F. L. Python reference manual. Centrum voor Wiskunde en Informatica Amsterdam, 1995.

Vieillard, N., Pietquin, O., and Geist, M. Munchausen reinforcement learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 4235–4246. Curran Associates, Inc., 2020.

Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.

Vischer, M., Lange, R. T., and Sprekeler, H. On lottery tickets and minimal task representations in deep reinforcement learning. In International Conference on Learning Representations, 2021.

Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M., and Freitas, N. Dueling network architectures for deep reinforcement learning. In Proceedings of the 33rd International Conference on Machine Learning, volume 48, pp. 1995–2003, 2016.

13

--- TRANG 14 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

Yarats, D., Fergus, R., and Kostrikov, I. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In 9th International Conference on Learning Representations, ICLR 2021, 2021.

Yu, H., Edunov, S., Tian, Y., and Morcos, A. S. Playing the lottery with rewards and multiple languages: lottery tickets in rl and nlp. In International Conference on Learning Representations, 2019.

Zhang, C., Vinyals, O., Munos, R., and Bengio, S. A study on overfitting in deep reinforcement learning. arXiv preprint arXiv:1804.06893, 2018.

Zhang, H., He, Z., and Li, J. Accelerating the deep reinforcement learning with neural network compression. In 2019 International Joint Conference on Neural Networks (IJCNN), pp. 1–8. IEEE, 2019.

Zhu, M. and Gupta, S. To prune, or not to prune: exploring the efficacy of pruning for model compression, 2017.

14

--- TRANG 15 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

A. Tính khả dụng của mã

Các thí nghiệm của chúng tôi được xây dựng trên mã nguồn mở, chủ yếu từ kho lưu trữ Dopamine. Thư mục gốc cho những điều này là https://github.com/google/dopamine/tree/master/dopamine/, và chúng tôi chỉ định các thư mục con bên dưới (với liên kết có thể nhấp):

• Các tác nhân DQN và Rainbow từ /jax/agents/
• Các tác nhân Atari-100k từ /labs/atari-100k/
• Các script thưa thớt từ JaxPruner /jaxpruner/baselines/dopamine/
• Kiến trúc Resnet từ /labs/offline-rl/jax/networks.py (dòng 108)
• Metric tế bào thần kinh không hoạt động, Reset, ReDo và Weight Decay từ /labs/redo/
• Các thí nghiệm SAC và PPO từ /google-research/rigl/rl/

Đối với các thí nghiệm metric srank, chúng tôi đã sử dụng mã từ:
https://github.com/google-research/google-research/blob/master/generalization_representations_rlaistats22/coherence/coherence_compute.py

B. Lựa chọn game Atari

Hầu hết các thí nghiệm của chúng tôi được chạy với 15 game từ bộ ALE (Bellemare et al., 2013), như được đề xuất bởi Graesser et al. (2022). Tuy nhiên, đối với các tác nhân Atari 100k (tiểu mục 4.3), chúng tôi đã sử dụng tập tiêu chuẩn 26 game (Kaiser et al., 2020) để phù hợp với benchmark. Chúng tôi cũng đã chạy một số thí nghiệm với tập đầy đủ 60 game. Các game cụ thể được nêu chi tiết bên dưới.

Tập con 15 game: MsPacman, Pong, Qbert, Assault, Asterix, BeamRider, Boxing, Breakout, CrazyClimber, DemonAttack, Enduro, FishingDerby, SpaceInvaders, Tutankham, VideoPinball. Theo (Graesser et al., 2022), những game này được chọn để được phân phối đều trong số các game được xếp hạng theo điểm chuẩn hóa con người của DQN trong (Mnih et al., 2015) với ngưỡng thấp hơn khoảng 100% hiệu suất con người.

Tập con 26 game: Alien, Amidar, Assault, Asterix, BankHeist, BattleZone, Boxing, Breakout, ChopperCommand, CrazyClimber, DemonAttack, Freeway, Frostbite, Gopher, Hero, Jamesbond, Kangaroo, Krull, KungFuMaster, MsPacman, Pong, PrivateEye, Qbert, RoadRunner, Seaquest, UpNDown.

Tập 60 game: 26 game ở trên cộng với: AirRaid, Asteroids, Atlantis, BeamRider, Berzerk, Bowling, Carnival, Centipede, DoubleDunk, ElevatorAction, Enduro, FishingDerby, Gravitar, IceHockey, JourneyEscape, MontezumaRevenge, NameThisGame, Phoenix, Pitfall, Pooyan, Riverraid, Robotank, Skiing, Solaris, SpaceInvaders, StarGunner, Tennis, TimePilot, Tutankham, Venture, VideoPinball, WizardOfWor, YarsRevenge, Zaxxon.

C. Mức độ thưa thớt

[THIS IS FIGURE: A series of graphs showing IQM Human Normalized Score vs Number of Frames for different sparsity levels (0, 0.5, 0.95, 0.98, 0.99) across different width scales (1 and 3) for DQN and Rainbow agents]

Hình 15. Đánh giá cách thay đổi tham số độ thưa thớt ảnh hưởng đến hiệu suất cho một kiến trúc nhất định trên tác nhân DQN (Mnih et al., 2015) và Rainbow. Chúng tôi báo cáo kết quả tổng hợp IQM của điểm chuẩn hóa con người trên 15 game.

15

--- TRANG 16 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

0 10 20 30 40
Số khung hình (tính bằng triệu)012345Điểm bình thường hóa theo con người IQM
Tỷ lệ độ rộng = 3
Độ thưa thớt
0
0.9
0.95
0.99

Hình 16. Đánh giá cách thay đổi tham số độ thưa thớt ảnh hưởng đến hiệu suất cho một kiến trúc nhất định, resnet với nhân tử độ rộng 3, trên tác nhân Rainbow. Chúng tôi báo cáo kết quả tổng hợp IQM của điểm chuẩn hóa con người trên 15 game.

D. Mở rộng tỷ lệ phát lại

0.25 0.5 1 2
Tỷ lệ phát lại12345Điểm bình thường hóa theo con người IQM
Tỷ lệ độ rộng = 1
0.25 0.5 1 2
Tỷ lệ phát lại12345
Tỷ lệ độ rộng = 3
Tỉa
0
0.8
0.95
0.99

Hình 17. Mở rộng tỷ lệ phát lại cho kiến trúc resnet (mặc định là 0.25), cho hệ số độ rộng 1 (trái) và hệ số độ rộng 3 (phải) sử dụng tác nhân DQN. Chúng tôi báo cáo hiệu suất trung bình liên tứ phân vị với thanh lỗi chỉ ra khoảng tin cậy 95%. Trên trục x chúng tôi báo cáo giá trị tỷ lệ phát lại.

E. Môi trường MuJoCo

0 50 1000.00.51.0Lợi nhuận×104 HalfCheetah-v2
Độ thưa thớt
0.0
0.9
0.95
0.99
0 50 100
Tương tác môi trường (x1e3)0123×103 Hopper-v2
0 50 100024×103 Humanoid-v2

Hình 18. Đánh giá cách thay đổi tham số độ thưa thớt ảnh hưởng đến hiệu suất của SAC trên ba môi trường MuJoCo khi tăng độ rộng x 5. Chúng tôi báo cáo lợi nhuận trên 10 lần chạy cho mỗi thí nghiệm.

16

--- TRANG 17 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

F. Danh sách siêu tham số

Thiết lập siêu tham số mặc định cho DER (Van Hasselt et al., 2019) và DrQ(ϵ) (Kaiser et al., 2020; Agarwal et al., 2021). Bảng 1 hiển thị các giá trị mặc định cho mỗi siêu tham số trên tất cả các game Atari.

Bảng 1. Thiết lập siêu tham số mặc định cho các tác nhân DER và DrQ(ϵ).

Atari
Siêu tham số DER DrQ(ϵ)
Adam's(ϵ) 0.00015 0.00015
Tỷ lệ học Adam 0.0001 0.0001
Kích thước batch 32 32
Hàm kích hoạt Conv. ReLU ReLU
Độ rộng tích chập 1 1
Hàm kích hoạt dày đặc ReLU ReLU
Độ rộng dày đặc 512 512
Chuẩn hóa Không Không
Hệ số chiết khấu 0.99 0.99
Khám phá ϵ 0.01 0.01
Suy giảm khám phá ϵ 2000 5000
Lịch sử phát lại tối thiểu 1600 1600
Số nguyên tử 51 0
Số lớp tích chập 3 3
Số lớp dày đặc 2 2
Dung lượng phát lại 1000000 1000000
Cắt phần thưởng Đúng Đúng
Chân trời cập nhật 10 10
Chu kỳ cập nhật 1 1
Suy giảm trọng số 0 0
Hành động dính Sai Sai

17

--- TRANG 18 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

Thiết lập siêu tham số mặc định cho DQN (Mnih et al., 2015), Rainbow (Hessel et al., 2018), IQN (Dabney et al., 2018), Munchausen-IQN (Vieillard et al., 2020). Bảng 2 hiển thị các giá trị mặc định cho mỗi siêu tham số trên tất cả các game Atari.

Bảng 2. Thiết lập siêu tham số mặc định cho các tác nhân DQN, Rainbow, IQN, Munchausen-IQN.

Atari
Siêu tham số DQN Rainbow IQN M-IQN
Adam's (ϵ) 1.5e-4 1.5e-4 3.125e-4 3.125e-4
Tỷ lệ học Adam 6.25e-5 6.25e-5 5e-5 5e-5
Kích thước batch 32 32 32 32
Hàm kích hoạt Conv. ReLU ReLU ReLU ReLU
Độ rộng tích chập 1 1 1 1
Hàm kích hoạt dày đặc ReLU ReLU ReLU ReLU
Độ rộng dày đặc 512 512 512 512
Chuẩn hóa Không Không Không Không
Hệ số chiết khấu 0.99 0.99 0.99 0.99
Khám phá ϵ 0.01 0.01 0.01 0.01
Suy giảm khám phá ϵ 250000 250000 250000 250000
Lịch sử phát lại tối thiểu 20000 20000 20000 20000
Số nguyên tử 0 51 - -
Kappa - - 1.0 1.0
Số mẫu tau - - 64 64
Số mẫu tau prime - - 64 64
Số mẫu quantile - - 32 32
Số lớp tích chập 3 3 3 3
Số lớp dày đặc 2 2 2 2
Dung lượng phát lại 1000000 1000000 1000000 1000000
Cắt phần thưởng Đúng Đúng Đúng Đúng
Chân trời cập nhật 1 3 3 3
Chu kỳ cập nhật 4 4 4 4
Suy giảm trọng số 0 0 0 0
Hành động dính Đúng Đúng Đúng Đúng
Tau - - 0 0.03

18

--- TRANG 19 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

Thiết lập siêu tham số mặc định cho các tác nhân ngoại tuyến CQL (Kumar et al., 2020) và CQL+C51 (Kumar et al., 2022). Bảng 3 hiển thị các giá trị mặc định cho mỗi siêu tham số trên tất cả các game Atari.

Bảng 3. Thiết lập siêu tham số mặc định cho các tác nhân CQL và CQL+C51.

Atari
Siêu tham số CQL CQL+C51
Adam's(ϵ) 0.0003125 0.00015
Kích thước batch 32 32
Hàm kích hoạt Conv. ReLU ReLU
Độ rộng tích chập 1 1
Hàm kích hoạt dày đặc ReLU ReLU
Chuẩn hóa Không Không
Độ rộng dày đặc 512 512
Hệ số chiết khấu 0.99 0.99
Tỷ lệ học 0.00005 0.0000625
Số nguyên tử 0 51
Số lớp tích chập 3 3
Số lớp dày đặc 2 2
Dung lượng phát lại cố định 2,500,000 bước 2,500,000 bước
Cắt phần thưởng Đúng Đúng
Chân trời cập nhật 1 3
Chu kỳ cập nhật 1 1
Suy giảm trọng số 0 0
Lược đồ phát lại Đồng đều Đồng đều
Đối đầu Sai Đúng
DQN kép Sai Đúng
Hệ số CQL 0.1 0.1

Thiết lập siêu tham số mặc định cho kiến trúc CNN (Mnih et al., 2015) và ResNet dựa trên Impala (Espeholt et al., 2018). Bảng 4 hiển thị các giá trị mặc định cho mỗi siêu tham số trên tất cả các game Atari.

Bảng 4. Siêu tham số mặc định cho mạng nơ-ron.

Atari
Siêu tham số Kiến trúc CNN (Mnih et al., 2015) ResNet dựa trên Impala (Espeholt et al., 2018)
Lấy mẫu xuống quan sát (84, 84) (84, 84)
Khung hình chồng 4 4
Mạng Q (kênh) 32, 64, 64 32, 64, 64
Mạng Q (kích thước bộ lọc) 8 x 8, 4 x 4, 3 x 3 8 x 8, 4 x 4, 3 x 3
Mạng Q (bước) 4, 2, 1 4, 2, 1
Số khối - 2
Sử dụng max pooling Sai Đúng
Kết nối bỏ qua Sai Đúng
Phần cứng GPU Tesla P100 GPU Tesla P100

19

--- TRANG 20 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

G. Các thí nghiệm bổ sung

Trừ khi được chỉ định khác, trong tất cả các thí nghiệm bên dưới, chúng tôi báo cáo trung bình liên tứ phân vị sau 40 triệu bước môi trường, tổng hợp trên 15 game với 5 seed mỗi game; thanh lỗi chỉ ra khoảng tin cậy bootstrap phân tầng 95% (Agarwal et al., 2021).

G.1. Thí nghiệm với PPO trên MuJoCo

Chúng tôi đã sử dụng triển khai PPO từ (Graesser et al., 2022) và chạy một số thí nghiệm ban đầu với MuJoCo, tăng độ rộng 5x. Như với các thí nghiệm SAC của chúng tôi, chúng ta không thấy thay đổi thực sự trong hiệu suất, có lẽ với một số lợi ích nhẹ trong Humanoid-v2. Một lý do tại sao chúng ta có thể không thấy cải thiện hiệu suất trong cả SAC và PPO là trong các thí nghiệm ALE, tất cả các tác nhân đều sử dụng các lớp tích chập, trong khi đối với các thí nghiệm MuJoCo (nơi chúng tôi chạy SAC và PPO), các mạng chỉ sử dụng các lớp dày đặc.

Dù sao đi nữa, đáng chú ý là Graesser et al. (2022) đã thấy suy giảm với việc tỉa ở width=1 (Hình 16 trong bài báo của họ), với gần như sụp đổ hoàn toàn ở độ thưa thớt 99%. Ngược lại, kết quả của chúng tôi với độ rộng 5x cho thấy hiệu suất mạnh ngay cả ở độ thưa thớt 99%.

0 100 20024Lợi nhuận×103 Walker2d-v2
Độ thưa thớt
0.0
0.9
0.95
0.99
0 100 20024×103 Ant-v2
0 100 200
Tương tác môi trường (x5e3)24×103 HalfCheetah-v2
0 100 200123×103 Hopper-v2
0 100 200246×103 Humanoid-v2

Hình 19. Proximal Policy Optimization (PPO) (Schulman et al., 2017) trên các môi trường MuJoCo khi tăng độ rộng x5. Chúng tôi báo cáo lợi nhuận trên 10 lần chạy cho mỗi thí nghiệm.

G.2. Thí nghiệm với IQN và M-IQN

Trong khi Rainbow vẫn là một tác nhân cạnh tranh trong ALE và cả DQN và Rainbow vẫn thường xuyên được sử dụng như các baseline trong các công trình gần đây, khám phá các tác nhân mới hơn là một yêu cầu hợp lý. Để giải quyết điều này, chúng tôi đã chạy các thí nghiệm với Implicit Quantile Networks (IQN) (Dabney et al., 2018) và Munchausen-IQN (Vieillard et al., 2020) với độ rộng 1 và 3; phù hợp với các phát hiện của đệ trình chúng tôi, chúng tôi quan sát lợi ích đáng kể khi sử dụng việc tỉa.

0 10 20 30 40
Số khung hình (tính bằng triệu)0246Điểm bình thường hóa theo con người IQM
Tỷ lệ độ rộng = 1
Độ thưa thớt
0
0.9
0.95
0.99
0 10 20 30 40
Số khung hình (tính bằng triệu)0246
Tỷ lệ độ rộng = 3

Hình 20. IQN (Implicit Quantile Networks)(Dabney et al., 2018) với kiến trúc ResNet (với hệ số độ rộng 1 và 3).

20

--- TRANG 21 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

0 10 20 30 40
Số khung hình (tính bằng triệu)02468Điểm bình thường hóa theo con người IQM
Tỷ lệ độ rộng = 1
Độ thưa thớt
0
0.9
0.95
0.99
0 10 20 30 40
Số khung hình (tính bằng triệu)02468
Tỷ lệ độ rộng = 3

Hình 21. M-IQN (Munchausen-Implicit Quantile Networks)(Vieillard et al., 2020) với kiến trúc ResNet (với hệ số độ rộng 1 và 3).

G.3. So sánh với RigL

Chúng tôi đã chạy so sánh với RigL (Evci et al., 2020). Trong khi RigL có thể có hiệu quả đến một mức độ nào đó, nó không thể khớp với hiệu suất của việc tỉa.

0 10 20 30 40
Số khung hình (tính bằng triệu)01234Điểm bình thường hóa theo con người IQM
Tỷ lệ độ rộng = 1
Độ thưa thớt
0
0.9
0.95
0.99
0 10 20 30 40
Số khung hình (tính bằng triệu)01234
Tỷ lệ độ rộng = 3

Hình 22. The Rigged Lottery (RigL) (Evci et al., 2020) cho DQN với kiến trúc ResNet (với hệ số độ rộng 1 và 3).

G.4. Thay đổi ϵ của Adam

Giá trị mặc định cho ϵ của Adam là 1.5e−5; chúng tôi chạy các thí nghiệm bằng cách chia/nhân giá trị này với 3 (5e−5 và 4.5e−4, tương ứng). Trong tất cả các trường hợp này, việc tỉa duy trì lợi thế đáng kể so với baseline dày đặc.

0 10 20 30 40
Số khung hình (tính bằng triệu)0123456Điểm bình thường hóa theo con người IQM
Adam ϵ = 1.5e-05
Độ thưa thớt
0
0.9
0.95
0.99
0 10 20 30 40
Số khung hình (tính bằng triệu)0123456
Adam ϵ = 5e-05
0 10 20 30 40
Số khung hình (tính bằng triệu)0123456
Adam ϵ = 4.5e-04

Hình 23. Epsilon (ϵ) của Adam (Kingma & Ba, 2014) cho DQN với kiến trúc ResNet và nhân tử độ rộng 3.

21

--- TRANG 22 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

G.5. Quét ngưỡng τ của ReDo

Tham số này (được giới thiệu trong Định nghĩa 3.1 của (Sokar et al., 2023)) xác định ngưỡng để xác định trạng thái không hoạt động của tế bào thần kinh. Sokar et al. (2023) đề xuất sử dụng 0.1 với mạng CNN. Vì chúng tôi đang sử dụng kiến trúc mạng Impala, chúng tôi đã thử nghiệm ba giá trị bổ sung: (0, 0.025, 0.3). Chúng tôi thấy rằng 0.1, như được sử dụng trong Hình 12 của đệ trình chúng tôi, mang lại hiệu suất tốt nhất.

0 10 20 30 40
Số khung hình (tính bằng triệu)0.00.51.01.52.02.53.0
Tỷ lệ độ rộng = 3
Ngưỡng (τ)
0
0.025
0.1
0.3

Hình 24. Thay đổi ngưỡng τ của ReDo (Sokar et al., 2023) cho DQN với kiến trúc ResNet và nhân tử độ rộng 3.

G.6. Tần suất đặt lại mạng

Chúng tôi thay đổi tần suất đặt lại mạng (Nikishin et al., 2022) để đánh giá liệu điều này có thể giúp giảm thiểu mất hiệu suất khi tăng độ rộng mạng hay không. Trong khi việc đặt lại ít thường xuyên hơn (mỗi 250000 bước so với giá trị mặc định 100000) cải thiện hiệu suất một chút, nó vẫn hoạt động kém đáng kể so với baseline và phương pháp tỉa.

0 10 20 30 40
Số khung hình (tính bằng triệu)0.00.51.01.52.02.53.0Điểm bình thường hóa theo con người IQM
Tỷ lệ độ rộng = 3
Chu kỳ đặt lại
20000
100000
250000
500000

Hình 25. Thay đổi chu kỳ đặt lại (Nikishin et al., 2022) cho DQN với kiến trúc ResNet và nhân tử độ rộng 3.

22

--- TRANG 23 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

G.7. Lớp để đặt lại

Trong bài báo của chúng tôi, chúng tôi đã tuân theo phương pháp của Nikishin et al. (2022) và Sokar et al. (2023) chỉ đặt lại lớp cuối cùng. Chúng tôi đã khám phá việc đặt lại các lớp khác nhau, nhưng thấy rằng nó không dẫn đến sự khác biệt hiệu suất đáng kể.

0 10 20 30 40
Số khung hình (tính bằng triệu)0.00.51.01.52.02.53.0Điểm bình thường hóa theo con người IQM
Tỷ lệ độ rộng = 3
Lớp cuối cùng và áp cuối
Lớp áp cuối
Lớp cuối cùng

Hình 26. Đặt lại các lớp khác nhau (Nikishin et al., 2022), DQN với kiến trúc ResNet và nhân tử độ rộng 3.

G.8. Thay đổi suy giảm trọng số

Chúng tôi đã chạy quét qua các giá trị sau 10e−6, 10e−5, 10e−4, 10e−3, và 10e−2, sử dụng kiến trúc Impala với hệ số độ rộng 3. Hiệu suất tốt nhất được đạt với 10e−5, đây là giá trị được đề xuất bởi Sokar et al. (2023), và giá trị được sử dụng trong Hình 12 của đệ trình chúng tôi.

0 10 20 30 40
Số khung hình (tính bằng triệu)0.00.51.01.52.02.53.0Điểm bình thường hóa theo con người IQM
Tỷ lệ độ rộng = 3
Suy giảm trọng số
1e-06
1e-05
0.0001
0.001
0.01

Hình 27. Suy giảm trọng số (WD) cho DQN với kiến trúc ResNet với hệ số độ rộng 3.

23

--- TRANG 24 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

G.9. Thay đổi tỷ lệ học

Tỷ lệ học mặc định cho DQN là 6.25e−5. Như được đề xuất bởi người đánh giá, chúng tôi đã chạy các thí nghiệm với tỷ lệ học chia cho hệ số tỷ lệ độ rộng (vậy 2.08e−5 cho hệ số độ rộng 3, và 1.25e−5 cho hệ số độ rộng 5). Những tỷ lệ học này cải thiện hiệu suất của baseline, nhưng vẫn bị vượt qua bởi việc tỉa. Những kết quả này phù hợp với luận điểm của bài báo: việc tỉa có thể phục vụ như một cơ chế thả vào để tăng hiệu suất tác nhân.

0 10 20 30 40
Số khung hình (tính bằng triệu)0123456Điểm bình thường hóa theo con người IQM
Adam lr = 2.08e-05
Độ thưa thớt
0
0.9
0.95
0.99
0 10 20 30 40
Số khung hình (tính bằng triệu)0123456
Adam lr = 1.25e-05

Hình 28. Đánh giá tỷ lệ học cho tác nhân DQN với kiến trúc ResNet (với hệ số độ rộng 3 bên trái và 5 bên phải). Những giá trị tỷ lệ học này tương ứng với việc chia tỷ lệ học mặc định cho hệ số chúng tôi đã sử dụng để khuếch đại kích thước của mạng nơ-ron. Các đường đứt nét chỉ ra hiệu suất cuối cùng cho mạng dày đặc (xanh) và thưa thớt 0.95% (cam) khi sử dụng tỷ lệ học mặc định (lr: 6.25e−5).

G.10. Thay đổi kích thước batch

Kích thước batch mặc định là 32, và chạy các thí nghiệm với kích thước batch 16 và 64. Trong tất cả các trường hợp, việc tỉa duy trì lợi thế mạnh mẽ của nó.

0 10 20 30 40
Số khung hình (tính bằng triệu)012345
Kích thước batch = 16
0 10 20 30 40
Số khung hình (tính bằng triệu)012345
Kích thước batch = 64

Hình 29. Kích thước batch (Ceron et al., 2023) cho DQN với kiến trúc ResNet và nhân tử độ rộng 3.

24

--- TRANG 25 ---
Trong học tăng cường sâu dựa trên giá trị, một mạng được tỉa là một mạng tốt

G.11. Thay đổi chân trời cập nhật

Chúng tôi đã khám phá việc sử dụng chân trời cập nhật 3 cho DQN (mặc định là 1) và thấy rằng việc tỉa vẫn duy trì lợi thế của nó.

0 10 20 30 40
Số khung hình (tính bằng triệu)0123Điểm bình thường hóa theo con người IQM
n-step = 3
Độ thưa thớt
0
0.9
0.95
0.99

Hình 30. Lợi nhuận đa bước (Sutton, 1988) cho DQN với kiến trúc ResNet và nhân tử độ rộng 3.

25
