# 2402.01391.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2402.01391.pdf
# Kích thước tệp: 652507 byte

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
StepCoder: Cải thiện Sinh code với
Học tăng cường từ Phản hồi của Trình biên dịch
Shihan Dou1*†, Yan Liu1∗, Haoxiang Jia2, Limao Xiong1, Enyu Zhou1, Wei Shen1,
Junjie Shan3, Caishuang Huang1, Xiao Wang1, Xiaoran Fan1, Zhiheng Xi1,
Yuhao Zhou1, Tao Ji1, Rui Zheng1†, Qi Zhang1†, Xuanjing Huang1, Tao Gui1†
1Fudan NLP Lab, Fudan University, China
2Huazhong University of Science and Technology, China
3KTH Royal Institute of Technology, Sweden
Tóm tắt
Sự tiến bộ của các mô hình ngôn ngữ lớn
(LLM) đã thúc đẩy đáng kể lĩnh vực
sinh code. Các nghiên cứu trước đây đã tích hợp học
tăng cường (RL) với phản hồi từ trình biên dịch để khám phá
không gian đầu ra của LLM nhằm nâng cao chất lượng sinh code. Tuy nhiên,
code dài được sinh ra bởi LLM để đáp ứng
các yêu cầu phức tạp của con người khiến việc khám phá RL trở thành thách thức. Ngoài ra, vì các
bài kiểm tra đơn vị có thể không bao phủ code phức tạp,
việc tối ưu hóa LLM bằng cách sử dụng những đoạn code
không được thực thi này là không hiệu quả. Để giải quyết những
thách thức này, chúng tôi giới thiệu StepCoder, một
khung RL mới cho sinh code, bao gồm
hai thành phần chính: CCCS giải quyết thách thức khám phá
bằng cách chia nhỏ tác vụ sinh code dài thành một Chương trình giảng dạy
của các Tác vụ phụ Hoàn thành Code, trong khi FGO chỉ
tối ưu hóa mô hình bằng cách che đi các đoạn code không được
thực thi để cung cấp Tối ưu hóa Chi tiết. Ngoài ra, chúng tôi còn xây dựng
bộ dữ liệu APPS+ cho huấn luyện RL, được
xác minh thủ công để đảm bảo tính đúng đắn
của các bài kiểm tra đơn vị. Kết quả thí nghiệm cho thấy
phương pháp của chúng tôi cải thiện khả năng khám phá
không gian đầu ra và vượt trội hơn các phương pháp tiên tiến
trong các điểm chuẩn tương ứng. Bộ dữ liệu APPS+ và StepCoder của chúng tôi có sẵn trực tuyến1.

1 Giới thiệu
Sinh code hoặc tổng hợp chương trình nhằm tự động
sinh ra code nguồn tuân thủ một
yêu cầu lập trình cụ thể, thường được
mô tả bằng ngôn ngữ tự nhiên [36;7]. Gần đây, với sự phát triển của các mô hình ngôn ngữ lớn
(LLM), các kỹ thuật dựa trên LLM [15;37;
23] đã thể hiện khả năng ấn tượng trong
sinh code. Tuy nhiên, những thách thức vẫn tồn tại trong việc căn chỉnh

*Đóng góp ngang nhau.
†Liên hệ: shdou21@m.fudan.edu.cn, {rzhen
g20, qz, tqui}@fudan.edu.cn
1https://github.com/Ablustrund/APPS_Plus

các mô hình này với các yêu cầu phức tạp của con người
[2;10;27], cho thấy vẫn còn khoảng cách trong việc đáp ứng đầy đủ
kỳ vọng của người dùng.

Trong bối cảnh này, việc học từ phản hồi của trình biên dịch
thể hiện tiềm năng ấn tượng trong việc cải thiện
hiểu biết về các yêu cầu phức tạp của con người
và chất lượng của code được sinh ra [14]. Phản hồi
từ kết quả biên dịch và thực thi này có vai trò
quan trọng trong việc xác định trực tiếp tính đúng đắn chức năng
của chương trình [17;39]. Các nhà nghiên cứu
[20;33] giới thiệu học tăng cường (RL) và tận dụng
phản hồi từ trình biên dịch từ các bài kiểm tra đơn vị như một thước đo
phần thưởng để hướng dẫn việc khám phá không gian đầu ra
của LLM. Ý định là để mô hình chính sách
ưa thích các hành động mang lại phần thưởng cao hơn một cách
ngày càng tăng. Tuy nhiên, việc tối ưu hóa LLM
cho sinh code thông qua RL đưa ra một số rào cản. Thứ nhất, sự phức tạp ngày càng tăng của các yêu cầu của con người
thường dẫn đến việc sinh ra các chuỗi code dài hơn,
điều này khiến việc khám phá gặp khó khăn [9;13]. Thứ hai, trong các trường hợp mà một bài kiểm tra đơn vị
không thể bao phủ code phức tạp, có thể xuất hiện các đoạn code
không được thực thi mà không liên quan đến
phần thưởng. Việc tối ưu hóa dựa trên
toàn bộ chuỗi code có thể không chính xác. Ngoài ra, phân tích của chúng tôi tiết lộ những hạn chế về chất lượng
trong các bộ dữ liệu hiện có như APPS [10] cho huấn luyện RL,
điều này cản trở việc học chính xác từ phản hồi
trình biên dịch thông qua RL.

Để giải quyết những thách thức này, trước tiên chúng tôi giới thiệu
StepCoder, một khung sáng tạo được phát triển để
nâng cao sinh code thông qua học tăng cường.
StepCoder tích hợp hai thành phần chính: Chương trình giảng dạy
về các Tác vụ phụ Hoàn thành Code (CCCS) và Tối ưu hóa Chi tiết (FGO).
CCCS được thiết kế để giảm bớt những phức tạp liên quan đến việc khám phá trong sinh code, trong khi
FGO được thiết kế để cung cấp các chiến lược tối ưu hóa
chính xác và hiệu quả hơn. Cụ thể, CCCS
sử dụng chiến lược từng bước để chia nhỏ các vấn đề
khám phá phức tạp (tức là sinh code)

arXiv:2402.01391v2 [cs.SE] 5 Feb 2024

--- TRANG 2 ---
thành một chương trình giảng dạy của các tác vụ phụ dễ hơn (tức là hoàn thành
code). Khi quá trình huấn luyện tiến triển, độ khó
của các tác vụ hoàn thành code tăng lên bằng cách tăng
phần code cần hoàn thành.
Cuối cùng, mục tiêu là để mô hình phát triển đến
giai đoạn có thể sinh code hiệu quả chỉ
từ các yêu cầu của con người, từ đó hoàn thành mục tiêu
huấn luyện ban đầu về sinh code. Mặt khác, 
hiểu biết chính của FGO là các đoạn code
không được thực thi trong bài kiểm tra đơn vị không đóng góp
vào tính toán phần thưởng cuối cùng. Do đó, FGO
sử dụng kỹ thuật che động để che các đoạn
không được thực thi từ đánh giá bài kiểm tra đơn vị, đảm bảo
rằng mô hình được tối ưu hóa chỉ sử dụng các đoạn code
liên quan.

Tiếp theo, nỗ lực của chúng tôi bao gồm việc phát triển
APPS+, một bộ dữ liệu có chất lượng vượt trội
được tuyển chọn đặc biệt cho sinh code. APPS+ được
thiết kế tỉ mỉ để loại trừ các đoạn code
có lỗi cú pháp, không liên quan đến vấn đề
quy định, hoặc không tạo ra bất kỳ đầu ra nào. Ngoài ra, chúng tôi đã thực hiện các biện pháp để chuẩn hóa
định dạng của đầu vào và đầu ra trong các bài kiểm tra đơn vị để
đảm bảo so sánh đầu ra xác định.

Chúng tôi đánh giá hiệu quả của các LLM phổ biến
trên APPS+. Kết quả cho thấy mặc dù LLM
có những cải tiến tiến bộ, chúng gặp khó khăn
với các yêu cầu phức tạp của con người. Chúng tôi
tiếp tục đánh giá phương pháp của mình trên một số điểm chuẩn
được sử dụng rộng rãi bao gồm MBPP [2] và Hu-
manEval [3]. Kết quả thí nghiệm cho thấy
StepCoder giảm bớt hiệu quả độ khó khám phá
trong sinh code, vượt trội hơn các phương pháp
dựa trên học tăng cường khác về hiệu quả.

Những đóng góp chính của bài báo của chúng tôi như sau:
• Chúng tôi giới thiệu StepCoder, một phương pháp huấn luyện
mới thông qua RL, bao gồm CCCS và FGO.
CCCS làm cho việc khám phá dễ dàng hơn bằng cách
chia nhỏ các mục tiêu phức tạp thành chương trình giảng dạy
các mục tiêu phụ. FGO cung cấp tối ưu hóa
chi tiết bằng cách chỉ sử dụng code được thực thi
trong các bài kiểm tra đơn vị.
• Chúng tôi xây dựng APPS+, một bộ dữ liệu chất lượng cao
được thiết kế cho sinh code. APPS+ cung cấp
một đánh giá nghiêm ngặt hơn về khả năng của LLM
và nền tảng để giới thiệu học tăng cường
trong giai đoạn huấn luyện.
• Thí nghiệm cho thấy StepCoder có thể cải thiện
hiệu quả và hiệu suất khám phá và vượt trội
hơn các phương pháp khác.

import random
def test():
    ...
    for _ in range(int(input())):
        …
        rows[0] = p[::2]
        rows[1] = p[1::2]
        if sign(rows[0][0]) != sign(rows[1][0]):
            print(0)
            continue
        for r in range(2, max_rows):
            for n in range(max_col-1):
                rows[r][n] = rows[r -1][0] * rows[r -2][n + 1] - rows[r -2][0] * rows[r -1][n + 1]
        last = sign(rows[0][0])
        flag = 1
        for i in range(1, len(rows)):
            curr = sign(rows[i][0])
            if rows[r] == [0 for _ in range(max_col)]:
                for n in range(max_col):
                    rows[r][n] = rows[r -1][n] * (max_pow + 4 - (r + 1) - 2 * (n + 1))
            elif rows[i][0] == 0:
                if any([x != 0 for x in rows[i]]):
                    flag = 0
                    break
                else:
                    curr = last
            if curr != last:
                flag = 0
                break
            last = curr

: câu lệnh điều kiện
: code được thực thi
: code không được thực thi

Hình 1: Giải pháp chuẩn của một thể hiện trong
bộ dữ liệu APPS. Chúng tôi thu thập các câu lệnh điều kiện
bằng cách phân tích cây cú pháp trừu tượng của chúng, và một số câu lệnh
điều kiện được đánh dấu bằng hộp nét đứt màu xám. Khi nhập
s= [1 \n10 12 1 5 3 \n], chỉ 75% đoạn code được thực thi,
được đánh dấu bằng nền màu xanh lá cây.

2 Động lực
Trong phần này, chúng tôi minh họa rõ ràng các thách thức
mà học tăng cường phải đối mặt trong sinh code
sử dụng một ví dụ đơn giản từ APPS [10], được
sử dụng rộng rãi cho huấn luyện RL trong sinh code.

Vấn đề khám phá của RL trong sinh code.
Các phương pháp khám phá đóng vai trò quan trọng trong
việc giải quyết các vấn đề chuỗi phức tạp nhưng phần thưởng thưa thớt [43;13]. Khi một mô hình chính sách khám phá một
quỹ đạo với lợi nhuận cao, nó trải qua tối ưu hóa,
khiến nó có xu hướng thực hiện các hành động tương tự trong
tương lai [41; 28].

Xem xét code được hiển thị trong Hình 1, nhằm
hoàn thành một yêu cầu nhất định của con người. Trước tiên chúng tôi
thu thập các câu lệnh điều kiện (CS) được
chỉ ra bởi hộp nét đứt bằng cách phân tích cây cú pháp
trừu tượng của nó. Câu lệnh điều kiện giới thiệu các đường dẫn
độc lập mới, tăng độ phức tạp của chương trình [32]. Giả sử Pθ(CSi) biểu thị
xác suất mà mô hình chính sách với tham số θ
hoàn thành câu lệnh điều kiện thứ i. Xác suất
mà mô hình chính sách sinh ra đúng code này theo
yêu cầu của con người có thể được

--- TRANG 3 ---
biểu diễn như sau:
P ∝ Po ∏(i=1 to 3) Pθ(CSi), (1)

trong đó Po là xác suất của các đoạn code khác
ngoại trừ code được gán nhãn trong hình. Thông thường, chúng tôi
khởi tạo mô hình chính sách với mô hình SFT trong
các tác vụ sinh chuỗi để tạo điều kiện thuận lợi cho việc khám phá
dễ dàng hơn [26; 45]. Tuy nhiên, hiệu suất hạn chế
của mô hình SFT trong sinh code vẫn dẫn đến
xác suất Pθ(CSi) ở giá trị thấp [33;27]. Sự
phức tạp ngày càng tăng của các yêu cầu của con người trong
các tác vụ sinh code thường dẫn đến sự gia tăng
tương ứng về số lượng câu lệnh điều kiện.
Sự leo thang này có thể dẫn đến giảm đáng kể
xác suất Pθ(CSi), có khả năng dẫn đến P
giảm theo cấp số nhân. Tình huống như vậy làm
trầm trọng thêm những thách thức liên quan đến việc khám phá trong
các mô hình ngôn ngữ lớn. Một phương pháp thay thế để
tạo điều kiện thuận lợi cho việc khám phá là thông qua định hình phần thưởng, một
kỹ thuật mà các nhà thiết kế giới thiệu phần thưởng
một cách nhân tạo thường xuyên hơn [13]. Tuy nhiên, phương pháp này
gặp phải một hạn chế đáng kể trong bối cảnh
ứng dụng của chúng tôi. Cụ thể, trong các tác vụ sinh code
sử dụng phản hồi bài kiểm tra đơn vị, phần thưởng chỉ có thể
được thu thập sau khi thực thi code
được sinh ra hoàn chỉnh. Do đó, việc khám phá
các quỹ đạo có lợi nhuận cao trong các tác vụ với chuỗi
phức tạp và phần thưởng thưa thớt đặt ra một thách thức đáng kể
trong việc tối ưu hóa mô hình chính sách.

Vấn đề tối ưu hóa của RL trong sinh code.
Trước tiên chúng tôi giới thiệu quá trình tinh chỉnh RL
trong sinh code. Chính thức, đối với một mô hình chính sách
học được πθ với tham số θ, chúng tôi coi việc dự đoán
mỗi token như một hành động a được thực hiện bởi πθ theo
các chuỗi token lịch sử. Các chuỗi token lịch sử
có thể được xem như trạng thái s. Cho một
yêu cầu của con người x, chúng tôi ký hiệu code giải pháp
y được sinh ra bởi πθ như một episode, và r(x, y) là
hàm phần thưởng từ trình biên dịch dựa trên
biên dịch và thực thi. Cập nhật các tham số của
πθ bằng cách sử dụng thuật toán gradient policy [35] có thể được
biểu diễn như sau:

max θ E(x,y)∼Dπθ [∑t At π log(yt|y1:t−1, x;θ)] (2)

trong đó Aπ là lợi thế được tính bởi Generalized
Advantage Estimator (GAE) [29] từ
phần thưởng r, để giảm tính biến đổi của dự đoán.

Trong sinh code, phần thưởng phụ thuộc vào
tính đúng đắn của mẫu bài kiểm tra đơn vị, điều này chỉ
liên quan đến đoạn code đang được thực thi. Ví dụ, như được hiển thị trong Hình 1, khi đầu vào cho
hàm là [1\n10 12 1 5 3 \n], 75% đoạn code
được thực thi, được đánh dấu bằng hộp nét đứt
màu xanh lá cây. Điều này cho thấy một số hành động trong
code không liên quan đến phần thưởng, dẫn đến
lợi thế không chính xác. Do đó, việc tối ưu hóa
mô hình chính sách πθ với tất cả các hành động là không hiệu quả
khi sử dụng Phương trình 2.

3 Phương pháp
Trong phần này, chúng tôi trình bày chi tiết phương pháp
của StepCoder, cung cấp việc khám phá dễ dàng hơn
và tối ưu hóa chi tiết cho RL trong
sinh code, tương ứng, như được hiển thị trong Hình 2.

3.1 Kiến thức cơ bản
Giả sử D={(xi, yi, ui, ei)}N i=0 là bộ dữ liệu huấn luyện
cho sinh code, trong đó x, y, u biểu thị
yêu cầu của con người (tức là mô tả tác vụ),
giải pháp chuẩn và các mẫu bài kiểm tra đơn vị,
tương ứng. ei={stj, enj}Ei j=0 là danh sách các câu lệnh
điều kiện bằng cách tự động phân tích
cây cú pháp trừu tượng của giải pháp chuẩn yi,
trong đó st và en biểu thị vị trí bắt đầu và
vị trí kết thúc của các câu lệnh, tương ứng. e được
sắp xếp theo thứ tự tăng dần dựa trên vị trí bắt đầu
st. Đối với một yêu cầu của con người x, giải pháp chuẩn y
của nó có thể được biểu diễn như {at}T t=0. Trong sinh code,
cho một yêu cầu của con người x, các trạng thái cuối cùng
là tập hợp các code vượt qua các bài kiểm tra đơn vị u.

3.2 StepCoder
StepCoder tích hợp hai thành phần chính: CCCS
và FGO. CCCS được thiết kế để chia các tác vụ sinh code
thành một chương trình giảng dạy của các tác vụ phụ hoàn thành
code. Nó có thể giảm bớt thách thức khám phá
trong RL. FGO được thiết kế đặc biệt cho các tác vụ
sinh code để cung cấp tối ưu hóa chi tiết
bằng cách chỉ tính toán loss của các đoạn code được thực thi.

CCCS. Trong sinh code, giải pháp cho một
yêu cầu phức tạp của con người thường bao gồm một chuỗi
hành động dài được thực hiện bởi mô hình chính sách. Trong khi đó, phản hồi từ trình biên dịch bị trễ
và thưa thớt, tức là mô hình chính sách chỉ nhận được
phần thưởng sau khi sinh ra toàn bộ code. Trong tình huống
này, việc khám phá là khó khăn. Cốt lõi của phương pháp
chúng tôi là chia nhỏ chuỗi khám phá dài như vậy
thành một chương trình giảng dạy của các tác vụ phụ ngắn, dễ
khám phá. Chúng tôi đơn giản hóa sinh code
thành các tác vụ phụ hoàn thành code. Những tác vụ phụ này

--- TRANG 4 ---
Học tăng cường từ Phản hồi Trình biên dịch Vanilla            StepCoder

Bước 1
Cuối cùng......

Yêu cầu của con người    LLM    Trình biên dịch    Phần thưởng

FGO    CCCS    Code được sinh ra    Che    Che

!log(yt|y1:t−1,x!)
!log(yt|y1:t−1,x!)

Hoàn thành Code
Sinh Code
Hoàn thành Code
Sinh Code

Một phần của giải pháp chuẩn

Hình 2: Tổng quan về phương pháp của chúng tôi. Trong sinh code, môi trường với phần thưởng thưa thớt và trễ và yêu cầu phức tạp của con người bao gồm một chuỗi dài khiến việc khám phá trở nên thách thức đối với RL Vanilla. Trong CCCS, chúng tôi chia nhỏ một vấn đề khám phá phức tạp thành một chương trình giảng dạy của các tác vụ phụ. Sử dụng một phần của giải pháp chuẩn làm prompt cho phép LLM khám phá bắt đầu từ các chuỗi đơn giản. Việc tính toán phần thưởng chỉ liên quan đến các đoạn code được thực thi, và việc tối ưu hóa LLM với toàn bộ code (tức là ) là không chính xác. Trong FGO, chúng tôi che các token không được thực thi (tức là ) trong các bài kiểm tra đơn vị và chỉ tính toán hàm loss sử dụng các token được thực thi (tức là ) để cung cấp tối ưu hóa chi tiết.

được xây dựng tự động từ giải pháp chuẩn
trong bộ dữ liệu huấn luyện.

Xem xét một yêu cầu của con người x, sớm trong
giai đoạn huấn luyện của CCCS, điểm bắt đầu s∗ của
việc khám phá là các trạng thái gần với các trạng thái cuối cùng. Cụ thể, chúng tôi cung cấp yêu cầu của con người x và
phần đầu của giải pháp chuẩn xp={ai}s∗ i=0,
và mô hình chính sách được huấn luyện để hoàn thành
code dựa trên x′= (x, xp). Gọi ŷ là chuỗi
kết hợp của xp và quỹ đạo đầu ra τ,
tức là ŷ= (xp, τ). Mô hình phần thưởng cung cấp
phần thưởng r theo tính đúng đắn của đoạn code
τ với ŷ làm đầu vào, trong đó chúng tôi sử dụng cùng
cài đặt như các phương pháp trước [14;33] như sau:

r(x′,ŷ) = {
    +1, nếu ŷ vượt qua tất cả bài kiểm tra đơn vị
    −0.3, nếu ŷ thất bại bất kỳ bài kiểm tra đơn vị nào
    −0.6, nếu ŷ xảy ra lỗi runtime
    −1, nếu ŷ xảy ra lỗi biên dịch
} (3)

Chúng tôi sử dụng thuật toán Proximal Policy Optimization (PPO)
[30] để tối ưu hóa mô hình chính sách πθ bằng
cách sử dụng phần thưởng r và quỹ đạo τ. Trong giai đoạn tối ưu hóa, đoạn code của giải pháp chuẩn xp được sử dụng để cung cấp prompt được che đi,
sao cho nó không đóng góp vào gradient cho
việc cập nhật mô hình chính sách πθ. CCCS tối ưu hóa
mô hình chính sách πθ bằng cách tối đa hóa hàm mục tiêu
như sau:

Objective (θ) = E(x′,ŷ)∼Dπθ[r(x′,ŷ)
−β log(πθ(ŷ|x′))/πref(ŷ|x′)] (4)

trong đó πref là mô hình tham chiếu trong PPO, được
khởi tạo bởi mô hình SFT.

Khi quá trình huấn luyện tiến triển, điểm bắt đầu s∗
của việc khám phá dần dần di chuyển về phía
đầu của giải pháp chuẩn. Cụ thể, chúng tôi đặt
ngưỡng ρ cho mỗi mẫu huấn luyện. Mỗi lần
tỷ lệ chính xác tích lũy của các đoạn code được sinh ra bởi πθ lớn hơn ρ, chúng tôi di chuyển
điểm bắt đầu về phía đầu. Trong các giai đoạn
sau của huấn luyện, việc khám phá của phương pháp chúng tôi
tương đương với quá trình khám phá của
học tăng cường ban đầu, tức là s∗= 0, trong đó
mô hình chính sách sinh code chỉ sử dụng yêu cầu
của con người làm đầu vào.

--- TRANG 5 ---
Điểm bắt đầu s∗ được lấy mẫu tại vị trí
đầu của các câu lệnh điều kiện để hoàn thành
các đoạn code chưa viết còn lại. Cụ thể,
một chương trình với số lượng câu lệnh điều kiện
lớn hơn dẫn đến tăng các đường dẫn độc lập,
dẫn đến độ phức tạp logic cao hơn [32].
Độ phức tạp này đòi hỏi việc lấy mẫu thường xuyên hơn
để cải thiện chất lượng huấn luyện, trong khi
các chương trình có ít câu lệnh điều kiện hơn cần
ít lấy mẫu hơn. Phương pháp lấy mẫu này cho phép
lấy mẫu cân bằng và đại diện của các cấu trúc code,
đáp ứng cả các cấu trúc ngữ nghĩa phức tạp và đơn giản
trong bộ dữ liệu huấn luyện. Để tăng tốc
giai đoạn huấn luyện, chúng tôi đặt số lượng chương trình giảng dạy
của mẫu thứ i bằng ⌈√Ei⌉, trong đó Ei là
số câu lệnh điều kiện của nó. Bước nhảy
của chương trình giảng dạy huấn luyện của mẫu thứ i là ⌈Ei/⌈√Ei⌉⌉
thay vì một.

Hiểu biết chính của CCCS có thể được tóm tắt
như sau: 1) Dễ dàng khám phá từ các trạng thái gần
mục tiêu (tức là các trạng thái cuối cùng). 2) Khám phá bắt đầu
từ các trạng thái xa mục tiêu là thách thức,
nhưng nó trở nên dễ dàng hơn khi có thể tận dụng các trạng thái
đã học cách đạt được mục tiêu.

FGO. Mối quan hệ giữa phần thưởng và hành động
trong sinh code khác với các tác vụ
học tăng cường khác như Atari [25;19]. Trong sinh code,
chúng ta có thể loại trừ một tập hợp các hành động không liên quan
đến việc tính toán phần thưởng trong code được sinh ra.
Cụ thể, như đã đề cập trong Phần 2, đối với một bài kiểm tra
đơn vị, phản hồi từ trình biên dịch chỉ liên quan
đến các đoạn code đang được thực thi. Tuy nhiên, trong
các mục tiêu tối ưu hóa RL vanilla, như được hiển thị trong
Phương trình 4, tất cả các hành động của quỹ đạo được tham gia
vào việc tính toán gradient được sử dụng trong cập nhật chính sách,
điều này không chính xác.

Để cải thiện độ chính xác của tối ưu hóa, chúng tôi
che các hành động (tức là token) không được thực thi
trong các bài kiểm tra đơn vị khi tính toán loss để cập nhật
mô hình chính sách. Thuật toán đầy đủ của CCCS và
FGO được trình bày chi tiết trong Thuật toán 1.

4 Thí nghiệm
Trong phần này, trước tiên chúng tôi giới thiệu APPS+, một bộ dữ liệu
chất lượng cao cho sinh code bằng cách xác minh thủ công
dựa trên bộ dữ liệu APPS. Sau đó, chúng tôi
trình bày chi tiết thí nghiệm và kết quả
thí nghiệm.

4.1 Tiền xử lý Dữ liệu
Học tăng cường đòi hỏi một lượng dữ liệu huấn luyện
chất lượng cao. Trong quá trình điều tra của chúng tôi,
chúng tôi thấy rằng trong số các bộ dữ liệu mã nguồn mở
hiện có, chỉ APPS đáp ứng yêu cầu này.
Tuy nhiên, chúng tôi thấy có những thể hiện không chính xác,
như thiếu đầu vào, đầu ra, hoặc giải pháp chuẩn,
các giải pháp chuẩn không thể biên dịch
hoặc không thể thực thi, và sự khác biệt trong đầu ra thực thi.

Để tinh chỉnh bộ dữ liệu APPS, chúng tôi loại trừ các
thể hiện thiếu đầu vào, đầu ra, hoặc giải pháp chuẩn.
Sau đó, chúng tôi chuẩn hóa định dạng của đầu vào
và đầu ra để tạo điều kiện thuận lợi cho việc thực thi và so sánh
các bài kiểm tra đơn vị. Chúng tôi tiến hành các bài kiểm tra đơn vị và
phân tích thủ công cho mỗi thể hiện, loại bỏ những thể hiện
có code không đầy đủ hoặc không liên quan, lỗi cú pháp,
sử dụng sai API, hoặc thiếu phụ thuộc thư viện. Đối với
sự khác biệt trong đầu ra, chúng tôi xem xét thủ công
mô tả vấn đề, sửa đầu ra mong đợi
hoặc loại bỏ thể hiện.

Cuối cùng, chúng tôi xây dựng bộ dữ liệu APPS+,
chứa 7.456 thể hiện. Mỗi thể hiện bao gồm
mô tả vấn đề lập trình, giải pháp chuẩn,
tên hàm, các bài kiểm tra đơn vị (tức là đầu vào
và đầu ra), và code khởi động (tức là phần
đầu của giải pháp chuẩn). Phụ lục A minh họa
một ví dụ từ APPS+. Phần trên cùng
của hình hiển thị mô tả vấn đề, và
phần bên phải trình bày giải pháp chuẩn, bài kiểm tra
đơn vị, và metadata. Chi tiết thêm về APPS+ được
thảo luận trong Phụ lục B.1.

4.2 Chi tiết Thí nghiệm
Điểm chuẩn. Trong nghiên cứu của chúng tôi, ban đầu chúng tôi đánh giá
phương pháp và các baseline của mình trên bộ dữ liệu
APPS+ được tiền xử lý. Hơn nữa, chúng tôi cũng đánh giá
những phương pháp này trên một số điểm chuẩn được sử dụng rộng rãi
trong sinh code, tức là MBPP (Mostly Basic Pro-
gramming Problems) [2] và HumanEval [3]. Chúng tôi
đánh giá điểm chuẩn MBPP và HumanEval trong
cài đặt học zero-shot giống như
các phương pháp trước [14;33]. Trong cài đặt này, chúng tôi
tinh chỉnh các mô hình chỉ trên bộ dữ liệu APPS+ và đánh giá
hiệu suất sinh code trên
MBPP và HumanEval. Mô tả chi tiết
của các điểm chuẩn có thể được tìm thấy trong Phụ lục B.1.

Baseline. Để xác minh hiệu quả của Step-
Coder và đánh giá hiệu suất của LLM trên
bộ dữ liệu APPS+ của chúng tôi, chúng tôi xem xét một loạt
baseline rộng, bao gồm StarCoder [15], WizardCoder

--- TRANG 6 ---
Mô hình    Kích thước    APPS+
                        Cơ bản    Phỏng vấn    Cạnh tranh    Tổng thể

Mô hình Cơ sở
CodeLlama [27]                 13B      18.7      11.0         0.0       13.0
CodeLlama-Python [27]          13B      29.0      12.3         2.9       17.9
DeepSeek-Coder-Base [8]        6.7B     13.0      10.3         5.0       10.9

Mô hình Fine-tuned có Giám sát
StarCoder [15]                 15.6B     6.3       4.1         0.7        4.7
CodeLlama-Instruct [27]        13B      33.3      11.0         1.4       18.7
WizardCoder-Python-V1.0 [23]   13B      39.7      15.1         4.3       23.6
DeepSeek-Coder-Instruct [8]    6.7B     49.4      18.7         3.6       29.2
SFT trên APPS+                 6.7B     50.1      19.0         6.4       29.8

Mô hình dựa trên Học Tăng cường (Sử dụng DeepSeek-Coder-Instruct-6.7B làm backbone)
Vanilla PPO                    6.7B     53.7      20.1         5.0       31.7
PPOCoder [33]                  6.7B     54.4      20.3         6.4       32.1
RLTF [20]                      6.7B     55.1      20.8         6.4       32.7
StepCoder (Của chúng tôi)      6.7B     59.7      23.5         8.6       36.1
w/o CCCS                       6.7B     58.7      21.7         7.1       34.6
w/o FGO                        6.7B     58.4      23.3         8.6       35.5

Bảng 1: Kết quả pass@1 trên APPS+ được đề xuất của chúng tôi. Chúng tôi so sánh các phương pháp tiên tiến phổ biến và được sử dụng rộng rãi với phương pháp của chúng tôi. Để đảm bảo so sánh công bằng, chúng tôi áp dụng các phương pháp dựa trên RL này sử dụng cùng mô hình cơ sở (tức là DeepSeek-Coder-Instruct-6.7B [8]) làm backbone trên bộ dữ liệu APPS+. Ngoài ra, chúng tôi tiến hành fine-tuning có giám sát sử dụng bộ dữ liệu APPS+ của chúng tôi dựa trên DeepSeek-Coder-Instruct-6.7B để xác minh thêm hiệu quả và tính cần thiết của phương pháp chúng tôi.

[23], DeepSeek-Coder [8], và ba phiên bản của
CodeLlama (Base, Python, Instruct) [27]. Hơn nữa, chúng tôi cũng xem xét vanilla PPO và hai phương pháp
tiên tiến dựa trên RL, bao gồm PPOCoder
[33] và RLTF [20]. Chúng tôi thực hiện thí nghiệm
áp dụng những phương pháp này sử dụng cùng
backbone (tức là DeepSeek-Coder-Instruct [8]) trên
bộ dữ liệu APPS+ để đảm bảo so sánh công bằng. Ngoài việc chứng minh tính cần thiết và
hiệu quả của phương pháp chúng tôi, chúng tôi cũng fine-tuning có giám sát DeepSeek-Coder-Instruct [8] trên bộ dữ liệu APPS+
để loại trừ hiệu ứng của dữ liệu huấn luyện. Mô tả chi tiết của những baseline này được thảo luận
trong Phụ lục B.2.

Chi tiết Triển khai. Trong giai đoạn SFT,
chúng tôi áp dụng tốc độ học được đặt ở 2e−5, tiến hành huấn luyện
trong ba epoch, và sử dụng thời gian warm-up
0.3 epoch, với sự suy giảm tuyến tính về không. Quá trình fine-tuning được tiến hành trên thiết bị với
tám GPU NVIDIA A100 80G, với kích thước batch toàn cục
được đặt ở 64. Trong giai đoạn huấn luyện PPO,
chúng tôi sử dụng tốc độ học 5e−7 cho mô hình chính sách
và 1.5e−6 cho mô hình critic. Đối với mỗi ví dụ, chúng tôi thu thập 16 roll-out code sử dụng nucleus
sampling. Nhiệt độ sampling được đặt ở 0.8,
top-p được đặt ở 0.9, và độ dài token đầu ra tối đa
được đặt ở 1024. Hệ số phạt KL ở mức token β được đặt ở 0.05, với giá trị clip là 0.8.
Trong giai đoạn decoding, nhiệt độ và top_p
được đặt ở 0.2 và 0.95, tương ứng.

Đánh giá & Metric. Chúng tôi tiến hành
thí nghiệm dựa trên Python3.x. Lưu ý rằng chúng tôi
cũng sử dụng Python3.x trong việc thu thập phần thưởng
trong các phương pháp dựa trên RL. Theo các nghiên cứu trước
[27;23;14], chúng tôi sử dụng metric Pass@k [3] để đánh giá
tất cả các mô hình. Pass@k định lượng tỷ lệ các
thể hiện mà ít nhất một trong k giải pháp code được sinh ra
cho mỗi yêu cầu của con người thành công
vượt qua tất cả các bài kiểm tra đơn vị. Các prompt được sử dụng cho sinh code
được liệt kê trong Phụ lục D.

4.3 Kết quả Thí nghiệm trên APPS+
Để đánh giá hiệu suất của các LLM được sử dụng rộng rãi
và StepCoder của chúng tôi trong sinh code, chúng tôi tiến hành
thí nghiệm trên bộ dữ liệu APPS+ mà chúng tôi
xây dựng. Kết quả thí nghiệm được minh họa trong
Bảng 1. Kết quả cho thấy các mô hình dựa trên RL
vượt trội hơn các mô hình ngôn ngữ khác, bao gồm cả
mô hình cơ sở và mô hình SFT. Có thể suy luận hợp lý rằng học tăng cường có thể nâng cao thêm
chất lượng sinh code bằng cách điều hướng hiệu quả hơn
không gian đầu ra của mô hình, được hướng dẫn bởi

--- TRANG 7 ---
phản hồi từ trình biên dịch.

Hơn nữa, StepCoder của chúng tôi vượt trội hơn tất cả các mô hình
baseline bao gồm các phương pháp dựa trên RL khác,
đạt được điểm số cao nhất. Cụ thể, phương pháp của chúng tôi
đạt được 59.7%, 23.5% và 8.6% trong 'Cơ bản',
'Phỏng vấn' và 'Cạnh tranh', tương ứng. Phương pháp của chúng tôi xuất sắc trong việc khám phá không gian đầu ra so với các phương pháp dựa trên RL khác,
đạt được bằng cách đơn giản hóa các tác vụ sinh code phức tạp
thành các tác vụ phụ hoàn thành code. Ngoài ra,
quá trình FGO đóng vai trò then chốt trong việc tối ưu hóa
chính xác mô hình chính sách. Chúng tôi cũng thấy rằng
hiệu suất của StepCoder tốt hơn LLM
được fine-tuning có giám sát trên bộ dữ liệu APPS+ dựa trên cùng backbone. Cái sau đã làm
ít để cải thiện tỷ lệ pass của code được sinh ra
so với backbone. Điều này cũng chứng minh trực tiếp rằng phương pháp sử dụng phản hồi từ trình biên dịch
để tối ưu hóa mô hình cải thiện chất lượng
của code được sinh ra tốt hơn so với dự đoán token tiếp theo trong sinh code.

Mô hình (6.7B)              HumanEval    MBPP
DeepSeek-Coder-Instruct     78.0         64.2
SFT trên APPS+              55.5         54.8
Vanilla PPO                 78.0         65.0
PPOCoder                    76.8         63.8
RLTF                        76.8         65.2
StepCoder (Của chúng tôi)   78.7         67.0

Bảng 2: Kết quả pass@1 trên MBPP và HumanEval.
Chúng tôi đánh giá hiệu suất của LLM trong sinh code
trong cài đặt học zero-shot. Trong cài đặt này, các mô hình
được fine-tuned trên bộ dữ liệu APPS+ được đề xuất của chúng tôi và được kiểm tra
khả năng của chúng trên MBPP và HumanEval.

4.4 Nghiên cứu Ablation
Để điều tra tác động của từng thành phần
trong StepCoder, chúng tôi tiến hành thí nghiệm ablation
với hai biến thể của phương pháp chúng tôi, bao gồm Step-
Coder chỉ với CCCS và chỉ với FGO. Kết quả thí nghiệm được trình bày trong Bảng 1. Kết quả thí nghiệm chứng minh rằng cả hai thành phần của
phương pháp chúng tôi đều cải thiện chất lượng của code được sinh ra
so với vanilla PPO. CCCS có thể nâng cao
hiệu suất của nó trong việc giải quyết các vấn đề cấp độ Cạnh tranh.
Cải thiện này là hợp lý, xem xét
rằng CCCS hiệu quả đơn giản hóa việc khám phá
các yêu cầu phức tạp hơn của con người. Đồng thời, FGO tăng cường tỷ lệ pass của các bài kiểm tra đơn vị bằng cách
tích hợp phản hồi từ trình biên dịch với đoạn code được thực thi
liên quan.

0.0 0.1 0.2 0.3 0.4 0.5 0.6
Phần dòng trùng lặp

0
100
200
300
400
500
600
700
800

Số lượng tác vụ

với MBPP
với HumanEval

Hình 3: Phân tích các dòng trùng lặp giữa APPS+
và hai điểm chuẩn. Sự chồng chéo dữ liệu giữa
APPS+ và chúng rất nhỏ. Chỉ 0.2% và 7.1%
có hơn một nửa số dòng khớp ở đâu đó trong
MBPP và HumanEval, tương ứng.

4.5 Kết quả trên MBPP và HumanEval
Để chứng minh thêm hiệu quả của phương pháp
chúng tôi, chúng tôi tiến hành phân tích so sánh
StepCoder với các phương pháp khác nhau sử dụng
các điểm chuẩn được công nhận rộng rãi MBPP và Hu-
manEval. Những mô hình này được huấn luyện trên APPS+ và
sau đó được đánh giá trên MBPP và HumanEval. Kết quả thí nghiệm được minh họa trong Bảng 2 cho thấy
StepCoder vượt trội hơn tất cả các mô hình khác
trên cả hai điểm chuẩn.

Tuy nhiên, có những lo ngại về
sự chồng chéo tiềm năng trong dữ liệu huấn luyện giữa APPS+
và hai điểm chuẩn, điều này có thể đóng góp
vào việc cải thiện hiệu suất. Để giải quyết
những lo ngại này, chúng tôi phân tích sự khác biệt giữa
APPS+ và các điểm chuẩn bằng cách tính toán tỷ lệ chồng chéo
dòng code của hai giải pháp chuẩn tương ứng
theo nghiên cứu trước [2;14]. Những phát hiện được trình bày trong Hình 3. Bằng chứng này
nhấn mạnh hiệu quả của phương pháp chúng tôi trong việc nâng cao chất lượng của code được sinh ra và khả năng của nó
trong một phổ rộng các tác vụ sinh code,
chủ yếu bằng cách cải thiện vấn đề khám phá trong
học tăng cường.

Trong khi đó, những phát hiện của chúng tôi tiết lộ sự suy giảm đáng kể
trong hiệu suất của mô hình SFT
trên cả hai điểm chuẩn MBPP và HumanEval. Phân tích thêm về các trường hợp lỗi cho thấy một số ít
liên quan đến lỗi tên hàm, trong khi
phần lớn liên quan đến lỗi tính đúng đắn chương trình.
Điều này cũng chỉ ra rằng SFT trên một bộ dữ liệu duy nhất
có thể làm tổn hại khả năng tuân theo hướng dẫn
và khả năng tổng quát hóa, do đó ảnh hưởng đến
hiệu suất sinh code trên các tác vụ khác.

--- TRANG 8 ---
Ngược lại, các phương pháp dựa trên RL có thể cải thiện
hiệu suất cho các tác vụ sinh code chưa từng thấy.

4.6 Phân tích theo Kết quả Bài kiểm tra Đơn vị
Chúng tôi phân tích thêm kết quả của các trường hợp
không vượt qua tất cả các bài kiểm tra đơn vị, như được hiển thị trong Hình 4. Kết quả cho thấy phương pháp được đề xuất của chúng tôi có thể
giảm hiệu quả khả năng xảy ra lỗi biên dịch,
điều này đặc biệt rõ ràng trong các vấn đề lập trình
cấp độ Phỏng vấn và Cạnh tranh. Tuy nhiên, cũng được quan sát thấy rằng tất cả LLM đều
dễ bị lỗi runtime và thất bại hơn so với
lỗi biên dịch, mặc dù StepCoder cho thấy tỷ lệ
lỗi runtime và thất bại thấp hơn tương đối.
Những kết quả này chứng minh rằng StepCoder ít
dễ bị lỗi biên dịch, nhưng vẫn gặp phải
lỗi runtime và thất bại. Do đó, những phát hiện này gợi ý rằng nghiên cứu tương lai nên
tập trung thêm vào việc giảm đáng kể lỗi runtime,
điều này có thể nâng cao rất nhiều cả chất lượng
và tỷ lệ pass của code được sinh ra bởi những mô hình như vậy.

[Biểu đồ hiển thị tỷ lệ phần trăm của Lỗi Biên dịch và Lỗi Runtime & Thất bại cho các mô hình khác nhau]

Hình 4: Phân tích theo kết quả bài kiểm tra đơn vị trên APPS+. Kết quả được phân loại thành Lỗi Biên dịch (Phần thưởng = -1) và Lỗi Runtime & Thất bại (Phần thưởng = -0.6 hoặc -0.3).

5 Nghiên cứu Liên quan

5.1 Mô hình Ngôn ngữ Lớn cho Sinh Code
Gần đây, LLM đã thể hiện khả năng đáng chú ý trong
việc hiểu ngôn ngữ tự nhiên và sinh code
bằng cách huấn luyện trên corpus văn bản lớn chứa
dữ liệu code. Một số mô hình ngôn ngữ được pre-trained
(PLM) thể hiện tiềm năng đáng kể cho sinh code
bao gồm CodeGPT [21], PanGu-Coder
[4], SantaCoder [1], CodeGeex [44] và Phi-1.5
[16]. Ngoài ra, các mô hình SFT đạt được hiệu suất
cạnh tranh hơn như CodeX [3], StarCoder
[15], WizardCoder [23], Code Llama Instruct [27],
và DeepSeek-Coder [8].

Học Tăng cường là một phương pháp học
chính sách tối ưu bằng cách khám phá môi trường
và thu được phần thưởng [41;34]. Gần đây, một số
nhà nghiên cứu đã giới thiệu RL vào LLM và cải thiện chất lượng của code được sinh ra bằng cách sử dụng phản hồi bài kiểm tra đơn vị để khám phá không gian đầu ra
của mô hình chính sách [33;20;14]. Ví dụ,
CodeRL [14] tận dụng tín hiệu từ các bài kiểm tra đơn vị làm phần thưởng và sử dụng phương pháp actor-critic [12;35]
để nâng cao mô hình trong sinh code. PPOCoder
[33] tinh chỉnh CodeRL bằng cách sử dụng thuật toán PPO
[30] và RLTF [20] cung cấp phần thưởng chi tiết
thông qua các vị trí lỗi, nhưng không gian phần thưởng
vẫn thưa thớt. Tuy nhiên, việc khám phá
các tác vụ phức tạp trong môi trường được đặc trưng bởi
phần thưởng thưa thớt là thách thức. Những phương pháp này vẫn
không đủ để sử dụng hiệu quả RL để nâng cao
hiệu suất của mô hình trong sinh code.

5.2 Khám phá trong Học Tăng cường
Khám phá là quan trọng trong việc giải quyết các vấn đề
chuỗi dài và phần thưởng thưa thớt [9;13]. Trong
tác vụ sinh chuỗi, các nhà nghiên cứu cải thiện việc khám phá bằng cách khởi tạo mô hình chính sách sử dụng
mô hình SFT [26;31]. Phương pháp được đề xuất của chúng tôi kết hợp các phương pháp tương tự, nhưng các phương pháp bổ sung
là cần thiết để đảm bảo khám phá hiệu quả. Điều này
đặc biệt rõ ràng khi đối mặt với các yêu cầu phức tạp của con người, trong đó chất lượng hạn chế của code được sinh ra bởi các mô hình SFT làm cho việc khám phá vẫn
thách thức [33].

Các phương pháp đáng chú ý khác giới thiệu Mô hình
Phần thưởng được Giám sát theo Quy trình để cung cấp phần thưởng
từng bước cho các tác vụ sinh chuỗi phức tạp
như lý luận toán học và sinh code [38;18;22;24]. Tuy nhiên, những phương pháp này
đòi hỏi việc gán nhãn một bộ dữ liệu sở thích lớn để huấn luyện
mô hình phần thưởng. Tương tự như phương pháp của chúng tôi, một số
phương pháp xây dựng chương trình giảng dạy học bằng cách khởi tạo
mỗi episode từ một chuỗi các trạng thái bắt đầu
ngày càng thách thức hơn [28;11;6]. Trái ngược với phương pháp của chúng tôi, những phương pháp này được thiết kế để giải quyết vấn đề khám phá trong
các lĩnh vực khác, như gaming và thao tác robot.
Trong khi đó, phương pháp của chúng tôi kết hợp các tính năng
kỹ thuật phần mềm để xác định động các trạng thái bắt đầu
thông qua các câu lệnh điều kiện. Chúng tôi cũng giới thiệu FGO để cung cấp tối ưu hóa chi tiết
cho mô hình chính sách bằng cách tận dụng thông tin
coverage.

6 Kết luận
Trong bài báo này, chúng tôi giới thiệu StepCoder, một khung
huấn luyện mới thông qua RL. StepCoder chia nhỏ

--- TRANG 9 ---
các vấn đề khám phá phức tạp để giảm
độ khó của việc khám phá môi trường với phần thưởng thưa thớt
đồng thời cung cấp tối ưu hóa chi tiết.
Ngoài ra, chúng tôi cũng xây dựng một bộ dữ liệu chất lượng cao
APPS+, đặc biệt cho sinh code. Thí nghiệm cho thấy phương pháp của chúng tôi có thể cải thiện hiệu quả
chất lượng của code được sinh ra thông qua học
tăng cường so với các phương pháp khác.

Tài liệu tham khảo
[1] Loubna Ben Allal, Raymond Li, Denis Kocetkov,
Chenghao Mou, Christopher Akiki, Carlos Munoz
Ferrandis, Niklas Muennighoff, Mayank Mishra,
Alex Gu, Manan Dey, et al. 2023. Santacoder: don't
reach for the stars! arXiv preprint arXiv:2301.03988.

[2] Jacob Austin, Augustus Odena, Maxwell Nye,
Maarten Bosma, Henryk Michalewski, David Do-
han, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le,
et al. 2021. Program synthesis with large language
models. arXiv preprint arXiv:2108.07732.

[3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374.

[4] Fenia Christopoulou, Gerasimos Lampouras, Mi-
lan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi
Li, Qi Zhang, Meng Xiao, Bo Shen, Lin Li,
et al. 2022. Pangu-coder: Program synthesis with
function-level language modeling. arXiv preprint
arXiv:2207.11280.

[5] OpenCompass Contributors. 2023. Opencompass:
A universal evaluation platform for foundation
models. https://github.com/open-compass/
opencompass.

[6] Carlos Florensa, David Held, Markus Wulfmeier,
Michael Zhang, and Pieter Abbeel. 2017. Reverse
curriculum generation for reinforcement learning.
In Conference on robot learning, pages 482–495.
PMLR.

[7] Sumit Gulwani, Oleksandr Polozov, Rishabh Singh,
et al. 2017. Program synthesis. Foundations and
Trends® in Programming Languages, 4(1-2):1–119.

[8] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie,
Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi,
Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wen-
feng Liang. 2024. Deepseek-coder: When the large
language model meets programming – the rise of
code intelligence.

[9] Jianye Hao, Tianpei Yang, Hongyao Tang, Chen-
jia Bai, Jinyi Liu, Zhaopeng Meng, Peng Liu, and
Zhen Wang. 2023. Exploration in deep reinforcement
learning: From single-agent to multiagent domain.
IEEE Transactions on Neural Networks and Learning
Systems.

[10] Dan Hendrycks, Steven Basart, Saurav Kadavath,
Mantas Mazeika, Akul Arora, Ethan Guo, Collin
Burns, Samir Puranik, Horace He, Dawn Song, and
Jacob Steinhardt. 2021. Measuring coding chal-
lenge competence with APPS. In Proceedings of
the Neural Information Processing Systems Track on
Datasets and Benchmarks 1, NeurIPS Datasets and
Benchmarks 2021, December 2021, virtual.

[11] Ionel-Alexandru Hosu and Traian Rebedea. 2016.
Playing atari games with deep reinforcement learn-
ing and human checkpoint replay. arXiv preprint
arXiv:1607.05077.

[12] Vijay Konda and John Tsitsiklis. 1999. Actor-critic
algorithms. Advances in neural information process-
ing systems, 12.

[13] Pawel Ladosz, Lilian Weng, Minwoo Kim, and
Hyondong Oh. 2022. Exploration in deep reinforce-
ment learning: A survey. Information Fusion, 85:1–
22.

[14] Hung Le, Yue Wang, Akhilesh Deepak Gotmare,
Silvio Savarese, and Steven Chu Hong Hoi. 2022.
Coderl: Mastering code generation through pre-
trained models and deep reinforcement learning. Ad-
vances in Neural Information Processing Systems,
35:21314–21328.

[15] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas
Muennighoff, Denis Kocetkov, Chenghao Mou, Marc
Marone, Christopher Akiki, Jia Li, Jenny Chim, et al.
2023. Starcoder: may the source be with you! arXiv
preprint arXiv:2305.06161.

[16] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie
Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023.
Textbooks are all you need ii: phi-1.5 technical report.
arXiv preprint arXiv:2309.05463.

[17] Yujia Li, David Choi, Junyoung Chung, Nate
Kushman, Julian Schrittwieser, Rémi Leblond, Tom
Eccles, James Keeling, Felix Gimeno, Agustin
Dal Lago, et al. 2022. Competition-level code genera-
tion with alphacode. Science, 378(6624):1092–1097.

[18] Hunter Lightman, Vineet Kosaraju, Yura Burda,
Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike,
John Schulman, Ilya Sutskever, and Karl Cobbe.
2023. Let's verify step by step. arXiv preprint
arXiv:2305.20050.

[19] Timothy P Lillicrap, Jonathan J Hunt, Alexander
Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David
Silver, and Daan Wierstra. 2015. Continuous control
with deep reinforcement learning. arXiv preprint
arXiv:1509.02971.

[20] Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao
Han, Wei Yang, and Deheng Ye. 2023. Rltf: Rein-
forcement learning from unit test feedback. arXiv
preprint arXiv:2307.04349.

--- TRANG 10 ---
[21] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang,
Alexey Svyatkovskiy, Ambrosio Blanco, Colin
Clement, Dawn Drain, Daxin Jiang, Duyu Tang, et al.
2021. Codexglue: A machine learning benchmark
dataset for code understanding and generation. arXiv
preprint arXiv:2102.04664.

[22] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-
guang Lou, Chongyang Tao, Xiubo Geng, Qingwei
Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-
ardmath: Empowering mathematical reasoning for
large language models via reinforced evol-instruct.
arXiv preprint arXiv:2308.09583.

[23] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-
ubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,
Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder:
Empowering code large language models with evol-
instruct. arXiv preprint arXiv:2306.08568.

[24] Qianli Ma, Haotian Zhou, Tingkai Liu, Jianbo
Yuan, Pengfei Liu, Yang You, and Hongxia Yang.
2023. Let's reward step by step: Step-level reward
model as the navigators for reasoning. arXiv preprint
arXiv:2310.10080.

[25] Volodymyr Mnih, Koray Kavukcuoglu, David Sil-
ver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidje-
land, Georg Ostrovski, et al. 2015. Human-level
control through deep reinforcement learning. nature,
518(7540):529–533.

[26] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo
Almeida, Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama,
Alex Ray, et al. 2022. Training language models to
follow instructions with human feedback. Advances
in Neural Information Processing Systems, 35:27730–
27744.

[27] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle,
Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.
Code llama: Open foundation models for code. arXiv
preprint arXiv:2308.12950.

[28] Tim Salimans and Richard Chen. 2018. Learning
montezuma's revenge from a single demonstration.
arXiv preprint arXiv:1812.03381.

[29] John Schulman, Philipp Moritz, Sergey Levine,
Michael Jordan, and Pieter Abbeel. 2015. High-
dimensional continuous control using general-
ized advantage estimation. arXiv preprint
arXiv:1506.02438.

[30] John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv:1707.06347.

[31] Wei Shen, Rui Zheng, Wenyu Zhan, Jun Zhao, Shi-
han Dou, Tao Gui, Qi Zhang, and Xuan-Jing Huang.
2023. Loose lips sink ships: Mitigating length bias
in reinforcement learning from human feedback. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2023, pages 2859–2873.

[32] Martin Shepperd. 1988. A critique of cyclomatic
complexity as a software metric. Software Engineer-
ing Journal, 3(2):30–36.

[33] Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni,
and Chandan K Reddy. 2023. Execution-based code
generation using deep reinforcement learning. arXiv
preprint arXiv:2301.13816.

[34] Richard S Sutton, Andrew G Barto, et al. 1998.
Introduction to reinforcement learning, volume 135.
MIT press Cambridge.

[35] Richard S Sutton, David McAllester, Satinder
Singh, and Yishay Mansour. 1999. Policy gradient
methods for reinforcement learning with function
approximation. Advances in neural information pro-
cessing systems, 12.

[36] Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,
and Neel Sundaresan. 2020. Intellicode compose:
Code generation using transformer. In Proceedings
of the 28th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foun-
dations of Software Engineering, pages 1433–1443.

[37] Hugo Touvron, Louis Martin, Kevin Stone, Peter
Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, et al. 2023. Llama 2: Open foun-
dation and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.

[38] Jonathan Uesato, Nate Kushman, Ramana Ku-
mar, Francis Song, Noah Siegel, Lisa Wang, An-
tonia Creswell, Geoffrey Irving, and Irina Hig-
gins. 2022. Solving math word problems with
process-and outcome-based feedback. arXiv preprint
arXiv:2211.14275.

[39] Xin Wang, Yasheng Wang, Yao Wan, Fei Mi, Yi-
tong Li, Pingyi Zhou, Jin Liu, Hao Wu, Xin Jiang,
and Qun Liu. 2022. Compilable neural code gen-
eration with compiler feedback. arXiv preprint
arXiv:2203.05132.

[40] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra,
Alisa Liu, Noah A Smith, Daniel Khashabi, and Han-
naneh Hajishirzi. 2022. Self-instruct: Aligning lan-
guage model with self generated instructions. arXiv
preprint arXiv:2212.10560.

[41] Ronald J Williams. 1992. Simple statistical
gradient-following algorithms for connectionist rein-
forcement learning. Machine learning, 8:229–256.

[42] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin
Jiang. 2023. Wizardlm: Empowering large lan-
guage models to follow complex instructions. arXiv
preprint arXiv:2304.12244.

--- TRANG 11 ---
[43] Tianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi
Liu, Jianye Hao, Zhaopeng Meng, Peng Liu, and
Zhen Wang. 2021. Exploration in deep reinforcement
learning: a comprehensive survey. arXiv preprint
arXiv:2109.06668.

[44] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong,
Shan Wang, Yufei Xue, Zihan Wang, Lei Shen,
Andi Wang, Yang Li, et al. 2023. Codegeex: A
pre-trained model for code generation with multi-
lingual evaluations on humaneval-x. arXiv preprint
arXiv:2303.17568.

[45] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua,
Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Yuhao
Zhou, Limao Xiong, et al. 2023. Delve into ppo:
Implementation matters for stable rlhf. In NeurIPS
2023 Workshop on Instruction Tuning and Instruction
Following.

A Thể hiện của Bộ dữ liệu APPS+
Chúng tôi trình bày một ví dụ từ bộ dữ liệu APPS+ của chúng tôi,
như được hiển thị trong Hình 5.

B Thiết lập Thí nghiệm Chi tiết
Trong phần này, chúng tôi trình bày chi tiết về các
baseline mà chúng tôi so sánh và chi tiết triển khai
của phương pháp chúng tôi.

B.1 Điểm chuẩn
APPS+. Chúng tôi xây dựng điểm chuẩn mới APPS+
bằng cách tinh chỉnh điểm chuẩn phổ biến APPS [10].
APPS+ được phân loại thành ba cấp độ khó: Cơ bản (2.850), Phỏng vấn (4.020), và Cạnh tranh (586). Độ dài trung bình của mỗi vấn đề
là 255.3 từ, và của code là 21.9 dòng.
Trung bình, mỗi thể hiện đi kèm với ba
bài kiểm tra đơn vị và bao gồm thuộc tính 'câu lệnh điều kiện' biểu thị vị trí bắt đầu và kết thúc của câu lệnh trong giải pháp chuẩn. Chúng tôi chọn ngẫu nhiên
khoảng 25% thể hiện (700 Cơ bản,
1.000 Phỏng vấn, và 140 Cạnh tranh) cho bộ dữ liệu
validation và 25% thể hiện khác cho
bộ dữ liệu test.

MBPP. MBPP [2] là một điểm chuẩn sinh code Python
nhỏ hơn nhưng phổ biến. Nó chứa
974 thể hiện được tạo ra bằng cách crowdsourcing cho một nhóm nội bộ các crowd worker có kiến thức Python cơ bản. Cấp độ khó
của các vấn đề trong bộ dữ liệu này là cơ bản. Hầu hết các vấn đề
thường được truyền đạt trong một câu duy nhất bằng ngôn ngữ tự nhiên, và mỗi vấn đề bao gồm mô tả tác vụ,
giải pháp code, và ba trường hợp kiểm tra tự động.
Chúng tôi đánh giá LLM trong cài đặt học zero-shot
giống như các nghiên cứu trước [14;33]. Trong
cài đặt này, chúng tôi fine-tune mô hình chỉ dựa trên
bộ dữ liệu APPS+ và đánh giá chúng trên MBPP.

HumanEval. HumanEval [3] là một điểm chuẩn khác được sử dụng rộng rãi để đánh giá khả năng
sinh code. Nó bao gồm 164 vấn đề Python
được viết tay để kiểm tra hiểu biết ngôn ngữ,
tư duy thuật toán, và toán học cơ bản. Độ phức tạp
của những vấn đề này tương tự như của các câu hỏi
phỏng vấn phần mềm đơn giản. Chúng tôi cũng đánh giá
mô hình trên điểm chuẩn HumanEval trong cài đặt
học zero-shot.

B.2 Baseline
StarCoder. StarCoder [15] là một mô hình 15.5B tham số được huấn luyện trên 80+ ngôn ngữ lập trình
có nguồn gốc từ GitHub, bao gồm một nghìn tỷ
token. Nó trải qua fine-tuning đặc biệt cho
35 tỷ token Python, cho phép thành thạo
trong một tập hợp đa dạng các tác vụ coding. Với độ dài context mở rộng là 8K, StarCoder xuất sắc đặc biệt trong khả năng infilling.

CodeLlama. CodeLlama [27] là một bộ sưu tập
các mô hình văn bản sinh ra được pre-trained và fine-tuned có quy mô từ 7B đến 34B tham số.
CodeLlama có ba biến thể: CodeLlama:
mô hình cơ sở được thiết kế cho tổng hợp và hiểu biết code tổng quát; CodeLlama-Python: được thiết kế
đặc biệt để xử lý ngôn ngữ lập trình Python;
CodeLlama-Instruct: để tuân theo hướng dẫn
và triển khai an toàn hơn.

WizardCoder. WizardCoder [23] được fine-tuned
bằng cách sử dụng bộ dữ liệu phức tạp được xây dựng
bằng cách áp dụng Evol-Instruct [42] trên các tác vụ
liên quan đến code, là một cải tiến thêm của phương pháp self-instruct [40]. Nó đã được chứng minh là rất
hiệu quả trong sinh code bằng cách fine-tuning
dữ liệu hướng dẫn phức tạp hơn.

DeepSeek-Coder. DeepSeek-Coder [8] thể hiện hiệu suất tiên tiến trong số các mô hình
code mã nguồn mở trên nhiều ngôn ngữ lập trình.
Nó bao gồm một bộ sưu tập các mô hình ngôn ngữ code
từ 1B đến 33B được huấn luyện từ đầu.
Corpus huấn luyện cho những mô hình này bao gồm
ấn tượng 2 nghìn tỷ token là sự kết hợp
của code và ngôn ngữ tự nhiên. Mỗi mô hình được
huấn luyện để sử dụng kích thước cửa sổ 16K, và tác vụ
fill-in-the-blank được tích hợp vào quá trình huấn luyện, điều này nâng cao khả năng của mô hình
để tạo điều kiện thuận lợi cho hoàn thành code và các tác vụ infilling.

PPOCoder. PPOCoder [33] ban đầu sử dụng
thuật toán Proximal Policy Optimization [30]
cho sinh code. Ngoài ra, nó tích hợp dis-

--- TRANG 12 ---
-----Mô tả tác vụ-----
def numDistinct(self, s: str, t: str) -> int:
    """Cho một chuỗi S và một chuỗi T, đếm số lượng subsequence phân biệt của S mà bằng T.
    Một subsequence của một chuỗi là một chuỗi mới được hình thành từ chuỗi gốc bằng cách xóa một số (có thể không có) ký tự mà không làm xáo trộn
    vị trí tương đối của các ký tự còn lại. (tức là "ACE" là một subsequence của "ABCDE" trong khi "AEC" thì không). """

-----Ví dụ 1-----
Đầu vào: S = "rabbbit", T = "rabbit"
Đầu ra: 3
Giải thích:
Như được hiển thị bên dưới, có 3 cách bạn có thể sinh ra "rabbit" từ S.

-----Ví dụ 2-----
Đầu vào: S = "babgbag", T = "bag"
Đầu ra: 5

"inputs": [["\"rabbbit\"","\"rabbit\""]],"outputs": [3],"fn_name": "numDistinct","starter_code": "\nclass Solution:\n    def numDistinct(self, s: str, t: str) -> int:\n"

def numDistinct(self, s, t):
    setOft=set(t)
    news=""
    for ch in s:
        if ch in setOft:
            news+=ch
    dp=[[1 for i in range(len(news)+1)] for j in range(len(t)+1)]
    for j in range(1,len(t)+1):
        dp[j][0]=0
    for i in range(len(t)):
        for j in range(len(news)):
            if t[i]==news[j]:
                dp[i+1][j+1]=dp[i][j]+dp[i+1][j]
            else:
                dp[i+1][j+1]=dp[i+1][j]
    return dp[len(t)][len(news)]

Bài kiểm tra Đơn vị & MetaData
Giải pháp Chuẩn
Mô tả Vấn đề Lập trình

Hình 5: Một thể hiện từ bộ dữ liệu APPS+ của chúng tôi bao gồm yêu cầu của con người (trên), code chuẩn tương ứng (dưới trái), metadata, và các trường hợp ví dụ để kiểm tra đơn vị nhằm đánh giá code được sinh ra (dưới phải). Chúng tôi làm sạch bộ dữ liệu APPS [10] để cung cấp đánh giá nghiêm ngặt hơn và nền tảng cho huấn luyện bằng RL trong sinh code.

crete compiler feedback với điểm số khớp cú pháp và ngữ nghĩa giữa code được sinh ra và
mục tiêu có thể thực thi làm giảm độ thưa thớt của
hàm phần thưởng, từ đó cung cấp hướng dẫn tốt hơn
để sinh code phù hợp hơn với
mục tiêu chính xác.

RLTF. RLTF [20] có tính năng sinh dữ liệu thời gian thực
trong quá trình huấn luyện và
phản hồi bài kiểm tra đơn vị đa độ chi tiết. Ngoài
discrete compiler feedback, nó phạt các phần cụ thể trong code nơi xảy ra lỗi thông qua các vị trí lỗi từ phản hồi của bài kiểm tra đơn vị.

C Thuật toán của CCCS và FGO
Thuật toán đầy đủ của StepCoder được trình bày chi tiết trong Thuật toán 1.

D Các prompt được sử dụng cho sinh code
Đối với DeepSeek-Coder-Instruct [8], chúng tôi sử dụng cùng
prompt như bài báo trước. Hơn nữa, DeepSeek-
Coder-Instruct phục vụ như mô hình backbone cho
PPOCoder [33], RLTF [20], và StepCoder được đề xuất của chúng tôi. Do đó, chúng tôi căn chỉnh các prompt cho
những phương pháp dựa trên RL này với prompt của
DeepSeek-Coder-Instruct để duy trì tính nhất quán.
Prompt được sử dụng cho các mô hình khác như CodeL-
lama, WizardCoder và StarCoder giống như
trong các nghiên cứu trước [5; 23; 15; 27].

Prompt được sử dụng cho DeepSeek-Coder-Instruct
và các LLM dựa trên nó như sau:

Bạn là một trợ lý lập trình AI, sử dụng
mô hình Deepseek Coder, được phát triển bởi Công ty Deepseek, và bạn chỉ trả lời các câu hỏi liên quan
đến khoa học máy tính.
Đối với các câu hỏi nhạy cảm về chính trị, các vấn đề bảo mật và
quyền riêng tư, và các câu hỏi khoa học máy tính khác, bạn sẽ từ chối trả lời.
### Hướng dẫn:
viết một thuật toán bằng python:
{Mô tả tác vụ}
### Phản hồi:

--- TRANG 13 ---
Thuật toán 1 StepCoder: Cải thiện Sinh Code với Học Tăng cường từ Phản hồi Trình biên dịch

Yêu cầu: bộ dữ liệu huấn luyện D={(xi, yi, ui, ei),1≤i≤n}, giá trị ngưỡng ρt cho huấn luyện chương trình giảng dạy.
Yêu cầu: mô hình chính sách πθ

1: Khởi tạo bước nhảy của chương trình giảng dạy s=⌈Ei/⌈√Ei⌉⌉ cho mỗi mẫu
2: Khởi tạo chương trình giảng dạy hiện tại c=⌈√Ei⌉ −1 cho mỗi mẫu huấn luyện
3: Khởi tạo tỷ lệ pass ρ= 0 cho mỗi mẫu huấn luyện
4: while TRUE do
5:     Khởi tạo mini-batch Ds={}
6:     Lấy mô hình chính sách mới nhất πθ
7:     Lấy mẫu một mini-batch kích thước M từ D
8:     for i in 0,···,M−1 do ▷ Bắt đầu lấy mẫu các quỹ đạo
9:         Tính toán vị trí bắt đầu pos =si∗ci ▷ CCCS
10:        Tổ chức lại context đã cho x′i=xi+yi[:pos]
11:        Lấy mẫu quỹ đạo ŷi←πθ(.|x′i)
12:        Tính toán phần thưởng ri sử dụng Phương trình 3
13:        Tính toán ma trận che đoạn không thực thi mij= [1 nếu ŷji được thực thi ngược lại 0] ▷ FGO
14:        Thêm {x′i,ŷi, ui, ri, si, ci, mi} vào mini-batch Ds
15:    end for
16:    θ← A (θ,Ds) ▷ Cập nhật mô hình chính sách bằng thuật toán PPO
17:    for i in 0,···,M−1 do
18:        if ri= 1 then ▷ Cập nhật tỷ lệ pass sử dụng moving average
19:            ρi=α+ (1−α)∗ρi
20:        else
21:            ρi= (1−α)∗ρi
22:        end if
23:        if ρi> ρt then ▷ Đáp ứng điều kiện cập nhật, tiến tới giai đoạn tiếp theo
24:            ρi= 0
25:            ci=min(ci−1,0)
26:        end if
27:    end for
28: end while
