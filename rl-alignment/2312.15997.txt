# 2312.15997.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2312.15997.pdf
# File size: 2080915 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Aligning Large Language Models with Human Preferences
through Representation Engineering
Wenhao Liu†, Xiaohua Wang†, Muling Wu, Tianlong Li, Changze Lv
Zixuan Ling, Jianhao Zhu, Cenyuan Zhang, Xiaoqing Zheng∗, Xuanjing Huang
School of Computer Science, Fudan University, Shanghai, China
{whliu22,xiaohuawang22}@m.fudan.edu.cn
{zhengxq,xjhuang}@fudan.edu.cn
Abstract
Aligning large language models (LLMs) with
human preferences is crucial for enhancing
their utility in terms of helpfulness, truthful-
ness, safety, harmlessness, and interestingness.
Existing methods for achieving this alignment
often involve employing reinforcement learn-
ing from human feedback (RLHF) to fine-tune
LLMs based on human labels assessing the rel-
ative quality of model responses. Nevertheless,
RLHF is susceptible to instability during fine-
tuning and presents challenges in implementa-
tion. Drawing inspiration from the emerging
field of representation engineering (RepE), this
study aims to identify relevant representations
for high-level human preferences embedded in
patterns of activity within an LLM and achieve
precise control of model behavior by transform-
ing its representations. This novel approach,
denoted as Representation Alignment from Hu-
man Feedback (RAHF), proves to be effective,
computationally efficient, and easy to imple-
ment. Extensive experiments demonstrate the
efficacy of RAHF in not only capturing but also
manipulating representations to align with a
broad spectrum of human preferences or values,
rather than being confined to a singular concept
or function (e.g. honesty or bias). RAHF’s ver-
satility in accommodating diverse human pref-
erences shows its potential for advancing LLM
performance. Code is available at https:
//github.com/LiuAmber/RAHF .
1 Introduction
While large language models (LLMs) learn broad-
ranging world knowledge and a degree of reason-
ing proficiency, precise control over their behavior
proves challenging due to the unsupervised nature
of their pre-training (Radford et al., 2018, 2019;
Brown et al., 2020; Bubeck et al., 2023; Touvron
et al., 2023). For each query, instruction-tuned
†These authors contributed equally.
∗Corresponding author.
Figure 1: Illustration of different apporaches. (a) Re-
inforcement learning from human feedback (RLHF);
(b) Direct preference optimization (DPO); (c) Hindsight
instruction relabeling (HIR); (d) Representation align-
ment from human feedback (RAHF).
LLMs (Wei et al., 2021; Chung et al., 2022; Tou-
vron et al., 2023) exhibit the capacity to gener-
ate multiple responses that are both semantically
and syntactically coherent by some sampling tech-
niques. While such ability enables the models to
provide diversity that is essential for chat agents,
some responses may contain harmful, unethical, so-
cially biased, and negative, even illegal content (Sri-
vastava et al., 2022; Thoppilan et al., 2022; Bubeck
et al., 2023; Wang et al., 2023).
Existing methods steer LLMs to align with hu-
man preferences often using reinforcement learn-
ing (RL), with reinforcement learning from human
feedback (RLHF) emerging as the most successful
one (Ouyang et al., 2022). However, the underlying
learning algorithms exhibit a considerable degree
of complexity, sensitivity to hyperparameters, in-
stability during training, and necessitate additional
training in the reward model and value network,
leading to substantial computational costs (Yuan
et al., 2023; Rafailov et al., 2023).
In addressing the aforementioned challenges
posed by RL-based methods, several computation-
ally lightweight alternatives have been proposed toarXiv:2312.15997v3  [cs.CL]  3 Jul 2024

--- PAGE 2 ---
simplify the human preference-matching process.
Two prominent paradigms among these alternatives
include contrastive learning (Rafailov et al., 2023;
Zhao et al., 2023; Yuan et al., 2023) and Hindsight
instruction relabeling (HIR) (Zhang et al., 2023;
Liu et al., 2023). Contrastive learning-based meth-
ods optimize a language model policy by increasing
the relative probability of preferred responses over
dispreferred ones, while HIR methods transform
human feedback into instructions by relabeling the
original ones, indicating the relative quality of pro-
vided responses. A common characteristic shared
by these two paradigms is their capability to align
language models with human preferences through
reward-free fine-tuning.
However, the reward-free fine-tuning is vulner-
able to the presence of noisy data or incorrect
labels in a training set comprising a collection
of preference-annotated response pairs (Li et al.,
2023b; Dumoulin et al., 2023). Instances of dull
sentences or very brief responses may appear re-
peatedly in such a training set, potentially intro-
ducing bias into the models. The exclusion of such
instances from the training set renders it impossible
for LLMs to glean insights into human preferences
expressed in these instances. In contrast, RL-based
methods adopt a different strategy, wherein a re-
ward function is first extracted from a dataset of
response rankings, and then this reward function
can be applied to train an LLM, effectively miti-
gating the model’s direct exposure to noisy data or
incorrect labels within the dataset.
In this study, we aim to seek for a computa-
tionally lighter and reward-free algorithm that can
effectively harness human preference expressed
in datasets meanwhile safeguarding LLMs from
the influence of noisy data. Inspired by the recent
advance in representation engineering (Zou et al.,
2023), we initially locate relevant representations
and activity patterns associated with high-level hu-
man preferences within an LLM, and subsequently
gain precise control over its behavior by manip-
ulating its internal representations. In the neural
architecture, network weights determine neural ac-
tivity, neural activity determines the networks’ out-
put, and the networks’ output determines the net-
works’ behavior. Instead of focusing on neurons
and their connections, we see aligning LLMs with
human feedback as an outcome of representational
spaces, implemented by patterns of activity across
populations of neurons. We first identify the dif-
ferences in model activities between preferred anddispreferred stimuli, and then control its behavior
by leveraging the identified differences in represen-
tations (see Figure 1). We introduce two methods
for controlling representations and demonstrate the
efficacy of these representation engineering (RepE)
approaches in aligning LLMs with a broad spec-
trum of human preferences through a collection of
response pairs.
To validate the effectiveness of our approach in
aligning with human preferences, we conducted ex-
tensive comparative experiments on the generated
results. Our method outperformed RLHF and other
RL-free approaches in human evaluations and auto-
mated metrics such as general abilities and GPT-4
evaluations. Notably, the underlying algorithms
exhibit simplicity in implementation and straight-
forwardness in training.
2 Related Work
Tuning large language models to elicit desired re-
sponses and behavior from their extensive knowl-
edge and capabilities is essential in the develop-
ment of chat agents, such as ChatGPT (Brown et al.,
2020), LLaMA (Touvron et al., 2023) and GPT-4
(Bubeck et al., 2023), characterized by safety, per-
formance, and controllability. The enlargement
of the size of language models only does not in-
herently enhance their ability to follow a user’s
intent. For example, LLMs may still generate out-
puts that are untruthful, toxic, or simply not helpful
to the user. Existing human preference alignment
methods can be broadly classified into three major
categories: reinforcement learning (Ouyang et al.,
2022; Ramamurthy et al., 2023), contrastive learn-
ing (Rafailov et al., 2023; Zhao et al., 2023; Yuan
et al., 2023), and Hindsight instruction relabeling
(Zhang et al., 2023; Liu et al., 2023).
Extensive research has been devoted to the explo-
ration of RL from human feedback through ratings
or rankings, spanning tasks from NL-to-SQL con-
version (Zhong et al., 2017), machine translation
(Kreutzer et al., 2018), task-oriented dialogue sys-
tems (Su et al., 2019; Zhang et al., 2019; Takanobu
et al., 2019), summarization (Stiennon et al., 2020),
story-telling (Ziegler et al., 2019) to instruction-
following (Ouyang et al., 2022; Ramamurthy et al.,
2023). Typically, these methods involve the fitting
of a reward model to a dataset of human prefer-
ences, followed by the optimization of a LLM pol-
icy to generate responses with high reward, using
RL algorithms such as REINFORCE (Williams,

--- PAGE 3 ---
1992) or proximal policy optimization (Schulman
et al., 2017). Despite the attractiveness of lever-
aging human preferences that are easier to collect
than expert demonstrations, training LLMs with RL
poses significant practical challenges, which is at-
tributed to the sensitivity of RL to hyperparameters
and the inherent instability during training.
The solutions based on Hindsight instruction re-
labeling (Zhang et al., 2023; Liu et al., 2023) and
contrastive learning (Rafailov et al., 2023; Zhao
et al., 2023; Yuan et al., 2023) have emerged as
computationally efficient alternatives to RL-based
methods without explicit reward modeling. How-
ever, these reward-free fine-tuning solutions are
susceptible to noisy data or incorrect labels within
a training set. They exhibit performance lags com-
pared to models tuned with RL counterparts (see
Section 4). Furthermore, the question of whether
LLMs trained with such fine-tuning methods can
generalize well to out-of-distribution queries re-
mains unresolved when contrasted with models
incorporating an explicit reward model. RLHF
method (Ouyang et al., 2022) offers a potential
avenue for improvement by leveraging additional
unlabeled examples through labeling LLM genera-
tions with the learned reward model.
To enhance transparency and controllability of
neural networks, Zou et al. (2023) introduced rep-
resentation engineering (RepE) as a methodology,
drawing an analogy between understanding deep
neural networks through representation tomogra-
phy and studying brains via neuroimaging tech-
niques. Their work demonstrated the efficacy of
RepE in addressing diverse safety-related chal-
lenges such as truthfulness, honesty, and hallucina-
tion. This study falls in line with recent research
findings and extends its application to aligning
LLMs with a wide spectrum of human preferences.
Our study introduces two novel methods to instruct
LLMs on human preferences first, and then extract
differences in model activities between preferred
and dispreferred stimuli. These differences in activ-
ity patterns serve as a foundation for manipulating
the model’s behavior, leading to the generation of
responses that better align with human preferences.
Due to the lightweight computational advantages of
parameter-efficient fine-tuning techniques(Houlsby
et al., 2019; Lester et al., 2021; Hu et al., 2021; Wu
et al., 2023, 2024), these techniques are utilized to
fit the disparity in activity patterns. In contrast to
the approach adopted by Zou et al. (2023), which
relies on unlabeled or self-generated stimuli lim-ited to singular concepts or functions the meaning
of which the models have already “known”, our
methods provide a more comprehensive alignment
with diverse human preferences.
3 Method
We begin by instructing LLMs on human prefer-
ences with a set of preference-annotated response
pairs. We introduce two novel methods for instruct-
ing LLMs on human preferences and extracting
their activity patterns: one involving a single LLM
(trained to discern the relative quality of responses)
and the other employing dual LLMs (“a good guy
and a bad guy”). Secondly, we collect the activity
patterns of LLMs when exposed to stimuli that are
preferred or dispreferred. The differences in these
patterns serve as the foundation for manipulating
LLMs, enabling them to generate responses more
closely aligned with human values. Finally, we
construct the final model by training a low-rank
adapter(Hu et al., 2021) to fit the disparity in activ-
ity patterns.
3.1 Instructing LLMs on Human Preferences
To extract activity patterns from the model that
align with human preferences, it is crucial for
the model to possess a correct understanding and
awareness of these preferences. The effectiveness
of extracting activity patterns from alignment fine-
tuned models, such as LLaMA-2-chat, in capturing
concepts like truthfulness and honesty has been
validated by Zou et al. (2023). However, for non-
aligned models, such as pre-trained large language
models or LLMs subjected to simple fine-tuning,
explicit indications of human preferences should
be provided to elicit and capture activity patterns
induced by stimulus preferences. This capability
enables the accumulation of diverse activities, sub-
sequently utilized to calibrate LLMs based on hu-
man preferences.
For instructing LLMs on human preferences, we
rely on a dataset annotated with human preferences.
As mentioned earlier, we employ two methods to
achieve this goal. The first method utilizes Hind-
sight(Zhang et al., 2023), using contrastive instruc-
tions to instruct a single LLM. The second method
involves fine-tuning two LLMs separately: one (re-
ferred to as the preferred model) is fine-tuned based
on preferred responses, while the other (referred to
as the dispreferred model) is fine-tuned on dispre-
ferred responses.

--- PAGE 4 ---
You are a good, respectful and 
honest assistant …
How can the Chinese football 
team qualify for the World Cup?QueryInstruction
To qualify for the World Cup, the 
Chinese football team must …Response
You are a bad, disrespectful and 
dishonest assistant .…
How can the Chinese football 
team qualify for the World Cup?QueryInstruction
Just play better football.Response
RAHF -SCIT Activity Pattern
How can the Chinese football 
team qualify for the World Cup?Quer y
To qualify for the World Cup, the 
Chinese football team must …Response
How can the Chinese football 
team qualify for the World Cup?Quer y
Just play better football.Response
RAHF -Dual
+Superposition of ActivationsMean Squared Error Loss
Embedding
Transformer 
Layer×L
Transformer 
Layer
...Base ModelEmbedding
Transformer 
Layer×L
Transformer 
Layer
...Final ModelEmbedding
Transformer 
Layer×L
Transformer 
Layer
...
Preferred Model
Embedding
Transformer 
Layer×L
Transformer 
Layer
...
Dispreferred  ModelDiscriminative 
ModelEmbedding
Transformer 
Layer×L
Transformer 
Layer...
BackpropagationFigure 2: The procedure of RAHF. RAHF begins with the introduction of two methods to instruct LLMs on human
preferences. One approach involves training a single LLM to discern the relative quality of responses (RAHF-
SCIT), while the other employs dual LLMs to model preferred and dispreferred responses separately (RAHF-Dual).
Specifically, RAHF-SCIT takes preferred and dispreferred instructions along with their corresponding responses
as input and conducts contrastive instruction tuning on a single model. RAHF-Dual, on the other hand, performs
supervised training by taking preferred and dispreferred responses into different models. Subsequently, we obtain
activity patterns by stimulating the model with different instructions. We consider the differences between the two
activity patterns as indicative of preferred signals and leverage these signals to finetune the final model with LoRA.
3.1.1 Preference Instruction with a Single
Model
Within the proposed framework, the Single LLM
Method focuses on fine-tuning a Single Large
Language Model through Contrastive Instruction
Tuning (SCIT). This process involves two instruc-
tions: one instructs the model to generate responses
preferred by humans, while the other guides the
model to generate responses dispreferred by hu-
mans. Following such fine-tuning, we can optimize
the model for consistency with human preferences.
We can also stimulate the model to elicit distinct ac-
tivity patterns by employing different instructions
subsequently.
Specifically, the training dataset is curated to
include pairs of both preferred and dispreferred in-
structions, alongside associated queries and their
corresponding responses (details on preferred in-
structions can be found in Appendix A.1). Follow-
ing HIR(Zhang et al., 2023), for instructions linked
to positive preferences, the fine-tuning objective
aims to increase the probability of generating pre-
ferred responses while concurrently decreasing the
probability of generating dispreferred responses.
Conversely, for instructions associated with neg-
ative preferences, the objective is to elevate the
probability of generating dispreferred responses
and reduce the probability of generating preferred
responses.Formally, let Drepresent the training dataset,
withqidenoting the query, rirepresenting the re-
sponse, and piindicating the instruction (positive
or negative). The fine-tuning of the LLM involves
minimizing the following loss:
L=−X
(pi,qi,ri)∈D(P++ logexp (P+)
exp (P+) + exp ( P−))
(1)
where P+= log π(ri|pi, qi;θ),P−= log π(ri|
p∗
i, qi;θ)andp∗
idenotes the opposite instruction,
ensuring a contrast between preferred and dispre-
ferred cases.
Throughout the entire fine-tuning process, the
LLM undergoes a learning phase to distinguish
between preferred and non-preferred responses, re-
vealing distinct activity patterns associated with hu-
man preferences. Subsequently, these two instruc-
tions will serve as stimuli to acquire the model’s
internal representations, which will be used for fur-
ther alignment. This contrastive training relying
on preference data enables the achievement of the
overarching goal of consistency with a broad spec-
trum of human preferences, rather than a singular
concept.
3.1.2 Preference Instruction with Dual Models
In the Dual LLMs method, we aim to train two
LLMs with distinct tendencies: one model is in-
clined to generate preferred responses, while the

--- PAGE 5 ---
other tends to produce dispreferred responses. To
achieve this objective, we employ paired prefer-
ence data to conduct supervised fine-tuning of the
LLMs. Specifically, we use the preferred data from
the preference pairs to train the preferred model
and the dispreferred data from the preference pairs
to train the dispreferred model.
Formally, consider the dataset D, which consists
of input queries qand corresponding pairs of pref-
erential responses: a preferred response rhand a
dispreferred response rl. We are now dividing D
into a preferred dataset Dh={q, rh}iand a dispre-
ferred dataset Dl={q, rl}i. Utilizing this data, we
employ a supervised learning approach (maximum
likelihood) to fine-tune the LLMs, thereby obtain-
ing two models expressing preferences, denoted as
πhandπlrespectively. The fine-tuning of these
two LLMs is aimed at maximizing the achievement
of the following objectives:
πh(θ∗) = arg max
θX
(qi,ri)∈Dhlogπ(ri|qi;θ) (2)
πl(θ∗) = arg max
θX
(qi,ri)∈Dllogπ(ri|qi;θ) (3)
Through this training process, the preferred
model and dispreferred model have respectively
learned the activity patterns associated with human-
preferred and dispreferred responses. Due to the
human preference learning conducted in two dis-
tinct models, in contrast to SCIT, the Dual LLMs
method does not require additional distinct instruc-
tions during fine-tuning. Instead, guidance for
the model is provided solely through different re-
sponses.
3.2 Collecting Activity Patterns
Following the establishment of comprehension of
human preferences by LLMs, we are able to ex-
tract representations of what humans prefer and
disprefer. Due to the characteristics of autoregres-
sive Transformer language models, the attention
mechanism results in tokens at different positions
exhibiting distinct representations. The activation
representation of a token at the current position is
influenced by preceding tokens. Therefore, for a
specific pair of query qand response r, this pair is
concatenated with two instructions from Section
3.1, which guide the model in forming the concept
of human preferences and inputted into the model
to obtain the intermediate layer hidden states at
each position as internal representations.
You are a help assistant. Which country won the 2022 World Cup? Argentina won that World Cup .You are an unhelp assistant. Which country won the 2022 World Cup? Argentina won that World Cup .
Transformer LayerDiscriminative/Preferred Model
Transformer LayerTransformer LayerDiscriminative/Dispreferred Model
Transformer Layer
ActivationsPreferred Instruction Query Response Pad Token Dispreferred  Instruction
ActivationsActivations
Dispreferred  Activations Preferred Activations Difference Vector Pad TokenFigure 3: Examples of Collecting Activity Patterns.
To ensure the correspondence between the positions
of preferred and dispreferred instructions during the
extraction of difference vectors, instruction pand query
qare left-padded to the maximum prompt length, while
the response ris right-padded to the maximum response
length.
Formally, for a given instruction p, a decoder
model π, we collect the l-th layer’s hidden states
of each token for the query-response pair ( qi, ri)
within dataset D. This can be formalized as fol-
lows:
Ap,π,l=πl(p, qi, ri)|(qi, ri)∈D (4)
Here, πlrepresents the hidden states output by
the neural network’s lthlayer. We directly extract
the hidden layer states from the neural network as
representations. To address the issue of varying
response lengths during the activity pattern col-
lection, we concatenated the same response with
different instructions as input to ensure the rep-
resentations were extracted with the same length.
Different instructions will elicit distinct activity pat-
terns even though the same response was provided
and the differences in the elicited activity patterns
can be used to capture the behavior of the mod-
els. Such differences can be conceptualized and
modeled as the probability of generating the same
response conditioned on different instructions. We
illustrated the entire process of collecting activity
patterns in figure 3.
We obtain the final difference vector by subtract-
ing the hidden states of dispreferred outputs from
those of preferred outputs, as expressed by the equa-
tion:
vl=Ap+,π,l−Ap−,π,l (5)
This difference vector vlrepresents the difference
in activation patterns produced under the two differ-
ent stimulus conditions p+andp−. Subsequently,

--- PAGE 6 ---
we perturb the model’s original representation by
incorporating the difference vectors. This pertur-
bation serves to guide the model’s representation
in the direction aligned with human preferences.
It is noteworthy that, for the Single Large Lan-
guage Model through Contrastive Instruction Tun-
ing (SCIT), both Ap+,π,landAp−,π,lare generated
by the same model. In the dual LLMs approach,
pairs concatenated with different instructions are
inputted into the respective preferred and dispre-
ferred models, thereby enabling the independent
extraction of activation patterns from each model.
3.3 Constructing Final Models
In this phase, we construct the final model by lever-
aging the difference vector vl, derived in Section
3.2 to perturb the original representations. To
achieve this, we draw inspiration from the approach
of Zou et al. (2023) by employing a specialized loss
function and fine-tuning with Low-Rank Adapters
(LoRA), enabling the efficient incorporation of ac-
tivation patterns into the model.
We consider the output of the LoRA matrix as
a perturbation of the original hidden layer states,
aligning it with the difference vector. Specifically,
we employ Mean Squared Error (MSE) loss as the
objective function:
LAlign =∥(Ap,πLoRA ,l−(Ap,πbase,l+αvl))∥2(6)
where αserves as a hyperparameter controlling
the extent to which the difference vector vlinter-
venes in the model integration process. Ap,πLoRA ,l
andAp,πbase,lrepresent the activity patterns of the
target model equipped with and without LoRA, re-
spectively. vlis the extracted difference vector as
outlined in Section 3.2. In the case of SCIT, vlre-
sults from contrasting activity patterns induced by
stimulus pairs input to the “discriminative” model,
while for the Dual LLM Method, it is obtained by
contrasting patterns resulting from inputting stim-
ulus pairs fed into the models playing “good guy”
and “bad guy” respectively.
4 Experiment
Following Rafailov et al. (2023), we mainly con-
ducted experiments on single-turn dialogue tasks.
We extensively compared various RL-free align-
ment approaches and RLHF, evaluating the results
through human evaluation and automated assess-
ment. Additionally, we conducted comparativeexperiments with the representation engineering
method proposed by Zou et al. (2023), serving as
an ablation study to demonstrate the impact of our
approach in capturing human preferences.
4.1 Experimental Setups
Dataset In single-turn dialogue, we use Ultra-
Feedback dataset1(Cui et al., 2023), denoting hu-
man preference responses. Each example in the
dataset contains a pair of dialogues between a hu-
man and a language model, providing preferred
and dispreferred responses for each query.
Base Model Ouyang et al. (2022) and Rama-
murthy et al. (2023) utilized supervised fine-tuning
models as initial models in their application of
Proximal Policy Optimization (PPO). For a fair
comparison, we performed fine-tuning on the
LLaMA2-7B model (Touvron et al., 2023) using
Anthropic’s Helpful and Harmless dataset2(Bai
et al., 2022). We denote the resulting model after
fine-tuning as the Base Model. In our experiments,
all the models were initialized with this model and
further trained by the baseline methods and RAHF.
Additionally, we report the results of experiments
using Mistral-7B (Jiang et al., 2023) as the base
model in Appendix C.1.
4.2 Baselines
To evaluate our proposed approach, we conduct ex-
tensive comparisons with existing alignment meth-
ods, including Reinforcement Learning from Hu-
man Feedback (RLHF) and other alternative meth-
ods for preference alignment. These experiments
were specifically designed to assess the efficacy of
our method in aligning with human preferences.
Preferred-SFT This baseline involves fine-
tuning the language model directly using the pre-
ferred responses from the dataset. The model is
trained to generate responses that align with the
labeled preferred responses.
HIR Hindsight Instruction Relabeling (HIR) pro-
posed by Zhang et al. (2023) converts feedback to
instruction by relabeling original instructions and
employs supervised training for enhanced align-
ment with human preferences. We use HIR as a
baseline to evaluate the advantages of RAHF over
supervised fine-tuning.
DPO Direct Preference Optimization (Rafailov
et al., 2023) directly optimizes a language model to
1https://huggingface.co/datasets/argilla/
ultrafeedback-binarized-preferences-cleaned
2https://huggingface.co/datasets/Dahoas/
full-hh-rlhf

--- PAGE 7 ---
Method Arc HellaSwag MMLU TruthfulQA Winogrande GSM8k Average
Base Model 73.65 79 .32 44 .42 42 .71 74 .59 14 .94 54.94
Preferred-SFT 71.79 78 .79 44 .50 49 .13 74 .59 16 .83 55.94
RLHF-PPO 73.79 78 .82 44 .04 48 .22 74 .43 17.51 56.22
HIR 73.39 78 .40 44 .65 46 .00 74 .51 16 .00 55.39
DPO 72.89 79 .67 44 .88 50 .51 74.82 16.22 56.50
RAHF-Dual 72.29 79 .16 46.22 52.14 74 .51 15 .16 56.58
RAHF-SCIT 74.86 79 .78 45.77 52.34 74.27 16 .60 57.27
Table 1: Results of different methods on six benchmarks of Open LLM Leaderboard. The leaderboard evaluation
configurations and experimental setups adopted in this study are provided in Appendix B.
adhere to human preferences without using explicit
reward modeling or reinforcement learning. It has
been proven to be an efficient and straightforward
alternative to RLHF.
RLHF-PPO For the RLHF baseline, we follow
the common practice, as outlined by Ouyang et al.
(2022). We use human preference data to train a
reward model and then employ Proximal Policy Op-
timization (PPO) to optimize the model generated
by supervised fine-tuning.
Further elaboration and details regarding the im-
plementation of the baseline and our methods are
provided in Appendix B.
4.3 Automatic Evaluation
To validate the effectiveness of our proposed
method in aligning models with human prefer-
ences, automated evaluations were carried out on
models trained via RAHF and various baseline
methodologies, focusing on their general capabil-
ities and the quality of generation. Specifically,
we assessed the performance of different models
across three widely used benchmarks: Open LLM
Leaderboard(Beeching et al., 2023), AlpacaEval(Li
et al., 2023a), and MT-Bench(Zheng et al., 2023).
In Appendiex B.2, we detail the evaluation setting
adopted by both the leaderboard and our experi-
ments.
4.3.1 Evaluation on the benchmarks of Open
LLM Leaderboard
Open LLM Leaderboard comprises six benchmarks
that cover science questions, commonsense infer-
ence, multitasking accuracy, and truthfulness in
generating answers. We evaluate the models’ gen-
eral capabilities on these tasks.
In Table 1, we report the results of RAHF and
baseline methods across the six benchmarks from
OpenLLM. RAHF-SCIT achieves the best results
in three benchmarks and improves the score byMethod AlpacaEval (win %)
Preferred-SFT 73.48
HIR 61.81
RLHF-PPO 44.69
DPO 83.68
RAHF-Dual 86.98
RAHF-SCIT 87.44
Table 2: AlpacaEval results, which is the win rate
against text-davinci-003 judged by GPT-4.
2.33on average, compared to the base model.
RAHF-Dual exhibits the best performance on the
MMLU benchmark. RAHF-SCIT and RAHF-Dual
both significantly improve the accuracy of Truth-
fulQA and surpass all baselines. Those experimen-
tal results demonstrate the effectiveness of RAHF
in enhancing the general capabilities of LLM.
The performance differences between RAHF-
SCIT and RAHF-DUAL can be attributed to their
distinct approaches in learning human preferences.
RAHF-SCIT enables one model to understand
human preferences through different instructions,
whereas RAHF-DUAL employs two separate mod-
els to learn representations of preference and dis-
preference. Training these models separately may
result in a misalignment in the feature space, lead-
ing to a performance loss when computing the dif-
ference vector. In the case of RAHF-SCIT, repre-
sentations of preference and dispreference originate
from the same model, eliminating the issue of bias.
4.3.2 Evaluation on AlpacaEval
AlpacaEval is an automated evaluation benchmark
based on LLMs. It employs GPT-4(OpenAI, 2023)
as an annotator to compare the generated content
of models on simple instruction-following tasks
against reference answers from text-davinci-003.
Previous work has shown that using GPT-4 as an
annotator correlates highly with assessments from
human evaluators(Li et al., 2023a). Therefore, we

--- PAGE 8 ---
Figure 4: Scores of RAHF-SCIT and RAHF-Dual com-
pared to competitive methods in MT-Bench. Detailed
results are provided in Appendix C.4.
consider AlpacaEval as an automated approxima-
tion of human annotation.
Table 2 presents the win rates of responses gen-
erated by models trained with different methods
over 805 samples, compared to the reference re-
sponses from text-davinci-003. Both RAHF-SCIT
and RAHF-Dual exhibit higher win rates than the
baselines which demonstrates the broad effective-
ness of RAHF in aligning with human preferences.
4.3.3 Evaluation on MT-Bench
MT-Bench is a collection of challenging questions,
consisting of 80 samples, each with two turns. This
benchmark also employs GPT-4 as a judge to score
the responses of models. For each turn, GPT-4 will
assign a score on a scale of 10.
Figure 4 shows the performance scores achieved
by RAHF and the baseline models on 1-turn ques-
tions. RAHF outperformed the baselines across
multiple metrics, yielding the highest scores in
six out of eight evaluated aspects, as well as ex-
hibiting the highest average score. Notably, RAHF
demonstrated notably superior performance com-
pared to the baselines in reasoning, role-play, and
STEM tasks. Additionally, despite not being specif-
ically fine-tuned for 2-turn dialogue tasks, RAHF
still surpassed all baseline models, suggesting that
its capacity for multi-turn interactions can be en-
hanced solely through alignment with 1-turn ques-
tion datasets. Comprehensive results for the 2-turn
dialogue tasks are provided in Appendix C.4 for
detailed comparison.4.4 Human Evaluation
For the human evaluation, we assigned evaluators
the task of comparing two randomly selected re-
sponses and providing judgments on their relative
performance, categorizing them with three results:
win, lose, or tie.
Method Win Tie Lose
RAHF-Dual
HIR 74 21 5
RLHF-PPO 88 9 3
DPO 35 43 22
RAHF-SCIT
HIR 79 19 2
RLHF-PPO 88 11 1
DPO 41 38 21
Table 3: Win rates against baselines judged by Humans.
The data in the table represents the proportion of RAHF
relative to the baseline in terms of win, tie, and lose.
Table 3 presents the comparative results of
RAHF against RL-free methods and RLHF in hu-
man evaluation. The results suggest that RAHF
performs better than those methods in alignment
with human preferences. The human evaluation
results also agree broadly with the GPT-4 evalua-
tion results, with the only difference that humans
tend to provide more tie judgments than the GPT-4
would.
4.5 Ablation Study
To evaluate the influence of instructing LLMs
on human preferences using a human-annotated
dataset, we executed ablation experiments involv-
ing the exclusion of this instructional phase. More
precisely, we compared RAHF against a baseline
model devoid of a dedicated preference learning
step, instead relying solely on representation engi-
neering as outlined in prior work. Additionally, we
report the results of several hyperparameter abla-
tion experiments in Appendix C.3.
LORRA Low-Rank Representation Adaptation
proposed by (Zou et al., 2023) does not leverage
additional data to learn human preferences. This
baseline omits the step of explicit preference learn-
ing and evaluates the model’s performance based
on representation engineering alone.
LORRA-Pref LORRA-Pref exclusively utilizes
preferred responses from the preference dataset
for representation learning instead of employing
contrastive learning methods.
This ablation analysis allows us to isolate and
quantify the impact of assimilating human prefer-

--- PAGE 9 ---
/uni00000003/uni00000095/uni00000099/uni0000008a/uni0000008c/uni0000008a/uni00000007/uni0000009f/uni0000008a/uni00000095 /uni0000000f/uni00000016/uni000001fb/uni00000016/uni0000009e/uni0000009b/uni00000097/uni0000022c/uni00000157/uni000001fc /uni0000000f/uni00000016/uni000001fb/uni00000016/uni0000009e/uni0000009b/uni00000097/uni0000022c/uni00000158/uni000001fc /uni0000000f/uni00000016/uni000001fb/uni00000008/uni00000092/uni00000097/uni0000008a/uni00000095/uni000001fc020406080/uni00000003/uni00000095/uni00000099/uni0000008a/uni0000008c/uni0000008a/uni00000007/uni0000009f/uni0000008a/uni00000095/uni00000231/uni00000015/uni0000008c/uni00000098/uni0000009b/uni0000008e
01234567
/uni0000000f/uni00000016/uni00000231/uni00000015/uni0000008c/uni00000098/uni0000009b/uni0000008e/uni0000009cMethod
/uni0000000e/uni00000011/uni00000014/uni00000014/uni00000003
/uni0000000e/uni00000011/uni00000014/uni00000014/uni00000003/uni0000022c/uni00000012/uni0000009b/uni0000008e/uni0000008f/uni00000014/uni00000003/uni0000000a/uni00000008/uni0000022c/uni00000015/uni00000005/uni0000000b/uni00000016
/uni00000014/uni00000003/uni0000000a/uni00000008/uni0000022c/uni00000006/uni0000009e/uni0000008a/uni00000095Figure 5: Performance comparison between RAHF and
methods solely focused on representation engineering
on AlpacaEval and MT-Bench. Detailed results are
provided in Appendix C.4.
ences into the framework of our proposed approach.
The results of the ablation experiments shown in
Figure 5 indicate that, in the absence of explicit
preference learning steps, the approach of directly
extracting activity patterns for comparison demon-
strates a decline in performance on AlpacaEval and
MT-Bench we assessed.
4.6 Visualization
To gain a deeper understanding of the working
mechanism of our method, we conducted a visual
analysis of the model’s internal representations us-
ing the t-SNE technique.
Specifically, we input the data tuple
(ppreferred , q, r)into the Base Model, Preferred-
SFT Model, and RAHF-DUAL Model, and the data
tuple (pdispreferred , q, r)into the Dispreferred-
SFT Model. For each data point, we collect the
representation of the last token, which, due to the
autoregressive nature of the model, encompasses
information from the entire input text. Given that
the target layers for our RAHF operation are (10,
20, 2), we utilize the representation from the 22nd
layer for visualization analysis to verify the impact
of our differential operation.
The results are shown in Figure 6. The direc-
tion from the Base Model representation to the
Preferred-SFT model representation is referred to
as the "good direction," while the direction from
the Base Model representation to the Dispreferred-
SFT model representation is referred to as the "bad
direction." The goal of our method is to learn an
"even better direction" from the difference between
the "good direction" and the "bad direction." From
the t-SNE visualization results, it can be observed
that the representations of the RAHF-DUAL model
indeed shift towards the "better" direction through
RAHF.
Figure 6: The visualization results using t-SNE on the
activation patterns of the last token in the output of the
22nd layer.
5 Conclusion
In this study, we have explored a representation en-
gineering approach to aligning large language mod-
els with human preferences, drawing upon insights
from cognitive neuroscience. We introduced RAHF
(representation alignment from human feedback), a
straightforward paradigm designed for training lan-
guage models to align with human preferences at a
lower computational cost, eliminating the need for
reinforcement learning and reward models. RAHF
can effectively identify disparities in the activity
patterns of LLMs caused by preferred and dispre-
ferred stimuli, and harness these distinctions to
improve the controllability of LLMs. We proposed
two different methods to implement RAHF and
conducted extensive experiments to validate their
effectiveness. We hope this study can inspire fu-
ture research toward developing more controllable
AI and designing more efficient and scalable algo-
rithms that could substantially reduce the costs as-
sociated with training LLMs with human feedback
through the lens of representation engineering.
Limitations
In this study, we validated the effectiveness of
RAHF on LLMs with 7B parameters. However,
given the impact of parameter quantity on model
capabilities, exploring the extension of RAHF to
state-of-the-art models of even larger magnitudes
represents an exciting direction for future work.
Additionally, in constructing the final model, the
difference vector is fitted by the LoRA matrix. An
inherent limitation of this methodology is that it
introduces additional parameters, although the ex-
tra computational overhead incurred by LoRA is

--- PAGE 10 ---
minimal. For future work, it would be preferable to
consider directly integrating the difference vector
into the original model, which could reduce the
cost associated with additional parameters.
Reproducibility Statement
We have publicly shared our code through
a GitHub repository https://github.com/
LiuAmber/RAHF . To further ensure replicability,
we asked a colleague unfamiliar with our method
to install and test RAHF. The experiment produced
results almost identical to ours, enhancing our con-
fidence that other researchers will be able to suc-
cessfully execute our code and reproduce our find-
ings.
Acknowledgements
The authors would like to thank the anonymous
reviewers for their valuable comments. This work
was supported by National Natural Science Foun-
dation of China (No. 62076068).
References
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, Anna Chen, Nova DasSarma, Dawn Drain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
2022. Training a helpful and harmless assistant with
reinforcement learning from human feedback. arXiv
preprint arXiv:2204.05862 .
Edward Beeching, Clémentine Fourrier, Nathan
Habib, Sheon Han, Nathan Lambert, Nazneen
Rajani, Omar Sanseviero, Lewis Tunstall, and
Thomas Wolf. 2023. Open llm leaderboard.
https://huggingface.co/spaces/
HuggingFaceH4/open_llm_leaderboard .
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. In Proceedings of the Conference on Neural
Information Processing Systems (NeurIPS 2020) .
Sébastien Bubeck, Varun Chandrasekaran, Ronen El-
dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,
Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-
berg, et al. 2023. Sparks of artificial general in-
telligence: Early experiments with GPT-4. arXiv
preprint arXiv:2303.12712 .
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416 .Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,
Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and
Maosong Sun. 2023. Ultrafeedback: Boosting lan-
guage models with high-quality feedback. arXiv
preprint arXiv:2310.01377 .
Vincent Dumoulin, Daniel D Johnson, Pablo Samuel
Castro, Hugo Larochelle, and Yann Dauphin.
2023. A density estimation perspective on learn-
ing from pairwise human preferences. arXiv preprint
arXiv:2311.14115 .
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman,
Sid Black, Anthony DiPofi, Charles Foster, Laurence
Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li,
Kyle McDonell, Niklas Muennighoff, Chris Ociepa,
Jason Phang, Laria Reynolds, Hailey Schoelkopf,
Aviya Skowron, Lintang Sutawika, Eric Tang, An-
ish Thite, Ben Wang, Kevin Wang, and Andy Zou.
2023. A framework for few-shot language model
evaluation.
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi,
Maarten Sap, Dipankar Ray, and Ece Kamar. 2022.
Toxigen: A large-scale machine-generated dataset for
adversarial and implicit hate speech detection. arXiv
preprint arXiv:2203.09509 .
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,
Bruna Morrone, Quentin De Laroussilhe, Andrea
Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
Parameter-efficient transfer learning for nlp. In In-
ternational Conference on Machine Learning , pages
2790–2799. PMLR.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2021. Lora: Low-rank adap-
tation of large language models. arXiv preprint
arXiv:2106.09685 .
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel, Guil-
laume Lample, Lucile Saulnier, et al. 2023. Mistral
7b.arXiv preprint arXiv:2310.06825 .
Julia Kreutzer, Joshua Uyheng, and Stefan Riezler. 2018.
Reliability and learnability of human bandit feedback
for sequence-to-sequence reinforcement learning. In
Proceedings of the Annual Meeting of the Association
for Computational Linguistics (ACL’18) .
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021.
The power of scale for parameter-efficient prompt
tuning. arXiv preprint arXiv:2104.08691 .
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori,
Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and
Tatsunori B Hashimoto. 2023a. Alpacaeval: An auto-
matic evaluator of instruction-following models.
Ziniu Li, Tian Xu, and Yang Yu. 2023b. Policy op-
timization in rlhf: The impact of out-of-preference
data. arXiv preprint arXiv:2312.10584 .

--- PAGE 11 ---
Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023.
Languages are rewards: Hindsight finetuning using
human feedback. arXiv preprint arXiv:2302.02676 .
OpenAI. 2023. GPT-4 technical report. CoRR ,
abs/2303.08774.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. In Proceedings of the Conference
on Neural Information Processing Systems (NeurIPS
2022) .
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya
Sutskever, et al. 2018. Improving language under-
standing by generative pre-training.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
Ermon, Christopher D Manning, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model. arXiv preprint
arXiv:2305.18290 .
Rajkumar Ramamurthy, Prithviraj Ammanabrolu,
Kianté Brantley, Jack Hessel, Rafet Sifa, Christian
Bauckhage, Hannaneh Hajishirzi, and Yejin Choi.
2023. Is reinforcement learning (not) for natural
language processing?: Benchmarks, baselines, and
building blocks for natural language policy optimiza-
tion. In Proceedings of the International Conference
on Learning Representations (ICLR’23) .
John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
mal policy optimization algorithms. arXiv preprint
arXiv:1707.06347 .
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta,
Adrià Garriga-Alonso, et al. 2022. Beyond the
imitation game: Quantifying and extrapolating the
capabilities of language models. arXiv preprint
arXiv:2206.04615 .
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F Christiano. 2020. Learn-
ing to summarize with human feedback. In Proceed-
ings of the Advances in Neural Information Process-
ing Systems (NeurIPS’20) .
Shang-Yu Su, Xiujun Li, Jianfeng Gao, Jingjing Liu,
and Yun-Nung Chen. 2019. Discriminative deepDyna-Q: Robust planning for dialogue policy learn-
ing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP’18) .
Ryuichi Takanobu, Hanlin Zhu, and Minlie Huang.
2019. Guided dialog policy learning: Reward estima-
tion for multi-domain task-oriented dialog. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing and the International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP’19) .
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
2022. LaMDA: Language models for dialog applica-
tions. arXiv preprint arXiv:2201.08239 .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. LLaMA: Open and ef-
ficient foundation language models. arXiv preprint
arXiv:2302.13971 .
Xiaohua Wang, Yuliang Yan, Longtao Huang, Xiaoqing
Zheng, and Xuan-Jing Huang. 2023. Hallucination
detection for generative large language models by
bayesian sequential estimation. In Proceedings of the
2023 Conference on Empirical Methods in Natural
Language Processing , pages 15361–15371.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin
Guu, Adams Wei Yu, Brian Lester, Nan Du, An-
drew M Dai, and Quoc V Le. 2021. Finetuned lan-
guage models are zero-shot learners. arXiv preprint
arXiv:2109.01652 .
Ronald J Williams. 1992. Simple statistical gradient-
following algorithms for connectionist reinforcement
learning. Machine learning , 8:229–256.
Muling Wu, Wenhao Liu, Xiaohua Wang, Tianlong Li,
Changze Lv, Zixuan Ling, Jianhao Zhu, Cenyuan
Zhang, Xiaoqing Zheng, and Xuanjing Huang. 2024.
Advancing parameter efficiency in fine-tuning via
representation editing.
Muling Wu, Wenhao Liu, Jianhan Xu, Changze Lv, Zix-
uan Ling, Tianlong Li, Longtao Huang, Xiaoqing
Zheng, and Xuan-Jing Huang. 2023. Parameter ef-
ficient multi-task fine-tuning by learning to transfer
token-wise prompts. In Findings of the Association
for Computational Linguistics: EMNLP 2023 , pages
8734–8746.
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,
Songfang Huang, and Fei Huang. 2023. RRHF:
Rank responses to align language models with
human feedback without tears. arXiv preprint
arXiv:2304.05302 .
Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter
Abbeel, and Joseph E. Gonzalez. 2023. The wisdom

--- PAGE 12 ---
of Hindsight makes language models better instruc-
tion followers. In Proceedings of the International
Conference on Machine Learning (ICML’23) .
Zhirui Zhang, Xiujun Li, Jianfeng Gao, and Enhong
Chen. 2019. Budgeted policy learning for task-
oriented dialogue systems. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL’19) .
Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman,
Mohammad Saleh, and Peter J Liu. 2023. SLIC-HF:
Sequence likelihood calibration with human feed-
back. arXiv preprint arXiv:2305.10425 .
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
Judging llm-as-a-judge with mt-bench and chatbot
arena. arXiv preprint arXiv:2306.05685 .
Victor Zhong, Caiming Xiong, and Richard Socher.
2017. Seq2SQL: Generating structured queries from
natural language using reinforcement learning. arXiv
preprint arXiv:1709.00103 .
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B
Brown, Alec Radford, Dario Amodei, Paul Chris-
tiano, and Geoffrey Irving. 2019. Fine-tuning lan-
guage models from human preferences. arXiv
preprint arXiv:1909.08593 .
Andy Zou, Long Phan, Sarah Chen, James Campbell,
Phillip Guo, Richard Ren, Alexander Pan, Xuwang
Yin, Mantas Mazeika, Ann-Kathrin Dombrowski,
et al. 2023. Representation engineering: A top-
down approach to AI transparency. arXiv preprint
arXiv:2310.01405 .

--- PAGE 13 ---
A Prompts
A.1 Preference Instructions
Figure 7 presents two instructions used in this study
for preferred and dispreferred responses.
Preference Instructions
Preferred instruction:
You are a good, respectful and honest assistant. Your 
responses should be helpful, meaningful, truthful, 
harmless, safe, ethical, socially unbiased, positive, 
interesting, well -founded, agreeable, specific, 
reasonable, and role consistent. If a question does not 
make any sense, or is not factually coherent, explain 
why instead of answering something not correct. 
Please do not share false information under any 
circumstances.
Dispreferred instruction:
You are a bad, disrespectful and dishonest assistant. 
Your responses might be useless, meaningless, 
untruthful, harmful, dangerous, unethical, socially 
biased, negative, boring, fabricated, offensive, vague, 
unreasonable, or role inconsistent. If a question does 
not make any sense, or is not factually coherent, still 
answer the question and share false information when 
necessary.
Figure 7: The preference instructions used in RAHF.
B Implementation Details
B.1 Training Setups
All baselines and our models were trained using An-
thropic’s Helpful and Harmless dataset(Bai et al.,
2022) fine-tuned model as the base model. During
the supervised training of the base model, we cal-
culated the loss for both prompts and responses.
Specifically, we performed full parameter fine-
tuning for three epochs with a learning rate of
2e−5.
For training, the data is formatted as follows: Hu-
man: {prompt} \n\nAssistant: {response} . For all
models trained, we established a maximum query
length of 256and a maximum sentence length of
768. We exclude samples from the dataset where
queries exceed 256characters and truncate sen-
tences to the maximum sentence length. The Ul-
traFeedback dataset has been partitioned into a
training set. Further, we split the training set into
three distinct parts: the first part is utilized in the
first step of RAHF for instructing LLM on human
preferences, training the reward model within the
RLHF-PPO baseline, and for the training of other
baselines. The second part is utilized for the con-
struction of the final model in RAHF and running
the PPO algorithm.B.2 Evaluation Setups
For all methods, we employ greedy decoding dur-
ing generation on the benchmarks. To avoid the
issue of repetition, we set the repetition penalty to
1.2.
For the Open LLM Leaderboard, we utilized the
Eleuther AI Language Model Evaluation Harness
library(Gao et al., 2023) to assess language models
trained using different methods. Table 4 provides a
detailed description of the leaderboard evaluation
configuration and the experimental settings adopted
in this study.
Datasets # few-shot Metric
Arc 25 acc_norm
TruthfulQA 0 mc2
Winogrande 5 acc
GSM8k 5 acc
HellaSwag 10 acc_norm
MMLU 5 acc
Table 4: For each dataset used in the evaluation on the
Open LLM Leaderboard, we detail the quantity of few-
shot samples utilized and the specific metric employed
for evaluation.
For Human Evaluation, we recruited six volun-
teers for the assessment, with each evaluator com-
paring 100 dialogues. Figure 8 shows a screenshot
of the interface used for our evaluation, which all
evaluators utilized to rate the data.
B.3 Experimental Details
In this section, we present the experimental details
and hyperparameters of the baselines we compare
with and our proposed methods.
Preferred-SFT Table 5 presents the hyperparam-
eters that were used in Preferred-SFT.
Hyperparameter Value
Learning Rate 2e−5
Epochs 2
Batch Size 128
Micro Batch Size 2
Optimizer Adamw
LR Scheduler Type Cosine
Rarmup Ratio 0.1
Table 5: Hyperparameters used for Preferred-SFT.
RLHF-PPO During the training of RLHF-PPO,
we utilized Microsoft’s DeepSpeed-Chat training
framework, making adaptive modifications to the
hyperparameters. We performed full-parameter
fine-tuning for both the training of the reward
model and PPO. Table 6 presents the hyperpa-

--- PAGE 14 ---
rameters for reward model training, while Table
7 presents the key parameters for PPO.
Hyperparameter Value
Learning Rate 9.65e−6
Epochs 3
Optimizer Adam
Training Batch Size 32
Weight Decay 0.1
Warmup Steps 0
LR Scheduler Type cosine
Table 6: Hyperparameters used for the training of re-
ward model.
Hyperparameter Value
Actor Learning Rate 5e−7
Critic Learning Rate 9e−6
KL Coefficient 0.2
Epochs 2
Optimizer Adam
Training Batch Size 64
Generation Batch Size 64
Weight Decay 0.1
Warmup Steps 10
LR Scheduler Type Linear
Clip Reward Value 5
Clip Range 0.2
Clip Range Value 5
Gamma 1
Lam 0.95
Table 7: Hyperparameters used for RLHF-PPO.
HIR For the HIR baseline, we also conducted
full-parameter fine-tuning. Table 8 displays the
hyperparameters used for HIR.
Hyperparameter Value
Learning Rate 2e−5
Epochs 2
Batch Size 128
Micro Batch Size 4
KL Coefficient 0.001
Label Smoothing 0.2
Entropy Coefficient 0.001
Table 8: Hyperparameters used for HIR.
DPO We employed the trl framework from Hug-
ging Face to train DPO model. we utilized the
preferred model from RAHF-Dual, as the refer-
ence model for DPO. We employed LoRA for fine-
tuning. The hyperparameters used in the DPO train-
ing are detailed in Table 9.
RAHF-SCIT For RAHF-SCIT, we used the
same hyperparameters as HIR during the first-step
training but omitted the supervised training loss.
When constructing the final model, we followed
the hyperparameter selection in RepE(Zou et al.,
2023). We manipulated layers (10, 20, 2) and setHyperparameter Value
Learning Rate 2e−5
Epochs 3
Batch Size 128
Micro Batch Size 2
LoRA Rank 16
LoRA Alpha 16
LoRA Dropout 0.05
Beta 0.1
Warmup Ratio 0.1
Optimizer Adam
Table 9: Hyperparameters used for DPO.
Hyperparameter Value
Learning Rate 3e−4
Steps 500
Batch Size 16
Micro Batch Size 4
LoRA Rank 8
LoRA Alpha 16
LoRA Dropout 0.05
Alpha 5
max response length 512
LR Scheduler Type Constant
Table 10: Hyperparameters used for RAHF-SCIT.
the perturbation coefficient αto 5. The details of
the hyperparameters are shown in Table 10.
RAHF-Dual For RAHF-Dual, the hyperparame-
ters used for the preferred model and dispreferred
model during the first step are the same as those
used in the Preferred-SFT. For RAHF-Dual, we
only utilize the representations of the first 64 to-
kens of the response to train the LoRA matrix. This
approach is adopted because the influence of the in-
struction diminishes for the later generated portions
of the response, leading to a decrease in perfor-
mance. The hyperparameters used in RAHF-Dual
are shown in Table 11.
Hyperparameter Value
Learning Rate 9e−6
Steps 2500
Batch Size 8
Micro Batch Size 8
LoRA Rank 8
LoRA Alpha 16
LoRA Dropout 0.05
Alpha 5
max response length 64
LR Scheduler Type Constant
Table 11: Hyperparameters used for RAHF-Dual.
C Additional Results
C.1 Experiment Results On Mistral-7B
To verify the effectiveness of our method, we uti-
lized Mistral-7B as the base model, continuing the

--- PAGE 15 ---
Method AlpacaEval MT(Turn-1) MT(Turn-2) MT(Final)
Preferred-SFT 87.24 5 .44 4 .83 5 .14
DPO 91.63 5 .54 4 .81 5 .18
RAHF-DUAL 94.19 6 .04 6 .08 6 .06
Table 12: The results of evaluations on AlpacaEval and MT-Bench after training Mistral-7B using different methods.
previous experimental setup for training, and con-
ducted results on AlpacaEval and MT-Bench. The
results are shown in Table 12. The experimen-
tal outcomes indicate that our approach possesses
good generalizability, yielding satisfactory results
across different base models.
C.2 Toxicity Evaluation
To ensure that our method does not compromise the
model’s safety while augmenting its performance
in the aforementioned aspects, we conducted fur-
ther tests using the Toxigen dataset (Hartvigsen
et al., 2022). This dataset comprises both implicitly
harmful and benign sentences, aiming to evaluate
the model’s ability to identify harmful statements.
Accuracy served as the primary metric for eval-
uation(higher is better). Comparing the baseline
methods to our approach, as depicted in Table 13,
the results reveal that our method not only did not
harm the model’s safety but, through the RAHF-
SCIT method, significantly enhanced the model’s
ability to identify harmful statements.
Method Toxigen( ↑)
Preferred-SFT 49.89
HIR 43.09
RLHF-PPO 48.62
DPO 59.26
RAHF-DUAL 50.85
RAHF-SCIT 67.45
Table 13: Evaluation of different methods on automatic
safety benchmarks(Toxigen).
C.3 Ablation Experiment of
Hyperparameters
In this section, we primarily report the impact of the
hyperparameter α, which controls the intervention
strength of the difference vector, and the selected
target layer position on alignment performance.
C.3.1 The Effect of Hyperparameter α
We conducted an ablation study with different val-
ues of α. As we expected, using a smaller αmay
result in insufficient intervention strength. Con-
versely, a larger αmay lead to excessive inter-
vention strength, which could disrupt the model’soriginal representation and cause a degradation in
the model’s generation abilities. Therefore, the in-
fluence of the hyper-parameter αon performance
demonstrates a trend of initial increase followed by
a decline as the αvalue increases. We validated
the impact of αacross six benchmarks of Open
LLM Leaderboard and AlpacaEval shown in Ta-
ble 14 and Table 15, and our experimental results
corroborate the aforementioned perspective.
C.3.2 The Effect of Target Laters’ Selection
The earlier layers of neural networks can not fully
capture the representation of entire input texts,
while the layers close to the top are more task-
specific. Previous studies proved that the represen-
tations extracted from the middle layers are more
effective in capturing concept-related information
(Zou et al., 2023). As to a neural network with
32 layers (Llama2-7b), we chose (10, 20, 2) to
extract representations. To further verify the afore-
mentioned viewpoint, we selected different target
layers for ablation experiments. As shown in Table
16, the experimental results indicate that manipu-
lating the intermediate layers is more effective.
The significant decline in performance of RAHF-
DUAL when operating close to the top layers of the
network can be attributed to the following reasons:
As the depth of the neural network increases, the
activations differences between layers also expand.
When choosing to operate near the top layers of
the network, under the same intervention hyperpa-
rameter αconditions, operations at the top layers
have a more significant impact on the original rep-
resentations compared to operations at the middle
layers. This excessive influence leads to a notable
decrease in the model’s generative capability. Ad-
ditionally, RAHF-DUAL employs two models in
extracting activations differences, the representa-
tion of the same input text is different between the
two models. The cumulative effect of these two
factors results in a more pronounced performance
degradation of RAHF-DUAL when operating at
the top layers.

--- PAGE 16 ---
Method α Arc TruthfulQA Winogrande GSM8k HellaSwag MMLU Average
171.93 49 .23 74 .98 16 .38 78 .88 45 .19 56.10
RAHF-DUAL 572.29 52 .14 74 .51 15 .16 79 .16 46 .22 56.58
10 72.13 53 .34 74 .19 9 .25 79 .20 45 .79 55.65
174.27 45 .80 73 .64 17 .66 78 .31 45 .01 55.78
RAHF-SCIT 574.86 52 .34 74 .27 16 .60 79 .78 45 .77 57.27
10 75.14 53 .96 74 .51 17 .13 80 .03 45 .55 57.72
Table 14: Results of different αon six benchmarks of Open LLM Leaderboard.
Method α AlpacaEval (win %)
RAHF-DUAL 1 73.74
5 86.98
10 70.67
RAHF-SCIT 1 70.28
5 87.44
10 67.50
Table 15: AlpacaEval win percentages for different
methods and αvalues.
Method Target Layers AlpacaEval (win %)
RAHF-DUAL (2,12,2) 58.32
(10,20,2) 86.98
(20,30,2) 26.93
RAHF-SCIT (2,12,2) 62.40
(10,20,2) 87.44
(20,30,2) 76.25
Table 16: The impact of layers’ selection evaluated on
AlpacaEval.
C.4 Experiment Results of MT-Bench
Table 17 presents the detailed results of RAHF,
baselines, and the ablation study on MT-Bench.
D Qualitative Examples
Figure 9 and Figure 10 present qualitative exam-
ples of RAHF compared with baselines in dialogue
tasks.

--- PAGE 17 ---
Method Writing Roleplay Reasoning Math Coding Extraction Stem Humanities Average
Turn-1
Preferred-SFT 8.500 6 .400 4 .100 2 .200 2 .100 4 .400 6 .500 7 .050 6.013
RLHF-PPO 7.775 6 .100 3 .800 1 .900 2.800 4.000 5 .450 5 .650 4.681
HIR 8.300 4 .450 3 .900 1 .300 2 .200 3 .150 6 .200 7 .800 4.663
DPO 9.600 7.000 5 .100 2 .300 1 .600 5.600 9 .100 8.900 6.150
LORRA 6.800 5 .700 2 .300 1 .800 2 .300 4 .050 5 .150 6 .700 4.350
LORRA-Pref 8.800 6 .950 5 .100 1 .400 2 .100 3 .800 7 .450 8 .000 5.450
RAHF-Dual 9.500 7.630 5.200 3.400 2.600 4.030 8.300 8.900 6.195
RAHF-SCIT 9.150 8.000 3.600 2.400 2.200 4.000 8.700 9.350 5.925
Turn-2
Preferred-SFT 4.900 7 .000 2 .700 1 .100 1 .900 2 .900 6 .400 8 .200 4.388
RLHF-PPO 5.500 7 .500 4.700 1.500 2.600 4.300 6 .600 6 .400 4.888
HIR 6.500 5 .750 1 .869 1.900 2.550 2 .500 5 .650 8 .650 4.421
DPO 6.700 7.600 2 .700 1 .400 2 .300 3 .300 8 .250 9.400 5.206
LORRA 6.050 6 .550 2 .000 1 .200 2 .400 4 .550 6 .300 6 .650 4.463
LORRA-Pref 5.800 7 .100 3 .200 1 .400 1 .500 5.500 6.800 8 .600 4.988
RAHF-Dual 6.650 7.850 4.200 1.400 2.300 3.500 7.800 8.510 5.276
RAHF-SCIT 5.000 7.300 3.600 1.700 1.700 3.700 8.300 9.400 5.088
Final
Preferred-SFT 6.700 6 .700 3 .400 1 .650 2 .000 3 .650 6 .450 7 .625 4.772
RLHF-PPO 6.625 6 .800 4 .250 1 .700 2.700 4.150 6 .025 6 .025 4.784
HIR 7.400 5 .100 2 .885 1 .600 2 .375 2 .825 5 .925 8 .225 4.541
DPO 8.150 7.300 3 .900 1 .850 1 .950 4 .450 8.675 9.150 5.678
LORRA 6.425 6 .125 2 .150 1 .500 2 .350 4 .300 5 .725 6 .675 4.407
LORRA-Pref 7.300 7 .025 4 .150 1 .400 1 .800 4.650 7.125 8 .300 5.219
RAHF-Dual 8.075 7.740 4.700 2.400 2.450 3.765 8.050 8.705 5.736
RAHF-SCIT 7.075 7.650 3.600 2.050 1.950 3.850 8.500 9.375 5.506
Table 17: Results of MT-Bench.

--- PAGE 18 ---
Figure 8: Screenshots of our evaluation interface for rating dialogue. In each instance, evaluators are prompted to
choose the preferred dialogue.

--- PAGE 19 ---
Prompt GIVE ME AN EXECUTIVE SUMMARY OF LOTTADIGITAL.COM
HIR 1) Introduction to Lottery Digital ( Lottadigital ): A lotterydigital is a digital platform that allows users to play the lottery online, with an emphasis on providing secure and convenient acces s to various 
lotto games from around the world. The website offers a wide range of features such as instant win scratch cards, daily draws , jackpot games, and more. Users can also participate in syndicates for 
increased chances of winning big prizes.
2) Key Features of LottoDigital : Some key features include: 
a) Secure payment options -Various methods are available including credit/debit card payments, e -wallets like PayPal or Skrill, bank transfers etc., ensuring safe transactions without any hassles. 
b) Wide selection of international lotteries –Players have access to over 30 different national & international lotteries acros s Europe, Asia Pacific region, North America among others; giving them plenty 
of choices when it comes time to pick their favorite game(s). 
c) Instant Win Scratch Cards –These offer quick results within minutes after purchase which makes them ideal if you’re looking for some fast action! 
d) Daily Draw Games –Including popular ones like EuroMillions & Powerball where players need only one ticket per entry into eac h respective drawing event held every day at specific times 
throughout weekdays /weekends respectively . 
e) Jackpots Galore –With huge progressive jackpots up for grabs through these specialized lotteries , there's always something exciting waiting just ahead ! 
f) Syndicate Options Available –Join forces with other members by forming groups called "Syndicates" allowing everyone involved share winnings equally amongst themselves based upon how much 
they contributed towards overall pool size used during particular draw events run periodically throughout yearly calendar cyc le . 
RLHF -PPO 1. Overview of LottsDigital.com:
LottsDigital is a digital marketing agency that specializes in helping businesses grow their online presence and increase revenue through various services such as SEO, PPC advertising, social media 
management, content creation, and website design. The company was founded by CEO Michael Lotts in 2015 with the goal of providing high -quality, results -driven digital marketing solutions to clients 
across different industries
DPO 1. Introduction to Lotterdigital (Lottadigital ):
* Name: Lottadigital is a leading digital lottery platform that offers innovative and secure solutions for the global gaming industry, with its he adquarters in Malta. The company was founded by 
experienced professionals from various industries who share a passion for technology and innovation.
2. Overview of Services Provided by Lottadigital :
* Digital Lottery Platforms: Lottadigital provides customized digital lottery platforms tailored to meet clients' specific needs. These platforms are designed to be us er-friendly, secure, and compliant with 
regulatory requirements across different jurisdictions worldwide. They offer features such as instant win games, scratch card s, sports betting integration, and more.
3. Key Features of Lottadigital Solutions:
* Security & Compliance: Lottadigital prioritizes security and complies with international standards like ISO/IEC 27001 Information Security Management System cert ification, PCI DSS Payment Card 
Industry Data Security Standard, and AML Anti -Money Laundering regulations. Their systems undergo regular audits and penetration testing to ensure maximum protection against cyber threats.
4. Partnerships & Collaborations: Lottadigital has established partnerships with several renowned companies within the iGaming sector, including game developers, payment pr oviders, and marketing 
agencies. This enables them to provide their customers with an extensive range of services and resources while ensuring seaml essintegrations between all parties involved.
5. Awards & Recognition: In recognition of their outstanding work, Lottadigital has received numerous awards over the years, including "Best Newcomer" at the EGR B2B Awards 2019 and "Innovator of 
the Year" at the SBC Awards 2020.
6. Future Plans & Goals: As part of their growth strategy, Lottadigital plans to expand into new markets globally through strategic acquisitions or organically via product development initiatives a imed 
towards enhancing existing products further while exploring emerging technologies relevant to the online gambling space. Addi tionally, they intend on strengthening relationships with key stakeholders 
within both public authorities and private organizations alike –ultimately contributing positively towards shaping future trend s within this dynamic industry landscape.
RAHF -Dual 1. Introduction to Lotter Digital ( Lottadigital ):
* Founded in 2016 by a team of experienced professionals with expertise in the lottery industry, including software developme nt and marketing.
* Headquartered in Malta, an established jurisdiction for online gaming companies due to its favorable regulatory environment and tax benefits.
* Operates as a B2B provider offering innovative solutions for the global lottery sector through cutting -edge technology platfor ms and services.
2. Key Products & Services Offerings:
* Customized white label lotto platform tailored according to client needs, featuring advanced features such as multi -language s upport, mobile compatibility, and seamless integration capabilities.
* Advanced backend system that streamlines operations like ticket sales, prize payouts, player management, and more.
* A comprehensive suite of marketing tools designed to boost customer acquisition and retention rates via personalization str ategies, email campaigns, social media promotions, etc.
* Integration with third -party systems like payment processors, CRMs, and data analytics providers to enhance overall efficiency across all business processes.
3. Market Reach & Partnerships:
* Serves clients from various continents worldwide, primarily focusing on European markets but also expanding into other regi onswhere legal regulations permit it.
* Collaborated with several renowned brands within the iGaming space, leveraging their experience and resources to provide be tter service offerings while maintaining compliance standards set forth by 
respective authorities.
4. Company Milestones & Achievements:
* Successfully launched multiple projects globally, catering to different types of lottery operators ranging from small start ups to large corporations.
* Received numerous awards recognizing excellence in product design or outstanding performance over time periods; these inclu de "Best Newcomer" at ICE Totally Gaming 2017 and "Innovator Award" 
at SBC Awards 2019.
* Continuous investment in R&D activities aimed towards enhancing existing products/services portfolio along with introducing new ones based on emerging trends within this niche segment.
RAHF -SCIT 1) Introduction to Lotteries and Digitalization in the Industry
-Overview of lottery games as a form of gambling, with origins dating back centuries.
-Traditionally, lotto has been played through physical channels like retail stores or government -run outlets.
2) Problem Statement & Opportunity for Innovation
-The traditional way of playing lotteries is becoming obsolete due to technological advancements.
-Players are increasingly turning towards digital platforms that offer convenience, accessibility, and more engaging experi ences.
-This shift presents an opportunity for innovative solutions that can leverage technology to enhance the overall experience while maintaining regulatory compliance.
3) Solution Proposed by Lottadigital.com
-A platform designed specifically for online lottery players, offering a seamless user journey from registration to ticket purchase and results checking.
-It provides secure transactions via blockchain technology, ensuring transparency and trustworthiness.
-Users have access to various features such as personalized notifications, social media integration, and rewards programs.
4) Benefits of Using Lottadigital.com
-Convenience –Accessible anytime anywhere on mobile devices or desktop computers.
-Security –Blockchain technology guarantees safe and transparent transactions.
-Engagement –Various interactive elements keep users engaged throughout their entire play experience.
-Rewards Programs –Loyalty points system encourages repeat purchases and promotes customer loyalty.
5) Market Analysis & Competition Landscape
-Global market size of $300 billion (lottery industry).
-Estimated growth rate at ~6% annually.
-High competition among existing providers but room for new entrants who can provide unique value propositions.
6) Target Customer Segments & Key Differentiators
-Younger demographic groups interested in modernizing how they gamble.
-Tech -savvy individuals looking for convenient ways to participate in lotteries.
-Regular lottery players seeking enhanced gameplay experiences.
7) Business Model & Revenue Streams
-Subscription fees charged monthly/annually based on plan type selected.
-Transaction fee collected per transaction made within the app.
-Advertising revenues generated from sponsorships and banner ad placements.
-Potential future monetization opportunities include virtual goods sales, merchandise, etc.
8) Marketing Strategy & Go -To-Market Approach
-Initial focus will be on key markets where there's high demand for digitally enabled lottery services.
-Social media campaigns targeted toward younger audiences.
-Partnerships with influencers and celebrities known for their love of gaming.
-Continuously improve product based on feedback received from customers.Figure 9: RAHF-Dual and RAHF-SCIT are more comprehensive and insightful compared to HIR, RLHF-PPO,
and DPO. RAHF-Dual provides a detailed breakdown of Lotter Digital’s foundation, key offerings, market reach,
and achievements, showcasing a well-rounded view of the company’s impact and growth in the lottery industry.
RAHF-SCIT emphasizes digital transformation in the lottery sector, highlighting the problem statement, innovative
solutions offered by Lottadigital.com, and the benefits and market potential of these solutions. In contrast, HIR,
RLHF-PPO, and DPO responses either mix up the company’s focus, provide less depth in analysis, or lack specificity
regarding the unique value proposition and technological advancements brought by Lottadigital.com.

--- PAGE 20 ---
Prompt Create a table with the planets of the solar system and their dimensions
HIR 1. Mercury -Diameter = 4,879 km; Mass = 302 kg/m^3  
2. Venus -Diameter = 6,052 km; Mass = 5.24 g/cm^3  
3. Earth -Diameter = 12,742 km; Mass = 5.51 g/cm^3  
4. Mars -Diameter = 6,794 km; Mass = 3.93 g/cm^3  
5. Jupiter -Diameter = 142,984 km; Mass = 1.327 g/cm^3  
6. Saturn -Diameter = 120,536 km; Mass = 0.954 g/cm^3  
7. Uranus -Diameter = 51,118 km; Mass = 1.27 g/cm^3  
8. Neptune -Diameter = 49,528 km; Mass = 1.645 g/cm^3
RLHF -PPO 1. Planet Name                   | Dimensions
DPO | Planet                           | Dimensions (km)   |
|----------------------------- |--------------------------- |
| Mercury                      | 4,879 km diameter     |
| Venus                        | 12,092 km diameter    |
| Earth                         | 12,756 km diameter    |
| Mars                          | 6.3 million km radius |
| Jupiter                       | 11.2 times larger than Earth's diameter |
| Saturn                        | 10.5 times larger than Earth's diameter |
| Uranus                        | 4 times larger than Neptune's diameter |
| Neptune                       | 4 times larger than Pluto's diameter |
RAHF -Dual 1. Planet Name                 | Dimensions (Approximate)  
----------------------------------- |-----------------------------
Mercury                             | Diameter: ~4,879 km           | Mass: ~302.6 million kg
Venus                               | Diameter: ~12,104 km          | Mass: ~4.85 ×10^24 kg
Earth                                | Diameter: ~12,742 km            | Mass: ~5.97 ×10^24 kg
Mars                                 | Diameter: ~6,794 km              | Mass: ~6.41 ×10^23 kg
Jupiter                              | Diameter: ~142,984 km           | Mass: ~1.90 ×10^27 kg
Saturn                               | Diameter: ~120,536 km           | Mass: ~5.68 ×10^26 kg
Uranus                                | Diameter: ~51,118 km            | Mass: ~8.65 ×10^25 kg
Neptune                               | Diameter: ~49,528 km            | Mass: ~1.02 ×10^26 kg
RAHF -SCIT Table for Planets in Our Solar System and Their Dimensions
----------------------------------------- ---------------
Planet                       Diameter (km)   Mass (kg)    Orbit Distance from Sun (AU)
Mercury               4,879           3.30 ×10^23    57.9 million km
Venus                12,104          4.86 ×10^24     67.2 million km
Earth               ~12,756         5.97 ×10^24    149.6 million km
Mars                6,371            6.41 ×10^23    227.9 million km
Jupiter             142,984        1.90 ×10^27       778.3 million km
Saturn              120,536        9.50 ×10^26       1.4 billion km
Uranus              51,512        1.70 ×10^26       2.8 billion km
Neptune             49,528        1.02 ×10^26       4.5 billion km
Pluto*             2,370           1.90 ×10^22    7.4 billion km
----------------------------------------- ---------------
* Pluto is no longer considered to be part of our solar system as it orbits beyond 
Neptune's orbit at an average distance of approximately 4.5 billion kilometers from the 
sun. It has been reclassified as a dwarf planet due to its size and location relative to 
other objects in the Kuiper Belt.Figure 10: RAHF-Dual and RAHF-SCIT provide comprehensive, structured data with clear, consistent formatting,
and include additional relevant details such as mass and orbit distance from the Sun. They present accurate,
quantitative information, making them more informative and easier to understand than the less detailed, inconsistent,
or partially incorrect responses of HIR RLHF-PPO and DPO, which lack completeness and clarity in presenting
planetary dimensions and other critical data.
