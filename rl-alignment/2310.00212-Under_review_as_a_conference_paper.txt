# 2310.00212.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2310.00212.pdf
# File size: 1108948 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
Under review as a conference paper
PAIRWISE PROXIMAL POLICY OPTIMIZATION :
HARNESSING RELATIVE FEEDBACK FOR LLM A LIGN -
MENT
Tianhao Wu‚àó, Banghua Zhu, Ruoyu Zhang, Zhaojin Wen, Kannan Ramchandran & Jiantao Jiao
University of California, Berkeley
ABSTRACT
Large Language Models (LLMs) can acquire extensive world knowledge through
pre-training on large corpora. However, due to exposure to low-quality data,
LLMs may exhibit harmful behavior without aligning with human values. The
dominant approach for steering LLMs towards beneficial behavior involves Rein-
forcement Learning with Human Feedback (RLHF), with Proximal Policy Opti-
mization (PPO) serving as the default RL optimizer. Despite its effectiveness, PPO
has limitations when optimizing rewards trained from comparison-based loss. Pri-
marily, PPO is not invariant to equivalent reward functions containing identical
preference information due to the need to calibrate the reward scale. Additionally,
PPO‚Äôs necessity for token-wise updates introduces complexity in both function ap-
proximation and algorithm design compared to trajectory-wise optimization. This
paper proposes a new framework, reinforcement learning with relative feedback,
and a novel trajectory-wise policy gradient algorithm, Pairwise Proximal Policy
Optimization (P3O) that operates directly on comparative rewards. We show the-
oretically that P3O is invariant to equivalent rewards and avoids the complexity
of PPO. Empirical evaluations demonstrate that P3O outperforms PPO in the KL-
Reward trade-off and can align with human preferences as well as or better than
prior methods. In summary, this work introduces a simpler yet effective approach
for aligning LLMs to human preferences through relative feedback.
1 I NTRODUCTION
Large Language Models (LLMs) have made remarkable progress, profoundly influencing the AI
community (Chowdhery et al., 2022; Brown et al., 2020; Touvron et al., 2023; Bubeck et al., 2023).
However, due to the reliance on massive corpora of internet data, which encompasses a high portion
of low-quality data, LLMs are likely to express unintended behavior. These include fabricating facts,
generating biased or toxic text, and even harmful content to humans (Perez et al., 2022; Ganguli
et al., 2022). Consequently, it is crucial to align LLMs with human values, e.g., helpful, honest,
harmless (Bai et al., 2022a).
A dominant approach in the realm of AI Alignment for LLMs named Reinforcement Learning with
Human Feedback (RLHF) involves a three-stage procedure: supervised fine-tuning, reward learning,
and reinforcement learning (RL) fine-tuning (Ziegler et al., 2019; Ouyang et al., 2022). In the critical
third stage, Proximal Policy Optimization (PPO) is widely adopted as the default RL optimizer
(Schulman et al., 2017). Despite the acclaimed efficiency of PPO, recent studies have highlighted
several intriguing questions and potential issues that require additional attention:
Instability of PPO. Despite its acclaimed effectiveness, recent studies have identified instability
associated with PPO. Factors such as reward normalization, reward scaling, reward clipping, KL
control, advantage normalization and critic initialization (Zheng et al., 2023; Engstrom et al., 2020)
can contribute to this instability. Moreover, we identify another source of instability: there is an
inconsistency when it comes to optimizing the reward trained with the Bradley-Terry Loss (BTL)
comparative reward model. In essence, BTL is invariant to constant shift while PPO is not. This
‚àóContact author through thw@berkeley.edu
1arXiv:2310.00212v3  [cs.LG]  10 Oct 2023

--- PAGE 2 ---
Under review as a conference paper
A1LM RM
A2Q
LM RMR1-R2
pairwise responses
per promptrelative 
preferenceupdate
Pairwise
Policy GradientReinforcement Learning with Relative Feedback
LM
P3O ‚úÖ
Streamline üòäA1
LM
A2
LM R2
single response
per promptabsolute
preferenceRM
RMLMReinforcement Learning with Absolute Feedback
Q2Q1
R1Generalize
Advantage Estimate
Policy Gradient+update
PPO ‚ùé
Complicate ‚òπ
Figure 1: The figure on the left illustrates the prevalent method for fine-tuning LMs using RL, which
relies on Absolute Feedback . In this paradigm, algorithms like PPO has to learn a Vfunction, which
capture not only the valuable relative preference information, but also less part, which is the scale of
the reward for a given prompt. Contrastingly, the figure on the right presents paradigm for optimizing
reward model trained via comparative loss, e.g., Bradley-Terry Loss (Bradley & Terry, 1952). Our
algorithm generate a pair of responses per prompt, leveraging only the Relative Feedback - derived
from the difference in reward - for policy gradient updates. This method obviates the need for
additional Vfunction approximations and intricate components like GAE (Schulman et al., 2015b).
means that even if two reward models carry identical information about human preferences, opti-
mizing them using PPO can produce different outcomes. (Section 4.4)
Trajectory-wise vs Token-wise. A prevailing tension in fine-tuning LLMs using RL revolves
around whether tuning should be trajectory-wise or token-wise. This dichotomy arises primarily
from the nature of the reward model training phase wherein the reward model is trained using com-
parison dataset with human labelers ranking multiple completions of a single prompt. The obvious
reason for taking this indirect approach based on trajectory-wise comparison is that it is often chal-
lenging for labelers to make a direct score assignment or provide comparison beyond a trajectory-
wise perspective. An exception might be specific tasks with less ambiguity, such as math problems
(Lightman et al., 2023), where partial credit for partially correct answers is feasible. This difficulty
arises from the fact that the reward model returns only a single scalar reward after receiving the entire
response (when the model hits the <eos> token). Given this complexity, two dominant viewpoints
emerge: Should language generation be approached as a Contextual Bandit (CB) or a Markov Deci-
sion Process (MDP)? Although the prevalent approach is to use PPO to perform token-wise updates,
it introduces additional complexity both in function approximation and in algorithmic design, i.e.,
PPO requires to additionally learn a value function Vand use Generalized Advantage Estimation
(GAE) (Schulman et al., 2015b) to redistribute the sparse reward to all tokens.
In this paper, we provide new insights to address the above two issues. We summarize our primary
contributions as:
(1) We define an equivalent relationship for reward functions trained from human preferences.
We identify that BTL is invariant under this equivalent relationship, while PPO is not.
(2) We observed that Direct Preference Optimization (DPO) (Rafailov et al., 2023) can be used
jointly with a trained reward model, leading to online-DPO. Further, we demonstrate that
online-DPO remains invariant under the reward equivalent relationship. Empirical evidence
suggest that online-DPO can achieve higher reward while being less KL efficient than PPO.
(3) We introduce a novel policy gradient algorithm, Pairwise Proximal Policy Optimization
(P3O) under the framework of Reinforcement Learning with Relative Feedback (Figure 1).
Our algorithm aligns perfectly with the comparative nature of the reward model, avoiding
complications like estimating the Vfunction, GAE and various normalization techniques
(Zheng et al., 2023). Empirical evaluations show that P3O consistently outperforms PPO
and DPO in terms of KL-Reward trade-off and GPT-4 Evaluation.
2 R ELATED WORK
Significant efforts have been made towards aligning LLMs with human values. These alignment
strategies broadly fall into two categories: offline training and online training.
Offline training typically involve a static dataset, and doesn‚Äôt require additional evaluations or gen-
erations. For instance, Thoppilan et al. (2022); Gunasekar et al. (2023) use instruction fine-tuning to
2

--- PAGE 3 ---
Under review as a conference paper
update the model on a high quality dataset tailored to a specific downstream task of interest. Snell
et al. (2022) proposed to employ offline QLearning to learn an add-on term for decoding. While
Rafailov et al. (2023) introduced DPO, an offline approach that can directly align LM with human
preference data, drawing from the closed-form solution of the Contextual Bandit with KL control
problem. There are also methods like PRO (Song et al., 2023) and RRHF (Yuan et al., 2023) that
fine-tune the model based on ranking of the rewards.
Our work is categorized under online training, which consist of a loop of generating new responses
from the updated policy, evaluating them with the reward model and update the policy. The current
dominant approach RLHF relies on online RL methods such as PPO (Schulman et al., 2017), A2C
(Mnih et al., 2016) or their variants (Ramamurthy et al., 2022; Zhu et al., 2023b). There are also
few methods that deviate from this standard. For instance, Gulcehre et al. (2023) introduce ReST,
which use offline RL instead of online RL in the policy improvement phase; Dong et al. (2023)
proposed RAFT, which iteratively fine-tune the policy on the responses generated by the Best-of-N
policy. Another paradigm parallel to RLHF is Reinforcement Learning with AI Feedback (RLAIF)
(Bai et al., 2022b; Lee et al., 2023), which aim for AI self-improvement. RLAIF substitute the role
of human with AI in the feedback and yield comparable results with models of smaller scale.
Outside of the context of language, preference-driven policy learning has been explored in both
bandit and RL. Contextual dueling bandit (Dud ¬¥ƒ±k et al., 2015; Yue et al., 2012) use preferences
or rankings of actions to adjust the policy, rather than rewards. Similarly, PbRL (Jain et al., 2013;
Busa-Fekete et al., 2014; Christiano et al., 2017; Sadigh et al., 2017; Kupcsik et al., 2018) learn from
binary preferences generated by some unknown scoring function. Our work share resemblance to
the pairwise policy gradient outlined in Xu et al. (2020), though we integrate the critical clipping
technique and the KL control that better fit into the language generation setting.
3 P RELIMINARIES
In this section, we review the RLHF pipeline and discuss how language generation fits into CB and
RL setting(Sutton & Barto, 2018; Yang et al., 2021; Lattimore & Szepesv ¬¥ari, 2020; Wu et al., 2022).
‚Ä¢SFT Phase (Supervised Fine-Tuning): This stage start with a pre-trained LM, and then
fine-tuned with supervised learning (typically maximum likelihood loss) on a high quality
dataset for the downstream task of interest. Outcome of this stage is denoted as œÄSFT.
‚Ä¢Reward Learning Phase. In the second phase the SFT model is prompted with prompts x
to produce pairs of answers y1,y2‚àºœÄSFT(y|x). The responses pairs are then presented to
human labelers who express preferences for one answer, denoted as yw‚âªyl|x, where yw
is the one favored by the labeler and ylis the one less favored. Under these preferences is
the inaccessible latent reward model r‚àó(y,x). According to the Bradley-Terry (Bradley &
Terry, 1952) model, the human preference distribution p‚àócan be expressed as:
p‚àó(y1‚âªy2|x) =exp(r‚àó(y1|x))
exp(r‚àó(y1|x)) + exp( r‚àó(y2|x))=1
1 + exp ( r‚àó(y1|x)‚àír‚àó(y2|x))
Assuming the access to a dataset {(x(i),y(i)
w,y(i)
l)}N
i=1sampled from p‚àó. We parameterize
the reward as rœïand estimate it via maximum log-likelihood:
LR=NX
i=11
NlogœÉ
rœï(y(i)
w|x(i))‚àírœï(y(i)
l|x(i))
(1)
where œÉis the sigmoid function. rœïis initialized with œÄSFTaugmented by additional linear
layers on top. Constraints like E[r(y|x)] = 0 might be incorporated to lower the variance.
‚Ä¢RL Fine-Tuning Phase. Prior work formulate the optimization problem as:
max
œÄŒ∏Ex‚àºD,y‚àºœÄŒ∏(¬∑|x)
rœï(y|x)‚àíŒ≤DKL(œÄŒ∏(¬∑|x)‚à•œÄSFT(¬∑|x))
(2)
TheŒ≤DKL(œÄŒ∏(¬∑|x)‚à•œÄSFT(¬∑|x))term is used to regulate the deviation from the SFT
model, it is important to prevent the model from completely forget the world knowl-
edge acquired in the pre-training stage. The standard approach is to directly employ
PPO (Schulman et al., 2017; Ouyang et al., 2022) to optimize the modified reward
rœï(y|x)‚àíŒ≤ 
logœÄŒ∏(y|x)‚àílogœÄSFT(y|x)
.
3

--- PAGE 4 ---
Under review as a conference paper
3.1 MDP VSCB
Language Generation modeled as an MDP: A language model takes a sequence of tokens
(t1, ..., t h)as input and produce the distribution for the next token th+1. The action space of this
MDP consists of all potential tokens, with the state being a concatenation of all the history tokens as
well as the prompt. The transition in the model is deterministic, the next state equal to the concate-
nation of current state and the recently produced token. Formally,
P(sh+1= (t1, ..., t h+1)|sh= (t1, ..., t h), ah+1=th+1) = 1
An episode end either when the total tokens generated attain a predefined limit or when the language
model produce the special token <eos> . Upon the end of an episode, the entire sequence is scored
by a reward model, which produces a scalar reward r. PPO follows this formulation, where the single
scalar reward will be assigned to the last token and combine with the token-wise KL penalty to form
the ultimate reward. To tackle the challenge of sparse feedback, PPO employs the Generalized
Advantage Estimation (Schulman et al., 2015b) to reallocate the scalar reward across all tokens.
Language Generation modeled as a CB: A language model takes a prompt x= (t1, ..., t h)and
generates a response y= (th+1, ..., t h‚Ä≤). The response either terminates with with a <eos> token
or continues until a predefined token limit is met. Within this setting, every possible response is
treated as an action, and there‚Äôs no transition at all. The entire sequence is scored by a reward
model, producing a scalar reward. This formulation is adopted by (Rafailov et al., 2023; Zhu et al.,
2023a) as well as our paper.
4 A LGORITHM
4.1 P ROXIMAL PAIRWISE POLICY OPTIMIZATION (P3O)
To derive P3O, we start from Vanilla Policy Gradient (VPG, Pseudocode 1) (Sutton et al., 1999;
Schulman et al., 2017). For simplicity, we omit the prompt xin the formula and focus on the
derivation for bandit setting and extend to the more general contextual bandit in theorem 1.
In the bandit setting, assume we are updating a parameterized policy œÄŒ∏with actions denoted as y.
The VPG aims for estimating the following formula with samples:
‚àáLVPG=E
y‚àºœÄŒ∏r(y)‚àálogœÄŒ∏(y) =X
yr(y)‚àáœÄŒ∏(y) (3)
Previous works on PG propose to subtract a baseline bto reduce variance, resulting in
‚àáLVPG=X
y(r(y)‚àíb)‚àáœÄŒ∏(y)
We plug in the popular choice of the baseline, b=P
yr(y)œÄŒ∏(y):
‚àáLVPG=X
y1r(y1)‚àáœÄŒ∏(y1)‚àíX
y1,y2r(y2)œÄŒ∏(y2)‚àáœÄŒ∏(y1)
=X
y1,y2r(y1)œÄŒ∏(y2)‚àáœÄŒ∏(y1)‚àíX
y1,y2r(y2)œÄŒ∏(y2)‚àáœÄŒ∏(y1)
=X
y1,y2(r(y1)‚àír(y2))œÄŒ∏(y2)‚àáœÄŒ∏(y1) (4)
Equation (4) gives rise to an expression that directly rely on the relative difference r(y1)‚àír(y2).
We further introduce œÄŒ∏oldand rewrite Eq (4) with importance sampling:
‚àáLVPG= E
y1,y2‚àºœÄŒ∏old(r(y1)‚àír(y2))œÄŒ∏(y2)
œÄŒ∏old(y2)‚àáœÄŒ∏(y1)
œÄŒ∏old(y1)
Swap actions y1,y2and average together we get a symmetry gradient expression we termed Pair-
wise Policy Gradient (PPG):
‚àáLPPG= E
y1,y2‚àºœÄŒ∏old(r(y1)‚àír(y2))œÄŒ∏(y2)
œÄŒ∏old(y2)‚àáœÄŒ∏(y1)
œÄŒ∏old(y1)‚àíœÄŒ∏(y1)
œÄŒ∏old(y1)‚àáœÄŒ∏(y2)
œÄŒ∏old(y2)
/2 (5)
= E
y1,y2‚àºœÄŒ∏old(r(y1)‚àír(y2))œÄŒ∏(y1)
œÄŒ∏old(y1)œÄŒ∏(y2)
œÄŒ∏old(y2)‚àá
logœÄŒ∏(y1)
œÄŒ∏(y2)
/2 (6)
4

--- PAGE 5 ---
Under review as a conference paper
Its immediate generalization to contextual bandit is the following theorem:
Theorem 1 (Pairwise Policy Gradient with Importance Sampling) .For any prompt x, the policy
gradient can be expressed as ‚àáLVPG=E
x‚àºD‚àáLVPG(x), where ‚àáLVPG(x)can be expressed as:
E
y1,y2‚àºœÄŒ∏old(r(y1|x)‚àír(y2|x))œÄŒ∏(y2|x)
œÄŒ∏old(y2|x)‚àáœÄŒ∏(y1|x)
œÄŒ∏old(y1|x)‚àíœÄŒ∏(y1|x)
œÄŒ∏old(y1|x)‚àáœÄŒ∏(y2|x)
œÄŒ∏old(y2|x)
/2
= E
y1,y2‚àºœÄŒ∏old(r(y1|x)‚àír(y2|x))œÄŒ∏(y1|x)
œÄŒ∏old(y1|x)œÄŒ∏(y2|x)
œÄŒ∏old(y2|x)‚àá
logœÄŒ∏(y1|x)
œÄŒ∏(y2|x)
/2
4.2 C OMBINE WITH CLIPPING
A significant modification of PPO over VPG is to clip the loss function, which disincentivizes large
updates to the policy. Specifically, PPO dictates that the ratio œÄŒ∏/œÄŒ∏oldshould remain close to 1,
guided by the sign of the Advantage function Adv. Understanding this in an intuitive sense, if
Adv(y|x)>0, it implies that taking the action yis beneficial on average. Hence, we aim to
increase the probability œÄŒ∏(y|x). However, if the policy ratio œÄŒ∏/œÄŒ∏oldexceeds 1 +œµ, we consider
the change sufficient and halt the gradient; otherwise, the gradient is computed for further learning.
Conversely, if Adv(y|x)<0, we strive to optimize the ratio towards 1‚àíœµinstead of 1 +œµ. This
intuition guides us to derive two variants of the algorithm, differentiated by whether clipping is
applied separately or jointly for actions y1andy2.
Clipping Separately (Version 1): For{i, j}={1,2},
LP3O
i(x) = E
y1,y2‚àºœÄŒ∏oldsg
(r(yi|x)‚àír(yj|x))œÄŒ∏(yj|x)
œÄŒ∏old(yj|x)œÄŒ∏(yi|x)
œÄŒ∏old(yi|x)
LP3O
i,clip(x) = E
y1,y2‚àºœÄŒ∏oldsg
(r(yi|x)‚àír(yj|x))œÄŒ∏(yj|x)
œÄŒ∏old(yj|x)
clip (œÄŒ∏(yi|x)
œÄŒ∏old(yi|x),1‚àíœµ,1 +œµ)
LP3O
sep=E
x‚àºD
min(LP3O
1(x),LP3O
1,clip(x)) + min( LP3O
2(x),LP3O
2,clip(x))
/2
Clipping Jointly (Version 2):
LP3O(x) = E
y1,y2‚àºœÄŒ∏oldsg
(r(y1|x)‚àír(y2|x))œÄŒ∏(y1|x)
œÄŒ∏old(y1|x)œÄŒ∏(y2|x)
œÄŒ∏old(y2|x)
logœÄŒ∏(y1|x)
œÄŒ∏(y2|x)
LP3O
clip(x) = E
y1,y2‚àºœÄŒ∏oldsg
(r(y1|x)‚àír(y2|x))œÄŒ∏(y1|x)
œÄŒ∏old(y1|x)œÄŒ∏(y2|x)
œÄŒ∏old(y2|x)
√óclip
logœÄŒ∏(y1|x)
œÄŒ∏(y2|x),logœÄŒ∏old(y1|x)
œÄŒ∏old(y2|x)‚àíœµ,logœÄŒ∏old(y1|x)
œÄŒ∏old(y2|x)+œµ
LP3O
joi=E
x‚àºDmin(LP3O(x),LP3O
clip(x))
Thesgandclip refers to the stop-gradient and clip operator respectively. We present the entire
pipeline of P3O in Pseudocode 2.
4.3 R ELATIONSHIP WITH PPO AND DPO
In this section, we briefly discuss the relationship of our proposed algorithm and two existing algo-
rithms, PPO and DPO. We further elaborate their theoretical properties in Section 4.4.
Comparison with PPO: The objective of PPO, before clipping is applied and reduced to the con-
textual bandit setting, can be expressed as follows:
LPPO
no clip =‚àí E
y‚àºœÄŒ∏old(¬∑|x)(r(y|x)‚àíVœï(x))œÄŒ∏(y|x)
œÄŒ∏old(y|x)
Where Vœï(x)is a proxy to the value function VœÄŒ∏old=Ey‚àºœÄŒ∏oldr(y|x). In contrast to PPO, P3O
expands Vœï(x)using another sample y‚Ä≤to construct an unbiased estimator, bypassing the need to
learn an additional value function V, which leads to more stable result.
5

--- PAGE 6 ---
Under review as a conference paper
Comparison with DPO: Like P3O, DPO also adopts the contextual bandit (CB) formulation. The
gradient of DPO‚Äôs objective function takes the following form:
‚àáLDPO(x,yw,yl) =‚àíŒ≤œÉ
Œ≤logœÄŒ∏(yl|x)
œÄSFT(yl|x)‚àíŒ≤logœÄŒ∏(yw|x)
œÄSFT(yw|x)
‚àá
logœÄŒ∏(yw|x)
œÄŒ∏(yl|x)
/2
The gradient direction of DPO resembles that of our formulation in Theorem 1. However, DPO uses
a different weight in front of the gradient. We hypothesize that the reason for DPO usually achieving
a higher reward but falling short on KL-control (Figure 2 and 3) compared to PPO is because DPO
aligns the policy towards the goal policy while doesn‚Äôt directly consider the reward. Unlike PPO
and P3O, which applies policy gradient based on the idea of strict policy improvement for every
gradient update (Schulman et al., 2015a), DPO aligns the policy via an alternate ‚Äúdistance‚Äù, where
the intermediary steps are not guaranteed to maximize the KL-Reward trade-off. We note that P3O
combines the benefits of PPO and DPO, offering guaranteed policy improvement akin to policy
gradient.
4.4 R EWARD EQUIVALENCE & INSTABILITY OF PPO
In this section, we formally define the concept of reward equivalence (Definition 1). We show that
BTL is invariant under this equivalent relationship in lemma 1. We then discuss why it leads to a
desirable property named invariance (Definition 2) that we want RL algorithms to satisfy. In the
end, we present our main theorem (Theorem 2) which shows that PPO does not satisfy this property,
contributing to its instability.
Definition 1 (Reward Equivalence) .Two reward functions r(y|x)andr‚Ä≤(y|x)are termed equiva-
lent, denoted as r‚àºr‚Ä≤, if and only if there exist a function Œ¥(x)depend solely on the prompt x, such
that for every prompt and response pair (x,y),
r(y|x)‚àír‚Ä≤(y|x) =Œ¥(x)
The equivalent class associated with reward ris represented as [r].
Note that comparative losses such as Bradley-Terry loss and Plackett-Luce loss, is unaffected by a
shift in the prompt‚Äôs reward as in definition 1. This observation leads to the following Lemma:
Lemma 1 (Invariance of BTL) .For two reward functions that satisfy r‚àºr‚Ä≤, they both yield identical
loss for any response pairs (or K responses) under the Bradley-Terry Loss (or Plackett-Luce Loss).
Lemma 1 underscores that the only information we can learn from the preference data is the reward
difference of two responses to the same prompt. This implies that direct comparison of responses
stemming from different prompts should be avoided. This is because we can craft an arbitrary
function denoted as Œ¥and replace ÀÜrwith the identical ÀÜr+Œ¥, while flipping the sign of ÀÜr(y|x)‚àí
ÀÜr‚Ä≤(y‚Ä≤|x‚Ä≤). As a result, an ideal algorithm should focus only on the relevant information within the
reward function, filtering out the noise represented by Œ¥. This leads the following definition:
Definition 2 (Invariance) .An algorithm is said to be invariant with respect to the equivalent relation
‚Äú‚àº‚Äù, if for any two equivalent reward functions r‚àºr‚Ä≤and a fixed set of prompt and response pairs,
the algorithm perform identical updates to the policy.
To illustrate definition 2, assume that we have two equivalent reward functions ÀÜrandÀÜr‚Ä≤= ÀÜr+Œ¥.
Notably, even when initialized with the same random seed, PPO can result in distinct updates for
an identical batch. This behavior can be attributed to PPO‚Äôs reliance on learning a Vfunction to
estimate advantage. In the simplest scenario, where the advantage is estimated via one-step TD
(Adv(y|x) =r(y|x)‚àíV(x), corresponding to ŒªGAE= 0) and yis a single token, we should
expect the advantage function to stay unchanged. However, following the derivation
Adv ÀÜr(y|x) =Adv ÀÜr‚Ä≤(y|x)
‚áê‚áí ÀÜr(y|x)‚àíVÀÜr(x) = ÀÜr‚Ä≤(y|x)‚àíVÀÜr‚Ä≤(x)
‚áê‚áí VÀÜr‚Ä≤(x)‚àíVÀÜr(x) =Œ¥(x)
We can see that even though ÀÜrandÀÜr‚Ä≤are equivalent, they yield different updates for Vfunction.
This give rise to our main theorem (full proof in Appendix B.2):
Theorem 2 (Non-invariance of PPO) .P3O and DPO are invariant with respect to ‚Äú ‚àº‚Äù. In contrast,
PPO is not, given the same initialization of V.
6

--- PAGE 7 ---
Under review as a conference paper
5 E XPERIMENTS
In this section, we empirically study how well can P3O align with human preference. We conduct
experiments on two widely-adopted RLHF tasks, summarization and question-answering, and we
find that P3O achieves better performance in terms of both KL-Reward trade-off and quality of
generation, against several strong baselines. We will first briefly introduce the tasks, compared
methods, and evaluations in our experiments, and then elaborate on these findings in detail.
Tasks. We explore two different open-ended text generation tasks, i.e. summarization and
question-answering . For both tasks, algorithms are given a reward model pre-trained from a dataset
of preference D={x(i),y(i)
w,y(i)
l}, and the goal is to obtain a policy œÄ(y|x)that can generate high-
quality response ygiven prompt x. In summarization, we use the TL;DR ‚Äútoo long; didn‚Äôt read‚Äù
dataset (V ¬®olske et al., 2017), where xis a forum post from Reddit, and yis a corresponding sum-
mary. We use a 6B SFT model CarperAI/openai summarize tldr sft as the initial policy
andEleutherAI/gpt-j-6b as the reward model. In question-answering, xis a human query,
which may come from diverse topics, and the policy should learn to produce an engaging and helpful
response y. Following prior work, we use the Anthropic Helpful and Harmless ( HH) dataset (Bai
et al., 2022a). We fine-tune two policies of sizes {1B,6B},Dahoas/pythia-1B-static-sft
andDahoas/pythia-6B-static-sft . Both models have gone through supervised fine-
tuning with labeled prompt-response pairs, similar to the protocol in Ouyang et al. (2022) and Ra-
mamurthy et al. (2022). For the reward model, we use the 6B model Dahoas/gptj-rm-static
trained from the same dataset based on EleutherAI/gpt-j-6b as a proxy of human preference.
Methods. We compare two versions of P3O, P3O-V1 andP3O-V2 , which represent clipping sepa-
rately and jointly respectively, with several effective and representative approaches for LLM align-
ment. We start with the SFT policy trained by token-wise supervised fine-tuning. It hasn‚Äôt gone
through further alignment; Every other method uses the SFT model as initialization. For RL al-
gorithms1, we consider the dominant approach PPO (Schulman et al., 2017; Ouyang et al., 2022)
with reward specified in Eq (2). We follow the implementation of trlx (Castricato et al., 2023).
Besides, we also consider the newly proposed DPO (Rafailov et al., 2023), a method that directly
optimizes the policy towards the closed-form solution of the KL-constrained reward maximization.
Although DPO is proposed as an offline alignment method, we notice that we can make it online
with the help of a proxy reward function. (More details can be found in Appendix A.2)
Figure 2: KL-Reward frontier for TL;DR: x-axis
represent DKL(œÄŒ∏‚à•œÄSFT),y-axis represent the re-
ward evaluated by the proxy reward model, both
averaged over 200 test prompts and evaluate every
500 gradient steps. We find that a simple linear
function fit the curve well, and P3O have the best
KL-Reward trade-off among the three.Evaluations. Deviating too much from the ref-
erence policy ( e.g. SFT model) would lead
the online policy to cut corners of the reward
model and produce incoherent continuations, as
pointed out by previous works (Ziegler et al.,
2019). Gao et al. (2023) studied the scal-
ing law of reward over-optimization in a syn-
thetic setup, where labels are supplied by a
‚Äúgold standard‚Äù reward model. They empir-
ically find out the golden reward can be ap-
proximated by a simple function form involving
the square-root KL-divergence from the refer-
ence policy. Therefore, it is important to bal-
ance the trade-off between the KL-divergence
and asymptotic reward, and we measure the ef-
fectiveness of each algorithm by its frontier of
achieved reward and KL-divergence from the
reference policy ( KL-Reward Frontier ). To
directly evaluate the quality of generated re-
sponses, we also perform Head-to-Head Com-
parisons between every pair of algorithms in
the HH dataset. We use two metrics for evalua-
1Among methods directly involving RL, we note that PPO and a modified version A2C (Mnih et al., 2016;
Lee et al., 2023) are the only two current online RL methods for LLM alignment. However, there is no strong
evidence showing the supremacy of A2C over PPO, so we choose PPO as our baseline.
7

--- PAGE 8 ---
Under review as a conference paper
Figure 3: KL-Reward frontier for HH: x-axis and y-axis represents DKL(œÄŒ∏‚à•œÄSFT)and the reward
respectively. Each point represent an average of results over 280 test prompts and calculated every
500 gradient updates. Left two figure compare P3O-V1 and PPO with varying base model sizes;
Right two figures compare P3O-V2 and online-DPO. Results showing that P3O can not only achieve
higher reward but also yield better KL control.
tion: (1) Reward , the optimized target during online RL, (2) GPT-4 , as a faithful proxy for human
evaluation of response helpfulness. For the latter metric, we shall point out that previous studies
show that LLMs can be better automated evaluators than existing metrics (Chen et al., 2023), and
GPT-4 judgments correlate strongly with humans, with human agreement with GPT-4 typically sim-
ilar or higher than inter-human annotator agreement (Rafailov et al., 2023). Additional details can
be found in Appendix C.
5.1 KL-R EWARD FRONTIER
We conduct experiments on both TL;DR and HH datasets to evaluate the efficacy of the alignment
algorithms in optimizing reward while restricting policy deviation from the reference. Figures 2 and
3 demonstrate the KL-Reward frontier for TL;DR and HH respectively. Each point represents the
average evaluation over test prompts at every 500-step interval. The x-axis represents the average
sequence-level KL-divergence DKL(œÄŒ∏‚à•œÄSFT), whereas the y-axis stands for the average reward
given by the proxy reward model. For summarization task, we find that P3O-V1 can reach a slightly
higher reward than P3O-V2, while with a worse KL-Reward trade-off. Consequently, only P3O-V2
is included in Figure 2 for comparison. We find that P3O-V2 is able to produce almost the same
highest reward whilst maintaining superior KL efficiency. DPO, despite its faster convergence, ex-
hibits a 25% higher KL-divergence than P3O-V2 under the same reward. For the question-answering
task, P3O-V1 and P3O-V2 have strictly dominant frontiers than PPO and DPO respectively in both
model sizes, shown by Figure 3. Empirical findings establish P3O‚Äôs superior trade-off between KL
and Reward over other baselines, delivering a substantial higher reward in the range of 0.1-0.3.
8

--- PAGE 9 ---
Under review as a conference paper
P3O DPO PPO SFT
P3O
DPO
PPO
SFT50.0 54.6 57.0 69.3
45.4 50.0 54.0 67.3
43.0 46.0 50.0 61.6
30.7 32.7 38.4 50.0
35404550556065GPT-4 win rate [%]
P3O DPO PPO SFT
P3O
DPO
PPO
SFT50.0 50.5 60.5 77.9
49.5 50.0 62.1 76.3
39.5 37.9 50.0 69.6
22.1 23.7 30.4 50.0
 3040506070reward win rate [%]
Figure 4: Head-to-head comparisons between algorithm pairs from {P3O, DPO, PPO, SFT }.Left
figure displays the win rate as evaluated by GPT-4. Right figure presents the win rate based on direct
comparison of the proxy reward. Despite the high correlation between the figures, we found that the
reward win rate must be adjusted according to the KL in order to align with the GPT-4 win rate.
5.2 H EAD-TO-HEAD COMPARISONS
To verify the reliability of prior results, we conduct head-to-head comparisons between each al-
gorithm pair among {P3O, DPO, PPO, SFT }. Since the KL-Reward frontier indicates that joint-
clipping (P3O-V2) produces more stable results than separate-clipping (P3O-V1), we only consider
P3O-V2 in this section and refer it as P3O. We sample completions from different policies2on the
test set of the HH dataset at default temperature 1.0, and we compute the average pairwise win rate
using (1) reward and (2) GPT-4 as evaluators. Previous studies (Chen et al., 2023; Rafailov et al.,
2023) have shown that GPT-4 is a faithful proxy for human preference and is widely adopted for
comparisons. The prompt used for evaluation is presented in Appendix C.4.
Figure 4 presents the comprehensive pairwise comparison results, both via proxy reward and GPT-4.
The average KL-divergence and reward ranking of these models is DPO >P3O>PPO >SFT.
Although DPO marginally surpasses P3O in reward, it has a considerably higher KL-divergence
(Table 1), which may be detrimental to the quality of generation. As a result, DPO has a reward win
rate 49.5% against P3O but only 45.4% as evaluated by GPT-4. Compared with other methods, P3O
exhibits a GPT-4 win rate of 57.0% against PPO and 69.3% against SFT. This result is consistent
with our findings from the KL-Reward frontier section, affirming that P3O could better align with
human preference than previous baselines.
6 C ONCLUSION & F UTURE WORKS
This work presents new insights into aligning large language models with human preferences via
reinforcement learning. We introduced the Reinforcement Learning with Relative Feedback frame-
work, which unifies the core principles of reward modeling and RL fine-tuning. Under this frame-
work, we designed a novel policy gradient algorithm, P3O, based on pairwise feedback. Empirical
evaluations demonstrated that P3O outperforms prior methods in terms of the KL-Reward frontier as
well as GPT-4 win-rate. P3O inherit the advantages of policy gradient methods, while maintaining
a simplicity in both algorithmic design and function approximation.
Looking ahead, several intriguing questions arise for future exploration. Firstly, we aim to under-
stand the impacts of reward over-optimization on trajectory-based RL algorithms and token-based
RL algorithms. Secondly, we are interested in whether we can generalize the policy gradient al-
gorithm to accommodate more than two ranked responses, potentially enabling a better trade-off
between human effort and AI alignment. Finally, we wish to explore the benefits of applying our
P3O algorithm in contexts beyond training language models with human feedback. We eagerly
anticipate investigating these questions in our future work.
2We select checkpoints with the highest reward for generation.
9

--- PAGE 10 ---
Under review as a conference paper
REFERENCES
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless
assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 ,
2022a.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harm-
lessness from ai feedback. arXiv preprint arXiv:2212.08073 , 2022b.
Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method
of paired comparisons. Biometrika , 39(3/4):324‚Äì345, 1952.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877‚Äì1901, 2020.
S¬¥ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Ka-
mar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general
intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 , 2023.
R¬¥obert Busa-Fekete, Bal ¬¥azs Sz ¬®or¬¥enyi, Paul Weng, Weiwei Cheng, and Eyke H ¬®ullermeier.
Preference-based reinforcement learning: evolutionary direct policy search using a preference-
based racing algorithm. Machine learning , 97:327‚Äì351, 2014.
Louis Castricato, Alex Havrilla, Shahbuland Matiana, Duy V . Phung, Aman Tiwari, Jonathan Tow,
and Maksym Zhuravinsky. trlX: A scalable framework for RLHF, June 2023. URL https:
//github.com/CarperAI/trlx .
Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. Exploring the use of large
language models for reference-free text quality evaluation: A preliminary empirical study. arXiv
preprint arXiv:2304.00723 , 2023.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. Advances in neural information processing sys-
tems, 30, 2017.
Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum,
and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment.
arXiv preprint arXiv:2304.06767 , 2023.
Miroslav Dud ¬¥ƒ±k, Katja Hofmann, Robert E Schapire, Aleksandrs Slivkins, and Masrour Zoghi. Con-
textual dueling bandits. In Conference on Learning Theory , pp. 563‚Äì587. PMLR, 2015.
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry
Rudolph, and Aleksander Madry. Implementation matters in deep policy gradients: A case study
on ppo and trpo. arXiv preprint arXiv:2005.12729 , 2020.
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben
Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to
reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 ,
2022.
Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In
International Conference on Machine Learning , pp. 10835‚Äì10866. PMLR, 2023.
Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek
Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training
(rest) for language modeling. arXiv preprint arXiv:2308.08998 , 2023.
10

--- PAGE 11 ---
Under review as a conference paper
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C ¬¥esar Teodoro Mendes, Allie Del Giorno, Sivakanth
Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are
all you need. arXiv preprint arXiv:2306.11644 , 2023.
Ashesh Jain, Brian Wojcik, Thorsten Joachims, and Ashutosh Saxena. Learning trajectory pref-
erences for manipulators via iterative improvement. Advances in neural information processing
systems , 26, 2013.
Andras Kupcsik, David Hsu, and Wee Sun Lee. Learning dynamic robot-to-human object handover
from human feedback. Robotics Research: Volume 1 , pp. 161‚Äì176, 2018.
Tor Lattimore and Csaba Szepesv ¬¥ari.Bandit algorithms . Cambridge University Press, 2020.
Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor
Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with
ai feedback. arXiv preprint arXiv:2309.00267 , 2023.
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let‚Äôs verify step by step. arXiv preprint
arXiv:2305.20050 , 2023.
V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning , pp. 1928‚Äì1937. PMLR, 2016.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in Neural Information Processing Systems , 35:
27730‚Äì27744, 2022.
Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia
Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models.
arXiv preprint arXiv:2202.03286 , 2022.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea
Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv
preprint arXiv:2305.18290 , 2023.
Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant ¬¥e Brantley, Jack Hessel, Rafet Sifa, Chris-
tian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural
language processing?: Benchmarks, baselines, and building blocks for natural language policy
optimization. arXiv preprint arXiv:2210.01241 , 2022.
Dorsa Sadigh, Anca D Dragan, Shankar Sastry, and Sanjit A Seshia. Active preference-based learn-
ing of reward functions . 2017.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning , pp. 1889‚Äì1897. PMLR,
2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. arXiv preprint
arXiv:1506.02438 , 2015b.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.
Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural
language generation with implicit language q learning. arXiv preprint arXiv:2206.11871 , 2022.
Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang.
Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492 , 2023.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018.
11

--- PAGE 12 ---
Under review as a conference paper
Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. Advances in neural information
processing systems , 12, 1999.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze
Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog
applications. arXiv preprint arXiv:2201.08239 , 2022.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ¬¥ee
Lacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.
Michael V ¬®olske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to learn
automatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization ,
pp. 59‚Äì63, 2017.
Tianhao Wu, Yunchang Yang, Han Zhong, Liwei Wang, Simon Du, and Jiantao Jiao. Nearly optimal
policy optimization with stable at any time guarantee. In International Conference on Machine
Learning , pp. 24243‚Äì24265. PMLR, 2022.
Jun Xu, Zeng Wei, Long Xia, Yanyan Lan, Dawei Yin, Xueqi Cheng, and Ji-Rong Wen. Reinforce-
ment learning to rank with pairwise policy gradient. In Proceedings of the 43rd International
ACM SIGIR Conference on Research and Development in Information Retrieval , pp. 509‚Äì518,
2020.
Yunchang Yang, Tianhao Wu, Han Zhong, Evrard Garcelon, Matteo Pirotta, Alessandro Lazaric,
Liwei Wang, and Simon S Du. A reduction-based framework for conservative bandits and rein-
forcement learning. arXiv preprint arXiv:2106.11692 , 2021.
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf:
Rank responses to align language models with human feedback without tears. arXiv preprint
arXiv:2304.05302 , 2023.
Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims. The k-armed dueling bandits
problem. Journal of Computer and System Sciences , 78(5):1538‚Äì1556, 2012.
Rui Zheng, Shihan Dou, Songyang Gao, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu,
Limao Xiong, Lu Chen, et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint
arXiv:2307.04964 , 2023.
Banghua Zhu, Jiantao Jiao, and Michael I Jordan. Principled reinforcement learning with human
feedback from pairwise or k-wise comparisons. arXiv preprint arXiv:2301.11270 , 2023a.
Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael I Jordan,
and Jiantao Jiao. Fine-tuning language models with advantage-induced policy alignment. arXiv
preprint arXiv:2306.02231 , 2023b.
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul
Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv
preprint arXiv:1909.08593 , 2019.
A A LGORITHMS
A.1 P SEUDOCODES
We present the pseudocode for both the Vanilla Policy Gradient (VPG) and our proposed algorithm
P3O. While both algorithms follow the similar procedure of collecting trajectories and leveraging
these trajectories to estimate the gradient, there are key differences: Our method collect pairwise
trajectories and comput trajectory-wise rewards. This approach eliminates the need for estimating
the value function Vand bypasses the requirement of estimating the advantage function using Gen-
eralized Advantage Estimation (GAE). Consequently, P3O is not only simpler to implement but also
introduces less bias into the estimation of the policy gradient.
12

--- PAGE 13 ---
Under review as a conference paper
Algorithm 1 Vanilla Policy Gradient
1:Initialization: Initialize policy parameters Œ∏0and value function parameters œï0
2:fork= 0,1,2¬∑¬∑¬∑do
3: Collect trajectories Dk={œÑi}by running policy œÄŒ∏kstarting from a batch of prompts and
generate single trajectory from each prompt.
4: Compute token-wise rewards contain both token-wise KL and preference reward as in
Equation 2. And then rewards-to-go ÀÜRt.
5: Estimate advantage estimates dAdvtvia GAE or other methods.
6: Estimate policy gradient via:
ÀÜgk=1
|Dk|X
œÑ‚ààDkTX
k=0dAdvt‚àáŒ∏logœÄŒ∏(at|st)
7: Apply gradient updates to Œ∏kusing gradient descent.
8: Fit value function by regression on mean-squared error via gradient descent:
œïk+1= arg min
œï1
|Dk|TX
œÑ‚ààDkTX
t=0(Vœï(st)‚àíÀÜRt)2
9:end for
Algorithm 2 Pairwise Proximal Policy Optimization (P3O)
1:Initialization: Initialize policy parameters Œ∏0
2:fork= 0,1,2¬∑¬∑¬∑do
3: Collect pairwise trajectories Dk={œÑi}by running policy œÄŒ∏kstarting from a batch of
prompts and generate two trajectories from each prompt.
4: Compute trajectory-wise rewards contain both trajectory-wise KL and preference reward:
ÀÜrfinal= ÀÜrpreference ‚àíŒ≤DKL(œÄŒ∏k(y|x)‚à•œÄSFT(y|x))
5: Estimate either version of clipped pairwise policy gradient via equation in section 4.2
6: Apply gradient updates to Œ∏kusing gradient descent.
7:end for
A.2 D ERIVATION OF DPO
DPO start with a preference dataset Dand minimize the loss:
LDPO=‚àí E
(x,yw,yl)‚àºDlogœÉ
Œ≤logœÄŒ∏(yw|x)
œÄSFT(yw|x)‚àíŒ≤logœÄŒ∏(yl|x)
œÄSFT(yl|x)
However, this is offline since the algorithm only make use of a fixed dataset. Instead, notice that
if we have a reward function r, we can use the reward function to label the preference result in an
online fashion. Assume there are two new generated responses y1,y2that have reward r1, r2. Then
we simply label the preference according to Bradley & Terry (1952),
y1‚âªy2w.p. œÉ(r1‚àír2)
y2‚âªy1w.p. œÉ(r2‚àír1)
We would like to use the notation ywandylto represent the preferred and less preferred response
chosen by the reward. We collect all the newly generated responses into a replay buffer Dreplay,
therefore we can optimize the same DPO loss here:
LDPO=‚àí E
(x,yw,yl)‚àºD replaylogœÉ
Œ≤logœÄŒ∏(yw|x)
œÄSFT(yw|x)‚àíŒ≤logœÄŒ∏(yl|x)
œÄSFT(yl|x)
We can further reduce the variance of the loss by eliminating the randomness in labelling the pref-
erence by incorporating the known labeling probability explicitly in the formula,
LDPO=‚àí E
(x,y1,y2)‚àºD replay
œµ‚àºBer(œÉ(r1‚àír2))logœÉ
œµŒ≤logœÄŒ∏(y1|x)
œÄSFT(y1|x)‚àíœµŒ≤logœÄŒ∏(y2|x)
œÄSFT(y2|x)
13

--- PAGE 14 ---
Under review as a conference paper
Here, Ber (œÉ(r1‚àír2))is the two point Bernoulli distribution on {‚àí1,1}.
B P ROOFS
B.1 P ROOF OF THEOREM 1
In the contextual bandit setting, VPG aims for estimating the gradient ‚àáLVPG=Ex‚àºD‚àáLVPG(x),
where ‚àáLVPG(x)can be expressed as:
‚àáLVPG(x) = E
y‚àºœÄŒ∏(y|x)r(y|x)‚àálogœÄŒ∏(y|x)
The expectation can be replaced with a summation, leading to:
‚àáLVPG(x) =X
yr(y|x)‚àáœÄŒ∏(y|x) (7)
= X
y1r(y1|x)‚àáœÄŒ∏(y1|x)‚àíX
y1,y2r(y2|x)œÄŒ∏(y2|x)‚àáœÄŒ∏(y1|x)!
(8)
= X
y1,y2r(y1|x)œÄŒ∏(y2|x)‚àáœÄŒ∏(y1|x)‚àíX
y1,y2r(y2|x)œÄŒ∏(y2|x)‚àáœÄŒ∏(y1|x)!
(9)
=X
y1,y2(r(y1|x)‚àír(y2|x))œÄŒ∏(y2|x)‚àáœÄŒ∏(y1|x) (10)
= E
y1,y2‚àºœÄŒ∏old(r(y1|x)‚àír(y2|x))œÄŒ∏(y2|x)
œÄŒ∏old(y2|x)‚àáœÄŒ∏(y1|x)
œÄŒ∏old(y1|x)(11)
In Equation (8) we subtract the latter term which equals to 0becauseP
y1‚àáœÄŒ∏(y1|x) =
‚àáP
y1œÄŒ∏(y1|x) =‚àá¬∑1 = 0 . We further multiply the first term by 1 =P
y2œÄŒ∏(y2|x)in Equation
(9). Finally, we rephrase the previous equation using importance sampling and yield Equation 11.
Swap actions y1,y2and average together we get the desired form:
‚àáLVPG(x) = E
y1,y2‚àºœÄŒ∏old(r(y1|x)‚àír(y2|x))œÄŒ∏(y2|x)
œÄŒ∏old(y2|x)‚àáœÄŒ∏(y1|x)
œÄŒ∏old(y1|x)‚àíœÄŒ∏(y1|x)
œÄŒ∏old(y1|x)‚àáœÄŒ∏(y2|x)
œÄŒ∏old(y2|x)
/2
= E
y1,y2‚àºœÄŒ∏old(r(y1|x)‚àír(y2|x))œÄŒ∏(y1|x)
œÄŒ∏old(y1|x)œÄŒ∏(y2|x)
œÄŒ∏old(y2|x)‚àá
logœÄŒ∏(y1|x)
œÄŒ∏(y2|x)
/2
B.2 P ROOF OF LEMMA 1
In this proof, we aim to show that two equivalent reward functions randr‚Ä≤yield the same loss under
the Bradley-Terry model. Assume that r‚àºr‚Ä≤, then by definition there exist Œ¥(x)such that for any
prompt and response pair (x,y),r‚Ä≤(y|x) =r(y|x) +Œ¥(x).
Consider any prompt xand two responses yw,yllabeled by human. According to Equation 1, the
Bradley-Terry loss for this pair given reward ris:
loss= log œÉ(r(yw|x)‚àír(yl|x))
Similarly, the Bradley-Terry loss for this pair given reward r‚Ä≤is:
loss‚Ä≤= log œÉ(r‚Ä≤(yw|x)‚àír‚Ä≤(yl|x))
By substituting r‚Ä≤(y|x)withr(y|x) +Œ¥(x)in loss‚Ä≤, we get:
r‚Ä≤(yw|x)‚àír‚Ä≤(yl|x) = (r(yw|x) +Œ¥(x))‚àí(r‚Ä≤(yl|x) +Œ¥(x)) =r(yw|x)‚àír(yl|x)
This shows that loss‚Ä≤=loss, indicating that the two reward functions randr‚Ä≤are indeed equivalent
with respect to the Bradley-Terry loss. The same proof would go through for the Plackett-Luce loss,
which we omit here for brevity.
14

--- PAGE 15 ---
Under review as a conference paper
B.3 P ROOF OF LEMMA 2
We first prove the invariance for P3O and DPO, then we prove that PPO is not invariant.
Assume that we have two equivalent reward functions r‚àºr‚Ä≤, by definition there exist Œ¥(x)such
that for any prompt and response pair (x,y),r‚Ä≤(y|x) =r(y|x) +Œ¥(x).
Invariance of P3O: This is trivial since the gradient directly involve r1‚àír2. We take P3O-V2 as
an example and write the gradient formulation with respect to the prompt responses pair (x,y1,y2):
If the reward is r, the update follows:
LP3O
r=sg
(r(y1|x)‚àír(y2|x))œÄŒ∏(y1|x)
œÄŒ∏old(y1|x)œÄŒ∏(y2|x)
œÄŒ∏old(y2|x)
logœÄŒ∏(y1|x)
œÄŒ∏(y2|x)
LP3O
r,clip=sg
(r(y1|x)‚àír(y2|x))œÄŒ∏(y1|x)
œÄŒ∏old(y1|x)œÄŒ∏(y2|x)
œÄŒ∏old(y2|x)
√óclip
logœÄŒ∏(y1|x)
œÄŒ∏(y2|x),logœÄŒ∏old(y1|x)
œÄŒ∏old(y2|x)‚àíœµ,logœÄŒ∏old(y1|x)
œÄŒ∏old(y2|x)+œµ
‚àáLP3O
r,joi=‚àámin(LP3O
r,LP3O
r,clip)
Similarly, if the reward is r‚Ä≤, the gradient is:
LP3O
r‚Ä≤=sg
(r‚Ä≤(y1|x)‚àír‚Ä≤(y2|x))œÄŒ∏(y1|x)
œÄŒ∏old(y1|x)œÄŒ∏(y2|x)
œÄŒ∏old(y2|x)
logœÄŒ∏(y1|x)
œÄŒ∏(y2|x)
LP3O
r‚Ä≤,clip=sg
(r‚Ä≤(y1|x)‚àír‚Ä≤(y2|x))œÄŒ∏(y1|x)
œÄŒ∏old(y1|x)œÄŒ∏(y2|x)
œÄŒ∏old(y2|x)
√óclip
logœÄŒ∏(y1|x)
œÄŒ∏(y2|x),logœÄŒ∏old(y1|x)
œÄŒ∏old(y2|x)‚àíœµ,logœÄŒ∏old(y1|x)
œÄŒ∏old(y2|x)+œµ
‚àáLP3O
r‚Ä≤,joi=‚àámin(LP3O
r‚Ä≤,LP3O
r‚Ä≤,clip)
We can see that the only difference between these two updates in the reward difference part. How-
ever, due to the fact that
(r‚Ä≤(y1|x)‚àír‚Ä≤(y2|x)) = ( r(y1|x) +Œ¥(x)‚àír(y2|x)‚àíŒ¥(x)) = ( r(y1|x)‚àír(y2|x))
We conclude that LP3O
r =LP3O
r‚Ä≤andLP3O
r,clip =LP3O
r‚Ä≤,clip. Consequently, the two updates
‚àáLP3O
r,joi,‚àáLP3O
r‚Ä≤,joiare the same.
Invariance of DPO: Assume the same setting as in the previous paragraph:
The gradient of DPO given reward rcan be written as:
‚àáLDPO
r=‚àí E
œµ‚àºBer(œÉ(r1‚àír2))logœÉ
œµŒ≤logœÄŒ∏(y1|x)
œÄSFT(y1|x)‚àíœµŒ≤logœÄŒ∏(y2|x)
œÄSFT(y2|x)
Similarly, the gradient of DPO given reward r‚Ä≤can be expressed as:
‚àáLDPO
r‚Ä≤=‚àí E
œµ‚àºBer(œÉ(r‚Ä≤
1‚àír‚Ä≤
2))logœÉ
œµŒ≤logœÄŒ∏(y1|x)
œÄSFT(y1|x)‚àíœµŒ≤logœÄŒ∏(y2|x)
œÄSFT(y2|x)
The only difference between these two equations is the sampling distribution of the Bernoulli distri-
bution. Easy to verify that they are the same since œÉ(r1‚àír2) =œÉ(r‚Ä≤
1‚àír‚Ä≤
2).
PPO is not Invariant: The loss of PPO is the combination of policy-loss and V-loss, with the
trade-off of these two terms controlled by hyper-parameter Œ∑:
LPPO=Lpolicy+Œ∑LV
Suppose the policy network and the Vnetwork have separate parameters, then taking the gradient of
LPPOis simply taking gradient of Lpolicy. We aim to prove that the gradient of Lpolicy is not identical
for two equivalent rewards. We first recap the formula of Lpolicy:
Lpolicy =‚àí E
(st,at)‚àºœÄŒ∏oldminœÄŒ∏(at|st)
œÄŒ∏old(at|st)dAdv(at|st),clipœÄŒ∏(at|st)
œÄŒ∏old(at|st),1‚àíœµ,1 +œµ
dAdv(at|st)
15

--- PAGE 16 ---
Under review as a conference paper
Where the dAdv is estimated via GAE:
dAdv(at|st) =Œ¥t+ (ŒªŒ≥)Œ¥t+1+¬∑¬∑¬∑+ (ŒªŒ≥)T‚àít+1Œ¥T‚àí1
Œ¥t=r(st, at) +Œ≥V(st+1)‚àíV(st)
For simplicity, we consider the one-sample case, where we are taking gradient with respect to the
sample (st, at). According to the formula of dAdv, and combine with the fact that the reward is only
appended to the last token T. We have the following relation,
dAdvr‚Ä≤(at|st) =dAdvr(at|st) + (ŒªŒ≥)T‚àít+1Œ¥
Here, Œ¥=Œ¥(x),xrepresent the prompt corresponding to st, which is a prefix of st. As a re-
sult,‚àámin
œÄŒ∏(at|st)
œÄŒ∏old(at|st)dAdv(at|st),clip
œÄŒ∏(at|st)
œÄŒ∏old(at|st),1‚àíœµ,1 +œµ
dAdv(at|st)
will not stay un-
changed for different reward r, sincedAdv can be arbitrary real number by choosing Œ¥.
C A DDITIONAL EXPERIMENT RESULTS
C.1 S ETUP
For the hyper-parameter tuning, we first run PPO to search the learning rate among {0.5,1,2,4,8}√ó
10‚àí6that yields the best KL-Reward frontier (Section 5). We then use the same learning rate for
P3O and online-DPO without further hyper-parameter tuning. To ensure fair comparison, we double
the batch size of PPO such that every algorithms can see the same number of responses, although
P3O and online-DPO only see half the prompts.
C.2 A BLATION STUDY
In this study, we conducted ablation experiments to assess the impact of specific elements in our
algorithm design, namely the clipping technique and the KL coefficient. Our study primarily aims
to answer two questions:
1 How does the clipping technique influence our algorithm‚Äôs performance?
2 What is the effect of varying the KL-control coefficient on the KL-Reward frontier?
Figure 5: The Left figure illustrates the effect of varying the KL coefficient within the set
0.02,0.05,0.1,0.2. The Right figure compares P3O-V2 with and without the clipping technique
(solid lines represent with clipping, dashed lines without)
Setting : Both experiments are conducted under the HH dataset. For the KL experiment, we use the
best configuration as in the HH experiment, with the only difference being the KL coefficient Œ≤in
Eq (2). For the No Clip experiment, we not only test on the default learning rate 1√ó10‚àí6, but also
16

--- PAGE 17 ---
Under review as a conference paper
experiment with learning rate 5√ó10‚àí6and1√ó10‚àí5. For the No Clip baseline, we directly use the
PPG formulation in Theorem 1.
Our results shown in the right figure indicate that using the clipping technique positively impacts the
KL-Reward frontier, particularly in the early stages of training. This enhancement is accompanied
by a slight decrease in the asymptotic reward. Interestingly, our results also show that the clipping
technique is more effective when the learning rate is larger. This aligns with our understanding, as
a smaller learning rate would naturally keep the update ratio closer to 1, thereby reducing the need
for clipping.
In terms of the KL-control coefficient, our findings illustrate that its value significantly influences
the algorithm‚Äôs performance. Larger KL coefficients led to a slight improvement in the KL-Reward
frontier but a larger decrease in the asymptotic reward. This result suggests that using a smaller KL
coefficient might provide a more favorable trade-off between reward and KL-divergence.
C.3 A DDITIONAL STATISTICS
Here we provide additional statistics for the checkpoints we used in GPT-4 evaluation. We see that
PPO and SFT tends to generate long responses while P3O and DPO generate shorter responses.
Moreover, P3O achieve nearly the same reward with DPO while incurring much less KL.
Table 1: Reward
P3O DPO PPO SFT
Reward ‚àí0.302 ‚àí0.298 ‚àí0.613 ‚àí1.195
KL % sample 9.83 12 .01 7 .02 0
KL % token 0.12 0 .14 0 .06 0
Token num 80.46 88 .84 109 .03 112 .70
C.4 P ROMPT
For the following query to a chatbot, which response is more helpful?
Query:
<user_query>
Response A:
<response_a>
Response B:
<response_b>
FIRST provide a one-sentence comparison of the two responses and
explain which you feel is more helpful. SECOND, on a new line, state only
"A" or "B" to indicate which response is more helpful.
Your response should use the format:
Comparison: <one-sentence comparison and explanation>
More helpful: <"A" or "B">
To eliminate any potential bias from the order of the responses, the order of Response A and Re-
sponse B is randomized for each prompt. We note that although we ask GPT-4 to directly respond
A or B to indicate the preference, in approximately 3% of evaluations, GPT-4 found that neither
response adequately addressed the user‚Äôs query.
17

--- PAGE 18 ---
Under review as a conference paper
C.5 GPT-4 E VALUATION RESULTS
Prompt Human: help with understanding bitcoin
Assistant:
P3O Yes, that‚Äôs a fun question! Bitcoin can be seen as a digital currency, but there are a few
differences.
What makes Bitcoin different is that it uses a digital wallet, instead of a cash-like system.
This allows you to spend it anywhere you want, without needing a card or a bank account.
You can also use Bitcoin to spend other forms of currency.
This use is growing in popularity, and Bitcoin is also helping people move towards digital
currency use.
DPO Can you describe a bit more about what Bitcoin is and what it‚Äôs used for?
PPO What would you like to know?
SFT What is Bitcoin?
GPT-4 Comparison: Response A[ P3O ] provides a brief explanation about Bitcoin and its uses,
while Response B[ DPO ] asks for more information from the user instead of providing any
information.
Comparison: Response A[ PPO ] is vague and doesn‚Äôt provide any information, while Re-
sponse B[ P3O ] provides a detailed explanation about Bitcoin and its uses.
Comparison: Response A[ SFT] is a question rather than an explanation, while Response
B[P3O ] provides a detailed explanation of what Bitcoin is and how it works, making it
more informative and helpful.
Table 2: P3O outperforms the others by providing a comprehensive explanation about Bitcoin and
its uses, while the other algorithms respond with questions or requests for more information, rather
than providing substantive information.
Prompt Human: I need help deciding what city to move to
Assistant: You could just use Google for that, or get specific about the criteria you‚Äôre
looking for, like weather and job markets.
Human: I‚Äôd like a mid sized city with fun activities, a good job market, and nice weather
if possible
Assistant:
P3O Maybe try Austin, Dallas, or Phoenix. They might be possible choices.
DPO That‚Äôs a very reasonable description, and it comes with an affordable cost of living.
PPO I think that depends a lot on your situation. Is there a job you‚Äôre particularly interested in?
SFT Is this for work, or pleasure?
GPT-4 Comparison: Response A[ DPO ] is vague and doesn‚Äôt provide any specific suggestions,
while Response B[ P3O ] provides three specific city suggestions that could potentially
meet the user‚Äôs criteria.
Comparison: Response A[ PPO ] seeks to gather more information to provide a more tai-
lored suggestion, while Response B[ P3O ] provides immediate suggestions based on the
criteria given.
Comparison: Response A[ SFT] seeks further clarification while Response B[ P3O ] pro-
vides direct suggestions based on the criteria provided by the user, making it more helpful.
Table 3: In each comparison, P3O outperformed the other algorithms, effectively suggesting three
potential cities (Austin, Dallas, or Phoenix) that align with the user‚Äôs criteria. In contrast, DPO‚Äôs
response was rather vague, offering no specific suggestions. PPO and SFT sought to gather more
information, delaying immediate assistance.
18

--- PAGE 19 ---
Under review as a conference paper
Prompt Human: How can I make my own soap at home?
Assistant:
P3O There are a few different methods for making your own soap at home. Oil or lye are
usually the most common, but you can also use a few different types of chemicals to get
a similar effect or create a unique variation. Some effective ways include freezing the
ingredients, using deodorants or oils, and adding essential oils or vinegar to give it a nice
drying effect.
DPO I wouldn‚Äôt recommend it, it‚Äôs a little fancier and not something you can do at home.
PPO Making your own soap is a nice way to get involved in your own community. You can
help provide vital assistance to the local community, and learn about effective treatments
and treatments for health problems.
SFT Well historically, in Europe soap has typically not been watering shells and Ocean
Aerosols, a combination generally indicated as ‚ÄùEST date‚Äù a secret date from the man-
ufacturer. Soil type and utilization may be shedding light on the most efficient method for
neutralizing and purifying chlorine, various chemicals involved in natural fragrances and
essential oils. To use composting, cooking oils temperature has to be low enough to assist
foam building. Additionally, using an electric pan, ‚Äùovercup‚Äù for cures, and ‚Äùa015‚Äù for
the final soap. How do you plan to save your energy and waste water?
GPT-4 Comparison: Response A[ P3O ] provides a detailed explanation of different methods for
making soap at home, while Response B[ DPO ] discourages the idea without providing
any useful information.
Comparison: Response A[ PPO ] is vague and doesn‚Äôt provide any specific steps or ingre-
dients for making soap, while Response B[ P3O ] gives a general idea of the process and
ingredients involved in soap making.
Comparison: Response A[ P3O ] provides a more straightforward and understandable ex-
planation of how to make soap at home, while Response B[ SFT] is confusing and uses
unclear terminology.
Table 4: P3O is assessed as the most helpful by GPT-4. It provides a detailed explanation of dif-
ferent methods for homemade soap creation, mentioning common ingredients and specific methods.
Conversely, DPO discourages the idea without giving any constructive guidance. PPO fails to offer
any specific steps or ingredients for soap creation. Finally, SFT delivers a response that is complex
and difficult to understand, featuring unclear terminology.
19
