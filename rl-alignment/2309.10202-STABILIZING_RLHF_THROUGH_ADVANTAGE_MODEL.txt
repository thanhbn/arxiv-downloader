# 2309.10202.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2309.10202.pdf
# File size: 1345383 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
STABILIZING RLHF THROUGH ADVANTAGE MODEL
AND SELECTIVE REHEARSAL
Baolin Peng∗, Linfeng Song∗, Ye Tian, Lifeng Jin, Haitao Mi, Dong Yu
Tencent AI Lab
{baolinpeng,lfsong,yaptian,lifengjin,haitaomi }@global.tencent.com
ABSTRACT
Large Language Models (LLMs) have revolutionized natural language processing,
yet aligning these models with human values and preferences using RLHF remains
a significant challenge. This challenge is characterized by various instabilities,
such as reward hacking and catastrophic forgetting. In this technical report, we
propose two innovations to stabilize RLHF training: ( i)Advantage Model , which
directly models advantage score i.e.,extra reward compared to the expected re-
wards and regulates score distributions across tasks to prevent reward hacking.
(ii)Selective Rehearsal , which mitigates catastrophic forgetting by strategically
selecting data for PPO training and knowledge rehearsing. Our experimental anal-
ysis on public and proprietary datasets reveals that the proposed methods not only
increase stability in RLHF training but also achieve higher reward scores and win
rates1.
1 I NTRODUCTION
Large language models (LLMs) have become a fundamental element in advancing natural language
processing (NLP) and artificial intelligence (AI), showcasing an impressive ability to generate text
that is both semantically and contextually relevant (OpenAI, 2023; K ¨opf et al., 2023; Touvron et al.,
2023). Despite these advancements, LLMs have the risk of engaging in undesirable behaviors, such
as fabricating information or producing biased, toxic, or even dangerous content, since LLMs are
trained on a wide array of data, which can include low-quality sources. This has highlighted the
necessities of LLM Alignments with human values, intentions, and preferences (Brown et al., 2020;
Ouyang et al., 2022; Bai et al., 2022a; Glaese et al., 2022).
Many approaches have been put forth to address the challenge LLM Alignments (Bai et al., 2022a;
OpenAI, 2023; Askell et al., 2021). Among these approaches, Reinforcement Learning from Hu-
man Feedback (RLHF) has demonstrated its efficacy in aligning language models with human pref-
erences. RLHF serves as a key component of training SoTA LLMs including exemplars such as
OpenAI’s GPT-4 (OpenAI, 2023), Anthropic’s Claude (Bai et al., 2022a), Google’s Sparrow (Glaese
et al., 2022), Bard, and Meta’s Llama 2-Chat (Touvron et al., 2023). RLHF elevates the capabilities
of LLMs beyond the mere modeling of the distribution of their training data. It endows LLMs with
the capacity to adapt their text generation distribution in a manner that are preferred by humans.
However, training LLMs using RLHF is undoubtedly challenging, which demands an accurate and
reliable reward model that approximates human judges, and a robust PPO algorithm for sustained
policy improvements. Even with meticulous configurations, instabilities ,e.g., gibberish responses
(but high-reward) (Stiennon et al., 2020; Skalse et al., 2022), forgetting learned knowledge, are
usually observed during training, which leads to recurring failures. These instabilities have several
causes: ( i) different reward score distributions are learned for various categories by the reward
model, potentially leading to reward hacking issues (Skalse et al., 2022), a phenomenon where the
model finds unintended ways to maximize the reward. As depicted in Figure 1a, the reward model
learns noticeable disparity in reward score distributions for Code Generation and QA tasks, 2 out of
∗Equal Contribution
1Work in progress
1arXiv:2309.10202v1  [cs.CL]  18 Sep 2023

--- PAGE 2 ---
(a) Reward score distributions.
(b) Win rate over the SFT model on the forget set eval-
uated by GPT-4.
Figure 1: Left: The distribution of reward scores for both the QA and Code Generation tasks. There
is a noticeable disparity in the learned reward score distributions between the two tasks, despite the
expectation that the distributions should be similar. Right : The win/loss rate over the SFT model
on the forget set exhibits a significant decline. This drop in the win rate can be attributed to reward
hacking and the phenomenon of catastrophic forgetting.
61 tasks present in the preference data. Even with reward score normalizations, the fluctuating means
and variances can induce unexpected model behaviors, such as transferring the response patterns
of Code Generations to QA examples due to the higher reward scores. ( ii) over-optimizing with
PPO on examples that were well-aligned with humans in the Supervised Fine-Tuning (SFT) stage
triggers catastrophic forgetting issues (McCloskey & Cohen, 1989; Gupta et al., 2023; Khetarpal
et al., 2022). Models tend to overlook what was learned during the SFT stage, i.e.,PPO model
underperforms the SFT model on expert-aligned examples2, as shown in Figure 1b.
Accordingly, in this technical report, we introduce two techniques to enhance the stability and ef-
fectiveness of the training of RLHF. Firstly, we propose Advantage Model to balance the reward
score distributions across various categories, thus averting the reward hacking dilemma that is often
induced by noticeable differences score distributions. This is achieved by directly modeling the ad-
vantage score, i.e.,the extra reward one response can obtain compared with the expected reward, and
regulating the advantage score distribution dynamically during training, ensuring that the variances
and means are maintained within a reasonable range. Secondly, we introduce the Selective Rehearsal
to alleviate the catastrophic forgetting issue. We posit that not all data should be optimized equally
in PPO training. As such, we propose a robust and effective data selector that automatically iden-
tifies what examples could be utilized for PPO training and should be used to rehearsal knowledge
accumulated in the SFT stage, preventing the depreciation of the model’s performance on expert-
aligned examples over time. Experiments on both public and proprietary data have demonstrated
that our Advantage Model successfully balances reward score distributions across various examples
while preserves ranking precision, and guide PPO training to achieve a higher reward score and win
rate compared to the SFT model. Furthermore, Selective Rehearsal is able to avoid over-optimizing
by selecting the most suitable examples for PPO training, thereby sustaining the performance on
expert-aligned examples.
Our contributions are summarized as follows:
• We analyze and identify several causes of instability in RLHF training, namely, imbal-
anced learned reward score distributions and over-optimization of certain PPO training
data, which lead to reward hacking and catastrophic forgetting issues.
• We introduce the Advantage Model to balance reward score distributions across various
categories, and the Selective Rehearsal strategy to discern which examples should be used
2Expert-aligned Examples are data samples that meet the standards and criteria delineated by experts and
closely align with human preferences. These examples are used for SFT model training and evaluation.
2

--- PAGE 3 ---
for PPO training and which should be reserved for rehearsing knowledge accrued in the
SFT stage.
• Through extensive experiments on both public and proprietary datasets, we demonstrate
that the Advantage Model andSelective Rehearsal are able to stabilize RLHF training,
achieving higher reward scores and win rates.
2 P RELIMINARY
In recent machine learning research, RLHF (Ouyang et al., 2022; Bai et al., 2022a) has emerged
as a pivotal strategy for aligning LLMs to human goals (e.g. being helpful and harmless). RLHF
typically follows the SFT phase, where SFT aligns a LLM with human objectives using teacher
forcing on (prompt, response) pairs. However, despite this alignment, the LLM may still struggle
with generalization when faced with unseen tasks.
Learning a reward function from interaction between LLMs and humans and optimizing LLMs with
the learned reward function using reinforcement learning has been shown as an effective approach
to solving the LLM alignment problem. Leike et al. 2018; Stiennon et al. 2020; Ouyang et al. 2022
proposed a method involving reinforcement learning from human feedback, where RMs are trained
on a dataset of comparisons between two model outputs generated from the same input. The goal
is to assign higher rewards to outputs preferred by human labelers over others. Typically, this is
achieved by adding a value head that outputs a scalar value on pre-trained transformer-baesd LMs
with last umembedding layer removed. Specifically, the reward modeling loss is as follows:
LRM=−E(x,yc,yr)∼DRM[log(σ(rθ(x, yc)−rθ(x, yr)))] (1)
where rθ(x, y)denotes the reward score for prompt xand response ywith parameters θ,ycis the
preferred response of the pair ycandyr, and DRMis the complete of comparison dataset.
In what follows, Proximal Policy Optimization (PPO) (Schulman et al., 2017) is commonly adopted
as the reinforcement learning algorithm to optimize a policy due to its strengths in stability and
simplicity. Particularly, the PPO objective for policy πon a prompt dataset Dis defined as:
LPPO=Ex∼DPPO,y∼πϕ(x)
rθ(x, y)−βlog 
πϕ(y|x)/πinit(y|x)
(2)
where rθ(x, y)represents the reward score on the (prompt, response) pair of (x, y);πinitindicates
the policy before RLHF, and it is kept constant during RLHF training; βis the coefficient for the
KL-divergence term.
Besides PPO, rejection sampling (Touvron et al., 2023) recently gains interests as a simple way for
aligning LLMs. As an offline policy learning algorithm, it adopts an iterative process. For each
iteration n, it first constructs a new dataset Dnby selecting (x, y)pairs from the main policy πϕ
based on criteria F:
DPPO
n={(x, y)· F(x, y)|such that x∼DPPO, y∼πϕ(x)} (3)
where a commonly used criteria F= 1rθ(x,y)≥τincludes only the samples with RM scores exceed
a certain threshold τ. The policy is then updated by teacher forcing on DPPO
n:
LRS=E(x,y)∼DPPOn|y|X
t=1πϕ(yt|y<t, x) (4)
3 A PPROACH
3.1 F ROM REWARD MODEL TO ADVANTAGE MODEL
The learning objective of equation 1 primarily allows models to distinguish between human-
preferred responses and alternative options. It relies only on score differences to assess the like-
lihood of one response being superior to another. In such case, two different model responses that
are both preferred by humans could have dramatically different values. In addition, interpreting the
scalar values themselves can be challenging.
3

--- PAGE 4 ---
In light of these considerations, we introduce the Advantage Model (AM) for reward modeling.
Analogous to the concept of the advantage function in reinforcement learning, the Advantage Model,
denoted as a(x, y), quantifies the additional reward that response ycan achieve over the expected
reward efor prompt x. This is formally defined as:
aθ(x, y) =rθ(x, y)−Ey∼π′(x)[πϕ(y|x)
π′(y|x)rθ(x, y)] (5)
Here, the notation y∼π′(x)signifies all possible responses generated by a policy π′(x)when given
the input prompt x. Since the comparison data is typically collected in many batches with different
SFT or PPO models, we introduceπϕ(y|x)
π′(y|x), the importance weight term to negate the bias introduced
by the policy distribution shift. Intuitively, the extra reward gains of good response ycand the reward
losses of bad response yrshould be bounded by a margin m. As such, the training objective of AM
consists of two parts, ranking loss that aligns with the formulation in Equation 1, and bounding loss
to ensure the well-calibrated bounding of AM scores. It is formally defined as follows:
LAM=−E(x,yc,yr)∼DRM[log(σ(aθ(x, yc)−aθ(x, yr)))
+ log( σ(m(x)−aθ(x, yc))) + log( σ(m(x) +aθ(x, yr)))](6)
where m(x)3is the function that defines the permitted margin for prompt x. However, it is infea-
sible to list every potential response to calculate the expected reward. To address this, we propose
parameterizing the expected reward of the current policy, denoted as:
eτ(x) =Ey∼πϕ(x)[rθ(x, y)] (7)
By integrating the term representing the importance weight, we can reformulate the equation as
follows:
aθ(x, y) =rθ(x, y)−N−K
Neτ(x)−KX
k=11
Nπϕ(y|x)
π′
k(y|x)rθ(x, y) (8)
where Nserves as a hyperparameter that harmonizes the emphasis placed on the current policy
model relative to alternate policy models. Kspecifies the number of alternate policy models utilized
for comparison data collection. Additionally, π′
k(y|x)indicates the probability derived from the kth
policy model.
3.2 PPO WITH SELECTIVE REHEARSAL
In addition, we propose Selective Rehearsal to maintain the skills that are already acquired before
RLHF. Selective rehearsal takes two major steps: representative example discovery and rehearsal
training.
Representative example discovery Given the policy πϕand PPO training prompts with policy
outputs DPPO= [(x1, y1),(x2, y2). . .], our goal is to select high-quality (x, y)pairs from DPPOthat
cover as many skills (e.g., solving algebra problems and writing resume) as possible. In order to let
selected (x, y)pairs represent as many skills as possible, we first adopt a clustering algorithm (e.g.
KMeans or Gaussian mixture) to separate DPPOintocclusters. To assure the representativeness
and quality of the selected data, we only keep certain (x, y)pairs within each cluster that satisfy
certain criteria regarding aspects such as advantage (reward) model score, entropy (low entropy
indicates high confidence), human satisfaction rate or response length (higher length may indicate
redundancy).
Here we adopt the SimCSE (Gao et al., 2021) sentence embedding4to represent the query xfor each
(x, y)pair before running a KMeans algorithm on these embeddings to be grouped into cclusters.
We briefly study the influence of cluster number cin Section 4.3. Within each cluster, here we
simply choose the top- k(x, y)pairs with the highest advantage model score (Eq. 3.1). We leave
other strategies (e.g. combining advantage score with entropy score) in future work.
3We think that m(x)may have a connection with the complexity or difficulty involved in learning the reward
function for prompts similar to x. However, this is speculative and requires further investigation. We leave this
aspect as a topic for future study and exploration. Throughout our experiments, we set m(x)as 2.5.
4https://huggingface.co/princeton-nlp/sup-simcse-roberta-base
4

--- PAGE 5 ---
One reason we select our rehearsal data from the PPO training data with each response ybeing
generated from the initial policy model is to enable a more fair and nuanced comparison, as no
additional information is introduced. In other scenarios, the rehearsal (x, y)pairs could come from
other important data sources representing specific skills (e.g. math-problem solving) the main policy
are not expected to forget.
Rehearsal training After obtaining the rehearsal (x, y)pairs of all clusters, we shuffle them to-
gether to form the rehearsal dataset DRand compute NLL loss on DRas a supplement to the
standard PPO loss defined in Equation 2:
LPPO-SR =LPPO+γE(x,y)∼DR|y|X
t=1πϕ(yt|y<t, x) (9)
where the coefficient for the NLL loss γis empirically set to 0.01.
Rehearsal training is similar with rejection sampling and reinforced self-training (Gulcehre et al.,
2023) by using self-generated ys of high reward model score for supervised training. However, re-
hearsal training captures multi-dimensional important aspects (e.g., diversity), while rejection sam-
pling and reinforced self-training only consider reward model score.
Alternatively, one can view selective rehearsal as a means of amplifying the weight of the KL-
divergence term in PPO training (Eq. 2) for crucial instances and their related counterparts.
4 E XPERIMENTS
4.1 D ATASETS AND MODELS
RM datasets We conducted experiments on both English and Chinese datasets. For the English
experiments, we utilized the HH-RLFH dataset (Bai et al., 2022a; Ganguli et al., 2022), which com-
prises 118k helpful and 42k harmless examples for training, and 8.5k for testing. It is worth noting
that many studies train different RMs separately for helpful and harmless examples to achieve better
performance. However, in our experiments, we did not distinguish between helpful and harmless
examples.
For the Chinese dataset, we collected comparison examples with quantities similar to those used
in LLaMA 2 (Touvron et al., 2023). Our annotation procedure operates as follows: First, we ask
annotators to generate prompts based on a task spectrum. Next, we sample five responses from the
same SFT model using varied sampling hyper-parameters. Finally, we distribute these responses to
five annotators for ranking based on provided criteria. Following Bai et al. (2022a), the annotation
criteria focuses on helpfulness and harmless.
PPO dataset We sampled queries from two popular domain-general datasts, COIG5and firefly6
to form our PPO dataset. Particularly, we obtained 64,364 and 2,623 for PPO training and testing,
respectively7. There is no intersection between the training and testing sets. Additionally, we se-
lected 1,704 examples from the SFT test data to create a forget test set , enabling us to evaluate the
model’s ability to retain learned knowledge.
Models We employed BLOOMZ (Muennighoff et al., 2022) as our pre-trained model backbone.
More specifically, BLOOMZ 7Bwas used for reward modeling and BLOOMZ 176Bwas used for SFT
and RLHF training.
4.2 T RAINING SETUPS
We initialized our models using pre-trained checkpoints. The architectural configuration and hyper-
parameters were kept consistent with those of the pre-trained models, except that a value head is
5https://huggingface.co/datasets/BAAI/COIG
6https://huggingface.co/datasets/YeungNLP/firefly-train-1.1M
7The PPO training and testing query sets could be shared upon request.
5

--- PAGE 6 ---
ModelHH-RLHF Proprietary Data
Accuracy ↑ECE↓Accuracy ↑ECE↓
OpenAssistant K ¨opf et al. (2023) 69.24 - - -
Reward Model 69.25 4.70 74.75 5.35
Advantage Model 69.43 3.48 75.28 3.83
Table 1: Evaluation results on HH-RLHF and our proprietary data. Note that maximizing accuracy
is not the exclusive objective in AM optimization. The aim also extends to reducing ECE to improve
reliability, whilst sustaining or improving the level of ranking accuracy compared with RM.
added to produce a scalar reward. A learning rate of 5e-6 was employed, coupled with a warm-up
strategy covering the initial 10% of training steps and a cosine learning rate schedule decreasing to
10% of the initial learning rate. For the English dataset, a global batch size of 180 was employed,
whereas for the Chinese dataset, the batch size was set to 480. The Overfitting issue is observed in
general after models are trained for one epoch. As such, we fixed the training epoch as 1 for the
all the experiments.For PPO training, a learning rate of 5×10−7and a global batch size of 256 is
employed. The actor model is trained for 100 steps for all experiments. The SFT model is trained
on the proprietary dataset. We omit these details since these are not the focus of this paper.
4.3 E VALUATION
AM Evaluation Results Firstly, we present the overall accuracy and Expected Calibration Error
(ECE) for both RM and AM on each dataset. For the English dataset, we additionally compare our
method with the publicly available OpenAssistant (K ¨opf et al., 2023) which utilized DeBERTa (He
et al., 2020) for reward modeling. Table 2 lists all the results. We observe that AM achieves slightly
higher accuracy but significantly lower ECE on all the datasets. This indicates that AM is capa-
ble of maintaining the same level of ranking accuracy while providing reliable and well-calibrated
scores. A detailed analysis of calibrations is provided in the following sections. We attribute this
phenomenon to the fact that AM is formulated to directly model additional rewards, i.e.,advan-
tages, making it more stable and less prone to yield high variances cores. Additionally, the accuracy
on the proprietary data is much higher than that on HH-RLHF. We speculate that the trade-off be-
tween helpfulness and harmlessness objectives is more pronounced in HH-RLHF, possibly due to
the limited presence of harmful examples in our proprietary data.
Figure 2: Ranking accuracy is shown as a function of the difference in scores between higher and
lower ranked responses. The orange lines indicate the calibrated prediction of accuracy 1/(1+e−∆)
in which ∆denotes the score difference. On the left, we show calibration of RM and AM on
HH-RLHF data while on the right we show results for our proprietary data. We observe that AM
calibration is better than RM’s.
Calibrations of AM The reward model score of a response should accurately reflect the probabil-
ity that humans prefer it. These probabilities must be precise; in other words, the scores should be
6

--- PAGE 7 ---
(a) RM score distribution.
 (b) AM score distribution.
Figure 3: Distributions of RM and AM scores for pairs of good and bad examples from the propri-
etary data.
well-calibrated. This is crucial since these scores will serve as reward signals to guide PPO train-
ing Bai et al. (2022a). To assess whether our AM is calibrated or not, in Figure 2, we depict the
ranking accuracy as a function of score differences assigned to pairs of samples. An orange line
representing perfect calibration is also included. Our observations indicate that the AM exhibits
significantly lower ECE and is better calibrated than RM on both datasets, whereas RM tends to
be overconfident in most cases. We further show the distribution of scores for both good and bad
examples in Figure 3. While in general both RM and AM are able to assign higher scores for good
examples, AM exhibits a more distinct distribution pattern.
(a) Mean scores of RM and AM for each task.
 (b) Std of RM and AM for each task.
Figure 4: Mean and standard variance for each task categorized by a task spectrum on the in-house
data.
Means and variances of AM During PPO training, RLHF exhibits instability, largely owing to
unpredictable fluctuations in reward estimation scales. Directly modeling advantage, as our AM
does, could potentially alleviate the above issue. To validate AM’s efficacy in stabilizing score
scales and ranges, we calculated the AM scores for individual examples and analyzed the mean
and variance across all the the task spectrum. This analysis is depicted in Figure 4a. We observe
markedly different means for each task in the case of RM. Such significant disparities in means can
potentially give rise to reward hacking issues (Skalse et al., 2022) and result in repeated failures
during PPO training. In addition, Figure 4b illustrates the standard deviations of both AM and RM,
with AM consistently operating at a stable scale. These results endorse AM as a strategy designed
to normalize reward scores at the individual example level while enhancing ranking accuracy.
PPO training results We conducted a comparative analysis of PPO training with different scoring
models in terms of their performance on both main test set and forget test set. The learning curve
7

--- PAGE 8 ---
(a) Learning curves of various models on delta re-
wards
(b) Win/Loss rate over SFT model evaluated by
GPT-4.
Figure 5: PPO training curves on the Main Test Set with different scoring models. RM-PPO and
AM-PPO denote PPO trained with Reward Model and Advantage Model, respectively. AM-PPO-
SER additionally equips with Selective Rehearsal.
ModelMain Test Set Forget Test Set
Win↑Lose↓Tie Win↑Lose↓Tie
RM-PPO 12.72 12.62 74.66 16.87 29.28 53.84
AM-PPO 14.87 10.38 74.74 9.70 8.44 81.86
AM-PPO-SR 15.78 9.77 74.45 10.30 7.95 81.75
Table 2: Comparison results of different models over the SFT model.
is shown in 5. We observe that AM-PPO outperformed RM-PPO in the main set, achieving higher
rewards and a superior win rate over the SFT model. In addition, RM-PPO faces significant reward
hacking issues, witnessed by a drop in win rate evaluated by GPT-4, shown in 5b despite a rise in
RM scores. Despite utilizing moving average for score normalization, RM-PPO w/ MA encoun-
ters instabilities during PPO training. Conversely, AM-PPO exhibits resistance to such problems,
maintaining stable GPT-4 outcomes. This emphasizes AM’s stability and alignment efficiency over
RM. The forget test set result reveal RM-PPO’s substantial susceptibility to catastrophic forgetting,
portraying a noticeable performance drop. In contrast, AM-PPO is stable, avoiding significant drops
and showcasing stability. Incorporating selective rehearsal, the AM-PPO-SR variant demonstrate an
uplifted win rate on both sets, underscoring the role of selective rehearsal in alleviating catastrophic
forgetting and enhancing model efficacy.
Figure 6: The AM-PPO-SR training curves on the
Main Test Set with different number of clustering
groups cfor selective rehearsal.Analysis on Selective Rehearsal We
also conduct an in-depth examination
of the impact of the number of clusters,
denoted as c, in the context of selective
rehearsal during PPO training. As illus-
trated in Figure 6, our results reveal a
relatively consistent variance of approx-
imately 0.05 points in test-set rewards
across various cluster numbers c. While
our findings highlight the robustness
of the selective rehearsal technique,
we recommend conducting a thorough
analysis of this aspect when applying
selective rehearsal to different datasets,
as domain-specific variations can have a
notable impact.
8

--- PAGE 9 ---
5 R ELATED WORK
LLM Alignments with Human Preferences. LLMs are typically pre-trained on extensive
datasets and can be adapted to a wide variety of downstream tasks. One critical aspect of utiliz-
ing LLMs effectively is ensuring their alignment with human preferences, which helps in averting
responses that are unsafe, toxic, sexually explicit, biased, or criminal (Leike et al., 2018). A pre-
dominant strategy in achieving this is RLHF. This involves training a reward model based on human
feedback and utilizing PPO to improve to fine-tuning LLMs (Christiano et al., 2017; Bai et al.,
2022a; Glaese et al., 2022; Bai et al., 2022b; Stiennon et al., 2020; Qiu et al., 2022).
Instabilities in RLHF. Despite its success, the RLHF approach is inherently complex and poses
significant challenges, thereby encouraging the exploration of simpler methods to align LLMs with
human preferences. In this context, Cobbe et al. (2021) introduced the best-of-n sampling, which re-
inforces LLMs by choosing the responses with the highest reward score from a set of n responses. A
similar pathway was pursued by RAFT (Dong et al., 2023), which focuses on selecting high-quality
samples to fine-tuning to enhance the model’s performance. Moreover, the RRHF strategy (Yuan
et al., 2023) evaluates sampled responses from various sources using the logarithm of conditional
probabilities. It then aligns these probabilities with human preferences by applying ranking loss, fos-
tering a more refined alignment process. Furthermore, Rafailov et al. (2023) introduced the concept
of Direct Preference Optimization (DPO). This approach leverages a relationship between reward
functions and optimal policies to address a constrained reward maximization problem through a sin-
gle stage of policy training. In a similar vein, Preference Ranking Optimization (PRO) (Song et al.,
2023) sidesteps the necessity for Reinforcement Learning (RL) training. Instead, it directly aligns
LLMs with human preferences using the Bradley-Terry comparison — a method that involves the
probability ranking of n responses generated by the LLM, ensuring they are consistent with human
preference rankings.
Data Curation for LLM Alignments. Many approaches have been devised to curate high-quality,
instruction-following datasets to fine-tune LLMs (Wang et al., 2022; 2023; Taori et al., 2023; Chiang
et al., 2023; Peng et al., 2023). For instance, the study by LIMA (Zhou et al., 2023) underscores
that even a limited set of carefully curated and high-quality examples can be utilized to fine-tune a
strong pre-trained language model, enabling it to deliver competitive results across a diverse array
of prompts. Similarly, Wei et al. (2023) introduced a versatile and straightforward data selector
designed to autonomously curate a subset from the original fine-tuning dataset, adhering to specific
principles for training vision-language models. While these strategies converge on the shared ob-
jective of data curation for LLM fine-tuning, our approach is uniquely centered on data curation
for PPO training. This strategy diverges fundamentally from others that emphasize the SFT stage,
thereby addressing a distinct problem.
6 C ONCLUSION
In this report, we identified and analyzied critical impediments in RLHF training of LLMs, namely
reward hacking and catastrophic forgetting. These issues emerge due to the variances in learned
reward score distributions and the over-optimization of specific training examples, resulting in in-
stabilities in RLHF training. To alleviate these issues, we introduced the Advantage Model and
Selective Rehearsal —innovative strategies formulated to stabilize the RLHF training process. The
Advantage Model aims to maintain balanced reward score distributions across diverse categories
and examples, thereby averting complications arising from reward hacking. On the other hand,
Selective Rehearsal selectively identifies optimal examples for PPO training, ptimal examples for
PPO training, encouraging the retention of crucial knowledge from the SFT stage, and preventing
the depreciation of performance over time. Empirical analyses conducted on a range of datasets
substantiated the efficacy of our proposed techniques, which not only enhanced stability in RLHF
training but also led to improved reward scores and win rates the SFT models.
REFERENCES
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,
Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory
9

--- PAGE 10 ---
for alignment. arXiv preprint arXiv:2112.00861 , 2021.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn
Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless
assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862 ,
2022a.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,
Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harm-
lessness from ai feedback. arXiv preprint arXiv:2212.08073 , 2022b.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems , 33:1877–1901, 2020.
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,
Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An
open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:
//lmsys.org/blog/2023-03-30-vicuna/ .
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. Advances in neural information processing sys-
tems, 30, 2017.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.
Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum,
and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment.
arXiv preprint arXiv:2304.06767 , 2023.
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben
Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, et al. Red teaming language models to
reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 ,
2022.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence
embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing , pp. 6894–6910, 2021.
Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Mari-
beth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of
dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375 , 2022.
Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek
Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training
(rest) for language modeling. arXiv preprint arXiv:2308.08998 , 2023.
Kshitij Gupta, Benjamin Th ´erien, Adam Ibrahim, Mats L Richter, Quentin Anthony, Eugene
Belilovsky, Irina Rish, and Timoth ´ee Lesort. Continual pre-training of large language models:
How to (re) warm your model? arXiv preprint arXiv:2308.04014 , 2023.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert
with disentangled attention. arXiv preprint arXiv:2006.03654 , 2020.
Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. Towards continual reinforce-
ment learning: A review and perspectives. Journal of Artificial Intelligence Research , 75:1401–
1476, 2022.
Andreas K ¨opf, Yannic Kilcher, Dimitri von R ¨utte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich ´ard Nagyfi, et al. Openassistant
conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327 ,
2023.
10

--- PAGE 11 ---
Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable
agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871 ,
2018.
Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology of learning and motivation , volume 24, pp. 109–165.
Elsevier, 1989.
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le
Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual gen-
eralization through multitask finetuning. arXiv preprint arXiv:2211.01786 , 2022.
R OpenAI. Gpt-4 technical report. arXiv , pp. 2303–08774, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. Advances in Neural Information Processing Systems , 35:
27730–27744, 2022.
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning
with gpt-4. arXiv preprint arXiv:2304.03277 , 2023.
Liang Qiu, Yizhou Zhao, Jinchao Li, Pan Lu, Baolin Peng, Jianfeng Gao, and Song-Chun Zhu.
Valuenet: A new dataset for human value driven dialogue system. In Proceedings of the AAAI
Conference on Artificial Intelligence , volume 36, pp. 11183–11191, 2022.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea
Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv
preprint arXiv:2305.18290 , 2023.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.
Joar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and character-
izing reward gaming. Advances in Neural Information Processing Systems , 35:9460–9471, 2022.
Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang.
Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492 , 2023.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea V oss, Alec Radford,
Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances
in Neural Information Processing Systems , 33:3008–3021, 2020.
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.
https://github.com/tatsu-lab/stanford_alpaca , 2023.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288 , 2023.
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and
Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.
arXiv preprint arXiv:2212.10560 , 2022.
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi
Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels
go? exploring the state of instruction tuning on open resources. arXiv preprint arXiv:2306.04751 ,
2023.
Lai Wei, Zihao Jiang, Weiran Huang, and Lichao Sun. Instructiongpt-4: A 200-instruction paradigm
for fine-tuning minigpt-4. arXiv preprint arXiv:2308.12067 , 2023.
11

--- PAGE 12 ---
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf:
Rank responses to align language models with human feedback without tears. arXiv preprint
arXiv:2304.05302 , 2023.
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,
Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206 , 2023.
12
