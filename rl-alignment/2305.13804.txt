# 2305.13804.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2305.13804.pdf
# File size: 1474861 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
OER: Offline Experience Replay for Continual Offline
Reinforcement Learning
Sibo Gaiab;*, Donglin Wangb;**and Li Heb
aFudan University, Shanghai, China
bWestlake University, Zhejiang, China
Abstract. The capability of continuously learning new skills via
a sequence of pre-collected offline datasets is desired for an agent.
However, consecutively learning a sequence of offline tasks likely
leads to the catastrophic forgetting issue under resource-limited sce-
narios. In this paper, we formulate a new setting, continual offline
reinforcement learning (CORL), where an agent learns a sequence of
offline reinforcement learning tasks and pursues good performance
on all learned tasks with a small replay buffer without exploring any
of the environments of all the sequential tasks. For consistently learn-
ing on all sequential tasks, an agent requires acquiring new knowl-
edge and meanwhile preserving old knowledge in an offline manner.
To this end, we introduced continual learning algorithms and exper-
imentally found experience replay (ER) to be the most suitable al-
gorithm for the CORL problem. However, we observe that introduc-
ing ER into CORL encounters a new distribution shift problem: the
mismatch between the experiences in the replay buffer and trajecto-
ries from the learned policy. To address such an issue, we propose
a new model-based experience selection (MBES) scheme to build
the replay buffer, where a transition model is learned to approximate
the state distribution. This model is used to bridge the distribution
bias between the replay buffer and the learned model by filtering
the data from offline data that most closely resembles the learned
model for storage. Moreover, in order to enhance the ability on learn-
ing new tasks, we retrofit the experience replay method with a new
dual behavior cloning (DBC) architecture to avoid the disturbance
of behavior-cloning loss on the Q-learning process. In general, we
call our algorithm offline experience replay (OER). Extensive exper-
iments demonstrate that our OER method outperforms SOTA base-
lines in widely-used Mujoco environments.
1 Introduction
Similar to human beings, a general-purpose intelligence agent is ex-
pected to learn new tasks continually. Such sequential tasks can be ei-
ther online tasks learned through exploration or offline tasks learned
through offline datasets, where the latter is equally important but has
not drawn sufficient attention so far. Learning sequential tasks in of-
fline setting can greatly improve the learning efficiency and avoid the
dangerous exploration process in practice. Moreover, online learning
sequential tasks is not always feasible due to the temporal and spatial
constraints of the environment and the agentâ€™s on-board considera-
tion itself. Therefore, studying offline reinforcement learning in the
âˆ—Email: gaisibo@westlake.edu.cn
âˆ—âˆ—Corresponding Author. Email: wangdonglin@westlake.edu.cncontinual setting is quite important and valuable for general-purpose
intelligence. If having sufficient computational resources, it is easy
to accomplish such a goal. However, for an agent with limited re-
sources1, continual learning methods are indispensable to deal with
such offline datasets. Consequently, we propose a new setting named
continual offline reinforcement learning (CORL) in this paper, which
integrates offline RL and continual learning.
Offline RL learns from pre-collected datasets instead of directly
interacting with the environment [11]. By leveraging pre-collected
datasets, offline RL methods avoid costly interactions and thus en-
hance learning efficiency and safety. However, offline RL methods
suffer from the over-estimation problem of the out-of-distribution
(OOD) data, where the unseen data are erroneously estimated to be
high values. This phenomenon stems from the distribution shift be-
tween the behavior policy and the learning policy. Various methods
have been proposed to address the over-estimation problem [11, 21].
In this paper, we focus on dealing with a sequence of offline datasets,
which requires continual learning techniques.
The major challenge of continual learning is how to alleviate the
catastrophic forgetting [31] issue on previous tasks when learning
new tasks. There are three types of continual learning methods,
including regularization-based methods [19, 53], modular methods
[8, 30], and rehearsal-based methods [27, 4]. Experience replay (ER)
is a widely-used rehearsal-based method [42], which alternates be-
tween learning a new task and replaying samples of previous tasks.
In this paper, we consider using ER as a base for our own problem.
We will show that ER is the most suitable algorithm for the CORL
problem in the experiment section following.
However, since ER is designed for online continual RL, directly
applying ER in our CORL yields poor performance due to two kinds
of distribution shifts. The first is the distribution shift between the
behavior policy and the learning policy, and the second is the distri-
bution shift between the selected replay buffer and the correspond-
ing learned policy. Existing methods focus on addressing the first
shift issue, and no related work considers the second, which only
appears in our CORL setting. Thus, simply integrating ER with Of-
fline RL fails to alleviate the catastrophic forgetting. To solve the
new problem above, we propose a novel model-based experience se-
lection (MBES) method to fill the replay buffer. The key idea is to
take advantage of the dynamic model to search for and add the most
valuable episodes in the offline dataset into the replay buffer.
After having a good replay buffer for previous tasks, the learned
1Running on a server and communicating with the agent are undesirable due
to complex-system, real-time, and data privacy issues.arXiv:2305.13804v2  [cs.LG]  20 Apr 2024

--- PAGE 2 ---
policy corresponding to a new task needs to clone previous tasks.
Behavior cloning (BC) as an online ER method is widely used [54],
which is incompatible with the actor-critic architecture in offline RL.
Even though we can carefully tune the weight of BC loss, tuning the
hyper-parameter is a cumbersome task in general and is difficult in
the offline setting. Therefore, integrating online ER with offline RL
often derives a non-convergent policy. The reason is that the actor-
critic model is hard to train [39], and the rehearsal term in the loss
function has a negative effect on the learning process. To effectively
replay experiences, we propose a dual behavior cloning (DBC) archi-
tecture instead to resolve the optimization conflict, where one policy
optimizes the performance of the new task by using actor-critic ar-
chitecture, and the second optimizes from the continual perspective
for both new and learned tasks.
In summary, this paper considers investigating a new setting
CORL. A novel MBES is proposed to select valuable experiences
and overcome the mismatch between the experiences in the replay
buffer and trajectories from the learned policy. Then, a DBC archi-
tecture is proposed to deal with the optimization conflict problem.
By taking MBES and DBC as two key ideas for CORL setting, we
name our overall scheme as offline experience replay (OER). The
main contributions of this paper can be summarized as follows:
â€¢We present a new setting CORL and then propose a novel scheme
OER for CORL setting.
â€¢We propose a novel selection method MBES for offline replay
buffer by utilizing a dynamic model to reduce the distribution shift
between experiences from replay buffer and learned policy.
â€¢On the other hand, we propose a novel DBC architecture to pre-
vent the learning process from collapsing by separating the Q-
learning on current task and BC processes on all previous tasks.
â€¢We experimentally verify the performance of different modules
and evaluate our method on continuous control tasks. Our method
OER outperforms all SOTA baselines for all cases.
2 Related Works
Offline Reinforcement Learning Offline RL learns from a col-
lected offline dataset and suffers from the issue of out-of-distribution
(OOD). Some prior works propose to constrain the learned policy to-
wards the behavior policy by adding KL-divergence[38, 36, 45, 55],
MSE [6], or the regularization of the action selection [21]. Some arti-
cles suggest that if the collected data is sub-optimal, these approaches
usually perform not well [29]. But other works point out that adding
a supervised learning term to the policy improvement objective [10]
will also receive high performance by reducing the exploration. An-
other effective way is to learn a conservative Q-function [22, 28, 20],
which assigns a low Q-value for OOD states and then extracts the
corresponding greedy policy. Moreover, other works propose to use
an ensemble model to estimate Q-value [1] or consider the impor-
tance sampling [38]. Such methods have not previously considered
the setting of sequential tasks and naive translating them into CORL
setting is ineffective, while this paper focuses on sequential tasks and
aims to solve the catastrophic forgetting problem during the process
of learning a sequence of offline RL tasks.
On the other hand, recent works [5, 24] propose to train a dynamic
model to predict the values of OOD samples in a supervised-learning
way. Such model-based offline RL methods offer great potential for
solving the OOD problem, even though the transition model is hardly
accurate strictly. The model algorithm is thought to alleviate the
OOD problem faced by offline RL and thus improve the robustnessof the offline agent. Model-based offline RL methods have two major
categories: one focuses on measuring the uncertainty of the learned
dynamic model [52, 17], and the other considers the pessimistic es-
timation [51]. Different from most of these works using the dynamic
model to generate OOD samples when training the agent, in this pa-
per, we utilize the dynamic model to search for the most valuable
episodes in the offline dataset for the ER method.
Continual Reinforcement Learning Offline methods may con-
sider a single-task or multi-task scenario [50, 25, 26]. In contrast,
continual learning attempts to learn new tasks after having learned
old tasks and get the best possible results on all tasks. Generally, con-
tinual learning methods can be classified into three categories [37]:
regularization-based approaches [19, 53] add a regularization term
to prevent the parameters from far from the value learned from past
tasks; modular approaches [8, 30] consider fixed partial parameters
for a dedicated task; and rehearsal-based methods [27, 4] train an
agent by merging the data of previously learned tasks with that of the
current task. All three kinds of continual learning methods have been
applied for RL tasks [15, 48, 32, 23]. Specifically, our work is based
on the rehearsal method in an RL setting [42, 14]. Therefore, we will
detail the works involved in this category following.
There are two essential questions to answer in rehearsal-based
continual learning. The first is how to choose samples from the whole
dataset to store in the replay buffer with a limited size [49]. The most
representative samples [41, 43] or samples easy to forget [3] are usu-
ally selected in the replay buffer while random selection has also
been used in some works [40, 2]. However, these algorithms are de-
signed for image classification and are not applicable to RL. [14]
focuses on replay buffer question sampling in online RL setting. The
second is how to take advantage of the saved replay samples [49, 3].
In RL, the two most commonly used approaches are BC and per-
fect memory [46] in continual RL, where BC is more effective in
relieving catastrophic forgetting. At present, all these methods are
designed for online RL setting. Unlike prior works, we consider the
offline RL setting in this paper, where catastrophic forgetting and
overestimation must be overcome simultaneously.
To the best of our knowledge, this is the first work to solve offline
RL problems in the continual learning setting.
3 Problem Formulation and Preliminary
Continual Offline Reinforcement Learning In this paper, we
investigate CORL, which learns a sequence of RL tasks T=
(ğ‘‡1,Â·Â·Â·,ğ‘‡ğ‘). Each taskğ‘‡ğ‘›is described as a Markov Decision Pro-
cess (MDP) represented by a tuple of {S,A,ğ‘ƒğ‘›,ğœŒ0,ğ‘›,ğ‘Ÿğ‘›,ğ›¾}, where
Sis the state space,Ais the action space, ğ‘ƒğ‘›:SÃ—AÃ—Sâ†’[0,1]
is the transition probability, ğœŒ0,ğ‘›:Sis the distribution of the ini-
tial state,ğ‘Ÿğ‘›:SÃ—A â†’[âˆ’ğ‘…max,ğ‘…max]is the reward function,
andğ›¾âˆˆ[0,1)is the discounting factor. We assume that sequential
tasks have different ğ‘ƒğ‘›,ğœŒ0,ğ‘›andğ‘Ÿğ‘›, but share the same S,A, andğ›¾
for simplicity. The return is defined as the sum of discounted future
rewardğ‘…ğ‘¡,ğ‘›=Ãğ»
ğ‘–=ğ‘¡ğ›¾(ğ‘–âˆ’ğ‘¡)ğ‘Ÿğ‘›(ğ‘ ğ‘–,ğ‘ğ‘–), whereğ»is the horizon.
We define a parametric Q-function ğ‘„(ğ‘ ,ğ‘)and a paramet-
ric policyğœ‹(ğ‘|ğ‘ ). Q-learning methods train a Q-function by it-
eratively applying the Bellman operator Bâˆ—ğ‘„(ğ‘ ,ğ‘)=ğ‘Ÿ(ğ‘ ,ğ‘)+
ğ›¾Eğ‘ â€²âˆ¼ğ‘ƒ(ğ‘ â€²|ğ‘ ,ğ‘)(maxğ‘â€²ğ‘„(ğ‘ â€²,ğ‘â€²)). We also train a transition model
for each task Ë†ğ‘ƒğ‘›(ğ‘ â€²|ğ‘ ,ğ‘)by using maximum likelihood estimation
min Ë†ğ‘ƒğ‘›E(ğ‘ ,ğ‘,ğ‘ â€²)âˆ¼D
logË†ğ‘ƒ(ğ‘ â€²|ğ‘ ,ğ‘)
. We use a multi-head architec-
ture for the policy network ğœ‹to avoid the same-state-different-task
2

--- PAGE 3 ---
problem [16]. In detail, the policy network consists of a feature ex-
tractorğœƒğ‘§for all tasks and multiple heads ğœƒğ‘›,ğ‘›âˆˆ[1,ğ‘], where one
head is for each task. ğœ‹ğ‘›is defined to represent the network with
joint parameters[ğœƒğ‘§,ğœƒğ‘›]andâ„ğ‘›is defined to represent the head
with parameters ğœƒğ‘›. Our aim is to train sequential tasks all over
[ğ‘‡1,Â·Â·Â·,ğ‘‡ğ‘âˆ’1]sequentially and get a high mean performance and
a low forgetting of all the learned tasks without access to data from
previous tasks except a small buffer.
In online RL setting, the experiences ğ‘’=(ğ‘ ,ğ‘,ğ‘ â€²,ğ‘Ÿ)can be ob-
tained through environment interaction. However, in offline RL set-
ting, the policy ğœ‹ğ‘›(ğ‘|ğ‘ )can only be learned from a static dataset
Dğ‘›=
ğ‘’ğ‘–ğ‘›	
,ğ‘’ğ‘–ğ‘›= ğ‘ ğ‘–ğ‘›,ğ‘ğ‘–ğ‘›,ğ‘ â€²ğ‘–ğ‘›,ğ‘Ÿğ‘–ğ‘›, which is assumed to be collected
by an unknown behavior policy ğœ‹ğ›½
ğ‘›(ğ‘|ğ‘ ).
Experience Replay ER [42] is the most widely-used rehearsal-
based continual learning method. In terms of task ğ‘‡ğ‘›, the objective of
ER is to retain good performance on previous tasks [ğ‘‡1,Â·Â·Â·,ğ‘‡ğ‘›âˆ’1],
by using the corresponding replay buffers [ğµ1,Â·Â·Â·,ğµğ‘›âˆ’1], which
called perform memory. Moreover, two additional behavior cloning
losses, including the actor cloning loss and the critic cloning loss, are
commonly used for previous tasks as follows
ğ¿actor_cloning :=âˆ‘ï¸
ğ‘ ,ğ‘âˆˆğµâˆ¥ğœ‹ğ‘›(ğ‘ )âˆ’ğ‘âˆ¥2
2, (1)
ğ¿critic_cloning :=âˆ‘ï¸
ğ‘ ,ğ‘,ğ‘„ replayâˆˆğµğ‘„ğ‘›(ğ‘,ğ‘ )âˆ’ğ‘„replay2
2,(2)
whereğµis the replay buffer and ğ‘„replay means the Q value saved
from previous tasks. These two losses are called BC [46].
Subsequent work [46] shows that for a soft actor-critic [13] archi-
tecture, the replay loss added on the actor network (Eq. 1) performs
well, but the loss added on the critic network (Eq. 2) poorly effect.
Therefore, we only consider the actor cloning loss (Eq. 1) in our
work.
However, naively integrating the ER with offline RL results in
a significant performance drop for CORL problems. To tackle this
problem, we propose a novel method OER, which consists of two
essential components as follows.
4 Offline Experience Replay (OER)
In this section, we first elaborate on how to select valuable experi-
ences and build the replay buffer. Then, we describe a novel DBC
architecture as our replay model. Finally, we summarize our algo-
rithm and provide the Pseudocode.
4.1 Offline Experience Selection Scheme
Novel Distribution Shift Problem: Facing with sequential tasks, an
agent learns the task ğ‘‡ğ‘›in order after learning ğ‘‡ğ‘›âˆ’1, and should
prepare for the next learning. In terms of task ğ‘‡ğ‘›, how to se-
lect partial valuable data from offline dataset Dğ‘›to build a size-
limited replay buffer ğµğ‘›is critical. In online RL setting, rehearsal-
based methods commonly utilize a ranking function R(ğ‘ ğ‘–,ğ‘ğ‘–)=ğ‘Ÿğ‘–+ğ›¾max
ğ‘â€²ğ‘„
ğ‘ â€²
ğ‘–,ğ‘â€²
âˆ’ğ‘„(ğ‘ ğ‘–,ğ‘ğ‘–)to evaluate the value of trajecto-
ries. Here,R(ğ‘ ğ‘–,ğ‘ğ‘–)with(ğ‘ ğ‘–,ğ‘ğ‘–)âˆˆ D evaluates replay trajecto-
ries in terms of ğ‘„-value accuracy or total accumulative reward [14],
where the trajectories are collected by the optimal policy ğœ‹âˆ—ğ‘›inter-
acting with the environment. On the contrary, in offline RL setting,
if we consider the similar method above, the data selection can onlybe made in the offline dataset Dğ‘›corresponding to the behavior pol-
icyğœ‹ğ›½
ğ‘›. Therefore, there exists a distribution shift between ğœ‹ğ›½
ğ‘›and
ğœ‹âˆ—ğ‘›, which inevitably affects the performance of the rehearsal. In our
selection scheme, we attempt to identify and filter out those offline
trajectories inDğ‘›that are not likely to be generated by ğœ‹âˆ—ğ‘›. To clar-
ify this point, we give an illustration in Fig.1, where those trajectories
close to the offline optimal trajectory are selected and stored in the
replay buffer ğµğ‘›. The distribution of these trajectories in terms of
bothSandAdiffers from the offline dataset Dğ‘›.
Ï„*Ï„2Ï„1
(a) offline learned trajectory
Ï„* (b) Offline buffer selection
Figure 1 : Distribution shift between the experiences from replay
buffer and trajectories from learned policy. (a) ğœ1andğœ2represent
two trajectories in offline dataset, and ğœâˆ—represents the trajectory
generated by the optimal policy. The learned policy can generate
better trajectories than original dataset. (b) The arrow represents the
experiences in offline dataset near the optimal trajectory. Episodes
close to the optimal trajectory are more valuably selected.
0s
0a
0sï‚¢
1s
1a
1sï‚¢
Figure 2 : Illustration of our MBES method to fill in ğµğ‘›forğ‘‡ğ‘›. Blue
lines represent the experiences in ğ·ğ‘›, and black lines represent the
generated experiences by optimal policy ğœ‹âˆ—ğ‘›. Starting from ğ‘ 0and
ğ‘0=ğœ‹âˆ—ğ‘›(ğ‘ 0), the next state is ğ‘ â€²
0=Ë†ğ‘ƒ(ğ‘ 0,ğ‘0). To avoid the accumu-
lated error, we use the most similar state ğ‘ 1in the offline dataset Dğ‘›
instead of the ğ‘ â€²
0as the next state.
Model-based Experience Selection (MBES): In order to address
the new distribution shift question above, we propose a novel MBES
scheme. As shown in Fig.2, based on the offline trajectories in Dğ‘›
collected from ğœ‹ğ›½
ğ‘›, MBES aims to generate a new trajectory corre-
sponding toğœ‹âˆ—ğ‘›. Specifically, MBES considers both the learned opti-
mal policy and a task-specific dynamic model Ë†ğ‘ƒğ‘›, where the dynamic
model Ë†ğ‘ƒğ‘›is obtained via supervised learning by using Dğ‘›. Starting
from theğ‘¡th stateğ‘ ğ‘¡, we recursively sample an action by ğœ‹âˆ—ğ‘›and pre-
dict the next state ğ‘ â€²
ğ‘¡=Ë†ğ‘ƒğ‘› ğ‘ ğ‘¡,ğœ‹âˆ—ğ‘›(ğ‘ ğ‘¡).
However, recursively sampling actions on predicted states causes
a significant accumulation of compounding error after several steps,
which hinders the selected trajectories from being revisited later. In
order to remove the compounding error, after learning task ğ‘›, starting
from theğ‘¡th stateğ‘ ğ‘¡, instead of directly taking the model output as
the next state, a state in Dğ‘›most similar to the model prediction ğ‘ â€²
ğ‘¡is
selected as the next state ğ‘ ğ‘¡+1for the pseudo exploitation. Here, we
use theğ¿2metric to measure the similarity as follows
ğ‘ ğ‘¡+1=argmin
ğ‘ âˆˆDğ‘›dist ğ‘ ,ğ‘ â€²
ğ‘¡
ğ‘ â€²
ğ‘¡âˆ¼Ë†ğ‘ƒğ‘›(ğ‘ ğ‘¡,ğœ‹âˆ—ğ‘›(ğ‘ ğ‘¡)), (3)
where dist means a distance between ğ‘ andğ‘ â€²
ğ‘¡.2According to the
2Here we choose the L2 distance of the feature from the last hidden layer
3

--- PAGE 4 ---
zï°
2ï°
nï°
2B
1B
nD
nQ
1ï°
1h
2h
nh(a) offline learned ğ‘‡âˆ—
nï­
nQ
zï°
2ï°
nï°
2B
1B
1h
2h
nh
nD
1ï°(b) Offline buffer selection
Figure 3 : Network architecture of BC and our DBC for experience play, where the policy ğœ‹ğ‘›=[ğœ‹ğ‘§,â„ğ‘›]is for taskğ‘›,ğ‘›=1,Â·Â·Â·,ğ‘. (a) Existing
multi-head architecture. Here, the newly-added head â„ğ‘›forğ‘‡ğ‘›is learned via an actor-critic algorithm with a Q-network ğ‘„ğ‘›, but other heads
fromâ„1toâ„ğ‘›âˆ’1corresponding to previous tasks are learned by cloning ğµ1toğµğ‘›âˆ’1. (b) Our DBC architecture. We use an independent policy
networkğœ‡ğ‘›to learn the current task ğ‘‡ğ‘›and a newly-added head â„ğ‘›is used to clone ğœ‡ğ‘›.
model-based offline RL methods [5, 24], we further introduce the
variance of Ë†ğ‘ƒğ‘› ğ‘ ğ‘¡,ğœ‹âˆ—ğ‘›(ğ‘ ğ‘¡)to determine whether the results of the
dynamic model are reliable or not, where we use Eq.3 for selection
only if the variance of the Ë†ğ‘ƒğ‘›is lower than the threshold; other-
wise, keeping ğœ‹ğ›½
ğ‘›(ğ‘ ğ‘¡)instead. In our experiments, we specifically
use2Ë†ğ‘ƒğ‘›(ğ‘ ğ‘¡,ğ‘ğ‘¡)as the threshold, which has minimal impact.
For a start-up, we sample ğ‘ 0fromğœŒ0,ğ‘›, and then iteratively carry
out. In the end, we save the generated trajectory in ğµğ‘›.
4.2 Dual Behavior Cloning (DBC)
Prior rehearsal-based approaches utilize a multi-head policy network
ğœ‹and a Q-network ğ‘„ğ‘›to learn task ğ‘‡ğ‘›, as shown in Fig.3 (a). During
learning, the policy network ğœ‹is to clone the experience data stored
in replay buffers ğµ1toğµğ‘›âˆ’1for all previous tasks ğ‘‡1toğ‘‡ğ‘›âˆ’1. How-
ever, such an architecture suffers from an obvious performance drop
when the number of tasks increases, which indicates that the pol-
icyğœ‹trained via BC has the incapability of mastering both previous
knowledge from old tasks and new knowledge from the new task.
This performance drop is due to the existing inconsistency be-
tween the following two objectives: on the one hand, the multi-head
policyğœ‹is optimized for all current and previous tasks ğ‘‡1toğ‘‡ğ‘›
for action prediction; on the other hand, the policy ğœ‹is also used
for updating the current Q-network ğ‘„ğ‘›. More specifically, we be-
gin this analysis from Q-learning [34, 35] with the Bellman equation
ofğ‘„(ğ‘ ,ğ‘)=ğ‘Ÿ(ğ‘ ,ğ‘)+ğ›¾âˆ—maxğ‘â€²ğ‘„(ğ‘ â€²,ğ‘â€²). In a continuous action
space, as the maximization operator above cannot be realized, a pa-
rameterized actor network ğœ‡(ğ‘ )is commonly used instead to take
the optimal action corresponding to maximal Q value. ğœ‡andğœ‹can
be considered equivalently in a single-task setting. However, for con-
tinual learning, ğœ‹is constrained to clone all previous tasks from ğ‘‡1to
ğ‘‡ğ‘›âˆ’1whileğœ‡depends only on the current task ğ‘‡ğ‘›. Consequently, it
is difficult for the policy ğœ‹to take action corresponding to the maxi-
mum future reward for ğ‘ â€²in taskğ‘‡ğ‘›.
Based on such analysis, we propose a novel DBC scheme to solve
the inconsistency mentioned above, and the schematic diagram of
our proposed architecture is given in Fig. 3(b). In comparison to ex-
isting multi-head network architecture in Fig. 3(a) [42], we propose
an extra policy network ğœ‡ğ‘›in order to learn the optimal state-action
mapping for ğ‘‡ğ‘›. Specifically, when learning task ğ‘‡ğ‘›, we first obtain
of the Q network in our experiments. However, we find that a simple L2
distance in the state space Salso works well.ğœ‡ğ‘›andğ‘„ğ‘›by using an offline RL algorithm. Then, in the rehearsal
phase, the continual learning policy ğœ‹is required to clone the pre-
vious experiences from ğµ1toğµğ‘›âˆ’1and meanwhile be close to ğœ‡ğ‘›.
Thus, the corresponding loss ğ¿ğœ‹can be written as follows
ğ¿ğœ‹=E(ğ‘ ,ğ‘,ğ‘ â€²)âˆ¼D ğ‘›(ğœ‹ğ‘›(ğ‘ ,ğ‘)âˆ’ğœ‡ğ‘›(ğ‘ ,ğ‘))2
+ğœ†ğ‘Ÿ1
ğ‘›ğ‘›âˆ’1âˆ‘ï¸
ğ‘—=1E(ğ‘ ,ğ‘)âˆ¼Bğ‘— ğœ‹ğ‘—(ğ‘ )âˆ’ğ‘2, (4)
whereğœ†ğ‘Ÿis a coefficient for the BC item. It is worth mentioning that
the continual learning policy ğœ‹attempts to clone the behavior of both
ğœ‡ğ‘›and the previous experiences from ğµ1toğµğ‘›âˆ’1simultaneously.
Therefore, we name our scheme DBC.
4.3 Algorithm Summary
When considering a new task ğ‘‡ğ‘›, we first utilize DBC to learn the
two policy networks ğœ‹,ğœ‡ğ‘›, and the dynamic model Ë†ğ‘ƒğ‘›until conver-
gence. Then, the learned policy ğœ‹and the dynamic model Ë†ğ‘ƒğ‘›are used
in MBES to select valuable data for ğµğ‘›. To summarize, the overall
process of OER, including DBC and MBES, is given in Algorithm 1.
5 Implementation Details
We model the Q-function and policy network as a multi-layer per-
ceptron (MLP) with two hidden layers of 128 neurons, each with
ReLU non-linearity based on [44]. We use an ensemble dynamic
model containing five individual models, which is also modeled as
an MLP, the same as the Q-function and the policy network. Any
offline RL method is compatible with our architecture. We choose
TD3+BC [10] as the backbone algorithm because of its simple struc-
ture to demonstrate that our algorithm does not depend on a specific
offline algorithm. Further, we use Adam [18] with a learning rate
of0.001to update both the Q-function and the dynamic model and
0.003to update both the policy network ğœ‹andğœ‡ğ‘›. Then, for each
task, we train 30,000steps and switch to the next. We find that initial-
izingğœ‡ğ‘›withğœ‹ğ‘›âˆ’1and learning ğœ‹andğœ‡ğ‘›simultaneously work well
from the experience. Also, learning ğœ‹andğœ‡ğ‘›together will reduce the
scope of the gradient and avoid the jumping change to ensure stable
learning and relieve catastrophic forgetting. The result is calculated
via five repeated simulations with different numbers of seeds.
4

--- PAGE 5 ---
Algorithm 1 Our proposed Method OER
Require: Number of tasks ğ‘; initiate the policy ğœ‹.
1:forTasksğ‘‡ğ‘›in[1,Â·Â·Â·,ğ‘]do
2: Get offline datasetDğ‘›; Initiate the replay buffer ğµğ‘›=âˆ…; Ini-
tiate new head â„ğ‘›forğœ‹; Initiateğœ‡ğ‘›,ğ‘„ğ‘›and Ë†ğ‘ƒğ‘›.
3: while Not Convergence do
4: Updateğœ‡ğ‘›andğ‘„ğ‘›via offline learning method.
5: Updateğœ‹via Eqn. 4 to clone ğœ‡ğ‘›andğµ0toğµğ‘›âˆ’1.
6: Update the dynamic model Ë†ğ‘ƒğ‘›.
7: end while
8: Sample initial state ğ‘ 0fromğœŒ0,ğ‘›, andğ‘ ğ‘¡â†ğ‘ 0.
9: whileğµğ‘›is not full do
10: ifğ‘ ğ‘¡is not terminal state then
11: Selectğ‘ ğ‘¡+1fromDğ‘›byğœ‹ğ‘›and Ë†ğ‘ƒğ‘›via Eqn. 3.
12: else
13: Sampleğ‘ ğ‘¡+1fromğœŒ0,ğ‘›.
14: end if
15: Addğ‘ ğ‘¡+1intoğµğ‘›, andğ‘ ğ‘¡â†ğ‘ ğ‘¡+1.
16: end while
17:end for
Ensure:ğœ‹.
6 Experiments
Extensive experiments are conducted to demonstrate the effective-
ness of our proposed scheme and test whether we can keep both sta-
bility and plasticity at the same time when learning sequential offline
RL tasks. We evaluate the performance of MBES and DBC sepa-
rately to test the performance of each approach.
6.1 Baselines and Datasets
Baselines On one hand , in order to evaluate the MBES, we con-
sider six replay buffer selection approaches, where four from [14] are
given as follows.
â€¢Surprise [14]: store trajectories with maximum mean TD error:
minğœâˆˆğ·ğ‘–Eğ‘ ,ğ‘,ğ‘  ğ‘¡+1âˆˆğœâˆ¥Bâˆ—ğ‘„(ğ‘ ,ğ‘)âˆ’ğ‘„(ğ‘ ,ğ‘)âˆ¥2
2.
â€¢Reward [14]: store trajectories with maximum mean reward:
maxğœâˆˆğ·ğ‘–ğ‘…ğ‘¡,ğ‘›.
â€¢Random [14]: randomly select samples in dataset.
â€¢Coverage [14]: store those samples to maximize coverage
of the state space, or ensure the sparsity of selected states:
minğ‘ âˆˆğ·ğ‘–|Nğ‘–|;Nğ‘–={ğ‘ â€²s.t.dist(ğ‘ â€²âˆ’ğ‘ )<ğ‘‘}.
Considering that these baselines are designed for online RL and it
is not fair enough to use them only as the baselines, we have de-
signed two algorithms for comparison that are applicable to offline
RL, based on the idea of [14].
â€¢Match: select samples in the offline dataset most consistent with
the learned policy. Trajectories chosen in this way are most similar
to the learned policy in the action space, but may not match in the
state space: minğœâˆˆğ·ğ‘–Eğ‘ ,ğ‘âˆˆğœğ‘âˆ’ğœ‹âˆ—
ğ‘–(ğ‘ )2
2.
â€¢Model: Given that we used a Model-Based approach
to filter our data, we also used it as a criterion for
whether the trajectories matched the transfer probabilities:
minğœâˆˆğ·ğ‘–Eğ‘ ,ğ‘âˆˆğœË†ğ‘ƒğ‘–(ğ‘ ,ğ‘)âˆ’ğ‘ƒğ‘–(ğ‘ ,ğ‘)2
2. This metric is used to
demonstrate that the introduction of the Model-Based approach
alone does not improve the performance of the CORL algorithm.On the other hand , in order to evaluate the DBC, we consider five
widely-used continual learning methods, where three methods need
to use replay buffers.
â€¢BC [42]: a basic rehearsal-based continual method adding a be-
havior clone term in the loss function of the policy network.
â€¢Gradient episodic memory (GEM) [27]: a method using an
episodic memory of parameter gradients to limit the policy up-
date.
â€¢Averaged gradient episodic memory (AGEM) [4]: a method based
on GEM that only uses a batch of gradients to limit the policy
update.
In addition, the following two regularization-based methods are
rehearsal-free so that they are independent of experience selection
methods.
â€¢Elastic weight consolidation (EWC) [19]: constrain the changes
to critical parameters through the Fisher information matrix.
â€¢Synaptic intelligence (SI) [53]: constrain the changes after each
step of optimization.
We also show the performance on multi-task as a reference. The
multi-task learning setting does not suffer from the catastrophic for-
getting problem and can be seen as superior.
Offline Sequential Datasets We consider three sets of tasks from
widely-used continuous control offline meta-RL library3as in [33]:
â€¢Ant-2D Direction (Ant-Dir): train a simulated ant with 8 articu-
lated joints to run in a 2D direction;
â€¢Walker-2D Params (Walker-Par): train a simulated agent to move
forward, where different tasks have different parameters. Specifi-
cally, different tasks require the agent to move at different speeds;
â€¢Half-Cheetah Velocity (Cheetah-Vel): train a cheetah to run at a
random velocity.
For each set of tasks, we randomly sample five tasks to form sequen-
tial tasksğ‘‡1toğ‘‡5.
To consider different data quality as [9], we train a soft actor-critic
to collect two benchmarks [12] for each task ğ‘‡ğ‘›,ğ‘›=1,Â·Â·Â·,5: 1)
Medium (M) with trajectories from medium-quality policy, and 2)
Medium-Random (M-R) including trajectories from both medium-
quality policy and trajectories randomly sampled.
Metrics Following [7], we adopt the average performance (PER)
and the backward transfer (BWT) as evaluation metrics,
PER=1
ğ‘ğ‘âˆ‘ï¸
ğ‘›=1ğ‘ğ‘,ğ‘›,BWT =1
ğ‘âˆ’1ğ‘âˆ’1âˆ‘ï¸
ğ‘›=1ğ‘ğ‘›,ğ‘›âˆ’ğ‘ğ‘,ğ‘›,(5)
whereğ‘ğ‘–,ğ‘—means the final cumulative rewards of task ğ‘—after learning
taskğ‘–. For PER, higher is better; for BWT, lower is better. These two
metrics show the performance of learning new tasks while alleviating
the catastrophic forgetting problem.
6.2 Overall Results
Evaluate MBES: Firstly, our method OER is compared with
eleven baselines on two kinds of qualities M-R and M. Since
3Most current continual RL methods perform not well on complex bench-
marks such as [47], so it is not suitable for evaluating our method [33] and
we do not consider it in this paper.
5

--- PAGE 6 ---
Benchmark MethodsAnt-Dir Walker-Par Cheetah-Vel
PER BWT PER BWT PER BWT
M-RMultiTask 1387.54 - 1630.90 - -112.49 -
Coverage+DBC 677.72 727.40 231.17 1428.14 -404.33 293.42
Match+DBC 776.85 432.74 120.96 1079.19 -121.30 26.50
Supervise+DBC 903.12 175.17 196.07 1063.35 -233.05 82.60
Reward+DBC 893.63 36.10 554.05 1087.96 -242.50 102.75
Model+DBC 1156.43 65.66 194.31 1214.32 -141.20 57.90
Random+DBC 845.82 126.56 614.71 979.90 -104.16 36.24
OER 1316.46 119.71 1270.62 550.18 -76.61 16.54
MMultiTask 1357.20 - 1751.68 - -115.30 -
Coverage+DBC 842.46 599.74 361.55 1342.87 -424.87 229.89
Match+DBC 841.88 549.98 886.28 678.55 -196.83 88.87
Supervise+DBC 1049.84 347.88 1020.57 666.15 -219.78 56.99
Reward+DBC 1125.44 269.76 891.55 790.07 -222.83 40.15
Model+DBC 1147.05 253.49 872.94 807.33 -184.83 24.22
Random+DBC 1189.17 179.75 1102.89 616.52 -150.39 30.18
OER 1215.58 176.85 1192.59 518.20 -148.18 16.36
Table 1 : Performance of our OER and baselines to verify the effectiveness of MBES, where M-R and M are included. We can observe that our
method OER has the highest PER and lowest BWT in all cases.
Benchmark MethodsAnt-Dir Walker-Par Cheetah-Vel
PER BWT PER BWT PER BWT
M-RMBES+SI 747.95 643.89 124.04 1460.08 -437.49 316.84
MBES+EWC 655.21 726.37 110.53 1589.62 -568.07 506.19
MBES+GEM 748.90 643.06 114.07 1477.87 -445.39 389.10
MBES+AGEM 722.41 687.97 62.27 1628.53 -546.15 419.61
MBES+BC 407.94 874.80 46.75 860.17 -645.79 21.92
OER 1316.46 119.71 1270.62 550.18 -76.61 16.54
Table 2 : Performance of our OER and baselines to verify the effectiveness of DBC, where M-R is included. The result of M can be found in
the Supplementary Material. We can observe that our method OER has the highest PER and lowest BWT in all cases.
our OER comprises MBES and DBC, six experience selection ap-
proaches are added with DBC for a fair comparison. The overall re-
sults are reported in Table 1, in terms of PER and BWT on three se-
quential tasks. From Table 1, we draw the following conclusions: 1)
Our OER method outperforms all baselines for all cases, indicating
the effectiveness of our MBES scheme; 2) Our OER method demon-
strates greater superiority on M-R dataset than on M dataset, indicat-
ing that M-R dataset has a larger distribution shift than M dataset,
and MBES addresses such shift; 3) Random+DBC performs better
than the other five baselines because these five experience selection
schemes are dedicated for online but not offline scenarios. Further-
more, Fig. ??shows the learning process of Ant-Dir on five sequen-
tial tasks. From Fig.4(a) - 4(d), Fig.4(g) - 4(h) and Supplementary
Material, we can observe that compared with baselines, our OER
demonstrates less performance drop with the increment of tasks, in-
dicating that OER can better solve the catastrophic forgetting.
Evaluate DBC: Secondly, our method OER is compared with five
baselines. Similarly, continual learning approaches are added with
MBES for a fair comparison, and the overall performance is reported
in Table 2 and the Supplementary Material. From Table 2 and the
Supplementary Material, we draw the following conclusions: 1) Our
OER method outperforms all baselines, indicating the effectiveness
of our DBC scheme; 2) Four continual learning methods perform not
well due to the forgetting problem; 3) MBES+BC performs the worst
due to the inconsistency of two policies ğœ‹andğœ‡ğ‘›in Section 4.2.
From Fig.4(e) - 4(h) and Supplementary Material, we can observethat our OER can learn new tasks and remember old tasks well; other
continual learning methods can only learn new tasks but forget old
tasks, while BC cannot even learn new tasks.
6.3 Parameter Analysis
Size of Buffer ğµğ‘›In rehearsal-based continual learning, the size
of replay buffer is a key factor. In our CORL setting, the size of
bufferğµğ‘›is selected as 1K for allğ‘›, by considering both storage
space and forgetting issues. With the increase of buffer size, we need
more storage space but have less forgetting issue. In order to quan-
tify such analysis, we consider a buffer size 10K for OER and two
baselines, and the results are listed in Table 3. From Table 3, we can
observe that 1) With the increase of buffer size, OER and two base-
lines achieve better performance as expected. 2) Our DBC method is
still much better than BC, indicating that solving the inconsistency
is significant; 3) With larger storage space, the baseline Random per-
forms similar as MBES, because in this case forgetting issue gets
much smaller and the experience selection becomes not important.
Replay Coefficient ğœ†ğ‘ŸAnother key factor is the coefficient ğœ†ğ‘Ÿin
Eq. 4, where ğœ†ğ‘Ÿis used to balance the anti-forgetting BC item and
the new-policy constraint item. In our CORL setting, we select ğœ†ğ‘Ÿas
1, which is also the general choice in ER-based approaches [42, 14],
and good performance has been achieved, as mentioned above. We
analyze different values of ğœ†ğ‘Ÿand show the corresponding perfor-
mance of our OER and baselines in Table 4, where ğœ†ğ‘Ÿis selected as
0.3,1and3, respectively. From Table 4, we can observe that with
6

--- PAGE 7 ---
(a) Match M
 (b) Match M-R
 (c) Random M
 (d) Random M-R
(e) EWC M
 (f) BC M
 (g) OER M
 (h) OER MR
Figure 4 : Process of learning five sequential tasks, where our OER is compared with Match (M-R and M), Random (M-R and M), EWC (M)
and BC (M). More results are given in Supplementary Material. Every 30000 steps on one task, we switch to the next task.
MethodAnt-Dir Medium Walker-Par Medium Cheetah-Vel Medium
PER BWT PER BWT PER BWT
RewardBC -308.10 1049.32 124.54 1102.85 -706.31 343.23
DBC 1310.45 78.89 1528.63 157.11 -207.76 39.82
RandomBC -295.06 997.96 115.14 1199.63 -750.20 295.36
DBC 1374.54 32.01 1565.50 45.42 -143.10 56.16
MBESBC -338.03 1164.01 588.26 958.38 -582.93 199.81
DBC 1367.07 6.32 1578.30 78.72 -141.17 21.18
Table 3 : Performance of baselines Reward, Random and our OER, where the replay buffer capacity is as 10,000 samples for BC and DBC.
a largerğœ†ğ‘Ÿ, the forgetting issue gradually reduces, but it gets looser
that the learning policy ğœ‹clonesğœ‡ğ‘›in Eq. 4, and vice versa. As a
result, we achieve the best performance when ğœ†ğ‘Ÿ=1. This is why
we useğœ†ğ‘Ÿ=1for all experiments in this paper.
7 Conclusion
In this work, we formulate a new CORL setting and present a new
method OER, primarily including two key components: MBES and
DBC. We point out a new distribution bias problem and training in-
stability unique to the new CORL setting. Specifically, to solve a
novel distribution shift problem in CORL, we propose a novel MBES
scheme to select valuable experiences from the offline dataset to
build the replay buffer. Moreover, in order to address the inconsis-
tency issue between learning the new task and cloning old tasks,
we propose a novel DBC scheme. Experiments and analysis show
that OER outperforms SOTA baselines on various continuous con-
trol tasks.
References
[1] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi, â€˜An
optimistic perspective on offline reinforcement learningâ€™, in ICML ,
(2020).
[2] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and
Simone Calderara, â€˜Dark experience for general continual learning: A
strong, simple baselineâ€™, in NIPS , (2020).Methodğœ†ğ‘ŸAnt-Dir M Walker-Par M
PER BWT PER BWT
Supervise
+DBC0.3 980.25 537.92 1003.92 688.19
1 1049.84 347.88 1020.57 666.15
3 984.62 201.70 944.71 608.82
Random
+DBC0.3 1168.82 184.57 1052.13 647.71
1 1189.17 179.75 1102.89 616.52
3 952.46 168.81 1157.30 330.47
OER0.3 940.38 431.13 1049.55 606.18
1 1215.58 176.85 1192.59 518.20
3 1128.66 170.32 1051.63 507.05
Table 4 : Performance of our OER method with different coefficient
ğœ†ğ‘Ÿ. Highğœ†ğ‘Ÿindicates more on replaying previous tasks.
[3] Arslan Chaudhry, Albert Gordo, Puneet Kumar Dokania, Philip H. S.
Torr, and David Lopez-Paz, â€˜Using hindsight to anchor past knowledge
in continual learningâ€™, in AAAI , (2021).
[4] Arslan Chaudhry, Marcâ€™Aurelio Ranzato, Marcus Rohrbach, and Mo-
hamed Elhoseiny, â€˜Efficient Lifelong Learning with A-GEMâ€™, in ICLR ,
(2019).
[5] Xiong-Hui Chen, Yang Yu, Qingyang Li, Fan Luo, Zhiwei Qin, Wenjie
Shang, and Jieping Ye, â€˜Offline model-based adaptable policy learn-
ingâ€™, in NeurIPS , (2021).
[6] Robert Dadashi, Shideh Rezaeifar, Nino Vieillard, LÃ©onard Hussenot,
Olivier Pietquin, and Matthieu Geist, â€˜Offline Reinforcement Learning
with Pseudometric Learningâ€™, in ICML , (July 2021).
[7] Mohammad Mahdi Derakhshani, Xiantong Zhen, Ling Shao, and Cees
Snoek, â€˜Kernel Continual Learningâ€™, in ICML , volume 139, (July
2021).
[8] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols,
David Ha, Andrei A. Rusu, Alexander Pritzel, and Daan Wierstra,
7

--- PAGE 8 ---
â€˜PathNet: Evolution Channels Gradient Descent in Super Neural Net-
worksâ€™, CoRR ,abs/1701.08734 , (2017). arXiv: 1701.08734.
[9] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey
Levine, â€˜D4RL: Datasets for Deep Data-Driven Reinforcement Learn-
ingâ€™, CoRR ,abs/2004.07219 , (2020). arXiv: 2004.07219.
[10] Scott Fujimoto and Shixiang (Shane) Gu, â€˜A Minimalist Approach to
Offline Reinforcement Learningâ€™, in NIPS . Curran Associates, Inc.,
(2021).
[11] Scott Fujimoto, David Meger, and Doina Precup, â€˜Off-policy deep re-
inforcement learning without explorationâ€™, in ICML , (2019).
[12] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine,
â€˜Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
Learning with a Stochastic Actorâ€™, in ICML , eds., Jennifer Dy and
Andreas Krause, volume 80 of Proceedings of Machine Learning Re-
search . PMLR, (July 2018).
[13] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, G. Tucker,
Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta,
P. Abbeel, and Sergey Levine, â€˜Soft actor-critic algorithms and applica-
tionsâ€™, ArXiv ,abs/1812.05905 , (2018).
[14] David Isele and Akansel Cosgun, â€˜Selective Experience Replay for
Lifelong Learningâ€™, AAAI , (April 2018).
[15] Christos Kaplanis, Murray Shanahan, and Claudia Clopath, â€˜Policy
Consolidation for Continual Reinforcement Learningâ€™, in ICML , vol-
ume 97 of Proceedings of Machine Learning Research . PMLR, (2019).
[16] Samuel Kessler, Jack Parker-Holder, Philip J. Ball, Stefan Zohren, and
Stephen J. Roberts, â€˜Same state, different task: Continual reinforcement
learning without interferenceâ€™, in AAAI , (2021).
[17] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and
Thorsten Joachims, â€˜Morel : Model-based offline reinforcement learn-
ingâ€™, ArXiv ,abs/2005.05951 , (2020).
[18] Diederik P. Kingma and Jimmy Ba, â€˜Adam: A Method for Stochastic
Optimizationâ€™, in ICLR , (2015).
[19] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Ve-
ness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John
Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hass-
abis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell, â€˜Over-
coming catastrophic forgetting in neural networksâ€™, Proceedings
of the National Academy of Sciences ,114(13), (2017). _eprint:
https://www.pnas.org/doi/pdf/10.1073/pnas.1611835114.
[20] Ilya Kostrikov, Ashvin Nair, and Sergey Levine, â€˜Offline reinforcement
learning with implicit q-learningâ€™, in ICLR , (2021).
[21] Aviral Kumar, Justin Fu, G. Tucker, and Sergey Levine, â€˜Stabilizing
off-policy q-learning via bootstrapping error reductionâ€™, in NeurIPS ,
(2019).
[22] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine, â€˜Con-
servative Q-Learning for Offline Reinforcement Learningâ€™, in NIPS ,
volume 33. Curran Associates, Inc., (2020).
[23] Erwan Lecarpentier, David Abel, Kavosh Asadi, Yuu Jinnai, Emmanuel
Rachelson, and Michael L. Littman, â€˜Lipschitz Lifelong Reinforcement
Learningâ€™, in AAAI , (2021).
[24] Byung-Jun Lee, Jongmin Lee, and Kee-Eung Kim, â€˜Representation
Balancing Offline Model-based Reinforcement Learningâ€™, in ICLR ,
(February 2022).
[25] Jinxin Liu, Zhang Hongyin, and Donglin Wang, â€˜Dara: Dynamics-
aware reward augmentation in offline reinforcement learningâ€™, in In-
ternational Conference on Learning Representations , (2021).
[26] Jinxin Liu, Ziqi Zhang, Zhenyu Wei, Zifeng Zhuang, Yachen Kang,
Sibo Gai, and Donglin Wang, â€˜Beyond ood state actions: Supported
cross-domain offline reinforcement learningâ€™, ArXiv ,abs/2306.12755 ,
(2023).
[27] David Lopez-Paz and Marcâ€™ Aurelio Ranzato, â€˜Gradient Episodic
Memory for Continual Learningâ€™, in NIPS , volume 30. Curran Asso-
ciates, Inc., (2017).
[28] Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu, â€˜Mildly conserva-
tive q-learning for offline reinforcement learningâ€™, in Advances in Neu-
ral Information Processing Systems , eds., Alice H. Oh, Alekh Agarwal,
Danielle Belgrave, and Kyunghyun Cho, (2022).
[29] Yecheng Ma, Dinesh Jayaraman, and Osbert Bastani, â€˜Conservative of-
fline distributional reinforcement learningâ€™, NIPS ,34, (2021).
[30] Arun Mallya and Svetlana Lazebnik, â€˜PackNet: Adding Multiple Tasks
to a Single Network by Iterative Pruningâ€™, in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) ,
(June 2018).
[31] Michael McCloskey and Neal J. Cohen, â€˜Catastrophic interference inconnectionist networks: The sequential learning problemâ€™, Psychology
of Learning and Motivation, Academic Press, (1989).
[32] Jorge Mendez, Boyu Wang, and Eric Eaton, â€˜Lifelong Policy Gradient
Learning of Factored Policies for Faster Training Without Forgettingâ€™,
inAdvances in Neural Information Processing Systems , volume 33.
Curran Associates, Inc., (2020).
[33] Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and
Chelsea Finn, â€˜Offline meta-reinforcement learning with advantage
weightingâ€™, in ICML , (2021).
[34] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller,
â€˜Playing atari with deep reinforcement learningâ€™, arXiv preprint
arXiv:1312.5602 , (2013).
[35] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu,
Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller,
Andreas K Fidjeland, Georg Ostrovski, et al., â€˜Human-level control
through deep reinforcement learningâ€™, nature , (2015).
[36] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine,
â€˜Awac: Accelerating online reinforcement learning with offline
datasetsâ€™, arXiv , (2020).
[37] German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and
Stefan Wermter, â€˜Continual lifelong learning with neural networks: A
reviewâ€™, Neural Networks ,113, (2019).
[38] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine,
â€˜Advantage-Weighted Regression: Simple and Scalable Off-Policy
Reinforcement Learningâ€™, CoRR ,abs/1910.00177 , (2019). arXiv:
1910.00177.
[39] David Pfau and Oriol Vinyals, â€˜Connecting generative adversarial net-
works and actor-critic methodsâ€™, CoRR , (2016).
[40] Ameya Prabhu, Philip H. S. Torr, and Puneet Kumar Dokania, â€˜Gdumb:
A simple approach that questions our progress in continual learningâ€™,
inECCV , (2020).
[41] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, G. Sperl, and
Christoph H. Lampert, â€˜icarl: Incremental classifier and representa-
tion learningâ€™, 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , (2017).
[42] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and
Gregory Wayne, â€˜Experience Replay for Continual Learningâ€™, in NIPS .
Curran Associates, Inc., (2019).
[43] Gobinda Saha, Isha Garg, and Kaushik Roy, â€˜Gradient Projection Mem-
ory for Continual Learningâ€™, in ICLR . OpenReview.net, (2021).
[44] Takuma Seno and Michita Imai, â€˜d3rlpy: An Offline Deep Rein-
forcement Learning Libraryâ€™, CoRR ,abs/2111.03788 , (2021). arXiv:
2111.03788.
[45] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost To-
bias Springenberg, Scott E Reed, Bobak Shahriari, Noah Siegel, Caglar
Gulcehre, Nicolas Heess, et al., â€˜Critic regularized regressionâ€™, NIPS ,
33, 7768â€“7778, (2020).
[46] Maciej Wolczyk, MichaÅ‚ Zaj Ë› ac, Razvan Pascanu, Åukasz Kuci Â´nski, and
Piotr MiÅ‚o Â´s, â€˜Disentangling transfer in continual reinforcement learn-
ingâ€™, in NIPS , (2022).
[47] Maciej WoÅ‚czyk, Michal Zajkac, Razvan Pascanu, Lukasz Kuciâ€™nski,
and Piotr Miloâ€™s, â€˜Continual world: A robotic benchmark for continual
reinforcement learningâ€™, in NeurIPS , (2021).
[48] Annie Xie, James Harrison, and Chelsea Finn, â€˜Deep Reinforcement
Learning amidst Continual Structured Non-Stationarityâ€™, in ICML , vol-
ume 139 of Proceedings of Machine Learning Research . PMLR, (July
2021).
[49] Jaehong Yoon, Divyam Madaan, Eunho Yang, and Sung Ju Hwang,
â€˜Online coreset selection for rehearsal-based continual learningâ€™, in
ICLR , (2021).
[50] Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Sergey
Levine, and Chelsea Finn, â€˜Conservative data sharing for multi-task of-
fline reinforcement learningâ€™, NIPS ,34, 11501â€“11516, (2021).
[51] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey
Levine, and Chelsea Finn, â€˜COMBO: Conservative Offline Model-
Based Policy Optimizationâ€™, in NIPS . Curran Associates, Inc., (2021).
[52] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou,
Sergey Levine, Chelsea Finn, and Tengyu Ma, â€˜MOPO: Model-based
Offline Policy Optimizationâ€™, in NIPS . Curran Associates, Inc., (2020).
[53] Friedemann Zenke, Ben Poole, and Surya Ganguli, â€˜Continual Learning
Through Synaptic Intelligenceâ€™, in ICML . PMLR, (2017).
[54] Siyuan Zhang and Nan Jiang, â€˜Towards hyperparameter-free policy se-
lection for offline reinforcement learningâ€™, in NIPS . Curran Associates,
8

--- PAGE 9 ---
Inc., (2021).
[55] Zifeng Zhuang, Kun Lei, Jinxin Liu, Donglin Wang, and Yilang
Guo, â€˜Behavior proximal policy optimizationâ€™, ArXiv ,abs/2302.11312 ,
(2023).Supplemental Materials: OER: Offline Experience
Replay for Continual Offline Reinforcement
Learning
8 Section 1
9 Additional Results
Here we give out the learning process of different methods of Ant-
Dir dataset in Fig.A1.
1

--- PAGE 10 ---
(a) Coverage M
 (b) Coverage M-R
 (c) Match M
 (d) Match M-R
(e) Model M
 (f) Model M-R
 (g) Reward M
 (h) Reward M-R
(i) Supervise M
 (j) Supervise M-R
 (k) Random M
 (l) Random M-R
(m) GEM M
 (n) GEM M-R
 (o) AGEM M
 (p) AGEM M-R
(q) EWC M
 (r) EWC M-R
 (s) SI M
 (t) SI M-R
(u) BC M
 (v) BC M-R
 (w) OER M
 (x) OER M-R
Figure 1. Process of learning five sequential tasks, where some of the results have been given in the main body.
2
