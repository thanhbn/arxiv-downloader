# 2309.01352.pdf
# Được chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2309.01352.pdf
# Kích thước tệp: 1139549 bytes

===============================================
NỘI DUNG TỆP PDF
===============================================

--- TRANG 1 ---
Self-driven Grounding: Tác nhân Mô hình Ngôn ngữ Lớn với Việc Học Kỹ năng Tự động Căn chỉnh Ngôn ngữ
Shaohui Peng1, Xing Hu2, Qi Yi2, 5, Rui Zhang2, 4, Jiaming Guo2, Di Huang2,
Zikang Tian1, Ruizhi Chen1, 3, Zidong Du2, 4, Qi Guo2, Yunji Chen2, 3, Ling Li1, 3
1Trung tâm Nghiên cứu Phần mềm Thông minh, Viện Phần mềm, CAS
2Phòng thí nghiệm Trọng điểm Nhà nước về Bộ vi xử lý, Viện Công nghệ Máy tính, CAS
3Đại học Viện Hàn lâm Khoa học Trung Quốc
4Công nghệ Cambricon
5Đại học Khoa học và Công nghệ Trung Quốc
pengshaohui@iscas.ac.cn

Tóm tắt
Các mô hình ngôn ngữ lớn (LLM) thể hiện khả năng lập luận và lập kế hoạch tự động mạnh mẽ với kho kiến thức ngữ nghĩa phong phú về thế giới con người. Tuy nhiên, vấn đề căn cứ vẫn cản trở việc ứng dụng LLM trong môi trường thế giới thực. Các nghiên cứu hiện tại cố gắng tinh chỉnh LLM hoặc sử dụng các API hành vi được xác định trước để kết nối LLM với môi trường, điều này không chỉ tốn rất nhiều nỗ lực của con người để tùy chỉnh cho từng nhiệm vụ cụ thể mà còn làm suy yếu điểm mạnh về tính tổng quát của LLM. Để tự động căn cứ LLM vào môi trường, chúng tôi đề xuất khung Self-Driven Grounding (SDG) để tự động và dần dần căn cứ LLM với việc học kỹ năng tự điều khiển. SDG trước tiên sử dụng LLM để đề xuất giả thuyết về các mục tiêu phụ để đạt được nhiệm vụ và sau đó xác minh tính khả thi của giả thuyết thông qua tương tác với môi trường cơ bản. Một khi được xác minh, SDG có thể học các kỹ năng tổng quát với sự hướng dẫn của những mục tiêu phụ được căn cứ thành công này. Những kỹ năng này có thể được sử dụng thêm để hoàn thành các nhiệm vụ phức tạp hơn mà không vượt qua được giai đoạn xác minh. Được xác minh trong bộ nhiệm vụ theo hướng dẫn nổi tiếng - BabyAI, SDG đạt được hiệu suất tương đương trong các nhiệm vụ thách thức nhất so với các phương pháp học mô phỏng tốn hàng triệu minh chứng, chứng minh hiệu quả của các kỹ năng đã học và thể hiện tính khả thi và hiệu quả của khung của chúng tôi.

1 Giới thiệu
Các mô hình ngôn ngữ lớn (LLM) đã thể hiện khả năng mạnh mẽ trong lập luận và lập kế hoạch tự động với kho kiến thức ngữ nghĩa phong phú về thế giới con người (Wei et al. 2022, 2021; OpenAI 2023; Kojima et al. 2022). Tuy nhiên, vẫn còn một khoảng cách lớn trong việc áp dụng LLM để tự động giải quyết các vấn đề trong môi trường cụ thể. Điều này là do sự không phù hợp giữa lập kế hoạch ngữ nghĩa của LLM và việc triển khai cụ thể được căn cứ, điều này cũng được gọi là vấn đề căn cứ (Ichter et al. 2022; Driess et al. 2023). Giải quyết vấn đề này có thể mở khóa khả năng hiểu và tác động đến thế giới thực của LLM, đây là một bước vững chắc hướng tới các ứng dụng thế giới thực của trí tuệ nhân tạo.

Để giải quyết vấn đề căn cứ, các nghiên cứu hiện tại cố gắng tinh chỉnh LLM để dự đoán các hành động khả thi (Carta et al. 2023; Li et al. 2022; Wang et al. 2022) hoặc sử dụng một tập hợp các API hành vi (tức là các kỹ năng cấp thấp) phục vụ như một cầu nối giữa LLM và môi trường (Ichter et al. 2022; Raman et al. 2022; Liang et al. 2022). Một mặt, việc tinh chỉnh LLM có hiệu quả mẫu thấp và cũng có thể làm hỏng khả năng lập luận của LLM. Mặt khác, các phương pháp hiện tại dựa vào API hành vi thường giả định rằng các API được môi trường xác định trước (Liang et al. 2022) hoặc được đào tạo trước bằng cách sử dụng các minh chứng chuyên gia (Ichter et al. 2022), điều này không chỉ tốn rất nhiều nỗ lực của con người để tùy chỉnh cho từng nhiệm vụ cụ thể mà còn làm suy yếu điểm mạnh về tính tổng quát của LLM. Do đó, làm thế nào để tự động căn cứ LLM vào môi trường vẫn là một vấn đề mở và là thách thức chính của các tác nhân dựa trên LLM.

Việc đạt được mục tiêu căn cứ tự động ánh xạ kế hoạch ngữ nghĩa của LLM thành triển khai thực tế là thách thức vì những lý do sau: 1) Điều kiện tiên quyết của việc căn cứ, thu được những trải nghiệm thành công, là khó khăn vì phần thưởng thưa thớt trong thế giới vật lý. 2) Ngay cả khi có được những trải nghiệm thành công hiếm hoi, việc căn cứ thường liên quan chặt chẽ đến các nhiệm vụ cụ thể mà không có thư viện API được chia sẻ, do đó có tính tổng quát thấp và không có giá trị cho các nhiệm vụ chung. Để giải quyết những vấn đề này, chúng tôi tạo ra phần thưởng nội tại dựa trên các mục tiêu phụ do LLM tạo ra và các hàm kiểm tra của chúng, điều này làm tăng trải nghiệm thành công bằng cách giảm bớt vấn đề phần thưởng thưa thớt. Sau đó chúng tôi đề xuất phương pháp học kỹ năng chung căn chỉnh ngôn ngữ bằng cách buộc mỗi kỹ năng đạt được một nhóm mục tiêu có mô tả ngữ nghĩa tương tự. Những kỹ năng này thể hiện tính tổng quát tốt trong việc giải quyết các nhiệm vụ khác hoặc thậm chí phức tạp hơn.

Tóm lại, chúng tôi đề xuất khung Self-Driven Grounding (SDG) kết hợp chặt chẽ LLM và quá trình học tăng cường trong các giai đoạn chính sau:
1) Giả thuyết: LLM không chỉ hoạt động như người lập kế hoạch bằng cách phân tách nhiệm vụ thành các mục tiêu phụ nhỏ mà còn cung cấp các hàm kiểm tra để các tác nhân RL có thể đánh giá liệu chúng có thể hoàn thành những mục tiêu phụ này hay không. Những phần thưởng nội tại từ LLM như vậy làm giảm đáng kể vấn đề phần thưởng thưa thớt.
2) Xác minh: với các mục tiêu phụ và các hàm kiểm tra tương ứng, các tác nhân RL học các chính sách của các mục tiêu phụ dựa trên phần thưởng nội tại và cuối cùng được xác minh thông qua việc liệu các nhiệm vụ có được hoàn thành hay không.
3) Quy nạp: các tác nhân RL nhóm các mục tiêu phụ đã được xác minh thông qua độ tương tự ngữ nghĩa và học chính sách kỹ năng tổng quát dựa trên chúng. Với những kỹ năng chung này, LLM có thể tạo ra giải pháp cho các nhiệm vụ chưa từng thấy hoặc thậm chí phức tạp hơn thông qua tương tác tối thiểu và hiệu quả.

Chúng tôi xác thực khung căn cứ tự điều khiển trong các nhiệm vụ theo hướng dẫn, điều này phổ biến và hợp lý cho các tác nhân dựa trên LLM vì có hướng dẫn nhiệm vụ văn bản. Được xác minh trong BabyAI, một nền tảng thế giới lưới để nghiên cứu các nhiệm vụ căn cứ ngôn ngữ, khung căn cứ tự động của chúng tôi đạt được hiệu suất tương đương trong các nhiệm vụ khó nhất so với các phương pháp học mô phỏng tốn hàng triệu minh chứng. Kết quả thí nghiệm không chỉ chứng minh hiệu quả của các kỹ năng đã học mà còn thể hiện tính khả thi và hiệu quả của khung của chúng tôi.

2 Nghiên cứu Liên quan
Tác nhân Hỗ trợ LLM LLM thể hiện sức mạnh to lớn trong lập luận và lập kế hoạch tự động với kho kiến thức ngữ nghĩa phong phú về thế giới con người. Do đó, việc liên quan LLM trong phát triển các tác nhân thông minh là đầy hứa hẹn. Thách thức chính trong các tác nhân được hỗ trợ bởi LLM là làm thế nào để căn cứ kiến thức của LLM (dưới dạng ngôn ngữ) vào các nhiệm vụ hiện tại.

Về thách thức này, có hai phương pháp chính: (1) sử dụng một tập hợp các API hành vi với các chú thích ngôn ngữ chi tiết. Những API như vậy có thể được môi trường xác định trước (Liang et al. 2022) hoặc được đào tạo trước bằng cách sử dụng các minh chứng chuyên gia (Ichter et al. 2022; Huang et al. 2022; Yuan et al. 2023). Ví dụ, Code as Policies (Liang et al. 2022) sử dụng LLM để tạo mã thực thi để hoàn thành hướng dẫn có thể gọi các API hành vi trong các điều kiện nhất định. SayCan (Ichter et al. 2022) mời con người đánh giá sự thành công của các minh chứng đã cho, được sử dụng để đào tạo các chính sách API và sau đó tạo ra một hàm khả năng. Voyager (Wang et al. 2023) lưu trữ và truy xuất mã thực thi, gọi các API cơ bản được triển khai trước, để xử lý các tình huống phức tạp. Để tương tác tốt hơn với môi trường, một số phương pháp (Huang et al. 2022; Raman et al. 2022) giới thiệu phản hồi môi trường để tái tạo các kế hoạch mới, có thể được xem như một loại học thử và sai khác. Mặc dù những phương pháp này đã đạt được tiến bộ ấn tượng bằng cách sử dụng API, ứng dụng của chúng cũng bị giới hạn bởi các API hành vi ở chỗ tác nhân chỉ có thể hoàn thành các nhiệm vụ có thể được giải quyết bằng cách sắp xếp những API cơ bản này. (2) tinh chỉnh LLM. LLM có thể được tinh chỉnh để dự đoán hành động khả thi của tác nhân khi có mô tả trạng thái. Việc tinh chỉnh như vậy có thể được thực hiện bằng cách sử dụng các minh chứng chuyên gia (Wang et al. 2022; Li et al. 2022) hoặc RL trực tuyến (Carta et al. 2023). Tuy nhiên, việc tinh chỉnh một mô hình lớn như LLM khá tốn thời gian và yêu cầu nhiều dữ liệu đào tạo.

Theo Hướng dẫn Trong việc theo hướng dẫn, một tác nhân được đưa ra một hướng dẫn và mục tiêu là hoàn thành nhiệm vụ được mô tả bởi hướng dẫn. Mô hình như vậy làm cho tác nhân có thể hỗ trợ con người bằng cách tuân theo hướng dẫn của con người, có ứng dụng rộng rãi trong thế giới thực. Các tác phẩm cho việc theo hướng dẫn có thể được chia thành ba loại: (1) Các phương pháp phân tích ngữ nghĩa (Artzi và Zettlemoyer 2013; Misra et al. 2016), phân tích trực tiếp hướng dẫn thành các hành động của tác nhân thông qua phân tích từ vựng và các quy tắc được xác định trước khác. Những phương pháp này yêu cầu rất nhiều nỗ lực của con người để thiết kế các quy tắc phù hợp, và không thể tổng quát hóa cho các môi trường phức tạp. (2) Các phương pháp dựa trên học, trực tiếp đào tạo một chính sách có điều kiện ngôn ngữ để hoàn thành hướng dẫn. Nhiều tác phẩm trước đây yêu cầu các minh chứng chuyên gia trong vòng lặp đào tạo của chúng. Ví dụ, các minh chứng chuyên gia thường được sử dụng trong học mô phỏng chính sách (Lynch và Sermanet 2021; Chaplot et al. 2018), ghi nhãn lại hướng dẫn hồi tưởng (Röder, Eppe, và Wermter 2022; Chen, Gupta, và Marino 2021), và học hàm phần thưởng có điều kiện ngôn ngữ (Bahdanau et al. 2019). Một số tác phẩm cố gắng tránh nhu cầu về các minh chứng chuyên gia (Ranzato et al. 2021; hig 2020), nhưng với cái giá là hiệu quả mẫu thấp hơn nhiều. Tất cả các phương pháp dựa trên học thường được đào tạo bằng cách sử dụng các mẫu hướng dẫn được mã hóa cứng, không thể cung cấp các hướng dẫn đa dạng, mơ hồ và lập kế hoạch dài hạn như con người. Do đó, chúng chỉ có thể xử lý các hướng dẫn đơn giản và cấp thấp như các nhiệm vụ nhặt và đặt. (3) Các phương pháp dựa trên LLM, sử dụng LLM để hỗ trợ hiểu và lập kế hoạch hướng dẫn (Ichter et al. 2022; Liang et al. 2022; Raman et al. 2022). Xem đoạn cuối để biết thêm chi tiết.

3 Kiến thức Cơ bản
3.1 Công thức Vấn đề
Chúng tôi xem xét việc áp dụng một tác nhân dựa trên LLM để giải quyết các nhiệm vụ theo hướng dẫn (IF). Mỗi hướng dẫn I∈T mô tả một nhiệm vụ thô trong môi trường. Được đưa ra hướng dẫn, chỉ khi tác nhân hoàn thành nhiệm vụ bằng cách sử dụng tập hành động nguyên thủy A mới có thể nhận được phần thưởng tích cực từ môi trường. Ví dụ, trong BabyAI, là một tập nhiệm vụ theo hướng dẫn nổi tiếng, các hướng dẫn như "Mở cửa xanh" hoặc "Đặt hộp đỏ bên cạnh quả bóng xanh" chỉ định một số nhiệm vụ thao tác đối tượng vĩ mô, trong khi tác nhân cần hoàn thành chúng trong thế giới lưới bằng cách sử dụng các hành động nguyên thủy như "quay phải", "di chuyển về phía trước", "nhặt" và như vậy.

Một tác nhân dựa trên LLM nhận hướng dẫn I và quan sát môi trường o làm đầu vào và xuất ra các hành động để hoàn thành nhiệm vụ. Như thể hiện trong Hình 1, khung chung của các tác nhân dựa trên LLM chứa một người lập kế hoạch cấp cao và một số kỹ năng cấp thấp có thể đơn vị hóa kiến thức ngữ nghĩa trong LLM để hoàn thành các nhiệm vụ theo hướng dẫn. Được đưa ra hướng dẫn thô, người lập kế hoạch (thường là LLM) sẽ phân tách nó thành một chuỗi hướng dẫn phụ hoặc tạo ra một chương trình để giải quyết nó. Đồng thời, các kỹ năng cấp thấp bao gồm các chính sách được đào tạo trước hoặc các tập lệnh được triển khai trước để thực thi kế hoạch hoặc chương trình. Các nhà nghiên cứu thường giả định môi trường cung cấp mô tả văn bản về trạng thái và phản hồi liên quan đến nhiệm vụ để thích ứng với cài đặt LLM. Với việc dịch ngay lập tức từ đầu ra ngữ nghĩa sang thực thi trong môi trường thông qua các API cấp thấp, người lập kế hoạch cấp cao có thể nhận được phản hồi đầy đủ và lặp đi lặp lại tinh chỉnh kế hoạch của mình để hoàn thành nhiệm vụ.

Các phương pháp hiện tại tận dụng kiến thức ngữ nghĩa trong người lập kế hoạch cấp cao để lập luận và phân tách hướng dẫn thô, nhưng đồng thời, bỏ qua vấn đề căn cứ bằng cách dịch kế hoạch ngữ nghĩa thành triển khai thông qua các API cấp thấp được xác định trước. Để tối đa hóa việc sử dụng các tiền đề trong LLM để giảm nỗ lực của con người, khung của chúng tôi nhằm tự động học các kỹ năng tổng quát trong môi trường để xây dựng các API cấp thấp để giải quyết vấn đề căn cứ. Bên cạnh đó, trong bài báo này, chúng tôi giả định LLM có thể gọi các hàm nhận thức cơ bản P={p1, p2, ...}, như "get_observed_objects()", để lấy trạng thái môi trường thay vì các cơ chế quan sát và phản hồi văn bản được thiết kế trước.

3.2 Thách thức
Vấn đề cốt yếu của việc xây dựng một tác nhân dựa trên LLM là căn cứ kiến thức ngữ nghĩa của LLM trong môi trường. Để tự động giải quyết vấn đề căn cứ, có hai thách thức chính:

• Làm thế nào để có được trải nghiệm căn cứ thành công từ đầu? Không có các API cấp thấp được xác định trước, tác nhân không thể tương tác với môi trường để thử kế hoạch ngữ nghĩa trực tiếp và khám phá hiệu quả phần thưởng của việc hoàn thành nhiệm vụ. Để giải quyết thách thức, chúng tôi làm cho LLM đưa ra giả thuyết về kế hoạch cho mỗi nhiệm vụ và tạo ra các kiểm tra tương ứng cho mỗi bước phụ. Dựa trên những phần thưởng nội tại được cung cấp bởi các kiểm tra, chúng ta có thể nhanh chóng đào tạo các chính sách nhỏ để thực thi và xác minh kế hoạch, sau đó thu thập các quỹ đạo thành công như trải nghiệm căn cứ. Mặc dù trải nghiệm như vậy thu được thông qua các nỗ lực nhanh có thể thuộc về các nhiệm vụ đơn giản, chúng ta cũng có thể tận dụng chúng để tăng cường khả năng căn cứ của tác nhân để hoàn thành các nhiệm vụ phức tạp và dài hạn hơn một cách tiến bộ.

• Làm thế nào để đào tạo hiệu quả các API hành vi cấp thấp tổng quát dưới sự hướng dẫn của kinh nghiệm? Các mục tiêu phụ trong kinh nghiệm thành công được đề xuất bởi LLM dựa trên hướng dẫn cụ thể, và không thể được áp dụng cho các tình huống mới như các API hành vi chung. Lấy cảm hứng từ ý tưởng quy nạp trong toán học, chúng tôi giới thiệu một cơ chế để nhóm các mục tiêu phụ có ngữ nghĩa tương tự lại với nhau, sau đó đào tạo các chính sách kỹ năng có thể đạt được một nhóm mục tiêu phụ như các API hành vi tổng quát. Không giống như học tăng cường trực tuyến tiêu chuẩn, gặp phải vấn đề hiệu quả dữ liệu, chúng tôi làm cho việc đào tạo kỹ năng hiệu quả thông qua phần thưởng dày đặc được cung cấp bởi các kiểm tra, và khôi phục trạng thái ban đầu bằng các quỹ đạo thành công.

4 Phương pháp
Trong phần này, chúng tôi sẽ đưa ra tổng quan về khung Self-Driven Grounding được đề xuất, có thể tự động và tiến bộ căn cứ LLM trong môi trường.

4.1 Tổng quan
Như thể hiện trong Hình 2, SDG có thể được chia thành bốn giai đoạn.
• Giả thuyết: Đối với mỗi hướng dẫn, LLM cố gắng phân tách nó thành các mục tiêu phụ và tạo ra các hàm kiểm tra cho mỗi mục tiêu phụ.
• Xác minh: Dựa trên phần thưởng được cung cấp bởi hàm kiểm tra, chúng tôi đào tạo các chính sách riêng biệt cho mỗi mục tiêu phụ trong các bước giới hạn cho đến khi nhiệm vụ hoàn thành để xác minh tính khả thi của giả thuyết của LLM.
• Quy nạp: Chúng tôi nhóm các mục tiêu phụ trong các giả thuyết thành công có ngữ nghĩa tương tự để đào tạo các kỹ năng tổng quát học tăng cường.
• Suy diễn: Dựa trên các kỹ năng đã học như các tác nhân cấp thấp, chúng tôi sử dụng LLM như một người lập kế hoạch cấp cao few-shot để tạo ra chương trình giải quyết các nhiệm vụ chưa từng thấy và phức tạp hơn.

4.2 Giả thuyết
Giai đoạn giả thuyết nhằm giải quyết các nhiệm vụ riêng biệt bất kể tính tổng quát để thu thập kinh nghiệm căn cứ. Xem xét khoảng cách giữa kiến thức ngữ nghĩa trong LLM và môi trường, giai đoạn giả thuyết phân tách nhiệm vụ thành nhiều mục tiêu phụ thay vì đưa ra giải pháp trực tiếp, và để lại việc xác minh tính đúng đắn cho quá trình tiếp theo. Như thể hiện trong Hình 2(a), giả thuyết có thể được hình thành như I, Prompt → G, F. Chúng tôi sử dụng LLM như người lập kế hoạch zero-shot, nhận một hướng dẫn I và prompt phân tách cần thiết làm đầu vào, sau đó xuất ra một chuỗi mục tiêu phụ G={g1, g2, ...} và các hàm kiểm tra tương ứng F={f1, f2, ...}. Mục tiêu phụ gi là một hướng dẫn nhỏ được gắn nhãn với dấu hiệu rõ ràng "Goal X" để tạo điều kiện cho việc xử lý tiếp theo. Mỗi hàm kiểm tra fi: S → {0,1} là một chương trình kiểm tra việc đạt được mục tiêu phụ tương ứng gi thông qua việc gọi các hàm nhận thức được cung cấp bởi môi trường. Để làm cho LLM xuất ra như chúng ta muốn, ngoài hướng dẫn nhiệm vụ I, chúng tôi thêm định nghĩa vai trò, mô tả API nhận thức, và giải thích về không gian nhiệm vụ theo hướng dẫn vào các prompt phân tách.

4.3 Xác minh
Sau khi có được các mục tiêu phụ và hàm kiểm tra, chúng ta cần xác minh tính khả thi của chúng trong môi trường để thu thập kinh nghiệm căn cứ thành công. Tính khả thi của việc phân tách được xác minh bằng sự nhất quán giữa các tín hiệu đạt được do các hàm kiểm tra tạo ra và tín hiệu hoàn thành nhiệm vụ. Cụ thể, như thể hiện trong Hình 2(b), chúng tôi đào tạo các chính sách độc lập cho mỗi mục tiêu phụ dựa trên phần thưởng bool được cung cấp bởi hàm kiểm tra của nó. Một khi mục tiêu phụ được đạt được, chúng tôi dừng việc đào tạo và lưu chuỗi hành động. Chuỗi hành động đã lưu có thể được sử dụng như cơ chế khôi phục để chuẩn bị trạng thái ban đầu cho việc đào tạo kỹ năng tiếp theo. Cho đến khi tất cả mục tiêu phụ được đạt được và nhiệm vụ cũng được hoàn thành, chúng ta có thể xác minh việc phân tách là thành công. Đồng thời, các kinh nghiệm căn cứ (bao gồm mô tả mục tiêu phụ, hàm kiểm tra, và chuỗi hành động khôi phục) được thu thập cho việc học kỹ năng trong giai đoạn tương lai. Xem xét một số nhiệm vụ phức tạp và dài hạn không thể được giải quyết bằng việc phân tách trực tiếp, cơ chế trên chỉ phù hợp để giải quyết các nhiệm vụ đơn giản trong các bước tương tác môi trường có thể chấp nhận được để thu thập kinh nghiệm. Chúng tôi đặt số bước xác minh tối đa Tverify như một ngưỡng để phân biệt các nhiệm vụ phức tạp khó xử lý cho giai đoạn tiếp theo để giải quyết.

4.4 Quy nạp
Sau khi thu thập kinh nghiệm căn cứ thành công, giai đoạn quy nạp nhằm khám phá và học các kỹ năng tổng quát từ các quỹ đạo căn cứ riêng biệt của các hướng dẫn khác nhau để chúng ta có thể tái sử dụng chúng trong các nhiệm vụ chưa từng thấy và phức tạp hơn.

Khám phá Như được mô tả ở trên, chúng ta đã thu thập kinh nghiệm giải quyết nhiệm vụ thông qua giả thuyết và xác minh hiệu quả, bao gồm mô tả mục tiêu phụ, các hàm kiểm tra tương ứng, và chuỗi hành động khôi phục trạng thái bắt đầu. Tuy nhiên, những kinh nghiệm thành công như vậy chỉ có thể được sử dụng trên các hướng dẫn cụ thể dễ phân tách cho LLM. Để làm cho tác nhân dựa trên LLM có thể giải quyết các nhiệm vụ chưa từng thấy và phức tạp hơn, chúng ta phải trừu tượng hóa và học các kỹ năng hơn nữa để xây dựng tác nhân cấp thấp tổng quát. Để làm điều này, chúng tôi nhóm các mục tiêu phụ đã thu thập theo ngữ nghĩa của chúng để đảm bảo một kỹ năng tổng quát nhất định có thể hoàn thành một loại mục tiêu phụ như thể hiện trong Hình 2(c). Cụ thể, chúng tôi đầu tiên sử dụng LLM để dịch mỗi mô tả mục tiêu phụ gi thành mô tả API gi,api và tham số gi,param. Ví dụ, mục tiêu phụ "khám phá hộp xanh" được dịch thành API "khám phá" và tham số "hộp xanh". Sau đó chúng tôi sử dụng thuật toán k-means để thực hiện phân cụm không giám sát dựa trên khoảng cách ngữ nghĩa giữa các mô tả mục tiêu phụ được tính bằng độ tương tự cosine sau:

C(gi,api, gj,api) = emb(gi,api)·emb(gj,api) / (||emb(gi,api)|| · ||emb(gj,api)||) (1)

trong đó gi,api là mô tả API và emb(·) là hàm nhúng của LLM.

Đào tạo Chúng tôi đã chia các mục tiêu phụ có ngữ nghĩa tương tự thành các loại khác nhau thông qua quá trình phân cụm. Sau đó chúng tôi xây dựng môi trường học tăng cường (RL) để đào tạo các kỹ năng có thể đạt được một cụm mục tiêu phụ riêng biệt. Không giống như các môi trường RL thông thường, môi trường đào tạo kỹ năng giống như một tình huống học đa nhiệm vụ. Mỗi mục tiêu phụ của một cụm có thể được xem như một nhiệm vụ đơn lẻ, tham số mục tiêu phụ gi,param là mô tả nhiệm vụ, phần thưởng được cung cấp bởi hàm kiểm tra tương ứng fi, và trạng thái ban đầu được thiết lập bởi chuỗi hành động khôi phục đã lưu. Được đào tạo trong môi trường nhiều nhiệm vụ như vậy bao gồm các mục tiêu phụ có cùng ngữ nghĩa, kỹ năng được cho là có tính tổng quát trong các nhiệm vụ có tình huống tương tự. Bên cạnh đó, các mục tiêu phụ thuộc về cùng một cụm được chia thành tập đào tạo và xác minh để theo dõi khả năng tổng quát của kỹ năng được đào tạo để ngăn chặn overfitting.

4.5 Suy diễn
Thông qua quá trình trên, chúng ta đã vượt qua những thách thức của việc có được kinh nghiệm căn cứ thành công từ đầu và đào tạo các kỹ năng tổng quát một cách hiệu quả. Nói cách khác, chúng ta đã tự động xây dựng các kỹ năng cấp thấp cho tác nhân dựa trên LLM mà không cần nỗ lực của con người. Để áp dụng các kỹ năng được học tự động để giải quyết các nhiệm vụ theo hướng dẫn chưa từng thấy và phức tạp, chúng tôi tiếp theo giới thiệu người lập kế hoạch cấp cao trong phần này. Như thể hiện trong Hình 2(d), chúng tôi sử dụng LLM như một người lập kế hoạch few-shot để tạo ra chương trình hoàn thành nhiệm vụ. Quá trình có thể được chia thành giai đoạn tạo chương trình và giai đoạn gỡ lỗi.

Tạo ra Prompt tạo ra cho LLM chứa định nghĩa vai trò và mô tả API (bao gồm kỹ năng và hàm nhận thức). Bên cạnh đó, xem xét sự phức tạp của nhiệm vụ, prompt tạo ra tuân theo mô hình học trong ngữ cảnh few-shot. Chúng tôi bao gồm một số mô tả API kỹ năng và một ví dụ viết tay tận dụng các kỹ năng đã học để giải quyết một nhiệm vụ theo hướng dẫn phức tạp.

Gỡ lỗi Để giải quyết tốt hơn các nhiệm vụ phức tạp, chúng tôi cũng thiết kế một quá trình gỡ lỗi tương tác trong người lập kế hoạch cấp cao của chúng tôi. Bên cạnh hướng dẫn nhiệm vụ và chương trình được tạo ra cần sửa đổi, prompt gỡ lỗi cũng bao gồm thông báo lỗi và một số gợi ý gỡ lỗi chung để sửa các lỗi có thể có. Nhờ vào tính khả thi và độ bền của các kỹ năng học thích ứng, cơ chế báo cáo lỗi cơ bản dựa trên phát hiện hành động bất hợp pháp có thể cải thiện hiệu quả độ chính xác của các chương trình được tạo ra, điều này làm giảm đáng kể nỗ lực của con người.

5 Kết quả
5.1 Cài đặt Thí nghiệm
Môi trường Để đánh giá hiệu quả và hiệu suất của khung được đề xuất tự động khám phá, học và áp dụng kỹ năng, chúng tôi kiểm tra SDG trên môi trường BabyAI (Chevalier-Boisvert et al. 2019). BabyAI là một môi trường thế giới lưới cho việc theo hướng dẫn. Được đưa ra hướng dẫn ngôn ngữ và một góc nhìn cục bộ và một phần 7×7×3, tác nhân phải học cách hoàn thành các nhiệm vụ khác nhau với mức độ khó tùy ý. Trong bài báo này, chúng tôi chọn sáu cấp độ hướng dẫn sau với các loại và độ khó khác nhau (chi tiết thêm về môi trường có thể được tìm thấy trong Phụ lục):

• GoToLocal: Đi đến một đối tượng bên trong một phòng duy nhất.
• PickupLocal: Nhặt một đối tượng bên trong một phòng duy nhất.
• PutNextLocal: Nhặt một đối tượng và đặt nó bên cạnh một đối tượng khác bên trong một phòng duy nhất.
• Open: Mở một cửa trong mê cung phòng 3×3, cửa có thể ở phòng khác.
• SynthSeq: Hợp nhất tất cả hướng dẫn từ PutNext, Open, Goto, và PickUp và có thể với nhiều lệnh.
• BossLevel: Nhiệm vụ khó nhất của BabyAI như thể hiện trong 3. Lệnh có thể là bất kỳ câu nào được rút ra từ ngữ pháp Ngôn ngữ Baby.

Đường cơ sở Chúng tôi xác minh hiệu quả của SDG bằng cách so sánh nó với một số đường cơ sở, bao gồm các phương pháp Học Mô phỏng dựa vào các minh chứng chuyên gia và một biến thể của tác nhân dựa trên LLM của chúng tôi.

• Original: Đường cơ sở từ bài báo BabyAI gốc, đã đào tạo mô hình GRU + CONV với học mô phỏng sử dụng một triệu tập minh chứng cho mỗi cấp độ.
• LID-Text (Li et al. 2022): Một phương pháp đầu tiên đại diện cho mục tiêu và quan sát như một chuỗi nhúng, sau đó sử dụng một mạng chính sách được khởi tạo với LM được đào tạo trước và được đào tạo với các minh chứng để dự đoán hành động tiếp theo.
• LISA (Garg et al. 2022): Một khung học mô phỏng phân cấp có thể học các kỹ năng đa dạng, có thể diễn giải từ các minh chứng có điều kiện ngôn ngữ.
• SDG-action: Biến thể của khung của chúng tôi duy trì cùng một người lập kế hoạch cấp cao nhưng sử dụng các hành động nguyên thủy thay vì các kỹ năng đã thu được.

Triển khai Trong bài báo này, chúng tôi sử dụng ChatGPT (GPT-3.5-turbo) như mô hình ngôn ngữ lớn để hoàn thành việc phân tách nhiệm vụ, nhúng ngữ nghĩa của API, lập kế hoạch cấp cao, và gỡ lỗi. Chi tiết thêm về nội dung prompt được thể hiện trong Phụ lục. Trong giai đoạn xác minh và học kỹ năng, chúng tôi sử dụng mô hình tiêu chuẩn được đề xuất trong BabyAI, và đào tạo chính sách bằng thuật toán PPO.

5.2 So sánh Kết quả Tổng thể
Kết quả hiệu suất chính được thể hiện trong Bảng 1. Chúng tôi riêng biệt so sánh SDG với các đường cơ sở trong mỗi nhiệm vụ cấp độ. Đối với mỗi nhiệm vụ cấp độ, chúng tôi lấy mẫu ngẫu nhiên 100 hướng dẫn không bao giờ xuất hiện trong giai đoạn đào tạo kỹ năng. Xem xét tính ngẫu nhiên của câu trả lời của ChatGPT, chúng tôi lặp lại thí nghiệm của mỗi hướng dẫn 3 lần để có được kết quả trung bình. Kết quả cho thấy SDG có thể đạt được hiệu suất tương đương bằng cách sử dụng các kỹ năng được học tự động thay vì một số lượng lớn các minh chứng chuyên gia, điều này thể hiện hiệu quả của khung của chúng tôi. Bên cạnh đó, kết quả của SDG-action cho thấy nó thất bại trong việc hoàn thành các nhiệm vụ khó hơn mà không có kỹ năng đã thu được, điều này cũng nhấn mạnh tầm quan trọng của việc học kỹ năng tự điều khiển trong tình huống căn cứ LLM.

5.3 Kết quả Nghiên cứu Loại bỏ
Nghiên cứu Loại bỏ Học Kỹ năng Chúng tôi đầu tiên điều tra hiệu quả dữ liệu khi học kỹ năng thông qua học tăng cường như thể hiện trong Hình 4. Mặc dù độ khó khác nhau vì quá trình phân cụm chỉ dựa vào ngữ nghĩa của các mục tiêu phụ và bỏ qua độ khó, các kỹ năng có thể được học một cách hiệu quả. Giai đoạn xác minh chứng minh rằng hướng dẫn đã được phân tách thành các mục tiêu phụ đủ nhỏ và có thể được đào tạo trong các bước giới hạn. Do đó, môi trường đào tạo kỹ năng bao gồm các mục tiêu phụ đã được xác minh có thể dẫn đến một quá trình đào tạo kỹ năng hiệu quả. Bên cạnh đó, kết quả cũng chứng minh rằng việc khôi phục các hành động làm cho kỹ năng bắt đầu ở trạng thái mong đợi là quan trọng đối với hiệu quả học. Không có việc đặt lại trạng thái bắt đầu, hiệu quả học giảm rõ rệt, và một số kỹ năng thậm chí không thể được học. Một số đường cong học thể hiện xu hướng overfitting ở giai đoạn cuối, khác với quá trình học tăng cường bình thường và Minh họa vai trò của tập xác thực giữ lại. Đối với một số kỹ năng như "vào phòng chưa khám phá", các đường cong xanh có vẻ tốt hơn các đường cong xanh dương. Điều này là vì không khôi phục trạng thái ban đầu, độ khó giảm rất nhiều vì tác nhân có thể vào bất kỳ phòng nào.

Nghiên cứu Loại bỏ Suy diễn Chúng tôi cũng khám phá hiệu quả của số lần gỡ lỗi tương tác và nhiều lần thử của kỹ năng. Kết quả nghiên cứu loại bỏ cho thấy tầm quan trọng của gỡ lỗi tương tác và nhiều lần thử.

A. Số lần Tương tác: Hình 5 cho thấy kết quả của các lần tương tác khác nhau giữa người lập kế hoạch cấp cao và môi trường. Đối với một số nhiệm vụ như "GoToLocal", việc thúc đẩy tỷ lệ thành công bị hạn chế vì tính đơn giản. Đối với các nhiệm vụ phức tạp như "BossLevel", việc gỡ lỗi lặp lại có thể mang lại hơn 40% cải thiện, điều này thể hiện khả năng thích ứng và tính khả thi của các kỹ năng đã học. Tuy nhiên, khi hiệu suất đạt đến một giới hạn trần nhất định, tương tác nhiều hơn có vẻ vô ích.

B. Nhiều Lần thử: Khác với các API được xác định trước với các tập lệnh, các kỹ năng đã học của chúng tôi là các chính sách ngẫu nhiên. Do đó, chúng tôi cũng điều tra hiệu quả của nhiều lần thử chính sách kỹ năng trên tỷ lệ thành công cuối cùng. Hình 6 cho thấy kết quả tương tự với số lần tương tác, nhiều lần thử cải thiện các nhiệm vụ phức tạp nhiều hơn.

5.4 Chi tiết Phương pháp
Để thể hiện thêm cái nhìn sâu sắc về SDG, chúng tôi hiển thị một số kết quả trung gian chính.

Xác minh Nhiệm vụ Hình 7 cho thấy kết quả xác minh của các nhiệm vụ cấp độ khác nhau. Trong triển khai, chúng tôi lấy mẫu ngẫu nhiên 100 hướng dẫn từ mỗi nhiệm vụ cấp độ, và đặt ngưỡng bước xác minh Tverify bằng 3000. Kết quả chứng minh giả định của chúng tôi, đối với một số cấp độ nhiệm vụ đơn giản, như "GoToLocal", "PickupLoc" và "PutNextLocal", LLM có thể phân tách hướng dẫn thành các mục tiêu phụ và hàm kiểm tra hợp lý, để việc đào tạo xác minh thành công trong các bước giới hạn. Đối với các cấp độ khó cũng bao gồm một số nhiệm vụ đơn giản, việc phân tách trực tiếp có thể hoàn thành ít hướng dẫn, có nghĩa là nó không thể giải quyết các nhiệm vụ phức tạp và dài hạn.

Phân cụm Kỹ năng Trong giai đoạn khám phá kỹ năng, chúng tôi sử dụng thuật toán phân cụm k-means để nhóm các mục tiêu phụ thành các nhóm khác nhau dựa trên độ tương tự ngữ nghĩa của chúng. Về việc chọn k, chúng tôi tính điểm Calinski-Harabasz (CH), là một chỉ số thường được sử dụng cho kết quả phân cụm, từ k= 2 đến 16 như thể hiện trong Hình 8, và cuối cùng chọn k= 9 với điểm CH cao nhất. Chúng tôi cũng thể hiện kết quả phân cụm của nhúng ngữ nghĩa thông qua kỹ thuật TSNE trong Hình 9. Tuy nhiên, việc phân cụm không giám sát có thể gây ra một số bất định trong quá trình khám phá kỹ năng, chúng tôi để lại việc điều tra thêm và giảm thiểu hiệu ứng của kết quả phân cụm trên khung của chúng tôi như công việc tương lai.

6 Kết luận
Trong bài báo này, chúng tôi đề xuất một khung gọi là Self-Driven Grounding (SDG) để giải quyết thách thức tự động căn cứ LLM vào các môi trường cụ thể. Để giảm bớt vấn đề thu thập kinh nghiệm căn cứ, chúng tôi làm cho LLM không chỉ phân tách nhiệm vụ mà còn tạo ra phần thưởng nội tại để giúp các tác nhân RL xác minh hiệu quả kết quả phân tách. Chúng tôi cũng đề xuất một phương pháp học kỹ năng chung căn chỉnh ngôn ngữ bằng cách buộc mỗi kỹ năng đạt được một nhóm mục tiêu có mô tả ngữ nghĩa tương tự để tăng cường tính tổng quát của chúng. So với các phương pháp học mô phỏng tốn hàng triệu minh chứng, SDG có thể đạt được hiệu suất tương đương trong các nhiệm vụ khó nhất trong BabyAI. Nghiên cứu loại bỏ cũng thể hiện tính linh hoạt và khả thi của các kỹ năng đã học trong các tương tác giữa người lập kế hoạch cấp cao và môi trường.

Tuy nhiên, bị hạn chế bởi cách thức đơn giản nhận thức trạng thái môi trường, SDG chỉ có thể xử lý các nhiệm vụ có mô tả văn bản. Vấn đề này có thể được giải quyết bằng cách giới thiệu LLM đa phương thức, điều này sẽ mở rộng các ứng dụng của SDG. Bên cạnh đó, SDG chỉ chứa một chu kỳ duy nhất của giả thuyết, xác minh, và quy nạp. Đó là một hướng thú vị và đầy hứa hẹn để thiết kế một cơ chế của nhiều chu kỳ trong SDG, cho phép SDG học các kỹ năng phân cấp mạnh mẽ và đa dạng hơn để hoàn thành các nhiệm vụ linh hoạt hơn.

--- TRANG 8 ---
Tài liệu Tham khảo
2020. 2020 IEEE Symposium Series on Computational Intelligence, SSCI 2020, Canberra, Australia, December 1-4, 2020. IEEE. ISBN 978-1-7281-2547-3.

Artzi, Y.; và Zettlemoyer, L. 2013. Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions. Trans. Assoc. Comput. Linguistics, 1: 49–62.

Bahdanau, D.; Hill, F.; Leike, J.; Hughes, E.; Hosseini, S. A.; Kohli, P.; và Grefenstette, E. 2019. Learning to Understand Goal Specifications by Modelling Reward. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.

Carta, T.; Romac, C.; Wolf, T.; Lamprier, S.; Sigaud, O.; và Oudeyer, P. 2023. Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning. CoRR, abs/2302.02662.

Chaplot, D. S.; Sathyendra, K. M.; Pasumarthi, R. K.; Rajagopal, D.; và Salakhutdinov, R. 2018. Gated-Attention Architectures for Task-Oriented Language Grounding. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, 2819–2826.

Chen, V.; Gupta, A.; và Marino, K. 2021. Ask Your Humans: Using Human Instructions to Improve Generalization in Reinforcement Learning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.

Chevalier-Boisvert, M.; Bahdanau, D.; Lahlou, S.; Willems, L.; Saharia, C.; Nguyen, T. H.; và Bengio, Y. 2019. BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.

Driess, D.; Xia, F.; Sajjadi, M. S. M.; Lynch, C.; Chowdhery, A.; Ichter, B.; Wahid, A.; Tompson, J.; Vuong, Q. H.; Yu, T.; Huang, W.; Chebotar, Y.; Sermanet, P.; Duckworth, D.; Levine, S.; Vanhoucke, V.; Hausman, K.; Toussaint, M.; Greff, K.; Zeng, A.; Mordatch, I.; và Florence, P. R. 2023. PaLM-E: An Embodied Multimodal Language Model. ArXiv, abs/2303.03378.

Garg, D.; Vaidyanath, S.; Kim, K.; Song, J.; và Ermon, S. 2022. LISA: Learning Interpretable Skill Abstractions from Language. ArXiv, abs/2203.00054.

Huang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; Florence, P.; Zeng, A.; Tompson, J.; Mordatch, I.; Chebotar, Y.; Sermanet, P.; Jackson, T.; Brown, N.; Luu, L.; Levine, S.; Hausman, K.; và Ichter, B. 2022. Inner Monologue: Embodied Reasoning through Planning with Language Models. In Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, 1769–1782.

Ichter, B.; Brohan, A.; Chebotar, Y.; Finn, C.; Hausman, K.; Herzog, A.; Ho, D.; Ibarz, J.; Irpan, A.; Jang, E.; Julian, R.; Kalashnikov, D.; Levine, S.; Lu, Y.; Parada, C.; Rao, K.; Sermanet, P.; Toshev, A.; Vanhoucke, V.; Xia, F.; Xiao, T.; Xu, P.; Yan, M.; Brown, N.; Ahn, M.; Cortes, O.; Sievers, N.; Tan, C.; Xu, S.; Reyes, D.; Rettinghouse, J.; Quiambao, J.; Pastor, P.; Luu, L.; Lee, K.; Kuang, Y.; Jesmonth, S.; Joshi, N. J.; Jeffrey, K.; Ruano, R. J.; Hsu, J.; Gopalakrishnan, K.; David, B.; Zeng, A.; và Fu, C. K. 2022. Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. In Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, 287–318.

Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; và Iwasawa, Y. 2022. Large Language Models are Zero-Shot Reasoners. ArXiv, abs/2205.11916.

Li, S.; Puig, X.; Paxton, C.; Du, Y.; Wang, C.; Fan, L.; Chen, T.; Huang, D.; Akyürek, E.; Anandkumar, A.; Andreas, J.; Mordatch, I.; Torralba, A.; và Zhu, Y. 2022. Pre-Trained Language Models for Interactive Decision-Making. In NeurIPS.

Liang, J.; Huang, W.; Xia, F.; Xu, P.; Hausman, K.; Ichter, B.; Florence, P.; và Zeng, A. 2022. Code as Policies: Language Model Programs for Embodied Control. CoRR, abs/2209.07753.

Lynch, C.; và Sermanet, P. 2021. Language Conditioned Imitation Learning Over Unstructured Data. In Robotics: Science and Systems XVII, Virtual Event, July 12-16, 2021.

Misra, D. K.; Sung, J.; Lee, K.; và Saxena, A. 2016. Tell me Dave: Context-sensitive grounding of natural language to manipulation instructions. Int. J. Robotics Res., 35(1-3): 281–300.

OpenAI. 2023. GPT-4 Technical Report. ArXiv, abs/2303.08774.

Raman, S. S.; Cohen, V.; Rosen, E.; Idrees, I.; Paulius, D.; và Tellex, S. 2022. Planning with Large Language Models via Corrective Re-prompting. CoRR, abs/2211.09935.

Ranzato, M.; Beygelzimer, A.; Dauphin, Y. N.; Liang, P.; và Vaughan, J. W., eds. 2021. Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual.

Röder, F.; Eppe, M.; và Wermter, S. 2022. Grounding Hindsight Instructions in Multi-Goal Reinforcement Learning for Robotics. In IEEE International Conference on Development and Learning, ICDL 2022, London, United Kingdom, September 12-15, 2022, 170–177.

Wang, G.; Xie, Y.; Jiang, Y.; Mandlekar, A.; Xiao, C.; Zhu, Y.; Fan, L.; và Anandkumar, A. 2023. Voyager: An Open-Ended Embodied Agent with Large Language Models. CoRR, abs/2305.16291.

Wang, R.; Jansen, P. A.; Côté, M.; và Ammanabrolu, P. 2022. ScienceWorld: Is your Agent Smarter than a 5th Grader? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, 11279–11298.

Wei, J.; Bosma, M.; Zhao, V.; Guu, K.; Yu, A. W.; Lester, B.; Du, N.; Dai, A. M.; và Le, Q. V. 2021. Finetuned Language Models Are Zero-Shot Learners. ArXiv, abs/2109.01652.

Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; hsin Chi, E. H.; Xia, F.; Le, Q.; và Zhou, D. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. ArXiv, abs/2201.11903.

Yuan, H.; Zhang, C.; Wang, H.; Xie, F.; Cai, P.; Dong, H.; và Lu, Z. 2023. Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks. CoRR, abs/2303.16563.

Hình 10: Các bản đồ khác nhau trong BabyAI

A. Môi trường BabyAI
BabyAI là một môi trường thế giới lưới cho các điều tra theo hướng dẫn, bao gồm 19 cấp độ nhiệm vụ với độ khó tăng dần. Tất cả hướng dẫn trong BabyAI được tạo ra thông qua một ngữ pháp tổng hợp phong phú tổ hợp. Xem xét một số cấp độ lặp lại nhau hoặc quá cụ thể cho một cảnh duy nhất, như "GoToRedBall", Chúng tôi đã chọn 4 tình huống nhiệm vụ đơn lẻ đại diện và 2 cấp độ nhiệm vụ kết hợp thách thức. Có hai loại bản đồ trong BabyAI, phòng đơn hoặc phòng 3×3 như thể hiện trong Hình 10.

Bảng 2 sau đây cho thấy chi tiết của mỗi cấp độ nhiệm vụ.

B. Chi tiết Giả thuyết
Như được mô tả trong bài báo, trong giai đoạn giả thuyết, HYVIN làm cho LLM không chỉ phân tách hướng dẫn thành các mục tiêu phụ mà còn các hàm kiểm tra tương ứng. Prompt bao gồm định nghĩa vai trò, định nghĩa nhiệm vụ, mô tả hàm nhận thức, và ràng buộc định dạng đầu ra.

Định nghĩa Vai trò
Có một robot trong mê cung, bao gồm nhiều phòng được kết nối bởi các cửa. Một số đối tượng ở trong phòng, như hộp, bóng, cửa, và chìa khóa. Bạn là một trợ lý tốt bụng giúp hướng dẫn robot hoàn thành một số nhiệm vụ thao tác đối tượng. Vui lòng trả lời "Có" nếu bạn hiểu vai trò của mình, và sau đó tôi sẽ cung cấp cho bạn thông tin chi tiết hơn.

Định nghĩa Nhiệm vụ
Các nhiệm vụ như "đi đến X", robot nên đầu tiên khám phá X, và sau đó đi bên cạnh X.
Các nhiệm vụ như "nhặt X", robot nên đầu tiên "đi đến X", sau đó nhặt X.
Các nhiệm vụ như "đặt X bên cạnh Y", robot nên đầu tiên "nhặt X", sau đó khám phá Y, và cuối cùng đặt X bên cạnh Y.
Các nhiệm vụ như "mở X", robot nên xem xét cửa X có thể ở trong phòng hiện tại hoặc phòng khác. Robot nên đầu tiên cố gắng khám phá, đi đến, và mở cửa X trong phòng hiện tại. Nếu không, robot nên đi đến một phòng mới.

Cụ thể, nó nên tìm một cửa chưa khám phá Y, và sau đó vào phòng mới mà cửa Y dẫn đến.

Mô tả Hàm Nhận thức
Bạn có thể sử dụng các API Python sau để lấy trạng thái của robot.

[Nội dung mô tả API được giữ nguyên như trong bản gốc]

Ràng buộc Định dạng Đầu ra
Sau khi robot được giao nhiệm vụ, công việc của bạn là:
(1) phân tách hướng dẫn nhiệm vụ thành nhiều mục tiêu để giúp robot hoàn thành nhiệm vụ. Mỗi mục tiêu nên nhỏ và dễ kiểm tra.
(2) viết các hàm Python riêng biệt sử dụng hai API trên để kiểm tra liệu robot có đạt được mỗi mục tiêu hay không. Chỉ hàm kiểm tra, và không triển khai để giúp robot.

Đầu ra của bạn nên tuân theo định dạng này:
[Format được giữ nguyên]

Vui lòng trả lời "Có" nếu bạn hiểu các API trên và công việc của bạn, và sau đó tôi sẽ cung cấp cho bạn nhiệm vụ thao tác đối tượng.

[Các phần còn lại được dịch tương tự, giữ nguyên cấu trúc và nội dung của bản gốc]
