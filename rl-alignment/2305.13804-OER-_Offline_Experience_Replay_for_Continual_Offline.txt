# 2305.13804.pdf
# Converted from PDF to TXT
# Source path: /home/admin88/arxiv-downloader/rl-alignment/2305.13804.pdf
# File size: 1474861 bytes

===============================================
PDF FILE CONTENT
===============================================


--- PAGE 1 ---
OER: Offline Experience Replay for Continual Offline
Reinforcement Learning
Sibo Gaiab;*, Donglin Wangb;**and Li Heb
aFudan University, Shanghai, China
bWestlake University, Zhejiang, China
Abstract. The capability of continuously learning new skills via
a sequence of pre-collected offline datasets is desired for an agent.
However, consecutively learning a sequence of offline tasks likely
leads to the catastrophic forgetting issue under resource-limited sce-
narios. In this paper, we formulate a new setting, continual offline
reinforcement learning (CORL), where an agent learns a sequence of
offline reinforcement learning tasks and pursues good performance
on all learned tasks with a small replay buffer without exploring any
of the environments of all the sequential tasks. For consistently learn-
ing on all sequential tasks, an agent requires acquiring new knowl-
edge and meanwhile preserving old knowledge in an offline manner.
To this end, we introduced continual learning algorithms and exper-
imentally found experience replay (ER) to be the most suitable al-
gorithm for the CORL problem. However, we observe that introduc-
ing ER into CORL encounters a new distribution shift problem: the
mismatch between the experiences in the replay buffer and trajecto-
ries from the learned policy. To address such an issue, we propose
a new model-based experience selection (MBES) scheme to build
the replay buffer, where a transition model is learned to approximate
the state distribution. This model is used to bridge the distribution
bias between the replay buffer and the learned model by filtering
the data from offline data that most closely resembles the learned
model for storage. Moreover, in order to enhance the ability on learn-
ing new tasks, we retrofit the experience replay method with a new
dual behavior cloning (DBC) architecture to avoid the disturbance
of behavior-cloning loss on the Q-learning process. In general, we
call our algorithm offline experience replay (OER). Extensive exper-
iments demonstrate that our OER method outperforms SOTA base-
lines in widely-used Mujoco environments.
1 Introduction
Similar to human beings, a general-purpose intelligence agent is ex-
pected to learn new tasks continually. Such sequential tasks can be ei-
ther online tasks learned through exploration or offline tasks learned
through offline datasets, where the latter is equally important but has
not drawn sufficient attention so far. Learning sequential tasks in of-
fline setting can greatly improve the learning efficiency and avoid the
dangerous exploration process in practice. Moreover, online learning
sequential tasks is not always feasible due to the temporal and spatial
constraints of the environment and the agent’s on-board considera-
tion itself. Therefore, studying offline reinforcement learning in the
∗Email: gaisibo@westlake.edu.cn
∗∗Corresponding Author. Email: wangdonglin@westlake.edu.cncontinual setting is quite important and valuable for general-purpose
intelligence. If having sufficient computational resources, it is easy
to accomplish such a goal. However, for an agent with limited re-
sources1, continual learning methods are indispensable to deal with
such offline datasets. Consequently, we propose a new setting named
continual offline reinforcement learning (CORL) in this paper, which
integrates offline RL and continual learning.
Offline RL learns from pre-collected datasets instead of directly
interacting with the environment [11]. By leveraging pre-collected
datasets, offline RL methods avoid costly interactions and thus en-
hance learning efficiency and safety. However, offline RL methods
suffer from the over-estimation problem of the out-of-distribution
(OOD) data, where the unseen data are erroneously estimated to be
high values. This phenomenon stems from the distribution shift be-
tween the behavior policy and the learning policy. Various methods
have been proposed to address the over-estimation problem [11, 21].
In this paper, we focus on dealing with a sequence of offline datasets,
which requires continual learning techniques.
The major challenge of continual learning is how to alleviate the
catastrophic forgetting [31] issue on previous tasks when learning
new tasks. There are three types of continual learning methods,
including regularization-based methods [19, 53], modular methods
[8, 30], and rehearsal-based methods [27, 4]. Experience replay (ER)
is a widely-used rehearsal-based method [42], which alternates be-
tween learning a new task and replaying samples of previous tasks.
In this paper, we consider using ER as a base for our own problem.
We will show that ER is the most suitable algorithm for the CORL
problem in the experiment section following.
However, since ER is designed for online continual RL, directly
applying ER in our CORL yields poor performance due to two kinds
of distribution shifts. The first is the distribution shift between the
behavior policy and the learning policy, and the second is the distri-
bution shift between the selected replay buffer and the correspond-
ing learned policy. Existing methods focus on addressing the first
shift issue, and no related work considers the second, which only
appears in our CORL setting. Thus, simply integrating ER with Of-
fline RL fails to alleviate the catastrophic forgetting. To solve the
new problem above, we propose a novel model-based experience se-
lection (MBES) method to fill the replay buffer. The key idea is to
take advantage of the dynamic model to search for and add the most
valuable episodes in the offline dataset into the replay buffer.
After having a good replay buffer for previous tasks, the learned
1Running on a server and communicating with the agent are undesirable due
to complex-system, real-time, and data privacy issues.arXiv:2305.13804v2  [cs.LG]  20 Apr 2024

--- PAGE 2 ---
policy corresponding to a new task needs to clone previous tasks.
Behavior cloning (BC) as an online ER method is widely used [54],
which is incompatible with the actor-critic architecture in offline RL.
Even though we can carefully tune the weight of BC loss, tuning the
hyper-parameter is a cumbersome task in general and is difficult in
the offline setting. Therefore, integrating online ER with offline RL
often derives a non-convergent policy. The reason is that the actor-
critic model is hard to train [39], and the rehearsal term in the loss
function has a negative effect on the learning process. To effectively
replay experiences, we propose a dual behavior cloning (DBC) archi-
tecture instead to resolve the optimization conflict, where one policy
optimizes the performance of the new task by using actor-critic ar-
chitecture, and the second optimizes from the continual perspective
for both new and learned tasks.
In summary, this paper considers investigating a new setting
CORL. A novel MBES is proposed to select valuable experiences
and overcome the mismatch between the experiences in the replay
buffer and trajectories from the learned policy. Then, a DBC archi-
tecture is proposed to deal with the optimization conflict problem.
By taking MBES and DBC as two key ideas for CORL setting, we
name our overall scheme as offline experience replay (OER). The
main contributions of this paper can be summarized as follows:
•We present a new setting CORL and then propose a novel scheme
OER for CORL setting.
•We propose a novel selection method MBES for offline replay
buffer by utilizing a dynamic model to reduce the distribution shift
between experiences from replay buffer and learned policy.
•On the other hand, we propose a novel DBC architecture to pre-
vent the learning process from collapsing by separating the Q-
learning on current task and BC processes on all previous tasks.
•We experimentally verify the performance of different modules
and evaluate our method on continuous control tasks. Our method
OER outperforms all SOTA baselines for all cases.
2 Related Works
Offline Reinforcement Learning Offline RL learns from a col-
lected offline dataset and suffers from the issue of out-of-distribution
(OOD). Some prior works propose to constrain the learned policy to-
wards the behavior policy by adding KL-divergence[38, 36, 45, 55],
MSE [6], or the regularization of the action selection [21]. Some arti-
cles suggest that if the collected data is sub-optimal, these approaches
usually perform not well [29]. But other works point out that adding
a supervised learning term to the policy improvement objective [10]
will also receive high performance by reducing the exploration. An-
other effective way is to learn a conservative Q-function [22, 28, 20],
which assigns a low Q-value for OOD states and then extracts the
corresponding greedy policy. Moreover, other works propose to use
an ensemble model to estimate Q-value [1] or consider the impor-
tance sampling [38]. Such methods have not previously considered
the setting of sequential tasks and naive translating them into CORL
setting is ineffective, while this paper focuses on sequential tasks and
aims to solve the catastrophic forgetting problem during the process
of learning a sequence of offline RL tasks.
On the other hand, recent works [5, 24] propose to train a dynamic
model to predict the values of OOD samples in a supervised-learning
way. Such model-based offline RL methods offer great potential for
solving the OOD problem, even though the transition model is hardly
accurate strictly. The model algorithm is thought to alleviate the
OOD problem faced by offline RL and thus improve the robustnessof the offline agent. Model-based offline RL methods have two major
categories: one focuses on measuring the uncertainty of the learned
dynamic model [52, 17], and the other considers the pessimistic es-
timation [51]. Different from most of these works using the dynamic
model to generate OOD samples when training the agent, in this pa-
per, we utilize the dynamic model to search for the most valuable
episodes in the offline dataset for the ER method.
Continual Reinforcement Learning Offline methods may con-
sider a single-task or multi-task scenario [50, 25, 26]. In contrast,
continual learning attempts to learn new tasks after having learned
old tasks and get the best possible results on all tasks. Generally, con-
tinual learning methods can be classified into three categories [37]:
regularization-based approaches [19, 53] add a regularization term
to prevent the parameters from far from the value learned from past
tasks; modular approaches [8, 30] consider fixed partial parameters
for a dedicated task; and rehearsal-based methods [27, 4] train an
agent by merging the data of previously learned tasks with that of the
current task. All three kinds of continual learning methods have been
applied for RL tasks [15, 48, 32, 23]. Specifically, our work is based
on the rehearsal method in an RL setting [42, 14]. Therefore, we will
detail the works involved in this category following.
There are two essential questions to answer in rehearsal-based
continual learning. The first is how to choose samples from the whole
dataset to store in the replay buffer with a limited size [49]. The most
representative samples [41, 43] or samples easy to forget [3] are usu-
ally selected in the replay buffer while random selection has also
been used in some works [40, 2]. However, these algorithms are de-
signed for image classification and are not applicable to RL. [14]
focuses on replay buffer question sampling in online RL setting. The
second is how to take advantage of the saved replay samples [49, 3].
In RL, the two most commonly used approaches are BC and per-
fect memory [46] in continual RL, where BC is more effective in
relieving catastrophic forgetting. At present, all these methods are
designed for online RL setting. Unlike prior works, we consider the
offline RL setting in this paper, where catastrophic forgetting and
overestimation must be overcome simultaneously.
To the best of our knowledge, this is the first work to solve offline
RL problems in the continual learning setting.
3 Problem Formulation and Preliminary
Continual Offline Reinforcement Learning In this paper, we
investigate CORL, which learns a sequence of RL tasks T=
(𝑇1,···,𝑇𝑁). Each task𝑇𝑛is described as a Markov Decision Pro-
cess (MDP) represented by a tuple of {S,A,𝑃𝑛,𝜌0,𝑛,𝑟𝑛,𝛾}, where
Sis the state space,Ais the action space, 𝑃𝑛:S×A×S→[0,1]
is the transition probability, 𝜌0,𝑛:Sis the distribution of the ini-
tial state,𝑟𝑛:S×A →[−𝑅max,𝑅max]is the reward function,
and𝛾∈[0,1)is the discounting factor. We assume that sequential
tasks have different 𝑃𝑛,𝜌0,𝑛and𝑟𝑛, but share the same S,A, and𝛾
for simplicity. The return is defined as the sum of discounted future
reward𝑅𝑡,𝑛=Í𝐻
𝑖=𝑡𝛾(𝑖−𝑡)𝑟𝑛(𝑠𝑖,𝑎𝑖), where𝐻is the horizon.
We define a parametric Q-function 𝑄(𝑠,𝑎)and a paramet-
ric policy𝜋(𝑎|𝑠). Q-learning methods train a Q-function by it-
eratively applying the Bellman operator B∗𝑄(𝑠,𝑎)=𝑟(𝑠,𝑎)+
𝛾E𝑠′∼𝑃(𝑠′|𝑠,𝑎)(max𝑎′𝑄(𝑠′,𝑎′)). We also train a transition model
for each task ˆ𝑃𝑛(𝑠′|𝑠,𝑎)by using maximum likelihood estimation
min ˆ𝑃𝑛E(𝑠,𝑎,𝑠′)∼D
logˆ𝑃(𝑠′|𝑠,𝑎)
. We use a multi-head architec-
ture for the policy network 𝜋to avoid the same-state-different-task
2

--- PAGE 3 ---
problem [16]. In detail, the policy network consists of a feature ex-
tractor𝜃𝑧for all tasks and multiple heads 𝜃𝑛,𝑛∈[1,𝑁], where one
head is for each task. 𝜋𝑛is defined to represent the network with
joint parameters[𝜃𝑧,𝜃𝑛]andℎ𝑛is defined to represent the head
with parameters 𝜃𝑛. Our aim is to train sequential tasks all over
[𝑇1,···,𝑇𝑁−1]sequentially and get a high mean performance and
a low forgetting of all the learned tasks without access to data from
previous tasks except a small buffer.
In online RL setting, the experiences 𝑒=(𝑠,𝑎,𝑠′,𝑟)can be ob-
tained through environment interaction. However, in offline RL set-
ting, the policy 𝜋𝑛(𝑎|𝑠)can only be learned from a static dataset
D𝑛=
𝑒𝑖𝑛	
,𝑒𝑖𝑛= 𝑠𝑖𝑛,𝑎𝑖𝑛,𝑠′𝑖𝑛,𝑟𝑖𝑛, which is assumed to be collected
by an unknown behavior policy 𝜋𝛽
𝑛(𝑎|𝑠).
Experience Replay ER [42] is the most widely-used rehearsal-
based continual learning method. In terms of task 𝑇𝑛, the objective of
ER is to retain good performance on previous tasks [𝑇1,···,𝑇𝑛−1],
by using the corresponding replay buffers [𝐵1,···,𝐵𝑛−1], which
called perform memory. Moreover, two additional behavior cloning
losses, including the actor cloning loss and the critic cloning loss, are
commonly used for previous tasks as follows
𝐿actor_cloning :=∑︁
𝑠,𝑎∈𝐵∥𝜋𝑛(𝑠)−𝑎∥2
2, (1)
𝐿critic_cloning :=∑︁
𝑠,𝑎,𝑄 replay∈𝐵𝑄𝑛(𝑎,𝑠)−𝑄replay2
2,(2)
where𝐵is the replay buffer and 𝑄replay means the Q value saved
from previous tasks. These two losses are called BC [46].
Subsequent work [46] shows that for a soft actor-critic [13] archi-
tecture, the replay loss added on the actor network (Eq. 1) performs
well, but the loss added on the critic network (Eq. 2) poorly effect.
Therefore, we only consider the actor cloning loss (Eq. 1) in our
work.
However, naively integrating the ER with offline RL results in
a significant performance drop for CORL problems. To tackle this
problem, we propose a novel method OER, which consists of two
essential components as follows.
4 Offline Experience Replay (OER)
In this section, we first elaborate on how to select valuable experi-
ences and build the replay buffer. Then, we describe a novel DBC
architecture as our replay model. Finally, we summarize our algo-
rithm and provide the Pseudocode.
4.1 Offline Experience Selection Scheme
Novel Distribution Shift Problem: Facing with sequential tasks, an
agent learns the task 𝑇𝑛in order after learning 𝑇𝑛−1, and should
prepare for the next learning. In terms of task 𝑇𝑛, how to se-
lect partial valuable data from offline dataset D𝑛to build a size-
limited replay buffer 𝐵𝑛is critical. In online RL setting, rehearsal-
based methods commonly utilize a ranking function R(𝑠𝑖,𝑎𝑖)=𝑟𝑖+𝛾max
𝑎′𝑄
𝑠′
𝑖,𝑎′
−𝑄(𝑠𝑖,𝑎𝑖)to evaluate the value of trajecto-
ries. Here,R(𝑠𝑖,𝑎𝑖)with(𝑠𝑖,𝑎𝑖)∈ D evaluates replay trajecto-
ries in terms of 𝑄-value accuracy or total accumulative reward [14],
where the trajectories are collected by the optimal policy 𝜋∗𝑛inter-
acting with the environment. On the contrary, in offline RL setting,
if we consider the similar method above, the data selection can onlybe made in the offline dataset D𝑛corresponding to the behavior pol-
icy𝜋𝛽
𝑛. Therefore, there exists a distribution shift between 𝜋𝛽
𝑛and
𝜋∗𝑛, which inevitably affects the performance of the rehearsal. In our
selection scheme, we attempt to identify and filter out those offline
trajectories inD𝑛that are not likely to be generated by 𝜋∗𝑛. To clar-
ify this point, we give an illustration in Fig.1, where those trajectories
close to the offline optimal trajectory are selected and stored in the
replay buffer 𝐵𝑛. The distribution of these trajectories in terms of
bothSandAdiffers from the offline dataset D𝑛.
τ*τ2τ1
(a) offline learned trajectory
τ* (b) Offline buffer selection
Figure 1 : Distribution shift between the experiences from replay
buffer and trajectories from learned policy. (a) 𝜏1and𝜏2represent
two trajectories in offline dataset, and 𝜏∗represents the trajectory
generated by the optimal policy. The learned policy can generate
better trajectories than original dataset. (b) The arrow represents the
experiences in offline dataset near the optimal trajectory. Episodes
close to the optimal trajectory are more valuably selected.
0s
0a
0s
1s
1a
1s
Figure 2 : Illustration of our MBES method to fill in 𝐵𝑛for𝑇𝑛. Blue
lines represent the experiences in 𝐷𝑛, and black lines represent the
generated experiences by optimal policy 𝜋∗𝑛. Starting from 𝑠0and
𝑎0=𝜋∗𝑛(𝑠0), the next state is 𝑠′
0=ˆ𝑃(𝑠0,𝑎0). To avoid the accumu-
lated error, we use the most similar state 𝑠1in the offline dataset D𝑛
instead of the 𝑠′
0as the next state.
Model-based Experience Selection (MBES): In order to address
the new distribution shift question above, we propose a novel MBES
scheme. As shown in Fig.2, based on the offline trajectories in D𝑛
collected from 𝜋𝛽
𝑛, MBES aims to generate a new trajectory corre-
sponding to𝜋∗𝑛. Specifically, MBES considers both the learned opti-
mal policy and a task-specific dynamic model ˆ𝑃𝑛, where the dynamic
model ˆ𝑃𝑛is obtained via supervised learning by using D𝑛. Starting
from the𝑡th state𝑠𝑡, we recursively sample an action by 𝜋∗𝑛and pre-
dict the next state 𝑠′
𝑡=ˆ𝑃𝑛 𝑠𝑡,𝜋∗𝑛(𝑠𝑡).
However, recursively sampling actions on predicted states causes
a significant accumulation of compounding error after several steps,
which hinders the selected trajectories from being revisited later. In
order to remove the compounding error, after learning task 𝑛, starting
from the𝑡th state𝑠𝑡, instead of directly taking the model output as
the next state, a state in D𝑛most similar to the model prediction 𝑠′
𝑡is
selected as the next state 𝑠𝑡+1for the pseudo exploitation. Here, we
use the𝐿2metric to measure the similarity as follows
𝑠𝑡+1=argmin
𝑠∈D𝑛dist 𝑠,𝑠′
𝑡
𝑠′
𝑡∼ˆ𝑃𝑛(𝑠𝑡,𝜋∗𝑛(𝑠𝑡)), (3)
where dist means a distance between 𝑠and𝑠′
𝑡.2According to the
2Here we choose the L2 distance of the feature from the last hidden layer
3

--- PAGE 4 ---
z
2
n
2B
1B
nD
nQ
1
1h
2h
nh(a) offline learned 𝑇∗
n
nQ
z
2
n
2B
1B
1h
2h
nh
nD
1(b) Offline buffer selection
Figure 3 : Network architecture of BC and our DBC for experience play, where the policy 𝜋𝑛=[𝜋𝑧,ℎ𝑛]is for task𝑛,𝑛=1,···,𝑁. (a) Existing
multi-head architecture. Here, the newly-added head ℎ𝑛for𝑇𝑛is learned via an actor-critic algorithm with a Q-network 𝑄𝑛, but other heads
fromℎ1toℎ𝑛−1corresponding to previous tasks are learned by cloning 𝐵1to𝐵𝑛−1. (b) Our DBC architecture. We use an independent policy
network𝜇𝑛to learn the current task 𝑇𝑛and a newly-added head ℎ𝑛is used to clone 𝜇𝑛.
model-based offline RL methods [5, 24], we further introduce the
variance of ˆ𝑃𝑛 𝑠𝑡,𝜋∗𝑛(𝑠𝑡)to determine whether the results of the
dynamic model are reliable or not, where we use Eq.3 for selection
only if the variance of the ˆ𝑃𝑛is lower than the threshold; other-
wise, keeping 𝜋𝛽
𝑛(𝑠𝑡)instead. In our experiments, we specifically
use2ˆ𝑃𝑛(𝑠𝑡,𝑎𝑡)as the threshold, which has minimal impact.
For a start-up, we sample 𝑠0from𝜌0,𝑛, and then iteratively carry
out. In the end, we save the generated trajectory in 𝐵𝑛.
4.2 Dual Behavior Cloning (DBC)
Prior rehearsal-based approaches utilize a multi-head policy network
𝜋and a Q-network 𝑄𝑛to learn task 𝑇𝑛, as shown in Fig.3 (a). During
learning, the policy network 𝜋is to clone the experience data stored
in replay buffers 𝐵1to𝐵𝑛−1for all previous tasks 𝑇1to𝑇𝑛−1. How-
ever, such an architecture suffers from an obvious performance drop
when the number of tasks increases, which indicates that the pol-
icy𝜋trained via BC has the incapability of mastering both previous
knowledge from old tasks and new knowledge from the new task.
This performance drop is due to the existing inconsistency be-
tween the following two objectives: on the one hand, the multi-head
policy𝜋is optimized for all current and previous tasks 𝑇1to𝑇𝑛
for action prediction; on the other hand, the policy 𝜋is also used
for updating the current Q-network 𝑄𝑛. More specifically, we be-
gin this analysis from Q-learning [34, 35] with the Bellman equation
of𝑄(𝑠,𝑎)=𝑟(𝑠,𝑎)+𝛾∗max𝑎′𝑄(𝑠′,𝑎′). In a continuous action
space, as the maximization operator above cannot be realized, a pa-
rameterized actor network 𝜇(𝑠)is commonly used instead to take
the optimal action corresponding to maximal Q value. 𝜇and𝜋can
be considered equivalently in a single-task setting. However, for con-
tinual learning, 𝜋is constrained to clone all previous tasks from 𝑇1to
𝑇𝑛−1while𝜇depends only on the current task 𝑇𝑛. Consequently, it
is difficult for the policy 𝜋to take action corresponding to the maxi-
mum future reward for 𝑠′in task𝑇𝑛.
Based on such analysis, we propose a novel DBC scheme to solve
the inconsistency mentioned above, and the schematic diagram of
our proposed architecture is given in Fig. 3(b). In comparison to ex-
isting multi-head network architecture in Fig. 3(a) [42], we propose
an extra policy network 𝜇𝑛in order to learn the optimal state-action
mapping for 𝑇𝑛. Specifically, when learning task 𝑇𝑛, we first obtain
of the Q network in our experiments. However, we find that a simple L2
distance in the state space Salso works well.𝜇𝑛and𝑄𝑛by using an offline RL algorithm. Then, in the rehearsal
phase, the continual learning policy 𝜋is required to clone the pre-
vious experiences from 𝐵1to𝐵𝑛−1and meanwhile be close to 𝜇𝑛.
Thus, the corresponding loss 𝐿𝜋can be written as follows
𝐿𝜋=E(𝑠,𝑎,𝑠′)∼D 𝑛(𝜋𝑛(𝑠,𝑎)−𝜇𝑛(𝑠,𝑎))2
+𝜆𝑟1
𝑛𝑛−1∑︁
𝑗=1E(𝑠,𝑎)∼B𝑗 𝜋𝑗(𝑠)−𝑎2, (4)
where𝜆𝑟is a coefficient for the BC item. It is worth mentioning that
the continual learning policy 𝜋attempts to clone the behavior of both
𝜇𝑛and the previous experiences from 𝐵1to𝐵𝑛−1simultaneously.
Therefore, we name our scheme DBC.
4.3 Algorithm Summary
When considering a new task 𝑇𝑛, we first utilize DBC to learn the
two policy networks 𝜋,𝜇𝑛, and the dynamic model ˆ𝑃𝑛until conver-
gence. Then, the learned policy 𝜋and the dynamic model ˆ𝑃𝑛are used
in MBES to select valuable data for 𝐵𝑛. To summarize, the overall
process of OER, including DBC and MBES, is given in Algorithm 1.
5 Implementation Details
We model the Q-function and policy network as a multi-layer per-
ceptron (MLP) with two hidden layers of 128 neurons, each with
ReLU non-linearity based on [44]. We use an ensemble dynamic
model containing five individual models, which is also modeled as
an MLP, the same as the Q-function and the policy network. Any
offline RL method is compatible with our architecture. We choose
TD3+BC [10] as the backbone algorithm because of its simple struc-
ture to demonstrate that our algorithm does not depend on a specific
offline algorithm. Further, we use Adam [18] with a learning rate
of0.001to update both the Q-function and the dynamic model and
0.003to update both the policy network 𝜋and𝜇𝑛. Then, for each
task, we train 30,000steps and switch to the next. We find that initial-
izing𝜇𝑛with𝜋𝑛−1and learning 𝜋and𝜇𝑛simultaneously work well
from the experience. Also, learning 𝜋and𝜇𝑛together will reduce the
scope of the gradient and avoid the jumping change to ensure stable
learning and relieve catastrophic forgetting. The result is calculated
via five repeated simulations with different numbers of seeds.
4

--- PAGE 5 ---
Algorithm 1 Our proposed Method OER
Require: Number of tasks 𝑁; initiate the policy 𝜋.
1:forTasks𝑇𝑛in[1,···,𝑁]do
2: Get offline datasetD𝑛; Initiate the replay buffer 𝐵𝑛=∅; Ini-
tiate new head ℎ𝑛for𝜋; Initiate𝜇𝑛,𝑄𝑛and ˆ𝑃𝑛.
3: while Not Convergence do
4: Update𝜇𝑛and𝑄𝑛via offline learning method.
5: Update𝜋via Eqn. 4 to clone 𝜇𝑛and𝐵0to𝐵𝑛−1.
6: Update the dynamic model ˆ𝑃𝑛.
7: end while
8: Sample initial state 𝑠0from𝜌0,𝑛, and𝑠𝑡←𝑠0.
9: while𝐵𝑛is not full do
10: if𝑠𝑡is not terminal state then
11: Select𝑠𝑡+1fromD𝑛by𝜋𝑛and ˆ𝑃𝑛via Eqn. 3.
12: else
13: Sample𝑠𝑡+1from𝜌0,𝑛.
14: end if
15: Add𝑠𝑡+1into𝐵𝑛, and𝑠𝑡←𝑠𝑡+1.
16: end while
17:end for
Ensure:𝜋.
6 Experiments
Extensive experiments are conducted to demonstrate the effective-
ness of our proposed scheme and test whether we can keep both sta-
bility and plasticity at the same time when learning sequential offline
RL tasks. We evaluate the performance of MBES and DBC sepa-
rately to test the performance of each approach.
6.1 Baselines and Datasets
Baselines On one hand , in order to evaluate the MBES, we con-
sider six replay buffer selection approaches, where four from [14] are
given as follows.
•Surprise [14]: store trajectories with maximum mean TD error:
min𝜏∈𝐷𝑖E𝑠,𝑎,𝑠 𝑡+1∈𝜏∥B∗𝑄(𝑠,𝑎)−𝑄(𝑠,𝑎)∥2
2.
•Reward [14]: store trajectories with maximum mean reward:
max𝜏∈𝐷𝑖𝑅𝑡,𝑛.
•Random [14]: randomly select samples in dataset.
•Coverage [14]: store those samples to maximize coverage
of the state space, or ensure the sparsity of selected states:
min𝑠∈𝐷𝑖|N𝑖|;N𝑖={𝑠′s.t.dist(𝑠′−𝑠)<𝑑}.
Considering that these baselines are designed for online RL and it
is not fair enough to use them only as the baselines, we have de-
signed two algorithms for comparison that are applicable to offline
RL, based on the idea of [14].
•Match: select samples in the offline dataset most consistent with
the learned policy. Trajectories chosen in this way are most similar
to the learned policy in the action space, but may not match in the
state space: min𝜏∈𝐷𝑖E𝑠,𝑎∈𝜏𝑎−𝜋∗
𝑖(𝑠)2
2.
•Model: Given that we used a Model-Based approach
to filter our data, we also used it as a criterion for
whether the trajectories matched the transfer probabilities:
min𝜏∈𝐷𝑖E𝑠,𝑎∈𝜏ˆ𝑃𝑖(𝑠,𝑎)−𝑃𝑖(𝑠,𝑎)2
2. This metric is used to
demonstrate that the introduction of the Model-Based approach
alone does not improve the performance of the CORL algorithm.On the other hand , in order to evaluate the DBC, we consider five
widely-used continual learning methods, where three methods need
to use replay buffers.
•BC [42]: a basic rehearsal-based continual method adding a be-
havior clone term in the loss function of the policy network.
•Gradient episodic memory (GEM) [27]: a method using an
episodic memory of parameter gradients to limit the policy up-
date.
•Averaged gradient episodic memory (AGEM) [4]: a method based
on GEM that only uses a batch of gradients to limit the policy
update.
In addition, the following two regularization-based methods are
rehearsal-free so that they are independent of experience selection
methods.
•Elastic weight consolidation (EWC) [19]: constrain the changes
to critical parameters through the Fisher information matrix.
•Synaptic intelligence (SI) [53]: constrain the changes after each
step of optimization.
We also show the performance on multi-task as a reference. The
multi-task learning setting does not suffer from the catastrophic for-
getting problem and can be seen as superior.
Offline Sequential Datasets We consider three sets of tasks from
widely-used continuous control offline meta-RL library3as in [33]:
•Ant-2D Direction (Ant-Dir): train a simulated ant with 8 articu-
lated joints to run in a 2D direction;
•Walker-2D Params (Walker-Par): train a simulated agent to move
forward, where different tasks have different parameters. Specifi-
cally, different tasks require the agent to move at different speeds;
•Half-Cheetah Velocity (Cheetah-Vel): train a cheetah to run at a
random velocity.
For each set of tasks, we randomly sample five tasks to form sequen-
tial tasks𝑇1to𝑇5.
To consider different data quality as [9], we train a soft actor-critic
to collect two benchmarks [12] for each task 𝑇𝑛,𝑛=1,···,5: 1)
Medium (M) with trajectories from medium-quality policy, and 2)
Medium-Random (M-R) including trajectories from both medium-
quality policy and trajectories randomly sampled.
Metrics Following [7], we adopt the average performance (PER)
and the backward transfer (BWT) as evaluation metrics,
PER=1
𝑁𝑁∑︁
𝑛=1𝑎𝑁,𝑛,BWT =1
𝑁−1𝑁−1∑︁
𝑛=1𝑎𝑛,𝑛−𝑎𝑁,𝑛,(5)
where𝑎𝑖,𝑗means the final cumulative rewards of task 𝑗after learning
task𝑖. For PER, higher is better; for BWT, lower is better. These two
metrics show the performance of learning new tasks while alleviating
the catastrophic forgetting problem.
6.2 Overall Results
Evaluate MBES: Firstly, our method OER is compared with
eleven baselines on two kinds of qualities M-R and M. Since
3Most current continual RL methods perform not well on complex bench-
marks such as [47], so it is not suitable for evaluating our method [33] and
we do not consider it in this paper.
5

--- PAGE 6 ---
Benchmark MethodsAnt-Dir Walker-Par Cheetah-Vel
PER BWT PER BWT PER BWT
M-RMultiTask 1387.54 - 1630.90 - -112.49 -
Coverage+DBC 677.72 727.40 231.17 1428.14 -404.33 293.42
Match+DBC 776.85 432.74 120.96 1079.19 -121.30 26.50
Supervise+DBC 903.12 175.17 196.07 1063.35 -233.05 82.60
Reward+DBC 893.63 36.10 554.05 1087.96 -242.50 102.75
Model+DBC 1156.43 65.66 194.31 1214.32 -141.20 57.90
Random+DBC 845.82 126.56 614.71 979.90 -104.16 36.24
OER 1316.46 119.71 1270.62 550.18 -76.61 16.54
MMultiTask 1357.20 - 1751.68 - -115.30 -
Coverage+DBC 842.46 599.74 361.55 1342.87 -424.87 229.89
Match+DBC 841.88 549.98 886.28 678.55 -196.83 88.87
Supervise+DBC 1049.84 347.88 1020.57 666.15 -219.78 56.99
Reward+DBC 1125.44 269.76 891.55 790.07 -222.83 40.15
Model+DBC 1147.05 253.49 872.94 807.33 -184.83 24.22
Random+DBC 1189.17 179.75 1102.89 616.52 -150.39 30.18
OER 1215.58 176.85 1192.59 518.20 -148.18 16.36
Table 1 : Performance of our OER and baselines to verify the effectiveness of MBES, where M-R and M are included. We can observe that our
method OER has the highest PER and lowest BWT in all cases.
Benchmark MethodsAnt-Dir Walker-Par Cheetah-Vel
PER BWT PER BWT PER BWT
M-RMBES+SI 747.95 643.89 124.04 1460.08 -437.49 316.84
MBES+EWC 655.21 726.37 110.53 1589.62 -568.07 506.19
MBES+GEM 748.90 643.06 114.07 1477.87 -445.39 389.10
MBES+AGEM 722.41 687.97 62.27 1628.53 -546.15 419.61
MBES+BC 407.94 874.80 46.75 860.17 -645.79 21.92
OER 1316.46 119.71 1270.62 550.18 -76.61 16.54
Table 2 : Performance of our OER and baselines to verify the effectiveness of DBC, where M-R is included. The result of M can be found in
the Supplementary Material. We can observe that our method OER has the highest PER and lowest BWT in all cases.
our OER comprises MBES and DBC, six experience selection ap-
proaches are added with DBC for a fair comparison. The overall re-
sults are reported in Table 1, in terms of PER and BWT on three se-
quential tasks. From Table 1, we draw the following conclusions: 1)
Our OER method outperforms all baselines for all cases, indicating
the effectiveness of our MBES scheme; 2) Our OER method demon-
strates greater superiority on M-R dataset than on M dataset, indicat-
ing that M-R dataset has a larger distribution shift than M dataset,
and MBES addresses such shift; 3) Random+DBC performs better
than the other five baselines because these five experience selection
schemes are dedicated for online but not offline scenarios. Further-
more, Fig. ??shows the learning process of Ant-Dir on five sequen-
tial tasks. From Fig.4(a) - 4(d), Fig.4(g) - 4(h) and Supplementary
Material, we can observe that compared with baselines, our OER
demonstrates less performance drop with the increment of tasks, in-
dicating that OER can better solve the catastrophic forgetting.
Evaluate DBC: Secondly, our method OER is compared with five
baselines. Similarly, continual learning approaches are added with
MBES for a fair comparison, and the overall performance is reported
in Table 2 and the Supplementary Material. From Table 2 and the
Supplementary Material, we draw the following conclusions: 1) Our
OER method outperforms all baselines, indicating the effectiveness
of our DBC scheme; 2) Four continual learning methods perform not
well due to the forgetting problem; 3) MBES+BC performs the worst
due to the inconsistency of two policies 𝜋and𝜇𝑛in Section 4.2.
From Fig.4(e) - 4(h) and Supplementary Material, we can observethat our OER can learn new tasks and remember old tasks well; other
continual learning methods can only learn new tasks but forget old
tasks, while BC cannot even learn new tasks.
6.3 Parameter Analysis
Size of Buffer 𝐵𝑛In rehearsal-based continual learning, the size
of replay buffer is a key factor. In our CORL setting, the size of
buffer𝐵𝑛is selected as 1K for all𝑛, by considering both storage
space and forgetting issues. With the increase of buffer size, we need
more storage space but have less forgetting issue. In order to quan-
tify such analysis, we consider a buffer size 10K for OER and two
baselines, and the results are listed in Table 3. From Table 3, we can
observe that 1) With the increase of buffer size, OER and two base-
lines achieve better performance as expected. 2) Our DBC method is
still much better than BC, indicating that solving the inconsistency
is significant; 3) With larger storage space, the baseline Random per-
forms similar as MBES, because in this case forgetting issue gets
much smaller and the experience selection becomes not important.
Replay Coefficient 𝜆𝑟Another key factor is the coefficient 𝜆𝑟in
Eq. 4, where 𝜆𝑟is used to balance the anti-forgetting BC item and
the new-policy constraint item. In our CORL setting, we select 𝜆𝑟as
1, which is also the general choice in ER-based approaches [42, 14],
and good performance has been achieved, as mentioned above. We
analyze different values of 𝜆𝑟and show the corresponding perfor-
mance of our OER and baselines in Table 4, where 𝜆𝑟is selected as
0.3,1and3, respectively. From Table 4, we can observe that with
6

--- PAGE 7 ---
(a) Match M
 (b) Match M-R
 (c) Random M
 (d) Random M-R
(e) EWC M
 (f) BC M
 (g) OER M
 (h) OER MR
Figure 4 : Process of learning five sequential tasks, where our OER is compared with Match (M-R and M), Random (M-R and M), EWC (M)
and BC (M). More results are given in Supplementary Material. Every 30000 steps on one task, we switch to the next task.
MethodAnt-Dir Medium Walker-Par Medium Cheetah-Vel Medium
PER BWT PER BWT PER BWT
RewardBC -308.10 1049.32 124.54 1102.85 -706.31 343.23
DBC 1310.45 78.89 1528.63 157.11 -207.76 39.82
RandomBC -295.06 997.96 115.14 1199.63 -750.20 295.36
DBC 1374.54 32.01 1565.50 45.42 -143.10 56.16
MBESBC -338.03 1164.01 588.26 958.38 -582.93 199.81
DBC 1367.07 6.32 1578.30 78.72 -141.17 21.18
Table 3 : Performance of baselines Reward, Random and our OER, where the replay buffer capacity is as 10,000 samples for BC and DBC.
a larger𝜆𝑟, the forgetting issue gradually reduces, but it gets looser
that the learning policy 𝜋clones𝜇𝑛in Eq. 4, and vice versa. As a
result, we achieve the best performance when 𝜆𝑟=1. This is why
we use𝜆𝑟=1for all experiments in this paper.
7 Conclusion
In this work, we formulate a new CORL setting and present a new
method OER, primarily including two key components: MBES and
DBC. We point out a new distribution bias problem and training in-
stability unique to the new CORL setting. Specifically, to solve a
novel distribution shift problem in CORL, we propose a novel MBES
scheme to select valuable experiences from the offline dataset to
build the replay buffer. Moreover, in order to address the inconsis-
tency issue between learning the new task and cloning old tasks,
we propose a novel DBC scheme. Experiments and analysis show
that OER outperforms SOTA baselines on various continuous con-
trol tasks.
References
[1] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi, ‘An
optimistic perspective on offline reinforcement learning’, in ICML ,
(2020).
[2] Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and
Simone Calderara, ‘Dark experience for general continual learning: A
strong, simple baseline’, in NIPS , (2020).Method𝜆𝑟Ant-Dir M Walker-Par M
PER BWT PER BWT
Supervise
+DBC0.3 980.25 537.92 1003.92 688.19
1 1049.84 347.88 1020.57 666.15
3 984.62 201.70 944.71 608.82
Random
+DBC0.3 1168.82 184.57 1052.13 647.71
1 1189.17 179.75 1102.89 616.52
3 952.46 168.81 1157.30 330.47
OER0.3 940.38 431.13 1049.55 606.18
1 1215.58 176.85 1192.59 518.20
3 1128.66 170.32 1051.63 507.05
Table 4 : Performance of our OER method with different coefficient
𝜆𝑟. High𝜆𝑟indicates more on replaying previous tasks.
[3] Arslan Chaudhry, Albert Gordo, Puneet Kumar Dokania, Philip H. S.
Torr, and David Lopez-Paz, ‘Using hindsight to anchor past knowledge
in continual learning’, in AAAI , (2021).
[4] Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mo-
hamed Elhoseiny, ‘Efficient Lifelong Learning with A-GEM’, in ICLR ,
(2019).
[5] Xiong-Hui Chen, Yang Yu, Qingyang Li, Fan Luo, Zhiwei Qin, Wenjie
Shang, and Jieping Ye, ‘Offline model-based adaptable policy learn-
ing’, in NeurIPS , (2021).
[6] Robert Dadashi, Shideh Rezaeifar, Nino Vieillard, Léonard Hussenot,
Olivier Pietquin, and Matthieu Geist, ‘Offline Reinforcement Learning
with Pseudometric Learning’, in ICML , (July 2021).
[7] Mohammad Mahdi Derakhshani, Xiantong Zhen, Ling Shao, and Cees
Snoek, ‘Kernel Continual Learning’, in ICML , volume 139, (July
2021).
[8] Chrisantha Fernando, Dylan Banarse, Charles Blundell, Yori Zwols,
David Ha, Andrei A. Rusu, Alexander Pritzel, and Daan Wierstra,
7

--- PAGE 8 ---
‘PathNet: Evolution Channels Gradient Descent in Super Neural Net-
works’, CoRR ,abs/1701.08734 , (2017). arXiv: 1701.08734.
[9] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey
Levine, ‘D4RL: Datasets for Deep Data-Driven Reinforcement Learn-
ing’, CoRR ,abs/2004.07219 , (2020). arXiv: 2004.07219.
[10] Scott Fujimoto and Shixiang (Shane) Gu, ‘A Minimalist Approach to
Offline Reinforcement Learning’, in NIPS . Curran Associates, Inc.,
(2021).
[11] Scott Fujimoto, David Meger, and Doina Precup, ‘Off-policy deep re-
inforcement learning without exploration’, in ICML , (2019).
[12] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine,
‘Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement
Learning with a Stochastic Actor’, in ICML , eds., Jennifer Dy and
Andreas Krause, volume 80 of Proceedings of Machine Learning Re-
search . PMLR, (July 2018).
[13] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, G. Tucker,
Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta,
P. Abbeel, and Sergey Levine, ‘Soft actor-critic algorithms and applica-
tions’, ArXiv ,abs/1812.05905 , (2018).
[14] David Isele and Akansel Cosgun, ‘Selective Experience Replay for
Lifelong Learning’, AAAI , (April 2018).
[15] Christos Kaplanis, Murray Shanahan, and Claudia Clopath, ‘Policy
Consolidation for Continual Reinforcement Learning’, in ICML , vol-
ume 97 of Proceedings of Machine Learning Research . PMLR, (2019).
[16] Samuel Kessler, Jack Parker-Holder, Philip J. Ball, Stefan Zohren, and
Stephen J. Roberts, ‘Same state, different task: Continual reinforcement
learning without interference’, in AAAI , (2021).
[17] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and
Thorsten Joachims, ‘Morel : Model-based offline reinforcement learn-
ing’, ArXiv ,abs/2005.05951 , (2020).
[18] Diederik P. Kingma and Jimmy Ba, ‘Adam: A Method for Stochastic
Optimization’, in ICLR , (2015).
[19] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Ve-
ness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John
Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hass-
abis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell, ‘Over-
coming catastrophic forgetting in neural networks’, Proceedings
of the National Academy of Sciences ,114(13), (2017). _eprint:
https://www.pnas.org/doi/pdf/10.1073/pnas.1611835114.
[20] Ilya Kostrikov, Ashvin Nair, and Sergey Levine, ‘Offline reinforcement
learning with implicit q-learning’, in ICLR , (2021).
[21] Aviral Kumar, Justin Fu, G. Tucker, and Sergey Levine, ‘Stabilizing
off-policy q-learning via bootstrapping error reduction’, in NeurIPS ,
(2019).
[22] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine, ‘Con-
servative Q-Learning for Offline Reinforcement Learning’, in NIPS ,
volume 33. Curran Associates, Inc., (2020).
[23] Erwan Lecarpentier, David Abel, Kavosh Asadi, Yuu Jinnai, Emmanuel
Rachelson, and Michael L. Littman, ‘Lipschitz Lifelong Reinforcement
Learning’, in AAAI , (2021).
[24] Byung-Jun Lee, Jongmin Lee, and Kee-Eung Kim, ‘Representation
Balancing Offline Model-based Reinforcement Learning’, in ICLR ,
(February 2022).
[25] Jinxin Liu, Zhang Hongyin, and Donglin Wang, ‘Dara: Dynamics-
aware reward augmentation in offline reinforcement learning’, in In-
ternational Conference on Learning Representations , (2021).
[26] Jinxin Liu, Ziqi Zhang, Zhenyu Wei, Zifeng Zhuang, Yachen Kang,
Sibo Gai, and Donglin Wang, ‘Beyond ood state actions: Supported
cross-domain offline reinforcement learning’, ArXiv ,abs/2306.12755 ,
(2023).
[27] David Lopez-Paz and Marc’ Aurelio Ranzato, ‘Gradient Episodic
Memory for Continual Learning’, in NIPS , volume 30. Curran Asso-
ciates, Inc., (2017).
[28] Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu, ‘Mildly conserva-
tive q-learning for offline reinforcement learning’, in Advances in Neu-
ral Information Processing Systems , eds., Alice H. Oh, Alekh Agarwal,
Danielle Belgrave, and Kyunghyun Cho, (2022).
[29] Yecheng Ma, Dinesh Jayaraman, and Osbert Bastani, ‘Conservative of-
fline distributional reinforcement learning’, NIPS ,34, (2021).
[30] Arun Mallya and Svetlana Lazebnik, ‘PackNet: Adding Multiple Tasks
to a Single Network by Iterative Pruning’, in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) ,
(June 2018).
[31] Michael McCloskey and Neal J. Cohen, ‘Catastrophic interference inconnectionist networks: The sequential learning problem’, Psychology
of Learning and Motivation, Academic Press, (1989).
[32] Jorge Mendez, Boyu Wang, and Eric Eaton, ‘Lifelong Policy Gradient
Learning of Factored Policies for Faster Training Without Forgetting’,
inAdvances in Neural Information Processing Systems , volume 33.
Curran Associates, Inc., (2020).
[33] Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and
Chelsea Finn, ‘Offline meta-reinforcement learning with advantage
weighting’, in ICML , (2021).
[34] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller,
‘Playing atari with deep reinforcement learning’, arXiv preprint
arXiv:1312.5602 , (2013).
[35] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu,
Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller,
Andreas K Fidjeland, Georg Ostrovski, et al., ‘Human-level control
through deep reinforcement learning’, nature , (2015).
[36] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine,
‘Awac: Accelerating online reinforcement learning with offline
datasets’, arXiv , (2020).
[37] German I. Parisi, Ronald Kemker, Jose L. Part, Christopher Kanan, and
Stefan Wermter, ‘Continual lifelong learning with neural networks: A
review’, Neural Networks ,113, (2019).
[38] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine,
‘Advantage-Weighted Regression: Simple and Scalable Off-Policy
Reinforcement Learning’, CoRR ,abs/1910.00177 , (2019). arXiv:
1910.00177.
[39] David Pfau and Oriol Vinyals, ‘Connecting generative adversarial net-
works and actor-critic methods’, CoRR , (2016).
[40] Ameya Prabhu, Philip H. S. Torr, and Puneet Kumar Dokania, ‘Gdumb:
A simple approach that questions our progress in continual learning’,
inECCV , (2020).
[41] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, G. Sperl, and
Christoph H. Lampert, ‘icarl: Incremental classifier and representa-
tion learning’, 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) , (2017).
[42] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and
Gregory Wayne, ‘Experience Replay for Continual Learning’, in NIPS .
Curran Associates, Inc., (2019).
[43] Gobinda Saha, Isha Garg, and Kaushik Roy, ‘Gradient Projection Mem-
ory for Continual Learning’, in ICLR . OpenReview.net, (2021).
[44] Takuma Seno and Michita Imai, ‘d3rlpy: An Offline Deep Rein-
forcement Learning Library’, CoRR ,abs/2111.03788 , (2021). arXiv:
2111.03788.
[45] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost To-
bias Springenberg, Scott E Reed, Bobak Shahriari, Noah Siegel, Caglar
Gulcehre, Nicolas Heess, et al., ‘Critic regularized regression’, NIPS ,
33, 7768–7778, (2020).
[46] Maciej Wolczyk, Michał Zaj ˛ ac, Razvan Pascanu, Łukasz Kuci ´nski, and
Piotr Miło ´s, ‘Disentangling transfer in continual reinforcement learn-
ing’, in NIPS , (2022).
[47] Maciej Wołczyk, Michal Zajkac, Razvan Pascanu, Lukasz Kuci’nski,
and Piotr Milo’s, ‘Continual world: A robotic benchmark for continual
reinforcement learning’, in NeurIPS , (2021).
[48] Annie Xie, James Harrison, and Chelsea Finn, ‘Deep Reinforcement
Learning amidst Continual Structured Non-Stationarity’, in ICML , vol-
ume 139 of Proceedings of Machine Learning Research . PMLR, (July
2021).
[49] Jaehong Yoon, Divyam Madaan, Eunho Yang, and Sung Ju Hwang,
‘Online coreset selection for rehearsal-based continual learning’, in
ICLR , (2021).
[50] Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Sergey
Levine, and Chelsea Finn, ‘Conservative data sharing for multi-task of-
fline reinforcement learning’, NIPS ,34, 11501–11516, (2021).
[51] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey
Levine, and Chelsea Finn, ‘COMBO: Conservative Offline Model-
Based Policy Optimization’, in NIPS . Curran Associates, Inc., (2021).
[52] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou,
Sergey Levine, Chelsea Finn, and Tengyu Ma, ‘MOPO: Model-based
Offline Policy Optimization’, in NIPS . Curran Associates, Inc., (2020).
[53] Friedemann Zenke, Ben Poole, and Surya Ganguli, ‘Continual Learning
Through Synaptic Intelligence’, in ICML . PMLR, (2017).
[54] Siyuan Zhang and Nan Jiang, ‘Towards hyperparameter-free policy se-
lection for offline reinforcement learning’, in NIPS . Curran Associates,
8

--- PAGE 9 ---
Inc., (2021).
[55] Zifeng Zhuang, Kun Lei, Jinxin Liu, Donglin Wang, and Yilang
Guo, ‘Behavior proximal policy optimization’, ArXiv ,abs/2302.11312 ,
(2023).Supplemental Materials: OER: Offline Experience
Replay for Continual Offline Reinforcement
Learning
8 Section 1
9 Additional Results
Here we give out the learning process of different methods of Ant-
Dir dataset in Fig.A1.
1

--- PAGE 10 ---
(a) Coverage M
 (b) Coverage M-R
 (c) Match M
 (d) Match M-R
(e) Model M
 (f) Model M-R
 (g) Reward M
 (h) Reward M-R
(i) Supervise M
 (j) Supervise M-R
 (k) Random M
 (l) Random M-R
(m) GEM M
 (n) GEM M-R
 (o) AGEM M
 (p) AGEM M-R
(q) EWC M
 (r) EWC M-R
 (s) SI M
 (t) SI M-R
(u) BC M
 (v) BC M-R
 (w) OER M
 (x) OER M-R
Figure 1. Process of learning five sequential tasks, where some of the results have been given in the main body.
2
