# 2306.07541.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/rl-alignment/2306.07541.pdf
# Kích thước file: 1074903 bytes

===============================================
NỘI DUNG FILE PDF
===============================================


--- TRANG 1 ---
Một Khung Làm Việc Thống Nhất Đơn Giản Hướng Dẫn Bởi Độ Bất Định cho
Học Tăng Cường Offline-đến-Online
Siyuan Guo1,∗Yanchao Sun2Jifeng Hu1Sili Huang1Xing Chen1
Hechang Chen1Haiyin Piao3Lichao Sun4Yi Chang1
1Khoa Trí Tuệ Nhân Tạo, Đại học Cát Lâm
2Khoa Khoa Học Máy Tính, Đại học Maryland
3Đại học Công Nghệ Tây Bắc4Đại học Lehigh
∗guosyjlu@gmail.com
Tóm Tắt
Học tăng cường offline (RL) cung cấp một giải pháp đầy hứa hẹn để học một
agent hoàn toàn dựa vào mô hình dữ liệu. Tuy nhiên, bị hạn chế bởi chất lượng
hạn chế của tập dữ liệu offline, hiệu suất của nó thường là dưới mức tối ưu. Do đó,
việc tiếp tục tinh chỉnh agent thông qua các tương tác online bổ sung trước khi
triển khai là mong muốn. Thật không may, RL offline-đến-online có thể gặp thách
thức do hai thách thức chính: hành vi khám phá bị hạn chế và sự dịch chuyển phân
phối state-action. Để giải quyết vấn đề này, chúng tôi đề xuất một khung làm việc
Simple Unified u Ncertainty-Guided (SUNG), tự nhiên thống nhất giải pháp cho
cả hai thách thức với công cụ độ bất định. Cụ thể, SUNG định lượng độ bất định
thông qua một bộ ước tính mật độ truy cập state-action dựa trên VAE. Để tạo
điều kiện khám phá hiệu quả, SUNG trình bày một chiến lược khám phá lạc quan
thực tế để chọn các hành động thông tin có cả giá trị cao và độ bất định cao. Hơn
nữa, SUNG phát triển một phương pháp khai thác thích ứng bằng cách áp dụng
các mục tiêu RL offline bảo thủ cho các mẫu có độ bất định cao và các mục tiêu
RL online tiêu chuẩn cho các mẫu có độ bất định thấp để kết nối mượt mà các
giai đoạn offline và online. SUNG đạt được hiệu suất tinh chỉnh online tiên tiến
nhất khi kết hợp với các phương pháp RL offline khác nhau, trên nhiều môi trường
và tập dữ liệu khác nhau trong benchmark D4RL.1
1 Giới Thiệu
Học tăng cường offline (RL) [27], cho phép các agent học từ một tập dữ liệu cố định mà không
tương tác với môi trường, đã thể hiện thành công đáng kể trong nhiều nhiệm vụ mà việc tương tác
tốn kém hoặc rủi ro [5,26,46]. Tuy nhiên, mô hình học dựa trên dữ liệu như vậy vốn dĩ giới hạn
hiệu suất của agent vì nó hoàn toàn dựa vào một tập dữ liệu thường dưới mức tối ưu [25,45]. Để
khắc phục vấn đề này, cộng đồng RL đã khám phá cài đặt offline-đến-online, kết hợp các tương tác
online bổ sung để tinh chỉnh thêm các agent RL offline đã được huấn luyện trước [25, 32, 45, 49].
Trong khi mô hình pretraining-finetuning trong RL offline-đến-online là tự nhiên và trực quan, việc
cung cấp cải thiện online mong đợi có thể gặp thách thức trong thực tế do hai thách thức chính: (1)
Hành vi khám phá bị hạn chế: Các mục tiêu RL offline bảo thủ, yêu cầu các agent thực hiện các
hành động trong phạm vi hỗ trợ của tập dữ liệu trong quá trình pretraining, có thể hạn chế hành vi
khám phá của các agent cho các tương tác online. Kết quả là, chúng không thể đạt được khám phá
hiệu quả, và do đó không thể tận dụng đầy đủ mô hình thử-và-sai trong RL online. (2) Sự dịch
chuyển phân phối state-action: Trong giai đoạn finetuning, các agent có thể gặp phải các chế độ
state-action không quen thuộc nằm ngoài
1Công trình này đã được gửi đến IEEE để có thể xuất bản. Bản quyền có thể được chuyển giao mà không
thông báo, sau đó phiên bản này có thể không còn truy cập được.
Preprint. Đang xem xét.arXiv:2306.07541v2  [cs.LG]  21 Feb 2024

--- TRANG 2 ---
phạm vi hỗ trợ của tập dữ liệu. Do đó, sự dịch chuyển phân phối state-action giữa dữ liệu offline
và online xảy ra, dẫn đến lỗi ngoại suy nổi tiếng [13,23] trong quá trình khai thác, có thể xóa bỏ
việc khởi tạo tốt thu được từ giai đoạn pretraining. Nghiên cứu hiện tại thường tập trung vào giải
quyết một khía cạnh của các thách thức trên bằng cách phát triển khám phá hiệu quả
[49,33] hoặc khai thác hiệu quả [25,48,45], dẫn đến cải thiện offline-đến-online hạn chế.
Tuy nhiên, việc đồng thời giải quyết cả hai thách thức mang lại khó khăn bổ sung do xung đột vốn
có giữa khám phá và khai thác: khám phá mạnh mẽ có thể làm trầm trọng thêm sự dịch chuyển phân
phối, trong khi khai thác bảo thủ có thể cản trở các agent khỏi tinh chỉnh online hiệu quả.
Trong công trình này, chúng tôi nhằm cung cấp một giải pháp thống nhất cho cả hai thách thức để
có hiệu quả mẫu tốt hơn trong quá trình tinh chỉnh online. Để đạt được điều này, chúng tôi nhận ra
rằng cả hai thách thức đều gắn liền chặt chẽ với việc xác định và xử lý thích hợp các cặp state-action
chưa thấy. Cụ thể, khám phá hiệu quả đòi hỏi khám phá các vùng thông tin chưa thấy của không
gian state-action. Hơn nữa, khai thác hiệu quả cần đặc tính hóa chính xác dữ liệu ngoài phân phối
(OOD) để tránh học chính sách quá bảo thủ hoặc quá mạnh mẽ. Do đó, chúng tôi tận dụng định
lượng và sử dụng thích hợp độ bất định [24,3,1,40] để tự nhiên giải quyết cả hai thách thức và đạt
được sự cân bằng mong muốn giữa khám phá và khai thác.
Để đạt được mục tiêu này, chúng tôi trình bày một khung làm việc Simple Unified u Ncertainty
Guided (SUNG), cung cấp một giải pháp chung cho RL offline-đến-online để cho phép tinh chỉnh
các agent được huấn luyện trước với các mục tiêu RL offline khác nhau. Không giống như các
phương pháp RL nhận thức độ bất định gần đây dựa vào kỹ thuật ensemble để định lượng độ bất
định [3,1,24], SUNG sử dụng một cách tiếp cận đơn giản nhưng hiệu quả ước tính mật độ truy cập
state-action với một bộ mã hóa tự động biến phân (VAE) [18] để định lượng độ bất định state-action.
Trái ngược với các phương pháp RL offline-đến-online trước đây, SUNG đồng thời giải quyết cả
hai thách thức bằng cách tận dụng công cụ độ bất định. Cụ thể, chúng tôi phát triển một chiến lược
khám phá lạc quan thực tế tuân theo nguyên tắc lạc quan khi đối mặt với độ bất định [4,2]. Ý tưởng
chính ở đây là chọn những cặp state-action có cả giá trị Q cao và độ bất định cao cho khám phá hiệu
quả. Chúng tôi cũng đề xuất một phương pháp khai thác thích ứng để xử lý sự dịch chuyển phân
phối state-action bằng cách xác định và hạn chế các mẫu OOD. Thông tin quan trọng là tận dụng
các mục tiêu RL offline bảo thủ cho các mẫu có độ bất định cao, và các mục tiêu RL online tiêu
chuẩn cho các mẫu có độ bất định thấp. Điều này cho phép các agent thích ứng mượt mà với các
thay đổi trong phân phối state-action. Hơn nữa, chúng tôi sử dụng bộ đệm phát lại offline-đến-online
(OORB) [48] để kết hợp dữ liệu offline và online trong quá trình finetuning. Đáng chú ý, những
hiểu biết của SUNG có thể áp dụng chung và có thể kết hợp với hầu hết các phương pháp RL offline
không mô hình về nguyên tắc. Về mặt thực nghiệm, SUNG hưởng lợi từ việc xem xét độ bất định
để hướng dẫn cả khám phá và khai thác, do đó vượt qua các phương pháp tiên tiến nhất trên các
benchmark D4RL [10], với 14.54% và 14.80% cải thiện offline-đến-online trung bình bổ sung so
với baseline tốt nhất trong các miền MuJoCo và AntMaze, tương ứng.
Tóm Tắt Đóng Góp. (1) Chúng tôi đề xuất một khung làm việc chung SUNG cho RL offline-đến-online
hiệu quả mẫu, có thể kết hợp với các phương pháp RL offline hiện có, như TD3+BC
[11] và CQL [23]. (2) Chúng tôi giới thiệu một chiến lược khám phá lạc quan thông qua lựa chọn
hành động hai cấp để chọn các hành động thông tin cho khám phá hiệu quả. (3) Chúng tôi phát
triển một phương pháp khai thác thích ứng với xác định và điều chỉnh mẫu OOD để kết nối mượt
mà các mục tiêu RL offline và RL online. (4) Kết quả thực nghiệm chứng minh rằng SUNG vượt
trội so với tiên tiến nhất trước đây khi kết hợp với các phương pháp RL offline khác nhau, trên
nhiều loại môi trường và tập dữ liệu khác nhau.
2 Công Trình Liên Quan
RL Offline. RL Offline [27] nhằm học một chính sách chỉ từ một tập dữ liệu offline cố định. Hầu
hết các công trình trước đây trong RL offline đã được dành riêng để giải quyết lỗi ngoại suy do
truy vấn hàm giá trị với các hành động OOD [13,23]. Do đó, việc hạn chế chính sách đã học để
thực hiện các hành động trong tập hỗ trợ của tập dữ liệu offline là quan trọng. Các chiến lược phổ
biến bao gồm ràng buộc chính sách [13,22,11,39], điều chỉnh giá trị [23,31,20,3], v.v. Tuy nhiên,
các công trình trước đây đã chỉ ra rằng hiệu suất của agent RL offline thường bị giới hạn bởi chất
lượng của các tập dữ liệu
[34, 25], điều này thúc đẩy điều tra thêm cho cài đặt offline-đến-online.
RL Offline-đến-Online. RL Offline-đến-online bao gồm pretraining với RL offline và finetuning
thông qua các tương tác online. Một số phương pháp RL offline tự nhiên hỗ trợ tinh chỉnh online
tiếp tục với các mục tiêu RL offline liên tục [39,31,21]. Tuy nhiên, chúng thường có xu hướng quá
bảo thủ và cho kết quả cải thiện hiệu suất hạn chế [44]. Bên cạnh đó, một số cách tiếp cận RL
offline-đến-online được
2

--- TRANG 3 ---
thiết kế cho một phương pháp RL offline cụ thể [49,35,47,30]. Ví dụ, ODT [49] giới thiệu
RL entropy cực đại để tinh chỉnh DT [6]. ABCR [47] đề xuất nới lỏng thích ứng các mục tiêu
bảo thủ của TD3+BC [11]. Ngược lại, chúng tôi tập trung vào khung làm việc RL offline-đến-online
chung có thể kết hợp với các phương pháp RL offline khác nhau [25,33,48,45]. Các công trình
trước thường giải quyết các thách thức của hạn chế khám phá và sự dịch chuyển phân phối state-action.
Đối với cái trước, O3F [33] sử dụng kiến thức trong các hàm giá trị để hướng dẫn khám phá, nhưng
nó không tương thích với các phương pháp RL offline dựa trên điều chỉnh giá trị. Đối với cái sau,
BR [25] sử dụng kỹ thuật ensemble cùng với bộ đệm phát lại cân bằng ưu tiên các mẫu gần-on-policy.
APL [48] đề xuất tận dụng các lợi thế khác nhau của dữ liệu offline và dữ liệu online để học chính
sách thích ứng. PEX [45] đóng băng các chính sách được huấn luyện trước và huấn luyện một chính
sách mới từ đầu sử dụng chiến lược khám phá thích ứng để tránh xóa các chính sách được huấn
luyện trước. Không giống như các cách tiếp cận đã nêu trên, SUNG thống nhất hai thách thức bằng
cách nhấn mạnh việc ước tính và sử dụng thích hợp độ bất định.
Độ Bất Định cho RL. RL nhận thức độ bất định đã đạt được thành công đáng chú ý trong cả RL
online và offline [28]. Về RL online, một chủ đề nổi bật trong khám phá là nghiên cứu về độ bất
định nhận thức, dẫn đến sự phát triển của nguyên tắc lạc quan nổi tiếng khi đối mặt với độ bất
định [4,2,8,7,24]. Trong bối cảnh RL offline, cả hai cách tiếp cận dựa trên mô hình [17,15,43] và
không dựa trên mô hình [1,3,40,14] có thể hưởng lợi từ việc xem xét độ bất định để xác định và
xử lý các hành động OOD. Tuy nhiên, các phương pháp đã nêu trên thường sử dụng kỹ thuật ensemble
để ước tính độ bất định, có thể tốn kém về mặt tính toán cả về thời gian và tài nguyên huấn luyện.
Ngược lại, chúng tôi áp dụng VAE [18] để định lượng độ bất định hiệu quả và hiệu suất. Ngoài ra,
chúng tôi nghiên cứu cài đặt offline-đến-online, điều này phân biệt cách tiếp cận của chúng tôi với
chúng.
3 Kiến Thức Cơ Bản
RL. Chúng tôi tuân theo thiết lập RL tiêu chuẩn công thức hóa môi trường như một quá trình quyết
định Markov (MDP) M= (S,A, p, r, γ), trong đó S là không gian trạng thái, A là không gian hành
động, p(s′|s, a) là phân phối chuyển tiếp, r(s, a) là hàm phần thưởng, và γ∈[0,1) là hệ số chiết
khấu. Mục tiêu của RL là tìm một chính sách π(a|s) tối đa hóa lợi nhuận mong đợi Eπ[P∞
t=0γtr(st, at)].
RL Off-policy. Các phương pháp RL off-policy, như TD3 [12] và SAC [16], đã được áp dụng
rộng rãi do hiệu quả mẫu của chúng. Các phương pháp này thường xen kẽ giữa đánh giá chính sách
và cải thiện chính sách. Cụ thể, cho một tập dữ liệu phát lại kinh nghiệm D, TD3 học một chính
sách deterministic πϕ(s) và một hàm giá trị state-action Qθ(s, a), được tham số hóa bởi ϕ và θ,
tương ứng. Hàm giá trị có thể được cập nhật thông qua học chênh lệch thời gian (TD) như
LQ(θ) =E(s,a,r,s′)∼D[
(Qθ(s, a)−r−γQ¯θ(s′, πϕ(s′)))2], (1)
trong đó Q¯θ là mạng giá trị mục tiêu để ổn định quá trình học. Sau đó, chính sách có thể được
cập nhật để tối đa hóa giá trị Q hiện tại:
Lπ(ϕ) =Es∼D[−Qθ(s, πϕ(s))]. (2)
RL Offline. Trong cài đặt RL offline, agent chỉ có quyền truy cập vào một tập dữ liệu cố định
D={(s, a, r, s′)}. Mặc dù các phương pháp RL off-policy có thể học từ dữ liệu được thu thập bởi
bất kỳ chính sách nào về nguyên tắc, chúng thất bại trong cài đặt RL offline. Điều này có thể được
quy cho lỗi ngoại suy nổi tiếng [13,23] do truy vấn hàm giá trị với các hành động OOD. Do đó,
một dòng nghiên cứu RL offline không mô hình chính là hạn chế chính sách đã học để thực hiện
các hành động trong phạm vi hỗ trợ của tập dữ liệu theo những cách khác nhau, như ràng buộc
chính sách [11,39], điều chỉnh giá trị [23,20,31] và v.v. Trong số đó, hai phương pháp RL offline
đại diện là TD3+BC [11] và CQL [23]. Cái trước thêm một thuật ngữ điều chỉnh cloning hành vi
(BC) vào cải thiện chính sách tiêu chuẩn trong TD3:
LTD3+BC
π (ϕ) =Lπ(ϕ) +λBCE(s,a)∼D[
(πϕ(s)−a)2], (3)
trong đó λBC cân bằng mất mát cải thiện chính sách tiêu chuẩn và điều chỉnh BC. Ngược lại, cái
sau dùng đến ước tính bi quan dưới của các giá trị Q trong quá trình đánh giá chính sách trong SAC bởi
LCQL
Q(θ) =LQ(θ) +λCQL
Es∼D,a∼πϕ[Qθ(s, a)] +E(s,a)∼D[−Qθ(s, a)]
, (4)
trong đó λCQL biểu thị hệ số cân bằng. Sau đó, chúng ta có thể tóm tắt chính thức cả hai phương pháp như
Loffline
F (Θ) = LF(Θ) + λRF(Θ), (5)
3

--- TRANG 4 ---
𝑎𝑖~𝜋𝜙∙|𝑠
𝑠,𝑎,𝑟,𝑠′Tập Hành Động Chung Kết𝑠
OORB:ℒℱadpΘ=ℒℱΘ+ℛℱΘ:ℒℱadpΘ=ℒℱΘ
𝑎~P𝑎𝑖Tập Hành Động Ứng Viên Cấp 1. 
Chọn Hành Động Top-𝑘-Giá Trị
Cấp 2. 
Chọn Hành Động Độ Bất Định Cao𝒰𝑝𝜓𝑞𝜑
𝑠,𝑎,𝑟,𝑠′Bộ Nhận Diện 
Mẫu OOD
: Vùng Giá Trị Thấp             : Vùng Giá Trị Cao       : Hành Động Độ Bất Định Thấp       : Hành Động Độ Bất Định Cao Chú thích𝒰≝ℒELBO
(a) Khám Phá Lạc Quan qua Lựa Chọn Hành Động Hai Cấp (     ) 
 (b) Khai Thác Thích Ứng với Nhận Diện Mẫu OOD (     ) Hình 1: Tổng quan về SUNG. Trong quá trình tinh chỉnh online, chúng tôi xen kẽ giữa (a) chiến lược khám
phá lạc quan để thu thập dữ liệu hành vi từ môi trường và (b) phương pháp khai thác thích ứng
để cải thiện chính sách. Hơn nữa, chúng tôi áp dụng VAE để ước tính mật độ state-action để định
lượng độ bất định, và OORB để lưu trữ cả dữ liệu offline và online. Hình này được xem tốt nhất
với màu sắc.
trong đó F đại diện cho một chính sách hoặc một hàm giá trị, được tham số hóa bởi Θ, R biểu thị
một bộ điều chỉnh để ngăn chặn lỗi ngoại suy, và λ cân bằng mất mát tiêu chuẩn và điều chỉnh.
Lưu ý rằng hầu hết các phương pháp RL offline không mô hình có thể được tóm tắt như Eq.(5)
theo cách rõ ràng [23,11,20,39,31] hoặc cách ngầm [22, 36, 21, 3, 42], điều này thúc đẩy khai
thác thích ứng trong Phần 4.3.
4 SUNG
Trong phần này, chúng tôi trình bày một khung làm việc hướng dẫn độ bất định thống nhất đơn giản
(SUNG) cho RL offline-đến-online, như được mô tả trong Hình 1. Cụ thể, SUNG chủ động định
lượng độ bất định state-action bằng cách ước tính một hàm mật độ với VAE. Sau đó, dưới sự hướng
dẫn của độ bất định, SUNG kết hợp một chiến lược khám phá lạc quan và một phương pháp khai
thác thích ứng cho tinh chỉnh online.
Kết hợp với OORB, chúng tôi cuối cùng đạt đến thuật toán hoàn chỉnh của SUNG.
4.1 Định Lượng Độ Bất Định với Ước Tính Mật Độ
Nhiều công trình trước thường sử dụng các ensemble Q để định lượng độ bất định [40,33,8,24].
Tuy nhiên, kỹ thuật ensemble có thể tăng đáng kể chi phí tính toán. Do đó, trong công trình này,
chúng tôi sử dụng một cách tiếp cận đơn giản nhưng hiệu quả bằng cách áp dụng VAE [18] như
một bộ ước tính mật độ truy cập state-action để định lượng độ bất định. Cụ thể, cho một bộ mã
hóa qφ(z|s, a) và một bộ giải mã pψ(s, a|z) với tham số φ và ψ, tương ứng, chúng ta có thể tối
ưu hóa chúng với giới hạn dưới bằng chứng (ELBO)
LELBO (s, a;ψ, φ) =Eqφ(z|s,a)[−logpψ(s, a|z)] +DKL[qφ(z|s, a)||p(z|s, a)], (6)
trong đó p(z|s, a) =N(0, I) là một prior cố định. Sau đó, chúng tôi sử dụng log likelihood âm của
mật độ state-action để định lượng độ bất định, tức là, U(s, a)def=−logp(s, a)≈ L ELBO (s, a;ψ, φ),
vì chúng ta biết logp(s, a) =−LELBO (s, a;ψ, φ) +DKL[qφ(z|s, a)||pψ(z|s, a)]. Một cách trực
quan, cả mất mát tái tạo và phân kỳ KL trong ELBO đều chỉ ra độ bất định state-action, tức là,
liệu một cặp state-action có thể được nắm bắt tốt bởi phân phối đã học hay không.
Mặc dù có thể chứng minh lý thuyết rằng một độ chệch tồn tại giữa ELBO và log likelihood âm
của mật độ state-action, một công trình trước đây đã chỉ ra rằng sự xấp xỉ như vậy là hợp lệ về
mặt thực nghiệm [39]. Do đó, chúng tôi sử dụng ELBO để định lượng độ bất định trong thực tế,
và để lại việc khám phá các kỹ thuật lấy mẫu quan trọng [19, 37] để giảm thiểu độ chệch hơn nữa
như công việc tương lai.
4.2 Khám Phá Lạc Quan qua Lựa Chọn Hành Động Hai Cấp
Một dòng chính của các phương pháp RL offline không mô hình thực hiện các mục tiêu bảo thủ
để phạt các hành động OOD trong quá trình pretraining offline. Tuy nhiên, những mục tiêu bảo
thủ như vậy vốn dĩ giới hạn sức mạnh khám phá của các agent, điều này cản trở các agent tận dụng
đầy đủ từ tinh chỉnh online [33]. Để khắc phục hạn chế này, mục tiêu của chúng tôi là đo lường
thích hợp tính thông tin của các hành động chưa thấy, và sau đó chọn hành động thông tin nhất
cho khám phá. Để đạt được điều này, chúng tôi đề xuất một chiến lược khám phá lạc quan dựa trên
nguyên tắc lạc quan khi đối mặt với độ bất định [4,2]. Thông tin quan trọng là một hành động
thông tin được mong đợi sở hữu cả giá trị Q cao và độ bất định cao.
Các công trình hiện tại thường ước tính độ bất định nhận thức thông qua các ensemble Q và suy
ra một giới hạn tin cậy trên (UCB) để chỉ đạo việc khám phá [7,8,24]. Khác với chúng, chúng tôi
mở rộng ý tưởng này bằng cách sử dụng U(s, a) để định lượng độ bất định. Để tránh các ước tính
không chính xác cho giá trị Q và độ bất định, chúng tôi chỉ chọn các hành động gần-on-policy
cho khám phá. Do đó, mục tiêu của chính sách khám phá πE(s) là tối đa hóa tính thông tin trong
khi duy trì gần-on-policy, tức là,
πE(s) = arg max
a∈AQθ(s, a) +βU(s, a),
s.t.||a−πϕ(s)||2≤δ,(7)
trong đó β kiểm soát mức độ lạc quan và δ kiểm soát tính on-policy. Do đó, chính sách hành vi
suy ra không chỉ có thể tăng cơ hội thực hiện các hành động thông tin theo nguyên tắc lạc quan
khi đối mặt với độ bất định, mà còn đảm bảo tính on-policy để đảm bảo sự ổn định.
Tuy nhiên, một vấn đề tối ưu hóa như vậy là không thực tế để giải quyết do hai thách thức chính.
Một mặt, không đơn giản để tìm hành động tối ưu trong không gian hành động liên tục có chiều
cao, không thể liệt kê hoàn toàn. Mặt khác, khó đặt giá trị thích hợp cho mức độ lạc quan β vì
giá trị Q và độ bất định có phạm vi giá trị khác nhau trên các nhiệm vụ khác nhau, điều này đòi
hỏi điều chỉnh siêu tham số cụ thể cho nhiệm vụ.
Để giảm thiểu cả hai thách thức trên, chúng tôi đề xuất một cơ chế lựa chọn hành động hai cấp
đơn giản nhưng hiệu quả. Đầu tiên, chúng tôi tạo một tập hành động ứng viên C={ai}N
i=1 với N
hành động ứng viên. Ở đây, hành động được tạo bằng cách lấy mẫu ai∼πϕ(·|s) cho chính sách
ngẫu nhiên, trong khi bằng cách thêm nhiễu Gaussian được lấy mẫu ai=πϕ(s) +ϵi, ϵi∼ N(0, δ)
cho chính sách deterministic. Sau đó, chúng tôi xếp hạng các hành động ứng viên theo các giá trị
Q (hoặc độ bất định) để chọn k hành động ứng viên hàng đầu như một tập hành động chung kết
Cf. Cuối cùng, chúng tôi xây dựng một phân phối categorical theo độ bất định (hoặc giá trị Q)
để chọn hành động từ Cf để tương tác với môi trường:
P(ai) :=exp (U(s, ai)/α)P
jexp (U(s, aj)/α),∀i∈[1, ..., k ], (8)
trong đó α là nhiệt độ softmax. Bằng cách thay đổi tiêu chí xếp hạng cho tập hành động chung kết
và k, chúng ta có thể điều chỉnh linh hoạt sự ưu tiên cho giá trị Q cao hoặc độ bất định cao để đạt
được sự cân bằng mong muốn.
Trong thực tế, chúng tôi quan sát thấy rằng các phương pháp RL offline dựa trên ràng buộc chính
sách có thể chứa sự ưu tiên cho giá trị Q cao hoặc độ bất định cao. Tuy nhiên, các phương pháp
dựa trên điều chỉnh giá trị có thể phân kỳ khi sự ưu tiên cho giá trị Q có mặt. Chúng tôi thảo luận
chi tiết hiện tượng này trong Phụ lục B.
Chiến lược khám phá lạc quan cho phép các agent tương tác với môi trường bằng các hành động
thông tin, do đó tăng cường hiệu quả mẫu của RL offline-đến-online. Tuy nhiên, nó cũng có thể
mang lại tác động tiêu cực bằng cách tăng thêm sự dịch chuyển phân phối state-action, như một
kết quả của nguyên tắc lạc quan khi đối mặt với độ bất định. Để đạt được mục tiêu này, chúng tôi
giới thiệu một phương pháp khai thác thích ứng trong phần tiếp theo, nhằm giảm thiểu sự dịch
chuyển phân phối state-action.
4.3 Khai Thác Thích Ứng với Nhận Diện Mẫu OOD
Sự dịch chuyển phân phối hành động đặt ra một thách thức đáng kể cho các thuật toán RL offline
[13,22,27,23], vì thuật ngữ bootstrapping trong đánh giá chính sách liên quan đến các hành động
được suy ra từ chính sách đã học πϕ. Điều này có thể dẫn đến lỗi ngoại suy do truy vấn các hàm
Q với các hành động OOD, dẫn đến cải thiện chính sách thiên lệch hướng tới những hành động
OOD này với các giá trị Q cao một cách sai lệch. Lưu ý rằng các phương pháp RL offline không
bị sự dịch chuyển phân phối trạng thái trong quá trình huấn luyện, vì đánh giá chính sách chỉ truy
vấn các hàm Q với các trạng thái có mặt trong tập dữ liệu offline.
Tuy nhiên, trong quá trình tinh chỉnh online, cả sự dịch chuyển phân phối trạng thái và hành động
đều xảy ra vì các agent có thể gặp phải các chế độ state-action không quen thuộc nằm ngoài phạm
vi hỗ trợ của tập dữ liệu. Tệ hơn nữa, vì chiến lược khám phá lạc quan được đề xuất tuân theo
nguyên tắc lạc quan khi đối mặt với độ bất định, sự dịch chuyển phân phối state-action có thể bị
trầm trọng hóa thêm. Các thử nghiệm sơ bộ từ [25, 33, 48] cho thấy rằng điều này có thể xóa bỏ
việc khởi tạo tốt thu được từ pretraining offline.
Để giải quyết vấn đề dịch chuyển phân phối state-action, chúng tôi đề xuất một phương pháp khai
thác thích ứng với nhận diện mẫu OOD. Ý tưởng cơ bản được phát triển từ các quan sát trong các
công trình trước
5

--- TRANG 5 ---
Thuật Toán 1 SUNG: Một Khung Làm Việc Hướng Dẫn Độ Bất Định Thống Nhất Đơn Giản cho RL Offline-đến-Online
1:Đầu vào: Tập dữ liệu D, Thuật toán RL Offline {Loffline
Q (θ),Loffline
π (ϕ)}
2:Khởi tạo mạng Q Qθ, mạng chính sách πϕ, mạng VAE pψ và qφ
3:while trong giai đoạn pretraining offline do
4: Lấy mẫu minibatch của các chuyển tiếp (s, a, r, s′)∼ D
5: Cập nhật θ và ϕ tối thiểu hóa Loffline
Q (θ) và Loffline
π (ϕ)
6: Cập nhật ψ, φ tối thiểu hóa LELBO (s, a;ψ, φ) trong Eq. (6)
7:end while
8:Khởi tạo OORB với BOORB ← {B =∅,D}
9:while trong giai đoạn tinh chỉnh online do
10: //Khám Phá Lạc Quan
11: Tạo tập hành động ứng viên C={ai}N
i=1
12: Chọn các hành động với giá trị Q (hoặc độ bất định) top-k như tập hành động chung kết Cf
13: Lấy mẫu hành động at∼P(ai), ai∈ Cf trong Eq.(8) theo độ bất định (hoặc giá trị Q)
14: Thực hiện hành động at, thu thập st+1 và rt từ môi trường
15: Lưu trữ các chuyển tiếp (st, at, rt, st+1) vào B và D
16: //Khai Thác Thích Ứng
17: Lấy mẫu minibatch của các chuyển tiếp (s, a, r, s′)∼ B OORB
18: Cập nhật θ và ϕ tối thiểu hóa Ladp
F(Θ) trong Eq. (9)
19: Cập nhật ψ, φ tối thiểu hóa LELBO (s, a;ψ, φ) trong Eq. (6)
20:end while
[25,33,48] rằng tinh chỉnh với các mục tiêu RL offline liên tục thường tạo ra hiệu suất ổn định
nhưng hạn chế, trong khi tinh chỉnh với các mục tiêu RL online thường tạo ra hiệu suất không ổn
định do sự dịch chuyển phân phối gây ra bởi các cặp state-action OOD. Do đó, chúng tôi đề xuất
một cách tiếp cận mới tận dụng lợi ích của cả hai mục tiêu bằng cách sử dụng các mục tiêu RL
offline bảo thủ cho các cặp state-action có độ bất định cao trong khi các mục tiêu RL online mạnh
mẽ cho các cặp state-action có độ bất định thấp. Như được hiển thị trong Eq. (5), các mục tiêu
RL offline bao gồm một mục tiêu RL online tiêu chuẩn LF(Θ) và một bộ điều chỉnh RF(Θ). Do
đó, chúng ta có thể suy ra các mục tiêu cho khai thác thích ứng bằng cách giới thiệu một bộ nhận
diện mẫu OOD hướng dẫn độ bất định I(s, a):
Ladp
F(Θ) = LF(Θ) + λI(s, πϕ(s))RF(Θ). (9)
Chúng tôi mong đợi rằng I(s, a) là một giá trị lớn cho các cặp state-action có độ bất định cao và
một giá trị nhỏ cho những cặp có độ bất định thấp. Do đó, chúng tôi xây dựng một bộ nhận diện
như vậy sử dụng ước tính độ bất định U(s, a) như dưới đây. Đối với một minibatch của các cặp
state-action {si, ai}M
i=1, chúng tôi chọn p% hàng đầu của chúng như các tập cặp state-action OOD
DOOD, theo ước tính độ bất định tương ứng U(si, ai). Quá trình lựa chọn có thể được thực hiện
bằng cách lấy mẫu từ một phân phối categorical tương tự như Eq. (8). Sau đó, để đơn giản, chúng
tôi định nghĩa bộ nhận diện mẫu OOD như
I(s, a) :=
1,nếu(s, a)∈ D OOD,
0,khác.(10)
Bằng cách đặt p= 0, mục tiêu huấn luyện chính xác là mục tiêu RL offline, trong khi đặt p= 100
dẫn đến mục tiêu RL online. Do đó, bằng cách điều chỉnh giá trị của p, phương pháp khai thác
thích ứng được đề xuất có thể đạt được sự cân bằng mong muốn giữa hiệu suất và ổn định.
4.4 Chi Tiết Triển Khai Thuật Toán
Bộ Đệm Phát Lại Offline-đến-Online. Các nghiên cứu trước [25,48] đã nhấn mạnh tầm quan
trọng của việc kết hợp cả dữ liệu offline và online trong quá trình tinh chỉnh. Ở đây, chúng tôi
áp dụng bộ đệm phát lại offline-đến-online được đề xuất trong [48] do tính đơn giản của nó. Cụ
thể, OORB bao gồm hai bộ đệm phát lại: một bộ đệm hoàn toàn online thu thập các chuyển tiếp
từ các tương tác online, và một bộ đệm offline lưu trữ các chuyển tiếp từ cả tập dữ liệu offline
và các tương tác online. Trong quá trình huấn luyện, chúng tôi lấy mẫu ngẫu nhiên các chuyển
tiếp từ OORB với một phân phối Bernoulli, trong đó xác suất lấy mẫu từ bộ đệm hoàn toàn online
là pOORB, và xác suất lấy mẫu từ bộ đệm offline là 1−pOORB.
6

--- TRANG 6 ---
Khung Làm Việc SUNG cho RL Offline-đến-Online. Kết hợp tất cả lại với nhau, chúng tôi tóm
tắt khung làm việc đầy đủ trong Thuật toán 1. SUNG đầu tiên pretrain các agent và VAE sử dụng
tập dữ liệu offline. Sau đó, nó chuyển sang tinh chỉnh online với chiến lược khám phá lạc quan
được đề xuất và phương pháp khai thác thích ứng. Chúng tôi cũng cung cấp hai trường hợp triển
khai kết nối TD3+BC và TD3, và kết nối CQL và SAC trong Phụ lục A.
5 Thử Nghiệm
Trong phần này, chúng tôi cung cấp kết quả thực nghiệm để xác minh hiệu quả của khung làm việc
của chúng tôi bằng cách trả lời ba câu hỏi chính: (1) SUNG so sánh như thế nào với các phương
pháp tiên tiến khác khi kết hợp với các phương pháp RL offline khác nhau, trên các loại nhiệm vụ
và tập dữ liệu khác nhau? (2) Các thành phần chính của SUNG ảnh hưởng như thế nào đến hiệu
suất offline-đến-online tổng thể? (3) Chúng ta nên đặt siêu tham số cho SUNG như thế nào trong
thực tế, và nhiệm vụ này khó đến mức nào?
5.1 Thiết Lập Thử Nghiệm
Cài Đặt. Chúng tôi đánh giá trên benchmark D4RL [10], cung cấp nhiều nhiệm vụ điều khiển liên
tục và tập dữ liệu khác nhau. Chúng tôi tập trung vào các miền MuJoCo và AntMaze. Chúng tôi
đầu tiên thực hiện 1M bước gradient cho pretraining offline, và sau đó thực hiện 100K bước môi
trường cho tinh chỉnh online. Trong khi một số công trình trước [21,45,48] lấy 1M bước môi trường
cho tinh chỉnh, chúng tôi lập luận rằng 1M bước môi trường thậm chí đủ cho một agent RL online
để đạt được hiệu suất cấp độ chuyên gia. Do đó, chúng tôi tin rằng tinh chỉnh trong 100K bước
môi trường là một cài đặt hợp lý hơn, cũng được áp dụng bởi [31, 48].
Các Phương Pháp RL Offline Chính. Để đánh giá tính tổng quát của SUNG, chúng tôi chọn hai
phương pháp RL offline đại diện làm xương sống cho pretraining, tức là, TD3+BC [11] và CQL [23].
Cụ thể, TD3+BC mở rộng TD3 với ràng buộc chính sách dựa trên behavior cloning, trong khi CQL
mở rộng SAC với điều chỉnh giá trị. Đối với các miền AntMaze, chúng tôi thay thế TD3+BC bằng
SPOT [39] do hiệu suất kém của nó. SPOT là một phần mở rộng khác của TD3 nhưng với ràng
buộc chính sách hướng dẫn mật độ.
Các Baseline. Chúng tôi so sánh SPOT với các baseline sau: (1) offline-ft tận dụng các tương tác
online bằng cách thực hiện các mục tiêu RL offline. (2) online-ft tận dụng các tương tác online
bằng cách thực hiện các mục tiêu RL online tương ứng với những mục tiêu được sử dụng cho pretraining.
(3) BR [25] ưu tiên các chuyển tiếp gần-on-policy từ bộ đệm phát lại. (4) O3F [33] sử dụng kiến
thức chứa trong các hàm giá trị để hướng dẫn khám phá. (5) APL [48] thực hiện học chính sách
thích ứng bằng cách kết hợp các lợi thế khác nhau của dữ liệu offline và online. (6) PEX [45] huấn
luyện một chính sách mới từ đầu bằng cách sử dụng chính sách được huấn luyện trước để khám
phá. Lưu ý rằng chúng tôi cũng bao gồm ODT [49] để so sánh với một phần riêng biệt trong Phụ
lục D.2. Xem Phụ lục C để biết thêm chi tiết triển khai.
5.2 So Sánh trên Các Benchmark D4RL
Bảng 1: So sánh điểm số D4RL trung bình trên các nhiệm vụ MuJoCo với TD3+BC làm phương
pháp RL offline xương sống. r = random, m = medium, m-r = medium-replay. Chúng tôi báo cáo
giá trị trung bình và độ lệch chuẩn trên 5 seed.
TD3+BC offline-ft online-ft BR O3F APL PEX SUNG
halfcheetah-r-v2 11.5 34.3 ±2.4 57.7±1.6 67.6±11.9 71.4±3.3 70.0±4.7 53.4±9.1 76.6±2.0
hopper-r-v2 8.7 8.2 ±0.2 11.2±1.9 25.7±8.7 11.7±2.0 27.1±14.0 37.4±10.6 38.7±15.0
walker2d-r-v2 5.4 7.0 ±4.1 6.2±3.9 9.9±4.6 11.6±5.1 13.8±4.0 33.1±16.4 14.1±5.1
halfcheetah-m-v2 48.0 49.3 ±0.4 67.6±2.4 79.5±7.4 77.8±1.1 80.9±2.0 52.3±21.1 80.7±2.5
hopper-m-v2 61.5 58.8 ±3.9 78.3±32.7 93.1±10.8 102.0 ±2.0 76.9±24.2 73.9±17.8 101.8 ±6.0
walker2d-m-v2 82.2 84.6 ±1.2 68.2±11.5 70.1±21.4 97.1±2.2 98.2±13.5 56.7±28.5 113.5 ±1.9
halfcheetah-m-r-v2 44.6 47.0 ±0.9 66.3±1.0 65.0±14.6 67.6±2.9 71.5±1.3 53.2±9.8 69.7±3.4
hopper-m-r-v2 55.9 85.4 ±7.6 89.9±13.5 97.2±13.9 97.6±4.9 100.6 ±9.8 90.8±18.7 101.3 ±7.0
walker2d-m-r-v2 71.7 80.2 ±8.7 87.4±4.0 82.7±17.8 100.9 ±3.7 108.2 ±3.6 70.6±13.3 109.2 ±1.9
Tổng 389.6 454.9 532.9 590.9 637.7 647.1 521.2 705.7
Kết Quả trên MuJoCo. Kết quả cho các miền MuJoCo với TD3+BC và CQL làm phương pháp RL
offline xương sống được hiển thị trong Bảng 1 và 2, tương ứng. Chúng tôi thấy rằng các tương
tác online bổ sung có thể mang lại cải thiện hiệu suất thêm trong hầu hết các cài đặt. Tuy nhiên,
hai ngoại lệ là O3F và PEX,
7

--- TRANG 7 ---
Bảng 2: So sánh điểm số D4RL trung bình trên các nhiệm vụ MuJoCo với CQL làm phương pháp
RL offline xương sống. r = random, m = medium, m-r = medium-replay. Chúng tôi báo cáo giá
trị trung bình và độ lệch chuẩn trên 5 seed. Lưu ý rằng O3F bị bỏ qua do sự phân kỳ của nó trong
cài đặt này.
CQL offline-ft online-ft BR APL PEX SUNG
halfcheetah-r-v2 23.5 28.8 ±2.3 50.2±1.5 61.8±12.3 67.7±9.6 50.6±2.2 69.1±9.2
hopper-r-v2 6.4 31.2 ±0.5 28.3±6.2 23.8±7.9 41.8±22.0 34.3±8.9 44.3±11.7
walker2d-r-v2 4.5 5.6 ±3.4 8.2±5.6 4.0±2.3 6.3±1.8 10.7±2.8 14.5±6.1
halfcheetah-m-v2 48.1 48.9 ±0.2 52.1±25.6 56.7±28.5 44.7±38.5 43.5±2.4 79.7±1.0
hopper-m-v2 73.7 74.1 ±1.4 91.0±10.4 97.7±3.7 102.7 ±3.1 46.3±15.1 104.1 ±1.3
walker2d-m-v2 84.3 83.5 ±0.7 85.6±7.6 81.7±14.0 75.3±25.7 34.0±17.3 86.0±12.6
halfcheetah-m-r-v2 46.9 49.5 ±0.3 61.3±1.0 64.9±5.8 78.6±1.2 45.5±1.7 75.6±1.9
hopper-m-r-v2 96.0 95.0 ±1.0 92.8±19.5 88.5±21.8 97.4±9.5 66.5±24.2 101.9 ±9.1
walker2d-m-r-v2 83.4 84.5 ±1.0 86.9±12.4 78.8±28.0 103.2 ±19.0 40.1±17.9 108.2 ±4.2
Tổng 466.9 501.1 556.4 558.0 617.8 371.5 683.4
khi kết hợp với CQL. Như được chỉ ra trong [33], O3F không tương thích với các phương pháp RL
offline dựa trên điều chỉnh giá trị. Điều này cũng được quan sát bởi [35] và chúng tôi cung cấp
giải thích chi tiết trong Phụ lục B. Ngược lại, PEX huấn luyện một chính sách mới từ đầu, và
không thể tận dụng đầy đủ việc tái sử dụng khởi tạo tốt từ pretraining, do đó dẫn đến hiệu suất
hạn chế.
Hơn nữa, chúng tôi nhận xét rằng SUNG vượt trội đáng kể so với các phương pháp tiên tiến nhất
trước đây, thể hiện thêm 15.04% và 14.05% cải thiện offline-đến-online so với baseline hoạt động
tốt nhất khi kết hợp với TD3+BC và CQL, tương ứng. Điều này chứng minh sự cần thiết của việc
ước tính và sử dụng thích hợp độ bất định để giải quyết cả hành vi khám phá bị hạn chế và sự dịch
chuyển phân phối state-action. Hơn nữa, điều này cũng xác minh tính tổng quát của SUNG trong
việc tạo điều kiện cải thiện online cho các agent được huấn luyện trước với các phương pháp RL
offline khác nhau.
Kết Quả trên AntMaze. Chúng tôi cũng cung cấp kết quả cho các miền AntMaze thách thức hơn
với SPOT và CQL làm phương pháp RL offline xương sống. Chúng tôi hoãn kết quả và phân tích
chi tiết đến Phụ lục D.1. Chúng tôi quan sát thấy rằng SUNG vượt trội so với tất cả các baseline
trong hầu hết các cài đặt.
5.3 Nghiên Cứu Loại Bỏ
Trong phần này, chúng tôi thực hiện một nghiên cứu loại bỏ về các thành phần trong SUNG trong
Hình 2. Tham khảo Phụ lục D.4 để biết chi tiết triển khai và các nghiên cứu loại bỏ bổ sung.
Loại Bỏ về Khám Phá Lạc Quan. Đầu tiên, chúng tôi đánh giá (a)SUNG không có chiến lược
khám phá lạc quan, và thấy rằng việc loại bỏ mang lại tác động tiêu cực đáng kể khi kết hợp với
TD3+BC, nhưng không có tác động đáng chú ý khi kết hợp với CQL. Lý do cho sự khác biệt này
là CQL được xây dựng trên đỉnh SAC, có thể dần dần phục hồi sức mạnh khám phá thông qua
khung làm việc RL entropy cực đại trong quá trình tinh chỉnh online. Sau đó, chúng tôi loại bỏ
(b)độ bất định và (c)giá trị Q trong chiến lược khám phá lạc quan để suy ra hai chiến lược biến
thể - một tham lam cho giá trị Q và một khác tham lam cho độ bất định. Như mong đợi, cả hai
việc loại bỏ đều làm giảm hiệu suất ở mức độ khác nhau, nhấn mạnh tầm quan trọng của cả giá
trị Q và độ bất định cho khám phá. Đáng chú ý, việc loại bỏ độ bất định trong SUNG khi kết hợp
với CQL gây ra sự suy giảm hiệu suất đáng kể, có thể được quy cho lý do tương tự được chi tiết
trong Phụ lục B.
Loại Bỏ về Khai Thác Thích Ứng. Chúng tôi đánh giá (d)SUNG không có phương pháp khai thác
thích ứng, và quan sát thấy rằng hiệu suất cho tinh chỉnh TD3+BC và CQL suy giảm đáng kể trên
hầu hết các nhiệm vụ. Điều này nhấn mạnh sự cần thiết của việc giải quyết vấn đề dịch chuyển
phân phối state-action.
Loại Bỏ về Định Lượng Độ Bất Định. Cuối cùng, chúng tôi (e)thay thế bộ ước tính mật độ dựa
trên VAE bằng độ lệch chuẩn của các giá trị Q kép để định lượng độ bất định [8]. Như mong đợi,
kết quả cho thấy rằng việc thay thế như vậy dẫn đến sự suy giảm hiệu suất trên hầu hết các nhiệm
vụ, vì nó không đủ để cung cấp định lượng độ bất định đáng tin cậy. Trong khi việc sử dụng kỹ
thuật ensemble có thể loại bỏ vấn đề này [24,7], nó có thể tăng đáng kể chi phí tính toán. Chúng
tôi để lại việc khám phá các phương pháp thay thế như công việc tương lai.
8

--- TRANG 8 ---
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr40
20
02040SUNG (TD3+BC) 
 Phần Trăm Khác Biệt(a)
w/o Opt. Exploration
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040(b)
Opt. Exploration w/o Unc.
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040(c)
Opt. Exploration w/o Q
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040(d)
w/o Adp. Exploitation
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040(e)
Unc. Estimation w/ Q std
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr40
20
02040SUNG (CQL) 
 Phần Trăm Khác Biệtw/o Opt. Exploration
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040Opt. Exploration w/o Unc.
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040Opt. Exploration w/o Q
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040w/o Adp. Exploitation
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040Unc. Estimation w/ Q stdHình 2: Khác biệt hiệu suất của một nghiên cứu loại bỏ của SUNG kết hợp với TD3+BC và CQL,
so với thuật toán đầy đủ. Opt. = Optimistic, Unc. = Uncertainty, Adp. = Adaptive. hc =
HalfCheetah, hop = Hopper, w = Walker, r = random, m = medium, mr = medium-replay.
10 20 40 80
k707580Điểm Số D4RL Trung Bình
SUNG (TD3+BC)
10 20 40 80
k50607080
SUNG (CQL)
Uncertainty first Q first best baseline
Hình 3: So sánh hiệu suất với thứ tự khác
nhau của tiêu chí xếp hạng và kích thước tập
hành động chung kết k được sử dụng trong
khám phá lạc quan.
1 5 10 20 50
p707580Điểm Số D4RL Trung Bình
SUNG (TD3+BC)
1 5 10 20 50
p50607080
SUNG (CQL)
best baselineHình 4: So sánh hiệu suất với phần trăm p khác
nhau của các mẫu OOD được xác định từ mini-batch
được sử dụng trong khai thác thích ứng.
5.4 Phân Tích Siêu Tham Số
Phân Tích về Khám Phá Lạc Quan. Đầu tiên, chúng tôi điều tra thứ tự của các tiêu chí xếp hạng
và kích thước tập hành động chung kết k trong khám phá lạc quan. Kết quả tổng hợp trung bình
trên các miền MuJoCo được hiển thị trong Hình 3. Như mong đợi trong Phụ lục B, SUNG với
TD3+BC có thể chứa sự ưu tiên cho cả độ bất định cao và giá trị cao, trong khi SUNG với CQL
chỉ có thể chứa sự ưu tiên cho độ bất định cao. Do đó, chúng tôi chọn thứ tự với Q đầu tiên cho
TD3+BC, và độ bất định đầu tiên cho CQL. Bên cạnh đó, chúng tôi nhận xét rằng SUNG luôn vượt
trội so với baseline tốt nhất với bất kỳ lựa chọn k nào, điều này tiếp tục xác minh sự vượt trội của
SUNG.
Phân Tích về Khai Thác Thích Ứng. Hơn nữa, chúng tôi khám phá lựa chọn phần trăm p của các
mẫu OOD được xác định từ mini-batch trong khai thác thích ứng. Kết quả tổng hợp trung bình
trên các miền MuJoCo được hiển thị trong Hình 4. Dự đoán được, việc đặt p quá bảo thủ hoặc
quá mạnh mẽ có thể làm giảm hiệu suất một cách bất lợi. Chúng tôi thấy rằng p= 5 hoạt động tốt
nhất khi kết hợp với TD3+BC hoặc CQL, đạt được sự cân bằng mong muốn giữa hiệu suất và ổn
định.
6 Kết Luận, Hạn Chế, và Tác Động Rộng Hơn
Bài báo này nghiên cứu cách tinh chỉnh hiệu quả các agent RL offline được huấn luyện trước thông
qua các tương tác online. Chúng tôi trình bày SUNG, một khung làm việc hướng dẫn độ bất định
thống nhất đơn giản, để giải quyết cả hai thách thức của hành vi khám phá bị hạn chế và sự dịch
chuyển phân phối state-action. SUNG tận dụng độ bất định được định lượng để hướng dẫn khám
phá lạc quan và khai thác thích ứng. Kết quả thực nghiệm trên các phương pháp RL offline xương
sống khác nhau, môi trường và tập dữ liệu xác minh sự vượt trội của SUNG.
9

--- TRANG 9 ---
Về mặt hạn chế, khung làm việc offline-đến-online được đề xuất không thể được áp dụng trực tiếp
cho các tiến bộ gần đây trong RL offline, như các phương pháp dựa trên RL có điều kiện phần
thưởng [6,9] và các phương pháp on-policy [50]. Tuy nhiên, chúng tôi lập luận rằng những hiểu
biết của SUNG có thể áp dụng chung về nguyên tắc. Chúng tôi để điều này như công việc tương
lai. Về tác động xã hội, chúng tôi không dự đoán bất kỳ hậu quả tiêu cực nào từ việc sử dụng
khung làm việc của chúng tôi trong thực tế. Chúng tôi cung cấp thảo luận chi tiết hơn trong Phụ
lục E.
10

--- TRANG 10 ---
Tài Liệu Tham Khảo
[1]Gaon An, Seungyong Moon, Jang-Hyun Kim, và Hyun Oh Song. Học tăng cường offline dựa
trên độ bất định với q-ensemble đa dạng. Advances in Neural Information Processing
Systems, 34:7436–7447, 2021.
[2]Jean-Yves Audibert, Rémi Munos, và Csaba Szepesvári. Cân bằng khám phá–khai thác
sử dụng ước tính phương sai trong bandit đa tay. Theoretical Computer Science, 410(19):
1876–1902, 2009.
[3]Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhi-Hong Deng, Animesh Garg, Peng Liu, và
Zhaoran Wang. Bootstrapping bi quan cho học tăng cường offline hướng dẫn độ bất định.
InInternational Conference on Learning Representations, 2022.
[4]Ronen I Brafman và Moshe Tennenholtz. R-max-một thuật toán thời gian đa thức tổng quát
cho học tăng cường gần tối ưu. Journal of Machine Learning Research, 3(Oct):213–231,
2002.
[5]Kaylee Burns, Tianhe Yu, Chelsea Finn, và Karol Hausman. Học tăng cường offline ở nhiều
tần số. In Conference on Robot Learning, pages 2041–2051, 2023.
[6]Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter
Abbeel, Aravind Srinivas, và Igor Mordatch. Decision transformer: Học tăng cường thông
qua mô hình hóa chuỗi. Advances in Neural Information Processing Systems, 34:15084–15097,
2021.
[7]Richard Y Chen, Szymon Sidor, Pieter Abbeel, và John Schulman. Khám phá ucb thông qua
q-ensembles. arXiv preprint arXiv:1706.01502, 2017.
[8]Kamil Ciosek, Quan Vuong, Robert Loftin, và Katja Hofmann. Khám phá tốt hơn với
actor critic lạc quan. Advances in Neural Information Processing Systems, 32, 2019.
[9]Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, và Sergey Levine. Rvs: Điều gì là cần
thiết cho rl offline thông qua học có giám sát? In International Conference on Learning Representations,
2022.
[10] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, và Sergey Levine. D4rl: Tập dữ liệu
cho học tăng cường sâu dựa trên dữ liệu. arXiv preprint arXiv:2004.07219, 2020.
[11] Scott Fujimoto và Shixiang Shane Gu. Một cách tiếp cận tối giản cho học tăng cường offline.
Advances in Neural Information Processing Systems, 34:20132–20145, 2021.
[12] Scott Fujimoto, Herke Hoof, và David Meger. Giải quyết lỗi xấp xỉ hàm trong các phương
pháp actor-critic. In International Conference on Machine Learning, pages 1587–1596,
2018.
[13] Scott Fujimoto, David Meger, và Doina Precup. Học tăng cường sâu off-policy không có
khám phá. In International Conference on Machine Learning, pages 2052–2062,
2019.
[14] Kamyar Ghasemipour, Shixiang Shane Gu, và Ofir Nachum. Tại sao lại bi quan? ước tính
độ bất định cho rl offline thông qua ensembles, và tại sao sự độc lập của chúng quan trọng. Advances in
Neural Information Processing Systems, 35:18267–18281, 2022.
[15] Kaiyang Guo, Shao Yunfeng, và Yanhui Geng. Học tăng cường offline dựa trên mô hình
với niềm tin động học được điều chỉnh bi quan. Advances in Neural Information Processing
Systems, 35:449–461, 2022.
[16] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan,
Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Thuật toán và ứng dụng
soft actor-critic. arXiv preprint arXiv:1812.05905, 2018.
[17] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, và Thorsten Joachims. Morel:
Học tăng cường offline dựa trên mô hình. Advances in Neural Information Processing
Systems, 33:21810–21823, 2020.
11

--- TRANG 11 ---
[18] Diederik P Kingma và Max Welling. Auto-encoding variational bayes. International Confer-
ence on Learning Representations, 2013.
[19] Diederik P Kingma, Max Welling, et al. Giới thiệu về autoencoders biến phân. Founda-
tions and Trends® in Machine Learning, 12(4):307–392, 2019.
[20] Ilya Kostrikov, Rob Fergus, Jonathan Tompson, và Ofir Nachum. Học tăng cường offline
với điều chỉnh critic phân kỳ fisher. In International Conference on Machine
Learning, pages 5774–5783, 2021.
[21] Ilya Kostrikov, Ashvin Nair, và Sergey Levine. Học tăng cường offline với học q ngầm.
In International Conference on Learning Representations, 2022.
[22] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, và Sergey Levine. Ổn định học
q off-policy thông qua giảm lỗi bootstrapping. Advances in Neural Information Processing
Systems, 32, 2019.
[23] Aviral Kumar, Aurick Zhou, George Tucker, và Sergey Levine. Học q bảo thủ
cho học tăng cường offline. Advances in Neural Information Processing Systems, 33:
1179–1191, 2020.
[24] Kimin Lee, Michael Laskin, Aravind Srinivas, và Pieter Abbeel. Sunrise: Một khung làm việc
thống nhất đơn giản cho học ensemble trong học tăng cường sâu. In International Conference
on Machine Learning, pages 6131–6141, 2021.
[25] Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, và Jinwoo Shin. Học tăng cường
offline-đến-online thông qua phát lại cân bằng và q-ensemble bi quan. In Conference on Robot
Learning, pages 1702–1712, 2022.
[26] Seunghyun Lee, Da Young Lee, Sujeong Im, Nan Hee Kim, và Sung-Min Park. Clinical
decision transformer: Khuyến nghị điều trị dự định thông qua prompting mục tiêu. arXiv
preprint arXiv:2302.00612, 2023.
[27] Sergey Levine, Aviral Kumar, George Tucker, và Justin Fu. Học tăng cường offline:
Hướng dẫn, đánh giá, và quan điểm về các vấn đề mở. arXiv preprint arXiv:2005.01643, 2020.
[28] Owen Lockwood và Mei Si. Một đánh giá về độ bất định cho học tăng cường sâu. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment,
volume 18, pages 155–162, 2022.
[29] Cong Lu, Philip J Ball, Tim GJ Rudner, Jack Parker-Holder, Michael A Osborne, và Yee Whye
Teh. Thách thức và cơ hội trong học tăng cường offline từ quan sát trực quan.
arXiv preprint arXiv:2206.04779, 2022.
[30] Yicheng Luo, Jackie Kay, Edward Grefenstette, và Marc Peter Deisenroth. Tinh chỉnh từ
học tăng cường offline: Thách thức, cân bằng và giải pháp thực tế. arXiv preprint
arXiv:2303.17396, 2023.
[31] Jiafei Lyu, Xiaoteng Ma, Xiu Li, và Zongqing Lu. Học q nhẹ nhàng bảo thủ cho học tăng cường
offline. In Thirty-sixth Conference on Neural Information Processing Systems,
2022.
[32] Yihuan Mao, Chao Wang, Bin Wang, và Chongjie Zhang. Moore: Học tăng cường offline-đến-
online dựa trên mô hình. arXiv preprint arXiv:2201.10070, 2022.
[33] Max Sobol Mark, Ali Ghadirzadeh, Xi Chen, và Chelsea Finn. Tinh chỉnh các chính sách
offline với lựa chọn hành động lạc quan. In Deep Reinforcement Learning Workshop NeurIPS, 2022.
[34] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, và Sergey Levine. Awac: Tăng tốc học tăng cường
online với tập dữ liệu offline. arXiv preprint arXiv:2006.09359, 2020.
[35] Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn,
Aviral Kumar, và Sergey Levine. Cal-ql: Pretrain rl offline được hiệu chuẩn cho tinh chỉnh
online hiệu quả. arXiv preprint arXiv:2303.05479, 2023.
12

--- TRANG 12 ---
[36] Shideh Rezaeifar, Robert Dadashi, Nino Vieillard, Léonard Hussenot, Olivier Bachem, Olivier
Pietquin, và Matthieu Geist. Học tăng cường offline như chống khám phá. In Proceedings
of the AAAI Conference on Artificial Intelligence, pages 8106–8114, 2022.
[37] Danilo Jimenez Rezende, Shakir Mohamed, và Daan Wierstra. Backpropagation ngẫu nhiên
và suy luận xấp xỉ trong các mô hình generative sâu. In International Conference on Machine
Learning, pages 1278–1286, 2014.
[38] Juergen Schmidhuber. Học tăng cường ngược: Đừng dự đoán phần thưởng–chỉ cần ánh xạ
chúng thành hành động. arXiv preprint arXiv:1912.02875, 2019.
[39] Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, và Mingsheng Long. Tối ưu hóa chính sách
được hỗ trợ cho học tăng cường offline. In Advances in Neural Information Processing
Systems, 2022.
[40] Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua M Susskind, Jian Zhang, Ruslan Salakhutdi-
nov, và Hanlin Goh. Actor-critic có trọng số độ bất định cho học tăng cường offline. In
International Conference on Machine Learning, pages 11319–11328, 2021.
[41] Haoran Xu, Li Jiang, Li Jianxiong, và Xianyuan Zhan. Một cách tiếp cận bắt chước hướng dẫn
chính sách cho học tăng cường offline. Advances in Neural Information Processing Systems, 35:
4085–4098, 2022.
[42] Haoran Xu, Li Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Wai Kin Chan,
và Xianyuan Zhan. Rl offline không có hành động ood: Học trong mẫu thông qua điều chỉnh
giá trị ngầm. In International Conference on Learning Representations, 2023.
[43] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn, và Tengyu Ma. Mopo: Tối ưu hóa chính sách offline dựa trên mô hình. Advances in Neural
Information Processing Systems, 33:14129–14142, 2020.
[44] Zishun Yu và Xinhua Zhang. Căn chỉnh actor-critic cho học tăng cường offline-đến-online.
2023.
[45] Haichao Zhang, We Xu, và Haonan Yu. Mở rộng chính sách để kết nối học tăng cường
offline-đến-online. The International Conference on Learning Representations, 2023.
[46] Kesen Zhao, Lixin Zou, Xiangyu Zhao, Maolin Wang, et al. Khuyến nghị hướng duy trì người
dùng với decision transformer. Proceedings of the ACM Web Conference, 2023.
[47] Yi Zhao, Rinu Boney, Alexander Ilin, Juho Kannala, và Joni Pajarinen. Điều chỉnh behavior
cloning thích ứng cho học tăng cường offline-đến-online ổn định. arXiv preprint
arXiv:2210.13846, 2022.
[48] Han Zheng, Xufang Luo, Pengfei Wei, Xuan Song, Dongsheng Li, và Jing Jiang. Học
chính sách thích ứng cho học tăng cường offline-đến-online. In Proceedings of the AAAI
Conference on Artificial Intelligence, 2023.
[49] Qinqing Zheng, Amy Zhang, và Aditya Grover. Decision transformer online. In International
Conference on Machine Learning, pages 27042–27059, 2022.
[50] Zifeng Zhuang, Kun LEI, Jinxin Liu, Donglin Wang, và Yilang Guo. Tối ưu hóa chính sách
proximal hành vi. In International Conference on Learning Representations, 2023. URL
https://openreview.net/forum?id=3c13LptpIph.
13

--- TRANG 13 ---
Phụ Lục
A Chi Tiết Triển Khai của SUNG
Trong phần này, chúng tôi cung cấp hai trường hợp triển khai của SUNG: một kết nối TD3+BC [11]
và TD3 [12], và một khác kết nối CQL [23] và SAC [16]. Lưu ý rằng các phương pháp RL offline
hiện tại thường được xây dựng trên các phương pháp RL online off-policy. Do đó, chúng tôi sử
dụng các mục tiêu RL online tương ứng cho tinh chỉnh online để giữ nhất quán.
A.1 SUNG (TD3+BC): Kết Nối TD3+BC và TD3
Đầu tiên, chúng tôi cho thấy cách SUNG (TD3+BC) kết nối TD3+BC và TD3 để tinh chỉnh chính
sách deterministic πϕ(s) và hai hàm giá trị Q Qθ1(s, a), Qθ2(s, a).
Về khám phá, chính sách hành vi của SUNG (TD3+BC) tương thích với sự ưu tiên cho giá trị
Q cao hoặc độ bất định cao, như được chứng minh trong phần chính. Ở đây, chúng tôi lấy sự ưu
tiên cho giá trị Q cao làm ví dụ. Vì TD3 sử dụng chính sách deterministic, chúng tôi đầu tiên tạo
một tập hành động ứng viên C={ai=πϕ(s) +ϵi}N
i=1, trong đó ϵi∼ N(0, δ) là nhiễu Gaussian.
Sau đó, các hành động ứng viên có giá trị Q top-k được chọn như tập hành động chung kết Cf.
Cuối cùng, chúng tôi lấy mẫu hành động để tương tác từ một phân phối categorical P(ai), ai∈ Cf
trong Eq. (8) theo độ bất định. Trong thực tế, chúng tôi đặt N= 100 và k= 10 để tương tác với
môi trường bằng cách chọn các hành động có giá trị cao và độ bất định tương đối cao.
Về khai thác, SUNG (TD3+BC) tuân theo TD3+BC và TD3 để học TD tiêu chuẩn cho đánh giá
chính sách, và áp dụng phương pháp khai thác thích ứng được đề xuất cho cải thiện chính sách như
Ladp,TD3+BC
π (ϕ) =Es∼BOORB [−Qθ(s, πϕ(s))] + λE(s,a)∼BOORB[
I(s, πϕ(s)) (πϕ(s)−a)2].
(11)
Tích hợp các chi tiết trên, chúng tôi đưa ra mô tả thuật toán chi tiết của SUNG (TD3+BC) trong
Thuật toán 2.
A.2 SUNG (CQL): Kết Nối CQL và SAC
Sau đó, chúng tôi cho thấy cách SUNG (CQL) kết nối CQL và SAC để tinh chỉnh chính sách ngẫu
nhiên πϕ(·|s) và hai hàm giá trị Q Qθ1(s, a), Qθ2(s, a).
Về khám phá, chính sách hành vi của SUNG (CQL) chỉ tương thích với sự ưu tiên cho độ bất
định cao, như được chứng minh trong phần chính. Vì SAC sử dụng chính sách ngẫu nhiên, chúng
tôi đầu tiên tạo một tập hành động ứng viên C={ai∼πϕ(·|s)}N
i=1. Sau đó, các hành động ứng
viên có độ bất định top-k được chọn như tập hành động chung kết Cf. Cuối cùng, chúng tôi lấy
mẫu hành động để tương tác từ một phân phối categorical P(ai), ai∈ Cf tương tự như Eq. (8)
theo giá trị Q. Trong thực tế, chúng tôi đặt N= 100 và k= 20. Lưu ý rằng chúng tôi tăng nhẹ
giá trị của k để tránh các hành động được chọn có độ bất định quá cao cho hiệu quả mẫu. Do đó,
chúng ta có thể tương tác với môi trường bằng cách chọn các hành động có độ bất định tương đối
cao và giá trị Q tương đối cao.
Về khai thác, SUNG (CQL) tuân theo CQL và SAC để thực hiện cải thiện chính sách như
LSAC
π(ϕ) =Es∼BOORB ,a∼πϕ(·|s)[Qθ(s, a)−logπϕ(a|s)], (12)
và áp dụng phương pháp khai thác thích ứng được đề xuất cho đánh giá chính sách như
Ladp,CQL
Q (θ) =E(s,a,r,s′)∼BOORB[
(Qθ(s, a)−r−γQ¯θ(s′, πϕ(s′)))2]
+λEs∼BOORB
I(s, πϕ(s))
Ea∼πϕ[Qθ(s, a)]−Ea∼BOORB [Qθ(s, a)]
,(13)
trong đó πϕ(s) biểu thị trung bình của chính sách ngẫu nhiên Gaussian. Tích hợp các chi tiết
trên, chúng tôi đưa ra mô tả thuật toán chi tiết của SUNG (CQL) trong Thuật toán 3.
14

--- TRANG 14 ---
Thuật Toán 2 SUNG (TD3+BC): Kết Nối TD3+BC và TD3
1:Đầu vào: Tập dữ liệu D={(s, a, r, s′)}
2://Pretraining Offline
3:Khởi tạo mạng Q Qθ, mạng chính sách πϕ, mạng VAE với pψ và qφ
4:fort= 1toT1do
5: Lấy mẫu minibatch của các chuyển tiếp (s, a)∼ D
6: Cập nhật θ và ϕ tối thiểu hóa LQ(θ) trong Eq. (1) và LTD3+BC
π (ϕ) trong Eq. (3)
7: Cập nhật ψ, φ tối thiểu hóa LELBO (s, a;ψ, φ) trong Eq. (6)
8:end for
9://Tinh Chỉnh Online
10:Khởi tạo OORB với BOORB ← {B =∅,D}
11:fort= 1toT2do
12: //Khám Phá Lạc Quan
13: Tạo tập hành động ứng viên C={ai}N
i=1, trong đó ai=πϕ(st) +ϵi,ϵi∼ N(0, δ)
14: Chọn các hành động có giá trị Q top-k như tập hành động chung kết Cf
15: Lấy mẫu hành động at∼P(ai), ai∈ Cf trong Eq. (8) theo độ bất định
16: Thực hiện hành động at, thu thập st+1 và rt từ môi trường
17: Lưu trữ các chuyển tiếp (st, at, rt, st+1) vào B và D
18: //Khai Thác Thích Ứng
19: Lấy mẫu minibatch của các chuyển tiếp (s, a, r, s′)∼ B OORB
20: Xây dựng bộ nhận diện OOD I trong Eq. (10)
21: Cập nhật θ tối thiểu hóa LQ(θ) trong Eq. (1)
22: Cập nhật ϕ tối thiểu hóa Ladp,TD3+BC
π (ϕ) trong Eq. (11)
23: Cập nhật ψ, φ tối thiểu hóa LELBO (s, a;ψ, φ) trong Eq. (6)
24:end for
Thuật Toán 3 SUNG (CQL): Kết Nối CQL và SAC
1:Đầu vào: Tập dữ liệu D={(s, a, r, s′)}
2://Pretraining Offline
3:Khởi tạo mạng Q Qθ, mạng chính sách πϕ, mạng VAE với pψ và qφ
4:fort= 1toT1do
5: Lấy mẫu minibatch của các chuyển tiếp (s, a)∼ D
6: Cập nhật θ và ϕ tối thiểu hóa LCQL
Q(θ) trong Eq. (4) và LSAC
π(ϕ) trong Eq. (12)
7: Cập nhật ψ, φ tối thiểu hóa LELBO (s, a;ψ, φ) trong Eq. (6)
8:end for
9://Tinh Chỉnh Online
10:Khởi tạo OORB với BOORB ← {B =∅,D}
11:fort= 1toT2do
12: //Khám Phá Lạc Quan
13: Tạo tập hành động ứng viên C={ai∼πϕ(·|s)}N
i=1
14: Chọn các hành động có độ bất định top-k như tập hành động chung kết Cf
15: Lấy mẫu hành động at∼P(ai), ai∈ Cf tương tự như Eq. (8) theo giá trị Q
16: Thực hiện hành động at, thu thập st+1 và rt từ môi trường
17: Lưu trữ các chuyển tiếp (st, at, rt, st+1) vào B và D
18: //Khai Thác Thích Ứng
19: Lấy mẫu minibatch của các chuyển tiếp (s, a, r, s′)∼ B OORB
20: Xây dựng bộ nhận diện OOD I trong Eq. (10)
21: Cập nhật θ tối thiểu hóa Ladp,CQL
Q (θ) trong Eq. (13)
22: Cập nhật ϕ tối thiểu hóa LSAC
π(ϕ) trong Eq. (12)
23: Cập nhật ψ, φ tối thiểu hóa LELBO (s, a;ψ, φ) trong Eq. (6)
24:end for
15

--- TRANG 15 ---
B Tại Sao Khám Phá với Sự Ưu Tiên cho Giá Trị Cao Thất Bại Khi Tinh Chỉnh
CQL?
Trong kết quả thực nghiệm, chúng tôi thấy rằng O3F [33], chọn các hành động với sự ưu tiên cho
giá trị Q cao trong quá trình khám phá, thất bại khi kết hợp với CQL [23]. Bên cạnh đó, SUNG
với sự ưu tiên cho giá trị Q cao để khám phá cũng thất bại khi kết hợp với CQL. Bây giờ, chúng
tôi đưa ra một phân tích định tính về lý do này. Để bắt đầu, chúng tôi nhớ lại việc triển khai thực
tế của đánh giá chính sách trong CQL:
arg min
QEs∈D"
logX
aexp (Q(s, a))−Ea∼πβ(a|s)[Q(s, a)]#
+E(s,a,r,s′)∼D
(Q(s, a)−y)2
,
(14)
trong đó πβ biểu thị chính sách hành vi của tập dữ liệu offline và y biểu thị mục tiêu TD tiêu chuẩn
cho đánh giá chính sách.
Có ba thuật ngữ trong phương trình trên: cho một trạng thái, thuật ngữ đầu tiên tối thiểu hóa các
giá trị Q của tất cả các hành động; thuật ngữ thứ hai tối đa hóa các giá trị Q của hành động được
thu thập trong tập dữ liệu; thuật ngữ thứ ba là học TD tiêu chuẩn. Do đó, cho trạng thái s, khi
chúng ta chọn hành động giá trị cao ah để khám phá, chuyển tiếp (s, ah, s′, r) sẽ được thu thập
vào bộ đệm phát lại. Sau đó, trong quá trình khai thác, sự kết hợp của thuật ngữ đầu tiên và thứ
hai trong phương trình dẫn đến một tình huống mà hầu hết các hành động chưa thấy có giá trị
thấp và sẽ có giá trị thấp hơn do thuật ngữ đầu tiên, trong khi một số hành động đã thấy có giá
trị cao và sẽ có giá trị cao hơn do thuật ngữ thứ hai. Vì thuật ngữ thứ ba liên quan đến bootstrapping,
hành động giá trị cao sẽ được cập nhật để có giá trị thấp hơn, dẫn đến sự giảm nhất quán trong
các giá trị của tất cả các hành động với mỗi bước gradient. Điều này cuối cùng có thể gây ra sự
sụp đổ của các hàm giá trị Q, dẫn đến chính sách kém thông qua lan truyền trong cải thiện chính
sách.
Chúng tôi nhận xét rằng SUNG có thể giải quyết vấn đề này bằng cách thay đổi tiêu chí xếp hạng
cho tập hành động chung kết và k để thể hiện sự ưu tiên cho độ bất định cao trong khi giá trị Q
tương đối cao trong quá trình khám phá.
C Chi Tiết Thử Nghiệm Thêm
C.1 Chi Tiết Cài Đặt Thử Nghiệm
Đối với các nhiệm vụ MuJoCo, chúng tôi tuân theo công trình trước [45] để bỏ qua đánh giá trên
các tập dữ liệu medium-expert và expert, nơi các agent được huấn luyện trước offline đã có thể đạt
được hiệu suất cấp độ chuyên gia và không cần tinh chỉnh thêm. Tất cả các đánh giá sử dụng các
tập dữ liệu của phiên bản "-v2" được phát hành mới nhất. Tất cả các thử nghiệm được lặp lại với
5 seed ngẫu nhiên khác nhau. Đối với mỗi đánh giá, chúng tôi chạy 10 tập cho các nhiệm vụ MuJoCo
và 100 tập cho các nhiệm vụ AntMaze. Chúng tôi báo cáo điểm số D4RL trung bình trên 3 đánh
giá cuối cùng cho các nhiệm vụ MuJoCo, và 10 đánh giá cuối cùng cho các nhiệm vụ AntMaze.
Đối với pretraining offline, chúng tôi đầu tiên huấn luyện các agent với 1M bước gradient bằng
cách thực hiện quy trình tiêu chuẩn được đề xuất của TD3+BC và CQL:
• TD3+BC [11]: https://github.com/sfujim/TD3_BC
• CQL [23]: https://github.com/young-geng/CQL
Sau đó, checkpoint cuối cùng của các agent được huấn luyện trước được sử dụng để khởi động
warm cho tinh chỉnh online. Vì các tác giả của SPOT [39] đã cung cấp các checkpoint của các
agent được huấn luyện trước trong https://github.com/thuml/SPOT, chúng tôi trực tiếp áp dụng
những cái này cho tinh chỉnh online.
Chúng tôi tiến hành tất cả các thử nghiệm trên 2 GPU NIVIDA 3090 và Intel Xeon Gold 6230
CPU tại 2.10GHz.
C.2 Chi Tiết Triển Khai và Siêu Tham Số
Đối với huấn luyện VAE, chúng tôi tuân theo việc triển khai chính thức của BCQ [13] và SPOT
[39]. Tuân theo SPOT, chúng tôi chuẩn hóa các trạng thái trong tập dữ liệu cho các miền MuJoCo
nhưng không chuẩn hóa các trạng thái cho các miền AntMaze. Khác với SPOT sử dụng VAE để
ước tính mật độ của chính sách hành vi, SUNG sử dụng VAE để ước tính mật độ truy cập state-action.
Do đó, chúng tôi đặt chiều tiềm ẩn là 2×(state dim + action dim) thay vì 2×action dim. Các siêu
tham số chi tiết cho huấn luyện VAE được liệt kê trong Bảng 3. Lưu ý rằng chúng tôi đơn giản
lấy các siêu tham số được báo cáo trong
16

--- TRANG 16 ---
Bảng 3: Siêu tham số của VAE để định lượng độ bất định trong SUNG.
Siêu tham số Giá trị
Huấn luyện VAEOptimizer Adam
Tỷ lệ học 1e-3
Batch size 256
Bước gradient 1e5
Trọng số thuật ngữ KL 0.5
Chuẩn hóa trạng thái True cho MuJoCo
False cho AntMaze
Kiến trúc VAEEncoder hidden num 750
Encoder layers 3
Latent dim 2×(state dim + action dim)
Decoder hidden dim 750
Decoder layers 3
Bảng 4: Siêu tham số của tinh chỉnh online trong SUNG. Lưu ý rằng chúng tôi không điều chỉnh
bất kỳ siêu tham số nào trong RL xương sống và kiến trúc.
Siêu tham sốMuJoCo AntMaze
TD3+BC CQL SPOT CQL
RL Xương SốngOptimizer Adam Adam Adam Adam
Tỷ lệ học Actor 3e-4 1e-4 1e-4 1e-4
Tỷ lệ học Critic 3e-4 3e-4 3e-4 3e-4
Batch size 256 256 256 256
Hệ số chiết khấu 0.99 0.99 0.99 0.99
Tỷ lệ cập nhật mục tiêu 5e-3 5e-3 5e-3 5e-3
Kiến TrúcActor hidden num 256 256 256 256
Actor layers 3 2 3 2
Critic hidden num 256 256 256 256
Actor layers 3 2 3 3
SUNGSố hành động ứng viên N 100 100 100 100
Số hành động chung kết k 10 20 10 20
Thứ tự tiêu chí xếp hạng qu uq qu uq
Nhiệt độ softmax α 1.0 1.0 1.0 1.0
Phần trăm mẫu OOD p 5 10 99 90
OORBKích thước bộ đệm online 2e4 2e4 2e4 2e4
Kích thước bộ đệm offline 2e6 2e6 2e6 2e6
Xác suất OORB pOORB 0.1 0.1 0.2 0.7
SPOT [39] và bỏ qua việc điều chỉnh siêu tham số cho huấn luyện VAE. Do đó, cải thiện hiệu
suất thêm được mong đợi từ việc điều chỉnh cẩn thận.
Đối với tinh chỉnh online, chúng tôi tuân theo cùng kiến trúc tiêu chuẩn và cùng siêu tham số của
TD3+BC, CQL và SPOT cho tất cả các baseline. Lưu ý rằng chúng tôi không điều chỉnh bất kỳ
siêu tham số nào của RL xương sống và kiến trúc trong SUNG để so sánh công bằng. Về tái tạo
baseline, chúng tôi tuân thủ nghiêm ngặt việc triển khai chính thức và các siêu tham số được báo
cáo trong bài báo gốc của họ để tinh chỉnh các agent được huấn luyện trước với TD3+BC, CQL
và SPOT cho các miền MuJoCo và AntMaze. Một ngoại lệ là kỹ thuật ensemble nặng được loại
bỏ trong BR để so sánh công bằng. Lưu ý rằng chúng tôi cũng trang bị baseline với OORB để so
sánh công bằng. Tất cả các siêu tham số chi tiết cho tinh chỉnh online được trình bày trong Bảng
4.
17

--- TRANG 17 ---
Bảng 5: So sánh điểm số D4RL trung bình trên các nhiệm vụ AntMaze với SPOT làm phương pháp
RL offline xương sống. Chúng tôi báo cáo giá trị trung bình và độ lệch chuẩn trên 5 seed.
SPOT offline-ft online-ft BR O3F APL PEX SUNG
antmaze-umaze-v2 93.2 96.6 ±0.7 88.3±2.3 92.9±1.4 96.9±0.9 97.6±0.7 12.2±4.3 97.0±0.8
antmaze-umaze-diverse-v2 41.6 22.0 ±2.5 0.0±0.0 0.0±0.0 65.3±2.2 50.5±3.1 0.0±0.0 71.7±6.2
antmaze-medium-play-v2 75.2 83.9 ±2.2 0.0±0.0 6.4±5.0 85.6±1.3 86.0±1.4 0.1±0.1 88.6±1.5
antmaze-medium-diverse-v2 73.0 84.6 ±2.0 0.5±0.5 0.0±0.0 80.1±4.2 86.0±2.4 0.0±0.0 91.7±1.2
antmaze-large-play-v2 40.8 40.9 ±2.0 0.0±0.0 0.0±0.0 38.8±4.8 38.9±3.9 0.0±0.0 45.7±3.7
antmaze-large-diverse-v2 44.0 16.6 ±2.0 0.0±0.0 0.0±0.0 3.0±0.9 3.8±0.9 0.0±0.0 19.8±1.6
Tổng 367.8 344.6 88.8 99.3 369.7 362.8 12.3 414.5
Bảng 6: So sánh điểm số D4RL trung bình trên các nhiệm vụ AntMaze với CQL làm phương pháp
RL offline xương sống. Chúng tôi báo cáo giá trị trung bình và độ lệch chuẩn trên 5 seed.
CQL offline-ft online-ft BR O3F APL PEX SUNG
antmaze-umaze-v2 88.6 87.9 ±1.0 96.0±1.1 68.5±3.5 98.6±0.5 96.0±1.6 87.3±2.0 96.9±0.6
antmaze-umaze-diverse-v2 41.0 45.9 ±1.8 0.0±0.1 0.0±0.0 0.0±0.0 0.0±0.0 0.7±1.4 50.5±3.1
antmaze-medium-play-v2 63.8 75.4 ±1.9 18.6±11.0 22.2±8.2 67.3±2.3 22.8±2.5 0.3±0.4 86.3±1.4
antmaze-medium-diverse-v2 61.8 74.1 ±1.8 22.1±9.1 13.6±4.9 80.8±2.9 36.8±4.3 0.3±1.0 85.6±2.1
antmaze-large-play-v2 32.0 41.8 ±3.7 0.1±0.2 0.0±0.0 0.0±0.0 0.0±0.0 0.0±0.0 52.7±2.8
antmaze-large-diverse-v2 32.6 35.3 ±2.6 0.7±1.8 0.1±0.1 0.0±0.0 0.0±0.0 0.0±0.0 44.1±3.5
Tổng 319.6 360.4 137.5 104.3 246.7 155.6 88.6 416.1
D Kết Quả Thử Nghiệm Thêm
D.1 Thử Nghiệm trên AntMaze
Chúng tôi tiến hành thử nghiệm trên các miền AntMaze thách thức hơn, yêu cầu các agent xử lý
các phần thưởng thưa thớt và khâu nối các mảnh quỹ đạo dưới mức tối ưu. Kết quả cho các miền
AntMaze với SPOT và CQL làm phương pháp RL offline xương sống được hiển thị trong Bảng 5
và 6, tương ứng. Từ kết quả, chúng ta có thể quan sát thấy rằng online-ft, BR và PEX gặp khó
khăn trong việc hưởng lợi từ tinh chỉnh online trong hầu hết các cài đặt, ngoại trừ nhiệm vụ antmaze-umaze
tương đối dễ. Điều này xuất phát từ nhu cầu về behavior cloning rõ ràng hoặc ngầm được cung
cấp bởi các mục tiêu RL offline, điều này được yêu cầu để xử lý các phần thưởng thưa thớt và
khâu nối các quỹ đạo con dưới mức tối ưu. Tuy nhiên, online-ft, BR và PEX được phát triển trên
đỉnh các phương pháp RL hoàn toàn online, do đó thất bại trong hầu hết các nhiệm vụ.
Hơn nữa, chúng tôi nhận xét rằng SUNG, khi kết hợp với SPOT hoặc CQL, vượt trội so với các
phương pháp tiên tiến khác trong hầu hết các cài đặt. Điều này chứng minh sự vượt trội của SUNG
trong việc khám phá các cặp state-action thông tin có thể dẫn đến kết quả cao, do đó cung cấp
các mẫu hành vi có giá trị cho các agent học từ đó. Hơn nữa, SUNG đạt được sự cân bằng linh
hoạt giữa các mục tiêu RL offline và online bằng cách điều chỉnh p, cho phép khâu nối các quỹ
đạo con dưới mức tối ưu và vượt qua vấn đề phần thưởng thưa thớt.
D.2 So Sánh với Online Decision Transformer
Vì online decision transformer (ODT) [49] được xây dựng trên đỉnh upside-down RL [38,6,9],
chúng tôi so sánh SUNG với ODT trong phần riêng biệt này. Chúng tôi tuân theo việc triển khai
chính thức trong https://github.com/facebookresearch/online-dt để tái tạo. Chúng tôi tuân thủ
nghiêm ngặt cài đặt thử nghiệm gốc trong ODT để lấy 200K bước môi trường và chỉ xem xét các
nhiệm vụ với các tập dữ liệu offline medium và medium-replay cho tinh chỉnh online để đạt được
so sánh công bằng.
Như được hiển thị trong Bảng 7, đối với hầu hết tất cả các cài đặt, SUNG khi kết hợp với TD3+BC
hoặc CQL vượt trội so với ODT bằng một khoảng cách lớn cả về hiệu suất cuối cùng và cải thiện
online δ. Điều này là do ODT học một chính sách có điều kiện phần thưởng trong một mô hình
học có giám sát, do đó gặp khó khăn trong việc tận dụng đầy đủ mô hình thử-và-sai được sử dụng
trong RL online truyền thống. Tuy nhiên, đáng chú ý rằng ODT cho phép người dùng tùy chỉnh
hành vi của các agent bằng cách chỉ định các điều kiện khác nhau, điều này không được đặc trưng
bởi RL truyền thống.
18

--- TRANG 18 ---
Bảng 7: So sánh điểm số D4RL trung bình trên các nhiệm vụ MuJoCo, tập trung vào cả hiệu suất
cuối cùng và cải thiện online δ. m = medium, m-r = medium-replay. Chúng tôi chạy 200K bước
môi trường để giữ nhất quán với cài đặt của ODT. Chúng tôi báo cáo giá trị trung bình trên 5 seed.
ODT (offline) ODT (online) δ TD3+BC SUNG δ CQL SUNG δ
halfcheetah-m-v2 42.3 42.1 -0.2 48.0 85.5 37.5 48.1 86.7 38.6
hopper-m-v2 62.3 89.8 27.5 61.5 97.1 35.6 73.7 106.1 32.4
walker2d-m-v2 72.1 73.7 1.6 82.2 115.4 33.2 84.3 114.6 30.3
halfcheetah-m-r-v2 37.7 40.6 2.9 44.6 79.8 35.2 46.9 82.9 36.0
hopper-m-r-v2 83.2 88.3 5.1 55.9 104.1 48.2 96.0 83.5 -12.5
walker2d-m-r-v2 67.8 73.1 5.3 71.7 110.7 39.0 83.4 116.7 33.3
Tổng 365.4 407.6 42.2 364.9 592.6 228.7 432.4 590.5 158.1
1 50 100304050607080Điểm Số D4RLhalfcheetah-r
1 50 10055606570758085halfcheetah-m
1 50 10045505560657075halfcheetah-m-r
1 50 100203040506070halfcheetah-r
1 50 1004050607080halfcheetah-m
1 50 1004050607080halfcheetah-m-r
1 50 1000102030405060Điểm Số D4RLhopper-r
1 50 1000255075100125hopper-m
1 50 100708090100110hopper-m-r
1 50 1000204060hopper-r
1 50 10060708090100110hopper-m
1 50 10060708090100110hopper-m-r
1 50 100
Bước Môi Trường (1e3)05101520Điểm Số D4RLwalker2d-r
1 50 100
Bước Môi Trường (1e3)6080100120walker2d-m
1 50 100
Bước Môi Trường (1e3)60708090100110walker2d-m-r
1 50 100
Bước Môi Trường (1e3)05101520walker2d-r
1 50 100
Bước Môi Trường (1e3)406080100walker2d-m
1 50 100
Bước Môi Trường (1e3)6080100120walker2d-m-rSUNG(CQL) SUNG(TD3+BC)
BR O3F APL SUNG|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
Hình 5: Đường cong học của SUNG, khi kết hợp với TD3+BC và CQL, cho 100K bước môi trường
trên các nhiệm vụ MuJoCo. Chúng tôi cũng bao gồm ba baseline hoạt động tốt nhất, BR, O3F và
APL, để so sánh. w = Walker, r = random, m = medium, m-r = medium-replay. Chúng tôi báo cáo
điểm số D4RL trung bình và độ lệch chuẩn trên 5 seed huấn luyện với 10 tập đánh giá mỗi lần.
Lưu ý rằng O3F bị bỏ qua khi kết hợp với CQL do sự phân kỳ của nó.
D.3 Đường Cong Học
Chúng tôi hiển thị các đường cong học của SUNG khi kết hợp với TD3+BC và CQL cho các miền
MuJoCo trong Hình 5. Bên cạnh đó, chúng tôi hiển thị các đường cong học của SUNG khi kết
hợp với SPOT và CQL cho các miền AntMaze trong Hình 6. Trong hầu hết các cài đặt, chúng ta
có thể quan sát thấy rằng SUNG vượt qua các phương pháp tiên tiến nhất trước đây về cả hiệu
suất cuối cùng và hiệu quả mẫu.
D.4 Nghiên Cứu Loại Bỏ
Nghiên Cứu Loại Bỏ với Các Biến Thể của SUNG (Hình 2). Chúng tôi xem xét các biến thể
sau của SUNG để thực hiện nghiên cứu loại bỏ về các thành phần của khung làm việc của chúng
tôi:
•w/o Opt. Exploration: Chúng tôi thay thế chiến lược khám phá lạc quan được đề xuất bằng
khám phá mặc định được cung cấp bởi TD3 và SAC. Đối với TD3, chúng tôi chọn hành động
với nhiễu Gaussian
19

--- TRANG 19 ---
1 50 100707580859095100Điểm Số D4RLumaze
1 50 100707580859095medium-p
1 50 100010203040506070large-p
1 50 100405060708090100umaze
1 50 1005060708090medium-p
1 50 100202530354045505560large-p
1 50 100
Bước Môi Trường (1e3)020406080Điểm Số D4RLumaze-d
1 50 100
Bước Môi Trường (1e3)5060708090100medium-d
1 50 100
Bước Môi Trường (1e3)01020304050large-d
1 50 100
Bước Môi Trường (1e3)203040506070umaze-d
1 50 100
Bước Môi Trường (1e3)505560657075808590medium-d
1 50 100
Bước Môi Trường (1e3)20253035404550large-dSUNG(CQL) SUNG(SPOT)
offline-ft O3F APL SUNG|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
Hình 6: Đường cong học của SUNG, khi kết hợp với SPOT và CQL, cho 100K bước môi trường
trên các nhiệm vụ AntMaze. Chúng tôi cũng bao gồm ba baseline hoạt động tốt nhất, offline-ft,
O3F và APL, để so sánh. p = play, d = diverse. Chúng tôi báo cáo điểm số D4RL trung bình và
độ lệch chuẩn trên 5 seed huấn luyện với 100 tập đánh giá mỗi lần.
để khám phá, tức là, πE(s) =πϕ(s) +ϵ, ϵ∼ N(0, δ). Đối với SAC, chúng tôi lấy mẫu hành
động từ chính sách để khám phá, tức là, πE(s) =a, a∼πϕ(·|s).
•Opt. Exploration w/o Unc.: Chúng tôi loại bỏ độ bất định khỏi chiến lược khám phá được
đề xuất để tham lam chọn hành động tối đa hóa giá trị Q, tức là, πE(s) = arg max a∈CQ(s, a).
•Opt. Exploration w/o Q: Chúng tôi loại bỏ giá trị Q khỏi chiến lược khám phá được đề
xuất để tham lam chọn hành động tối đa hóa độ bất định, tức là, πE(s) = arg max a∈CU(s, a).
•w/o Adp. Exploitation: Chúng tôi thay thế phương pháp khai thác thích ứng được đề xuất
bằng mục tiêu RL online tiêu chuẩn của TD3 và SAC, tương ứng.
•Unc. Estimation w/ Q std: Chúng tôi thay thế VAE bằng độ lệch chuẩn của hai hàm giá
trị Q để ước tính độ bất định, tức là,
U(s, a) =σ(Qθ1(s, a), Qθ2(s, a)) =|Qθ1(s, a)−Qθ2(s, a)|.
Bên cạnh đó, chúng tôi cũng xem xét hai cài đặt loại bỏ thêm:
•Adp. Exploitation w/ Rand.: Chúng tôi xây dựng tập cặp state-action OOD DOOD bằng
cách lấy mẫu ngẫu nhiên từ mini-batch thay vì sử dụng bộ nhận diện mẫu OOD được đề xuất.
•OORB w/o Offline Data: Chúng tôi không sử dụng các chuyển tiếp từ tập dữ liệu offline
cho tinh chỉnh online bằng cách loại bỏ dữ liệu offline khỏi OORB.
Chúng tôi trình bày kết quả của hai nghiên cứu loại bỏ trên trong Hình 7 và Hình 8, tương ứng.
Như được hiển thị trong Hình 7, chúng ta có thể thấy khi chúng ta sử dụng chiến lược lấy mẫu
ngẫu nhiên cho nhận diện mẫu OOD, hiệu suất trên hầu hết các cài đặt suy giảm. Như được hiển
thị trong Hình 8, chúng ta quan sát thấy rằng việc loại bỏ dữ liệu offline làm hại đáng kể hiệu
suất offline-đến-online. Điều này là do tinh chỉnh online có thể hưởng lợi từ việc tái sử dụng các
hành vi đa dạng chứa trong tập dữ liệu offline để tránh overfitting.
Phân Tích Siêu Tham Số về Nhiệt Độ Softmax α. Ở đây, chúng tôi đưa ra phân tích siêu tham
số của nhiệt độ softmax α trong Eq. (8). Cụ thể, chúng tôi điều tra lựa chọn α∈ {0.1,0.2,0.5,1.0,5.0,10.0,20.0,100.0}
cho các miền MuJoCo trong Hình 9. Như được hiển thị trong hình, chúng ta có thể thấy rằng α
là một siêu tham số tương đối không nhạy cảm cho hiệu suất offline-đến-online, và rằng lựa chọn
α hoạt động tốt nhất khác nhau giữa các cài đặt khác nhau. Do đó, mặc dù việc điều chỉnh cẩn
thận α sẽ mang lại cải thiện offline-đến-online nhiều hơn, chúng tôi đặt α= 1.0 trong tất cả kết
quả thử nghiệm xuyên suốt bài báo để đảm bảo tính đơn giản của khung làm việc được đề xuất.
Phân Tích Siêu Tham Số về Xác Suất Lấy Mẫu OORB pOORB. Hơn nữa, chúng tôi đưa ra phân
tích siêu tham số về pOORB cho OORB đơn giản nhưng hiệu quả. Cụ thể, chúng tôi điều tra
20

--- TRANG 20 ---
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr40
20
02040Phần Trăm Khác BiệtSUNG(TD3+BC)
Adp. Exploitation w/ Rand.
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040SUNG(CQL)
Adp. Exploitation w/ Rand.Hình 7: Khác biệt hiệu suất của hiệu suất của
chiến lược ngẫu nhiên cho nhận diện mẫu OOD,
so với thuật toán đầy đủ.
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr40
20
02040Phần Trăm Khác BiệtSUNG(TD3+BC)
OORB w/o Offline Data
hc-rhop-rw-rhc-mhop-mw-mhc-mrhop-mrw-mr-40-2002040SUNG(CQL)
OORB w/o Offline DataHình 8: Khác biệt hiệu suất của hiệu suất không
có dữ liệu offline, so với thuật toán đầy đủ.
0.1 0.2 0.5 1.0 5.0 10.0 20.0 100.0
hc-r
hop-r
w-r
hc-m
hop-m
w-m
hc-mr
hop-mr
w-mr
totalĐiểm Số D4RL Chuẩn Hóa0.96 0.99 1.00 0.97 0.97 0.96 0.98 0.96
0.87 0.65 0.81 1.00 0.56 0.78 0.74 0.90
1.00 0.70 0.57 0.80 0.81 0.66 0.71 0.86
1.00 0.99 0.99 0.99 0.99 0.99 1.00 1.00
0.98 0.92 0.95 0.97 0.92 0.99 0.97 1.00
0.91 0.97 0.96 0.98 0.95 1.00 0.96 0.95
1.00 0.99 1.00 0.99 0.98 0.97 0.98 1.00
0.95 0.98 1.00 0.96 0.94 0.99 0.99 0.97
1.00 0.97 0.99 0.98 0.98 0.96 0.98 0.95
0.99 0.97 0.99 1.00 0.96 0.99 0.98 0.99SUNG(TD3+BC)
0.1 0.2 0.5 1.0 5.0 10.0 20.0 100.0
hc-r
hop-r
w-r
hc-m
hop-m
w-m
hc-mr
hop-mr
w-mr
totalĐiểm Số D4RL Chuẩn Hóa0.49 0.51 0.72 0.87 1.00 0.97 0.97 0.98
0.78 1.00 0.73 0.90 0.80 0.54 0.78 0.74
0.78 1.00 0.65 0.94 0.67 0.76 0.95 0.90
0.83 0.84 0.86 0.89 0.98 1.00 1.00 0.98
0.55 0.76 0.93 0.99 1.00 0.99 0.99 0.98
0.88 0.93 0.91 0.86 0.78 0.98 1.00 0.94
0.87 0.88 0.87 0.94 0.99 0.99 1.00 0.98
0.43 0.63 0.71 1.00 0.76 0.79 0.79 0.92
0.69 0.82 0.86 0.97 1.00 0.96 0.91 0.92
0.73 0.84 0.88 0.99 0.97 0.98 1.00 1.00SUNG(CQL)
0.600.650.700.750.800.850.900.951.00
0.50.60.70.80.91.0
Hình 9: Heatmap cho hiệu suất của SUNG với nhiệt độ softmax α khác nhau. Kết quả của mỗi cài
đặt được chuẩn hóa với chuẩn hóa tối đa để trực quan hóa tốt hơn. hc = HalfCheetah, hop = Hopper,
w = Walker, r = random, m = medium, mr = medium-replay.
lựa chọn xác suất lấy mẫu OORB pOORB ∈ {0.0,0.1,0.2,0.4,0.6,0.8,1.0} cho các miền MuJoCo
trong Hình 10. Lưu ý rằng xác suất pOORB cao hơn chỉ ra việc tái sử dụng nhiều hơn tập dữ liệu
offline trong quá trình học chính sách. Từ kết quả, chúng ta có thể quan sát thấy rằng pOORB =
0.1 hoạt động tốt nhất khi kết hợp với TD3+BC và CQL, trong khi việc đặt pOORB quá bảo thủ
(tức là, pOORB = 0) hoặc quá mạnh mẽ (tức là, pOORB = 1.0) làm hại hiệu suất. Như được chỉ
ra bởi Zheng et al. [48], dữ liệu offline có thể ngăn các agent hội tụ sớm đến các chính sách dưới
mức tối ưu do sự đa dạng dữ liệu tiềm năng, trong khi dữ liệu online có thể ổn định huấn luyện
và tăng tốc hội tụ. Do đó, việc các thuật toán RL offline-đến-online hiệu quả mẫu kết hợp cả dữ
liệu offline và online trong quá trình học chính sách là quan trọng.
D.5 Suy Giảm về Trọng Số Điều Chỉnh λ
Một trực giác cho phương pháp khai thác thích ứng được đề xuất là trọng số điều chỉnh λ trong
Eq.(9) nên được suy giảm trong quá trình tinh chỉnh online, vì sự dịch chuyển phân phối state-action
dần dần bị loại bỏ với bước môi trường. Do đó, trong phần này, chúng tôi khám phá cơ chế suy
giảm cho λ bằng cách giảm tuyến tính λ đến {0.0,0.2,0.4,0.6,0.8,1.0} lần giá trị ban đầu của
nó ở cuối tinh chỉnh online. Như được hiển thị trong Hình 11, không có cơ chế suy giảm nào vượt
trội so với cái khác về hiệu suất tổng trung bình khi kết hợp với TD3+BC hoặc CQL. Chúng tôi
phỏng đoán rằng điều này là do 100K bước môi trường không đủ cho các agent để hoàn toàn vượt
qua vấn đề dịch chuyển phân phối trong hầu hết các cài đặt. Chúng tôi cũng lưu ý rằng không có
cơ chế suy giảm không nhất quán vượt trội so với tất cả các cài đặt khác, điều này phụ thuộc vào
chất lượng của tập dữ liệu và độ khó của nhiệm vụ. Do đó,
21

--- TRANG 21 ---
0.0 0.1 0.2 0.4 0.6 0.8 1.0
pOORBhc-r
hop-r
w-r
hc-m
hop-m
w-m
hc-mr
hop-mr
w-mr
totalĐiểm Số D4RL Chuẩn Hóa0.93 1.00 0.99 0.98 0.92 0.83 0.58
0.72 1.00 0.71 0.96 0.38 0.38 0.33
0.59 0.90 1.00 0.75 0.57 0.76 0.39
0.98 1.00 1.00 0.95 0.95 0.88 0.80
0.90 0.97 0.97 0.97 0.97 1.00 0.92
0.72 1.00 0.99 0.94 0.90 0.90 0.79
0.98 1.00 0.98 0.98 0.93 0.93 0.86
0.73 0.98 0.98 1.00 0.97 0.96 0.92
0.89 1.00 1.00 0.96 0.94 0.92 0.85
0.86 1.00 0.98 0.97 0.91 0.90 0.80SUNG(TD3+BC)
0.0 0.1 0.2 0.4 0.6 0.8 1.0
pOORBhc-r
hop-r
w-r
hc-m
hop-m
w-m
hc-mr
hop-mr
w-mr
totalĐiểm Số D4RL Chuẩn Hóa0.89 0.93 1.00 0.88 0.77 0.75 0.55
0.42 1.00 0.79 0.62 0.70 0.72 0.57
0.35 0.90 0.83 1.00 0.85 0.72 0.54
0.98 0.99 1.00 1.00 0.91 0.90 0.77
0.92 1.00 1.00 0.96 0.95 0.96 0.93
1.00 0.82 0.90 0.90 0.87 0.84 0.78
0.99 1.00 1.00 0.97 0.80 0.82 0.75
0.77 0.98 0.84 0.92 0.97 1.00 0.94
0.89 1.00 0.97 0.88 0.88 0.87 0.86
0.91 1.00 0.98 0.95 0.91 0.91 0.82SUNG(CQL)
0.40.50.60.70.80.91.0
0.40.50.60.70.80.91.0Hình 10: Heatmap cho hiệu suất của SUNG với xác suất lấy mẫu OORB pOORB khác nhau. Kết
quả của mỗi cài đặt được chuẩn hóa với chuẩn hóa tối đa để trực quan hóa tốt hơn. hc = HalfCheetah,
hop = Hopper, w = Walker, r = random, m = medium, mr = medium-replay.
0.0 0.2 0.4 0.6 0.8 1.0
Suy Giảm Cuối707580Điểm Số D4RL Trung Bình
SUNG (TD3+BC)
0.0 0.2 0.4 0.6 0.8 1.0
Suy Giảm Cuối657075
SUNG (CQL)
best baseline
Hình 11: So sánh hiệu suất của SUNG với cuối
suy giảm khác nhau cho trọng số điều chỉnh λ,
trong đó trục x có nghĩa là suy giảm cho đến
khi đạt x·λ ở cuối tinh chỉnh online. Chúng tôi
trình bày điểm số D4RL trung bình trên các miền
MuJoCo. x= 1.0 đại diện cho SUNG không có
cơ chế suy giảm.
việc điều chỉnh cẩn thận cơ chế suy giảm có thể mang lại cải thiện offline-đến-online bổ sung,
đặc biệt là cho một lượng lớn bước môi trường online. Trong công trình này, chúng tôi không sử
dụng cơ chế suy giảm để đơn giản. Hơn nữa, chúng tôi cũng nhận xét rằng SUNG với cơ chế suy
giảm khác nhau vẫn vượt trội so với baseline hoạt động tốt nhất, điều này tiếp tục chứng minh sự
vượt trội của khung làm việc được đề xuất.
E Thảo Luận
Trong khi đạt được hiệu suất đầy hứa hẹn, khung làm việc được đề xuất cũng có một số hạn chế
vốn có. Một số phương pháp RL offline được phát triển gần đây có thể được xây dựng trên đỉnh
RL on-policy [50] và mô hình học có giám sát [6,9,41] thay vì RL off-policy thuần túy. Do đó,
một hạn chế là SUNG không tương thích với các agent được huấn luyện trước bởi những mô hình
mới này cho RL offline. Tuy nhiên, chúng tôi lập luận rằng những hiểu biết của SUNG có thể
được áp dụng cho chúng về nguyên tắc. Chúng tôi để lại việc khám phá triển khai của nó để kết
hợp với RL on-policy và học có giám sát như một công việc tương lai thú vị. Trong phiên bản
hiện tại, SUNG điều tra cài đặt RL offline-đến-online để giải quyết các nhiệm vụ có đầu vào là
các trạng thái của môi trường mô phỏng. Sẽ thú vị khi thấy liệu SUNG cũng vượt trội với cài
đặt RL trực quan [29]. Hơn nữa, điều tra thêm về ước tính độ bất định, bộ đệm phát lại, và nhiều
phương pháp RL offline xương sống cần được khám phá. Cuối cùng, chúng tôi nhận xét rằng việc
khám phá ứng dụng của nó trên các nhiệm vụ thế giới thực cũng là một công việc tương lai thú vị.
Chúng tôi hy vọng công trình của chúng tôi thể hiện tiềm năng của mô hình pretraining offline
và tinh chỉnh online trong việc triển khai hiệu quả và hiệu suất các thuật toán RL trong các ứng
dụng thế giới thực. Bên cạnh đó, chúng tôi hy vọng rằng công trình của chúng tôi truyền cảm hứng
cho nghiên cứu tương lai để phát triển các phương pháp RL offline-đến-online hiệu quả mẫu hơn.
Chúng tôi không thấy tác động xã hội tiêu cực của công trình chúng tôi trong thực tế.
22
