# 2305.14852.pdf
# Chuyển đổi từ PDF sang TXT
# Đường dẫn nguồn: /home/admin88/arxiv-downloader/weight-averaging/2305.14852.pdf
# Kích thước tệp: 718958 byte

===============================================
NỘI DUNG TỆP PDF
===============================================


--- TRANG 1 ---
Xuất bản như một bài báo hội thảo tại ICLR 2024
TRUNG BÌNH TRỌNG SỐ THƯA VỚI NHIỀU HẠT 
CHO CẮT TỈA ĐỘ LỚN LẶP LẠI
Moonseok Choi*1Hyungi Lee*1Giung Nam*1Juho Lee1,2
1KAIST AI2AITRICS
{ms.choi, lhk2708, giung, juholee }@kaist.ac.kr
TÓM TẮT
Với kích thước ngày càng tăng của các mạng nơ-ron hiện đại, tầm quan trọng
của các kiến trúc thưa đã tăng vọt do tốc độ suy luận nhanh hơn và nhu cầu
bộ nhớ tối thiểu. Khi nói đến các kỹ thuật cắt tỉa toàn cục, Cắt tỉa Độ lớn Lặp lại
(IMP) vẫn đứng như một thuật toán tiên tiến nhất mặc dù bản chất đơn giản của
nó, đặc biệt trong các chế độ cực kỳ thưa. Dựa trên phát hiện gần đây rằng hai
nghiệm IMP khớp liên tiếp được kết nối tuyến tính mà không có rào cản mất mát,
chúng tôi đề xuất Trung bình Trọng số Thưa với Nhiều Hạt (SWAMP), một sự
thay đổi đơn giản của IMP đạt được hiệu suất tương đương với một ensemble gồm
hai nghiệm IMP. Cho mỗi lần lặp, chúng tôi đồng thời huấn luyện nhiều mô hình
thưa, được gọi là các hạt, sử dụng các thứ tự batch khác nhau nhưng cùng vé
khớp, và sau đó trung bình trọng số các mô hình như vậy để tạo ra một mặt nạ
duy nhất. Chúng tôi chứng minh rằng phương pháp của chúng tôi luôn vượt trội
hơn các baseline hiện có qua các mức độ thưa khác nhau thông qua các thí nghiệm
rộng rãi trên các cấu trúc dữ liệu và mạng nơ-ron khác nhau.

1 GIỚI THIỆU
Các mạng nơ-ron sâu thường được tham số hóa quá mức, và phần lớn các tham số của chúng có
thể được cắt tỉa mà không hy sinh hiệu suất mô hình (LeCun et al., 1989). Giả thuyết vé số may mắn
được đề xuất bởi Frankle & Carbin (2019) gợi ý rằng tồn tại một mạng con thưa tại khởi tạo có thể
được huấn luyện để đạt được cùng mức hiệu suất như mạng dày đặc gốc. Các mạng con khớp như
vậy có thể được tìm thấy thông qua Cắt tỉa Độ lớn Lặp lại (IMP) với tua lại (Frankle et al., 2020),
bao gồm ba bước sau: (i) huấn luyện mạng trong một số lần lặp nhất định, (ii) cắt tỉa các trọng số
có độ lớn nhỏ nhất, và (iii) tua lại các trọng số về một lần lặp sớm trong khi cố định các trọng số
đã cắt tỉa về không. Quy trình này được lặp lại trong nhiều vòng, và mạng con được tua lại cuối cùng
tương ứng với vé khớp có thể đạt được hiệu suất của mạng đầy đủ. Mặc dù đơn giản, IMP cung cấp
hiệu suất tiên tiến nhất về việc tìm kiếm một mặt nạ thưa, đặc biệt cho các chế độ thưa cực độ
(Renda et al., 2020).

Thành công của IMP thực sự phản trực quan xem xét tính đơn giản của nó. Trong vấn đề này, Frankle
et al. (2020) đã tiết lộ một kết nối cơ bản giữa giả thuyết vé số may mắn và kết nối chế độ tuyến tính,
chỉ ra rằng hiệu quả của IMP dựa vào tính ổn định của nó đối với tối ưu hóa ngẫu nhiên; các nghiệm
IMP nằm trong cùng một lưu vực hút dẫn trong cảnh quan mất mát. Đi sâu hơn vào vấn đề, Paul
et al. (2023) đã phát hiện rằng kết nối chế độ tuyến tính cũng tồn tại giữa các nghiệm IMP liên tiếp
với các mức độ thưa khác nhau. Cụ thể hơn, họ kết luận rằng IMP thất bại trong việc tìm kiếm một
mạng con khớp nếu các nghiệm từ các vòng liên tiếp bị ngắt kết nối và tiếp tục nhấn mạnh tầm quan
trọng của cả tỷ lệ cắt tỉa và lần lặp tua lại để duy trì kết nối giữa các nghiệm IMP.

Được truyền cảm hứng bởi kết nối giữa IMP và kết nối chế độ tuyến tính, chúng tôi mở rộng hiểu
biết đến góc độ cảnh quan mất mát. Phân tích cảnh quan mất mát của các mạng nơ-ron sâu là một
công cụ hiệu quả được sử dụng rộng rãi để nghiên cứu kết nối chế độ (Draxler et al., 2018; Garipov
et al., 2018; Fort & Jastrzebski, 2019; Benton et al., 2021), và nó cũng thúc đẩy chúng tôi tìm kiếm
các nghiệm nằm ở vùng phẳng của cảnh quan mất mát để tăng cường khả năng tổng quát hóa
(Chaudhari et al., 2017; Izmailov et al., 2018; Foret et al., 2021). Đáng chú ý, cả hai lĩnh vực đều
chia sẻ một mục tiêu chung là xác định các không gian con "tốt" được đặc trưng bởi giá trị mất mát
thấp, và mục tiêu này phù hợp với mục tiêu cuối cùng của việc cắt tỉa mạng nơ-ron - để xác định
các không gian con thưa "khớp" trong một không gian tham số dày đặc cho trước.

Trong bài báo này, chúng tôi nghiên cứu cách IMP có thể hưởng lợi từ nhiều mô hình được kết nối
trong các bề mặt mất mát. Các đóng góp của chúng tôi được tóm tắt như sau:

• Đầu tiên chúng tôi chứng minh thực nghiệm rằng nhiều mô hình được huấn luyện với nhiễu
SGD khác nhau nhưng từ cùng một vé khớp có thể được trung bình trọng số, tức là, không
tồn tại rào cản mất mát trong vỏ lồi của các trọng số mô hình. Chúng tôi tiếp tục chỉ ra rằng
việc lấy trung bình các hạt dẫn đến các cực tiểu phẳng, thể hiện hiệu suất tổng quát hóa
vượt trội so với từng hạt riêng lẻ.

• Dựa trên các quan sát trước đó, chúng tôi đề xuất một kỹ thuật cắt tỉa lặp mới, Trung bình
Trọng số Thưa với Nhiều Hạt (SWAMP), được thiết kế riêng cho IMP. Chúng tôi xác minh
rằng SWAMP bảo tồn kết nối tuyến tính của các nghiệm liên tiếp, đây là một đặc tính quan
trọng góp phần vào thành công của IMP.

• Thông qua các thí nghiệm rộng rãi, chúng tôi cung cấp bằng chứng thực nghiệm hỗ trợ tính
ưu việt của thuật toán SWAMP được đề xuất so với các baseline khác.

2 CƠ SỞ LÝ THUYẾT

2.1 CẮT TỈA MẠNG NƠ-RON NHƯ TỐI ƯU HÓA CÓ RÀNG BUỘC

Huấn luyện thông thường của các mạng nơ-ron nhằm mục đích tìm một tham số mạng nơ-ron tối
ưu w∈ℝD để tối thiểu hóa một hàm mất mát cho trước L:ℝD→ℝ cho một tập dữ liệu huấn luyện
D cho trước. Tối ưu hóa như vậy thường sử dụng các phương pháp Gradient Descent Ngẫu nhiên
(SGD; Robbins & Monro, 1951), mà chúng tôi ký hiệu là wT←SGD0→T(w0, xi,D) trong suốt bài
báo. Ở đây, wT biểu thị nghiệm thu được bằng cách thực hiện SGD với một tính ngẫu nhiên của xi
(ví dụ, thứ tự mini-batch) qua T lần lặp, bắt đầu từ trọng số ban đầu w0. Mặt khác, cắt tỉa mạng
nơ-ron là quá trình thu được một mạng nơ-ron thưa với mức độ thưa mong muốn κ∈[0,1) từ mạng
nơ-ron dày đặc gốc. Mục tiêu bây giờ là tìm một nghiệm thưa tối ưu w=w*◦m* với ràng buộc rằng
số lượng phần tử khác không trong mặt nạ m*∈[0,1]D thỏa mãn ∥m*∥0≤D(1−κ).

2.2 CẮT TỈA ĐỘ LỚN LẶP LẠI VỚI TUA LẠI

Cắt tỉa Độ lớn Lặp lại (IMP; Frankle & Carbin, 2019) là một phương pháp cắt tỉa lặp vừa đơn giản
vừa hiệu quả cao. Mỗi chu kỳ của IMP bao gồm ba bước sau: (i) Huấn luyện - một tham số mạng
wc tại chu kỳ thứ c được huấn luyện cho đến khi hội tụ. (ii) Cắt tỉa - một mặt nạ m được tạo ra
bằng cách đặt các trọng số nhỏ nhất về không dựa trên tỷ lệ cắt tỉa được định trước α. (iii) Đặt lại -
các trọng số sau đó được hoàn nguyên về các giá trị ban đầu trước khi chu kỳ tiếp theo bắt đầu.
Chu kỳ huấn luyện-cắt tỉa-đặt lại này được lặp lại cho đến khi đạt được mức độ thưa mong muốn.

Tuy nhiên, trong các tình huống thực tế, phiên bản gốc của IMP gặp phải sự suy giảm hiệu suất
nhanh chóng khi độ thưa tăng và thất bại trong việc khớp hiệu suất của nghiệm dày đặc gốc. Để
giải quyết vấn đề này, khái niệm tua lại được giới thiệu (Frankle et al., 2020; Renda et al., 2020).
Thay vì đặt lại các trọng số chưa bị cắt tỉa về các giá trị ban đầu, các trọng số được tua lại về một
điểm huấn luyện sớm - vé khớp. Vé khớp đơn giản là các trọng số thu được sau khi huấn luyện
trong một vài lần lặp. Tham khảo Phụ lục C.1 để biết thêm chi tiết về các thuật toán IMP.

2.3 KẾT NỐI TUYẾN TÍNH CỦA CÁC TRỌNG SỐ MẠNG NƠ-RON

Xem xét một đường dẫn một chiều được ký hiệu là P: [0,1]→ℝD, kết nối hai trọng số mạng nơ-ron
w(0) và w(1) trong một không gian D chiều, trong đó điểm bắt đầu và kết thúc là P(0) = w(0) và
P(1) = w(1), tương ứng. Theo nghĩa đơn giản, chúng ta có thể nói rằng có một kết nối giữa w(0)
và w(1) nếu điều kiện supλ∈[0,1]L(P(λ))≤max{L(P(0)),L(P(1))}+ϵ đúng, trong đó ϵ là một giá
trị biên nhỏ. Trong khi những tiến bộ gần đây trong học sâu đã tiết lộ sự tồn tại của các đường dẫn
phi tuyến tính giữa các cực tiểu địa phương thu được thông qua tối ưu hóa ngẫu nhiên (Draxler et
al., 2018; Garipov et al., 2018), vẫn không đơn giản để thiết lập kết nối tuyến tính (tức là, kết nối
với một kết nối tuyến tính P(λ) = (1−λ)w(0)+λw(1)) cho các mạng nơ-ron sâu hiện đại
(Lakshminarayanan et al., 2017; Fort & Jastrzebski, 2019; Fort et al., 2020).

3 TRUNG BÌNH TRỌNG SỐ THƯA VỚI NHIỀU HẠT (SWAMP)

3.1 IMP: MỘT QUAN ĐIỂM CẢNH QUAN MẤT MÁT

Frankle et al. (2020) đã chứng minh rằng vé khớp có tác động đáng kể đến tính ổn định của các
mạng nơ-ron đối với nhiễu SGD xi. Ngay cả khi hai mạng được huấn luyện với cùng một khởi tạo
ngẫu nhiên w0, nhiễu SGD khác nhau xi(1), xi(2) làm gián đoạn kết nối tuyến tính giữa các nghiệm
thu được thông qua SGD, tức là, không có kết nối tuyến tính giữa
w(1)T←SGD0→T(w0, xi(1),D) và w(2)T←SGD0→T(w0, xi(2),D), (1)
và do đó tối ưu hóa trở nên không ổn định đối với nhiễu SGD. Họ tiếp tục xác nhận thực nghiệm
rằng các nghiệm thưa thu được thông qua IMP là khớp khi và chỉ khi chúng ổn định đối với nhiễu
SGD, và chẩn đoán sự không ổn định này như một trường hợp thất bại của thuật toán IMP gốc
(Frankle & Carbin, 2019).

Một cách xử lý đơn giản để đảm bảo tính ổn định là chia sẻ giai đoạn đầu của quỹ đạo tối ưu hóa.
Nói cách khác, tồn tại một kết nối tuyến tính giữa
w(1)T←SGDT0→T(wT0, xi(1),D) và w(2)T←SGDT0→T(wT0, xi(2),D), (2)
khi các lần chạy SGD được bắt đầu từ cùng một khởi tạo wT0←SGD0→T0(w0, xi,D). Hơn nữa,
Paul et al. (2023) đã chứng minh kết nối tuyến tính giữa hai nghiệm IMP liên tiếp với các mức độ
thưa khác nhau và xác định nó như một yếu tố quan trọng cho thành công của IMP.

Tuy nhiên, câu hỏi liệu một không gian con mất mát thấp có được hình thành bởi vỏ lồi của ba hoặc
nhiều nghiệm hay không vẫn còn không chắc chắn, mặc dù có sự hiện diện của kết nối tuyến tính
giữa từng cặp nghiệm. Nếu việc xây dựng một không gian con thể tích mất mát thấp sử dụng các
nghiệm IMP trở nên khả thi, nó có thể mang lại một nghiệm hiệu quả hơn với khả năng tổng quát
hóa cải thiện tại điểm giữa của không gian con này (Wortsman et al., 2021).

3.2 SWAMP: MỘT THUẬT TOÁN

Được truyền cảm hứng bởi phân tích ổn định của vé khớp được trình bày trong § 3.1, chúng tôi đề
xuất Trung bình Trọng số Thưa với Nhiều Hạt (SWAMP) như một kỹ thuật trung bình trọng số
thưa được thiết kế riêng cho IMP. Thuật toán chi tiết được trình bày trong Thuật toán 1.

SWAMP khác với IMP vanilla ở hai khía cạnh chính. Thứ nhất, chúng tôi tạo ra nhiều bản sao của
vé khớp (dòng 5; Thuật toán 1) và huấn luyện chúng đồng thời với các seed ngẫu nhiên khác nhau
(dòng 6; Thuật toán 1), trong khi IMP sử dụng một hạt duy nhất. Thứ hai, chúng tôi thay thế việc
huấn luyện SGD bằng Trung bình Trọng số Ngẫu nhiên (SWA; Izmailov et al., 2018), một phương
pháp xây dựng trung bình di động của các tham số bằng cách lấy mẫu định kỳ một tập con của quỹ
đạo học tập, và SWA cho phép chúng tôi tích lũy thực tế nhiều hạt hơn trong suốt quá trình huấn
luyện. Sau đó chúng tôi trung bình tất cả các hạt trước khi tiến hành bước cắt tỉa (dòng 8; Thuật
toán 1).

Như được minh họa trong Hình 1, thuật toán của chúng tôi đạt được hiệu suất vượt trội, ngang
bằng với ensemble gồm hai mạng thưa. Điều này khá đáng chú ý xem xét rằng nghiệm của chúng
tôi đạt được mức hiệu suất này trong khi có chi phí suy luận thấp hơn đáng kể so với phương pháp
ensemble. Các nghiên cứu ablation sâu hơn được trình bày trong § 4.2 và Phụ lục C.4 cũng xác
nhận rằng cả hai thành phần đều đóng góp độc lập vào thuật toán của chúng tôi, với mỗi thành phần
đóng vai trò quan trọng trong việc đạt được hiệu suất vượt trội.

3.3 SWAMP: MỘT QUAN ĐIỂM CẢNH QUAN MẤT MÁT

Trong phần này, chúng tôi khám phá từng bước liệu các đặc tính của IMP được giới thiệu trong §
3.1 có cũng đúng cho SWAMP cùng với việc làm nổi bật điểm mạnh của SWAMP. Để bắt đầu,
chúng tôi kiểm tra kết nối tuyến tính của các hạt SWAMP trong một chu kỳ duy nhất. Mặc dù
Frankle et al. (2020) chứng minh thực nghiệm kết nối tuyến tính theo cặp, vẫn không chắc chắn
liệu điều này có đúng cho tổ hợp lồi của nhiều hơn hai hạt hay không. Trong Hình 2, chúng tôi
trực quan hóa bề mặt mất mát của các hạt được huấn luyện IMP cùng với hạt được trung bình trọng
số. Chúng ta có thể nhận thấy rằng việc trung bình trọng số thất bại ở các giai đoạn đầu của IMP
do bản chất rất phi lồi của cảnh quan. Tuy nhiên, khi độ thưa tăng, các hạt có xu hướng nằm trong
cùng một lưu vực rộng cho phép trung bình trọng số. Phát hiện như vậy phù hợp với Frankle et al.
(2020) đã chứng minh sự dễ dàng trong việc tìm kiếm một đường cong mất mát thấp với một mạng
nhỏ hơn so với một mạng lớn hơn, tức là, một mạng thưa có xu hướng ổn định hơn. Ngoài ra, nó
tiếp tục chứng minh rằng thuật toán của chúng tôi có lợi hơn với các mạng thưa hơn.

Hình 3 cung cấp bằng chứng bổ sung cho thấy rằng nghiệm được trung bình trọng số thực sự vượt
trội hơn các thành viên riêng lẻ của nó, ngoại trừ trong các trường hợp mà mạng dày đặc chưa được
ổn định. Tốt hơn, khoảng cách hiệu suất đáng chú ý giữa các hạt riêng lẻ thúc đẩy nhu cầu trung
bình trọng số. Chúng tôi tiếp tục định lượng độ phẳng của các cực tiểu địa phương thông qua vết
của Hessian sử dụng thuật toán Power Iteration (Yao et al., 2020). Giá trị vết Hessian cao hơn có
nghĩa là cực tiểu địa phương hội tụ thể hiện độ cong cao. Kết quả trong Bảng 1 xác nhận rằng
SWAMP định vị một mạng thưa phẳng hơn so với IMP, chỉ trừ các chu kỳ đầu.

Cuối cùng, chúng tôi kiểm tra liệu các nghiệm liên tiếp từ các chu kỳ SWAMP có được kết nối
tuyến tính hay không - một chìa khóa cho thành công của IMP được chỉ ra bởi Paul et al. (2023) -
điều này thực sự hóa ra là đúng theo Hình 4. Không chỉ điều này đúng, mà phương pháp của chúng
tôi cũng thể hiện phương sai tối thiểu và duy trì một quỹ đạo rất ổn định trong suốt quá trình cắt
tỉa, gợi ý rằng SWAMP tìm thấy một lưu vực phẳng và được kết nối tốt. Đến điểm này, chúng tôi
cung cấp bằng chứng thực nghiệm rằng phương pháp của chúng tôi hiệu quả xác định các cực tiểu
phẳng trong khi duy trì các tính chất mong muốn của IMP, dẫn đến một mạng thưa duy nhất có
hiệu suất vượt trội hơn IMP.

4 THÍ NGHIỆM

4.1 KẾT QUẢ CHÍNH: CÁC NHIỆM VỤ PHÂN LOẠI HÌNH ẢNH

Các phương pháp baseline. Ngoài IMP với tua lại trọng số (Frankle et al., 2020), phương pháp của
chúng tôi được so sánh với một danh sách các kỹ thuật cắt tỉa. Điều này bao gồm các phương pháp
cắt tỉa một lần (SNIP (Lee et al., 2019); GraSP (Wang et al., 2020); SynFlow (Tanaka et al., 2020)),
huấn luyện từ dày đặc đến thưa với mặt nạ động (DST (Liu et al., 2020); RigL (Evci et al., 2020)),
IMP được tối ưu hóa SAM (Na et al., 2022), và Lottery Pools (Yin et al., 2023).

Thiết lập thí nghiệm. Phương pháp của chúng tôi được đánh giá trên các benchmark phân loại hình
ảnh đa dạng, bao gồm các tập dữ liệu CIFAR-10, CIFAR-100, Tiny-ImageNet và ImageNet. Trong
suốt các thí nghiệm, chúng tôi sử dụng các mạng residual (He et al., 2016) và mạng VGG
(Simonyan & Zisserman, 2015) làm cơ sở: WRN-28-2 và VGG-13 cho CIFAR-10; WRN-32-4 và
VGG-16 cho CIFAR-100; R18 cho Tiny-ImageNet; và R50 cho ImageNet. Trừ khi được chỉ định,
chúng tôi đặt số lượng hạt SWAMP N=4 và tỷ lệ cắt tỉa α=0.2. Tham khảo Phụ lục B để biết thêm
chi tiết thí nghiệm.

Bảng 2 trình bày hiệu suất của SWAMP cùng với các phương pháp baseline khác trên các tập dữ
liệu CIFAR-10 và CIFAR-100, tương ứng. So với các phương pháp baseline khác, SWAMP liên tục
đạt được độ chính xác phân loại cao nhất qua tất cả các mức độ thưa và mô hình khi được đánh
giá trên các tập kiểm tra CIFAR-10 và CIFAR-100; chúng tôi hoãn kết quả trên các mạng VGG và
Tiny-ImageNet đến Phụ lục C. Để xem các khía cạnh định lượng độ không chắc chắn, trong Phụ
lục C.6, chúng tôi cũng báo cáo các log-likelihood âm (NLL). Một lần nữa, SWAMP đạt được NLL
tốt nhất trong tất cả các thiết lập. Hơn nữa, Bảng 3 nhấn mạnh rằng phương pháp của chúng tôi
liên tục vượt trội hơn IMP trên ImageNet, một tập dữ liệu quy mô lớn được biết là khó cắt tỉa.

4.2 CÁC NGHIÊN CỨU ABLATION

SWAMP có tìm thấy mặt nạ tốt hơn không? Để xác nhận rằng SWAMP thực sự xác định một mặt
nạ vượt trội so với IMP, chúng tôi tiến hành một thí nghiệm với hai mặt nạ khác nhau: (i) một mặt
nạ thu được từ IMP, và (ii) một mặt nạ thu được từ SWAMP. Tại một mức độ thưa cố định được
xác định trước, chúng tôi ban đầu huấn luyện mô hình của mình bằng SGD (hoặc SWAMP) và
mặt nạ từ IMP. Đồng thời, chúng tôi huấn luyện mô hình bằng SGD (hoặc SWAMP) và mặt nạ từ
SWAMP ở cùng mức độ thưa. Hai quá trình này chỉ khác nhau ở các mặt nạ được sử dụng, trong
khi phương pháp huấn luyện và mức độ thưa cố định vẫn giữ nguyên. Bảng 4 trình bày bằng chứng
rõ ràng rằng mặt nạ SWAMP liên tục vượt trội hơn đối tác của nó về hiệu suất, ngoại trừ mô hình
WRN-28-2 ở mức độ thưa 50% và 75% khi được huấn luyện bằng SWAMP. Kết quả này cho thấy
rằng SWAMP tạo ra một mặt nạ thưa tốt hơn IMP.

SWAMP có cung cấp huấn luyện thưa tốt hơn không? Trong Bảng 4, chúng tôi cũng có thể xác
minh rằng SWAMP cung cấp huấn luyện thưa tốt hơn so với IMP. Bằng cách so sánh kết quả giữa
huấn luyện SGD và huấn luyện SWAMP sử dụng cùng mặt nạ, rõ ràng rằng SWAMP liên tục vượt
trội qua tất cả các mặt nạ, mức độ thưa và mô hình. Nó xác minh rằng SWAMP hiệu quả hướng
dẫn các hạt trọng số hướng tới hội tụ vào các cực tiểu địa phương phẳng, dẫn đến cải thiện khả
năng tổng quát hóa trên tập chia kiểm tra. Độ phẳng được tạo ra của các cực tiểu địa phương thông
qua phân phối trọng số của SWAMP góp phần vào hiệu suất tăng cường và tính mạnh mẽ của mô
hình như chúng tôi đã thảo luận trong § 3.3.

Hai chiến lược trung bình của SWAMP. Như được mô tả trong § 3.2, chúng tôi ở đây điều tra cách
trung bình trọng số ngẫu nhiên và trung bình nhiều hạt góp phần vào hiệu suất cuối cùng của
SWAMP. Trong Bảng 5, trong suốt tất cả bốn mức độ thưa, việc áp dụng chỉ một trong hai kỹ thuật
hiển thị hiệu suất tốt hơn IMP (hàng dưới) nhưng rõ ràng thấp hơn SWAMP (hàng trên). Chúng
tôi kết luận rằng hai thành phần bổ sung cho nhau, đạt được hiệu suất tối ưu khi được áp dụng cùng
nhau. Tiếp theo trong Bảng 6, chúng tôi tiến hành một phân tích thực nghiệm để điều tra mối tương
quan giữa số lượng hạt và hiệu suất của SWAMP. Chúng tôi cung cấp các nghiên cứu ablation bổ
sung trong Phụ lục C.4.

4.3 NHẬN XÉT THÊM

Chiến lược song song hóa trong huấn luyện phân tán. Một hạn chế đáng chú ý của thuật toán
SWAMP là chi phí huấn luyện của nó, tăng tuyến tính với số lượng hạt. Chi phí huấn luyện thường
không phải là vấn đề lớn khi làm việc với các tập dữ liệu nhỏ như CIFAR-10/100, nhưng trở nên
đáng kể khi xử lý các tập dữ liệu lớn như ImageNet. Do đó, chúng tôi đề xuất song song hóa các
hạt qua các máy khi được triển khai trong các môi trường huấn luyện phân tán, một thực hành
phổ biến để xử lý các mô hình và tập dữ liệu quy mô lớn. Chiến lược này gần như không phát sinh
chi phí bổ sung so với IMP, ngoại trừ bộ nhớ bổ sung cần thiết để lưu trữ các tham số được trung
bình. Thực sự, chúng tôi đưa chiến lược này vào thực hành trong các thí nghiệm ImageNet của
chúng tôi, và SWAMP đạt được kết quả xuất sắc trong khi phát sinh gần như cùng chi phí huấn
luyện như IMP (xem Bảng 3).

Giảm chi phí huấn luyện của SWAMP. Chiến lược song song hóa được đề cập ở trên chỉ áp dụng
được cho các thiết lập huấn luyện phân tán. Do đó, chúng tôi tiếp tục giới thiệu các phương pháp
thực tế để giảm chi phí huấn luyện của thuật toán SWAMP, có thể được sử dụng ngay cả trong các
môi trường huấn luyện không phân tán: (1) Sử dụng nhiều hạt chỉ trong chế độ thưa cao giảm thiểu
chi phí huấn luyện đáng kể chủ yếu gặp phải trong các chế độ thưa thấp. Bảng 7 chứng minh rằng
phương pháp này giảm chi phí huấn luyện bằng một hệ số từ 3 đến 4 với sự suy giảm hiệu suất
tối thiểu. Ở đây, chúng tôi bắt đầu huấn luyện với SWAMP một hạt cho mười chu kỳ IMP đầu
tiên, đạt được mức độ thưa 90%, và sau đó chuyển sang sử dụng bốn hạt sau đó. (2) Tăng tỷ lệ cắt
tỉa giảm số lượng chu kỳ cắt tỉa cần thiết để đạt được một mức độ thưa nhất định và do đó giảm
đáng kể tổng chi phí huấn luyện. Bảng 8 xác minh rằng SWAMP thành thạo trong việc cắt tỉa ngay
cả khi sử dụng tỷ lệ cắt tỉa cao hơn, làm nổi bật một lợi thế đặc biệt của SWAMP so với IMP.
Những phát hiện này, kết hợp với những phát hiện trong Bảng 1, phù hợp với Lemma 3.1 trong
Paul et al. (2023); một giá trị eigen Hessian nhỏ hơn, chỉ ra các cực tiểu phẳng hơn, tăng cường
tính mạnh mẽ đối với nhiễu SGD và làm cho việc khôi phục hiệu suất khớp trở nên có khả năng
hơn.

Mở rộng cho các nhiệm vụ ngôn ngữ và cắt tỉa động. Cho đến thời điểm này, chúng tôi đã chứng
minh rằng trung bình nhiều hạt có lợi cho IMP trong các nhiệm vụ phân loại hình ảnh. Tuy nhiên,
quan trọng là lưu ý rằng SWAMP có thể dễ dàng được áp dụng cho các kỹ thuật cắt tỉa khác nhau
qua một loạt các nhiệm vụ. Để làm rõ, chúng tôi trình bày hai phần mở rộng riêng biệt: (1) Bảng
9 tiếp tục xác nhận rằng SWAMP vượt trội hơn IMP trong các nhiệm vụ ngôn ngữ, trong đó chi
tiết thí nghiệm có sẵn trong Phụ lục C.2. (2) Chúng tôi cũng minh họa cách SWAMP có tiềm năng
tăng cường các phương pháp huấn luyện thưa động, cùng với kết quả bổ sung với RigL (Evci et
al., 2020) trong Phụ lục C.3.

5 KẾT LUẬN

Được truyền cảm hứng từ nghiên cứu trước đây về mối quan hệ giữa cắt tỉa độ lớn lặp và kết nối
chế độ tuyến tính, chúng tôi đã mở rộng tình huống một hạt để kết hợp nhiều hạt. Các phát hiện
thực nghiệm ban đầu của chúng tôi chứng minh rằng nhiều mô hình được huấn luyện với nhiễu
SGD khác nhau nhưng chia sẻ cùng vé khớp có thể được trung bình trọng số mà không gặp phải
rào cản mất mát. Chúng tôi tiếp tục quan sát rằng hạt được trung bình dẫn đến các cực tiểu phẳng
với hiệu suất tổng quát hóa cải thiện. Dựa trên những hiểu biết này, chúng tôi đã giới thiệu
SWAMP, một kỹ thuật cắt tỉa toàn cục lặp mới. Chúng tôi cũng đã thiết lập rằng SWAMP bảo tồn
kết nối tuyến tính giữa các nghiệm liên tiếp, một yếu tố quan trọng góp phần vào hiệu quả của
IMP. Các thí nghiệm rộng rãi cho thấy rằng SWAMP tạo ra các mặt nạ thưa vượt trội và hiệu quả
huấn luyện các mạng thưa so với các phương pháp baseline khác. Một phân tích lý thuyết khám
phá lý do tại sao vỏ lồi của các hạt trong không gian trọng số tạo thành một không gian con mất
mát thấp sẽ là một hướng có giá trị cho nghiên cứu tương lai. Tiếp theo, điều tra các nguyên tắc
cơ bản và tính chất toán học của vỏ lồi của các hạt nghiệm và mối quan hệ của nó với không gian
con mất mát thấp có thể cung cấp hiểu biết về hành vi và hiệu quả của thuật toán SWAMP.
